{"id": "FVvf69a5rx", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nWe introduce MOMENT, a family of open-source foundation models for general-purpose time series analysis. Pre-training large models on time series data is challenging due to (1) the absence of a large and cohesive public time series repository, and (2) diverse time series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time series, called the Time series Pile, and systematically tackle time series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data and task-specific fine-tuning. Finally, we present several interesting empirical observations about large pre-trained time series models. Pre-trained models (AutonLab/MOMENT-1-large) and Time Series Pile (AutonLab/Timeseries-PILE) are available on https://huggingface.co/AutonLab.\\n\\n1. Introduction\\nTime series analysis is an important field encompassing a wide range of applications ranging from forecasting weather patterns (Schneider & Dickinson, 1974) or detecting irregular heartbeats using Electrocardiograms (Goswami et al., 2021), to identifying anomalous software deployments (Xu et al., 2018). Due to its significant practical value and the unique challenges that modeling time series data poses, time series analysis continues to receive substantial interest from academia and industry alike. However, modeling such data typically requires substantial domain expertise, time, and task-specific design.\\n\\nLarge pre-trained language (Touvron et al., 2023; Devlin et al., 2019; Chung et al., 2022), vision (Li et al., 2023a), and video (Day et al., 2023) models, typically perform well on a variety of tasks on data from diverse domains, with little or no supervision, and they can be specialized to perform well on specific tasks. We unlock these key capabilities for time series data and release the first family of open-source large pre-trained time series models, which we call MOMENT.\\n\\nThe models in this family (1) serve as a building block for diverse time series analysis tasks (e.g., forecasting, classification, anomaly detection, and imputation, etc.), (2) are effective out-of-the-box, i.e., with no (or few) particular task-specific exemplars (enabling e.g., zero-shot forecasting, few-shot classification, etc.), and (3) are tunable using in-distribution and task-specific data to improve performance. MOMENT is a family of high-capacity transformer models, pre-trained using a masked time series prediction task on large amounts of time series data drawn from diverse domains. Below we summarize our key contributions.\\n\\nC1: Pre-training data. A key limiting factor for pre-training large time series models from scratch was the lack of a large and cohesive public time series repository.\"}"}
{"id": "FVvf69a5rx", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\nof a large cohesive public time series data repositories (Zhou et al., 2023; Gruver et al., 2023; Jin et al., 2023; Ekambaram et al., 2024; Cao et al., 2023). Therefore, we compiled The Time series Pile, a large collection of publicly available data from diverse domains, ranging from healthcare to engineering to finance. The Time Series Pile comprises of over 5 public time series databases, from several diverse domains for pre-training and evaluation (Tab. 11).\\n\\nC2: Multi-dataset pre-training.\\n\\nUnlike text and images, which have largely consistent sampling rates and number of channels, time series frequently vary in their temporal resolution, number of channels, lengths, and amplitudes, and sometimes have missing values. As a result, large-scale mixed dataset pre-training is largely unexplored. Instead, most methods are trained on a single dataset, and transferred across multiple datasets, but with modest success (Wu et al., 2023; Oreshkin et al., 2021; Narwariya et al., 2020).\\n\\nC3: Evaluation.\\n\\nHolistic benchmarks to evaluate time series foundation models on diverse datasets and tasks are in their nascent stages. Recent studies (Goswami et al., 2023b) have highlighted the importance of well-defined benchmarks and large-scale experimentation in order to accurately assess the impact and effectiveness of novel methodologies. To evaluate MOMENT, we build on the multi-task time series modeling benchmark first proposed by Wu et al. (2023) along multiple dimensions. For each of the 5 time series modeling tasks, namely, short- and long-horizon forecasting, classification, anomaly detection, and imputation we evaluate MOMENT against (1) both state-of-the-art deep learning as well as statistical baselines, on (2) more task-specific datasets, (3) using multiple evaluation metrics, (4) exclusively in limited supervision settings (e.g., zero-shot imputation, linear probing for forecasting, unsupervised representation learning for classification).\\n\\nFinally, we explore various properties of these pre-trained time series models. In particular, we study whether MOMENT is aware of intuitive time series characteristics such as frequency and trend, and the impact of initialization, model size scaling, and cross-modal transfer.\\n\\n2. Related Work\\n\\nTransformers and patching for time series modeling.\\n\\nThere is a growing body of work utilizing transformers for various time series analysis tasks (Wen et al., 2023). One issue with applying transformers to time series data is the complexity of the self-attention mechanism, which grows quadratically with the size of input tokens (or length of time series) (Li et al., 2019). Nie et al. (2023) demonstrated that treating time series sub-sequences (or patches) as tokens instead of individual time points is a simple, efficient, and effective mechanism for learning useful representations for forecasting. Drawing inspiration from prior work, we build on top of the transformer architecture which takes disjoint time series sub-sequences (or patches) as input.\\n\\nMasked Representation Learning.\\n\\nMasked pre-training is a widely-used self-supervised learning task where a model learns to accurately reconstruct masked portions of its input. Masked language (Devlin et al., 2019; Raffel et al., 2020) and image modeling (Xie et al., 2022; Li et al., 2023b) have been successfully utilized to learn models from vast quantities of unlabeled data, which can generalize to a variety of downstream tasks.\\n\\nFor time series data, prior work has primarily focused on contrastive representation learning (Yue et al., 2022; Eldele et al., 2021; Franceschi et al., 2019). However, contrastive learning relies on data augmentation, which is both subjective and data-dependent. In contrast, some studies mask portions of time series using zeros and learn a model to reconstruct them (Nie et al., 2023; Zerveas et al., 2021; Dong et al., 2023; Li et al., 2023c).\\n\\nRepresentation learning via masking is well-suited to all the downstream tasks we care about, especially forecasting and imputation, as they are instances of the masked reconstruction problem. Due to its simplicity and success in vision and language domains, we use the masked prediction task to pre-train our model, using a special embedding (see [MASK] in Fig. 3) to mask time series patches instead of zeros.\\n\\nCross-modal transfer learning using language models.\\n\\nLu et al. (2022) had first shown that transformers pre-trained on text data (LLMs) can effectively solve sequence modeling tasks in other modalities. Subsequently, Shen et al. (2023) introduced ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities by adapting to a target task via an align-then-refine workflow. Given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pre-training modality, then the pretrained model is fine-tuned on the embedded data, exploiting the knowledge shared across modalities. Some recent studies have leveraged this inherent ability of language pre-trained transformers to \\\"reprogram\\\" LLMs for time series analysis using parameter efficient fine-tuning and suitable tokenization strategies (Zhou et al., 2023; Gruver et al., 2023; Jin et al., 2023; Cao et al., 2023; Ekambaram et al., 2024). However, some of these models (Jin et al., 2023; Gruver et al., 2023) with billions of parameters demand significant memory and computational resources to perform well. We complement this line of research with three empirical observations (Sec 4.3): we show that (1) transformers trained on time series can also...\"}"}
{"id": "FVvf69a5rx", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\nFigure 2. Time Series Pile data splits. To avoid data contamination, we carefully partition all datasets into disjoint train, validation, and test splits. We adhere to the predefined splits provided by the creators of each dataset. In cases where such splits are unavailable, we randomly sample 60% of the data for training, 10% for validation, and 30% for testing. We only use the training splits of all datasets for pre-training.\\n\\nModel sequences across modalities, (2) during pre-training, randomly initializing weights lead to lower pre-training loss, than initializing with language modeling weights, and (3) models pre-trained on time series outperform LLM-based models such as (Zhou et al., 2023; Jin et al., 2023) on many tasks and datasets.\\n\\nUnanswered Questions. To the best of our knowledge, two questions remain largely unanswered in prior work on time series modeling. First, all existing time series models are (pre-)trained and fine-tuned on individual datasets (Nie et al., 2023; Yue et al., 2022; Wu et al., 2023; Zhou et al., 2023), and the benefits (or drawbacks) of large-scale multi-dataset pre-training remains unexplored (Wen et al., 2023). Second, there is very limited work on time series modeling in limited supervision settings, such as zero-shot forecasting (Oreshkin et al., 2021), or few-shot classification (Narwariya et al., 2020). In our work, we consider both these questions and show that pre-training a model of sufficient capacity on a large corpus of unlabeled time series data can in fact enable it to provide reasonably accurate predictions in limited-supervision settings.\\n\\n3. Methodology\\n\\nWe first collect a large number of public time series data into the Time Series Pile and then use it to pre-train a transformer model on the masked time series prediction task. We discuss each of these steps in the following sections.\\n\\n3.1. The Time Series Pile\\n\\nUnlike natural language processing and computer vision, where large-scale datasets such as The Pile (Gao et al., 2020), and ImageNet-1K (Russakovsky et al., 2015) are easily available for pre-training, public time series datasets are much smaller, scattered, and largely task-specific (Ma et al., 2023; Zhou et al., 2023; Gruver et al., 2023). To bridge this gap, we collate multiple time series from 4 task-specific, widely-used public repositories resulting in a large number of time series spanning diverse domains, and time series characteristics such as lengths, amplitudes, and temporal resolutions. We call this collection the Time Series Pile.\"}"}
{"id": "FVvf69a5rx", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\nThus ensuring that MOMENT only observes the training splits of datasets during pre-training.\\n\\n3.2. Model Architecture\\n\\n![Diagram of MOMENT model architecture]\\n\\nFigure 3. Overview of MOMENT. A time series is broken into disjoint fixed-length sub-sequences called patches, and each patch is mapped into a \\\\( D \\\\)-dimensional patch embedding. During pre-training, we mask patches uniformly at random by replacing their patch embeddings using a special mask embedding \\\\([MASK]\\\\). The goal of pre-training is to learn patch embeddings which can be used to reconstruct the input time series using a light-weight reconstruction head.\\n\\nMOMENT receives a univariate time series \\\\( T \\\\in \\\\mathbb{R}^{1 \\\\times T} \\\\), and a mask \\\\( M = \\\\{0, 1\\\\}^{1 \\\\times T} \\\\) of length \\\\( T \\\\). 0 and 1 denote unobserved and observed time-stamps, respectively. Reversible instance normalization (Kim et al., 2022) is applied to the observed time series before breaking it into \\\\( N \\\\) disjoint patches of length \\\\( P \\\\). Each patch is then mapped to a \\\\( D \\\\)-dimensional embedding, using a trainable linear projection if all time steps are observed, and a designated learnable mask embedding \\\\([MASK]\\\\) \\\\( \\\\in \\\\mathbb{R}^{1 \\\\times D} \\\\), otherwise. These \\\\( N \\\\) patch embeddings serve as input to the transformer model which retains their shape (1 \\\\( \\\\times \\\\) \\\\( D \\\\)) throughout its operations. Each transformed patch embedding is then used to reconstruct both masked and unmasked time series patches, using a lightweight prediction head. The goal of the prediction head is to map the transformed patch embeddings to the desired output dimensions. Since this particular prediction head enables time series reconstruction, we call it the reconstruction head. Fig. 3 shows an overview of our model.\\n\\nOur transformer encoder retains the modifications proposed by Raffel et al. (2020) to the original Transformer (Vaswani et al., 2017). Specifically, we remove the additive bias from the Layer Norm (Ba et al., 2016), and place it before the residual skip connections (He et al., 2016), and use the relation positional embedding scheme (Shaw et al., 2018).\\n\\nBelow we summarize the intuition behind some of our key design decisions.\\n\\nHandling varying time series characteristics. Time series vary in length, number of channels, amplitudes, and temporal resolutions. We address variable length by restricting MOMENT's input to a univariate time series of a fixed length \\\\( T = 512 \\\\). As is common practice, we sub-sample longer time series, and pad shorter ones with zeros on the left. Moreover, segmenting time series into patches quadratically reduces MOMENT's memory footprint and computational complexity, and linearly increases the length of time series it can take as input. We handle multi-variate time series by independently operating on each channel along the batch dimension. Like recent studies (Zhou et al., 2023; Nie et al., 2023), we found that modeling each channel independently is an effective strategy for modeling multivariate time series. Finally, re-scaling and centering time series using reversible instance normalization enables MOMENT to model time series with significantly different temporal distributions (Kim et al., 2022). We did not explicitly model the temporal resolution of time series, since this information is often unavailable outside of time series forecasting datasets.\\n\\nIntentionally simple encoder. Closely following the design of transformers in the language domain allows us to leverage their scalable and efficient implementations (e.g., gradient checkpointing, mixed precision training).\\n\\nLight-weight prediction head. We use a lightweight prediction head instead of a decoder of the same size as the encoder, to enable the necessary architectural modifications for task-specific fine-tuning of a limited number of trainable parameters while keeping the bulk of parameters and the high-level features learned by the encoder intact.\\n\\n3.3. Pre-training using Masked Time series Modeling\\n\\nWe pre-train MOMENT using the masked time series modeling task. Fig. 3 presents an overview of our pre-training procedure. During training, we first mask a small number of patches uniformly at random by replacing their patch embeddings with a learnable mask embedding \\\\([MASK]\\\\). The corrupted time series patches are then fed into the transformer encoder to learn patch representations, which are used to reconstruct the original time series using a lightweight reconstruction head. The pre-training objective is to minimize the masked reconstruction error i.e. the Mean Squared Error between the ground truth and the prediction, averaged over the masked patches.\\n\\nPre-training Setup. We pre-train three different sizes of MOMENT, roughly corresponding to the sizes of encoders in T5-Small, Base, and Large. Specifically, the Base (Small, Large) model uses a 12 (6, 24) layer Transformer with hidden dimensions of size \\\\( D = 768 \\\\) (512, 1024), 12 (8, 16) attention heads, and feed-forward networks of size \\\\( 2 \\\\). We found a large majority of classification datasets to have time series shorter than 512. Besides, a look-back window of length 512 was found to be sufficient for accurate long-horizon forecasting (Nie et al., 2023).\"}"}
{"id": "FVvf69a5rx", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\nTasks Supervision Datasets Metrics Baselines Experimental Setting\\n\\nLong-horizon Forecasting Linear Probing ETT-h1/h2/m1/m2, Electricity, Traffic, Weather, Exchange, ILIMSE, MAE\\n\\nTime-LLM, GPT4TS, TimesNet, PatchTST, FEDFormer, DLinear, N-BEATS, Stationary, LightTS\\n\\nLook-back window $L = 512$, Forecast horizon $H = \\\\{24, 60\\\\} ($ILI$), \\\\{96, 720\\\\} ($rest$)$\\n\\nShort-horizon Forecasting Zero-shot M3 and M4 competition datasets (subset) sMAPE\\n\\n3. GPT4TS, TimesNet, N-BEATS, AutoARIMA, AutoTheta, AutoETS, Seasonal Naive, Naive, Random Walk\\n\\nStatistical methods fit on individual time series. Deep learning methods are trained on a source dataset & evaluated on a target dataset of the same temporal resolution.\\n\\nClassification Unsupervised representation learning\\n\\nUCR Classification Archive Accuracy\\n\\nGPT4TS, TimesNet, TS2Vec, T-Loss, TNC, TS-TCC, TST, CNN, Encoder, FCN, MCNN, MLP, ResNet, t-LeNet, TWIESNDTW\\n\\nAll models except MOMENT were trained on each individual dataset. Quality of unsupervised representations measured using the accuracy of a SVM trained on them.\\n\\nAnomaly Detection Linear probing, Zero-shot UCR Anomaly Archive Adjusted Best F1 VUS-ROC\\n\\nGPT4TS, TimesNet, Anomaly Transformer, DGHL, Anomaly Nearest Neighbor\\n\\nReconstruction-based anomaly detection with window size $= 512$ MSE between observed and predicted time series is used as the anomaly criterion\\n\\nImputation Linear probing, Zero-shot ETT-h1/h2/m1/m2, Electricity, Weather MSE, MAE\\n\\nGPT4TS, TimesNet, Linear, Naive, Cubic Spline, Nearest Neighbors\\n\\nRandomly mask contiguous sub-sequences of length 8 Masking ratios: \\\\{12.5%, 25%, 37.5%, 50%\\\\}\\n\\nTable 1. Experimental benchmark. We evaluate MOMENT on 5 time series analysis tasks with an emphasis on limited memory, compute, and supervision settings.\\n\\n3072 (2048, 4096), resulting in approximately 125 (40, 385) million parameters. All weights are randomly initialized before pre-training. All models take an input time series of length $T = 512$, breaking it into $N = 64$ disjoint patches of length $P = 8$. We mask 30% of the patches uniformly at random during pre-training.\\n\\nWe use the Adam optimizer with weight decay (Loshchilov & Hutter, 2019) with $\\\\lambda = 0.05$, $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.999$. We clip the gradient at 5.0, train models using a batch size of 2048, and use cosine learning rate schedule with initial and final learning rates of $1 \\\\times 10^{-4}$ and $1 \\\\times 10^{-5}$, respectively. We use gradient checkpointing (Radford et al., 2021) to improve training throughput and save memory, and train all models in a mixed precision setting, using float-32 for numerically unstable operations, e.g. layer normalization, and bfloat-16, otherwise. We train all models for 2 epochs.\\n\\n3.4. Fine-tuning on Downstream Tasks\\n\\nMOMENT can be seamlessly used for multiple time series analysis tasks. In this work, we consider 5 practical time series analysis tasks as examples, namely: long- and short-horizon forecasting, classification, anomaly detection, and imputation. For forecasting tasks with horizon $H$, we replace the reconstruction head with a forecasting head, which first flattens all the $N \\\\times D$-dimensional patch embeddings into a $N \\\\times D$-dimensional vector, and then projects it into a $H$-dimensional time series via a linear projection layer. For all other tasks, we retain the reconstruction head. We provide detailed descriptions of each task and MOMENT's configuration in App. E.\\n\\nFine-tuning settings. MOMENT can either be fine-tuned end-to-end, or linear probed (MOMENT LP) by freezing all parameters except for those in the reconstruction or forecasting head. Additionally, for some tasks such as anomaly detection, unsupervised representation learning and imputation, MOMENT can also be used in a zero-shot (MOMENT 0) setting by retaining its reconstruction head.\\n\\n4. Experimental Setup and Results\\n\\nWe extend the experimental benchmark introduced by Wu et al. (2023) across various dimensions. Below, we outline the design choices of our benchmark and highlight its key distinctions from TimesNet.\\n\\nTime series modeling with limited supervision. Our benchmark comprises of 5 major time series modeling tasks of significant practical value, namely long- and short-horizon forecasting, imputation, classification, and anomaly detection, as outlined in Tab. 1. In contrast to TimesNet, we exclusively consider scenarios characterized by limited compute and supervision resources. These scenarios mimic practical situations where training (or fine-tuning) a deep neural network is infeasible due to resource limitations or insufficiently characterized data. Accordingly, we assess MOMENT in zero-shot settings whenever feasible and through linear probing for a few epochs otherwise.\\n\\nFor classification, we consider the unsupervised representation learning problem, where the goal is to learn representations of time series that are useful for downstream classification, without access to labeled data. As is common in prior work (Yue et al., 2022; Franceschi et al., 2019), the quality of representations is measured using the accuracy of a Support Vector Machine trained on them (App. E.2). For short-horizon forecasting, we consider the zero-shot setting introduced by Oreshkin et al. (2021). In particular, we fine-tune MOMENT LP on the Electricity dataset.\\n\\n5 In this section, we use TimesNet to refer to the benchmark proposed by Wu et al. (2023) instead of their model.\"}"}
{"id": "FVvf69a5rx", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3.\\n\\nZero-shot short-horizon forecasting performance on a subset of the M3 and M4 datasets measured using sMAPE. Statistical long-term forecasting performance measured using Mean Squared Error (MSE) and Mean Absolute Error (MAE). PatchTST methods.\\n\\n| Method     | MSE       | MAE       |\\n|------------|-----------|-----------|\\n| MOMENT     | 0.653     | 0.355     |\\n| GPT4TS     | 0.618     | 0.328     |\\n| N-BEATS    | 0.613     | 0.340     |\\n| Naive      | 0.613     | 0.386     |\\n| Random     | 0.601     | 0.382     |\\n| Naive Random| 0.615    | 0.391     |\\n| Naive Random| 0.230    | 0.333     |\\n| Naive Random| 0.213    | 0.316     |\\n| Naive Random| 0.207    | 0.307     |\\n| Naive Random| 0.728    | 1.985     |\\n| Naive Random| 0.730    | 1.982     |\\n| Naive Random| 0.631    | 1.902     |\\n| Naive Random| 0.831    | 2.144     |\\n| Naive Random| 0.675    | 0.587     |\\n| Naive Random| 0.442    | 0.466     |\\n| Naive Random| 0.311    | 0.382     |\\n| Naive Random| 0.209    | 0.308     |\\n| Naive Random| 0.527    | 0.502     |\\n| Naive Random| 0.438    | 0.438     |\\n| Naive Random| 0.400    | 0.407     |\\n| Naive Random| 0.374    | 0.400     |\\n| Naive Random| 0.863    | 0.672     |\\n| Naive Random| 0.626    | 0.559     |\\n| Naive Random| 0.520    | 0.504     |\\n| Naive Random| 0.397    | 0.437     |\\n| Naive Random| 0.547    | 0.533     |\\n| Naive Random| 0.518    | 0.488     |\\n| Naive Random| 0.475    | 0.462     |\\n| Naive Random| 0.424    | 0.432     |\\n| Naive Random| 0.331    | 0.359     |\\n| Naive Random| 0.258    | 0.311     |\\n| Naive Random| 0.199    | 0.260     |\\n| Naive Random| 0.152    | 0.210     |\\n\\nMOMENT: A Family of Open Time-series Foundation Models\"}"}
{"id": "FVvf69a5rx", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\nSequential nature of time series. Instead, we measure anomaly detection performance with the widely used adjusted best $F_1$ score (Goswami et al., 2023a; Challu et al., 2022), and the recently proposed VUS-ROC (Paparrizos et al., 2022a).\\n\\nBaselines. We compare MOMENT with state-of-the-art deep learning and statistical machine learning models across tasks (Tab. 35). This is in contrast to TimesNet which primarily compared with transformer-based approaches. These comparisons are crucial for assessing the practical utility of the proposed methods. We found that statistical and non-transformer-based approaches like ARIMA for short-horizon forecasting, N-BEATS for long-horizon forecasting, and $k$-nearest neighbors for anomaly detection outperform many deep and transformer-based models.\\n\\nHyper-parameter tuning. We do not perform hyper-parameter tuning. In all experiments that follow, unless mentioned otherwise, we fine-tune MOMENT-Large with a batch size of 64, and one cycle learning rate schedule with a peak learning rate between $5 \\\\times 10^{-5}$ and $1 \\\\times 10^{-3}$ (Smith & Topin, 2019). For baseline methods, we capture recommended settings from their papers and public repositories. We report all hyper-parameters settings for MOMENT and baselines in App. E.\\n\\nResearch questions. Through the following experiments we aim to answer 3 broad research questions.\\n\\nRQ1: Effectiveness. Is MOMENT effective for multiple time series analysis tasks in limited supervision settings?\\n\\nRQ2: Interpretability. What is MOMENT learning? Does it capture intuitive time series characteristics such as varying frequencies, trends, and amplitudes?\\n\\nRQ3: Properties. What is the impact of the size of scaling model size? Can MOMENT, akin to LLMs, be used for cross-modal transfer learning?\\n\\n4.1. MOMENT can solve multiple time series modeling tasks in limited supervision settings\\n\\nLong-horizon forecasting. Linearly probing MOMENT achieves near state-of-the-art performance on most datasets and horizons, and is only second to PatchTST which generally achieves the lowest MSE (Tab. 2). On many datasets and horizons, forecasting models based on LLMs\u2013TimeLLM and GPT4TS perform worse than MOMENT. Notably, N-BEATS outperforms several recent methods, emphasizing the importance of comparing forecasting performance beyond transformer-based approaches.\\n\\nZero-shot short-horizon forecasting. Among all tasks, we found zero-shot short-horizon forecasting to have the largest scope for improvement (Tab. 3). Statistical methods such as Theta and ETS outperformed their deeper counterparts. However, on some datasets, MOMENT achieved lower sMAPE than ARIMA.\\n\\nClassification. Without any data-specific fine-tuning, MOMENT can learn distinct representations for different classes of data (Fig. 5), and an SVM trained on its representations performs better than all but 4 methods specifically built for time series classification models and trained on each individual dataset. Recently proposed GPT4TS and TimesNet perform poorly despite being trained on each individual dataset with labels.\\n\\nAnomaly detection. On 44 time series from the UCR anomaly detection archive, MOMENT consistently outperformed both TimesNet and GPT4TS, as well as 2 state-of-the-art deep learning models tailored for anomaly detection, in both zero-shot and linear probing configurations. However, $k$-nearest neighbors performed marginally better in terms of VUS-ROC score, but had a lower adjusted best $F_1$ score.\\n\\nImputation. Tab. 6 contains imputation performance of all models averaged over 4 different masking rates. MOMENT with linear probing achieved the lowest reconstruction error on all ETT datasets. In the zero-shot setting, MOMENT consistently outperformed all statistical interpolation methods with the exception of linear interpolation.\\n\\n4.2. What is MOMENT Learning?\\n\\nWe found that MOMENT can capture changes in intuitive time series characteristics such as trend, amplitude, frequencies, and phases of time series. However, it cannot differentiate between vertically shifted time series as it normalizes each signal prior to modeling (Fig. 4, 7). Furthermore, on many classification datasets, MOMENT learns distinct representations of different classes, even in a zero-shot setting without access to labels (Fig. 5, 8).\\n\\n4.3. Properties of Large Time Series Models\\n\\nModel scaling improves training loss. Like LLMs, we found that increasing the size of the model leads to lower training loss, even before the first epoch (Fig. 6, left). An immediate next step is to assess how effectively this phenomenon extends to time series modeling tasks under limited supervision.\\n\\nMOMENT can solve cross-modal sequence learning tasks. Lu et al. (2022) first showed that large pre-trained language and vision transformers can solve general sequence learning tasks for modalities outside of text and images with minimal fine-tuning. Several recent studies have leveraged these properties to reprogram LLMs for time series tasks. We explore whether transformers pre-trained on time series can also be used to solve sequence classification tasks on image, text, and binary data. Our results confirm that by freezing 7\"}"}
{"id": "FVvf69a5rx", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\n| Model     | Bit Memory | MNIST     | CIFAR-10   | IMDb       |\\n|-----------|------------|-----------|------------|------------|\\n| GPT-2     | 1.000      | 0.975     | 0.711      | 0.867      |\\n| Flan-T5   | 1.000      | 0.987     | 0.672      | 0.861      |\\n| MOMENT    | 1.000      | 0.982     | 0.620      | 0.872      |\\n\\nTable 5. Cross-modal transfer experiments. Accuracy measured on the test set, from the checkpoint with the lowest train loss. Even with frozen self-attention and feed-forward layers, MOMENT is able to model cross-modal sequences on par with GPT-2 and Flan-T5 models of similar scale. MOMENT with randomly initialized weights converges to a lower training loss. Our observations suggest that with sufficient data, pre-training our model from scratch results in a lower training loss than continually pre-training a model of similar size initialized with language modeling weights (Fig. 6, 12). This also underscores that there is sufficient publicly accessible pre-training data available in the Time Series Pile to facilitate pre-training time series foundation models from scratch.\"}"}
{"id": "FVvf69a5rx", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. Imputation Results. MOMENT with linear probing achieved the lowest reconstruction error on all ETT datasets. In the zero-shot setting, MOMENT consistently outperformed all statistical interpolation methods with the exception of linear interpolation. Complete results in Tab. 29.\\n\\n| Metric                  | MOMENT | MOMENT LP | GPT4TS | TimesNet | Naive | Linear | Nearest | Cubic |\\n|-------------------------|--------|-----------|--------|----------|-------|--------|---------|-------|\\n| MSE                     | 0.082  | 0.031     | 0.036  | 0.119    | 0.065 | 0.083  | 0.601   | 0.153 |\\n| MAE                     | 0.130  | 0.071     | 0.098  | 0.108    | 0.067 | 0.078  | 0.153   | 0.601 |\\n| ETTh1                   | 0.402  | 0.227     | 0.225  | 1.185    | 0.175 | 0.264  | 1.920   | 0.579 |\\n| ETTh2                   | 0.125  | 0.109     | 0.170  | 0.225    | 0.135 | 0.234  | 0.166   | 0.252 |\\n| ETTm1                   | 0.202  | 0.076     | 0.087  | 0.455    | 0.165 | 0.229  | 0.230   | 0.260 |\\n| ETTm2                   | 0.078  | 0.052     | 0.112  | 0.113    | 0.062 | 0.138  | 0.079   | 0.152 |\\n| Electricity             | 0.250  | 0.124     | 0.123  | 1.474    | 0.455 | 0.365  | 0.858   | 0.494 |\\n\\nTable 7. Anomaly detection performance averaged over 248 time series from the UCR Anomaly Archive. MOMENT LP achieves near state-of-the-art anomaly detection results. Complete results in Tab. 22.\"}"}
{"id": "FVvf69a5rx", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task                  | Method Type          | Reimplementation/ Rerun | Source                                                                 |\\n|-----------------------|----------------------|-------------------------|----------------------------------------------------------------------|\\n| Long-horizon Forecasting | Time-LLM LLM-based   | \u2713                       | Time-LLM, GPT4TS, PatchTST, Fedformer, Autoformer, Stationary, ETSformer, LightTS, Informer, Reformer Transformer-based |\\n|                       |                      | \u00d7                       | One Fits All, Pyraformer, LogTrans                                   |\\n|                       |                      | \u00d7                       | TimesNet, DLinear Deep learning                                      |\\n|                       |                      | \u2713                       | One Fits All, N-BEATS                                                |\\n| Short-horizon Forecasting | GPT4TS LLM-based   | \u2713                       | N-BEATS                                                              |\\n|                       |                      | \u00d7                       | TimesNet                                                             |\\n|                       |                      | \u2713                       | One Fits All, AutoARIMA, AutoTheta, AutoETS, Seasonal Naive, Naive, Random Walk Statistical learning |\\n| Classification        | GPT4TS LLM-based     | \u2713                       | One Fits All, TimesNet                                              |\\n|                       |                      | \u00d7                       | TS2Vec, T-Loss, TNC, TS-TCC, TST Unsupervised Representation learning |\\n|                       |                      | \u00d7                       | CNN, Encoder, FCN, MCNN, ResNet, t-LeNet, TWIESN                    |\\n|                       |                      | \u00d7                       | DL4TSC Repository                                                   |\\n|                       |                      | \u00d7                       | DTW                                                                   |\\n| Anomaly Detection     | GPT4TS LLM-based     | \u2713                       | One Fits All, TimesNet                                              |\\n|                       |                      | \u00d7                       | Anomaly Transformer Transformer-based                              |\\n|                       |                      | \u2713                       | DGHL                                                                  |\\n| Time Series Model Selection | k-NN Statistical learning | \u2713                       | Time Series Model Selection                                         |\\n| Imputation            | GPT4TS LLM-based     | \u2713                       | One Fits All, TimesNet                                              |\\n|                       |                      | \u00d7                       | PatchTST, ETSformer, LightTS, Fedformer, Stationary, Autoformer, Informer, Reformer Transformer-based |\\n|                       |                      | \u00d7                       | DLinear Deep learning                                               |\\n|                       |                      | \u00d7                       | Naive                                                                |\\n|                       |                      | \u2713                       | Pandas FFill, Pandas BFill, Linear, Nearest, Cubic                   |\\n|                       |                      | \u00d7                       | Scipy Interp1D                                                       |\\n\\nTable 35. Source for the results for each baseline for all downstream task.\"}"}
{"id": "FVvf69a5rx", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\nknowledge the insightful exchanges with Yuyang (Bernie) Wang, Abdul Fatir Ansari, Ingo Guering, Xiyuan Zhang, and Anoop Deoras. Special thanks to Cherie Ho for suggesting a creative and befitting name for our model. Lastly, we would like to thank Cecilia Morales for her insightful comments, especially on the broader impacts of this work, and for helping us proofread this manuscript. Finally, we would like to thank the anonymous ICML reviewers for their comments which helped improve our work.\\n\\nData.\\nWe extend our gratitude to the authors and data curators whose meticulous efforts were instrumental in curating the datasets utilized for both pre-training and evaluation purposes: UCR Time Series Classification Archive (Dau et al., 2018), TSB-UAD Anomaly Benchmark (Paparrizos et al., 2022b), Monash Forecasting Archive (Godahewa et al., 2021), and the long-horizon forecasting datasets (Zhou et al., 2021).\\n\\nSoftware and Models.\\nOur training and evaluation library was inspired from Time-Series-Library. We would also like to thank the authors of the following libraries for their implementations: universal-computation, Anomaly-Transformer, VUS, tsad-model-selection, One-Fits-All and Statsforecast (Garza et al., 2022).\\n\\nReproducibility statement\\nAll models were trained and evaluated on a computing cluster consisting of 128 AMD EPYC 7502 CPUs, 503 GB of RAM, and 8 NVIDIA RTX A6000 GPUs each with 49 GiB RAM. All MOMENT variants were trained on a single A6000 GPU (with any data or model parallelism). We have made MOMENT-large and the Time Series Pile publically available on Huggingface. We are working on open-sourcing MOMENT-base and MOMENT-small, and our research code public. The latter is currently available anonymously at https://github.com/moment-timeseries-foundation-model/moment-research. We enlist an exhaustive list of hyper-parameters in App. E to aid reproducibility. We would like to emphasize that all datasets used in this study are publicly available.\\n\\nImpact statement\\nTransparency Index.\\nGiven the exponential rise in societal reliance on large foundation models, ensuring transparency in their training approach, architecture, and downstream application is crucial for public accountability, scientific advancement, and effective governance. To uphold this objective, we publicly release our training code base, data sources, and evaluation pipeline. We assess the transparency of MOMENT using the criteria outlined by Bommasani et al. (2023), focusing on upstream resources utilized during training and model description, encompassing 32 and 33 transparency indicators, respectively. We report expected upstream and model transparency scores for MOMENT in Tab. 34. Notably, MOMENT is expected to have one of the highest levels of upstream transparency. However, its model transparency scores are lower, primarily due to comprehensive (external and third-party) harm and trustworthiness evaluations, which are not well understood in the context of time series modeling.\\n\\nEnvironmental Impact.\\nWe train multiple models over many days resulting in significant energy usage and a sizeable carbon footprint. However, we hope that releasing our models will ensure that future time series modeling efforts are quicker and more efficient, resulting in lower carbon emissions. We follow prior work (Bender et al., 2021; Patterson et al., 2021; Touvron et al., 2023; Wu et al., 2022; Dodge et al., 2022) and estimate the carbon footprint of pre-training all variants of MOMENT based on the GPU device used and the carbon efficiency of the electricity grid. Our estimated CO2 generation estimates are shown in Tab. 8. We use the Total Graphics Power (TGP) to calculate the total power consumed for training MOMENT models, although the total power consumed by the GPU will likely vary a little based on the GPU utilization while training our model. Our calculations do not account for power demands from other sources of our compute. We use 336.566 Kg CO2/MWH as the standard value of CO2 emission per megawatt hour of energy consumed for Pittsburgh.\\n\\nWe share an upper limit of the individual CO2 emission for each model, as well as a more realistic actual estimate for the carbon emissions from MOMENT-small and MOMENT-base, since they were trained simultaneously on a single Nvidia RTX A6000 GPU, and thus the power consumed by the GPU was shared for the training of both variants. MOMENT-large was trained independently on a single RTX A6000 GPU.\\n\\nEthical considerations and potential misuse.\\nDespite MOMENT\u2019s promising performance in limited-data settings, it is important to use its predictions with care, especially in high-stakes settings such as healthcare. Before MOMENT is used for high-stakes decision-making, we recommend fine-tuning and evaluating the model with task-specific in-domain data.\"}"}
{"id": "FVvf69a5rx", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8. Total carbon emission induced upon training the MOMENT family of models. MOMENT-small and MOMENT-base were trained simultaneously on a single GPU, thus the TGP required for each model would likely be much less than 300W, and the total time for both models combined is equal to the maximum of the time required for each model. Actual total power consumption and carbon emission values account for this.\\n\\nReferences\\n\\nAnsari, A. F., Stella, L., Turkmen, C., Zhang, X., Mercado, P., Shen, H., Shchur, O., Rangapuram, S. S., Arango, S. P., Kapoor, S., et al. Chronos: Learning the language of time series. arXiv preprint arXiv:2403.07815, 2024.\\n\\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization, 2016.\\n\\nBao, H., Dong, L., Piao, S., and Wei, F. BEit: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=p-BhZSz59o4.\\n\\nBender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, pp. 610\u2013623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.\\n\\nBommasani, R., Klyman, K., Longpre, S., Kapoor, S., Maslej, N., Xiong, B., Zhang, D., and Liang, P. The foundation model transparency index, 2023.\\n\\nCai, Y., Goswami, M., Choudhry, A., Srinivasan, A., and Dubrawski, A. Jolt: Jointly learned representations of language and time-series. In Deep Generative Models for Health Workshop NeurIPS 2023, 2023.\\n\\nCalifornia Department of Transportation. Performance measurement system (pems), 2024. URL http://pems.dot.ca.gov/. Accessed: 2024-02-01.\\n\\nCao, D., Jia, F., Arik, S. O., Pfister, T., Zheng, Y., Ye, W., and Liu, Y. Tempo: Prompt-based generative pre-trained transformer for time series forecasting, 2023.\\n\\nCenters for Disease Control and Prevention. Fluview: Flu activity & surveillance, 2024. URL https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html. Accessed: 2024-02-01.\\n\\nChallu, C., Olivares, K. G., Oreshkin, B. N., Garza Ramirez, F., Mergenthaler Canseco, M., and Dubrawski, A. NHITS: Neural hierarchical interpolation for time series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 37(6):6989\u20136997, Jun. 2023. doi: 10.1609/aaai.v37i6.25854. URL https://ojs.aaai.org/index.php/AAAI/article/view/25854.\\n\\nChallu, C. I., Jiang, P., Nian Wu, Y., and Callot, L. Deep generative model with hierarchical latent factors for time series anomaly detection. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I. (eds.), Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pp. 1643\u20131654. PMLR, 28\u201330 Mar 2022. URL https://proceedings.mlr.press/v151/challu22a.html.\\n\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\\n\\nCui, Z., Chen, W., and Chen, Y. Multi-scale convolutional neural networks for time series classification, 2016.\\n\\nDas, A., Kong, W., Sen, R., and Zhou, Y. A decoder-only foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688, 2023.\\n\\nDau, H. A., Keogh, E., Kamgar, K., Yeh, C.-C. M., Zhu, Y., Gharghabi, S., Ratanamahatana, C. A., Yanping, Hu, B., Begum, N., Bagnall, A., Mueen, A., Batista, G., and Hexagon-ML. The ucr time series classification archive, October 2018. https://www.cs.ucr. edu/eamonn/time_series_data_2018/.\\n\\nDay, K., Christl, D., Salvi, R., and Sriram, P. Video pre-trained transformer: A multimodal mixture of pre-trained experts, 2023.\"}"}
{"id": "FVvf69a5rx", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "FVvf69a5rx", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\nHundman, K., Constantinou, V., Laporte, C., Colwell, I., and Soderstrom, T. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 387\u2013395, 2018.\\n\\nIsmail Fawaz, H., Forestier, G., Weber, J., Idoumghar, L., and Muller, P.-A. Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33(4):917\u2013963, 2019.\\n\\nJin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., and Wen, Q. Time-llm: Time series forecasting by reprogramming large language models, 2023.\\n\\nKim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=cGDAkQo1C0p.\\n\\nLai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling long- and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR \u201918, pp. 95\u2013104, New York, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450356572. doi: 10.1145/3209978.3210006. URL https://doi.org/10.1145/3209978.3210006.\\n\\nLe Guennec, A., Malinowski, S., and Tavenard, R. Data Augmentation for Time Series Classification using Convolutional Neural Networks. In ECML/PKDD Workshop on Advanced Analytics and Learning on Temporal Data, Riva Del Garda, Italy, September 2016. URL https://shs.hal.science/halshs-01357973.\\n\\nLi, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023a.\\n\\nLi, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.-X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in neural information processing systems, 32, 2019.\\n\\nLi, Y., Fan, H., Hu, R., Feichtenhofer, C., and He, K. Scaling language-image pre-training via masking. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 23390\u201323400, Los Alamitos, CA, USA, jun 2023b. IEEE Computer Society. doi: 10.1109/CVPR52729.2023.02240. URL https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.02240.\\n\\nLi, Z., Rao, Z., Pan, L., Wang, P., and Xu, Z. Ti-mae: Self-supervised masked time series autoencoders, 2023c.\\n\\nLiu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and Dustdar, S. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0EXmFzUn5I.\\n\\nLiu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023.\\n\\nLoshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\\n\\nLu, K., Grover, A., Abbeel, P., and Mordatch, I. Frozen pre-trained transformers as universal computation engines. Proceedings of the AAAI Conference on Artificial Intelligence, 36(7):7628\u20137636, Jun. 2022. doi: 10.1609/aaai.v36i7.20729. URL https://ojs.aaai.org/index.php/AAAI/article/view/20729.\\n\\nMa, Q., Liu, Z., Zheng, Z., Huang, Z., Zhu, S., Yu, Z., and Kwok, J. T. A survey on time-series pre-trained models, 2023.\\n\\nMax Planck Institute for Biogeochemistry. Weather data, 2024. URL https://www.bgc-jena.mpg.de/wetter/. Accessed: 2024-02-01.\\n\\nNarwariya, J., Malhotra, P., Vig, L., Shroff, G., and Vishnu, T. V. Meta-learning for few-shot time series classification. In Proceedings of the 7th ACM IKDD CoDS and 25th COMAD, CoDS COMAD 2020, pp. 28\u201336, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450377386. doi: 10.1145/3371158.3371162. URL https://doi.org/10.1145/3371158.3371162.\\n\\nNie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. A time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Jbdc0vTOcol.\\n\\nOreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=r1ecqn4YwB.\"}"}
{"id": "FVvf69a5rx", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 9.\\n\\nTime series foundation modeling has a young but growing literature. Latest ArXiv pre-prints of most contemporary work are from February 2024 with the exception of TimeGPT-1. None of the contemporary works have evaluated their foundation models in terms of cross modal transfer, environmental impact and transparency. ++ denotes that the Time Series Pile is a living database, is still growing in size and diversity. Moirai (Woo et al., 2024) uses a mixture of Student\u2019s $t$, log-normal, negative binomial, and low variance normal distribution.\\n\\n#### B. Contemporary Work\\n\\n#### C. Interesting directions for future work\\n\\nWe note some interesting directions for future work:\\n\\n- Study the impact of design choices such as the impact of the choice of the loss function (Huber, $L_1$, $L_2$), patch length ($4, 8$), and masking percentage ($0.3, 0.6$) on pre-training loss and time series modeling performance.\\n\\n- Pre-training data. Two interesting directions include using augmentation and synthetic data (Ansari et al., 2024) to improve the quality of pre-training, and looking at tuning dataset mixtures in the Time Series Pile.\"}"}
{"id": "FVvf69a5rx", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We compiled a large collection of publicly available datasets from diverse domains into the Time Series Pile. It has 13 unique domains of data, which includes 20.085 GB worth of 13M unique time series and 1.23 billion timestamps (including channels).\\n\\n### Unique Domains\\n\\n| Domain       | Examples                          |\\n|--------------|-----------------------------------|\\n| Healthcare   | ECG, EEG, Hospital                |\\n| Human Body   | Tongue movement, Finger movement, Muscle Signals |\\n| Nature       | Fish outlines, Flower outlines, River flow |\\n| Audio        | Arabic speech, Japanese Speech, Phonetics |\\n| Power        | Power consumption, Electricity, Home appliance usage |\\n| Economics    | Exchange Rate, Bitcoin, Tourism   |\\n| Traffic      | Road Traffic, Pedestrian cross, Line occupancy rate |\\n| Weather      | Temperature, Rain, Wind           |\\n| Facilities   | Machine Status, Spacecraft Status, Web traffic |\\n| Web Services | IOPS, NAB                         |\\n| Synthetic    | MGAB                              |\\n| Sensors      | NASA MSL, NASA SMAP               |\\n| Gait         | Daphnet                           |\\n\\n### Table 10.\\n\\n| Task          | Dataset | Channels | Series Length | Frequency/Number of Classes | Data Size (Train, Val, Test) |\\n|---------------|---------|----------|---------------|-----------------------------|------------------------------|\\n| Long horizon  |         |          |               |                             |                              |\\n| Forecasting   |         |          |               |                             |                              |\\n| Informer      | ETTm1, ETTm2 | 7        | {96, 720}     |                             | (33953, 11425, 11425)        |\\n|               | ETTh1, ETTh2 | 7        |               |                             | (8033, 2785, 2785)           |\\n|               | Electricity | 321      |               |                             | (17805, 2537, 5165)          |\\n|               | Traffic    | 862      |               |                             | (11673, 1661, 3413)          |\\n|               | Weather    | 21       |               |                             | (36280, 5175, 10444)         |\\n|               | Exchange   | 8        |               |                             | (4704, 665, 1422)            |\\n|               | ILI        | 7        | {24, 60}      |                             | (69, 2, 98)                  |\\n| Short horizon |         |          |               |                             |                              |\\n| Forecasting   |         |          |               |                             |                              |\\n| Monash        | M4-Yearly | 1        |               |                             | (16099, 2301, 4600)          |\\n|               | M4-Quarterly | 8       |               |                             | (16800, 2400, 4800)          |\\n|               | M4-Monthly | 18       |               |                             | (33600, 4800, 9600)          |\\n|               | M3-Yearly | 6        |               |                             | (451, 65, 129)               |\\n|               | M3-Quarterly | 8      |               |                             | (529, 76, 151)               |\\n|               | M3-Monthly | 18       |               |                             | (999, 144, 285)              |\\n| Imputation    |         |          |               |                             |                              |\\n| Informer      | ETTm1, ETTm2 | 7        |               |                             | (33953, 11425, 11425)        |\\n|               | ETTh1, ETTh2 | 7        |               |                             | (8033, 2785, 2785)           |\\n|               | Electricity | 321      |               |                             | (17805, 2537, 5165)          |\\n|               | Weather    | 21       |               |                             | (36280, 5175, 10444)         |\\n| Classification|         |          |               |                             |                              |\\n| UCR          | UWaveGestureLibraryX | 1       | 315           |                             | (640, 256, 3582)            |\\n|               | ECG5000    | 140      |               |                             | (357, 143, 4500)            |\\n|               | OSULeaf    | 427      |               |                             | (142, 58, 242)              |\\n|               | MedicalImages | 99     |               |                             | (272, 109, 760)             |\\n|               | Ham        | 431      |               |                             | (77, 32, 105)               |\\n| Anomaly detection |         |          |               |                             |                              |\\n| TSB-UAD      | 1sddb40   | 1        |               |                             | (24489, 9489, 3969)         |\\n|              | BIDMC1     |          |               |                             | (1274, 204, 7988)           |\\n|              | CIMIS44AirTemperature3 | 1  |               |                             | (2346, 632, 3672)           |\\n|              | CIMIS44AirTemperature5 | 1  |               |                             | (2346, 632, 3672)           |\\n|              | ECG2       |          |               |                             | (10203, 3775, 14488)        |\\n\\nTable 11.\\n\\nA brief description of datasets that collectively make the Time Series Pile. Due to space constraints, we only include metadata for the subsets of the M3 and M4 datasets in our experiments, as well as 5 classification and anomaly detection datasets. Characteristics of all short-horizon forecasting, classification and anomaly detection datasets in the Time Series Pile can be found in our official repository, and Monash archive, UCR/UEA classification archive, and TSB-UAD anomaly benchmark, respectively.\"}"}
{"id": "FVvf69a5rx", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset       | Domain        | # time-series | # Classes | # Channels | Time Series Length | # Time Series (w. channels) | Total Observations |\\n|--------------|---------------|---------------|-----------|------------|--------------------|---------------------------|-------------------|\\n| SemgHandGenderCh2 | Human Body  | 900           | 2         | 1          | 1500               | 900                        | 1350000           |\\n| GestureMidAirD2   | Human Body  | 338           | 26        | 1          | 360                | 338                        | 121680            |\\n| UWaveGestureLibraryAll | Human Body | 4478          | 8         | 1          | 945                | 4478                       | 4231710           |\\n| SelfRegulationSCP1 | Healthcare | 561           | 2         | 6          | 896                | 3366                       | 3015936           |\\n| UWaveGestureLibraryX | Human Body | 4478          | 8         | 1          | 315                | 4478                       | 1410570           |\\n| GesturePebbleZ2   | Human Body  | 304           | 6         | 1          | 455                | 304                        | 138320            |\\n| Healthcare5000    | Healthcare  | 5000          | 5         | 1          | 140                | 5000                       | 700000            |\\n| OSULeaf           | Nature       | 442           | 6         | 1          | 427                | 442                        | 188734            |\\n| MedicalImages     | Healthcare  | 1141          | 10        | 1          | 99                 | 1141                       | 112959            |\\n| Haptics           | Human Body  | 463           | 5         | 1          | 1092               | 463                        | 505596            |\\n| LargeKitchenAppliances | Power | 750           | 3         | 1          | 720                | 750                        | 540000            |\\n| JapaneseVowels    | Audio        | 640           | 9         | 12         | 26                 | 7680                       | 199680            |\\n| Worms             | Nature       | 258           | 5         | 1          | 900                | 258                        | 232200            |\\n| Ham               | Facilities  | 214           | 2         | 1          | 431                | 214                        | 92234             |\\n| DistalPhalanxTW    | Nature       | 539           | 6         | 1          | 80                 | 539                        | 43120             |\\n| ProximalPhalanxOutlineCorrect | Nature | 891           | 2         | 1          | 80                 | 891                        | 71280             |\\n| SemgHandMovementCh2 | Human Body | 900           | 6         | 1          | 1500               | 900                        | 1350000           |\\n| RefrigerationDevices | Power | 750           | 3         | 1          | 720                | 750                        | 540000            |\\n| FreezerRegularTrain | Facilities | 3000          | 2         | 1          | 301                | 3000                       | 903000            |\\n| PigAirwayPressure | Nature       | 312           | 52        | 1          | 2000               | 312                        | 624000            |\\n| TwoLeadECG        | Healthcare  | 1162          | 2         | 1          | 82                 | 1162                       | 95284             |\\n| GunPointMaleVersusFemale | Human Body | 451           | 2         | 1          | 150                | 451                        | 67650             |\\n| Trace             | Power        | 200           | 4         | 1          | 275                | 200                        | 55000             |\\n| SmoothSubspace    | Generated    | 300           | 3         | 1          | 15                 | 300                        | 4500              |\\n| MiddlePhalanxTW    | Nature       | 553           | 6         | 1          | 80                 | 553                        | 44240             |\\n| AtrialFibrillation | Healthcare | 30            | 3         | 2          | 640                | 60                         | 38400             |\\n| SyntheticControl | Generated    | 600           | 6         | 1          | 60                 | 600                        | 36000             |\\n| ShapesAll         | Generated    | 1200          | 60        | 1          | 512                | 1200                       | 614400            |\\n| Human BodyVerticalSignal | Human Body | 724           | 12        | 1          | 1250               | 724                        | 905000            |\\n| PLAID             | Facilities   | 1074          | 11        | 1          | 1344               | 1074                       | 1443456           |\\n| AllGestureWiimoteX | Human Body  | 1000          | 10        | 1          | 385                | 1000                       | 385000            |\\n| Heartbeat         | Healthcare  | 409           | 2         | 61         | 405                | 24949                       | 10104345          |\\n| Wafer             | Facilities  | 7164          | 2         | 1          | 152                | 7164                       | 1088928           |\\n| FaceFour          | Generated    | 112           | 4         | 1          | 350                | 112                        | 39200             |\\n| Phoneme           | Audio        | 2110          | 39        | 1          | 1024               | 2110                       | 2160640           |\\n| InlineSkate       | Human Body  | 650           | 7         | 1          | 1882               | 650                        | 1223300           |\"}"}
{"id": "FVvf69a5rx", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "FVvf69a5rx", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\nOreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. Meta-learning framework with applications to zero-shot time-series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 35(10):9242\u20139250, May 2021. doi: 10.1609/aaai.v35i10.17115. URL https://ojs.aaai.org/index.php/AAAI/article/view/17115.\\n\\nPaparrizos, J., Boniol, P., Palpanas, T., Tsay, R. S., Elmore, A., and Franklin, M. J. Volume under the surface: A new accuracy evaluation measure for time-series anomaly detection. Proc. VLDB Endow., 15(11):2774\u20132787, jul 2022a. ISSN 2150-8097. doi: 10.14778/3551793.3551830. URL https://doi.org/10.14778/3551793.3551830.\\n\\nPaparrizos, J., Kang, Y., Boniol, P., Tsay, R. S., Palpanas, T., and Franklin, M. J. Tsb-uad: An end-to-end benchmark suite for univariate time-series anomaly detection. Proc. VLDB Endow., 15(8):1697\u20131711, apr 2022b. ISSN 2150-8097. doi: 10.14778/3529337.3529354. URL https://doi.org/10.14778/3529337.3529354.\\n\\nPatterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D., So, D., Texier, M., and Dean, J. Carbon emissions and large neural network training, 2021.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8748\u20138763. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/radford21a.html.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.\\n\\nRamaswamy, S., Rastogi, R., and Shim, K. Efficient algorithms for mining outliers from large data sets. In Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, SIGMOD \u201900, pp. 427\u2013438, New York, NY, USA, 2000. Association for Computing Machinery. ISBN 1581132174. doi: 10.1145/342009.335437. URL https://doi.org/10.1145/342009.335437.\\n\\nRasul, K., Ashok, A., Williams, A. R., Khorasani, A., Adamopoulos, G., Bhagwatkar, R., Bilo\u02c7s, M., Ghonia, H., Hassen, N. V., Schneider, A., et al. Lag-llama: Towards foundation models for time series forecasting. arXiv preprint arXiv:2310.08278, 2023.\\n\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.\\n\\nSchmidl, S., Wenig, P., and Papenbrock, T. Anomaly detection in time series: A comprehensive evaluation. Proc. VLDB Endow., 15(9):1779\u20131797, may 2022. ISSN 2150-8097. doi: 10.14778/3538598.3538602. URL https://doi.org/10.14778/3538598.3538602.\\n\\nSchneider, S. H. and Dickinson, R. E. Climate modeling. Reviews of Geophysics, 12(3):447\u2013493, 1974.\\n\\nSerr`a, J., Pascual, S., and Karatzoglou, A. Towards a universal neural network encoder for time series. In International Conference of the Catalan Association for Artificial Intelligence, 2018. URL https://api.semanticscholar.org/CorpusID:13675490.\\n\\nShaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.\\n\\nShen, J., Li, L., Dery, L. M., Staten, C., Khodak, M., Neubig, G., and Talwalkar, A. Cross-modal fine-tuning: Align then refine, 2023.\\n\\nSmith, L. N. and Topin, N. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pp. 369\u2013386. SPIE, 2019.\\n\\nSu, Y., Zhao, Y., Niu, C., Liu, R., Sun, W., and Pei, D. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 2828\u20132837, 2019.\\n\\nTalukder, S., Yue, Y., and Gkioxari, G. Totem: Tokenized time series embeddings for general time series analysis, 2024.\\n\\nTanisaro, P. and Heidemann, G. Time series classification using time warping invariant echo state networks. In 2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA), pp. 831\u2013836, 2016. doi: 10.1109/ICMLA.2016.0149.\\n\\nTolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, 14\"}"}
{"id": "FVvf69a5rx", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\nD., Uszkoreit, J., et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34:24261\u201324272, 2021.\\n\\nTonekaboni, S., Eytan, D., and Goldenberg, A. Unsupervised representation learning for time series with temporal neighborhood coding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=8qDwejCuCN.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023.\\n\\nTrindade, A. ElectricityLoadDiagrams20112014. UCI Machine Learning Repository, 2015. DOI: https://doi.org/10.24432/C58C86.\\n\\nVan Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017.\\n\\nvan der Maaten, L. Accelerating t-sne using tree-based algorithms. Journal of Machine Learning Research, 15(93):3221\u20133245, 2014. URL http://jmlr.org/papers/v15/vandermaaten14a.html.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n\\nWang, Z., Yan, W., and Oates, T. Time series classification from scratch with deep neural networks: A strong baseline. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578\u20131585, 2017. doi: 10.1109/IJCNN.2017.7966039.\\n\\nWen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L. Transformers in time series: A survey. In Elkind, E. (ed.), Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, pp. 6778\u20136786. International Joint Conferences on Artificial Intelligence Organization, 8 2023. doi: 10.24963/ijcai.2023/759. URL https://doi.org/10.24963/ijcai.2023/759. Survey Track.\\n\\nWoo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., and Sahoo, D. Unified training of universal time series forecasting transformers. arXiv preprint arXiv:2402.02592, 2024.\\n\\nWu, C.-J., Raghavendra, R., Gupta, U., Acun, B., Ardalani, N., Maeng, K., Chang, G., Behram, F. A., Huang, J., Bai, C., Gschwind, M., Gupta, A., Ott, M., Melnikov, A., Candido, S., Brooks, D., Chauhan, G., Lee, B., Lee, H.-H. S., Akyildiz, B., Balandat, M., Spisak, J., Jain, R., Rabbat, M., and Hazelwood, K. Sustainable AI: Environmental implications, challenges and opportunities, 2022.\\n\\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=I55UqU-M11y.\\n\\nWu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ju_Uqw384Oq.\\n\\nWu, R. and Keogh, E. J. Current time series anomaly detection benchmarks are flawed and are creating the illusion of progress. IEEE Transactions on Knowledge & Data Engineering, 35(03):2421\u20132429, mar 2023. ISSN 1558-2191. doi: 10.1109/TKDE.2021.3112126.\\n\\nXie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., and Hu, H. Simmim: a simple framework for masked image modeling. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 9643\u20139653, 2022. doi: 10.1109/CVPR52688.2022.00943.\\n\\nXu, H., Chen, W., Zhao, N., Li, Z., Bu, J., Li, Z., Liu, Y., Zhao, Y., Pei, D., Feng, Y., et al. Unsupervised anomaly detection via variational auto-encoder for seasonal kpis in web applications. In Proceedings of the 2018 world wide web conference, pp. 187\u2013196, 2018.\"}"}
{"id": "FVvf69a5rx", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yue, Z., Wang, Y., Duan, J., Yang, T., Huang, C., Tong, Y., and Xu, B. Ts2vec: Towards universal representation of time series. Proceedings of the AAAI Conference on Artificial Intelligence, 36(8):8980\u20138987, Jun. 2022. doi: 10.1609/aaai.v36i8.20881. URL https://ojs.aaai.org/index.php/AAAI/article/view/20881.\\n\\nZebik, M., Korytkowski, M., Angryk, R., and Scherer, R. Convolutional Neural Networks for Time Series Classification, pp. 635\u2013642. Springer International Publishing, Cham, 2017. ISBN 978-3-319-59060-8. doi: 10.1007/978-3-319-59060-8_57. URL https://doi.org/10.1007/978-3-319-59060-8_57.\\n\\nZerveas, G., Jayaraman, S., Patel, D., Bhamidipaty, A., and Eickhoff, C. A transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, KDD '21, pp. 2114\u20132124, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383325. doi: 10.1145/3447548.3467401. URL https://doi.org/10.1145/3447548.3467401.\\n\\nZhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. Informer: Beyond efficient transformer for long sequence time-series forecasting. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):11106\u201311115, May 2021. doi: 10.1609/aaai.v35i12.17325. URL https://ojs.aaai.org/index.php/AAAI/article/view/17325.\\n\\nZhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39th International Conference on Machine Learning (ICML 2022), 2022.\\n\\nZhou, T., Niu, P., Wang, X., Sun, L., and Jin, R. One fits all: Power general time series analysis by pretrained LM. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=gMS6FVZvmF.\"}"}
{"id": "FVvf69a5rx", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Related Work\\n\\nTransformers and Patching for Time series Modeling. There is a growing body of work utilizing transformers for various time series analysis tasks, for example PatchTST (Nie et al., 2023), Informer (Zhou et al., 2021), Autoformer (Wu et al., 2021), FEDformer (Zhou et al., 2022), Pyraformer (Liu et al., 2022) for forecasting; Anomaly Transformer (Xu et al., 2022) for anomaly detection, and TST (Zerveas et al., 2021), TS-TCC (Eldele et al., 2021) for representation learning.\\n\\nOne issue with applying transformers to time series data is the complexity of the self-attention mechanism, which grows quadratically with the size of input tokens (or length of time series). Consequently, the primary focus of most initial applications of transformers to time series, especially for forecasting where longer look-back windows typically improve performance, was to redesign the self-attention mechanism to reduce its complexity (Zhou et al., 2021; 2022; Liu et al., 2022). Nie et al. (2023) demonstrated that treating time series sub-sequences (or patches) as tokens instead of individual time points is a simple, efficient yet effective mechanism for learning useful representations for forecasting. The authors drew inspiration from language and vision domains where sub-words (vs. characters) (Devlin et al., 2019) and 2-D patches (vs. raw pixels) (Bao et al., 2022; Dosovitskiy et al., 2021) are used as inputs to transformers. Drawing inspiration from prior work, we build on top of the transformer architecture which takes disjoint time series sub-sequences (or patches) as input.\\n\\nMasked Representation Learning.\\n\\nMasked pre-training is a widely-used self-supervised learning task where a model learns to accurately reconstruct masked portions of its input. Masked language (Devlin et al., 2019; Raffel et al., 2020) and image modeling (Xie et al., 2022; Li et al., 2023b) have been successfully utilized to learn models from vast quantities of unlabeled data, which can generalize to a variety of downstream tasks.\\n\\nFor time series data, prior work has primarily focused on contrastive representation learning (Yue et al., 2022; Eldele et al., 2021; Franceschi et al., 2019). The goal of contrastive learning is to learn a representation space where \u201cpositive\u201d pairs of time series are close while \u201cnegative\u201d pairs are far apart. However, the notion of positive and negative pairs is subjective and data-dependent, and popular transformations such as flipping and cropping invariance may not be appropriate for time series data (Yue et al., 2022). In contrast, some studies mask portions of time series using zeros and learn a model to reconstruct them (Nie et al., 2023; Zerveas et al., 2021; Dong et al., 2023; Li et al., 2023c).\\n\\nRepresentation learning via masking is well-suited to all the downstream tasks we care about, especially forecasting and imputation, as they are instances of the masked reconstruction problem. Owing to its simplicity and success in vision and language domains, we use the masked prediction task to pre-train our model, using a special embedding (see [MASK] in Fig. 3) to mask time series patches instead of zeros.\\n\\nCross-modal transfer learning using language models.\\n\\nLu et al. (2022) had first shown that transformers pre-trained on text data (LLMs) can effectively solve sequence modeling tasks in other modalities. Some recent studies have leveraged this inherent ability of language pre-trained transformers to \u201creprogram\u201d LLMs for time series analysis using parameter efficient fine-tuning and suitable tokenization strategies (Zhou et al., 2023; Gruver et al., 2023; Jin et al., 2023; Cao et al., 2023; Ekambaram et al., 2024). However, some of these models (Jin et al., 2023; Gruver et al., 2023) with billions of parameters demand significant memory and computational resources to perform well. We complement this line of research with three empirical observations (Sec 4.3): we show that (1) transformers trained on time series can also model sequences across modalities, (2) during pre-training, randomly initializing weights lead to lower pre-training loss, than initializing with language modeling weights, and (3) models pre-trained on time series outperform LLM-based models such as (Zhou et al., 2023; Jin et al., 2023) on many tasks and datasets.\\n\\nUnanswered Questions.\\n\\nTo the best of our knowledge, two questions remain largely unanswered in prior work on time series modeling. First, all existing time series models are (pre-)trained and fine-tuned on individual datasets (Nie et al., 2023; Yue et al., 2022; Wu et al., 2023; Zhou et al., 2023), and the benefits (or drawbacks) of large-scale multi-dataset pre-training remains unexplored (Wen et al., 2023). Second, there is very limited work on time series modeling in limited supervision settings, such as zero-shot forecasting (Oreshkin et al., 2021), or few-shot classification (Narwariya et al., 2020). In our work, we consider both these questions and show that pre-training a model of sufficient capacity on a large corpus of unlabeled time series data can in fact enable it to provide reasonably accurate predictions in limited-supervision settings.\"}"}
{"id": "FVvf69a5rx", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 20. Hyper-parameter values for short-horizon forecasting models.\\n\\n| Model          | Sequence length | Patch length | Patch stride length | Initial learning rate | Max epochs |\\n|----------------|-----------------|--------------|--------------------|-----------------------|------------|\\n| MOMENT LP      | 512             | 8            | 8                  | 0.002                 | {5, 10}    |\\n| MOMENT 0       | 512             | 8            | 8                  | 0.001                 |            |\\n| N-BEATS        | 512             |              |                    |                       |            |\\n| GPT4TS         | 512             | 1            | 1                  |                       |            |\\n| TimesNet       | 512             |              |                    |                       |            |\\n\\n### Table 21. Experimental settings for short-horizon forecasting experiments for varying source and target datasets.\\n\\n| Source Dataset | Target Dataset |\\n|----------------|----------------|\\n| M4 Yearly      | M4 Yearly      |\\n| M4 Yearly      | M4 Yearly      |\\n| M4 Yearly      | M4 Yearly      |\\n| M3 Yearly      | M3 Yearly      |\\n| M3 Yearly      | M3 Yearly      |\\n| M3 Yearly      | M3 Yearly      |\\n\\n### E.2. Classification\\n\\nThe classification problem comprises of learning a mapping $f: T \\\\rightarrow \\\\{1, ..., C\\\\}$ from a time series to a finite set of classes, using a training dataset of the form $\\\\{(\\\\mathbf{T}_0, c_0), ..., (\\\\mathbf{T}_n, c_n)\\\\}$, $c_i \\\\in \\\\{1, ..., C\\\\}$. One straightforward way to use MOMENT to learn $f$ is to replace its reconstruction head with a linear head that maps patch representations to the $C$ logits. Another way would be to learn $f$ in two stages, as is common in prior work on unsupervised representation learning (Yue et al., 2022; Franceschi et al., 2019): in the first stage, we obtain sequence-level representations for each time series without access to labels. The second stage involves learning any ML classifier (e.g., Support Vector Machine with RBF kernel) using these representations and labels.\\n\\n### Datasets.\\n\\nWe conduct experiments on a subset of 95 datasets from the UCR Classification Archive (Dau et al., 2018). These datasets (listed in Table 10) comprise of equal-length univariate time series shorter than 512 time steps.\\n\\n### Baselines.\\n\\nWe compare MOMENT against 5 unsupervised representation learning methods (TS2Vec (Yue et al., 2022), TST (Zerveas et al., 2021), TS-TCC (Eldele et al., 2021), TNC (Tonekaboni et al., 2021), and T-Loss (Franceschi et al., 2019)), 8 supervised deep learning methods (CNN (Zebik et al., 2017), Encoder (Serra et al., 2018), FCN (Wang et al., 2017), MCNN (Cui et al., 2016), MLP (Wang et al., 2017), ResNet (Wang et al., 2017), t-LeNet (Le Guennec et al., 2016), TWIESN (Tanisaro & Heidemann, 2016)), 1 supervised statistical learning method DTW (Dau et al., 2018)), TimesNet (Wu et al., 2023) and GPT4TS (Zhou et al., 2023).\\n\\n### Experimental Setting.\\n\\nAll models except for MOMENT were trained on each dataset individually, either with labels for supervised deep and statistical learning methods), or without labels for representation learning methods. We collect baseline results for deep learning methods from Ismail Fawaz et al. (2019), representation learning methods from Yue et al. (2022), and DTW from Dau et al. (2018).\"}"}
{"id": "FVvf69a5rx", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 22. Anomaly detection performance measured using adj. best $F_1$ and VUS-ROC for a subset of 45 datasets sampled from the UCR Anomaly archive.\\n\\n### Task Description\\nGiven a time series $T$, anomaly detection is a binary classification problem, where the goal is to detect whether a time step $x_i$ is indicative of an anomaly or not. As shown in Fig. 4 (v), to detect anomalies in $T$, we retain MOMENT's reconstruction head and use it to reconstruct the input time series. Then, time steps where observations and predictions differ beyond a certain threshold are classified as anomalies.\\n\\n### Datasets\\nWe conduct experiments on a subset of 46 univariate time series from the UCR Anomaly Archive (Wu & Keogh, 2023), as enumerated in Table 11. When choosing the subset of time series, we prioritized coverage over different domains and data sources represented in the archive.\\n\\n### Baselines\\nWe compare MOMENT with 2 state-of-the-art anomaly detection methods DGHL (Challu et al., 2022) and Anomaly Transformer (Xu et al., 2022) along with TimesNet and GPT4TS. We also include $k$-Nearest Neighbors (with $k = 5$) (Ramaswamy et al., 2000), a classical anomaly detection method in our experiments. In the zero-shot setting, we compare MOMENT to randomly initialized DGHL ($DGHL_0$) and $k$-NN.\\n\\n---\\n\\n14 Estimating good thresholds for anomaly detection is beyond the scope of this study and an active area of research (Goswami et al., 2023a; Schmidl et al., 2022).\\n\\n15 Randomly initialized DGHL is not a trivial zero-shot baseline, since it performs gradient descent to find the best latent $z$ that minimizes reconstruction error during inference time (Challu et al., 2022).\"}"}
{"id": "FVvf69a5rx", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Experimental Setting. All algorithms use a fixed anomaly detection window size (= 512). Based on prior work (Wu et al., 2023; Zhou et al., 2023), we use the mean squared error between predictions and observations as the anomaly criterion. Following prior work (Goswami et al., 2023a), we downsample all time series longer than 2560 timesteps by a factor of 10 to speed up the training and evaluation process.\\n\\nWe report two anomaly detection metrics: adjusted best $F_1$ which is frequently used in practice (Goswami et al., 2023a; Challu et al., 2022), and the recently proposed volume under ROC surface (VUS-ROC) metric (Paparrizos et al., 2022a). For both metrics, higher scores are better.\\n\\nHyperparameters. The hyperparameters used for training all models in our anomaly detection experiments are shown in Table 26.\\n\\nE.4. Imputation\\n\\nTask Description. Consider a time series $T = [x_1, ..., x_L]$ and an observation mask $M = [m_1, ..., m_L]$, where $m_i = 0$ if $x_i$ is missing and $m_i = 1$ if $x_i$ is observed. Then imputation is the task of estimating the missing values $T$ by exploiting its observed values.\\n\\nWe treat a patch as observed only if all its time steps are observed. For the remaining patches, we replace their patch embeddings with [MASK] and use MOMENT\u2019s default reconstruction head to impute its values (Fig. 4 (iv)).\\n\\nDatasets. We evaluate imputation performance on 6 real-world datasets from domains where missing data is a common problem: 4 subsets of Electricity Transformer Temperature (ETT), Weather, and Electricity (Wu et al., 2023; Zhou et al., 2023).\\n\\nBaselines. We compare the two variants of MOMENT with 3 state-of-the-art deep learning methods, TimesNet, FPT, and DGHL; and 3 statistical interpolation methods, Cubic Spline, Linear, and 1-D Nearest Neighbor interpolation.\\n\\nExperimental Setting. To evaluate the models\u2019 ability to interpolate missing values, we randomly mask contiguous sub-sequences of length 8. Instead of masking contiguous sub-sequences, previous studies (Wu et al., 2023; Zhou et al., 2023) mask individual time points, making the imputation task much easier. The results from prior studies are shown in Table 28. We observe that the statistical methods perform similarly to transformer methods, owing to the ease of the task. For our experiments involving randomly masking patches of length 8, our results are shown in Table 29. We measure the imputation performance of models using mean squared error, over 4 different masking rates: 12.5%, 25%, 37.5%, and 50%.\\n\\nHyperparameters. The hyperparameters used for training all models in our imputation experiments are shown in Table 27.\\n\\nE.5. What is MOMENT Learning?\\n\\nTo investigate what MOMENT is learning, we conducted a series of experiments using synthetically generated sine waves to evaluate MOMENT\u2019s ability to capture changes in trend, amplitude, frequencies, baselines, and phase of time series. In each experiment, $c$ controls the factor of interest, for example the power of the trend polynomial $c \\\\in (1/8, 8)$ (Oreshkin et al., 2020) (Fig. 9), and frequency $c \\\\in (1, 32)$ of the generated sine waves (Fig. 9). We generate multiple sine waves by varying $c$, derive their sequence-level representations using MOMENT (Sec. 3.4), and visualize them in a 2-dimensional space using PCA and t-SNE (van der Maaten, 2014) in Fig. 4 and Fig. 7.\\n\\nWe also study the composition of the learnable mask embedding and the relationship between frequency and reconstruction error in a zero-shot setting. We find that the learned mask embedding is approximately composed of numbers drawn from the standard normal and that MOMENT can reconstruct lower frequency signals better. We observed a curious spike in reconstruction error around time series of frequency $c = 64$. (Fig. 11)\\n\\nE.6. Impact of Model Size\\n\\nWe studied the impact of scaling the size of the model and training data on zero-shot forecasting, imputation, and anomaly detection performance. As shown in Fig. x, we found that increasing the size of the model generally improved zero-shot performance (lower MSE and sMAPE, higher VUS-ROC). Since varying the size of the pre-training dataset is expensive, we instead look at the zero-shot performance of model checkpoints before completing the first epoch. Our findings suggest that increasing the diversity in training data may also improve zero-shot performance.\"}"}
{"id": "FVvf69a5rx", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MOMENT: A Family of Open Time-series Foundation Models\\n\\n\\\\[ y = c \\\\sin(32x) + \\\\sin(2cx) + f \\\\]\\n\\nFigure 7. What is MOMENT learning? Structure in the PCA (top) and t-SNE (bottom) visualizations of the embeddings of synthetically generated sinusoids suggest that MOMENT can capture subtle trend, scale, frequency, and auto-correlation information.\\n\\n\\\\( \\\\epsilon \\\\) denotes gaussian noise with 0 mean and 0.1 standard deviation. \\\\( c \\\\) controls the factor of interest, i.e. the power of the trend polynomial, amplitude, and frequency of the sine waves in experiments (i), (ii) & (iii), respectively.\\n\\nFigure 8. PCA (top) and t-SNE (bottom) visualizations of representations learned by MOMENT on the 5 largest UCR datasets. Different colors represent different classes. Even without dataset-specific fine-tuning, MOMENT learns distinct representations for different classes.\\n\\nE.7. Training losses\\nE.8. Efficiency Analysis\\nF. Transparency Index\\nG. Results Sources\\nH. Radar Plot\\n\\nWe generate a radar plot (Fig. 1) to visually compare MOMENT with GPT4TS and TimesNet. The values obtained by each method for a given task are min-max normalized with respect to the other methods for each of the 5 downstream tasks. For imputation, long- and short-horizon forecasting, we report \\\\( 1 - \\\\) the normalized MSE or sMAPE for the methods on the weather and (subset of) M4 datasets, respectively. For classification and anomaly detection, we report the average accuracy and VUS-ROC of the methods across all the datasets.\\n\\nTo ensure a fair comparison, we do not use Anomaly Transformer's joint criterion as the anomaly score. We believe that this might put the Anomaly Transformer at some disadvantage in our experiments.\"}"}
{"id": "FVvf69a5rx", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: Classification datasets in Time Series Pile taken from (Ismail Fawaz et al., 2019) [UCR Archive]. The total number of observations is 634,084,943, while the total number of unique time series is 290,226.\"}"}
{"id": "FVvf69a5rx", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Is MOMENT effective for multiple time series analysis tasks in limited and rich supervision settings? We conduct large-scale experiments on widely used benchmarks to evaluate MOMENT on forecasting, classification, anomaly detection, and imputation as outlined in Table 8. The limited supervision setting mimics practical scenarios in which it is infeasible to train (or fine-tune) a deep...\"}"}
{"id": "FVvf69a5rx", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset Domain          | Channels | Time Series Length | Total Observations | Frequency | Forecast Horizon |\\n|-------------------------|----------|--------------------|--------------------|-----------|-----------------|\\n| Electricity Power       | 321      | 26304              | 8443584            | Hourly    |                 |\\n|                         |          |                    |                    |           | {96, 192, 336, 720} |\\n| ETTh1 Power             | 7        | 17420              | 121940             | Hourly    |                 |\\n|                         |          |                    |                    |           | {96, 192, 336, 720} |\\n| ETTh2 Power             | 7        | 17420              | 121940             | Hourly    |                 |\\n|                         |          |                    |                    |           | {96, 192, 336, 720} |\\n| ETTm1 Power             | 7        | 69680              | 487760             | 15 Minute |                 |\\n|                         |          |                    |                    |           | {96, 192, 336, 720} |\\n| ETTm2 Power             | 7        | 69680              | 487760             | 15 Minute |                 |\\n|                         |          |                    |                    |           | {96, 192, 336, 720} |\\n| Exchange Finance        | 8        | 7588               | 60704              | Daily     |                 |\\n|                         |          |                    |                    |           | {96, 192, 336, 720} |\\n| Illness Epidemiology    | 7        | 966                | 6762               | Weekly    |                 |\\n|                         |          |                    |                    |           | {24, 36, 48, 60} |\\n| Traffic Traffic         | 862      | 17544              | 15122928           | Hourly    |                 |\\n|                         |          |                    |                    |           | {96, 192, 336, 720} |\\n| Weather Weather         | 21       | 52696              | 1106616            | 10 Minute |                 |\\n|                         |          |                    |                    |           | {96, 192, 336, 720} |\\n\\nTable 15. Long-horizon forecasting datasets in the Time Series Pile taken from Zhou et al. (2021). The total number of observations is 25,959,994, while the total number of unique time series is 1,247.\\n\\nneural network due to limited compute and, little or inadequately characterized data. In these settings, MOMENT provides predictions without any explicit (re)training on target data. On the other hand, the rich supervision setting allows us to examine whether MOMENT can utilize task-specific data to improve its performance via end-to-end fine-tuning or linear probing.\\n\\nWhat does MOMENT learn? We evaluated MOMENT\u2019s ability to model time series characteristics such as varying frequencies, trends, and scales. Structure in the PCA and t-SNE (Fig. 9) visualizations of the embeddings of synthetically generated sinusoids suggest that MOMENT can capture subtle trend, scale, frequency, and auto-correlation information.\\n\\n$\\\\epsilon$ denotes gaussian noise with 0 mean and 0.1 standard deviation. $c$ controls the factor of interest, i.e. the power of the trend polynomial, amplitude, and frequency of the sine waves in experiments (i), (ii) & (iii), respectively.\\n\\nHyper-parameter Tuning. We do not perform extensive hyper-parameter tuning. In all experiments that follow, unless mentioned otherwise, we fine-tune MOMENT-Base with a batch size of 16, and cosine learning rate schedule with an initial learning rate of $1 e^{-5}$. For baseline methods, we capture recommended settings from their respective papers and public repositories. We report all hyper-parameters settings for MOMENT and baselines in Appendix D.\\n\\nE.1. Forecasting Task description. Given a time series $T = [x_1, ..., x_L]$ where $x_i \\\\in \\\\mathbb{R}$, the univariate forecasting problem is to predict the next $H$ time-steps $[x_{L+1}, ..., x_{L+H}]$. Depending on the length of the horizon, forecasting can be categorized as short or long-horizon. We consider both tasks in our experiments. We propose two configurations of MOMENT for the forecasting problem: (1) we can produce short-horizon forecasts without any explicit training or fine-tuning, by appending masked patches and predicting them using the default reconstruction head (Fig. 4 (ii)); (2) alternatively, we can replace the reconstruction head to a forecasting head and then fine-tune it (Fig. 4 (i)).\\n\\nE.1.1. LONG-HORIZON FORECASTING Datasets. We use all the long-horizon forecasting datasets (Sec 3.1). But to speed up our experiments, we drop all exogenous variables from multi-variate datasets and only consider the target time series for forecasting.\\n\\nBaselines. We compare our methods with various transformer-based and deep learning baselines. These models can be found in Table 18. For Time-LLM we could not run experiments on Weather, electricity, and traffic datasets, due to time constraints, and since we could not fit them into a single GPU.\\n\\nExperimental Setting. We train all models with a look-back window of length $L = 512$ to forecast $T = 24, 60$ time-steps for the ILI dataset and $T = 96, 720$ for the rest. We evaluate the Mean Squared Error (MSE) and Mean Absolute Error (MAE) as metrics.\\n\\nHyperparameters. The hyperparameters used for training all models in our long-horizon forecasting experiments are shown in Table 17.\\n\\n11 For classification, the quality of MOMENT\u2019s representations is measured using the accuracy of a Support Vector Machine trained on them, as is common in prior work on unsupervised representation learning (Yue et al., 2022; Franceschi et al., 2019). However, unlike prior work, MOMENT embeds time series without any data-specific training.\\n\\n12 The distinction between long and short-horizon forecasting is rather arbitrary. For instance, most of the default forecasting horizons for the long-horizon forecasting benchmark Influenza-like Illness (24, 36, 48, 60) are shorter than the Hourly subset of the M4 dataset, a popular short-horizon forecasting benchmark.\"}"}
{"id": "FVvf69a5rx", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The hyperparameters used for training all models in our short-horizon forecasting experiments are shown in Table 18. Our results are reported only (1) on 40% of the M3 and M4 datasets that were unseen during pre-training, (2) a subset of frequencies with plenty of data and we could not get promising zero-shot performance (Makridakis et al., 2020).\\n\\nTo evaluate zero-shot forecasting performance, we conduct experiments on the M3 and M4 datasets (Sec. 3.1). Long-term forecasting performance with a look-back window of 96 time-steps. Results are taken from iTransformer (Liu et al., 2021). We follow the same experimental procedure as outlined in (Oreshkin et al., 2021) with two exceptions: we leverage data from frequencies with plenty of data. We also believe that ensembling played an important part in N-BEATS promising performance for any of the deep learning models. Some ways that prior work (Oreshkin et al., 2021) had overcome this issue was by some ways that prior work (Oreshkin et al., 2021) had overcome this issue was by\\n\\nExperimental Setting.\\nEach statistical method is bm benchmarking forecasting methods: AutoARIMA, AutoTheta, AutoETS, Naive, Seasonal Naive, and Random Walk. Baselines.\\nWe compare models with GPT4TS (Zhou et al., 2023), TimesNet (Wu et al., 2023), N-BEATS (Oreshkin et al., 2020), 3\\n\\n| Dataset | Pred | Method | MOMENT | iTransformer | RLinear | PatchTST | Crossformer | TiDE | TimesNet | DLinear | SCINet | FEDFormer | Stationary | Autoformer | LP | Time-LLM | GPT4TS | PatchTST | DLinear | TimesNet | FEDformer | Pyraformer | Autoformer | ETSformer | LightTS | Informer | Reformer | LogTrans | N-BEATS |\\n|---------|------|--------|---------|-------------|---------|----------|------------|------|----------|---------|--------|------------|------------|------------|----|---------|--------|---------|----------|------------|----------|---------|------------|--------|--------|---------|---------|---------|\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |             |         |          |             |     |          |         |        |            |             |            |    |         |        |         |          |            |          |         |            |        |        |         |         |         |\\n|         |      |        |         |"}
{"id": "FVvf69a5rx", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Mask Ratio | 0.043 | 0.126 | 0.128 | 0.233 | 0.076 | 0.183 | 0.032 | 0.108 |\\n|-----------|-------|-------|-------|-------|-------|-------|-------|-------|\\n| 1.311     | 0.129 | 0.101 | 0.207 | 0.052 | 0.133 | 0.112 | 0.220 | 0.026 |\\n| 0.686     | 0.373 | 0.392 | 0.142 | 0.046 | 0.129 | 0.101 | 0.207 | 0.052 |\\n| 0.833     | 0.398 | 0.398 | 0.142 | 0.046 | 0.129 | 0.101 | 0.207 | 0.052 |\\n| 0.540     | 0.076 | 0.183 | 0.032 | 0.108 |       |       |       |       |\\n| 0.954     | 0.071 | 0.181 | 0.032 | 0.109 |       |       |       |       |\\n| 0.581     | 0.076 | 0.181 | 0.032 | 0.109 |       |       |       |       |\\n| 1.433     | 0.232 | 0.263 | 0.195 | 0.274 |       |       |       |       |\\n| 0.772     | 0.125 | 0.238 | 0.061 | 0.159 |       |       |       |       |\\n| 0.043     | 0.187 | 0.319 | 0.102 | 0.229 | 0.096 | 0.229 | 0.080 | 0.196 |\\n| 0.126     | 0.202 | 0.329 | 0.171 | 0.264 | 0.199 | 0.198 | 0.194 | 0.200 |\\n| 0.128     | 0.108 | 0.242 | 0.158 | 0.254 | 0.069 | 0.180 | 0.126 | 0.249 |\\n| 0.233     | 0.075 | 0.185 | 0.126 | 0.249 | 0.113 | 0.191 | 0.062 | 0.138 |\\n| 0.076     | 0.196 | 0.285 | 0.105 | 0.208 | 0.134 | 0.225 | 0.452 | 0.404 |\\n| 0.183     | 0.110 | 0.216 | 0.170 | 0.291 | 0.115 | 0.215 | 0.163 | 0.277 |\\n| 0.032     | 0.076 | 0.171 | 0.035 | 0.098 | 0.035 | 0.096 | 0.036 | 0.076 |\\n| 0.108     | 0.081 | 0.129 | 0.035 | 0.075 | 0.196 | 0.285 | 0.105 | 0.208 |\\n| 0.076     | 0.146 | 0.089 | 0.200 |       |       |       |       |       |\\n| 0.146     | 0.080 | 0.194 | 0.036 | 0.076 | 0.035 | 0.096 | 0.036 | 0.076 |\\n| 0.032     | 0.076 | 0.181 | 0.032 | 0.109 |       |       |       |       |\\n\\n| Methods   | MSE   | MAE   |\\n|-----------|-------|-------|\\n| Nearest   | 0.021 | 0.084 |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5%     |       |       |\\n| 12.5"}
{"id": "FVvf69a5rx", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10. Masking using a [MASK] tokens allows MOMENT to reconstruct time series in a zero-shot setting. Since zeros contain information, they bias model predictions. For two datasets ETTh1 and Weather, we mask the time series with zeros on the left and special mask tokens on the right.\\n\\nTheoretical quantiles\\nObserved Values\\nR\u00b2 = 0.9692\\nProbability Plot of Mask Embeddings\\n\\nFigure 11. (Left) MOMENT can reconstruct lower frequency time series better in a zero-shot setting. (Right) The learned mask token is approximately composed of numbers drawn from a standard normal.\\n\\nDataset Pred horizon MOMENT small MOMENT base MOMENT large\\n\\n| Metric       | 96  | 192 | 336 | 720 |\\n|--------------|-----|-----|-----|-----|\\n| MSE          | 0.167 | 0.224 | 0.156 | 0.207 |\\n| MAE          | 0.167 | 0.224 | 0.156 | 0.207 |\\n\\nTable 30. Long-horizon forecasting scaling experiments for MOMENT small, MOMENT base, and MOMENT large.\\n\\n| Metric       | 96  | 192 | 336 | 720 |\\n|--------------|-----|-----|-----|-----|\\n| Adj. F1 Mean | 0.480 | 0.572 | 0.569 |\\n| Median       | 0.450 | 0.641 | 0.607 |\\n| Std.         | 0.378 | 0.383 | 0.372 |\\n\\nTable 31. Zero-shot anomaly detection scaling experiments for MOMENT small, MOMENT base, and MOMENT large.\"}"}
{"id": "FVvf69a5rx", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 32. Zero-shot classification scaling experiments for \\\\textsc{Moment} small, \\\\textsc{Moment} base, and \\\\textsc{Moment} large.\\n\\n![Graph showing training losses (MSE). A dashed vertical line denotes the first epoch. All models were trained with a batch size of 131072 patches.](image)\\n\\nEventually, randomly initialized \\\\textsc{MOMENT-small} outperform the same model initialized with Flan-T5 weights. The figure on the right is in log scale.\\n\\n| Model       | Total Param. (M) | Trainable Param. (M) | Mem. (MiB) |\\n|-------------|------------------|----------------------|------------|\\n| ETTh1-96    | 347.53           | 6.29                 | 2079       |\\n| \\\\textsc{Moment} | 82.28           | 1.12                 | 1031       |\\n| \\\\textsc{GPT4TS} | 0.89            | 0.89                 | 683        |\\n| \\\\textsc{Time-LLM} | 3623.71         | 254.37               | 4537       |\\n\\nTable 33. Efficiency analysis of \\\\textsc{MOMENT} against other forecasting models on the ETTh1 with prediction horizon set to 96. \\\\textsc{MOMENT} outperforms all the listed models and has a fraction of parameters as the most recent LLM-based forecasting method.\"}"}
{"id": "FVvf69a5rx", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 34. Expected (left) upstream and (right) model transparency scores. MOMENT has one of the highest upstream transparency. Our model transparency scores are lower due to (third-party) harm, mitigations, trustworthiness evaluation, which are not well understood for time series modeling.\"}"}
{"id": "FVvf69a5rx", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9. Examples of sinusoids used in the interpretability experiments.\"}"}
{"id": "FVvf69a5rx", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 23. Classification accuracy of methods across 91 UCR datasets.\\n\\n| GestureMidAirD2 | GestureMidAirD3 | GestureMidAirD1 | GestureMidAirD2 | GestureMidAirD3 | GestureMidAirD1 |\\n|----------------|----------------|----------------|----------------|----------------|----------------|\\n| 0.770          | 0.658          | 0.936          | 0.623          | 0.646          | 0.746          |\\n| 0.877          | 0.816          | 0.765          | 0.838          | 0.914          | 0.874          |\\n| 0.936          | 0.816          | 0.765          | 0.646          | 0.894          | 0.729          |\\n| 0.753          | 0.652          | 0.894          | 0.936          | 0.981          | 0.438          |\\n| 0.729          | 0.936          | 0.981          | 0.438          | 0.537          | 0.669          |\\n| 0.620          | 0.537          | 0.537          | 0.537          | 0.669          | 0.738          |\\n| 0.712          | 0.669          | 0.738          | 0.738          | 0.731          | 0.731          |\\n| 0.863          | 0.731          | 0.731          | 0.731          | 0.879          | 0.961          |\\n| 0.936          | 0.879          | 0.961          | 0.961          | 0.902          | 0.925          |\\n| 0.981          | 0.902          | 0.925          | 0.925          | 0.802          | 0.802          |\\n| 0.438          | 0.925          | 0.925          | 0.925          | 0.802          | 0.798          |\\n| 0.537          | 0.802          | 0.798          | 0.798          | 0.960          | 0.960          |\\n| 0.669          | 0.798          | 0.960          | 0.960          | 0.666          | 0.791          |\\n| 0.738          | 0.960          | 0.791          | 0.791          | 0.876          | 0.876          |\\n| 0.731          | 0.791          | 0.876          | 0.876          | 0.917          | 0.917          |\\n| 0.879          | 0.876          | 0.917          | 0.917          | 0.826          | 0.826          |\\n| 0.961          | 0.917          | 0.917          | 0.917          | 0.802          | 0.774          |\\n| 0.902          | 0.917          | 0.917          | 0.917          | 0.798          | 0.774          |\\n| 0.925          | 0.917          | 0.917          | 0.917          | 0.960          | 0.960          |\\n| 0.802          | 0.917          | 0.917          | 0.917          | 0.666          | 0.608          |\\n| 0.798          | 0.917          | 0.917          | 0.917          | 0.791          | 0.876          |\\n| 0.960          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.666          | 0.917          | 0.917          | 0.917          | 0.666          | 0.666          |\\n| 0.791          | 0.917          | 0.917          | 0.917          | 0.876          | 0.876          |\\n| 0.876          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.917          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.826          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.802          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.774          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.874          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.791          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.876          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.917          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.826          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.802          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.774          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.874          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.791          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.876          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.917          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.826          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.802          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.774          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.874          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.791          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.876          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.917          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.826          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.802          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.774          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.874          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.791          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.876          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.917          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.826          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.802          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.774          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.874          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.791          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.876          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.917          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.826          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.802          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.774          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.874          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.791          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.876          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.917          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.826          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.802          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.774          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.874          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.791          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.876          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.917          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.826          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.802          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.774          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.874          | 0.917          | 0.917          | 0.917          | 0.917          | 0.917          |\\n| 0.791          | 0.917          | 0.917          | 0.917          | 0.917          | 0."}
{"id": "FVvf69a5rx", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset                  | DTW | TS2Vec | T-Loss | TNC | TS-TCC | TST | Median Rank | Wins/Losses |\\n|-------------------------|-----|--------|--------|-----|--------|-----|-------------|-------------|\\n| Articulatory Word       | 0.987 | 0.990  | 0.987  | 0.943 | 0.973  | 0.953 | 4.5         | 119.0/54.0  |\\n| Atrial Fibrillation     | 0.200 | 0.200  | 0.133  | 0.133 | 0.267  | 0.067 | 4.5         | 97.5/75.5   |\\n| Basic Motions           | 1.000 | 0.975  | 1.000  | 0.975 | 1.000  | 0.975 | 4.5         | 82.5/90.5   |\\n| Cricket                 | 0.986 | 0.972  | 0.972  | 0.958 | 0.917  | 1.000 | 4.5         | 75.5/97.5   |\\n| Duck Duck Geese         | 0.600 | 0.680  | 0.650  | 0.460 | 0.380  | 0.620 | 4.5         | 55.0/118.0  |\\n| Eigen Worms             | 0.809 | 0.847  | 0.840  | 0.840 | 0.779  | 0.748 | 4.5         | 72.0/96.0   |\\n| Epilepsy                | 0.993 | 0.964  | 0.971  | 0.957 | 0.957  | 0.949 | 4.5         | 82.5/90.5   |\\n| ERing                   | 0.959 | 0.874  | 0.133  | 0.852 | 0.904  | 0.874 | 4.5         | 55.0/118.0  |\\n| Ethanol Concentration   | 0.357 | 0.308  | 0.205  | 0.297 | 0.285  | 0.262 | 4.5         | 72.0/96.0   |\\n| Face Detection          | 0.633 | 0.501  | 0.513  | 0.536 | 0.544  | 0.534 | 4.5         | 72.0/96.0   |\\n| Finger Movements        | 0.490 | 0.480  | 0.580  | 0.470 | 0.460  | 0.560 | 4.5         | 72.0/96.0   |\\n| Hand Movement Direction | 0.324 | 0.338  | 0.351  | 0.324 | 0.243  | 0.243 | 4.5         | 72.0/96.0   |\\n| Handwriting             | 0.308 | 0.515  | 0.451  | 0.249 | 0.498  | 0.225 | 4.5         | 72.0/96.0   |\\n| Heartbeat               | 0.722 | 0.683  | 0.741  | 0.746 | 0.751  | 0.746 | 4.5         | 72.0/96.0   |\\n| Japanese Vowels         | 0.716 | 0.984  | 0.989  | 0.978 | 0.930  | 0.978 | 4.5         | 72.0/96.0   |\\n| Libras                  | 0.850 | 0.867  | 0.883  | 0.817 | 0.822  | 0.656 | 4.5         | 72.0/96.0   |\\n| LSST                    | 0.411 | 0.537  | 0.509  | 0.595 | 0.474  | 0.408 | 4.5         | 72.0/96.0   |\\n| Motor Imagery           | 0.500 | 0.510  | 0.580  | 0.500 | 0.610  | 0.500 | 4.5         | 72.0/96.0   |\\n| NATOPS                  | 0.828 | 0.928  | 0.917  | 0.911 | 0.822  | 0.850 | 4.5         | 72.0/96.0   |\\n| PEMS-SF                 | 0.896 | 0.682  | 0.676  | 0.699 | 0.734  | 0.740 | 4.5         | 72.0/96.0   |\\n| Pen Digits              | 0.972 | 0.989  | 0.981  | 0.979 | 0.974  | 0.560 | 4.5         | 72.0/96.0   |\\n| Phoneme Spectra         | 0.233 | 0.233  | 0.222  | 0.207 | 0.252  | 0.085 | 4.5         | 72.0/96.0   |\\n| Racket Sports           | 0.796 | 0.855  | 0.855  | 0.776 | 0.816  | 0.809 | 4.5         | 72.0/96.0   |\\n| Self Regulation SCP1    | 0.840 | 0.812  | 0.843  | 0.799 | 0.823  | 0.754 | 4.5         | 72.0/96.0   |\\n| Self Regulation SCP2    | 0.478 | 0.578  | 0.539  | 0.550 | 0.533  | 0.550 | 4.5         | 72.0/96.0   |\\n| Spoken Arabic Digits    | 0.981 | 0.988  | 0.905  | 0.934 | 0.970  | 0.923 | 4.5         | 72.0/96.0   |\\n| Stand Walk Jump         | 0.400 | 0.467  | 0.333  | 0.400 | 0.333  | 0.267 | 4.5         | 72.0/96.0   |\\n| UWave Gesture Library   | 0.909 | 0.906  | 0.875  | 0.759 | 0.753  | 0.575 | 4.5         | 72.0/96.0   |\\n| Insect Wingbeat         | 0.246 | 0.466  | 0.156  | 0.469 | 0.264  | 0.105 | NaN         |             |\\n\\nTable 24. Classification accuracy of methods across 29 UEA datasets. MOMENT without fine-tuning on individual datasets demonstrates promising accuracy.\\n\\n| Model       | Hyper-parameters |\\n|-------------|------------------|\\n| **MOMENT**  | sequence length: 512, patch length: 8, patch stride length: 8 |\\n| **SVM**     | C: {0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000}, kernel: RBF, degree: 3, cache size: 200, max iterations: 10000000, decision function shape: One versus rest |\\n\\nTable 25. Hyper-parameter values for classification.\"}"}
{"id": "FVvf69a5rx", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 26. Hyperparameter values for anomaly detection.\\n\\n| Model        | Hyper-parameters                |\\n|--------------|---------------------------------|\\n| MOMENT       | sequence length: 512            |\\n|              | patch length: 8                 |\\n|              | patch stride length: 8          |\\n| MOMENT LP    | sequence length: 512            |\\n|              | patch length: 8                 |\\n|              | patch stride length: 8          |\\n|              | initial lr: 5 \\\\times 10^{-5}    |\\n| Anomaly Transformer | sequence length: 512 |\\n|              | number of channels: 1           |\\n|              | \\\\( k \\\\): 3                     |\\n|              | anomaly ratio: 4.00              |\\n|              | model dimensions: 512           |\\n|              | number of heads: 8              |\\n|              | embedding layers: 3             |\\n|              | dimension of feedforward layer: 512 |\\n| DGHL         | sequence length: 512            |\\n|              | number of channels: 1           |\\n|              | hidden multiplier: 32           |\\n|              | max filters: 256                |\\n|              | kernel multiplier: 1            |\\n|              | sub-windows: 4                  |\\n|              | size of latent z vector: 50      |\\n|              | number of iteration in Langevin dynamics inference formula: 100 |\\n|              | z step size: 0.1                |\\n|              | noise std: 0.001                |\\n| GPT4TS       | sequence length: 512            |\\n|              | gpt layers: 3                   |\\n|              | patch length: 1                 |\\n|              | patch stride length: 1          |\\n|              | transformer backbone: GPT-2      |\\n| TimesNet     | sequence length: 512            |\\n|              | dimension of model: 16          |\\n|              | dimension of feedforward layer: 16 |\\n|              | top k: 3                        |\\n|              | number of kernels: 6            |\\n\\n### Table 27. Hyperparameter values for imputation.\\n\\n| Model | Hyper-parameters |\\n|-------|------------------|\\n| MOMENT       | sequence length: 512 |\\n|              | gpt layers: 3       |\\n|              | patch length: 1      |\\n|              | patch stride length: 1 |\\n|              | transformer backbone: GPT-2 |\\n|              | dimension of feedforward layer: 16 |\\n| DGHL         | sequence length: 512 |\\n|              | number of channels: 1 |\\n|              | hidden multiplier: 32 |\\n|              | max filters: 256     |\\n|              | kernel multiplier: 1  |\\n|              | sub-windows: 4       |\\n|              | size of latent z vector: 50 |\\n|              | number of iteration in Langevin dynamics inference formula: 100 |\\n|              | z step size: 0.1     |\\n|              | noise std: 0.001     |\"}"}
