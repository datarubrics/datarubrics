{"id": "ATvN9JnqZ8", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates\\n\\nZhenqiao Song, Yunlong Zhao, Wenxian Shi, Wengong Jin, Yang Yang, Lei Li\\n\\nAbstract\\nEnzymes are genetically encoded biocatalysts capable of accelerating chemical reactions. How can we automatically design functional enzymes? In this paper, we propose EnzyGen, an approach to learn a unified model to design enzymes across all functional families. Our key idea is to generate an enzyme's amino acid sequence and their three-dimensional (3D) coordinates based on functionally important sites and substrates corresponding to a desired catalytic function. These sites are automatically mined from enzyme databases. EnzyGen consists of a novel interleaving network of attention and neighborhood equivariant layers, which captures both long-range correlation in an entire protein sequence and local influence from nearest amino acids in 3D space. To learn the generative model, we devise a joint training objective, including a sequence generation loss, a position prediction loss and an enzyme-substrate interaction loss. We further construct EnzyBench, a dataset with 3157 enzyme families, covering all available enzymes within the protein data bank (PDB). Experimental results show that our EnzyGen consistently achieves the best performance across all 323 testing families, surpassing the best baseline by 10.79% in terms of substrate binding affinity. These findings demonstrate EnzyGen's superior capability in designing well-folded and effective enzymes binding to specific substrates with high affinities. The code, model and dataset are released at https://github.com/LeiLiLab/EnzyGen.\\n\\n1. Introduction\\nEnzymes are biological catalysts to accelerate challenging chemical reactions underlying a range of biological processes. They enjoy widespread applications in the production of pharmaceuticals (Wu et al., 2012), specialty chemicals (Carbonell et al., 2018) and biofuels (Liao et al., 2016). In enzymatic reactions, the substrate is a small molecule being converted by the enzyme catalyst. By binding with and acting on specific substrates, enzymes allow for dramatically accelerated rates in the transformation of their substrates (Bar-Even et al., 2011). Designing enzymes that can bind to specific substrates is a critical yet challenging problem.\\n\\nRecently, deep learning methods are promising in protein design (Huang et al., 2016; Pearce & Zhang, 2021). Existing deep learning approaches for functional protein design fall into three categories: (a) generative models for protein sequence design guided by fitness landscape (Brookes et al., 2019; Rives et al., 2021; Ren et al., 2022; Song & Li, 2023; Wang et al., 2023); (b) structure to sequence generation based on a targeted protein backbone (Ingraham et al., 2019; Dauparas et al., 2022; Zheng et al., 2023); (c) co-design of a backbone structure and a protein sequence that encodes it (Anishchenko et al., 2021; Wang et al., 2022; Shi et al., 2022; Yeh et al., 2023). However, these methods face many limitations on enzyme design. First, fitness-guided approaches are limited by the lack of fitness data for the vast majority of enzyme families. Second, the structures of many enzymes remain unknown (Binz et al., 2005; Fischman & Ofran, 2018). Third, prior approaches do not model substrates in enzyme design process. Finally, there is no unified model applicable to all enzyme families.\\n\\nIn this paper, we aim to learn a unified generative model to design functional enzymes across thousands of enzyme families. The key design idea lies in the notion that the biological function of an enzyme is enabled by a subset of residues (amino acids), known as functionally important sites (Bickel et al., 2002; Chakrabarti & Lanczycki, 2007; Wang et al., 2022), and that an ideal enzyme should be able to bind its substrate(s) in enzymatic reactions. Therefore, we formulate the enzyme design problem as jointly generating the enzyme sequence and backbone structure given automatically mined functionally important sites and their substrates. To learn the generative model, we devise a joint training objective, including a sequence generation loss, a position prediction loss and an enzyme-substrate interaction loss.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Section A. Proof: SE(3) Equivariance\\n\\nA.1. Proof: NAEL Is SE(3) Equivariant\\n\\nIn this section, we prove that our proposed NAEL is translation equivariant on $x$ for any translation vector $t \\\\in \\\\mathbb{R}^3$ and rotation equivariant for any $R$ from the SO(3) group. More formally, we will prove the NAEL satisfies:\\n\\n$$H_{l+1}, R x_{l+1} + t = \\\\text{NAEL}(H_l, R x_l + t)$$\\n\\nWhen $l = 0$:\\n\\nFor message update, the distance between two residues is invariant as $d_{l_{ij}} = ||R x_l + t - (R x_j + t)||^2 = ||x_l - x_j||^2$. $h_{l_{i}}$ is the embedding of residue or [mask] token, and thus it is always invariant. Therefore, $h_{l_{i}+0}$ is also invariant. Since $h_{l_{i}+0}$, $h_{l_{j}+0}$ and $d_{l_{ij}}$ are all invariant, and thus $m_{l+1_{ij}}$ is also invariant to translation $t$ and rotation $R$ on $x$.\\n\\nFor coordinate update, updated $x_{l+1}$ is equivariant to the translation $t$ and rotation $R$ on input $x_l$:\\n\\n$$R(x_l + t) + \\\\sum_{j \\\\in \\\\text{Neighbor}(i)} (R(x_l + t) - (R(x_j + t))) \\\\cdot \\\\text{FFN}(m_{l+1_{ij}}) = R(x_l + t) + \\\\sum_{j \\\\in \\\\text{Neighbor}(i)} (x_l - x_j) \\\\cdot \\\\text{FFN}(m_{l+1_{ij}}) + t = R x_{l+1} + t$$\\n\\nWhen $l > 0$:\\n\\nWe have proved that when $l = 0$, $h_{1_{i}}$ is invariant to rotation and translation on $x$. Taking $H_1$ as the second layer input and following the above process, we can prove $h_{2_{i}}$ is also invariant. Repeating this process from $l = 1$ to $L - 1$, we can get the same conclusion.\\n\\nCombining the above two scenarios together, we have $H_{l+1}, R x_{l+1} + t = \\\\text{NAEL}(H_l, R x_l + t)$ for $l = 0$ to $L - 1$.\\n\\nTherefore, our proposed NAEL is SE(3)-equivariant.\\n\\nA.2. Proof: EnzyGen Is SE(3) Equivariant\\n\\nWe provide the proof of corollary 3.2 as follows:\\n\\n$$\\\\text{EnzyGen}(H_0, R x_0 + t) = \\\\text{NAEL}_{L-1} \\\\circ \\\\text{NAEL}_{L-2} \\\\circ \\\\cdots \\\\circ \\\\text{NAEL}_0(H_0, R x_0 + t) = \\\\text{NAEL}_{L-1} \\\\circ \\\\text{NAEL}_{L-2} \\\\circ \\\\cdots \\\\circ \\\\text{NAEL}_1(H_1, R x_1 + t) = \\\\ldots = \\\\text{NAEL}_{L-1}(H_{L-1}, R x_{L-1} + t) = H_L, R x_L + t$$\\n\\nB. Enzyme Classification Tree\\n\\nWe provide an illustration of enzyme classification (EC) tree in figure 5. These categories are applied by our EnzyGen to guide the enzyme design with specific functions.\\n\\nC. Data Statistics\\n\\nWe provide the third-level category information in the validation and test sets in Table 6.\\n\\nD. Additional Experimental Results\\n\\nD.1. EnzyGen Performance on 323 Testing Fourth-Level Categories\\n\\nThe ESP scores, substrate binding affinities and AlphaFold2 pLDDT results for the 323 testing fourth-level categories are respectively provided in Table 7, 9 and 11.\\n\\nD.2. Performance on Different Candidate Set\\n\\nD.2.1. TOP-1 Candidate Results\\n\\nESP scores of top-1 candidate and their corresponding substrate binding affinities as well as pLDDT are respectively reported in Table 8, 10 and 12. The tables show that our EnzyGen achieves the best average scores across different categories on all metrics. Notably, the ESP scores of different models consistently surpass those of the top-5 candidate. This observation is reasonable, as candidates with higher probabilities are more likely to achieve higher function scores.\\n\\nD.2.2. TOP-5 Candidate Results\\n\\nESP scores of top-5 candidate and their corresponding substrate binding affinities as well as pLDDT are respectively reported in Table 13, 14 and 15. The tables show that our EnzyGen achieves the best average scores across different categories on all metrics.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates\\n\\n1.1.1 With NAD+ or NADP+ as acceptor\\n\\n1.11.1 Peroxidases\\n\\n1.14.13 With NADH or NADPH as one donor, and incorporation of one atom of oxygen into the other donor\\n\\n1.14.14 With reduced flavin or flavoprotein as one donor and incorporation of one atom of oxygen into the other donor\\n\\n1.2.1 With NAD+ or NADP+ as acceptor\\n\\n2.1.1 Methyltransferases\\n\\n2.3.1 Transferring groups other than aminoacyl groups\\n\\n2.4.1 Hexosyltransferases\\n\\n2.4.2 Pentosyltransferases\\n\\n2.5.1 Transferring alkyl or aryl groups, other than methyl groups (only sub-subclass identified to date)\\n\\n2.6.1 Transaminases\\n\\n2.7.1 Phosphotransferases with an alcohol group as acceptor\\n\\n2.7.10 Protein-tyrosine kinases\\n\\n2.7.11 Protein-serine/threonine kinases\\n\\n2.7.4 Phosphotransferases with a phosphate group as acceptor\\n\\n2.7.7 Nucleotidyltransferases\\n\\n3.1.1 Carboxylic-ester hydrolases\\n\\n3.1.3 Phosphoric-monoester hydrolases\\n\\n3.1.4 Phosphoric-diester hydrolases\\n\\n3.2.2 Hydrolysing N-glycosyl compounds\\n\\n3.4.19 Omega peptidases\\n\\n3.4.21 Serine endopeptidases\\n\\n3.5.1 In linear amides\\n\\n3.5.2 In cyclic amides\\n\\n3.6.1 In phosphorus-containing anhydrides\\n\\n3.6.4 Acting on acid anhydrides to facilitate cellular and subcellular movement\\n\\n3.6.5 Acting on GTP to facilitate cellular and subcellular movement\\n\\n4.1.1 Carboxy-lyases\\n\\n4.2.1 Hydro-lyases\\n\\n4.6.1 Phosphorus-oxygen lyases (only sub-subclass identified to date)\\n\\nD.2.3. T -10 C ANDIDATE ROES\\n\\nTable 6. Third-level category information for validation and test sets.\\n\\nESP scores of top-10 candidate and their corresponding sub-strate binding affinities as well as pLDDT are respectively reported in Table 16, 17 and 18. Again, our model achieves the best average performance in all cases. It is worth noting that our EnzyGen achieves an average enzyme-substrate ESP score of 0.93, an average substrate binding affinity of -10.12 and an average pLDDT of 90.52, demonstrating EnzyGen's ability to design both well-folded and functional enzymes.\\n\\nD.3. More Novel Cases\\n\\nWe provide more designed enzymes in Figure 6. All these cases achieve a pLDDT higher than 80, a blastp amino acid identity rate lower than 65% in Uniprot, and a substrate binding affinity lower than -10. After Gnina docking, the corresponding substrates demonstrate polar contacts (hydrogen bonds) with the enzymes (shown in purple), affirming the enzyme-substrate interaction functions. We also provide the enzyme sequences of all the analyzed cases in Table 19.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates\\n\\nGlobal Attention Sub-layer (MHA + FFN)\\n\\nNeighborhood Equivariant Sub-layer\\n\\nNAEL\\n\\nLinear + Softmax\\n\\nFigure 5. Enzyme Classification (EC) Tree in BRENDA.\\n\\n(a) Complex of designed 1L1E and S-adenosyl-L-methionine zwitterion. pLDDT=86.86, binding affinity=-11.02, Uniprot blastp recovery rate=56.5%\\n\\n(b) Complex of designed 8DSO and ATP. pLDDT=80.95, binding affinity=-11.19, Uniprot blastp recovery rate=58.1%\\n\\n(c) Complex of designed 5TMP and ATP(4-). pLDDT=85.34, binding affinity=-11.76, Uniprot blastp recovery rate=59.8%\\n\\n(d) Complex of designed 6W7O and ATP. pLDDT=80.84, binding affinity=-10.52, Uniprot blastp recovery rate=58.0%\\n\\n(e) Complex of designed 3UYO and ATP. pLDDT=81.69, binding affinity=-11.6, Uniprot blastp recovery rate=64.7%\\n\\n(f) Complex of designed 4NI2 and ATP. pLDDT=81.32, binding affinity=-11.92, Uniprot blastp recovery rate=55%\\n\\nFigure 6. Complexes of the EnzyGen designed enzymes and their corresponding substrates.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Protein Family          | PROTSEED | RFDiffusion+IF | ESM2+EGNN | EnzyGen |\\n|------------------------|----------|----------------|-----------|---------|\\n| 1.1.1 1.11.1 1.14.13 1.14.14 1.2.1 2.1.1 2.3.1 2.4.1 2.4.2 2.5.1 2.6.1 2.7.1 2.7.10 2.7.11 2.7.4 Avg | 0.98     | 0.89           | 0.96      | 0.99    |\\n| PROTSEED                | 0.98     | 0.95           | 0.98      | 1.00    |\\n| RFDiffusion+IF          | 0.89     | 0.72           | 0.69      | 0.73    |\\n| ESM2+EGNN               | 0.96     | 0.72           | 0.97      | 0.97    |\\n| EnzyGen                 | 0.99     | 1.00           | 1.00      | 1.00    |\\n\\n| Protein Family          | PROTSEED | RFDiffusion+IF | ESM2+EGNN | EnzyGen |\\n|------------------------|----------|----------------|-----------|---------|\\n| 2.7.7 3.1.1 3.1.3 3.1.4 3.2.2 3.4.19 3.4.21 3.5.1 3.5.2 3.6.1 3.6.4 3.6.5 4.1.1 4.2.1 4.6.1 Avg | 0.98     | 0.98           | 0.98      | 1.00    |\\n| PROTSEED                | 0.98     | 0.93           | 0.93      | 1.00    |\\n| RFDiffusion+IF          | 0.98     | 0.93           | 0.93      | 1.00    |\\n| ESM2+EGNN               | 0.98     | 0.93           | 0.93      | 1.00    |\\n| EnzyGen                 | 1.00     | 1.00           | 1.00      | 1.00    |\\n\\nTable 7. Enzyme-substrate ESP score (\u2191) for the 323 testing fourth-level categories. Note 0.6 or higher ESP score indicates a positive binding. Avg denotes average.\\n\\nTable 8. Enzyme-substrate ESP score (\u2191) of the top-1 candidate. IF denotes the inverse folding model ProteinMPNN. Note 0.6 or higher ESP score indicates a positive binding. Avg denotes average.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates\\n\\nTo this end, we propose EnzyGen, a unified model to co-design enzyme sequence and backbone structure. EnzyGen comprises an enzyme modeling module and a substrate representation module. We devise neighborhood attentive equivariant layers (NAELs) to jointly model enzyme amino acid sequence and residue coordinates. Each NAEL consists of a global attention sub-layer and a neighborhood equivariant sub-layer. The global attention sub-layer captures correlations among all residues within an entire enzyme sequence, while the neighborhood equivariant sub-layer updates residue representations and coordinates based on nearest neighbors in the 3D space. This architectural design facilitates information exchange with varying levels of granularity, promoting a comprehensive interaction among different residues. The substrate representation module also uses neighborhood equivariant layers to model a given substrate. Furthermore, we add category tags that correspond to BRENDA enzyme families to guide the design of enzymes with specific functions. To train EnzyGen, we design a joint training objective, which includes an amino acid type prediction loss, a coordinate reconstruction loss and an enzyme-substrate interaction loss.\\n\\nOur contributions are listed as follows:\\n\\n- We propose EnzyGen to jointly design enzyme sequence and backbone structure. EnzyGen is the first unified enzyme design model across thousands of enzyme families.\\n- We create EnzyBench, a comprehensive benchmark for enzyme design, including 101,974 PDB entries across 3,157 enzyme families, and a set of three metrics to evaluate enzyme quality before wet-lab experiments. EnzyBench includes a test set with 323 families for various function validation.\\n- We conduct experiments to evaluate EnzyGen and prior functional protein design methods. These results show that enzymes designed by EnzyGen lead to the best average enzyme-substrate ESP score (Kroll et al., 2023) of 0.65, substrate binding affinity of -9.44, and AlphaFold2 pLDDT (Jumper et al., 2021) of 87.45 across 323 families (BRENDA enzyme classification fourth-level categories). EnzyGen outperforms the best baseline by 10.79% in terms of substrate binding affinity.\\n\\n2. Related Work\\n\\nMethods for Functional Protein Design. Functional protein design has been studied with a wide variety of methods. Some works focus on designing functional proteins guided by fitness landscape, including searching (Brookes & Listgarten, 2018; Brookes et al., 2019; Kumar & Levine, 2020; Das et al., 2021; Hoffman et al., 2022; Melnyk et al., 2021; Anishchenko et al., 2021; Ren et al., 2022) or directly generating (Jain et al., 2022; Song & Li, 2023) protein sequences through deep generative models. Another category of methods concentrates on designing protein sequences capable of folding into backbone structures to fulfill specific functions (Wang et al., 2018; Strokach et al., 2020; Dauparas et al., 2022; Hsu et al., 2022; Sumida et al., 2023). Owing to the availability of large-scale data, some models are trained on extensive protein sequences from diverse families (Ferruz et al., 2022) or incorporate additional control tags for enhanced modeling (Madani et al., 2023). On the path from sequence to function, the role of structure emerges as a crucial intermediary. Consequently, some studies focus on the design of functional structures (Trippe et al., 2022; Watson et al., 2023). Recognizing the inherently complex interplay between protein sequence and structure, another set of works relies on the simultaneous generation of a new backbone structure and an amino acid sequence that folds into it (Anishchenko et al., 2021; Wang et al., 2022; Shi et al., 2022; Yeh et al., 2023; Song et al., 2023). Compared to the design of other functional proteins, functional enzyme design is largely underdeveloped. Some of the above methods have been applied to specific enzyme families, such as generating satisfactory enzyme sequences using deep generative models (Detlefsen et al., 2022; Lin et al., 2022a; Giesel et al., 2022). However, there is currently no unified generative model that is applicable to all enzyme families.\\n\\nFunctionally Important Site Discovery in Enzymes. As our method conditions enzyme design on automatically mined functionally important sites, crucial in determining their structures or functions (Tristem, 2000), it is necessary to revisit research on identifying functionally important sites in enzymes. Several databases, such as PROSITE (Hulo et al., 2006), Gene Ontology (Ashburner et al., 2000) and InterPro (Hunter et al., 2009) identify and annotate protein functional sites based on sequentially conserved residues and information extracted from experimental studies and literature searches. Some efforts have also been undertaken to derive functional insights using 3D structural information (Fetrow & Skolnick, 1998; Fetrow et al., 2001; Ivanisenko et al., 2005). Other investigators have predicted functional sites using multiple sequence alignments (Armon et al., 2001; Panchenko et al., 2004; Bray et al., 2009; Hosseini & Ilie, 2022).\\n\\n3. Proposed Method: EnzyGen\\n\\nAn enzyme consists of a chain of amino acids (also called residues) connected by peptide bonds, which folds into a proper 3D structure. Let \\\\( A \\\\) be the set of 20 common amino acids. We denote the sequence of a \\\\( N \\\\)-residue enzyme by...\"}"}
{"id": "ATvN9JnqZ8", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We propose a model named EnzyGen, as illustrated in Figure 1 (a), to simultaneously generate an enzyme sequence and its 3D backbone structure, constrained by the enzyme\u2019s EC tree (EC) in BRENDA (Schomburg et al., 2004) as shown in Appendix Figure 5. To incorporate protein family tags to facilitate enzyme design with desired functions, we leverage the enzyme classifica-\\ntion (EC) tree in BRENDA (Schomburg et al., 2004) as an input to guide model generation in specific target languages (John-\\nson et al., 2017; Liu et al., 2020), we similarly use enzyme family tags to facilitate enzyme design with desired func-\\ntions.\\n\\n3.1. Overall Model Architecture\\n\\nThe problem studied in this paper can be formulated as follows: given a desired enzyme family, a small-molecule substrate, and their 3D backbone structure, constrained by the enzyme\u2019s EC tree in BRENDA (Schomburg et al., 2004), we aim to learn a generative model with probability distribution \\\\( p_{\\\\text{opt}}(y|x_1, x_2) = \\\\Pi_{i=1}^n p_{\\\\text{opt}}(y_i|x_i) \\\\) where \\\\( x \\\\) is the number of its atoms, and \\\\( y \\\\) denotes an atom where the enzyme can bind with a small-molecule substrate. EnzyGen is a deep neural network consisting of an enzyme modeling module and a substrate representation module. The enzyme modeling module aims to generate the enzyme sequence and backbone structure, and the substrate representation module targets small-molecule substrates. EnzyGen is a deep neural network illustrated in Fig-\\nure 1 (b), to simultaneously generate an enzyme sequence and its 3D backbone structure, constrained by the enzyme\u2019s EC tree.\\n\\nHow's it going?\"}"}
{"id": "ATvN9JnqZ8", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We adopt the Transformer layer (Vaswani et al., 2017) with a special \\\\([\\\\text{mask}]\\\\) token embedding for other residues. The residue embedding will then be enhanced by the enzyme family embedding \\\\(\\\\text{Emb}(\\\\text{enzyme family})\\\\). The calculation of the global sequence attention can be formulated as follows:\\n\\n\\\\[\\nL_{\\\\theta}^2 = \\\\min_{L} \\\\left( \\\\log P(x_{1:N} | y_{1:m}, \\\\theta) \\\\right) = -\\\\sum_{i=1}^{N} \\\\log P(x_i | y_{1:m}, \\\\theta)\\n\\\\]\\n\\nTherefore, the overall training objective is:\\n\\n\\\\[\\n\\\\mathcal{L}(\\\\theta) = \\\\lambda \\\\mathcal{L}_{\\\\text{likelihood}}(\\\\theta) + (1 - \\\\lambda) \\\\mathcal{L}_{\\\\text{likelihood}}(\\\\theta) = \\\\lambda \\\\log P(x_{1:N} | y_{1:m}, \\\\theta) + (1 - \\\\lambda) \\\\log P(x_{1:N} | y_{1:m}, \\\\theta)\\n\\\\]\\n\\n3.2. NAEL Global Attention Sub-Layer\\n\\nThis sub-layer computes global contextual embeddings for all enzyme residues, which does not consider the closeness to all other residues across the whole sequence, we facilitate information flow through the entire enzyme sequence. To this end, we propose the neighborhood equivariant sub-layer. This sub-layer includes three components: neighborhood message update, neighborhood coordinate update, and neighborhood node feature update (Figure 1 (b)). Updation is kept the equivariance under 3D translation and rotation. To this end, we propose the neighborhood equivariant subnetwork while properly modeling the interactions of a given residue and its nearest neighboring residues in the 3D space can lead to improved residue representations. We intend to model this impact with a carefully designed subnetwork while keeping the equivariance under 3D translation and rotation.\\n\\nProperly modeling the interactions of a given residue and its nearest neighboring residues in the 3D space can lead to improved residue representations. We intend to model this impact with a carefully designed subnetwork while keeping the equivariance under 3D translation and rotation.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates\\n\\nFigure 2. Discoverying functionally important sites. Each row is a protein sequence in a same enzyme family (BRENDA fourth-level enzyme classification tree category). We use ClustalW2 to perform multiple sequence alignment and select common residuals above the identity threshold $\\\\tau$. According to the aligned sequences, E, G and M are common in all the sequences therefore these are selected as important sites. In experiment, $\\\\tau = 30\\\\%$.\\n\\nNeighborhood node feature update. We update the $i^{th}$ residue feature by gathering information from its $K$-nearest neighbors using a gating mechanism (Figure 1 (b) red region):\\n\\n$$c_{l+1}^i = X_k \\\\in \\\\text{Neighbor}(i) m_{l+1}^i$$\\n\\n$$h_{l+1}^i = h_{l}^i + \\\\sigma(FFN(c_{l+1}^i)) \\\\circ c_{l+1}^i$$\\n\\nwhere FFN is a two-layer fully connected feed-forward network with ReLU activation function after its first layer. $\\\\sigma$ denotes the sigmoid activation function.\\n\\nWe stack $L$ layers of NAEL to model enzymes. The output embedding $h_{L}^i$ and coordinate $x_{L}^i$ for the $i^{th}$ residue are both from the last layer. Then the output probability of amino acid type for $i^{th}$ residue is calculated as:\\n\\n$$P(s_i = a | s_{M}, x_{M}, c) = \\\\exp(h_{o}^i,a) \\\\sum_{a'=1}^{20} \\\\exp(h_{o}^i,a')$$\\n\\n$$h_{o}^i = W_A \\\\cdot h_{L}^i$$\\n\\n3.4. Substrate Representation Module\\n\\nTo facilitate effective and efficient message passing inside the substrate, we stack $L_s$ neighborhood equivariant layers to learn the substrate representations. With the substrate atom embedding $h_{l}^j$ and the corresponding coordinate $x_{l}^j$ at the $l^{th}$ layer, we perform the substrate message passing as follows:\\n\\n$$m_{l+1}^j = \\\\phi_m(h_{l}^j, h_{l}^k, ||x_{l}^j - x_{l}^k||^2)$$\\n\\n$$h_{l+1}^j = h_{l}^j + \\\\phi_n(h_{l}^j, X_k \\\\in \\\\text{Neighbor}(j) m_{l+1}^k)$$\\n\\nwhere $\\\\phi_m$ and $\\\\phi_n$ respectively denotes neighborhood message update and neighborhood node feature update. Following Guan et al. (2022), we initialize the node feature as\\n\\n| Dataset | Training | Validation | Test |\\n|---------|----------|------------|------|\\n| Third-Level Category | 256 | 30 | 30 |\\n| Fourth-Level Category | 3157 | 428 | 323 |\\n| Size | 98974 | 1500 | 1500 |\\n| Average Seq. Length | 327 | 331 | 328 |\\n\\nTable 1. EnzyBench statistics. The first two rows are third and fourth-level categories in BRENDA enzyme classification tree. Size indicates the number of protein entries in PDB under the categories. The last two rows are the statistics of the enzyBench dataset.\\n\\nfive chemical features $\\\\hat{h}_{0}^j \\\\in \\\\mathbb{R}^5$ and $h_{0}^j = W_s \\\\cdot \\\\hat{h}_{0}^j$ where $W_s \\\\in \\\\mathbb{R}^{d \\\\times 5}$ is a mapping matrix and $d$ is the representation dimensionality. Since we have the real 3D structure of the substrate, we do not need to update its atom coordinates. Then based on the learned enzyme and substrate representations, we can predict whether the enzyme can bind with the given substrate as follows:\\n\\n$$P(y | s_{M}, x_{M}, c, \\\\theta) = \\\\text{Softmax}(W_b \\\\cdot [f(H_{Le}; f(H_{Ls})])$$\\n\\nwhere $W_b \\\\in \\\\mathbb{R}^{2 \\\\times 2d}$ is a mapping matrix and $f$ denotes sum pooling operation. $H_{Le} = [h_{Le}^1, ..., h_{Le}^N]^T$ and $H_{Ls} = [h_{Ls}^1, ..., h_{Ls}^m]^T$ denote the output embeddings of enzyme and substrate at the last layer of enzyme modeling module and substrate representation module, respectively.\\n\\n3.5. Functionally Important Enzyme Site Discovery\\n\\nFunctional sites are often evolutionarily conserved in a given enzyme family (Tristem, 2000; Bickel et al., 2002). Following previous methods (Armon et al., 2001; Panchenko et al., 2004; Chakrabarti & Lanczycki, 2007; Bray et al., 2009), we leverage multiple sequence alignment (MSA) to mine evolutionarily conserved patterns, which are known as functionally or structurally important sites. Designing enzymes conditioning on these conserved patterns can facilitate the generation of functional enzymes. We employ the ClustalW2 method (Anderson et al., 2011) to perform multiple sequence alignment for all proteins in each enzyme family corresponding to BRENDA fourth-level EC tree category. We select the residues that occur at the same site in more than $\\\\tau$ sequences. These residues are designated as functionally important sites. In practice, we set $\\\\tau$ to be $30\\\\%$. Figure 2 illustrate the procedure to select functionally important sites.\\n\\n4. Experiments\\n\\n4.1. EnzyBench Construction\\n\\nWe aim to evaluate the effectiveness of our EnzyGen across a diverse range of enzymes. To achieve this, we gather all enzymes from BRENDA (Schomburg et al., 2002), yielding...\"}"}
{"id": "ATvN9JnqZ8", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates\\n\\nProtein Family 1.1.1 1.11.1 1.14.13 1.14.14 1.2.1 2.1.1 2.3.1 2.4.1 2.4.2 2.5.1 2.6.1 2.7.1 2.7.10 2.7.11 2.7.4\\n\\nPROTSEED 0.54 0.67 0.24 0.39 0.57 0.83 0.52 0.29 0.75 0.58 0.45 0.77 0.88 0.81 0.78\\n\\nRFDiffusion+IF 0.45 0.94 0.54 0.39 0.47 0.43 0.48 0.39 0.52 0.46 0.53 0.50 0.51 0.60 0.55\\n\\nESM2+EGNN 0.58 0.90 0.35 0.35 0.63 0.79 0.53 0.32 0.80 0.59 0.51 0.76 0.88 0.88 0.77\\n\\nEnzyGen 0.64 0.98 0.38 0.42 0.72 0.80 0.61 0.38 0.86 0.66 0.53 0.76 0.92 0.93 0.80\\n\\nProtein Family 2.7.7 3.1.1 3.1.3 3.1.4 3.2.2 3.4.19 3.4.21 3.5.1 3.5.2 3.6.1 3.6.4 3.6.5 4.1.1 4.2.1 4.6.1 Avg\\n\\nPROTSEED 0.69 0.70 0.90 0.84 0.48 0.29 0.69 0.31 0.10 0.50 0.57 0.37 0.84 0.83 0.42 0.59\\n\\nRFDiffusion+IF 0.53 0.33 0.61 0.62 0.49 0.62 0.45 0.47 0.44 0.55 0.63 0.59 0.84 0.45 0.53\\n\\nESM2+EGNN 0.70 0.71 0.78 0.82 0.43 0.22 0.56 0.35 0.11 0.61 0.73 0.37 0.81 0.89 0.54 0.61\\n\\nEnzyGen 0.79 0.76 0.62 0.88 0.47 0.26 0.73 0.40 0.14 0.66 0.78 0.40 0.80 0.93 0.57 0.65\\n\\nTable 2. ESP scores (\u2191) of generated enzymes for 30 testing BRENDA EC categories. ESP evaluates the enzyme-substrate interaction function. Each number is the average ESP score of 50 testing cases for each category. IF denotes the inverse folding model ProteinMPNN. A ESP score of 0.6 or higher indicates a positive substrate binding. The last column (Avg) is the average across 30 categories. Notice that our EnzyGen outperforms all previous methods in ESP by a big margin.\\n\\nTable 3. Enzyme-substrate binding affinity (\u2193) (kcal/mol) for 30 testing BRENDA EC categories, evaluated by a docking software Gnina. IF denotes the inverse folding model ProteinMPNN. The last column (Avg) is the average across 30 categories. Notice that our EnzyGen outperforms all previous methods in binding affinity by a big margin.\\n\\nTable 4. pLDDT (\u2191) predicted by AlphaFold2. IF denotes the inverse folding model ProteinMPNN. The last column (Avg) is the average across 30 categories. Notice that our EnzyGen outperforms all previous methods in pLDDT. It shows EnzyGen's enzyme can fold more stably.\\n\\na total of 101,974 PDB entries. These entries are classified into 3,157 fourth-level enzyme categories based on the EC Tree (Appendix Figure 5). It is noteworthy that there exist a total of 8,422 fourth-level categories; however, our focus lies specifically on those with experimentally confirmed structures. This focus results in our dataset comprising 3,157 fourth-level categories. We then conduct MSAs for each fourth-level category to identify functionally important sites, using an MSA threshold (\u03c4) of 30%. Given the relatively small size of fourth-level categories, we aggregate data belonging to the same third-level category in the EC Tree, resulting in 256 third-level categories. Then we select 30 third-level categories for validation and testing, respectively including 428 and 323 fourth-level categories. For each of the 30 third-level categories, we randomly split 100 PDB entries with 50 for validation and 50 for testing, while the remaining entries are utilized for training. To prevent data leakage, we cluster the PDB entries with a sequence identity threshold of 50%. We ensure that no PDB entries in the validation or test sets belong to the same cluster as those in the training set. We further collect 23,711 experimentally confirmed <enzyme, substrate> pairs from Kroll et al. (2023), ensuring enzymes without a corresponding substrate have a randomly sampled negative one during the training process. We also make sure that all entries in the test set have a corresponding substrate to ensure function evaluation. Detailed data statistics are provided in Table 1 and Appendix C.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates\\n\\n(a) ESP score ($\\\\uparrow$)\\n\\n(b) pLDDT ($\\\\uparrow$)\\n\\n(c) Substrate binding affinity ($\\\\downarrow$)\\n\\n(d) ESP score ($\\\\uparrow$)\\n\\nFigure 3. (a) Ablation study comparing EnzyGen against ESM2+EGNN on ESP score. (b) Ablation study comparing EnzyGen against ESM2+EGNN on AlphaFold2 pLDDT. (c) Ablation study comparing EnzyGen against the model removing enzyme-substrate interaction constraint (EnzyGen-w/o-sub). (d) Ablation study on different model scales.\\n\\n4.2. Experimental Setup\\n\\nImplementation Details. We use 33 NAELs to model enzymes and 3 neighborhood equivariant layers for substrate representation module with node feature dimensionality set to 1280. As suggested by Ying et al. (2018), the number of graph layers typically ranges from 2 to 6. To prevent over-fitting and facilitate efficiency, we utilize 3 neighborhood equivariant sub-layers for enzyme modeling, comprising a modified NAEL with every 11 global attention sub-layers followed by one neighborhood equivariant sub-layer. The parameters of the global attention sub-layer are initialized with the released 650M ESM-2 (Lin et al., 2022b) parameters. The total parameter count is 714M. The hyperparameters $\\\\lambda/2$ and $K$ are set to 1.0 and 30, respectively. The model undergoes training for 1,000,000 steps using 8 NVIDIA RTX A6000 GPU cards, among which only the sequence generation and position prediction losses are adopted for the first 200,000 steps. The batch size and learning rate are set to 8192 tokens and $3\\\\times10^{-4}$ respectively. Sequences are decoded using the greedy decoding strategy. We incorporate four-level enzyme tags in the EC tree. For example, an enzyme in the category 1.1.1.1 should have four tags: 1, 1.1, 1.1.1, and 1.1.1.1.\\n\\nBaseline Models. We compare the proposed EnzyGen against the following representative baselines: (1) ESM2+EGNN employs ESM2 (Lin et al., 2022b) as the sequence encoder and EGNN (Satorras et al., 2021) as the backbone structure predictor. The node features in EGNN are initialized with the output from ESM2. Both ESM2 and EGNN share the same layer configurations as our EnzyGen. (2) PROTSEED (Shi et al., 2022) co-designs protein sequences and backbone structures based on secondary structure and binary contact maps. (3) RFDiffusion+ProteinMPNN first applies RFDiffusion (Watson et al., 2023) to design an enzyme structure based on the given functionally important sites and then uses ProteinMPNN (Dauparas et al., 2022) to design a sequence based on the generated structure. To ensure a fair and reliable comparison, we train ESM2+EGNN, PROTSEED and ProteinMPNN on our EnzyBench using their official codes and implementations. Since RFDiffusion does not provide a training script, we encountered challenges in training their code and reproducing their results. Consequently, we directly apply their released model to design the structures.\\n\\nFunction Evaluation. We evaluate the enzyme-substrate interaction function by the following metrics: (1) Following previous work (Li et al., 2013; Vidal-Limon et al., 2022), we use enzyme-substrate binding affinity, calculated by Gnina (McNutt et al., 2021) to quantify the strength of interactions between the designed enzymes and their substrates. A lower binding affinity indicates stronger binding. (2) We also use ESP score, developed by Kroll et al. (2023) to assess the ability of the designed enzymes to bind their corresponding substrates. In their paper, ESP model predicts enzyme-substrate interaction with 91% accuracy across multiple benchmarks. The scale of ESP score is 0-1, and a higher ESP score indicates a stronger enzyme-substrate interaction. Additionally, to evaluate if the designed enzymes are well-folded, we compute $pLDDT$ using AlphaFold2.\\n\\n4.3. Main Results\\n\\nThe ESP scores, substrate binding affinities, and $pLDDT$ results are shown in Tables 2, 3 and 4. For clarity, we average the performance of fourth-level categories under the same third-level one, with detailed results for 323 families in Appendix D.1. The results for top-1, top-5 and top-10 candidate are also provided in Appendix D.2. EnzyGen excels in designing enzymes that bind their respective substrates with high affinities. Table 2 and 3 highlight the ability of EnzyGen in achieving the highest average ESP score and substrate binding affinity across 323 families. Notably, EnzyGen obtains an average ESP score of 0.65, surpassing the suggested enzyme-substrate interaction threshold of 0.6 from the ESP evaluator developer. In comparison, PROTSEED achieves 0.59, RFDiffusion+ProteinMPNN achieves 0.53, and ESM2+EGNN achieves 0.61 of average ESP score. EnzyGen's ESP score is significantly higher than the others.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This is further verified by the binding affinity scores. The enzymes generated by our EnzyGen exhibit stronger binding affinity to their corresponding substrates than all other methods across all 30 third-level testing EC categories. EnzyGen gains an average improvement of $-0.87$ in binding affinity over the previous best method (RFDiffusion+ProteinMPNN), which is significant. These results demonstrate that enzymes designed by our EnzyGen are able to bind their corresponding substrates with high affinities.\\n\\nEnzyGen is able to design well-folded enzymes. According to the findings of Guo et al. (2022) and Binder et al. (2022), high pLDDT scores (e.g., > 80) indicate high confidence of the residue structure. EnzyGen attains an average pLDDT score of $87.45$ across 323 families. It outperforms all three baselines (PROTSEED, RFDiffusion+ProteinMPNN, ESM2+EGNN). This demonstrates the capability of our model to design enzymes with stable folding, validating the value of incorporating structural information into the enzyme design process.\\n\\n5. Analysis: Diving Deep into EnzyGen\\n\\n5.1. Ablation Study: Do Interleaving Network and Substrate Constraint Help?\\n\\nThey both contribute to designed enzymes with better substrate binding. In comparison to directly concatenating ESM2 and EGNN, Figure 3 (a) illustrates that EnzyGen consistently enhances the average ESP score across 323 families when considering different candidate sets. In Figure 3 (b), EnzyGen achieves higher AlphaFold2 pLDDT than ESM2+EGNN across various candidate sets. These results affirm that the interleaving network within our proposed NAEL facilitates information exchange at different granularities, thereby aiding the design of well-folded enzymes with better substrate binding functions. We further study whether the proposed enzyme-substrate interaction constraint is useful. As shown in Figure 3 (c), incorporating the enzyme-substrate interaction constraint into the training process improves substrate binding affinities, boosting enzyme design with stronger substrate binding.\\n\\n5.2. Ablation Study: Does Model Scale Help?\\n\\nEnzyGen is scalable. To assess the scalability of our EnzyGen, we compare our EnzyGen (714M) with a randomly initialized 12-layer model (117M) trained on the same dataset. Figure 3 (d) presents the average ESP score across 323 categories for different candidate set. The results indicate that EnzyGen attains a higher enzyme-substrate interaction score as the model scales up, and this difference becomes more obvious when more candidates are considered. This confirms the scalability advantage of our EnzyGen.\\n\\n5.3. Does Further Finetuning Bring Additional Benefits?\\n\\nEnzyGen shows improvement with further finetuning. Drawing inspiration from the strategy of finetuning pretrained multilingual models for specific languages (Liu et al., 2020; Conneau et al., 2020), we conduct further finetuning on each third-level category. Each category undergoes an additional finetuning of 30 epochs, leading to EnzyGen-finetune. As presented in Table 5, 15 categories exhibit improvements on ESP scores after finetuning. Simultaneously, the performance of 8 categories shows degradation, and it is noted that all of them have training sizes smaller than 1000, except for 2.3.1. These findings demonstrate that EnzyGen can indeed benefit from the further finetuning technique, with larger training data sizes yielding more substantial improvements.\\n\\n5.4. Zero-shot Generalization Capability\\n\\nTo assess the generalization capabilities of our EnzyGen, we respectively select two unseen substrates (magnesium(2+), zinc(2+)) and two unseen fourth-level categories (2.7.13.3, 6.3.2.4) from Swiss-Prot, which are not included in our EnzyBench benchmark. For these enzyme cases, we utilize their structures provided by the AlphaFold Database. The ESP scores of the top-32 candidates for each new fourth-level category or substrate are presented in Figure 4 (a). Remarkably, the average ESP score is $0.83$ across two fourth-level categories and $0.89$ across two substrates, with more than $85\\\\%$ of the cases scoring over 0.6. These observations confirm that functionally important sites are crucial to design functional enzymes. Even not trained on these categories, our EnzyGen exhibits the capability to design enzymes with substrate binding ability.\\n\\n5.5. Does EnzyGen Learn the Family Relationship?\\n\\nFamilies with closer functions have closer embeddings. We cluster the 3,157 fourth-level family category embeddings in Figure 4 (b). It shows that enzyme families from the same superfamily are clustered together, displaying closer tag representations in the embedding space. This observation confirms that our family tag learns useful function information which can provide guidance for the design of desirable enzymes.\\n\\n5.6. Designing Novel Enzymes with EnzyGen\\n\\nEnzyGen is able to design novel enzymes that bind to specific substrates. Figure 4 (c) and (d) showcase two designed enzymes based on PDB 1KAG (catalyzing the specific phosphorylation of the 3-hydroxyl group of shikimic acid) and 5L2P (hydrolyzing various p-nitrophenyl phosphates, aromatic esters and p-nitrophenyl fatty acids) respectively. Both of the two designed enzymes achieve pLDDT\"}"}
{"id": "ATvN9JnqZ8", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates\\n\\nNew Fourth-level Class\\n\\nZero-shot Generalization Capability\\n\\nFigure 4. (a) ESP scores of designed enzymes from new fourth-level classes or with new substrates. Dash line denotes median. (b) Fourth-level category embedding clustering. Case Study: (c) Complex of designed 1KAG (2.7.1.71, catalyzing the specific phosphorylation of the 3-hydroxyl group of shikimic acid) and substrate ATP(-4), with pLDDT=90.39, Uniprot blastp recovery rate = 58.5%, (d) Complex of designed 5L2P (3.1.1.2, hydrolyzing various p-nitrophenyl phosphates, aromatic esters and p-nitrophenyl fatty acids) and substrate paraoxon, with pLDDT=89.44, Uniprot blastp recovery rate = 49.4%. Both cases show polar contacts (hydrogen bonds) depicted in purple.\\n\\nTable 5. ESP score (\u2191) of EnzyGen and EnzyGen-finetune. Avg denotes average. Finetuning leads to improvement in ESP scores.\\n\\nProtein Family 1.1.1 1.11.1 1.14.13 1.14.14 1.2.1 2.1.1 2.3.1 2.4.1 2.4.2 2.5.1 2.6.1 2.7.1 2.7.10 2.7.11 2.7.4\\nEnzyGen-finetune 0.68 0.95 0.38 0.37 0.68 0.82 0.57 0.38 0.86 0.69 0.54 0.80 0.93 0.94 0.82\\nEnzyGen 0.64 0.98 0.38 0.42 0.72 0.80 0.61 0.38 0.86 0.66 0.53 0.76 0.92 0.93 0.80\\n\\nProtein Family 2.7.7 3.1.1 3.1.3 3.1.4 3.2.2 3.4.19 3.4.21 3.5.1 3.5.2 3.6.1 3.6.4 3.6.5 4.1.1 4.2.1 4.6.1 Avg\\nEnzyGen-finetune 0.79 0.77 0.86 0.88 0.48 0.24 0.63 0.40 0.20 0.68 0.79 0.37 0.89 0.93 0.56 0.66 0.89\\nEnzyGen 0.79 0.76 0.62 0.88 0.47 0.26 0.73 0.40 0.14 0.66 0.78 0.40 0.80 0.93 0.57 0.65 0.65\\n\\n6. Discussion\\nEnzymes, as genetically encoded biocatalysts, play a crucial role in accelerating chemical reactions and find extensive applications across various fields. Designing enzymes that can specifically bind to target substrates has always been a complex challenge. Our EnzyGen demonstrates a remarkable ability to design well-folded and effective enzymes that bind to specific substrates across diverse enzyme categories. Despite its impressive performance, certain limitations remain, which we will address in this section.\\n\\nFirst, we collected enzyme data from the PDB, covering 3,157 fourth-level enzyme classes. Although there are 8,422 fourth-level classes in total, our dataset includes only a subset of these categories. Future work could focus on expanding this dataset by incorporating data from larger databases, such as Swiss-Prot and the AlphaFold structure database, to encompass a broader range of enzyme classes.\\n\\nSecond, we address the substrate-binding constraint using an enzyme-substrate binding prediction loss. While this provides a basic constraint, it is relatively weak concerning the enzyme's catalytic functions. Future research could focus on designing enzymes that bind to a specific 3D substrate structure, potentially leading to the formation of a more effective enzyme-substrate complex.\\n\\n7. Conclusion\\nThis paper introduces EnzyGen, a unified generative model for designing functional enzymes across diverse families. EnzyGen simultaneously generates enzyme sequence and backbone structure guided by automatically identified functionally important sites and a given substrate. We employ a set of three losses to train EnzyGen. To comprehensively evaluate EnzyGen, we construct EnzyBench, a benchmark for enzyme design, including all available enzymes within PDB across 3157 enzyme families. Experimental results demonstrate EnzyGen's capability to design well-folded enzymes with strong enzyme-substrate interaction functions.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThis research is supported by the National Institutes of Health (R35GM147387 to Y.Y.), a seed grant from the NSF Molecule Maker Lab Institute (grant #2019897), the UC Santa Barbara Faculty Research Grant (to L.L.), and the generous support by Pittsburgh Supercomputing Center Neo-cortex grant. The authors thank the anonymous reviewers and Siqi Ouyang, Yuwei Yang, Yufei Song, Jielin Qiu, and Yujia Gao for their valuable comments.\\n\\nImpact Statement\\n\\nThis paper contributes to the effective and efficient design of functional enzymes, thereby advancing the fields of functional protein design and AI for molecule design. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\\n\\nReferences\\n\\nAnderson, C. L., Strope, C. L., and Moriyama, E. N. Suitems: visual tools for multiple sequence alignment comparison and molecular sequence simulation. BMC bioinformatics, 12(1):1\u201314, 2011.\\n\\nAnishchenko, I., Pellock, S. J., Chidyausiku, T. M., Ramelot, T. A., Ovchinnikov, S., Hao, J., Bafna, K., Norn, C., Kang, A., Bera, A. K., et al. De novo protein design by deep network hallucination. Nature, 600(7889):547\u2013552, 2021.\\n\\nArmon, A., Graur, D., and Ben-Tal, N. Consurf: an algorithmic tool for the identification of functional regions in proteins by surface mapping of phylogenetic information. Journal of molecular biology, 307(1):447\u2013463, 2001.\\n\\nAshburner, M., Ball, C. A., Blake, J. A., Botstein, D., Butler, H., Cherry, J. M., Davis, A. P., Dolinski, K., Dwight, S. S., Eppig, J. T., et al. Gene ontology: tool for the unification of biology. Nature genetics, 25(1):25\u201329, 2000.\\n\\nBar-Even, A., Noor, E., Savir, Y., Liebermeister, W., Davidi, D., Tawfik, D. S., and Milo, R. The moderately efficient enzyme: evolutionary and physicochemical trends shaping enzyme parameters. Biochemistry, 50(21):4402\u20134410, 2011.\\n\\nBickel, P. J., Kechris, K. J., Spector, P. C., Wedemayer, G. J., and Glazer, A. N. Finding important sites in protein sequences. Proceedings of the National Academy of Sciences, 99(23):14764\u201314771, 2002.\\n\\nBinder, J. L., Berendzen, J., Stevens, A. O., He, Y., Wang, J., Dokholyan, N. V., and Oprea, T. I. Alphafold illuminates half of the dark human proteins. Current Opinion in Structural Biology, 74:102372, 2022.\\n\\nBinz, H. K., Amstutz, P., and Pl\u00fcckthun, A. Engineering novel binding proteins from non-immunoglobulin domains. Nature biotechnology, 23(10):1257\u20131268, 2005.\\n\\nBray, T., Chan, P., Bougouffa, S., Greaves, R., Doig, A. J., and Warwicker, J. Sitesidentify: a protein functional site prediction tool. BMC bioinformatics, 10(1):1\u201312, 2009.\\n\\nBrookes, D., Park, H., and Listgarten, J. Conditioning by adaptive sampling for robust design. In International conference on machine learning, pp. 773\u2013782. PMLR, 2019.\\n\\nBrookes, D. H. and Listgarten, J. Design by adaptive sampling. arXiv preprint arXiv:1810.03714, 2018.\\n\\nCarbonell, P., Wong, J., Swainston, N., Takano, E., Turner, N. J., Scrutton, N. S., Kell, D. B., Breitling, R., and Faulon, J.-L. Selenzyme: enzyme selection tool for pathway design. Bioinformatics, 34(12):2153\u20132154, 2018.\\n\\nChakrabarti, S. and Lanczycki, C. J. Analysis and prediction of functionally important sites in proteins. Protein Science, 16(1):4\u201313, 2007.\\n\\nChi, Z., Dong, L., Wei, F., Wang, W., Mao, X.-L., and Huang, H. Cross-lingual natural language generation via pre-training. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7570\u20137577, 2020.\\n\\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020.\\n\\nDas, P., Sercu, T., Wadhawan, K., Padhi, I., Gehrmann, S., Cipcigan, F., Chenthamarakshan, V., Strobelt, H., Dos Santos, C., Chen, P.-Y., et al. Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations. Nature Biomedical Engineering, 5(6):613\u2013623, 2021.\\n\\nDauparas, J., Anishchenko, I., Bennett, N., Bai, H., Ragotte, R. J., Milles, L. F., Wicky, B. I., Courbet, A., de Haas, R. J., Bethel, N., et al. Robust deep learning\u2013based protein sequence design using proteinmpnn. Science, 378(6615):49\u201356, 2022.\\n\\nDetlefsen, N. S., Hauberg, S., and Boomsma, W. Learning meaningful representations of protein sequences. Nature communications, 13(1):1914, 2022.\\n\\nFerruz, N., Schmidt, S., and H\u00f6cker, B. Protgpt2 is a deep unsupervised language model for protein design. Nature communications, 13(1):4348, 2022.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "ATvN9JnqZ8", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates\\n\\nSercu, T., Candido, S., et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022b.\\n\\nLiu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M., and Zettlemoyer, L. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726\u2013742, 2020.\\n\\nMadani, A., Krause, B., Greene, E. R., Subramanian, S., Mohr, B. P., Holton, J. M., Olmos Jr, J. L., Xiong, C., Sun, Z. Z., Socher, R., et al. Large language models generate functional protein sequences across diverse families. Nature Biotechnology, pp. 1\u20138, 2023.\\n\\nMcNutt, A. T., Francoeur, P., Aggarwal, R., Masuda, T., Meli, R., Ragoza, M., Sunseri, J., and Koes, D. R. Gnina 1.0: molecular docking with deep learning. Journal of cheminformatics, 13(1):1\u201320, 2021.\\n\\nMelnyk, I., Das, P., Chenthamarakshan, V., and Lozano, A. Benchmarking deep generative models for diverse antibody sequence design. arXiv preprint arXiv:2111.06801, 2021.\\n\\nPanchenko, A. R., Kondrashov, F., and Bryant, S. Prediction of functional sites by analysis of sequence and structure conservation. Protein science, 13(4):884\u2013892, 2004.\\n\\nPearce, R. and Zhang, Y. Deep learning techniques have significantly impacted protein structure prediction and protein design. Current opinion in structural biology, 68:194\u2013207, 2021.\\n\\nRen, Z., Li, J., Ding, F., Zhou, Y., Ma, J., and Peng, J. Proximal exploration for model-guided protein sequence design. bioRxiv, 2022.\\n\\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L., Ma, J., et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences, 118(15):e2016239118, 2021.\\n\\nSatorras, V. G., Hoogeboom, E., and Welling, M. Equivariant graph neural networks. In International conference on machine learning, pp. 9323\u20139332. PMLR, 2021.\\n\\nSchomburg, I., Chang, A., and Schomburg, D. Brenda, enzyme data and metabolic information. Nucleic acids research, 30(1):47\u201349, 2002.\\n\\nSchomburg, I., Chang, A., Ebeling, C., Gremse, M., Heldt, C., Huhn, G., and Schomburg, D. Brenda, the enzyme database: updates and major new developments. Nucleic acids research, 32(suppl_1):D431\u2013D433, 2004.\\n\\nShi, C., Wang, C., Lu, J., Zhong, B., and Tang, J. Protein sequence and structure co-design with equivariant translation. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nSong, Z. and Li, L. Importance weighted expectation-maximization for protein sequence design. In International Conference on Machine Learning, pp. 32349\u201332364. PMLR, 2023.\\n\\nSong, Z., Zhou, H., Qian, L., Xu, J., Cheng, S., Wang, M., and Li, L. switch-glat: Multilingual parallel machine translation via code-switch decoder. In International Conference on Learning Representations, 2021.\\n\\nSong, Z., Zhao, Y., Shi, W., Yang, Y., and Li, L. Functional geometry guided protein sequence and backbone structure co-design. arXiv preprint arXiv:2310.04343, 2023.\\n\\nStrokach, A., Becerra, D., Corbi-Verge, C., Perez-Riba, A., and Kim, P. M. Fast and flexible protein design using deep graph neural networks. Cell systems, 11(4):402\u2013411, 2020.\\n\\nSumida, K. H., N\u00fa\u00f1ez Franco, R., Kalvet, I., Pellock, S. J., Wicky, B. I., Milles, L. F., Dauparas, J., Wang, J., Kipnis, Y., Jameson, N., et al. Improving protein expression, stability, and function with proteinmpnn. bioRxiv, pp. 2023\u201310, 2023.\\n\\nTrippe, B. L., Yim, J., Tischer, D., Baker, D., Broderick, T., Barzilay, R., and Jaakkola, T. S. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. In The Eleventh International Conference on Learning Representations, 2022.\\n\\nTristem, M. Molecular evolution\u2014a phylogenetic approach. Heredity, 84(1):131\u2013131, 2000.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nVidal-Limon, A., Aguilar-Toala, J. E., and Liceaga, A. M. Integration of molecular docking analysis and molecular dynamics simulations for studying food proteins and bioactive peptides. Journal of Agricultural and Food Chemistry, 70(4):934\u2013943, 2022.\\n\\nWang, J., Cao, H., Zhang, J. Z., and Qi, Y. Computational protein design with deep learning neural networks. Scientific reports, 8(1):1\u20139, 2018.\\n\\nWang, J., Lisanza, S., Juergens, D., Tischer, D., Watson, J. L., Castro, K. M., Ragotte, R., Saragovi, A., Milles, L. F., Baek, M., et al. Scaffolding protein functional sites using deep learning. Science, 377(6604):387\u2013394, 2022.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "ATvN9JnqZ8", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Protein Family | PROTSEED | RFDiffusion+IF | ESM2+EGNN | EnzyGen |\\n|---------------|----------|---------------|-----------|---------|\\n| 1.1.1 1.11.1 1.14.13 1.14.14 1.2.1 2.1.1 2.3.1 2.4.1 2.4.2 2.5.1 2.6.1 2.7.1 2.7.10 2.7.11 2.7.4 Avg | -8.37 -6.90 -7.08 -11.69 -6.61 -7.94 -12.26 -11.83 -12.93 -10.09 -8.30 -8.15 -10.09 -12.70 -10.01 | -6.54 -7.39 -7.09 -11.63 -11.89 -12.59 -12.39 -9.49 -14.11 -7.87 -8.81 -10.99 -14.19 -14.71 -11.53 | -12.93 -10.03 -8.74 -10.39 -8.20 -14.38 -12.40 -11.41 -14.42 -12.00 -11.01 -8.74 -13.46 -13.32 -12.23 | -16.63 -7.20 -6.85 -13.03 -14.72 -12.43 -14.21 -11.84 -16.31 -15.36 -9.23 -13.31 -12.92 -15.01 -13.56 |\\n| 2.7.7 3.1.1 3.1.3 3.1.4 3.2.2 3.4.19 3.4.21 3.5.1 3.5.2 3.6.1 3.6.4 3.6.5 4.1.1 4.2.1 4.6.1 Avg | -9.06 -9.79 -8.27 -9.02 -13.22 -9.77 -10.59 -6.70 -7.65 -10.32 -11.32 -13.50 -8.97 -5.74 -10.11 -9.63 | -9.56 -6.81 -10.50 -10.00 -12.11 -12.35 -12.05 -9.66 -7.22 -13.16 -14.26 -11.10 -9.47 -5.92 -9.84 -10.51 | -11.30 -10.19 -10.85 -10.24 -13.65 -12.42 -12.80 -8.80 -9.61 -12.59 -15.68 -13.29 -10.36 -10.35 -14.61 -11.68 | -11.34 -13.10 -11.36 -12.66 -12.79 -15.25 -11.95 -11.15 -13.08 -12.24 -13.93 -14.17 -11.60 -9.34 -11.76 -12.61 |\"}"}
{"id": "ATvN9JnqZ8", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Protein Family     | PROTSEED | RFDiffusion+IF | ESM2+EGNN | EnzyGen |\\n|--------------------|----------|----------------|-----------|---------|\\n| 1.1.1               | 86.28    | 83.88          | 96.35     | 96.49   |\\n| 1.1.1               | 82.47    | 97.98          | 97.19     | 97.81   |\\n| 1.1.1               | 73.36    | 92.53          | 88.50     | 95.67   |\\n| 1.1.1               | 86.48    | 84.18          | 92.12     | 91.55   |\\n| 1.1.1               | 91.38    | 88.08          | 96.64     | 41.93   |\\n| 1.1.1               | 64.90    | 84.90          | 77.03     | 97.35   |\\n| 1.1.1               | 91.83    | 76.39          | 95.73     | 85.52   |\\n| 1.1.1               | 74.33    | 98.00          | 94.09     | 97.30   |\\n| 1.1.1               | 89.81    | 71.68          | 94.69     | 98.55   |\\n| 1.1.1               | 74.43    | 76.93          | 96.40     | 91.58   |\\n| 1.1.1               | 72.72    | 76.79          | 96.44     | 77.43   |\\n| 1.1.1               | 79.61    | 97.88          | 93.70     | 96.18   |\\n| 1.1.1               | 78.34    | 77.24          | 94.43     | 95.08   |\\n| 1.1.1               | 70.53    | 95.80          | 92.31     | 74.63   |\\n| 1.1.1               | 92.71    | 81.25          | 97.50     | \u2013       |\\n| 2.1.1               | 64.90    | 84.90          | 95.49     | \u2013       |\\n| 2.1.1               | 91.83    | 76.39          | 95.73     | \u2013       |\\n| 2.1.1               | 74.33    | 98.00          | 94.09     | \u2013       |\\n| 2.1.1               | 89.81    | 71.68          | 94.69     | \u2013       |\\n| 2.1.1               | 74.43    | 76.93          | 96.44     | \u2013       |\\n| 2.1.1               | 72.72    | 76.79          | 96.44     | \u2013       |\\n| 2.1.1               | 79.61    | 97.88          | 93.70     | \u2013       |\\n| 2.1.1               | 78.34    | 77.24          | 94.43     | \u2013       |\\n| 2.1.1               | 70.53    | 95.80          | 92.31     | \u2013       |\\n| 2.1.1               | 92.71    | 81.25          | 97.50     | \u2013       |\\n| 2.3.1               | 96.35    | 93.70          | 94.43     | \u2013       |\\n| 2.3.1               | 97.19    | 95.49          | 92.31     | \u2013       |\\n| 2.3.1               | 88.50    | 77.03          | 79.01     | \u2013       |\\n| 2.3.1               | 92.12    | 98.18          | 97.69     | \u2013       |\\n| 2.3.1               | 96.64    | 42.35          | 97.69     | \u2013       |\\n| 2.3.1               | 77.03    | 24.73          | 24.73     | \u2013       |\\n| 2.3.1               | 95.73    | 79.01          | 97.69     | \u2013       |\\n| 2.3.1               | 94.09    | 98.18          | 97.69     | \u2013       |\\n| 2.3.1               | 94.69    | 42.35          | 97.69     | \u2013       |\\n| 2.3.1               | 96.40    | 24.73          | 24.73     | \u2013       |\\n| 2.3.1               | 96.44    | 79.01          | 97.69     | \u2013       |\\n| 2.3.1               | 93.70    | 98.18          | 97.69     | \u2013       |\\n| 2.3.1               | 94.43    | 42.35          | 24.73     | \u2013       |\\n| 2.3.1               | 92.31    | 97.69          | 97.69     | \u2013       |\\n| 2.3.1               | 97.50    | 97.69          | 97.69     | \u2013       |\\n| 2.3.1               | 81.25    | 97.69          | 97.69     | \u2013       |\\n| 2.3.1               | 84.60    | 24.73          | 24.73     | \u2013       |\\n| 2.3.1               | 93.36    | 79.01          | 97.69     | \u2013       |\\n| 3.1.1               | 92.04    | 77.49          | 96.63     | \u2013       |\\n| 3.1.1               | 77.49    | 94.79          | 79.85     | \u2013       |\\n| 3.1.1               | 79.17    | 77.33          | 66.33     | \u2013       |\\n| 3.1.1               | 80.26    | 87.29          | 80.00     | \u2013       |\\n| 3.1.1               | 76.65    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 96.63    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 74.94    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 79.85    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 66.33    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 80.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 54.46          | 82.00     | \u2013       |\\n| 3.1.1               | 82.00    | 94.79          | 66.33     | \u2013       |\\n| 3.1.1               | 82.00    | 77.33          | 80.00     | \u2013       |\\n| 3.1.1               | 82.00    | 87.29          | 82.00     |"}
{"id": "ATvN9JnqZ8", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 13. Enzyme-substrate ESP score ($\u2191$) for the top-5 candidate. IF denotes the inverse folding model ProteinMPNN. Note 0.6 or higher ESP score indicates a positive binding. Avg denotes average.\\n\\n| Protein Family | PROTSEED | RFDiffusion+IF | ESM2+EGNN | EnzyGen |\\n|---------------|----------|---------------|-----------|---------|\\n| 1.1.1         | -5.99    | -7.03         | -6.85     | -14.11  |\\n| 1.11.1        | -5.93    | -6.43         | -8.46     | -5.63   |\\n| 1.14.13       | -5.55    | -6.70         | -7.82     | -6.71   |\\n| 1.14.14       | -11.17   | -13.24        | -14.01    | -11.02  |\\n| 1.2.1         | -6.64    | -8.42         | -6.97     | -10.05  |\\n| 2.1.1         | -9.61    | -12.59        | -12.04    | -10.93  |\\n| 2.3.1         | -11.83   | -11.36        | -12.65    | -10.93  |\\n| 2.4.1         | -8.80    | -10.01        | -11.80    | -11.58  |\\n| 2.4.2         | -11.84   | -12.74        | -15.14    | -13.94  |\\n| 2.5.1         | -8.57    | -9.80         | -9.34     | -9.51   |\\n| 2.6.1         | -6.01    | -7.99         | -5.87     | -11.09  |\\n| 2.7.1         | -8.72    | -11.90        | -10.01    | -11.58  |\\n| 2.7.10        | -11.30   | -12.30        | -12.65    | -13.83  |\\n| 2.7.11        | -12.59   | -12.82        | -12.20    | -12.43  |\\n| 2.7.4         | \u2013        | \u2013             | \u2013         | \u2013       |\\n\\n### Table 14. Substrate binding affinity (kcal/mol, $\u2193$) for the top-5 candidate. IF denotes the inverse folding model ProteinMPNN. Avg denotes average.\\n\\n| Protein Family | PROTSEED | RFDiffusion+IF | ESM2+EGNN | EnzyGen |\\n|---------------|----------|---------------|-----------|---------|\\n| 1.1.1         | 81.07    | 82.80         | 93.35     | 92.85   |\\n| 1.11.1        | 81.80    | 93.51         | 91.25     | 92.68   |\\n| 1.14.13       | 74.37    | 81.12         | 90.00     | 90.25   |\\n| 1.14.14       | 84.17    | 81.17         | 87.68     | 90.25   |\\n| 1.2.1         | 79.28    | 82.44         | 95.28     | 98.29   |\\n| 2.1.1         | 72.95    | 78.82         | 69.96     | 82.15   |\\n| 2.3.1         | 69.86    | 88.36         | 91.43     | 93.39   |\\n| 2.4.1         | 78.31    | 83.53         | 86.50     | 93.34   |\\n| 2.4.2         | 80.06    | 82.32         | 86.93     | 95.11   |\\n| 2.5.1         | 79.16    | 94.44         | 91.76     | 95.11   |\\n| 2.6.1         | 76.61    | 78.41         | 93.92     | 75.91   |\\n| 2.7.1         | 79.83    | 81.93         | 89.53     | 98.22   |\\n| 2.7.10        | 80.33    | 87.38         | 90.64     | 76.31   |\\n| 2.7.11        | 79.20    | 81.50         | 88.98     | 91.46   |\\n| 2.7.4         | \u2013        | \u2013             | \u2013         | \u2013       |\\n\\n### Table 15. AlphaFold2 pLDDT ($\u2191$) for the top-5 candidate. IF denotes the inverse folding model ProteinMPNN. Avg denotes average.\\n\\n| Protein Family | PROTSEED | RFDiffusion+IF | ESM2+EGNN | EnzyGen |\\n|---------------|----------|---------------|-----------|---------|\\n| 1.1.1         | 0.90     | 0.68          | 0.89      | 0.95    |\\n| 1.11.1        | 0.67     | 0.95          | 0.96      | 1.00    |\\n| 1.14.13       | 0.38     | 0.54          | 0.59      | 0.64    |\\n| 1.14.14       | 0.98     | 0.47          | 0.94      | 0.99    |\\n| 1.2.1         | 0.78     | 0.69          | 0.62      | 1.00    |\\n| 2.1.1         | 0.99     | 0.54          | 0.62      | 0.88    |\\n| 2.3.1         | 0.82     | 0.55          | 0.88      | 0.88    |\\n| 2.4.1         | 0.55     | 0.48          | 0.95      | 0.99    |\\n| 2.4.2         | 0.98     | 0.71          | 0.95      | 0.99    |\\n| 2.5.1         | 0.96     | 0.53          | 0.94      | 0.27    |\\n| 2.6.1         | 0.99     | 0.73          | 0.93      | 0.99    |\\n| 2.7.1         | 0.80     | 0.66          | 0.98      | 0.99    |\\n| 2.7.10        | 0.99     | 0.96          | 0.99      | 0.99    |\\n| 2.7.11        | 0.99     | 0.98          | 1.00      | 1.00    |\\n| 2.7.4         | \u2013        | \u2013             | \u2013         | \u2013       |\\n\\n### Table 16. Enzyme-substrate ESP score ($\u2191$) for the top-10 candidate. IF denotes the inverse folding model ProteinMPNN. Note 0.6 or higher ESP score indicates a positive binding. Avg denotes average.\\n\\n| Protein Family | PROTSEED | RFDiffusion+IF | ESM2+EGNN | EnzyGen |\\n|---------------|----------|---------------|-----------|---------|\\n| 2.7.7         | 0.98     | 0.88          | 0.94      | 0.99    |\\n| 3.1.1         | 0.78     | 0.49          | 0.95      | 0.99    |\\n| 3.1.3         | 0.91     | 0.85          | 0.95      | 1.00    |\\n| 3.1.4         | 0.97     | 0.95          | 0.97      | 1.00    |\\n| 3.2.2         | 0.92     | 0.55          | 0.84      | 0.91    |\\n| 3.4.19        | 0.81     | 0.92          | 0.62      | 0.69    |\\n| 3.4.21        | 0.98     | 0.47          | 0.86      | 1.00    |\\n| 3.5.1         | 0.20     | 0.58          | 0.23      | 0.27    |\\n| 3.5.2         | 0.50     | 0.85          | 0.93      | 0.94    |\\n| 3.6.1         | 0.50     | 0.73          | 0.71      | 0.78    |\\n| 3.6.4         | 0.97     | 0.83          | 0.89      | 0.99    |\\n| 3.6.5         | 0.97     | 0.76          | 0.89      | 0.99    |\\n| 4.1.1         | 0.91     | 0.84          | 0.83      | 0.99    |\\n| 4.2.1         | 0.99     | 0.95          | 0.97      | 1.00    |\\n| 4.6.1         | 0.96     | 0.60          | 0.93      | 0.93    |\\n| Avg           | 0.85     | 0.77          | 0.92      | 0.95    |\\n\\nNote: PROTSEED, RFDiffusion+IF, ESM2+EGNN, and EnzyGen are different modeling techniques used in the study.\"}"}
{"id": "ATvN9JnqZ8", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 17.\\nSubstrate binding affinity (kcal/mol, \u2193) for the top-10 candidate. IF denotes the inverse folding model ProteinMPNN. Avg denotes average.\\n\\n| Protein Family | PROTSEED | RFDiffusion+IF | ESM2+EGNN | EnzyGen |\\n|---------------|----------|----------------|-----------|---------|\\n| 1.1.1 1.11.1 1.14.13 1.14.14 1.2.1 2.1.1 2.3.1 2.4.1 2.4.2 2.5.1 2.6.1 2.7.1 2.7.10 2.7.11 2.7.4 Avg | 75.84 81.80 72.90 83.84 77.32 77.66 75.62 77.34 79.14 75.58 74.20 77.36 79.65 78.30 77.38 | 75.57 91.36 81.12 87.85 79.33 81.03 85.67 86.62 77.50 88.98 79.52 86.97 86.75 81.85 87.24 | 93.22 91.68 90.10 86.29 94.90 78.73 90.94 87.93 87.07 91.15 93.99 87.45 90.51 88.13 94.98 | 90.05 91.30 92.56 97.56 98.08 89.81 88.91 93.67 91.56 86.30 97.75 73.66 90.53 94.81 96.02 |\\n| 2.7.7 3.1.1 3.1.3 3.1.4 3.2.2 3.4.19 3.4.21 3.5.1 3.5.2 3.6.1 3.6.4 3.6.5 4.1.1 4.2.1 4.6.1 Avg | 76.90 76.29 77.89 79.41 77.11 72.06 79.91 73.83 79.13 75.16 68.74 83.46 76.33 78.37 85.71 77.47 | 80.92 82.53 81.13 85.91 94.10 91.67 76.09 80.76 87.57 66.85 83.48 82.41 81.11 77.05 77.93 82.90 | 89.00 92.74 87.41 86.43 89.05 58.58 82.56 91.94 90.93 85.09 74.46 82.17 88.15 89.26 84.14 87.30 | 96.47 97.62 90.15 85.19 95.71 71.34 96.32 97.54 81.33 81.01 92.42 86.13 89.63 94.89 96.02 90.52 |\\n\\n### Table 18.\\nAlphaFold2 pLDDT (\u2191) for the top-10 candidate. IF denotes the inverse folding model ProteinMPNN. Avg denotes average.\\n\\n| PDB Source ID | Sequence |\\n|---------------|----------|\\n| 1KAG | MLPPIFLVGPMGAGKTSVGRELARRLGLEFLDSDREIEERVGLTV AWIFEELGEANFRRREEEVLRELFSLEPPVLATGGGA VTNRKNREFLKRHGTVVYLEVPV EELLRRLRALPLLAKLEEKFRALFERLV ALYRAAADIVVRNGLLVNLLVLLVR |\\n| 5L2P | MPLPPHLEYIIQMMLEKGGKGFTTMSEVEEIRDSLLQSASNTEPVEVDIDKIKETFKTSYGVKARQYFPIDNKAYPVVLYLHGGSWVIGSKFTHDKVCRAITVSC NCKVISVDYRLAPEYKFPAA VYECYDATKWIYENAKKINIDITKIAIAGNSAGGNLAAA VSLLSKEKNIKLKYQILLYPA VSFDLDTKSYKEFADGHFLDTDMIKFI GNNYLFNSKEDKLNPY ASPINFADLSNLPPALIVTAEYDPLRDVGEAY ANKLLQAGVDVTSIRYKGLVHAFASVLDEIGDTINIMGKLLKEYFK |\\n| 1L1E | MNLEERFFVLFLDPTQTYNCAYFERREMAEQEAQIAKIDAALGALGPEPGMTLLDIGCGWGATTRRLIEKYDVTVVGLDLAKEQANHVRQLLANSDNLRVRAA MLEKWDFFEEPVDRVVSIGAFEHFGYQDRAALFQKTKEHLKPDGGLLLHHIVVPKRREKQEQLLSPQHPLAKFKKLIKKEIFPGGNLPSYPQLMQAAEKAGWE VTREESLQLDGAYWWDWW ATAAAAHADEAIAIQSEFAYEYQHEALAAAAGDFELGYWDIQIFVLQK |\\n| 8DSO | HHHNSKSESLLLLKKKGLLGFGGKVFYGKWRGQYVV A VKKIRSQSSDESQFIEEVQVMQQLRHENLVQLYGVCTEGKDIYIITEYMAAGLLLYYVKKKKFQM QQILTICKAICSALEYLEEKTFLHHDLLKRNLLLNSEGV AKISDFGLAKFISSSSSTKEPFPPVVLEQSKFSSKSDIW AFGVLMWEEYSYGKMPFEKFTRQETLEHV AQGLRLYPPDIASEPVYKIMKECWHEKADERPSFSILLSNLEELQEELA |\\n| 5TMP | RGKFIVIEGLDGAGKSTNIDVVVEQLQRDGRREEWVFTREPGGTPLAEKLRELLLTPDDEPDDVDMDTEMVLMFEAARSQLVETVIKPALARGAWVLCDRHDL STY AYQGGGRGLPV AKLQALESFAINGLRPDLTLYLDVPVEIGLRRAKQRGKLERFEQERFQQRV AA VY AGRLERAAQQDDSWQTIDATQPLSTVSDAIRTHLQRL QSEL |\\n| 6W7O | HHLKLTFLFKKGELGGFFGGVFYGKWRGQYEV A VKKIRKQSSSESQFIDEVQVMQQLSHENLVQLYGCCTEGDDIYIITEYMANGSLLQYYLKKQHQFQQSQL LQIAKDIVSALEYLEELTLHHPGLSKRNLLLNSEGV AKITDFGLSRRVLDSDISIISSSFSLLPPIPEEELQQSKFTSKSDIWSFGVLMWEEYSYGKMPWQRFSQQE MEEHIRQQLRLYPPDLASPPVYTIMKECWHEKADERPSFSILLSNLVLLLE |\\n| 3UYO | GSTKSTSEESHSWYHGPIPREDAERLLVSGIHGSFLVRESESTAGDHSLSLFYEGVKYHYRINTALDGKLYVSEGIKFDTLAELVQHYKTHADGLCYVLSEPCPPV GEEEEEEE |\\n| 4NI2 | QVIAQKHDNVSILFSDIVGFTSLASQCSAQDLVMTLNELFSRFDKLCAENDVYKIETIGDAYCCVSGLPRKEPDHSQQICEMALDMMEVSSQVKTPHGKPINMRI GIHSGRVV AGVVGLMRRYYCLFGNDVNLANHMESGGVPGKINVPKETYELLKNDFSIEPRRGGSEECPENFPKKIVGYCLFVRRAAA |\\n\\n### Table 19.\\nEnzyme sequences for all analyzed cases.\"}"}
