{"id": "mUSPhG4uDW", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEB LINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nProject Information\\n\\nWe are collecting data for evaluating automated web navigation systems. The data consists of demonstrations of interactions between the user and the navigator. In each demonstration, the user and the system cooperate to achieve tasks in a web browser. The user controls the system via natural language instructions.\\n\\nHow To\\n\\nIngredients\\n\u2022 two people:\\n  \u2013 Instructor: creative, giving instructions\\n  \u2013 Navigator: systematic, following instructions\\n\u2022 Google Chrome\\n\u2022 Zoom\\n\u2022 internet connection\\n\\nPreparation\\n\\nYou need to do this process just once:\\n1. Download the Chrome extension ZIP file and unpack the extension folder to your local filesystem.\\n2. If you are using Chrome as your primary browser, create a new profile for the experiments.\\n3. Install the Chrome extension in the repository:\\n   \u2022 Open a new Google Chrome window.\\n   \u2022 Go to chrome://extensions/\\n   \u2022 At the top right, turn on Developer mode.\\n   \u2022 Click Load unpacked.\\n   \u2022 Find and select the extension folder you have unpacked before (make sure you are inside the folder).\\n   \u2022 Click on the \u201cpuzzle\u201d icon in the task bar with Chrome extensions and pin this extension.\\n4. Setup Zoom:\\n   \u2022 Open Zoom and log in.\\n   \u2022 Go to https://zoom.us/profile/setting\\n   \u2022 On the Meeting tab, turn on Auto saving chats (learn more here).\\n   \u2022 On the Recording tab:\\n     i. enable Local Recording\\n     ii. enable \u201cHosts can give meeting participants permission to record locally\u201d.\\n     iii. enable automatic recording on a local computer.\\n   \u2022 Setup your Zoom name to Instructor or Navigator according to your role.\\n\\nUpdating the extension\\n\\nCheck regularly if you are using an up-to-date version of the extesion:\\n\u2022 The current version can be found at the top of this document.\\n\u2022 Your version is at chrome://extensions/ next to the extension name.\\n\\nIf there is a never version of the extension, remove the extension and repeat points 1) and 3) in the Preparation section.\"}"}
{"id": "mUSPhG4uDW", "page_num": 49, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nDemonstrations\\n\\n1. Navigator calls Instructor via Zoom (Participants \u2192 Invite)\\n   \u2022 Ensure that both have video and microphone are disabled.\\n\\n2. After the call is accepted:\\n   \u2022 Instructor opens a Zoom chat window,\\n   \u2022 Navigator:\\n     \u2013 opens a Zoom chat window,\\n     \u2013 opens a Chrome window,\\n     \u2013 shares the screen with their Chrome window (only),\\n     \u2013 starts recording a Zoom call video (ignore the warning about audio).\\n\\n3. Navigator clicks on the extension button in the navigation bar and selects New recording.\\n   \u2022 A new tab will open with an overlay Starting recording for 1 second (make sure that it is visible on the Zoom recording), followed by a prompt for waiting for instructions.\\n   \u2022 Use the opened tab, do not open any new tab!\\n\\n4. Instructor gives Navigator instructions through the chat interface for accomplishing a task (see Tasks for details).\\n   \u2022 Instructor has no other way of communicating with Navigator than through the chat interface.\\n   \u2022 Instructor can give intermediate instructions or answer system questions.\\n\\n5. Navigator performs actions in the web browser according to Instructor\u2019s instructions.\\n   \u2022 Navigator should use the chat interface to ask the user for any missing details and to provide answers if necessary.\\n\\n6. After the task is finished, Navigator:\\n   \u2022 clicks on the extension button, selects Save recording and wait until the recording gets saved to their computer,\\n   \u2022 stops the video recording and screen sharing,\\n   \u2022 ends the call,\\n   \u2022 submits the recording (see Recording for details).\\n\\nRecording\\n\\nThe recording is submitted through the web interface. The recording consists of:\\n\\n\u2022 a \\\"<recording id>.zip\\\" file, which is a ZIP archive with:\\n  \u2013 metadata,\\n  \u2013 events,\\n  \u2013 screenshots,\\n  \u2013 HTML snapshots,\\n\u2022 Zoom chat history \\\"meeting saved chat.txt\\\",\\n\u2022 Zoom invite link\\n\\nThe Zoom recording folder depends on your platform. The default directories are:\\n\\n\u2022 Windows: C:\\\\Users\\\\[Username]\\\\Documents\\\\Zoom\\n\u2022 Linux: /home/[Username]/Documents/Zoom\\n\u2022 Mac: /Users/[Username]/Documents/Zoom\\n\\nActions\\n\\nNavigator can perform the following actions in the browser:\\n\\n\u2022 go to a URL through the navigation bar,\\n\u2022 click on an element,\\n\u2022 input text into an input field,\\n\u2022 scroll up and down the page,\"}"}
{"id": "mUSPhG4uDW", "page_num": 50, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEB LINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nThe actions which should not be performed:\\n\u2022 opening a new tab (it is ok if the page opens a tab by itself),\\n\u2022 horizontal scrolling,\\n\u2022 page search (Ctrl+F),\\n\u2022 keyboard shortcuts,\\n\u2022 drag & drop (e.g. Google Maps)\\n\\nTasks\\nInstructor can give the system any tasks which an automated web assistant should be able to handle. Use your imagination!\\n\\nThe tasks can be unspecified at first. It is the job of the system to ask for intermediate details throughout the tasks demonstration.\\n\\nStop the demonstration before doing any real action in the world: booking a table, buying a ticket, etc.\\n\\nWebsites\\nFor your inspiration, here is a spreadsheet with the list of websites and the task categories you can use them for.\\n\\nWe have created a shared account for these websites which you should use in case you need to login.\\n\\nOf course feel free to use any other websites (just do not fill in any other personal details there, preferably use the shared account as well).\\n\\nTips\\nNavigator\\n\u2022 Don\u2019t do things too quickly! Saving the actions, screenshots and pages takes time and performing the actions in a quick succession can introduce errors in the recording, especially on heavy websites.\\n\u2022 Do not perform any unnecessary actions (all the actions will be recorded and we want to minimize the amount of mindless clicking and scrolling)\\n\u2022 Wait until the page fully loads.\\n\u2022 Do not use autofill for text fields, always type everything from scratch.\\n\u2022 Do not change the size of the browser window if not necessary.\\n\\nInstructor\\n\u2022 Be creative: assign tasks starting from very simple (\u201csubmit the form\u201d) to very complex (multi-turn conversation with changing topics).\\n\u2022 Ask only about things that are relevant to the webpage.\\n\u2022 Wait until the system performs their actions.\\n\\n\u2013 However, feel free to interrupt if something does not seem right or you have changed your mind.\\n\\n\u2022 Finalize all the tasks right before changing the actual state of the world (i.e. ordering products, submitting issues etc.).\\n\\nNote that the extension does not work in an anonymous window. If you want to clear your history afterwards, use Ctrl+Shift+Delete.\"}"}
{"id": "mUSPhG4uDW", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"prompt template. The remaining hyper-parameters can be found in Table 14, or otherwise follow the default parameters specified in the transformers library (Wolf et al., 2019).\\n\\nB.8 Input Samples\\n\\nSamples for models using one of the templates in Appendix B.5 is provided: Appendix B.8.1 for MindAct, Appendix B.8.3 for chat-based models, Appendix B.8.2 for instruct-based models, and Figure 7 for Pix2Act.\\n\\nFigure 7: Sample input for Pix2Act, which contains embedded header text above the screenshot.\\n\\nB.8.1 SAMPLE INPUT FOR MINDACT\\n\\n(html(body(div container(div row(div col hdr-r d-flex(div(a id=0 rc-link(span id=1 textEXPLORE)(i id=2 fa ency-down ))(div rc-flyout\\n\\nYou will find above the HTML elements available for the current webpage. You are an AI assistant tasked with helping a user (aka Instructor) by answering with the action needed to perform a task on a webpage. Here are the instructor's utterances, truncated to first and last 4 instances preceded by the relative timestamp: \\n\\n[00:05] Hello ; Only the last 5 actions are available. Here are the top candidates for this turn: (uid = 67e2a5fb-8b1d-41a0) (input id=6 search q what do you want to searchbox (uid = fedfb512-949e-42b3) (input id=7 submit search button js-form-submit form-submit ) (uid = c7fbc11c-0949-4ab2) (form id=4(div id=5 js-form-item form-item form-item-keys form-no-label (span field-preffix (input (uid = 6c7fe1f1-f640-4dce) (span id=1 textEXPLORE) (uid = 0ffc6f0e-808a-4c2a) (span id=9 adthrive-close (uid = 8d8afc84-5b97-477a) (div id=5 js-form-item form-item form-item-keys form-no-label (span field-preffix (input submit (uid = 1ea51e98-3fcd-4e30) (h4 id=3The World's #1 Online Encyclopedia) (uid = 769785af-485e-4cf1) (a id=0 rc-link(span id=1 textEXPLORE)(i id=2 fa ency-down )) (uid = e7b7879f-45ae-48a5) (i id=2 fa ency-down ) (uid = bf33a062-fb67-44f0) (a id=8Read more) about our content and why so many\\n\\nAssistant: action(intent=\\\"say\\\", speaker=\\\"navigator\\\", utterance=\\\"Hi\\\") action(intent=\\\"say\\\", speaker=\\\"instructor\\\", utterance=\\\"Open Encyclopedia website.\\\") action(intent=\\\"say\\\", speaker=\\\"navigator\\\", utterance=\\\"Yes, sure\\\") action(intent=\\\"load\\\", url=\\\"https://www.encyclopedia.com/\\\") action(intent=\\\"say\\\", speaker=\\\"instructor\\\", utterance=\\\"Search for biotechnology\\\")\"}"}
{"id": "mUSPhG4uDW", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mUSPhG4uDW", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chat\\nsay(speaker=\\\"navigator\\\", utterance=\\\"Hi\\\")\\nsay(speaker=\\\"instructor\\\", utterance=\\\"Open Encyclopedia website.\\\")\\nsay(speaker=\\\"navigator\\\", utterance=\\\"Yes, sure\\\")\\nload(url=\\\"https://www.encyclopedia.com/\\\")\\nsay(speaker=\\\"instructor\\\", utterance=\\\"Search for biotechnology\\\")\"}"}
{"id": "mUSPhG4uDW", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Ground Truth | Click: (uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n|--------------|-----------------------------------|\\n| Flan-T5-250M | Click (uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| Flan-T5-780M | Click (uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| Flan-T5-3B   | Click (uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| Fuyu-8B      | Click (uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| GPT-3.5T     | Text_input (text=\\\"biotechnology\\\", uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| GPT-4T       | Text_input (text=\\\"biotechnology\\\", uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| GPT-4V       | Text_input (text=\\\"biotechnology\\\", uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| Llama-2-7B   | Click (uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| Llama-2-13B  | Click (uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| MindAct-250M | Action (uid=\\\"67e2a5fb-8b1d-41a0\\\", intent=\\\"click\\\") |\\n| MindAct-780M | Action (uid=\\\"67e2a5fb-8b1d-41a0\\\", intent=\\\"click\\\") |\\n| MindAct-3B   | Action (uid=\\\"67e2a5fb-8b1d-41a0\\\", intent=\\\"click\\\") |\\n| Pix2Act-282M | Click (x=1536, y=27) |\\n| Pix2Act-1.3B | Click (x=716, y=508) |\\n| ShearedLLaMA-1.3B | Click (uid=\\\"67e2a5fb-8b1d-41a0\\\") |\\n| ShearedLLaMA-2.7B | Click (uid=\\\"67e2a5fb-8b1d-41a0\\\") |\"}"}
{"id": "mUSPhG4uDW", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX \u2013 a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings. Our code, data and models are available for research: https://mcgill-nlp.github.io/weblinx.\\n\\n$^{\\\\dagger}$ Equal contribution\\n\\nMila Quebec AI Institute\\nMcGill University\\nInstitute of Formal and Applied Linguistics, Charles University\\nFacebook CIFAR AI Chair. Correspondence to: Xing Han Lu <xing.han.lu@mail.mcgill.ca>, Zden\u02c7ek Kasner <kasner@ufal.mff.cuni.cz>, Siva Reddy <siva.reddy@mila.quebec>.\"}"}
{"id": "mUSPhG4uDW", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEB LINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nTable 1: WEB LINX is the first benchmark featuring real-world websites with multi-turn dialogue. The columns indicate: use of multi-turn dialogue (Chat), if tasks are general or specialized (Gener.), a web browser is used (Browse), number of app/website domains (# Dom.), number of instances (# Inst.), average number of HTML elements per page (Avg. # El.), average number of turns per instance (Avg. # Turns). *AITW has 30K unique prompts with multiple demos each and the browsing data is strictly from Android devices.\\n\\n| Setting               | Chat | Gener. | Browse | # Dom. | # Inst. | Avg. # El. | Avg. # Turns |\\n|-----------------------|------|--------|--------|--------|---------|------------|-------------|\\n| MiniWob++ (Liu et al., 2018) | \u2717    | \u2717      | \u2717      | 100    | 100     | 28         | 3.6         |\\n| WebShop (Yao et al., 2022)   | \u2717    | \u2717      | \u2713      | 1      | 12K     | 38         | 11.3        |\\n| WebArena (Zhou et al., 2023)  | \u2717    | \u2713      | \u2713      | 6      | 812     | -          | -           |\\n| VW A (Koh et al., 2024)       | \u2717    | \u2713      | \u2713      | 3      | 910     | -          | -           |\\n| Mind2Web (Deng et al., 2023)  | \u2717    | \u2713      | \u2713      | 137    | 2350    | 1135       | 7.3         |\\n| AITW (\u2217)(Rawles et al., 2023) | \u2717    | \u2713      | \u2713      | 357    | -       | -          | 6.5         |\\n| WebVoyager (He et al., 2024)  | \u2717    | \u2713      | \u2713      | 15     | 300     | -          | -           |\\n| RUSS (Xu et al., 2021)        | \u2713    | \u2717      | \u2713      | 22     | 80      | 801        | 5.4         |\\n| WorkArena (Drouin et al., 2024) | \u2713    | \u2717      | \u2713      | 1      | 23K     | -          | 10          |\\n| META-GUI (Sun et al., 2022)   | \u2713    | \u2713      | \u2717      | 11     | 1125    | 79         | 4.3         |\\n| WEB LINX (ours)              | \u2713    | \u2713      | \u2713      | 155    | 2337    | 1775       | 43.0        |\\n\\nA naive way to use this benchmark would be to give the full DOM tree directly to an agent and instruct it to predict the correct action. As some HTML pages contain thousands of elements, fitting them completely within the context of a LLM poses a significant challenge; even if it was possible, existing LLMs would be unable to process them in real-time. Consequently, we design a method called Dense Markup Ranking (\u00a75.1), which compares each element in an HTML page with the full action history. By using a similarity-based approach to both learn and rank elements, we can leverage compact architectures used in text retrieval. This lets us find the most relevant elements and prune irrelevant ones to obtain a compact representation of the DOM. We combine it with the action history, detailed instruction and screenshot (in a multimodal context) to construct an input representation for LLMs, which can now meaningfully predict which actions to take. However, even if a predicted action is correct, it may be identified as incorrect by existing metrics, which can happen when there are minor differences in an agent's response or when an overlapping element is selected. Thus, we design a suite of evaluation metrics (\u00a74) tailored for specific types of action (for instance, clicking should be evaluated differently from what the navigator says).\\n\\nWe examine 19 models based on 8 architectures (\u00a76), including smaller image-to-text, larger text-only decoders, LLMs, and multimodal models (capable of accessing both image and text). Among them, 5 are in the zero-shot setting, and the remaining are finetuned using the training split of WEB LINX. We find that even the best zero-shot model, GPT-4V (OpenAI, 2023a), is surpassed by finetuned models (\u00a76.1). Notably, a smaller model like Sheared-LLaMA (Xia et al., 2023) outperforms the much larger Fuyu (Bavishi et al., 2023), which was pretrained with browser screenshots. However, all models face challenges in generalizing to new settings, such as unseen websites from a different geographic location or when the instructor gives instructions.\"}"}
{"id": "mUSPhG4uDW", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nOur contributions are summarized as follows:\\n\\n- We introduce the problem of real-world conversational web navigation and a large-scale expert-annotated benchmark for it, named WEBLINX (\u00a73).\\n- We propose a suite of action-specific metrics, which we combine to assess overall model performance (\u00a74).\\n- We design a method to simplify HTML pages (\u00a75.1), allowing us to evaluate a wide range of models (\u00a75.2).\\n- We find that smaller text-only decoders outperform multimodal LLMs, but all finetuned models struggle to generalize to novel scenarios (\u00a76).\\n\\n2 Related Work\\n\\n2.1 Web Navigation Agents\\n\\nPrevious work predominantly focused on building web agents for a single task. A prominent work for task-driven web navigation is MiniWoB++ (Shi et al., 2017; Liu et al., 2018), a simulated web environment with an extensive list of task primitives (e.g., select value from a dropdown or date from a calendar). Its well-defined input space and the flexibility of its simulated environments lead to reinforcement learning approaches reaching human-level performance (Liu et al., 2018; Humphreys et al., 2022). However, the ability of those methods to transfer to realistic settings have been limited, even after introducing environment extensions (Gur et al., 2021) and sample-efficient methods (Kim et al., 2023).\\n\\nOther works also explored grounding language commands to web elements and mobile UIs (Pasupat et al., 2018; Li et al., 2020; Burns et al., 2022), or question answering (QA) by navigating Wikipedia (Nogueira & Cho, 2016).\\n\\nIn an effort to build more realistic environments, Yao et al. (2022) introduced WebShop, an e-commerce environment with over 12K human-written task instructions. Models trained on WebShop achieved strong performance, but still relied on clean HTML and simple visual representations (Furuta et al., 2023). Instead, we aim to build agents that can act on any real-world website, often existing in noisy and dynamic environments.\\n\\nThe prospect of using LLMs to act on real websites (Nakano et al., 2021) has lead to the development of LLM-based navigation services (Adept, 2023; Multi-On, 2023; HyperWrite, 2023), which has set the stage for academic counterparts. MIND2WEB (Deng et al., 2023), WebArena (Zhou et al., 2023) and VisualWebArena (Koh et al., 2024) are large-scale resources for building autonomous navigation agents like SeeAct (Zheng et al., 2024) and WebVoyager (He et al., 2024). On the other hand, WEBLINX is a benchmark for building agents that can interact with users in a multi-turn dialogue fashion, allowing them to be steered towards precise goals. To this end, our problem formulation significantly expand and generalize upon exploratory work on simulated instructors for movie ticket booking (Gur & Yan, 2019), semantic parsing-based agents for online help centers (Xu et al., 2021), and iterative tool resolution for crowd-source platforms (Xu et al., 2024).\\n\\n3 Experiments\\n\\nFigure 2: Distribution of demonstrations in WEBLINX across categories (Section 5.2) and splits (Table 2). Each category has many subcategories as shown in Appendix A.2.\\n\\nTable 2: Demonstration (Demo) splits for training and evaluation.\\n\\n| Split Description | Description |\\n|-------------------|-------------|\\n| TRAIN | Demos used to train models in Section 5 |\\n| VALID | In-domain demos for hyperparameters selection |\\n| TEST | In-domain demos to test in-domain generalization |\\n| TEST-IID | Aggregation of splits for OOD evaluation |\\n| TEST-ODD | Unseen websites from the same subcategories |\\n| TEST-CAT | New subcategories within the same categories |\\n| TEST-GEO | Geographic locations not in TRAIN |\\n| TEST-VIS | Instructor does not see the screen |\"}"}
{"id": "mUSPhG4uDW", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nZhang, Z., Takanobu, R., Zhu, Q., Huang, M., and Zhu, X.\\n\\nRecent advances and challenges in task-oriented dialog systems. Science China Technological Sciences, 63(10):2011\u20132027, 2020b.\\n\\nZhao, Y., Gu, A., Varma, R., Luo, L., Huang, C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., Desmaison, A., Balioglu, C., Damania, P., Nguyen, B., Chauhan, G., Hao, Y., Mathews, A., and Li, S. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. Proc. VLDB Endow., 16(12):3848\u20133860, 2023. URL https://www.vldb.org/pvldb/vol16/p3848-huang.pdf.\\n\\nZheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. GPT-4VISION is a Generalist Web Agent, if Grounded. CoRR, abs/2401.01614, 2024. URL https://doi.org/10.48550/arXiv.2401.01614.\\n\\nZhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Bisk, Y., Fried, D., Alon, U., and Neubig, G. WebArena: A Realistic Web Environment for Building Autonomous Agents. CoRR, abs/2307.13854, 2023. URL https://doi.org/10.48550/arXiv.2307.13854.\\n\\nZhou, Y., Sheng, Y., Vo, N., Edmonds, N., and Tata, S. Simplified DOM Trees for Transferable Attribute Extraction from the Web. CoRR, abs/2101.02415, 2021. URL https://arxiv.org/abs/2101.02415.\\n\\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models. CoRR, abs/2304.10592, 2023. URL https://doi.org/10.48550/arXiv.2304.10592.\"}"}
{"id": "mUSPhG4uDW", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nContents\\n1 Introduction 1\\n2 Related Work 3\\n  2.1 Web Navigation Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n  2.2 Website Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n  2.3 Conversational Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n3 WEBLINX 4\\n  3.1 Representing actions and states for modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n4 Evaluation Framework 5\\n  4.1 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n  4.2 Turn-level score and overall score . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n5 Methods 5\\n  5.1 Dense Markup Ranking (DMR) for Candidate Selection and Input Representation . . . . . . . . . . . . 5\\n  5.2 Modeling Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n6 Experimental Results 7\\n  6.1 Overview of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n  6.2 Qualitative Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n7 Discussion 8\\n  7.1 Experimental Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n  7.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n8 Conclusion 9\\nA Dataset Details 19\\n  A.1 Supplementary Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n  A.2 Categories and Subcategories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n  A.3 Input Processing Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n  A.4 Output Processing Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n  A.5 Data Collection Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n  A.6 Actions and Intents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\nB Modeling Details 27\"}"}
{"id": "mUSPhG4uDW", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.1 Optimal Text Representation (OTR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n\\nB.2 Strategic Truncation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n\\nB.3 Understanding the categorization of pretrained models . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n\\nB.4 Technical Aspects of Dense Markup Ranking (DMR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n\\nB.4.1 Empirical Speed Improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n\\nB.5 Input Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n\\nB.5.1 Template for Pix2Act . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n\\nB.5.2 Template for chat-based models (LLaMA, GPT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n\\nB.5.3 Template for instruction-based models (Flan, Fuyu, MindAct) . . . . . . . . . . . . . . . . . . . . 30\\n\\nB.6 Model Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n\\nB.7 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n\\nB.8 Input Samples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n\\nB.8.1 Sample input for MindAct . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n\\nB.8.2 Sample input for instruction-based models (Flan, Fuyu) . . . . . . . . . . . . . . . . . . . . . . . . 32\\n\\nB.8.3 Sample input for chat-based models (LLaMA, GPT) . . . . . . . . . . . . . . . . . . . . . . . . . . 33\\n\\nB.9 Output Sample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n\\nC Supplementary Results 36\\n\\nC.1 Comparison of Mind2Web representation with OTR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n\\nC.2 Comparison of image-only baseline with multimodal models . . . . . . . . . . . . . . . . . . . . . . . 36\\n\\nC.3 Assessing impact of model size for text-only decoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\\n\\nC.4 Generalization capabilities of evaluated models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n\\nC.5 Extended Qualitative Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n\\nC.6 Comparison with human performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n\\nC.7 Augmenting non-finetuned models with in-context examples . . . . . . . . . . . . . . . . . . . . . . . . 41\\n\\nD Additional Result Tables 43\\n\\nE Instructions for the Annotators 48\"}"}
{"id": "mUSPhG4uDW", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Section 3, we introduce WEBLINX. In this section, we provide supplementary statistics for readers wishing to gain a deeper understanding of the dataset.\\n\\nIn Table 7, we report demo and turn statistics by intent. We observe that say, click and load are heavily represented across demos. However, the latter happens less often than other intents. This is because the user loads new links only when they move to a new website, and many tasks can be accomplished within the same page (such as booking a flight). Therefore, there is no need to load new pages as frequently as other intents. Additionally, hover is less represented due to the removal of unnecessary hovering, which can be accidentally recorded when moving the cursor across non-target elements with callbacks.\\n\\nIn Table 8, we present the number of demos for each split and mean number of turns. Although most demos are in the range of 40-50 turns, the number of demos in the TESTVIS split is substantially lower. This can be attributed to the lack of follow-up based on what is happening on the screen. For example, an instructor with vision can request the navigator to apply some specific filters (e.g., by saying \u201cPlease apply the filter for Japan Airlines under the Airlines filter option\u201d), whereas an instructor without vision would not have this request unless they are using a screen-reader.\\n\\nTable 6: Complete list of WEBLINX observed action space. Note that a speaker can either be navigator or instructor, but an agent is only permitted to choose navigator, since speaker=\u201cinstructor\u201d is not a valid action by an agent. Tab actions (create, remove, switch) are under \u2018chrome.tabs\u2019. (*)\u2019onload\u2019 and \u2018location\u2019 are both methods of \u2018window\u2019.\\n\\n| Action Description | Listener Method/event trigger |\\n|--------------------|-----------------------------|\\n| say(speaker=[role],utterance=[str]) | \u2014 \u2014 |\\n| click(uid=[element]) | onclick HTMLElement.click() |\\n| click(x=[int],y=[int]) | onclick HTMLElement.click() |\\n| hover(uid=[element]) | onmouseover MouseEvent('mouseenter') |\\n| hover(x=[int],y=[int]) | onmouseover MouseEvent('mouseenter') |\\n| textinput(uid=[element],value=[str]) | oninput Event('input') |\\n| change(uid=[element],value=[str]) | onchange Event('change') |\\n| load(url=[link]) | onload* location.href |\\n| submit(uid=[element]) | onsubmit HTMLFormElement.submit() |\\n| scroll(x=[int],y=[int]) | onscroll window.scrollTo(x,y) |\\n| copy(uid=[element],text=[str]) | oncopy ClipboardEvent('copy') |\\n| paste(uid=[element],text=[str]) | onpaste ClipboardEvent('paste') |\\n| tabCreate() | tabs.onCreated tabs.create() |\\n| tabRemove(target=[tabId]) | tabs.onRemoved tabs.remove() |\\n| tabSwitch(origin=[tabId],target=[tabId]) | onUpdated tabs.update() |\\n\\nFigure 5: Overview of the actions in our benchmark, including 10 browser actions and 1 chat action. An argument of an action can be a string (str), an integer (int), an element (el), or a browser tab id (tab). The intents are described in Table 6.\\n\\nIn Table 9, we highlight the usage frequency of AI tools, which are listed in Table 12. For certain tasks, such as summarizing...\"}"}
{"id": "mUSPhG4uDW", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Section 6, we provide an overview of our results on the average of out-of-domain split. In this section, we provide in-depth analysis of both in-domain and out-of-domain results. We start by looking at the impact of our improved text representation (OTR) compared to MindAct (Appendix C.1), before moving on to a comparison of baseline image-to-text models with larger multimodal models (Appendix C.2), followed by an assessment of various text-only decoders (Appendix C.3).\\n\\nC.1 Comparison of Mind2Web representation with OTR\\n\\nMindACt is a prior method proposed by Deng et al. (2023) that only receives text as input. We use the MindAct checkpoints and use the Mind2Web data structure. To understand what happens for larger DOM trees and longer history, we compare it against our optimal text representation introduced in Section 5.2. In Table 16, we observed that Flan-T5 with OTR outperforms MindAct in both overall performance and when looking at individual groups. We further observe that the gap between the model also increases for larger models, which leads us to believe that a careful strategy when constructing\\n\\n\\\\[(s_t, a_{t-1})\\\\]\\n\\nis crucial as we scale to more parameters.\\n\\nTable 16: Comparing Flan-T5 using OTR with MindAct using Mind2Web formatting. Reported on valid with metrics from \u00a74.\\n\\n| Models     | Overall Score | Element Text | Micro-Avg IM IoU F1 |\\n|------------|---------------|--------------|---------------------|\\n| MindAct-T5-250M | 17.78  | 77.05 | 19.02 | 9.87 |\\n| MindAct-T5-780M | 21.39  | 77.58 | 22.46 | 15.32 |\\n| MindAct-T5-3B | 27.86  | 79.91 | 24.24 | 24.79 |\\n| Flan-T5-250M  | 21.91  | 79.27 | 24.10 | 11.02 |\\n| Flan-T5-780M  | 23.94  | 80.26 | 24.90 | 15.99 |\\n| Flan-T5-3B    | 31.97  | 82.00 | 31.18 | 27.81 |\\n\\nC.2 Comparison of image-only baseline with multimodal models\\n\\nIn Table 5.2, we introduce Pix2Act, which only uses screenshots as input (embedding \\\\(v_t, u_r\\\\) and \\\\(a_r\\\\) as header text). We also consider larger multimodal models (Table 5.2) that can take the complete \\\\(P\\\\) the same way as text-only models. In Table 17, we observe that the larger variant of Pix2Act offers meaningful improvements over the base variant, but that Fuyu-8B outperforms both models in the element group and achieves similar performance for the text group and intent match, resulting in a better overall performance. On the other hand, GPT-4V, which was never finetuned for the task, is consistently outperformed by Fuyu-8B and is also behind Pix2Act in each scenario except the element group. Those results highlights the importance of finetuning the models whenever it is possible, using models with greater number of parameters, and incorporating more complete textual information (including candidates).\\n\\nTable 17: Comparing image-only baselines with multimodal models. Reported on valid with metrics from \u00a74. (*) GPT-4V is the only model not finetuned.\\n\\n| Models     | Overall Score | Element Text | Micro-Avg IM IoU F1 |\\n|------------|---------------|--------------|---------------------|\\n| Pix2Act-282M | 14.39  | 79.09 | 6.70 | 18.11 |\\n| Pix2Act-1.3B | 24.21  | 83.40 | 13.38 | 31.61 |\\n| Fuyu-8B    | 31.60  | 81.36 | 26.34 | 30.99 |\\n| GPT-4V*    | 14.26  | 41.00 | 14.44 | 6.06 |\\n\\nC.3 Assessing impact of model size for text-only decoders\\n\\nIn addition to differences in architectures, we also seek to understand the role of model size (in terms of parameter count) on the training. In Table 18, we only examine the scenario of decoder-only models (LLaMA and GPT) that solely takes text as input. In the zero-shot setting, we observe that the performance of a model increases as models become larger. However, for finetuned models, the improvements are not as important, since the largest variant (13B) of LLaMA-2 only\"}"}
{"id": "mUSPhG4uDW", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nsurpasses the 2.7B variant by a small margin. When comparing zero-shot with finetuning, it is clear that the latter yields considerable improvements, with models as small as 2.7B surpassing the best zero-shot model (GPT-4T) on scenarios. In parallel, even though GPT-3.5T surpasses LLaMA-2-13B in zero-shot performance, the finetuned variants of GPT-3.5T (reported as GPT-3.5F) trails behind even the smallest LLaMA model. This could potentially be attributed to non-optimal hyperparameters, since API users can only control the batch size and number of epochs.\\n\\nTable 18: Performance of decoder-only text models, both zero-shot (above) and finetuned (below). Reported on valid with metrics from \u00a74. We use the chat-hf variants of LLaMA-2.\\n\\n| Models             | Overall Score | Element | Text | IM | IoU | F1 |\\n|--------------------|---------------|---------|------|----|-----|----|\\n| Llama-2-13B        | 6.07          | 39.55   | 5.54 | 1.62 |\\n| GPT-3.5T           | 11.48         | 41.93   | 11.67| 3.16 |\\n| GPT-4T             | 13.75         | 41.64   | 13.83| 6.58 |\\n| Sheared-LLaMA-2.7B | 35.47         | 86.14   | 33.80| 34.20|\\n| Llama-2-13B        | 38.03         | 86.49   | 36.43| 36.54|\\n| GPT-3.5F           | 28.98         | 79.03   | 27.42| 25.99|\\n\\nC.4 Generalization capabilities of evaluated models\\n\\nAt this stage, we have validated that strategically truncating text and better candidate representation via OTR achieve better results compared to MindAct baselines (Appendix C.1, larger multimodal models like Fuyu-8B and GPT-4V offer important improvements over prior approaches like Pix2Act (Appendix C.2), and choosing larger text-only decoder models (LLaMA, GPT-Turbo) will consistently outperform smaller ones in the zero-shot setting, but does not show a large improvement when finetuned (Appendix C.3). Those results lead to relevant questions: do those models transfer to out-of-domain splits (unseen websites, new subdomains, different geographies, and visionless instructors), and can we draw the same conclusions in those cases?\\n\\nIn Table 4, we observe, in the zero-shot setting, that the gap between GPT-4T and GPT-4V becomes narrower (likely due to the decrease in performance in the element group). In the finetuned setting, we observe a sharp decrease in overall performance for all models, which highlights the challenge of applying models on new scenarios. However, we can reassert that OTR, multimodality and finetuning are necessary to achieve better overall performance, and that decoder-only models remain the strongest models we evaluated. However, the gap between Sheared-LLaMA-2.7B and LLama-2-13B is substantially narrower than on the validation split, indicating that Sheared-LLaMA is more robust to changes to the environment. Finally, we see that, even on out-of-domain splits, multimodal models remain behind their text-only counterpart.\\n\\nC.5 Extended Qualitative Assessment\\n\\nIn Section 6.2, we highlight the main takeaways of our qualitative assessment. We can find below the complete assessment, including supplementary scenarios.\\n\\nAssessing click\\n\\nIn Figure 10, we examine multiple scenarios involving GPT-4V and compare them against LLaMA-2-13B. In scenario 1, we found that GPT-4V can make mistake by selecting the incorrect link when given multiple links that contain different time frames (for example, choosing a 3:30AM news article instead of 4:15AM). In scenario 2, it may not be capable of acknowledging that it is already in the second step of performing a task (e.g., changing the current location of the site), and may try to repeat the task from start (e.g., re-open the details window when it is already open). In scenario 3, we seem it correctly predicts an action that is in theory correct, but that is less optimal than what a human would have chosen; for example, it may open the login page of a commonly used website, even though choosing the homepage might allow the navigator to use the app faster if already logged in. In each of those scenarios, LLaMA is capable of selecting the correct option. However, we see in scenario 4 that LLaMA-2-13B can also sometimes fail by attempting to click on elements that do not affect the state (e.g., a text-only heading), whereas GPT-4V can make the correct decision in the same example.\\n\\n18 A learning rate multiplier also exists, but it is unclear what the base rate and optimizers are.\"}"}
{"id": "mUSPhG4uDW", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nS1: On a news website, Instructor wants Navigator to open a specific tab on the page, i.e., \u201cSportsday on 28 May 2023 at 4.15 AM\u201d.\\n\\nS2: Instructor requests the location on a food delivery website to be set to Las Vegas, Nevada. The Delivery Details page is already open.\\n\\nGPT-4V (R) clicks on an incorrect (3:30 AM) tab. GPT-4V (R) attempts to exit the Delivery details page and reopen it, which could lead to a loop.\\n\\nLLaMA (B) clicks on the correct 4:15 AM tab. LLaMA (B) correctly clicks on the Change button.\\n\\nS3: Instructor wants Navigator to compose an email. Navigator uses Bard for the draft.\\n\\nS4: Instructor requests Navigator to send the top questions of the week.\\n\\nGPT-4V (R) attempts to click directly on the login page, which is less optimal.\\n\\nGPT-4V (B) selects the \u201cWeek\u201d button, which matches the reference action.\\n\\nLLaMA (B) opens the homepage (corresponds to reference). LLaMA (R) clicks on a text-only heading (Top Questions).\\n\\nFigure 10: Comparison of GPT-4V and LLaMA-2-13B (finetuned) on predicting click actions. Incorrectly predicted actions are in red (R), reference actions are in blue (B). We show 4 scenarios (S1-S4).\\n\\nAssessing textinput\\n\\nIn Figure 11, we observe that GPT-4 will sometimes attempt to perform illogical actions when performing tasks like sending an email; it may write the name of a recipient when the email has already been specified, whereas LLaMA will correctly input the subject specified by the instructor (Scenario 1). Additionally, GPT-4 can mix up username and password forms on login pages by trying to type in the email address given by the instructor into the password field; on the other hand, LLaMA can correctly input the password (S2). Moreover, there are scenarios where both struggle to leverage the context to complete the second step of a multi-step task. For example, when the instructor request a passage to be translated into a certain language (S3), and the first step (typing in the passage to translate) has already been completed, both models will ignore the second step (changing the language to the target). Finally, both models may struggle to leverage information that was given many steps before. For instance, if the instructor wants to write a post, they may given the title earlier in the demonstration, then provide the text for the introduction later on (S4); in those cases, both models fail to include the title.\\n\\nAssessing submit\\n\\nOn a restaurant booking page with a filled text box for the \u201cLocation\u201d (Figure 9), we found that GPT-4T would try to type a date inside the text box, whereas GPT-4V would simply repeat what was already written (e.g. \u201cMadrid, Spain\u201d). However, the correct action, in this case, is to press the \u201cSubmit\u201d button, which LLaMA-2.7B correctly predicts.\\n\\nThus, even though GPT-4V can effectively read the text, the action it predicts would not be what a human would logically do.\"}"}
{"id": "mUSPhG4uDW", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEB LINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nS1: Compose a \\\"Invitation to Collaboration\\\" email.\\n\\nS2: Open Google translate and sign in using the following credentials: [email] [password]\\n\\nReference (B): Invitation to Collaboration\\nReference (B): [password]\\n\\nGPT-4V (R): Leon Tales\\nGPT-4V (R): [email]\\n\\nLLaMA (B): Invitation to Collaboration\\nLLaMA (B): [password]\\n\\nS3: Translate a passage into French using Reverso. The language selection window is currently open.\\n\\nS4: Create a post about Effective Communication with a title and an introduction\\n\\nReference (B): fr\\nReference (B): 5 Strategies for Building Stronger Relationships.\\n\\nIntroduction: Effective...\\n\\nGPT-4V (R): The sun began its slow...\\nGPT-4V (R): Introduction: Effective...\\n\\nLLaMA (R): click (incorrect action)\\nLLaMA (B): Introduction: Effective...\\n\\nFigure 11: Comparison of GPT-4V and LLaMA-2-13B (finetuned) on predicting text input actions. Actions predicted by GPT-4V are in red (R), references are in blue (B), LLaMA's action is indicated in parenthesis. We show 4 scenarios (S1-S4).\\n\\nTable 19: Comparison of LLaMA-2-13B (finetuned) and GPT-4V on predicting say actions.\\n\\nRequest Like & Bookmark this listing.\\nReference Alright\\nLLaMA Alright\\nGPT-4V Understood. If you need further assistance, just let me know.\\n\\nRequest Please share the link.\\nReference Alright.\\nLLaMA Okay.\\nGPT-4V Here is the link to the discussion: [incorrect link]\\n\\nRequest Please share the link with me.\\nReference Sure.\\nLLaMA Sure.\\nGPT-4V I'm sorry, but I can't assist with that.\\n\\nRequest Could you help me compose an email for a discount offer?\\nReference Alright. Who should receive this email?\\nLLaMA Okay.\\nGPT-4V Would you like to mention any specific terms or expiration date for the offer?\"}"}
{"id": "mUSPhG4uDW", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nB.3 Understanding the categorization of pretrained models\\n\\nIn Section 5.2, we distinguish three types of models depending on their modality:\\n\\nText-Only Models\\n\\nBy text-only models, we denote the encoder-decoder or decoder-only Transformer models (Vaswani et al., 2017) using text as their only input modality (Chung et al. 2022a; Touvron et al. 2023a;b; Jiang et al. 2023, i.a). There are certain inherent limitations text-only models used for web navigation, e.g., the inability to process images or page layouts. Another practical challenge is the length of the HTML code, containing potentially thousands of elements to interact with.\\n\\nImage-to-text Models\\n\\nBy image-to-text models, we denote the models with an image (i.e., the screenshot of the website) as their only input modality. Image-to-text models representing websites from raw pixels have a long tradition in web navigation research, starting with RL approaches based on convolutional networks (Humphreys et al., 2022). In our work, we focus on Pix2Act (Shaw et al., 2023), an encoder-decoder model specialized at text generation when given screenshots of browsers. It uses a Vision Transformer-based (Dosovitskiy et al., 2021) encoder and is finetuned from the Pix2Struct model (Lee et al., 2023) on web navigation tasks, using only pixels as input. The main challenge for image-to-text models is their inability to process longer input instructions (since the text must be embedded inside the image as headers), forcing it to rely on the screenshot.\\n\\nMultimodal Models\\n\\nBy multimodal models, we denote the models which accept both image and text as their input modality (Alayrac et al., 2022; Laurenc\u00b8on et al., 2023; Zhu et al., 2023). Multimodal models have the potential to mitigate the disadvantages of text-only and image-to-text models. However, due to their novelty, their use for web navigation is underexplored in research. However, there are publicly available multimodal models capable of recognizing browser screenshots (Bavishi et al., 2023), but they are mainly offered as commercial products; in Section 5, we describe our experiments with the public variant of this model. Thus, the main challenge of using multimodal models for web navigation is the lack of models pretrained to simultaneously parse HTML code and process website screenshots.\\n\\nB.4 Technical Aspects of Dense Markup Ranking (DMR)\\n\\nIn Section 5.1, we introduce the Dense Markup Ranking (DMR) method as a way to efficiently select candidate elements for the downstream task. In this section, we take a closer look at the technical aspects of the method.\\n\\nDefinition\\n\\nLet $E(x)$ be the encoder output vector for an input text $x$. For turn $t$, we have the the processed text representation of the state $P_{DMR}(s_t)$, which we use to score candidate element $c_{t,i}$, which is represented as text. We set the label $y(c_{t,i}) = 1$ when $c_{t,i}$ is the target candidate, otherwise $y(c_{t,i}) = 0$. The cosine similarity loss is defined as the following mean-squared error:\\n\\n$$L_t = \\\\|y(c_{t,i}) - \\\\text{sim}_{\\\\cos}(E(P_{DMR}(s_t)), E(c_{t,i}))\\\\|_2,$$\\n\\nwhere the cosine similarity is defined as $\\text{sim}_{\\\\cos}(x, y) = (x \\\\cdot y) / (\\\\|x\\\\|\\\\|y\\\\|)$. During inference, the cosine similarity is used to generate a score for each instance representing the similarity between $P_{DMR}(s_t)$ and candidate at turn $t$. The score is used to rank the candidates and choose the top-$k$ candidates for the action prediction stage.\\n\\nComputational Efficiency\\n\\nFor a sequence length $n$ and a model embedding size $e$, the complexity of self-attention is $O(n^2 \\\\cdot e)$ (Vaswani et al., 2017). Given the lengths of a state $|s_t|$ and a candidate $|c_{t,i}|$, the complexity of a cosine-based scoring is $O(|P_{DMR}(s_t)|^2 + |c_{t,i}|^2)$ instead of $O((|P_{DMR}(s_t)| + |c_{t,i}|)^2)$ for the cross-encoder approach of Deng et al. (2023). This difference makes a major impact when $|P_{DMR}(s_t)|$ and $|c_{t,i}|$ become large. We also purposefully finetune encoder models with smaller $e$ (Reimers & Gurevych, 2019; Li et al., 2023a; Xiao et al., 2023b).\\n\\nSelecting ranking model\\n\\nOur task can be formulated as a text retrieval task: we have a model (DMR) that encodes a query $P_{DMR}(s_t)$ and compare it with a document $c_{t,i}$, resulting in a score that can be used to rank candidates. Thus, we examine various models that were trained on text retrieval tasks, as they tend to transfer well to adjacent retrieval tasks. As we aim to achieve a high inference speed, we specifically choose smaller models, allowing us to maximize the computation budget of the downstream language model. We first choose all-MiniLM-L6-v2, a model developed by Reimers & Gurevych (2019) based on the MiniLM model (Wang et al., 2020). We also use bge-small-en-v1.5 (Xiao et al., 2023a) and gte-base (Li et al., 2023b), which are two smaller models that achieve competitive results on the MTEB benchmark (Muennighoff et al., 2023). This benchmark was specifically chosen because it thoroughly evaluates retrievers across a diverse range of tasks.\\n\\nFinetuning and results\\n\\nWe finetune each of the models above, as well as the cross-encoder proposed by Deng et al. (2023) (using the original author\u2019s training code). The results are shown in Table 13, where we report the recall@10, a metric that evaluates how often the correct result is in the top-10 candidates retrieved. We observe that MiniLM achieves better overall...\"}"}
{"id": "mUSPhG4uDW", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEB LINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nresults compared to other retrievers and is close to the DeBERTa cross-encoder from MindAct, while being substantially more computationally efficient. Based on those improvements, we use the finetuned MiniLM model as the backbone of our DMR method. All downstream results include the same candidates proposed by DMR.\\n\\nB.4.1 E MPIRICAL SPEED\\n\\nUsing the same environment, CPU (AMD EPYC 7453) and GPU (RTX A6000), we observe that DMR-MiniLM took 4545 seconds to process the entire training set, whereas M2W-DeBERTa took 22,385 seconds. Since there are 24,418 active turns, M2W-DeBERTa needed on average 916 ms to selected candidates at every turn, whereas DMR-MiniLM needed 186 ms. It is important to highlight that a high latency for selecting candidate could restrict the potential real-time use cases (especially with larger HTML pages), since the selected candidates need to be sent to the model in charge of generation actions; in the case of LLM, the inference could take a significant amount of time, and may include a network overhead for web APIs like GPT-4V. Network latency is difficult to reduce due to various external factors, whereas LLMs' inference time can be reduced through algorithmic improvements, such as Flash Attention (Dao et al., 2022; Dao, 2023), quantization, such as 4-bit quantization (Dettmers & Zettlemoyer, 2023), and hardware optimization at the hardware level (OpenAI, 2021; Kwon et al., 2023, inter alia). Our method can be combined with such improvements to minimize delay between actions and avoid interrupting the user's flow of thoughts, which would require the total time to be under 1 second (Carroll & Rosson, 2014).\\n\\nTable 13: Comparison of candidate selection methods (DMR and MindAct-RoBERTa) for the combined in-domain (ID) and out-of-domain (OOD) splits. We report Recall@10 scores.\\n\\n| Model     | ID TEST | ID TEST | ID TEST | ID TEST | ID TEST | ID TEST |\\n|-----------|---------|---------|---------|---------|---------|---------|\\n| BGE       | 74.44   | 60.07   | 48.82   | 43.61   | 47.55   | 50.01   |\\n| GTE       | 73.24   | 56.91   | 44.46   | 42.74   | 48.39   | 48.16   |\\n| MiniLM    | 74.27   | 59.73   | 50.95   | 44.05   | 52.75   | 51.87   |\\n| DeBERTa   | 76.86   | 63.28   | 52.76   | 48.43   | 54.65   | 54.78   |\\n\\nB.5 Input Templates\\n\\nWe provide the templates for Pix2Act's headers (Appendix B.5.1), for chat-based models like LLaMA-2 and GPT (Appendix B.5.2), and for the instruct-based models (Appendix B.5.3).\\n\\nB.5.1 TEMPLATE FOR Pix2Act\\n\\nViewport(height={{HEIGHT}}, width={{WIDTH}}) ---- Instructor Utterances: {{FIRST UTTERANCE}} ---- {{PAST UTTERANCES x (W-1)}}\\n\\nPrevious Turns: {{PAST ACTIONS}}\\n\\nB.5.2 TEMPLATE FOR CHAT-BASED MODELS (LLAMA, GPT)\\n\\n{{HTML REPRESENTATION}}\\n\\nAbove are the pruned HTML contents of the page. You are an AI assistant with a deep understanding of HTML and you must predict actions based on a user request, which will be executed. Use one of the following, replacing [ ] with an appropriate value:\\n\\nchange(value=[str], uid=[str]) ; click(uid=[str]) ; load(url=[str]) ; say(speaker=\"navigator\", utterance=[str]) ; scroll(x=[int], y=[int]) ; submit(uid=[str]) ; text_input(text=[str], uid=[str]) ;\\n\\nThe user's first and last 4 utterances are: {{PAST UTTERANCES}}; Viewport size: {{HEIGHT}}h x {{WIDTH}}w; Only the last {{W}} turns are provided. Here are the top candidates for this turn: {REPEAT 10 TIMES} (uid = ...) [tag] ... [xpath] ... [bbox] x=X y=Y width=W height=H [attributes] attr1=val1 ... [children] {{TAG}} {END REPEAT} {{PAST ACTIONS}} Please select the best action using the correct format, do not provide any other information or explanation.\"}"}
{"id": "mUSPhG4uDW", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.6 Model Implementation\\n\\nIn Section 5, we provide an overview of all models used in our experiments. An in-depth description of the models can be found below. Each model was finetuned once for a given set of hyperparameters due to the computational cost associated with each experiment; we also consider that no random initialization were introduced for the task, and we use a fixed seed for reproducibility.\\n\\nMindAct\\n\\nDeng et al. (2023) proposes a two-stage text-only web navigation model consisting of the candidate generation and the action prediction stage. For the candidate generation stage, we used our custom DMR model described in Section 5.1. For the action prediction stage, we reuse their hyperparameters, implement their text formatting methods, and also start from the MindAct checkpoints finetuned from Flan-T5 (Chung et al., 2022a). However, their proposed multi-step elimination method requires $k = 50$ candidates, which substantially increases latency and computation cost. Instead, we use the top $k = 10$ candidates output by DMR, which only requires a single generation step.\\n\\nPix2Act\\n\\nFollowing the behavior cloning method proposed in Pix2Act (Shaw et al., 2023), we finetune the model starting from the Pix2Struct backbone (Lee et al., 2023) to directly predict action $a_t$ for a given $P(s_t, a_1:t\u22121)$. The model uses an image encoder and text decoder based on the Vision Transformer (Dosovitskiy et al., 2021) and it was pretrained for parsing screenshots into structured representations. We embed the prompt and text in the header area of the screenshot, resulting in a single screenshot for each state. Since it does not have access candidate elements, we finetuned this model to predict the $x$ and $y$ coordinates, which is mapped to the most relevant element (see Section A.4), making the resulting output comparable to candidate-augmented models.\\n\\nFlan-T5 with OTR\\n\\nFor Flan-T5 experiments, we use the same hyperparameters as MindAct, and start from the Flan-T5 checkpoints (Chung et al., 2022b), which is a T5 model (Raffel et al., 2020) based on FLAN (Wei et al., 2022). However, whereas MindAct uses the Mind2Web format, we use the OTR format introduced in this work.\\n\\nLLaMA-2\\n\\nWhereas all the models above use the encoder-decoder architecture, we further explore decoder-only approaches. To this end, we finetune the variant of LLaMA-2 (Touvron et al., 2023a;b) with 7B and 13B parameters that was trained on human feedback for chat. We chose this model due its strong performance on a wide range of benchmark, including MMLU (Hendrycks et al., 2021) and HumanEval (Chen et al., 2021). Unlike the base models, we can leverage the prior capabilities of the chat-hf variant to follow instructions through turn-based language modeling, allowing a better start during finetuning. Following our Flan-T5 experiments, we also use OTR.\\n\\nSheared-LLAMA\\n\\nAs a faster and smaller replacement for LLAMA-2, we explore Sheared-LLAMA (Xia et al., 2023), which prunes LLAMA-2-7B and continues pretraining on 50B tokens from the RedPajama dataset (Together, 2023). This allows it to outperform models of comparable sizes that were trained from scratch. Using OTR, we finetune both the 1.3B\"}"}
{"id": "mUSPhG4uDW", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WebLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nWe explore the text-only Turbo variants of the GPT API services offered by OpenAI. In the zero-shot setting, we explore both the GPT-3.5-Turbo-1106 (Brown et al., 2020; Peng et al., 2023) and GPT-4-1106-Preview (OpenAI, 2023b). Additionally, we finetune GPT-3.5-Turbo-1106 for 3 epochs through the finetuning services (Peng et al., 2023), using the validation split for evaluation.\\n\\nIn addition to the text-base version of GPT-4 Turbo, we further explore the variant capable of taking image inputs (OpenAI, 2023c). Apart from adding full-resolution screenshots, the input remains the same as the non-vision variant of GPT-4. Since the input size is already large, including few-shot examples would dramatically increase cost and latency; for example, a 32-shot input for a given turn would result in over 30M pixels (assuming HD resolution) and 66k input tokens, whereas zero-shot results in 2M pixels and 2k tokens in the zero-shot setting.\\n\\nWe finetune the 8B parameter version of Fuyu (Bavishi et al., 2023), a base model released by Adept.ai that is designed to jointly model images and text in a unified decoder transformer-based architecture (Vaswani et al., 2017), relying on linear projection of image patches to avoid using separate image encoders. The model was notably pretrained on high resolution images, and is capable of performing various tasks requiring visual reasoning, reporting competitive results on VQAv2 (Goyal et al., 2019), OKVQA (Marino et al., 2019) and AI2D (Kembhavi et al., 2016). It is also capable of locating objects on real websites, making it a particularly suitable model for our task.\\n\\n### B.7 Hyperparameters\\n\\nTable 14: The training hyperparameters of all models. We give the number of epochs, the batch size (batch), the learning rate (LR), the number of gradient accumulation steps (Accum.), the number of warmup steps (Warm.) and if the model uses flash attention (FA2; Dao et al. 2022; Dao 2023).\\n\\n| Model Size   | Epochs | Batch | LR     | Accum. | Warm. | Vision | FA2 |\\n|--------------|--------|-------|--------|--------|-------|--------|-----|\\n| Sheared-LLaMA 1.3B | 3      | 4     | $5 \\\\cdot 10^{-5}$ | 4     | 0     | \u2717     | \u2713   |\\n| Sheared-LLaMA 2.7B | 3      | 4     | $5 \\\\cdot 10^{-5}$ | 4     | 0     | \u2717     | \u2713   |\\n| Llama-2 (chat-hf) 7B | 3      | 16    | $5 \\\\cdot 10^{-5}$ | 1     | 0     | \u2717     | \u2713   |\\n| Llama-2 (chat-hf) 13B | 3      | 6     | $5 \\\\cdot 10^{-5}$ | 3     | 0     | \u2717     | \u2713   |\\n| Fuyu 8B | 3      | 4     | $5 \\\\cdot 10^{-5}$ | 4     | 0     | \u2713     | \u2717   |\\n| Pix2Act* 282M | 5      | 4     | $2 \\\\cdot 10^{-5}$ | 8     | 100   | \u2713     | \u2717   |\\n| Pix2Act* 1.3B | 5      | 1     | $2 \\\\cdot 10^{-5}$ | 16    | 100   | \u2713     | \u2717   |\\n| MindAct 250M | 5      | 16    | $5 \\\\cdot 10^{-5}$ | 1     | 0     | \u2717     | \u2717   |\\n| MindAct 780M | 5      | 16    | $5 \\\\cdot 10^{-5}$ | 1     | 0     | \u2717     | \u2717   |\\n| MindAct 3B | 5      | 2     | $5 \\\\cdot 10^{-5}$ | 8     | 0     | \u2717     | \u2717   |\\n| Flan-T5 250M | 5      | 8     | $2 \\\\cdot 10^{-5}$ | 2     | 0     | \u2717     | \u2717   |\\n| Flan-T5 780M | 5      | 8     | $2 \\\\cdot 10^{-5}$ | 2     | 0     | \u2717     | \u2717   |\\n| Flan-T5 3B | 5      | 2     | $5 \\\\cdot 10^{-5}$ | 8     | 0     | \u2717     | \u2717   |\\n| GPT-3.5 (Turbo) \u2013 3 \u2013 \u2013 \u2013 \u2013 | \u2717     | \u2013     | \u2717     | \u2013     | \u2013     | \u2013     | \u2013   |\\n\\nAll models presented in Section 5 have the following hyperparameters:\\n\\n- Scheduler: Linear\\n- Maximum Output Tokens: 256\\n- Precision: Brain float16, also known as bf16 (Dean et al., 2012; Google, 2023)\\n- Optimizer: AdamW (Loshchilov & Hutter, 2019), based on the Adam optimizer (Kingma & Ba, 2015)\\n- Parallelization: Fully Sharded Data Parallel (FSDP; Zhao et al. 2023) only for models with 7B+ parameters.\\n- OTR Strategic Truncation (see Section B.6): Target of 2048 tokens. 700 tokens per DOM tree, 40 tokens per utterance in ur, 50 tokens per action in ar, and 65 tokens per candidate string, remaining (approximately 248 tokens) for the\"}"}
{"id": "mUSPhG4uDW", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nFigure 3: Data collection setup (\u00a73). We record interactions (chat and browser actions) between an instructor and human navigator.\\n\\n*Instructor can see the screen except in TEST split.\\n\\nrepresentation of the website \u2013 its HTML code or DOM tree \u2013 include rule-based algorithms (Zhou et al., 2021), accessibility-tree representations offered by browsers (As-souel et al., 2023), graph embeddings (Wang et al., 2022), and model-based approaches (Deng et al., 2022; Li et al., 2022; Aghajanyan et al., 2022; Gur et al., 2024). Previous works for representing the visual information of the webpage usually rely on feature extraction (Liu et al., 2010; Cormer et al., 2017), closely following the research on graphical UIs (Wu et al., 2021; Bunian et al., 2021). Inspired by Deng et al. (2023), we propose a novel dense markup ranker which selects relevant DOM elements, and use these elements optionally combined high-resolution browser screenshots.\\n\\n2.3 Conversational Interfaces\\n\\nUsing conversational interfaces to complete tasks is the basis of task-oriented dialogue (Chen et al., 2017; Zhang et al., 2020b). End-to-end solutions have shown promising results (Zhang et al., 2020a; Kann et al., 2022), but the use of LLMs remains under scrutiny (Hude\u02c7cek & Du\u02c7sek, 2023). For real-world services, Dialog2API (Shu et al., 2022) proposed an interface for interacting with API-based services, whereas META-GUI (Sun et al., 2022) introduced a dataset focused on automating actions in mobile apps rather than general websites. In terms of dialogue-centric web navigation, RUSS (Xu et al., 2021) is the first dataset designed to help support services through 80 demonstrations annotated with a domain-specific language. WEBLINX extends previous dialogue-centric datasets by covering a wide range of real-world tasks spanning 2337 demonstrations, with considerably longer demonstrations due to dynamic topic switching, a subject studied by Adlakha et al. (2022).\\n\\n3 WEBLINX\\n\\nIn this section, we introduce WEBLINX, a large-scale benchmark for conversational web navigation consisting of 2337 demonstrations with an average of 43 turns. It contains interactions between a human user (referred to as instructor) and human assistant (navigator) aiming to complete tasks across 155 real-world websites selected from 15 geographic areas. We classify the websites into 8 categories and 50 subcategories based on their domains.\\n\\nStatistics\\n\\nThe data statistics are summarized in Table 1 and a breakdown by category and split is illustrated by Figure 2. Additional statistics about the dataset, including the number of demonstrations in split, can be found in Appendix A.1, along with the list of categories in Appendix A.2.\\n\\nDemonstration Framework\\n\\nThe demonstrations capture real-time interactions, which are recorded by the navigator controlling the web browser. Each demonstration \\\\( D = \\\\{s_1, a_1, \\\\ldots, s_n, a_n\\\\} \\\\) is a sequence of \\\\( n \\\\) states \\\\( s \\\\in S \\\\) and actions \\\\( a \\\\in A \\\\). At each turn \\\\( t \\\\in \\\\{1, \\\\ldots, n\\\\} \\\\), the state \\\\( s_t \\\\) contains the representation of the website. Each action follows one of the 5 core intents described in Table 3. The full list of intents is provided in Section A.6.\\n\\nData Collection\\n\\nTo collect the demonstrations, we worked with a professional data labeling company, who enlisted 8 expert annotators that received detailed instructions and extensive training to complete our tasks. The annotators worked in pairs: an instructor interacts with a navigator who completes the tasks in a web browser (see Figure 3). Both use the chat interface to communicate, but only the navigator controls the browser. We designed an app, browser extension, and processing pipeline to record the demonstrations, which are subsequently validated by a different annotator under the supervision of the original navigator (details in Appendix A.5).\\n\\nEvaluation Splits\\n\\nIn addition to a TEST split, we create VALID and TESIID to assess in-domain generalization, and 4 out-of-domain splits for various scenarios (see Table 2).\\n\\n3.1 Representing actions and states for modeling\\n\\nAt each turn \\\\( t \\\\), we have access to the state \\\\( s_t \\\\) to predict an action \\\\( a_t \\\\). The state consists of the following (if available):\\n\\n- \\\\( c_t \\\\): Candidate elements that can be targeted by \\\\( a_t \\\\),\\n- \\\\( d_t \\\\): Current DOM tree of the page,\\n- \\\\( i_t \\\\): Screenshot of the navigator's browser,\\n- \\\\( p_t \\\\): Previous action.\\n\\nTable 3: Overview of the WEBLINX core action space. For full set of actions, see Table 6.\\n\\n| Action Description | Description |\\n|--------------------|-------------|\\n| click(element)     | click on an element |\\n| load(url)          | load URL of a new page |\\n| say(text)          | navigator's utterance |\\n| submit(element)    | submit a form |\\n| textinput(element,value) | type text into the element |\"}"}
{"id": "mUSPhG4uDW", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEB LINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\n- u: Instructor's utterance,\\n- v: Viewport size (height and width),\\n- h: Interaction history.\\n\\nNote that a state need not contain all of the above. For example, at the start of a demonstration, the instructor and navigator may need multiple rounds of dialogue to properly define the objective, in which case the initial states do not have DOM trees or screenshots. A model predicts an action \\\\(a\\\\) for a given state \\\\(s\\\\) based on a prompt template \\\\(pm\\\\) which indicates how to make use of the contents in a state.\\n\\nInteraction history\\nSince a model has a limited input length in practice, we represent history \\\\(h\\\\) as the set of past five actions (denoted as \\\\(a_r\\\\)) and five utterances (\\\\(u_r\\\\)). We could not include the representation of past states such as elements or screenshots.\\n\\nParsing Action Output\\nAn action consists of an intent and argument and can be generated by an agent in a textual format. It must follow a pre-defined structure (see Table 3) that allows it to be parsed into a structured form, which can be executed in a browser using tools like Selenium. We discuss additional details in Appendix A.4.\\n\\n4 Evaluation Framework\\nIn this section, we describe the evaluation metrics (\u00a74.1) and their applicability to specific groups of intents (\u00a74.2).\\n\\n4.1 Metrics\\nA commonly used metric in prior work on web navigation is task success rate, which measures the proportion of demonstrations where the model reached the desired final state (Shi et al., 2017; Yao et al., 2022; Deng et al., 2023). However, this metric is inappropriate for our benchmark because the objective is not fully defined in the first turn or later turns; instead, it evolves as the conversation proceeds. We instead leverage turn-level automatic evaluation metrics, following established approaches in dialogue systems (Rastogi et al., 2020; Zhang et al., 2020a). The aim of the metrics is to provide a heuristic estimate of the similarity between the predicted action and the reference action.\\n\\nIntent Match (IM)\\nGiven prediction \\\\(a'\\\\) and reference \\\\(a\\\\), the intent match is \\\\(IM(a', a) = 1\\\\) if the intents are equal, otherwise \\\\(IM(a', a) = 0\\\\). This tells us if a model can correctly identify which action to perform, but does not indicate if the model can predict the correct arguments.\\n\\nElement Similarity using IoU\\nFor actions with elements as arguments (click, textinput, submit), we compute the intersection over union (IoU; Jaccard 1912). Given the area of a bounding box \\\\(B\\\\), we have:\\n\\n\\\\[\\nIM(a', a) \\\\times B_{\\\\text{reference}} \\\\cap B_{\\\\text{predicted}}\\n\\\\]\\n\\nTo compute the area, we use \\\\((x,y)\\\\) coordinates of the reference and predicted bounding boxes. This formulation (1) favors elements with high visual overlap, (2) penalizes predicting elements much smaller or larger than reference elements even if one is completely contained by the other, and (3) assigns 0 if the elements do not overlap.\\n\\nText Similarity using F1\\nTo measure lexical similarity of text arguments in say and textinput, we calculate chrF (Popovic, 2015), an F1-score for character n-gram matches (we use the default setting of \\\\(n = 6\\\\)). Similar to IoU, we scale by the IM, resulting in \\\\(IM(a', a) \\\\times \\\\text{CHR}_F(a', a)\\\\). In the case of load intent, URLs follow a structure that can be consistently segmented, which leads us to apply the F1-score on segments instead of n-grams; we call this measure URLF.\\n\\nWe use F1 to refer to either chrF and URLF, depending on whether an action contains a text or URL argument.\\n\\n4.2 Turn-level score and overall score\\nTo allow better comparisons between models, we divide the intents into groups: The element group (EG) contains click, textinput, and submit, and is evaluated with IoU. The text group (TG) encompasses load, say, and textinput, and is evaluated with F1.\\n\\nWe assign a turn level score based on the following: If the turn involves an action in EG, the score is the same as IoU, i.e. score is 0 when the intent is incorrect or the element doesn't overlap, it is 1 when intent is correct and the element perfectly overlaps, and it is somewhere in between for the rest. For TG actions load and say, the score is same as F1, i.e., score is 0 when either intent is incorrect or there is no text overlap, it is 1 when intent is correct and the text matches exactly, and it is somewhere in between for the rest. For textinput, the turn score is IoU \\\\(\\\\times\\\\) F1 since it contains both text and element arguments. Finally, we compute the overall score using the micro-average of turn-level scores.\\n\\n5 Methods\\nIn this section, we describe a method for selecting candidate elements (\u00a75.1) and how to use them in textual input. We use these methods to build models that can accurately predict actions (\u00a75.2). We report results in Section 6 and provide implementation details in Appendix B.\\n\\n5.1 Dense Markup Ranking (DMR) for Candidate Selection and Input Representation\\nTo choose a set of suitable candidates for the model input (\u00a73.1), we need a candidate selection stage that filters the\"}"}
{"id": "mUSPhG4uDW", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nDeng et al. (2023) proposed to pair each DOM element with the task query and input them into a DeBERTa model (He et al., 2021), which is finetuned using a cross-encoder loss (Reimers & Gurevych, 2019). We found this method takes on average 916ms to select candidates for a given turn. When factoring in network latency and LLM inference, this would result in poor processing time. It is thus crucial that we use efficient ranking method to build agents that can operate in real time and learn from interactions with users.\\n\\nTo solve this, we propose Dense Markup Ranking (DMR), which is 5 times faster than the previous approach, at the cost of slightly lower recall. The method consists of: (1) a simplified element representation to reduce computational overhead; (2) a dual encoder-based approach (Reimers & Gurevych, 2019; Karpukhin et al., 2020); (3) similarity-based learning between the text representation of $s_t \u2212 s_{t\u22121}$ and corresponding HTML elements. Using this method, we finetune a variant of MiniLM (Wang et al., 2020).\\n\\nWe formulate the cosine-based learning objective, examine the inference speed improvements, and evaluate alternatives in Appendix B.4.\\n\\nEven after our candidate selection, the input sequence length to a model can exceed its limit, so we truncate the sequence. To reduce information loss from traditional truncation (e.g., for large DOM elements and long history), we design a strategy that leverages the hierarchical nature of the input to determine which subsection should be truncated. We introduce several improvements to the representation used in prior works by including the full HTML attributes, viewport size, XML Path, and the bounding boxes of candidate elements (implementation details in Appendices B.1 and B.2).\\n\\n5.2 Modeling Actions\\n\\nUpon selecting the most promising candidates for a given state $s_t$, we can combine them with the remaining information in $s_t$ to construct a representation that can be used to predict action strings, which can be parsed and executed (\u00a73.1). To understand which factors matter for predicting actions, we examine 19 zero-shot and finetuned models (using the TRAIN split) with different input modalities: image-only, text-only, and both. We provide implementation details in Appendix B.6 and hyperparameters in Appendix B.7.\\n\\nModel Categories\\n\\nWe categorize action models by the input modality, since the output is always in a structured format (\u00a73.1). We define the following types: (1) text-only, which receives instructions, pruned DOM tree, candidate description and history; (2) image-to-text, which receives the screenshot, instructions and past actions directly embedded in the image; (3) multimodal, which receives the screenshot, instructions, pruned DOM tree, candidate description and history directly as text. Additional discussions are found in Appendix B.3.\\n\\nText-only models\\n\\nThe recent MindAct (Deng et al., 2023) model is a Flan-T5 (Chung et al., 2022b) model that has been finetuned on Mind2Web. We further fine-tune it on WEBLINX using its original configuration.\\n\\nTo quantify the improvements brought by DMR-based representation (\u00a75.1), we directly finetune Flan-T5 checkpoints, allowing us to control for size and architecture with respect to MindAct. We also finetune Llama-2 (Touvron et al., 2023a;b) and a distilled version, Sheared-LLaMA (S-LLaMA; Xia et al. 2023).\\n\\nProprietary text-only LLMs\\n\\nWe report results for GPT-3.5 Turbo (Brown et al., 2020; Peng et al., 2023), in both zero-shot (3.5T) and finetuned (3.5F) settings. We also include zero-shot results for GPT-4T (OpenAI, 2023b).\\n\\nImage-to-text modeling\\n\\nWe explore Pix2Act (Shaw et al., 2023) an encoder-decoder (Vaswani et al., 2017) purely finetuned on pixels. It uses a Pix2Struct backbone (Lee et al., 2023), which is pretrained on screenshots using a Vision Transformer encoder (Dosovitskiy et al., 2021) and a text decoder. We follow the behavior cloning approach used by Pix2Act by finetuning the same backbone on WEBLINX.\\n\\nMultimodal models\\n\\nWe finetune Fuyu-8B (Bavishi et al., 2023), a base model pretrained on browser screenshots by.\\n\\n### Table 4: Aggregated results (\u00a76) across major models (\u00a75), sorted by parameter count (Size). Following metrics from Section 4, we report results of intent match (using IM), element group (IoU), text group (F1), and the overall score (using micro-average on turn-level scores). All results are on TESTOOD except the last column which is on TESTIID. \u1f621 indicates models with access to screenshot; every model except Pix2Act has access to text inputs.\\n\\n| Models       | Size | IM   | IoU  | F1   | TESTOOD Score | TESTIID Score |\\n|--------------|------|------|------|------|---------------|---------------|\\n| Zero-shot    |      |      |      |      |               |               |\\n| Llama-2 13B | 43.7 | 4.8  | 1.3  | 5.2  | 5.6           |               |\\n| GPT-3.5T \u2013  | 42.8 | 8.6  | 3.5  | 8.5  | 10.3          |               |\\n| GPT-4T \u2013     | 41.7 | 10.9 | 6.8  | 10.7 | 12.2          |               |\\n| GPT-4V \u1f621 \u2013 | 42.4 | 10.9 | 6.2  | 10.4 | 12.9          |               |\\n| Finetuned    |      |      |      |      |               |               |\\n| Pix2Act \u1f621   | 1.3B | 81.8 | 8.3  | 25.2 | 16.9          | 23.9          |\\n| S-LLaMA 2.7B| 84.0 | 22.6 | 27.2 | 25.0 | 37.4          |               |\\n| MindAct 3B   | 79.9 | 16.5 | 23.2 | 20.9 | 25.7          |               |\\n| Flan-T5 3B   | 81.1 | 20.3 | 25.8 | 23.8 | 31.1          |               |\\n| Fuyu \u1f621      | 8B   | 80.1 | 15.7 | 22.3 | 20.0          | 30.9          |\\n| Llama-2 13B | 83.0 | 22.8 | 26.6 | 25.2 | 37.0          |               |\\n| GPT-3.5F \u2013   | 77.6 | 18.6 | 22.4 | 21.2 | 30.8          |               |\\n\\nAdditional discussions are found in Appendix B.3.\"}"}
{"id": "mUSPhG4uDW", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Results on out-of-domain splits (\u00a72) for finetuned LLaMA-2-13B (\u00a75.2). Among the splits, TESTCAT seems to be the hardest, indicating that models struggle on unseen subcategories (e.g., restaurant appointment vs. medical appointment).\\n\\n| Splits | Intent | Element | Text | Overall IM | IoU | F1 |\\n|--------|--------|---------|------|-----------|-----|----|\\n| TESTWEB | 82.7 | 24.2 | 28.7 | 27.0 |\\n| TESTCAT | 81.0 | 20.7 | 26.1 | 24.3 |\\n| TESTGEO | 78.6 | 22.0 | 27.7 | 25.9 |\\n| TESTVIS | 85.3 | 26.1 | 23.9 | 25.0 |\\n\\nIn Section 6.1, we observe that MindAct trails behind Flan-T5 finetuned using DMR-based input representation (\u00a75.1), when comparing the 3B-parameter variants. Although MindAct was finetuned for a related task, it was never exposed to multi-turn dialogue. However, Flan-T5 was never trained on any navigation actions. Thus, DMR-based representation plays an important role in achieving a better performance for the same architecture and model size. Moreover, both LLaMa-based models outperform Flan-T5 and MindAct despite Sheared-LLaMa being smaller than Flan-T5. This could be due to the high quality training of LLaMa models on a large number of instruction-following tasks compared to Flan-T5. However, it is intriguing that Sheared-LLaMa performs equally well compared to LLaMA-2 13B.\\n\\nIn the zero-shot setting, where models solely rely on the instructions, we observe that proprietary models (GPT-3.5T and GPT-4T) outperform the open-sourced LLaMA-2. However, when finetuned, GPT-3.5F is outperformed by Sheared-LLaMA and LLaMA-2, but the cause is unclear as most hyperparameters are inaccessible for commercial training. Finally, GPT-4V and GPT-4T achieve similar performance, suggesting that existing multimodal models might not be able to effectively use screenshots for predicting actions.\\n\\nWhen comparing TESTOOD with TESTIID results, we observe a major difference across all finetuned models. This highlights a weakness of finetuned models: although they perform well on familiar websites, they will struggle to generalize to unseen websites. For example, we observe in Table 5 that LLaMa-13B achieves poor results on TESTCAT, indicating that unseen subcategories are more challenging than new websites from the same categories. For instance, if the model learns how to book seats at a restaurant, it can adapt to a different restaurant but will struggle to book a medical appointment.\"}"}
{"id": "mUSPhG4uDW", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Websites overview\\n\\nTable 12 shows all entrypoints (website where a demo starts). We choose popular and also lesser known sites to achieve categorical and geographic diversity. The websites are either specifically chosen by the authors or the annotators, who collaboratively ensured they are appropriate for our tasks \u2013 consequently, we do not include unsafe websites. In the case of social interactions, we choose websites with terms of use prohibiting offensive content. For instance, https://facebook.com states that \u201cWe remove content that could contribute to a risk of harm to the physical security of persons. Content that threatens people has the potential to intimidate, exclude or silence others and isn\u2019t allowed on Facebook.\u201d\\n\\n| Name          | Category       | Subcategory     | Geography      | URL                     |\\n|---------------|----------------|-----------------|----------------|-------------------------|\\n| Airbnb        | Booking        | Stays           | International  | https://www.airbnb.com  |\\n| Airtable      | Productivity   | Spreadsheet     | International  | https://airtable.com    |\\n| Aldi (Australia) | Shopping   | Grocery         | Australia      | https://www.aldi.com/en/|\\n| Aliexpress    | Shopping       | Online Shopping | International  | https://www.aliexpress.com/|\\n| AllenAI\u2019s CV  | Explore        | AI Tools        | Computer Vision | USA                     | https://vision-explorer.allenai.org/|\\n| Amazon        | Shopping       | Online Shopping | International  | https://www.amazon.com  |\\n| Asana         | Productivity   | Kanban          | International  | https://asana.com       |\\n| ASOS          | Shopping       | Clothing        | International  | https://www.asos.com/men/|\\n| BBC News      | Summarizing    | News Articles   | International  | https://www.bbc.com    |\\n| Bing Image Creator | AI Tools | Image Generation | International  | https://www.bing.com/create |\\n| Bing Translator | AI Tools      | Auto. Translation | International  | https://www.bing.com/translator |\\n| Blogger       | Composing      | Blog            | International  | https://www.blogger.com/|\\n| Booking.com   | Booking        | Stays           | International  | https://www.booking.com  |\\n| Booknbook     | Booking        | Restaurant      | International  | https://www.booknbook.com/|\\n| Brandmark     | AI Tools       | Image Generation | International  | https://brandmark.io/  |\\n| Britannica    | Info. Lookup   | Encyclopedia    | International  | https://www.britannica.com/|\\n| Calculator.net| Investment     | Productivity    | Finance        | International  | https://www.calculator.net/investment-calculator.html |\\n| ChatGPT       | AI Tools       | Chatbot         | International  | https://openai.com      |\\n| cheaptickets  | Booking        | Transport       | International  | https://www.cheaptickets.com/|\\n| CIA World Factbook | Info. Lookup | Agency          | USA            | https://www.cia.gov/the-world-factbook/ |\\n| CNN           | Summarizing    | News Articles   | International  | https://edition.cnn.com/|\\n| Copy AI       | AI Tools       | Writing Assistant | International  | https://www.copy.ai/  |\\n| DeepL         | AI Tools       | Auto. Translation | International  | https://www.deepl.com  |\\n| delivery      | Shopping       | Delivery        | USA            | https://www.delivery.com/|\\n| Dictionary    | Info. Lookup   | Encyclopedia    | International  | https://www.dictionary.com/|\\n| Discord       | Social Interaction | Instant Messaging | International  | https://discord.com |\\n| Discourse     | Social Interaction | Discussion Platf. | International  | https://try.discourse.org/ |\\n| Doordash      | Shopping       | Delivery        | International  | https://www.doordash.com/|\\n| eBay          | Shopping       | Online Shopping | International  | https://www.ebay.com/  |\\n| Encyclopedia.com | Info. Lookup | Encyclopedia    | International  | https://www.encyclopedia.com/|\\n| Etsy          | Shopping       | Handmade        | International  | https://www.etsy.com/in-en |\\n| European Commission | Info. Lookup | Government      | Europe         | https://europa.eu/ |\\n| Eventbrite    | Booking        | Social Event    | International  | https://www.eventbrite.com/|\\n| Eventbrite (AU) | Booking      | Social Event    | Australia      | https://www.eventbrite.com/au/ |\\n| Expedia       | Booking        | Stay            | International  | https://www.expedia.com/|\\n| Facebook      | Social Interaction | Social Network | International  | https://www.facebook.com/login/ |\\n| Fandom        | Info. Lookup   | Entertainment   | International  | https://www.fandom.com/|\\n| Fastmail      | Composing      | Email           | International  | https://fastmail.com/ |\\n| Flickr        | Social Interaction | Image Sharing | International  | https://www.flickr.com/|\\n| Frontiers     | Summarizing    | Scientific Articles | International  | https://www.frontiersin.org/journals/ |\\n| Genius        | Social Interaction | Music Sharing | International  | https://genius.com |\\n| Gmail         | Composing      | Email           | International  | https://mail.google.com/|\\n| GMX Email     | Composing      | Email           | International  | https://www.gmx.com/  |\\n| Google Bard   | AI Tools       | Chatbot         | International  | https://bard.google.com/|\\n| Google Calendar| Productivity   | Calendar        | International  | https://calendar.google.com/calendar/ |\\n| Google Docs   | Composing      | Note taking     | International  | https://docs.google.com/document |\\n| Google Flights| Booking        | Transport       | International  | https://www.google.com/travel/flights |\\n| Google Keep   | Composing      | Tasks           | International  | https://keep.google.com/ |\\n| Google Scholar | Info. Lookup | Research Directory | International  | https://scholar.google.com/ |\\n| Google Sheets | Productivity   | Spreadsheet     | International  | https://docs.google.com/spreadsheets |\\n| Google Slides | Productivity   | Presentation    | International  | https://docs.google.com/presentation |\\n| Google Translate | AI Tools  | Auto. Translation | International  | https://translate.google.com |\\n| Gov. of Canada | Productivity   | Finance         | Canada         | https://itools-ioutils.fcac-acfc.gc.ca/BP-PB/budget-planner-tool|\\n| Grammarly (Paraphrasing) | AI Tools | Writing Assistant | International  | https://www.grammarly.com/paraphrasing-tool |\\n| Grubhub       | Shopping       | Delivery        | International  | https://www.grubhub.com/|\\n| Gutenberg     | Summarizing    | Books           | International  | https://www.gutenberg.org/|\\n| Hacker News   | Social Interaction | Discussion Platf. | USA           | https://news.ycombinator.com/ |\\n\\nContinued on next page\"}"}
{"id": "mUSPhG4uDW", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Name                      | Category       | Subcategory | Geography | URL                                      |\\n|---------------------------|----------------|-------------|-----------|-----------------------------------------|\\n| Hostelworld Booking       | Booking        | Stays       | International | https://www.hostelworld.com/           |\\n| hoteldolux                | Booking        | Stay        | International | https://hoteldolux.com/               |\\n| howStuffWorks             | Info. Lookup   | Educational  | International | https://www.howstuffworks.com/        |\\n| IKEA                      | Shopping       | Furniture   | International | https://www.ikea.com/                 |\\n| IMDB                      | Info. Lookup   | Entertainment | International | https://www.imdb.com/                 |\\n| Imgur                     | Social Interaction | Image Sharing | International | https://imgur.com/                   |\\n| Independent.ie            | Summarizing    | News Articles | Ireland     | https://www.independent.ie/           |\\n| Instacart                 | Shopping       | Delivery    | North America | https://www.instacart.com/            |\\n| Instagram                 | Social Interaction | Image Sharing | International | https://www.instagram.com/            |\\n| Investopedia              | Info. Lookup   | Media       | International | https://www.investopedia.com/         |\\n| Jack's 50 top food bloggers | Summarizing  | Cooking     | International | https://jacksfoodblog.com/2020/04/26/50-top-food-bloggers-of-2020-the-best-recipe-sites-ranked/ |\\n| JamesOnlineBookClub       | Summarizing    | Reviews     | International | https://jamesonlinebookclub.com/      |\\n| Kayak                     | Booking        | Transport   | International | https://www.kayak.com/                |\\n| Khan Academy              | Info. Lookup   | Educational  | USA         | https://www.khanacademy.org/          |\\n| Koo                       | Social Interaction | Social Network | India      | https://www.kooapp.com/feed           |\\n| LinkedIn                  | Social Interaction | Prof. Network | International | https://www.linkedin.com/             |\\n| Loblaws                   | Shopping       | Grocery     | Canada      | https://www.loblaws.ca/               |\\n| Luko.eu                   | Booking        | Medical     | Europe      | https://de.luko.eu/en/advice/guide/best-rated-tierartz-veterinarians-by-states/ |\\n| Macy's                    | Shopping       | Clothing    | USA         | https://www.macys.com/                |\\n| Marie Claire              | Summarizing    | Magazine    | International | https://www.marieclaire.com/          |\\n| MarketWatch               | Productivity   | Finance     | USA         | https://www.marketwatch.com/          |\\n| Medium                    | Composing      | Blog        | International | https://medium.com/                  |\\n| Meetup (Glasgow, Scotland)| Booking       | Social Event | Scotland    | https://www.meetup.com/find/?eventType=inPerson&source=EVENTS&location=gb--v2--Glasgow |\\n| momondo                   | Booking        | Transport   | International | https://www.momondo.com/              |\\n| MyFitnessPal              | Composing      | Recipe      | International | https://www.myfitnesspal.com/recipe/calculator |\\n| Myntra                    | Shopping       | Clothing    | India       | https://www.myntra.com/               |\\n| NASA                      | Info. Lookup   | Agency      | USA         | https://www.nasa.gov/                 |\\n| National Geographic       | Summarizing    | Magazine    | International | https://www.nationalgeographic.com/magazine |\\n| New Yorker                | Summarizing    | Magazine    | USA         | https://www.newyorker.com/            |\\n| New Zealand Government    | Info. Lookup   | Government  | New Zealand | https://www.govt.nz/                  |\\n| Nextdoor                  | Social Interaction | Discussion Platf. | International | https://nextdoor.com/                |\\n| NHS - Find a dentist      | Booking        | Medical     | UK          | https://www.nhs.uk/service-search/find-a-dentist |\\n| Nightcafe                 | AI Tools       | Image Generation | International | https://creator.nightcafe.studio/   |\\n| Nirvanahq                 | Composing      | Tasks       | International | https://www.nirvanahq.com/            |\\n| Notion                    | Composing      | Note taking | International | https://www.notion.so/               |\\n| NYT                       | Info. Lookup   | Media       | USA         | https://www.nytimes.com/              |\\n| Ontario Veterinarians     | Booking        | Medical     | Canada      | https://www.ovma.org/pet-owners/find-a-veterinarian/ |\\n| OpenStax                  | Summarizing    | Books       | International | https://openstax.org/subjects         |\\n| OpenTables                | Booking        | Restaurant  | International | https://www.opentable.com/            |\\n| Orbitz                    | Booking        | Transport   | International | https://www.orbitz.com/               |\\n| Outlook                   | Composing      | Email       | International | https://outlook.live.com/             |\\n| Penzu                     | Composing      | Note taking | International | https://penzu.com/                   |\\n| Perplexity                | AI Tools       | Chatbot     | International | https://www.perplexity.ai/            |\\n| Pinterest                 | Social Interaction | Image Sharing | International | https://www.pinterest.com/           |\\n| Plos ONE                  | Summarizing    | Scientific Articles | International | https://plos.org/                   |\\n| Postmates                 | Shopping       | Delivery    | USA         | https://postmates.com/               |\\n| Proton                    | Composing      | Email       | International | https://proton.me/mail               |\\n| Quandoo                   | Booking        | Restaurant  | International | https://www.quandoo.com/              |\\n| QuillBot                  | AI Tools       | Writing Assistant | International | https://quillbot.com/                |\\n| Quora                     | Social Interaction | Question Answering | International | https://quora.com/                 |\\n| Reader's Digest           | Summarizing    | Magazine    | Australia   | https://www.readersdigest.com.au/     |\\n| Reddit                    | Info. Lookup   | Forum       | International | https://www.reddit.com/              |\\n| Resy                      | Booking        | Restaurant  | International | https://resy.com/                    |\\n| Reverso                   | Translation AI Tools | Auto. Translation | International | https://www.reverso.net/text-translation |\\n| Seamless                 | Shopping       | Delivery    | USA         | https://www.seamless.com/             |\\n| Semantic Scholar          | Info. Lookup   | Research Directory | International | https://www.semanticscholar.org/      |\\n| Simplenote                | Composing      | Note taking | International | https://simplenote.com/              |\\n| Singapore Food Blogs      | Summarizing    | Cooking     | Singapore   | https://ordinarypatrons.com/popular-singapore-food-blogs/ |\\n| Skyscanner                | Booking        | Transport   | International | https://www.skyscanner.com/          |\\n| Slack                     | Social Interaction | Instant Messaging | International | https://slack.com/                  |\\n| SNCF                      | Booking        | Transport   | France      | https://sncf.com/                    |\\n| Soundcloud                | Social Interaction | Music Sharing | International | https://soundcloud.com/              |\\n| Squarespace               | Composing      | Blog        | International | https://squarespace.com/             |\\n| Stable Diffusion          | AI Tools       | Image Generation | International | https://huggingface.co/spaces/stabilityai/stable-diffusion/ |\"}"}
{"id": "mUSPhG4uDW", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Name          | Category        | Subcategory      | Geography     | URL                        |\\n|---------------|-----------------|------------------|---------------|----------------------------|\\n| StackExchange | Info. Lookup    |                  | International | https://stackexchange.com/ |\\n| TableAgent    | Booking         | Restaurant       | International | https://tableagent.com/     |\\n| Target        | Shopping        | Online Shopping  | International | https://www.target.com/    |\\n| The Guardian  | Summarizing     | News Articles    | International | https://www.theguardian.com/|\\n| The Marshalla Project | Summarizing News Articles | USA | https://www.themarshallproject.org/ |\\n| The Fork      | Booking         | Restaurant       | Europe        | https://www.thefork.com/   |\\n| Todoist       | Productivity    | Kanban           | International | https://todoist.com/app/   |\\n| Tome AI Tools | Writing Assistant |                | International | https://tome.app/          |\\n| Travelocity   | Booking         | Stay             | International | https://www.travelocity.com/|\\n| Trello        | Productivity    | Kanban           | International | https://trello.com/        |\\n| Trip          | Booking         | Transport        | International | https://www.trip.com/     |\\n| TripAdvisor   | Booking         | Stay             | International | https://www.tripadvisor.com/|\\n| Trivago       | Booking         | Stay             | India         | https://www.trivago.in/en-IN/ |\\n| Tumblr        | Social Interaction | Social Network | International | https://www.tumblr.com/   |\\n| Twitch        | Social Interaction | Video Sharing | International | https://www.twitch.tv/    |\\n| Twitter       | Social Interaction | Social Network | International | https://twitter.com       |\\n| UberEats      | Shopping        | Delivery         | International | https://www.ubereats.com/ |\\n| UNIQLO (Europe)| Shopping        | Clothing         | Europe        | https://www.uniqlo.com/eu/en/home/|\\n| Via Rail      | Booking         | Transport        | Canada        | https://www.viarail.ca/en/ |\\n| VRBO          | Booking         | Stay             | International | https://www.vrbo.com/     |\\n| Walmart      | Shopping        | Online Shopping  | International | https://www.walmart.com/  |\\n| Wattpad       | Composing       | Blog             | International | https://www.wattpad.com/  |\\n| Wayfair       | Shopping        | Online Shopping  | International | https://www.wayfair.com/  |\\n| Wealthsimple  | Tax Calculator   | Productivity     | Finance       | https://www.wealthsimple.com/en-ca/tool/tax-calculator/ |\\n| When2meet     | Productivity    | Calendar         | International | https://www.when2meet.com/|\\n| Wikipedia     | Info. Lookup    | Encyclopedia     | International | https://wikipedia.org/    |\\n| World Atlas   | Info. Lookup    | Geography        | International | https://www.worldatlas.com/|\\n| World Health Organization | Info. Lookup Agency | International | https://www.who.int/            |\\n| Yahoo Mail    | Composing       | Email            | International | https://mail.yahoo.com/   |\\n| YouWrite      | AI Tools        | Writing Assistant | International | https://you.com/write   |\\n| YouTube       | Social Interaction | Video Sharing  | International | https://youtube.com       |\\n| Zalora        | Shopping        | Clothing         | Southeast Asia | https://www.zalora.com/   |\\n| Zappos        | Shopping        | Online Shopping  | USA           | https://www.zappos.com/   |\\n| Zara (Philippines) | Shopping | Clothing        | Philippines   | https://www.zara.com/ph/en/ |\\n\\n26\"}"}
{"id": "mUSPhG4uDW", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEB LINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nB Modeling Details\\n\\nB.1 Optimal Text Representation (OTR)\\n\\nSimilar to Mind2Web (Deng et al., 2023), we use the top-10 candidates selected by DMR (\u00a75.1) and start by pruning the DOM tree to contain elements relevant to the candidates. However, we make the following changes:\\n\\n1. **HTML**: In addition to tags and children, we incorporate attributes and values of elements in the DOM tree. For example, a div element with attributes class mapping to container would be provided as div class=\\\"container\\\"(...), where ... would be the children elements.\\n\\n2. **Viewport**: We specify the viewport size, which can be used by the model to calculate the coordinates of the bounding boxes with respect to the screen.\\n\\n3. **Candidate representation**: We include the XML Path and bounding box coordinates, and use two square brackets to separate the two elements. We use a template \\\\[[xpath]\\\\] /html/<...>/<tag> \\\\[[bbox]\\\\] x=<x> y=<y> width=<w> height=<h>, where <x>, <y>, <w>, <h> are the bounding box coordinates, and <tag> is the tag of the target element, with <...> replaced with the parents. Furthermore, instead of mapping each candidate its alphabetical order, we prefix it with its unique ID, allowing the model to directly refer to an element rather than having to remap the alphabetical order back to an element reference.\\n\\n4. **Truncation**: We truncate the final result as described in Section 5.1 and Appendix B.2. We choose limits that maximize the information included in the context while remaining under an ideal limit that is compatible with all models considered (see Appendix B.7 for hyperparameter details).\\n\\nB.2 Strategic Truncation\\n\\nIn Section 5.1, we highlight the importance of reducing the input sequence length, i.e., to avoid exceeding the limit allowed by models used in our experiments. Although certain models can process longer sequences, shorter sequences are faster to process, requires less memory and require lower running cost when using proprietary LLMs. Naively truncating from the right or left side could lead to major information loss. To avoid this, we set a limit to each component of the input text (dt, ur, ct, ar). Then, we truncate each component based on the limit by decomposing them into sub-components and strategically truncating each sub-components until the limit is reached.\\n\\n**Definition**\\n\\nFor a given limit (in number of tokens), our goal is to truncate a component (one of dt, ur, ct, ar) until we reach the limit. If a component was already under the limit, then the difference is saved for ct, which is computed last.\\n\\n**Rendering-based reduction**\\n\\nSince a component is an object (e.g., dt is an element tree), we need to obtain the text representation before being able to estimate the number of tokens. We thus need a rendering function that converts a component or sub-component into text, which can then be tokenized. Then, we can estimate the reduction (number of tokens to take away) in order to reach the limit.\\n\\n**Sub-components**\\n\\nEach component is composed of sub-components, which we can render, tokenize and truncate individually. In the case of dt, since we have a tree of elements where the attribute should be preserved, we only count the values and text content as sub-components. For ct, we consider the xpath, attributes and children tags to be sub-components, protecting the tag and bounding box, as well as the keys inside the square brackets. For ur, we simply consider each utterance as a sub-component. For ar, each action is considered a sub-component.\\n\\n**Reducing by length**\\n\\nAlthough it is simpler to reduce all sub-components equally, this may lead to scenarios where short sub-components are heavily penalized due to very long sub-components making up most of the token counts. To avoid this, we instead find a threshold such that, by reducing all sub-components above this threshold, the sub-components' truncated lengths sum up to the target limit. This threshold can be easily computed by first sorting the sub-components, then iterate through the lengths until the cumulative sum is greater than the limit, before finally reducing the length of the sub-components until the cumulative sum is under the limit.\\n\\nBy applying the steps above, we can ensure that each component respects a limit, which we can set in a way that they add up to a desired total limit, such as L = 2048.\"}"}
{"id": "mUSPhG4uDW", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mUSPhG4uDW", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mUSPhG4uDW", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEB LINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nOpenAI. New models and developer products announced at DevDay, November 2023. URL https://openai.com/blog/new-models-and-developer-products-announced-at-devday.\\n\\nOpenAI. ChatGPT Plugins. https://openai.com/blog/chatgpt-plugins, 2023. Accessed: 2023/09/03.\\n\\nPasupat, P., Jiang, T., Liu, E. Z., Guu, K., and Liang, P. Mapping natural language commands to web elements. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J. (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 4970\u20134976, 2018. URL https://aclanthology.org/D18-1540/.\\n\\nPeng, A., Wu, M., Kilpatrick, L., and Heidel, S. GPT-3.5 Turbo fine-tuning and API updates, August 2023. URL https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates.\\n\\nPinsky, Y. Bard can now connect to your google apps and services, Sep 2023. URL https://blog.google/products/bard/google-bard-new-features-update-sept-2023/.\\n\\nPopovic, M. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT at EMNLP 2015, 17-18 September 2015, Lisbon, Portugal, pp. 392\u2013395, 2015. URL https://doi.org/10.18653/v1/w15-3049.\\n\\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. CoRR, abs/2305.18290, 2023. URL https://doi.org/10.48550/arXiv.2305.18290.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the Limits of Transfer Learning with a Unified Text-to-text Transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67, 2020. URL http://jmlr.org/papers/v21/20-074.html.\\n\\nRastogi, A., Zang, X., Sunkara, S., Gupta, R., and Khaitan, P. Towards Scalable Multi-domain Conversational Agents: The Schema-guided Dialogue Dataset. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 8689\u20138696, 2020. URL https://doi.org/10.1609/aaai.v34i05.6394.\\n\\nRawles, C., Li, A., Rodriguez, D., Riva, O., and Lillicrap, T. P. Android in the Wild: A Large-scale Dataset for Android Device Control. CoRR, abs/2307.10088, 2023. URL https://doi.org/10.48550/arXiv.2307.10088.\\n\\nReimers, N. and Gurevych, I. Sentence-BERT: Sentence Embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982\u20133992, Hong Kong, China, November 2019. URL https://aclanthology.org/D19-1410.\\n\\nShaw, P., Joshi, M., Cohan, J., Berant, J., Pasupat, P., Hu, H., Khandelwal, U., Lee, K., and Toutanova, K. From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces. CoRR, abs/2306.00245, 2023. URL https://doi.org/10.48550/arXiv.2306.00245.\\n\\nShi, T., Karpathy, A., Fan, L., Hernandez, J., and Liang, P. World of Bits: An Open-domain Platform for Web-based Agents. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 3135\u20133144, 2017. URL http://proceedings.mlr.press/v70/shi17a.html.\\n\\nShu, R., Mansimov, E., Alkhouli, T., Pappas, N., Romeo, S., Gupta, A., Mansour, S., Zhang, Y., and Roth, D. Dialog2API: Task-oriented Dialogue with API Description and Example Programs. CoRR, abs/2212.09946, 2022. URL https://doi.org/10.48550/arXiv.2212.09946.\\n\\nSun, L., Chen, X., Chen, L., Dai, T., Zhu, Z., and Yu, K. META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 6699\u20136712, 2022. URL https://doi.org/10.18653/v1/2022.emnlp-main.449.\\n\\nTogether. Redpajama: an open dataset for training large language models, october 2023. URL https://github.com/togethercomputer/RedPajama-Data.\\n\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. LLaMA: Open and Efficient Foundation Language Models. CoRR, abs/2302.13971, 2023. URL https://doi.org/10.48550/arXiv.2302.13971.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., 14\\n\\nTogether. Redpajama: an open dataset for training large language models, october 2023. URL https://github.com/togethercomputer/RedPajama-Data.\"}"}
{"id": "mUSPhG4uDW", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mUSPhG4uDW", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nInstructor wants Navigator to open a specific tab on a News site, i.e., \u201cSportsday on 28 May 2023 at 4.15 AM\u201d.\\n\\nInstructor requests the location on a food delivery site to be set to Las Vegas. The Delivery details window is already open.\\n\\nGPT-4V (R) clicks on an incorrect (3:30AM) tab, even though the instructor requested a different time (4:15AM).\\n\\nGPT-4V (R) attempts to exit the Delivery details page and reopen it, which could potentially lead to a loop.\\n\\nLLaMA-WL (B) clicks on the correct 4:15AM tab.\\n\\nLLaMA-WL (B) correctly clicks on the Change button.\\n\\nT1: Compose a \u201cInvitation to Collaboration\u201d email.\\n\\nT2: Open Google translate and sign in using the following credentials: [email] [password]\\n\\nReference (B): Invitation to Collaboration\\n\\nReference (B): [password]\\n\\nGPT-4V (R): Leon Tales\\n\\nGPT-4V (R): [email]\\n\\nLLaMA-WL (B): Invitation to Collaboration\\n\\nLLaMA-WL (B): [password]\\n\\nS1: Instructor says \u201cLike & Bookmark this listing.\u201d\\n\\nS2: Instructor says \u201cPlease share the link.\u201d\\n\\nReference: Alright\\n\\nReference: Alright.\\n\\nLLaMA-WL: Alright\\n\\nLLaMA-WL: Okay.\\n\\nGPT-4V: Understood. If you need further assistance, just let me know.\\n\\nGPT-4V: Here is the link to the discussion: [incorrect link]\\n\\nFigure 4: Comparison of GPT-4V and LLaMA-2-13B (finetuned) on predicting click actions. Incorrectly predicted actions are in red (R), reference actions are in blue (B). We show two scenarios for click (C1, C2), textinput (T1, T2) and say (S1, S2).\\n\\nunhelpful responses by sharing irrelevant links (S2) and refused to assist the instructor even when it is possible. Even though LLaMA-2 is finetuned, it missed certain follow-up questions (such as asking \u201cWho should receive this?\u201d when asked to write an email).\\n\\n7 Discussion\\n\\n7.1 Experimental Findings\\n\\nThrough our experiments (Section 5), we find that larger multimodal models can surpass smaller image-only models when finetuned, but they are still behind finetuned text-only models. We also find that employing an DMR-based representation leads to better performance (\u00a76.1). When evaluated on out-of-domain splits, the performance of text-only decoders are very close to smaller variant; nonetheless, zero-shot models are consistently surpassed by their finetuned counterparts. We confirm, through qualitative assessments (\u00a76.2), that even the best zero-shot models can make simple and unjustified errors. Our findings highlight the need to build models that can better generalize to unseen scenarios if we want to build agents that will work in the real world.\\n\\n7.2 Limitations\\n\\nOur benchmark contains only static demonstrations, which means we cannot meaningfully evaluate the behavior of models on alternative trajectories. However, this approach lets us train models on a diverse set of real websites that do not need to be recreated from scratch.\\n\\n8\"}"}
{"id": "mUSPhG4uDW", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nGeneralizability\\nThere are inherent limitations of the architectures we evaluate. For example, we cannot expect a text-only model to draw on a canvas or describe images. Such limitations can be addressed through multimodal-specific technical contributions in future works.\\n\\n8 Conclusion\\nWe introduced WEBLINX, a large-scale expert-built benchmark covering a wide range of demonstrations for conversational web navigation on real-world websites. The framework we built around the benchmark includes the task definition, data representation, and evaluation metrics. We also introduced a dense markup ranker (DMR) to effectively summarize webpages. We evaluated finetuned and zero-shot models with various modalities, and found that chat-based decoder models finetuned on WEBLINX achieve the best results, but still struggle to generalize to out-of-domain splits. We believe that multi-turn dialogue can enhance flexibility and steerability of agents for web navigation, leading to their wider adoption.\\n\\nTo overcome these model limitations, we suggest the following future directions:\\n\\n\u2022 Designing multimodal architectures that can efficiently process visual input with structured information.\\n\u2022 Evaluating models in environments covering wider ranges of scenarios, including complex websites, advanced browser events.\\n\u2022 Expand to tasks beyond the browser, such as OS-level interactions (Xie et al., 2024).\\n\u2022 Leveraging reward-based methods like RLHF (Christiano et al., 2017) and DPO (Rafailov et al., 2023).\\n\u2022 Leveraging alternative training approaches such as self-experience and grounded synthesis (Gur et al., 2024).\\n\\nAcknowledgments\\nXHL acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) [funding reference no. 579403]. ZK is supported by the European Union (ERC, NG-NLG, 101039303) and Charles University project SVV 260 698. SR is supported by a Facebook CIFAR AI Chair and NSERC Discovery Grant program. The project is supported partially by the Google-Mila grant. We thank Esycommerce for providing their data annotation services and actively working with us in order to reach a consistent and high quality data collection process. We thank Benno Krojer, Chris Pal, Dilek Hakkani-T\u00fcr, Gokhan Tur, Ismail Haritaoglu, Nicolas Chapados, Ond\u0159ej Du\u0161ek, Peter Shaw, Sai Rajeswar, Vaibhav Adlakha, the UI Assist team at ServiceNow Research, and the McGill NLP group members for helpful discussions.\\n\\nImpact Statement\\nWeb navigation agents have the potential to become a powerful technology with large societal impacts. Therefore, multiple aspects need to be taken into consideration when conducting further research in this area:\\n\\nAutomating vs. Elevating Users\\nA major risk of fully automating web navigation is the automation of work traditionally performed by knowledge workers; deploying highly capable models could lead to job losses. However, one major difference between autonomous navigation and our framework is that we require the inclusion of a human instructor to provide the real-time instructions needed to complete the task. Thus, conversational web navigation's ultimate purpose is not to automate what a user does, but automate difficult, repetitive, and error-prone steps so that the user can focus on reliably solving high-level problems.\\n\\nMalicious Usage and Mitigation\\nAs web navigation models become increasingly sophisticated, there are risks that they will be used for malicious purposes at scale. These models can automate harmful activities, e.g., for creating spam messages and impersonating individuals for fraudulent purposes. While these activities can already be partially automated using open-source tools, web navigation agents could make automation easier and more robust. However, malicious actors can build such models in private using existing commercial services, independent of on-going research on agents. On the other hand, by making our models and data accessible to researchers, our work can be used to research ways to mitigate the risk of malicious usage; for instance, by incorporating our models as part of red teaming procedures. The resulting research can be used to build systems that are robust against malicious agents.\\n\\nUnintended Actions\\nNavigation agents can cause harm if they misinterpret instructions and perform unintended actions; for instance, booking the wrong flight could result in significant financial loss. For this reason, we assert that conversational web navigation models should be used under human supervision (where multi-turn dialogue cannot be disabled), and that it should only be deployed after exhaustive testing with proper safeguards. Our models should not be deployed and should only be used for research.\\n\\nData Collection\\nTo build WEBLINX, we worked with expert annotators, who received training, familiarized with the task and the purpose of the project, and were paid fair wage relative to their country of employment. The websites in our dataset are publicly accessible and safe. Any account appearing in the dataset was specifically created for the data collection; there are no references to their identity to preserve their privacy.\\n\\nFor example, Selenium: https://www.selenium.dev/\"}"}
{"id": "mUSPhG4uDW", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nReferences\\n\\nAdept. Adept ACT-1 \u2013 \u201cA machine learning model that can interact with everything on your computer.\u201d https://www.adept.ai/blog/act-1, 2023. Accessed: 2023/08/31.\\n\\nAdlakha, V., Dhuliawala, S., Suleman, K., de Vries, H., and Reddy, S. TopiOCQA: Open-domain Conversational Question Answering with Topic Switching. Trans. Assoc. Comput. Linguistics, 10:468\u2013483, 2022. URL https://doi.org/10.1162/tacl00471.\\n\\nAghajanyan, A., Okhonko, D., Lewis, M., Joshi, M., Xu, H., Ghosh, G., and Zettlemoyer, L. HTLM: Hyper-text Pre-training and Prompting of Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. URL https://openreview.net/forum?id=P-pPW1nxf1r.\\n\\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J. L., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. Flamingo: a Visual Language Model for Few-shot Learning. In NeurIPS, 2022. URL http://papers.nips.cc/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html.\\n\\nAssouel, R., Marty, T., Caccia, M., Laradji, I., Drouin, A., Rajeswar, S., Palacios, H., Cappart, Q., Vazquez, D., Chapados, N., Gasse, M., and Lacoste, A. The unsolved challenges of LLMs in open-ended web tasks: A case study. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. URL https://openreview.net/forum?id=jt3il4fC5B.\\n\\nBavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena, A., Somani, A., and Tasl\u0131lar, S. Fuyu-8B: A Multimodal Architecture for AI Agents, October 2023. URL https://www.adept.ai/blog/fuyu-8b/.\\n\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language Models are Few-shot Learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract-Conference.html.\\n\\nBunian, S., Li, K., Jemmali, C., Harteveld, C., Fu, Y., and El-Nasr, M. S. VINS: Visual Search for Mobile User Interface Design. In CHI '21: CHI Conference on Human Factors in Computing Systems, Virtual Event / Yokohama, Japan, May 8-13, 2021, pp. 423:1\u2013423:14, 2021. URL https://doi.org/10.1145/3411764.3445762.\\n\\nBurns, A., Arsan, D., Agrawal, S., Kumar, R., Saenko, K., and Plummer, B. A. A Dataset for Interactive Vision-language Navigation with Unknown Command Feasibility. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII, volume 13668 of Lecture Notes in Computer Science, pp. 312\u2013328, 2022. URL https://doi.org/10.1007/978-3-031-20074-8_18.\\n\\nCarroll, J. M. and Rosson, M. B. Usability Engineering. In Topi, H. and Tucker, A. (eds.), Computing Handbook, Third Edition: Information Systems and Information Technology, pp. 32:1\u201322. CRC Press, 2014.\\n\\nChang, C., Kayed, M., Girgis, M. R., and Shaalan, K. F. A Survey of Web Information Extraction Systems. IEEE Trans. Knowl. Data Eng., 18(10):1411\u20131428, 2006. URL https://doi.org/10.1109/TKDE.2006.152.\\n\\nChen, H., Liu, X., Yin, D., and Tang, J. A Survey on Dialogue Systems: Recent Advances and New Frontiers. SIGKDD Explor., 19(2):25\u201335, 2017. URL https://doi.org/10.1145/3166054.3166058.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarbian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating Large Language Models Trained on Code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.\"}"}
{"id": "mUSPhG4uDW", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "mUSPhG4uDW", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Turn-level stats by intent.\\n\\n| Intent | Demos | \u00b5 turns | \u03c3 turns | Total  |\\n|--------|-------|---------|---------|--------|\\n| say    | 2337  | 16.82   | 5.62    | 39305  |\\n| click  | 2333  | 14.52   | 10.16   | 33865  |\\n| load   | 2324  | 1.59    | 1.07    | 3702   |\\n| copy   | 1587  | 4.08    | 3.05    | 6477   |\\n| textInput | 1465 | 3.28    | 3.06    | 4799   |\\n| paste  | 1130  | 1.89    | 1.95    | 2141   |\\n| scroll | 1046  | 3.82    | 3.00    | 3999   |\\n| tabswitch | 800  | 3.28    | 3.65    | 2621   |\\n| tabcreate | 712  | 1.71    | 1.12    | 1220   |\\n| submit | 645   | 1.40    | 1.11    | 904    |\\n| hover  | 361   | 1.55    | 1.11    | 560    |\\n| tabremove | 309  | 1.94    | 1.17    | 599    |\\n| change | 165   | 1.95    | 1.34    | 322    |\\n\\nTable 8: Turn-level stats by split. Active turns are used for either finetuning or evaluation. Total includes turns used in history.\\n\\n| Split  | Demos | \u00b5 turns | \u03c3 turns | Active | Total  |\\n|--------|-------|---------|---------|--------|--------|\\n| TRAIN  | 969   | 44.93   | 17.37   | 24418  | 43538  |\\n| VALID  | 100   | 40.76   | 14.51   | 1717   | 4076   |\\n| TEST   | 100   | 43.18   | 16.08   | 1846   | 4318   |\\n| IID    | 223   | 45.30   | 25.43   | 4979   | 10102  |\\n| CAT    | 211   | 40.47   | 18.17   | 4184   | 8540   |\\n| WEB    | 444   | 36.05   | 20.09   | 7725   | 16006  |\\n| VIS    | 290   | 48.05   | 18.66   | 6141   | 13934  |\\n\\nTable 9: Turn-level stats by use of AI tools (e.g., ChatGPT).\\n\\n| Uses AI | Demos | \u00b5 turns | \u03c3 turns | Total |\\n|---------|-------|---------|---------|-------|\\n| \u2717       | 2057  | 42.50   | 19.5    | 87414 |\\n| \u2713       | 280   | 46.79   | 16.9    | 13100 |\\n\\nA.2 Categories and Subcategories\\n\\nIn Section 3, discuss the use of categories to classify demonstrations. We have in total 8 categories, each with their own subcategories, which add up to a total of 50 (*\u00a711*). We assign one category and subcategory to each of the 155 URL sub-domain associated with a demo turn (*\u00a712*). Since a demo may leverage multiple websites (e.g., composing and information lookup), a demo will have one or more subcategory. We give the full list of categories, subcategories, and the number of demonstrations associated with each in Table 11.\\n\\nIn Table 10, we show the breakdown of subcategories for the TEST split (designed to test generalization to new subcategories). We note that the subcategories were automatically chosen to be the ones with the fewer occurrences across demos, allowing to have a reasonable split size.\\n\\nTable 10: List of subcategories based on splits.\\n\\n| TEST CAT | Spreadsheet, Handmade, Reviews, Computer Vision, Chatbot, Transport, Presentation, Furniture, Professional Network, Books, Tasks, Automatic Translation, Question Answering, Encyclopedia, Recipe, Geography |\\n|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| Others   | Stay, Stays, Transport, Scientific Articles, Online Shopping, Tasks, Blog, Discussion Platform, Recipe, Spreadsheet, Email, Research Directory, Music Sharing, Chatbot, Presentation, Grocery, Delivery, Image Sharing, Automatic Translation, Video Sharing, Encyclopedia, News Articles, Forum, Entertainment, Magazine, Medical, Furniture, Educational, Kanban, Social Network, Image Generation, Question Answering, Media, Note taking, Agency, Government, Social Event, Cooking, Instant Messaging, Finance, Books, Clothing, Restaurant, Calendar, Writing Assistant |\\n| Difference | Handmade, Reviews, Computer Vision, Professional Network, Geography |\\n\\nA.3 Input Processing Details\\n\\nIn Section 3.1, we introduce the components of a state $s_t$. More formally, we define the input of a model $m$ to be $P_m(s_t, a_{1:t-1})$, consisting of a processing function $P_m$ that receives $s_t$ and $a_{1:t-1}$ and returns a representation that can serve as an input to a model. We provide details of our method below.\\n\\nAdapting $P$ per model\\n\\nFor each model $m$, we tailor the function $P_m$ to accommodate for differences in methodology. For image-to-text models, we sequentially render $v_t, u_r, a_r$ as header text of the screenshot $i_t$ (viewport $v_t$ is included so models can locate bounding boxes of $c_t$). For text-only models, we provide $d_t, v_t, u_r, c_t, a_r$, which are formatted with prompt $p_m$. In multimodal settings, we include $i_t$ in addition to the formatted prompt. Templates and samples can be found. \\n\\n\"}"}
{"id": "mUSPhG4uDW", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category       | Subcategory                    | Total | Train | Valid | ID | Vis | Geo | Cat | Web | # URLs |\\n|----------------|-------------------------------|-------|-------|-------|----|-----|-----|-----|-----|--------|\\n| AI Tools       | Auto. Translation             | 53    | 0     | 0     | 0  | 10  | 0   | 43  | 0   | 4      |\\n|                | Chatbot                       | 408   | 178   | 19    | 21 | 82  | 42  | 31  | 35  | 3      |\\n| Computer Vision|                               | 13    | 0     | 0     | 0  | 0   | 13  | 0   | 1    |\\n| Image Generation|                             | 59    | 33    | 7     | 3  | 5   | 0   | 0   | 11  | 4      |\\n| Writing Assistant|                           | 70    | 44    | 3     | 2  | 11  | 0   | 0   | 10  | 5      |\\n| Booking        | Medical Booking               | 34    | 0     | 0     | 0  | 9   | 25  | 0   | 0   | 3      |\\n|                | Restaurant                    | 77    | 28    | 6     | 5  | 14  | 24  | 0   | 0   | 6      |\\n|                | Social Event                  | 14    | 0     | 0     | 0  | 0   | 14  | 0   | 0   | 3      |\\n|                | Stay                          | 64    | 44    | 0     | 0  | 5   | 15  | 0   | 0   | 7      |\\n|                | Stays                         | 37    | 24    | 0     | 0  | 11  | 0   | 0   | 2   | 3      |\\n| Transport      |                               | 757   | 314   | 27    | 31 | 252 | 36  | 61  | 36  | 8      |\\n| Composing      | Blog                          | 62    | 34    | 2     | 3  | 15  | 0   | 0   | 8   | 4      |\\n|                | Email                         | 135   | 86    | 10    | 17 | 16  | 0   | 0   | 6   | 6      |\\n|                | Note taking                   | 47    | 31    | 0     | 5  | 11  | 0   | 0   | 0   | 4      |\\n|                | Recipe                        | 20    | 0     | 0     | 0  | 3   | 0   | 17  | 0   | 1      |\\n|                | Tasks                         | 31    | 0     | 0     | 0  | 10  | 0   | 21  | 0   | 2      |\\n| Information    | Lookup                        | 46    | 29    | 2     | 3  | 0   | 0   | 12  | 3    |\\n|                | Agency                        | 56    | 28    | 3     | 2  | 8   | 0   | 15  | 2    |\\n|                | Educational                   | 97    | 56    | 8     | 7  | 11  | 0   | 1  | 14  | 4      |\\n|                | Encyclopedia                   | 36    | 13    | 0     | 0  | 0   | 0   | 13  | 2    |\\n|                | Entertainment                 | 37    | 12    | 4     | 1  | 9   | 0   | 0   | 11  | 2      |\\n| Geography      |                               | 13    | 0     | 0     | 0  | 0   | 13  | 0   | 1    |\\n| Government     |                               | 36    | 0     | 0     | 0  | 9   | 27  | 0   | 0   | 2      |\\n| Media          |                               | 60    | 23    | 2     | 3  | 10  | 0   | 0   | 22  | 2      |\\n| Research       | Directory                     | 10    | 0     | 0     | 0  | 10  | 0   | 0   | 0   | 2      |\\n| Productivity   | Calendar                      | 50    | 17    | 3     | 2  | 11  | 3   | 0   | 14  | 2      |\\n| Finance        |                               | 59    | 21    | 0     | 0  | 10  | 28  | 0   | 0   | 4      |\\n| Kanban         |                               | 50    | 20    | 2     | 3  | 16  | 0   | 0   | 9   | 3      |\\n| Presentation   |                               | 32    | 0     | 0     | 0  | 6   | 0   | 26  | 0   | 1      |\\n| Spreadsheet    |                               | 27    | 0     | 0     | 0  | 10  | 0   | 17  | 0   | 2      |\\n| Shopping       | Clothing                      | 93    | 18    | 6     | 4  | 8   | 57  | 0   | 0   | 6      |\\n|                | Delivery                      | 91    | 67    | 4     | 6  | 14  | 0   | 0   | 0   | 7      |\\n|                | Furniture                     | 6     | 0     | 0     | 0  | 5   | 0   | 1  | 0   | 1      |\\n|                | Grocery                       | 38    | 0     | 0     | 0  | 8   | 30  | 0   | 0   | 2      |\\n|                | Handmade                      | 15    | 0     | 0     | 0  | 0   | 0   | 15  | 0   | 1      |\\n| Online Shopping|                               | 87    | 51    | 3     | 2  | 31  | 0   | 0   | 0   | 7      |\\n| Social Interaction|                           | 32    | 18    | 4     | 1  | 9   | 0   | 0   | 0   | 3      |\\n|                | Image Sharing                 | 60    | 30    | 6     | 9  | 0   | 0   | 15  | 4    |\\n|                | Instant Messaging              | 32    | 11    | 0     | 0  | 11  | 0   | 0   | 10  | 2      |\\n|                | Music Sharing                 | 36    | 14    | 0     | 0  | 9   | 0   | 0   | 13  | 2      |\\n|                | Professional Network          | 14    | 0     | 0     | 0  | 0   | 0   | 14  | 0   | 1      |\\n|                | Question Answering            | 20    | 0     | 0     | 0  | 5   | 0   | 15  | 0   | 1      |\\n|                | Social Network                | 62    | 28    | 4     | 2  | 13  | 14  | 0   | 1   | 4      |\\n|                | Video Sharing                 | 20    | 10    | 0     | 0  | 1  | 0   | 0   | 9   | 1      |\\n| Summarizing    | Books                         | 25    | 0     | 0     | 0  | 10  | 0   | 15  | 0   | 2      |\\n| Cooking        |                               | 40    | 13    | 0     | 0  | 11  | 16  | 0   | 0   | 2      |\\n| Magazine       |                               | 49    | 24    | 0     | 1  | 11  | 13  | 0   | 0   | 4      |\\n| News Articles  |                               | 124   | 75    | 11    | 11 | 15  | 12  | 0   | 0   | 5      |\\n| Reviews        |                               | 13    | 0     | 0     | 0  | 0   | 0   | 13  | 0   | 1      |\\n| Scientific     | Articles                      | 35    | 10    | 4     | 2  | 10  | 0   | 0   | 9   | 2      |\"}"}
{"id": "mUSPhG4uDW", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: The data collection process. We record interactions between an instructor and a human navigator, including chat and browser actions. *Instructor can see the screen except in TEST VIS split.\\n\\nCandidate selection\\nFollowing Deng et al. (2023), we employ a separate candidate selection stage in order to reduce the number of the input elements to interact with. In the candidate selection stage, a ranking model selects a subset of $k$ relevant elements from the DOM tree, which is then presented to the model in a multi-choice setup; in Section 5.1, we describe a novel approach towards candidate selection designed for real-time use cases. When the candidate is selected, $c$ is returned to be used in $P$. Each candidate contains a tag, XPath, bounding box, attributes and children tags, which are delimited with square brackets (e.g., $[\\\\text{tag}]$ $...$ $[\\\\text{xpath}]$ $...$). Examples of candidates used inside prompts can be found in Appendix B.8.\\n\\nRestricting history for input\\nTo accommodate the maximum input length a model can receive, we can restrict $a_{t-1}$ and $u_{t-1}$ to select a subset window of $w$. For actions, we select the last $w$ instances by either the instructor or navigator. For instructor utterances, we only select the first and last $w-1$ instances, allowing us to keep track of the initial request while focusing on the latest updates to the instruction. For simplicity, we denote the restricted set of actions as $a_r$ and utterances as $u_r$. Similar to Deng et al. (2023), we choose $w=5$, allowing the model to attend recent actions without going over context limits.\\n\\nA.4 Output Processing Details\\nAlthough the model is finetuned to generate a string in the format described in Section 3, the raw output is not consistently suitable for direct execution, and may contain unnecessary artifacts. We process the output by using Regex pattern matching to find the first suitable intent call, then parse the $\\\\alpha$ into key/value pairs, which can be compared with the ground truth actions.\\n\\nMapping coordinates to elements\\nVision models without access to candidate elements will instead be instructed and finetuned to choose an element by specifying its $(x, y)$ coordinates. If there are overlapping elements at a coordinate, we choose the element with the smallest area at the given $(x, y)$ coordinates (which should be the target of the interaction due to the properties of the CSS box model). Technically, the click targets the element with the highest $z$-index (the depth axis in HTML), but since we do not have access to CSS properties of the object, we rely on the default render order.\\n\\nSegmenting URLs for load actions\\nWe use $\\\\text{urllib}$ to first segment the URL into a network location ($\\\\text{netloc}$) and the remaining hierarchical path ($\\\\text{path}$). To normalize the $\\\\text{netloc}$, we remove the leading www from it. Since a $\\\\text{path}$ is separated by a forward slash (/), we use this character to separate each segment in the path. The final result is a list of tokens, each representing a part of the initial URL.\"}"}
{"id": "mUSPhG4uDW", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEBLINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nA.5 Data Collection Details\\n\\nIn Table 3, we provide an overview of the data collection process to build the dataset component of WEBLINX. The overview of the process is outlined in Figure 6. In this section, we dive into the technical and supplementary details of the process.\\n\\nWebsite Selection\\n\\nWe assembled the list of recommended websites to be used as starting points, but the annotators were allowed to visit any websites they deemed appropriate for the task (full list available in Section A.7). The annotators were given the time to become acquainted with the specific websites before recording the demonstrations. We encouraged the annotators to record both shorter, single-task demonstrations, and more complex demonstrations consisting of multiple sub-tasks. The demonstration ends once the instructor notifies the navigator that they wish to terminate the demonstration.\\n\\nRecording Demonstrations\\n\\nTo capture the states and actions during the demonstration, we implemented a custom Chrome browser extension. For each action in the browser, the extension captured the screenshot of the page, the DOM tree of the page, and bounding boxes of the elements in the viewport. The user actions were captured using web event handlers, and the Chrome tabCapture API was used to save the state of the page for each action in the background. For screen recording, screen sharing, and chat interface, the annotators used Zoom, a free video meeting software. We combined the chat with the browser states and actions in the postprocessing stage. Finally, the annotators validate demonstrations to ensure there are no unnecessary or incorrectly ordered actions, and that there are no typographic errors.\\n\\nCurating Demonstrations\\n\\nThe annotators uploaded the recorded demonstration into our custom web interface to perform basic quality checks. Using the review mode, the annotators then removed unnecessary actions (such as hovering over elements not necessary for completing the task), corrected the order of actions (which was occasionally incorrect due to asynchronous processing), and fixed typographical errors. We also improved the alignment between screenshots and actions by re-aligning the screenshots based on their similarity to the respective video frames. Moreover, it is possible that an action is performed before the DOM tree is fully rendered on screen. When the screen presents sufficient information for an action to be taken, then it is marked as valid during the validation process. However, if the screenshot does not provide enough information, then they are marked as invalid.\\n\\nAnnotator Pay\\n\\nWe paid US$7.5 per hour for the demonstration recording and US$5 per hour for overhead (preparation, upload, and quality review), leading to an average US$2.58 per demonstration. The rate is substantially higher than the minimum wage in the region where the data is collected, but also includes other overhead fees.\\n\\nA.6 Actions and Intents\\n\\nThe action \\\\( a \\\\) has a structure \\\\( \\\\text{intent}(\\\\alpha_1, \\\\ldots, \\\\alpha_m) \\\\), where our core intents are: click, load (new page via URL), say (navigator's utterance), submit (e.g., a form), textinput (e.g., typing text in the search bar); we show examples of these actions in Figures 1 and 4. The set of arguments \\\\( \\\\alpha \\\\) will be different from each action. Commonly used arguments are the unique ID of an element in \\\\( d \\\\) and the text argument for say or textinput. To complement the intents described in Section 3, we show a diagram of possible arguments for each intent is provided in Figure 5, with the full list shown in Table 6.\\n\\nEvaluating intents\\n\\nAmong the 13 recorded intent types, we focus on evaluating 5 types: click, load, say, submit, textinput. We also use change and scroll as prediction targets during finetuning as they are necessary to complete a demonstration. However, we do not evaluate them as change does not appear in every split (see Table 7) and scroll cannot be reliably evaluated. The other intents (copy, paste, tabswitch, tabcreate, hover, tabremove) are included in the history and the associated states are available alongside active intents; copy, paste, and hover do not affect the state of the website, whereas the tab actions are not mandatory to navigate a website, as load is sufficient to go to any website.\"}"}
{"id": "mUSPhG4uDW", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"**Table 24: Full TEST split (test) results.** We abbreviate submit to sbmt and textinput to input. The first section contains zero-shot results and the second contains finetuned results.\\n\\n| Model                  | IoU  | IoU  | IoU  | chrF | chrF | F1  | IM  | IM  | IM  | IM  | IM  |\\n|------------------------|------|------|------|------|------|-----|-----|-----|-----|-----|-----|\\n| Llama-2-7B             | 5.45 | 7.58 | 0.57 | 4.06 | 0.57 | 17.15 | 47.89 | 22.94 | 39.02 | 15.15 | 2.28 |\\n| Llama-2-13B            | 10.55| 1.00 | 0.85 | 1.48 | 1.01 | 18.97 | 77.63 | 22.02 | 17.62 | 3.03 | 1.71 |\\n| GPT-3.5T               | 11.92| 0.03 | 25.20| 1.43 | 15.67| 21.57 | 75.24 | 24.77 | 10.14 | 3.03 | 33.90|\\n| GPT-4T                 | 11.23| 0.00 | 48.03| 3.08 | 35.26| 15.61 | 61.01 | 23.85 | 14.97 | 0.00 | 67.52|\\n| GPT-4V                 | 11.42| 1.52 | 46.01| 2.61 | 33.18| 14.07 | 65.42 | 17.43 | 12.87 | 1.52 | 65.53|\\n| MindAct-250M           | 15.17| 0.00 | 0.28 | 12.72| 0.55 | 0.00 | 83.57 | 0.00  | 100.00| 0.00 | 0.57 |\\n| MindAct-780M           | 13.62| 0.00 | 27.16| 19.78| 19.03| 9.17 | 78.36 | 9.17  | 100.00| 0.00 | 32.76|\\n| MindAct-3B             | 16.62| 27.27| 37.34| 36.57| 24.98| 11.17| 93.53 | 15.60 | 99.93 | 31.82 | 40.17|\\n| Flan-T5-250M           | 18.37| 0.00 | 0.00 | 14.82| 0.16 | 0.00 | 99.39 | 0.00  | 100.00| 0.00 | 0.85 |\\n| Flan-T5-780M           | 17.90| 0.00 | 6.01 | 20.71| 3.01 | 0.00 | 99.13 | 0.00  | 100.00| 0.00 | 7.98 |\\n| Flan-T5-3B             | 18.35| 34.85| 46.44| 38.70| 31.32| 13.46| 91.52 | 19.27 | 99.93 | 39.39 | 51.00|\\n| Pix2Act-282M           | 9.33 | 0.00 | 0.00 | 28.00| 3.16 | 15.52| 98.36 | 19.27 | 100.00| 0.00 | 3.70 |\\n| Pix2Act-1.3B           | 11.80| 0.00 | 0.00 | 37.21| 21.83| 15.32| 97.60 | 20.18 | 100.00| 10.61 | 30.48|\\n| Fuyu-8B                | 15.27| 42.52| 28.50| 34.15| 22.85| 14.80| 94.90 | 35.78 | 96.50 | 48.48 | 32.48|\\n| S-LLaMA-1.3B           | 18.44| 34.85| 41.57| 38.23| 30.14| 19.95| 95.97 | 38.53 | 99.86 | 43.94 | 45.87|\\n| S-LLaMA-2.7B           | 20.45| 39.48| 51.44| 37.96| 32.84| 19.14| 95.70 | 33.03 | 99.86 | 42.42 | 56.41|\\n| Llama-2-7B             | 18.58| 42.44| 57.08| 37.76| 36.61| 27.01| 90.26 | 46.79 | 100.00| 53.03 | 61.54|\\n| Llama-2-13B            | 18.12| 51.53| 57.11| 37.00| 35.05| 31.71| 84.98 | 47.71 | 100.00| 57.58 | 61.25|\\n| GPT-3.5F               | 15.97| 43.94| 47.21| 29.79| 30.27| 21.26| 85.89 | 32.11 | 91.96 | 45.45 | 55.27|\\n\\n**Table 25: Full TEST GEO split (test) results.** We abbreviate submit to sbmt and textinput to input. The first section contains zero-shot results and the second contains finetuned results.\\n\\n| Model                  | IoU  | IoU  | IoU  | chrF | chrF | F1  | IM  | IM  | IM  | IM  | IM  |\\n|------------------------|------|------|------|------|------|-----|-----|-----|-----|-----|-----|\\n| Llama-2-7B             | 4.21 | 4.00 | 2.54 | 4.45 | 1.58 | 14.04| 43.35| 17.11| 34.58| 7.00 | 11.61|\\n| Llama-2-13B            | 7.21 | 2.00 | 2.27 | 1.25 | 1.50 | 10.62| 77.83| 13.16| 12.93| 2.00 | 8.75 |\\n| GPT-3.5T               | 14.58| 4.00 | 14.98| 1.90 | 15.22| 19.97| 73.55| 25.00| 10.24| 5.00 | 33.75|\\n| GPT-4T                 | 13.20| 4.00 | 36.16| 5.78 | 26.16| 25.32| 57.30| 30.26| 21.43| 6.00 | 69.11|\\n| GPT-4V                 | 14.56| 5.00 | 36.09| 4.07 | 26.50| 17.86| 62.16| 21.05| 16.00| 7.00 | 65.00|\\n| MindAct-250M           | 16.58| 0.00 | 0.54 | 18.08| 0.01 | 0.00 | 86.32| 0.00  | 100.00| 0.00 | 0.71 |\\n| MindAct-780M           | 14.74| 0.00 | 19.29| 30.93| 10.39| 7.89 | 90.73| 7.89  | 100.00| 0.00 | 23.04|\\n| MindAct-3B             | 15.68| 7.00 | 30.40| 41.64| 17.88| 14.05| 91.04| 23.68 | 99.94| 8.00 | 34.64|\\n| Flan-T5-250M           | 20.41| 0.00 | 0.00 | 20.10| 0.00 | 0.00 | 99.86| 0.00  | 100.00| 0.00 | 0.00 |\\n| Flan-T5-780M           | 19.77| 0.00 | 2.37 | 32.25| 1.67 | 0.00 | 98.91| 0.00  | 100.00| 1.00 | 4.29 |\\n| Flan-T5-3B             | 17.92| 25.77| 41.82| 42.03| 27.16| 13.17| 90.32| 25.00 | 99.94| 33.00 | 49.29|\\n| Pix2Act-282M           | 9.05 | 0.00 | 0.00 | 31.90| 0.18 | 14.49| 99.93| 21.05 | 100.00| 0.00 | 0.36 |\\n| Pix2Act-1.3B           | 8.80 | 0.00 | 0.00 | 42.42| 20.91| 13.39| 92.82| 22.37 | 100.00| 0.00 | 29.82|\\n| Fuyu-8B                | 14.92| 22.46| 30.36| 35.50| 18.87| 9.87 | 86.83| 27.63 | 97.82 | 36.00 | 45.36|\\n| S-LLaMA-1.3B           | 18.79| 26.29| 36.46| 41.14| 22.89| 12.50| 90.56| 32.89 | 99.78 | 44.00 | 48.93|\\n| S-LLaMA-2.7B           | 18.85| 32.00| 54.14| 41.75| 31.52| 13.71| 91.11| 30.26 | 99.72 | 32.00 | 66.25|\\n| Llama-2-7B             | 17.73| 51.00| 52.21| 40.42| 32.23| 21.91| 85.63| 43.42 | 99.78 | 53.00 | 64.64|\\n| Llama-2-13B            | 19.68| 56.00| 52.98| 41.87| 33.52| 29.72| 86.45| 50.00 | 100.00| 58.00 | 61.07|\\n| GPT-3.5F               | 14.90| 45.00| 49.71| 35.34| 35.53| 21.14| 81.05| 34.21 | 94.57 | 45.00 | 59.64|\"}"}
{"id": "mUSPhG4uDW", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 26: Full TEST split (test) results\\n\\nWe abbreviate `submit` to `sbmt` and `textinput` to `input`. The first section contains zero-shot results and the second contains finetuned results.\\n\\n| Model          | IoU 1 | IoU 2 | IoU 3 | chrF 1 | chrF 2 | F1   | IM 1 | IM 2 | IM 3 | IM 4 |\\n|----------------|-------|-------|-------|--------|--------|------|------|------|------|------|\\n| Llama-2-7B     | 4.35  | 0.01  | 1.16  | 4.15   | 0.87   | 10.61| 38.04| 12.86| 33.41| 3.05  |\\n| Llama-2-13B    | 8.82  | 0.00  | 1.15  | 1.75   | 0.97   | 8.19 | 74.76| 11.43| 14.70| 1.53  |\\n| GPT-3.5T       | 14.93 | 5.34  | 16.44 | 0.53   | 14.97  | 10.48| 74.03| 15.00| 4.60 | 6.11  |\\n| GPT-4T         | 15.04 | 2.31  | 44.22 | 2.96   | 37.58  | 16.15| 60.67| 20.71| 13.84| 3.05  |\\n| GPT-4V         | 15.26 | 0.82  | 44.73 | 1.66   | 36.59  | 13.10| 65.03| 17.14| 8.90 | 2.29  |\\n| MindAct-250M   | 18.94 | 0.00  | 0.16  | 11.65  | 0.08   | 0.00 | 92.28| 0.00 | 100.00| 0.32 |\\n| MindAct-780M   | 17.57 | 0.00  | 16.33 | 15.31  | 9.80   | 4.29 | 89.95| 4.29 | 100.00| 22.47|\\n| MindAct-3B     | 19.36 | 10.74 | 29.83 | 24.41  | 13.78  | 9.64 | 93.01| 15.00| 100.00| 36.44|\\n| Flan-T5-250M   | 23.12 | 0.00  | 0.00  | 11.70  | 0.00   | 0.00 | 99.89| 0.00 | 100.00| 0.00 |\\n| Flan-T5-780M   | 22.77 | 0.00  | 6.18  | 17.04  | 4.19   | 0.00 | 98.84| 0.00 | 100.00| 11.08|\\n| Flan-T5-3B     | 22.86 | 30.76 | 42.78 | 26.32  | 29.75  | 11.43| 93.92| 15.71| 99.96 | 51.52|\\n| Pix2Act-282M   | 6.86  | 0.00  | 0.00  | 16.60  | 0.32   | 6.43 | 99.68| 10.00| 100.00| 2.27 |\\n| Pix2Act-1.3B   | 12.31 | 0.00  | 0.00  | 25.93  | 15.44  | 16.09| 96.29| 32.86| 100.00| 25.52|\\n| Fuyu-8B        | 17.56 | 22.78 | 27.10 | 23.43  | 18.64  | 20.12| 93.20| 37.86| 93.64 | 36.12|\\n| S-LLaMA-1.3B   | 23.65 | 24.79 | 40.19 | 26.03  | 20.76  | 23.71| 96.18| 47.86| 99.85 | 51.52|\\n| S-LLaMA-2.7B   | 24.14 | 35.88 | 52.27 | 26.26  | 30.36  | 21.55| 95.62| 37.14| 99.81 | 59.87|\\n| Llama-2-7B     | 23.23 | 40.46 | 58.71 | 26.72  | 36.19  | 34.74| 90.65| 56.43| 99.51 | 68.86|\\n| Llama-2-13B    | 23.03 | 40.46 | 57.31 | 27.87  | 35.02  | 33.63| 88.98| 50.71| 99.89 | 64.04|\\n| GPT-3.5F       | 17.97 | 43.51 | 50.27 | 22.99  | 32.31  | 29.91| 84.09| 39.29| 91.32 | 59.39|\\n\\n### Table 27: Full TEST split (test) results\\n\\nWe abbreviate `submit` to `sbmt` and `textinput` to `input`. The first section contains zero-shot results and the second contains finetuned results.\\n\\n| Model          | IoU 1 | IoU 2 | IoU 3 | chrF 1 | chrF 2 | F1   | IM 1 | IM 2 | IM 3 | IM 4 |\\n|----------------|-------|-------|-------|--------|--------|------|------|------|------|------|\\n| Llama-2-7B     | 5.33  | 0.47  | 2.78  | 4.27   | 0.41   | 22.38| 42.92| 24.05| 35.12| 5.26  |\\n| Llama-2-13B    | 8.98  | 0.00  | 1.77  | 1.36   | 0.76   | 16.47| 73.48| 17.72| 16.04| 0.00  |\\n| GPT-3.5T       | 12.41 | 1.71  | 21.46 | 1.88   | 15.21  | 33.00| 72.46| 34.18| 11.43| 7.89  |\\n| GPT-4T         | 12.75 | 2.97  | 45.83 | 5.64   | 36.43  | 38.15| 61.56| 41.77| 18.83| 10.53 |\\n| GPT-4V         | 13.03 | 1.71  | 45.12 | 4.19   | 36.06  | 30.47| 65.26| 32.91| 15.21| 7.89  |\\n| MindAct-250M   | 16.79 | 0.00  | 1.01  | 14.68  | 0.26   | 0.00 | 92.90| 0.00 | 99.92 | 1.01 |\\n| MindAct-780M   | 15.09 | 0.00  | 21.86 | 19.99  | 14.69  | 10.13| 91.44| 10.13| 100.00| 23.23|\\n| MindAct-3B     | 16.50 | 0.00  | 35.63 | 38.93  | 19.97  | 17.72| 92.99| 22.78| 100.00| 38.64|\\n| Flan-T5-250M   | 21.79 | 0.00  | 0.00  | 15.64  | 0.00   | 0.00 | 99.95| 0.00 | 100.00| 0.00 |\\n| Flan-T5-780M   | 21.78 | 0.00  | 10.55 | 21.60  | 6.36   | 0.00 | 98.54| 0.00 | 100.00| 13.89|\\n| Flan-T5-3B     | 20.48 | 19.84 | 49.79 | 39.59  | 33.80  | 18.57| 92.51| 24.05| 100.00| 55.05|\\n| Pix2Act-282M   | 9.57  | 0.00  | 0.00  | 30.30  | 2.27   | 14.77| 97.71| 25.32| 100.00| 2.27 |\\n| Pix2Act-1.3B   | 13.24 | 0.00  | 0.00  | 39.57  | 23.63  | 19.20| 95.33| 31.65| 100.00| 32.58|\\n| Fuyu-8B        | 18.44 | 13.50 | 29.89 | 34.95  | 24.58  | 17.24| 94.26| 36.71| 96.88 | 37.63|\\n| S-LLaMA-1.3B   | 22.46 | 11.58 | 46.89 | 38.14  | 28.92  | 26.78| 95.04| 37.97| 99.92 | 53.03|\\n| S-LLaMA-2.7B   | 22.61 | 21.05 | 51.99 | 40.77  | 33.68  | 27.62| 95.67| 40.51| 99.92 | 62.12|\\n| Llama-2-7B     | 21.11 | 36.84 | 52.23 | 37.74  | 35.99  | 35.20| 88.76| 54.43| 99.84 | 64.90|\\n| Llama-2-13B    | 22.58 | 44.74 | 52.14 | 38.03  | 35.93  | 39.13| 89.98| 58.23| 99.84 | 61.87|\\n| GPT-3.5F       | 17.91 | 42.11 | 50.68 | 33.88  | 37.47  | 41.20| 83.89| 51.90| 94.41 | 68.69|\"}"}
{"id": "mUSPhG4uDW", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 28: Element Group (EG), Text Group (TG) and overall results for TEST IID (left) and TESTOOD (right) splits. The top section contains zero-shot results and the bottom contains finetuned results.\\n\\n| Model                      | EG Micro Avg | EG IoU | EG F1  | TG Micro Avg | TG IoU | TG F1  | Overall Micro Avg | Overall IoU | Overall F1  |\\n|----------------------------|--------------|--------|--------|--------------|--------|--------|-------------------|-------------|--------------|\\n| Llama-2-7B                 | 5.32         | 33.80  | 4.01   | 3.06         | 4.04   | 33.96  | 2.92              | 2.14        |\\n| Llama-2-13B                | 5.61         | 42.85  | 5.29   | 1.97         | 5.16   | 43.68  | 4.80              | 1.31        |\\n| GPT-3.5T                   | 10.35        | 42.42  | 10.68  | 3.98         | 8.51   | 42.77  | 8.62              | 3.45        |\\n| GPT-4T                     | 12.24        | 42.69  | 12.55  | 7.85         | 10.72  | 41.66  | 10.85             | 6.75        |\\n| GPT-4V                     | 12.99        | 42.47  | 13.68  | 7.28         | 10.45  | 42.36  | 10.91             | 6.21        |\\n| MindAct-250M               | 16.88        | 76.54  | 18.01  | 8.46         | 12.63  | 74.25  | 12.05             | 7.67        |\\n| MindAct-780M               | 19.61        | 78.06  | 20.12  | 14.04        | 15.13  | 75.87  | 13.39             | 13.58       |\\n| MindAct-3B                 | 25.71        | 80.99  | 22.50  | 24.50        | 20.94  | 79.89  | 16.50             | 23.16       |\\n| Flan-T5-250M               | 20.93        | 80.28  | 23.68  | 9.51         | 14.99  | 79.69  | 14.86             | 9.21        |\\n| Flan-T5-780M               | 23.71        | 81.91  | 25.35  | 16.17        | 17.27  | 80.02  | 15.36             | 14.05       |\\n| Flan-T5-3B                 | 31.12        | 83.48  | 29.56  | 29.06        | 23.77  | 81.14  | 20.31             | 25.75       |\\n| Pix2Act-282M               | 12.30        | 80.50  | 4.86   | 17.29        | 12.51  | 79.71  | 6.20              | 16.40       |\\n| Pix2Act-1.3B               | 23.91        | 83.42  | 13.15  | 32.59        | 16.88  | 81.80  | 8.28              | 25.21       |\\n| Fuyu-8B                   | 30.92        | 84.51  | 25.73  | 33.66        | 19.97  | 80.07  | 15.70             | 22.30       |\\n| S-LLaMA-1.3B              | 33.99        | 87.81  | 32.41  | 34.68        | 23.73  | 83.32  | 20.54             | 25.85       |\\n| S-LLaMA-2.7B              | 37.43        | 87.70  | 35.54  | 37.66        | 25.02  | 84.00  | 22.60             | 27.17       |\\n| Llama-2-7B                | 38.12        | 88.08  | 36.71  | 38.58        | 24.57  | 82.64  | 22.26             | 26.50       |\\n| Llama-2-13B                | 37.09        | 87.70  | 35.92  | 37.43        | 25.21  | 81.91  | 22.82             | 26.60       |\\n| GPT-3.5F                  | 30.89        | 82.34  | 30.22  | 29.62        | 21.22  | 77.56  | 18.64             | 22.39       |\\n\\n### Table 29: Element Group (EG), Text Group (TG) and overall results for TEST CAT (left) and TEST GEO (right) splits. The top section contains zero-shot results and the bottom contains finetuned results.\\n\\n| Model                      | EG Micro Avg | EG IoU | EG F1  | TG Micro Avg | TG IoU | TG F1  | Overall Micro Avg | Overall IoU | Overall F1  |\\n|----------------------------|--------------|--------|--------|--------------|--------|--------|-------------------|-------------|--------------|\\n| Llama-2-7B                 | 4.57         | 38.32  | 3.46   | 2.18         | 3.61   | 33.48  | 2.60              | 2.11        |\\n| Llama-2-13B                | 6.50         | 47.52  | 6.03   | 1.59         | 4.03   | 43.04  | 3.87              | 1.09        |\\n| GPT-3.5T                   | 8.23         | 45.91  | 8.42   | 3.35         | 8.86   | 42.09  | 8.78              | 3.66        |\\n| GPT-4T                     | 9.48         | 42.14  | 9.90   | 5.63         | 10.61  | 40.86  | 10.53             | 6.38        |\\n| GPT-4V                     | 9.26         | 43.66  | 9.80   | 5.30         | 10.74  | 41.33  | 11.05             | 5.86        |\\n| MindAct-250M               | 11.69        | 72.93  | 11.27  | 6.61         | 13.15  | 70.25  | 11.20             | 8.93        |\\n| MindAct-780M               | 14.36        | 72.83  | 12.85  | 12.37        | 16.99  | 74.48  | 12.39             | 17.68       |\\n| MindAct-3B                 | 21.60        | 81.70  | 16.59  | 25.01        | 21.42  | 76.00  | 14.65             | 24.75       |\\n| Flan-T5-250M               | 13.96        | 81.26  | 13.61  | 8.98         | 15.56  | 76.63  | 13.70             | 11.15       |\\n| Flan-T5-780M               | 15.61        | 81.64  | 13.86  | 12.93        | 18.92  | 76.58  | 13.58             | 18.02       |\\n| Flan-T5-3B                 | 23.67        | 81.72  | 18.99  | 26.85        | 23.52  | 77.46  | 18.09             | 26.52       |\\n| Pix2Act-282M               | 13.31        | 81.34  | 6.95   | 17.95        | 13.77  | 76.96  | 6.10              | 18.20       |\\n| Pix2Act-1.3B               | 17.44        | 84.03  | 8.93   | 26.64        | 16.96  | 77.84  | 6.07              | 26.72       |\\n| Fuyu-8B                   | 20.42        | 82.29  | 15.03  | 24.47        | 19.53  | 75.87  | 14.58             | 21.45       |\\n| S-LLaMA-1.3B              | 23.76        | 84.72  | 18.82  | 28.39        | 23.48  | 78.64  | 18.19             | 25.82       |\\n| S-LLaMA-2.7B              | 25.06        | 85.08  | 21.38  | 28.39        | 24.62  | 80.04  | 20.60             | 27.50       |\\n| Llama-2-7B                | 24.57        | 83.65  | 20.86  | 27.96        | 24.38  | 78.78  | 20.54             | 26.50       |\\n| Llama-2-13B                | 24.27        | 81.00  | 20.72  | 26.12        | 25.93  | 78.62  | 21.97             | 27.67       |\\n| GPT-3.5F                  | 20.21        | 78.07  | 17.31  | 21.16        | 21.94  | 74.69  | 17.97             | 23.91       |\"}"}
{"id": "mUSPhG4uDW", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 30: Element Group (EG), Text Group (TG) and overall results for TESTVIS (left) and TESTWEB (right) splits. The top section contains zero-shot results and bottom contains finetuned results.\\n\\n| Model           | EG Micro Avg | EG IoU      | EG F1  | TG Micro Avg | TG IoU      | TG F1  | Overall Micro Avg | Overall IoU | Overall F1 |\\n|-----------------|--------------|-------------|--------|--------------|-------------|--------|-------------------|--------------|------------|\\n| Llama-2-7B      | 3.77         | 30.99       | 2.50   | 2.10         | 4.35        | 33.03  | 3.26              | 2.19         |            |\\n| Llama-2-13B     | 5.06         | 42.05       | 4.60   | 1.34         | 5.21        | 42.11  | 4.86              | 1.23         |            |\\n| GPT-3.5T        | 8.58         | 40.14       | 8.84   | 2.63         | 8.42        | 42.95  | 8.51              | 4.16         |            |\\n| GPT-4T          | 11.14        | 40.38       | 11.65  | 6.36         | 11.77       | 43.28  | 11.43             | 8.62         |            |\\n| GPT-4V          | 10.73        | 40.45       | 11.59  | 5.72         | 11.20       | 44.00  | 11.35             | 7.96         |            |\\n| MindAct-250M    | 13.16        | 79.07       | 13.97  | 7.26         | 12.54       | 74.76  | 11.75             | 7.89         |            |\\n| MindAct-780M    | 14.46        | 79.81       | 14.97  | 10.87        | 14.74       | 76.36  | 13.40             | 13.42        |            |\\n| MindAct-3B      | 19.11        | 82.98       | 18.46  | 17.81        | 21.64       | 78.90  | 16.28             | 25.07        |            |\\n| Flan-T5-250M    | 15.18        | 82.71       | 17.02  | 7.80         | 15.25       | 78.15  | 15.09             | 8.93         |            |\\n| Flan-T5-780M    | 17.09        | 83.09       | 17.53  | 11.90        | 17.48       | 78.78  | 16.49             | 13.33        |            |\\n| Flan-T5-3B      | 22.91        | 84.92       | 23.01  | 21.73        | 25.06       | 80.47  | 21.25             | 27.91        |            |\\n| Pix2Act-282M    | 9.16         | 82.81       | 5.06   | 11.39        | 13.79       | 77.75  | 6.67              | 18.07        |            |\\n| Pix2Act-1.3B    | 15.33        | 84.63       | 9.27   | 20.18        | 18.57       | 80.69  | 9.39              | 27.31        |            |\\n| Fuyu-8B         | 18.63        | 82.29       | 16.83  | 18.73        | 21.97       | 79.85  | 17.10             | 24.57        |            |\\n| S-LLaMA-1.3B    | 22.80        | 86.83       | 23.49  | 21.32        | 25.68       | 83.10  | 22.66             | 27.86        |            |\\n| S-LLaMA-2.7B    | 24.12        | 87.29       | 25.81  | 22.79        | 26.82       | 83.60  | 23.31             | 30.01        |            |\\n| Llama-2-7B      | 24.70        | 86.56       | 26.36  | 23.78        | 25.80       | 81.57  | 22.81             | 27.74        |            |\\n| Llama-2-13B     | 25.00        | 85.31       | 26.09  | 23.89        | 27.00       | 82.72  | 24.24             | 28.72        |            |\\n| GPT-3.5F        | 20.46        | 79.37       | 20.49  | 19.36        | 23.24       | 78.13  | 19.95             | 25.13        |            |\"}"}
{"id": "mUSPhG4uDW", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"One major difference between GPT-4V and LLaMA-2-13B is that the former will have a completely different writing style, whereas the latter can learn the style of the navigator during finetuning. For example, the navigators may employ acknowledging terms like \u201cAlright\u201d and \u201cSure\u201d that can be learned by LLaMA-2-13B, whereas GPT-4V tends to use \u201cUnderstood\u201d and \u201cAcknowledged\u201d. Beyond those superficial differences, we notice some patterns of failure in Table 19. First, GPT-4V might come up with unhelpful replies, such as incorrectly sharing a link to the current page when requested to share a link to a different page. In the same scenario, it might simply refuse to assist the instructor, even when the action is achievable. Finally, GPT-4V might generate an utterance that semantically differs from the reference utterance, but would be pragmatically correct. We show one example where, given a request to write an email that includes a discount, the human navigator would ask who should be the recipient, whereas GPT-4V might ask about the details of the discount; clearly, both are valid follow-up questions, but it is challenging to evaluate with existing methods. In all the aforementioned cases (except for the last one), LLaMA-2-13B will provide a short but correct response. Although it may seem less verbose, we found that they are in reality almost as verbose as GPT-4V; the models respectively have, on average, 58.29 (n=1194) and 60.41 characters (n=220) when predicting a say intent on the validation and in-domain test sets.\\n\\nC.6 Comparison with human performance\\n\\nTo understand how well a model would compare to a human annotator at selecting a plausible trajectory, we recruited 3 annotators to predict the best action to take at a given turn in a subset of the demonstrations in the validation set. Then, we compute the agreement score for a given turn as:\\n\\n$$\\\\text{Agreement}(a_p, A) = \\\\max_{a_r \\\\in A} \\\\text{M}(a_p, a_r)$$\\n\\nwhere $M$ is the selected metric for a given turn (see Section 4.1), $a_r$ is the reference annotation and $a_p$ is the prediction by the annotator we are evaluating. Where $A$ is the set of annotations, including the 3 alternative actions selected by the annotators and the original trajectory. To get the same result for the model, we simply compute the simplified version of the equation above (replacing $a_p$ with the model prediction $\\\\hat{a}$):\\n\\n$$\\\\text{Agreement}(\\\\hat{a}, A) = \\\\max_{a_r \\\\in A} \\\\text{M}(\\\\hat{a}, a_r)$$\\n\\nIn total, we collected 402 annotations across 134 turns from the validation set. Using those annotations, we compared the reference and model predicted actions with the closest alternative annotations, using our proposed metrics. As shown in the Table 20, LLaMA-2-13B only achieves 65% of the overall score achieved by the original human navigator, whereas zero-shot GPT-4V achieves 31%; this reflects the major gap we found in Table 4. Moreover, this was performed on a subset of the validation split, so the result for each of the test splits may differ. However, we estimate that annotating the entire test splits would take the 3 annotators around 10 months, without counting the logistics involved with designing efficient annotation tools. Thus, we believe this would be a valuable contribution as part of a follow-up work.\\n\\n| Intent Text Group (F1) | Element Group (IoU) | Overall | Overall (Norm) |\\n|------------------------|---------------------|---------|----------------|\\n| Annotator Mean         | 92.79               | 36.20   | 58.40          | 46.62          | 96.97 |\\n| Annotator 1            | 87.31               | 33.21   | 56.82          | 44.84          | 93.27 |\\n| Annotator 2            | 94.78               | 39.05   | 56.33          | 47.56          | 98.93 |\\n| Annotator 3            | 96.27               | 35.72   | 58.76          | 47.41          | 98.62 |\\n| Original               | 95.52               | 40.19   | 56.20          | 48.07          | 100.00 |\\n| Llama-2-13B            | 91.04               | 34.37   | 28.44          | 31.45          | 65.42 |\\n| GPT-4V                 | 54.48               | 09.57   | 20.49          | 14.95          | 31.09 |\\n| GPT-4T                 | 58.21               | 12.47   | 21.46          | 16.90          | 35.15 |\\n| Fuyu-8B                | 84.33               | 25.07   | 27.44          | 26.24          | 54.58 |\\n\\nTable 20: Agreement scores of models and annotators with respect to the original and alternative trajectories (see Appendix C.6). The results are computed on a subset of the validation set, totalling 402 annotations across 134 turns.\"}"}
{"id": "mUSPhG4uDW", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 21: Comparison on the TEST IID split of zero-shot prompt (0S) with prompts that include a description and example (D&E) for each action. We do not observe substantial differences.\\n\\nC.7 Augmenting non-finetuned models with in-context examples\\n\\nIn most of the experiments, we use the same system prompts (Appendix B.5) to ensure a consistent comparison between models. We specifically chose prompts to ensure that we do not go over the common token limit of 2048 tokens. However, we also consider that providing a description of the actions alongside a concrete example taken from the training set could improve the performance, thus we include a variant prompt template that includes description and example (D&E) for each action, allowing the model to decide what is the best action to take based on a few examples. To ensure that the examples fit, we truncate parts of the examples. The results can be found in Table 21.\\n\\nThe template can be found below:\\n\\n```\\n{html}\\n```\"}"}
{"id": "mUSPhG4uDW", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WEB LINX: Real-World Website Navigation with Multi-Turn Dialogue\\n\\nPast Actions:\\ntabcreate(target=1482537091) ... tabswitch(origin=1482537091, target=1482537067)\\nTarget:\\nload(url=\\\"https://www.kayak.co.in/flights\\\")\\n\\n----------\\n\\nHTML:\\n(html(body class=\\\"not...late\\\" style=\\\"overflow...;\\\"(div(div class=\\\"ae\\\"(div class=\\\"aj\\\"...am\\\"(div ...))\\n\\nUtterances:\\n[-00:24] Hello [-00:16] Please open UberEATS. [-00:08] ... [-03:06] Tell me about some items from the Ice Cream section. ;\\n\\nTop candidates:\\n(uid = e69a70be-b695-443c) \\[tag\\] div \\[xpath\\] /html/.../div \\[bbox\\] x=166.5 ... height=578 \\[attributes\\] class=f794...443c\\n[children\\] div div \u2192 ...\\n\\n(uid = 3d33eeed-440d-42e9) \\[tag\\] span...\\nPast Actions:\\nclick(uid=\\\"0b39661e-11ae-40ca\\\") ... copy(text=\\\"Talenti Gelato Layers Vanilla Fudge Cookie 10.6oz\\\", timestamp=\\\"03:51\\\")\\nTarget:\\nscroll(x=0, y=-200)\\n\\n----------\\n\\nHTML:\\n(html(body class=\\\"show-...sticked\\\" (div class=\\\"feed-layout\\\" (div class=\\\"feed-header\\\" style=\\\"position: ...))\\n\\nUtterances:\\n[00:04] Hello [00:09] Open website fandom. ;\\n\\nTop candidates:\\n(uid = 06bfbae6-1a0e-433b) \\[tag\\] input \\[xpath\\] /html/.../input \\[bbox\\] x=406 ... height=32 \\[attributes\\] required=\\\"\\\"...\\n...\\n...\\n\\n(uid = 87fc2e27-e4f8-498c) \\[tag\\] a...\\nPast Actions:\\nclick(uid=\\\"f8223148-2066-4544\\\") text_input(text=\\\"Emilia Clarke\\\", uid=\\\"06bfbae6-1a0e-433b\\\") click(uid=\\\"6af45bdf-41da-4e0f\\\")\\nTarget:\\nsubmit(uid=\\\"6bfa288c-555a-4bd5\\\")\\n\\n----------\\nThe user's first and last 4 utterances are: {utterances};\\nViewport size: {height}h x {width}w ;\\nOnly the last 5 turns are provided.\\nHere are the top candidates for this turn: {top_10_candidates}\"}"}
{"id": "mUSPhG4uDW", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"To complement Section 6, we include the scores for each split: in-domain (\u00a722), out-of-domain mean (\u00a723), TextCat (\u00a724), TextGeo (\u00a725), TextVis (\u00a726), and TextWeb (\u00a727). We report the intent match (IM) to identify which models fail due to their inability to predict the correct intent. We also include the grouped results in tables Tables 28 to 30.\\n\\n### Table 22: Full in-domain test results\\n- We abbreviate submit to sbmt and textinput to input.\\n- The first section contains zero-shot results and the second contains finetuned results.\\n\\n| Model            | IoU    | IoU    | IoU    | chrF   | chrF   | F1     | IM   | IM   | IM   | IM   |\\n|------------------|--------|--------|--------|--------|--------|--------|------|------|------|------|\\n| Llama-2-7B       | 6.19   | 5.83   | 4.97   | 4.33   | 4.57   | 29.47  | 43.23| 36.67| 32.17| 6.90 |\\n| Llama-2-13B      | 9.42   | 0.00   | 4.97   | 1.25   | 4.82   | 20.57  | 75.65| 23.33| 14.93| 0.00 |\\n| GPT-3.5T         | 16.90  | 9.62   | 21.68  | 1.78   | 16.81  | 18.90  | 73.27| 23.33| 8.79 | 13.79|\\n| GPT-4T           | 15.92  | 3.45   | 41.33  | 4.53   | 37.50  | 18.90  | 59.61| 30.00| 18.24| 3.45 |\\n| GPT-4V           | 17.36  | 6.90   | 46.64  | 4.20   | 35.05  | 15.57  | 63.03| 16.67| 14.76| 6.90 |\\n| MindAct-250M     | 25.47  | 0.00   | 0.00   | 14.54  | 0.00   | 0.00   | 92.15| 0.00 | 100.00| 0.00 |\\n| MindAct-780M     | 24.37  | 0.93   | 19.34  | 20.26  | 12.39  | 10.00  | 90.33| 10.00| 100.00| 3.45 |\\n| MindAct-3B       | 24.60  | 24.14  | 30.44  | 35.19  | 21.80  | 16.67  | 89.65| 20.00| 100.00| 27.59|\\n| Flan-T5-250M     | 33.49  | 0.00   | 0.00   | 15.25  | 0.00   | 0.00   | 100.00| 0.00| 100.00| 0.00 |\\n| Flan-T5-780M     | 32.66  | 0.00   | 15.52  | 22.61  | 12.16  | 0.00   | 98.63| 0.00 | 100.00| 0.00 |\\n| Flan-T5-3B       | 31.22  | 48.38  | 42.00  | 37.46  | 34.34  | 24.47  | 92.26| 30.00| 100.00| 51.72|\\n| Pix2Act-282M     | 6.85   | 0.00   | 0.00   | 27.00  | 0.00   | 13.33  | 99.89| 16.67| 100.00| 0.00 |\\n| Pix2Act-1.3B     | 17.94  | 0.00   | 0.00   | 43.78  | 21.75  | 42.10  | 95.56| 46.67| 100.00| 13.79|\\n| Fuyu-8B          | 26.14  | 62.21  | 37.93  | 41.83  | 30.18  | 66.10  | 93.97| 66.67| 94.36| 75.86|\\n| S-LLaMA-1.3B     | 32.51  | 57.59  | 49.90  | 42.04  | 36.61  | 52.23  | 95.90| 63.33| 100.00| 75.86|\\n| S-LLaMA-2.7B     | 34.75  | 75.86  | 57.25  | 45.32  | 39.30  | 69.10  | 95.79| 73.33| 99.67| 75.86|\\n| Llama-2-7B       | 33.71  | 82.76  | 62.98  | 45.21  | 43.94  | 73.43  | 92.38| 76.67| 99.83| 86.21|\\n| Llama-2-13B      | 32.25  | 75.86  | 64.64  | 43.53  | 45.77  | 77.43  | 90.44| 80.00| 100.00| 75.86|\\n| GPT-3.5F         | 26.78  | 72.41  | 61.91  | 36.58  | 42.40  | 45.77  | 84.76| 50.00| 97.01| 72.41|\\n\\n### Table 23: Out-of-domain test results (average)\\n- We abbreviate submit to sbmt and textinput to input.\\n- The first section contains zero-shot results and the second contains finetuned results.\\n\\n| Model            | IoU    | IoU    | IoU    | chrF   | chrF   | F1     | IM   | IM   | IM   | IM   |\\n|------------------|--------|--------|--------|--------|--------|--------|------|------|------|------|\\n| Llama-2-7B       | 4.84   | 3.01   | 1.76   | 4.23   | 0.86   | 16.05  | 43.05| 19.24| 35.53| 7.62 |\\n| Llama-2-13B      | 8.89   | 0.50   | 1.51   | 1.46   | 1.06   | 13.56  | 75.93| 16.08| 15.32| 1.64 |\\n| GPT-3.5T         | 13.46  | 2.77   | 19.52  | 1.44   | 15.27  | 21.26  | 73.82| 24.74| 9.10 | 5.51 |\\n| GPT-4T           | 13.05  | 2.32   | 43.56  | 4.37   | 33.86  | 23.81  | 60.14| 29.15| 17.27| 4.89 |\\n| GPT-4V           | 13.57  | 2.26   | 42.99  | 3.13   | 33.08  | 18.87  | 64.47| 22.13| 13.25| 4.67 |\\n| MindAct-250M     | 16.87  | 0.00   | 0.50   | 14.28  | 0.22   | 0.00   | 88.77| 0.00 | 99.98| 0.00 |\\n| MindAct-780M     | 15.25  | 0.00   | 21.16  | 21.50  | 13.48  | 7.87   | 87.62| 7.87 | 100.00| 0.00 |\\n| MindAct-3B       | 17.04  | 11.25  | 33.30  | 35.39  | 19.15  | 13.15  | 92.64| 19.27| 99.97| 15.17|\\n| Flan-T5-250M     | 20.92  | 0.00   | 0.00   | 15.56  | 0.04   | 0.00   | 99.77| 0.00 | 100.00| 0.00 |\\n| Flan-T5-780M     | 20.56  | 0.00   | 6.28   | 22.90  | 3.81   | 0.00   | 98.85| 0.00 | 100.00| 0.25 |\\n| Pix2Struct-282M  | 8.70   | 0.00   | 0.00   | 26.70  | 1.48   | 12.80  | 98.92| 18.91| 100.00| 0.00 |\\n| Pix2Struct-1.3B  | 11.54  | 0.00   | 0.00   | 36.28  | 20.45  | 16.00  | 95.51| 26.76| 100.00| 4.18 |\\n| Fuyu-8B          | 16.55  | 25.31  | 28.96  | 32.01  | 21.24  | 15.51  | 92.30| 34.49| 96.21| 35.82|\\n| Flan-T5-3B       | 19.90  | 27.80  | 45.21  | 36.66  | 30.51  | 14.16  | 92.07| 21.01| 99.96| 32.71|\\n| S-LLaMA-1.3B     | 20.84  | 24.38  | 41.28  | 35.89  | 25.68  | 20.74  | 94.44| 39.31| 99.85| 33.48|\\n| S-LLaMA-2.7B     | 21.51  | 32.10  | 52.46  | 36.68  | 32.10  | 20.50  | 94.52| 35.23| 99.83| 36.89|\\n| Llama-2-7B       | 20.16  | 42.68  | 55.06  | 35.66  | 35.25  | 29.71  | 88.82| 50.27| 99.78| 50.18|\\n| Llama-2-13B      | 20.85  | 48.18  | 54.89  | 36.19  | 34.88  | 33.55  | 87.60| 51.66| 99.93| 53.88|\\n| GPT-3.5F         | 16.69  | 43.64  | 49.47  | 30.50  | 33.90  | 28.38  | 83.73| 39.38| 93.06| 46.29|\"}"}
