{"id": "xlWcdtCyOC", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nRongjie Huang 1\\nRuofan Hu 1\\nYongqi Wang 1\\nZehan Wang 1\\nXize Cheng 1\\nZiyue Jiang 1\\nZhenhui Ye 1\\nDongchao Yang 2\\nLuping Liu 1\\nPeng Gao 3\\nZhou Zhao 1\\n\\nAbstract\\nInstruction-guided speech editing aims to follow the user's natural language instruction to manipulate the semantic and acoustic attributes of a speech. In this work, we construct triplet paired data (instruction, input speech, output speech) to alleviate data scarcity and train a multi-task large language model named InstructSpeech. To mitigate the challenges of accurately executing user's instructions, we 1) introduce the learned task embeddings with a fine-tuned Flan-T5-XL to guide the generation process towards the correct generative task; 2) include an extensive and diverse set of speech editing and speech processing tasks to enhance model capabilities; 3) investigate multi-step reasoning for free-form semantic content editing; and 4) propose a hierarchical adapter that effectively updates a small portion of parameters for generalization to new tasks. To assess instruction speech editing in greater depth, we introduce a benchmark evaluation with contrastive instruction-speech pretraining (CISP) to test the speech quality and instruction-speech alignment faithfulness. Experimental results demonstrate that InstructSpeech achieves state-of-the-art results in eleven tasks, for the first time unlocking the ability to edit the acoustic and semantic attributes of speech following a user's instruction.\\n\\n1. Introduction\\nSpeech editing (Borsos et al., 2022; Bai et al., 2022) is a widely-used application that millions engage with every day, which allows the user to edit the recorded speech without degrading the quality and naturalness. However, existing efforts (Tan et al., 2021; Yang et al., 2023b) are quite limited due to 1) providing only a predefined set of content operations such as inserting missed words, replacing mispronounced words, and removing unwanted speech, while the acoustic attributes (e.g., timbre, volume, speed, emotion, and style) have been relatively overlooked, and 2) requiring a user-drawn mask, or per-example prompt to ensure that iterative edits are applied only to the target region.\\n\\nTo address the aforementioned issues, instruction-based speech editing aims to follow the user's natural language instruction to manipulate both semantic and acoustic attributes in speech, which benefits practicality as such guidance is more aligned with human intuition. For instance, a user can provide a model with a speech sample and instruct it to \\\"Make it sound happy\\\" or \\\"Speed up pronunciation\\\", effortlessly describing editing goals using natural language instructions. Despite the significant benefits, achieving high-quality instruction-guided speech inpainting remains challenging due to 1) data scarcity and 2) the complexity of accurately executing instruction.\\n\\nIn this work, we propose InstructSpeech, introducing the first speech editing model to follow human-written instructions. To mitigate the data scarcity, we generate triplet paired data (instruction, input speech, output speech) by combining large models pretrained on text and speech modality. InstructSpeech casts conditional generation as a sequence-to-sequence modeling task and trains a large language model (LLM) using instruction and input speech as conditions and generating output (edited) speech. To accurately process a variety of instructions, we 1) include the learned task embeddings to steer the generation process towards the correct generative task and fine-tune a Flan-T5-XL to identify the task given any instruction, and 2) train a LLM on an extensive and diverse set of tasks, including both speech editing and speech processing tasks to enhance its capabilities; 3) investigate the multi-step reasoning in free-form semantic editing to alleviate the difficulties of following human's instruction; 4) propose a hierarchical adapter and show that InstructSpeech can generalize to new tasks.\"}"}
{"id": "xlWcdtCyOC", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.1. Unit-based Vocoder\\n\\nThe generator of the unit-based vocoder is built from a set of look-up tables (LUT) that embed the discrete representation, and a series of blocks composed of transposed convolution and a residual block with dilated layers. We train the enhanced vocoder with the weighted sum of the least-square adversarial loss, the feature matching loss, and the spectral regression loss on mel-spectrogram, where the training objective formulation and hyperparameters follow (Kong et al., 2020; Lee et al., 2022).\\n\\nE. Case study\\n\\nIn this section, we visualize pairs of speech samples (i.e., before and after the edit) via semantic or acoustic manipulation using InstructSpeech.\\n\\nFigure 6: UMAP of emotion manipulation results.\\n\\nFigure 7: Acoustic editing results. Left: \u201cmanipulate the emotion to angry\u201d. Right: \u201cchanges its volume to high\u201d.\\n\\nWe present several examples sampled from the free-form editing in Table 9,\\n\\n| Instruction | Source | Target |\\n|-------------|--------|--------|\\n| Add the word \u201cvirtue\u201d between the word \u201cI\u201d and \u201chave\u201d | I have played the flute to the hurricane. | I virtue have played the flute to the hurricane. |\\n| Target frame-level duration alignment | 13 | Predicted frame-level duration alignment | 12 |\\n| Add the word \u201cpreceded\u201d between the word \u201cwere\u201d and \u201cof\u201d | The walls were of mud, the roof was of straw | The walls were preceded of mud, the roof was of straw |\\n| Target frame-level duration alignment | 37 | Predicted frame-level duration alignment | 37 |\\n\\nTable 9: Two examples comparing free-form editing produced by InstructSpeech.\"}"}
{"id": "xlWcdtCyOC", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we include the detailed extensive experiments to further demonstrate the results.\\n\\nF.1. Region-based content editing\\n\\nFor region-based content editing, we include a subjective evaluation by respectively scoring MOS and SMOS, rated from 1 to 5 and reported with 95% confidence intervals (CI). For easy comparison, the results are compiled and presented in the following table.\\n\\n|               | SIM | MOS | SMOS |\\n|---------------|-----|-----|------|\\n| GT            |     | 5.8 | 5.7  |\\n|               |     |     |      |\\n| EditSpeech    |     | 4.2 | 3.8  |\\n|               |     |     |      |\\n| A3T           |     | 6.4 | 6.9  |\\n|               |     |     |      |\\n| Base          |     | 5.1 | 4.9  |\\n|               |     |     |      |\\n| Medium        |     | 6.4 | 5.8  |\\n|               |     |     |      |\\n| Large         |     | 5.1 | 4.9  |\\n|               |     |     |      |\\n\\nInstructSpeech (medium) achieves high perceptual quality with MOS and SMOS of 3.96 and 4.01. It indicates that raters prefer our model synthesis against baselines in terms of speech intelligibility and style similarity, which is consistent with the objective evaluation results.\\n\\nF.2. Free-form content editing\\n\\nWe include the optimized way for speech editing, which can be decomposed into multiple sub-tasks using the following external cascaded models:\\n\\n- The speech sample is transcribed by the whisper ASR system.\\n- The transcription is tokenized into phones using grapheme-to-phoneme tools.\\n- We mask the original speech we want to update based on phone-frame duration alignment, which is generated using the MFA tool trained on LibriTTS dataset.\\n- Given phone and masked speech, the edited speech is generated by a region-based speech editing model (i.e., InstructSpeech).\\n\\nInstructSpeech prompts step-by-step, significantly tackling the challenges of accurately locating and manipulating the target context following the user's instruction. InstructSpeech presents its advantages in multitask prediction since it is trained on an extensive and diverse set of tasks to enhance capabilities, while these cascaded models are usually optimized in varying ways and datasets, where the domain gap can lead to cascaded error and quality degradation.\\n\\nF.3. Acoustic editing\\n\\nWe include the comparison with several baselines (i.e. VALL-E (Wang et al., 2023a) and Spear-TTS (Zhang et al., 2023b)) on the benchmark zero-shot TTS tasks in speaker transferring. Specifically, we report the WER and SIM to respectively assess the audio quality and style similarity, using a small-scale test set with the examples provided on the demo page.\"}"}
{"id": "xlWcdtCyOC", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nAlso score MOS and SMOS for subjective evaluation, rated from 1 to 5 and reported with 95% confidence intervals (CI). For easy comparison, the results are compiled and presented in the following table.\\n\\n| Model      | MOS (\u2191) | SMOS (\u2191) | WER (\u2193) | SIM (\u2191) |\\n|------------|---------|----------|---------|---------|\\n| V ALL-E    | 3.92    | 3.81     | 6.5     | 0.79    |\\n| Spear-TTS  | 3.97    | 3.89     | 5.7     | 0.83    |\\n| InstructSpeech | 4.04    | 3.94     | 5.0     | 0.85    |\\n\\nInstructSpeech presents a 1.5 lower score WER and 0.06 higher SIM than V ALL-E, also achieving superior results compared to Spear-TTS. To conclude, InstructSpeech avoids cascaded errors (V ALL-E's cascaded NAR and AR models, and Spear-TTS's cascaded semantic and acoustic tokens), and trains the LLM on an extensive and diverse set of tasks at the scale of around 100K hours including both speech editing and speech processing tasks to enhance its capabilities.\\n\\nG. Evaluation\\n\\nG.1. Objective Evaluation\\n\\nFor emotion and style controlling accuracy, we train an open-source GE2E with our speech data. We train the emotion and style classifiers respectively on ESD dataset and the constructed LibriTTS-style dataset with GE2E loss, which achieve the high accuracy of 99.8 and 97.6 averaged across emotions and styles, serving as metrics to evaluate the similarity of generated samples.\\n\\nFor controlling accuracies on volume, pitch, and speaking speed, considering that the values of generated singing may slightly deviate from the boundaries used for categorization, we adopt a soft-margin mechanism for accuracy calculation. Specifically, we take the accuracy of data falling within the correct range as 100, and calculate the accuracy with $100 \\\\times \\\\exp\\\\left(-k\\\\epsilon\\\\right)$ for data outside the correct range, where $\\\\epsilon$ is the error between the data value and the boundary, and $k$ is a hyper-parameter controlling the decay rate of accuracy at the margins, with larger $k$ corresponding to faster decay. We take accuracy curves of high vocal-range of female, low speed, and medium volume as examples and illustrate them in Figure 8.\\n\\nG.2. Subjective Evaluation\\n\\nAll our Mean Opinion Score (MOS) tests are crowd-sourced and conducted by native speakers. The scoring criteria have been included in Table 10 for completeness. The samples are presented and rated one at a time by the testers, each tester is asked to evaluate the subjective naturalness of a sentence on a 1-5 Likert scale. The screenshots of instructions for testers are shown in Figure 9. We paid $8 to participants hourly and totally spent about $400 on participant compensation.\\n\\nTable 10: Ratings that have been used in the evaluation of speech naturalness of synthetic and ground truth samples.\\n\\n| Rating | Naturalness Definition                  |\\n|--------|----------------------------------------|\\n| 1      | Bad - Very annoying and objectionable dist. |\\n| 2      | Poor - Annoying but not objectionable dist. |\\n| 3      | Fair - Perceptible and slightly annoying dist. |\\n| 4      | Good - Just perceptible but not annoying dist. |\\n| 5      | Excellent - Imperceptible distortions     |\\n\\nH. Limitation and Potential Risks\\n\\nWe control synthesized speech tempo by multiplying durations by a factor $\\\\lambda$, and thus InstructSpeech supports global speed editing. To enable fine-grained control, the model may be prompted with phoneme-level durations, which can be left for future work.\\n\\nAlthough InstructSpeech as a voice LLM is successfully applied to zero-shot voice signals at scale, it still suffers from some limitations: 1) InstructSpeech introduces a strong dependency on the quality of the audio tokenizer. 2) We test in-context learning ability of our model on manipulation testing set, and there are still challenges in open-domain instruction editing.\"}"}
{"id": "xlWcdtCyOC", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Following Speech Editing Instructions via Large Language Models\\n\\nFemale high Label: \\\\[ y = 100 e^{0.1 \\\\min(\\\\text{abs}(x_{180}), \\\\text{abs}(x_{220}))} \\\\]\\n\\nLow Label: \\\\[ y = 100 e^{5 \\\\min(\\\\text{abs}(x_{1.5}), \\\\text{abs}(x_{2.1}))} \\\\]\\n\\nMedium Label: \\\\[ y = 100 e^{150 \\\\min(\\\\text{abs}(x_{0.05}), \\\\text{abs}(x_{0.07}))} \\\\]\\n\\nFigure 8: Soft-margin accuracy curve.\\n\\n(a) Pitch.\\n\\n(b) Speed.\\n\\n(c) Volume.\\n\\nFigure 9: Screenshots of subjective evaluations.\"}"}
{"id": "xlWcdtCyOC", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nIn Appendix G.1.\\n\\nConstrastive instruction-speech pretraining score evaluation. Most existing contrastive pretraining models are optimized using image (CLIP (Radford et al., 2021)) or audio (CLAP (Elizalde et al., 2023)) data, which are difficult to distinguish human speech's fine-grained prosody information such as speaking speed, volume, style or emotion. As such, we fine-tune the CLAP model to learn a multimodal space of speech and text encoders in our instruction datasets using contrastive loss (Vyas et al., 2023). After training, we evaluate the model in downstream audio-text and text-audio retrieval tasks, where we compute the similarity between the audio and text embeddings. Take audio-text retrieval as an example, top-N descriptions are computed by picking the descriptions corresponding to the top N values in similarity. More details are included in Appendix C.\\n\\n|          | R@1  | R@5  | R@10 |\\n|----------|------|------|------|\\n| Base     | 6.7  | 26.7 | 36.7 |\\n| Ours     | 56.6 | 86.6 | 96.7 |\\n\\nTable 2: Retrieval accuracy using a contrastive instruction-speech pretraining model.\\n\\nAs can be seen in Table 2, the constrastive instruction-speech pretraining model (ours) achieves the highest retrieval accuracy with T2A R@10 96.7 and A2T R@10 99.8. It indicates the outperformed capabilities in assessing the coherence of the generated speech in relation to the natural language instruction. After training the CISP model, we use it to evaluate instruction-guided speech editing models by calculating: 1) CISP text-speech direction similarity (CISPtext) \u2013 measuring alignment between instruction and edited speech, and 2) CISP speech similarity (CISPspeech) \u2013 measuring the change between edited and input speech.\\n\\nSubjective evaluation.\\n\\nWe also conduct a crowd-sourced human evaluation via Amazon Mechanical Turk, which is reported with 95% confidence intervals (CI), and analyze two aspects: style similarity (speaker, emotion, and prosody) and audio quality (clarity, high-frequency), respectively scoring SMOS and MOS. More information on evaluation has been attached in Appendix G.2.\\n\\n6. Training setup\\n\\n6.1. Dataset\\n\\nFor speech processing and speech editing tasks, we use Librilight (Kahn et al., 2020), LibriSpeech (Panayotov et al., 2015), LibriTTS (Zen et al., 2019), and VCTK (Veaux et al., 2017) datasets. Besides, the ESD (Zhou et al., 2022) dataset with 5 emotion categories and the synthesized dataset LibriTTS-style with 19 style categories are further included, respectively for emotion and style editing. For region-based content editing, we adopt Montreal Forced Aligner (McAuliffe et al., 2017) to calculate the alignment between phoneme and speech. We tokenize text into the phoneme sequence with an open-source grapheme-to-phoneme conversion tool (Sun et al., 2019) and convert the sampling rate of all data to 16kHz. The detailed data statistics for each task are included in Appendix A.\\n\\n6.2. Model Configurations\\n\\nFor acoustic representation, we train the SoundStream model with 12 quantization levels, each with a codebook of size 1024 and the same downsampling rate of 320. We train three sets of InstructSpeech, with 160M (base), 520M (medium), and 1.2B (large) parameters. As for the unit-based vocoder, we use the modified V1 version of BigVGAN. A comprehensive table of hyperparameters is available in Appendix D. Except explicitly stated, we use our 520M (medium) model for downstream evaluation.\\n\\nDuring training, we train InstructSpeech for 100K steps using 8 V100 GPUs with a batch size of 6000 tokens for each GPU on the publicly-available fairseq framework (Ott et al., 2019). Adam optimizer is used with $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.98$, $\\\\epsilon = 10^{-9}$. BigVGAN is optimized with a segment size of 8192 and a learning rate of $1 \\\\times 10^{-4}$ until 500K steps using 4 V100 GPUs. For sampling, we employ top-p (Holtzman et al., 2019) sampling with $p = 0.25$.\\n\\n6.3. Baseline models\\n\\nWe also compare the InstructSpeech with other systems, including 1) GT, the ground-truth audio; 2) GT (voc.), where we first convert the ground-truth audio into tokens and then convert them back to audio using BigVGAN; 3) EditSpeech (Tan et al., 2021) and A3T (Bai et al., 2022) perform region-based content editing with deletion, insertion and replacement of words in a given speech utterance; 4) YourTTS (Casanova et al., 2022), as we are investigating a new task with no previous work to compare with, we train YourTTS with speech prompt for emotion or style guidance.\\n\\n7. Results\\n\\n7.1. Quantitative Results\\n\\nThe objective and subjective evaluation is presented in tables, and extensive experiments are included in Appendix F.\"}"}
{"id": "xlWcdtCyOC", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nTable 3: Acoustic editing results. We modify YourTTS to include speech prompt for emotion or style guidance.\\n\\n|        | GT | GT (voc.) | YourTTS | Base | Medium | Large |\\n|--------|----|-----------|---------|------|--------|-------|\\n| Speed  | 86.4 | 91.6     | 82.8    | 81.2 | 84.0   | 84.9  |\\n| Volume | 96.1 |           | 93.1    |      |        |       |\\n| Emotion|      |           | 4.32    |      |        |       |\\n\\nTable 4: Region-based content editing. We report WER of adding, removing, and replacing operation, as well as the overall SIM in LibriSpeech-test set.\\n\\n|        | SIM |\\n|--------|-----|\\n| GT     | 0.98|\\n| EditSpeech | 0.97|\\n| A3T    | 0.96|\\n\\nMulti-step reasoning\\n\\nTable 5: Multi-step reasoning for free-form content editing. We evaluate our models in LibriSpeech-test set.\\n\\n|        | Add | Remove | Replace |\\n|--------|-----|--------|---------|\\n| Base   | 6.3 | 4.2    | 5.9     |\\n| Medium | 6.4 | 5.5    | 5.8     |\\n| Large  | 5.1 | 4.9    | 5.6     |\\n\\nQuality as previous non-autoregressive speech editing families. 2) For free-form semantic editing, InstructSpeech has demonstrated an overall PER of 7.3 in phone recognition, as well as a mean absolute error (MAE) of 0.61 in frame-level phone alignment prediction averaged across different operations. InstructSpeech is aware of the speech segmentation and their corresponding phones, which to the best of our knowledge is first model available for free-form editing following human's instruction, attributing to the multi-step reasoning.\\n\\nAcoustic attribute editing. We also evaluate our model to manipulate acoustic attributes (e.g., gender, emotion, and style) and keep the semantic content unchanged: 1) Regarding similarity, InstructSpeech scores the highest accuracy of 55.2 and 90.9 respectively in emotion and style editing, showing that InstructSpeech excels at transferring the prosody of custom voices following instruction. Informally, InstructSpeech is optimized in a large amount of self-supervised data, which contains many speakers with various accents and diverse demographics to improve robustness and generalization. 2) For speed and volume, InstructSpeech also effectively alters its speaking style guided by human instruction. Note that speed alteration only supports speeches in the LJSpeech speaker identity. 3) Regarding CISP direction similarity (CISPtext and CISPSpeech), InstructSpeech presents that strong coherence of 0.59 between text instruction and edited speech, as well as a high similarity of 0.68 between the speech before and after edit. It suggests the precise speech editing following user's instruction while keeping the other attributes in consistent with the speech before edit.\\n\\nSubjective Human Evaluation\\n\\nThe evaluation of the instruction editing models is very challenging due to its subjective nature in perceptual quality, and thus we include a human evaluation: InstructSpeech achieves the high perceptual quality with MOS of 4.01 and SMOS of 3.95. It indicates that raters prefer our model synthesis against baselines in terms of audio naturalness and faithfulness.\\n\\n7.2. Qualitative Findings\\n\\nFirstly, we explore the region-based content editing and compare the results with baseline models (i.e., A3T or EditSpeech). As shown in Figure 4, InstructSpeech presents a smooth transition (i.e., pitch contours) between the edited and origin region and demonstrates good intelligibility. We attach more mel-spectrograms of edited samples and corresponding pitch tracks in Appendix E.\\n\\nWe present the free-form semantic editing examples in Table 9: InstructSpeech step-by-step recognizes the phone sequence and frame-level duration alignment, in the following InstructSpeech leverages the region-based speech editing with user-drawn mask to manipulate the sequence. As such, the multi-step reasoning process tackles the challenges of executing user's instructions, empowering InstructSpeech to accurately locate and manipulate the target.\"}"}
{"id": "xlWcdtCyOC", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\n**Task**\\n- Content editing\\n- Emotion editing\\n\\n**WER SIM**\\n- With task embedding: 5.9, 0.98, 55.2, 0.41\\n- Without task embedding: 7.4, 0.97, 51.3, 0.43\\n\\n**Multitask learning**\\n- Predicted embed: 73.2%\\n\\n**Task embedding conditions**\\n- All: 438M, 0.32, 0.86\\n- Lora: 8.97M, 0.41, 0.84\\n- HA: 3.03M, 0.38, 0.85\\n\\n**Finetuning in emotion task**\\n\\nTable 6: Ablation studies. In Figure (a), we train two models on all tasks except frame-level TTS and VC task, and test them respectively in region-based content editing and emotion editing. In Figure (c), we use HA to denote the hierarchical adapter.\\n\\nTo visualize whether different styles are identified in generated samples, we randomly sample 19 styles; each is converted into a 256-dimensional embedding and reduced to 2-dimensional with Uniform Manifold Approximation and Projection (UMAP). As can be seen in Figure 3, InstructSpeech presents style-aware acoustic editing with significant inter-class distance. We also include the UMAP of emotional samples in Figure 6 in Appendix E.\\n\\nFigure 3: UMAP of style manipulation results.\\n\\n7.3. Analysis and Ablation Studies\\n\\nTo verify the capabilities of InstructSpeech, we conduct ablation studies and discuss the key findings in this section.\\n\\n**Combining speech tasks.**\\nTo demonstrate the effectiveness of optimizing instruction editing models on an extensive and diverse set of tasks, we train two additional models on all tasks except:\\n- (i) frame-level TTS task,\\n- (ii) VC task.\\nAs we show in Table 6, adding the frame-level TTS improves the model performance in region-based content editing, where it assists to encode aligned phone sequences and generate intelligible speech. Similarly, VC assists the model in understanding semantic representation, thereby improving model robustness and generalization.\\n\\n**Scalability to improve performance.**\\nWe also report results for different model sizes, namely 160M (base), 520M (medium), and 1.2B (large) parameter models. As expected, scaling the model size results in better scores. For example, increasing the model size from 520M to 1.2B leads to a 0.75% reduction in WER averaged across region-based speech editing tasks and 3.7% accuracy improvement in speed manipulation tasks.\\n\\n**Task embedding.**\\nInspired by (Sheynin et al., 2023), we compare to condition InstructSpeech on the ground-truth task embedding or the task embedding predicted by the task predictor. Additionally, a model without a task embedding (still having T5 instruction condition) is also investigated.\\n\\nTo conclude, the T5 task embedding predictor demonstrates high accuracy in recognizing task categories from human instruction, and thus, whether the task embedding is predicted or not makes no difference. In contrast, we observe that without conditioning on the task type, the model may perform the wrong editing operation.\\n\\n**Few-shot learning with hierarchical adapter.**\\nTo enable few-shot learning of new tasks without losing the general abilities, we fine-tune InstructSpeech in only 1-hour unseen ESD data, and compare the results among different adaptation methods. Illustrated in Table 6(c), as a lightweight plug-and-play module, the proposed hierarchical adapter enjoys superior training efficiency with only 1% parameters in contrast to lora or full finetuning and demonstrates...\"}"}
{"id": "xlWcdtCyOC", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nFollowing Speech Editing Instructions via Large Language Models\\n\\nThe outperformed WER and SIM score. This enables us to fine-tune instruction-editing LLMs on cheap devices.\\n\\n8. Conclusion\\n\\nIn this work, we presented InstructSpeech with the first attempt to edit the acoustic and semantic attributes of speech given a user's instruction input. To steer the generation process toward the correct generative task, we included learned task embeddings and fine-tuned a Flan-T5-XL to identify the task given the input instruction. InstructSpeech included multitask learning on an extensive and diverse set of speech editing and speech processing tasks, enhancing its capabilities to manipulate a speech's semantic and acoustic attributes. We investigated the multi-step reasoning to alleviate the difficulties in following human instruction, and thus, InstructSpeech was the only model available for free-form semantic editing. To generalize to unseen tasks, we proposed a hierarchical adapter to update only 1% of parameters efficiently. The comprehensive metrics with contrastive instruction-speech pretraining (CISP) demonstrated that InstructSpeech achieved state-of-the-art results in 11 editing tasks with superior speech quality and instruction-speech alignment faithfulness. We envisage that our work serves as a basis for future speech editing studies.\\n\\nImpact Statement\\n\\nThis paper aims to advance open-domain instruction-guided speech editing, which will ease the effort of speech and digital art creation. The multitask learning on an extensive and diverse set of speech editing and speech processing tasks, enhancing its capabilities to manipulate a speech's semantic and acoustic attributes. A negative impact is the risk of misinformation. To alleviate it, we can train an additional classifier to discriminate the fakes. We believe the benefits outweigh the downsides.\\n\\nInstructSpeech lowers the requirements for high-quality instruction-guided speech editing, which may cause unemployment for people with related occupations, such as speech engineers and radio hosts. In addition, there is the potential for harm from non-consensual voice cloning or the generation of fake media, and the voices in the recordings might be overused than they expect.\\n\\nAcknowledgements\\n\\nThis work was supported in part by the National Natural Science Foundation of China under Grant No. 62222211.\\n\\nReferences\\n\\nBai, H., Zheng, R., Chen, J., Ma, M., Li, X., and Huang, L. A3t: Alignment-aware acoustic and text pretraining for speech synthesis and editing. In International Conference on Machine Learning, pp. 1399\u20131411. PMLR, 2022.\\n\\nBorsos, Z., Sharifi, M., and Tagliasacchi, M. Speech-painter: Text-conditioned speech inpainting. arXiv preprint arXiv:2202.07273, 2022.\\n\\nCasanova, E., Weber, J., Shulby, C. D., Junior, A. C., G\u00a8olge, E., and Ponti, M. A. Yourtts: Towards zero-shot multispeaker tts and zero-shot voice conversion for everyone. In International Conference on Machine Learning, pp. 2709\u20132720. PMLR, 2022.\\n\\nChan, C. H., Qian, K., Zhang, Y ., and Hasegawa-Johnson, M. Speechsplit2. 0: Unsupervised speech disentanglement for voice conversion without tuning autoencoder bottlenecks. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6332\u20136336. IEEE, 2022.\\n\\nChen, M., Tan, X., Li, B., Liu, Y ., Qin, T., Zhao, S., and Liu, T.-Y . Adaspeech: Adaptive text to speech for custom voice. arXiv preprint arXiv:2103.00993, 2021.\\n\\nChen, S., Wang, C., Chen, Z., Wu, Y ., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):1505\u20131518, 2022.\\n\\nD\u00b4efossez, A., Copet, J., Synnaeve, G., and Adi, Y . High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022.\\n\\nElizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H. Clap learning audio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\\n\\nGuo, Z., Leng, Y ., Wu, Y ., Zhao, S., and Tan, X. Prompttts: Controllable text-to-speech with text descriptions. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\\n\\nHan, B., Dai, J., Song, X., Hao, W., He, X., Guo, D., Chen, J., Wang, Y ., and Qian, Y . Instructme: An instruction guided music edit and remix framework with latent diffusion models. arXiv preprint arXiv:2308.14360, 2023.\\n\\nHoltzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y . The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.\"}"}
{"id": "xlWcdtCyOC", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nFigure 1: Multi-turn speech editing. Each subsequent speech is derived from the prior one, using its associated instruction.\\n\\nTo assess instruction speech editing in greater depth, a comprehensive benchmark with contrastive instruction-speech pretraining (CISP) is introduced. InstructSpeech exhibits superior speech quality and instruction-speech alignment faithfulness. Experimental results demonstrate that InstructSpeech achieves state-of-the-art results in eleven tasks, for the first time unlocking the ability to edit the acoustic and semantic attributes of speech following a user's instruction.\\n\\nKey contributions of the paper include:\\n\\n\u2022 We collect triplet training data (instruction, input speech, output speech) and propose InstructSpeech\u2014the first speech editing model to follow human instructions.\\n\\n\u2022 We introduce the learned task embeddings and train InstructSpeech on an extensive and diverse set of tasks to enhance its capabilities.\\n\\n\u2022 We investigate multi-step reasoning to eliminate the difficulties in free-form speech editing.\\n\\n\u2022 We propose a hierarchical adapter for efficient adaptation to new tasks, only updating 1% parameters on top.\\n\\n\u2022 We introduce a benchmark evaluation with contrastive instruction-speech pretraining (CISP), and present state-of-the-art quantitative results with qualitative findings.\\n\\n2. Related Work\\n\\n2.1. Speech editing\\n\\nSpeech editing systems expect to correct mispronunciation and improve fluency. EditTTS (Tae et al., 2021) is an off-the-shelf speech editing methodology based on score-based generative modeling for pitch control and content replacement. VoiceBox (Le et al., 2024) supports region-based content editing, which is created by filling frames mapped to unreplaced phones with the original frames and leaving those for new phones with zeros. AudioBox (Vyas et al., 2023) takes as input a masked speech with an accompanying transcript and an optional description and infills the masked portion. EditSpeech (Tan et al., 2021) allows a user to perform deletion, insertion, and replacement of words in a given speech utterance, where partial inference and bidirectional fusion are proposed to achieve smooth transition at both left and right boundaries. Another line of works operates on acoustic attributes (e.g., timbre, emotion, and prosody) and keeps the semantic content representation unchanged: voice conversion (Qian et al., 2019; Chan et al., 2022) aims to alter the voice of a person to suit different styles while conserving the linguistic content. However, these methods still require a user-drawn mask or per-example prompt and are constrained by a predefined set of operations. In this work, we train the speech editing model to follow human-written instructions on an extensive and diverse set of tasks.\\n\\n2.2. Learning to follow instructions\\n\\nSeveral recent studies propose to control audio style through instruction-guided generative models. Prompt-TTS (Guo et al., 2023) takes a prompt with both style and content descriptions as input to synthesize the corresponding speech. Prompt-TTS 2 (Leng et al., 2023) adopts a variation network to provide variability information of voice not captured by text prompts. Instruct-TTS (Yang et al., 2023a) takes advantage of cross-modal learning and captures semantic information from the style prompts to control the speaking style. AUDIT (Wang et al., 2023b) proposes to edit background sound effects, and InstructME (Han et al., 2023) offers a latent diffusion model for instruction-guided music editing and remixing. However, previous works focus on TTS or instruction-guided music/sound editing, following instructions to edit human speech is relatively overlooked.\\n\\n2.3. Adapting speech generative models\\n\\nSpeech generative models have achieved remarkable advances in recent years, opening up a wide array of applications that leverage their power by adapting models. UniAudio (Yang et al., 2023b) is designed to continuously support new generation tasks (e.g., audio editing) through fine-tuning the whole parameters. Liu et al. (2023) achieve better performance by finetuning low-rank adaptation (LoRA) which adds the linear input projection to each self-attention layer. Vyas et al. (2023) include two-stage full fine-tuning to improve our model fidelity and quality where all parameters are optimized together. Chen et al. (2021) introduce conditional layer normalization and fine-tune this part in\"}"}
{"id": "xlWcdtCyOC", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nAddition to speaker embedding for new speaker adaptation. In this work, we present a hierarchical adapter to efficiently fine-tune the cross-attention mechanism in the global transformer and bias/norm for the local transformer, which updates only 1% of the parameters on top.\\n\\n3. Multi-Task Dataset for Instruction Speech Editing\\n\\nRegion-Based Semantic Content Editing\\n\\nAdd: Insert a new word into the speech given mask.\\nRemove: Erase a word from the speech given mask.\\nReplace: Replace the word by another given mask.\\n\\nFree-Form Semantic Content Editing\\n\\nAdd: Insert a new word following instruction.\\nRemove: Erase a word following instruction.\\nReplace: Replace the word following instruction.\\n\\nAcoustic Editing\\n\\nStyle: Change the style of speech.\\nEmotion: Change the emotion of speech.\\nSpeed: Change the speaking speed of speech.\\nVolume: Change the volume of speech.\\nGender: Change the gender of speech.\\n\\nSpeech tasks\\n\\nTTS: Convert phone into corresponding audio.\\nFrame-level TTS: Convert frame-level phone into audio.\\nVC: Transform the timbre of a speech.\\nASR: Transcribe speech into corresponding text.\\nDuration: Predict the frame-level phone alignment.\\n\\nTable 1: Description of the tasks forming the InstructSpeech dataset.\\n\\nTraining a robust and accurate speech editing model to follow human-written instructions typically requires a highly diverse dataset on an extensive and diverse set of tasks, while there are very few resources providing triplet paired data (instruction, input speech, output speech) due to the heavy workload.\\n\\nTable 1 includes the complete list of tasks. To mitigate the data scarcity, we combine the abilities of two large-scale pretrained models that operate on different modalities: a text large language model (i.e., GPT-3.5-Turbo) and a speech generative model to generate triplet paired data.\\n\\n3.1. Generating paired speech\\n\\nWe use several different pre-trained speech generative models to synthesize speech samples (after editing) that align well with human instruction.\\n\\nEmotion, gender and style editing. We train a large-scale, in-context learning Speech LLM and use the speech prompt on the ESD (Zhou et al., 2022), LibriTTS (Zen et al., 2019), and LibriTTS-style datasets to respectively control the emotion, gender, and style. The models are trained in wild data at the scale of around 100K hours (e.g., Librilight (Kahn et al., 2020)), which leads to better generalization for synthesizing unseen speech styles in a zero-shot fashion. To construct LibriTTS-style, we use texts from LibriTTS and 19 provided styles in Microsoft Azure TTS API [6] to synthesize corresponding speech. More details have been included in Appendix B.\\n\\nContent editing with adding, removing, replacing words.\\n\\nWe train a speech editing model to generate target-edited speech samples, which require a user-drawn mask to ensure that editions are applied only to the target region. In practice, we train a TTS model and randomly choose some phonemes to mask during the training stage, where we expect it to recover the whole speech based on the phoneme sequence. In the inference stage, we can mask the region we want to update in the speech and input the new words to obtain the speech samples after the edit.\\n\\nSpeed editing.\\n\\nWe train a multi-speaker non-autoregressive TTS model (Popov et al., 2021) with duration predictor to tell us how many frames each element of text input lasts. Following common practice, we control speech tempo by multiplying predicted durations by some factor $\\\\lambda$. We set $\\\\lambda = 0.5, 1, 1.3$ to generate speech respectively with \\\"slow\\\", \\\"normal\\\", and \\\"fast\\\" pronouncing speed while keeping the speaker/content unchanged.\\n\\nEnergy editing.\\n\\nWe build three categories of \\\"low\\\", \\\"medium\\\", and \\\"high\\\", indicating the amplitude root mean square (RMS) ranges of $[0.02, 0.04]$, $[0.07, 0.10]$ and $[0.16, 0.20]$, respectively. To construct the dataset, we rescale audio into different ranges dynamically.\\n\\n3.2. Generating text instruction\\n\\nFor each task, we leverage GPT-3.5-Turbo to generate diverse instructions, where we provide the LLM with a task description and a few task-specific exemplars. For acoustic editing (e.g., timbre, emotion, and prosody), we expect the LLM to output diverse instructions such as \\\"Make this speech sound happy\\\", or \\\"change the style to broadcasting\\\". For semantic content editing, we randomly choose operations (i.e., add, remove, and replace) and indicate the edited words, such as \\\"Add sunny between good and day\\\", \\\"Delete the word sunny\\\", \\\"Replace the word today by tomorrow\\\".\\n\\n4. InstructSpeech\\n\\nWe train a large language model (LLM) using instruction and input speech as conditions and generating output (edited) speech. In this section, we overview the discrete speech tokens and text representation, and then introduce...\"}"}
{"id": "xlWcdtCyOC", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\n**Figure 2:** A high-level overview of InstructSpeech. We use B/N to denote the bias tuning and norm tuning.\\n\\n4.1. Speech Representation\\nFor semantic tokens, we apply Hubert (Hsu et al., 2021) and use k-means to discretize 12th-layer embeddings into semantic tokens with a codebook of size 1000 and a total downsampling rate of 320. For acoustic tokens, we tokenize speech clips with the pre-trained SoundStream tokenizer (Zeghidour et al., 2021; D\u00b4efossez et al., 2022). The audio encoder $E$ of codec models consists of several convolutional blocks with a total downsampling rate of 320 and generates representations at every 20-ms frame in 16kHz, where we flatten $n$ codebooks.\\n\\n4.2. Text representation\\nText-guided synthesis models need powerful text encoders to capture the meaning of arbitrary language inputs. We use pre-trained Flan-T5-XL (Raffel et al., 2020) and freeze the weights to derive text representation, which is trained on text-only corpus significantly larger than multimodal data, thus being exposed to a rich distribution of the text. As illustrated in Figure 2(a), to guide the generation process toward the correct task, we further signal to the model which task it should perform on a given input by prefixing the information with a tag specifying the task. In the inference stage, we predict the task index by fine-tuning a Flan-T5-XL model to identify the task at hand given the input instruction. As expected, conditioning InstructSpeech on task embedding demonstrates a high accuracy in recognizing task categories from human instruction. We refer the reader to Section 7.3 for our findings.\\n\\n4.3. Architecture\\nInstructSpeech $\\\\theta$ is built upon end-to-end differentiable multiscale transformers (Yu et al., 2023; Yang et al., 2023b) to predict long sequences with sub-quadratic self-attention. As illustrated in Figure 2(b): 1) the token embedding matrix $E_G$ maps integer-valued tokens $a_1, a_2, ..., c_2, c_3$ to $m$-dimensional embeddings, and concatenate with continuous T5 representation in time axis, following which 2) we chunk it into patches of size $P$ of length $K = TP$, 3) a large global transformer $\\\\theta_{global AR}$ module outputs patch representations $G_1:K_0 = \\\\theta_{global AR}(G_0:K_{-1}i)$, and 4) a small local transformer module operates on a single patch containing $P$ elements, each of which is the sum of an output from the global model and an embedding of the previous tokens, and autoregressively predict the next patch $L_1:K_0 = \\\\theta_{local AR}(L_0:K_{-1}i+G_1:K_0)$.\\n\\nInstructSpeech presents the improvements from scaling the models' size in depth and width without the requirement of scattered model-specific methodologies. As expected, scaling the model size (160M (base), 520M (medium), and 1.2B (large) parameter) results in better scores. We refer the reader to Section 7.3 for our findings.\\n\\n4.4. Few-shot adaptation\\nTo enable few-shot learning for generalization to new tasks, we propose a hierarchical adapter to finetune the LLM given a few examples of a new task. For the global transformer, we...\"}"}
{"id": "xlWcdtCyOC", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nadopt a set of learnable adaption prompts $P$ and learn a new task embedding $E_e$. $E_e$ denotes a new learnable randomly-initialized embedding vector for task adaptation, which is learned by the language modeling loss objective given a few examples of a new task. For the local transformer, we include a normalization tuning bias tuning strategy, where all parameters in normalization layers as well as the bias/scale in linear layers are set to be updated.\\n\\nSuppose we have instruction representation $I \\\\in \\\\mathbb{R}^{K \\\\times C}$ of new tasks with length $K$ and feature dimension $C$, we initialize learnable adaption prompt $\\\\{P_l\\\\}_{l=1}^L$ for $L$ transformer layers, where we have each layer's prompt $P_l \\\\in \\\\mathbb{R}^{K \\\\times C}$ and speech tokens $T_l \\\\in \\\\mathbb{R}^{M \\\\times C}$. Then, the refined tokens is conducted an element-wise addition with instruction tokens:\\n\\n$$P'_l = P_l + I \\\\in \\\\mathbb{R}^{K \\\\times C}.$$ \\n\\nSuppose the model is processing with the speech tokens $T_l$ and refined tokens $P'_l$, The attention score related to the learnable prompt is calculated as\\n\\n$$S_p_l = \\\\text{Attention}(T_l, P'_l, P'_l) = \\\\text{Softmax}(T_l P'_l T / \\\\sqrt{C}) P'_l,$$\\n\\nwe have $S_t$ self-attention score for original speech tokens. We consider a learnable gating factor (Zhang et al., 2023a) and inject the encoded adaptation prompts to different Transformer layers, gradually providing instruction semantics to avoid disturbing the speech tokens at the beginning of training. A learnable gating factor $g_l$ is adapted to adaptively control the importance of $S_p_l$ in the attention with $S_l = S_p_l g_l + S_t$, which represents how much information the learnable prompt contributes. Initialized by zero, $g_l$ can first eliminate the influence of under-fitted prompts and then increase its magnitude to provide more instruction semantics. We compare different adaptation methods in Section 7.3.\\n\\n4.5. Multi-step reasoning\\n\\nIn this section, we investigate the multi-step reasoning capabilities in InstructSpeech. Multi-step reasoning includes a step-by-step thought process for arriving at the answer, where solutions typically come before the final answer. Specifically, we use multi-step reasoning for free-form semantic editing tasks to perform \u201cadd\u201d, \u201cremove\u201d and \u201creplace\u201d operations following the user's instruction without a predefined mask region. For example, when being asked \u201cDelete the word sunny\u201d, InstructSpeech decomposes the problems into intermediate steps as shown in Algorithm 1. As such, InstructSpeech tackles the challenges of accurately locating and manipulating the target context following the user's instruction. We refer the reader to Section 7.2 for a summary of our findings.\\n\\nAlgorithm 1 Multi-step reasoning for free-form editing.\\n\\nWe use $E_{ASR}$, $E_{Dur}$, $E_I$ respectively to denote the task embedding of automatic speech recognition, frame-level duration prediction, and task categories prediction tasks.\\n\\n1: Input: InstructSpeech $\\\\theta$, tuned Flan-T5-XL $\\\\alpha$, speech before edit $x$, instruction $c$, task embedding matrix $E$\\n2: Predict phone $y = \\\\theta(E_{ASR}, x)$.\\n3: Predict phone frame-level duration alignment $d_{MFA} = \\\\theta(E_{Dur}, x, y)$.\\n4: Predict task categories $I = \\\\alpha(c)$.\\n5: Obtain masked speech $x_m$ given predicted alignment $d_{MFA}$ and derive phones to be edited $\\\\tilde{y}$ from instruction $c$.\\n6: Deteriorating to region-based semantic content editing: $\\\\tilde{x} = \\\\theta(E_I, x_m, \\\\tilde{y})$.\\n7: RETURN $\\\\tilde{x}$\\n\\n4.6. Reconstructing High-Fidelity Waveforms\\n\\nWe train a unit-based neural vocoder from scratch for the acoustic unit to waveform generation. Inspired by BigV-GAN (Lee et al., 2022), the synthesizer includes the generator and multi-resolution discriminator (MRD). The generator is built from a set of look-up tables (LUT) that embed the discrete representation and a series of blocks composed of transposed convolution and a residual block with dilated layers. The transposed convolutions upsample the encoded representation to match the input sample rate. Details are included in Appendix D.1.\\n\\n5. Evaluating Instruction-guided Speech Editing Models\\n\\nWe create the benchmark to evaluate instruction-guided speech editing models. Specifically, for each pair of input speech and editing instructions, we use the following the metrics:\\n\\n- Speech intelligibility. We report word error rate (WER) or phone error rate (PER) to evaluate the intelligibility of speech by transcribing it using a whisper (Radford et al., 2023) ASR system following (Wang et al., 2023a).\\n- Style similarity. SIM assesses the coherence of the generated speech in relation to the speaker's characteristics, and we employ the speaker verification model WavLM-TDNN (Chen et al., 2022) to evaluate the speaker similarity. F0 Frame Error (FFE) measures the prosody similarity of synthesized and reference audio. For emotion and style, we train the classifiers to recognize the categories of output speech with GE2E loss (Wan et al., 2018), which measures if the model can accurately produce the target style or emotion given instruction. For pitch, speaking speed, and volume, we adopt a soft-margin mechanism for accuracy calculation.\"}"}
{"id": "xlWcdtCyOC", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\n1. Clarity is key in speech editing, as even minor changes can significantly improve the quality of the output.\\n2. Precise and clear instructions are essential for ensuring that the edits are understood and applied correctly.\\n3. A longer sequence length typically requires more computational resources, and degradation could be witnessed with decreased training data.\"}"}
{"id": "xlWcdtCyOC", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nHsu, W.-N., Bolte, B., Tsai, Y.-H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451\u20133460, 2021.\\n\\nKahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazar\u00e9, P.-E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., et al. Libri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7669\u20137673. IEEE, 2020.\\n\\nKong, J., Kim, J., and Bae, J. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Proc. of NeurIPS, 2020.\\n\\nLe, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz, R., Williamson, M., Manohar, V., Adi, Y., Mahadeokar, J., et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems, 36, 2024.\\n\\nLee, S.-g., Ping, W., Ginsburg, B., Catanzaro, B., and Yoon, S. Bigvgan: A universal neural vocoder with large-scale training. arXiv preprint arXiv:2206.04658, 2022.\\n\\nLeng, Y., Guo, Z., Shen, K., Tan, X., Ju, Z., Liu, Y., Liu, Y., Yang, D., Zhang, L., Song, K., et al. Prompttts 2: Describing and generating voices with text prompt. arXiv preprint arXiv:2309.02285, 2023.\\n\\nLiu, A. H., Le, M., Vyas, A., Shi, B., Tjandra, A., and Hsu, W.-N. Generative pre-training for speech with flow matching. arXiv preprint arXiv:2310.16338, 2023.\\n\\nMcAuliffe, M., Socolof, M., Mihuc, S., Wagner, M., and Sonderegger, M. Montreal forced aligner: Trainable text-speech alignment using kaldi. In Interspeech, volume 2017, pp. 498\u2013502, 2017.\\n\\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038, 2019.\\n\\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5206\u20135210. IEEE, 2015.\\n\\nPopov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudinov, M. Grad-tts: A diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pp. 8599\u20138608. PMLR, 2021.\\n\\nQian, K., Zhang, Y., Chang, S., Yang, X., and Hasegawa-Johnson, M. Autovc: Zero-shot voice style transfer with only autoencoder loss. In International Conference on Machine Learning, pp. 5210\u20135219. PMLR, 2019.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748\u20138763. PMLR, 2021.\\n\\nRadford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pp. 28492\u201328518. PMLR, 2023.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367, 2020.\\n\\nSheynin, S., Polyak, A., Singer, U., Kirstain, Y., Zohar, A., Ashual, O., Parikh, D., and Taigman, Y. Emu edit: Precise image editing via recognition and generation tasks. arXiv preprint arXiv:2311.10089, 2023.\\n\\nSun, H., Tan, X., Gan, J.-W., Liu, H., Zhao, S., Qin, T., and Liu, T.-Y. Token-level ensemble distillation for grapheme-to-phoneme conversion. arXiv preprint arXiv:1904.03446, 2019.\\n\\nTae, J., Kim, H., and Kim, T. Editts: Score-based editing for controllable text-to-speech. arXiv preprint arXiv:2110.02584, 2021.\\n\\nTan, D., Deng, L., Yeung, Y. T., Jiang, X., Chen, X., and Lee, T. Editspeech: A text based speech editing system using partial inference and bidirectional fusion. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 626\u2013633. IEEE, 2021.\\n\\nVeaux, C., Yamagishi, J., MacDonald, K., et al. Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 6:15, 2017.\\n\\nVyas, A., Shi, B., Le, M., Tjandra, A., Wu, Y.-C., Guo, B., Zhang, J., Zhang, X., Adkins, R., Ngan, W., et al. Audiobox: Unified audio generation with natural language prompts. arXiv preprint arXiv:2312.15821, 2023.\\n\\nWan, L., Wang, Q., Papir, A., and Moreno, I. L. Generalized end-to-end loss for speaker verification. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4879\u20134883. IEEE, 2018.\\n\\nWang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., et al. Neural codec.\"}"}
{"id": "xlWcdtCyOC", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nLanguage models are zero-shot text to speech synthesizers. (Wang et al., 2023a)\\n\\nWang, Y., Ju, Z., Tan, X., He, L., Wu, Z., Bian, J., and Zhao, S. Audit: Audio editing by following instructions with latent diffusion models. (Wang et al., 2023b)\\n\\nYang, D., Liu, S., Huang, R., Lei, G., Weng, C., Meng, H., and Yu, D. Instructts: Modelling expressive tts in discrete latent space with natural language style prompt. (Yang et al., 2023a)\\n\\nYang, D., Tian, J., Tan, X., Huang, R., Liu, S., Chang, X., Shi, J., Zhao, S., Bian, J., Wu, X., et al. Uniaudio: An audio foundation model toward universal audio generation. (Yang et al., 2023b)\\n\\nYu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. Megabyte: Predicting million-byte sequences with multiscale transformers. (Yu et al., 2023)\\n\\nZeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. Soundstream: An end-to-end neural audio codec. (Zeghidour et al., 2021)\\n\\nZen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. Libritts: A corpus derived from librispeech for text-to-speech. (Zen et al., 2019)\\n\\nZhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. (Zhang et al., 2023a)\\n\\nZhang, Z., Zhou, L., Wang, C., Chen, S., Wu, Y., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., et al. Speak foreign languages with your own voice: Cross-lingual neural codec language modeling. (Zhang et al., 2023b)\\n\\nZhou, K., Sisman, B., Liu, R., and Li, H. Emotional voice conversion: Theory, databases and esd. (Zhou et al., 2022)\"}"}
{"id": "xlWcdtCyOC", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nAppendices\\n\\nA. Data\\n\\nSemantic Editing\\nAdd/Remove/Replace: LibriTTS, VCTK\\n\\nAcoustic Editing\\nStyle: LibriTTS-style\\nEmotion: ESD\\nSpeed/V olume/Gender: LibriTTS\\nBackground sound: Audioset\\n\\nSpeech tasks\\nTTS: VCTK, LibriTTS\\nVC: Librilight\\nASR: LibriSpeech\\n\\nTable 7: Original data to construct instruction speech editing dataset.\\n\\nB. Instruction speech editing dataset construction\\n\\nTo construct instruction speech editing datasets, we train speech LLMs to synthesize speech samples (after editing) that align well with human instruction, where we use the same training objective and architecture as InstructSpeech. Specifically, we 1) tokenize speech samples and construct sequence for in-context learning. 2) build the architecture upon end-to-end differentiable multiscale transformers to predict long sequences as described in Section 4.3; and 3) train the model with a language modeling objective (i.e., next-token prediction task) with the same configurations in Section 6.2.\\n\\nAs illustrated in Figure (a), given a training sample (phone \\\\( a \\\\) and speech \\\\( b \\\\) pair) in the ESD dataset, the overall token sequence includes 1) phones \\\\( a \\\\) or Hubert tokens \\\\([5]\\\\) of \\\\( b \\\\), where we use k-means to discretize 12th-layer embeddings into Hubert tokens with a codebook of size 1000; 2) emotion prompt (acoustic tokens of a randomly chosen sample with the same emotion as \\\\( b \\\\)\u2019s), 3) speaker prompt (acoustic tokens of a randomly chosen sample with the same speaker as \\\\( b \\\\)\u2019s), and 4) acoustic tokens of \\\\( b \\\\).\\n\\nDuring inference, the model can be prompted with phones/Hubert tokens and emotion, speaker prompts to generate the target with acoustic attributes to be coherent with the prompt. For example, given a sample and instruction with \u201cconvert it into happy emotion\u201d, we take its phone or Hubert tokens as input, and utilize a randomly chosen \u201chappy\u201d sample as the emotion prompt, and a randomly chosen sample with the same timbre as the speaker prompt, to synthesize speech samples (after editing) that align well with human instruction.\\n\\nLibrilight is included to promote the data scale. As illustrated in Figure 2 (a), given a training speech, the overall token sequence becomes 1) Hubert tokens \\\\([5]\\\\), 2) emotion prompt (acoustic tokens of a randomly chosen Librilight sample to denote the Neutral emotion), 3) speaker prompt (acoustic tokens of a randomly chosen sample with the same speaker), and 4) acoustic target. Inspired by \\\\([4]\\\\), we combine both unsupervised and supervised datasets in speech generative models for better generalization to unseen speaking style prompts, in a zero-shot fashion.\\n\\nC. Contrastive instruction-speech pretraining (CISP)\\n\\nThe CISP model jointly trains audio and text encoder to learn a common multimodal space using contrastive learning. Let the training data be \\\\( D = \\\\{ (X_{ai}, X_{ti}) \\\\}_{i=1}^{N} \\\\). Let \\\\( f_{audio} \\\\) be the audio encoder and \\\\( f_{text} \\\\) be the text encoder which are learnable embedding functions. The audio encoder converts the raw audio into a log Mel spectrogram followed by a learnable\"}"}
{"id": "xlWcdtCyOC", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"InstructSpeech: Following Speech Editing Instructions via Large Language Models\\n\\nThe model is trained with the contrastive learning paradigm between the audio embedding function $E_{a}$ and text embeddings $E_{t}$ in pair:\\n\\n$$E_{a_{i}} = MLP_{audio}(f_{audio}(X_{a_{i}}))$$\\n\\n$$E_{t_{i}} = MLP_{text}(f_{text}(X_{t_{i}}))$$\\n\\n$$L = \\\\frac{1}{2N} \\\\sum_{i=1}^{N} \\\\log \\\\exp \\\\left( \\\\frac{E_{a_{i}} \\\\cdot E_{t_{i}}}{\\\\tau} \\\\right)$$\\n\\n$$+ \\\\log \\\\exp \\\\left( \\\\frac{E_{t_{i}} \\\\cdot E_{a_{i}}}{\\\\tau} \\\\right)$$\\n\\nWhere $\\\\tau$ is a learnable temperature parameter for scaling the loss, and $N$ is the number of data. Following (Radford et al., 2021; Elizalde et al., 2023), two logarithmic terms consider either audio-to-text logits or text-to-audio logits.\\n\\n**Figure 5:** The contrastive instruction-speech pretraining process.\\n\\n**D. Model Architectures**\\n\\nIn this section, we list the model hyper-parameters of InstructSpeech in Table 8.\\n\\n| Hyperparameter                      | MV oice | Global Base | Global Medium | Global Large | Local | BigVGAN Vocoder |\\n|-------------------------------------|---------|-------------|---------------|--------------|-------|----------------|\\n| Transformer Layer                   | 16      | 20          | 24            | 24           | 6     | Same as global |\\n| Transformer Embed Dim               | 768     | 1152        | 1536          | 1536         | Same as global |\\n| Transformer Attention Headers       | 12      | 16          | 32            | 32           | 8     |\\n| Number of Parameters                | 114 M   | 320 M       | 930 M         | 930 M        | 46/101/303 M |\\n| Upsample Rates                      |         |             |               |              |       |\\n| Hop Size                            |         |             |               |              |       |\\n| Upsample Kernel Sizes               |         |             |               |              |       |\\n| Number of Parameters                |         |             |               |              | 121.6M |\\n\\n**Table 8:** Hyperparameters of InstructSpeech.\"}"}
