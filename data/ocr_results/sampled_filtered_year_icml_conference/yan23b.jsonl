{"id": "yan23b", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporally Consistent Transformers for Video Prediction\\n\\nA Sampling Process\\n\\nB Samples\\n\\nC Performance versus Horizon\\n\\nD Performance versus Training Sequence Length\\n\\nE Sampling Time\\n\\nF Ablations\\n\\nG Metrics During Training\\n\\nH High Quality Spatio-Temporal Compression\\n\\nI Trade-off Between Fidelity and Learning Long-Range Dependencies\\n\\nJ Full Experimental Results\\n\\nK Scaling Results\\n\\nL Related Work\\n\\nM Dataset Details\\n\\nN Hyperparameters\"}"}
{"id": "yan23b", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Sampling Process\\n\\nGiven a sequence of conditioning frames, $o_1, \\\\ldots, o_t$, we encode each frame using the pretrained VQ-GAN to produce $x_1, \\\\ldots, x_t$, and then use the conditional encoder to compute $z_1, \\\\ldots, z_t$. In order to generate the next frame, we use the temporal transformer to compute $h_t$, and feed it into the MaskGit dynamics prior to predict $\\\\hat{z}_{t+1}$. Let $z_{t+1} = \\\\hat{z}_{t+1}$ and feed it through the temporal transformer and MaskGit to predict $\\\\hat{z}_{t+2}$. We repeat this process until the entire trajectory is predicted, $\\\\hat{z}_{t+1}, \\\\ldots, \\\\hat{z}_T$. In order to decode back into frames, we first decode into the VQ-GAN latents, and then decode to RGB using the VQ-GAN decoder. Note that generation can be completely done in latent space, and rendering back to RGB can be done in parallel over time once the latents for all timesteps are computed.\"}"}
{"id": "yan23b", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporally Consistent Transformers for Video Prediction\\n\\nB. Samples\\n\\nB.1. DMLab\\n\\nFigure B.1. 156 frames generated conditioned on 144 (action-conditioned)\\n\\nFigure B.2. 264 frames generated conditioned on 36 (no action-conditioning)\"}"}
{"id": "yan23b", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure B.3. 3D visualizations of the resulting generated DMLab mazes.\"}"}
{"id": "yan23b", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure B.4. 156 frames generated conditioned on 144 (action-conditioned)\\n\\nFigure B.5. 264 frames generated conditioned on 36 (action-conditioned)\"}"}
{"id": "yan23b", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure B.6. 156 frames generated conditioned on 144 (action-conditioned)\\n\\nFigure B.7. 264 frames generated conditioned on 36 (no action-conditioning)\"}"}
{"id": "yan23b", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure B.8. 80 frames generated conditioned on 20 (no top-k sampling)\\n\\nFigure B.9. 80 frames generated conditioned on 20 (with top-k sampling)\"}"}
{"id": "yan23b", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure C.1. All plots show PSNR, SSIM, and LPIPS on 150 predicted frames conditioned on 144 frames. The 144 conditioned frames are not shown on the graphs and timestep 0 corresponds to the first predicted frame.\\n\\nFigure C.1 shows PSNR, SSIM, and LPIPS as a function of prediction horizon for each dataset. Generally, each plot reflected the corresponding aggregated metrics in Table 1. For DMLab, TECO shows much better temporal consistency for the full trajectory, with Latent FDM coming in second. CW-VAE is able to retain some consistency but drops fairly quickly. Lastly, FitVid and Perceiver AR lose consistency very quickly. We see a similar trend in Minecraft, with Latent FDM coming closer in matching TECO. For Habitat, all methods generally have trouble producing consistent predictions, primarily due to the difficulty of the environment.\"}"}
{"id": "yan23b", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nTo generate accurate videos, algorithms have to understand the spatial and temporal dependencies in the world. Current algorithms enable accurate predictions over short horizons but tend to suffer from temporal inconsistencies. When generated content goes out of view and is later revisited, the model invents different content instead. Despite this severe limitation, no established benchmarks on complex data exist for rigorously evaluating video generation with long temporal dependencies. In this paper, we curate 3 challenging video datasets with long-range dependencies by rendering walks through 3D scenes of procedural mazes, Minecraft worlds, and indoor scans. We perform a comprehensive evaluation of current models and observe their limitations in temporal consistency. Moreover, we introduce the Temporally Consistent Transformer (TECO), a generative model that substantially improves long-term consistency while also reducing sampling time. By compressing its input sequence into fewer embeddings, applying a temporal transformer, and expanding back using a spatial MaskGit, TECO outperforms existing models across many metrics. Videos are available on the website: https://wilson1yan.github.io/teco\\n\\n1. Introduction\\nRecent work in video generation has seen tremendous progress (Ho et al., 2022; Clark et al., 2019; Yan et al., 2021; Le Moing et al., 2021; Ge et al., 2022; Tian et al., 2021; Luc et al., 2020) in producing high-fidelity and diverse samples on complex video data, which can largely be attributed to a combination of increased computational resources and more compute efficient high-capacity neural architectures. However, much of this progress has focused on generating short videos, where models perform well by basing their predictions on only a handful of previous frames. Video prediction models with short context windows can generate long videos in a sliding window fashion. While the resulting videos can look impressive at first sight, they lack temporal consistency. We would like models to predict temporally consistent videos \u2014 where the same content is generated if a camera pans back to a previously observed location. On the other hand, the model should imagine a new part of the scene for locations that have not yet been observed, and future predictions should remain consistent to this newly imagined part of the scene.\\n\\nPrior work has investigated techniques for modeling long-term dependencies, such as temporal hierarchies (Saxena et al., 2021) and strided sampling with frame-wise interpolation (Ge et al., 2022; Hong et al., 2022). Other methods train on sparse sets of frames selected out of long videos (Harvey et al., 2022; Skorokhodov et al., 2021; Clark et al., 2019; Saito & Saito, 2018; Yu et al., 2022), or model videos via compressed representations (Yan et al., 2021; Rakhimov et al., 2020; Le Moing et al., 2021; Seo et al., 2022; Gupta et al., 2022; Walker et al., 2021). Refer to Appendix L for more detailed discussion on related work.\\n\\nDespite this progress, many methods still have difficulty scaling to datasets with many long-range dependencies. While Clockwork-VAE (Saxena et al., 2021) trains on long...\"}"}
{"id": "yan23b", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. TECO generates sharp and consistent video predictions for hundreds of frames on challenging datasets. The figure shows evenly spaced frames of the 264 frame predictions, after being conditioned on 36 context frames. From top to bottom, the datasets are DMLab, Minecraft, Habitat, and Kinetics-600.\\n\\nIn this paper, we introduce a set of novel long-horizon video generation benchmarks, as well as corresponding evaluation metrics to better capture temporal consistency. In addition, we propose Temporally Consistent Video Transformer (TECO), a vector-quantized latent dynamics model that effectively models long-term dependencies in a compact representation space using efficient transformers.\\n\\nThe key contributions are summarized as follows:\\n\\n\u2022 To better evaluate temporal consistency in video predictions, we propose 3 video datasets with long-range dependencies including metrics, generated from 3D scenes in DMLab (Beattie et al., 2016), Minecraft (Guss et al., 2019), and Habitat (Szot et al., 2021; Savva et al., 2019).\\n\\n\u2022 We benchmark SOTA video generation models on the datasets and analyze capabilities of each in learning long-horizon dependencies.\\n\\n\u2022 We introduce TECO, an efficient and scalable video generation model that learns compressed representations to allow for efficient training and generation. We show that TECO has strong performance on a variety of difficult video prediction tasks, and is able to leverage long-term temporal context to generate high quality videos with consistency while maintaining fast sampling speed.\\n\\n2. Preliminaries\\n\\n2.1. VQ-GAN\\n\\nVQ-GAN (Esser et al., 2021; Van Den Oord et al., 2017) is an autoencoder that learns to compress data into discrete latents, consisting of an encoder $E$, decoder $G$, codebook $C$, and discriminator $D$. Given an image $x \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}$, the encoder $E$ maps $x$ to its latent representation $h \\\\in \\\\mathbb{R}^{H' \\\\times W' \\\\times D}$, which is quantized by nearest neighbors lookup in a codebook of embeddings $C = \\\\{e_i\\\\}_{i=1}^{K}$ to produce $z \\\\in \\\\mathbb{R}^{H' \\\\times W' \\\\times D}$. $z$ is fed through decoder $G$ to re-\"}"}
{"id": "yan23b", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporally Consistent Transformers for Video Prediction\\n\\n(a) Prior work on video generation models over VQ codes adopt a single spatio-temporal transformer over all codes. This is prohibitive when scaling to long sequences due to the quadratic complexity of attention.\\n\\n(b) We propose a novel and efficient architecture that aggressively downsamples in space before feeding into a temporal transformer, and then expands back out with a spatial MaskGit that is separately applied per frame. In the figure, the transformer blocks show the number of attention links. On training sequences of 300 frames, TECO sees orders of magnitude more efficiency over existing models, allowing the use of larger models for a given compute budget.\\n\\nFigure 3.\\n\\nThe architectural design of TECO. (a) Prior work on video generation models over VQ codes adopt a single spatio-temporal transformer over all codes. This is prohibitive when scaling to long sequences due to the quadratic complexity of attention. (b) We propose a novel and efficient architecture that aggressively downsamples in space before feeding into a temporal transformer, and then expands back out with a spatial MaskGit that is separately applied per frame. In the figure, the transformer blocks show the number of attention links. On training sequences of 300 frames, TECO sees orders of magnitude more efficiency over existing models, allowing the use of larger models for a given compute budget.\\n\\nConstruct x. A straight-through estimator (Bengio, 2013) is used to maintain gradient flow through the quantization step. The codebook optimizes the following loss:\\n\\n\\\\[ L_{VQ} = \\\\| \\\\text{sg}(h) - e \\\\|_2^2 + \\\\beta \\\\| h - \\\\text{sg}(e) \\\\|_2^2 \\\\] (1)\\n\\nwhere \\\\( \\\\beta = 0.25 \\\\) is a hyperparameter, and \\\\( e \\\\) is the nearest-neighbors embedding from \\\\( C \\\\). For reconstruction, VQ-GAN replaces the original \\\\( \\\\ell_2 \\\\) loss with a perceptual loss (Zhang et al., 2012), \\\\( L_{LPIPS} \\\\). Finally, in order to encourage higher-fidelity samples, patch-level discriminator \\\\( D \\\\) is trained to classify between real and reconstructed images, with:\\n\\n\\\\[ L_{GAN} = \\\\log D(x) + \\\\log(1 - D(\\\\hat{x})) \\\\] (2)\\n\\nOverall, VQ-GAN optimizes the following loss:\\n\\n\\\\[ \\\\min_{E,G,C} \\\\max_D L_{LPIPS} + L_{VQ} + \\\\lambda L_{GAN} \\\\] (3)\\n\\nwhere \\\\( \\\\lambda = \\\\| \\\\nabla_{GL} L_{LPIPS} \\\\|_2 \\\\| \\\\nabla_{GL} L_{GAN} \\\\|_2 + \\\\delta \\\\) is an adaptive weight, \\\\( G_L \\\\) is the last decoder layer, \\\\( \\\\delta = 10^{-6} \\\\), and \\\\( L_{LPIPS} \\\\) is the same distance metric described in Zhang et al. (2012).\\n\\n2.2. MaskGit\\n\\nMaskGit (Chang et al., 2022) models distributions over discrete tokens, such as produced by a VQ-GAN. It generates images with competitive sample quality to autoregressive models at a fraction of the sampling cost by using a masked token prediction objective during training. Formally, we denote \\\\( z \\\\in \\\\mathbb{Z}^{H \\\\times W} \\\\) as the discrete latent tokens representing an image. For each training step, we uniformly sample \\\\( t \\\\in [0, 1) \\\\) and randomly generate a mask \\\\( m \\\\in \\\\{0, 1\\\\}^{H \\\\times W} \\\\) with \\\\( N = \\\\lceil \\\\gamma HW \\\\rceil \\\\) masked values, where \\\\( \\\\gamma = \\\\cos \\\\frac{\\\\pi}{2} t \\\\).\\n\\nThen, MaskGit learns to predict the masked tokens with the following objective:\\n\\n\\\\[ L_{\\\\text{mask}} = -\\\\mathbb{E}_{z \\\\in D} \\\\log p(z | z \\\\odot m) \\\\] (4)\\n\\nDuring inference, because MaskGit has been trained to model any set of unconditional and conditional probabilities, we can sample any subset of tokens per sampling iteration. (Chang et al., 2022) introduces a confidence-based sampling mechanism whereas other work (Lee et al., 2022) proposes an iterative sample-and-revise approach.\\n\\n3. TECO\\n\\nWe present Temporally Consistent Video Transformer (TECO), a video generation model that more efficiently scales to training on longer horizon videos.\\n\\n3.1. Architectural Overview\\n\\nOur proposed framework is shown in Figure 3, where \\\\( x_1:T \\\\) consists of a sequence of video frames. Our primary innovation centers around designing a more efficient architecture that can scale to long sequences. Prior SOTA methods (Yan et al., 2021; Ge et al., 2022; Villegas et al., 2022) over VQ-\"}"}
{"id": "yan23b", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporally Consistent Transformers for Video Prediction\\n\\ncodes all train a single spatio-temporal transformer to model every code, however, this becomes prohibitively expensive with sequences containing tens of thousands of tokens. On the other hand, these models have shown to be able to learn highly multi-modal distributions and scale well on complex video. As such, we design the TECO architecture with the intention to retain its high-capacity scaling properties, while ensuring orders of magnitude more efficient training and inference. In the following sections, we motivate each component for our model, with several specific design choices to ensure efficiency and scalability.\\n\\nTECO consists of four components:\\n\\n**Encoder:**\\n\\n\\\\[ z_t = E(x_t, x_{t-1}) \\\\]\\n\\n**Temporal Transformer:**\\n\\n\\\\[ h_t = H(z_{\\\\leq t}) \\\\]\\n\\n**Spatial MaskGit:**\\n\\n\\\\[ p(z_t | h_{t-1}) \\\\]\\n\\n**Decoder:**\\n\\n\\\\[ p(x_t | z_t, h_{t-1}) \\\\]\\n\\nEncoder\\n\\nWe can achieve compressed representations by leveraging spatio-temporal redundancy in video data. To do this, we learn a CNN encoder\\n\\n\\\\[ z_t = E(x_t, x_{t-1}) \\\\]\\n\\nwhich encodes the current frame \\\\( x_t \\\\) conditioned on the previous frame by channel-wise concatenating \\\\( x_{t-1} \\\\), and then quantizes the output using codebook \\\\( C \\\\) to produce \\\\( z_t \\\\). We apply the VQ loss defined in Equation (1) per timestep. In addition, we \\\\( \\\\ell_2 \\\\)-normalize the codebook and embeddings to encourage higher codebook usage (Yu et al., 2021). The first frame is concatenated with zeros and does not quantize \\\\( z_1 \\\\) to prevent information loss.\\n\\nTemporal Transformer\\n\\nCompressed, discrete latents are more lossy and tend to require higher spatial resolutions compared to continuous latents. Therefore, before modeling temporal information, we apply a single strided convolution to downsample each discrete latent \\\\( z_t \\\\), where visually simpler datasets allow for more downsampling and visually complex datasets require less downsampling. Afterwards, we learn a large transformer to model temporal dependencies, and then apply a transposed convolution to upsample our representation back to the original resolution of \\\\( z_t \\\\). In summary, we use the following architecture:\\n\\n\\\\[ h_t = H(z_{<t}) = \\\\text{ConvT}(\\\\text{Transformer}(\\\\text{Conv}(z_{<t}))) \\\\]\\n\\nDecoder\\n\\nThe decoder is an upsampling CNN that reconstructs \\\\( \\\\hat{x}_t = D(z_t, h_t) \\\\), where \\\\( z_t \\\\) can be interpreted as the posterior of timestep \\\\( t \\\\), and \\\\( h_t \\\\) the output of the temporal transformer which summarizes information from previous timesteps. \\\\( z_t \\\\) and \\\\( h_t \\\\) are concatenated channel-wise and into the decoder. Together with the encoder, the decoder optimizes the following cross entropy reconstruction loss\\n\\n\\\\[ L_{\\\\text{recon}} = -\\\\frac{1}{T} \\\\sum_{t=1}^{T} \\\\log p(x_t | z_t, h_t) \\\\]\\n\\nwhich encourages \\\\( z_t \\\\) features to encode relative information between frames since the temporal transformer output \\\\( h_t \\\\) aggregates information over time, learning more compressed codes for efficient modeling over longer sequences.\\n\\nSpatial MaskGit\\n\\nLastly, we use a MaskGit (Chang et al., 2022) to model the prior, \\\\( p(z_t | h_t) \\\\). We show that using a MaskGit prior allows for not just faster but also higher quality sampling compared to an autoregressive prior. During every training iteration, we follow prior work to sample a random mask \\\\( m_t \\\\) and optimize\\n\\n\\\\[ L_{\\\\text{prior}} = -\\\\frac{1}{T} \\\\sum_{t=1}^{T} \\\\log p(z_t | z_t \\\\odot m_t) \\\\]\\n\\nwhere \\\\( h_t \\\\) is concatenated channel-wise with masked \\\\( z_t \\\\) to predict the masked tokens. During generation, we follow Lee et al. (2022), where we initially generate each frame in chunks of 8 at a time and then go through 2 revise rounds of re-generating half the tokens each time.\\n\\nTraining Objective\\n\\nThe final objective is the following:\\n\\n\\\\[ L_{\\\\text{TECO}} = L_{\\\\text{VQ}} + L_{\\\\text{recon}} + L_{\\\\text{prior}} \\\\]\\n\\n3.2. DropLoss\\n\\nWe propose DropLoss, a simple trick to allow for more scalable and efficient training (Figure 4). Due to its architecture design, TECO can be separated into two components: (1) learning temporal representations, consisting of the encoder and the temporal transformer, and (2) predicting future frames, consisting of the dynamics prior and decoder. We can increase training efficiency by dropping out random timesteps that are not decoded and thus omitted from the reconstruction loss. For example, given a video of \\\\( T \\\\) frames, we compute \\\\( h_t \\\\) for all \\\\( t \\\\in \\\\{1, \\\\ldots, T\\\\} \\\\), and then compute the losses \\\\( L_{\\\\text{prior}} \\\\) and \\\\( L_{\\\\text{recon}} \\\\) for only 10% of the indices. Because random indices are selected each iteration, the model still needs to learn to accurately predict all timesteps. This reduces training costs significantly because the decoder and dynamics prior require non-trivial computations. DropLoss is applicable to both a wide class of architectures and to tasks beyond video prediction.\"}"}
{"id": "yan23b", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporally Consistent Transformers for Video Prediction\\n\\nFigure 5. Quantitative comparisons between TECO and baseline methods in long-horizon temporal consistency, showing LPIPS between generated and ground-truth frames for each timestep. Timestep 0 corresponds to the first predicted frame (conditioning frames are not included in the plot). Our method is able to remain more temporally consistent over hundreds of timesteps of prediction compared to SOTA models.\\n\\n4. Experiments\\n\\n4.1. Datasets\\n\\nWe introduce three challenging video datasets to better measure long-range consistency in video prediction, centered around 3D environments in DMLab (Beattie et al., 2016), Minecraft (Guss et al., 2019), and Habitat (Savva et al., 2019), with videos of agents randomly traversing scenes of varying difficulty. These datasets require video prediction models to reproduce observed parts of scenes, and newly generate unobserved parts. In contrast, many existing video benchmarks do not have strong long-range dependencies, where a model with limited context is sufficient. Refer to Appendix M for further details on each dataset.\\n\\n**DMLab-40k**\\n\\nDeepMind Lab is a simulator that procedurally generates random 3D mazes with random floor and wall textures. We generate 40k action-conditioned $64 \\\\times 64$ videos of 300 frames of an agent randomly traversing $7 \\\\times 7$ mazes by choosing random points in the maze and navigating to them via the shortest path. We train all models for both action-conditioned and unconditional prediction (by periodically masking out actions) to enable both types of generations. We further discuss the use cases of both action and unconditional models in Section 4.3.\\n\\n**Minecraft-200k**\\n\\nThis popular game features procedurally generated 3D worlds that contain complex terrain such as hills, forests, rivers, and lakes. We collect 200k action-conditioned videos of length 300 and resolution $128 \\\\times 128$ in Minecraft's marsh biome. The player iterates between walking forward for a random number of steps and randomly rotating left or right, resulting in parts of the scene going out of view and coming back into view later. We train action-conditioned for all models for ease of interpreting and evaluating, though it is generally easy for video models to unconditionally learn these discrete actions.\\n\\n**Habitat-200k**\\n\\nHabitat is a simulator for rendering trajectories through scans of real 3D scenes. We compile $\\\\sim 1400$ indoor scans from HM3D (Ramakrishnan et al., 2021), MatterPort3D (Chang et al., 2017), and Gibson (Xia et al., 2018) to generate 200k action-conditioned videos of 300 frames at a resolution of $128 \\\\times 128$ pixels. We use Habitat's in-built path traversal algorithm to construct action trajectories that move our agent between randomly sampled locations. Similar to DMLab, we train all video models to perform both unconditional and action-conditioned prediction.\\n\\n**Kinetics-600**\\n\\nKinetics-600 (Carreira & Zisserman, 2017) is a highly complex real-world video dataset, originally proposed for action recognition. The dataset contains $\\\\sim 400k$ videos of varying length of up to 300 frames. We evaluate our method in the video prediction without actions (as they do not exist), generating 80 future frames conditioned on 20. In addition, we filter out videos shorter than 100 frames, leaving 392k videos that are split for training and evaluation. We use a resolution of $128 \\\\times 128$ pixels. Although Kinetics-600 does not have many long-range dependencies, we evaluate our method on this dataset to show that it can scale to complex, natural video.\\n\\n4.2. Baselines\\n\\nWe compare against SOTA baselines selected from several different families of models: latent-variable-based variational models, autoregressive likelihood models, and diffusion models. In addition, for efficiency, we train all models on VQ codes using a pretrained VQ-GAN for each dataset. For our diffusion baseline, we follow (Rombach et al., 2022) and use a VAE instead of a VQ-GAN. Note that we do not have any GANs for our baselines, since to the best of our knowledge, there does not exist a GAN that trains on latent space instead of raw pixels, an important aspect for properly scaling to long video sequences.\\n\\n**Space-time Transformers**\\n\\nWe compare TECO to several variants of space-time transformers as depicted in Figure 3: VideoGPT (Yan et al., 2021) (autoregressive over space-time), Phenaki (Villegas et al., 2022) (MaskGit over space-time full attention), MaskViT (Gupta et al., 2022) (MaskGit over space-time axial attention), and Hourglass transformers (Nawrot et al., 2021) (hierarchical autoregressive over space-time). Note that we do not include the text-conditioning for Phenaki as it is irrelevant in our case. We only evaluate these models on DMLab, as Table 2 and Table 1 show that Perceiver-AR (a space-time transformer with improvements specifically for learning long dependencies) is a stronger baseline.\"}"}
{"id": "yan23b", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative evaluation on all four datasets. Detailed results in Appendix J.\\n\\n| Method           | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | FVD \u2193 |\\n|------------------|--------|--------|---------|-------|\\n| DMLab Minecraft  |        |        |         |       |\\n| FitVid           | 176    | 12     | 0.356   | 0.491 |\\n| CW-V AE          | 125    | 12     | 0.372   | 0.304 |\\n| Perceiver AR     |        |        |         |       |\\n| Latent FDM       | 181    | 17     | 0.588   | 0.222 |\\n| TECO (ours)      | 48     | 21     | 0.703   | 0.157 |\\n\\nFitVid (Babaeizadeh et al., 2021) is a state-of-the-art variational video model based on CNNs and LSTMs that scales to complex video by leveraging efficient architectural design choices in its encoder and decoder.\\n\\nClockwork V AE\\nCW-V AE (Saxena et al., 2021) is a variational video model that is designed to learn long-range dependencies through a hierarchy of latent variables with exponentially slower tick speeds for each new level.\\n\\nPerceiver AR\\nWe use Perceiver AR (Hawthorne et al., 2022) as our AR baseline over VQ-GAN discrete latents, which has been shown to be an effective generative model that can efficiently incorporate long-range sequential dependencies. Conceptually, this baseline is similar to HARP (Seo et al., 2022) with a Perceiver AR as the prior instead of a sparse transformer (Child et al., 2019). We choose Perceiver AR over other autoregressive baselines such as VideoGPT (Yan et al., 2021) or TATS (Ge et al., 2022) primarily due to the prohibitive costs of transformers when applied to tens of thousands of tokens.\\n\\nLatent FDM\\nWe train a Latent FDM model for our diffusion baseline. Although FDM (Harvey et al., 2022) is originally trained on pixel observations, we also train in latent space for a more fair comparison with our method and other baselines, as training on long sequences in pixel space is too expensive. We follow LDM (Rombach et al., 2022) to separately train an autoencoder to encode each frame into a set of continuous latents.\\n\\n4.3. Experimental Setup\\n\\nTraining\\nAll models are trained for 1 million iterations under fixed compute budgets allocated for each dataset (measured in TPU-v3 days) on TPU-v3 instances ranging from v3-8 to v3-128 TPU pods (similar to 4 V100s to 64 V100s) with training times of roughly 3-5 days. For DMLab, Minecraft, and Habitat we train all models on full 300 frames videos, and 100 frames for Kinetics-600. Our VQ-GANs are trained on 8 A5000 GPUs, taking about 2-4 days for each dataset, and downsample all videos to 16 \u00d7 16 grids of discrete latents per frame regardless of original video resolution. More details on exact hyperparameters and compute budgets for each dataset can be found in Appendix N.\\n\\nEvaluation\\nStandard methods for evaluating video prediction quality (FVD (Unterthiner et al., 2019) or per-frame metrics PSNR (Huynh-Thu & Ghanbari, 2008), SSIM (Wang et al., 2004), and LPIPS (Zhang et al., 2012)) do not measure long-consistency well. FVD is more sensitive to image fidelity, and relies on an I3D network trained on short Kinetics-600 clips. Evaluations using PSNR, SSIM, and LPIPS generally require sampling hundreds of futures and compare the sample that most accurately matches ground-truth. However, this does not align well with the goal of temporal consistency, as we would like the model to deterministically re-generate observed parts of the environment, and not accidentally generate the correct future after many samples. Therefore, we propose a modified evaluation metric using PSNR, SSIM, and LPIPS that better measures temporal consistency in video generation by leveraging sufficient conditioning. Intuitively, if a video model is conditioned with\"}"}
{"id": "yan23b", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. 3D visualization of predicted trajectories in DMLab for each model, generating 156 frames conditioned on 144. TECO is the only model that retains maze consistency with ground-truth, whereas baselines tend to extend out of the maze or create fictitious corridors that did not exist.\\n\\nVideo predictions use only the first-person RGB frames. Refer to Appendix M.1 for more details on 3D evaluation. A video corresponding to this figure is available at: https://wilson1yan.github.io/teco.\\n\\nEnough information, future predictions should be approximately deterministic, meaning that only one sample should be needed to expect an accurate match with ground-truth. In the case of 3D environments, we can approximately make generation deterministic by conditioning on past frames (after the model has already seen most of the 3D environment) and actions (to remove stochasticity of movement). As such, for DMLab, Minecraft, and Habitat, we condition on 144 past frames as well as actions, and measure PSNR, SSIM, and LPIPS with 156 future ground-truth frames. However, note that per-frame metrics only capture temporal consistency, and do not capture a video model's ability to model the stochasticity of video data. Therefore, we also compute FVD on 300 frame videos, conditioned on 36 frames (264 predicted frames). For Kinetics-600, we evaluate FVD on 100 frame videos, conditioned on 20 frames (80 predicted frames). We compute all metrics over batches of 256 examples, averaged over 4 runs to make 1024 total samples.\\n\\n4.4. Benchmark Results\\n\\nDMLab & Minecraft\\n\\nTable 1 shows quantitative results on the DMLab and Minecraft datasets. TECO performs the best across all metrics for both datasets when training on the full 300 frame videos. Figure 6 shows sample trajectories and 3D visualizations of the generated DMLab mazes, where TECO is able to generate more consistent 3D mazes.\"}"}
{"id": "yan23b", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For both datasets, CW-V AE, FitVid, and Perceiver AR can produce sharp predictions, but do not model long-horizon context well, with per-frame metrics sharply dropping as the future prediction horizon increases as seen in Figure C.1. Latent FDM has consistent predictions, but high FVD most likely due to FVD being sensitive to high frequency errors.\\n\\nTable 1 shows results for our Habitat dataset. We only evaluate our strongest baselines, Perceiver AR and Latent FDM due to the need to implement model parallelism. Because of high complexity of Habitat videos, all models generally perform equally as bad in per-frame metrics. However, TECO has significantly better FVD. Qualitatively, Latent FDM quickly collapses to blurred predictions with poor sample quality, and Perceiver AR generates high quality frames, though less temporally consistent than TECO: agents in Habitat videos navigate to far points in the scene and back whereas Perceiver AR tends to generate samples where the agent constantly turns. TECO generates traversals of a scene that match the data distribution more closely.\\n\\nTable 1 shows FVD for predicting 80 \\\\(128 \\\\times 128\\\\) frames conditioned on 20 for Kinetics-600. Although Kinetics-600 does not have many long-range dependencies, we found that TECO is able to produce more stable generations that degrade slower by incorporating longer contexts. In contrast, Perceiver AR tends to degrade quickly, with Latent FDM performing in between.\\n\\nSampling Speed\\n\\nFigure 5 reports sampling speed for all models on Minecraft. We observed similar results for the different model sizes used on other datasets. FitVid and CW-V AE are both significantly faster than other methods, but have poor sample quality. On the other end, Perceiver AR and Latent FDM can produce high quality samples, but are 20-60x slower than TECO, which has comparably fast sampling speed while retaining high sample quality.\\n\\n4.5. Ablations\\n\\nIn this section, we perform ablations on various architectural decisions of our model. For simplicity, we evaluate our methods on short sequences of 16 frames from Something-Something-v2 (SSv2), as it provides insight into scaling our method on complex real-world data more similar to Kinetics-600 while being computationally cheaper to run. Details can be found in the Appendix, Table F.1. We demonstrate that using VQ-latent dynamics with a MaskGit prior outperforms other formulations for latent dynamics models such as variational methods. In addition, we show that conditional encodings learn better representations for video predictions. We also ablate the codebook size, showing that although there exists an optimal codebook size, it does not matter too much as along as there are not too many codes, which may the prior more difficult to learn. Lastly, we show the benefits of DropLoss, with up to 60% faster training and a minimal increase in FVD. The benefits are greater for longer sequences, and allow video models to better account for long horizon context with little cost in performance.\\n\\n4.6. Further Insights\\n\\nWe highlight a few key experimental insights for designing long-horizon video generation models. Further details can be found in Appendix I and Appendix G.\\n\\nTrade-off between fidelity and learning long-range dependencies\\n\\nGiven a network with fixed capacity, there exists an inherent trade-off between generating high fidelity and temporally consistent videos. We find that long-horizon information can be prioritized through bottlenecking representations, whereas allocating more computation towards higher resolution representations encourages higher fidelity. Due to TECO learning more compact representations, it achieves a better trade-off between fidelity and temporal consistency compared to our baseline models, demonstrated by better PSNR / SSIM / LPIPS, in addition to FVD. Although frame quality saturates early-on, long-term consistency improves when training longer.\\n\\nDuring training, we observe an interesting phenomenon where short-horizon metrics tend to saturate earlier on during training, while long-horizon metrics continue to improve until end of training. We hypothesize that this may be due to the likelihood objective, where modeling bits from neighboring frames is easier than learning long-horizon bits scattered throughout the video. This finding motivates the use of an efficient video architecture for TECO, which can be trained for more gradient steps given a fixed computational budget.\\n\\n5. Discussion\\n\\nWe introduced TECO, an efficient video prediction model that leverages hundreds of frames of temporal context, as well as a comprehensive benchmark to evaluate long-horizon consistency. Our evaluation demonstrated that TECO accurately incorporates long-range context, outperforming SOTA baselines across a wide range of datasets. In addition, we introduce several difficult video datasets, which we hope make it easier to evaluate temporal consistency in future video prediction models. We identify several limitations as directions for future work:\\n\\n- Although we show that PSNR, SSIM, and LPIPS can be reliable metrics to measure consistency when video models are properly conditioned, there remains room for better evaluation metrics that provide a reliable signal as the prediction horizon grows, since new parts of a scene that are generated are unlikely to correlate with ground truth.\\n- Our focus was on learning a compressed tokens and an expressive prior, which we combined with a simple full\"}"}
{"id": "yan23b", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"## Temporally Consistent Transformers for Video Prediction\\n\\n### D. Performance versus Training Sequence Length\\n\\n| Train Sequence Length (Fewer FLOPs Per Frame) | TECO (ours) | Latent FDM | Perceiver-AR | CW-VAE | FitVid |\\n|---------------------------------------------|------------|------------|--------------|--------|--------|\\n| 50                                          | FVD        | FVD        | FVD          | FVD    | FVD    |\\n| 100                                         | FVD        | FVD        | FVD          | FVD    | FVD    |\\n| 150                                         | FVD        | FVD        | FVD          | FVD    | FVD    |\\n| 200                                         | FVD        | FVD        | FVD          | FVD    | FVD    |\\n| 250                                         | FVD        | FVD        | FVD          | FVD    | FVD    |\\n| 300                                         | FVD        | FVD        | FVD          | FVD    | FVD    |\\n\\n| LPIPS ( ) vs Train Sequence Length |\\n|-----------------------------------|\\n| TECO (ours) | Latent FDM | Perceiver-AR | CW-VAE | FitVid |\\n| 0.2         | 0.3        | 0.4          | 0.5    | 0.6    |\\n\\n| PSNR ( ) vs Train Sequence Length |\\n|-----------------------------------|\\n| TECO (ours) | Latent FDM | Perceiver-AR | CW-VAE | FitVid |\\n| 12.5        | 15.0       | 17.5         | 20.0   | 22.5   |\\n\\n| SSIM ( ) vs Train Sequence Length |\\n|-----------------------------------|\\n| TECO (ours) | Latent FDM | Perceiver-AR | CW-VAE | FitVid |\\n| 0.3         | 0.4        | 0.5          | 0.6    | 0.7    |\\n\\nFigure D.1. DMLab 21\"}"}
{"id": "yan23b", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Figure D.2. Figure D.1 and Figure D.2 show plots comparing performance with training models on different sequence lengths. Under a fixed compute budget and batch size, training on shorter videos enables us to scale to larger models. This can also be interpreted as model capacity or FLOPs allocated per image. In general, training on shorter videos enables higher quality frames (per-image) but at a cost of worse temporal consistency due to reduced context length. We can see a very clear trend in DMLab, in that TECO is able to better scale on longer sequences, and correspondingly benefits from it. Latent FDM has trouble when training on full sequences. We hypothesize that this may be due to diffusion models being less amenable towards downsamples, it it needs to model and predict noise. In Minecraft, we see the best performance at around 50-100 training frames, where a model has higher fidelity image predictions, and also has sufficient context.\"}"}
{"id": "yan23b", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Temporally Consistent Transformers for Video Prediction\\n\\n**E. Sampling Time**\\n\\n| Sampling Time per Frame (s) | FVD | TECO | Latent FDM | Perceiver AR | CW-VAE | Fitvid |\\n|-----------------------------|-----|------|------------|--------------|--------|--------|\\n| 200                         |     |      |            |              |        |        |\\n| 400                         |     |      |            |              |        |        |\\n| 600                         |     |      |            |              |        |        |\\n| 800                         |     |      |            |              |        |        |\\n| 1000                        |     |      |            |              |        |        |\\n\\n#### FVD ( ) vs Sampling Time\\n\\n![Graph showing FVD performance vs sampling time]\\n\\n### LPIPS ( ) vs Sampling Time\\n\\n| Sampling Time per Frame (s) | FVD | TECO | Latent FDM | Perceiver AR | CW-VAE | Fitvid |\\n|-----------------------------|-----|------|------------|--------------|--------|--------|\\n| 0.35                        |     |      |            |              |        |        |\\n| 0.40                        |     |      |            |              |        |        |\\n| 0.45                        |     |      |            |              |        |        |\\n| 0.50                        |     |      |            |              |        |        |\\n\\n#### MINE vs Sampling Time\\n\\n- **TECO (ours)**: 186 ms\\n- **Latent FDM**: 3606 ms\\n- **Perceiver-AR**: 8443 ms\\n- **CW-VAE**: 0.062 s\\n- **FitVid**: 0.074 s\\n\\n### F. Ablations\\n\\n**DropLoss Rate FVD Train Step (ms)**\\n\\n| DropLoss Rate | TECO (ours) | Latent FDM | Perceiver-AR | CW-VAE | FitVid |\\n|---------------|-------------|------------|--------------|--------|--------|\\n| 0.8           | 187         | 125        |              |        |        |\\n| 0.6           | 186         | 143        |              |        |        |\\n| 0.4           | 184         | 155        |              |        |        |\\n| 0.2           | 184         | 167        |              |        |        |\\n| 0.0           | 182         | 182        |              |        |        |\\n\\n**Posteriors FVD**\\n\\n- VQ (+ MaskGit prior) (ours): 189 ms\\n- OneHot (+ MaskGit prior): 199 ms\\n- OneHot (+ Block AR prior): 209 ms\\n- OneHot (+ Independent prior): 228 ms\\n- Argmax (+ MaskGit prior): 336 ms\\n\\n### Prior Networks\\n\\n**Dynamics Prior FVD**\\n\\n- MaskGit (ours): 189 ms\\n- Independent: 220 ms\\n- Autoregressive: 207 ms\\n\\n**Conditional Encoding FVD**\\n\\n- Yes (ours): 189 ms\\n- No: 208 ms\\n\\n**VQ Codebook Size**\\n\\n| Number of Codes | FVD Codebook Size |\\n|-----------------|-------------------|\\n| 64              | 191               |\\n| 256             | 195               |\\n| 1024            | 186               |\\n| 4096            | 200               |\"}"}
{"id": "yan23b", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table F.2. Ablations on scaling different parts of TECO.\\n\\n| Model          | TECO (ours) | MaskGit | Autoregressive |\\n|----------------|-------------|---------|----------------|\\n| PSNR           | 21.9         | 19.3    | 20.1           |\\n| SSIM           | 0.703        | 0.605   | 0.640          |\\n| LPIPS          | 0.703        | 0.605   | 0.640          |\\n| Train Step Time (ms) | 157         | 154    | 197            |\\n\\n### Table F.3. DMLab dataset comparisons against similar model as TECO without latent dynamics, and Maskgit or AR model on VQ-GAN tokens directly.\\n\\nTable F.3 shows comparisons between TECO and alternative architectures that do not use latent dynamics. Architecturally, MaskGit and Autoregressive are very similar to TECO, with a few small changes: (1) there is no CNN decoder and (2) MaskGit and AR directly predict the VQ-GAN latents (as opposed to the learned VQ latents in TECO). In terms of training time, MaskGit and AR are a little slower since they operate on $16 \\\\times 16$ latents instead of $8 \\\\times 8$ latents for TECO. In addition, conditioning for the AR model is done using cross attention, as channel-wise concatenation does not work well due to unidirectional masking. Both models without latent dynamics have worse temporal consistency, as well as overall sample quality. We hypothesize that TECO has better temporal consistency due to weak bottlenecking of latent representation, as a lot of time can be spent modeling likelihood of imperceptible image / video statistics. MaskGit shows very high FVD due to a tendency to collapse in later frames of prediction, which FVD is sensitive to.\"}"}
{"id": "yan23b", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Temporally Consistent Transformers for Video Prediction\\n\\n| Hyperparameters | DMLab Minecraft | TPU-v3 Days | Resolution | Batch Size | Learning Rate | LR Schedule | Optimizer | Warmup Steps | Total Training Steps |\\n|-----------------|-----------------|-------------|------------|------------|---------------|-------------|-----------|--------------|---------------------|\\n| CW-V AE         |                 | 80          | 64         | 32         | $1 \\\\times 10^{-4}$ | cosine      | Adam      | 10k          | 1M                  |\\n|                 |                 |             | 128        | 32         | $1 \\\\times 10^{-4}$ | cosine      | Adam      | 10k          | 1M                  |\\n|                 |                 |             |            |            |               |             |           |              |                     |\\n\\n| Encoder Kernels | 4,4,4           | 4,4,4       |\\n| Filters         | 256,512,1024    | 256,512,1024|\\n| Decoder Depths  | 256,512         | 256,512     |\\n| Blocks          | 4               | 8           |\\n| Dynamics Levels | 3               | 3           |\\n| Abs Factor      | 6               | 6           |\\n| Enc Dense Layers| 3               | 3           |\\n| Enc Dense Embed | 1024            | 1024        |\\n| Cell Stoch Size | 128             | 256         |\\n| Cell Deter Size | 1024            | 1024        |\\n| Cell Embed Size | 1024            | 1024        |\\n| Cell Min Stddev | 0.001           | 0.001       |\\n\\n### FitVid\\n\\n| Hyperparameters | DMLab Minecraft | TPU-v3 Days | Resolution | Batch Size | Learning Rate | LR Schedule | Optimizer | Warmup Steps | Total Training Steps |\\n|-----------------|-----------------|-------------|------------|------------|---------------|-------------|-----------|--------------|---------------------|\\n|                 |                 | 80          | 64         | 32         | $1 \\\\times 10^{-4}$ | cosine      | Adam      | 10k          | 1M                  |\\n|                 |                 |             | 128        | 32         | $1 \\\\times 10^{-4}$ | cosine      | Adam      | 10k          | 1M                  |\\n|                 |                 |             |            |            |               |             |           |              |                     |\\n|                 |                 |             |            |            |               |             |           |              |                     |\\n\\n| g Dim           | 256             | 256         |\\n| RNN Size        | 512             | 768         |\\n| z Dim           | 64              | 128         |\\n| Filters         | 128,128,256,512 | 128,128,256,512 |\\n\\n### Tables\\n\\n| Hyperparameters for CW-V AE |\\n|-----------------------------|\\n| Hyperparameters for FitVid  |\\n|-----------------------------|\"}"}
{"id": "yan23b", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | Seq Len | Params | FVD     | PSNR   | SSIM   | LPIPS   |\\n|-------------|---------|--------|---------|--------|--------|---------|\\n| TECO (ours) | 32      | 169M   | 48.2\u00b12  | 21.9\u00b10.02 | 0.368 \u00b10.0114 | 0.157\u00b10.0119 |\\n| TECO (ours) | 200     | 169M   | 59.7\u00b12  | 19.9\u00b10.186 | 0.628\u00b10.00821 | 0.187\u00b10.00460 |\\n| TECO (ours) | 100     | 86M    | 63.9\u00b17  | 15.4\u00b10.199 | 0.476\u00b10.00745 | 0.322\u00b10.00792 |\\n| TECO (ours) | 50      | 195M   | 52.7\u00b16  | 13.9\u00b10.0311 | 0.418\u00b10.000659 | 0.383\u00b10.000302 |\\n| Latent FDM  | 32      | 31M    | 181.2\u00b12 | 17.8\u00b10.111 | 0.588\u00b10.00453 | 0.222\u00b10.00493 |\\n| Latent FDM  | 200     | 62M    | 66.4\u00b13  | 17.7\u00b10.114 | 0.561\u00b10.00623 | 0.253\u00b10.00550 |\\n| Latent FDM  | 100     | 80M    | 55.6\u00b11  | 15.5\u00b10.233 | 0.468\u00b10.00776 | 0.336\u00b10.00511 |\\n| Latent FDM  | 50      | 110M   | 68.3\u00b13  | 14.0\u00b10.0445 | 0.414\u00b10.000414 | 0.385\u00b10.000151 |\\n| Latent FDM  | 80      | 274M   | 116.5\u00b15 | 15.4\u00b10.0603 | 0.381\u00b10.00192 | 0.340\u00b10.00192 |\\n| Latent FDM  | 200     | 261M   | 109.5\u00b11 | 15.4\u00b10.0906 | 0.379\u00b10.00263 | 0.343\u00b10.00263 |\\n| Latent FDM  | 100     | 257M   | 85.1\u00b14  | 15.7\u00b10.0916 | 0.385\u00b10.00244 | 0.325\u00b10.00121 |\\n| Latent FDM  | 50      | 140M   | 80.7\u00b11  | 14.8\u00b10.0404 | 0.369\u00b10.00197 | 0.360\u00b10.00197 |\\n\\nTable K.1. DM Lab scaling\\n\\nTable K.2. Minecraft scaling\"}"}
{"id": "yan23b", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Video Generation\\n\\nPrior video generation methods can be divided into a few classes of models: variational models, exact likelihood models, and GANs. SV2P (Babaeizadeh et al., 2017), SVP (Denton & Fergus, 2018), SVG (Villegas et al., 2019), and FitVid (Babaeizadeh et al., 2021) are variational video generation methods models videos through stochastic latent dynamics, optimized using the ELBO (Kingma & Welling, 2013) objective extended in time. SA VP (Lee et al., 2018) adds an adversarial (Goodfellow et al., 2014) loss to encourage more realistic and high-fidelity generation quality. Diffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2014) have recently emerged as a powerful class of variational generative models which learn to iteratively denoise an initial noise sample to generate high-quality images. There have been several recent works that extend diffusion models to video, through temporal attention (Ho et al., 2022; Harvey et al., 2022), 3D convolutions (Hoppe et al., 2022), or channel stacking (Voleti et al., 2022). Unlike variational models, autoregressive models (AR) and flows (Kumar et al., 2019) model videos by optimizing exact likelihood. Video Pixel Networks (Kalchbrenner et al., 2017) and Subscale Video Transformers (Weissenborn et al., 2019) autoregressively model each pixel. For more compute efficient training, some prior methods (Yan et al., 2021; Le Moing et al., 2021; Seo et al., 2022; Rakhimov et al., 2020; Walker et al., 2021) propose to learn an AR model in a spatio-temporally compressed latent space of a discrete autoencoder, which has shown to be orders of magnitudes more efficient compared to pixel-based methods. Instead of a VQ-GAN, (Le Moing et al., 2021) learns a frame conditional autoencoder through a flow mechanism. Lastly, GANs (Goodfellow et al., 2014) offer an alternative method to training video models. MoCoGAN (Tulyakov et al., 2018) generates videos by disentangling style and motion. MoCoGAN-HD (Tian et al., 2021) can efficiently extend to larger resolutions by learning to navigate the latent space of a pretrained image generator. TGANv2 (Saito & Saito, 2018), DVD-GAN (Clark et al., 2019), StyleGAN-V (Skorokhodov et al., 2021), and TrIVD-GAN (Luc et al., 2020) introduce various methods to scale to complex video, such as proposing sparse training, or more efficient discriminator design.\\n\\nThe main focus of this work lies with video prediction, a specific interpretation of conditional video generation. Most prior methods are trained autoregressive in time, so they can be easily extended to video prediction. Video Diffusion, although trained unconditionally proposes reconstruction guidance for prediction. GANs generally require training a separate model for video prediction. However, some methods such as MoCoGAN-HD and DI-GAN can approximate frame conditioning by inverting the generator to compute a corresponding latent for a frame.\\n\\nLong-Horizon Video Generation\\n\\nCW-V AE (Saxena et al., 2021) learns a hierarchy of stochastic latents to better model long term temporal dynamics, and is able to generate videos with long-term consistency for hundreds of frames. TATS (Ge et al., 2022) extends VideoGPT which allows for sampling of arbitrarily long videos using a sliding window. In addition, TATs and CogVideo (Hong et al., 2022) propose strided sampling as a simple method to incorporate longer horizon contexts. StyleGAN-V (Skorokhodov et al., 2021) and DI-GAN (Yu et al., 2022) learn continuous-time representations for videos which allow for sampling of arbitrary long videos as well. (Brooks et al., 2022) proposes an efficient video GAN architecture that is able to generate high resolution videos of 128 frames on complex video data for dynamic scenes and horseback riding. FDM (Harvey et al., 2022) proposes a diffusion model that is trained to be able to flexibly condition on a wide range of sampled frames to better incorporate context of arbitrarily long videos. (Lee et al., 2021) is able to leverage a hierarchical prediction framework using semantic segmentations to generate long videos.\\n\\nLong-Horizon Video Understanding\\n\\nOutside of generative modeling, prior work such as MeMViT (Wu et al., 2022) and Vis4mer (Mohaiminul Islam & Bertasius, 2022) introduce architectures for modeling long-horizon dependencies in videos.\"}"}
{"id": "yan23b", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"M. Dataset Details\\n\\nM.1. DMLab\\nWe generate random $7 \\\\times 7$ mazes split into four quadrants, with each quadrant containing a random combination of wall and floor textures. We generate 40k trajectories of 300 frames, each $64 \\\\times 64$ images. Actions in this environment consist of $20^\\\\circ$ left turn, $20^\\\\circ$ right turn, and walk forward. In order to maximally traverse the maze, we code an agent that traverses to the furthest unvisited point in the maze, with some added noise for stochasticity. Since the maze is a grid, we can easily hard-code a navigation policy to move to any specified point in the maze.\\n\\nFor 3D visualizations, we also collect depth, camera intrinsics and camera extrinsics (pose) for each timestep. Given this information, we can project RGB points into a 3D coordinate space and reconstruct the maze as a 3D pointcloud. Note that since videos are generated only using RGB as input, they do not have groundtruth depth and pose. Therefore, we train depth and pose estimators that are used during evaluation. Specifically, we train a depth estimator to map from RGB frame to depth, and a pose estimator that takes in two adjacent RGB frames and predicts the relative change in orientation. During evaluation, we are given an initial ground truth orientation that we apply sequentially to predicted frames.\\n\\nAlthough the GQN Mazes (Eslami et al., 2018) already exists as a video prediction dataset, it is difficult to properly measure temporal consistency. The 3D scenes are relatively simple, and it does not have actions to help reduce stochasticity in using metrics such as PSNR, SSIM, and LPIPS. As a result, FVD is the reliable metric used in GQN Mazes, but tends to be sensitive to noise in video predictions. In addition, we perform 3D visualizations using our dataset that are not possible with GQN Mazes.\\n\\nM.2. Minecraft\\nWe generate 200k trajectories (each of a different Minecraft world) of 300 $128 \\\\times 128$ frames in the Minecraft marsh biome. We hardcode an agent to randomly traverse the surroundings by taking left, right, and forward actions with different probabilities. In addition, we let the agent constantly jump, which we found to help traverse simple hills, and prevent itself from drowning. We specifically chose the marsh biome, as it contains hilly turns with sparse collections of trees that act as clear landmarks for consistent generation. Forest and jungle biomes tend to be too dense for any meaningfully clear consistency, as all surroundings look nearly identical. On the other hand, plains biomes had the opposite issue where the surroundings were completely flat. Mountain biomes were too hilly and difficult to traverse.\\n\\nWe opt to introduce an alternative to the MineRL Navigate (Guss et al., 2019) since this dataset primarily consists of human demonstrations of people navigating to specific points. This means that trajectories usually follow a relatively straight line, so there are not many long-term dependencies in this dataset, as only a few past frames of context are necessary for prediction.\\n\\nM.3. Habitat\\nHabitat is a 3D simulator that can render realistic trajectories in scans of 3D scenes. We compile roughly 1400 3D scans from HM3D (Ramakrishnan et al., 2021), MatterPort3D (Chang et al., 2017) and Gibson (Xia et al., 2018), and generate a total of 200k trajectories of $300 128 \\\\times 128$ frames. We use the in-built path traversal algorithm provided in Habitat to construct action trajectories that allow our agent to move between randomly sampled locations in the 3D scene. Similar to Minecraft and DMLab, the agent action space consists of left turn, right turn, and move forward.\"}"}
{"id": "yan23b", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### N. Hyperparameters\\n\\n#### N.1. VQ-GAN & VAE\\n\\n| Dataset          | GPU Days | Resolution | Batch Size | LR    | Num Res Blocks | Attention Resolutions | Channel Mult |\\n|------------------|----------|------------|------------|-------|----------------|-----------------------|--------------|\\n| DMLab / Minecraft Habitat / Kinetics-600 | 16 32    | 64 / 128   | 64         | \\\\(3 \\\\times 10^{-4}\\\\) | 2              | 16                    | 1,2,2,2     |\\n|                   |          | 128        |            | \\\\(3 \\\\times 10^{-4}\\\\) | 2              | 16                    | 1,2,3,4     |\\n|                   |          | 128        |            | \\\\(3 \\\\times 10^{-4}\\\\) | 2              | 16                    | 1           |\\n|                   |          | 128        |            | \\\\(3 \\\\times 10^{-4}\\\\) | 2              | 16                    | 1           |\\n\\n| Dataset          | Latent Size (VQ-GAN) | Embedding Dim (VQ-GAN) | Codebook Size (VQ-GAN) |\\n|------------------|----------------------|------------------------|-------------------------|\\n| DMLab / Minecraft Habitat / Kinetics-600 | 16 \\\\times 16 | 256                    | 1024                    |\\n|                   | 16 \\\\times 16         | 256                    | 8192                    |\\n\\n#### N.2. TECO\\n\\n| Dataset          | TPU-v3 Days | Params    | Resolution | Batch Size | Sequence Length | LR    | LR Schedule | Warmup Steps | Total Training Steps | DropLoss Rate |\\n|------------------|-------------|-----------|------------|------------|----------------|-------|-------------|--------------|----------------------|---------------|\\n| DMLab / Minecraft Habitat / Kinetics-600 | 32 80 275 640 | 169M 274M 386M 1.09B | 64 128 128 128 | 32 32 32 32 | 300 300 300 100 | \\\\(1 \\\\times 10^{-4}\\\\) \\\\(1 \\\\times 10^{-4}\\\\) \\\\(1 \\\\times 10^{-4}\\\\) \\\\(1 \\\\times 10^{-4}\\\\) | cosine | 10k          | 1M           | 0.9                |\\n\\n| Dataset          | Encoder Depths | Blocks | Codebook Size | Embedding Dim | Decoder Depths | Blocks | Temporal Transformer |\\n|------------------|----------------|--------|---------------|--------------|----------------|--------|----------------------|\\n| DMLab / Minecraft Habitat / Kinetics-600 | 256, 512       | 2 4 8 8 | 1024          | 32           | 256, 512       | 4 8 10 | 8 12 8 24             |\\n\\n| Dataset          | Hidden Dim | Feedforward Dim | Heads | Layers | Dropout | Mask Schedule | Hidden Dim | Feedforward Dim | Heads | Layers | Dropout |\\n|------------------|------------|-----------------|-------|--------|---------|---------------|------------|-----------------|-------|--------|---------|\\n| DMLab / Minecraft Habitat / Kinetics-600 | 1024       | 4096            | 16    | 8      | 0        | cosine         | 512        | 2048            | 8     | 8      | 0        |\"}"}
{"id": "yan23b", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Train Sequence Length | Fewer FLOPs per Frame | Hyperparameters | TPU-v3 Days | Params | Resolution | Batch Size | LR | Warmup Steps | Total Training Steps | DropLoss Rate |\\n|-----------------------|-----------------------|-----------------|-------------|--------|------------|------------|----|-------------|--------------------|-------------|\\n|                       |                       | 300             | 200         | 100    | 50         | 32         | 1$ \\\\times 10^{-4}$ | cosine       | 1M          | 0.9            |\\n|                       |                       | 200             | 100         | 50     | 50         | 32         | 1$ \\\\times 10^{-4}$ | cosine       | 1M          | 0.85           |\\n|                       |                       | 100             | 50          | 50     | 50         | 32         | 1$ \\\\times 10^{-4}$ | cosine       | 1M          | 0.85           |\\n|                       |                       | 50              | 50          | 50     | 50         | 32         | 1$ \\\\times 10^{-4}$ | cosine       | 1M          | 0.85           |\\n\\n| Encoder Depths        | Blocks | Codebook Size | Embedding Dim | Decoder Depths | Blocks | Temporal Transformer | Downsample Factor | Hidden Dim | Feedforward Dim | Heads | Layers | Dropout |\\n|-----------------------|--------|---------------|---------------|----------------|--------|-----------------------|-------------------|------------|-----------------|-------|--------|---------|\\n| 256, 512              | 2      | 1024          | 32            | 256, 512      | 4      | 8                     | 8                 | 1024       | 4096            | 16    | 8      | 0       |\\n| 256, 512              | 2      | 1024          | 32            | 256, 512      | 4      | 8                     | 8                 | 512        | 2048            | 8     | 8      | 0       |\\n| 256, 512              | 2      | 1024          | 32            | 256, 512      | 4      | 8                     | 8                 | 1024       | 4096            | 16    | 8      | 0       |\\n| 256, 512              | 2      | 1024          | 32            | 256, 512      | 4      | 8                     | 8                 | 1024       | 4096            | 16    | 8      | 0       |\\n\\n| Encoder Depths        | Blocks | Codebook Size | Embedding Dim | Decoder Depths | Blocks | Temporal Transformer | Downsample Factor | Hidden Dim | Feedforward Dim | Heads | Layers | Dropout |\\n|-----------------------|--------|---------------|---------------|----------------|--------|-----------------------|-------------------|------------|-----------------|-------|--------|---------|\\n| 256, 512              | 2      | 1024          | 32            | 256, 512      | 4      | 8                     | 8                 | 1024       | 4096            | 16    | 8      | 0       |\\n| 256, 512              | 2      | 1024          | 32            | 256, 512      | 4      | 8                     | 8                 | 512        | 2048            | 8     | 8      | 0       |\\n| 256, 512              | 2      | 1024          | 32            | 256, 512      | 4      | 8                     | 8                 | 1024       | 4096            | 16    | 8      | 0       |\\n| 256, 512              | 2      | 1024          | 32            | 256, 512      | 4      | 8                     | 8                 | 1024       | 4096            | 16    | 8      | 0       |\\n\\n| MaskSchedule | Hidden Dim | Feedforward Dim | Heads | Layers | Dropout |\\n|--------------|------------|-----------------|-------|--------|---------|\\n| cosine       | 512        | 2048            | 8     | 8      | 0       |\\n| cosine       | 512        | 2048            | 8     | 8      | 0       |\\n| cosine       | 512        | 2048            | 8     | 8      | 0       |\\n| cosine       | 768        | 3072            | 12    | 8      | 0       |\\n\\nTable N.1. Hyperparameters for scaling TECO on DMLab.\"}"}
{"id": "yan23b", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Train Sequence Length | Fewer FLOPs per Frame |\\n|-----------------------|-----------------------|\\n| Hyperparameters       | 300 200 100 50        |\\n| TPU-v3 Days           | 80 80 80 80           |\\n| Params                | 274M 261M 257M 140M   |\\n| Resolution            | 128 128 128 128       |\\n| Batch Size            | 32 32 32 32           |\\n| LR                    | $1 \\\\times 10^{-4}$ $1 \\\\times 10^{-4}$ $1 \\\\times 10^{-4}$ $1 \\\\times 10^{-4}$ |\\n| LR Schedule           | cosine cosine cosine cosine |\\n| Warmup Steps          | 10k 10k 10k 10k       |\\n| Total Training Steps  | 1M 1M 1M 1M           |\\n| DropLoss Rate         | 0.9 0.85 0.25 0.25    |\\n| Encoder Depths        | 256, 512 256, 512 256, 512 256, 512 |\\n| Blocks                | 4 4 4 4              |\\n| Codebook Size         | 1024 1024 1024 1024   |\\n| Embedding Dim         | 32 32 32 32           |\\n| Decoder Depths        | 256, 512 256, 512 256, 512 256, 512 |\\n| Blocks                | 8 8 8 8              |\\n| Temporal Transformer  | Downsample Factor 8 4 2 2 |\\n| Hidden Dim            | 1024 1024 1024 512    |\\n| Feedforward Dim       | 4096 4096 4096 2048   |\\n| Heads                 | 16 16 16 8            |\\n| Layers                | 12 12 12 12           |\\n| Dropout               | 0 0 0 0              |\\n| MaskGit               |                       |\\n| Mask Schedule         | cosine cosine cosine cosine |\\n| Hidden Dim            | 768 768 768 768       |\\n| Feedforward Dim       | 3072 3072 3072 3072   |\\n| Heads                 | 12 12 12 12           |\\n| Layers                | 6 6 6 8              |\\n| Dropout               | 0 0 0 0              |\\n\\nTable N.2. Hyperparameters for scaling TECO on Minecraft.\"}"}
{"id": "yan23b", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Hyperparameters | Train Sequence Length | TPU-v3 Days | Params | Resolution | Batch Size | LR | LR Schedule | Optimizer | Warmup Steps | Total Training Steps | Base Channels | Num Res Blocks | Head Dim | Attention Resolutions | Dropout | Channel Mult |\\n|-----------------|----------------------|-------------|--------|------------|------------|----|-------------|-----------|--------------|----------------------|--------------|---------------|----------|------------------------|---------|--------------|\\n| Latent FDM      |                      |             |        |            |            |    |             |           |              |                      |              |               |          |                        |         |              |\\n| hyperparameters |                      |             |        |            |            |    |             |           |              |                      |              |               |          |                        |         |              |\\n|                 | 300                  | 32          | 31M    | 64         | 32         | 1  | cosine      | Adam      | 10k          | 1M                   | 128          | 1,1,1,2       | 64       | 4,2                     | 0       | 1,1,1,2      |\\n|                 | 200                  | 32          | 62M    | 64         | 32         | 1  | cosine      | Adam      | 10k          | 1M                   | 128          | 1,1,2,2       | 64       | 4,2                     | 0       | 1,2,2,2      |\\n|                 | 100                  | 32          | 80M    | 64         | 32         | 1  | cosine      | Adam      | 10k          | 1M                   | 128          | 1,2,2,4       | 64       | 4,2                     | 0       | 1,2,2,4      |\\n|                 | 50                   | 32          | 110M   | 64         | 32         | 1  | cosine      | Adam      | 10k          | 1M                   | 128          | 2,2,2,2       | 64       | 8,4,2                    | 0       | 1,2,3,8      |\\n\\n**Table N.3.** Hyperparameters for Latent FDM\\n\\n| Hyperparameters | Train Sequence Length | TPU-v3 Days | Params | Resolution | Batch Size | LR | LR Schedule | Optimizer | Warmup Steps | Total Training Steps | Base Channels | Num Res Blocks | Head Dim | Attention Resolutions | Dropout | Channel Mult |\\n|-----------------|----------------------|-------------|--------|------------|------------|----|-------------|-----------|--------------|----------------------|--------------|---------------|----------|------------------------|---------|--------------|\\n| Latent FDM      |                      |             |        |            |            |    |             |           |              |                      |              |               |          |                        |         |              |\\n| hyperparameters |                      |             |        |            |            |    |             |           |              |                      |              |               |          |                        |         |              |\\n|                 |                      |             |        |            |            |    |             |           |              |                      |              |               |          |                        |         |              |\\n|                 |                      |             |        |            |            |    |             |           |              |                      |              |               |          |                        |         |              |\\n|                 |                      |             |        |            |            |    |             |           |              |                      |              |               |          |                        |         |              |\\n|                 |                      |             |        |            |            |    |             |           |              |                      |              |               |          |                        |         |              |\\n\\n**Table N.4.** Hyperparameters for scaling Latent FDM on DMLab\"}"}
{"id": "yan23b", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Train Sequence Length | Fewer FLOPs per Frame |\\n|-----------------------|-----------------------|\\n| Hyperparameters       | 300 200 100 50        |\\n| TPU-v3 Days           | 80 80 80 80           |\\n| Params                | 33M 80M 69M 186M      |\\n| Resolution            | 128 128 128 128       |\\n| Batch Size            | 32 32 32 32           |\\n| LR                    | $1 \\\\times 10^{-4}$ $1 \\\\times 10^{-4}$ $1 \\\\times 10^{-4}$ $1 \\\\times 10^{-4}$ |\\n| LR Schedule           | cosine cosine cosine cosine |\\n| Optimizer             | Adam Adam Adam Adam   |\\n| Warmup Steps          | 10k 10k 10k 10k       |\\n| Total Training Steps  | 1M 1M 1M 1M           |\\n| Base Channels         | 128 128 128 192       |\\n| Num Res Blocks        | 1,1,2,2 2,2,2,2 3,3,3,3 2,2,2,2 |\\n| Head Dim              | 64 64 64 64           |\\n| Attention Resolutions | 4,2 4,2 8,4,2 8,4,2   |\\n| Dropout               | 0 0 0 0               |\\n| Channel Mult          | 1,2,2,2 1,2,3,4 1,2,2,3 1,2,3,4 |\\n\\nTable N.5. Hyperparameters for scaling Latent FDM on Minecraft\"}"}
{"id": "yan23b", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporally Consistent Transformers for Video Prediction\\n\\nattention transformer as the sequence model. Leveraging prior work on efficient sequence models (Choromanski et al., 2020; Wang et al., 2020; Zhai et al., 2021; Gu et al., 2021; Hawthorne et al., 2022) would likely allow for further scaling.\\n\\n\u2022 We trained all models on top of pretrained VQ-GAN codes to reduce the data dimensionality. This compression step lets us train on longer sequences at a cost of reconstruction error, which causes noticeable artifacts in Kinetics-600, such as corrupted text and incoherent faces. Although TECO can train directly on pixels, a $\\\\ell_2$ loss results in slightly blurry predictions. Training directly on pixels with diffusion or GAN losses would be promising.\\n\\n6. Acknowledgements\\n\\nThis work was in part supported by Panasonic through BAIR Commons, Darpa RACER, the Hong Kong Centre for Logistics Robotics, and BMW. In addition, we thank the TRC program (https://sites.research.google/trc/about/) and Cirrascale Cloud Services (https://cirrascale.com/) for providing compute resources.\"}"}
{"id": "yan23b", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nBabaeizadeh, M., Finn, C., Erhan, D., Campbell, R. H., and Levine, S. Stochastic variational video prediction. arXiv preprint arXiv:1710.11252, 2017.\\n\\nBabaeizadeh, M., Saffar, M. T., Nair, S., Levine, S., Finn, C., and Erhan, D. Fitvid: Overfitting in pixel-level video prediction. arXiv preprint arXiv:2106.13195, 2021.\\n\\nBeattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., K\u00a8uttler, H., Lefrancq, A., Green, S., Vald\u00b4es, V ., Sadik, A., et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.\\n\\nBengio, Y . Estimating or propagating gradients through stochastic neurons. arXiv preprint arXiv:1305.2982, 2013.\\n\\nBrooks, T., Hellsten, J., Aittala, M., Wang, T.-C., Aila, T., Lehtinen, J., Liu, M.-Y ., Efros, A. A., and Karras, T. Generating long videos of dynamic scenes. arXiv preprint arXiv:2206.03429, 2022.\\n\\nCarreira, J. and Zisserman, A. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299\u20136308, 2017.\\n\\nChang, A., Dai, A., Funkhouser, T., Halber, M., Niessner, M., Savva, M., Song, S., Zeng, A., and Zhang, Y . Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.\\n\\nChang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315\u201311325, 2022.\\n\\nChild, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\n\\nChoromanski, K., Likhosherstov, V ., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.\\n\\nClark, A., Donahue, J., and Simonyan, K. Adversarial video generation on complex datasets, 2019.\\n\\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\\n\\nDenton, E. and Fergus, R. Stochastic video generation with a learned prior. arXiv preprint arXiv:1802.07687, 2018.\\n\\nEslami, S. A., Jimenez Rezende, D., Besse, F., Viola, F., Morcos, A. S., Garnelo, M., Ruderman, A., Rusu, A. A., Danihelka, I., Gregor, K., et al. Neural scene representation and rendering. Science, 360(6394):1204\u20131210, 2018.\\n\\nEsser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12873\u201312883, 2021.\\n\\nGe, S., Hayes, T., Yang, H., Yin, X., Pang, G., Jacobs, D., Huang, J.-B., and Parikh, D. Long video generation with time-agnostic vqgan and time-sensitive transformer. arXiv preprint arXiv:2204.03638, 2022.\\n\\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y . Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014.\\n\\nGu, A., Goel, K., and R \u00b4e, C. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021.\\n\\nGupta, A., Tian, S., Zhang, Y ., Wu, J., Mart\u00b4\u0131n-Mart\u00b4\u0131n, R., and Fei-Fei, L. Maskvit: Masked visual pre-training for video prediction. arXiv preprint arXiv:2206.11894, 2022.\\n\\nGuss, W. H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., and Salakhutdinov, R. Minerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019.\\n\\nHarvey, W., Naderiparizi, S., Masrani, V ., Weilbach, C., and Wood, F. Flexible diffusion modeling of long videos. arXiv preprint arXiv:2205.11495, 2022.\\n\\nHawthorne, C., Jaegle, A., Cangea, C., Borgeaud, S., Nash, C., Malinowski, M., Dieleman, S., Vinyals, O., Botvinick, M., Simon, I., et al. General-purpose, long-context autoregressive modeling with perceiver ar. arXiv preprint arXiv:2202.07765, 2022.\\n\\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. arXiv preprint arXiv:2006.11239, 2020.\\n\\nHo, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022.\\n\\nHong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.\\n\\nH\u00a8oppe, T., Mehrjou, A., Bauer, S., Nielsen, D., and Dittadi, A. Diffusion models for video prediction and infilling. arXiv preprint arXiv:2206.07696, 2022.\"}"}
{"id": "yan23b", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "yan23b", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporally Consistent Transformers for Video Prediction\\n\\nTulyakov, S., Liu, M.-Y., Yang, X., and Kautz, J. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1526\u20131535, 2018.\\n\\nUnterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., and Gelly, S. Fvd: A new metric for video generation. 2019.\\n\\nVan Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems, pp. 6306\u20136315, 2017.\\n\\nVillegas, R., Pathak, A., Kannan, H., Erhan, D., Le, Q. V., and Lee, H. High fidelity video prediction with large stochastic recurrent neural networks. Advances in Neural Information Processing Systems, 32:81\u201391, 2019.\\n\\nVillegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo, H., Zhang, H., Saffar, M. T., Castro, S., Kunze, J., and Erhan, D. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022.\\n\\nVoleti, V., Jolicoeur-Martineau, A., and Pal, C. Masked conditional video diffusion for prediction, generation, and interpolation. arXiv preprint arXiv:2205.09853, 2022.\\n\\nWalker, J., Razavi, A., and Oord, A. v. d. Predicting video with vqvae. arXiv preprint arXiv:2103.01950, 2021.\\n\\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.\\n\\nWang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004.\\n\\nWeissenborn, D., T\u00e4ckstr\u00f6m, O., and Uszkoreit, J. Scaling autoregressive video models. arXiv preprint arXiv:1906.02634, 2019.\\n\\nWu, C.-Y., Li, Y., Mangalam, K., Fan, H., Xiong, B., Malik, J., and Feichtenhofer, C. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13587\u201313597, 2022.\\n\\nXia, F., Zamir, A. R., He, Z., Sax, A., Malik, J., and Savarese, S. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9068\u20139079, 2018.\\n\\nYan, W., Zhang, Y., Abbeel, P., and Srinivas, A. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021.\\n\\nYu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021.\\n\\nYu, S., Tack, J., Mo, S., Kim, H., Kim, J., Ha, J.-W., and Shin, J. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571, 2022.\\n\\nZhai, S., Talbott, W., Srivastava, N., Huang, C., Goh, H., Zhang, R., and Susskind, J. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021.\\n\\nZhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586\u2013595, 2012.\"}"}
{"id": "yan23b", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G. Metrics During Training\\n\\nFigure G.1 shows plots of FVD (over chunks of generated 16-frame video) and LPIPS during training, evaluated at saved model checkpoints every 50k iterations over 1M iterations. We can see that although FVD (measuring frame fidelity) tends to saturate early on during training (at around 200k iterations), the long-term consistency metric (LPIPS) continues to improve until the end of training. We hypothesize that this may be due to the model first learning the \u201ceasier bits\u201d more local in time, and then learning long-horizon bits once the easier bits have been learned.\\n\\nH. High Quality Spatio-Temporal Compression\\n\\nTable H.1.\\n\\n| Model     | Dataset FVD |\\n|-----------|-------------|\\n| TATS Video VQGAN | 53          |\\n| TECO      | 54          |\\n\\nTable H.1 compares reconstruction FVD between TECO and TATS. At the same compression rate (same number of discrete codes), TECO learns far better spatio-temporal codes that TATS, with more of a difference on more visually complex scenes (Minecraft vs DMLab).\"}"}
{"id": "yan23b", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporally Consistent Transformers for Video Prediction\\n\\nI. Trade-off Between Fidelity and Learning Long-Range Dependencies\\n\\n| Resolution | PSNR | SSIM | LPIPS |\\n|------------|------|------|-------|\\n| 1\u00d71        | 20.4 | 0.666| 0.170 |\\n| 2\u00d72        | 18.6 | 0.597| 0.221 |\\n| 4\u00d74        | 17.7 | 0.578| 0.242 |\\n\\nTable I.1. Comparing different input resolutions to the temporal transformer\\n\\nLatent FDM Arch\\n\\n| Resolution | PSNR | SSIM | LPIPS |\\n|------------|------|------|-------|\\n|             | 18.1 | 0.588| 0.222 |\\n|             | 15.6 | 0.501| 0.277 |\\n\\nTable I.2. Comparing different Latent FDM architectures with more computation at different resolutions\\n\\nTable I.1 and Table I.2 show a trade-off between fidelity (frame or image quality) and temporal consistency (long-range dependencies) for video prediction architectures (both TECO, and Latent FDM).\"}"}
{"id": "yan23b", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Temporally Consistent Transformers for Video Prediction\\n\\n#### J. Full Experimental Results\\n\\n| Method       | TPU-v3 Days | Params | FVD \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2191 |\\n|--------------|-------------|--------|-------|--------|--------|---------|\\n| TECO (ours)  | 80          | 386M   | 76.3 \u00b1 1.72 | 12.8 \u00b1 0.139 | 0.363 \u00b1 0.00122 | 0.604 \u00b1 0.00451 |\\n| Latent FDM   | 275         | 87M    | 433 \u00b1 2.67 | 12.5 \u00b1 0.121 | 0.311 \u00b1 0.000829 | 0.582 \u00b1 0.000492 |\\n| Perceiver-AR | 275         | 200M   | 164 \u00b1 12.6 | 12.8 \u00b1 0.0423 | 0.405 \u00b1 0.00248 | 0.676 \u00b1 0.00282 |\\n\\n(a) Using top-k sampling for Perceiver AR and TECO\\n\\n| Method       | TPU-v3 Days | Params | FVD \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2191 |\\n|--------------|-------------|--------|-------|--------|--------|---------|\\n| TECO (ours)  | 640         | 1.09B  | 649 \u00b1 16.5 | 640 \u00b1 52 | 0.799 \u00b1 23.4 | 0.799 \u00b1 23.4 |\\n| Latent FDM   | 640         | 831M   | 960 \u00b1 52 | 0.799 \u00b1 23.4 | 0.799 \u00b1 23.4 |\\n| Perceiver-AR | 640         | 1.06B  | 1022 \u00b1 32 | 0.799 \u00b1 23.4 | 0.799 \u00b1 23.4 |\\n\\n(b) No top-k sampling\\n\\n### Table J.1.\\n\\n#### DMLab\\n\\n| Method       | TPU-v3 Days | Params | FVD \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2191 |\\n|--------------|-------------|--------|-------|--------|--------|---------|\\n| TECO (ours)  | 32          | 169M   | 27.5 \u00b1 1.77 | 0.77 \u00b1 0.368 | 0.709 \u00b1 0.0119 | 0.155 \u00b1 0.00958 |\\n| Latent FDM   | 32          | 31M    | 181 \u00b1 2.20 | 0.8 \u00b1 0.111 | 0.588 \u00b1 0.00453 | 0.222 \u00b1 0.00493 |\\n| Perceiver-AR | 32          | 30M    | 96.3 \u00b1 3.64 | 11.2 \u00b1 0.00381 | 0.304 \u00b1 0.0000456 | 0.487 \u00b1 0.00123 |\\n| CW-V AE      | 32          | 111M   | 125 \u00b1 7.95 | 0.6 \u00b1 0.0585 | 0.372 \u00b1 0.000330 | 0.465 \u00b1 0.00156 |\\n| FitVid       | 32          | 165M   | 176 \u00b1 4.86 | 0.6 \u00b1 0.126 | 0.356 \u00b1 0.00171 | 0.491 \u00b1 0.00108 |\\n\\n### Table J.2.\\n\\n#### Minecraft\\n\\n| Method       | TPU-v3 Days | Params | FVD \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2191 |\\n|--------------|-------------|--------|-------|--------|--------|---------|\\n| TECO (ours)  | 80          | 274M   | 116 \u00b1 5.08 | 15.4 \u00b1 0.603 | 0.381 \u00b1 0.00192 | 0.340 \u00b1 0.00264 |\\n| Latent FDM   | 80          | 33M    | 167 \u00b1 6.26 | 13.4 \u00b1 0.0904 | 0.349 \u00b1 0.00327 | 0.429 \u00b1 0.00284 |\\n| Perceiver-AR | 80          | 166M   | 76.3 \u00b1 1.72 | 13.2 \u00b1 0.0711 | 0.323 \u00b1 0.00336 | 0.441 \u00b1 0.00207 |\\n| CW-V AE      | 80          | 140M   | 397 \u00b1 15.5 | 13.4 \u00b1 0.0610 | 0.338 \u00b1 0.00274 | 0.441 \u00b1 0.00367 |\\n| FitVid       | 80          | 176M   | 956 \u00b1 15.8 | 13.0 \u00b1 0.00895 | 0.343 \u00b1 0.00380 | 0.519 \u00b1 0.00380 |\\n\\n### Table J.3.\\n\\n#### Habitat\\n\\n| Method       | TPU-v3 Days | Params | FVD \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2191 |\\n|--------------|-------------|--------|-------|--------|--------|---------|\\n| TECO (ours)  | 640         | 1.09B  | 799 \u00b1 23.4 | 640 \u00b1 52 | 0.799 \u00b1 23.4 | 0.799 \u00b1 23.4 |\\n| Latent FDM   | 640         | 831M   | 960 \u00b1 52 | 0.799 \u00b1 23.4 | 0.799 \u00b1 23.4 |\\n| Perceiver-AR | 640         | 1.06B  | 1022 \u00b1 32 | 0.799 \u00b1 23.4 | 0.799 \u00b1 23.4 |\\n\\n### Table J.4.\\n\\n#### Kinetics\\n\\n| Method       | TPU-v3 Days | Params | FVD \u2193 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2191 |\\n|--------------|-------------|--------|-------|--------|--------|---------|\\n| TECO (ours)  | 275         | 1.09B  | 799 \u00b1 23.4 | 640 \u00b1 52 | 0.799 \u00b1 23.4 | 0.799 \u00b1 23.4 |\\n| Latent FDM   | 275         | 831M   | 960 \u00b1 52 | 0.799 \u00b1 23.4 | 0.799 \u00b1 23.4 |\\n| Perceiver-AR | 275         | 1.06B  | 1022 \u00b1 32 | 0.799 \u00b1 23.4 | 0.799 \u00b1 23.4 |\"}"}
{"id": "yan23b", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure J.1. FVD on Kinetics-600 with different top-k values for Perceiver-AR and TECO.\"}"}
