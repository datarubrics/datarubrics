{"id": "velickovic22a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Validation results on all 30 algorithms in CLRS-30, averaged over three seeds.\"}"}
{"id": "velickovic22a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm               | Activity Selector | Articulation Points | Bellman-Ford | BFS | Binary Search | Bridges | Bubble Sort | DAG Shortest Paths | DFS | Dijkstra | Find Max. Subarray | Floyd-Warshall | Graham Scan | Heapsort | Insertion Sort | Jarvis' March | KMP Matcher | LCS Length | Matrix Chain Order | Minimum | MST-Kruskal | MST-Prim | Na\u00efve String Match | Optimal BST | Quickselect | Quicksort | Segments Intersect | SCC | Task Scheduling | Topological Sort |\\n|-------------------------|-------------------|---------------------|--------------|-----|--------------|---------|-------------|-------------------|-----|----------|------------------|---------------|-------------|----------|--------------|--------------|------------|-----------|--------------|-----------|-------------|----------|----------------|-------|---------------|------------|----------------|---|---------------|------------|\\n| Deep Sets              | L                 | L                   | L            |     | L            |         |             |                   |     | L        | L                | L             | L           | L         | L            | L            | L          | L         | L            | L         | L           | L         | L             | L        | L             | L         |\\n| GAT                    |                   |                     |              |     |              |         |             |                   |     | L        | L                | L             | L           |           | L            | L            |            | L         |             |           |             | L         | L             | L        | L             | L         |\\n| Memnet                 |                   |                     |              |     |              |         |             |                   |     |           |                  | L             |             |           | L            | L            |            |           | L         |             |           |             | L         | L             | L        | L             | L         |\\n| MPNN                   |                   |                     |              |     |              |         |             |                   |     |           |                  | L             |             |           | L            | L            |            |           |           |             |           |             | L         | L             | L        | L             | L         |\\n| PGN                    |                   |                     |              |     |              |         |             |                   |     |           |                  | L             |             |           | L            | L            |            |           |           |             |           |             | L         | L             | L        | L             | L         |\"}"}
{"id": "velickovic22a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Early-stopped validation results of all models on all algorithms.\\n\\n| Algorithm    | Avg. Accuracy | \u00b1        | \u00b1        | \u00b1        | \u00b1        |\\n|--------------|--------------|----------|----------|----------|----------|\\n| Topological Sort | 92%         | \u00b180%     | \u00b161%     | \u00b159%     | \u00b146%     |\\n| Activity Selector | 03%         | \u00b182%     | \u00b158%     | \u00b184%     | \u00b166%     |\\n| Algorithm Deep Sets | 00% | \u00b150%     | \u00b136%     | \u00b151%     | \u00b134%     |\\n| GAT | 76% | \u00b125% | \u00b136% | \u00b151% | \u00b134% |\\n| Memnet | 41% | \u00b116% | \u00b152% | \u00b151% | \u00b134% |\\n| PGN | 00% | \u00b123% | \u00b152% | \u00b151% | \u00b134% |\\n\\nThe CLRS Algorithmic Reasoning Benchmark\"}"}
{"id": "velickovic22a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nLearning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with classical algorithms. Several important works have investigated whether neural networks can effectively reason like algorithms, typically by learning to execute them. The common trend in the area, however, is to generate targeted kinds of algorithmic data to evaluate specific hypotheses, making results hard to transfer across publications, and increasing the barrier of entry. To consolidate progress and work towards unified evaluation, we propose the CLRS Algorithmic Reasoning Benchmark, covering classical algorithms from the Introduction to Algorithms textbook. Our benchmark spans a variety of algorithmic reasoning procedures, including sorting, searching, dynamic programming, graph algorithms, string algorithms and geometric algorithms. We perform extensive experiments to demonstrate how several popular algorithmic reasoning baselines perform on these tasks, and consequently, highlight links to several open challenges. Our library is readily available at https://github.com/deepmind/clrs.\\n\\n1. Introduction\\n\\nNeural networks and classical algorithms are two techniques that operate on diametrically opposite (and complementary) sides of problem-solving: neural networks can adapt and generalise to raw inputs, automatically extracting appropriate features and a single neural network setup is often applicable to many separate tasks (Zamir et al., 2018). However, they are hard to interpret, notoriously unreliable when extrapolating outside of the dataset they have been trained on, and rely on massive quantities of training data. On the other hand, algorithms trivially strongly generalise to inputs of arbitrary sizes, and can be verified or proven to be correct, with interpretable step-wise operations. Their shortcoming is that inputs must be made to conform to a particular algorithm specification, and looking at a separate task often requires coming up with an entirely new algorithm (Veli\u0107 & Blundell, 2021).\\n\\nBringing the two sides closer together can therefore yield the kinds of improvements to performance, generalisation and interpretability that are unlikely to occur through architectural gains alone. Accordingly, algorithmic modelling as a domain for testing neural networks has been gaining popularity over the last few years (Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Trask et al., 2018; Vinyals et al., 2015; Kool et al., 2018; Freivalds et al., 2019; Dwivedi et al., 2020; Chen et al., 2020; Tang et al., 2020; Veli\u0107 et al., 2019; Yan et al., 2020; Deac et al., 2020) due to its ability to highlight various reasoning limitations of existing architectures.\\n\\nEarlier work (Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015) focused on the need of long-term memory capabilities when executing algorithms, which offered a good test-bed for various recurrent and memory architectures. Recently, algorithmic tasks have been used to highlight the efficiency of graph neural networks (Dwivedi et al., 2020; Chen et al., 2020; Veli\u0107 et al., 2019; Yan et al., 2020; Corso et al., 2020; Tang et al., 2020; Georgiev & Li\u00f3, 2020; Veli\u0107 et al., 2020) and to distinguish between different variations of them, typically through the lens of algorithmic alignment\u2014architectures that align better with the underlying algorithm can be proven to have better sample complexity (Xu et al., 2019). Unfortunately, many of these works remain disconnected in terms of the algorithms they target, how the data is presented to the model or through the training and testing protocols they use, making direct comparison somewhat difficult.\\n\\nTo make a first step towards a unified benchmark for algorithmic reasoning tasks, we propose a comprehensive dataset which we will refer to as The CLRS Algorithmic Reasoning Benchmark, in homage to the Introduction to Algorithms textbook by Cormen, Leiserson, Rivest and Stein (Cormen et al., 2009).\"}"}
{"id": "velickovic22a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Within this benchmark, we propose and evaluate on CLRS-30: a dataset containing trajectories\u2014a trajectory is formed of inputs, the corresponding outputs and optional intermediary targets\u2014of 30 classical algorithms covering various forms of reasoning, including sorting, searching, dynamic programming, geometry, graphs and strings. Some of these algorithms are depicted in Figure 1. The appeal and motivation for such a benchmark goes beyond unifying or providing a common ground for previous works, as we will describe. We believe that CLRS-30 is well positioned to explore out-of-distribution (OOD) generalization and transfer (as potentially part of a meta-learning setting) given the explicit and known relationship between different algorithms (e.g. what subroutines are shared and so forth).\\n\\n2. Motivation\\n\\nTimely posed benchmarks have led to a significant progress in the field, from the impact of ImageNet (Russakovsky et al., 2015) on the vision community, to that of Wikipedia and Penn Treebank in popularizing neural networks for language modelling (Merity et al., 2016; Mikolov et al., 2011) or Atari-2600 for deep reinforcement learning (Bellemare et al., 2013). The prevalence of recent works focusing on algorithmic reasoning, as well as a history of disparate work on a variety of bespoke benchmarks (Graves et al., 2014; Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Trask et al., 2018), suggests significant utility in a benchmark covering a wide-range of classical CS algorithms.\\n\\nLearning to mimic an algorithm also provides an opportunity to extensively test the limitations of architectures both in terms of their representation capacity and processing. This can then be related back directly onto underlying operations and qualities of the well-studied CS algorithms being mimicked as we are aware of both the process used to generate the inputs and the specifics of the underlying function producing the corresponding outputs. Hence, benchmarking in this area can be used to better understand the limitations of current architectures and the optimisation schemes used. This benchmarking can come in many forms:\\n\\n- Data can be easily generated, allowing the neural network behaviour to be probed under different regimes: from few-shot learning all the way to infinite-data.\\n- Algorithms can be used to understand the efficiency of different inductive biases and neural components. For example, a recent study (Tang et al., 2020) has demonstrated the direct benefits of choosing inductive biases that align well with iterative algorithms. Algorithms have also been used to highlight the importance of attention mechanisms (Graves et al., 2014) or to disambiguate various message passing mechanisms for graph neural networks (Richter & Wattenhofer, 2020; Joshi et al., 2020; Veli\u010dkovi\u0107 et al., 2019).\\n- Algorithms can require repeated computation, recursion, or performing very different forms of computations conditioned on the input, providing an excellent test-bed for evaluating compositionality; i.e. whether an algorithm executor can effectively exploit these repeated computations.\\n- One can control the amount of memory required to solve a problem instance, hence test the memorization ability of neural networks. Moreover, one can build a curriculum of tasks of increasing memory requirements (Zaremba & Sutskever, 2014).\\n\\nControl over the difficulty of problem instances also allows the behaviour of a trained model to be tested on OOD samples. While neural networks are highly efficient on solving complex perceptual tasks, current theoretical understanding suggests that their power relies on their ability to interpolate (Liu et al., 2020; Belkin et al., 2019; Jacot et al., 2018), limiting them to in-distribution generalisation. General reasoning systems, however, need to be able to expand beyond this type of generalization. OOD generalization (Li et al., 2020) is paramount, as generally one can not control the distribution a model will face over time when deployed.\\n\\nUnderstanding how algorithms operate on corner cases is a standard approach for analysing their correctness. Similarly, understanding the behaviour of a trained model on larger instances of the problem, or instances that expose such corner cases that were not covered in the training set, can elucidate to what degree the model has truly learned the algorithm (as opposed to overfitting to specific statistics of the training data). Particularly, we can control how far from the training distribution a test instance is, potentially allowing us to understand to what extent the model generalizes OOD, and under which circumstances. In turn, this can offer insight into the effectiveness of different inductive biases, highlighting what kinds of inductive biases are useful for mimicking reasoning processes.\"}"}
{"id": "velickovic22a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One would also expect a general reasoning system to be able to reuse parts of learned computations when learning a new task, and to compose learnt computational subroutines (Lake, 2019; Griffiths et al., 2019; Alet et al., 2018). These forms of generalization have been the aim of several learning paradigms from transfer learning to meta-learning and continual learning or domain adaptation. However, many of these paradigms rely on the concept of a task, and measuring or understanding the ability of a learned system to reuse or compose requires the ability to decompose a task into sub-tasks and to be able to relate tasks among themselves. In many scenarios, such decompositions are ambiguous. Without a clear segmentation into sub-tasks, there can be no clearly defined distance metric between tasks (Du et al., 2018). Conversely, algorithms are built based on subroutines that tend to be extensively shared, providing a good playground for formalizing and measuring reuse and composition, making an algorithmic reasoning benchmark potentially attractive to meta-learning practitioners.\\n\\nLastly and fundamentally, computer scientists rely on a relatively small number of algorithms to address an extremely vast set of problems. They can be seen as a very powerful basis that spans most forms of reasoning processes. On one hand, this means that any generic reasoning system likely has to be able to reproduce all such kinds of procedures, hence, building a system that properly learns all of them is a major stepping stone towards generic reasoning. On the other hand, this means that they can be used to discover inductive biases that will enable tackling more complex problems. This is either because these complex problems can be seen as a combination of several algorithms, or because learning certain algorithms can provide a reliable way for the model to learn how to access its own memory or how to attend to its input or other such internal mechanisms. So by first training on algorithms\u2014potentially controlling the difficulty of training instances\u2014one can pre-train for tasks where full trajectories may not be available (Veli\u02c7ckovi\u00b4c et al., 2021). One such example is discovering novel polynomial-time heuristics for combinatorial optimisation (Bengio et al., 2020; Cappart et al., 2021; Khalil et al., 2017) or reinforcement learning (Deac et al., 2021). Note that our focus with this benchmark lies in learning the basic algorithms themselves only\u2013this in itself proves sufficiently challenging for neural networks, and is itself a useful outcome for the reasons highlighted above. However, we speculate that once a neural network can learn not only individual algorithms but novel combinations of multiple algorithms or even discover new algorithms, such networks will be useful in a wide variety of problems from scientific problems such as protein folding and genomics to simulated environments such as those used by reinforcement learning and control\u2013much as classic CS algorithms already make inroads into these domains but lack the ability to learn from data.\\n\\nGuided by these observations, we regard CLRS-30 as a first step towards a pragmatic setting to test many of these different aspects of current architectures. While we do not directly target all of the scenarios outlined above, the benchmark was built with ease of expansion in mind; enabling for extensive tweaking of training/testing setups, kinds of information captured in algorithm trajectories, as well as including additional algorithms, which we aim to do consistently over time.\\n\\n3. CLRS Algorithmic Reasoning Benchmark\\n\\nOwing to its name, CLRS-30 consists only of algorithms which may be encountered in the CLRS textbook (Cormen et al., 2009). Further, all algorithm trajectories and relevant variables have been designed to match the pseudocode in the textbook as closely as possible. We begin by describing the selection criteria we applied when determining which algorithms to include in CLRS-30.\\n\\nOur initial survey of the textbook yielded 94 algorithms and data structures of interest. From this point, we set out to filter this set to algorithms suitable for inclusion in the initial version of our benchmark. The criteria we applied, with justification and remarks, are as follows:\\n\\nWe want to be able to reliably generate ground-truth outputs for large inputs. As such, NP-hard tasks (and approximation algorithms thereof) have been excluded. Our decision is backed up by theoretical work suggesting impossibility of accurately modelling NP-hard problems using polynomial-time samplers, unless NP = co-NP (Yehuda et al., 2020).\\n\\nTasks requiring numerical outputs have been excluded. Evaluating their performance is ambiguous, and may be dependent on the way architectures choose to represent numbers. For example, Yan et al. (2020) (which represents numbers in binary) and Veli\u02c7ckovi\u00b4c et al. (2019) (which represents them in floating-point) report different metrics on predicting shortest-path lengths. This excludes most number-theoretic algorithms, linear programming, and max-flow. It does not exclude shortest-path algorithms: we can treat them as tasks of finding edges belonging to the shortest path, as was done in Veli\u02c7ckovi\u00b4c et al. (2019); Tang et al. (2020). The numerical values of path lengths are then treated as intermediate parts of the trajectory, and not directly evaluated on.\\n\\nStandalone data structures do not directly represent a task. It should be noted that, by the max-flow min-cut theorem (Ford Jr & Fulkerson, 2015), any max-flow problem can be cast as finding the minimum cut containing the source vertex. This is a discrete decision problem over input vertices, which hence doesn\u2019t violate our constraints, and could be included in future iterations.\\n\\nIn programming language terms, their algorithms tend to be...\"}"}
{"id": "velickovic22a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rather, their target is appropriately updating the internal state of the data structure. Hence, we don't include their operations, unless they appear as components of algorithms. We, of course, look forward to including them in subsequent versions of the dataset, as they can provide useful building blocks for learning complex algorithms.\\n\\nLastly, there are representational issues associated with dynamically allocated memory\u2014it may be unclear what is the best way to represent the internal memory storage and its usage in algorithm trajectories. One example of the ambiguity is in asking whether the algorithm executor should start with a \\\"scratch space\\\" defined by the space complexity of the problem that gets filled up, or dynamically generate such space (Strathmann et al., 2021). As such, we for now exclude all algorithms that require allocating memory which cannot be directly attached to the set of objects provided at input time. This excludes algorithms like merge sort, Hierholzer's algorithm for finding Euler tours (Hierholzer & Wiener, 1873), or string matching using finite automata.\\n\\nAll of the above applied, we arrive at the 30 algorithms that are selected into CLRS-30, which we categorize as follows:\\n\\n**Sorting:**\\n- Insertion sort, bubble sort, heapsort (Williams, 1964), quicksort (Hoare, 1962).\\n\\n**Searching:**\\n- Minimum, binary search, quickselect (Hoare, 1961).\\n\\n**Divide and Conquer (D&C):**\\n- Maximum subarray (Kadane's variant (Bentley, 1984)).\\n\\n**Greedy:**\\n- Activity selection (Gavril, 1972), task scheduling (Lawler, 1985).\\n\\n**Dynamic Programming:**\\n- Matrix chain multiplication, longest common subsequence, optimal binary search tree (Aho et al., 1974).\\n\\n**Graphs:**\\n- Depth-first and breadth-first search (Moore, 1959), topological sorting (Knuth, 1973), articulation points, bridges, Kosaraju's strongly-connected components algorithm (Aho et al., 1974), Kruskal's and Prim's algorithms for minimum spanning trees (Kruskal, 1956; Prim, 1957), Bellman-Ford and Dijkstra's algorithms for single-source shortest paths (Bellman, 1958; Dijkstra et al., 1959) (+ directed acyclic graphs version), Floyd-Warshall algorithm for all-pairs shortest paths (Floyd, 1962).\\n\\n**Strings:**\\n- Na\u00efve string matching, Knuth-Morris-Pratt (KMP) string matcher (Knuth et al., 1977).\\n\\n**Geometry:**\\n- Segment intersection, Convex hull algorithms: Graham scan (Graham, 1972), Jarvis' march (Jarvis, 1973).\\n\\nThe chosen algorithms span a wide variety of reasoning of the *void* type. Akin to *malloc*-like calls in C++.\\n\\n### 3.1. Implementation, probes and representation\\n\\nWe have implemented the selected 30 algorithms in an idiomatic way, which aligns as closely as possible to the original pseudocode from Cormen et al. (2009). This allows us to automatically generate input/output pairs for all of them, enabling full control over the input data distribution, so long as it conforms to the preconditions of the algorithm. Further, we capture the intermediate algorithm trajectory in the form of \\\"hints\\\" (detailed in section 3.2), which allow insight into the inner workings of the algorithm. Such trajectories have already been extensively used in related work (Veli\u010dkovi\u0107 et al., 2019; 2020; Georgiev & Li\u00f3, 2020; Deac et al., 2020) and are typically crucial for OOD generalisation.\\n\\nIn the most generic sense, algorithms can be seen as manipulating sets of objects, along with any relations between them (which can themselves be decomposed into binary relations). If the sets are (partially) ordered (e.g. arrays or rooted trees), this can be imposed by including predecessor links. Therefore, algorithms generally operate over graphs.\\n\\nMotivated by existing theoretical results showing that graph neural networks align well with dynamic programming-style computations (Xu et al., 2019; Dudzik & Veli\u010dkovi\u0107, 2022), we propose a graph-oriented way to encode the data. Generally, our data is represented as a set of *n* vertices, where *n* is a hyperparameter that is provided as part of the dataset generation process.\\n\\nWhen the semantics of these nodes are not immediately clear from the task (e.g. graph algorithms naturally operate over a graph of *n* nodes), we make an appropriate modification to derive nodes. For example, in sorting algorithms, we treat every input list element as a separate node, and in string matching, we treat each character of the two input strings as a separate node.\\n\\nAll information over these graphs falls under the following categorisation:\\n\\n**Stage:**\\n- Every feature, i.e. observation in the trajectory, is either part of the input, output, or the hints. As we do not cover algorithms that perform on-line querying, for all 30 algorithms there will be exactly one snapshot of the input and output values, whereas hints will be a time-series of intermediate algorithm states.\\n\\n**Location:**\\n- Every feature is either present within the nodes, edges (pairs of nodes) or the graph. Edges are only present to represent the predecessor vertex if the input is a partially ordered. This also determines shapes of each feature, e.g. node features...\"}"}
{"id": "velickovic22a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The CLRS Algorithmic Reasoning Benchmark\\n\\nType:\\n\\nEvery feature can be of five possible types, which can determine the appropriate method for encoding/decoding it, and the appropriate loss function to use when learning to predict it:\\n\\n- **scalar**: Floating-point scalar feature. This would typically be fit using mean-squared error.\\n- **categorical**: Categorical feature over $K$ possible classes. The type corresponds typically to cross-entropy loss over the classes.\\n- **mask**: Categorical feature over two classes. This can be fit using binary cross-entropy.\\n- **mask one**: Categorical feature over two classes, where exactly one node is active (\u201cone-hot\u201d). One would generally optimise this argmax operation using categorical cross-entropy.\\n- **pointer**: Categorical feature over the $n$ nodes. To predict \u201csimilarity\u201d score against every node, and typically optimised using categorical cross-entropy (as introduced in Pointer Graph Networks (PGN) (Veli\u010dkovi\u0107 et al., 2020)).\\n\\nSpecifying a feature\u2019s stage, location and type fully determines its role in the dataflow. A tuple $(\\\\text{stage}, \\\\text{loc}, \\\\text{type}, \\\\text{values})$ is referred to as a probe. Each of the 30 algorithms has a static (w.r.t. stage, location and type) set of probes, which are considered to be a spec for the algorithm. We will later describe how these specs may be used to construct baseline architectures for the benchmark.\\n\\nEvery node is always endowed with a position scalar input probe, which uniquely indexes it\u2014the values are linearly spaced between 0 and 1 along the node index. This allows not only representing the data sequentially (when this is appropriate), but also serves as a useful tie-breaker when algorithms could make an arbitrary choice on which node to explore next\u2014we force the algorithms to favour nodes with smaller position values.\\n\\nTo illustrate these concepts further, at the end of this section we will describe the probes in detail for a popular algorithm (insertion sort).\\n\\nNote that, while we format the data in a way that clearly favours graph neural network executors, it can be easily adapted for different types of neural architectures; for example, sequence to sequence models (Sutskever et al., 2014) are of shape $n \\\\times f$; edge features are of shape $n \\\\times n \\\\times f$; graph features are of shape $f$, where $f$ is the dimension of this feature (excluding batch axis).\\n\\nGiven our current restriction on numerical predictions, scalar types will never be given in the output stage.\\n\\nOverall, CLRS-30 requires $\\\\sim 1h$ to generate, and occupies $\\\\sim 4.5\\\\text{GB}$ when uncompressed, across all 30 tasks.\\n\\n### 3.2. Hints\\n\\nHints are an important component of our benchmark, which we find fundamental in order to make progress on algorithmic reasoning. As we previously argued, the advantage of algorithms as a task is our understanding of their behaviour, and our ability to decompose them into useful subroutines that can be shared or repeatedly applied.\\n\\nWhile, implicitly, we hope that such a decomposition would happen in any learned system, even when trained just using inputs and outputs (as studied in Xu et al. (2019)), the degree to which we can measure or encourage this is limited in the typical end-to-end learning process, and often most of the generalisation happens only in-distribution (as observed by Veli\u010dkovi\u0107 et al. (2019); Xu et al. (2020); Bevilacqua et al. (2021)). The underlying algorithm may not be statistically identifiable from a small set of input/output pairs.\\n\\nConversely, a perfect decomposition of a task into small subtasks can be generated for algorithmic problems. Then, individual models for each subtask may be trained and re-composed into a solution. Such an approach will, by construction, provide strong decompositional benefits: as studied by Yan et al. (2020), perfect OOD generalisation can be observed with such models, and they can even generalise zero-shot to test algorithms that reuse their modules.\\n\\nHowever, the downstream applicability of this is potentially limited; when faced with a novel task which cannot be easily decomposed into subtasks, it can be hard to decide how to reuse the learnt modules.\\n\\nWe believe hints to lie in-between these two approaches. On one hand, they represent intermediate targets which the network should be able to predict if it performs reasoning similar to the ground truth algorithm it is supposed to mimic. Indeed, several lines of recent work (Veli\u010dkovi\u0107 et al., 2019; Georgiev & Li\u00f3, 2020; Veli\u010dkovi\u0107 et al., 2020; Deac et al., 2020) make favourable conclusions about using them, when it comes to achieving stronger OOD generalisation. Furthermore, models leveraging hints are still end-to-end models; when faced with a novel task at test-time, we don't need explicit knowledge of that task's hints in order to re-use the weights learnt on a task which had them.\\n\\nAlgorithms specify one way of attacking a problem, that is explicitly detailed through the hints. In this sense, insertion sort (to be presented shortly) is one way of implementing this.\"}"}
{"id": "velickovic22a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The CLRS Algorithmic Reasoning Benchmark\\n\\nFigure 2. A sequence of hints for insertion sorting a list \\\\([5, 2, 4, 3, 1]\\\\).\\n\\nGreen pointers correspond to the predecessor pointers (specifying the list's state throughout the algorithm's execution. Note how the head of the list always points to itself, by convention. Further, note how, at every step, the list is rewired such that the node selected by the blue pointer (slot) will point to the current iterator (pointed in red).\\n\\nTo illustrate all of the concepts outlined above, we observe the trajectories extracted by our data collection procedure on an example: insertion sorting the array \\\\([5, 2, 4, 3, 1]\\\\).\\n\\nInsertion sort uses one pointer \\\\((j)\\\\) to scan through the array, and then another pointer \\\\((i)\\\\) to slot the \\\\(j\\\\)-th item into the correct place within \\\\([0..j]\\\\). This ascertains the invariant that, after \\\\(k\\\\) steps, the subarray of the first \\\\(k\\\\) elements is completely sorted. Hence the trajectory (with \\\\(i\\\\) and \\\\(j\\\\) marked) is: \\\\([5_{i,j}, 2, 4, 3, 1]\\\\) \\\\(\\\\rightarrow\\\\) \\\\([2_{i}, 5_{j}, 4, 3, 1]\\\\) \\\\(\\\\rightarrow\\\\) \\\\([2, 4_{i}, 5_{j}, 3, 1]\\\\) \\\\(\\\\rightarrow\\\\) \\\\([2, 3_{i}, 4, 5_{j}, 1]\\\\) \\\\(\\\\rightarrow\\\\) \\\\([1_{i}, 2, 3, 4, 5_{j}]\\\\).\\n\\nAt each step, \\\\(j\\\\) scans along the array, and \\\\(i\\\\) indicates the correct place for the element that was \\\\(j\\\\)-th at the start of each iteration.\\n\\nConverting this trajectory into a graph representation requires some considerations. Requiring the model to perform explicit swapping of node values would, ultimately, require numerical predictions. To avoid it, we ask the model to predict the predecessor pointer of each node (by convention, the head of the array points to itself). Hence the actual recorded trajectory can be realised as depicted in Figure 2.\\n\\nIn this figure, green pointers correspond to the predecessor pointers, red ones point to \\\\(j\\\\), and blue ones point to \\\\(i\\\\). \\\\(i\\\\) and \\\\(j\\\\) are realised as type mask one, whereas predecessors are of type pointer\u2014and all three are stored in the nodes. The red and blue pointers represent the \\\"hints\\\" for this task.\\n\\nFinally, note that the original insertion sort pseudocode mandates that, at each iteration, \\\\(i\\\\) starts at position \\\\(j\\\\) and shifts backward until the right position is found. However, this procedure can be performed in one step by a GNN, as it can locate the correct position by examining all relevant positions, and we can omit all of those intermediate steps.\\n\\nIn order to further illustrate how these hints are collected, we also provide an informal pseudocode for collecting hints for insertion sort in Algorithm 1:\\n\\n**Algorithm 1: Hint updates for Insertion Sort**\\n\\n**Input:**\\n- Input array \\\\(val\\\\)\\n- Positions \\\\(pos\\\\)\\n\\n**Hints:**\\n- Predecessors \\\\(pred\\\\)\\n- Iterator \\\\(iter\\\\)\\n- swap slot \\\\(slot\\\\)\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{pred}[i] &\\\\leftarrow \\\\begin{cases} \\n0 & i = 0 \\\\\\\\\\n i - 1 & i > 0 \\n\\\\end{cases} \\\\\\\\\\n\\\\text{slot} &\\\\leftarrow 0 \\\\\\\\\\n\\\\text{iter} &\\\\leftarrow 0 \\\\\\\\\\n\\\\text{while} \\\\ &\\\\text{iter} < n \\\\\\\\\\n\\\\text{iter} &\\\\leftarrow \\\\text{iter} + 1 \\\\\\\\\\n\\\\text{max node} &\\\\leftarrow \\\\text{argmax}_{j} : pos[j] < pos[iter], val[j] < val[iter] \\\\\\\\\\n\\\\text{if} \\\\ &\\\\text{val[max node]} < \\\\text{val[iter]} \\\\\\\\\\n\\\\text{slot} &\\\\leftarrow \\\\text{max node} \\\\\\\\\\n\\\\text{pred}[i] &\\\\leftarrow \\\\begin{cases} \\n\\\\text{iter} & i = \\\\text{slot} \\\\\\\\\\n\\\\text{iter} & i = \\\\text{iter} \\\\\\\\\\n\\\\text{pred}[\\\\text{slot}] & \\\\text{otherwise} \\n\\\\end{cases} \\\\\\\\\\n\\\\text{else} \\\\ &\\n\\\\text{slot} &\\\\leftarrow \\\\text{argmin}_{j} : pos[j] < pos[iter], \\\\text{val}[j] \\\\geq \\\\text{val}[iter] \\\\\\\\\\n\\\\text{pred}[i] &\\\\leftarrow \\\\begin{cases} \\n\\\\text{iter} & i = \\\\text{slot} \\\\\\\\\\n\\\\text{iter} & i = \\\\text{iter} \\\\\\\\\\n\\\\text{pred}[\\\\text{slot}] & \\\\text{otherwise} \\n\\\\end{cases} \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\n\\\\[\\n\\\\text{return pred}; \\\\\\\\\\n\\\\]\\n\\nIn the interest of illustrating the hint structures further, we provide worked examples of trajectories for three more al-\"}"}
{"id": "velickovic22a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It should be remarked that we directly expose all of the hint collection routines as Python code inside the CLRS library, allowing for direct inspection.\\n\\n4. Empirical evaluation\\n\\nHaving surveyed the specifics of CLRS-30, we now present experimental results on it for several proposed algorithmic reasoning models. We primarily investigate whether a natural ladder of model performance will emerge when extrapolating to larger inputs. Beyond this, we believe the benchmark will be useful for empirically examining many other properties of algorithmic models, such as evaluating generalisation across different graph types, task types, or various multi-task (Xhonneux et al., 2021) or continual learning setups. We make available complete implementations of our data generating, probing and model training subroutines, which should make evaluating on such settings simple to deploy.\\n\\n4.1. Baseline models\\n\\nEncode-process-decode\\n\\nFor our experimental validation, we adopt the encode-process-decode paradigm of Hamrick et al. (2018), which is a common direction for several hint-based architectures (Veli\u0107 et al., 2019; Georgiev & Li\u00f3, 2020; Veli\u0107 et al., 2020; Deac et al., 2020).\\n\\nNamely, we consider a setup with inputs \\\\( x_i \\\\) in nodes, \\\\( e_{ij} \\\\) in edges, and \\\\( g \\\\) in the graph. We first encode each of these using linear layers \\\\( f_n, f_e, f_g \\\\), to obtain encodings\\n\\n\\\\[\\n    h_i = f_n(x_i) \\\\quad h_{ij} = f_e(e_{ij}) \\\\quad h_g = f_g(g)\\n\\\\]\\n\\nWe then feed these latents through a processor network to perform one step of computation. As we are focusing on graph representation learning in the current data format, most of our processors will be realised as graph neural networks (Gilmer et al., 2017). Most generally, along every edge \\\\( (i,j) \\\\), a message from node \\\\( i \\\\) to node \\\\( j \\\\), \\\\( m_{ij} \\\\) is computed (using a message function \\\\( f_m \\\\)), and these messages are then aggregated across all neighbouring nodes using a permutation-invariant aggregation function, \\\\( \\\\bigoplus \\\\). Finally, a readout network \\\\( f_r \\\\) transforms these aggregated messages and the node encodings into processed node encodings:\\n\\n\\\\[\\n    m_{ij} = f_m(h_i, h_j, h_{ij}, h_g) \\\\\\\\\\n    m_i = \\\\bigoplus_{j \\\\in N} m_{ji} \\\\\\\\\\n    h'_i = f_r(h_i, m_i)\\n\\\\]\\n\\nOnce node encodings are updated, we can decode them to make various predictions for this step of reasoning, depending on the type of the prediction required (using relevant decoder functions \\\\( g \\\\cdot \\\\)), as prescribed in Section 3.1. Further, we keep track of previous-step node encodings \\\\( h(t-1)i \\\\), to explicitly use in a recurrent cell update (exactly as done by Veli\u0107 et al. (2019)). We opt to provide this recurrent update in order to provide long-range capacity to the model.\\n\\nLastly, we need to decide in what capacity will hints be used. We provide results for the option where hints are both decoded (used for computing the loss function) and encoded (considered as part of \\\\( x, e_{ij} \\\\) and \\\\( g \\\\)). At testing time, the encoded hint is equal to the hints decoded by the previous step, whereas we can stabilise these trajectories at training time by performing noisy teacher forcing \u2014 inspired by Noisy Nodes (Godwin et al., 2021), at each step we feed back ground-truth hints with probability 0.5. The quantity of hints is still used to determine the number of processor steps to perform at evaluation time. This requirement of knowing the hint-size can be lifted by, e.g., using termination networks (Veli\u0107 et al., 2019; Banino et al., 2021) or aligning to iterative algorithms (Tang et al., 2020).\\n\\nProcessor networks\\n\\nThe only remaining component to specify is the processor network used by our models. As this component carries the most computational load, it is also the most obvious module to sweep over. We provide all implementations and hyperparameters within our codebase. Unless otherwise specified, we assume fully-connected graphs, i.e. \\\\( N_i = \\\\{1, 2, ..., n\\\\} \\\\), hence every node is connected to every other node. We consider the following baseline processor networks:\\n\\n- **Deep Sets** (Zaheer et al., 2017); where each node is only connected to itself: \\\\( N_i = \\\\{i\\\\} \\\\) (i.e., choice of \\\\( \\\\bigoplus \\\\) is irrelevant). Such a model is popular for summary statistic tasks.\\n\\n- **Graph Attention Networks** (Veli\u0107 et al., 2017), where the aggregation function \\\\( \\\\bigoplus \\\\) is self-attention (Vaswani et al., 2017), and the message function \\\\( f_m \\\\) merely extracts the sender features: \\\\( f_m(h_i, h_j, h_{ij}, h_g) = W_h h_i \\\\). We report the best performance across GAT (Veli\u0107 et al., 2017) and GATv2 (Brody et al., 2021) attention mechanisms.\\n\\n- **Message-passing Neural Networks** (Gilmer et al., 2017), which correspond exactly to the formulation in Equation 2, with \\\\( \\\\bigoplus = \\\\max \\\\), as prescribed by previous work (Veli\u0107 et al., 2019). As a sanity check, we also attempted \\\\( \\\\bigoplus = \\\\sum \\\\) finding it underperformed on all tasks compared to \\\\( \\\\max \\\\).\\n\\n- **Pointer Graph Networks** (Veli\u0107 et al., 2020), which use only graph neighbourhoods \\\\( N_i \\\\) specified by a union of all node pointer and edge mask hints, and \\\\( \\\\bigoplus = \\\\max \\\\). This restricts the model to only reason over the edges deemed important by the inputs and hints.\\n\\n- **Memory Networks** (Sukhbaatar et al., 2015) have been\"}"}
{"id": "velickovic22a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The CLRS Algorithmic Reasoning Benchmark\\n\\nFigure 3. Validation results on eight representative algorithms in CLRS-30 (activity selector, Bellman-Ford, binary search, find maximum subarray, Graham scan, insertion sort, matrix chain order, na\u00efve string matcher), averaged over three seeds. In all cases the y-axis is between [0, 100%]. Legend: MPNN red, PGN purple, Deep Sets blue, GAT orange, Memory Networks green. Validation results for all 30 individual algorithms can be found in Appendix D.\\n\\nused in the past as baseline for investigating reasoning in neural networks (e.g. Banino et al., 2020), as they provide an alternative way to use structural dependencies in a graph by treating edges as memories and nodes as queries. Here we used latents representing node features \\\\( h_i \\\\) as queries and latents representing edge features \\\\( h_{ij} \\\\) (where there is a connecting edge and 0 otherwise) as memory inputs.\\n\\n4.2. Dataset statistics\\n\\nFor each algorithm in CLRS-30, we provide a canonical set of training, validation and test trajectories for benchmarking in- and out-of-distribution generalisation. We obtain these trajectories by running the algorithms on randomly sampled inputs that conform to their input specification. This implies, e.g., that the inputs to most graph algorithms are Erd\u0151s-R\u00e9nyi graphs (Erd\u0151s & R\u00e9nyi, 2011) with a certain edge probability. All scalar inputs are sampled from \\\\( U(0, 1) \\\\).\\n\\nFor validation, our aim is to measure in-distribution generalisation. Hence we sample inputs of 16 nodes for both, and generate 1,000 trajectories for training and 32 for validation. For testing, we measure out-of-distribution generalisation, and sample 32 trajectories for inputs of 64 nodes. For algorithms where the output is on the graph stage (rather than node/edge), we generate \\\\( 64 \\\\times 64 \\\\) more trajectories, in order to equalise the number of targets across tasks.\\n\\nWe optimise our models on the training trajectories in a teacher-forced fashion, with a batch size of 32, using the Adam optimiser (Kingma & Ba, 2014) with an initial learning rate of \\\\( \\\\eta = 0.001 \\\\). We train for 10,000 steps, early stopping on the validation performance. Our models are trained on one V100 Volta GPU, requiring roughly between 1h and 30h to train, depending on the algorithm\u2019s time complexity. For example, linear-time algorithms have significantly fewer hints\u2014hence message passing steps\u2014than cubic-time ones.\\n\\n4.3. Validation (in-distribution) performance\\n\\nWe provide the in-distribution performance throughout training in Figure 3, for eight representative tasks in CLRS-30 (one per each algorithm type); see Appendix D for the full results on all 30 algorithms. In this regime, the MPNN appears to dominate for most tasks: achieving over 90% F1 score for nearly all of them. While this might seem like strong evidence in favour of the fully-connected MPNNs, their added degrees of freedom may also make MPNNs more prone to overfitting to specifics of the input (e.g. the input graphs\u2019 sizes), rather than truly learning the underlying reasoning rule. We present the out-of-distribution results next, in order to make this distinction clear.\"}"}
{"id": "velickovic22a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Average test micro-F$_1$ score of all models on all algorithm classes. The full test results for all 30 algorithms, along with a breakdown of the \\\"win/tie/loss\\\" metric, are given in Appendix C.\\n\\n| Algorithm      | MPNN | PGN  | Deep Sets | Memnet | GAT | Divide & Conquer |\\n|----------------|------|------|-----------|--------|-----|------------------|\\n| Divide & Conquer | 12.48%\u00b10.67 | 24.43%\u00b10.74 | 13.05%\u00b10.00 | 20.30%\u00b10.85 | 65.23%\u00b14.44 |\\n| Dynamic Prog.   | 66.05%\u00b17.79 | 67.19%\u00b15.33 | 67.94%\u00b17.75 | 65.10%\u00b16.44 | 70.58%\u00b16.48 |\\n| Geometry        | 64.08%\u00b16.60 | 73.27%\u00b111.18 | 45.14%\u00b111.65 | 73.11%\u00b117.19 | 61.19%\u00b17.01 |\\n| Graphs          | 37.65%\u00b18.09 | 46.80%\u00b18.66 | 24.12%\u00b15.20 | 62.79%\u00b18.75 | 60.25%\u00b18.42 |\\n| Greedy          | 75.47%\u00b16.81 | 78.96%\u00b14.59 | 53.42%\u00b120.73 | 82.39%\u00b13.01 | 75.84%\u00b16.59 |\\n| Search          | 43.79%\u00b118.29 | 37.35%\u00b119.81 | 34.35%\u00b121.67 | 41.20%\u00b119.87 | 56.11%\u00b121.56 |\\n| Sorting         | 39.60%\u00b17.19 | 14.35%\u00b14.64 | 71.53%\u00b11.09 | 11.83%\u00b18.78 | 15.45%\u00b18.46 |\\n| Strings         | 2.64%\u00b10.68 | 3.02%\u00b11.08 | 1.51%\u00b10.21 | 3.51%\u00b10.94 | 2.04%\u00b10.20 |\\n\\n4.4. Test (out-of-distribution) performance\\n\\nThe averaged out-of-distribution performance (using the early-stopped model on validation) across each of the eight algorithm types is provided in Table 1; see Appendix C for the full results on all 30 algorithms. MPNNs are unable to transfer their impressive gains to graphs that are four times larger: in fact, the PGN takes over as the most performant model when averaged across task types\u2014this aligns well with prior research (Veli\u010dkovi\u0107 et al., 2020). The outperformance is also observed when we count how frequently each model is among the best-performing models for a given algorithm, as per our \\\"win/tie/loss\\\" metric, which we explain in Appendix C. GNN models, additionally, outperform models like Deep Sets and Memory Nets, reinforcing that GNNs are a useful primitive for algorithmic reasoning (Xu et al., 2019; Dudzik & Veli\u010dkovi\u0107, 2022).\\n\\nAside from all of the above, we note that the OOD version of the CLRS-30 benchmark is highly challenging and far from solved for most tasks, making it a meaningful informant of future progress in the area. In particular, PGNs struggled on tasks requiring long-range rollouts (such as DFS), or recursive reasoning (such as Quicksort and Quickselect). This invites further research in algorithmic reasoners that can support such computation. It is further revealed that more specialised inductive biases and training regimes may be required to deal with string matching algorithms (such as KMP), and that the processor studied here tended to perform the best on tasks which were of favourable (sublinear) complexity in terms of hint counts (such as BFS, Bellman-Ford, and task scheduling).\\n\\nThe specific results we obtain with our baselines validate several bits of prior research in the area, but also demonstrate we still have a long way to go, with even simple OOD scenarios only being fit to about 50% micro-F$_1$ performance.\\n\\n5. Conclusion\\n\\nWe introduce CLRS-30, a dataset that contains trajectories from 30 classical algorithms. This benchmark constitutes an effective way to test out-of-distribution generalization and transfer, and brings a means to evaluate algorithmic reasoning learnt by neural network models. The dataset provides input/output pairs for all algorithms, as well as intermediate trajectory information (\u201chints\u201d).\\n\\nIt is our hope that CLRS-30 will be a useful tool to shepherd future research in algorithmic reasoning, as prior art in the area largely generated their own datasets, making progress tracking challenging. Further, we hope that CLRS-30 will make algorithmic reasoning a more accessible area: one does not need a background in theoretical computer science to generate the dataset, and can focus on the modelling. If we convinced you to try out our library, please consult Appendix A for detailed instructions on most common ways to interact with our platform. CLRS is in constant development, and we welcome any and all feedback.\\n\\nAcknowledgements\\n\\nCLRS-30 was developed over a long time-frame, with many useful contributions, which we kindly acknowledge here. We would like to particularly thank Borja Ibarz for numerous fixes and additions, and laying foundation for future iterations. Additionally, we warmly thank Jonathan Godwin, Sadegh Mahdavi, Euan Ong, MohamedElfatih Salah, Ahmed Elhag, Andreea Deac, Frederik Nijweide, Andrew Dudzik, Thomas Kipf, Amin Barekatain and Dobrik Georgiev for their support, and identifying numerous bugs during development. Finally, we thank Kim Stachenfeld, Nate Kushman and Daan Wierstra for reviewing the paper prior to submission, and anonymous reviewers for their careful feedback, strengthening the paper significantly.\"}"}
{"id": "velickovic22a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The CLRS Algorithmic Reasoning Benchmark\\n\\nReferences\\n\\nAho, A. V., Hopcroft, J. E., and Ullman, J. D. The design and analysis of computer algorithms. Reading, 1974.\\n\\nAlet, F., Lozano-Perez, T., and Kaelbling, L. P. Modular meta-learning. volume 87 of Proceedings of Machine Learning Research. PMLR, 2018.\\n\\nBanino, A., Badia, A. P., K\u00f6ster, R., Chadwick, M. J., Zambaldi, V., Hassabis, D., Barry, C., Botvinick, M., Kumaran, D., and Blundell, C. Memo: A deep network for flexible combination of episodic memories. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rJxlc0EtDr.\\n\\nBanino, A., Balaguer, J., and Blundell, C. Pondernet: Learning to ponder. arXiv preprint arXiv:2107.05407, 2021.\\n\\nBelkin, M., Hsu, D., and Xu, J. Two models of double descent for weak features. arXiv preprint arXiv:1903.07571, 2019.\\n\\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.\\n\\nBellman, R. On a routing problem. Quarterly of applied mathematics, 16(1):87\u201390, 1958.\\n\\nBengio, Y., Lodi, A., and Prouvost, A. Machine learning for combinatorial optimization: a methodological tour d\u2019horizon. European Journal of Operational Research, 2020.\\n\\nBentley, J. Programming pearls: Algorithm design techniques. Communications of the ACM, 27(9):865\u2013873, 1984.\\n\\nBevilacqua, B., Zhou, Y., and Ribeiro, B. Size-invariant graph representations for graph classification extrapolations. In International Conference on Machine Learning, pp. 837\u2013851. PMLR, 2021.\\n\\nBrody, S., Alon, U., and Yahav, E. How attentive are graph attention networks? arXiv preprint arXiv:2105.14491, 2021.\\n\\nCappart, Q., Ch\u00e9telat, D., Khalil, E., Lodi, A., Morris, C., and Veli\u010dkovi\u0107, P. Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021.\\n\\nChen, Z., Chen, L., Villar, S., and Bruna, J. Can graph neural networks count substructures? arXiv preprint arXiv:2002.04025, 2020.\\n\\nCormen, T. H., Leiserson, C. E., Rivest, R. L., and Stein, C. Introduction to algorithms. MIT press, 2009.\\n\\nCorso, G., Cavalleri, L., Beaini, D., Li\u00f2, P., and Veli\u010dkovi\u0107, P. Principal neighbourhood aggregation for graph nets. arXiv preprint arXiv:2004.05718, 2020.\\n\\nDeac, A., Bacon, P.-L., and Tang, J. Graph neural induction of value iteration. arXiv preprint arXiv:2009.12604, 2020.\\n\\nDeac, A.-I., Veli\u010dkovi\u0107, P., Milinkovic, O., Bacon, P.-L., Tang, J., and Nikolic, M. Neural algorithmic reasoners are implicit planners. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nDijkstra, E. W. et al. A note on two problems in connection with graphs. Numerische mathematik, 1(1):269\u2013271, 1959.\\n\\nDu, Y., Czarnecki, W. M., Jayakumar, S. M., Pascanu, R., and Lakshminarayanan, B. Adapting auxiliary losses using gradient similarity, 2018.\\n\\nDudzik, A. and Veli\u010dkovi\u0107, P. Graph neural networks are dynamic programmers. arXiv preprint arXiv:2203.15544, 2022.\\n\\nDwivedi, V. P., Joshi, C. K., Laurent, T., Bengio, Y., and Bresson, X. Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.\\n\\nErd\u0151s, P. and R\u00e9nyi, A. On the evolution of random graphs. In The structure and dynamics of networks, pp. 38\u201382. Princeton University Press, 2011.\\n\\nFloyd, R. W. Algorithm 97: shortest path. Communications of the ACM, 5(6):345, 1962.\\n\\nFord Jr, L. R. and Fulkerson, D. R. Flows in networks. Princeton university press, 2015.\\n\\nFreivalds, K., Ozolins, E., and \u0160ostaks, A. Neural shuffle-exchange networks-sequence processing in o(n log n) time. In Advances in Neural Information Processing Systems, pp. 6630\u20136641, 2019.\\n\\nGavril, F. Algorithms for minimum coloring, maximum clique, minimum covering by cliques, and maximum independent set of a chordal graph. SIAM Journal on Computing, 1(2):180\u2013187, 1972.\\n\\nGeorgiev, D. and Li\u00f2, P. Neural bipartite matching. arXiv preprint arXiv:2005.11304, 2020.\\n\\nGilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\"}"}
{"id": "velickovic22a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The CLRS Algorithmic Reasoning Benchmark\\n\\nGodwin, J., Schaarschmidt, M., Gaunt, A. L., Sanchez-Gonzalez, A., Rubanova, Y., Veli\u010dkovi\u0107, P., Kirkpatrick, J., and Battaglia, P. Simple gnn regularisation for 3d molecular property prediction and beyond. In International Conference on Learning Representations, 2021.\\n\\nGraham, R. L. An efficient algorithm for determining the convex hull of a finite planar set. Info. Pro. Lett., 1:132\u2013133, 1972.\\n\\nGraves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.\\n\\nGriffiths, T., Callaway, F., Chang, M., Grant, E., Krueger, P., and Lieder, F. Doing more with less: meta-reasoning and meta-learning in humans and machines. Current Opinion in Behavioral Sciences, October 2019.\\n\\nHamrick, J. B., Allen, K. R., Bapst, V., Zhu, T., McKee, K. R., Tenenbaum, J. B., and Battaglia, P. W. Relational inductive bias for physical construction in humans and machines. arXiv preprint arXiv:1806.01203, 2018.\\n\\nHennigan, T., Cai, T., Norman, T., and Babuschkin, I. Haiku: Sonnet for JAX, 2020. URL http://github.com/deepmind/dm-haiku.\\n\\nHierholzer, C. and Wiener, C. \u00a8Uber die m\u00a8oglichkeit, einen linienzug ohne wiederholung und ohne unterbrechung zu umfahren. Mathematische Annalen, 6(1):30\u201332, 1873.\\n\\nHoare, C. A. Algorithm 65: find. Communications of the ACM, 4(7):321\u2013322, 1961.\\n\\nHoare, C. A. Quicksort. The Computer Journal, 5(1):10\u201316, 1962.\\n\\nJacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31. 2018.\\n\\nJarvis, R. A. On the identification of the convex hull of a finite set of points in the plane. Information processing letters, 2(1):18\u201321, 1973.\\n\\nJoshi, C. K., Cappart, Q., Rousseau, L.-M., Laurent, T., and Bresson, X. Learning tsp requires rethinking generalization. arXiv preprint arXiv:2006.07054, 2020.\\n\\nKaiser, \u0141. and Sutskever, I. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.\\n\\nKhalil, E., Dai, H., Zhang, Y., Dilkina, B., and Song, L. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp. 6348\u20136358, 2017.\\n\\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\nKnuth, D. E. Fundamental algorithms. 1973.\\n\\nKnuth, D. E., Morris, Jr, J. H., and Pratt, V. R. Fast pattern matching in strings. SIAM journal on computing, 6(2):323\u2013350, 1977.\\n\\nKool, W., van Hoof, H., and Welling, M. Attention, learn to solve routing problems! arXiv preprint arXiv:1803.08475, 2018.\\n\\nKruskal, J. B. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical society, 7(1):48\u201350, 1956.\\n\\nLake, B. M. Compositional generalization through meta sequence-to-sequence learning. In Advances in Neural Information Processing Systems 32, pp. 9791\u20139801. 2019.\\n\\nLawler, E. L. The traveling salesman problem: a guided tour of combinatorial optimization. Wiley-Interscience Series in Discrete Mathematics, 1985.\\n\\nLi, Y., Gimeno, F., Kohli, P., and Vinyals, O. Strong generalization and efficiency in neural programs. arXiv preprint arXiv:2007.03629, 2020.\\n\\nLiu, C., Zhu, L., and Belkin, M. Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning. CoRR, abs/2003.00307, 2020.\\n\\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\\n\\nMikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernock\u00b4y, J. Empirical evaluation and combination of advanced language modeling techniques. In INTERSPEECH, pp. 605\u2013608, 2011.\\n\\nMoore, E. F. The shortest path through a maze. In Proc. Int. Symp. Switching Theory, 1959, pp. 285\u2013292, 1959.\\n\\nPrim, R. C. Shortest connection networks and some generalizations. The Bell System Technical Journal, 36(6):1389\u20131401, 1957.\\n\\nRichter, O. and Wattenhofer, R. Normalized attention without probability cage. arXiv preprint arXiv:2005.09561, 2020.\\n\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.\"}"}
{"id": "velickovic22a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "velickovic22a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Interfacing with the CLRS benchmark\\n\\nThe CLRS benchmark is publicly hosted on GitHub: https://github.com/deepmind/clrs. All code and artifacts are released under an Apache 2.0 license, which is highly permissive.\\n\\nWithin `clrs/examples/run.py`, we demonstrate an extensively configurable example script that evaluates a specific baseline on CLRS-30. Our baselines are provided in JAX and Haiku (Hennigan et al., 2020), but the dataset is generated using NumPy, making it possible to create learning pipelines in virtually any framework, including PyTorch and TensorFlow.\\n\\nWe will now highlight three key ways in which researchers can interface with the library.\\n\\nA.1. Evaluating a new baseline on CLRS-30\\n\\nTo support a new baseline, the recommended path depends on how fundamentally different the baseline is to an encode-process-decode GNN. In most cases, we anticipate that only the processor network needs changing, and the remainder of the architecture can match our baselines. In this case, it is only necessary to implement the new processor network within `clrs/src/processors.py` and appropriately set `self.mpnn` within the `construct` processor method in `clrs/src/baselines.py`.\\n\\nFor more fundamentally different baselines, it is necessary to create a new class that extends the `Model` API (as found within `clrs/src/model.py`). `clrs/src/baselines.py` provides one example of how this can be done efficiently, for the case of our baselines.\\n\\nA.2. Modifying the data distribution of CLRS-30\\n\\nIf users want to train and/or evaluate the models on different versions of the tasks given in CLRS-30, the key routines to modify are located in `clrs/src/samplers.py`. The easiest modification concerns the graph sizes and/or numbers of trajectories. They can be directly changed by modifying the `CLRS30` dictionary near the top of the file.\\n\\nFor more elaborate modifications, e.g. to the specific data sampling distributions, the users would need to modify and/or extend the relevant sampler class. As a guiding example, we provide a `SortingSampler` class which is convenient for generating inputs for sorting algorithms. The specific sampler used for each task is provided in the `SAMPLERS` dictionary towards the end of the file.\\n\\nA.3. Adding new algorithms to CLRS\\n\\nAs the most elaborate of the three workflows, adding a new algorithm to the task suite requires following several steps, which are potentially comprehensive, depending on the complexity of the algorithm. However, the CLRS benchmark code still provides may helper routines for probing and batching that facilitate inclusion of novel algorithms. The steps are as follows:\\n\\n1. First, determine the input/hint/output specification of your algorithm, and include it within the `SPECS` dictionary of `clrs/src/specs.py`.\\n2. Implement the desired algorithm in an abstractified form. Examples of this can be found throughout the `clrs/src/algorithms/` folder.\\n3. Next, choose appropriate moments within the algorithm's execution to create probes that capture the inputs, outputs and all intermediate state (using the `probing.push` function).\\n4. Once generated, probes can be prepared using the `probing.finalize` method, and should be returned together with the algorithm output.\\n5. Lastly, implement an appropriate input data sampler for your algorithm, and include it within the `SAMPLERS` dictionary within `clrs/src/samplers.py`.\"}"}
{"id": "velickovic22a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Matrix Chain Order\\n\\nAs a representative dynamic programming algorithm, we visualise the steps of the procedure for optimising the order of multiplications in a chain of matrices, for multiplying matrices of size $(10 \\\\times 30)(30 \\\\times 5)(5 \\\\times 60)$, assuming a $O(n^3)$-time multiplication algorithm.\\n\\nThe algorithm proceeds by filling up an \u201cupper-triangular\u201d part of a dynamic programming matrix, where cell $[i,j]$ corresponds to the optimal number of operations when multiplying all the matrices between the $i$th and $j$th. Such an algorithm may also be represented in a \u201cpyramidal\u201d form as below:\\n\\nAdditionally, the algorithm maintains (and returns) the optimal way to recursively divide each subsequence into two (by storing the optimal dividing point, in green). Here, it is optimal to first multiply $(10 \\\\times 30)(30 \\\\times 5)$ (yielding 1,500 operations), then multiply the remaining matrices as $(10 \\\\times 5)(5 \\\\times 60)$ (yielding 3,000 operations; 4,500 in total).\\n\\nNote that every pointer points into one of the original $n$ input nodes (at the lowest level), and how each cell of the pyramid corresponds to a pair of input nodes (specifying the corresponding range). Therefore, rather than creating $O(n^2)$ auxiliary nodes, we instead record all relevant values above as edge scalars and edge pointers, and store nodes only for the lowest level of the pyramid. Further, whether or not a particular edge has been populated yet (the \u201c\u221e\u201d indicator above) is stored as an additional binary flag.\\n\\nBellman-Ford\\n\\nAs a representative graph algorithm, we visualise the steps of the Bellman-Ford algorithm for finding single-source shortest paths in a given graph.\\n\\nInitially, the source node is labelled with distance zero, and all other nodes with distance \u201c\u221e\u201d (which, once again, is represented as a binary node hint). The algorithm then iteratively relaxes all edges as follows, until convergence is achieved:\\n\\nBesides updating the distance values, the algorithm also maintains, and returns, the predicted shortest path tree \u2013 for each node, a pointer to its predecessor along the optimal path from the source. By convention, the source node points to itself. These pointers are visualised in green.\\n\\nNa\u00efve String Matcher\\n\\nAs a representative string algorithm, we visualise the steps of the na\u00efve string matcher, for detecting string \u201cab\u201d inside the string \u201caab\u201d.\"}"}
{"id": "velickovic22a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this case, each character of the two strings is given a separate node, and three sets of indices are maintained: indicating the start of the current candidate match (in blue); and the current position being checked in both the haystack (red) and the needle (purple). The algorithm scans candidate positions left-to-right until a full match is detected for the first time.\\n\\nAdditionally, each character is tagged with its predecessor in the string (in green), and a binary flag indicating which of the two strings it belongs to (not shown here).\\n\\nC. Test results for all algorithms\\n\\nTest performance for all 30 algorithms in CLRS-30 may be found in Table 2. In addition, we provide a \\\"win-tie-loss\\\" metric as another way of differentiating model performance, which is less sensitive to outliers. The resulting counts are provided in Table 3, and are computed as follows:\\n\\n- Let $\\\\mu_A(M)$ and $\\\\sigma_A(M)$ be the mean and standard deviation of model $M$'s test performance on algorithm $A$ (as in Table 2).\\n- We say that model $A$ outperforms model $B$ on algorithm $A$\u2014denoted by $A \\\\succ A B$\u2014if $\\\\mu_A(A) - \\\\sigma_A(A) > \\\\mu_A(B)$.\\n- If $\\\\forall X \\\\neq A. A \\\\succ A X$, then model $A$ wins on algorithm $A$.\\n- Otherwise, if $\\\\exists X. X \\\\succ A A$, then model $A$ loses on algorithm $A$.\\n- Otherwise, model $A$ is tied on algorithm $A$.\\n\\nThe win/tie/loss counts are then aggregated across all algorithms to obtain a metric for each model. As already mentioned, the details of this on a per-algorithm level are given in Table 3.\\n\\nD. Validation results individual plots\\n\\nValidation performance for all 30 algorithms in CLRS-30 may be found in Figure 4. For convenience, we also report the early-stopped validation performance in Table 4.\"}"}
{"id": "velickovic22a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm          | Test performance of all models on all algorithms. |\\n|--------------------|--------------------------------------------------|\\n| Insertion Sort     | 40 \u00b1 28                                          |\\n| Heapsort           | 64 \u00b1 7                                           |\\n| Graham Scan        | 64 \u00b1 7                                           |\\n| DFS                | 7 \u00b1 7                                            |\\n| Floyd-Warshall     | 73 \u00b1 7                                           |\\n| Binary Search      | 98 \u00b1 94%                                         |\\n| BFS                | 7 \u00b1 7                                            |\\n| Bellman-Ford       | 50 \u00b1 25%                                         |\\n| DFS                | 7 \u00b1 7                                            |\\n| Activity Selector  | 12 \u00b1 22%                                         |\\n| Find Max. Subarray | 36 \u00b1 36%                                         |\\n| Dijkstra           | 98 \u00b1 94%                                         |\\n| Bridges            | 60 \u00b1 32%                                         |\\n| Minimum            | 78 \u00b1 78%                                         |\\n| KMP Matcher        | 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1 3 \u00b1"}
