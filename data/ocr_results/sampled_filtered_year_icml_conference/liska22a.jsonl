{"id": "liska22a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"StreamingQA: A Benchmark for Adaptation to New Knowledge over Time\\n\\nAdam Li\u0161ka\\n\\nTom\u00e1\u0161 Kocisk\u00fd*\\n\u2660\\n\\nElena Gribovskaya*\\n\u2660\\n\\nTayfun Terzi\\n\\nEren Sezener\\n\\nDevang Agrawal\\n\\nCyprien de Masson d'Autume\\n\\nTim Scholtes\\n\\nManzil Zaheer\\n\\nSusannah Young\\n\\nEllen Gilsenan-McMahon\\n\\nSophia Austin\\n\\nPhil Blunsom\\n\\nAngeliki Lazaridou\\n\\nProceedings of the 39th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).\\n\\nAbstract\\n\\nKnowledge and language understanding of models evaluated through question answering (QA) has been usually studied on static snapshots of knowledge, like Wikipedia. However, our world is dynamic, evolves over time, and our models\u2019 knowledge becomes outdated. To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM under-perform those with a retrained LM. For questions about higher-frequency named entities, parametric updates are particularly beneficial. In our dynamic world, the StreamingQA dataset enables a more realistic evaluation of QA models, and our experiments highlight several promising directions for future research.\\n\\n1. Introduction\\n\\nQuestion answering (QA) allows us to interrogate models for their language understanding, knowledge, and reasoning abilities, while also being useful in various knowledge-oriented applications such as personal assistants or web search. The questions that people ask span all our knowledge and can be about any point in the history, although often they are about the most recent events that happened in the last few weeks or days. Consider examples in Table 1 that ask about events as distant as 4 years before or as recent as the day when the question was asked. As the world and knowledge evolve, we need our QA models to adapt to new information, to not forget the past, and to maintain an up-to-date world model to make our interaction with such systems more meaningful. To evaluate and improve models\u2019 ability to adapt, we need a dataset with temporal grounding of both questions and knowledge\u2014dates when questions were asked and publication dates of documents. As the currently available QA datasets are not suitable for this, we propose a novel dataset and subsequently perform a systematic study of adaptation in the state-of-the-art QA models.\\n\\nPrevious research has often focused on answering questions about individual passages or books (Rajpurkar et al., 2016; Ko\u010disk\u00fd et al., 2018), answering about static structured (Jia et al., 2021), or unstructured knowledge corpora such as Wikipedia (Lee et al., 2019a). More recent work has considered answering questions about knowledge with temporal grounding of facts in a knowledge graph (Saxena et al., 2021; Jia et al., 2021) or news articles (Wang et al., 2021; Dhingra et al., 2021) but without grounding the questions.\"}"}
{"id": "liska22a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"StreamingQA with a question date; or have repurposed questions from other datasets and added question dates (Zhang & Choi, 2021) but still answered over a static snapshot of Wikipedia. We present a new dataset, StreamingQA, that provides temporal context of both, the questions and the knowledge required to answer them. The dataset contains questions written by annotators or generated with a large-scale LM. The questions are answerable from a streaming knowledge corpus of time-stamped English WMT news articles published between 2007 and 2020 (see Figure 1). Having temporal metadata for questions and articles enables us to ingest new knowledge periodically, in a streaming setup, and evaluate on questions asked during that period. We consider questions about recent and past knowledge separately to measure adaptation and forgetting. Moreover, question dates allow to ask questions with relative time specifications (e.g., \u201c3 months ago\u201d), which are under-represented in the existing QA datasets. Lastly, news domain, compared to often used Wikipedia, provides more realistic challenges for open-book retrieval with redundant, noisy, and sometimes conflicting information.\\n\\nPrevious work demonstrated that large LMs struggle with temporal generalization, a type of domain shift that occurs when a model at test time needs to understand new knowledge, named entities, and topics (Lazaridou et al., 2021; R\u00f6ttger & Pierrehumbert, 2021). In this work, we leverage StreamingQA to quantify similar adaptation in existing parametric (closed-book) and semi-parametric (open-book) QA models that today are frequently based on such LMs. Our findings suggest that parametric adaptation improves QA performance for open-book approaches like RAG (Lewis et al., 2020a) and FiD (Izacard & Grave, 2020) (Section 4.2). Moreover, a more granular, frequency-based analysis (Section 4.3) suggests that parametric and semi-parametric approaches to adaptation are complementary: parametric adaptation improves accuracy of questions about frequent knowledge/named entities, where semi-parametric, dense retrieval-based methods can under-perform (Liu et al., 2021). In contrast, semi-parametric adaptation helps with less frequent knowledge where retrieval is less confused by redundancy and ambiguity of information associated with more frequent names, where the parametric LMs struggle. In the closed-book setup, we find that incremental fine-tuning works reasonably well without causing catastrophic forgetting (Section 4.1), and it also results in substantially lower computational costs than full model retraining.\\n\\nLastly, we also establish benchmarks for less computation-intensive QA tasks (Section 4.5): one-step adaptation and the usual static open-book QA task.\\n\\n2. StreamingQA Dataset and Task\\n\\nIn this section, we introduce a new QA dataset and a task to evaluate models' adaptation and forgetting of knowledge. We require, in addition to questions, temporal metadata: a question date (when the question could have been asked) and a knowledge date (when an article that answers this question was published). Using this metadata enables us to evaluate how well the model understands new knowledge that becomes available incrementally at evaluation time (see Figure 1). We also use the timestamps to split the training and evaluation sets into non-overlapping historical periods. See Table 2 for examples of questions.\\n\\nTo construct the StreamingQA dataset, we consider 14 years (2007\u20132020) of English WMT news articles (Akhbardeh et al., 2021), together with their publication dates, as our knowledge corpus (approx. 11M articles). Specifically, given an article, we first generate a question date, that is, the date when we want the question to be asked (Section 2.2), and subsequently, either (i) automatically generate questions (Section 2.3), or (ii) ask human annotators to write questions (Section 2.4). Lastly, to reduce noise, we apply automatic and human filtering to the questions, and collect additional reference answers (Section 2.5). We present additional statistics in Section 2.6.\\n\\nWe consider a streaming task (Section 2.1): we split questions into four quarterly sets over 2020 based on their question dates, where questions in each quarter are to be answered from articles published up to and including that quarter. For the adaptation and forgetting analysis, we further split the quarterly evaluation sets into the recent subset and the past subset. The dataset is constructed to have an approximately equal number of each. As recent questions cover 2020 and past questions cover all history uniformly, the overall distribution is biased towards the present, as we\"}"}
{"id": "liska22a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Examples of written and generated questions from the recent and past subsets.\\n\\n| Question Date | Question | Answer | Passage Date | Subset | Added Time | Gen./Wri. |\\n|---------------|----------|--------|--------------|--------|------------|-----------|\\n| 30.05.2020    | What does Donald Trump, US president, call his 2020 plan to expedite the development of a COVID-19 vaccine? | Operation Warp Speed | 19.05.2020 | Recent | Written | | |\\n| 18.04.2020    | In the UK, as of Sunday, April 12, 2020, how much money in loans had been given to firms seeking cash to survive the coronavirus crisis? | \u00a3800 million | 12.04.2020 | Recent | Written | | |\\n| 28.04.2020    | Which soft drinks company was 2010 Australian favourite racing driver Mark Webber's team sponsored by? | Red Bull | 29.03.2010 | Past | Written | | |\\n| 26.03.2020    | Where did the research that claimed up to 50 per cent of the UK population may have already contracted the coronavirus, take place? | University of Oxford | 25.03.2020 | Recent | Generated | | |\\n| 11.09.2020    | Which player scored for St Mirren in November 2008? | Franco Miranda | 16.11.2008 | Past | Absolute | Generated | |\\n| 08.05.2020    | Which hospital is Jamie Cooper receiving treatment at 13 years ago? | Selly Oak Hospital | 01.11.2007 | Past | Relative | Generated | |\\n\\nwould expect it in an actual QA system. The recent subset contains questions about the most recent knowledge, i.e., articles published in the month before the question date; and the past subset contains questions asked during that quarter about articles published between 2007 and the question date. Furthermore, for training and validation, we use automatically generated questions asked during 2007\u20132019 with an analogous split between the recent and past subsets.\\n\\n2.1. StreamingQA Task: QA with Temporal Metadata\\n\\nFormally, our dataset consists of triples of question date, question, and answer, \\\\( \\\\{ (d_q,i, q_i, a_i) \\\\} \\\\), and a knowledge corpus of publication dates and documents, \\\\( \\\\{ (d_c,j, c_j) \\\\} \\\\). For a given time period \\\\( t = [t_s, t_e] \\\\) (e.g., January to March 2020), we consider questions asked during that period, \\\\( Q_t = \\\\{ (d_q,i, q_i, a_i) \\\\in Q : t_s \\\\leq d_q,i \\\\leq t_e \\\\} \\\\), about the corresponding subset of the knowledge corpus published until then, \\\\( C_{\\\\leq t} = \\\\{ (d_c,j, c_j) \\\\in C : d_c,j \\\\leq t_e \\\\} \\\\). To answer a question in \\\\( Q_t \\\\), we generate an answer using the corresponding knowledge corpus, \\\\( p(a_i | q_i, d_q,i, C_{\\\\leq t}) \\\\).\\n\\n2.2. Questions Dates\\n\\nWe need to generate question dates that are plausible with respect to article dates and make sure that the dates and events are consistent. For evaluation sets, to create recent subset questions we sample a document with a publication date \\\\( d_c \\\\sim U[Dec2019, Dec2020] \\\\) and a question date \\\\( d_q \\\\sim U[d_c, d_c + 30 \\\\text{days}] \\\\). To create past subset questions, we sample a document with a publication date \\\\( d_c \\\\sim U[2007, Dec2020] \\\\) and a question date \\\\( d_q \\\\sim U[\\\\max(d_c, \\\\text{Jan2020}), \\\\max(d_c, \\\\text{Jan2020}) + 365 \\\\text{days}] \\\\). The articles are distributed uniformly within a month, or across all available history prior to the question date 4, for the two subsets respectively. For training and validation sets, we consider question dates in \\\\( [2007, Dec2019] \\\\) and aim for an article distribution given a question date similar to above.\\n\\n2.3. Automatically Generated Questions\\n\\nWe use automatic question generation as a scalable way to obtain questions grounded at different points in time. 4 We filter out samples with question date not in 2020.\\n\\n(a) Distribution of first word for written and generated questions.\\n(b) Question length histograms and means.\\n(c) Answer length histograms and means.\\n\\nFigure 2. Details of the StreamingQA dataset. Questions are generated through few-shot prompting of a large LM (Rae et al., 2021), given an evidence document and a target answer drawn from named entities, dates, and prepositional phrases contained as spans in the document. A challenge with creating questions for open-domain QA is that they need to be specific enough when considered in the context of all articles in the knowledge corpus. We first over-generate questions-answer pairs and then apply heuristic filters to eliminate trivial and/or low quality candidates (Appendix A.4). For questions included in the past subset, we append absolute or relative time specifications to question text, e.g., \u201c3 months ago\u201d or \u201cin May 2017\u201d (Appendix A.5), unless the text already contains such a specification or the answer is a date.\\n\\n2.4. Human Written Questions\\n\\nHuman annotators were asked to write questions about a news article provided together with its publication date and desired question date. We chose annotators whose first language is English, who are in the US or the UK, and\"}"}
{"id": "liska22a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. StreamingQA dataset statistics.\\n\\n|             | Recent | Past | All   |\\n|-------------|--------|------|-------|\\n|             | Questions | Articles | Questions | Articles | Questions | Articles |\\n| Train       | 49,451  | 49,339 | 49,951 | 49,784 | 99,402  | 98,872   |\\n| Valid       | 4,966   | 4,965  | 4,973  | 4,971  | 9,939   | 9,932    |\\n| Eval-Generated | 15,550 | 15,385 | 12,070 | 11,848 | 27,620  | 26,852   |\\n| Eval-Written | 4,521  | 1,545  | 4,237  | 1,448  | 8,758   | 2,993    |\\n\\nhave a university education. Each annotator had to write up to five questions and answers about the details of the events described in the article, and framed these questions as if they were asking another person. Each participant created about 15 questions on average. We explicitly asked the annotators to include enough context in the questions to make the questions as unambiguous as possible for the open-book setup.\\n\\nThe full details of our study design, including compensation rates, were reviewed by DeepMind's independent ethical review committee. All participants provided informed consent prior to completing tasks and were reimbursed for their time. It is our policy that researchers must pay workers/participants at least the living wage for their location.\\n\\n2.5. Quality Filtering\\n\\nWe filtered both generated and human written questions for quality in two stages. First, we asked annotators to filter for good/bad question, similar to Kwiatkowski et al. (2019), filtering for factual, unambiguous, grammatical questions. To judge ambiguity, we have additionally provided the question date and asked not to assume a particular location (e.g., US). To include a question, we need 3 annotators to agree.\\n\\nSecondly, we asked annotators to answer each question given the original passage, its publication date, and the question date. Annotators first selected parts of the passage that supported the answer, and then wrote a short answer in their own words. We did not require the answers to be sub-strings of the passage. We only kept questions where annotators could provide answers, and obtained additional references in the same way.\\n\\n2.6. Statistics\\n\\nThe StreamingQA dataset contains about 28k generated questions and about 8.8k human written questions for evaluation, and 100k and 10k questions for training and validation, respectively. See Table 3 for details. In Figure 2, we see that human written questions and answers tend to be somewhat longer. Based on the first question word distribution, we have a diverse set of both human written and generated questions, with the latter slightly biased to \\\"Which\\\"/\\\"Where\\\"/\\\"When\\\", likely due to prompting of the large LM. For written questions, many start with \\\"In\\\", which is often annotators providing temporal context so that questions stand without the article in the open-book setting. Examining the answer types on the written evaluation sets by automatic labeling, we have about 46.9%, 40.6%, and 12.4% of named entity, phrase, and date answers, respectively (see Appendix A.2). About 6% of evaluation reference answers are seen among answers to questions in the training set, and 23% of training questions have an answer contained in evaluation reference answers.\\n\\n3. Experimental Setup\\n\\nTo evaluate how well parametric and semi-parametric approaches to QA ingest and understand unseen information, we consider an auto-regressive, left-to-right Transformer-XL (TXL) (Dai et al., 2019) language model as our parametric, closed-book QA model (CB), and use it as the underlying LM for our RAG-style (Lewis et al., 2020b) semi-parametric, open-book QA model (OB). We also consider a more recent open-book model based on Fusion-in-Decoder (FID) that uses a T5 (Raffel et al., 2020) sequence-to-sequence model.\\n\\nWe use the standard metrics for QA: the F1 and the exact match (EM), after normalizing the answers, in the same way as Rajpurkar et al. (2016). These are suitable for the on average short answers in our dataset.\\n\\nTo include the temporal metadata, we prefix publication dates to articles (\u201cThursday, February 7, 2019. [article text]\u201d), and for questions we add question dates (\u201cToday is Wednesday, May 6, 2020. [question text]\u201d).\\n\\n3.1. Language Model Pretraining and Finetuning\\n\\nWe consider three setups of TXL training: TXLS model is trained on WMT articles until the end of December 2019 (approx. 10.1M articles), and is missing knowledge required for answering questions in the recent subset. TXLR model is re-trained from scratch on all WMT articles until the end of December 2020, i.e., including the evaluation period (approx. 11.4M articles). Lastly, TXLFT model is TXLS that we additionally iteratively fine-tune on the 2020 monthly article sets (each approx. 100k articles). TXLS and TXLR are trained for 200k steps on 32 TPUv3, whereas fine-tuning is performed for 10k steps.\\n\\nOur TXL has 18 layers and 1,280 hidden units, resulting in\\n\\n7 For T5, due to its existing pre-training, we control the knowledge of our model less precisely. However, both the T5 model and the BERT model (in DPR) were released in or before 2019, and so don\u2019t contain any knowledge from 2020, our evaluation period.\\n\\n8 The best checkpoint of month $N$ is used as the starting point for fine-tuning on the articles of month $N + 1$.\\n\"}"}
{"id": "liska22a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"StreamingQA highlights challenges of temporal reasoning and invites further research into this area: the past subset contains questions with the relative time specification, where retrieval struggles to extract relevant passages. For fine-tuning, we consider a vanilla setup without delving deeply into more sophisticated continual learning approaches. Future work should take in-depths look at how best to adapt QA models, and the problem of what to compress into weights or what to add to the search space. While we study adaptation to new knowledge, retrieving conflicting information due to updated knowledge (e.g., \u201cHow many seasons are in Game of Thrones?\u201d) is another important direction we did not tackle here.\\n\\n7. Dataset Toxicity Discussion\\n\\nToxic content is a concern in both human created and automatically generated content. We provide a discussion and describe our filtering of such content here. Our setup poses particular challenges as our questions and answers are based on news. First, answers in the dataset follow information in the articles regardless of the factual basis of the articles. While most of the news articles in WMT are from reputable news sources, news in general can contain content that may be considered toxic, such as graphic descriptions of crimes, or some quotes or opinions. Second, as some questions and answers are generated using a large language model, there is a risk that it may generate toxic content; however, we want to note that the generation process is constrained by conditioning on the article and a substring answer and the subsequent automatic filtering. Third, our dataset overall is intended to evaluate adaptation of models to new information in news over time, and therefore, it may not be applicable to settings where the assumptions we made don't apply.\\n\\nWe aimed to create a balanced process that identifies most of the toxic content while decreasing the risk of removing false positives. To identify toxic content, we used the Perspective API which provides classifiers for several categories of toxic content (identity attack, insult, threat, profanity, sexually explicit, severe toxicity). We decided to use the specific classifiers instead of the generic toxicity classifier because our initial annotations indicated that the specific classifiers perform better. Removing content needs to be done with care as these classifiers do contain false positives (e.g., people, [Republic of] Niger, shoot, death, abuse, balls [in sports], and [last] names which bear phonetic similarity to insults), and removing too many such examples may cause harm by decreasing representation of some groups (e.g., black, muslim, jewish, LGBTQIA+ minorities). Through manual annotation of the questions with the highest toxicity scores, we have determined thresholds for removing questions as follows: for each 0.05 band of the scores from 1 to 0 (e.g., [1.0, 0.95], [0.95, 0.90], . . . ), we remove questions in each band until two subsequent bands contain fewer than 30% toxic questions (judged by two annotators on a sample of 50 per band). The first of these two subsequent bands is also removed. We annotated more than 5.5k examples throughout this process. As the annotation judgements for filtering were done by a small group of annotators, we cannot claim that the annotation had perfect representivity nor that the annotators had full cultural context from all possible views. For our manual annotation, we adapted the Perspective API classifier definitions in a minor way (see Appendix A.6). This filtering resulted in removing about 0.57% of questions (0.60%, 0.61%, 0.43%, 0.65% from Train, Valid, Eval-Generated, Eval-Written), and thresholds of 0.75 for identity attack, 0.80 for insult, 0.65 for profanity, 0.55 for severe toxicity, 0.85 for sexually explicit, and 0.90 for threat. Subsequently, we estimated that 0.5% of toxic questions remain (sample of 1k questions). We provide the automatic toxicity scores as part of the data release. This approach was formed with input from DeepMind\u2019s ethics and safety teams, and with guidance from our multidisciplinary leadership group which advises on societal impacts associated with research.\\n\\nAcknowledgements\\n\\nWe thank our human annotators for helping create a large part of the dataset. We also thank John Aslanides and Kevin McKee for advice on the initial setup of the human annotation data collection. We would particularly like to thank Boxi Wu for her support in helping to organize panels with experts to advise on our toxicity filtering approach. Lastly, we would like to acknowledge Dani Yogatama and Lisa Anne Hendricks who acted as our internal reviewers and thank Chris Dyer for his continuous input.\\n\\nReferences\\n\\nAkhbardeh, F., Arkhangorodsky, A., Biesialska, M., Bojar, O., Chatterjee, R., Chaudhary, V., Costa-juss\u00e1, M. R., Espa\u00f1a-Bonet, C., Fan, A., Federman, C., Freitag, M., Graham, Y., Grundkiewicz, R., Haddow, B., Harter, L., Heafield, K., Homan, C. M., Huck, M., Amponsah-Kaakyire, K., Kasai, J., Khashabi, D., Knight, K., Kocmi, T., Koehn, P., Lourie, N., Monz, C., Morishita, M., Na-\"}"}
{"id": "liska22a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "liska22a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "liska22a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sciavolino, C., Zhong, Z., Lee, J., and Chen, D. Simple entity-centric questions challenge dense retrievers, 2021.\\n\\nSultan, M. A., Chandel, S., Fernandez Astudillo, R., and Castelli, V. On the importance of diversity in question generation for QA. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5651\u20135656, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.500. URL https://aclanthology.org/2020.acl-main.500.\\n\\nWang, J., Jatowt, A., and Yoshikawa, M. Archivalqa: A large-scale benchmark dataset for open domain question answering over archival news collections, 2021.\\n\\nZhang, M. and Choi, E. SituatedQA: Incorporating extra-linguistic contexts into QA. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7371\u20137387, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-main.586.\"}"}
{"id": "liska22a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adaptation: knowledge lags eval\\nForgetting: knowledge covers eval\\n\\nFigure 14. Adaptation and forgetting on past written questions: Open-book versus closed-book.\\n\\nFigure 15. Negative log likelihood of masked span prediction for evaluation documents of the T5. We show the vanilla T5, T5 fine-tuned on WMT up to 2019 (\u201cStale\u201d), monthly fine-tuned, and trained on all WMT up to 2020. We see forgetting on the past subset, and a slight recency bias on the recent subset, compared to retraining.\\n\\nB.2. Open-book\\nWe show adaptation and forgetting on past written questions in Figure 14, metrics for all subsets in Table 6, and the masked span prediction performance for evaluation documents of the T5 model in Figure 15.\\n\\nB.3. Temporal accuracy of DPR trained on news\\nFigure 16 shows temporal distribution of gold and retrieved passages for recent questions for Q4\u20192020: DPR with timestamp matches the temporal distribution of the gold passages much closer. Interestingly, the model seems to get somewhat confused about the year: the second lower spike is in Q4\u20192019.\\n\\nB.4. One-step Streaming and Static QA Benchmarks\\nWe provide the EM in Figure 17 and all the metrics in Table 7.\"}"}
{"id": "liska22a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 16. Temporal distribution of gold and retrieved passages of the recent questions for Q4'2020. DPR with timestamp matches the temporal distribution of the gold passages much closer. Interestingly, the model seems to get somewhat confused about the year - the second lower spike is in Q4'2019.\\n\\nFigure 17. Static setup with all questions. Solid filled bars are models that had 2020 knowledge added incrementally.\"}"}
{"id": "liska22a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|                | 0Q   | +1Q  | +2Q  | +3Q  |\\n|----------------|------|------|------|------|\\n| **Eval-Written** |      |      |      |      |\\n|                |      |      |      |      |\\n| **Eval-Generated** |      |      |      |      |\\n|                |      |      |      |      |\\n| **CB + Stale**  |      |      |      |      |\\n|                |      |      |      |      |\\n| **CB + FT**     |      |      |      |      |\\n|                |      |      |      |      |\\n| **CB + Retr.**  |      |      |      |      |\\n|                |      |      |      |      |\\n| **OB + IU + FT**|      |      |      |      |\\n|                |      |      |      |      |\\n| **OB + IU + Retr.** |      |      |      |      |\\n|                |      |      |      |      |\\n| **FiD + IU**    |      |      |      |      |\\n|                |      |      |      |      |\\n| **FiD + IU + FT**|      |      |      |      |\\n|                |      |      |      |      |\\n| **FiD + IU + Retr.** |      |      |      |      |\\n|                |      |      |      |      |\\n\\n**Observe**\\n\\n|                | 0Q   | +1Q  | +2Q  | +3Q  |\\n|----------------|------|------|------|------|\\n|                |      |      |      |      |\\n| **Eval-Written** |      |      |      |      |\\n|                |      |      |      |      |\\n| **Eval-Generated** |      |      |      |      |\\n|                |      |      |      |      |\\n| **CB + Stale**  |      |      |      |      |\\n|                |      |      |      |      |\\n| **CB + FT**     |      |      |      |      |\\n|                |      |      |      |      |\\n| **CB + Retr.**  |      |      |      |      |\\n|                |      |      |      |      |\\n| **OB + IU + FT**|      |      |      |      |\\n|                |      |      |      |      |\\n| **OB + IU + Retr.** |      |      |      |      |\\n|                |      |      |      |      |\\n| **FiD + IU**    |      |      |      |      |\\n|                |      |      |      |      |\\n| **FiD + IU + FT**|      |      |      |      |\\n|                |      |      |      |      |\\n| **FiD + IU + Retr.** |      |      |      |      |\\n|                |      |      |      |      |\\n\\n**Table 7.**\\n\\n|                | 0Q   | +1Q  | +2Q  | +3Q  |\\n|----------------|------|------|------|------|\\n|                |      |      |      |      |\\n| **Observe** |      |      |      |      |\\n\\n|                | 0Q   | +1Q  | +2Q  | +3Q  |\\n|----------------|------|------|------|------|\\n|                |      |      |      |      |\\n| **Static setup.** |      |      |      |      |\\n\\n|                | 0Q   | +1Q  | +2Q  | +3Q  |\\n|----------------|------|------|------|------|\\n|                |      |      |      |      |\\n| **Observe** |      |      |      |      |\"}"}
{"id": "liska22a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Related datasets overview. Abbreviations: Wri-other=filtered from other datasets; KC=knowledge corpus; QD=question date; APD=article publication date.\\n\\n| Dataset                              | Knowledge Corpus (KC) | Closed/Open | KC Size | Train | Valid | Eval | QD | APD | Temporal Qs | Answer types |\\n|--------------------------------------|-----------------------|-------------|---------|-------|-------|------|-----|-----|-------------|--------------|\\n| StreamingQA (this work)              |                       | OB          | 11M / 47.6M | 100k  | 10k  | 28k + 8.8k | Yes | Yes | Yes         |              |\\n| TempLama (Dhingra et al., 2021)      | Templ./Cloze          | CB          | \u2013       | 10k   | 5k   | 35k  | Yes | Yes | Yes         |              |\\n| ArchivalQA (Wang et al., 2021)       | Gen News              | OB          | 1.8M    | 850k  | 100k | 100k | X   | Yes (transform relative time using APD) | Yes          |\\n| SituatedQA-temporal (Zhang & Choi, 2021) | Wri-other Wikipedia   | OB          | 6M / 21M | 6k   | 3.4k | 2.8k | Yes |     |             |              |\\n| Time-Sensitive QA (Chen et al., 2021) | Templ./Wri Wikipedia  | OB          | 6M / 21M | 29k  | 6.1k | 6.1k | X   | X   | Yes         |              |\\n| Natural Questions (Kwiatkowski et al., 2019; Lee et al., 2019a) | Wri Wikipedia | OB          | 6M / 21M | 79.2k | 8.8k | 3.6k | X   | X   | X           |              |\\n| PAQ (Lewis et al., 2021)             | Gen Wikipedia         | OB          | 6M / 21M | 65M   | X    | X    | X   | X   | X           | X            |\\n| CronQuestions (Saxena et al., 2021)   | Templ. KG KG         | KG          | 350k   | 30k   | 30k  | X    | KG with temporal information | Yes          |\\n| TempQuestions (Jia et al., 2018)      | Wri-other KG KG       | KG          | 1.2k   | X     |     |     |     |     |             |              |\\n| TimeQuestions (Jia et al., 2021)      | Wri-other KG KG       | KG          | 9.7k   | 3.2k  | 3.2k | X    | KG with temporal information | Yes          |\\n| TORQUE (Ning et al., 2020)           | Wri                    | 3.2k        | short passages | Known-source | 24.5k | 1.5k | 4.6k | X   | X           | Yes          |\\n\\nFigure 9. Answer type proportions in the StreamingQA evaluation sets.\\n\\nA. StreamingQA Dataset\\nA.1. Comparison of StreamingQA with other related datasets\\nIn Table 4, we present a comparison of StreamingQA with other related datasets.\\nA.2. Answer types in evaluation set\\nWe provide further detail on the answer types in our evaluation set in Figure 9.\\n\\nTable 5. Examples of questions with high and low frequency named entities.\\n\\nQuestions with high frequency named entities\\n- When will Google have its annual I/O conference?\\n- What is the name of the Greek Prime Minister's residence?\\n- Which former New York City Mayor is developing mobile apps to help New York state trace coronavirus cases?\\n\\nQuestions with low frequency named entities\\n- What is the name of the Managing Director and Chief Executive of Latitude Financial Services?\\n- Which actress plays Vanessa Woodfield?\\n- Why must John Momis step down?\"}"}
{"id": "liska22a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3. Examples of questions with high- and low-frequency named entities\\n\\nSee examples in Table 5 of question with high and low frequency named entities.\\n\\nA.4. Automatic filtering of trivial and/or low-quality generated questions\\n\\nIn order to remove trivial and/or low-quality questions, we apply the following filters: (i) remove questions that contain their answer as a sub-span; (ii) few-shot prompt a large LM for QA and ensure it generated the original target answer given the evidence document and the generated question for named entity or date answers exactly, and for phrases with 40% words overlap; (iii) additionally we perform Google Search via the Google Search API with the question text and evidence publication date as a query, and keep only questions for which the answer is present in the top 10 search results; and (iv) for phrase-answer questions, we only keep questions that contain a named entity in the question, hence eliminating questions that are too generic.\\n\\nA.5. Adding time specification to past generated questions\\n\\nFor all questions we include an absolute or a relative time specification, choosing randomly between the two. For absolute time specification we include the month and the year of the article\u2019s publication date, for example, \u201cin May 2017\u201d. For relative, we compute the difference between the question date and the article publication date, and include \u201c\\\\(<7\\\\) days\u201d, \u201ca \\\\(<1\\\\) week ago\u201d, \u201c\\\\(N\\\\) weeks ago\u201d (\\\\(<8\\\\) weeks), \u201c\\\\(N\\\\) months ago\u201d (\\\\(<2\\\\) years), or \u201c\\\\(N\\\\) years ago\u201d.\\n\\nhttps://developers.google.com/custom-search\"}"}
{"id": "liska22a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.6. Dataset Toxicity Filtering\\n\\nWe have adapted the Perspective API definitions of the classifiers from the Perspective API to better fit our domain for our manual annotation to determine the filtering thresholds.\\n\\n- **IDENTITY ATTACK**: Negative, discriminatory, stereotyping, or hateful against a group of people based on criteria including (but not limited to) race or ethnicity, religion, gender, nationality or citizenship, disability, age, or sexual orientation. As well as the above we also consider a QA if it: Unnecessarily strengthens negative, discriminatory, stereotyping, or hateful representations of minorities. The implication of the QA could be viewed as potentially negative, discriminatory, stereotyping, or hateful and the veracity is questionable - i.e. we would need to fact-check.\\n\\n- **INSULT**: Inflammatory, insulting, or negative language towards a person or a group of people. Not necessarily identity specific.\\n\\n- **PROFANITY**: Swear words, curse words, or other obscene or profane language.\\n\\n- **SEVERE TOXICITY**: A very hateful, aggressive, disrespectful comment or otherwise very likely to make a user leave a discussion or give up on sharing their perspective. This attribute is much less sensitive to more mild forms of toxicity, such as comments that include positive uses of curse words.\\n\\n- **SEXUALLY EXPLICIT**: Contains references to lewd content. References sexual acts or body parts that are unnecessarily graphic or detailed.\\n\\n- **THREAT**: Language that is threatening or encouraging violence or harm, including self-harm. Language that is unnecessarily graphic or detailed when reporting about a violent incident.\\n\\nB. Experiments and Results\\n\\nB.1. Closed-book\\n\\nWe present the F1 scores broken down by answer type in Figure 11, and the effect of temporal lag between model knowledge and question dates for written questions in Figure 12.\\n\\nPerplexity vs closed-book QA performance.\\n\\nAn interesting point of comparison is between closed-book QA performance and the perplexity of the underlying LM on test documents. As TXL FT is fine-tuned on more months, we expect its perplexity on evidence documents of the recent subset to reduce, while its perplexity on evidence documents of the past subset to either stay the same (in the optimal scenario) or increase, if the model forgets. Figure 13 shows these two effects.\\n\\n14 [https://developers.perspectiveapi.com/s/about-the-api-attributes-and-languages]\"}"}
{"id": "liska22a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12. The effect of temporal lag between the final training month of CB\\\\textsubscript{+FT} and question dates for written questions, relative to CB\\\\textsubscript{+R\\\\textsubscript{ETR}}.\\n\\nFigure 13. Relationship between adaptation and forgetting of LM (solid) and CB QA models (dashed). Red lines show fine-tuned vs stale adaptation/improvements to recent articles on the recent subset. Green lines show fine-tuned vs retrained forgetting of past articles on the past subset. LM forgetting is expressed as TXL\\\\textsubscript{S\\\\textsubscript{TALE}} perplexity / TXL\\\\textsubscript{FT} perplexity and F1 deterioration as CB\\\\textsubscript{+FT} F1 / CB\\\\textsubscript{+S\\\\textsubscript{TALE} F1}, whereas LM adaptation is expressed as TXL\\\\textsubscript{R\\\\textsubscript{ETR}} perplexity / TXL\\\\textsubscript{FT} perplexity and F1 improvement as CB\\\\textsubscript{+FT} F1 / CB\\\\textsubscript{+R\\\\textsubscript{ETR} F1}. \\n\\n**Lag between model knowledge and question dates**\\n\\n-3Q -2Q -1Q 0 +1Q +2Q +3Q\\n\\nF1 / Retrained F1\\n\\nRecent questions\\n\\nPast questions\\n\\n**Normalized perplexity or normalized F1**\\n\\nLM forgetting on past articles\\n\\nF1 deterioration on past questions\\n\\nLM adaptation to recent articles\\n\\nF1 improvement on recent questions\\n\\n2020-01 2020-02 2020-03 2020-04 2020-05 2020-06 2020-07 2020-08 2020-09 2020-10 2020-11 2020-12\\n\\nFinetuned model\\n\\n0.80 0.85 0.90 0.95 1.00\"}"}
{"id": "liska22a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Left: F1 score on the whole evaluation dataset of CB + S TALE, CB + R ET R, and CB + FT fine-tuned on articles published until the specified cut-off dates. Right: The effect of a temporal lag between the final training month of CB + FT and question dates for generated questions, relative to CB + R ET R.\\n\\n4.3. Open-book QA\\n\\nIn this task, we answer questions given a knowledge corpus of articles, \\\\( p(a_i | q_i) \\\\) for some \\\\( q_i \\\\), \\\\( C \\\\leq t \\\\) \\\\( \\\\leq t \\\\). We use WMT news articles, sliced into 6-sentence chunks as our knowledge corpus, resulting in 42.1M (up to 2019) and 47.6M (up to 2020) passages.\\n\\nThe OB model is a variation of the Retrieval Augmented Generation model (RAG-sequence; Lewis et al. (2020b)) with a TXL-based generator, the same LMs as for our closed-book experiments. As a retriever we use Dense Passage Retrieval (DPR; Karpukhin et al. (2020)), trained on question/passage pairs from our training set (including the question and publication dates), with embedding size of 768. We retrieve 20 passages. We also consider the Fusion-in-Decoder model (FI D; Izacard & Grave (2020)), which was shown to outperform RAG on a number of QA tasks, with the same pre-trained DPR retriever as OB but with 64 retrieved passages. In contrast to RAG, the FI D\u2019s generator attends to all retrieved passages at the same time instead of individually. We train both models with a restricted search space that contains gold evidence of all training and validation questions; this helps to reduce computation time (we did not see a material performance degradation due to this).\\n\\n4.4. Results and Analyses\\n\\nIn this section we analyse the performance of the closed-book and open-book models on StreamingQA. We have three closed-book models: CB + S TA LE, the iteratively fine-tuned CB + FT, and the retrained CB + R ET R. In order to adapt open-book models to new information from 2020, we always include new articles in the search index and then either keep the stale generator (OB + IU, FI D + IU), fine-tune the generator on new articles (OB + IU+FT, FI D + IU+FT), or use a retrained generator (OB + IU+R ET R, FI D + IU+R ET R).\\n\\n4.1. Adaptation to New Knowledge and Forgetting in Closed-Book QA\\n\\nIterative LM fine-tuning improves performance on StreamingQA but lags retraining. We consider all questions in our evaluation sets (recent+past) and evaluate CB models, in Figure 3 (left). First, we observe that the CB + S TA LE is outperformed by each of the CB + FT models\u2014the fine-tuning is able to incorporate new information for the half of the questions that are only answerable from 2020 documents. With each additional month of documents, the CB + FT models perform better for all answer types (named entities, phrases, dates; Appendix B.1). The improved performance is not simply due to more data, but is driven by better accuracy on the recent subset, while the performance on the past subset remains mostly unchanged (Appendix B.1).\\n\\nSecondly, we observe that CB + R ET R outperforms or is on par with all other models, and so vanilla adaptation that we consider for CB + FT should be improved to bridge the gap from fine-tuning to retraining.\\n\\nAdaptation and forgetting\\n\\nWe use question dates to split Eval-Generated and Eval-Written into quarterly sets. To understand how adaptation to new information is offset by forgetting of past information, we investigate the effect of a temporal lag between the question date and the end date of knowledge in the underlying LM. Note that the question date and the knowledge date is on average much closer for the recent subset (a few weeks) compared to the past subset (years), and so adaptation to new articles is more crucial for the recent subset performance.\\n\\nWhen the lag is negative, the model knowledge is lagging\"}"}
{"id": "liska22a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Adaptation and forgetting on recent subsets (generated, left; written, right). We observe that adapting the generator helps the FiD model, and helps the OB model when fully retrained, compared to index update only. Open-book models allow for much faster adaptation to recent knowledge than closed-book, with almost no forgetting. (IU = search index updated, FT = fine-tuned LM).\\n\\nFigure 5. Adaptation and forgetting on past generated questions. We see only a slight improvement as the model acquires knowledge about 2020. We do not observe forgetting.\\n\\nWhen the lag is positive, the questions are in the past with respect to the most recent information in the model, and in these settings, some previous information needed to answer these questions might have been overwritten\u2014forgetting may occur.\\n\\nWe aggregate the model answers for each lag, and plot the corresponding F1 relative to that of CB + RETR. in Figure 3 (right). As we fine-tune and the lag between the model knowledge and question month increases, the performance on the past subset slightly deteriorates until we under-perform CB + RETR. by about 5%. On the recent subset, the performance first improves significantly, and then as we pass the question quarter and continue fine-tuning on further data, we start seeing minor forgetting. Similar conclusions hold for the written questions (Appendix B.1).\\n\\n4.2. Adaptation to New Knowledge and Forgetting in Open-Book QA\\n\\nAdaptation and forgetting Similarly to the closed-book experiment, we examine adaptation and forgetting by aggregating model answers by a temporal lag between the evaluation set and the end of the model knowledge. We consistently observe that on the recent subset of both generated and written questions (Figure 4), the open-book models (OB, FI) have a steep adaptation rate (from -1Q to 0Q) for all model variants, including just adding new articles into the search index without LM fine-tuning (OB + IU, FI + IU).\\n\\nFor all of the models, we see almost no forgetting on the recent subsets. In Figure 5, for generated past questions, there is no forgetting (see Appendix B.2 for written questions).\\n\\nNote that a small fraction of questions from the past subset reference articles from 2020, so seeing 2020 knowledge slightly helps compared to a lagging model.\\n\\nIs updating search space sufficient, or do we need to update underlying LMs? Recent open-book QA models consist of a search index, a retriever, and a generator. One strong argument in favour of the open-book models is that new information can be added directly into the search index, potentially without any additional training. Our results indicate that although the major performance gain comes from updating the search index, updating knowledge in the LM generator improves performance on recent subset questions too. We compare OB + IU, FI + IU, where the new document...\"}"}
{"id": "liska22a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. QA performance given question (left) or answer (right) named entity frequency quartiles.\\n\\nWe observe improvements for FID + IU + FT (vs FID + IU) on the recent subset of generated and written questions, and on the past subset (Figure 5) at 0Q followed by minor forgetting. Moreover, retraining the generator improves performance for all models and all subsets.\\n\\n4.3. Parametric vs Semi-parametric Adaptation\\n\\nSeeing above that fine-tuning or retraining the generator helps, we want to understand for which questions the updated generator is particularly important compared to a stale generator. Lazaridou et al. (2021) previously demonstrated that one driving factor behind deteriorating temporal LM performance are changing frequencies of words, particularly named entities. We analyze QA performance by frequency of named entities appearing in (a) questions, and (b) answers, computed over the knowledge corpus up to 2019, and in 2020 only, respectively.\\n\\nFirst, close-book performance is substantially better for questions that contain frequent named entities (see Appendix A.3 for examples): F1 is higher by absolute 10% (Figure 6, left), likely due to higher-frequency named entities in the knowledge corpus providing a stronger learning signal for the parametric model. Second, open-book performance does not show strong dependency on the frequency in the question, but this is due to two offsetting factors: DPR Recent with ts Past with ts.\\n\\n4.4. Further Analyses\\n\\nTemporal retrieval\\n\\nUsing dates improves DPR performance for recent questions. In Figure 7, median temporal difference between retrieved and gold articles is 41 and 1101 days for DPR with and without dates, respectively. Improved temporal accuracy translates into better recall overall, recall@20 for the recent generated questions is 57% and 43% for DPR with and without dates, respectively. For past questions we do not see improvements, which suggests that the model may incorrectly interpret the two time specifications, i.e., the prepended question date and absolute or relative time specification in the question text. See Appendix B.3 for more.\\n\\nTime specification in questions\\n\\nGenerated questions in the past subset may contain an absolute or relative time specification in the question text, and we generally find that the open-book models perform best on questions without time specification, followed by absolute, and relative. For example, for FID + IU + FT, F1 is 0.711, 0.469, 0.359, respectively, and 0.441 overall.\\n\\nStatic and updated questions\\n\\nFor a preliminary analysis of \u201cstatic\u201d (knowledge that likely will not change) and \u201cupdated\u201d (that might change) questions, we observed that the\"}"}
{"id": "liska22a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One-step Streaming and Static QA Benchmarks\\n\\nStreamingQA dataset allows us to consider further two tasks, and we provide benchmarks to encourage research on these directions: one-step streaming setting in Figure 8 (solid bars), and the usual static open-book QA setup (diagonal-line bars), evaluated on all 2020 questions. There is still a large gap to human performance; moreover the dataset creates challenges to retrieval and news articles reading comprehension, see models with gold evidence versus retrieved (cross-pattern bars). For the human benchmark we collected a fourth annotator answer. See Appendix B.4 for EM and a table with all metrics.\\n\\n5. Related Work\\n\\nQA datasets\\nWe summarize previous QA work on understanding of knowledge with temporal context in Section 1 and provide a dataset comparison table in Appendix A.1.\\n\\nQuestion generation for QA\\nAutomatic question generation trained with supervision has been explored in QA for data augmentation (Alberti et al., 2019; Dong et al., 2019; Sultan et al., 2020), or as a way to enrich knowledge bases for QA-pair retriever models (Lewis et al., 2021). Here we instead leverage few-shot generation capabilities of large LMs (Rae et al., 2021; Brown et al., 2020) to generate questions and use them for both training and evaluation.\\n\\nOpen-domain QA\\nProgress in neural information retrieval (Karpukhin et al., 2020; Lee et al., 2019b) enables open-domain QA models that are trained end-to-end as both the retriever and the reader are differentiable (Guu et al., 2020; Lewis et al., 2020b; Izacard & Grave, 2020; Sachan et al., 2021). Recent work in the domain has focused on improving performance by combining information from multiple documents efficiently (Sachan et al., 2021; Izacard & Grave, 2020) and on performance analysis of the dense retrievals, for instance, when dealing with named entities (Sciavolino et al., 2021; Liu et al., 2021).\\n\\nContinual learning and distribution shift in LM and downstream tasks\\nContinual learning in language is a long-standing research topic (Carlson et al., 2010; Parisi et al., 2018) that has recently seen an increase in interest. Lazaridou et al. (2021) show that performance of Transformer-XL deteriorates when evaluated on data published after the training period, and use dynamic evaluation (Krause et al., 2017) to partially make up for this degradation. Lazaridou et al. (2021) and Hu et al. (2020) release large scale benchmarks for studying temporal adaptation in the language modeling task. Jang et al. (2021) propose new metrics for knowledge updates and establish strong baselines. In contrast, we focus on studying adaptation in a downstream task of question answering: we demonstrate that deterioration in perplexity translates into worse downstream performance and that adaptation through unsupervised fine-tuning or access to retrieval improves QA performance. R\u00f6ttger & Pierrehumbert (2021) study temporal adaptation of BERT models for the classification task and find that unsupervised temporal adaptation does not help downstream performance as much and task specific temporal adaptation is needed. Amba Hombaiah et al. (2021) propose new incremental methods for online BERT training using vocabulary expansion. In the context of semi-parametric models Khandelwal et al. (2020) and Lewis et al. (2020a) describe flexible approaches to adaptation through updating information in the retrieval.\\n\\n6. Conclusion\\nIn order to enable a more realistic evaluation of QA models, we introduced the first QA dataset and task for studying adaptation to new information over time in open and close-book settings with temporally non-overlapping training and evaluation sets.\\n\\nAs language models grow bigger, the cost of maintaining them up-to-date increases, and therefore adaptation ability of the models becomes more important. Our experimental results show that open-book QA models allow for fast and flexible adaptation through adding new articles into the search space, with fine-tuning or retraining generally further improving performance. The ability to inject new knowledge through the search space depends on retrieval...\"}"}
