{"id": "liu22f", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nFigure 13. The hyperparameter tuning of baseline methods.\"}"}
{"id": "liu22f", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 14. Illustration of the function $\\\\psi(z)$ used to generate the different synthetic-data scenarios.\"}"}
{"id": "liu22f", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 15. Histograms of the outcomes ($y_i$) for the different synthetic-data scenarios.\\n\\nFigure 16. Estimated probability of survival grouped by pathological stages. The plot shows median, samples between 25th to 75th percentile in the box, samples between 0th and 100th percentile on the line, and the outliers as dots. Deep ensemble produces similar probability estimates for patients across all the stages; CE is more discriminative but has a very large variance; CaPE achieves a trade-off between the two baselines.\"}"}
{"id": "liu22f", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 17. Comparison of reliability diagrams for real-world data with different scenarios of simulated data. For the cancer survival dataset, the empirical probabilities are clustered in around (0.4-0.6), similar to the Centered scenario. For the weather forecasting dataset, the probabilities are uniformly distributed across 0.1-0.8, similar to the Linear scenario. For the collision prediction dataset, the majority of the output probabilities are clustered in the lower probability region, similar to the Skewed scenario.\"}"}
{"id": "liu22f", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\n\\\\[ Z = \\\\prod_{p \\\\in I(q)} \\\\exp(- (p - q)^2 / \\\\sigma^2) \\\\]\\n\\nis the normalization factor. An empirical estimate of the conditional probability would then be\\n\\n\\\\[ P(y = 1 | f(x) \\\\in I(q)) \\\\approx \\\\frac{1}{Z} \\\\sum_{x_i \\\\in I(q)} y_i \\\\exp(- (f(x_i) - q)^2 / \\\\sigma^2) \\\\].\\n\\nBased on these two approximation methods, we can design an algorithm to estimate \\\\( p_i \\\\) emp.\\n\\n**Bin**\\n\\nWe divide our data into \\\\( B \\\\) bins of equal size. \\\\( Q_1, \\\\ldots, Q_B \\\\) are the data B-quantiles. We wish to estimate\\n\\n\\\\[ P(y = 1 | f(x) \\\\in [Q_{b-1}, Q_b]) \\\\], \\\\( b = 1, \\\\ldots, B \\\\), \\\\( Q_0 = 0 \\\\).\\n\\nDenote \\\\( I_b := [Q_{b-1}, Q_b] \\\\cap \\\\{ f(x_i) \\\\} \\\\), set of all predictions \\\\( \\\\in [Q_{b-1}, Q_b] \\\\), and \\\\( \\\\text{Index}(I_b) = \\\\{ i | f(x_i) \\\\in I_b \\\\} \\\\).\\n\\nWe have,\\n\\n\\\\[ P(y = 1 | f(x) \\\\in [Q_{b-1}, Q_b]) \\\\approx p(b) \\\\] emp \\\\( \\\\frac{1}{|I_b|} \\\\sum_{i \\\\in \\\\text{Index}(I_b)} y_i \\\\]\\n\\nWe assign \\\\( p(b) \\\\) emp to all data points \\\\( i \\\\) in the \\\\( b \\\\)-th quantile.\\n\\n**Kernel**\\n\\nIn this case we use kernel estimation:\\n\\n\\\\[ p_i \\\\] emp \\\\( = \\\\) \\\\( \\\\frac{1}{|N(i, r)|} \\\\sum_{k \\\\in N(i, r)} K(i, k) y_k \\\\)\\n\\n\\\\( \\\\text{NN}(i, r) \\\\) defines \\\\( r \\\\) data points whose predictions are nearest to \\\\( \\\\hat{p}_i = f(x_i) \\\\).\\n\\n\\\\( K(i, j) \\\\) is the Gaussian kernel\\n\\n\\\\[ K(i, j) = \\\\exp(- (\\\\hat{p}_i - \\\\hat{p}_j)^2 / \\\\sigma^2) \\\\],\\n\\nwith hyperparameter \\\\( \\\\sigma \\\\).\\n\\n**H Calibration Baselines**\\n\\nThis section provides a review of the baseline methods, discussed in Section 7.\\n\\n**Post-processing**\\n\\nPostprocessing consists of finding a function \\\\( f : [0, 1] \\\\to [0, 1] \\\\), that transforms the model outputs \\\\( \\\\hat{p}_i \\\\) \\\\( \\\\to f(\\\\hat{p}_i) \\\\) to improve their calibration.\\n\\n- **Platt scaling** (Platt, 1999) the model predictions are used as inputs to a logistic regression model optimized using a validation set,\\n\\n\\\\[ f_1(\\\\hat{p}_i) = \\\\sigma(W^T \\\\hat{p}_i + b) \\\\]\\n\\nwhere \\\\( W \\\\in \\\\mathbb{R}^2 \\\\), \\\\( b \\\\in \\\\mathbb{R} \\\\) and \\\\( \\\\sigma \\\\) is the sigmoid function.\\n\\n- **Temperature scaling** (Guo et al., 2017) is a single-parameter variant of Platt Scaling where we change a temperature parameter in the logistic function.\\n\\n- **Beta/Dirichlet calibration** (Dir-ODIR) (Kull et al., 2017; 2019) assumes that the probabilities can be parametrized by a Beta/Dirichlet distribution\\n\\n\\\\[ f_j \\\\sim \\\\text{Beta}(\\\\alpha_j, \\\\beta_j) \\\\]\\n\\nAssuming the prior to be \\\\( p(y = j) = \\\\pi_j \\\\), \\\\( \\\\pi_j \\\\in [0, 1] \\\\), we have\\n\\n\\\\[ P(y | f_j) \\\\propto \\\\pi_j f_j \\\\]\\n\\nand then \\\\( \\\\alpha_j, \\\\beta_j \\\\) are estimated by maximizing the posterior.\\n\\n**Ensembling**\\n\\nThese calibration methods simultaneously train several neural networks, varying parameters in the training process. The final output is a function of all the different outputs.\"}"}
{"id": "liu22f", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\n\u2022 Mix-n-Match (Zhang et al., 2020) improves calibration by ensembling parametric and non-parametric calibrators. Denote the temperature scaling function with $g(\\\\hat{y}_i, T)$. Then Mix-n-Match ensembles different temperatures $f_j(\\\\hat{p}_i) = w_1 g_j(\\\\hat{p}_i, T) + w_2 g_j(\\\\hat{p}_i, 0) + w_3 g_j(\\\\hat{p}_i, \\\\infty)$.\\n\\n\u2022 Deep ensemble (Lakshminarayanan et al., 2017) trains $M$ copies of the neural network with different initializations. The final estimate is the average of all single model outputs $p(y_i|\\\\mathbf{x}_i) = \\\\frac{1}{M} \\\\sum_{j=1}^{M} p_\\\\theta(j)(y_i|\\\\mathbf{x}_i)$.\\n\\nModified training These calibration methods train the neural networks from end to end, modifying the training process to improve calibration.\\n\\n\u2022 Confidence penalty (Pereyra et al., 2017) penalizes low entropy output distributions (confidence penalty), $L(\\\\theta) = -\\\\sum_i \\\\log p_\\\\theta(y_i|\\\\mathbf{x}_i) - \\\\beta H(p_\\\\theta(y_i|\\\\mathbf{x}_i))$.\\n\\n\u2022 Focal loss (Mukhoti et al., 2020) maximizes entropy while minimizing the KL divergence between the predicted and the target distributions. It also regularizes the weights of the model to avoid overfitting, $L(\\\\theta) = -\\\\sum_i (1 - p_\\\\theta(y_i|\\\\mathbf{x}_i)) \\\\beta \\\\log p_\\\\theta(y_i|\\\\mathbf{x}_i)$, $\\\\beta \\\\in \\\\mathbb{R}$.\\n\\n\u2022 Kernel MMCE (Kumar et al., 2018) is a reproducing kernel Hilbert space (RKHS) kernel based measure of calibration that is efficiently trainable, alongside the negative likelihood loss. Given data samples $D = \\\\{(c_i, r_i)\\\\}_{m=0}^{m}$, where $c_i = \\\\chi\\\\{\\\\hat{y}_i = y_i\\\\}$ and $r_i = P(c_i = 1|\\\\hat{y}_i)$, MMCE is computed on samples $D$ as follows, $\\\\text{MMCE}_2(D) = \\\\sum_{i,j} (c_i - r_i)(c_j - r_j) k(r_i, r_j)$.\\n\\nHyperparameter tuning for baseline methods Most postprocessing methods do not involve hyperparameters, and are optimized based on a validation set. The modified training methods all use a single hyperparameter to control the strength of regularization. We tune the hyperparameter, using a validation set. Figure 13 shows that MMCE is robust to the choice of hyperparameter. Focal loss and entropy regularization result in inferior performance to that of MMCE for all hyperparameter choices.\\n\\nI Synthetic data experiments We use a ResNet-18 backbone architecture for all our experiments with synthetic data. The synthetic data is split into training, validation, and test sets with 16641, 4738, and 2329 samples, respectively. The training and validation sets contain only images $\\\\mathbf{x}_i$ and 0-1 labels $y_i$ for training and tuning the model. In order to evaluate the performance of the model for probability estimation, the held-out test set contains the ground truth probabilities $p_i$, in addition to $\\\\mathbf{x}_i$ and $y_i$. Note that we do not use the ground-truth probability labels $p_i$ values during training or inference - we only use them to compare the performance of different models.\\n\\nGround Truth Probability Generation The ground truth probability associated with example $i$ is simulated by $p_i = \\\\psi(z_i)$ where $z_i$ is age of the person.\"}"}
{"id": "liu22f", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Results of on CIFAR 10 with simulated probabilistic labels. CaPE outperforms a CE early stopped model.\\n\\n| Name | CE early stop | CaPE (bin) | CaPE (kd) |\\n|------|---------------|------------|-----------|\\n| MSE  | 0.0297        | 0.0252     | 0.0247    |\\n| KL   | 0.4051        | 0.2468     | 0.2598    |\\n\\nLabel distribution\\n\\nAfter determining the probability $p_i$ using $\\\\psi(z)$, the label $y_i$ is sampled from a Bernoulli distribution parametrized by $p_i$, so that it takes the value 1 with probability $p_i$. The distributions of $y_i$ under five different scenarios are illustrated in Fig.15.\\n\\nAdditional Synthetic-Data Experiments on CIFAR-10\\n\\nWe simulated probabilistic labels for CIFAR-10 to perform additional experiments. Each of the ten classes of CIFAR-10 was assigned a different ground-truth probability. To this end we assigned an integer $c$ between 0 and 9 to each class and set the corresponding probability equal to $c/10$. The training and validation sets were built by assigning each image $x_i$ to a binary label $y_i$ sampled from the corresponding class probability $p_i$. Note that we do not use the ground-truth probability labels during training or inference - we only use them to evaluate the performance on the test set.\\n\\nTable 11 shows the results on the test set. We again observe that CaPE outperforms the cross-entropy baseline based on early stopping.\\n\\nAdditional Details on Real-World Data and Experiments\\n\\nWe present here supplementary information for the real-world datasets used in our experiments.\\n\\nCancer Survival\\n\\nHistopathological features are useful in identification of tumor cells, cancer subtypes, and the stage and level of differentiation of the cancer. Hematoxylin and Eosin (H&E)-stained slides are the most common type of histopathology data and the basis for decision making in the clinics. With these properties, H&E are used for mortality prediction of cancer (Wulczyn et al., 2020). In this experiment, we use the H&E slides of non-small cell lung cancers from The Cancer Genome Atlas Program (TCGA) to predict the 5-year survival. The dataset has 1512 whole slide images from 1009 patients, and 352 of them died in 5-years. We split the samples by patients and source institutions into training, validation, and test set, which has 1203, 151, and 158 samples respectively.\\n\\nThe whole slide images contain numerous pixels, so we cropped the slides into tiles at 10x magnification with 1/4 overlapping, resized them to $299 \\\\times 299$ with bicubic interpolation, and filtered out the tiles with more than 85% area covered by the background. The representations of each tile are trained with self-supervised momentum contrastive learning (MoCo) (Chen et al., 2020), and the slide-level prediction is obtained from a multiple-instance learning network (Ilse et al., 2018) trained with the binary label of survival in 5 years.\\n\\nWeather Forecasting\\n\\nWe use the German Weather service dataset, which contains quality-controlled rainfall-depth composites from 17 operational Doppler radars. Three precipitation maps from the past 30 minutes serve as an input. The training labels are the 0/1 events indicating whether the mean precipitation increases (1) or not (0).\\n\\nThe German Weather service (DWD - Deutshce Wetter Dienst) dataset contains quality-controlled rainfall-depth composites from 17 operational DWD Doppler radars. It has a spatial extent of 900x900 km, and covers the entirety of Germany. Data exists since 2006, with a spatial and temporal resolution of 1x1 km and 5 minutes, respectively. The dataset has been used to train RainNet, a precipitation nowcasting model (Ayzel, 2020).\\n\\nThe network architecture is ResNet18, with 3 input channels and 2 output channels. The input to the network are 3 precipitation maps which cover a fixed area of 300km \u00d7 300 km in the center of the grid (300 \u00d7 300 pixels), set 10 minutes apart. The training, validation and test datasets consist of 20000, 6000 and 3000 samples, respectively, all separated...\"}"}
{"id": "liu22f", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Collision Prediction\\n\\nVehicle collision is one of the leading causes of death in the world. Reliable collision prediction systems which can warn drivers about potential collisions can save a significant number of lives. A standard way to design such a system is to train a convolutional model for identifying if a particular vehicle in the dash-cam video feed might collide with the car in next few seconds. More formally, at time $t = T$ the system tries to predict if any car in the video might collide with our given car in time $t \\\\in [T, T + T_{look-ahead}]$. Each labelled training sample consists of features $X = (X_{T - \\\\delta}, X_{T - 2\\\\delta}, \\\\ldots, X_{T - d\\\\delta})$ and a binary label $Y \\\\in \\\\{0, 1\\\\}$ denoting if an accident will occur in $t \\\\in [T, T + T_{look-ahead}]$.\\n\\nEach $X_t$ is a tensor with 4 channels where the first 3 channels corresponds to an RGB image of the dashcam view at time $t$, and the fourth channel consists of a mask with a bounding box on a particular vehicle of interest. In this work, we use YouTubeCrash dataset (Kim et al., 2019) to train and test our model, which uses $\\\\delta = 0.1$ s, $T_{look-head} = 18$ s, and $d = 3$. Following (Kim et al., 2019) we used a VGG-16 network architecture. The dataset contains 122 accident scenes, and 2096 non-accident scenes, which after feature extraction gives us 2096 positive samples, and 11486 negative samples (the dataset is severely imbalanced, and similar to the Skewed situation in Section 7.1). We further split the dataset into train (6453 samples for label 0, and 1023 samples for label 1), validation (2348 samples for label 0, and 545 samples for label 1), and test (2685 samples for label 0, and 528 samples for label 1) sets. The samples in train, validation and test sets are generated from disjoint scenes/dashcam videos.\\n\\nAnalysis of Cancer Survival Results\\n\\nFor cancer survival prediction, we visualize the estimated probabilities on the test set in different pathological stages in Figure 16. In general, patients in earlier stages should have higher probabilities of survival. Deep ensemble produces similar probability estimates for all stages (i.e the model is less discriminative). Cross-entropy minimization (CE) is more discriminative, but has very wide confidence intervals. CaPE is more discriminative than deep ensemble, while having narrower confidence intervals than CE.\\n\\nCalibrating from the Beginning\\n\\nCaPE exploits a calibration-based cost function to improve its probability estimates without overfitting. The empirical probabilities in this loss are computed from the model itself. Consequently, applying this strategy from the beginning of training can be counterproductive, because the model predictions are essentially random. This is demonstrated in the following table, which compares CaPE with a model trained using the calibration loss from the beginning (in the same way as CaPE, alternating with cross-entropy minimization).\\n\\n| Methods   | Linear | Sigmoid | Centered | Skewed | Discrete | ($\\\\times 10^{-2}$) |\\n|-----------|--------|---------|----------|--------|----------|-------------------|\\n| MSE       | 0.48   | 0.51    | 0.40     | 0.38   | 0.39     | 1.83              |\\n| p-KL      | 0.98   | 2.37    | 0.78     | 0.78   | 0.81     | 4.46              |\\n| MSE       | 2.74   | 2.40    | 1.83     | 1.85   | 1.85     | 1.83              |\\n| p-KL      | 6.36   | 5.63    | 4.31     | 4.36   | 4.36     | 4.41              |\\n\\nTable 12: Comparison between CaPE and a model that uses the calibration loss from the beginning (in the same way as CaPE, alternating with cross-entropy minimization) on synthetic data. All numbers are downscaled by $10^{-2}$.\\n\\nCorrespondence of Real-World Datasets and the Scenarios of the Simulated Dataset\\n\\nFigure 17 illustrates the similarity between the empirical probability curves of different real-world datasets and the different scenarios of our synthetic dataset. For the cancer survival dataset, the empirical probabilities are clustered in the center (0.4-0.6) similar to the Centered scenario. For the weather forecasting dataset, the probabilities are uniformly distributed across 0.1-0.8 similar to the Linear scenario. For the collision prediction dataset, the majority of the data points are clustered in the lower probability region which makes it similar to the Skewed scenario.\"}"}
{"id": "liu22f", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nTable 4: Performance on Face-based Risk Prediction.\\n\\n| Method               | ECE    | MCE    | KS     | Brier  | MSE    |\\n|----------------------|--------|--------|--------|--------|--------|\\n| Sigmoid              | 6.4\u00b10.71 | 20.63\u00b13.44 | 2.74\u00b10.45 | 16.28\u00b10.44 | 5.34\u00b10.20 |\\n| CE + label resampling| 6.19\u00b10.75 | 17.0\u00b13.68  | 5.86\u00b10.80 | 16.68\u00b10.42 | 6.16\u00b10.17  |\\n| Temperature          | 5.57\u00b10.71 | 15.32\u00b13.09 | 5.02\u00b10.83 | 16.58\u00b10.34 | 6.13\u00b10.17  |\\n| Platt Scaling        | 3.45\u00b10.68 | 10.32\u00b12.79 | 1.3\u00b10.43  | 16.33\u00b10.34 | 5.78\u00b10.19  |\\n| Dirichlet Cal.       | 14.5\u00b11.15 | 25.68\u00b13.02 | 4.67\u00b10.32  | 19.21\u00b10.43 | 8.64\u00b10.26  |\\n| Focal Loss           | 4.65\u00b10.70 | 11.84\u00b12.78 | 2.66\u00b10.77  | 16.96\u00b10.34 | 6.86\u00b10.21  |\\n| Mix-n-match          | 5.65\u00b10.76 | 15.32\u00b13.41 | 5.09\u00b10.94  | 16.6\u00b10.36  | 6.12\u00b10.17  |\\n| Entropy Reg.         | 9.51\u00b10.79 | 18.77\u00b12.38 | 7.26\u00b10.78  | 17.17\u00b10.31 | 7.02\u00b10.17  |\\n| MMCE Reg.            | 4.67\u00b10.76 | 13.63\u00b12.59 | 2.5\u00b10.53   | 15.9\u00b10.51  | 5.35\u00b10.18  |\\n| Deep Ensemble        | 5.17\u00b10.74 | 16.12\u00b13.11 | 2.04\u00b10.44  | 16.39\u00b10.45 | 5.86\u00b10.22  |\\n| CaPE (bin)           | 3.78\u00b10.60 | 11.96\u00b12.59 | 2.22\u00b10.70  | 15.84\u00b10.43 | 5.17\u00b10.20  |\\n| CaPE (kernel)        | 3.90\u00b10.75 | 11.73\u00b12.79 | 2.05\u00b10.54  | 15.85\u00b10.41 | 5.16\u00b10.20  |\\n\\nTable 5: Performance on Face-based Risk Prediction.\\n\\n| Method               | ECE    | MCE    | KS     | Brier  | MSE    |\\n|----------------------|--------|--------|--------|--------|--------|\\n| CE + label resampling| 4.29\u00b10.74 | 12.38\u00b12.92 | 2.68\u00b10.80 | 24.22\u00b10.13 | 0.2\u00b10.01 |\\n| CE early-stop        | 5.76\u00b10.84 | 15.32\u00b13.07 | 4.19\u00b11.02 | 24.68\u00b10.08 | 0.48\u00b10.01 |\\n| Temperature          | 6.09\u00b10.82 | 15.83\u00b12.91 | 4.74\u00b10.96 | 24.74\u00b10.06 | 0.48\u00b10.01 |\\n| Platt Scaling        | 4.57\u00b10.76 | 11.85\u00b12.50 | 2.79\u00b10.85 | 24.62\u00b10.08 | 0.41\u00b10.01 |\\n| Dirichlet Cal.       | 4.84\u00b11.15 | 13.13\u00b17.61 | 2.16\u00b10.86 | 24.7\u00b10.1  | 0.46\u00b10.01 |\\n| Mix-n-match          | 6.05\u00b10.83 | 15.71\u00b12.92 | 4.68\u00b10.98 | 24.74\u00b10.06 | 0.48\u00b10.01 |\\n| Focal Loss           | 5.09\u00b10.83 | 13.4\u00b12.87 | 3.44\u00b11.02 | 24.8\u00b10.05 | 0.48\u00b10.01 |\\n| Entropy Reg.         | 5.02\u00b10.86 | 12.69\u00b13.42 | 3.27\u00b10.96 | 24.74\u00b10.06 | 0.45\u00b10.01 |\\n| MMCE Reg.            | 5.56\u00b10.86 | 13.59\u00b12.65 | 2.71\u00b10.93 | 24.7\u00b10.08 | 0.44\u00b10.01 |\\n| Deep Ensemble        | 4.84\u00b10.78 | 12.39\u00b12.52 | 2.64\u00b10.71 | 24.69\u00b10.07 | 0.44\u00b10.01 |\\n| CaPE (bin)           | 4.73\u00b10.82 | 11.81\u00b12.54 | 2.07\u00b10.60 | 24.56\u00b10.11 | 0.38\u00b10.01 |\\n| CaPE (kernel)        | 5.41\u00b10.87 | 12.71\u00b12.50 | 2.39\u00b10.78 | 24.59\u00b10.11 | 0.40\u00b10.01 |\\n\\nC.3 Additional Reliability Diagram\\n\\nFigure 9 shows additional reliability curves on the real world data, supplementing the ones illustrated in Figure 6. Figure 10 shows reliability curves for the different synthetic data scenarios.\\n\\nD Kolmogorov-Smirnov Error\\n\\nWe derive the KS-error, mentioned in Section 3. For a calibrated estimator \\\\( P(y = 1|f(x) \\\\in I(q)) = q, \\\\) for some small interval \\\\( I(q) \\\\) around \\\\( q. \\\\)\\n\\nHence \\\\( P(y = 1, f(x) \\\\in I(q)) = P(f(x) \\\\in I(q)) q, \\\\) for all \\\\( 0 \\\\leq q \\\\leq 1. \\\\)\\n\\nSimilarly to the Kolmogorov-Smirnov (KS) test for distribution functions, we can recast this property in integral form \\\\( \\\\phi_1(\\\\sigma) = \\\\sigma \\\\int_0^Z P(y = 1, f(x) \\\\in I(q)) dq, \\\\) and \\\\( \\\\phi_2(\\\\sigma) = \\\\sigma \\\\int_0^Z P(f(x) \\\\in I(q)) dq. \\\\)\"}"}
{"id": "liu22f", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 6: Performance on Face-based Risk Prediction. Skewed scenario. All numbers are downscaled by $10^{-2}$.\\n\\n| Method                        | ECE     | MCE     | KS       | Brier   | MSE     |\\n|-------------------------------|---------|---------|----------|---------|---------|\\n| CE + label resampling         | 2.7     | \u00b10.46   | 7.64     | \u00b12.14   | 1.05    | \u00b10.39   | 11.0    | \u00b10.51   | 0.22    | \u00b10.01   | 0.92    | \u00b10.03   |\\n| CE early-stop                 | 3.07    | \u00b10.57   | 7.88     | \u00b11.88   | 1.28    | \u00b10.41   | 11.18   | \u00b10.5   | 0.4     | \u00b10.01   | 1.79    | \u00b10.06   |\\n| Temperature                   | 3.14    | \u00b10.49   | 7.92     | \u00b11.84   | 1.12    | \u00b10.33   | 11.22   | \u00b10.47  | 0.4     | \u00b10.02   | 1.76    | \u00b10.06   |\\n| Platt Scaling                 | 2.99    | \u00b10.53   | 7.73     | \u00b11.59   | 1.07    | \u00b10.37   | 11.1    | \u00b10.54  | 0.39    | \u00b10.01   | 1.72    | \u00b10.06   |\\n| Dirichlet Cal.                | 3.04    | \u00b10.73   | 7.81     | \u00b12.43   | 0.97    | \u00b10.3   | 11.22   | \u00b10.42  | 0.47    | \u00b10.02   | 2.31    | \u00b10.07   |\\n| Focal Loss                    | 8.29    | \u00b10.67   | 14.93    | \u00b11.43   | 6.16    | \u00b10.67   | 12.01   | \u00b10.41  | 1.28    | \u00b10.03   | 1.63    | \u00b10.66   |\\n| Mix-n-match                   | 2.99    | \u00b10.53   | 7.78     | \u00b11.78   | 1.08    | \u00b10.32   | 11.18   | \u00b10.49  | 0.4     | \u00b10.01   | 1.75    | \u00b10.05   |\\n| Entropy Reg.                  | 7.67    | \u00b10.57   | 14.43    | \u00b11.5   | 5.2     | \u00b10.71   | 11.94   | \u00b10.45  | 1.18    | \u00b10.03   | 10.74   | \u00b10.65   |\\n| MMCE Reg.                     | 3.68    | \u00b10.59   | 10.94    | \u00b12.76   | 1.47    | \u00b10.31   | 11.14   | \u00b10.44  | 0.54    | \u00b10.02   | 2.44    | \u00b10.08   |\\n| Deep Ensemble                 | 2.87    | \u00b10.5   | 7.21     | \u00b11.63   | 1.36    | \u00b10.44   | 11.28   | \u00b10.5  | 0.55    | \u00b10.02   | 2.58    | \u00b10.07   |\\n| CaPE (bin)                    | 3.29    | \u00b10.5   | 8.18     | \u00b11.51   | 1.17    | \u00b10.34   | 11.07   | \u00b10.47  | 0.4     | \u00b10.02   | 1.73    | \u00b10.06   |\\n| CaPE (kernel)                 | 3.16    | \u00b10.5  | 8.14     | \u00b11.58   | 1.09    | \u00b10.33   | 11.17   | \u00b10.53  | 0.39    | \u00b10.01   | 1.69    | \u00b10.06   |\\n\\n### Table 7: Performance on Face-based Risk Prediction. Discrete scenario. All numbers are downscaled by $10^{-2}$.\\n\\n| Method                        | ECE     | MCE     | KS       | Brier   | MSE     |\\n|-------------------------------|---------|---------|----------|---------|---------|\\n| CE + label resampling         | 4.23    | \u00b10.74   | 11.16    | \u00b12.5   | 1.45    | \u00b10.49   | 20.38   | \u00b10.35  | 1.52    | \u00b10.05   | 3.63    | \u00b10.12   |\\n| CE early-stop                 | 6.7     | \u00b10.86   | 18.62    | \u00b13.52  | 2.61    | \u00b10.53   | 21.91   | \u00b10.36  | 2.24    | \u00b10.08   | 5.27    | \u00b10.17   |\\n| Temperature                   | 6.12    | \u00b10.87   | 16.82    | \u00b13.56  | 3.37    | \u00b10.86   | 21.76   | \u00b10.35  | 2.21    | \u00b10.08   | 5.15    | \u00b10.18   |\\n| Platt Scaling                 | 4.7     | \u00b10.72   | 11.69    | \u00b12.44  | 1.67    | \u00b10.51   | 21.44   | \u00b10.32  | 2.06    | \u00b10.08   | 4.83    | \u00b10.17   |\\n| Dirichlet Cal.                | 7.13    | \u00b10.86   | 22.67    | \u00b15.08  | 3.18    | \u00b10.68   | 22.1    | \u00b10.34  | 2.74    | \u00b10.1   | 6.53    | \u00b10.22   |\\n| Focal Loss                    | 5.7     | \u00b10.75   | 13.68    | \u00b12.32  | 4.62    | \u00b10.91   | 21.77   | \u00b10.28  | 2.92    | \u00b10.09   | 6.77    | \u00b10.21   |\\n| Mix-n-match                   | 6.27    | \u00b10.76   | 16.83    | \u00b12.95  | 3.47    | \u00b10.93   | 21.77   | \u00b10.33  | 2.21    | \u00b10.08   | 5.14    | \u00b10.18   |\\n| Entropy Reg.                  | 6.69    | \u00b10.87   | 15.38    | \u00b12.43  | 6.03    | \u00b11.13   | 21.79   | \u00b10.31  | 2.84    | \u00b10.08   | 6.62    | \u00b10.19   |\\n| MMCE Reg.                     | 3.96    | \u00b10.7   | 10.4     | \u00b12.4  | 1.51    | \u00b10.47   | 21.12   | \u00b10.35  | 2.09    | \u00b10.08   | 4.92    | \u00b10.18   |\\n| Deep Ensemble                 | 4.76    | \u00b10.74   | 11.49    | \u00b12.23  | 2.04    | \u00b10.61   | 21.17   | \u00b10.31  | 1.97    | \u00b10.08   | 4.61    | \u00b10.17   |\\n| CaPE (bin)                    | 5.41    | \u00b10.74   | 14.45    | \u00b13.15  | 2.24    | \u00b10.59   | 21.33   | \u00b10.36  | 1.81    | \u00b10.08   | 4.28    | \u00b10.18   |\\n| CaPE (kernel)                 | 4.96    | \u00b10.8  | 12.97    | \u00b12.63  | 2.18    | \u00b10.58   | 21.21   | \u00b10.42  | 1.84    | \u00b10.08   | 4.35    | \u00b10.17   |\"}"}
{"id": "liu22f", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9. Reliability diagrams of all the baselines on the real-world datasets. We train all baseline methods on each of the datasets and plot the empirical probability (y-axis) against predicted probability (x-axis). The axis labels are removed to ease visualization (they are the same as in Figure 6).\"}"}
{"id": "liu22f", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Figure 10. Reliability diagrams for different synthetic data scenarios. We can see that CaPE outperforms early stopping, prevents overfitting, and achieves a performance on par with training on large amount of resampled data.\\n\\n| Synthetic Data Scenario | Predicted Probability | Empirical Probability |\\n|-------------------------|-----------------------|-----------------------|\\n| Deep Probability Estimation | Linear | 0.0 0.5 1.0 | 0.0 |\\n| Infinite Data | 0.0 0.5 1.0 | 0.0 0.5 1.0 |\\n| Overfitting | 0.0 0.5 1.0 | 0.0 0.5 1.0 |\\n| Early Stopping | 0.0 0.5 1.0 | 0.0 0.5 1.0 |\\n| CaPE | 0.0 0.5 1.0 | 0.0 0.5 1.0 |\\n\\nSigmoid\\n\\nCentered\\n\\nSkewed\\n\\nDiscrete\"}"}
{"id": "liu22f", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nwhere we used\\n\\n$$\\\\| \\\\sigma(x) - \\\\sigma(y) \\\\|_2 \\\\leq \\\\| x - y \\\\|_2$$\\n\\nfor any $$x, y \\\\in \\\\mathbb{R}^p$$. By Gr\u00f6nnwall's inequality,\\n\\n$$\\\\hat{\\\\theta}_t \\\\leq e^{\\\\| X \\\\|_2^2 n t} \\\\hat{\\\\theta}_0^2 + 2 \\\\sqrt{p \\\\| X \\\\|_2^2 n t}.$$ (14)\\n\\nNext, compute\\n\\n$$d dt \\\\tilde{\\\\theta}_t \\\\eta, n^2 \\\\leq d dt \\\\tilde{\\\\theta}_t \\\\eta, n^2 - \\\\hat{\\\\theta}_t \\\\eta, n^2 \\\\leq 1 n X (\\\\sigma(X^T \\\\tilde{\\\\theta} \\\\lfloor s \\\\rfloor \\\\eta, n) - \\\\sigma(X^T \\\\hat{\\\\theta} s n)) \\\\leq \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} \\\\lfloor s \\\\rfloor \\\\eta, n - \\\\hat{\\\\theta} s n^2 \\\\leq \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} \\\\lfloor s \\\\rfloor \\\\eta, n - \\\\hat{\\\\theta} s n^2 + \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} \\\\lfloor s \\\\rfloor \\\\eta, n - \\\\tilde{\\\\theta} s \\\\eta, n^2 \\\\leq \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} \\\\eta, n - \\\\hat{\\\\theta} s n^2 + \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} \\\\eta, n - \\\\tilde{\\\\theta} s \\\\eta, n^2.$$ \\n\\nSince\\n\\n$$\\\\tilde{\\\\theta} s \\\\eta, n - \\\\tilde{\\\\theta} \\\\lfloor s \\\\rfloor \\\\eta, n = (s - \\\\lfloor s \\\\rfloor)n X (y' - \\\\sigma(X^T \\\\tilde{\\\\theta} \\\\lfloor s \\\\rfloor \\\\eta, n)),$$\\n\\nwe can bound\\n\\n$$\\\\tilde{\\\\theta} s \\\\eta, n - \\\\tilde{\\\\theta} \\\\lfloor s \\\\rfloor \\\\eta, n^2 \\\\leq 2 \\\\eta \\\\sqrt{p \\\\| X \\\\|_2^2 n} + \\\\eta \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} \\\\lfloor s \\\\rfloor \\\\eta, n - \\\\tilde{\\\\theta} s \\\\eta, n^2 \\\\leq 2 \\\\eta \\\\sqrt{p \\\\| X \\\\|_2^2 n} + \\\\eta \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} \\\\lfloor s \\\\rfloor \\\\eta, n - \\\\tilde{\\\\theta} s \\\\eta, n^2.$$ \\n\\nCombining the above inequalities, we get\\n\\n$$d dt sup_s \\\\leq t \\\\tilde{\\\\theta} s \\\\eta, n - \\\\hat{\\\\theta} s n^2 \\\\leq d dt \\\\tilde{\\\\theta} t \\\\eta, n - \\\\hat{\\\\theta} t n^2 \\\\leq \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} t \\\\eta, n - \\\\hat{\\\\theta} t n^2 + \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} t \\\\eta, n - \\\\tilde{\\\\theta} t \\\\eta, n^2 \\\\leq \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} t \\\\eta, n - \\\\hat{\\\\theta} t n^2 + \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} t \\\\eta, n - \\\\tilde{\\\\theta} t \\\\eta, n^2!.$$ \\n\\nTogether with inequality (14), by Gr\u00f6nnwall's inequality we eventually have\\n\\n$$sup_s \\\\leq t \\\\tilde{\\\\theta} s \\\\eta, n - \\\\hat{\\\\theta} s n^2 \\\\leq \\\\eta t 2 \\\\sqrt{p \\\\| X \\\\|_2^2 n} + \\\\gamma \\\\| X \\\\|_2^4 n^2 + \\\\| X \\\\|_2^4 n \\\\tilde{\\\\theta} \\\\eta, n - \\\\tilde{\\\\theta} t \\\\eta, n^2 \\\\leq \\\\| X \\\\|_2^2 n \\\\tilde{\\\\theta} t \\\\eta, n - \\\\hat{\\\\theta} t n^2 + \\\\gamma \\\\| X \\\\|_2^4 n^2 \\\\tilde{\\\\theta} t \\\\eta, n - \\\\tilde{\\\\theta} t \\\\eta, n^2!.$$ \\n\\nAgain, by a standard random matrix result [see e.g. (Vershynin, 2018), Theorem 4.4.5], it holds that\\n\\n$$\\\\| X \\\\|_2 \\\\leq C(\\\\sqrt{p} + \\\\sqrt{n}),$$\\n\\nwith probability $$1 - 2e^{-n}$$. Plug this bound into inequality (15), we get for any $$n > N_0$$,\\n\\n$$sup_s \\\\leq t \\\\tilde{\\\\theta} s \\\\eta, n - \\\\hat{\\\\theta} s n^2 \\\\leq \\\\eta t C^2 (\\\\kappa, t, \\\\delta, \\\\gamma),$$ (16)\\n\\nwith probability $$1 - 2e^{-n}$$, where\\n\\n$$C^2(\\\\kappa, t, \\\\delta, \\\\gamma_0) = C(t + \\\\gamma_0 + 1)(\\\\kappa^6 + 1)e^{C(\\\\delta + 1)(\\\\kappa^2 + 1)t},$$\\n\\nfor some $$C > 0$$. \"}"}
{"id": "liu22f", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nDefine the event $A_n = \\\\sup_{\\\\tilde{\\\\theta} \\\\leq t, \\\\eta, n - \\\\hat{\\\\theta}_n \\\\leq \\\\eta t C_2(\\\\kappa, t, \\\\delta, \\\\gamma_0)}$. Since $|\\\\sigma|, |\\\\sigma'| < 1$, we have for any $n > N_0$, $\\sup_{k \\\\leq t/\\\\eta} |MSE_n(k \\\\eta) - MSE_{\\\\eta,n}(k)| \\\\leq \\\\sup_{k \\\\leq t/\\\\eta} 4 E|\\\\sigma(\\\\langle \\\\hat{\\\\theta}_k \\\\eta, z \\\\rangle) - \\\\sigma(\\\\langle \\\\hat{\\\\theta}_k \\\\eta, z \\\\rangle)| = \\\\sup_{k \\\\leq t/\\\\eta} 4 E|\\\\sigma(\\\\langle \\\\hat{\\\\theta}_k n, z \\\\rangle) - \\\\sigma(\\\\langle \\\\hat{\\\\theta}_k \\\\eta,n, z \\\\rangle)| + 8 e^{-n} \\\\leq 4 E}\\\\sup_{k \\\\leq t/\\\\eta} \\\\hat{\\\\theta}_k \\\\eta = \\\\hat{\\\\theta}_k \\\\eta \\\\eta,n + 8 e^{-n} \\\\leq 4 C_2(\\\\kappa, t, \\\\delta, \\\\gamma_0) t \\\\eta + 8 e^{-n} = 2 C_1(\\\\gamma, \\\\gamma_0) \\\\eta^4 + 16 e^{-N_2} = -2 C_1(\\\\gamma, \\\\gamma_0) \\\\eta^2 < 0$.\\n\\nWe are now ready to prove Theorem A.1.\\n\\nProof of Theorem A.1. Pick a small $T_2(\\\\kappa, \\\\delta, \\\\gamma_0) > 0$ that satisfies $C_2(\\\\kappa, T_2, \\\\delta, \\\\gamma_0) T_2 \\\\leq C_1(\\\\gamma, \\\\gamma_0)^3$. This is possible since $C_2(\\\\kappa, t, \\\\delta, \\\\gamma_0)$ is bounded as $t \\\\to 0$. Let $N_1$ and $T_1$ be as in Lemma A.3, and let $T^*(\\\\gamma, \\\\gamma_0, \\\\kappa, \\\\delta) = \\\\min\\\\{T_1(\\\\gamma, \\\\gamma_0), T_2(\\\\kappa, \\\\delta, \\\\gamma_0)\\\\}$. For any $k < T^* \\\\eta - 1$, $MSE_{\\\\eta,n}(k + 1) - MSE_{\\\\eta,n}(k) = (MSE_n((k + 1) \\\\eta) - MSE_n(k \\\\eta)) + (MSE_{\\\\eta,n}(k + 1) - MSE_n((k + 1) \\\\eta)) + (MSE_{\\\\eta,n}(k) - MSE_n((k) \\\\eta)) \\\\leq (MSE_n((k + 1) \\\\eta) - MSE_n(k \\\\eta)) + 2 \\\\sup_{m \\\\leq T^*/\\\\eta} |MSE_n(m \\\\eta) - MSE_{\\\\eta,n}(m)| \\\\leq (MSE_n((k + 1) \\\\eta) - MSE_n(k \\\\eta)) + 4 C_2(\\\\kappa, T^*, \\\\delta, \\\\gamma_0) T^* \\\\eta + 8 e^{-n} = (MSE_n((k + 1) \\\\eta) - MSE_n(k \\\\eta)) + 4 C_1(\\\\gamma, \\\\gamma_0) \\\\eta^4 + 16 e^{-N_2} = -2 C_1(\\\\gamma, \\\\gamma_0) \\\\eta^2 < 0$.\\n\\nLet $N_2(\\\\eta, \\\\gamma, \\\\gamma_0)$ satisfy $8 e^{-N_2} < C_1(\\\\gamma, \\\\gamma_0) \\\\eta^4$, and let $N^*(\\\\eta, \\\\gamma, \\\\gamma_0) = \\\\max\\\\{N_1(\\\\gamma, \\\\gamma_0), N_2(\\\\eta, \\\\gamma, \\\\gamma_0), N_2(\\\\eta, \\\\gamma, \\\\gamma_0)\\\\}$. We therefore have for any $n > N^*$ and $k < T^* \\\\eta - 1$, $MSE_{\\\\eta,n}(k + 1) - MSE_{\\\\eta,n}(k) \\\\leq -2 C_1(\\\\gamma, \\\\gamma_0) \\\\eta^2 < 0$. \\n\\n\"}"}
{"id": "liu22f", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Reliability curves for a linear model (Appendix B). The horizontal and vertical coordinate of each blue point represent the predicted (\\\\( \\\\hat{p}_i \\\\)) and true (\\\\( p_i \\\\)) probabilities of a test example respectively. We also show a reliability diagram of binned mean predicted and empirical probabilities in black (see Section 3). The dashed diagonal line indicates perfect calibration. The model is initialized randomly (first column) and initially improves the estimation, with the reliability curve trending towards the diagonal line (second column). However, because the dataset is finite, it eventually overfits, and the predicted probabilities collapse to 0 or 1 (third column). When labels are resampled to generate large amount of data, the estimates converge to the ground-truth probability labels (right column).\\n\\nB Early Learning and Memorization in a Linear Model\\nWe provide a numerical example that illustrates the theoretical results of Section 4, and demonstrates the similarities between the behavior of linear and deep-learning models in high dimensions.\\n\\nWe train a logistic regression model in an overparametrized regime (\\\\( p_n = 1 \\\\)). To generate the features, we draw \\\\{ \\\\( x_i \\\\) \\\\} \\\\( i = 1, 500 \\\\), \\\\( x_i \\\\sim \\\\text{N}(0, I_{500}) \\\\). The ground-truth unobserved probability labels \\\\( p_i \\\\) are generated according to equation 6. i.e. \\\\( p_i = \\\\sigma(\\\\langle \\\\theta^*, x_i \\\\rangle) \\\\). The true parameter \\\\( \\\\theta^* = (1, 0, \\\\ldots, 0) \\\\) is fixed to equal the first standard basis vector, and \\\\( \\\\sigma(\\\\mathbf{x}) = (1 + e^{\\\\mathbf{x}})^{-1} \\\\) is the sigmoid function.\\n\\nThe 0-1 labels \\\\{ \\\\( y_i \\\\) \\\\} \\\\( i = 1, 500 \\\\) are drawn from Bernoulli distribution with probability \\\\( p_i \\\\), \\\\( y_i \\\\sim \\\\text{Bern}(p_i) \\\\).\\n\\nWe use stochastic gradient descent (to fit a logistic regression model with parameter \\\\( \\\\theta \\\\in \\\\mathbb{R}^{500} \\\\) on the simulated data \\\\{ \\\\( (x_i, y_i) \\\\) \\\\} \\\\( i = 1, 500 \\\\), using a cross-entropy loss function. We compare the model trained on two datasets:\\n\\n(a) finite \\\\( (x, y) \\\\) pairs,\\n\\n(b) a large amount of data set, generated by repeatedly resampling new outcomes \\\\( y_i \\\\) from the ground-truth probabilities \\\\( p_i \\\\) at every iteration.\\n\\nFigure 7 shows that the linear model trained with cross entropy begins by improving the estimate of the true probabilities (at the early-learning stage), but eventually memorizes the 0-1 labels (the overfitting stage). Figure 8 illustrates that the MSE of cross entropy training increases when the model starts memorizing the 0-1 labels, as predicted by the results of Appendix A.\\n\\nC Additional Results\\nWe present here supplementary results to the ones presented in Section 8.\\n\\nC.1 Face-based Risk Prediction\\nFull evaluation with confidence intervals derived using 1000 bootstraps for the five simulated scenarios are examined: Linear (Table 3); Sigmoid (Table 4); Centered (Table 5); Skewed (Table 6); Discrete (Table 7). Note that all numbers are upscaled by \\\\( 10^{-2} \\\\) in the tables.\"}"}
{"id": "liu22f", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nFigure 8. Validation loss for training a linear model (Appendix B). Training with finite data (green line) decreases MSE initially but eventually overfits to the 0-1 labels. Training with resampled labels (red line) avoids overfitting and results in accurate estimation of the true probabilities.\\n\\nTable 3: Performance on Face-based Risk Prediction.\\n\\n| Method                      | ECE   | MCE   | KS    | Brier | MSE \\\\( p \\\\) | KL \\\\( p \\\\) |\\n|-----------------------------|-------|-------|-------|-------|-------------|------------|\\n| CE + label resampling       | 4.14  | \u00b1 0.81| 12.07 | \u00b1 3.29| 2.24 \u00b1 0.88 | 18.97 \u00b1 0.33|\\n| CE early-stop               | 12.32 | \u00b1 0.83| 21.79 | \u00b1 1.97| 12.16 \u00b1 0.83| 21.82 \u00b1 0.51|\\n| Temperature                 | 5.7  | \u00b1 0.74| 13.82 | \u00b1 2.71| 2.36 \u00b1 0.74 | 20.47 \u00b1 0.37|\\n| Platt Scaling               | 4.29  | \u00b1 0.77| 10.94 | \u00b1 2.55| 1.3 \u00b1 0.45  | 20.18 \u00b1 0.36|\\n| Dirichlet Cal.              | 7.38  | \u00b1 1.12| 22.58 | \u00b1 7.24| 3.78 \u00b1 0.46 | 21.32 \u00b1 0.33|\\n| Focal Loss                  | 5.34  | \u00b1 0.68| 13.31 | \u00b1 2.67| 3.56 \u00b1 0.85 | 21.99 \u00b1 0.28|\\n| Mix-n-match                 | 5.46  | \u00b1 0.84| 12.9  | \u00b1 2.51| 1.92 \u00b1 0.44 | 20.43 \u00b1 0.35|\\n| Entropy Reg.                | 5.7  | \u00b1 0.74| 13.52 | \u00b1 1.67| 4.94 \u00b1 0.9  | 20.42 \u00b1 0.3|\\n| MMCE Reg.                   | 4.89  | \u00b1 0.74| 12.57 | \u00b1 2.27| 1.92 \u00b1 0.46 | 20.04 \u00b1 0.38|\\n| Deep Ensemble               | 4.26  | \u00b1 0.72| 11.33 | \u00b1 2.38| 1.95 \u00b1 0.61 | 19.88 \u00b1 0.32|\\n| CaPE (bin)                  | 4.58  | \u00b1 0.75| 11.85 | \u00b1 2.49| 1.71 \u00b1 0.51 | 19.68 \u00b1 0.36|\\n| CaPE (kernel)               | 4.62  | \u00b1 0.62| 12.25 | \u00b1 2.31| 1.65 \u00b1 0.38 | 19.71 \u00b1 0.34|\\n\\nC.2 Supplementary Metrics on Real-world Dataset\\n\\nWe present here additional metrics on the real-world data: Cancer Survival (Table 8); Climate Forecasting (Table 9); Collision Prediction (Table 10).\"}"}
{"id": "liu22f", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this appendix, we propose a simple analytical model that helps to demonstrate the early learning and memorization phenomena in probability estimation. Consider the following standard logistic regression problem: we are given i.i.d. data \\\\( \\\\{(y_i, x_i)\\\\}_{i \\\\leq n} \\\\) where \\\\( y_i \\\\in \\\\{+1, -1\\\\} \\\\) are labels and \\\\( x_i \\\\sim N(0, I_p) \\\\) are standard Gaussian covariate vectors. Given \\\\( x_i = x \\\\), the label \\\\( y_i \\\\) is distributed according to\\n\\n\\\\[\\nP(y_i = 1 | x_i = x) = \\\\sigma(\\\\langle \\\\theta^*, p, x \\\\rangle)\\n\\\\]\\n\\nwhere \\\\( \\\\sigma(x) = (1 + e^{-x})^{-1} \\\\) is the sigmoid function and \\\\( \\\\theta^* \\\\in R^p \\\\) is the ground-truth parameter with Euclidean norm \\\\( \\\\|\\\\theta^*\\\\| = \\\\gamma > 0 \\\\). Throughout, we assume that we are working in the high dimensional regime: the ratio \\\\( p/n \\\\to \\\\kappa \\\\in (0, +\\\\infty) \\\\) as \\\\( n \\\\to \\\\infty \\\\). For convenience, we denote \\\\( N_0 \\\\) an integer such that \\\\( p/n < 2\\\\kappa \\\\) for \\\\( \\\\forall n > N_0 \\\\).\\n\\nIn Section B of the appendix, we provide a numerical example to illustrate this theoretical model.\\n\\nGiven a fresh sample \\\\( z \\\\in R^p \\\\), we predict the probability of the associated label being one via\\n\\n\\\\[\\n\\\\sigma(\\\\langle \\\\hat{\\\\theta}, z \\\\rangle)\\n\\\\]\\n\\nfor some \\\\( \\\\hat{\\\\theta} \\\\in R^p \\\\), and we train the estimator \\\\( \\\\hat{\\\\theta} \\\\) through gradient descent to minimize the cross-entropy:\\n\\n\\\\[\\nL_n(\\\\theta) = -\\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left[ \\\\frac{1}{2} y_i \\\\langle \\\\theta, x_i \\\\rangle + \\\\log \\\\left( 1 + e^{-\\\\langle \\\\theta, x_i \\\\rangle} \\\\right) \\\\right] \\\\quad (7)\\n\\\\]\\n\\n\\\\[\\n\\\\hat{\\\\theta}_{k+1} = \\\\hat{\\\\theta}_k - \\\\eta \\\\nabla L_n(\\\\hat{\\\\theta}_k), \\\\quad k = 0, 1, \\\\ldots \\\\quad (9)\\n\\\\]\\n\\nwhere we choose constant step size \\\\( \\\\eta < \\\\delta \\\\), for some prefixed \\\\( \\\\delta > 0 \\\\), to be determined later. For the initialization, we draw \\\\( \\\\hat{\\\\theta}_0 \\\\) uniformly from the sphere of radius \\\\( \\\\gamma_0 \\\\).\\n\\nDefine the mean squared error\\n\\n\\\\[\\nMSE_{\\\\eta,n}(k) := E[(\\\\sigma(\\\\langle \\\\hat{\\\\theta}_k, z \\\\rangle) - \\\\sigma(\\\\langle \\\\theta^*, p, z \\\\rangle))^2]\\n\\\\]\\n\\nwhere \\\\( z \\\\sim N(0, I_p) \\\\) is a fresh Gaussian vector, independent of the data. Note that the expectation is taken with respect to all sources of randomness (including the randomness of data and initialization).\\n\\nWe present two main theorems that summarize our results. The first theorem states that for sufficiently large \\\\( n \\\\) and \\\\( p \\\\), the mean squared error decreases during the initial iterates of gradient descent. This phenomenon justifies the name \\\"early learning\\\": during early stages of training, the predictions obtained by the iterates of gradient descent improve.\\n\\n**Theorem A.1.** For any \\\\( n > N^* \\\\), the function \\\\( k \\\\mapsto MSE_{\\\\eta,n}(k) \\\\) decreases for \\\\( k \\\\in [0, T^*_{\\\\eta}) \\\\), where the constants \\\\( N^* = N^*(\\\\eta, \\\\gamma, \\\\gamma_0) \\\\) depends on \\\\( \\\\eta, \\\\gamma, \\\\gamma_0 \\\\) and \\\\( T^*_{\\\\eta} = T^*(\\\\kappa, \\\\gamma, \\\\gamma_0, \\\\delta) > 0 \\\\) depends on \\\\( \\\\kappa, \\\\gamma, \\\\gamma_0, \\\\delta \\\\).\\n\\nThe second theorem, however, states that when the dimension is sufficiently large (with respect to the number of samples), the predictor eventually overfits, and converges to a predictor that outputs only the probabilities 0 and 1.\\n\\n**Theorem A.2.** Let \\\\( p(z) = \\\\lim_{k \\\\to \\\\infty} \\\\sigma(\\\\langle \\\\hat{\\\\theta}_k, z \\\\rangle) \\\\). Then there exists a \\\\( \\\\kappa^* = \\\\kappa^*(\\\\gamma) \\\\), such that when \\\\( n \\\\to +\\\\infty \\\\) and \\\\( p_n \\\\to \\\\kappa > \\\\kappa^* \\\\), we have\\n\\n\\\\[\\nP\\\\left\\\\{ p(z) \\\\in \\\\{0, 1\\\\} \\\\text{ for a.e. } z \\\\in R^p \\\\right\\\\} \\\\to 1.\\n\\\\]\\n\\nWe begin with the proof of Theorem A.2. **Proof of Theorem A.2.** Define the event\\n\\n\\\\[\\nS_n = \\\\{\\\\exists w \\\\in R^p \\\\text{ such that for } \\\\forall i \\\\leq n: y_i \\\\langle w, x_i \\\\rangle > 0\\\\}.\\n\\\\]\\n\\nFor mathematical convenience, in this appendix we work with labels in \\\\( \\\\{\\\\pm 1\\\\} \\\\) rather than \\\\( \\\\{0, 1\\\\} \\\\).\"}"}
{"id": "liu22f", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nFor some large $C > 0$ to be determined later, define the event $G_n = \\\\{\\\\|X\\\\|_2 \\\\leq CG(\\\\sqrt{n} + \\\\sqrt{p})\\\\}$, where $X = (x_1, ..., x_n) \\\\in \\\\mathbb{R}^{p \\\\times n}$ is the covariate matrix and $\\\\|X\\\\|_2$ denotes the maximum singular value of $X$.\\n\\nPick $\\\\delta < 2C^2(\\\\sqrt{\\\\kappa} + 1)^2$.\\n\\nWe claim that $S_n \\\\cap G_n \\\\subset E_n = \\\\{p(z) \\\\in \\\\{0, 1\\\\} \\\\text{ for a.e. } z \\\\in \\\\mathbb{R}^p\\\\}$. Indeed, assume that $S_n$ and $G_n$ happen. Then the step size $\\\\eta$ satisfies $\\\\eta < \\\\delta < 2C^2(\\\\sqrt{\\\\kappa} + 1)^2 < 2n\\\\|X\\\\|_2^2$.\\n\\nOn the event $S_n$, the data points are separable, and Soudry et al. (2018, Theorem 3) implies that as $k \\\\to \\\\infty$, $\\\\hat{\\\\theta}_{k\\\\eta,n}^2 \\\\to +\\\\infty$, $\\\\hat{\\\\theta}_{k\\\\eta,n}^2 \\\\to \\\\hat{\\\\theta}_{\\\\text{MM}}$, where $\\\\hat{\\\\theta}_{\\\\text{MM}} \\\\in \\\\mathbb{R}^p$ denotes the max-margin separator. Thus for any $z \\\\in \\\\mathbb{R}^p$ with $\\\\langle z, \\\\hat{\\\\theta}_{\\\\text{MM}} \\\\rangle > 0$ (resp. $< 0$), we have $\\\\langle z, \\\\hat{\\\\theta}_{k\\\\eta,n} \\\\rangle \\\\to +\\\\infty$ (resp. $-\\\\infty$), and thus $p(z) = \\\\lim_{k \\\\to \\\\infty} \\\\sigma(\\\\langle \\\\hat{\\\\theta}_{k\\\\eta,n}, z \\\\rangle) = 1$ (resp. $= 0$). Since $\\\\{z \\\\in \\\\mathbb{R}^p : \\\\langle z, \\\\hat{\\\\theta}_{\\\\text{MM}} \\\\rangle = 0\\\\}$ has Lebesgue measure 0, we have shown that $S_n \\\\cap G_n \\\\subset E_n$.\\n\\nIt remains to show that $S_n$ and $G_n$ hold with probability tending to one. Candes & Sur (2018, Theorem 1) show that there exists a finite threshold $\\\\kappa^*$, such that when $p_n \\\\to \\\\kappa > \\\\kappa^*$, it holds that $P(S_n) \\\\to 1$. Moreover, by standard results from random matrix theory [see e.g. (Vershynin, 2018), Theorem 4.4.5], there exists some constant $C_G$ such that $P(G_n) \\\\geq 1 - 2e^{-n} \\\\to 1$. Therefore, we can conclude that $P(E_n) \\\\geq P(S_n \\\\cap G_n) \\\\to 1$.\\n\\nIn the remainder of the appendix, we prove Theorem A.1. Consider the following gradient flow, which is the analogue of gradient descent in the $\\\\eta \\\\to 0$ limit.\\n\\n$$d\\\\hat{\\\\theta}_t n \\\\ dt = -\\\\nabla L_n(\\\\hat{\\\\theta}_t n), \\\\quad \\\\forall t \\\\geq 0.$$ (11)\\n\\nSet $\\\\hat{\\\\theta}_0 n = \\\\hat{\\\\theta}_0 \\\\eta,n$ and define the corresponding mean square error $\\\\text{MSE}_n(t) := E[(\\\\sigma(\\\\langle \\\\hat{\\\\theta}_t n, z \\\\rangle) - \\\\sigma(\\\\langle \\\\theta^* p, z \\\\rangle))^2]$.\\n\\n(12)\\n\\nWe will first show that the mean squared error decreases along the trajectory of gradient flow, and then establish that this result extends to the iterates of gradient descent. We denote $\\\\text{MSE}_n'(t)$ as the derivative of $\\\\text{MSE}_n(t)$ at time $t \\\\geq 0$.\\n\\nLemma A.3. (a) $\\\\lim_{n \\\\to \\\\infty} \\\\text{MSE}_n'(0) < 0$.\\n\\n(b) There exists some positive constants $N_1(\\\\gamma, \\\\gamma_0)$, $T_1(\\\\gamma, \\\\gamma_0)$ and $C_1(\\\\gamma, \\\\gamma_0)$, such that for any $n > N_1$ and $t < T_1$, it holds that $\\\\text{MSE}_n'(t) \\\\leq -C_1$.\\n\\nProof. (a) For convenience, we write equation (11) in matrix form $d\\\\hat{\\\\theta}_t n \\\\ dt = \\\\frac{1}{n}X(y' - \\\\sigma(X^T \\\\hat{\\\\theta}_t n))$, where $y' = (y_1, ..., y_n)$ and $X = (x_1, ..., x_n)$.\"}"}
{"id": "liu22f", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We first calculate the derivative of the mean square prediction error:\\n\\n\\\\[ E \\\\text{ where in the last inequality we used Gaussian integration by parts and} \\\\]\\n\\n\\\\[ Z \\\\hat{\\\\theta} \\\\]\\n\\n\\\\[ \\\\text{where} \\\\]\\n\\n\\\\[ X \\\\]\\n\\n\\\\[ \\\\sigma, \\\\sigma \\\\]\\n\\nThus, \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\)\\n\\n\\\\[ \\\\frac{\\\\partial^2}{\\\\partial b^2} \\\\text{MSE} \\\\]\\n\\n\\\\[ \\\\sigma, \\\\sigma \\\\]\\n\\nSince \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\), the pair \\\\( X t \\\\) \\\\( X t \\\\) \\\\( X t \\\\) are independent centered Gaussian random variables with variance \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma \\\\) \\\\( \\\\sigma"}
{"id": "liu22f", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where in the first equality, we use dominated convergence to interchange differentiation and expectation, and in the second inequality we used the fact that $z \\\\sim N(0, I_p)$ is independent of the data.\\n\\nBy a simple calculation,\\n\\n$$d \\\\hat{\\\\theta}_t dt^2 \\\\leq \\\\frac{1}{n} \\\\|X\\\\|_2^2 \\\\left(y' - \\\\sigma(X^T \\\\hat{\\\\theta}_t)\\\\right)^2 \\\\leq 2 \\\\sqrt{p} \\\\|X\\\\|_2^2 \\\\frac{1}{n},$$\\n\\n$$d^2 \\\\hat{\\\\theta}_t dt^2 = \\\\frac{1}{n} X D_t X^T (y' - \\\\sigma(X^T \\\\hat{\\\\theta}_t))^2 \\\\leq 2 \\\\sqrt{p} \\\\|X\\\\|_2^2 \\\\frac{1}{n^2},$$\\n\\nwhere $D_t = \\\\text{Diag}(\\\\sigma'(\\\\langle \\\\hat{\\\\theta}_t, x_i \\\\rangle))_{i=1}^n$ is a diagonal matrix with $\\\\|D_t\\\\|_2 \\\\leq 1$. By standard random matrix results [see e.g. (Vershynin, 2018), Theorem 4.4.5],\\n\\n$$E[\\\\|X\\\\|_2^2] \\\\leq C (\\\\sqrt{p} + \\\\sqrt{n})^2,$$\\n\\n$$E[\\\\|X\\\\|_3^2] \\\\leq C (\\\\sqrt{p} + \\\\sqrt{n})^3.$$\\n\\nWhence, for any $t \\\\geq 0$ and $n > N_0$,\\n\\n$$|\\\\text{MSE}''_n(t)| \\\\leq C^3 p (\\\\sqrt{p} + \\\\sqrt{n})^2 \\\\frac{1}{n^2} + \\\\sqrt{p} (\\\\sqrt{p} + \\\\sqrt{n})^3 \\\\frac{1}{n^2} := L,$$\\n\\nwhere $L = C^4 (k_2 + 1)$ is a constant that only depends on $\\\\kappa$.\\n\\nLet $\\\\tau = \\\\tau(\\\\gamma, \\\\gamma_0) = \\\\lim_{n \\\\to \\\\infty} \\\\text{MSE}_n(0)$, and let $N_1 = N_1(\\\\gamma, \\\\gamma_0) > N_0$ satisfy $\\\\text{MSE}_n(0) \\\\leq \\\\tau^2$ for any $n > N_1$. By Claim (a), $\\\\tau < 0$. Then for any $t \\\\leq T_1(\\\\gamma, \\\\gamma_0) = -\\\\tau(\\\\gamma, \\\\gamma_0) \\\\frac{4}{L}$,\\n\\n$$\\\\text{MSE}_n(t) = \\\\text{MSE}_n(0) + (\\\\text{MSE}_n(t) - \\\\text{MSE}_n(0)) \\\\leq \\\\tau^2 + Lt \\\\leq \\\\tau^4.$$\"}"}
{"id": "liu22f", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 8: Baselines with full metrics for cancer survival. All numbers are downscaled by $10^{-2}$.\\n\\n| Methods          | AUC   | ECE  | MCE  | NLL  | Brier | KS   |\\n|------------------|-------|------|------|------|-------|------|\\n| CE Early-stop    | 58.88 | 12.25| 25.35| 67.92| 23.96 | 6.44 |\\n| Temperature      | 58.88 | 12.07| 24.65| 67.11| 23.73 | 6.92 |\\n| Platt Scaling    | 58.91 | 10.28| 27.69| 66.11| 23.33 | 4.91 |\\n| Dirichlet Cal.   | 49.89 | 13.83| 35.52| 67.57| 24.08 | 6.00 |\\n| Mix-n-match      | 58.88 | 12.16| 24.52| 66.89| 23.67 | 7.18 |\\n| Focal loss       | 55.02 | 12.15| 26.34| 65.92| 23.31 | 6.38 |\\n| Entropy Reg.     | 56.29 | 11.73| 30.81| 66.49| 23.62 | 6.83 |\\n| MMCE Reg.        | 48.45 | 11.84| 37.36| 66.83| 23.73 | 3.64 |\\n| Deep Ensemble    | 52.26 | 9.99 | 28.30| 66.22| 23.47 | 5.02 |\\n| CaPE (bin)       | 61.44 | 12.31| 25.27| 65.75| 23.20 | 2.59 |\\n| CaPE (kernel)    | 61.22 | 9.48 | 32.40| 65.70| 23.18 | 3.70 |\\n\\n### Table 9: Baselines with full metrics for weather prediction. All numbers are downscaled by $10^{-2}$.\\n\\n| Methods          | AUC   | ECE  | MCE  | NLL  | Brier | KS   |\\n|------------------|-------|------|------|------|-------|------|\\n| CE Early-stop    | 77.64 | 10.91| 25.50| 59.97| 20.57 | 11.03|\\n| Temperature      | 77.64 | 8.66 | 23.56| 58.77| 20.21 | 7.41 |\\n| Platt Scaling    | 77.65 | 6.97 | 16.47| 57.38| 19.53 | 3.26 |\\n| Dirichlet Cal.   | 77.51 | 14.29| 30.09| 62.83| 21.89 | 5.21 |\\n| Mix-n-match      | 77.64 | 8.65 | 23.58| 58.77| 20.21 | 7.39 |\\n| Focal Loss       | 76.18 | 8.32 | 21.25| 59.01| 20.27 | 4.45 |\\n| Entropy Reg.     | 79.01 | 10.53| 20.72| 57.83| 19.77 | 5.00 |\\n| MMCE Reg.        | 76.69 | 8.46 | 19.73| 59.25| 20.12 | 7.31 |\\n| Deep Ensemble    | 79.86 | 7.41 | 18.24| 55.28| 18.82 | 7.57 |\\n| CaPE (bin)       | 78.99 | 5.16 | 15.09| 79.00| 18.37 | 2.34 |\\n| CaPE (kernel)    | 79.00 | 5.08 | 13.28| 54.32| 18.39 | 2.34 |\\n\\n### Table 10: Baselines with full metrics for collision prediction. All numbers are downscaled by $10^{-2}$.\\n\\n| Methods          | AUC   | ECE  | MCE  | NLL  | Brier | KS   |\\n|------------------|-------|------|------|------|-------|------|\\n| CE Early-stop    | 85.68 | 4.36 | 19.87| 31.67| 8.59  | 1.54 |\\n| Temperature      | 85.68 | 4.56 | 16.79| 30.36| 8.52  | 2.9  |\\n| Platt Scaling    | 85.76 | 3.04 | 12.39| 29.42| 8.23  | 1.52 |\\n| Dirichlet Cal.   | 83.36 | 5.78 | 18.13| 30.90| 8.77  | 1.60 |\\n| Mix-n-match      | 85.68 | 4.40 | 17.41| 30.25| 8.52  | 2.60 |\\n| Focal Loss       | 82.21 | 9.07 | 19.85| 34.41| 9.82  | 8.72 |\\n| Entropy Reg.     | 83.15 | 14.54| 21.27| 38.74| 11.10 | 13.44|\\n| MMCE Reg.        | 85.18 | 2.94 | 8.95 | 30.65| 8.48  | 2.44 |\\n| Deep Ensemble    | 85.27 | 3.15 | 16.53| 30.20| 8.54  | 2.01 |\\n| CaPE (bin)       | 85.70 | 3.16 | 12.21| 30.61| 8.18  | 2.13 |\\n| CaPE (kernel)    | 85.95 | 3.22 | 13.32| 30.44| 8.13  | 2.10 |\\n\\nWe can evaluate $\\\\phi_1, \\\\phi_2$ from a finite sample $(x_i, y_i), i=1,...,n$, $\\\\phi_1(\\\\sigma) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} 1(y_i = 1, f(x_i) \\\\leq \\\\sigma)$, $\\\\phi_2(\\\\sigma) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} 1(f(x_i) \\\\leq \\\\sigma)$.\"}"}
{"id": "liu22f", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The KS error is defined as\\n\\\\[ KS = \\\\max_{1 \\\\leq \\\\sigma \\\\leq 1} |\\\\phi_1(\\\\sigma) - \\\\phi_2(\\\\sigma)|. \\\\]\\n\\n\\\\( \\\\phi_1, \\\\phi_2 \\\\) can be efficiently computed by sorting the data points with respect to their confidence scores \\\\( f(x_i) \\\\). The KS error has the advantage of being independent of binning configurations, unlike ECE and MCE.\\n\\n**EBrier Score Decomposition**\\n\\nWe present here a decomposition of the Brier score into two components, discussed in Section 3. The Brier score can be interpreted as a sum of two terms, calibration and refinement. Assume the network can output one of \\\\( K \\\\) distinct possible predictions, i.e., \\\\( \\\\hat{p} \\\\in \\\\{\\\\hat{q}_1, \\\\ldots, \\\\hat{q}_K\\\\} \\\\).\\n\\nDenote \\\\( S_k \\\\), the set of all inputs with output \\\\( \\\\hat{q}_k \\\\) and \\\\( \\\\bar{q}_k \\\\) the empirical probability over \\\\( S_k \\\\), i.e.,\\n\\\\[ S_k = \\\\{ x | f(x) = \\\\hat{q}_k \\\\}, \\\\quad |S_k| = n_k, \\\\quad \\\\bar{q}_k = \\\\frac{1}{n_k} \\\\sum_{x_i \\\\in S_k} y_i. \\\\]\\n\\nThen we can write\\n\\\\[ Brier = \\\\frac{1}{N} \\\\sum_{i=1}^{N} (\\\\hat{p}_i - y_i)^2 = \\\\frac{1}{N} \\\\sum_{k=1}^{K} \\\\frac{n_k}{n} (\\\\hat{q}_k - \\\\bar{q}_k)^2 + \\\\frac{1}{N} \\\\sum_{k=1}^{K} \\\\frac{n}{n} \\\\bar{q}_k (1 - \\\\bar{q}_k), \\\\]\\n\\nThe first term on the RHS, calibration, is similar to MSE \\\\( p \\\\), with the empirical probabilities \\\\( \\\\bar{q}_k \\\\) substituting for the true labels.\\n\\nThe second term, refinement, is an estimate of the confidence in determining \\\\( \\\\bar{q}_k \\\\). It is related to the area under curve (AUC), which measures to the achievable accuracy of the network as a classifier. The term is smaller as the prediction classes \\\\( \\\\bar{q}_k \\\\) tend towards 0 or 1. Thus, this term penalizes empirically calibrated predictors, with low discriminative power, as in Figure 2.\\n\\n**F Metric Comparison**\\n\\nFigure 11 shows the correlation between different calibration and accuracy metrics, and two gold-standard metric that use ground truth probabilities: MSE \\\\( p \\\\) and KL-divergence. The correlations are computed using all five scenarios in our Face-based Risk Prediction synthetic dataset.\\n\\n**G Estimation of Empirical Probability in CaPE**\\n\\nWe describe in further detail the two ways to estimate the conditional probability \\\\( P(y=1 | f(x) \\\\in I(q)) \\\\), introduced in Section 5.\\n\\nWe wish to estimate the conditional probability of an output \\\\( y \\\\) given a network prediction \\\\( f(x) \\\\),\\n\\\\[ P(y=1 | f(x) \\\\in I(q)) \\\\approx \\\\frac{1}{|I(q)|} \\\\sum_{\\\\hat{p} \\\\in I(q)} P(y=1 | f(x) = \\\\hat{p}). \\\\] (17)\\n\\nAn empirical estimate of \\\\( P(y=1 | f(x) \\\\in I(q)) \\\\) would be\\n\\\\[ P(y=1 | f(x) \\\\in I(q)) \\\\approx \\\\frac{1}{|Index(I(q))|} \\\\sum_{i \\\\text{ s.t. } f(x_i) \\\\in I(q)} y_i, \\\\] (18)\\nwhere \\\\( Index(I(q)) = \\\\{i | f(x_i) \\\\in I(q)\\\\} \\\\).\\n\\nAlternatively, we can use kernel estimation:\\n\\\\[ P(y=1 | f(x) \\\\in I(q)) \\\\approx \\\\frac{1}{Z} \\\\sum_{\\\\hat{p} \\\\in I(q)} P(y=1 | f(x) = \\\\hat{p}) \\\\cdot \\\\exp\\\\left(-\\\\frac{(p - q)^2}{\\\\sigma^2}\\\\right), \\\\] (19)\"}"}
{"id": "liu22f", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11. Correlation between MSE \\\\( p \\\\) and other metrics on synthetic data. Brier score achieves the most consistent correlation with MSE \\\\( p \\\\).\"}"}
{"id": "liu22f", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12. Correlation between $KL_p$ and other metrics on synthetic data. Brier score achieves the most consistent correlation with $KL_p$. \"}"}
{"id": "liu22f", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Results on cancer-survival prediction, weather forecasting, and collision prediction. All numbers are downscaled by $10^{-2}$. Tables with all the metrics described in Section 3 are provided in Appendix C.2. The proposed method CaPE outperforms existing techniques in terms of Brier score, the metric that best captures probability-estimation accuracy.\\n\\n| Method           | Cancer Survival | Weather Forecasting | Collision Prediction |\\n|------------------|-----------------|---------------------|----------------------|\\n|                  | AUC (\u00d710^{-2})  | ECE                 | Brier                |\\n| CE early-stop    | 58.88           | 12.25               | 23.96                |\\n| Temperature      | 58.88           | 12.07               | 23.73                |\\n| Platt Scaling    | 58.91           | 10.28               | 23.33                |\\n| Dirichlet Cal.   | 49.89           | 13.83               | 24.08                |\\n| Mix-n-match      | 58.88           | 12.16               | 23.67                |\\n| Focal Loss       | 55.02           | 12.15               | 23.31                |\\n| Entropy Reg.     | 56.29           | 11.73               | 23.62                |\\n| MMCE Reg.        | 48.45           | 11.84               | 23.73                |\\n| Deep Ensemble    | 52.46           | 9.99                | 23.47                |\\n| CaPE (bin)       | 61.44           | 12.31               | 23.20                |\\n| CaPE (kernel)    | 61.22           | 9.48                | 23.18                |\\n\\nSince the underlying ground-truth probabilities are unobserved. As discussed in Section 3, Brier score is the metric that best captures the quality of probability estimates. CaPE has the lowest Brier score in all three datasets, while also achieving lower ECE values and higher AUC values than most other methods. This demonstrates that enforcing better probability estimation during training also yields a more discriminative model. The reliability diagrams in Figure 6 depict the probability estimates produced by CE, CaPE and the best baseline method on the three datasets, demonstrating that CaPE produces outputs that are better calibrated on real data.\\n\\nFigure 6 also shows that each real-world dataset closely aligns with a particular synthetic scenario: cancer survival with Centered; weather forecasting with Linear; collision prediction with Skewed. This supports the significance of our synthetic benchmark dataset, and provides insights in the differences among baseline models. For example, model averaging with deep ensemble performs well on weather forecasting but has higher Brier scores than Platt scaling on the other two datasets (see Appendix L for further analysis based on pathological stages). Accordingly, deep ensemble also underperforms in the synthetic scenarios where ground-truth probabilities are clustered closely (Sigmoid, Centered), but is effective for Linear. Finally, as in the synthetic Skewed scenario, all methods had similar performance on the collision prediction task. This highlights the importance of considering different scenarios when evaluating methodology for probability estimation.\\n\\n## Conclusion\\nIn this work we evaluate existing approaches to improve the output probabilities of neural networks on probability-estimation problems. To this end, we introduce a new synthetic benchmark dataset designed to reproduce several realistic scenarios, and also gather three real-world datasets relevant to medicine, climatology, and self-driving cars. In addition, we provide theoretical analysis showing that early learning and memorization are fundamental phenomena in high-dimensional probability estimation. Motivated by this, we propose a novel approach that outperforms existing approaches on our simulated and real-world benchmarks. An important application for probability estimation is in the context of survival analysis, which can be recast as estimation of conditional probabilities (Lee et al., 2018; Shamout et al., 2020; Goldstein et al., 2021). Another interesting research direction is to consider problems with several possible uncertain outcomes (analogous to multiclass classification).\\n\\n## Acknowledgements\\nThe authors gratefully acknowledge support from Alzheimer's Association grant AARG-NTF-21-848627 (S.L.), NIH grant R01 LM01331 (A.K., B.Y.), NSF grants HDR-1940097 (M.L.), DMS2009752 (C.F.G., S.L.) and NRT-1922658 (A.K., S.L., S.M., B.Y., W.Z.), and the NYU Langone Health Predictive Analytics Unit (N.R., W.Z.).\\n\\nFunding for the Multiscale Machine Learning in coupled Earth System Modeling (M2LInES) project was provided to Laure Zanna by the generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program. We would also like to thank Juan Argote for useful discussions, and the reviewers for their comments.\\n\\n## References\\nAgrawal, S., Barrington, L., Bromberg, C., Burge, J., Gazen, C., and Hickey, J. Machine learning for precipitation nowcasting from radar images. arXiv preprint arXiv:1912.12132, 2019.\\n\\nAyzel, G. Rainnet: a convolutional neural network for radar-based precipitation nowcasting. https://github.com/hydrogo/rainnet, 2020.\\n\\nBuja, A., Stuetzle, W., and Shen, Y. Loss functions for binary classification.\"}"}
{"id": "liu22f", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nnary class probability estimation and classification: Structure and applications,\" manuscript, available at www-stat.wharton.upenn.edu/buja, 2005.\\n\\nCandes, E. J. and Sur, P. The phase transition for the existence of the maximum likelihood estimate in high-dimensional logistic regression, 2018.\\n\\nChen, X., Fan, H., Girshick, R. B., and He, K. Improved baselines with momentum contrastive learning. CoRR, abs/2003.04297, 2020. URL https://arxiv.org/abs/2003.04297.\\n\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\\n\\nGal, Y. and Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertainty in deep learning, 2016.\\n\\nGoldstein, M., Han, X., Puli, A., Perotte, A. J., and Ranganath, R. X-cal: Explicit calibration for survival analysis, 2021.\\n\\nGoodfellow, I., Bengio, Y., Courville, A., and Bengio, Y. Deep learning, volume 1. MIT Press, 2016.\\n\\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In International Conference on Machine Learning, pp. 1321\u20131330. PMLR, 2017.\\n\\nGupta, K., Rahimi, A., Ajanthan, T., Mensink, T., Sminchisescu, C., and Hartley, R. Calibration of neural networks using splines. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=eQe8DEWNN2W.\\n\\nH\u00fcllermeier, E. and Waegeman, W. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine Learning, 110(3):457\u2013506, 2021.\\n\\nIlse, M., Tomczak, J., and Welling, M. Attention-based deep multiple instance learning. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 2127\u20132136. PMLR, 10\u201315 Jul 2018. URL https://proceedings.mlr.press/v80/ilse18a.html.\\n\\nKim, H., Lee, K., Hwang, G., and Suh, C. Crash to not crash: Learn to identify dangerous vehicles using a simulator. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 978\u2013985, 2019.\\n\\nKrizhevsky, A. Learning multiple layers of features from tiny images. Technical report, 2009.\\n\\nKuleshov, V., Fenner, N., and Ermon, S. Accurate uncertainties for deep learning using calibrated regression. In International conference on machine learning, pp. 2796\u20132804. PMLR, 2018.\\n\\nKull, M., Filho, T. M. S., and Flach, P. Beyond sigmoids: How to obtain well-calibrated probabilities from binary classifiers with beta calibration. Electronic Journal of Statistics, 11(2):5052 \u2013 5080, 2017. doi: 10.1214/17-EJS1338SI. URL https://doi.org/10.1214/17-EJS1338SI.\\n\\nKull, M., Perell\u00f3-Nieto, M., K\u00e4ngsepp, M., de Menezes e Silva Filho, T., Song, H., and Flach, P. A. Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration. In NeurIPS, 2019.\\n\\nKumar, A., Sarawagi, S., and Jain, U. Trainable calibration measures for neural networks from kernel mean embeddings. In ICML, 2018.\\n\\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles, 2017.\\n\\nLee, C., Zame, W., Yoon, J., and van der Schaar, M. Deephit: A deep learning approach to survival analysis with competing risks. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018. URL https://ojs.aaai.org/index.php/AAAI/article/view/11842.\\n\\nLiang, G., Zhang, Y., Wang, X., and Jacobs, N. Improved trainable calibration method for neural networks on medical imaging classification. In British Machine Vision Conference (BMVC), 2020.\\n\\nLiu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C. Early-learning regularization prevents memorization of noisy labels. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nMaddox, W., Garipov, T., Izmailov, P., Vetrov, D. P., and Wilson, A. G. A simple baseline for bayesian uncertainty in deep learning. CoRR, abs/1902.02476, 2019. URL http://arxiv.org/abs/1902.02476.\\n\\nMinderer, M., Djolonga, J., Romijnders, R., Hubis, F., Zhai, X., Houlsby, N., Tran, D., and Lucic, M. Revisiting the calibration of modern neural networks. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nMukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P. H. S., and Dokania, P. Calibrating deep neural networks using focal loss. ArXiv, abs/2002.09437, 2020.\"}"}
{"id": "liu22f", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "liu22f", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nZhang, H., Ciss\u00e9, M., Dauphin, Y., and Lopez-Paz, D.\\n\\nmixup: Beyond empirical risk minimization. ArXiv, abs/1710.09412, 2018.\\n\\nZhang, J., Kailkhura, B., and Han, T. Y. Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning. In ICML, 2020.\\n\\nZhang, Z., Song, Y., and Qi, H. Age progression/regression by conditional adversarial autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017b.\\n\\nZhao, S. and Ermon, S. Right decisions from wrong predictions: A mechanism design alternative to individual calibration. In International Conference on Artificial Intelligence and Statistics, pp. 2683\u20132691. PMLR, 2021.\"}"}
{"id": "liu22f", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nFigure 4. Miscalibration due to overfitting and how to avoid it. The top row shows the histogram of predicted probabilities for the synthetic scenario (see Section 7.1). Ideally each histogram should be concentrated around the corresponding value of $p_i$. The bottom row shows results for the Linear scenario. The horizontal and vertical coordinate of each blue point represent the predicted ($\\\\hat{p}_i$) and true ($p_i$) probabilities of a test example respectively. We also show a reliability diagram of binned mean predicted and empirical probabilities in black (see Section 3). The dashed line indicates perfect calibration. When trained on infinite data obtained by resampling outcome labels at each epoch according to ground-truth probabilities, models minimizing cross-entropy are well calibrated (first column). However, when trained on fixed observed outcomes, the model eventually overfits and the probabilities collapse to either 0 or 1 (second column). This is mitigated via early stopping (i.e. selecting the model based on validation cross-entropy loss), which yields relatively good calibration (third column). The proposed Calibration Probability Estimation (CaPE) method exploits this to further improve the model while ensuring that the output remains well calibrated. Appendix C.3 shows plots for all synthetic data scenarios.\\n\\n5 Calibrated Probability Estimation (CaPE)\\n\\nWe propose to exploit the training dynamics of cross-entropy minimization through a method that we name Calibrated Probability Estimation (CaPE). Our starting point is a model obtained via early stopping using validation data on the cross-entropy loss. CaPE is designed to further improve the discrimination ability of the model, while ensuring that it remains well calibrated. This is achieved by alternatively minimizing the following two loss functions:\\n\\n**Discrimination loss**\\n\\n$$L_D = -\\\\sum_{i=1}^{N} [y_i \\\\log(f(x_i)) + (1-y_i) \\\\log(1-f(x_i))]$$\\n\\n**Calibration loss**\\n\\n$$L_C = -\\\\sum_{i=1}^{N} p_i^{\\\\text{emp}} \\\\log(f(x_i)) + (1-p_i^{\\\\text{emp}}) \\\\log(1-f(x_i)),$$\\n\\nwhere $p_i^{\\\\text{emp}}$ is an estimate of the conditional probability $P[y = 1 | f(x) \\\\in I(f(x_i))]$ and $I(f(x_i))$ is a small interval centered at $f(x_i)$. As explained in Section 3 if $f(x_i)$ is close to this value, then the model is well calibrated. We consider two approaches for estimating $p_i^{\\\\text{emp}}$. (1) CaPE (bin) where we divide the training set into bins, select the bin $b_i$ containing $f(x_i)$ and set $p_i^{\\\\text{emp}} = p(b_i)$ in equation 2. (2) CaPE (kernel) where $p_i^{\\\\text{emp}}$ is estimated through a moving average with a kernel function (see Appendix G for more details). Both methods are efficiently computed by sorting the predictions $\\\\hat{p}_i$. The calibration loss requires a reasonable estimation of the empirical probabilities $p_i^{\\\\text{emp}}$, which can be obtained from the model after early learning. Therefore using the calibration loss from the beginning is counterproductive, as demonstrated in Section M. We note that a variant of CaPE can be implemented using a weighted sum of the calibration and discrimination loss.\\n\\nCaPE is summarized in Algorithm 1. Figures 4 and 5 show that incorporating the calibration-loss minimization step indeed preserves calibration as training proceeds (this is not necessarily expected because CaPE minimizes a calibration loss on the training data), and prevents the model from overfitting the observed outputs. This is beneficial also for the discriminative ability of the model, because it enables it to further reduce the cross-entropy loss without overfitting.\"}"}
{"id": "liu22f", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation, as shown in Figure 5. The experiments with synthetic and real-world data reported in Section 7 suggest that this approach results in accurate probability estimates across a variety of realistic scenarios.\\n\\n6 Related Work\\n\\nNeural networks trained for classification often generate a probability associated with their prediction which quantifies its uncertainty. These estimates have been found to be inaccurate in certain situations (Mukhoti et al., 2020; Guo et al., 2017; Zhao & Ermon, 2021) (although a recent study suggests that transformer-based models tend to be well calibrated in vision-based classification tasks (Minderer et al., 2021)).\\n\\nCalibration methods to mitigate this issue broadly fall into three categories depending on whether they: (1) postprocess the outputs of a trained model, (2) combine multiple model outputs, or (3) modify the training process.\\n\\nPost-processing methods transform the output probabilities in order to improve calibration on held-out data (Zadrozny & Elkan, 2001; Gupta et al., 2021; Kull et al., 2017; 2019). For example, Platt scaling (Platt, 1999) fits a logistic function that minimizes the negative log-likelihood loss. Temperature scaling (Guo et al., 2017) does the same with a temperature parameter augmenting the softmax function. Another approach trains a recalibration model on the outputs of an uncalibrated model (Kuleshov et al., 2018). In contrast to these methods, CaPE enforces calibration during training, which has the advantage of enabling further improvements in the discriminative abilities of the model.\\n\\nEnsembling methods combine multiple models to improve generalization. Mix-n-Match (Zhang et al., 2020) uses a single model, and ensembles predictions using multiple temperature scaling transformations. Other methods (Lakshminarayanan et al., 2017; Maddox et al., 2019) ensemble multiple models obtained using different initializations. These approaches are compatible with the proposed method CaPE; how to combine them effectively is an interesting future research direction.\\n\\nModified training methods can be divided into two groups. The first group smooths the target 0/1 labels in order to prevent output estimates from collapsing to 0/1 (Mukhoti et al., 2020; Szegedy et al., 2016; Zhang et al., 2018; Thulasidasan et al., 2020). The second group, attaches additional calibration penalties to a cross entropy loss (Kumar et al., 2018; Pereyra et al., 2017; Liang et al., 2020). CaPE is most similar in spirit to the latter methods, although its data-driven calibration loss is different to the penalties used in these techniques.\\n\\nDatasets for evaluation\\n\\nThe methods discussed in this section were developed for calibration in classification, and tested on datasets such as CIFAR-10/100 (Krizhevsky, 2009), SVHN (Netzer et al., 2011), and ImageNet (Deng et al., 2009) where the relationship between labels and input data is completely deterministic. Here, we evaluate these methods on synthetic and real-world probability-estimation problems with inherent uncertainty.\\n\\n7 Experiments\\n\\n7.1 Synthetic Dataset: Face-based Risk Prediction\\n\\nTo benchmark probability-estimation methods, we build a synthetic dataset based on UTKFace (Zhang et al., 2017b), containing face images and associated ages. We use the age of the $i^{th}$ person $z_i$ to assign them a risk of contracting a disease $p_i = \\\\psi(z_i)$ for a fixed function $\\\\psi: Z \\\\geq 0 \\\\rightarrow [0, 1]$. Then we simulate whether the person actually contracts the illness (label $y_i = 1$) or not ($y_i = 0$) with probability $p_i$. The probability-estimation task is to estimate the ground-truth probability $p_i$ from the face image $x_i$ using a model that only has access to the images and the binary observations during training. This requires learning to discriminate age and map it to the corresponding risk. We design $\\\\psi$ to create five scenarios, inspired by real-world data (see Appendix I):\\n\\n- **Linear**: Equally-spaced, inspired by weather forecasting: $\\\\psi(z) = z/100$\\n- **Sigmoid**: Concentrated near two extremes: $\\\\psi(z) = \\\\sigma(25((z/100 - 0.29)))$\\n- **Skewed**: Clustered close to zero, inspired by vehicle-collision detection: $\\\\psi(z) = z/250$\\n- **Centered**: Clustered in the center, inspired by cancer-survival prediction: $\\\\psi(z) = z/300 + 0.35$\\n- **Discrete**: Discretized: $\\\\psi(z) = 0.2[\\\\{z > 20\\\\} + \\\\{z > 40\\\\} + \\\\{z > 60\\\\} + \\\\{z > 80\\\\}] + 0.1$ In addition, we report an experiment with simulated probabilistic labels on CIFAR-10 in Appendix J.\\n\\n7.2 Real-World Datasets\\n\\nWe use three open-source, real-world datasets to benchmark probability-estimation approaches (see Appendix K for further details on the datasets and experiments).\\n\\nSurvival of Cancer Patients. Histopathology aims to identify tumor cells, cancer subtypes, and the stage and level of differentiation of cancer. Hematoxylin and Eosin (H&E)-stained slides are the most common type of histopathology data used for clinical decision making. In particular, they can be used for survival prediction (Wulczyn et al., 2020), which is critical in evaluating the prognosis of patients. Treatments assigned to patients after diagnosis are not personalized and their impact on cancer trajectory is complex, so the survival status of a patient is not deterministic. In this work, we use the H&E slides of non-small cell lung cancers from The Cancer Genome Atlas Program...\"}"}
{"id": "liu22f", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nFigure 5.\\nCalibrated Probability Estimation prevents overfitting. Comparison between the learning curves of cross-entropy (CE) minimization and the proposed calibrated probability estimation (CaPE), smoothed with a 5-epoch moving average. After an early-learning stage where both training and validation losses decrease, CE minimization overfits (first and second graph), with disastrous consequences in terms of probability estimation (third and fourth graph). In contrast, CaPE prevents overfitting, continuing to improve the model while maintaining calibration (see Figure 4).\\n\\nAlgorithm 1\\n\\nPseudocode for CaPE\\n\\nRequire:\\n\\\\( f \\\\) \\\\hspace{1cm} \\\\text{\\\\textit{early stopped model}}\\n\\nRequire:\\n\\\\( m \\\\) \\\\hspace{1cm} \\\\text{\\\\textit{freq. of training with } L_{\\\\text{C}}} \\\\hspace{1cm} \\\\text{\\\\textit{kernel}}\\n\\nRequire:\\n\\\\( \\\\{ x_i, y_i \\\\}_{i=1}^N \\\\) \\\\hspace{1cm} \\\\text{\\\\textit{training set}}\\n\\nRequire:\\n\\\\( K(p, q) := \\\\exp \\\\left( -\\\\frac{(p - q)^2}{\\\\sigma^2} \\\\right) \\\\) \\\\hspace{1cm} \\\\text{\\\\textit{Gaussian kernel}}\\n\\nfor \\\\( t = 1 \\\\) to num\\\\text{\\\\textit{epochs}} do\\n\\nif \\\\( t \\\\mod m = 0 \\\\) then\\n\\n\\\\( \\\\hat{p}_i \\\\leftarrow f(x_i), \\\\forall i \\\\)\\n\\nUpdate \\\\( p_{\\\\text{emp}} \\\\), \\\\( \\\\forall i \\\\), with B\\\\text{\\\\textit{IN}} or K\\\\text{\\\\textit{ERNEL}}\\n\\n\\\\( L \\\\leftarrow L_{\\\\text{C}} \\\\) \\\\hspace{1cm} \\\\text{\\\\textit{compute discrimination loss}}\\n\\nelse\\n\\n\\\\( L \\\\leftarrow L_{\\\\text{D}} \\\\) \\\\hspace{1cm} \\\\text{\\\\textit{compute calibration loss}}\\n\\nend if\\n\\nend for\\n\\nfunction B\\\\text{\\\\textit{IN}}(B) \\\\hspace{1cm} \\\\text{\\\\textit{B-number of bins}}\\n\\nI_1, \\\\ldots, I_B \\\\leftarrow \\\\text{partitions by quantile of } \\\\{ \\\\hat{p}_j \\\\}_{j=1}^N\\n\\nFind \\\\( b \\\\) such that \\\\( \\\\hat{p}_i \\\\in I_b \\\\)\\n\\nIndex(\\\\( I_b \\\\)) \\\\leftarrow \\\\{ j | \\\\hat{p}_j \\\\in I_b \\\\} \\\\hspace{1cm} \\\\text{\\\\textit{get indices in bin } b}\\n\\n\\\\( p_{\\\\text{emp}} \\\\leftarrow \\\\frac{1}{|I_b|} \\\\sum_{j \\\\in \\\\text{Index}(I_b)} y_i \\\\hspace{1cm} \\\\text{\\\\textit{empirical mean of bin } b} \\\\)\\n\\nend function\\n\\nfunction K\\\\text{\\\\textit{ERNEL}}(r, K) \\\\hspace{1cm} \\\\text{\\\\textit{r-window size; } K\\\\text{\\\\textit{-kernel}}}\\n\\nN_r(i) \\\\leftarrow \\\\text{\\\\textit{r-nearest neighbor of } \\\\hat{p}_i (output probability space)}\\n\\nZ \\\\leftarrow \\\\sum_{\\\\hat{p}_j \\\\in N_r(i)} K(\\\\hat{p}_i, \\\\hat{p}_j) \\\\hspace{1cm} \\\\text{\\\\textit{normalization factor}}\\n\\np_{\\\\text{emp}} \\\\leftarrow \\\\frac{\\\\sum_{\\\\hat{p}_j \\\\in N_r(i)} K(\\\\hat{p}_i, \\\\hat{p}_j) y_j}{Z} \\\\hspace{1cm} \\\\text{\\\\textit{kernel smooth}}\\n\\nend function\\n\\nTCGA\\n\\n3 to estimate the 5-year survival probability of cancer patients. The outcome distribution is similar to the Centered scenario in Section 7.1.\\n\\nWeather Forecasting.\\n\\nThe atmosphere is governed by nonlinear dynamics, hence weather forecast models possess inherent uncertainties (Richardson, 2007). Nowcasting, weather prediction in the near future, is of great operational significance, especially with increasing number of extreme inclement weather conditions (Agrawal et al., 2019; Ravuri et al., 2021). We use the German Weather service dataset, which contains quality-controlled rainfall-depth composites from 17 operational Doppler radars. We use 30 minutes of precipitation data to predict if the mean precipitation over the area covered will increase or decrease one hour after the most recent measurement. Three precipitation maps from the past 30 minutes serve as an input. The outcome distribution is similar to the Linear scenario in Section 7.1.\\n\\nCollision Prediction.\\n\\nVehicle collision is one of the leading causes of death in the world. Reliable collision prediction systems are therefore instrumental in saving human lives. These systems predict potential collisions from dashcam cameras. Collisions are influenced by many unknown factors, and hence are not deterministic. Following (Kim et al., 2019), we use 0.3 seconds of real dashcam videos from YouTubeCrash dataset as input, and predict the probability of a collision in the next 2 seconds. The data are very imbalanced as the number of collisions is very low, so the outcome distribution is similar to the Skewed scenario in Section 7.1.\\n\\n7.3 Baselines\\n\\nWe apply existing calibration methods developed for classification to probability estimation (as well as cross-entropy minimization with early-stopping): (1)\"}"}
{"id": "liu22f", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Results on synthetic data. All numbers are downscaled by\\n\\n| Method                  | MSE 0.2 | MSE 0.6 | MSE 1.0 |\\n|-------------------------|---------|---------|---------|\\n| CE Early-stop           | 0.0     | 0.0     | 0.0     |\\n| Deep Ensemble           | 0.0     | 0.0     | 0.0     |\\n| CaPE (kernel)           | 0.0     | 0.0     | 0.0     |\\n| MMCE Reg.               | 0.0     | 0.0     | 0.0     |\\n| Entropy Reg.            | 0.0     | 0.0     | 0.0     |\\n| Mix-n-match             | 0.0     | 0.0     | 0.0     |\\n| Platt Scaling           | 0.0     | 0.0     | 0.0     |\\n| Temperature             | 0.0     | 0.0     | 0.0     |\\n| Focal Loss              | 0.0     | 0.0     | 0.0     |\\n| CE + resampled labels*  | 0.0     | 0.0     | 0.0     |\\n| Linear                  | 0.0     | 0.0     | 0.0     |\\n\\nSection 3): The dashed line indicates perfect calibration. The results are computed on test data for cross-entropy minimization with early\\n\\n...and hyperparameter optimization.\\n\\nFor real-world data, we also compare against a model trained\\n\\nby repeatedly sampling new\\n\\nindividual baseline models in most scenarios, in both metrics, except for the skewed case, where the difference is statistically\\n\\ninsignificant.\\n\\nNo baseline outperforms the proposed method CaPE in any of the scenarios, and CaPE outperforms all the\\n\\nintervals obtained via bootstrapping. * is a model trained via cross-entropy minimization from data obtained by continuous\\n\\nresampling.\"}"}
{"id": "liu22f", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nSheng Liu\\nAakash Kaku\\nWeicheng Zhu\\nMatan Leibovich\\nSreyas Mohan\\nBoyang Yu\\nHaoxiang Huang\\nLaure Zanna\\nNarges Razavian\\nJonathan Niles-Weed\\nCarlos Fernandez-Granda\\n\\nAbstract\\n\\nReliable probability estimation is of crucial importance in many real-world applications where there is inherent (aleatoric) uncertainty. Probability-estimation models are trained on observed outcomes (e.g. whether it has rained or not, or whether a patient has died or not), because the ground-truth probabilities of the events of interest are typically unknown. The problem is therefore analogous to binary classification, with the important difference that the objective is to estimate probabilities rather than predicting the specific outcome. This work investigates probability estimation from high-dimensional data using deep neural networks. There exist several methods to improve the probabilities generated by these models but they mostly focus on model (epistemic) uncertainty. For problems with inherent uncertainty, it is challenging to evaluate performance without access to ground-truth probabilities. To address this, we build a synthetic dataset to study and compare different computable metrics. We evaluate existing methods on the synthetic data as well as on three real-world probability estimation tasks, all of which involve inherent uncertainty: precipitation forecasting from radar images, predicting cancer patient survival from histopathology images, and predicting car crashes from dashcam videos. We also give a theoretical analysis of a model for high-dimensional probability estimation which reproduces several of the phenomena evinced in our experiments. Finally, we propose a new method for probability estimation using neural networks, which modifies the training process to promote output probabilities that are consistent with empirical probabilities computed from the data. The method outperforms existing approaches on most metrics on the simulated as well as real-world data.\\n\\n1 Introduction\\n\\nWe consider the problem of building models that answer questions such as: Will it rain? Will a patient survive? Will a car collide with another vehicle? Due to the inherently-uncertain nature of these real-world phenomena, this requires performing probability estimation, i.e. evaluating the likelihood of each possible outcome for the phenomenon of interest. Models for probability prediction must be trained on observed outcomes (e.g. whether it rained, a patient died, or a collision occurred), because the ground-truth probabilities are unknown. The problem is therefore analogous to binary classification, with the important difference that the objective is to estimate probabilities rather than predicting specific outcomes. In probability estimation, two identical inputs (e.g. histopathology images from cancer patients) can potentially result in two different outcomes (death vs. survival). In contrast, in classification the class label is usually completely determined by the data (a picture either shows a cat or it does not).\\n\\nThe goal of this work is to investigate probability estimation from high-dimensional data using deep neural networks. Probability estimation is a fundamental problem in machine learning (Murphy, 2013). Deep networks trained for classification often generate probabilities, which quantify the uncertainty of the estimate (i.e. how likely the network is to classify correctly). This quantification has been observed to be inaccurate, and several methods have been developed to improve it (Platt, 1999; Guo et al., 2017; Szegedy et al., 2016; Zhang et al., 2020; Thulasidasan et al., 2020; Mukhoti et al., 2020; Thagaard et al., 2020), including Bayesian neural networks (Gal & Ghahramani, 2016; Wang et al., 2016; Shekhovtsov & Flach, 2019; Postels et al., 2019). However, these works restrict their attention almost exclusively to classification in datasets (e.g. CIFAR-10/100 (Krizhevsky, 1999)).\\n\\nCode available at https://jackzhu727.github.io/deep-probability-estimation/.\"}"}
{"id": "liu22f", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nFigure 1. The probability-estimation problem. In probability estimation, we assume that each observed outcome $y_i$ (e.g., death or survival in cancer patients) in the training set is randomly generated from a latent unobserved probability $p_i$ associated to the corresponding data $x_i$ (e.g., histopathology images).\\n\\nTraining (left): Only $x_i$ and $y_i$ can be used for training, because $p_i$ is not observed.\\n\\nInference (right): Given new data $x$, the trained network $f$ produces a probability estimate $\\\\hat{p} \\\\in [0, 1]$.\\n\\n2009), or ImageNet (Deng et al., 2009) where the label itself is not uncertain: it quantifies the confidence of the model in its own prediction, not the probability of an event of interest. In the literature, this is known as epistemic (model) uncertainty (H\u00fcllermeier & Waegeman, 2021; Tagasovska & Lopez-Paz, 2019). We focus on aleatoric uncertainty, stemming from inherent uncertainty in the problem under study.\\n\\nTo formalize this distinction, we propose and rigorously analyze a simple high-dimensional model with uncertain labels, and show that even in this simple model, classification-based methods fail to yield good probability estimates.\\n\\nProbability estimation from high-dimensional data is a problem of critical importance in medical prognostics (Wulczyn et al., 2020), weather prediction (Agrawal et al., 2019), and autonomous driving (Kim et al., 2019). In order to advance deep-learning methodology for probability estimation it is crucial to build appropriate benchmark datasets. Here we build a synthetic dataset and gather three real-world datasets, which we use to systematically evaluate existing methodology. In addition, we propose a novel approach for probability estimation, which outperforms current state-of-the-art methods. Our contributions are the following:\\n\\n\u2022 We give a theoretical analysis of the probability estimation problem for a high-dimensional logistic model, and establish that predictors trained by minimizing the cross-entropy loss overfit the observed outcomes and fail to yield calibrated outcomes. However, we show that the predictions are well calibrated during the initial stages of training.\\n\\n\u2022 We introduce a new synthetic dataset for probability estimation where a population of people may have a certain disease associated with age. The task is to estimate the probability that they contract the disease from an image of their face. The data are generated using the UTKFaces dataset (Zhang et al., 2017b), which contains age information. The dataset contains multiple versions of the synthetic labels, which are generated according to different distributions designed to mimic real-world probability-prediction datasets. The dataset serves two objectives. First, it allows us to evaluate existing methodology. Second, it enables us to evaluate different metrics in a controlled scenario where we have access to ground-truth probabilities.\\n\\n\u2022 We have used publicly available data to build probability-estimation benchmark datasets for three real-world applications: (1) precipitation forecasting from radar images, (2) prediction of cancer-patient survival from histopathology images, and (3) prediction of vehicle collisions from dashcam videos. We use these datasets to systematically evaluate existing approaches, which have been previously tested mainly on classification datasets.\\n\\n\u2022 We propose Calibrated Probability Estimation (CaPE), a novel technique which modifies the training process so that output probabilities are consistent with empirical probabilities computed from the data. CaPE outperforms existing approaches on most metrics on synthetic and real-world data.\"}"}
{"id": "liu22f", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nThe corresponding label $y_i$ is either 0 or 1 depending on whether or not the event in question occurred. For example, in the cancer-survival application $x_i$ is a histopathology image of a patient, and $y_i$ equals 1 if the patient survived for 5 years after $x_i$ was collected. The data have inherent uncertainty: $y_i$, the patient\u2019s survival, does not depend deterministically on the histopathology image (due e.g. to comorbidities and other health factors). Instead, we assume that $y_i$ equals 1 with a certain probability $p_i$ associated with $x_i$, as illustrated in Figure 1, because the input data provides key information about the patient\u2019s survival chances.\\n\\nAt inference, a probability-estimation model aims to generate an estimate $\\\\hat{p}$ of the underlying probability $p$, associated with a new input data point (e.g. the probability of survival for over 5 years for new patients based on their histopathology data). To summarize, this is not just a classification problem, because it involves aleatoric uncertainty. Instead, the goal is to predict the probability of the outcome, which is critical in choosing a course of treatment for the patient.\\n\\nEvaluation Metrics\\n\\nProbability estimation shares similar target labels and network outputs with binary classification. However, classification accuracy is not an appropriate metric for evaluating probability-estimation models due to the inherent uncertainty of the outcomes. This is illustrated by the example in Figure 2a where a perfect probability estimate would result in a classification accuracy of just 75%.\\n\\nMetrics when ground-truth probabilities are available. For synthetic datasets, we have access to the ground truth probability labels and can use them to evaluate performance. Two reasonable metrics are the mean squared error or $\\\\ell_2$ distance $\\\\text{MSE}_p$ and the Kullback\u2013Leibler divergence $\\\\text{KL}_p$ between the estimated and ground-truth probabilities:\\n\\n$$\\\\text{MSE}_p = \\\\frac{1}{N} \\\\sum_{i=1}^{N} (\\\\hat{p}_i - p_i)^2,$$\\n\\n$$\\\\text{KL}_p = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\hat{p}_i \\\\log \\\\frac{\\\\hat{p}_i}{p_i} + (1 - \\\\hat{p}_i) \\\\log \\\\frac{1 - \\\\hat{p}_i}{1 - p_i}.$$\\n\\n$N$ is the number of data points, and $p_i, \\\\hat{p}_i$ are the ground-truth and predicted probabilities, respectively.\\n\\nCalibration metrics.\\n\\nGround-truth probabilities are not available for real-world data. In order to evaluate the probabilities estimated by a model, we need to compare them to the observed probabilities. To this end, we aggregate the examples for which the model output equals a certain value (e.g. 0.5), and verify what fraction of them have outcomes equal to 1. If the fraction is close to the model output, then the model is said to be well calibrated.\\n\\nDefinition 3.1. A model $f$ is well calibrated if\\n\\n$$P(y = 1 | f(x) \\\\in I(q)) = q, \\\\forall 0 \\\\leq q \\\\leq 1,$$\\n\\nwhere $y$ is the observed outcome, $f(x)$ is the probability predicted by model $f$ for input $x$, and $I(q)$ is a small interval around $q$.\\n\\nModel calibration can be evaluated using the expected calibration error (ECE) (Guo et al., 2017) (note however that the definition in (Guo et al., 2017) is specific to classification). Given a probability-estimation model $f$ and a dataset of input data $x_i$ and associated outcomes $y_i, 1 \\\\leq i \\\\leq N$, we partition the examples into $B$ bins, $I_1, I_2, \\\\cdots, I_B$, according to the probabilities assigned to the examples by the model. Let $Q_1, \\\\ldots, Q_{B-1}$ the $B$-quantiles of the set $\\\\{f(x_1), \\\\ldots, f(x_N)\\\\}$, we have\\n\\n$$I_b := [Q_{b-1}, Q_b] \\\\cap \\\\{f(x_i)\\\\}_{i=1}^{N} (\\\\text{setting } Q_0 = 0).$$\\n\\nFor each bin, we compute the mean empirical and predicted probabilities,\\n\\n$$p_{\\\\text{emp}}(b) = \\\\frac{1}{|I_b|} \\\\sum_{i \\\\in \\\\text{Index}(I_b)} y_i,$$\\n\\n$$q(b) = \\\\frac{1}{|I_b|} \\\\sum_{i \\\\in \\\\text{Index}(I_b)} f(x_i),$$\\n\\nwhere $\\\\text{Index}(I_b) = \\\\{i | f(x_i) \\\\in I_b\\\\}$.\\n\\nThe pairs $(q(b), p_{\\\\text{emp}}(b))$ can be plotted as a reliability diagram, shown in the second row of Figure 4 and in Figure 6. ECE is then defined as\\n\\n$$\\\\text{ECE} = \\\\frac{1}{B} \\\\sum_{b=1}^{B} p_{\\\\text{emp}}(b) - q(b).$$\\n\\nOther metrics for calibration include the maximum calibration error (MCE) defined as\\n\\n$$\\\\text{MCE} = \\\\max_{b=1, \\\\ldots, B} p_{\\\\text{emp}}(b) - q(b),$$\\n\\nand the Kolmogorov-Smirnov error (KS-error) (Gupta et al., 2021), a metric based on the cumulative distribution function, which is described in more detail in Appendix D.\\n\\nBrier score. Crucially, a model without any discriminative power can be perfectly calibrated (see Figure 2). The Brier score is a metric designed to evaluate both calibration and discriminative power. It is the mean squared error between the predicted probability and the observed outcomes:\\n\\n$$\\\\text{Brier} = \\\\frac{1}{N} \\\\sum_{i=1}^{N} (\\\\hat{p}_i - y_i)^2.$$\"}"}
{"id": "liu22f", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deep Probability Estimation\\n\\nFigure 3. Evaluating evaluation metrics. We use synthetic data to compare different metrics to the gold-standard MSE that uses ground-truth probabilities. Brier score is highly correlated with MSE, in contrast to the classification metric AUC and the calibration metrics ECE, MCE and KS-Error. The graphs show the results of the proposed method CaPE, as well as the baselines described in Section 7.3 on the Linear scenario (see Section 7.1). Results on other scenarios and a similar comparison with KL are reported in Appendix F.\\n\\nThis score can be decomposed into two terms associated with calibration and discrimination, as shown in Appendix E.\\n\\nUsing the synthetic data in Section 7.1, where the ground-truth probabilities are known, we show that Brier score is indeed a reliable proxy for the gold-standard MSE metric based on ground-truth probabilities MSE, in contrast to calibration metrics such as ECE, MCE or KS-error, and to classification metrics such as AUC (see Figure 3 and Appendix F).\\n\\n4 Early Learning and Memorization\\n\\nPrediction models based on deep learning are typically trained by minimizing the cross entropy between the model output and the training labels (Goodfellow et al., 2016). This cost function is a proper scoring rule, meaning that it evaluates probability estimates in a consistent manner and is therefore guaranteed to be well calibrated in an infinite-data regime (Buja et al., 2005), as illustrated by Figure 4 (first column).\\n\\nUnfortunately, in practice, prediction models are trained on finite data. This is crucial in the case of deep neural networks, which are highly overparametrized and therefore prone to overfitting (Goodfellow et al., 2016). For classification, deep neural networks have been shown to be capable of fitting arbitrary random labels (Zhang et al., 2017a). In probability estimation, we observe that neural networks indeed eventually overfit and memorize the observed outcomes completely. Moreover, the estimated probabilities collapse to 0 or 1 (Figure 4, second column), a phenomenon that has also been reported in classification (Mukhoti et al., 2020). However, calibration is preserved during the first stages of training (Figure 4, third column). This is reminiscent of the early-learning phenomenon observed for classification from partially corrupted labels (Yao et al., 2020; Xia et al., 2020), where neural networks learn from the correct labels before eventually overfitting the false ones (Liu et al., 2020).\\n\\nThough early learning and memorization are typically observed when training prediction models based on deep neural networks, we argue that these observations represent a much more general phenomenon, intrinsic to the problem of probability estimation with finite data when the dimension is large. To substantiate this claim, we propose a simple analytical model, where data samples $x_i \\\\in \\\\mathbb{R}^d$ are drawn from a high dimensional normal distribution $x_i \\\\sim N(0, I_d)$.\\n\\nThe probability of each data point is determined by a generalized linear model $p_i(\\\\theta) = \\\\frac{1 + e^{-\\\\langle \\\\theta, x_i \\\\rangle}}{2}$, with true parameter $\\\\theta^*$. For $k \\\\geq 1$ we denote by $\\\\hat{p}_k$ the predictor obtained by running $k$ iterations of gradient descent on the cross-entropy loss with step size $\\\\eta$.\\n\\nWe prove the following.\\n\\n**Theorem 4.1** (Informal). There exists $\\\\kappa^* \\\\in (0, +\\\\infty)$ such that the following holds: if $p$ and $n$ are sufficiently large, the mean squared error of $\\\\hat{p}_k$ decreases monotonically during the first $k = O(1/\\\\eta)$ iterations of gradient descent, but if $p_n > \\\\kappa^*$, then as $k \\\\to \\\\infty$, $\\\\hat{p}_k$ collapses to a predictor that only predicts probabilities 0 and 1.\\n\\nA precise statement and proof can be found in Appendix A.\\n\\nTheorem 4.1 identifies a sharp threshold ($\\\\kappa^*$) at which the memorization phenomenon occurs and separates early learning stage and memorization. It indicates that even simple generalized linear models exhibit the early learning and memorization phenomena: in high dimensions, predictors obtained by cross-entropy minimization eventually memorize the data. This is likely to plague any overparametrized model, including neural networks. However, this phenomenon does not occur if gradient descent is stopped early. This is illustrated in Figures 8 and 7 in Appendix B, which demonstrate that empirically the linear model has this qualitative behavior. These observations motivate our proposed methodology.\"}"}
