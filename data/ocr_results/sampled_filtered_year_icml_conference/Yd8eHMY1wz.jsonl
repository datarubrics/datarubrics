{"id": "Yd8eHMY1wz", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nGerald Woo 1 2\\nChenghao Liu 1\\nAkshat Kumar 2\\nCaiming Xiong 1\\nSilvio Savarese 1\\nDoyen Sahoo 1\\n\\nAbstract\\nDeep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed MOIR-based Universal Time Series Forecasting Transformer (MOIRAI). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, MOIRAI achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts.\\n\\n1. Introduction\\nIn the era of foundation models (FMs) (Bommasani et al., 2021), the landscape of deep learning for time series forecasting is experiencing a revolution. In contrast to FMs capable of tackling a multitude of downstream tasks, the current deep forecasting paradigm, involving training a model on a single dataset with a fixed context and prediction length, appears increasingly antiquated, lacking the capacity to generalize or adapt to diverse scenarios or datasets. Given the unreasonable effectiveness of large pre-trained models in improving performance and data efficiency via transfer learning in modalities like vision and language (Dosovitskiy et al., 2020; Brown et al., 2020), we are starting to see a push to transition away from the existing paradigm, towards a universal forecasting paradigm (see Figure 1), where a single large pre-trained model is able to handle any time series forecasting problem. However, the road to building a universal time series forecasting model is mired with challenges. Unlike the modalities of vision and language which have the unified formats of images and text respectively, time series data is highly heterogeneous. Firstly, the frequency (e.g. minutely, hourly, daily sampling rates) of time series plays an important role in determining the patterns present in the time series. Cross-frequency learning has been shown to be a challenging task due to negative interference (Van Ness et al., 2023), with existing work simply avoiding this problem for multi-frequency datasets by learning one model per frequency (Oreshkin et al., 2020). Secondly, time series data are heterogeneous in terms of dimensionality, whereby multivariate time series can have different number of variates. Furthermore, each variate measures a semantically different quantity across datasets. While considering each\"}"}
{"id": "Yd8eHMY1wz", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Following Awasthi et al. (2022), we implement a continuous extension of the negative binomial distribution. A random variable \\\\( x \\\\) following such a distribution has p.d.f.:\\n\\n\\\\[\\np(x; r, p) \\\\propto \\\\Gamma(x + r) \\\\Gamma(x + 1) \\\\Gamma(r)(1 - p)^r p^x\\n\\\\]\\n\\ngiven parameters \\\\( r > 0 \\\\) and \\\\( p \\\\in [0, 1] \\\\), and \\\\( \\\\Gamma \\\\) is the gamma function. We predict both parameters, applying the softplus function for positivity, and sigmoid function to constrain to a probability.\\n\\nA random variable \\\\( x \\\\) following a normal distribution has p.d.f.:\\n\\n\\\\[\\np(x; \\\\mu, \\\\sigma) = \\\\frac{1}{\\\\sigma \\\\sqrt{2\\\\pi}} \\\\exp\\\\left(-\\\\frac{(x - \\\\mu)^2}{2\\\\sigma^2}\\\\right)\\n\\\\]\\n\\nwhere \\\\( \\\\mu \\\\in \\\\mathbb{R} \\\\), \\\\( \\\\sigma > 0 \\\\). For a low variance normal distribution, we only predict \\\\( \\\\mu \\\\), and fix \\\\( \\\\sigma \\\\) to a small number, in this case we fix \\\\( \\\\sigma = 1e^{-3} \\\\).\\n\\nB.3. Discussion on \u201cFlexible Distribution\u201d\\n\\nTable 1 categorizes various pre-trained forecasting models with the notion of a \u201cflexible distribution\u201d \u2013 this is largely a qualitative categorization rather than a quantitative one. As of writing, only 3 other models considered probabilistic forecasting \u2013 Lag-llama, TimeGPT, and LLMTime. The other models only considered point forecasts, and thus the concept of \u201cflexible distribution\u201d does not apply to them. The following are specific reasons on why we made the categorization for the 3 models which can handle probabilistic forecasting:\\n\\n- Lag-llama uses a Student-T distribution which is only able to model symmetric distributions. This is an inflexible distribution which is unable to model asymmetric distributions, as demonstrated in Figure 4 of our paper. They also raise this issue in their paper (Section 4.3), citing the use of more expressive distribution heads such as normalizing flows and copulas in future work.\\n\\n- TimeGPT uses conformal prediction to construct prediction intervals. We refer to a tweet from the creators, which claim: \u201cSome prediction intervals don\u2019t account for domain constraints. A few users highlighted intervals suggesting negative values for time series that only take positive values.\u201d Thus, we consider it to be inflexible.\\n\\n- LLMTime uses a categorical distribution. In their paper (paragraph titled \u201cLanguage models as flexible distributions\u201d in Section 3), they demonstrated that this approach is a flexible distribution which can approximate many different kinds of continuous distributions.\\n\\n---\\n\\n2 https://twitter.com/nixtlainc/status/1694466983927153131\"}"}
{"id": "Yd8eHMY1wz", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C. Probabilistic Forecasting\\n\\nC.1. Evaluation Metrics\\n\\nContinuous Ranked Probability Score\\n\\nThe CRPS (Gneiting & Raftery, 2007) is a probabilistic forecasting evaluation metric, given a predicted distribution with c.d.f. $F$ and ground truth $y$, it is defined as:\\n\\n$$\\n\\\\text{CRPS} = \\\\int_0^1 \\\\Lambda_\\\\alpha(F - 1(\\\\alpha), y) \\\\, d\\\\alpha\\n$$\\n\\nwhere $\\\\Lambda_\\\\alpha$ is the $\\\\alpha$-quantile loss, also known as the pinball loss at quantile level $\\\\alpha$.\\n\\nIn practice, the CRPS is intractable or computationally expensive to compute, and we also want to compute a normalized metric, thus we compute a normalized discrete approximation, the mean weighted sum quantile loss (Park et al., 2022), defined as the average of $K$ quantiles:\\n\\n$$\\n\\\\text{CRPS} \\\\approx \\\\frac{1}{K} \\\\sum_{k=1}^{K} w_{QL}[\\\\alpha_k]\\n$$\\n\\n$$\\n\\\\text{w}_{QL}[\\\\alpha] = \\\\frac{2}{P_t} \\\\Lambda_\\\\alpha(\\\\hat{q}_t(\\\\alpha), y_t) \\\\mathbb{I}_{y_t < \\\\hat{q}_t(\\\\alpha)} + \\\\frac{2}{P_t} \\\\Lambda_\\\\alpha(y_t - \\\\hat{q}_t(\\\\alpha)) \\\\mathbb{I}_{y_t > \\\\hat{q}_t(\\\\alpha)}\\n$$\\n\\nwhere $\\\\hat{q}_t(\\\\alpha)$ is the predicted $\\\\alpha$-quantile at time step $t$. We take $K=9$, $\\\\alpha_1=0.1$, $\\\\alpha_2=0.2$, ..., $\\\\alpha_9=0.9$ in practice.\\n\\nMean Scaled Interval Score\\n\\nThe MSIS is a metric to evaluate uncertainty around point forecasts, introduced in the M4 Competition (Makridakis et al., 2020). Given an upper bound prediction, $U_t$, and lower bound prediction $L_t$, the MSIS is defined as:\\n\\n$$\\n\\\\text{MSIS} = \\\\frac{1}{h} \\\\sum_{t=1}^{h} (U_t - L_t) + 2\\\\alpha (L_t - Y_t) \\\\mathbb{I}_{Y_t < L_t} + 2\\\\alpha (Y_t - U_t) \\\\mathbb{I}_{Y_t > U_t}\\n$$\\n\\n$$\\n1/n - m \\\\sum_{t=1}^{n} |Y_t - Y_t - m|\\n$$\\n\\nwhere $\\\\alpha = 0.05$ is the significance level for a 95% prediction interval, over a forecast horizon of length $h$, and $m$ is the seasonal factor.\\n\\nC.2. Evaluation Setup\\n\\nTable 18. Summary of datasets used in the out-of-distribution probabilistic forecasting evaluation setting.\\n\\n| Dataset       | Domain | Frequency | Prediction Length | Rolling Evaluations |\\n|---------------|--------|-----------|-------------------|--------------------|\\n| Electricity   | Energy | H         | 24                | 7                  |\\n| Solar         | Energy | H         | 24                | 7                  |\\n| Walmart      | Sales  | W         | 8                 | 4                  |\\n| Weather      | Climate| 10T       | 144               | 7                  |\\n| Istanbul Traffic | Transport | H     | 24                | 7                  |\\n| Turkey Power | Energy | H         | 24                | 7                  |\\n\\nWe perform evaluation in a non-overlapping rolling window fashion, i.e. stride is equal to prediction length. The test set is defined as the last $h \\\\times r$ time steps where $h$ is the prediction length of the forecast horizon, and $r$ is the number of rolling evaluation windows. We take the validation set to be the last forecast horizon before the test set, and the train set to be everything before that. From Table 18, our evaluation spans four domains, from minute-level to weekly frequencies. Prediction length and rolling evaluations are defined for each dataset based on frequency, making day ahead predictions for sub-hourly frequencies for seven days, and eight week ahead predictions for 32 weeks for weekly frequency.\\n\\n3. https://www.kaggle.com/datasets/leonardo00/istanbul-traffic-index\\n4. https://www.kaggle.com/datasets/dharanikra/electrical-power-demand-in-turkey\"}"}
{"id": "Yd8eHMY1wz", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 19. Hyperparameter search values for probabilistic forecasting evaluation baselines.\\n\\n| Model       | Hyperparameter Values |\\n|-------------|-----------------------|\\n| PatchTST    | \\\\{64, 128, 256\\\\}     |\\n| num_encoder layers | [2, 6]               |\\n| DeepAR      | \\\\{64, 128, 256\\\\}     |\\n| num_layers  | [2, 6]               |\\n| TFT         | \\\\{64, 128, 256\\\\}     |\\n| TiDE        | \\\\{64, 128, 256\\\\}     |\\n\\nFor the four deep learning baselines, DeepAR (Salinas et al., 2020), PatchTST (Nie et al., 2023), TiDE (Das et al., 2023a), and TFT (Lim et al., 2021), we perform hyperparameter tuning based on the values presented in Table 19, and also tune learning rate $[1e^{-6}, 1e^{-3}]$ in log scale, and the context length as $l = m \\\\times h$, where $m$ is tuned in the range $[2, 20]$, and $h$ is the prediction length. We perform random search through these values over 15 training runs, and report results on 5 independent training runs based on the best validation CRPS.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nD. Full Experimental Results\\n\\nD.1. In-distribution Forecasting: Monash Time Series Forecasting Benchmark\\n\\nFigure 7. Extended aggregate results of the Monash Time Series Forecasting Benchmark as per Figure 3. GPT3.5 is our reproduction of LLMTime based on the GPT3.5 API, whereas LLaMA2 is based on the results reported in Gruver et al. (2023).\\n\\nTable 20. Full results of Monash Time Series Forecasting Benchmark. MAE is reported.\\n\\nWe include the full breakdown of results for the Monash benchmark in Table 20, including two versions of LLMTime: GPT3.5 (our reproduction), and LLaMA2 (results from Gruver et al. (2023)). GPT3.5 is our reproduction by running their code on the full dataset, using GPT3.5-Turbo-Instruct since text-davinci-003 has been deprecated. LLaMA2 only has results for a subset of datasets in Table 20, thus in Figure 7, we present two aggregated results, one aggregated on the full Table 20, and one on the subset with results available for LLaMA2. As observed, MoIRAI retains the top rankings for with the base and large models in all settings.\\n\\nD.2. Out-of-distribution Forecasting: Probabilistic Forecasting\\n\\nTable 21 provides the full results of the probabilistic forecasting experiments with additional point forecasting metrics, including the symmetric mean absolute percentage error (sMAPE) (Hyndman, 2014), mean absolute scaled error (MASE) (Hyndman & Koehler, 2006), normalized deviation (ND), and normalized root mean squared error (NRMSE) (Yu et al., 2016).\\n\\nD.3. Out-of-distribution Forecasting: Long Sequence Forecasting\\n\\nTable 22 provides the full breakdown of results for the long sequence forecasting experiments, listing results for each prediction length.\\n\\n5 https://github.com/ngruver/llmtime\"}"}
{"id": "Yd8eHMY1wz", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 11. Datasets and key properties from the GluonTS library.\\n\\n| Dataset Domain | Frequency | # Time Series | # Targets | # Past Covariates | # Obs.     |\\n|----------------|-----------|---------------|-----------|------------------|-----------|\\n| Taxi Transport | 30T       | 67,984        | 1         | 0                | 54,999,060|\\n| Uber TLC Daily Transport | 262 | 1 | 0 | 47,087 |\\n| Uber TLC Hourly Transport | 262 | 1 | 0 | 1,129,444 |\\n| Wiki-Rolling Web Daily | 47,675 | 1 | 0 | 40,619,100 |\\n| M5 Sales Daily | 30,490 | 1 | 0 | 58,327,370 |\\n\\nThe GluonTS library (Alexandrov et al., 2020) provides many datasets popularly used in time series forecasting. We only include the datasets presented in Table 11 as we either hold out the other datasets, or are already included in the Monash repository.\\n\\nTable 12. Key properties of the LargeST Benchmark Dataset.\\n\\n| Dataset Domain | Frequency | # Time Series | # Targets | # Past Covariates | # Obs.     |\\n|----------------|-----------|---------------|-----------|------------------|-----------|\\n| LargeST Transport | 5T | 42,333 | 1 | 0 | 4,452,510,528 |\\n\\nLargeST (Liu et al., 2023a) (Table 12) collects the largest dataset from the California Department of Transportation Performance Measurement System (PeMS) (Chen et al., 2001) to date. The PeMS is a popular source of data for many traffic forecasting datasets such as PEMS03, PEMS04, PEMS07, PEMS08, and PEMS Bay, as well as the popular Traffic dataset from (Lai et al., 2018). Since we use such a large amount of data from the same source, we can no longer consider forecasting on any of these datasets as an out-of-distribution or zero-shot forecasting task anymore, and thus omit the Traffic dataset of the LSF benchmark from our evaluations.\\n\\nTable 13. Datasets and key properties from the LibCity library.\\n\\n| Dataset Domain | Frequency | # Time Series | # Targets | # Past Covariates | # Obs.     |\\n|----------------|-----------|---------------|-----------|------------------|-----------|\\n| PEMS03 Transport | 5T | 358 | 1 | 0 | 9,382,464 |\\n| PEMS04 Transport | 5T | 307 | 3 | 0 | 5,216,544 |\\n| PEMS07 Transport | 5T | 883 | 1 | 0 | 24,921,792 |\\n| PEMS08 Transport | 5T | 170 | 3 | 0 | 3,035,520 |\\n| PEMS Bay Transport | 5T | 325 | 1 | 0 | 16,937,700 |\\n| Los-Loop Transport | 5T | 207 | 1 | 0 | 7,094,304 |\\n| Loop Seattle Transport | 5T | 323 | 1 | 0 | 33,953,760 |\\n| SZ-Taxi Transport | 15T | 156 | 1 | 0 | 464,256 |\\n| Beijing Subway Transport | 30T | 276 | 2 | 11 | 248,400 |\\n| SHMetro Transport | 15T | 288 | 2 | 0 | 1,934,208 |\\n| HZMetro Transport | 15T | 80 | 2 | 0 | 146,000 |\\n| Rotterdam Transport | 2T | 208 | 1 | 0 | 4,813,536 |\\n| Q-Traffic Transport | 15T | 45,148 | 1 | 0 | 264,386,688 |\\n\\nLibCity (Wang et al., 2023a) (Table 13) provides a collection of urban spatio-temporal datasets. We drop the spatial aspect of these datasets and consider them as time series data.\\n\\nMonash The Monash Time Series Forecasting Repository (Godahewa et al., 2021) (Table 14) is a large collection of diverse time series datasets, the most popular source for building LTMs. We use Monash for in-distribution evaluation, and thus apart from the exceptions listed below, we only include the train region in LOTSA, by holding out the final forecast horizon as the test set. The final forecast horizon is defined for each dataset by (Godahewa et al., 2021). For the following datasets, we include their entirety in LOTSA without a held-out test set for the following reasons:\"}"}
{"id": "Yd8eHMY1wz", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset         | Domain   | Frequency | # Time Series | # Targets | # Past Covariates | Observations   |\\n|-----------------|----------|-----------|---------------|-----------|-------------------|----------------|\\n| London Smart Meters Energy | Energy   | 30T       | 5,520         | 1         | 0                 | 166,238,880    |\\n| Wind Farms Energy | Energy   | T         | 337           | 1         | 0                 | 172,165,370    |\\n| Wind Power Energy  | Energy   | 4S        | 1             | 1         | 0                 | 7,397,147      |\\n| Solar Power Energy  | Energy   | 4S        | 1             | 1         | 0                 | 7,397,222      |\\n| Oikolab Weather Climate | Climate | H         | 8             | 1         | 0                 | 800,456        |\\n| Elecdemand Energy   | Energy   | 30T       | 1             | 1         | 0                 | 17,520         |\\n| Covid Mobility Transport | Transport | D       | 362           | 1         | 0                 | 148,602        |\\n| Kaggle Web Traffic Weekly Web | Web | W | 145,063 | 1 | 0 | 16,537,182 |\\n| Extended Web Traffic Web | Web | D | 145,063 | 1 | 0 | 370,926,091 |\\n| M1 Yearly Econ/Fin | Y         | 106       | 1             | 0         | 0                 | 3,136          |\\n| M1 Quarterly Econ/Fin | Q       | 198       | 1             | 0         | 0                 | 9,854          |\\n| M1 Monthly Econ/Fin | M        | 617       | 1             | 0         | 0                 | 44,892         |\\n| M3 Yearly Econ/Fin | Y         | 645       | 1             | 0         | 0                 | 18,319         |\\n| M3 Quarterly Econ/Fin | Q       | 756       | 1             | 0         | 0                 | 37,004         |\\n| M3 Monthly Econ/Fin | M        | 1,428     | 1             | 0         | 0                 | 141,858        |\\n| M3 Other Econ/Fin | Q         | 174       | 1             | 0         | 0                 | 11,933         |\\n| M4 Yearly Econ/Fin | Y         | 22,739    | 1             | 0         | 0                 | 840,644        |\\n| M4 Quarterly Econ/Fin | Q       | 24,000    | 1             | 0         | 0                 | 2,214,108      |\\n| M4 Monthly Econ/Fin | M        | 48,000    | 1             | 0         | 0                 | 10,382,411     |\\n| M4 Weekly Econ/Fin | W         | 359       | 1             | 0         | 0                 | 366,912        |\\n| M4 Hourly Econ/Fin | H         | 414       | 1             | 0         | 0                 | 353,500        |\\n| M4 Daily Econ/Fin | D         | 4,227     | 1             | 0         | 0                 | 9,964,658      |\\n| NN5 Daily Econ/Fin | D         | 111       | 1             | 0         | 0                 | 81,585         |\\n| NN5 Weekly Econ/Fin | W       | 111       | 1             | 0         | 0                 | 11,655         |\\n| Tourism Yearly Econ/Fin | Y       | 419       | 1             | 0         | 0                 | 11,198         |\\n| Tourism Quarterly Econ/Fin | Q | 427 | 1 | 0 | 0 | 39,128 |\\n| Tourism Monthly Econ/Fin | M | 366 | 1 | 0 | 0 | 100,496 |\\n| CIF 2016 Econ/Fin | M         | 72        | 1             | 0         | 0                 | 6,334          |\\n| Traffic Weekly Transport | W | 862 | 1 | 0 | 0 | 82,752 |\\n| Traffic Hourly Transport | H | 862 | 1 | 0 | 0 | 14,978,112 |\\n| Australian Electricity Demand Energy | Energy | 30T | 5 | 1 | 0 | 1,153,584 |\\n| Rideshare Transport | H         | 2,304     | 1             | 0         | 0                 | 859,392        |\\n| Saugeen Nature | D         | 1         | 1             | 0         | 0                 | 23,711          |\\n| Sunspot Nature | D         | 1         | 1             | 0         | 0                 | 73,894          |\\n| Temperature Rain Nature | D | 32,072 | 1 | 0 | 0 | 22,290,040 |\\n| Vehicle Trips Transport | D | 329 | 1 | 0 | 0 | 32,512 |\\n| Weather Climate | D         | 3,010     | 1             | 0         | 0                 | 42,941,700     |\\n| Car Parts Sales | M         | 2,674     | 1             | 0         | 0                 | 104,286        |\\n| FRED MD Econ/Fin | M         | 107       | 1             | 0         | 0                 | 76,612          |\\n| Pedestrian Counts Transport | H | 66 | 1 | 0 | 0 | 3,130,762 |\\n| Hospital Healthcare | M         | 767       | 1             | 0         | 0                 | 55,224          |\\n| COVID Deaths Healthcare | D | 266 | 1 | 0 | 0 | 48,412 |\\n| KDD Cup 2018 Energy | H         | 270       | 1             | 0         | 0                 | 2,897,004      |\\n| Bitcoin Econ/Fin | D         | 18        | 1             | 0         | 0                 | 74,824          |\\n| US Births Healthcare | D | 1 | 1 | 0 | 0 | 7,275 |\"}"}
{"id": "Yd8eHMY1wz", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\n- London Smart Meters, Wind Farms, Wind Power, Solar Power, Oikolab Weather, Covid Mobility: Originally not included in the Monash benchmark.\\n- Extended Web Traffic, Kaggle Web Traffic Weekly: We include the extended version of Web Traffic which contains overlap with the original Web Traffic dataset.\\n- M1 Yearly, M1 Quarterly, M3 Yearly, M3 Quarterly, M4 Yearly, M4 Quarterly, Tourism Yearly: Some time series in these datasets are too short after train/test splits, thus we do not split them (setting a minimum of 16 time steps to achieve at least 2 patches).\\n\\nWe omit Electricity (Trindade, 2015) and Solar (Lai et al., 2018) datasets for out-of-distribution evaluation. Note that the \u201cWeather\u201d from Monash is a different dataset from that used in the out-of-distribution evaluations.\\n\\n**Table 15. Datasets and key properties from the ProEnFo library.**\\n\\n| Dataset       | Domain | Frequency | # Time Series | # Targets | # Past Covariates | # Obs.   |\\n|---------------|--------|-----------|---------------|-----------|-------------------|---------|\\n| Covid19 Energy | Energy | H         | 1             | 1         | 6                 | 31,912  |\\n| GEF12 Energy  | Energy | H         | 20            | 1         | 1                 | 788,280 |\\n| GEF14 Energy  | Energy | H         | 1             | 1         | 1                 | 17,520  |\\n| GEF17 Energy  | Energy | H         | 8             | 1         | 1                 | 140,352 |\\n| PDB Energy    | Energy | H         | 1             | 1         | 1                 | 17,520  |\\n| Spanish Energy| Energy | H         | 1             | 1         | 1                 | 35,064  |\\n| BDG-2 Hog     | Energy | H         | 24            | 1         | 5                 | 421,056 |\\n| BDG-2 Bull    | Energy | H         | 41            | 1         | 3                 | 719,304 |\\n| BDG-2 Cockatoo| Energy | H         | 1             | 1         | 5                 | 17,544  |\\n| ELF Energy    | Energy | H         | 1             | 1         | 0                 | 21,792  |\\n\\nProEnFo (Wang et al., 2023b) (Table 15) provides a range of datasets for load forecasting. Unlike Buildings-Bench, ProEnFo provides various covariates such as temperature, humidity, and wind speed. We again omit Electricity (Trindade, 2015) which is originally included in ProEnFo for out-of-distribution evaluations.\\n\\n**Table 16. Datasets and key properties from the SubseasonalClimateUSA library.**\\n\\n| Dataset       | Domain | Frequency | # Time Series | # Targets | # Past Covariates | # Obs.   |\\n|---------------|--------|-----------|---------------|-----------|-------------------|---------|\\n| Subseasonal Climate | D     | 862       | 4             | 0         | 1                 | 14,097,148 |\\n| Subseasonal Precipitation | Climate | D | 862 | 1 | 0 | 9,760,426 |\\n\\nSubseasonalClimateUSA (Mouatadid et al., 2023) (Table 16) provides climate time series data at a lower granularity (daily) for subseasonal level forecasting. We extract two datasets Subseasonal Precipitation which is the precipitation data from 1948 - 1978, and Subseasonal, which is precipitation and temperature data from 1979 - 2023.\\n\\n**Others**\\n\\nFinally, detailed in Table 17, we further collect datasets from miscellaneous sources not provided by a library or collection. These datasets require more extensive pre-processing since they are not provided by a library, and are raw data instead. We take a standard approach of filtering out time series which are either too short, or have too many missing values. For each time series, we consider all variates to be targets, unless otherwise specified by the creators of the dataset (e.g. KDD Cup 2022 is a competition dataset, for which only the \u201cPatv\u201d variate is defined to be the target, with 9 other covariates).\"}"}
{"id": "Yd8eHMY1wz", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 17. Datasets and key properties from other miscellaneous sources.\\n\\n| Dataset Source         | Domain          | Frequency | # Time Series | # Targets | # Past Covariates | Obs.       |\\n|------------------------|-----------------|-----------|---------------|-----------|-------------------|------------|\\n| KDD Cup 2022           | Energy          | 10T       | 134           | 1         | 9                 | 4,727,519  |\\n| GoDaddy Kaggle         | Econ/Fin        | M         | 3,135         | 2         | 0                 | 128,535    |\\n| Favorita Sales         | Sales           | D         | 111,840       | 1         | 0                 | 139,179,538|\\n| Favorita Transactions  | Sales           | D         | 54            | 1         | 0                 | 84,408     |\\n| Restaurant             | Sales           | D         | 216           | 1         | 0                 | 76,573     |\\n| Hierarchical Sales     | Sales           | D         | 118           | 1         | 0                 | 212,164    |\\n| China Air Quality      | Nature          | H         | 437           | 6         | 0                 | 5,739,234  |\\n| Beijing Air Quality    | Nature          | H         | 12            | 11        | 0                 | 420,768    |\\n| Residential Load Power | Energy          | T         | 271           | 3         | 0                 | 145,994,559|\\n| Residential PV Power   | Energy          | T         | 233           | 3         | 0                 | 125,338,950|\\n| CDC Fluview ILINet     | Healthcare      | W         | 75            | 5         | 0                 | 63,903     |\\n| CDC Fluview WHO NREVSS | Healthcare      | W         | 74            | 4         | 0                 | 41,760     |\\n| Project Tycho          | Healthcare      | W         | 1,258         | 1         | 0                 | 1,377,707  |\\n\\nB. M\\n\\nB.1. Multi Patch Size Projection Layers\\nEach multi patch size projection is a simple Linear layer, for input projections, mapping patch size to hidden state, and for output projections, mapping hidden state to distribution parameters. In practice, we pre-define the frequency to patch size mapping heuristically, selecting smaller patch sizes for low frequency data and larger patch sizes for high frequency data as follows:\\n\\n- Yearly, Quarterly: 8\\n- Monthly: 8, 16, 32\\n- Weekly, Daily: 16, 32\\n- Hourly: 32, 64\\n- Minute-level: 32, 64, 128\\n- Second-level: 64, 128\\n\\nNote that we only learn one Linear layer per patch size, and share them across frequencies if there is overlap. This means that we learn five input projection layers and five output projection layers.\\n\\nB.2. Mixture Distribution\\nAs described in Salinas et al. (2020), our model predicts the parameters of a probability distribution, in this case, a mixture distribution. We apply a softmax layer to the parameters associated to the mixture weights, constraining them to the probability simplex. The mixture components are as described.\\n\\nStudent's t-distribution\\nA random variable $x$ following the Student's t-distribution has p.d.f.:\\n\\n$$p(x; \\\\nu, \\\\mu, \\\\tau) = \\\\frac{1}{\\\\sqrt{\\\\pi \\\\nu \\\\tau}} \\\\frac{\\\\Gamma(\\\\frac{\\\\nu+1}{2})}{\\\\Gamma(\\\\frac{\\\\nu}{2})} \\\\left(1 + \\\\frac{1}{\\\\nu} \\\\left(\\\\frac{x-\\\\mu}{\\\\tau}\\\\right)^2\\\\right)^{-\\\\frac{\\\\nu+1}{2}}$$\\n\\nwith parameters $\\\\nu > 0$, $\\\\mu \\\\in \\\\mathbb{R}$, $\\\\tau > 0$, the degrees-of-freedom (df), location, and scale parameters respectively, and $\\\\Gamma$ is the gamma function. We predict the df, location, and scale parameters, and apply a softplus function for the positivity constraint. We further lower bound the df parameter to $2$, since variance is undefined otherwise.\\n\\nLog-normal distribution\\nA random variable $x$ which follows a log-normal distribution has p.d.f.:\\n\\n$$p(x; \\\\mu, \\\\sigma) = \\\\frac{1}{x\\\\sigma\\\\sqrt{2\\\\pi}} \\\\exp\\\\left(-\\\\frac{(\\\\ln x - \\\\mu)^2}{2\\\\sigma^2}\\\\right)$$\\n\\nwith parameters $\\\\mu \\\\in \\\\mathbb{R}$, $\\\\sigma > 0$. We predict both parameters, applying softplus function for positivity.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nEmami, P., Sahu, A., and Graf, P. Buildingsbench: A large-scale dataset of 900k buildings and benchmark for short-term load forecasting. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=c5rqd6PZn6.\\n\\nFeng, S., Miao, C., Zhang, Z., and Zhao, P. Latent diffusion transformer for probabilistic time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 11979\u201311987, 2024.\\n\\nGarza, A. and Mergenthaler-Canseco, M. Timegpt-1. arXiv preprint arXiv:2310.03589, 2023.\\n\\nGarza, F., Canseco, M. M., Chall\u00fa, C., and Olivares, K. G. StatsForecast: Lightning fast forecasting with statistical and econometric models. PyCon Salt Lake City, Utah, US 2022, 2022. URL https://github.com/Nixtla/statsforecast.\\n\\nGneiting, T. and Raftery, A. E. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359\u2013378, 2007.\\n\\nGodahewa, R. W., Bergmeir, C., Webb, G. I., Hyndman, R., and Montero-Manso, P. Monash time series forecasting archive. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=wEc1mgAjU-.\\n\\nGruver, N., Finzi, M. A., Qiu, S., and Wilson, A. G. Large language models are zero-shot time series forecasters. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=md68e8iZK1.\\n\\nHenry, A., Dachapally, P. R., Pawar, S. S., and Chen, Y. Query-key normalization for transformers. In Cohn, T., He, Y., and Liu, Y. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4246\u20134253, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.379. URL https://aclanthology.org/2020.findings-emnlp.379.\\n\\nHyndman, R. J. Errors on percentage errors, 4 2014. URL https://robjhyndman.com/hyndsight/smape/.\\n\\nHyndman, R. J. and Athanasopoulos, G. Forecasting: principles and practice. OTexts, 2018.\\n\\nHyndman, R. J. and Koehler, A. B. Another look at measures of forecast accuracy. International journal of forecasting, 22(4):679\u2013688, 2006.\\n\\nJin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023.\\n\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\\n\\nKim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J. Reversible instance normalization for accurate time-series forecasting against distribution shift. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=cGDAkQo1C0p.\\n\\nLai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pp. 95\u2013104, 2018.\\n\\nLim, B., Ar\u0131k, S. \u00a8O., Loeff, N., and Pfister, T. Temporal fusion transformers for interpretable multi-horizon time series forecasting. International Journal of Forecasting, 37(4):1748\u20131764, 2021.\\n\\nLiu, M., Zeng, A., Chen, M., Xu, Z., Lai, Q., Ma, L., and Xu, Q. Scinet: Time series modeling and forecasting with sample convolution and interaction. Advances in Neural Information Processing Systems, 35:5816\u20135828, 2022.\\n\\nLiu, X., Xia, Y., Liang, Y., Hu, J., Wang, Y., Bai, L., Huang, C., Liu, Z., Hooi, B., and Zimmermann, R. Largest: A benchmark dataset for large-scale traffic forecasting. arXiv preprint arXiv:2306.08259, 2023a.\\n\\nLiu, X., Hu, J., Li, Y., Diao, S., Liang, Y., Hooi, B., and Zimmermann, R. Unitime: A language-empowered unified model for cross-domain time series forecasting. In Proceedings of the ACM Web Conference 2024, 2024.\\n\\nLiu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625, 2023b.\\n\\nMa, Q., Liu, Z., Zheng, Z., Huang, Z., Zhu, S., Yu, Z., and Kwok, J. T. A survey on time-series pre-trained models. arXiv preprint arXiv:2305.10716, 2023.\\n\\nMakridakis, S., Spiliotis, E., and Assimakopoulos, V. The m4 competition: 100,000 time series and 61 forecasting methods. International Journal of Forecasting, 36(1):54\u201374, 2020.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "Yd8eHMY1wz", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nArXiv preprint arXiv:2310.05063, 2023.\\n\\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:22419\u201322430, 2021.\\n\\nWu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M. Timesnet: Temporal 2d-variation modeling for general time series analysis. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=ju_Uqw384Oq.\\n\\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 10524\u201310533. PMLR, 2020.\\n\\nYang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W., and Gao, J. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. ArXiv preprint arXiv:2203.03466, 2022a.\\n\\nYang, J., Gupta, A., Upadhyay, S., He, L., Goel, R., and Paul, S. TableFormer: Robust transformer modeling for table-text encoding. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 528\u2013537, Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.40. URL https://aclanthology.org/2022.acl-long.40.\\n\\nYu, H.-F., Rao, N., and Dhillon, I. S. Temporal regularized matrix factorization for high-dimensional time series prediction. Advances in neural information processing systems, 29, 2016.\\n\\nYue, Z., Wang, Y., Duan, J., Yang, T., Huang, C., Tong, Y., and Xu, B. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 8980\u20138987, 2022.\\n\\nZeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 11121\u201311128, 2023.\\n\\nZerveas, G., Jayaraman, S., Patel, D., Bhamidipaty, A., and Eickhoff, C. A transformer-based framework for multivariate time series representation learning. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining, pp. 2114\u20132124, 2021.\\n\\nZhang, B. and Sennrich, R. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nZhang, K., Wen, Q., Zhang, C., Cai, R., Jin, M., Liu, Y., Zhang, J., Liang, Y., Pang, G., Song, D., et al. Self-supervised learning for time series analysis: Taxonomy, progress, and prospects. ArXiv preprint arXiv:2306.10125, 2023.\\n\\nZhang, Y. and Yan, J. Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=vSVLM2j9eie.\\n\\nZheng, Y., Yi, X., Li, M., Li, R., Shan, Z., Chang, E., and Li, T. Forecasting fine-grained air quality based on big data. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pp. 2267\u20132276, 2015.\\n\\nZhou, J., Lu, X., Xiao, Y., Su, J., Lyu, J., Ma, Y., and Dou, D. Sdwpf: A dataset for spatial dynamic wind power forecasting challenge at kdd cup 2022. ArXiv preprint arXiv:2208.04360, 2022a.\\n\\nZhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R. FEDformer: Frequency enhanced decomposed transformer for long-term series forecasting. In Proc. 39th International Conference on Machine Learning (ICML 2022), 2022b.\\n\\nZhou, T., Niu, P., Wang, X., Sun, L., and Jin, R. One fits all: Power general time series analysis by pretrained LM. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=gMS6FVZvmF.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LOTSA is a collection of time series datasets curated for pre-training of LTMs. In the following, we describe its constituent datasets and their respective sources, listing any pre-processing and data splitting performed. We further details on the key properties of these datasets, providing the domain, frequency, number of time series, number of target variates, number of past covariates (covariates whose values in the forecast horizon are unknown), and total number of observations in the dataset (defined as $P \\\\prod_{i=1}^{N} T_i$ for a dataset with $N$ time series). Of note, if we consider number of observations to include the number of variates, i.e. $P \\\\prod_{i=1}^{N} T_i \\\\times d_y i$, LOTSA would have 231,082,956,489 ($231 \\\\text{B}$) total observations.\\n\\n### Table 8. Datasets and key properties from BuildingsBench.\\n\\n| Dataset  | Domain     | Frequency | # Time Series | # Targets | # Past Covariates | # Obs.  |\\n|----------|------------|-----------|---------------|-----------|------------------|--------|\\n| BDG-2 Panther Energy H | 105       | 1         | 0             | 919,800   |                  |        |\\n| BDG-2 Fox Energy H    | 135       | 1         | 0             | 2,324,568 |                  |        |\\n| BDG-2 Rat Energy H    | 280       | 1         | 0             | 4,728,288 |                  |        |\\n| BDG-2 Bear Energy H   | 91        | 1         | 0             | 1,482,312 |                  |        |\\n| Low Carbon London Energy H | 713    | 1         | 0             | 9,543,348 |                  |        |\\n| SMART Energy H        | 5         | 1         | 0             | 95,709    |                  |        |\\n| IDEAL Energy H        | 219       | 1         | 0             | 1,265,672 |                  |        |\\n| Sceaux Energy H       | 1         | 1         | 0             | 34,223    |                  |        |\\n| Borealis Energy H     | 15        | 1         | 0             | 83,269    |                  |        |\\n| Buildings900K Energy H | 1,792,328 | 1         | 0             | 15,702,590,000 |          |        |\\n\\nBuildingsBench (Emami et al., 2023) (Table 8) provides a collection of datasets for residential and commercial building energy consumption. The BDG-2 datasets, Low Carbon London, SMART, IDEAL, Sceaux, and Borealis are real building energy consumption datasets from various sources. The Electricity dataset (Trindade, 2015) is also included in BuildingsBench but we omit it from LOTSA and use it for out-of-distribution evaluation instead. They further release the Buildings-900K dataset a large-scale dataset of 900K simulated buildings. Emami et al. (2023) introduce a pre-train/test split based on Public Use Microdata Area, but we use include both splits in LOTSA for pre-training. No special pre-processing was applied to these datasets. More information regarding these datasets can be found in Emami et al. (2023).\\n\\n### Table 9. Datasets and key properties from ClimateLearn.\\n\\n| Dataset  | Domain     | Frequency | # Time Series | # Targets | # Past Covariates | # Obs.  |\\n|----------|------------|-----------|---------------|-----------|------------------|--------|\\n| CMIP6 Climate | 6H        | 1,351,680 | 53            | 1,973,453,000 |                  |        |\\n| ERA5 Climate | H        | 245,760   | 45            | 2,146,959,000 |                  |        |\\n\\nClimateLearn (Nguyen et al., 2023) (Table 9) includes the ERA5 and CMIP6 datasets from the ClimateLearn library. The ERA5 and CMIP6 datasets provide time series of various climate related variables such as temperature, and humidity and various pressure levels, based on a grid structure. We use the $2.8125^\\\\circ$ resolution which contains time series in a $64 \\\\times 128$ grid.\\n\\n### Table 10. Datasets and key properties from CloudOps TSF.\\n\\n| Dataset  | Domain     | Frequency | # Time Series | # Targets | # Past Covariates | # Obs.  |\\n|----------|------------|-----------|---------------|-----------|------------------|--------|\\n| Azure VM Traces 2017 CloudOps | 5T        | 159,472   | 1             | 2         | 885,522,908      |        |\\n| Borg Cluster Data 2011 CloudOps | 5T        | 143,386   | 2             | 5         | 537,552,854      |        |\\n| Alibaba Cluster Trace 2018 CloudOps | 5T        | 58,409    | 2             | 6         | 95,192,530       |        |\\n\\nWoo et al. (2023) introduces three large-scale CloudOps time series datasets (Table 10) measuring various variables such as CPU and memory utilization. We follow their pre-train/test split and only include the pre-train time series in LOTSA, holding out the test set.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nTable 1. Comparison between pre-trained forecasting models. Further discussion on the notion of a \\\"flexible distribution\\\" can be found in Appendix B.3.\\n\\n|                          | Any-variate | Probabilistic | Forecasting | Flexible Distribution | Pre-training Data (Size) | Open-source |\\n|--------------------------|-------------|---------------|-------------|-----------------------|--------------------------|-------------|\\n| MOIRAI                   | \u2713           | \u2713            | \u2713           | \u2713                     | \u2713 \u2713 \u2713                    |             |\\n| LOTSA (>27B)             | \u2713           |              |             |                       | \u2713 \u2713 \u2713                    |             |\\n| TimeGPT-1                | \u2713           | \u2713            | \u2717           |                       | Unknown (100B)           | \u2717           |\\n| ForecastPFN              | \u2717           | \u2717            | \u2717           |                       | Synthetic Data (60M)     | \u2713           |\\n| Lag-Llama                | \u2717           | \u2713            | \u2717           |                       | Monash (<1B)             | \u2713           |\\n| TimesFM                  | \u2717           | \u2717            | \u2717           |                       | Wiki + Trends + Others (>100B) | \u2713           |\\n| TTM                      | \u2717           | \u2717            | \u2717           |                       | Monash (<1B)             | \u2713           |\\n| LLMTime                  | \u2717           | \u2713            | \u2713 \u2713 \u2713       |                       | Web-scale Text          |             |\\n\\nAny-variate of a multivariate time series independently (Nie et al., 2023; Ekambaram et al., 2023) can sidestep this problem, we expect a universal model to be sufficiently flexible to consider multivariate interactions and take into account exogenous covariates. Thirdly, probabilistic forecasting is a critical feature often required by practitioners, yet, different datasets have differing support and distributional properties \u2013 for example, using a symmetric distribution (e.g. Normal, Student-T) as the predictive distribution is not suitable for positive time series \u2013 making standard approaches of pre-defining a simple parametric distribution (Salinas et al., 2020) to be insufficiently flexible to capture a wide variety of datasets. Lastly, a large pre-trained model capable of universal forecasting requires a large-scale dataset from diverse domains. Existing time series datasets are insufficiently large to support the training of such models.\\n\\nStarting from a masked encoder architecture which has been shown to be a strong candidate architecture for scaling up pre-trained time series forecasting models (Woo et al., 2023), we alleviate the above issues by introducing novel modifications which allows the architecture to handle the heterogeneity of arbitrary time series data. Firstly, we propose to learn multiple input and output projection layers to handle the differing patterns from time series of varying frequencies. Using patch-based projections with larger patch sizes for high-frequency data and vice versa, projection layers are specialized to learn the patterns of that frequency. Secondly, we address the problem of varying dimensionality with our proposed Any-variate Attention, which simultaneously considers both time and variate axes as a single sequence, leveraging Rotary Position Embeddings (RoPE) (Su et al., 2024), and learned binary attention biases (Yang et al., 2022b) to encode time and variate axes respectively. Importantly, Any-variate Attention allows the model to take as input arbitrary number of variates. Thirdly, we overcome the issue of requiring flexible predictive distributions with a mixture of parametric distributions. Furthermore, optimizing the negative log-likelihood of a flexible distribution has the added benefit of being competitive with target metric optimization (Awasthi et al., 2022), a powerful feature for pre-training universal forecasters, given that it can be evaluated with any target metric subsequently.\\n\\nTo power the training of our Large Time Series Model (LTM), we introduce the Large-scale Open Time Series Archive (LOTSA), the largest collection of open time series datasets with 27B observations across nine domains. We optimize the negative log-likelihood of the mixture distribution, and randomly sample context and prediction lengths during training, allowing for flexible downstream usage of the pre-trained model. We train our proposed method, MOIRAI, in three sizes \u2013 MOIRAI Small, MOIRAI Base, and MOIRAI Large, with 14m, 91m, and 311m parameters respectively. We perform experimental evaluations on both in and out-of-distribution settings, and show that MOIRAI consistently achieves competitive or superior performance compared to state-of-the-art full-shot baselines. Our contributions are summarized as follows:\\n\\n1. We introduce a novel Transformer architecture to support the requirements of universal forecasting. Crucially, the components we propose extend beyond masked encoders and are versatile, applicable to a broad range of Transformer variants.\\n\\n2. We introduce LOTSA, a new large-scale collection of open time series datasets to empower pre-training of LTMs. LOTSA, the model weights, and our library for unified training of universal time series models, UNI2TS, will be fully open sourced.\\n\\n3. Trained on LOTSA data, MOIRAI achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models.\\n\\n2. Related Work\\n\\nPre-training for Zero-shot Forecasting\\n\\nTable 1 provides a summary of the key differences between recent pre-trained models with zero-shot forecasting capabilities, which is a recently emerging field. TimeGPT-1 (Garza & Mergenthaler-Canseco, 2023) first presented a closed-source model, offering zero-shot forecasting capabilities as well as supporting fine-tuning through an API, currently only available to their beta users. ForecastPFN (Dooley et al., 2023) proposes to pre-train on synthetic time series, which can be subsequently be leveraged as a zero-shot forecaster, albeit specialized for data or time limited settings. Lag-llama (Rasul et al., 2023) works towards a foundation model for time series forecasting, leveraging the LLaMA (Touvron et al., 2023) architecture design with lagged time series features, and also presents neural scaling laws for time series forecasting. TimesFM (Das et al., 2023b) is a patch-based decoder-only foundation model for time series forecasting, introducing a\\n\\n1 In ancient Greek religion and mythology, the Moirai, often known in English as the Fates, were the personifications of destiny. (Wikipedia contributors, 2024)\"}"}
{"id": "Yd8eHMY1wz", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nThey collected a massive amount of data from Google Trends and Wiki pageviews to pre-train their model in combination with open-data. Tiny Time Mixers (TTMs) (Ekambaram et al., 2024) is a concurrent work leveraging lightweight mixer-style architecture. They perform data augmentation by downsampling high-frequency time series, and support multivariate downstream tasks by fine-tuning an exogenous mixer.\\n\\nLeverage Large Language Models (LLMs), pre-trained on web-scale text data, have been leveraged for zero-shot forecasting. Specifically, LLMTime (Gruver et al., 2023) treats time series as strings, applying careful pre-processing based on the specific LLMs' tokenizer, showing that LLMs have the inherent capability to perform zero-shot forecasting.\\n\\nPre-train + Fine-tune for Time Series Forecasting\\n\\nPre-training with subsequent fine-tuning on downstream forecasting tasks has predated the recent zero-shot forecasting efforts. Denoising autoencoders (Zerveas et al., 2021) and contrastive learning (Yue et al., 2022; Woo et al., 2022) have been shown to be effective pretext tasks for time series forecasting, but have largely been applied to the existing paradigm of pre-training and fine-tuning on the same dataset, without exploring their generalization capabilities. More recently, Dong et al. (2023) explored combining both reconstruction and contrastive based pre-training approaches, and performed initial explorations into cross-dataset transfer. The topic has been well explored, and we refer readers to more comprehensive surveys (Zhang et al., 2023; Ma et al., 2023). \\\"Reprogramming\\\" is a recent direction which involves fine-tuning the model weights of an LLM which has been pre-trained on text data, for downstream tasks for other modalities. Zhou et al. (2023); Jin et al. (2023) introduce modules and fine-tuning methods to adapt LLMs for time series tasks including forecasting. Liu et al. (2024) has explored leveraging pre-trained LLMs on the cross-dataset setting.\\n\\n3. Method\\n\\nProblem Formulation\\n\\nConsider a dataset of $N$ time series $D = \\\\{ (Y_i, Z_i) \\\\}_{i=1}^N$, where $Y_i = (y_{i1}, y_{i2}, \\\\ldots, y_{iT_i}) \\\\in \\\\mathbb{R}^{d_y} \\n\\\\times T_i$ is a target time series of $d_y$ variates and $T_i$ time steps. Each time series is associated with a set of covariates $Z_i = (z_{i1}, z_{i2}, \\\\ldots, z_{iT_i}) \\\\in \\\\mathbb{R}^{d_z}$. The goal is to forecast the predictive distribution $p(Y_{t: t+h}|D)$ by predicting distribution parameters $\\\\hat{\\\\phi}$ via a learned model $f_{\\\\theta}: (Y_{t-l:t}, Z_{t-l:t+h}) \\\\rightarrow \\\\hat{\\\\phi}$ which maximizes the log-likelihood:\\n\\n$$\\\\max_{\\\\theta} \\\\mathbb{E}_{(Y, Z) \\\\sim p(D)}(t, l, h) \\\\sim p(T|D) \\\\log p(Y_{t:h}|\\\\hat{\\\\phi})$$\\n\\ns.t. $\\\\hat{\\\\phi} = f_{\\\\theta}(Y_{t-l:t}, Z_{t-l:t+h})$, (1)\\n\\nwhere $p(D)$ is the data distribution which samples for a time series, $(Y, Z)$, and $p(T|D)$ is the task distribution which defines the lookback window, $Y_{t-l:t} = (y_{t-l}, \\\\ldots, y_{t})$ with context length $l$ and forecast horizon, $Y_{t:h} = (y_{t}, \\\\ldots, y_{t+h-1})$ with prediction length $h$.\\n\\n3.1. Architecture\\n\\nIllustrated in Figure 2, M\\\\textsuperscript{OIRAI} follows a (non-overlapping) patch-based approach to modeling time series with a masked encoder architecture. One of our proposed modifications to extend the architecture to the any-variate setting is to \\\"flatten\\\" multivariate time series, considering all variates as a single sequence. Patches are subsequently projected into vector representations via a multi-patch size input projection layer. The $[\\\\text{mask}]$ signifies a learnable embedding which replaces patches falling within the forecast horizon. The output tokens are then decoded via the multi-patch size output projection into the parameters of the mixture distribution. While not visualized, (non-learnable) instance normalization (Kim et al., 2022) is applied to inputs/outputs, aligning with the current standard practice for deep forecasting models. The core Transformer module is an encoder-only Transformer architecture, leveraging various improvements as proposed by recent state-of-the-art LLM architectures. We use pre-normalization (Xiong et al., 2020) and replace all LayerNorms with RMSNorm (Zhang & Sennrich, 2019), and also apply query-key normalization (Henry et al., 2020). The non-linearity in FFN layers are replaced with SwiGLU (Shazeer, 2020), adjusting the hidden dimension to have equal number of parameters as the original FFN layer. We omit biases in all layers of the Transformer module.\\n\\n3.1.1. Multi-Patch Size Projection Layers\\n\\nIn the context of universal forecasting, a single model should possess the capability to handle time series spanning a wide range of frequencies. Existing patch-based architectures rely on a single patch size hyperparameter, a legacy feature from the prevailing one-model-per-dataset paradigm. Instead, we aim for a more flexible strategy: opting for a larger patch size to handle high-frequency data, thereby lowering the burden of the quadratic computation cost of attention while maintaining a long context length. Simultaneously, we advocate for a smaller patch size for low-frequency data to transfer computation to the Transformer layers, rather than relying solely on simple linear embedding layers. To implement this approach, we propose learning multiple input and output embedding layers, each associated with varying patch sizes. The selection of the appropriate patch size for a given time series frequency relies on pre-defined settings (see Appendix B.1). Note that we only learn one set of projection weights per patch size, which is shared amongst frequencies if there is an overlap based on the settings.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nVariate 0\\nVariate 1\\nVariate 2\\n\\nPatch Size 8\\nPatch Size 16\\nPatch Size 32\\nPatch Size 64\\nPatch Size 128\\n\\nMulti Patch Size\\n\\nMulti Patch Size\\n\\nInput Projection\\n\\nOutput Projection\\n\\nTransformer (Full Self-Attention)\\n\\nMixture Distribution\\n\\nFigure 2. Overall architecture of MOIRAI. Visualized is a 3-variate time series, where variates 0 and 1 are target variables (i.e. to be forecasted, and variate 2 is a dynamic covariate (values in forecast horizon known). Based on a patch size of 64, each variate is patchified into 3 tokens. The patch embeddings along with sequence and variate id are fed into the Transformer. The shaded patches represent the forecast horizon to be forecasted, whose corresponding output representations are mapped into the mixture distribution parameters.\\n\\n3.1.2. Any-variate Attention\\n\\nUniversal forecasters must be equipped to handle arbitrary multivariate time series. Existing time series Transformers often rely on an independent variate assumption or are limited to a single dimensionality due to embedding layers mapping $\\\\mathbb{R}^d \\\\rightarrow \\\\mathbb{R}^{d_h}$, where $\\\\mathbb{R}^{d_h}$ is the hidden dimension.\\n\\nWe overcome this limitation as shown in Figure 2, by flattening a multivariate time series to consider all variates as a single sequence. This introduces a new requirement of having variate encodings to enable the model to disambiguate between different variates in the sequence. Furthermore, we need to ensure that permutation equivariance w.r.t. variate ordering, and permutation invariance w.r.t. variate indices are respected. Conventional approaches like sinusoidal or learned embeddings do not meet these requirements, and are unable to handle an arbitrary number of variates. To address this, we propose Any-variate Attention, leveraging binary attention biases to encode variate indices.\\n\\nDropping layer and attention head indices, and scaling factor for brevity, the attention score between the $(i, m)$-th query where $i$ represents the time index and $m$ represents the variate index, and the $(j, n)$-th key, $A_{ij,mn} \\\\in \\\\mathbb{R}$, is given by:\\n\\n$$E_{ij,mn} = (W_{Qx}i,m)^T R_{i-j}(W_{Kx}j,n) + u(1) \\\\cdot 1\\\\{m = n\\\\} + u(2) \\\\cdot 1\\\\{m \\\\neq n\\\\},$$\\n\\n$$A_{ij,mn} = \\\\exp\\\\{E_{ij,mn}\\\\} P_{k,o} \\\\exp\\\\{E_{ik,mo}\\\\},$$\\n\\nwhere $W_{Qx}i,m, W_{Kx}j,n \\\\in \\\\mathbb{R}^{d_h}$ are the respective query and key vectors, $R_{i-j} \\\\in \\\\mathbb{R}^{d_h \\\\times d_h}$ is the rotary matrix (Su et al., 2024), $u(1), u(2) \\\\in \\\\mathbb{R}$ are learnable scalars for each head in each layer, and $1\\\\{\\\\text{cond}\\\\}$ is the indicator function. The binary attention bias component allows for disambiguation between variates via attention scores, fulfills the criteria of permutation equivariance/invariance w.r.t. variate ordering/indices, and can extend to arbitrary number of variates.\\n\\n3.1.3. Mixture Distribution\\n\\nTo achieve the goal of having a flexible distribution, yet ensuring that operations of sampling and evaluating the loss function remains simple, we propose to use a mixture of parametric distributions. A mixture distribution of $c$ components has p.d.f.:\\n\\n$$p(Y_{t:t+h}|\\\\hat{\\\\phi}) = \\\\sum_{i=1}^c w_i p_i(Y_{t:t+h}|\\\\hat{\\\\phi}_i),$$\\n\\nwhere $\\\\hat{\\\\phi} = \\\\{w_1, \\\\hat{\\\\phi}_1, \\\\ldots, w_c, \\\\hat{\\\\phi}_c\\\\}$, and $p_i$ is the $i$-th component's p.d.f. While the choice of mixture components is flexible and implementing any combination of parametric distributions is straightforward, we specifically propose to use the following mixture components: i) a Student's $t$-distribution which has shown to be a robust option for general time series, ii) a negative binomial distribution for positive count data, iii) a log-normal distribution to model right-skewed data commonly across economic and natural phenomena, and iv) a low variance normal distribution for high confidence predictions. Further details can be found in Appendix B.2.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nTable 2. Key statistics of LOTSA by domain.\\n\\n| Domain       | # Datasets | # Obs.            | %       |\\n|--------------|------------|-------------------|---------|\\n| Energy       | 30         | 16,358,600,896    | 59.17%  |\\n| Transport    | 23         | 4,900,453,419     | 17.73%  |\\n| Climate      | 6          | 4,188,011,890     | 15.15%  |\\n| CloudOps     | 3          | 1,518,268,292     | 5.49%   |\\n| Web          | 3          | 428,082,373       | 1.55%   |\\n| Sales        | 6          | 197,984,339       | 0.72%   |\\n| Nature       | 5          | 28,547,647        | 0.09%   |\\n| Econ/Fin     | 23         | 24,919,596        | 0.10%   |\\n| Healthcare   | 6          | 1,594,281         | 0.01%   |\\n\\nTable 3. Key statistics of LOTSA by frequency.\\n\\n| Frequency     | # Datasets | # Obs.            | %       |\\n|---------------|------------|-------------------|---------|\\n| Yearly        | 4          | 873,297           | 0.003%  |\\n| Quarterly     | 5          | 2,312,027         | 0.008%  |\\n| Monthly       | 10         | 11,040,648        | 0.040%  |\\n| Weekly        | 7          | 18,481,871        | 0.067%  |\\n| Daily         | 21         | 709,017,118       | 2.565%  |\\n| (Multi) Hourly| 31         | 19,875,993,973    | 71.893% |\\n| (Multi) Minute| 25         | 7,013,949,430     | 25.370% |\\n| (Multi) Second| 2          | 14,794,369        | 0.054%  |\\n\\n3.2. Unified Training\\n\\n3.2.1. LOTSA Data\\n\\nExisting work has predominantly relied on three primary sources of data \u2013 the Monash Time Series Forecasting Archive (Godahewa et al., 2021), datasets provided by the GluonTS library (Alexandrov et al., 2020), and datasets from the popular long sequence forecasting benchmark (Lai et al., 2018; Wu et al., 2021). While Monash and GluonTS comprise of datasets from diverse domains, they are constrained in size, with approximately $1B$ observations combined. In comparison, LLMs are trained on trillions of tokens. Das et al. (2023b) builds a private dataset mainly based on Google Trends and Wiki pageviews, but lacks diversity in terms of the domains these time series originate from. The effectiveness of FMs heavily stem from large-scale pre-training data. Given that existing data sources fall short of supporting such a paradigm, attempting to train an LTM on them may result in misleading conclusions. Thus, we tackle this issue head-on by building a large-scale archive of open time series datasets by collating publicly available sources of time series datasets. This effort aims to cover a broad spectrum of domains, consolidating datasets from diverse sources with varying formats. We design a unified storage format using Arrow (Richardson et al., 2023) which is ready for deep learning pipelines. The resulting collection, LOTSA, spans nine domains, with a total of $27,646,733$ observations, with key statistics in Tables 2 and 3, and in-depth details in Appendix A.\\n\\n3.2.2. PRE-TRAINING\\n\\nAs introduced in Equation (1), our pre-training task is formulated to optimize the mixture distribution log-likelihood. The design of both the data distribution and task distribution are two critical aspects of the pre-training pipeline. This design imparts versatile capabilities to our LTM, enabling it to adapt to a range of downstream tasks. This flexibility stands in contrast to the prevailing deep forecasting paradigm, where models are typically specialized for specific datasets and settings.\\n\\nData Distribution\\n\\nThe data distribution, \\\\((Y, Z) \\\\sim p(D)\\\\), defines how time series are sampled from the dataset. Trained on LOTSA, which is a dataset of datasets, we introduce the notion of sub-datasets, by decomposing the data distribution into a sub-dataset distribution, and a time series distribution conditioned on a sub-dataset, \\\\(p(D) = p(Y, Z | D) p(D)\\\\). Thus, we first sample a sub-dataset from \\\\(p(D)\\\\), and given that sub-dataset, we sample a time series. For \\\\(K\\\\) sub-datasets, where \\\\(D_k\\\\) represents the set of indices of time series belonging to sub-dataset \\\\(k\\\\), the structure of \\\\(p(Y(i), Z(i) | D_k) = T_i \\\\ast 1\\\\{i \\\\in D_k\\\\}\\\\) proportionate to the number of observations, is straightforward. However, due to data imbalance across domains and frequency, we avoid sampling sub-datasets proportionately, and instead cap the contribution of each sub-dataset at \\\\(\\\\epsilon = 0.001\\\\), before re-normalizing: \\\\(p(D_k) = \\\\omega_k \\\\sum_{i=1}^{K} \\\\omega_i\\\\), where \\\\(\\\\omega_k = \\\\min(|D_k|/|D|, \\\\epsilon)\\\\), and \\\\(|D_k| = \\\\sum_{i \\\\in D_k} T_i\\\\).\\n\\nTask Distribution\\n\\nDifferent from the existing deep forecasting paradigm, we aim to train a model with forecasting capabilities over varying context and prediction lengths. Rather than defining a fixed context and prediction length, we sample from a task distribution, \\\\((t, l, h) \\\\sim p(T | D)\\\\) which defines the lookback window and forecasting horizon, given a time series. In practice, rather than sampling \\\\(t, l, h\\\\), given a time series, we crop a uniformly sampled window, whose length is uniformly sampled from a range. This range is defined by a minimum sequence length per variate of \\\\(2\\\\), and a total maximum sequence length of \\\\(512\\\\). The window is then split into lookback and horizon segments, where the prediction length is uniformly sampled as a proportion (within the range \\\\([0, 0.5]\\\\)) of the window. We further augment training by i) uniformly subsampling multivariate time series in the variate dimension, and ii) constructing multivariate time series from univariate time series, by randomly concatenating them. The number of variates is sampled from a beta-binomial distribution with parameters \\\\(n = 128, a = 2, b = 5\\\\) which supports a maximum of \\\\(128\\\\) variates, with mean \\\\(\\\\approx 37\\\\) for efficiency.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nTable 4. Details of MOIRAI model sizes.\\n\\n| Layers | $d$ | $df$ | Heads | $d_{kv}$ | Params |\\n|--------|-----|------|-------|---------|--------|\\n| Small  | 6   | 384  | 1536  | 6       | 64     |\\n| MOIRAI |     |       |       |         | 14m    |\\n| Base   | 12  | 768  | 3072  | 12      | 64     |\\n| MOIRAI |     |       |       |         | 91m    |\\n| Large  | 24  | 1024 | 4096  | 16      | 64     |\\n| MOIRAI |     |       |       |         | 311m   |\\n\\nFigure 3. Aggregate results of the Monash Time Series Forecasting Benchmark. The normalized MAE is reported, which normalizes the MAE of each dataset by the naive forecast's MAE, and aggregated by taking the geometric mean across datasets.\\n\\nTraining\\n\\nWe train MOIRAI in three sizes \u2013 small, base, and large, with key parameter details listed in Table 4. The small model is trained for 100,000 steps, while base and large models are trained for 1,000,000 steps with a batch size of 256. For optimization, we use the AdamW optimizer with the following hyperparameters, $\\\\text{lr} = 1 \\\\times 10^{-3}$, $\\\\text{weight decay} = 1 \\\\times 10^{-1}$, $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.98$. We also apply a learning rate scheduler with linear warmup for the first 10,000 steps, and cosine annealing thereafter.\\n\\nModels are trained on NVIDIA A100-40G GPUs with TF32 precision. We implement sequence packing (Raffel et al., 2020) to avoid large amounts of padding due to the disparity of sequence lengths in the new setting with varying context, prediction, and variate lengths, thereby increasing the effective batch size.\\n\\n4. Experiments\\n\\n4.1. In-distribution Forecasting\\n\\nWe first perform an in-distribution evaluation using the Monash benchmark, which aims to measure generalization capability across diverse domains. Described in Appendix A, LOTSA includes the Monash Time Series Forecasting Archive as a source of data. For a large portion of these datasets, we only include the train set, holding out the test set which we now use for in-distribution evaluation. In this evaluation, we consider a standard setting with a context length of 1000, and a patch size of 32 for all frequencies, except for quarterly data with a patch size of 8. Figure 3 summarizes the results based on the normalized mean absolute error (MAE), in comparison with the baselines presented in the Monash benchmark. It is worth noting that each baseline in the Monash benchmark is typically trained individually per dataset or per time series within a dataset. In contrast, MOIRAI stands out by being a single model evaluated across various datasets. Full results as well as a comparison with LLMTime (Gruver et al., 2023) can be found in Appendix D.1.\\n\\nWe observe that MOIRAI outperforms all baselines from the Monash benchmark regardless of model size, displaying the strong in-distribution and cross-domain capabilities arising from our unified training methodology. We highlight that each instance of MOIRAI is a single model evaluated across datasets, compared to baselines for which one model is trained per dataset. Further analysis on computational costs can be found in Appendix D.4.\\n\\n4.2. Out-of-distribution / Zero-shot Forecasting\\n\\nNext, we perform an out-of-distribution evaluation on unseen target datasets. Here, MOIRAI is a zero-shot forecaster compared with state-of-the-art full-shot baselines which have been trained on the individual target datasets. While the ideal scenario would be to include other universal forecasters, this proves to be a challenging task. As a nascent field, most universal forecasters currently do not yet have open weights available for evaluation. Furthermore, the problem of comparing zero-shot methods is exacerbated by not having a standard held-out test split, making it challenging to collate a set of datasets which all the models have not been trained on. Thus, we establish the strong zero-shot capabilities of MOIRAI by displaying competitive or stronger results compared with SOTA full-shot methods \u2013 datasets used in the following have not been included in LOTSA.\\n\\nProbabilistic Forecasting\\n\\nWe evaluate on six datasets across energy, transport, climate, and sales domains, following a rolling evaluation setup with stride equal to prediction length. Prediction lengths and number of rolling evaluations are defined for each dataset based on frequency. We report the Continuous Ranked Probability Score (CRPS) and Mean Scaled Interval Score (MSIS) metrics (definitions in Appendix C), comparing against four full-shot baselines \u2013 DeepAR (Salinas et al., 2020), PatchTST (Nie et al., 2023), and TiDE (Das et al., 2023a) with Student's t-distribution prediction heads, and TFT based on quantile prediction (Lim et al., 2021), all implemented with the GluonTS library (Alexandrov et al., 2020), as well as simple baselines AutoARIMA (Garza et al., 2022) and Seasonal Naive (Hyndman & Athanasopoulos, 2018). For each dataset and baseline, we perform hyperparameter tuning on a validation CRPS, and report results averaged over five training runs with different seeds. For MOIRAI, we perform inference time tuning, selecting context length from $\\\\{1000, 2000, 3000, 4000, 5000\\\\}$ and patch sizes based on frequency, on the validation CRPS. Full details of the evaluation setting can be found in Appendix C.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nTable 5.\\nProbabilistic forecasting results. Best results are highlighted in bold, and second best results are underlined. Baseline results are aggregated over five training runs with different seeds, reporting the mean and standard deviation.\\n\\nZero-shot Full-shot Baseline\\n\\n| Dataset      | MOIRAI Small | MOIRAI Base | MOIRAI Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|--------------|--------------|-------------|--------------|----------|------|-----|--------|-----------|----------|-------|\\n| Electricity  |              |             |              |          |      |     |        |           |          |       |\\n| CRPS         | 0.072        | 0.055       | 0.050        | 0.052    | 0.048| 0.050| 0.065   | 0.327     | 0.070    |\\n| MSIS         |              |             |              |          |      |     |        |           |          |       |\\n|              | 7.999        | 6.172       | 5.875        | 5.744    | 5.672| 6.278| 6.893   | 29.412    | 35.251   |\\n| Solar        |              |             |              |          |      |     |        |           |          |       |\\n| CRPS         | 0.471        | 0.419       | 0.406        | 0.518    | 0.420| 0.446| 0.431   | 1.055     | 0.512    |\\n| MSIS         | 8.425        | 7.011       | 6.250        | 8.447    | 13.754| 8.057| 11.181  | 25.849    | 48.130   |\\n| Walmart      |              |             |              |          |      |     |        |           |          |       |\\n| CRPS         | 0.103        | 0.093       | 0.098        | 0.082    | 0.077| 0.087| 0.121   | 0.124     | 0.151    |\\n| MSIS         | 9.371        | 8.421       | 8.520        | 6.005    | 6.258| 8.718| 12.502  | 9.888     | 49.458   |\\n| Weather      |              |             |              |          |      |     |        |           |          |       |\\n| CRPS         | 0.049        | 0.041       | 0.051        | 0.059    | 0.054| 0.043| 0.132   | 0.252     | 0.068    |\\n| MSIS         | 5.236        | 5.136       | 4.962        | 7.759    | 8.095| 7.791| 4.094   | 21.651    | 17.34    |\\n| Istanbul Traffic |          |             |              |          |      |     |        |           |          |       |\\n| CRPS         | 0.173        | 0.116       | 0.112        | 0.112    | 0.110| 0.110| 0.108   | 0.589     | 0.257    |\\n| MSIS         | 5.937        | 4.461       | 4.277        | 3.813    | 4.752| 4.057| 4.094   | 16.317    | 45.473   |\\n| Turkey Power |              |             |              |          |      |     |        |           |          |       |\\n| CRPS         | 0.048        | 0.040       | 0.036        | 0.054    | 0.046| 0.039| 0.066   | 0.116     | 0.085    |\\n| MSIS         | 7.127        | 6.766       | 6.341        | 8.978    | 8.579| 7.943| 4.094   | 13.520    | 36.256   |\\n\\nTable 6. Long sequence forecasting results. Results are averaged across prediction lengths \\\\{96, 192, 336, 720\\\\}. Best results are highlighted in bold, and second best results are underlined. Full-shot results are obtained from Liu et al. (2023b).\\n\\nZero-shot Full-shot\\n\\n| Dataset      | MOIRAI Small | MOIRAI Base | MOIRAI Large | iTransformer | TimesNet | PatchTST | Crossformer | TiDE | DLinear | SCINet | FEDformer |\\n|--------------|--------------|-------------|--------------|--------------|----------|----------|-------------|------|---------|--------|-----------|\\n| ETTh1        | 0.400        | 0.434       | 0.510        | 0.454        | 0.458    | 0.469    | 0.529       | 0.541| 0.456   | 0.747  | 0.44      |\\n| MSE          |              |             |              |              |          |          |             |      |          |        |           |\\n| MAE          | 0.424        | 0.438       | 0.469        | 0.448        | 0.450    | 0.455    | 0.522       | 0.507| 0.452   | 0.647  | 0.46      |\\n| ETTh2        | 0.341        | 0.345       | 0.354        | 0.383        | 0.414    | 0.387    | 0.942       | 0.611| 0.559   | 0.954  | 0.437     |\\n| MSE          |              |             |              |              |          |          |             |      |          |        |           |\\n| MAE          | 0.379        | 0.382       | 0.376        | 0.407        | 0.497    | 0.407    | 0.684       | 0.550| 0.515   | 0.723  | 0.449     |\\n| ETTm1        | 0.448        | 0.381       | 0.390        | 0.407        | 0.400    | 0.387    | 0.513       | 0.419| 0.403   | 0.486  | 0.448     |\\n| MSE          |              |             |              |              |          |          |             |      |          |        |           |\\n| MAE          | 0.409        | 0.388       | 0.389        | 0.410        | 0.406    | 0.400    | 0.495       | 0.419| 0.407   | 0.481  | 0.452     |\\n| ETTm2        | 0.300        | 0.272       | 0.276        | 0.288        | 0.291    | 0.281    | 0.757       | 0.358| 0.35    | 0.571  | 0.305     |\\n| MSE          |              |             |              |              |          |          |             |      |          |        |           |\\n| MAE          | 0.341        | 0.321       | 0.320        | 0.332        | 0.333    | 0.326    | 0.611       | 0.404| 0.401   | 0.537  | 0.349     |\\n\\nElectricity MSE 0.233 0.188 0.188 0.178 0.193 0.216 0.244 0.252 0.212 0.268 0.214\\nMAE 0.320 0.274 0.273 0.270 0.295 0.304 0.334 0.344 0.3 0.365 0.327\\nWeather MSE 0.242 0.238 0.259 0.258 0.259 0.259 0.259 0.271 0.265 0.292 0.309\\nMAE 0.267 0.261 0.275 0.278 0.287 0.281 0.315 0.320 0.317 0.363 0.36\\n\\nTable 5 reports the CRPS and MSIS, with full results including deterministic metrics in Appendix D.2. We observe that MOIRAI Base and MOIRAI Large consistently achieve strong zero-shot performance, obtaining either best or second best results for all datasets except Walmart and Istanbul Traffic. Even for these datasets, performance is still close to the best performance, despite being a single zero-shot model compared to baselines which have been tuned and trained on the train sets.\\n\\nLong Sequence Forecasting\\nWe evaluate on a subset of the popular long sequence forecasting benchmark (Wu et al., 2021), omitting datasets which have datasets from the same source present in our pre-training data and cannot be considered zero-shot. We report the Mean Squared Error (MSE) and MAE, comparing against six state-of-the-art baselines, iTransformer (Liu et al., 2023b), TimesNet (Wu et al., 2023), PatchTST, Crossformer (Zhang & Yan, 2023), TiDE, DLinear (Zeng et al., 2023), SCINet (Liu et al., 2022), and FEDformer (Zhou et al., 2022b). Point forecasts are obtained from MOIRAI by taking the median from the samples of the predictive distribution. Tuning for MOIRAI was based on the average validation MSE across prediction lengths, further including the options between channel independent and channel mixing strategies (Nie et al., 2023) for the low dimension datasets (ETT and Weather).\\n\\nTable 6 reports the average performance across prediction lengths, with full results in Appendix D.3. We observe that MOIRAI achieves strong results compared to full-shot baselines. While MOIRAI Base consistently achieves strong performance across datasets with either best or second-best performance, the large model is less consistent, with slightly weaker but competitive results. The relationship between performance and model size is tenuous in this setting, however, this does not constitute strong evidence against the potential of scaling, since these results are based on models trained on a fixed dataset size and settings. Rather, this calls for more comprehensive neural scaling laws (Kaplan et al., 2020) for LTMs, to build a stronger understanding of their scaling behavior.\\n\\n4.3. Ablation Study\\nArchitecture\\nWe perform a series of ablations in Table 7, starting from the default MOIRAI Small. Firstly, we ablate the multi patch size component, removing the constraints by allowing any frequency to have any patch size during training, and also simply fixing the patch size to 32. In both cases, we observe a deterioration in normalized MAE. Removing Any-variate Attention and using additive learned embeddings (randomizing variate index during training to encourage permutation invariance) instead, leads to suboptimal results, showcasing the strength of Any-variate.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Training of Universal Time Series Forecasting Transformers\\n\\nTable 7. Ablation study on Monash benchmark. The aggregated normalized MAE, similarly calculated as in Figure 3 is reported.\\n\\n| Configuration                           | Normalized MAE |\\n|-----------------------------------------|----------------|\\n| MOIRAI Small                            | 0.655          |\\n| w/o patch size constraints              | 0.720          |\\n| w/o multi patch size                    | 1.156          |\\n| w/o Any-variate Attention               | 0.904          |\\n| w/o mixture distribution                | 0.740          |\\n| w/o LOTSA                                | 0.809          |\\n| w/o packing                             | 0.785          |\\n\\nFigure 4. Visualization of probabilistic forecasts by two variants of MOIRAI Small on the Traffic Hourly dataset. Both models forecast peaks, however, the Student\u2019s t-distribution has a symmetric distribution, giving inappropriate prediction intervals for a peak, as highlighted in red.\\n\\nAttention. We see similar deterioration when replacing the mixture distribution with a Student\u2019s t-distribution, and further visualize the necessity of flexible distributions for probabilistic forecasts in Figure 4.\\n\\nTraining Methodology\\nWe study the impact of a large and diverse dataset by training MOIRAI Small only on the GluonTS and Monash datasets, observing that diversity of data is critical for cross-domain training even on in-distribution evaluation. Finally, given the same batch size and training iterations, we show that packed training significantly boosts performance. This is because packing increases the effective batch size and increases the number of observations the model is trained on, given the same amount of compute.\\n\\n4.4. Further Analysis\\n\\nContext Length\\nOur pre-training methodology varies context length defined by the task distribution. We verify that MOIRAI has the capability to take as input arbitrary context lengths by visualizing in Figure 5 the relationship between performance and increasing context lengths over three datasets in the zero-shot setting. Zeng et al. (2023); Liu et al. (2023b) previously observed that the desiderata of continuously improving performance with increasing context length is not present in conventional Transformer-based forecasters. Here, we observe that MOIRAI indeed achieves this desired property, in fact,\\n\\nFigure 5. Plot of performance (MAE) against context length (x-axis in log scale) with prediction length 96 and patch size 32 on the validation set of the ETTm1, Electricity, and Weather datasets.\\n\\nPacking\\nPacking has long been applied in training LLMs and other Transformer-based models, but not for time series Transformers. While we can get away with inefficiencies when dealing with small-scale data, we start to suffer from longer training times as we scale towards the paradigm of FMs and LTMs. This is further exacerbated by our \u201cflattened\u201d setting which increases the disparity in sequence lengths. As evidenced in Section 4.3, keeping compute (batch size, iterations, etc.) constant, packing improves performance by 16%. To understand why this is the case, we visualize sequence length distribution in Figure 6. With a large portion of the data being shorter than the maximum sequence length, padding represents a whopping 61.08% of input tokens without packed training, and only 0.38% with our packed implementation (calculated over 1000 iterations).\\n\\n5. Conclusion\\nIn this work, we introduced MOIRAI, a masked encoder-based universal time series forecasting Transformer which alleviates the issues faced in the universal forecasting paradigm. We also introduce the LOTSA, the largest collection of open-data for pre-training time series forecasting models. MOIRAI is evaluated on the in-distribution and out-of-distribution settings, and is capable of probabilistic and long sequence forecasting. We show that as a zero-shot forecaster, MOIRAI achieves competitive or superior performance compared to full-shot models.\\n\\nLimitations & Future Work\\nWhile MOIRAI achieves phenomenal in and out-of-distribution performance, this is just a first step in the universal forecasting paradigm.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Due to resource constraints, little to no hyperparameter tuning was performed \u2013 efficient tuning techniques such as \u00b5P (Yang et al., 2022a) can be applied. In terms of architecture, our approach to tackling cross-frequency learning with a multi patch size mapping is somewhat heuristic, and future work should design a more flexible and elegant approach. Also, the current architecture has limited support for high-dimensional time series, and efficient methods for extending Transformer input length can alleviate this issue.\\n\\nThe masked encoder structure also makes it amenable to exploration of a latent diffusion architecture (Feng et al., 2024). In terms of data, LOTSA can be further enhanced with greater diversity in terms of domain and frequency. Finally, incorporating multi-modality such as tabular or text inputs is an exciting new direction which universal forecasting has unlocked.\\n\\nImpact Statement\\nThis paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.\\n\\nReferences\\nAlexandrov, A., Benidis, K., Bohlke-Schneider, M., Flunkert, V., Gasthaus, J., Januschowski, T., Maddix, D. C., Rangapuram, S., Salinas, D., Schulz, J., Stella, L., T\u00c1\u00bcrkmen, A. C., and Wang, Y. Gluonts: Probabilistic and neural time series modeling in python. Journal of Machine Learning Research, 21(116):1\u20136, 2020. URL http://jmlr.org/papers/v21/19-820.html.\\n\\nAwasthi, P., Das, A., Sen, R., and Suresh, A. T. On the benefits of maximum likelihood estimation for regression and forecasting. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=zrW-LVXj2k1.\\n\\nBergmeir, C., Bui, Q., de Nijs, F., and Stuckey, P. Residential power and battery data, August 2023. URL https://doi.org/10.5281/zenodo.8219786.\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-lut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\nCDC. Flu portal dashboard, 2017. URL https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html.\\n\\nChen, C., Petty, K., Skabardonis, A., Varaiya, P., and Jia, Z. Freeway performance measurement system: mining loop detector data. Transportation Research Record, 1748(1):96\u2013102, 2001.\\n\\nChen, S. Beijing Multi-Site Air-Quality Data. UCI Machine Learning Repository, 2019. DOI: https://doi.org/10.24432/C5RK5G.\\n\\nDas, A., Kong, W., Sen, R., and Yu, R. Long-term forecasting with tiDE: Time-series dense encoder. Transactions on Machine Learning Research, 2023a. ISSN 2835-8856. URL https://openreview.net/forum?id=pCbC3aQB5W.\\n\\nDas, A., Kong, W., Sen, R., and Zhou, Y. A decoder-only foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688, 2023b.\\n\\nDong, J., Wu, H., Zhang, H., Zhang, L., Wang, J., and Long, M. SimMTM: A simple pre-training framework for masked time-series modeling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=ginTcBUnL8.\\n\\nDooley, S., Khurana, G. S., Mohapatra, C., Naidu, S. V., and White, C. ForecastPFN: Synthetically-trained zero-shot forecasting. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=tScBQRNgjk.\\n\\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\nEkambaram, V., Jati, A., Nguyen, N., Sinthong, P., and Kalagnanam, J. Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '23, pp. 459\u2013469, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599533. URL https://doi.org/10.1145/3580305.3599533.\\n\\nEkambaram, V., Jati, A., Nguyen, N. H., Dayama, P., Reddy, C., Gifford, W. M., and Kalagnanam, J. Ttms: Fast multi-level tiny time mixers for improved zero-shot and few-shot forecasting of multivariate time series. arXiv preprint arXiv:2401.03955, 2024.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Table 22. | Electricity | Istanbul Traffic | Weather | Turkey Power |\\n|-----------|-------------|-----------------|---------|--------------|\\n| ETTm2     | ETTm1       | ETTh2           | ETTh1   |\\n\\nFull results for probabilistic forecasting experiments. Best results are highlighted in bold.\\n\\n|         | MSE | MAE | sMAPE | NRMSE | MASE |\\n|---------|-----|-----|-------|-------|------|\\n| Walmart |     |     |       |       |      |\\n| Solar   |     |     |       |       |      |\\n\\n**Zero-shot Full-shot** Baseline\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**OIRAI**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Small**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**OIRAI**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**Large**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**PatchTST**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**TimesNet**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**Base**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**iTransformer**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**OIRAI**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**Base**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**OIRAI**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**Zero-shot**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Base**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**iTransformer**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**TimesNet**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**PatchTST**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**TiDE**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**OIRAI**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**TimesNet**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**PatchTST**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**TiDE**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**TFT**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**DeepAR**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\n|---------|-------|------|--------|-------|\\n| Walmart |       |      |        |       |\\n| Solar   |       |      |        |       |\\n\\n**Seasonal**\\n\\n|         | Large | PatchTST | TiDE | TFT | DeepAR | AutoARIMA | Seasonal | Naive |\\n|---------|-------|----------|------|-----|--------|-----------|----------|-------|\\n| Walmart |       |          |      |     |        |           |          |       |\\n| Solar   |       |          |      |     |        |           |          |       |\\n\\n**Naive**\\n\\n|         | CRPS | MSIS | ND  | OIRAI |\\n|---------|------|------|-----|-------|\\n| Walmart |     |     |     |       |\\n| Solar   |     |     |     |       |\\n\\n**AutoARIMA**\\n\\n|         | Small | Base | Medium | Large |\\"}
{"id": "Yd8eHMY1wz", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We perform an analysis on the computation cost of MOIRAI compared to other deep learning based models, while varying the context and prediction lengths. Overall, given the same model size and setting, the cost of inference compared to other deep learning models would be similar. From an architecture perspective, MOIRAI has the following benefits:\\n\\n- Patch based inputs: This decreases the computation cost significantly by reducing the number of input tokens.\\n- Masked encoder architecture: Unlike decoder-only Transformers, the masked encoder architecture can make multi step predictions in a single forward pass. For decoder-only Transformers and RNNs, they need to autoregressively make predictions, making multiple forward passes for a multi step forecast. For long horizons, this can be quite costly.\\n\\nFurthermore, compared to standard baselines, MOIRAI performs zero-shot forecasting. The standard baseline approach has to be trained (multiple times with hyperparameter tuning) for each dataset, leading to increased costs. As MOIRAI continues to be utilized on new datasets, the pre-training costs are amortized and only becomes cheaper, while standard approaches need to be trained over and over again on new datasets. We note that while MOIRAI indeed incurs increased costs due to model size, inference is still highly competitive, taking under 1 second to construct forecasts even with extremely long context/prediction lengths.\\n\\n|                  | 1000   | 2000   | 3000   | 4000   | 5000   | 1000   | 2000   | 3000   | 4000   | 5000   |\\n|------------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\\n| MOIRAI Small     | 0.03   | 0.04   | 0.05   | 0.06   | 0.07   | 0.03   | 0.04   | 0.05   | 0.06   | 0.07   |\\n| MOIRAI Base      | 0.05   | 0.06   | 0.08   | 0.11   | 0.13   | 0.05   | 0.06   | 0.08   | 0.11   | 0.13   |\\n| MOIRAI Large     | 0.09   | 0.14   | 0.19   | 0.25   | 0.30   | 0.09   | 0.14   | 0.19   | 0.25   | 0.30   |\\n| PatchTST         | 0.01   | 0.02   | 0.02   | 0.03   | 0.04   | 0.01   | 0.01   | 0.01   | 0.01   | 0.02   |\\n| TiDE             | 0.01   | 0.01   | 0.01   | 0.01   | 0.01   | 0.01   | 0.01   | 0.01   | 0.01   | 0.01   |\\n| TFT              | 0.02   | 0.04   | 0.06   | 0.08   | 0.09   | 0.03   | 0.07   | 0.12   | 0.17   | OOM    |\\n| DeepAR           | 0.26   | 0.32   | 0.37   | 0.43   | 0.49   | 2.02   | 4.06   | 6.10   | 8.17   | 10.24  |\"}"}
{"id": "Yd8eHMY1wz", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8. Visualizations of zero-shot forecasts from MOIRAI Base on ETTh1 and ETTm1 datasets.\"}"}
{"id": "Yd8eHMY1wz", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9. Visualizations of zero-shot forecasts from MOIRAI Base on Istanbul Traffic and Turkey Power datasets.\"}"}
