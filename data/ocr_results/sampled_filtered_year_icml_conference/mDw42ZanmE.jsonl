{"id": "mDw42ZanmE", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\ntransactions on visualization and computer graphics, 25(1):364\u2013373, 2018.\\n\\nZheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. Gpt-4v(ision) is a generalist web agent, if grounded, 2024.\\n\\nZou, X., Yang, J., Zhang, H., Li, F., Li, L., Wang, J., Wang, L., Gao, J., and Lee, Y. J. Segment everything everywhere all at once, 2023.\"}"}
{"id": "mDw42ZanmE", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nAppendix\\n\\nA.\\n\\nMAIA\\n\\nThe full MAIA API provided in the system prompt is reproduced below.\\n\\n```python\\nimport torch\\nfrom typing import List, Tuple\\n\\nclass System:\\n    def __init__(self, neuron_num: int, layer: str, model_name: str, device: str):\\n        self.neuron_num = neuron_num\\n        self.layer = layer\\n        self.device = torch.device(f\\\"cuda:{device}\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n        self.model = self.load_model(model_name)\\n    \\n    def load_model(self, model_name: str) -> torch.nn.Module:\\n        return load_model(model_name)\\n\\n    def neuron(self, image_list: List[torch.Tensor]) -> Tuple[List[int], List[str]]:\\n        pass\\n```\\n\\nThe above code defines a `System` class that contains a Python class containing the vision model and the specific neuron to interact with.\\n\\n- **Attributes**\\n  - `neuron_num`: int\\n  - `layer`: str\\n  - `model_name`: str\\n  - `model`: nn.Module\\n\\n- **Methods**\\n  - `load_model(model_name: str) -> nn.Module`: Gets the model name and returns the vision model from PyTorch library.\\n  - `neuron(image_list: List[torch.Tensor]) -> Tuple[List[int], List[str]]`: returns the neuron activation for each image in the input image_list as well as the activation map of the neuron over that image, that highlights the regions of the image where the activations are higher (encoded into a Base64 string).\\n\\n  ```python\\n  def __init__(self, neuron_num: int, layer: str, model_name: str, device: str):\\n      self.neuron_num = neuron_num\\n      self.layer = layer\\n      self.device = torch.device(f\\\"cuda:{device}\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n      self.model = self.load_model(model_name)\\n\\n  def load_model(self, model_name: str) -> torch.nn.Module:\\n      return load_model(model_name)\\n\\n  def neuron(self, image_list: List[torch.Tensor]) -> Tuple[List[int], List[str]]:\\n      pass\\n  ```\\n\\nInitializes a neuron object by specifying its number and layer location and the vision model that the neuron belongs to.\\n\\n- **Parameters**\\n  - `neuron_num`: int\\n  - `layer`: str\\n  - `model_name`: str\\n  - `device`: str\\n\\nExamples\\n--------\\n\\n```python\\n>>> # load \\\"resnet152\\\"\\n>>> def run_experiment(model_name) -> nn.Module:\\n>>>     model = load_model(model_name)\\n>>>     return model\\n```\"}"}
{"id": "mDw42ZanmE", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nThe function returns the neuron's maximum activation value (in int format) for each of the images in the list as well as the activation map of the neuron over each of the images that highlights the regions of the image where the activations are higher (encoded into a Base64 string).\\n\\nParameters\\n----------\\nimage_list : List[torch.Tensor]\\nThe input image\\n\\nReturns\\n-------\\nTuple[List[int], List[str]]\\nFor each image in image_list returns the activation value of the neuron on that image, and a masked image, with the region of the image that caused the high activation values highlighted (and the rest of the image is darkened). Each image is encoded into a Base64 string.\\n\\nExamples\\n--------\\n>>> # test the activation value of the neuron for the prompt \\\"a dog standing on the grass\\\"\\n>>> def run_experiment(system, tools) -> Tuple[int, str]:\\n>>>     prompt = ['a dog standing on the grass']\\n>>>     image = tools.text2image(prompt)\\n>>>     activation_list, activation_map_list = system.neuron(image)\\n>>>     return activation_list, activation_map_list\\n\\n>>> # test the activation value of the neuron for the prompt \\\"a dog standing on the grass\\\" and the neuron activation value for the same image but with a lion instead of a dog\\n>>> def run_experiment(system, tools) -> Tuple[int, str]:\\n>>>     prompt = ['a dog standing on the grass']\\n>>>     edits = ['replace the dog with a lion']\\n>>>     all_image, all_prompts = tools.edit_images(prompt, edits)\\n>>>     activation_list, activation_map_list = system.neuron(all_images)\\n>>>     return activation_list, activation_map_list\\n\\n\\\"\\\"\\\"\\nreturn neuron(image_list)\\n\\\"\\\"\\\"\\n\\nclass Tools:\\n\\nA Python class containing tools to interact with the neuron implemented in the system class, in order to run experiments on it.\\n\\nAttributes\\n----------\\nexperiment_log: str\\nA log of all the experiments, including the code and the output from the neuron.\\n\\nMethods\\n-------\\ndataset_exemplars(system: object) -> Tuple[List[int],List[str]]\\nThis experiment provides good coverage of the behavior observed on a very large dataset of images and therefore represents the typical behavior of the neuron on real images.\\n\\nThis function characterizes the prototypic behavior of the neuron by computing its activation on all images in the ImageNet dataset and returning the 15 highest activation values and the images that produced them.\\n\\nThe images are masked to highlight the specific regions that produce the maximal activation. The images are overlaid with a semi-opaque mask, such that the maximally activating regions remain unmasked.\\n\\nedit_images(prompt_list_org_image : List[str], editing_instructions_list : List[str]) -> Tuple[List[Image.Image], List[str]]\\nThis function enables localized testing of specific hypotheses about how variations on the content of a single image affect neuron activations.\\n\\nGets a list of input prompt and a list of corresponding editing instructions, then generate images according to the input prompts and edits each image based on the instructions given in the prompt using a text-based image editing model.\\n\\nThis function is very useful for testing the causality of the neuron in a controlled way, for example by testing how the neuron activation is affected by changing one aspect of the image.\\n\\nIMPORTANT: Do not use negative terminology such as \\\"remove ...\\\", try to use terminology like \\\"replace ... with ...\\\" or \\\"change the color of ... to ...\\\".\\n\\ntext2image(prompt_list: str) -> Tuple[torch.Tensor]\\nGets a list of text prompt as an input and generates an image for each prompt in the list using a text to image model.\\n\\nThe function returns a list of images.\\n\\nsummarize_images(self, image_list: List[str]) -> str:\\nThis function is useful to summarize the mutual visual concept that appears in a set of images.\\nIt gets a list of images at input and describes what is common to all of them, focusing specifically on unmasked regions.\\n\\ndescribe_images(synthetic_image_list: List[str], synthetic_image_title:List[str]) -> str\\nProvides impartial descriptions of images. Do not use this function on dataset exemplars.\"}"}
{"id": "mDw42ZanmE", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nGets a list of images and generate a textual description of the semantic content of the unmasked regions within each of them.\\n\\nThe function is blind to the current hypotheses list and therefore provides an unbiased description of the visual content.\\n\\n```\\nlog_experiment(activation_list: List[int], image_list: List[str], image_titles: List[str], image_textual_information: Union[str, List[str]]) -> None\\n```\\ndocuments the current experiment results as an entry in the experiment log list. If self.activation_threshold was updated by the dataset_exemplars function, the experiment log will contain instructions to continue with experiments if activations are lower than activation_threshold.\\n\\nResults that are logged will be available for future experiments (unlogged results will be unavailable).\\n\\nThe function also updates the attribute \\\"result_list\\\", such that each element in the result_list is a dictionary of the format: `{\"<prompt>\": {\"activation\": act, \"image\": image}}`\\n\\nso the list contains all the results that were logged so far.\\n\\n```\\nself.experiment_log = []\\nself.results_list = []\\n```\\n\\n```\\ndef __init__(self):\\n```\\n\\nInitializes the Tools object.\\n\\n```\\nParameters\\n----------\\nexperiment_log: store all the experimental results\\n```\\n\\n```\\ndef dataset_exemplars(self, system: object) -> Tuple(List[int], List[str]):\\n```\\n\\nThis method finds images from the ImageNet dataset that produce the highest activation values for a specific neuron.\\n\\nIt returns both the activation values and the corresponding exemplar images that were used to generate these activations (with the highly activating region highlighted and the rest of the image darkened).\\n\\nThe neuron and layer are specified through a 'system' object.\\n\\nThis experiment is performed on real images and will provide a good approximation of the neuron behavior.\\n\\n```\\nParameters\\n----------\\nsystem : object\\n```\\n\\nAn object representing the specific neuron and layer within the neural network.\\n\\nThe 'system' object should have 'layer' and 'neuron_num' attributes, so the dataset_exemplars function can return the exemplar activations and masked images for that specific neuron.\\n\\n```\\nReturns\\n-------\\ntuple\\n```\\n\\nA tuple containing two elements:\\n- The first element is a list of activation values for the specified neuron.\\n- The second element is a list of exemplar images (as Base64 encoded strings) corresponding to these activations.\\n\\n```\\nExample\\n-------\\n```f\\n```\"}"}
{"id": "mDw42ZanmE", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nA list of input prompts for image generation. These prompts are used to generate images which are to be edited by the prompts in editing_instructions_list.\\n\\n`editing_instructions_list : List[str]`\\nA list of instructions for how to edit the images in `image_list`. Should be the same length as `prompt_list_org_image`.\\n\\nEdits should be relatively simple and describe replacements to make in the image, not deletions.\\n\\nReturns\\n-------\\nTuple[List[Image.Image], List[str]]\\nA list of all images where each unedited image is followed by its edited version. And a list of all the prompts corresponding to each image (e.g. the input prompt followed by the editing instruction).\\n\\nExamples\\n--------\\n```python\\ndef run_experiment(system, tools) -> Tuple[int, str]:\\n    prompt = ['a dog standing on the grass']\\n    edits = ['replace the dog with a cat']\\n    all_image, all_prompts = tools.edit_images(prompt, edits)\\n    activation_list, activation_map_list = system.neuron(all_image)\\n    return activation_list, activation_map_list\\n```\\n```python\\ndef run_experiment(system, tools) -> Tuple[int, str]:\\n    prompts = ['a dog standing on the grass'] * 3\\n    edits = ['make the dog sit', 'make the dog run', 'make the dog eat']\\n    all_images, all_prompts = tools.edit_images(prompts, edits)\\n    activation_list, activation_map_list = system.neuron(all_images)\\n    return activation_list, activation_map_list\\n```\\n\\n```python\\ndef text2image(self, prompt_list: List[str]) -> List[Image.Image]:\\n    \"\"\"Gets a list of text prompts as input, generates an image for each prompt in the list using a text to image model. The function returns a list of images.\\n\\n    Parameters\\n    ----------\\n    prompt_list : List[str]\\n        A list of text prompts for image generation.\\n\\n    Returns\\n    -------\\n    List[Image.Image]\\n        A list of images, corresponding to each of the input prompts.\\n    \"\"\"\\n    return [image for image in image_list]\\n```\\n```python\\ndef summarize_images(self, image_list: List[str]) -> str:\\n    \"\"\"This function is useful to summarize the mutual visual concept that appears in a set of images. It gets a list of images at input and describes what is common to all of them, focusing specifically on unmasked regions.\\n\\n    Parameters\\n    ----------\\n    image_list : List[str]\\n        A list of images to be summarized.\\n\\n    Returns\\n    -------\\n    str\\n        A description of the common visual concept in the set of images.\\n    \"\"\"\\n    return ' '.join(image_list)\\n```\"}"}
{"id": "mDw42ZanmE", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nimage_list : list\\nA list of images in Base64 encoded string format.\\n\\nReturns\\n-------\\nstr\\nA string with a description of what is common to all the images.\\n\\nExample\\n-------\\n>>> # tests dataset exemplars and return textual summarization of what is common for all the maximally activating images\\n>>> def run_experiment(system, tools):\\n>>>     activation_list, image_list = self.dataset_exemplars(system)\\n>>>     prompt_list = []\\n>>>     for i in range(len(activation_list)):\\n>>>         prompt_list.append(f'dataset exemplar {i}') # for the dataset exemplars we don't have prompts, therefore need to provide text titles\\n>>>     summarization = tools.summarize_images(image_list)\\n>>>     return summarization\\n\\nreturn summarize_images(image_list)\\n\\ndef describe_images(self, image_list: List[str], image_title: List[str]) -> str:\\n\\nProvides impartial description of the highlighted image regions within an image.\\n\\nThis function translates the visual content of the highlighted region in the image to a text description. The function operates independently of the current hypothesis list and thus offers an impartial description of the visual content. It iterates through a list of images, requesting a description for the highlighted (unmasked) regions in each synthetic image. The final descriptions are concatenated and returned as a single string, with each description associated with the corresponding image title.\\n\\nParameters\\n----------\\nimage_list : list\\nA list of images in Base64 encoded string format.\\nimage_title : callable\\nA list of strings with the image titles that will be used to list the different images. Should be the same length as image_list.\\n\\nReturns\\n-------\\nstr\\nA concatenated string of descriptions for each image, where each description is associated with the image's title and focuses on the highlighted regions in the image.\\n\\nExample\\n-------\\n>>> def run_experiment(system, tools):\\n>>>     prompt_list = [\\n>>>         \\\"a fox and a rabbit watch a movie under a starry night sky\\\",\\n>>>         \\\"a fox and a bear watch a movie under a starry night sky\\\",\\n>>>         \\\"a fox and a rabbit watch a movie at sunrise\\\"\\n>>>     ]\\n>>>     images = tools.text2image(prompt_list)\\n>>>     activation_list, image_list = system.neuron(images)\\n>>>     descriptions = tools.describe_images(image_list, prompt_list)\\n>>>     return descriptions\"}"}
{"id": "mDw42ZanmE", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nA list of the images that were generated using the text2image model and were tested. Should be the same length as activation_list.\\n\\nimage_titles : List[str]\\n\\nA list of the text labels for the images. Should be the same length as activation_list.\\n\\nimage_textual_information: Union[str, List[str]]\\n\\nA string or a list of strings with additional information to log such as the image summarization and/or the image textual descriptions.\\n\\nReturns\\n-------\\nNone\\n\\nExamples\\n--------\\n\\n>>> # tests the activation value of the neuron for the prompts \\\"a fox and a rabbit watch a movie under a starry night sky\\\", \\\"a fox and a bear watch a movie under a starry night sky\\\", \\\"a fox and a rabbit watch a movie at sunrise\\\", describes the images and logs the results and the image descriptions\\n>>> def run_experiment(system, tools):\\n...    prompt_list = [\\\"a fox and a rabbit watch a movie under a starry night sky\\\", \\\"a fox and a bear watch a movie under a starry night sky\\\", \\\"a fox and a rabbit watch a movie at sunrise\\\"]\\n...    images = tools.text2image(prompt_list)\\n...    activation_list, activation_map_list = system.neuron(images)\\n...    descriptions = tools.describe_images(images, prompt_list)\\n...    tools.log_experiment(activation_list, activation_map_list, prompt_list, descriptions)\\n...    return\\n\\n>>> # tests dataset exemplars, use umage summarizer and logs the results\\n>>> def run_experiment(system, tools):\\n...    activation_list, image_list = self.dataset_exemplars(system)\\n...    prompt_list = []\\n...    for i in range(len(activation_list)):\\n...        prompt_list.append(f'dataset_exemplars {i}')  # for the dataset exemplars we don't have prompts, therefore need to provide text titles\\n...    summarization = tools.summarize_images(image_list)\\n...    log_experiment(activation_list, activation_map_list, prompt_list, summarization)\\n...    return\\n\\n>>> # test the effect of changing a dog into a cat. Describes the images and logs the results.\\n>>> def run_experiment(system, tools) -> Tuple[int, str]:\\n...    prompt = [\\\"a dog standing on the grass\\\"]\\n...    edits = [\\\"replace the dog with a cat\\\"]\\n...    all_images, all_prompts = tools.edit_images(prompt, edits)\\n...    activation_list, activation_map_list = system.neuron(all_images)\\n...    descriptions = tools.describe_images(activation_map_list, all_prompts)\\n...    tools.log_experiment(activation_list, activation_map_list, all_prompts, descriptions)\\n...    return\\n\\n>>> # test the effect of changing the dog's action on the activation values. Describes the images and logs the results.\\n>>> def run_experiment(system, prompt_list) -> Tuple[int, str]:\\n...    prompts = [\\\"a dog standing on the grass\\\"] * 3\\n...    edits = [\\\"make the dog sit\\\", \\\"make the dog run\\\", \\\"make the dog eat\\\"]\\n...    all_images, all_prompts = tools.edit_images(prompts, edits)\\n...    activation_list, activation_map_list = system.neuron(all_images)\\n...    descriptions = tools.describe_images(activation_map_list, all_prompts)\\n...    tools.log_experiment(activation_list, activation_map_list, all_prompts, descriptions)\\n...    return\\n\\n\\\"\\\"\\\"return log_experiment(activation_list, image_list, prompt_list, description)\\\"\\\"\"}"}
{"id": "mDw42ZanmE", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nB. MAIA\\n\\nuser prompt: neuron description\\n\\nYour overall task is to describe the visual concepts that maximally activate a neuron inside a deep network for computer vision. To do that you are provided with a library of Python functions to run experiments on the specific neuron (inside the \u201cSystem\u201d class) given the functions provided in the \u201cTools\u201d class. Make sure to use a variety of tools from the library to maximize your experimentation power. Some neurons might be selective for very specific concepts, a group of unrelated concepts, or a general concept, so try to be creative in your experiment and try to test both general and specific concepts. If a neuron is selective for multiple concepts, you should describe each of those concepts in your final description.\\n\\nAt each experiment step, write Python code that will conduct your experiment on the tested neuron, using the following format:\\n\\n[CODE]:\\n```\\npython\\ndef run_experiment(system, tools):\\n    # gets an object of the system class, an object of the tool class, and performs experiments on the neuron\\n    ...\\n    tools.log_experiment(...)\\n```\\n\\nFinish each experiment by documenting it by calling the \u201clog_experiment\u201d function. Do not include any additional implementation other than this function. Do not call \u201cexecute_command\u201d after defining it. Include only a single instance of experiment implementation at each step. Each time you get the output of the neuron, try to summarize what the inputs that activate the neuron have in common (where that description is not influenced by previous hypotheses). Then, write multiple hypotheses that could explain the visual concept(s) that activate the neuron. Note that the neuron can be selective for more than one concept.\\n\\nFor example, these hypotheses could list multiple concepts that the neuron is selective for (e.g. dogs OR cars OR birds), provide different explanations for the same concept, describe the same concept at different levels of abstraction, etc. Some of the concepts can be quite specific, test hypotheses that are both general and very specific.\\n\\nThen write a list of initial hypotheses about the neuron selectivity in the format:\\n\\n[HYPOTHESIS LIST]:\\nHypothesis_1: <hypothesis_1>\\n...\\nHypothesis_n: <hypothesis_n>.\\n\\nAfter each experiment, wait to observe the outputs of the neuron. Then your goal is to draw conclusions from the data, update your list of hypotheses, and write additional experiments to test them. Test the effects of both local and global differences in images using the different tools in the library.\\n\\nIf you are unsure about the results of the previous experiment you can also rerun it, or rerun a modified version of it with additional tools. Use the following format:\\n\\n[HYPOTHESIS LIST]: ## update your hypothesis list according to the image content and related activation values.\\n[CODE]: ## conduct additional experiments using the provided python library to test *ALL* the hypotheses. Test different and specific aspects of each hypothesis using all of the tools in the library. Write code to run the experiment in the same format provided above. Include only a single instance of experiment implementation.\\n\\nContinue running experiments until you prove or disprove all of your hypotheses. Only when you are confident in your hypothesis after proving it in multiple experiments, output your final description of the neuron in the following format:\\n\\n[DESCRIPTION]: <final description> ## Your description should be selective (e.g. very specific: \u201cdogs running on the grass\u201d and not just \u201cdog\u201d) and complete (e.g. include all relevant aspects the neuron is selective for). In cases where the neuron is selective for more than one concept, include in your description a list of all the concepts separated by logical \u201cOR\u201d.\\n\\n[LABEL]: <final label derived from the hypothesis or hypotheses> ## a label for the neuron generated from the hypothesis (or hypotheses) you are most confident in after running all experiments. They should be concise and complete, for example, \u201cgrass surrounding animals\u201d, \u201ccurved rims of cylindrical objects\u201d, \u201ctext displayed on computer screens\u201d, \u201cthe blue sky background behind a bridge\u201d, and \u201cwheels on cars\u201d are all appropriate. You should capture the concept(s) the neuron is selective for. Only list multiple hypotheses if the neuron is selective for multiple distinct concepts. List your hypotheses in the format:\\n\\n[LABEL 1]: <label 1>\\n[LABEL 2]: <label 2>\"}"}
{"id": "mDw42ZanmE", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nFigure 3. Predictive evaluation protocol. We compare neuron labeling methods by assessing how well their labels predict neuron activation values on unseen data. For each neuron we perform the following steps: (a) An LM uses candidate neuron labels to generate a set of image prompts that should maximally/neutrally activate the neuron. (b) All prompts (positive and neutral) from all methods are combined into one dataset. (c) For each labeling method, a new LM selects prompts from the Prompt Dataset that are likely to produce maximal and neutral neuron activations, if that label were accurate. (d) A text-to-image model generates all corresponding images, and the average activation values for positive and neutral images are recorded. A predictive neuron label will produce exemplars with maximally positive activations relative to the neutral baseline.\\n\\nWe observe that MAIA uses this tool in situations where previous hypotheses failed or when observing complex combinations of visual content.\\n\\nExperiment log. MAIA can document the results of each experiment (e.g. images, activations) using the log_experiment tool, to make them accessible during subsequent experiments. We prompt MAIA to finish experiments by logging results, and let it choose what to log (e.g. data that clearly supports or refutes a particular hypothesis).\\n\\n4. Evaluation\\n\\nThe MAIA framework is task-agnostic and can be adapted to new applications by specifying an interpretability task in the user prompt to the VLM. Before tackling model-level interpretability problems (Section 5), we evaluate MAIA\u2019s performance on the black-box neuron description task, a widely studied interpretability subroutine that serves a variety of downstream model auditing and editing applications (Gandelsman et al., 2024; Yang et al., 2023; Hernandez et al., 2022). For these experiments, the user prompt to MAIA specifies the task and output format (a longer-form [DESCRIPTION] of neuron behavior, followed by a short [LABEL]), and MAIA\u2019s System class instruments a particular vision model (e.g. ResNet-152) and an individual unit indexed inside that model (e.g. Layer 4 Unit 122).\\n\\nTask specifications for these experiments may be found in Appendix B. We find MAIA correctly predicts behaviors of individual vision neurons in three trained architectures (Section 4.1), and in a synthetic setting where ground-truth neuron selectivities are known (Section 4.2). We also find descriptions produced by MAIA\u2019s interactive procedure to be more predictive of neuron behavior than descriptions of a fixed set of dataset exemplars, using the MILAN baseline from Hernandez et al. (2022). In many cases, MAIA descriptions are on par with those by human experts using the MAIA API. In Section 4.3, we perform ablation studies to test how components of the MAIA API differentially affect description quality.\\n\\n4.1. Neurons in vision models\\n\\nWe use MAIA to produce natural language descriptions of a subset of neurons across three vision architectures trained under different objectives: ResNet-152, a CNN for supervised image classification (He et al., 2016), DINO (Caron et al., 2021), a Vision Transformer trained for unsupervised representation learning (Grill et al., 2020; Chen & He, 2021), and the CLIP visual encoder (Radford et al., 2021), a ResNet-50 model trained to align image-text pairs. For each model, we evaluate descriptions of 100 units randomly sampled from a range of layers that capture features at different levels of granularity (ResNet-152 conv. 1, res. 1-4, DINO MLP 1-11, CLIP res. 1-4). Figure 2 shows examples of MAIA experiments on neurons from all three models, and final MAIA labels. We also evaluate a baseline non-interactive approach that only labels dataset exemplars of each neuron\u2019s behavior using the MILAN model from Hernandez et al. (2022). Finally, we collect human annotations of a random subset (25%) of neurons labeled by MAIA and MILAN, in an experimental setting where human experts write programs to perform interactive analyses of neurons using the MAIA API. Human experts receive the MAIA user prompt, write programs that run experiments on the neurons, and return neuron descriptions in the same format. See Appendix C3 for details on the human labeling experiments.\\n\\nWe evaluate the accuracy of neuron descriptions produced by MAIA, MILAN, and human experts by measuring how well they predict neuron behavior on unseen test images (Figure 3). Similar to evaluation approaches that produce contrastive or counterfactual exemplars to reveal model decision boundaries (Gardner et al., 2020; Kaushik et al., 2020),\"}"}
{"id": "mDw42ZanmE", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nFigure 4. Predictive evaluation results. The average positive activation values (\u201c+\u201d) for MAIA labels outperform MILAN and are comparable to human descriptions for both real and synthetic neurons. Neutral activations (\u201c\u2212\u201d) are comparable across methods.\\n\\nWe use candidate neuron labels to generate new images that should elicit maximally positive activations relative to a neutral baseline. For a given neuron, we generate a pool of image candidates by providing MAIA, MILAN, and human labels to a Prompt Generator model (implemented with a new instance of GPT-4). For each candidate label (e.g. intricate masks), the Prompt Generator is instructed to write 7 image prompts that should generate maximally activating images (e.g. A Venetian mask, A tribal mask,...), and 7 prompts for neutral images (unrelated to the label) that should elicit baseline activations (e.g. A red bus, A field of flowers,...). All positive and neutral prompts from all labeling methods (MAIA, MILAN, and human experts) form a Prompt Dataset of 42 prompts per neuron. Next, we evaluate the accuracy of each candidate label by using a Prompt Selector LM (implemented with another GPT-4 instance) to match that label with the 7 prompts it is most and least likely to entail. We then generate the corresponding images using a text-to-image model (DALL-E3) and measure neuron activation values on those images. If a neuron label is predictive of activations, it will be matched with positive exemplars that maximally activate the neuron relative to the neutral baseline. Combining prompts from all methods into one test set (vs. evaluating each model separately) more rigorously evaluates the completeness of each candidate label: an incomplete description produced by one labeling method (e.g. trains or dogs) could be matched with a \u201cneutral\u201d image prompt describing dogs, which would in fact elicit high activation.\\n\\nThis method primarily discriminates between labeling procedures: whether it is informative depends on the labeling methods themselves producing relevant exemplar prompts. We report the average activation values of positive and neutral exemplars for MAIA, MILAN, and human labels across all tested models in Figure 4. MAIA outperforms MILAN across all models and is often on par with expert predictions. This trend persists across different averaging techniques (such as normalizing by activation percentile, see Appendix C1). While MILAN is a relevant neuron labeling baseline, we note that comparisons to task-specific procedures that use learned models to label a fixed set of exemplars only evaluate part of MAIA\u2019s full functionality. MAIA is easily adaptable to downstream auditing applications that require additional experimentation, where one-shot neuron labeling procedures are insufficient (see Section 5.1). Table A3 provides additional comparisons of MAIA to neuron labeling baselines, and shows evaluation results by layer.\\n\\n4.2. Synthetic neurons\\n\\nFollowing the procedure in Schwettmann et al. (2023) for validating the performance of automated interpretability methods on synthetic test systems mimicking real-world behaviors, we construct a set of synthetic vision neurons with known ground-truth selectivity. We simulate concept detection performed by neurons inside vision models using semantic segmentation. Synthetic neurons are built using an open-set concept detector that combines Grounded DINO (Liu et al., 2023) with SAM (Kirillov et al., 2023) to perform text-guided image segmentation. The ground-truth behavior of each neuron is determined by a text description of the concept(s) the neuron is selective for (Figure 5). To capture real-world behaviors, we derive neuron labels from MILAN-NOTATIONS, a dataset of 60K human annotations of neurons across seven trained vision models (Hernandez et al., 2022).\\n\\nNeurons in the wild display a diversity of behaviors: some respond to individual concepts, while others respond to complex combinations of concepts (Bau et al., 2017; Fong & Vedaldi, 2018; Olah et al., 2020; Mu & Andreas, 2021; Gurnee et al., 2023). We construct three types of synthetic neurons with increasing levels of complexity: monosemantic neurons that recognize single concepts (e.g. stripes), polysemantic neurons selective for logical disjunctions of concepts (e.g. trains OR instruments), and conditional neurons that only recognize a concept in presence of another concept (e.g. dog | leash). Following the instrumentation of real neurons in the MAIA API, synthetic vision neurons accept image input and return a masked image highlighting the concept they are selective for (if present), and an activation...\"}"}
{"id": "mDw42ZanmE", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nFigure 6. MAIA synthetic neuron interpretation.\\n\\nvalue (corresponding to the confidence of Grounded DINO in the presence of the concept). Dataset exemplars for synthetic neurons are calculated by computing 15 top-activating images per neuron from the CC3M dataset (Sharma et al., 2018). Figure 5 shows examples of each type of neuron; the full list of 85 synthetic neurons is provided in Appendix C4.\\n\\nThe set of concepts that can be represented by synthetic neurons is limited to simple concepts by the fidelity of open-set concept detection using current text-guided segmentation methods. We verify that all concepts in the synthetic neuron dataset can be segmented by Grounded DINO in combination with SAM, and provide further discussion of the limitations of synthetic neurons in Appendix C4.\\n\\nMAIA interprets synthetic neurons using the same API and procedure used to interpret neurons in trained vision models (Section 4.1). In contrast to neurons in the wild, we can evaluate descriptions of synthetic neurons directly against ground-truth neuron labels. We collect comparative annotations of synthetic neurons from MILAN, as well as expert Table 1.\\n\\n| Procedure | Human vs. MILAN | Human vs. MAIA | MAIA vs. MILAN |\\n|-----------|----------------|---------------|---------------|\\n| Human     | 0.53 \u00b1 1.4e-3  | 0.83 \u00b1 0.5e-4 | 0.73 \u00b1 4.4e-4 |\\n\\nTable 1 shows the results of the 2AFC study (the proportion of trials in which procedure A was favored over B, and 95% confidence intervals). According to human judges, MAIA labels better agree with ground-truth labels when compared to MILAN, and are even slightly preferred over expert labels on the subset of neurons they described (while human labels are largely preferred over MILAN labels). We also use the predictive evaluation framework described in Section 4.1 to generate positive and neutral sets of exemplar images for all synthetic neurons. Figure 4 shows MAIA descriptions are better predictors of synthetic neuron activations than MILAN descriptions, on par with labels produced by human experts.\\n\\n4.3. Tool ablation study\\n\\nMAIA\u2019s modular design enables straightforward addition and removal of tools from its API. We test three different settings to quantify sensitivity to different tools: (i) labeling neurons using only the dataset_exemplar function without the ability to synthesize images, (ii) relying only on generated inputs without the option to compute maximally activating dataset exemplars, and (iii) replacing the Stable Diffusion text2image backbone with DALL-E 3.\\n\\nWhile the first two settings do not fully compromise performance, neither ablated API achieves the same average accuracy as the full MAIA system (Figure 7). These results emphasize the combined utility of tools for experimenting with real-world and synthetic inputs: MAIA performs best when initializing experiments with dataset exemplars and running additional tests with synthetic images. Methods like MILAN that label precomputed exemplars could thus be incorporated into the MAIA API as tools, and used to initialize experimentation. We also find that using DALL-E as the text2image backbone improves performance (Figure 7). This suggests that the agent is bounded by the performance of its tools rather than its ability to use them\u2014and as interpretability tools grow in sophistication, so will MAIA.\"}"}
{"id": "mDw42ZanmE", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nFigure 7. Ablation study. We use the predictive evaluation protocol to quantify MAIA's sensitivity to different tools. Top performance is achieved when experimenting with both real and synthetic data, and when using DALL-E 3 for image generation. More details in Appendix C2.\\n\\n4.4. MAIA failure modes\\n\\nConsistent with the result in Section 4.3 that MAIA performance improves with DALL-E 3, we additionally observe that SD-v1.5 and InstructPix2Pix sometimes fail to faithfully generate and edit images according to MAIA's instructions. To mitigate these failures, we instruct MAIA to prompt positive image-edits (e.g. replace the bowtie with a plain shirt) rather than negative edits (e.g. remove the bowtie), but occasional failures still occur (see Figure 8 and Appendix D). While proprietary versions of tools may be of higher quality, they also introduce prohibitive rate limits and costs associated with API access. As similar limitations apply to the GPT-4V backbone itself, we tested the performance of free and non-proprietary VLMs as alternative MAIA backbones. Currently, off-the-shelf alternatives still significantly lag behind GPT-4V performance (consistent with evaluation of open-source models' ability to interpret functions in Schwettmann et al. (2023)), but our initial experiments suggest their performance may improve with fine-tuning (see Appendix D3). The MAIA system is designed modularly so that open-source alternatives can be incorporated in the future as their performance improves.\\n\\n5. Applications\\n\\nMAIA is a flexible system that automates model understanding tasks at different levels of granularity: from labeling individual features to diagnosing model-level failure modes. To demonstrate the utility of MAIA for producing actionable insights for human users (Vaughan & Wallach, 2020), we conduct experiments that apply MAIA to two model-level tasks: (i) spurious feature removal and (ii) bias identification in a downstream classification task. In both cases MAIA uses the API as described in Section 3. In an additional experiment, we evaluate the downstream utility of MAIA descriptions by measuring the extent to which they equip humans to make predictions about system behavior (see details in Appendix E).\\n\\nFigure 8. MAIA tool failures. MAIA is limited by the reliability of its tools. Common image editing failure modes (using InstructPix2Pix) include failing to remove objects, misinterpreting the instructions (e.g. removing the incorrect object), and changing too much or too little of the image. MAIA's image generation tool (SD-v1.5) is sometimes unreliable for negative instructions (e.g. a flagpole without a flag), and sometimes deviates from the text prompt by adding or excluding image components.\\n\\n5.1. Removing spurious features\\n\\nLearned spurious features impose a challenge when machine learning models are applied in real-world scenarios, where test distributions differ from training set statistics (Storkey et al., 2009; Beery et al., 2018; Bissoto et al., 2020; Xiao et al., 2020; Singla et al., 2021). We use MAIA to remove learned spurious features inside a classification network, finding that with no access to unbiased examples nor grouping annotations, MAIA can identify and remove such features, improving model robustness under distribution shift by a wide margin, with an accuracy approaching that of fine-tuning on balanced data.\\n\\nWe run experiments on ResNet-18 trained on the Spawrious dataset (Lynch et al., 2023), a synthetically generated dataset involving four dog breeds with different backgrounds. In the train set, each breed is spuriously correlated with a certain background type, while in the test set, the breed-background pairings are changed (see Figure 9). We use MAIA to find a subset of final layer neurons that robustly predict a single dog breed independently of spurious features (see Appendix F3). While other methods like Kirichenko et al. (2023) remove spurious correlations by retraining the last layer on balanced datasets, we only provide MAIA access to top-activating images from the unbalanced validation set and prompt MAIA to run experiments to determine robustness. We then use the features MAIA selects to train an unregularized logistic regression model on the unbalanced data.\"}"}
{"id": "mDw42ZanmE", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nability of the backbone VLM to make and update hypotheses in light of experimental findings. After receiving image outputs of experiments, both LLaV A-Next and Gemini are biased toward describing the images rather than updating their hypothesis about system behavior (suggesting both were trained for the task of image captioning, which might be solvable with fine-tuning).\\n\\nHallucination. We observed that both models often output hypotheses unrelated to experimental outcomes, and sometimes hallucinate experimental results. For example, when Gemini got the activation value and masked image for the prompt \\\"a dog standing on the grass\\\" it replied with: a cat laying on the grass, activation: 14.21, which is an image never generated, and a hallucinated activation value.\\n\\nOverfitting to the examples of the tool usage presented in the MAIA API. Rather than designing new experiments, both models often reproduce code that appears in the MAIA API usage examples. For example, both Llava and Gemini will generate images of \\\"a dog standing on the grass\\\" which is provided as an example for an input prompt to the text2image tool.\\n\\nGemini did show some advantages over LLaV A: while Gemini occasionally produced some syntax errors (e.g. calling system.dataset exemplars instead of tools.dataset exemplars), Llava code was often not executable. Furthermore, LLaVa is restricted to getting only one input image at each interaction, which severely restricts its experimentation ability. Even when inputting several images in a row, the model is biased toward analyzing the last one.\\n\\nE. Utility of MAIA descriptions for human users We run additional crowd-sourcing experiments to quantify the extent to which neuron descriptions equip humans to predict system behavior. Given a text description of a neuron (produced either by MILAN, MAIA or human experts), human participants predicted the neuron's activations on images from the ImageNet validation set. In this setting, a correct description would inform more accurate predictions of behavior.\\n\\nSpecifically, participants saw a language description of a neuron (e.g. \\\"this neuron is selective for human hands interacting with weightlifting equipment\\\") and four sets of images. One of those sets contains images that strongly activate the neuron described in text, and the other 3 contain randomly sampled distractors (capturing \\\"baseline activity\\\"). Humans are asked to use the text description to select the strongly activating set of images (chance is 25%).\\n\\nTwo notes on experimental design: (i) We used sets of images (4 images each) in the multiple choice task instead of single images to provide better coverage of concept space (some text labels describe more concepts than a single image could show). (ii) In this task, humans distinguish strongly activating images from weakly activating images (like the automated evaluation in the main paper), instead of ordering images along a continuum by how strongly they activate the neuron. This is because the explanations produced by the interpretability methods only describe concepts that strongly activate the neuron, and do not provide enough information for human observers to characterize the full distribution of the neuron's activity.\\n\\nThe table below shows results from this experiment and 95% confidence intervals across interpretation procedures for a given model. For each model we evaluated the same subset of 25 neurons as in Section 4. 10 human participants performed each task (using the same crowdworker selection criteria as in Section 4.2).\\n\\n|        | MILAN | MAIA | Human |\\n|--------|-------|------|-------|\\n| ResNet-152 | 66.0 \u00b1 0.06 | 78.5 \u00b1 0.05 | 85.45 \u00b1 0.04 |\\n| CLIP-RN50 | 57.19 \u00b1 0.06 | 65.21 \u00b1 0.06 | 69.6 \u00b1 0.06 |\\n| DINO-ViT | 48.0 \u00b1 0.06 | 67.39 \u00b1 0.06 | 81.2 \u00b1 0.05 |\\n\\nF. Spurious feature removal experiment\\n\\nF1. Dataset Selection\\n\\nWe use the Spawrious dataset as it provides a more complex classification task than simpler binary spurious classification benchmarks like Waterbirds (Wah et al., 2011; Sagawa et al., 2020) and CelebA (Liu et al., 2015; Sagawa et al., 2020). All images in the dataset are generated with Stable Diffusion v1.4 (Rombach et al., 2022b), which is distinct from the Stable...\"}"}
{"id": "mDw42ZanmE", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"F2. Experiment Details\\n\\nHere, we describe the experiment details for each row from Table 2. Note that for all the logistic regression models that we train, we standardize the input features to have a zero mean and variance of one. We use the saga solver from sklearn.linear_model.LogisticRegression for the $\\\\ell_1$ regressions and the lbfgs solver for the unregularized regressions (Pedregosa et al., 2011).\\n\\nAll, Original Model, Unbalanced: We train a ResNet-18 model (He et al., 2016) for one epoch on the O2O-Easy dataset from Spawrious using a learning rate of 1e-4, a weight decay of 1e-4, and a dropout of 0.1 on the final layer. We use a 90-10 split to get a training set of size 22810 and a validation set of size 2534.\\n\\n$\\\\ell_1$ Top 50, All, Unbalanced: We tune the $\\\\ell_1$ regularization parameter on the full unbalanced validation set such that there are 50 neurons with non-zero weights, and we extract the corresponding neurons indices. We then directly evaluate the performance of the logistic regression model on the test set.\\n\\n$\\\\ell_1$ Top 50, Random, Unbalanced: To match MAIA\u2019s sparsity level, we extract 100 sets of 22 random neuron indices and perform unregularized logistic regression on the unbalanced validation set.\\n\\n$\\\\ell_1$ Top 50, $\\\\ell_1$ Top 22, Unbalanced: We also use $\\\\ell_1$ regression to match MAIA\u2019s sparsity in a more principled manner, tuning the $\\\\ell_1$ parameter until there are only 22 neurons with non-zero weights. We then directly evaluate the performance of the regularized logistic regression model on the test set.\\n\\n$\\\\ell_1$ Top 50, MILAN, Unbalanced: We use MILAN to caption all 50 neurons. Since MILAN was not trained to annotate neurons as selective or spurious, we manually select all neurons with captions related to dogs (and not to their backgrounds). After filtering, this set contains 23 neurons. We then perform unregularized logistic regression with this neuron subset on the unbalanced validation set.\\n\\n$\\\\ell_1$ Top 50, MILAN (GPT-4V), Unbalanced: We repeat the same process as in $\\\\ell_1$ Top 50, MILAN, Unbalanced, but this time...\"}"}
{"id": "mDw42ZanmE", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\ntime we use GPT-4V to annotate maximally activating exemplars instead of the original MILAN model. This subset contains 23 neurons. We then perform unregularized logistic regression with this neuron subset on the unbalanced validation set.\\n\\n\\\\[ \\\\ell_1 \\\\]\\n\\nTop 50, MAIA, Unbalanced: We run MAIA on each of the 50 neurons separately, and it ultimately selects 22 out of the 50 neurons. We then perform unregularized logistic regression with this neuron subset on the unbalanced validation set. We use a modified user prompt which we include in Section F4.\\n\\nNext, for the balanced validation fine-tuning experiments, we sample ten balanced validation sets of size 320 and report the mean performances of each method. While Kirichenko et al. (2023) uses multiple subsampled balanced validation sets for fine-tuning and then aggregates the models for scoring on the test set, we only allow the following experiments to see a single balanced validation set since we seek to compare the performance of MAIA to methods which have access to a small balanced fine-tuning dataset, rather than spurious feature labels for every data sample in a large validation set.\\n\\n\\\\[ \\\\ell_1 \\\\] Tuning, Balanced: We perform a hyperparameter search for the \\\\( \\\\ell_1 \\\\) parameter, evaluating each hyperparameter value with five 50-50 splits of the balanced validation dataset, training on one half and evaluating on the other half. We then train the final model with the best \\\\( \\\\ell_1 \\\\) parameter on the entire balanced validation dataset. For the number of neurons, we report the median number of neurons with non-zero weights across the ten trials.\\n\\n\\\\[ \\\\ell_1 \\\\] Top 22, Balanced: We also investigate the performance when we match MAIA's chosen neuron sparsity level by tuning the \\\\( \\\\ell_1 \\\\) parameter to only have 22 neurons with non-zero weights. We train the model directly on the balanced validation dataset.\\n\\nF3. Example MAIA Neuron Robustness Interactions\\n\\nIn Figure A13, we show examples of MAIA's interaction with neurons in the final layer of the Resnet-18 model trained on Spawrious. Overall, we find that MAIA will often test all four dog breeds across different environments, and if it finds that the exemplars all have the same background, it will test to see if the background is a spurious feature.\\n\\nF4. MAIA Prompting Details\\n\\nThe user prompt to MAIA specifying the spurious neuron identification task is similar to the prompt specifying the neuron description task. It is shown in full below. We only specify to MAIA the domain of images the model was trained on and the definitions of spurious and selective neurons so that it can output a final classification using experiments that will not generate significantly out-of-distribution images (such as unrelated dog breeds or close-up poses not present in the training data). We do not ever specify what the spurious correlations are.\"}"}
{"id": "mDw42ZanmE", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are analyzing the prototypical behavior of individual neurons inside a deep neural network which classifies the breed of a dog in a natural image as one of the following breeds that were in its training dataset:\\n\\n1. Labrador\\n2. Welsh Corgi\\n3. Bulldog\\n4. Dachshund\\n\\nYour overall task is to classify the neuron as SELECTIVE (if it is selective for one and only one dog breed) or SPURIOUS (if it is not). Conduct experiments until you meet the following criteria for SELECTIVE or SPURIOUS.\\n\\n**SELECTIVE**\\n\\nCriteria:\\n1. The neuron only activates for one single breed of dog. It does not activate for any other breeds of dog, even at lower magnitudes of activation. We consider a neuron to be activated if it returns a highlighted image region with a red outline. Observe the presence or absence of a mask, and the magnitude of an activation value, to determine whether a neuron is active.\\n2. The neuron is active for that single dog breed robustly across a variety of contexts. However, keep in mind that the neuron has only seen images of full-body dogs in the following environments: beach, desert, muddy park, jungle, and snow. Do not generate images that are very dissimilar to what the neural network has seen before (e.g., do not generate close-ups, do not generate humans). A selective neuron does not need to activate for these dissimilar images.\\n\\n**SPURIOUS**\\n\\nCriteria:\\n1. The neuron activates for multiple breeds of dogs. The amount the neuron activates for each breed of dog may vary but the neuron is considered spurious if it activates enough for multiple breeds to return a highlighted image region with a red outline.\\n2. The neuron may activate for a single breed of dog, but only does so in specific contexts. If the neuron's activation is dependent on non-dog related concepts, it is considered spurious.\\n\\nTo complete your task, you are provided with a library of Python functions to run experiments on the specific neuron (inside the \\\"System\\\" class) given the functions provided in the \\\"Tools\\\" class. Make sure to use a variety of tools from the library to maximize your experimentation power.\\n\\nSome neurons might be selective for very specific concepts, a group of unrelated concepts, or a general concept, so try to be creative in your experiment and try to test both general and specific concepts. If a neuron is selective for multiple concepts, you should describe each of those concepts in your final description. At each experiment step, write Python code that will conduct your experiment on the tested neuron, using the following format:\\n\\n```\\n```python\\ndef execute_command(system, tools):\\n    # gets an object of the system class, an object of the tool class, and performs experiments on the neuron\\n    ...\\n    tools.save_experiment_log(...)\\n```\\n\\nFinish each experiment by documenting it by calling the \\\"save_experiment_log\\\" function. Do not include any additional implementation other than this function. Do not call \\\"execute_command\\\" after defining it. Include only a single instance of experiment implementation at each step. Each time you get the output of the neuron, try to summarize what inputs that activate the neuron have in common (where that description is not influenced by previous hypotheses), and make a hypothesis regarding whether the neuron is SELECTIVE (activates strongly for only one dog breed across contexts) or SPURIOUS (activates for more than one dog breed or non-dog related features). After each experiment, wait to observe the outputs of the neuron. Then your goal is to draw conclusions from the data, update your hypothesis, and write additional experiments to test it. Test the effects of concepts like different dog breeds or background features using the different tools in the library. If you are unsure about the results of the previous experiment you can also rerun it, or rerun a modified version of it with additional tools.\\n\\n**[HYPOTHESIS]**: Update your hypothesis according to the image content and related activation values. Only update your hypotheses if image activation values are higher than previous experiments.\\n\\n**[CODE]**: Test different and specific aspects of your hypothesis using all of the tools in the library. Write code to run the experiment in the same format provided above. Include only a single instance of experiment implementation.\\n\\nContinue running experiments until you meet one of the following stopping criteria.\\n\\n**SPURIOUS**: If you find multiple pieces of evidence which show that the neuron does not only activate for one breed or activates for non-dog related features you should deem the neuron to be spurious, even if you are not entirely sure what the neuron activates for. Remember that we consider a neuron to have activation if it returns a highlighted image region with a red outline. If you see this happen for a feature that is not dog related or for multiple breeds you should deem the neuron to be spurious.\\n\\n**SELECTIVE**: The neuron is selective for a single dog breed and activates strongly for that breed across contexts. If you find any evidence suggesting that a neuron is spurious (such as an image outside one particular breed showing activations), you should conduct more experiments to test your hypotheses. If you are ever unsure about the result, you should lean towards outputting SPURIOUS. The neuron must be perfectly selective across many variations of dog breeds and contexts to be considered to be selective. Once you have met one of the stopping criteria, output your final classification of the neuron in the following format:\\n\\n**[REASONING]**: why the neuron is SELECTIVE or SPURIOUS, and if SELECTIVE, the breed it is SELECTIVE for\\n\\n**[LABEL]**: SELECTIVE/SPURIOUS\"}"}
{"id": "mDw42ZanmE", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G. Bias identification experiment\\n\\nThe user prompt to MAIA specifying the bias identification task is similar to the prompt specifying the neuron description task. We produce it in full below.\\n\\nYour overall task is to describe the visual concepts that maximally activate a neuron inside a deep network for computer vision.\\n\\nYou will get an access to a neuron inside a deep neural network that is sensitive to the presence of {class_label} inside the input image. Your overall task is to reveal biases in the neuron's behavior. That is, your task is to find specific contexts or settings in which the neuron tends to have relatively higher activation values, but only covers partial aspect of the {class_label} class, or specific scenarios when the neuron tends to have relatively lower activations although {class_label} is still present.\\n\\nTry to look for different type of biases (e.g. gender, ethnicity, context-dependencies, breed-dependencies, etc.).\\n\\nTo do that you are provided with a library of Python functions to run experiments on the specific neuron (inside the \u201cSystem\u201d class) given the functions provided in the \u201cTools\u201d class. Make sure to use a variety of tools from the library to maximize your experimentation power. Some neurons might be selective for very specific concepts, a group of unrelated concepts, or a general concept, so try to be creative in your experiment and try to test both general and specific concepts. If a neuron is selective for multiple concepts, you should describe each of those concepts in your final description. At each experiment step, write Python code that will conduct your experiment on the neuron, using the following format:\\n\\n```\\npython\\ndef execute_command(system, tools)\\n# gets an object of the system class, an object of the tool class, and performs experiments on the neuron\\nwith tools:\\n    ...\\n    tools.save_experiment_log(...)\\n```\\n\\nFinish each experiment by documenting it by calling the \u201csave_experiment_log\u201d function. Do not include any additional implementation other than this function. Do not call \u201cexecute_command\u201d after defining it. Include only a single instance of experiment implementation at each step.\\n\\nEach time you get the output of the neuron, try to summarize what inputs that activate the neuron have in common (where that description is not influenced by previous hypotheses). Then, write multiple hypotheses that could explain the biases of the neuron.\\n\\nFor example, these hypotheses could list multiple context that the neuron is less selective for.\\n\\nThen write a list of initial hypotheses about the neuron biases in the format:\\n\\n```\\n[HYPOTHESIS LIST]:\\nHypothesis_1: <hypothesis_1>\\n...\\nHypothesis_n: <hypothesis_n>.\\n```\\n\\nAfter each experiment, wait to observe the outputs of the neuron. Then your goal is to draw conclusions from the data, update your list of hypotheses, and write additional experiments to test them. Test the effects of both local and global differences in images using the different tools in the library. If you are unsure about the results of the previous experiment you can also rerun it, or rerun a modified version of it with additional tools.\\n\\nUse the following format:\\n\\n```\\n[HYPOTHESIS LIST]: ## update your hypothesis list according to the image content and related activation values.\\n[HYPOTHESIS LIST]: ## update your hypothesis list according to the image content and related activation values.\\n[CODE]: ## conduct additional experiments using the provided python library to test *ALL* the hypotheses. Test different and specific aspects of each hypothesis using all of the tools in the library. Write code to run the experiment in the same format provided above. Include only a single instance of experiment implementation.\\n```\\n\\nContinue running experiments until you prove or disprove all of your hypotheses. Only when you are confident in your hypothesis after proving it in multiple experiments, output your final description of the neuron in the following format:\\n\\n```\\n[BIAS]: <final description of the neuron bias>\\n```\"}"}
{"id": "mDw42ZanmE", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nTamar Rott Shaham\\nSarah Schwettmann\\nFranklin Wang\\nAchyuta Rajaram\\nEvan Hernandez\\nJacob Andreas\\nAntonio Torralba\\n\\nAbstract\\n\\nThis paper describes MAIA, a Multimodal Automated Interpretability Agent. MAIA is a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery. It equips a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior. These include tools commonly used by human interpretability researchers: for synthesizing and editing inputs, computing maximally activating exemplars from real-world datasets, and summarizing and describing experimental results. Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior. We evaluate applications of MAIA to computer vision models. We first characterize MAIA\u2019s ability to describe (neuron-level) features in learned representations of images. Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters. We then show that MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified.\\n\\n1. Introduction\\n\\nUnderstanding of a neural model can take many forms. Given an image classifier, for example, we may wish to recognize when and how it relies on sensitive features like race or gender, identify systematic errors in its predictions, or learn how to modify the training data and model architecture to improve accuracy and robustness. Today, this kind of understanding requires significant effort on the part of researchers\u2014involving exploratory data analysis, formulation of hypotheses, and controlled experimentation (Nushi et al., 2018; Zhang et al., 2018). As a consequence, this kind of understanding is slow and expensive to obtain even about the most widely used models. Recent work on automated interpretability (e.g. Hern\u00e1ndez et al., 2022; Bills et al., 2023; Schwettmann et al., 2023) has begun to address some of these limitations by using learned models themselves to assist with model understanding tasks\u2014for example, by assigning natural language descriptions to learned representations, which may then be used to surface features of concern. But current methods are useful almost exclusively as tools for hypothesis generation; they characterize model behavior on a limited set of inputs, and are often low-precision (Huang et al., 2023). How can we build tools that help users understand models, while combining the flexibility of human experimentation with the scalability of automated techniques?\"}"}
{"id": "mDw42ZanmE", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nFigure 2. MAIA experiments for labeling neurons. MAIA iteratively writes programs that compose common interpretability tools to conduct experiments on other systems. At each step, MAIA autonomously makes and updates hypotheses in light of experimental outcomes, showing sophisticated scientific reasoning capabilities. Generated code is executed with a Python interpreter and the outputs (shown above, neuron activation values overlaid in white, masks thresholded at 0.95 percentile of activation maps) are returned to MAIA.\"}"}
{"id": "mDw42ZanmE", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper introduces a prototype system we call the Multimodal Automated Interpretability Agent (MAIA), which combines a pretrained vision-language model backbone with an API containing tools designed for conducting experiments on deep networks. MAIA is prompted with an explanation task (e.g., \\\"describe the behavior of unit 487 in layer 4 of CLIP\\\" or \\\"in which contexts does the model fail to classify labradors?\\\") and designs an interpretability experiment that composes experimental modules to answer the query. MAIA's modular design (Figure 1) enables flexible evaluation of arbitrary systems and straightforward incorporation of new experimental tools. Section 3 describes the current tools in MAIA's API, including modules for synthesizing and editing novel test images, which enable direct hypothesis testing during the interpretation process.\\n\\nWe evaluate MAIA's ability to produce predictive explanations of vision system components using the neuron description paradigm (Bau et al., 2017; 2020; Oikarinen & Weng, 2022; Bills et al., 2023; Singh et al., 2023; Schwettmann et al., 2023) which appears as a subroutine of many interpretability procedures. We additionally introduce a novel dataset of synthetic vision neurons built from an open-set concept detector with ground-truth selectivity specified via text guidance. In Section 4, we show that MAIA descriptions of both synthetic neurons and neurons in the wild are more predictive of behavior than baseline description methods, and in many cases on par with human labels.\\n\\nMAIA also automates model-level interpretation tasks where descriptions of learned representations produce actionable insights about model behavior. We show in a series of experiments that MAIA's iterative experimental approach can be applied to downstream model auditing and editing tasks including spurious feature removal and bias identification in a trained classifier. Both applications demonstrate the adaptability of the MAIA framework across experimental settings: novel end-use cases are described in the user prompt to the agent, which can then use its API to compose programs that conduct task-specific experiments. While these applications show preliminary evidence that procedures like MAIA which automate both experimentation and description have high potential utility to the interpretability workflow, we find that MAIA still requires human steering to avoid common pitfalls including confirmation bias and drawing conclusions from small sample sizes. Fully automating end-to-end interpretation of other systems will not only require more advanced tools, but agents with more advanced capabilities to reason about how to use them.\\n\\n2. Related work\\n\\nInterpreting deep features. Investigating individual neurons inside deep networks reveals a range of human-interpretable features. Approaches to describing these neurons use exemplars of their behavior as explanation, either by visualizing features they select for (Zeiler & Fergus, 2014; Girshick et al., 2014; Karpathy et al., 2015; Mahendran & Vedaldi, 2015; Olah et al., 2017) or automatically categorizing maximally-activating inputs from real-world datasets (Bau et al., 2017; 2020; Oikarinen & Weng, 2022; Dalvi et al., 2019). Early approaches to translating visual exemplars into language descriptions drew labels from fixed vocabularies (Bau et al., 2017), or produced descriptions in the form of programs (Mu & Andreas, 2021).\\n\\nAutomated interpretability. Later work on automated interpretability produced open-ended descriptions of learned features in the form of natural language text, either curated from human labelers (Schwettmann et al., 2021) or generated directly by learned models (Hernandez et al., 2022; Bills et al., 2023; Gandelsman et al., 2024). However, these labels are often unreliable as causal descriptions of model behavior without further experimentation (Huang et al., 2023). Schwettmann et al. (2023) introduced the Automated Interpretability Agent protocol for experimentation on black-box systems using a language model agent, though this agent operated purely on language-based exploration of inputs, which limited its action space. MAIA similarly performs iterative experimentation rather than labeling features in a single pass, but has access to a library of interpretability tools as well as built-in vision capabilities. MAIA's modular design also supports experiments at different levels of granularity, ranging from analysis of individual features to sweeps over entire networks, or identification of more complex network subcomponents (Conmy et al., 2023).\\n\\nLanguage model agents. Modern language models are promising foundation models for interpreting other networks due to their strong reasoning capabilities (OpenAI, 2023a). These capabilities can be expanded by using the LM as an agent, where it is prompted with a high-level goal and has the ability to call external tools such as calculators, search engines, or other models in order to achieve it (Schick et al., 2023; Qin et al., 2023). When additionally prompted to perform chain-of-thought style reasoning between actions, agentic LMs excel at multi-step reasoning tasks in complex environments (Yao et al., 2023).\\n\\nMAIA leverages an agent architecture to generate and test hypotheses about neural networks trained on vision tasks. While ordinary LM agents are generally restricted to tools with textual interfaces, previous work has supported interfacing with the images through code generation (Suri\u00b4es et al., 2023; Wu et al., 2023). More recently, large multimodal LMs like GPT-4V have enabled the use of image-based tools directly (Zheng et al., 2024; Chen et al., 2023). MAIA follows this design and is, to our knowledge, the first multimodal agent equipped with tools for interpreting deep networks.\"}"}
{"id": "mDw42ZanmE", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. **MAIA Framework**\\n\\nMAIA is an agent that autonomously conducts experiments on other systems to explain their behavior, by composing interpretability subroutines into Python programs. Motivated by the promise of using language-only models to complete one-shot visual reasoning tasks by calling external tools (Sur\u00ed et al., 2023; Gupta & Kembhavi, 2023), and the need to perform iterative experiments with both visual and numeric results, we build MAIA from a pretrained multimodal model with the ability to process images directly. MAIA is implemented with a GPT-4V vision-language model (VLM) backbone (OpenAI, 2023b). Given an interpretability query (e.g. Which neurons in Layer 4 are selective for forested backgrounds?), MAIA runs experiments that test specific hypotheses (e.g. computing neuron outputs on images with edited backgrounds), observes experimental outcomes, and updates hypotheses until it can answer the user query.\\n\\nWe enable the VLM to design and run interpretability experiments using the MAIA API, which defines two classes: the System class and the Tools class, described below. The API is provided to the VLM in its system prompt. We include a complete API specification in Appendix A. The full input to the VLM is the API specification followed by a \u201cuser prompt\u201d describing a particular interpretability task, such as explaining the behavior of an individual neuron inside a vision model with natural language (see Section 4).\\n\\nTo complete the task, MAIA uses components of its API to write a series of Python programs that run experiments on the system it is interpreting. MAIA outputs function definitions as strings, which we then execute internally using the Python interpreter. The Pythonic implementation enables flexible incorporation of built-in functions and existing packages, e.g. the MAIA API uses the PyTorch library (Paszke et al., 2019) to load common pretrained vision models.\\n\\n### 3.1. System API\\n\\nThe System class inside the MAIA API instruments the system to be interpreted and makes subcomponents of that system individually callable. For example, to probe single neurons inside ResNet-152 (He et al., 2016), MAIA can use the System class to initialize a neuron object by specifying its number and layer location, and the model that the neuron belongs to:\\n\\n```python\\nsystem = System(unit_id, layer_id, model_name)\\n```\\n\\nMAIA can then design experiments that test the neuron\u2019s activation value on different image inputs by running `system.neuron(image_list)`, to return activation values and masked versions of the images in the list that highlight maximally activating regions (See Figure 2 for examples). While existing approaches to common interpretability tasks such as neuron labeling require training specialized models on task-specific datasets (Hernandez et al., 2022), the MAIA system class supports querying arbitrary vision systems without retraining.\\n\\n### 3.2. Tool API\\n\\nThe Tools class consists of a suite of functions enabling MAIA to write modular programs that test hypotheses about system behavior. MAIA tools are built from common interpretability procedures such as characterizing neuron behavior using real-world images (Bau et al., 2017) and performing causal interventions on image inputs (Hernandez et al., 2022; Casper et al., 2022), which MAIA then composes into more complex experiments (see Figure 2). When programs written by MAIA are compiled internally as Python code, these functions can leverage calls to other pretrained models to compute outputs. For example, `tools.text2image(prompt_list)` returns synthetic images generated by a text-guided diffusion model, using prompts written by MAIA to test a neuron\u2019s response to specific visual concepts. The modular design of the tool library enables straightforward incorporation of new tools as interpretability methods grow in sophistication. For the experiments in this paper we use the following set:\\n\\n- **Dataset exemplar generation.** Previous studies have shown that it is possible to characterize the prototypical behavior of a neuron by recording its activation values over a large dataset of images (Bau et al., 2017; 2020). We give MAIA the ability to run such an experiment on the validation set of ImageNet (Deng et al., 2009) and construct the set of 15 images that maximally activate the system it is interpreting. Interestingly, MAIA often chooses to begin experiments by calling this tool (Figure 2). We analyze the importance of the `dataset_exemplars` tool in our ablation study (4.3).\\n\\n- **Image generation and editing tools.** MAIA is equipped with a `text2image(prompts)` tool that synthesizes images by calling Stable Diffusion v1.5 (Rombach et al., 2022a) on text prompts. Generating inputs enables MAIA to test system sensitivity to fine-grained differences in visual concepts, or test selectivity for the same visual concept across contexts (e.g. the bowtie on a pet and on a child in Figure 2). We analyze the effect of using different text-to-image models in Section 4.3. In addition to synthesizing new images, MAIA can also edit images using Instruct-Pix2Pix (Brooks et al., 2022) by calling `edit_images(image, edit_instructions)`. Generating and editing synthetic images enables hypothesis tests involving images lying outside real-world data distributions, e.g. the addition of antelope horns to a horse (Figure 2, see Causal intervention on image input).\\n\\n- **Image description and summarization tools.** To limit confirmation bias in MAIA\u2019s interpretation of experimental results, we use a multi-agent framework in which MAIA can ask a new instance of GPT-4V with no knowledge of experimental history to describe highlighted image regions in individual images, `describe_images(image_list)`, or summarize what they have in common across a group of...\"}"}
{"id": "mDw42ZanmE", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Table A3 we provide full evaluation results by layer, as well as the number of units evaluated in each layer. Units were sampled uniformly at random, for larger numbers of units in later layers with more interpretable features.\\n\\n### Table A3. Evaluation results by layer\\n\\n| Arch. Layer | # Units | MILAN | CLIP-Dissect | MAIA | Human |\\n|-------------|---------|-------|--------------|------|-------|\\n| ResNet-152  |         |       |              |      |       |\\n| conv. 1     | 10      | 7.23  | 3.38         | 6.7  | 2.71  |\\n| res. 1      | 15      | 0.82  | 0.73         | 0.83 | 0.55  |\\n| res. 2      | 20      | 0.98  | 0.92         | 1.02 | 0.66  |\\n| res. 3      | 25      | 1.28  | 0.72         | 0.98 | 0.68  |\\n| res. 4      | 30      | 5.41  | 2.04         | 7.1  | 1.61  |\\n| Avg.        |         | 2.99  | 1.42         | 3.37 | 1.14  |\\n| DINO-ViT    |         |       |              |      |       |\\n| MLP 1       | 5       | 1.10  | 0.94         | 0.76 | 0.94  |\\n| MLP 3       | 5       | 0.63  | 0.96         | 0.96 | 1.06  |\\n| MLP 5       | 20      | 0.85  | 1.01         | 1.11 | 0.94  |\\n| MLP 7       | 20      | 1.42  | 0.77         | 1.16 | 0.66  |\\n| MLP 9       | 25      | 3.50  | -1.15        | 0.81 | -0.83 |\\n| MLP 11      | 25      | -1.56 | -1.94        | -1.12| -1.65 |\\n| Avg.        |         | 1.03  | -0.32        | 0.44 | -0.20 |\\n| CLIP-RN50   |         |       |              |      |       |\\n| res. 1      | 10      | 1.92  | 2.16         | 2.34 | 1.82  |\\n| res. 2      | 20      | 2.54  | 2.46         | 2.61 | 1.91  |\\n| res. 3      | 30      | 2.24  | 1.70         | 2.33 | 1.45  |\\n| res. 4      | 40      | 3.56  | 1.30         | 4.75 | 1.36  |\\n| Avg.        |         | 2.79  | 1.74         | 3.36 | 1.55  |\\n\\n### C2. Ablation studies\\n\\nWe use the subset of 25% neurons labeled by human experts to perform the ablation studies. Results of the predictive evaluation procedure described in Section 4 are shown below. Using DALL-E 3 improves performance over SD-v1.5.\\n\\n### Table A5. Numerical data for the ablations in Figure 7.\\n\\n| ImageNet | SD-v1.5 | DALL-E 3 |\\n|----------|---------|----------|\\n| ResNet-152 |         |          |\\n| +         | 3.53    | 4.64     |\\n| -         | 1.54    | 1.53     |\\n| DINO-ViT  |         |          |\\n| +         | 1.48    | 1.88     |\\n| -         | -0.37   | -0.23    |\\n| CLIP-RN50 |         |          |\\n| +         | 2.34    | 4.34     |\\n| -         | 1.90    | 1.90     |\\n\\nC1. Predictive evaluation with activation normalizing.\\n\\nAveraging the raw activation values might be largely affected by neurons with a relatively high activation range. Optimally activation values should be reported in percentile, relative to the activation range of each neuron individually, however neuron activation ranges are difficult to be precisely estimated. Instead, we perform the following estimation: we normalized the activation value of each neuron by its 95% percentile activation value (assuming 0% corresponds to an activation value of 0, which is a valid assumption for convolutional networks with ReLU activation like ResNet and CLIP). Results are reported in Table C1, showing the same trends as pre-normalization.\"}"}
{"id": "mDw42ZanmE", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nC3. Human expert neuron description using the MAIA tool library\\n\\nFigure A11. Example interface for humans interpreting neurons with the same tool library used by MAIA.\\n\\nWe recruited 8 human interpretability researchers to use the MAIA API to run experiments on neurons in order to describe their behavior. This data collection effort was approved by MIT's Committee on the Use of Humans as Experimental Subjects. Humans received task specification via the MAIA user prompt, wrote programs using the functions inside the MAIA API, and produced neuron descriptions in the same format as MAIA. All human subjects had knowledge of Python. Humans labeled 25% of the units in each layer labeled by MAIA (one human label per neuron). Testing was administered via JupyterLab (Kluyver et al., 2016), as displayed in Figure A11. Humans also labeled 25% of the synthetic neurons using the same workflow. The median number of interactions per neuron for humans was 7. However, for more difficult neurons the number of interactions were as high as 39.\\n\\nC4. Synthetic neurons\\n\\nTo provide a ground truth against which to test MAIA, we constructed a set of synthetic neurons that reflect the diverse response profiles of neurons in the wild. We used three categories of synthetic neurons with varying levels of complexity: monosemantic neurons that respond to single concepts, polysemantic neurons that respond to logical disjunctions of concepts, and conditional neurons that respond to one concept conditional on the presence of another. The full set of synthetic neurons across all categories is described in Table A6. To capture real-world neuron behaviors, concepts are drawn from MILANNOTATIONS, a dataset of 60K human annotations of prototypical neuron behaviors (Hernandez et al., 2022). Synthetic neurons are constructed using Grounded DINO (Liu et al., 2023) in combination with SAM (Kirillov et al., 2023). Specifically, Grounded-DINO implements open-vocabulary object detection by generating image bounding boxes corresponding to an input text prompt. These bounding boxes are then fed into SAM as a soft prompt, indicating which part of the image to segment. To ensure the textual relevance of the bounding box, we set a threshold to filter out bounding boxes that do not correspond to the input prompt, using similarity scores which are also returned as synthetic neuron activation values. We use the default thresholds of 0.3 for bounding box accuracy and 0.25 for text similarity matching, as recommended in (Liu et al., 2023). After the final segmentation maps are generated, per-object masks are combined and dilated to resemble outputs of neurons inside trained vision models, instrumented via MAIA's System class.\\n\\nWe also implement compound synthetic neurons that mimic polysemantic neurons found in the wild (via logical disjunction), and neurons that respond to complex combinations of concepts (via logical conjunction). To implement polysemantic neurons (e.g. selective for A OR B), we check if at least one concept is present in the input image (if both are present, we merge segmentation maps across concepts and return the mean of the two activation values). To implement conditional neurons (e.g. selective for A | B), we check if A is present, and if the condition is met (B is present) we return the mask and activation value corresponding to concept A.\\n\\nThe set of concepts that can be represented by synthetic neurons is limited by the fidelity of open-set concept detection using current text-guided segmentation methods. We manually verify that all concepts in the synthetic neuron dataset can be consistently segmented by Grounded DINO in combination with SAM. There are some types of neuron behavior, however, that cannot be captured using current text-guided segmentation methods. Some neurons inside trained vision models implement low-level procedures (e.g. edge-detection), or higher level perceptual similarity detection (e.g. sensitivity to radial wheel-and-spoke patterns common to flower petals and bicycle tires) that Grounded DINO + SAM cannot detect. Future implementations could explore whether an end-to-end single model open-vocabulary segmentation system, such as Segment Everything Everywhere All at Once (Zou et al., 2023), could perform segmentation for richer neuron labels.\"}"}
{"id": "mDw42ZanmE", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nTable A6. Synthetic neurons. Concepts are drawn from MILANNOTATIONS.\\n\\n| Monosemantic | Polysemantic | Conditional |\\n|--------------|--------------|-------------|\\n| **Arch** | animal, door | **Ball** | hand | **Bird** |\\n| **Monsoon** | ship, beach | **People** |\\n| **Blue** | baby, dog | **Tree** | **Boat** | **Bird** | **Dog** | **Bridge** | **Sky** |\\n| **Brick** | blue, building | **Sky** |\\n| **Bridge** | bookshelf, building | **Cup** | handle |\\n| **Bug** | cup, road | **Doe** | **Fire** | **Whisker** | **Instrument** | **Hand** |\\n| **Building** | dog, car | **Fence** | **Animal** |\\n| **Button** | dog, horse | **Fish** | **Water** |\\n| **Car** | window | **Dog** | **Instrument** | **Grass** | **Dog** | **Firework** | **Whisker** | **Hand** |\\n| **Circle** | fire | **Fur** | **Grass** |\\n| **Dog** | firework, whisker | **Instrument** | **Hand** |\\n| **Ears** | hand, skyline | **Water** |\\n| **Feathers** | necklace, flower | **Sky** | **Bird** |\\n| **Flame** | people, building | **Snow** | **Road** |\\n| **Frog** | people, wood | **Suit** | **Tie** |\\n| **Grass** | red, purple | **Tent** | **Mountain** |\\n| **Hair** | shoe, boat | **Water** | **Blue** |\\n| **Hands** | sink, pool | **Wheel** | **Racecar** |\\n| **Handle** | skirt, water |\\n| **Hat** | stairs, fruit |\\n| **Jeans** | temple, playground |\\n| **Jigsaw** | truck, train |\\n| **Light** | window, wheel |\\n| **Legs** | **Wheel** |\\n| **Wheel** | **Wheel** |\\n| **Wheel** | **Wheel** |\\n| **Light** | **Wheel** |\\n| **Light** | **Wheel** |\\n| **Light** | **Wheel** |\\n| **Light** | **Wheel** |\\n\\nEvaluation of synthetic neuron labels using human judges. This data collection effort was approved by MIT's Committee on the Use of Humans as Experimental Subjects. To control for quality, workers were required to have a HIT acceptance rate of at least 99%, be based in the U.S., and have at least 10,000 approved HITs. Workers were paid $0.10 per annotation. 10 human judges performed each comparison task.\"}"}
{"id": "mDw42ZanmE", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nD. Failure modes\\n\\nD1. Tool Failures\\n\\nMAIA is often constrained by the capabilities of its tools. As shown in Figure 8, the Instruct-Pix2Pix (Brooks et al., 2022) and Stable Diffusion (Rombach et al., 2022b) models sometimes fail to follow the precise instructions in the caption. Instruct-Pix2Pix typically has trouble making changes which are relative to objects within the image and also fails to make changes that are unusual (such as the example of replacing a person with a vase). Stable Diffusion typically has difficulty assigning attributes in the caption to the correct parts of the image. These errors in image editing and generation sometimes confuse MAIA and cause it to make the wrong prediction.\\n\\nD2. Confirmation Bias\\n\\nIn some scenarios, when MAIA generates an image that has a higher activation than the dataset exemplars, it will assume that the neuron behaves according to that single exemplar. Instead of conducting additional experiments to see if there may be a more general label, MAIA sometimes stops experimenting and outputs a final label that is specific to that one image. For instance, in Figure A12 MAIA generates one underwater image that attains a higher activation and outputs an overly specific description without doing any additional testing.\\n\\nD3. Failure modes of non-proprietary VLMs as alternative MAIA backbones.\\n\\nWe tested the performance of two models as the MAIA backbone VLM:\\n\\n1. LLaVA-Next (Liu et al., 2024), a top-performing (improved over LLaVA-1.5) open source VLM built from LLaMA.\\n2. Gemini 1.0 Pro (Anil et al., 2023), which Google currently provides for free with a low rate limit (60 RPM).\\n\\nWhen used as the MAIA backbone for the neuron interpretation task, both models show initial potential: they understand the task, use the System class and the Tools class from MAIA's API, and write code specifying interpretation experiments. However, while both models show promise, we found significant shortcomings compared to GPT-4V which limit their current usefulness as backbones of MAIA. Some of these shortcomings we observed in both models:\\n\\nWeaker hypothesis generation.\"}"}
{"id": "mDw42ZanmE", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nFigure 9. Spurious dataset examples. Train data contains spurious correlations between dog breeds and their backgrounds.\\n\\nTable 2. Final layer spurious feature removal results.\\n\\n| Subset Selection Method | # Units Balanced Test Acc. |\\n|-------------------------|---------------------------|\\n| All Original Model      | 512 \u2713 0.731               |\\n| \u2113\u2081 Top 50               | 50 \u2713 0.779 \u00b1 0.05         |\\n| Random                  | 22 \u2713 0.705 \u00b1 0.05         |\\n| \u2113\u2081 Top 22               | 22 \u2713 0.757 \u00b1 0.05         |\\n| MILAN                   | 23 \u2713 0.786 \u00b1 0.05         |\\n| MILAN (GPT-4V)          | 23 \u2713 0.690 \u00b1 0.05         |\\n| MAIA                    | 22 \u2713 0.837 \u00b1 0.05         |\\n| All \u2113\u2081 Hyper Tuning     | 147 \u2713 0.830 \u00b1 0.05        |\\n| \u2113\u2081 Top 22               | 22 \u2713 0.865 \u00b1 0.05         |\\n\\nAs a demonstration, we select 50 of the most informative neurons using \u2113\u2081 regularization on the unbalanced dataset and have MAIA run experiments on each one. MAIA selects 22 neurons it deems to be robust. Training an unregularized model on this subset significantly improves accuracy, as reported in Table 2. For comparison, we repeat the same task using interpretability procedures like MILAN that rely on precomputed exemplars (both with the original model of (Hernandez et al., 2022) and with GPT-4V, see Appendix F2 for experimental details). Both achieved significantly lower accuracy. To further show that the sparsity of MAIA\u2019s neuron selection is not the only reason for its performance improvements, we also benchmark MAIA\u2019s performance against \u2113\u2081 regularized fitting on both unbalanced and balanced versions of the dataset. On the unbalanced dataset, \u2113\u2081 drops in performance when subset size reduces from 50 to 22 neurons. Using a small balanced dataset to hyperparameter tune the \u2113\u2081 parameter and train the logistic regression model on all neurons achieves performance comparable to MAIA\u2019s chosen subset, although MAIA did not have access to any balanced data. For a fair comparison, we test the performance of an \u2113\u2081 model which matches the sparsity of MAIA, but trained on the balanced dataset. See Appendix F2 for more details.\\n\\n5.2. Revealing biases\\n\\nMAIA can be used to automatically surface model-level biases. Specifically, we apply MAIA to investigate biases in the outputs of a CNN (ResNet-152) trained on a supervised ImageNet classification task. The MAIA system is easily adaptable to this experiment: the output logit corresponding to a specific class is instrumented using the system class, and returns class probability for input images. MAIA is provided with the class label and instructed (see Appendix G) to find settings in which the classifier ranks images related to that class with relatively lower probability values, or shows a clear preference for a subset of the class. Figure 10 presents results for a subset of ImageNet classes. This simple paradigm suggests that MAIA\u2019s generation of synthetic data could be widely useful for identifying regions of the input distribution where a model exhibits poor performance. While this exploratory experiment surfaces only broad failure categories, MAIA enables other experiments targeted at end-use cases identifying specific biases.\\n\\n6. Conclusion\\n\\nWe introduce MAIA, an agent that automates interpretability tasks including feature interpretation and bias discovery. By composing pretrained modules, MAIA conducts experiments to make and test hypotheses about the behavior of other systems. While human supervision is needed to maximize its effectiveness and catch common mistakes, initial experiments with MAIA show promise, and we anticipate that interpretability agents will be increasingly useful as they grow in sophistication.\"}"}
{"id": "mDw42ZanmE", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nImpact statement\\n\\nAs AI systems take on higher-stakes roles and become more deeply integrated into research and society, scalable approaches to auditing for reliability will be vital. MAIA is a prototype for a tool that can help human users ensure AI systems are transparent, reliable, and equitable. We think MAIA augments, but does not replace, human oversight of AI systems. MAIA still requires human supervision to catch mistakes such as confirmation bias and image generation/editing failures. Absence of evidence (from MAIA) is not evidence of absence: though MAIA\u2019s toolkit enables causal interventions on inputs in order to evaluate system behavior, MAIA\u2019s explanations do not provide formal verification of system performance.\\n\\nAcknowledgements\\n\\nWe are grateful for the support of the MIT-IBM Watson AI Lab, the Open Philanthropy foundation, Hyundai Motor Company, ARL grant W911NF-18-2-021, Intel, the National Science Foundation under grant CCF-2217064, the Zuckerman STEM Leadership Program, and the Viterbi Fellowship. The funders had no role in experimental design or analysis, decision to publish, or preparation of the manuscript. The authors have no competing interests to report.\\n\\nReferences\\n\\nAnil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nBau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A. Network dissection: Quantifying interpretability of deep visual representations. In Computer Vision and Pattern Recognition, 2017.\\n\\nBau, D., Zhu, J.-Y., Strobelt, H., Lapedriza, A., Zhou, B., and Torralba, A. Understanding the role of individual units in a deep neural network. Proceedings of the National Academy of Sciences, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1907375117. URL https://www.pnas.org/content/early/2020/08/31/1907375117.\\n\\nBeery, S., Van Horn, G., and Perona, P. Recognition in terra incognita. In Proceedings of the European conference on computer vision (ECCV), pp. 456\u2013473, 2018.\\n\\nBills, S., Cammarata, N., Mossing, D., Tillman, H., Gao, L., Goh, G., Sutskever, I., Leike, J., Wu, J., and Saunders, W. Language models can explain neurons in language models. https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html, 2023.\\n\\nBissoto, A., Valle, E., and Avila, S. Debiasing skin lesion datasets and models? not so fast. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 740\u2013741, 2020.\\n\\nBrooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022.\\n\\nCaron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9650\u20139660, 2021.\\n\\nCasper, S., Hariharan, K., and Hadfield-Menell, D. Diagnostics for deep neural networks with automated copy/paste attacks. arXiv preprint arXiv:2211.10024, 2022.\\n\\nChen, L., Zhang, Y., Ren, S., Zhao, H., Cai, Z., Wang, Y., Wang, P., Liu, T., and Chang, B. Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4-vision and beyond, 2023.\\n\\nChen, X. and He, K. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 15750\u201315758, 2021.\\n\\nConmy, A., Mavor-Parker, A. N., Lynch, A., Heimersheim, S., and Garriga-Alonso, A. Towards automated circuit discovery for mechanistic interpretability. arXiv preprint arXiv:2304.14997, 2023.\\n\\nDalvi, F., Durrani, N., Sajjad, H., Belinkov, Y., Bau, A., and Glass, J. What is one grain of sand in the desert? analyzing individual neurons in deep nlp models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6309\u20136317, 2019.\\n\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248\u2013255. Ieee, 2009.\\n\\nFong, R. and Vedaldi, A. Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8730\u20138738, 2018.\\n\\nGandelsman, Y., Efros, A. A., and Steinhardt, J. Interpreting clip's image representation via text-based decomposition, 2024.\"}"}
{"id": "mDw42ZanmE", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nGardner, M., Artzi, Y., Basmova, V., Berant, J., Bogin, B., Chen, S., Dasigi, P., Dua, D., Elazar, Y., Gottumukkala, A., Gupta, N., Hajishirzi, H., Ilharco, G., Khashabi, D., Lin, K., Liu, J., Liu, N. F., Mulcaire, P., Ning, Q., Singh, S., Smith, N. A., Subramanian, S., Tsarfaty, R., Wallace, E., Zhang, A., and Zhou, B. Evaluating models\u2019 local decision boundaries via contrast sets, 2020.\\n\\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580\u2013587, 2014.\\n\\nGrill, J.-B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in Neural Information Processing Systems, 33:21271\u201321284, 2020.\\n\\nGupta, T. and Kembhavi, A. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14953\u201314962, 2023.\\n\\nGurnee, W., Nanda, N., Pauly, M., Harvey, K., Trotinskii, D., and Bertsimas, D. Finding neurons in a haystack: Case studies with sparse probing. arXiv preprint arXiv:2305.01610, 2023.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016.\\n\\nHernandez, E., Schwettmann, S., Bau, D., Bagashvili, T., Torralba, A., and Andreas, J. Natural language descriptions of deep visual features. In International Conference on Learning Representations, 2022.\\n\\nHuang, J., Geiger, A., D\u2019Oosterlinck, K., Wu, Z., and Potts, C. Rigorously assessing natural language explanations of neurons. arXiv preprint arXiv:2309.10312, 2023.\\n\\nKarpathy, A., Johnson, J., and Fei-Fei, L. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.\\n\\nKaushik, D., Hovy, E., and Lipton, Z. C. Learning the difference that makes a difference with counterfactually-augmented data, 2020.\\n\\nKirichenko, P., Izmailov, P., and Wilson, A. G. Last layer re-training is sufficient for robustness to spurious correlations, 2023.\\n\\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Doll\u00e1r, P., and Girshick, R. Segment anything. arXiv:2304.02643, 2023.\\n\\nKluyver, T., Ragan-Kelley, B., P\u00e9rez, F., Granger, B., Bussonnier, M., Frederic, J., Kelley, K., Hamrick, J., Grout, J., Corlay, S., Ivanov, P., Avila, D., Abdalla, S., and Willing, C. J. Jupyter notebooks \u2013 a publishing format for reproducible computational workflows. In Loizides, F. and Schmidt, B. (eds.), Positioning and Power in Academic Publishing: Players, Agents and Agendas, pp. 87 \u2013 90. IOS Press, 2016.\\n\\nLiu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., and Lee, Y. J. Llava-next: Improved reasoning, OCR, and world knowledge, January 2024. URL https://llava-vl.github.io/blog/2024-01-30-llava-next/.\\n\\nLiu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023.\\n\\nLiu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild, 2015.\\n\\nLynch, A., Dovonon, G. J.-S., Kaddour, J., and Silva, R. Spurious: A benchmark for fine control of spurious correlation biases, 2023.\\n\\nMahendran, A. and Vedaldi, A. Understanding deep image representations by inverting them. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5188\u20135196, 2015.\\n\\nMu, J. and Andreas, J. Compositional explanations of neurons, 2021.\\n\\nNushi, B., Kamar, E., and Horvitz, E. Towards accountable AI: hybrid human-machine analyses for characterizing system failure. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 6, pp. 126\u2013135, 2018.\\n\\nOikarinen, T. and Weng, T.-W. Clip-dissect: Automatic description of neuron representations in deep vision networks. arXiv preprint arXiv:2204.10965, 2022.\\n\\nOlah, C., Mordvintsev, A., and Schubert, L. Feature visualization. Distill, 2(11):e7, 2017.\\n\\nOlah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. Zoom in: An introduction to circuits. Distill, 5(3):e00024\u2013001, 2020.\\n\\nOpenAI. Gpt-4 technical report, 2023a.\"}"}
{"id": "mDw42ZanmE", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Multimodal Automated Interpretability Agent\\n\\nOpenAI. Gpt-4v(ision) technical work and authors.\\n\\nhttps://openai.com/contributions/\\ngpt-4v, 2023b. Accessed: [insert date of access].\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\\n\\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.\\n\\nQin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., Zhao, S., Hong, L., Tian, R., Xie, R., Zhou, J., Gerstein, M., Li, D., Liu, Z., and Sun, M. Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.\\n\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021.\\n\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684\u201310695, June 2022a.\\n\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models, 2022b.\\n\\nSagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization, 2020.\\n\\nSchick, T., Dwivedi-Yu, J., Dess`\u0131, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools, 2023.\\n\\nSchwettmann, S., Hernandez, E., Bau, D., Klein, S., Andreas, J., and Torralba, A. Toward a visual concept vocabulary for gan latent space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 6804\u20136812, 2021.\\n\\nSchwettmann, S., Shaham, T. R., Materzynska, J., Chowdhury, N., Li, S., Andreas, J., Bau, D., and Torralba, A. Find: A function description benchmark for evaluating interpretability methods, 2023.\\n\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2556\u20132565, 2018.\\n\\nSingh, C., Hsu, A. R., Antonello, R., Jain, S., Huth, A. G., Yu, B., and Gao, J. Explaining black box text modules in natural language with language models, 2023.\\n\\nSingla, S., Nushi, B., Shah, S., Kamar, E., and Horvitz, E. Understanding failures of deep networks via robust feature extraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12853\u201312862, 2021.\\n\\nStorkey, A. et al. When training and test sets are different: characterizing learning transfer. Dataset shift in machine learning, 30(3-28):6, 2009.\\n\\nSur\u00b4\u0131s, D., Menon, S., and V ondrick, C. Vipergpt: Visual inference via python execution for reasoning, 2023.\\n\\nVaughan, J. W. and Wallach, H. A human-centered agenda for intelligible machine learning. Machines We Trust: Getting Along with Artificial Intelligence, 2020.\\n\\nWah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S. The Caltech-UCSD Birds-200-2011 Dataset. Caltech Vision Lab, Jul 2011.\\n\\nWu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N. Visual chatgpt: Talking, drawing and editing with visual foundation models, 2023.\\n\\nXiao, K., Engstrom, L., Ilyas, A., and Madry, A. Noise or signal: The role of image backgrounds in object recognition. arXiv preprint arXiv:2006.09994, 2020.\\n\\nYang, Y., Panagopoulou, A., Zhou, S., Jin, D., Callison-Burch, C., and Yatskar, M. Language in a bottle: Language model guided concept bottlenecks for interpretable image classification, 2023.\\n\\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models, 2023.\\n\\nZeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13, pp. 818\u2013833. Springer, 2014.\\n\\nZhang, J., Wang, Y., Molino, P., Li, L., and Ebert, D. S. Manifold: A model-agnostic framework for interpretation and diagnosis of machine learning models. IEEE\"}"}
