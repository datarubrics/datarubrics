{"id": "qin23b", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nFLOPs on a CPU with 64-bit instruction size (Zhou et al., 2016; Liu et al., 2018b; Li et al., 2019). The compression ratio \\\\( r_c \\\\) and speedup ratio \\\\( r_s \\\\) are\\n\\n\\\\[\\n    r_c = \\\\frac{|M|_{\\\\ell_0}}{32} - \\\\frac{|\\\\hat{M}|_{\\\\ell_0}}{32},\\n\\\\]\\n\\n\\\\[\\n    r_s = \\\\frac{FLOPs_M}{164} - \\\\frac{FLOPs_{\\\\hat{M}}}{164} + \\\\frac{FLOPs_{\\\\hat{M}}}{164},\\n\\\\]\\n\\nwhere \\\\( M \\\\) and \\\\( \\\\hat{M} \\\\) are the number of full-precision parameters that remains in the original and binarized models, respectively, and \\\\( FLOPs_M \\\\) and \\\\( FLOPs_{\\\\hat{M}} \\\\) represent the computation related to these parameters. The overall metric for theoretical complexity is\\n\\n\\\\[\\n    OM_{\\\\text{comp}} = \\\\frac{1}{2} \\\\left( E_2(r_c) + E_2(r_s) \\\\right),\\n\\\\]\\n\\nHardware Inference. As binarization is not widely supported in hardware deployment, only two inference libraries, Larq\u2019s Compute Engine (Geiger & Team, 2020) and JD\u2019s daBNN (Zhang et al., 2019), can deploy and evaluate binarized models on ARM hardware in practice. We focus on ARM CPU inference on mainstream hardware for edge scenarios, such as HUAWEI Kirin, Qualcomm Snapdragon, Apple M1, MediaTek Dimensity, and Raspberry Pi (details in Appendix A.4).\\n\\nFor a given binarization algorithm, we use the savings in storage and inference time under different inference libraries and hardware as evaluation metrics:\\n\\n\\\\[\\n    OM_{\\\\text{infer}} = \\\\frac{1}{2} \\\\left( E_2(T_{\\\\text{infer}}) + E_2(S_{\\\\text{infer}}) \\\\right),\\n\\\\]\\n\\nwhere \\\\( T_{\\\\text{infer}} \\\\) is the inference time and \\\\( S_{\\\\text{infer}} \\\\) is the storage used on different devices.\\n\\n4. BiBench Implementation\\n\\nThis section presents the implementation details, training, and inference pipelines of BiBench.\\n\\nImplementation details. BiBench is implemented using the PyTorch (Paszke et al., 2019) package. The definitions of the binarized operators are contained in individual, separate files, enabling the flexible replacement of the corresponding operator in the original model when evaluating different tasks and architectures. When deployed, well-trained binarized models for a particular binarization algorithm are exported to the Open Neural Network Exchange (ONNX) format (developers, 2021) and provided as input to the appropriate inference libraries (if applicable for the algorithm).\\n\\nTraining and inference pipelines. Hyperparameters: Binarized networks are trained for the same number of epochs as their full-precision counterparts. Inspired by the results in Section 5.2.1, we use the Adam optimizer for all binarized models for well converging. The default initial learning rate is \\\\( 10^{-3} \\\\) (or \\\\( 0.1 \\\\times \\\\) the default learning rate), and the learning rate scheduler is CosineAnnealingLR (Loshchilov & Hutter, 2017).\\n\\nArchitecture: BiBench follows the original architectures of full-precision models, binarizing their convolution, linear, and multiplication units with the selected binarization algorithms. Hardtanh is uniformly used as the activation function to prevent all-one features.\\n\\nPretraining: All binarization algorithms use finetuning. For each one, all binarized models are initialized using the same pre-trained model for the specific neural architecture and learning task to eliminate inconsistency at initialization.\\n\\n5. BiBench Evaluation and Analysis\\n\\nThis section presents and analyzes the evaluation results in BiBench. The main accuracy results are in Table 2, and the efficiency results are in Table 3. Details are in Appendix B.\\n\\n5.1. Accuracy Tracks\\n\\nThe accuracy results for network binarization are presented in Table 2. These results were obtained using the metrics defined in Section 3.1 for each accuracy-related track.\\n\\n5.1.1. Learning Task: Performance Varies Greatly by Algorithms and Modalities\\n\\nWe present the evaluation results of binarization on various learning tasks. In addition to the overall metric \\\\( OM_{\\\\text{task}} \\\\), we also provide the relative accuracy of binarized networks compared to their full-precision counterparts.\\n\\nThe impact of binarized operators is crucial and significant. With fully unified training pipelines and architectures, a substantial variation in performance appears among binarization algorithms across every learning task. For example, the SOTA FDA algorithm exhibits a 21.3% improvement in accuracy on GLUE datasets compared to XNOR++, and the difference is even greater at 33.0% on ShapeNet between XNOR and ReCU. This suggests that binarized operators play a crucial role in the learning task track, and their importance is also confirmed in other tracks.\\n\\nBinarization algorithms vary greatly under different data modalities. When comparing various learning tasks, it is notable that binarized networks suffer a significant drop in accuracy on the language understanding GLUE benchmark but can approach full-precision performance on the ModelNet40 point cloud classification task. This and similar phenomena suggest that the direct transfer of binarization insights across learning tasks is non-trivial.\\n\\nFor overall performance, both ReCU and ReActNet have\"}"}
{"id": "qin23b", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nTable 2: Accuracy benchmark for network binarization. Blue: best in a row. Red: worst in a row.\\n\\n| Track | Metric | Binarization Algorithm |\\n|-------|--------|------------------------|\\n|       | Learning | BNN XNOR DoReFa Bi-Real XNOR++ ReActNet ReCU FDA |\\n|       | Task (%) | CIFAR10 94.54 94.73 95.03 95.61 94.52 95.92 96.72 94.66 |\\n|       | ImageNet | 75.81 77.24 76.61 78.38 75.01 78.64 77.98 78.15 |\\n|       | VOC07 | 76.97 74.61 76.35 80.07 74.41 81.38 81.65 79.02 |\\n|       | COCO17 | 77.94 75.37 78.31 81.62 79.41 83.82 85.66 82.35 |\\n|       | ModelNet40 | 54.19 93.86 93.74 93.23 85.20 92.41 95.07 94.38 |\\n|       | ShapeNet | 48.96 73.62 70.79 68.13 41.16 68.51 40.65 71.16 |\\n|       | GLUE | 49.75 59.63 66.60 69.42 49.33 67.64 50.66 70.61 |\\n|       | SpeechCom. | 75.03 76.93 76.64 82.42 68.65 81.86 76.98 77.90 |\\n|       | OM task | 70.82 78.97 79.82 81.63 72.89 81.81 77.96 81.49 |\\n| Neural Architecture (%) | CNNs | 72.90 83.74 83.86 85.02 78.95 86.20 83.50 86.34 |\\n|       | Transformers | 49.75 59.63 66.60 69.42 49.33 67.64 50.66 70.61 |\\n|       | MLPs | 64.61 85.40 85.19 87.83 76.92 87.13 86.02 86.14 |\\n|       | OM arch | 63.15 77.16 79.01 81.16 69.72 80.82 75.14 81.36 |\\n| Robustness Corruption (%) | CIFAR10-C | 95.26 100.97 81.43 96.56 92.69 94.01 103.29 98.35 |\\n|       | OM corr | 95.26 100.97 81.43 96.56 92.69 94.01 103.29 98.35 |\\n\\nHigh accuracy across various learning tasks. While ReCU performs best on most individual tasks, ReActNet ultimately stands out in the overall metric comparison. Both algorithms apply reparameterization in the forward propagation and gradient approximation in the backward propagation.\\n\\n5.1.2. Neural Architecture: Binarization on Transformers\\n\\nBinarization exhibits a clear advantage on CNN- and MLP-based architectures compared to transformer-based ones. Advanced binarization algorithms can achieve 78%-86% of full-precision accuracy on CNNs, and binarized networks with MLP architectures can even approach full-precision performance (e.g., Bi-Real 87.83%). In contrast, transformer-based architectures suffer significant performance degradation when binarized, and none of the algorithms achieve an overall accuracy higher than 70%.\\n\\nThe main reason for the worse performance of transformer-based architectures is the activation binarization in the attention mechanism. Compared to CNNs/MLPs that mainly use convolution or linear operations on the input data, the transformer-based architectures heavily rely on the attention mechanism, which involves multiplications operation between two binarized activations (between the query and key, attention possibilities, and value) and causes twice as much loss of information in one computation. Moreover, the binarization of activations is determined and cannot be adjusted during training, while the binarized weight can be learned through backward propagation during training. Since the attention mechanism requires more binarization of activation, the training of binarized transformers is more difficult and the inference is more unstable compared to CNNs/MLPs, while the binarized weights of the latter participate in each binarization computation and can be continuously optimized during training. We present more detailed discussions and empirical results in Appendix A.7.\\n\\nThese results indicate that transformer-based architectures, with their unique attention mechanisms, require specialized binarization designs rather than direct binarization. And the overall winner on the architecture track is the FDA algorithm, which performs best on both CNNs and transformers. The evaluation of these two tracks shows that binarization algorithms that use statistical channel-wise scaling factors and custom gradient approximation, such as FDA and ReActNet, have some degree of stability advantage.\\n\\n5.1.3. Corruption Robustness: Binarization exhibits robust to corruption.\\n\\nBinarized networks can approach full-precision level robustness for corruption. Interestingly, binarized networks demonstrate robustness comparable to full-precision counterparts when evaluated for corruption. Evaluation results on the CIFAR10-C dataset reveal that binarized networks perform similarly to full-precision networks on typical 2D image corruption tasks. In some cases, such as ReCU and XNOR-Net, binarized networks outperform their full-precision counterparts. If the same level of robustness to corruption is required, the binarized network version typically requires little additional design or supervision to achieve it. As such, binarized networks generally exhibit similar robustness to corruption as full-precision networks, which appears to be a general property of binarized networks rather than a\"}"}
{"id": "qin23b", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Efficiency benchmark for network binarization. Blue: best in a row. Red: worst in a row.\\n\\n| Track       | Metric          | Binarization Algorithm |\\n|-------------|-----------------|------------------------|\\n|             | Training        | BNN | XNOR | DoReFa | Bi-Real | XNOR++ | ReActNet | FDA |\\n|             | Consumption (% )| 27.28 | 175.53 | 113.40 | 144.59 | 28.66 | 146.06 | 53.33 | 36.62 |\\n|             | Sensitivity     | 82.19 | 71.43 | 76.92 | 45.80 | 68.18 | 45.45 | 58.25 | 20.62 |\\n|             | OM train        | 61.23 | 134.00 | 96.89 | 107.25 | 52.30 | 108.16 | 55.84 | 29.72 |\\n|             | Theoretical     | 12.60 | 12.26 | 12.37 | 12.37 | 12.26 | 12.26 | 12.37 | 12.37 |\\n|             | Complexity      | 13.27 | 13.20 | 13.20 | 13.20 | 13.16 | 13.20 | 13.20 | 13.20 |\\n|             | Speedup         | 12.94 | 12.74 | 12.79 | 12.79 | 12.71 | 12.74 | 12.79 | 12.79 |\\n|             | Compression     | 15.62 | 15.62 | 15.62 | 15.62 | 15.52 | 15.62 | 15.62 | 15.62 |\\n|             | OM infer        | 11.70 | 11.70 | 11.70 | 11.70 | 11.51 | 11.70 | 11.70 | 11.70 |\\n\\nSpecific characteristic of certain algorithms. And our results also suggest that the reason binarized models are robust to data corruption cannot be directly attributed to the smaller model scale, but is more likely to be a unique characteristic of some binarization algorithms (Appendix A.8).\\n\\nWe also analyze that the robustness of BNN to data corruption originates from the high discretization of parameters from the perspective of interpretability. Previous studies pointed out that the robustness of the quantization network is related to both the quantization bucket (range) and the magnitude of the noise (Lin et al., 2018), where the former depends on the bit-width and quantizer and the latter depends on the input noise. Specifically, when the magnitude of the noise is small, the quantization bucket is capable of reducing the errors by eliminating small perturbations; however, when the magnitude of perturbation is larger than a certain threshold, quantization instead amplifies the errors. This fact precisely leads to the natural advantages of the binary network in the presence of natural noise. (1) 1-bit binarization enjoys the largest quantization bucket among all bits quantization. In binarization, the application of the sign function allows the binarizer to be regarded as consisting of a semi-open closed interval \\\\([0, +\\\\infty)\\\\) and an open interval \\\\((-\\\\infty, 0)\\\\) quantization bucket. So binarization is with the largest quantization bucket among all bit-width quantization (most of their quantization buckets are with closed intervals with limited range) and brings the largest noise tolerance to BNNs. (2) Natural data corruption (we evaluated in BiBench) is typically considered to have a smaller magnitude than adversarial noise which is always considered worst-case (Ren et al., 2022). (1) and (2) show that when BNNs encounter corrupted inputs, the high discretization of parameters makes them more robust.\\n\\n5.2. Efficiency Tracks\\n\\nWe analyze the efficiency metrics of training consumption, theoretical complexity, and hardware inference (Table 3).\"}"}
{"id": "qin23b", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nCompare learning scheduler (a) Compare optimizer (b) Compare learning rate (c)\\n\\nFigure 2: Comparisons of accuracy under different training settings.\\n\\n5.2.2. Theoretical Complexity: Different algorithms have similar complexity.\\n\\nThere is a minor difference in theoretical complexity among binarization algorithms. The leading cause of the difference in compression rate is each model's definition of the static scaling factor. For example, BNN does not use any factors and has the highest compression. In terms of theoretical acceleration, the main difference comes from two factors: the reduction in the static scaling factor also improves theoretical speedup, and real-time re-scaling and mean-shifting for activation add additional computation, such as in the case of ReActNet, which reduces the speedup by 0.11 times. In general, the theoretical complexity of each method is similar, with overall metrics in the range of $[12.71, 12.94]$. These results suggest that binarization algorithms should have similar inference efficiency.\\n\\n5.2.3. Hardware Inference: Immense potential on edge devices.\\n\\nOne of the advantages of the hardware inference track is that it provides valuable insights into the practicalities of deploying binarization in real-world settings. This track stands out from other tracks in this regard. Limited inference libraries lead to almost fixed paradigms of binarization deployment. The availability of open-source inference libraries that support the deployment of binarization algorithms on hardware is quite limited. After investigating the existing options, we found that only Larq (Geiger & Team, 2020) and daBNN (Zhang et al., 2019) offer complete deployment pipelines and primarily support deployment on ARM devices. As shown in Table 4, both libraries support channel-wise scaling factors in the floating-point form that must be fused into the Batch Normalization (BN) layer. However, neither of them supports dynamic activation statistics or re-scaling during inference. Larq also includes support for mean-shifting activation with a fixed bias. These limitations in the available inference libraries' deployment capabilities have significantly impacted the practicality of deploying binarization algorithms. For example, the scale factor shape of XNOR++ caused its deployment to fail and XNOR also failed due to its activation re-scaling technique; BNN and DoReFa have different theoretical complexities but have the exact same true computation efficiency when deployed. These constraints have resulted in a situation where the vast majority of binarization methods have almost identical inference performance, with the mean-shifting operation of ReActNet on activation having only a slight impact on efficiency. As a result, binarized models must adhere to fixed deployment paradigms and have almost identical efficiency performance.\\n\\nBorn for the edge: more promising for lower-power edge computing. After evaluating the performance of binarized models on a range of different chips, we found that the average speedup of the binarization algorithm was higher on chips with lower computing power (Figure 3). This counter-intuitive result is likely since higher-performance chips tend to have more acceleration from multi-threading when running floating-point models, leading to a relatively slower speedup of binarized models on these chips. In contrast, binarization technology is particularly effective on edge chips with lower performance and cost. Its extreme compression and acceleration capabilities can enable the deployment of advanced neural networks at the edge. These findings suggest that binarization is well-suited for low-power, cost-sensitive edge devices.\\n\\n5.3. Suggested Paradigm of Binarization Algorithm: Based on our evaluation and analysis, we propose the following paradigm for achieving accurate and efficient network binarization using existing techniques: (1) Soft gradient approximation presents great potential. It improves performance by increasing training rather than deployment cost.\"}"}
{"id": "qin23b", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Deployment capability of different inference libraries on real hardware.\\n\\n| Infer. Lib. Provider | Granularity | Form Flod | BN Act. | Re-scaling Act. | Mean-shifting |\\n|----------------------|-------------|-----------|---------|-----------------|---------------|\\n| Larq                | Channel-wise FP32 | \u221a | \u00d7 | \u221a | |\\n| daBNN JD           | Channel-wise FP32 | \u221a | \u00d7 | \u00d7 | |\\n| BNN                 | N/A          | N/A       | N/A     | N/A             | N/A           |\\n| XNOR                | Channel-wise FP32 | \u221a | \u00d7 | \u00d7 | |\\n| DoReFa              | Channel-wise FP32 | \u221a | \u00d7 | \u00d7 | |\\n| Bi-Real             | Channel-wise FP32 | \u221a | \u00d7 | \u00d7 | |\\n| XNOR++              | Spatial-wise FP32 | \u00d7 | \u00d7 | \u00d7 | |\\n| ReActNet            | Channel-wise FP32 | \u221a | \u00d7 | \u221a | |\\n| ReCU                | Channel-wise FP32 | \u221a | \u00d7 | \u00d7 | |\\n| FDA                 | Channel-wise FP32 | \u221a | \u00d7 | \u00d7 | |\\n\\nFigure 3: The lower the chip's computing power, the higher the inference speedup of deployed binarized models.\\n\\nAnd all accuracy-winning algorithms adopt this technique, i.e., ReActNet, ReCU, and FDA. By further exploring this technique, FDA outperforms previous algorithms on the architecture track, indicating the great potential of the soft gradient approximation technique. (2) Channel-wise scaling factors are currently the optimal option for binarization. The gain from the floating-point scaling factor is demonstrated in accuracy tracks, and deployable consideration limits its form to channel-wise. This means a balanced trade-off between accuracy and efficiency. (3) Pre-binarization parameter redistributing is an optional but beneficial operation that can be implemented as a mean-shifting for weights or activations before binarization. Our findings indicate that this technique can significantly enhance accuracy with little added inference cost, as seen in ReActNet and ReCU.\\n\\nIt is important to note that, despite the insights gained from benchmarking on evaluation tracks, none of the binarization techniques or algorithms work well across all scenarios so far. Further research is needed to overcome the current limitations and mutual restrictions between production and deployment, and to develop binarization algorithms that consider both deployability and efficiency. Additionally, it would be helpful for inference libraries to support more advanced binarized operators. In the future, the focus of binarization research should be on addressing these issues.\\n\\n6. Discussion\\n\\nIn this paper, we propose BiBench, a versatile and comprehensive benchmark toward the fundamentals of network binarization. BiBench covers 8 network binarization algorithms, 9 deep learning datasets (including a corruption one), 13 different neural architectures, 2 deployment libraries, 14 real-world hardware, and various hyperparameter settings. Based on these scopes, we develop evaluation tracks to measure the accuracy under multiple conditions and efficiency when deployed on actual hardware. By benchmark results and analysis, BiBench summarizes an empirically optimized paradigm with several critical considerations for designing accurate and efficient binarization algorithms. BiBench aims to provide a comprehensive and unbiased resource for researchers and practitioners working in model binarization. We hope BiBench can facilitate a fair comparison of algorithms through a systematic investigation with metrics that reflect the fundamental requirements for model binarization and serve as a foundation for applying this technology in broader and more practical scenarios.\\n\\nAcknowledgement\\n\\nSupported by the National Key Research and Development Plan of China (2021ZD0110503), the National Natural Science Foundation of China (No. 62022009), the State Key Laboratory of Software Development Environment (SKLSDE-2022ZX-23), NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), and under the RIE2020 Industry Alignment Fund - Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).\"}"}
{"id": "qin23b", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nReferences\\n\\nAlizadeh, M., Fern\u00e1ndez-Marqu\u00e9s, J., Lane, N. D., and Gal, Y. A systematic study of binary neural networks' optimisation. In ICLR, 2019.\\n\\nAMD. Amd64 architecture programmer's manual. https://developer.arm.com/documentation/ddi0596/2020-12/SIMD-FP-Instructions/, 2022.\\n\\nArm. Arm a64 instruction set architecture. https://www.amd.com/system/files/TechDocs/24594.pdf, 2020.\\n\\nBai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., Liu, Q., Lyu, M. R., and King, I. Binarybert: Pushing the limit of bert quantization. In ACL, 2021.\\n\\nBethge, J., Yang, H., Bornstein, M., and Meinel, C. Back to simplicity: How to train accurate bnns from scratch? arXiv preprint arXiv:1906.08637, 2019.\\n\\nBethge, J., Bartz, C., Yang, H., Chen, Y., and Meinel, C. Meliusnet: Can binary neural networks achieve mobilenet-level accuracy? arXiv preprint arXiv:2001.05936, 2020.\\n\\nBulat, A., Tzimiropoulos, G., and Center, S. A. Xnor-net++: Improved binary neural networks. In BMVC, 2019.\\n\\nBulat, A., Martinez, B., and Tzimiropoulos, G. High-capacity expert binary networks. In ICLR, 2020.\\n\\nChang, A. X., Funkhouser, T., Guibas, L., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\\n\\nChen, H., Wen, Y., Ding, Y., Yang, Z., Guo, Y., and Qin, H. An empirical study of data-free quantization's tuning robustness. In CVPR, 2022.\\n\\nChen, Y., Zhang, Z., and Wang, N. Darkrank: Accelerating deep metric learning via cross sample similarities transfer. In AAAI, 2018.\\n\\nChoi, J., Wang, Z., Venkataramani, S., Chuang, P. I.-J., Srinivasan, V., and Gopalakrishnan, K. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018.\\n\\nChoukroun, Y., Kravchik, E., Yang, F., and Kisilev, P. Low-bit quantization of neural networks for efficient inference. In ICCVW, 2019.\\n\\nCourbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y. Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1. arXiv preprint arXiv:1602.02830, 2016a.\\n\\nCourbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y. Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1. arXiv preprint arXiv:1602.02830, 2016b.\\n\\nCurtis, R. O. and Marshall, D. D. Why quadratic mean diameter? WJAF, 2000.\\n\\nCygert, S. and Czyzewski, A. Robustness in compressed neural networks for object detection. In IJCNN, 2021.\\n\\nDeng, J., Dong, W., Socher, R., Li, L. J., Li, K., and Li, F. F.Imagenet: a large-scale hierarchical image database. In CVPR, 2009.\\n\\nDenton, E. L., Zaremba, W., Bruna, J., LeCun, Y., and Fergus, R. Exploiting linear structure within convolutional networks for efficient evaluation. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D., and Weinberger, K. Q. (eds.), NeurIPS, 2014.\\n\\ndevelopers, O. R. Onnx runtime. https://onnxruntime.ai/, 2021.\\n\\nDiffenderfer, J. and Kailkhura, B. Multi-prize lottery ticket hypothesis: Finding accurate binary neural networks by pruning a randomly weighted network. arXiv preprint arXiv:2103.09377, 2021.\\n\\nDing, X., Hao, T., Tan, J., Liu, J., Han, J., Guo, Y., and Ding, G. Resrep: Lossless cnn pruning via decoupling remembering and forgetting. In CVPR, 2021.\\n\\nDing, Y., Qin, H., Yan, Q., Chai, Z., Liu, J., Wei, X., and Liu, X. Towards accurate post-training quantization for vision transformer. In ACM MM, 2022.\\n\\nEsser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R., and Modha, D. S. Learned step size quantization. In ICLR, 2019.\\n\\nGeiger, L. and Team, P. Larq: An open-source library for training binarized neural networks. Journal of Open Source Software, 2020.\\n\\nGholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., and Keutzer, K. A survey of quantization methods for efficient neural network inference. In LPCV, 2021.\\n\\nGong, R., Liu, X., Jiang, S., Li, T., Hu, P., Lin, J., Yu, F., and Yan, J. Differentiable soft quantization: Bridging full-precision and low-bit neural networks. In IEEE ICCV, 2019.\\n\\nGong, Y., Liu, L., Yang, M., and Bourdev, L. D. Compressing deep convolutional networks using vector quantization. arXiv: Computer Vision and Pattern Recognition, 2014.\"}"}
{"id": "qin23b", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "qin23b", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\nLebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I. V., and Lempitsky, V. S. Speeding-up convolutional neural networks using fine-tuned cp-decomposition. In ICLR, 2015.\\n\\nLi, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H. P. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.\\n\\nLi, Y., Dong, X., and Wang, W. Additive powers-of-two quantization: An efficient non-uniform discretization for neural networks. In ICLR, 2019.\\n\\nLin, J., Gan, C., and Han, S. Defensive quantization: When efficiency meets robustness. In ICLR, 2018.\\n\\nLin, M., Ji, R., Xu, Z., Zhang, B., Wang, Y., Wu, Y., Huang, F., and Lin, C.-W. Rotated binary neural network. NeurIPS, 2020.\\n\\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In ECCV, 2014.\\n\\nLin, X., Zhao, C., and Pan, W. Towards accurate binary convolutional neural network. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), NeurIPS, 2017.\\n\\nLiu, A., Liu, X., Fan, J., Ma, Y., Zhang, A., Xie, H., and Tao, D. Perceptual-sensitive gan for generating adversarial patches. In AAAI, 2019.\\n\\nLiu, A., Liu, X., Yu, H., Zhang, C., Liu, Q., and Tao, D. Training robust deep neural networks via adversarial noise propagation. IEEE TIP, 2021a.\\n\\nLiu, A., Guo, J., Wang, J., Liang, S., Tao, R., Zhou, W., Liu, C., Liu, X., and Tao, D. X-adv: Physical adversarial object attacks against x-ray prohibited item detection. In USENIX Security, 2023.\\n\\nLiu, C., Chen, P., Zhuang, B., Shen, C., Zhang, B., and Ding, W. Sa-bnn: State-aware binary neural network. In AAAI, 2021b.\\n\\nLiu, J., Guo, J., and Xu, D. Apsnet: Toward adaptive point sampling for efficient 3d action recognition. IEEE TIP, 2022a.\\n\\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., and Berg, A. C. Ssd: Single shot multibox detector. In ECCV, 2016.\\n\\nLiu, Z., Wu, B., Luo, W., Yang, X., Liu, W., and Cheng, K.-T. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In ECCV, 2018a.\\n\\nLiu, Z., Wu, B., Luo, W., Yang, X., Liu, W., and Cheng, K.-T. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In ECCV, 2018b.\\n\\nLiu, Z., Shen, Z., Savvides, M., and Cheng, K.-T. Reactnet: Towards precise binary neural network with generalized activation functions. In ECCV, 2020.\\n\\nLiu, Z., Oguz, B., Pappu, A., Xiao, L., Yih, S., Li, M., Krishnamoorthi, R., and Mehdad, Y. Bit: Robustly binarized multi-distilled transformer. In NeurIPS, 2022b.\\n\\nLoshchilov, I. and Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In ICLR, 2017.\\n\\nMa, N., Zhang, X., Zheng, H.-T., and Sun, J. Shufflenet v2: Practical guidelines for efficient cnn architecture design. In ECCV, 2018.\\n\\nMartinez, B., Yang, J., Bulat, A., and Tzimiropoulos, G. Training binary neural networks with real-to-binary convolutions. In ICLR, 2019.\\n\\nMediaTek. Mediatek 5g, 2022. URL https://i.mediatek.com/mediatek-5g.\\n\\nNVIDIA. Tensorrt: A c++ library for high performance inference on nvidia gpus and deep learning accelerators, 2022. URL https://github.com/NVIDIA/TensorRT.\\n\\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019.\\n\\nQi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, 2017.\\n\\nQin, H. Hardware-friendly deep learning by network quantization and binarization. arXiv preprint arXiv:2112.00737, 2021.\\n\\nQin, H., Cai, Z., Zhang, M., Ding, Y., Zhao, H., Yi, S., Liu, X., and Su, H. Bipointnet: Binary neural network for point clouds. In ICLR, 2020a.\\n\\nQin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., and Song, J. Forward and backward information retention for accurate binary neural networks. In CVPR, 2020b.\\n\\nQin, H., Ding, Y., Zhang, M., Qinghua, Y., Liu, A., Dang, Q., Liu, Z., and Liu, X. Bibert: Accurate fully binarized bert. In ICLR, 2021.\\n\\nQin, H., Ma, X., Ding, Y., Li, X., Zhang, Y., Tian, Y., Ma, Z., Luo, J., and Liu, X. Bifsmn: Binary neural network for keyword spotting. In IJCAI, 2022a.\"}"}
{"id": "qin23b", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nHaotong Qin\\nMingyuan Zhang\\nYifu Ding\\nAoyu Li\\nZhongang Cai\\nZiwei Liu\\nFisher Yu\\nXianglong Liu\\n\\nAbstract\\n\\nNetwork binarization emerges as one of the most promising compression approaches offering extraordinary computation and memory savings by minimizing the bit-width. However, recent research has shown that applying existing binarization algorithms to diverse tasks, architectures, and hardware in realistic scenarios is still not straightforward. Common challenges of binarization, such as accuracy degradation and efficiency limitation, suggest that its attributes are not fully understood. To close this gap, we present BiBench, a rigorously designed benchmark with in-depth analysis for network binarization. We first carefully scrutinize the requirements of binarization in the actual production and define evaluation tracks and metrics for a comprehensive and fair investigation. Then, we evaluate and analyze a series of milestone binarization algorithms that function at the operator level and with extensive influence. Our benchmark reveals that 1) the binarized operator has a crucial impact on the performance and deployability of binarized networks; 2) the accuracy of binarization varies significantly across different learning tasks and neural architectures; 3) binarization has demonstrated promising efficiency potential on edge devices despite the limited hardware support. The results and analysis also lead to a promising paradigm for accurate and efficient binarization. We believe that BiBench will contribute to the broader adoption of binarization and serve as a foundation for future research. The code for our BiBench is released here.\\n\\n1. Introduction\\n\\nThe rising of deep learning leads to the persistent contradiction between larger models and the limitations of deployment resources. Compression technologies have been widely studied to address this issue, including quantization (Gong et al., 2014; Wu et al., 2016; Vanhoucke et al., 2011; Gupta et al., 2015), pruning (Han et al., 2015; 2016; He et al., 2017), distillation (Hinton et al., 2015; Xu et al., 2018; Chen et al., 2018; Yim et al., 2017; Zagoruyko & Komodakis, 2017), lightweight architecture design (Howard et al., 2017; Sandler et al., 2018; Zhang et al., 2018b; Ma et al., 2018), and low-rank decomposition (Denton et al., 2014; Lebedev et al., 2015; Jaderberg et al., 2014; Lebedev & Lempitsky, 2016). These technologies are essential for the practical application of deep learning.\\n\\nAs a compression approach that reduces the bit-width to 1-bit, network binarization is regarded as the most aggressive quantization technology (Rusci et al., 2020; Choukroun et al., 2019; Qin et al., 2022a; Shang et al., 2022b; Zhang et al., 2022b; Bethge et al., 2020; 2019; Martinez et al., 2019; Helwegen et al., 2019). The binarized models take little storage and memory and accelerate the inference by efficient bitwise operations. Compared to other compression technologies like pruning and architecture design, network binarization has potent topological generics, as it only applies to parameters. As a result, it is widely studied in academic research as a standalone compression technique rather than just a 1-bit specialization of quantization (Gong et al., 2019; Gholami et al., 2021). Some state-of-the-art (SOTA) binarization algorithms have even achieved full-precision performance with binarized models on large-scale tasks (Deng et al., 2009; Liu et al., 2020).\\n\\nHowever, existing network binarization is still far from practical, and two worrisome trends appear in current research: Trend-1: Accuracy comparison scope is limited. In recent research, several image classification tasks (such as CIFAR-10 and ImageNet) have become standard options for comparing accuracy among different binarization algorithms. While this helps to clearly and fairly compare accuracy performance, it causes most binarization algorithms to be only engineered for image inputs (2D visual modality), and their insights and conclusions are rarely verified in other modalities and tasks. The use of monotonic tasks also hinders a comprehensive evaluation from an architectural perspective. Furthermore, data noise, such as corruption (Hendrycks & Dietterich, 2018), is a common problem on low-cost edge devices and is widely studied in compression, but this situation is hardly considered existing binarization algorithms.\\n\\n1\"}"}
{"id": "qin23b", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nEvaluation Tracks for Network Binarization\\n\\nHyperparameter sensitivity\\nTraining time\\n\\nCNN-based\\nTransformer-based\\nMLP-based\\n\\nRobustness\\nCorruption\\nLearning Task\\nTheoretical Complexity\\nNeural Architecture\\nHardware\\nInference\\nTrain\\nConsumption\\nAccuracy\\nEfficiency\\n\\nImage classification\\nPoint clouds segmentation\\nLanguage understanding\\nSpeech recognition\\n...\\n\\nFigure 1: Evaluation tracks of BiBench. Our benchmark evaluates the performance of binarization algorithms on a range of comprehensive evaluation tracks, including: \u201cLearning Task\u201d, \u201cNeural Architecture\u201d, \u201cCorruption Robustness\u201d, \u201cTraining Consumption\u201d, \u201cTheoretical Complexity\u201d, and \u201cHardware Inference\u201d.\\n\\nTrend-2: Efficiency analysis remains theoretical.\\n\\nNetwork binarization is widely recognized for its significant storage and computation savings, with theoretical savings of up to $32 \\\\times$ and $64 \\\\times$ for convolutions, respectively (Rastegari et al., 2016; Bai et al., 2021). However, these efficiency claims lack experimental evidence due to the lack of hardware library support for deploying binarized models on real-world hardware. Additionally, the training efficiency of binarization algorithms is often ignored in current research, leading to negative phenomena during the training of binary networks, such as increased demand for computation resources and time consumption, sensitivity to hyperparameter settings, and the need for detailed optimization tuning.\\n\\nThis paper presents BiBench, a network binarization benchmark designed to evaluate binarization algorithms comprehensively in terms of accuracy and efficiency (Table 1). Using BiBench, we select 8 representative binarization algorithms that are extensively influential and function at the operator level (details and selection criteria are in Appendix A.1) and benchmark algorithms on 9 deep learning datasets, 13 neural architectures, 2 deployment libraries, 14 hardware chips, and various hyperparameter settings. We invested approximately 4 GPU years of computation time in creating BiBench, intending to promote a comprehensive evaluation of network binarization from both accuracy and efficiency perspectives. We also provide in-depth analysis of the benchmark results, uncovering insights and offering suggestions for designing practical binarization algorithms.\\n\\nWe emphasize that our BiBench includes the following significant contributions: (1) The first systematic benchmark enables a new view to quantitatively evaluate binarization algorithms at the operator level. BiBench is the first effort to facilitate systematic and comprehensive comparisons between binarized algorithms. It provides a brand new perspective to decouple the binarized operators from the neural architectures for quantitative evaluations at the generic operator level. (2) Revealing a practical binarized operator design paradigm. BiBench reveals a practical paradigm of binarized operator designing. Based on the systemic and quantitative evaluation, superior techniques for more satisfactory binarization operators can emerge, which is essential for pushing binarization algorithms to be accurate and efficient. A more detailed discussion is in Appendix A.5.\\n\\n2. Background\\n\\n2.1. Network Binarization\\n\\nBinarization compresses weights $w \\\\in \\\\mathbb{R}^{c_{in} \\\\times c_{out}} \\\\times k \\\\times k$ and activations $a \\\\in \\\\mathbb{R}^{c_{in} \\\\times w \\\\times h}$ to 1-bit in computationally dense convolution, where $c_{in}$, $k$, $c_{out}$, $w$, and $h$ denote the input channel, kernel size, output channel, input width, and input height. The computation can be expressed as\\n\\n$$o = \\\\alpha \\\\, \\\\text{popcount} \\\\, (\\\\text{xnor} \\\\, (\\\\text{sign} (a), \\\\text{sign} (w))),$$\\n\\nwhere $o$ denotes the outputs and $\\\\alpha \\\\in \\\\mathbb{R}^{c_{out}}$ denotes the optional scaling factor calculated as $\\\\alpha = \\\\|w\\\\|_n$ (Courbariaux et al., 2016b; Rastegari et al., 2016), $\\\\text{xnor}$ and $\\\\text{popcount}$ are bitwise instructions defined as (Arm, 2020; AMD, 2022). The $\\\\text{popcount}$ counts the number of bits with the \u201cone\u201d value in the input vector and writes the result to the targeted register. While binarized parameters of networks offer significant compression and acceleration benefits, their limited representation can lead to decreased accuracy. Various algorithms have been proposed to address this issue to improve the accuracy of binarized networks (Yuan & Agaian, 2021). The majority of binarization algorithms aim to improve binarized operators (as Eq. (1) shows), which play a crucial role in the optimization and hardware efficiency of binarized models (Alizadeh et al., 2019; Geiger & Team, 2020). These operator improvements are also flexible across different neural architectures and learning tasks, demonstrating the generalizability of bit-width compression (Wang et al., 2020b; Qin et al., 2022a; Zhao et al., 2022a). Our BiBench considers 8 extensively influential binarization algorithms that...\"}"}
{"id": "qin23b", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison between BiBench and existing binarization works along evaluation tracks.\\n\\n| Algorithm          | Technique | Accurate | Robust | Train | Infer |\\n|--------------------|-----------|----------|--------|-------|-------|\\n| BNN (Courbariaux et al., 2016b) | \u221a | \u00d7 | \u221a | #Task | #Arch |\\n| XNOR (Rastegari et al., 2016) | \u221a | \u00d7 | \u00d7 | #Task | #Arch |\\n| DoReFa (Zhou et al., 2016) | \u221a | \u00d7 | \u00d7 | #Task | #Arch |\\n| Bi-Real (Liu et al., 2018b) | \u00d7 | \u00d7 | \u221a | #Task | #Arch |\\n| XNOR++ (Bulat et al., 2019) | \u221a | \u00d7 | \u00d7 | #Task | #Arch |\\n| ReActNet (Liu et al., 2020) | \u00d7 | \u221a | \u00d7 | #Task | #Arch |\\n| ReCU (Xu et al., 2021b) | \u00d7 | \u221a | \u221a | #Task | #Arch |\\n| FDA (Xu et al., 2021a) | \u00d7 | \u00d7 | \u221a | #Task | #Arch |\\n| **Our Benchmark (BiBench)** | \u221a | \u221a | \u221a | #Task | #Arch |\\n\\n\\\"\u221a\\\" and \\\"\u00d7\\\" indicates the track is considered in the original binarization algorithm, while \\\"*\\\" indicates only being studied in other related studies. \\\"s\\\", \\\"\u03c4\\\", or \\\"g\\\" indicates \\\"scaling factor\\\", \\\"parameter redistribution\\\", or \\\"gradient approximation\\\" techniques proposed in this work, respectively. And we also present a more detailed list of these techniques (Table 6) for binarization algorithms in Appendix A.1.\\n\\nFocus on improving operators and can be broadly classified into three categories: scaling factors, parameter redistribution, and gradient approximation (Courbariaux et al., 2016b; Rastegari et al., 2016; Zhou et al., 2016; Liu et al., 2018b; Bulat et al., 2019; Liu et al., 2020; Xu et al., 2021b;a). Note that for selected binarization algorithms, the techniques requiring specified local structures or training pipelines are excluded for fairness, i.e., the bi-real shortcut of Bi-Real (Liu et al., 2018a) and duplicate activation of ReActNet (Liu et al., 2020) in CNN neural architectures. See Appendix A.1 for more information on the algorithms in our BiBench.\\n\\n2.2. Challenges for Binarization\\n\\nSince around 2015, network binarization has garnered significant attention in various fields of research, including but not limited to vision and language understanding. However, several challenges still arise during the production and deployment of binarized networks in practice. The goal of binarization production is to train accurate binarized networks that are resource-efficient. Some recent studies have demonstrated that the performance of binarization algorithms on image classification tasks may not always generalize to other learning tasks and neural architectures (Qin et al., 2020a; Wang et al., 2020b; Qin et al., 2021; Liu et al., 2022b). In order to achieve higher accuracy, some binarization algorithms may require several times more training resources compared to full-precision networks. Ideally, binarized networks should be hardware-friendly and robust when deployed on edge devices. However, most mainstream inference libraries do not currently support the deployment of binarized networks on hardware (NVIDIA, 2022; HUAWEI, 2022; Qualcomm, 2022), which limits the performance of existing binarization algorithms in practice. In addition, the data collected by low-cost devices in natural edge scenarios is often of low quality and even be corrupted, which can negatively impact the robustness of binarized models (Lin et al., 2018; Ye et al., 2019; Cygert & Czyzewski, 2021). However, most existing binarization algorithms do not consider corruption robustness when designed.\\n\\n3. BiBench: Tracks and Metrics\\n\\nIn this section, we present BiBench, a benchmark for accurate and efficient network binarization. Our evaluation consists of 6 tracks and corresponding metrics, as shown in Figure 1, which address the practical challenges of producing and deploying binarized networks. Higher scores on these metrics indicate better performance.\\n\\n3.1. Towards Accurate Binarization\\n\\nIn our BiBench, the evaluation tracks for accurate binarization are \\\"Learning Task\\\", \\\"Neural Architecture\\\" (for production), and \\\"Corruption Robustness\\\" (for deployment).\u2460 Learning Task. We comprehensively evaluate network binarization algorithms using 9 learning tasks across 4 different data modalities. For the widely-evaluated 2D visual modality tasks, we include image classification on CIFAR-10 (Krizhevsky et al., 2014) and ImageNet (Krizhevsky et al., 2012) datasets, as well as object detection on PASCAL VOC (Hoiem et al., 2009) and COCO (Lin et al., 2014) datasets. In the 3D visual modality tasks, we evaluate the algorithm on ModelNet40 classification (Wu et al., 2015) and ShapeNet segmentation (Chang et al., 2015) datasets of 3D point clouds. For the textual modality tasks, we use the natural language understanding in the GLUE benchmark (Wang et al., 2018a). For the speech modality tasks,\"}"}
{"id": "qin23b", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nWe evaluate the algorithms on the Speech Commands KWS dataset (Warden, 2018). See Appendix A.2 for more details on the tasks and datasets.\\n\\nTo evaluate the performance of a binarization algorithm on this track, we use the accuracy of full-precision models as a baseline and calculate the mean relative accuracy for all architectures on each task. The Overall Metric (OM) for this track is then calculated as the quadratic mean of the relative accuracies across all tasks (Curtis & Marshall, 2000). The equation for this evaluation metric is as follows:\\n\\n\\\\[ OM_{task} = \\\\sqrt{\\\\frac{1}{N} \\\\sum_{i=1}^{N} E \\\\left( A_{bi}^{task} / A_{task}^{i} \\\\right)^2} \\\\]\\n\\nwhere \\\\( A_{bi}^{task} \\\\) and \\\\( A_{task}^{i} \\\\) denote the accuracy of the binarized and full-precision models on the \\\\( i \\\\)-th task, respectively, \\\\( N \\\\) is the number of tasks, and \\\\( E(\\\\cdot) \\\\) is the mean operation.\\n\\nNote that the quadratic mean form is used uniformly in BiBench to unify all overall metrics of tracks, which helps prevent certain poor performers from disproportionately influencing the metric and allows for a more accurate measure of overall performance on each track.\\n\\n\u2461 Neural Architecture. We evaluate various neural architectures, including mainstream CNN-based, transformer-based, and MLP-based architectures, to assess the generalizability of binarization algorithms from the perspective of neural architecture. Specifically, we use standard ResNet-18/20/34 (He et al., 2016) and VGG (Simonyan & Zisserman, 2015) to evaluate CNN architectures, and apply the Faster-RCNN (Ren et al., 2015) and SSD300 (Liu et al., 2016) frameworks as detectors. To evaluate transformer-based architectures, we binarize BERT-Tiny4/Tiny6/Base (Kenton & Toutanova, 2019) with the bi-attention mechanism (Qin et al., 2021) for convergence. We also evaluate MLP-based architectures, including PointNet vanilla and PointNet (Qi et al., 2017) with EMA aggregator (Qin et al., 2020a), FSMN (Zhang et al., 2015), and Deep-FSMN (Zhang et al., 2018a), due to their linear unit composition. Detailed descriptions of these architectures can be found in Appendix A.3.\\n\\nSimilar to the overall metric for learning task track, we build the overall metric for neural architecture track:\\n\\n\\\\[ OM_{arch} = \\\\sqrt{\\\\left( \\\\sum_{i} E \\\\left( A_{bi}^{CNN} / A_{CNN}^{i} \\\\right)^2 \\\\right) + \\\\left( \\\\sum_{i} E \\\\left( A_{bi}^{Transformer} / A_{Transformer}^{i} \\\\right)^2 \\\\right) + \\\\left( \\\\sum_{i} E \\\\left( A_{bi}^{MLP} / A_{MLP}^{i} \\\\right)^2 \\\\right)} \\\\]\\n\\n\u2462 Corruption Robustness. The corruption robustness of binarization on deployment is important to handle scenarios such as perceptual device damage, a common issue with low-cost equipment in real-world implementations. To assess the robustness of binarized models to corruption of 2D visual data, we evaluate algorithms on the CIFAR10-C (Hendrycks & Dietterich, 2018) benchmark. Therefore, we evaluate the performance of binarization algorithms on corrupted data compared to normal data using the corruption generalization gap (Zhang et al., 2022a):\\n\\n\\\\[ G_{task}^{i} = A_{norm}^{task} - A_{corr}^{task} \\\\]\\n\\nwhere \\\\( A_{corr}^{task} \\\\) and \\\\( A_{norm}^{task} \\\\) denote the accuracy results under all architectures on the \\\\( i \\\\)-th corruption task and corresponding normal task, respectively. The overall metric on this track is then calculated by\\n\\n\\\\[ OM_{robust} = \\\\sqrt{\\\\frac{1}{C} \\\\sum_{i=1}^{C} E \\\\left( G_{task}^{i} / G_{bi}^{task} \\\\right)^2} \\\\]\\n\\n3.2. Towards Efficient Binarization\\n\\nWe evaluate the efficiency of network binarization in terms of \u201cTraining Consumption\u201d for production, \u201cTheoretical Complexity\u201d and \u201cHardware Inference\u201d for deployment.\\n\\n\u2463 Training Consumption. We consider the occupied training resource and the hyperparameter sensitivity of binarization algorithms, which impact the consumption of a single training and the overall tuning process. To evaluate the ease of tuning binarization algorithms to optimal performance, we train their binarized networks with various hyperparameter settings, including different learning rates, learning rate schedulers, optimizers, and even random seeds. We align the epochs for binarized and full-precision networks and compare their consumption and time.\\n\\nThe metric used to evaluate the training consumption track is based on both training time and sensitivity to hyperparameters. For one binarization algorithm, we have\\n\\n\\\\[ OM_{train} = \\\\sqrt{\\\\frac{1}{2} \\\\left( \\\\sum_{i} E(T_{train} / T_{bi}^{train})^2 \\\\right) + \\\\frac{1}{\\\\text{std}(A_{hyper})^2} \\\\text{std}(A_{bi_{hyper}})^2} \\\\]\\n\\nwhere the set \\\\( T_{train} \\\\) represents the time spent on a single training instance, \\\\( A_{hyper} \\\\) is the set of results obtained using different hyperparameter configurations, and \\\\( \\\\text{std}(\\\\cdot) \\\\) calculates standard deviation values.\\n\\n\u2464 Theoretical Complexity. To evaluate complexity, we compute the compression and speedup ratios before and after binarization on architectures such as ResNet18. The evaluation metric is based on model size (MB) and computational floating-point operations (FLOPs) savings during inference. Binarized parameters occupy 1/32 the storage of their 32-bit floating-point counterparts (Rastegari et al., 2016). Binarized operations, where a 1-bit weight is multiplied by a 1-bit activation, take approximately 1*1/64\"}"}
{"id": "qin23b", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: The considered operator-level binarization algorithms and our final selections in BiBench. Bold means that the algorithm has an advantage in that column.\\n\\n| Algorithm          | Year | Conference | Citation | Open Source | Specified Structure/Training-pipeline |\\n|--------------------|------|------------|----------|-------------|-----------------------------------------|\\n| BitwiseNN (Kim & Smaragdis, 2016) | 2016 | ICMLW      | 274      | Yes         | No                                      |\\n| DoReFa             | 2016 | ArXiv      | 1831     | Yes         | Yes                                     |\\n| XNOR-Net (Rastegari et al., 2016) | 2016 | ECCV       | 4474     | Yes         | Yes                                     |\\n| BNN (Courbariaux et al., 2016a) | 2016 | NeurIPS    | 2804     | Yes         | Yes                                     |\\n| LBCNN (Juefei-Xu et al., 2017) | 2017 | CVPR       | 257      | Yes         | Yes                                     |\\n| LAB (Hou et al., 2017) | 2017 | ICLR       | 204      | Yes         | Yes                                     |\\n| ABC-Net (Lin et al., 2017) | 2017 | NeurIPS    | 599      | Yes         | Yes                                     |\\n| DBF (Tseng et al., 2018) | 2018 | IJCAI      | 10       | Yes         | No                                      |\\n| MCNs (Wang et al., 2018b) | 2018 | CVPR       | 30       | Yes         | No                                      |\\n| SBDs (Hu et al., 2018) | 2018 | ECCV       | 93       | Yes         | No                                      |\\n| Bi-Real Net (Liu et al., 2018a) | 2018 | ECCV       | 412      | Yes         | Opt                                     |\\n| PCNN (Gu et al., 2019) | 2019 | AAAI       | 68       | Yes         | No                                      |\\n| CI-BCNN (Wang et al., 2019) | 2019 | CVPR       | 90       | Yes         | Yes                                     |\\n| XNOR-Net++ (Bulat et al., 2019) | 2019 | BMVC       | 131      | Yes         | No                                      |\\n| ProxyBNN (He et al., 2020) | 2020 | ECCV       | 16       | Yes         | No                                      |\\n| Si-BNN (Wang et al., 2020a) | 2020 | AAAI       | 28       | Yes         | No                                      |\\n| EBNN (Bulat et al., 2020) | 2020 | ICLR       | 38       | Yes         | Yes                                     |\\n| RBNN (Lin et al., 2020) | 2020 | NeurIPS    | 79       | Yes         | No                                      |\\n| ReActNet (Liu et al., 2020) | 2020 | ECCV       | 182      | Yes         | Opt                                     |\\n| SA-BNN (Liu et al., 2021b) | 2021 | AAAI       | 7        | Yes         | No                                      |\\n| $S_2$-BNN (Shen et al., 2021) | 2021 | CVPR       | 11       | Yes         | Yes                                     |\\n| MPT (Diffenderfer & Kailkhura, 2021) | 2021 | ICLR       | 43       | Yes         | Yes                                     |\\n| FDA (Xu et al., 2021a) | 2021 | NeurIPS    | 18       | Yes         | No                                      |\\n| ReCU (Xu et al., 2021b) | 2021 | ICCV       | 27       | Yes         | No                                      |\\n| LCR-BNN (Shang et al., 2022a) | 2022 | ECCV       | 1        | Yes         | Yes                                     |\\n| PokeBNN (Zhang et al., 2022b) | 2022 | CVPR       | 6        | Yes         | Yes                                     |\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{sign}(x) &= \\\\begin{cases} \\n1 & \\\\text{if } x \\\\geq 0 \\\\\\\\\\n-1 & \\\\text{otherwise}\\n\\\\end{cases} \\\\\\\\\\n\\\\text{g}_x &= \\\\begin{cases} \\n\\\\text{g}_b & \\\\text{if } x \\\\in (\\\\pm 1, 1) \\\\\\\\\\n0 & \\\\text{otherwise}\\n\\\\end{cases},\\n\\\\end{align*}\\n\\\\]\\n\\nAnd during inference, the computation process is expressed as\\n\\\\[\\no = \\\\text{sign}(a) \\\\odot \\\\text{sign}(w),\\n\\\\]\\nwhere $\\\\odot$ indicates a convolutional operation using XNOR and bitcount operations.\\n\\nXNOR-Net (Rastegari et al., 2016): XNOR-Net obtains the channel-wise scaling factors $\\\\alpha = \\\\|w\\\\|_{\\\\infty}$ for the weight and $K$ contains scaling factors $\\\\beta$ for all sub-tensors in activation $a$. We can approximate the convolution between activation $a$ and weight $w$ mainly using binary operations:\\n\\\\[\\no = \\\\text{sign}(a) \\\\odot \\\\text{sign}(w) \\\\odot K \\\\alpha,\\n\\\\]\\nwhere $w \\\\in \\\\mathbb{R}^{c \\\\times w \\\\times h}$ and $a \\\\in \\\\mathbb{R}^{c \\\\times w \\\\times h}$ denote the weight and input tensor, respectively. And the STE is also applied in the backward propagation of the training process.\\n\\nDoReFa-Net (Zhou et al., 2016): DoReFa-Net applies the following function for 1-bit weight and activation:\"}"}
{"id": "qin23b", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\n\\\\[ o = (\\\\text{sign}(a) \\\\odot \\\\text{sign}(w)) \\\\odot \\\\alpha. \\\\] (15)\\n\\nAnd the STE is also applied in the backward propagation with the full-precision gradient.\\n\\n**Bi-Real Net** (Liu et al., 2018b): Bi-Real Net proposes a piece-wise polynomial function as the gradient approximation function:\\n\\n\\\\[ \\\\text{bireal}(a) = \\\\begin{cases} \\n-1 & \\\\text{if } a < -\\\\frac{1}{2} \\\\\\\\\\n\\\\frac{2}{3}a + a^2 & \\\\text{if } -\\\\frac{1}{2} \\\\leq a < 0 \\\\\\\\\\n\\\\frac{2}{3}a - a^2 & \\\\text{if } 0 \\\\leq a < 1 \\\\\\\\\\n1 & \\\\text{otherwise}\\n\\\\end{cases} \\\\]\\n\\n\\\\[ \\\\frac{\\\\partial \\\\text{bireal}(a)}{\\\\partial a} = \\\\begin{cases} \\n\\\\frac{4}{3} + 2a & \\\\text{if } -\\\\frac{1}{2} \\\\leq a < 0 \\\\\\\\\\n\\\\frac{2}{3} - 2a & \\\\text{if } 0 \\\\leq a < 1 \\\\\\\\\\n0 & \\\\text{otherwise}\\n\\\\end{cases}. \\\\] (16)\\n\\nAnd the forward propagation of Bi-Real Net is the same as Eq. (15).\\n\\n**XNOR-Net++** (Bulat et al., 2019): XNOR-Net++ proposes to re-formulate Eq. (14) as:\\n\\n\\\\[ o = (\\\\text{sign}(a) \\\\odot \\\\text{sign}(w)) \\\\odot \\\\Gamma. \\\\] (17)\\n\\nand we adopt the \\\\( \\\\Gamma \\\\) as the following form in experiments (achieve the best performance in the original paper):\\n\\n\\\\[ \\\\Gamma = \\\\alpha \\\\otimes \\\\beta \\\\otimes \\\\gamma, \\\\]\\n\\n\\\\( \\\\alpha \\\\in \\\\mathbb{R}^o, \\\\beta \\\\in \\\\mathbb{R}^{h_{\\\\text{out}}}, \\\\gamma \\\\in \\\\mathbb{R}^{w_{\\\\text{out}}}, \\\\) (18)\\n\\nwhere \\\\( \\\\otimes \\\\) denotes the outer product operation, and \\\\( \\\\alpha, \\\\beta, \\\\) and \\\\( \\\\gamma \\\\) are learnable during training.\\n\\n**ReActNet** (Liu et al., 2020): ReActNet defines an RSign as a binarization function with channel-wise learnable thresholds:\\n\\n\\\\[ x = rsign(x) = \\\\begin{cases} \\n+1 & \\\\text{if } x > \\\\alpha - 1 \\\\\\\\\\n-1 & \\\\text{if } x \\\\leq \\\\alpha \\n\\\\end{cases}. \\\\] (19)\\n\\nwhere \\\\( \\\\alpha \\\\) is a learnable coefficient controlling the threshold. And the forward propagation is\\n\\n\\\\[ o = (\\\\text{rsign}(a) \\\\odot \\\\text{sign}(w)) \\\\odot \\\\alpha. \\\\] (20)\\n\\nwhere \\\\( \\\\alpha \\\\) denotes the channel-wise scaling factors of weights. Parameters in RSign are optimized end-to-end with other parameters in the network. The gradient of \\\\( \\\\tau \\\\) in RSign can be simply derived by the chain rule as:\\n\\n\\\\[ \\\\frac{\\\\partial L}{\\\\partial \\\\tau} = \\\\frac{\\\\partial L}{\\\\partial h(x)} \\\\frac{\\\\partial h(x)}{\\\\partial \\\\tau}, \\\\] (21)\\n\\nwhere \\\\( L \\\\) represents the loss function and \\\\( \\\\frac{\\\\partial L}{\\\\partial h(x)} \\\\) denotes the gradients from deeper layers. The derivative \\\\( \\\\frac{\\\\partial h(x)}{\\\\partial \\\\tau} \\\\) can be easily computed as\\n\\n\\\\[ \\\\frac{\\\\partial h(x)}{\\\\partial \\\\tau} = -1. \\\\] (22)\\n\\nThe estimator of the sign function for activation is\\n\\n\\\\[ \\\\text{react}(a) = \\\\begin{cases} \\n-1 & \\\\text{if } a < -1; \\\\\\\\\\n\\\\frac{2}{3}a + a^2 & \\\\text{if } -1 \\\\leq a < 0; \\\\\\\\\\n\\\\frac{2}{3}a - a^2 & \\\\text{if } 0 \\\\leq a < 1; \\\\\\\\\\n1 & \\\\text{otherwise}\\n\\\\end{cases}. \\\\] (23)\"}"}
{"id": "qin23b", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\n\\\\[ \\\\partial \\\\text{react}(a) = \\\\begin{cases} 2 + 2a & \\\\text{if } -1 \\\\leq a < 0; \\\\\\\\ 2 - 2a & \\\\text{if } 0 \\\\leq a < 1; \\\\\\\\ 0 & \\\\text{otherwise}. \\\\end{cases} \\\\] (24)\\n\\nAnd the estimator for weight is the STE form straightforwardly as Eq. (12).\\n\\nReCU (Xu et al., 2021b): As described in their paper, ReCU is formulated as\\n\\n\\\\[ \\\\text{recu}(w) = \\\\max \\\\min w, Q(\\\\tau), Q(1 - \\\\tau), \\\\] (25)\\n\\nwhere \\\\( Q(\\\\tau) \\\\) and \\\\( Q(1 - \\\\tau) \\\\) denote the \\\\( \\\\tau \\\\) quantile and \\\\( 1 - \\\\tau \\\\) quantile of \\\\( w \\\\), respectively. After applying balancing, weight standardization, and the ReCU to \\\\( w \\\\), the generalized probability density function of \\\\( w \\\\) can be written as follows\\n\\n\\\\[ f(w) = \\\\begin{cases} \\\\frac{1}{2b} \\\\exp \\\\left( -\\\\frac{|w|}{b} \\\\right) & \\\\text{if } |w| < Q(\\\\tau); \\\\\\\\ 1 - \\\\tau & \\\\text{if } |w| = Q(\\\\tau); \\\\\\\\ 0 & \\\\text{otherwise}. \\\\end{cases} \\\\] (26)\\n\\nwhere \\\\( b \\\\) is obtained via\\n\\n\\\\[ b = \\\\text{mean}(|\\\\cdot|) \\\\] (27)\\n\\nwhere \\\\( \\\\text{mean}(\\\\cdot) \\\\) returns the mean of the absolute values of the inputs. The gradient of the weights is obtained by applying the chain derivation rule to the above equation, where the estimator applied to the sign function is STE. As for the gradient w.r.t. the activations, ReCU considers the piecewise polynomial function as follows\\n\\n\\\\[ \\\\partial L \\\\partial \\\\text{sign}(a) = \\\\partial L \\\\partial \\\\text{sign}(a) \\\\cdot \\\\partial \\\\text{sign}(a) \\\\partial a \\\\approx \\\\partial L \\\\partial \\\\text{sign}(a) \\\\cdot \\\\partial F(a) \\\\partial a, \\\\] (28)\\n\\nwhere\\n\\n\\\\[ \\\\partial F(a) \\\\partial a = \\\\begin{cases} 2 + 2a & \\\\text{if } -1 \\\\leq a < 0; \\\\\\\\ 2 - 2a & \\\\text{if } 0 \\\\leq a < 1; \\\\\\\\ 0 & \\\\text{otherwise}. \\\\end{cases} \\\\] (29)\\n\\nAnd other implementations also strictly follow the original paper and official code.\\n\\nFDA (Xu et al., 2021a): FDA applies the following forward propagation in the operator:\\n\\n\\\\[ o = (r \\\\text{sign}(a) \\\\odot \\\\text{sign}(w)) \\\\odot \\\\alpha, \\\\] (30)\\n\\nand computes the gradients of \\\\( o \\\\) in the backward propagation as:\\n\\n\\\\[ \\\\partial \\\\ell \\\\partial t = \\\\partial \\\\ell \\\\partial ow \\\\top \\\\odot ((t w_1) \\\\geq 0) w_1 \\\\top + \\\\partial \\\\ell \\\\partial o \\\\eta' (t) + \\\\partial \\\\ell \\\\partial o \\\\odot 4 \\\\omega \\\\pi X_i \\\\cos((2i + 1) \\\\omega t), \\\\] (31)\\n\\nwhere \\\\( \\\\partial \\\\ell \\\\partial o \\\\) is the gradient from the upper layers, \\\\( \\\\odot \\\\) represents element-wise multiplication, and \\\\( \\\\partial \\\\ell \\\\partial t \\\\) is the partial gradient on \\\\( t \\\\) that backward propagates to the former layer. And \\\\( w_1 \\\\) and \\\\( w_2 \\\\) are weights in the original models and the noise adaptation modules respectively. FDA updates them as\"}"}
{"id": "qin23b", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\frac{\\\\partial \\\\ell}{\\\\partial w} = \\\\left( \\\\frac{\\\\partial \\\\ell}{\\\\partial o} \\\\right)^\\\\top \\\\frac{\\\\partial \\\\ell}{\\\\partial w} \\\\cdot 2 \\\\circ \\\\left( \\\\left( t \\\\cdot w \\\\right) \\\\geq 0 \\\\right) \\\\]\\n\\n\\\\[ \\\\frac{\\\\partial \\\\ell}{\\\\partial w} = \\\\sigma \\\\left( t \\\\cdot w \\\\right) \\\\left( \\\\frac{\\\\partial \\\\ell}{\\\\partial o} \\\\right) \\\\]\\n\\n(32)\\n\\nIn Table 6, we detail the techniques included in the selected binarization algorithms.\\n\\n| Algorithm       | Scaling Factor | Parameter Redistribution | Gradient Approximation |\\n|-----------------|----------------|--------------------------|------------------------|\\n| BNN w/o w/o w/o w/o STE STE |                |                          |                        |\\n| XNOR Statistics by Channel w/o w/o STE STE |                |                          |                        |\\n| DoReFa Statistics by Layer w/o w/o w/o STE STE |                |                          |                        |\\n| Bi-Real Statistics by Channel w/o w/o w/o STE |                |                          |                        |\\n| XNOR++ |                |                          |                        |\\n| ReActNet Statistics by Channel w/o w/o w/o STE |                |                          |                        |\\n| ReCU Statistics by Channel w/o balancing (mean-shifting) w/o Rectified Clamp Unit |                |                          |                        |\\n| FDA Statistics by Channel w/o w/o mean-shifting Decomposing Sign with Fourier Series w/o Decomposing Sign with Fourier Series |                |                          |                        |\\n\\n1. \\\"STE\\\" indicates the Straight Through Estimator, and \\\"w/o\\\" means no special technique is used.\\n\\nA.2. Details of Learning Tasks\\n\\nSelection Rules:\\nTo evaluate the performance of the binarization algorithm in a wide range of learning tasks, we must select a variety of representative tasks. Firstly, we choose representative perception modalities in deep learning, including (2D/3D) vision, text, and speech. These modalities have seen rapid progress and broad impact, so we select specific tasks and datasets within these modalities. Specifically, (1) in the 2D vision modality, we select the essential image classification task and the prevalent object detection task, with datasets including CIFAR10 and ImageNet for the former and Pascal VOC and COCO for the latter. ImageNet and COCO datasets are more challenging and significant, while CIFAR10 and Pascal VOC are more fundamental. For other modalities, binarization is still challenging even with the underlying tasks and datasets in the field, since there are few related binarization studies: (2) in the 3D vision modality, the basic point cloud classification ModelNet40 dataset is selected to evaluate the binarization performance, which is regarded as one of the most fundamental tasks in 3D point cloud research and is widely studied. (3) In the text modality, the General Language Understanding Evaluation (GLUE) benchmark is usually recognized as the most popular dataset, including nine sentence- or sentence-pair language understanding tasks. (4) The keyword spotting task was chosen as the base task in the speech modality, specifically the Google Speech Commands classification dataset.\\n\\nBased on the above reasons and rules, we have selected a series of challenging and representative tasks for BiBench to evaluate the performance of the binarization algorithm in a wide range of learning tasks.\"}"}
{"id": "qin23b", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Results of the structure binarization.\\n\\n| Size  | Attention (Token) | CNN (W\u00d7H) | MLP (#Point) |\\n|-------|-------------------|-----------|--------------|\\n| Dim   | 64                | 128       | 256          | 512       |\\n|       | 0.97              | 0.98      | 0.99         | 0.97      |\\n|       | 1.02              | 1.01      |              | 1.01      |\\n|       | 1.03              |           |              | 0.99      |\\n|       | 0.98              |           |              |           |\\n| CNN   | 14\u00d714             | 28\u00d728     | 56\u00d756        | 112\u00d7112   |\\n|       | 0.59              | 0.59      | 0.59         | 0.59      |\\n|       | 0.59              | 0.59      | 0.59         | 0.59      |\\n|       | 0.59              | 0.59      | 0.59         | 0.59      |\\n|       | 0.59              | 0.59      | 0.59         | 0.59      |\\n\\net al., 2018a; 2020). Therefore, our BiBench applies binarization operators to the entire network to allow us to fairly reflect the accuracy and efficiency of various binarization operators. Furthermore, the goal of BiBench is to systematically study various binarization algorithms through the results of each evaluation track (including learning tasks, neural architecture, etc.). So we only consider generic binarization techniques at the operator level when benchmarking, while techniques like special structural design and quantization for specific layers are excluded. We believe that our results provide valuable insights into the practical trade-offs and challenges of using binary activations, which can serve as a foundation for future research in this area.\\n\\nA.7. Discussion of Binarized Architectures\\n\\nTo empirically verify our analysis in Section 5.1.2, we design the following experiments to demonstrate that the impact of different architectures on binarization is mainly caused by different local structures: (1) Local structures definition. We first abstract the typical local structures of CNN-based, transformer-based, and MLP-based architectures. The first is the bottleneck unit in ResNet, which is mainly composed of three 2D convolutions and a shortcut. The second is the self-attention structure in BERT, which is mainly composed of three linear units generating the query, key, and value, and two multiplication operations between activations (without weights). The last one is the sub-MLP structure in PointNet, which is mainly formed by stacking 3 linear units in series. All binarized structure examples unified contain three convolution or linear units to ensure consistency. (2) Initialization and binarization. To get the impact of the structure level on binarization, we randomly initialize the weights and inputs in all structures and then compare the differences between their full precision and binarized versions. We use the most basic BNN binarization to reveal essential effects. (3) Metric definition. We define 16 inputs of different sizes for each structure, and compare the average error caused by binarization on them:\\n\\n\\\\[ E_f = \\\\frac{\\\\text{std}(f(x)) - \\\\text{std}(\\\\hat{f}(x))}{\\\\ell_1}, \\\\]\\n\\nwhere \\\\( x \\\\in X \\\\) denotes the input set, \\\\( \\\\| \\\\cdot \\\\|_{\\\\ell_1} \\\\) denotes L1 norm, and \\\\( f(\\\\cdot) \\\\) and \\\\( \\\\hat{f}(\\\\cdot) \\\\) denote the full-precision and binarized structures, respectively. As shown in the results in Table 7, the transformer exhibits a larger binarization error at the local structure level. This empirically validates our analysis that binarization has a greater impact on the forward propagation of transformers with the attention mechanism compared to other architectures.\\n\\nA.8. Discussion of Binarization Robustness\\n\\nwe first compare corruption robustness between full-precision architectures of different sizes to determine whether smaller model sizes necessarily lead to better robustness. We compare the full-precision ResNet-20 evaluated by BiBench. ResNet-20 has a consistent residual structure compared with ResNet-18, and the latter has a smaller parameter amount (about 1/40 of the former). However, our results in Table 8 counter-intuitively show that the robustness indicator of full-precision\"}"}
{"id": "qin23b", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Full Results\\n\\nB.1. Evaluation Results of All Tracks\\n\\nTable 10-11 shows the accuracy of different binarization algorithms on 2D and 3D vision tasks, including CIFAR10, ImageNet, PASCAL VOC07, COCO17 for 2D vision tasks and ModelNet40 for 3D vision task. And for each task, we cover several representative model architectures and binarize them with the binarization algorithms mentioned above. We also evaluate binarization algorithms on language and speech tasks, for which we test TinyBERT (4 layers and 6 layers) on GLUE Benchmark and FSMN and D-FSMN on the SpeechCommand dataset. Results are listed in Table 12.\\n\\nTo demonstrate the robustness corruption of binarized algorithms, we show the results on the CIFAR10-C benchmark, which is used to benchmark the robustness to common perturbations in Table 13 and Table 14. It includes 15 kinds of noise, blur, weather, and digital corruption, each with five levels of severity.\\n\\nThe sensitivity of hyperparameters while training is shown in Table 15-16. For each binarization algorithm, we use SGD or Adam optimizer, $\\\\frac{1}{10}$ or $\\\\frac{0.1}{10}$ of the original learning rate, cosine or step learning scheduler, and 200 training epochs. Each case is tested five times to show the training stability. We also calculate the mean and standard deviation (std) of accuracy. The best accuracy and the lowest std for each binarization algorithm are bolded.\\n\\nWe conduct comprehensive deployment and inference on various kinds of hardware, including the Kirin series (970, 980, 985, 990, and 9000E), Dimensity series (820 and 9000), Snapdragon series (855+, 870 and 888), Raspberrypi (3B+ and 4B) and Apple M1 series (M1 and M1 Max). Limited to the support of frameworks, we can only test BNN and ReAct with Larq compute engine and only BNN with daBNN. We convert models to enable the actual inference on real hardware, including ResNet18/34 and VGG-Small on Larq, and only ResNet18/34 on daBNN. And we test 1, 2, 4, and 8 threads for each hardware and additionally test 16 threads for Apple Silicons on Larq. And daBNN only supports single-thread inference. Results are showcased in Table 17-20.\\n\\nB.2. Comparison Results against Other Compression Technologies\\n\\nWe further evaluated representative multi-bit quantization algorithms (Zhou et al., 2016; Choi et al., 2018; Esser et al., 2019) (with INT2 and INT8) and dropout (pruning) algorithms (Li et al., 2016; Ding et al., 2021) in the limited time to demonstrate their accuracy and efficiency metrics and compare them to network binarization. The results show that compared with multi-bit quantization and dropout, binarization brings more significant compression and acceleration while facing greater challenges from the decline in accuracy.\\n\\nTo highlight the characteristics of network binarization, we compare it with other mainstream compression approaches, including multi-bit quantization and pruning, from accuracy and efficiency perspectives (Table 21). Overall, the results\"}"}
{"id": "qin23b", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 10: Accuracy on 2D and 3D Vision Tasks.\\n\\n| Task Arch. | FP32 | Binarization Algorithm | BNN | XNOR | DoReFa | Bi-Real | XNOR++ | ReActNet | ReCU | FDA |\\n|------------|------|------------------------|------|------|--------|---------|--------|----------|------|-----|\\n| CIFAR10    |      |                        |      |      |        |         |        |          |      |     |\\n| ResNet20   | 91.99|                        | 85.31| 85.53| 85.18  | 85.56   | 85.41  | 86.18    | 86.42|     |\\n| ResNet18   | 94.82|                        | 89.69| 91.4  | 91.55  | 91.20   | 90.04  | 91.55    | 92.79| 90.42|\\n| ResNet34   | 95.34|                        | 90.82| 89.58| 90.95  | 92.50   | 90.59  | 92.69    | 93.64| 89.59|\\n| VGG-Small  | 93.80|                        | 89.66| 89.65| 89.66  | 90.25   | 89.34  | 90.27    | 90.84| 89.48|\\n| ImageNet ResNet18 | 69.90|                        | 52.99| 53.99| 53.55  | 54.79   | 52.43  | 54.97    | 54.51| 54.63|\\n| VOC07      |      |                        |      |      |        |         |        |          |      |     |\\n| Faster-RCNN | 76.06|                        | 58.54| 56.75| 58.07  | 60.90   | 56.60  | 61.90    | 62.10| 60.10|\\n| SSD300     | 77.34|                        | 9.09 | 33.72| 30.70  | 31.90   | 9.41   | 38.41    | 9.80 | 43.68|\\n| COCO17     |      |                        |      |      |        |         |        |          |      |     |\\n| Faster-RCNN | 27.20|                        | 21.20| 20.50| 21.30  | 22.20   | 21.60  | 22.80    | 23.30| 22.40|\\n| ModelNet40 |      |                        |      |      |        |         |        |          |      |     |\\n| PointNet   |      |                        |      |      |        |         |        |          |      |     |\\n| vanilla    |      |                        |      |      |        |         |        |          |      |     |\\n\\nExpress the intuitive conclusion that there is a trade-off between accuracy and efficiency for different compression approaches. The ultra-low bit-width of network binarization brings acceleration and compression beyond multi-bit quantization and pruning. For example, binarization achieves $12.32 \\\\times$ FLOPs saving, while INT8 quantization achieves $1.87 \\\\times$. However, binarization algorithms also introduce a significant performance drop, the largest among all compression approaches (e.g., CIFAR10-Res20 binary 85.74 vs. pruning 91.54). These results show that network binarization pursues a more radical efficiency improvement among existing compression approaches and is oriented for deployment on edge devices, which is consistent with the conclusions in BiBench.\"}"}
{"id": "qin23b", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task   | Arch. | Category | FP32 |\\n|--------|-------|----------|------|\\n|        |       |          |      |\\n| Airplane | 83.7  | 37.5     | 74.14|\\n| Bag     | 79.6  | 44.2     | 49   |\\n| Cap     | 92.3  | 44.3     | 73.32|\\n| Car     | 76.8  | 24.3     | 55.27|\\n| Chair   | 90.9  | 61.6     | 85.62|\\n| Earphone| 70.2  | 38.5     | 30.97|\\n| Guitar  | 91.1  | 32.9     | 69.17|\\n| Knife   | 85.7  | 43       | 78.16|\\n| Lamp    | 82    | 51.2     | 69   |\\n| Laptop  | 95.5  | 49.4     | 93.29|\\n| Motorbike| 64.4 | 16.3     | 19.04|\\n| Mug     | 93.6  | 49.1     | 64.32|\\n| Pistol  | 80.8  | 25.5     | 62.29|\\n| Rocket  | 54.4  | 26.9     | 30.95|\\n| Skateboard| 70.7 | 41.2     | 45.7 |\\n| Table   | 81.4  | 51.3     | 73.68|\\n| Overall | 80.81 | 39.82    | 60.87|\\n\\nTable: Accuracy on ShapeNet dataset.\"}"}
{"id": "qin23b", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nevaluate binarization comprehensively and have obtained a series of reliable and informative conclusions and experiences.\\n\\nDataset Details:\\n\\nCIFAR10 (Krizhevsky et al., 2014): The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images commonly used to train machine learning and computer vision algorithms. This dataset is widely used for image classification tasks. There are 60,000 color images, each of which measures 32x32 pixels. All images are categorized into 10 different classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Each class has 6000 images, where 5000 are for training and 1000 are for testing. The evaluation metric of the CIFAR-10 dataset is accuracy, defined as:\\n\\n\\\\[\\n\\\\text{Accuracy} = \\\\frac{TP}{TP + TN} = \\\\frac{TP}{TP + TN + FP + FN},\\n\\\\]\\n\\nwhere \\\\(TP\\\\) (True Positive) means cases correctly identified as positive, \\\\(TN\\\\) (True Negative) means cases correctly identified as negative, \\\\(FP\\\\) (False Positive) means cases incorrectly identified as positive and \\\\(FN\\\\) (False Negative) means cases incorrectly identified as negative. To estimate the accuracy, we should calculate the proportion of \\\\(TP\\\\) and \\\\(TN\\\\) in all evaluated cases.\\n\\nImageNet (Krizhevsky et al., 2012): ImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images are collected from the web and labeled by human labelers using a crowd-sourced image labeling service called Amazon Mechanical Turk. As part of the Pascal Visual Object Challenge, ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) was established in 2010. There are approximately 1.2 million training images, 50,000 validation images, and 150,000 testing images in total in ILSVRC. ILSVRC uses a subset of ImageNet, with about 1000 images in each of the 1000 categories. ImageNet also uses accuracy to evaluate the predicted results, which is defined above.\\n\\nPascal VOC07 (Hoiem et al., 2009): The PASCAL Visual Object Classes 2007 (VOC07) dataset contains 20 object categories including vehicles, households, animals, and other: airplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. As a benchmark for object detection, semantic segmentation, and object classification, this dataset contains pixel-level segmentation annotations, bounding box annotations, and object class annotations. The VOC07 dataset uses mean average precision (mAP) to evaluate results, which is defined as:\\n\\n\\\\[\\nmAP = \\\\frac{1}{n} \\\\sum_{k=1}^{n} \\\\text{AP}_k\\n\\\\]\\n\\nwhere \\\\(\\\\text{AP}_k\\\\) denotes the average precision of the k-th category, which calculates the area under the precision-recall curve:\\n\\n\\\\[\\n\\\\text{AP}_k = \\\\int_{p_k(r)}^{1} dr.\\n\\\\]\\n\\nEspecially for VOC07, we apply 11-point interpolated mAP, which divides the recall value to \\\\(\\\\{0.0, 0.1, \\\\ldots, 1.0\\\\}\\\\) and then computes the average of maximum precision value for these 11 recall values as:\\n\\n\\\\[\\n\\\\text{mAP} = \\\\frac{1}{11} \\\\sum_{r \\\\in \\\\{0.0, \\\\ldots, 1.0\\\\}} \\\\text{p}_{\\\\text{interp}}(r).\\n\\\\]\\n\\n\\\\[\\n\\\\text{p}_{\\\\text{interp}}(r) = \\\\max_{\\\\tilde{r} \\\\geq r} p(\\\\tilde{r})\\n\\\\]\\n\\nCOCO17 (Lin et al., 2014): The MS COCO (Microsoft Common Objects in Context) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images. According to community feedback, in the 2017 release, the training/validation split was changed from 83K/41K to 118K/5K. And the images and annotations are the same. The 2017 test set is a subset of 41K images from the 2015 test set. Additionally, 123K images are included in the unannotated dataset. The COCO17 dataset also uses mean average precision (mAP) as defined above PASCAL VOC07 uses, which is defined as above.\\n\\nModelNet40 (Wu et al., 2015): The ModelNet40 dataset contains point clouds of synthetic objects. As the most widely used benchmark for point cloud analysis, ModelNet40 is popular due to the diversity of categories, clean shapes, and...\"}"}
{"id": "qin23b", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nIn the original ModelNet40, 12,311 CAD-generated meshes are divided into 40 categories, where 9,843 are for training, and 2,468 are for testing. The point cloud data points are sampled by a uniform sampling method from mesh surfaces and then scaled into a unit sphere by moving to the origin. The ModelNet40 dataset also uses accuracy as the metric, which has been defined above in CIFAR10.\\n\\nShapeNet (Chang et al., 2015): ShapeNet is a large-scale repository for 3D CAD models developed by researchers from Stanford University, Princeton University, and the Toyota Technological Institute in Chicago, USA. Using WordNet hypernym-hyponym relationships, the repository contains over 300M models, with 220,000 classified into 3,135 classes. There are 31,693 meshes in the ShapeNet Parts subset, divided into 16 categories of objects (i.e., tables, chairs, planes, etc.). Each shape contains 2-5 parts (with 50 part classes in total).\\n\\nGLUE (Wang et al., 2018a): General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE, and WNLI. Among them, SST-2, MRPC, QQP, MNLI, QNLI, RTE, and WNLI use accuracy as the metric, which is defined in CIFAR10. CoLA is measured by Matthews Correlation Coefficient (MCC), which is better in binary classification since the number of positive and negative samples are extremely unbalanced:\\n\\n$$\\\\text{MCC} = \\\\frac{TP \\\\times TN - FP \\\\times FN}{(TP + FP)(TP + FN)(TN + FN)(TN + FP)}$$\\n\\n(38)\\n\\nAnd STS-B is measured by Pearson/Spearman Correlation Coefficient:\\n\\n$$r_{\\\\text{Pearson}} = \\\\frac{n}{n-1} \\\\sum_{i=1}^{n} (X_i - \\\\bar{X})s_X Y_i - \\\\bar{Y} s_Y$$\\n\\n$$r_{\\\\text{Spearman}} = 1 - \\\\frac{6 \\\\sum d_i^2}{n(n^2-1)}$$\\n\\n(39)\\n\\nwhere $$n$$ is the number of observations, $$s_X$$ and $$s_Y$$ indicate the sum of squares of $$X$$ and $$Y$$ respectively, and $$d_i$$ is the difference between the ranks of corresponding variables.\\n\\nSpeechCom. (Warden, 2018): As part of its training and evaluation process, SpeechCom provides a collection of audio recordings containing spoken words. Its primary goal is to provide a way to build and test small models that detect a single word that belongs to a set of ten target words. Models should detect as few false positives as possible from background noise or unrelated speech while providing as few false positives as possible. The accuracy metric for SpeechCom is also the same as CIFAR10.\\n\\nCIFAR10-C (Hendrycks & Dietterich, 2018): CIFAR10-C is a dataset generated by adding 15 common corruptions and 4 extra corruptions to the test images in the Cifar10 dataset. It benchmarks the frailty of classifiers under corruption, including noise, blur, weather, and digital influence. And each type of corruption has five levels of severity, resulting in 75 distinct corruptions. We report the accuracy of the classifiers under each level of severity and each corruption. Meanwhile, we use the mean and relative corruption error as metrics. Denote the error rate of Network under Settings as $$E_{\\\\text{Network Settings}}$$. The classifier's aggregate performance across the five severities of the corruption types. The Corruption Errors of a certain type of Corruption is computed with the formula:\\n\\n$$CE_{\\\\text{Network Corruption}} = \\\\frac{1}{5} \\\\sum_{s=1}^{5} \\\\frac{E_{\\\\text{Network}}}{E_{\\\\text{AlexNet}}}$$\\n\\n(40)\\n\\nTo make Corruption Errors comparable across corruption types, the difficulty is usually adjusted by dividing by AlexNet\u2019s errors.\\n\\nA.3. Details of Neural Architectures\\n\\nResNet (He et al., 2016): Residual Networks, or ResNets, learn residual functions concerning the layer inputs instead of learning unreferenced functions. Instead of making stacked layers directly fit a desired underlying mapping, residual nets let these layers fit a residual mapping. There is empirical evidence that these networks are easier to optimize and can achieve higher accuracy with considerably increased depth.\"}"}
{"id": "qin23b", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nVGG (Simonyan & Zisserman, 2015): VGG is a classical convolutional neural network architecture. It is proposed by an analysis of how to increase the depth of such networks. It is characterized by its simplicity: the network utilizes small $3 \\\\times 3$ filters, and the only other components are pooling layers and a fully connected layer.\\n\\nMobileNetV2 (Sandler et al., 2018): MobileNetV2 is a convolutional neural network architecture that performs well on mobile devices. This model has an inverted residual structure with residual connections between the bottleneck layers. The intermediate expansion layer employs lightweight depthwise convolutions to filter features as a source of nonlinearity. In MobileNetV2, the architecture begins with an initial layer of 32 convolution filters, followed by 19 residual bottleneck layers.\\n\\nFaster-RCNN (Ren et al., 2015): Faster R-CNN is an object detection model that improves Fast R-CNN by utilizing a region proposal network (RPN) with the CNN model. The RPN shares full-image convolutional features with the detection network, enabling nearly cost-free region proposals. A fully convolutional network is used to predict the bounds and objectness scores of objects at each position simultaneously. RPNs use end-to-end training to produce region proposals of high quality and instruct the unified network where to search. Sharing their convolutional features allows RPN and Fast R-CNN to be combined into a single network. Faster R-CNN consists of two modules. The first module is a deep, fully convolutional network that proposes regions, and the second is the detector that uses the proposals for giving the final prediction boxes.\\n\\nSSD (Liu et al., 2016): SSD is a single-stage object detection method that discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. During prediction, each default box is adjusted to match better the shape of the object based on its scores for each object category. In addition, the network automatically handles objects of different sizes by combining predictions from multiple feature maps with different resolutions.\\n\\nBERT (Kenton & Toutanova, 2019): BERT, or Bidirectional Encoder Representations from Transformers, improves upon standard Transformers by removing the unidirectionality constraint using a masked language model (MLM) pre-training objective. By masking some tokens from the input, the masked language model attempts to estimate the original vocabulary id of the masked word based solely on its context. An MLM objective differs from a left-to-right language model in that it enables the representation to integrate the left and right contexts, which facilitates pre-training a deep bidirectional Transformer. Additionally, BERT uses a next-sentence prediction task that pre-trains text-pair representations along with the masked language model. Note that we replace the direct binarized attention with a bi-attention mechanism to prevent the model from completely crashing (Qin et al., 2021).\\n\\nPointNet (Qi et al., 2017): PointNet is a unified architecture for applications ranging from object classification and part segmentation to scene semantic parsing. The architecture directly receives point clouds as input and outputs either class labels for the entire input or point segment/part labels.\\n\\nPointNet-Vanilla is a variant of PointNet, which drops off the T-Net module. And for all PointNet models, we apply the EMA-Max (Qin et al., 2020a) as the aggregator, because directly following the max pooling aggregator will cause the binarized PointNets to fail to converge.\\n\\nFSMN (Zhang et al., 2015): Feedforward sequential memory networks or FSMN is a novel neural network structure to model long-term dependency in time series without using recurrent feedback. It is a standard fully connected feedforward neural network containing some learnable memory blocks. As a short-term memory mechanism, the memory blocks encode long context information using a tapped-delay line structure.\\n\\nDeep-FSMN (Zhang et al., 2018a): The Deep-FSMN architecture is an improved feedforward sequential memory network (FSMN) with skip connections between memory blocks in adjacent layers. By utilizing skip connections, information can be transferred across layers, and thus the gradient vanishing problem can be avoided when building very deep structures.\\n\\nA.4. Details of Hardware\\n\\nHisilicon Kirin (Hisilicon, 2022): Kirin is a series of ARM-based systems-on-a-chip (SoCs) produced by HiSilicon. Their products include Kirin 970, Kirin 980, Kirin 985, etc.\\n\\nMediaTek Dimensity (MediaTek, 2022): Dimensity is a series of ARM-based systems-on-a-chip (SoCs) produced by MediaTek. Their products include Dimensity 820, Dimensity 8100, Dimensity 9000, etc.\\n\\nQualcomm Snapdragon (Singh & Jain, 2014): Snapdragon is a family of mobile systems-on-a-chip (SoC) processor architecture provided by Qualcomm. The original Snapdragon chip, the Scorpio, was similar to the ARM Cortex-A8 core based upon the ARMv7 instruction set, but it was enhanced by the use of SIMD operations, which provided higher\"}"}
{"id": "qin23b", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization performance. Qualcomm Snapdragon processors are based on the Krait architecture. They are equipped with an integrated LTE modem, providing seamless connectivity across 2G and 3G LTE networks. Raspberry Pi is a series of small single-board computers (SBCs) developed in the United Kingdom by the Raspberry Pi Foundation in association with Broadcom. Raspberry Pi was originally designed to promote the teaching of basic computer science in schools and in developing countries. As a result of its low cost, modularity, and open design, it is used in many applications, including weather monitoring, and is sold outside the intended market. It is typically used by computer hobbyists and electronic enthusiasts due to the adoption of HDMI and USB standards. Apple M1 is a series of ARM-based systems-on-a-chip (SoCs) designed by Apple Inc. As a central processing unit (CPU) and graphics processing unit (GPU) for Macintosh desktops and notebooks, as well as iPad Pro and iPad Air tablets. In November 2020, Apple introduced the M1 chip, followed by the professional-oriented M1 Pro and M1 Max chips in 2021. Apple launched the M1 Ultra in 2022, which combines two M1 Max chips in a single package. The M1 Max is a higher-performance version of the M1 Pro, with larger GPU cores and memory bandwidth.\\n\\nA.5. Discussion of Novelty and Significance\\nWe emphasize that our BiBench includes the following significant contributions: (1) the first system benchmark that enables a new view to quantitative evaluate binarization algorithms at the operator level, and (2) revealing a practical binarized operator design paradigm.\\n\\n(1) BiBench is the first effort to facilitate systematic and comprehensive comparisons between binarized algorithms. It provides a brand new perspective to decouple the binarized operators from the architectures for quantitative evaluations at the operator level. In existing works, the binarized operator and specific structure/training pipeline are often designed simultaneously or coupled closely (up to 15 (in 26) algorithms in Table 5 of the manuscript), the latter is often difficult to generalize to various architectures and tasks. Their training and inference settings are also different from each other. This makes the direct comparison among binarization algorithms difficult. Our BiBench enables a new approach towards a fair comparison of binarization algorithms by building a unified evaluation track for each algorithm on learning tasks and neural architectures.\\n\\n(2) BiBench reveals a practical paradigm of binarized operator designing. Based on the systemic and quantitative evaluation, superior techniques for better binarization operators can emerge, which is essential for pushing binarization algorithms to be accurate and efficient. For instance, after excluding the bi-real shortcut in Bi-Real Net, the difference between it and the DoReFa operator is solely the soft estimator in the backward propagation, yet this change yields a 2% difference in the learning task track (as Table 2 in the manuscript shows). These unprecedented quantitative insights identify which techniques are effective and low-cost for improving binarization operators, which will strongly facilitate the emergence of more powerful generic binarization algorithms. We summarize and present these operator-level binarization techniques in Section 5.3.\\n\\nA.6. Discussion of Bit-width Constraints\\nIn real practice, applying flexible bit-width or specialized quantizers in certain layers can lead to significant improvements in binarization, enabling a better balance between task performance and inference efficiency. There is a common practice to keep the input and output layers at 32-bit full precision since binarizing these two layers brings a severe task performance drop while having a relatively low efficiency gain (Rastegari et al., 2016; Liu et al., 2018a; Zhou et al., 2016). And we also follow this practice in our BiBench. Moreover, some binarization algorithms optimized for practical deployment also compress these layers while maintaining the task performance, they usually make the two layers fixed-point (e.g., 8-bit) or design specific quantizers for them (Zhang et al., 2021b; Zhao et al., 2017; Courbariaux et al., 2016b). As mentioned by the reviewer, FracBNN (Zhang et al., 2021b) is one of the most representative works among them, which employs a dual-precision activation scheme to compute features with up to two bits using an additional sparse binary convolution. In practice, these algorithms can help BNNs achieve better accuracy-efficiency trade-offs.\\n\\nWe would also highlight that there is significant value in the exploration of full binarization (weight and activation) algorithms at the operator level, which is the focus of our BiBench. Since the application of the most aggressive bit-widths, the binarized parts in BNNs can be implemented by extremely efficient bitwise instructions, which makes it a more efficient solution compared to other bit-width quantization and partial binarization. Hence, the weight and activation binarization is adopted by various papers that propose binarization algorithms as a standard setting (Rastegari et al., 2016; Zhou et al., 2016; Liu 2018a).\"}"}
{"id": "qin23b", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\nQin, H., Zhang, X., Gong, R., Ding, Y., Xu, Y., and Liu, X.\\nDistribution-sensitive information retention for accurate binary neural network.\\nIJCV, 2022b.\\nQin, H., Ding, Y., Zhang, X., Wang, J., Liu, X., and Lu, J.\\nDiverse sample generation: Pushing the limit of generative data-free quantization.\\nIEEE TPAMI, 2023a.\\nQin, H., Ma, X., Ding, Y., Li, X., Zhang, Y., Ma, Z., Wang, J., Luo, J., and Liu, X.\\nBifsmnv2: Pushing binary neural networks for keyword spotting to real-network performance.\\nIEEE TNNLS, 2023b.\\nQualcomm. Snpe: Qualcomm neural processing sdk for ai, 2022. URL https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk.\\nRastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.\\nXnor-net: Imagenet classification using binary convolutional neural networks. In ECCV, 2016.\\nRen, J., Pan, L., and Liu, Z.\\nBenchmarking and analyzing point cloud classification under corruptions. In ICML, 2022.\\nRen, S., He, K., Girshick, R., and Sun, J.\\nFaster r-cnn: Towards real-time object detection with region proposal networks. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), NeurIPS, 2015.\\nRusci, M., Capotondi, A., and Benini, L.\\nMemory-driven mixed low precision quantization for enabling deep network inference on microcontrollers.\\nMLSys, 2020.\\nSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.\\nMobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.\\nShang, Y., Xu, D., Duan, B., Zong, Z., Nie, L., and Yan, Y.\\nLipschitz continuity retained binary neural network. In ECCV, 2022a.\\nShang, Y., Xu, D., Zong, Z., Nie, L., and Yan, Y.\\nNetwork binarization via contrastive learning. In ECCV, 2022b.\\nShen, Z., Liu, Z., Qin, J., Huang, L., Cheng, K.-T., and Savvides, M.\\nS2-bnn: Bridging the gap between self-supervised real and 1-bit neural networks via guided distribution calibration. In CVPR, 2021.\\nSimonyan, K. and Zisserman, A.\\nVery deep convolutional networks for large-scale image recognition. In ICLR, 2015.\\nSingh, M. P. and Jain, M. K.\\nEvolution of processor architecture in mobile phones.\\nIJCA, 2014.\\nTseng, V.-S., Bhattachara, S., Fern\u00e1ndez-Marqu\u00e9s, J., Alizadeh, M., Tong, C., and Lane, N. D.\\nDeterministic binary filters for convolutional neural networks. IJCAI, 2018.\\nVanhoucke, V., Senior, A. W., and Mao, M. Z.\\nImproving the speed of neural networks on cpus. 2011.\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R.\\nGlue: A multi-task benchmark and analysis platform for natural language understanding. In ICLR, 2018a.\\nWang, J.\\nAdversarial examples in physical world. In IJCAI, 2021.\\nWang, J., Liu, A., Bai, X., and Liu, X.\\nUniversal adversarial patch attack for automatic checkout using perceptual and attentional bias.\\nIEEE TIP, 2021a.\\nWang, J., Yin, Z., Hu, P., Liu, A., Tao, R., Qin, H., Liu, X., and Tao, D.\\nDefensive patches for robust recognition in the physical world. In CVPR, 2022a.\\nWang, P., He, X., Li, G., Zhao, T., and Cheng, J.\\nSparsity-inducing binarized neural networks. In AAAI, 2020a.\\nWang, X., Zhang, B., Li, C., Ji, R., Han, J., Cao, X., and Liu, J.\\nModulated convolutional networks. In CVPR, 2018b.\\nWang, Y., Wang, J., Yin, Z., Gong, R., Wang, J., Liu, A., and Liu, X.\\nGenerating transferable adversarial examples against vision transformers. In ACM MM, 2022b.\\nWang, Z., Lu, J., Tao, C., Zhou, J., and Tian, Q.\\nLearning channel-wise interactions for binary convolutional neural networks. In CVPR, 2019.\\nWang, Z., Wu, Z., Lu, J., and Zhou, J.\\nBidet: An efficient binarized object detector. In CVPR, 2020b.\\nWarden, P.\\nSpeech commands: A dataset for limited-vocabulary speech recognition.\\narXiv preprint arXiv:1804.03209, 2018.\\nWikipedia.\\nApple m1 - wikipedia, 2022a. URL https://en.wikipedia.org/wiki/Apple_M1.\\nWikipedia.\\nRaspberry pi - wikipedia, 2022b. URL https://en.wikipedia.org/wiki/Raspberry_Pi.\\nWu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J.\\nQuantized convolutional neural networks for mobile devices. In CVPR, 2016.\"}"}
{"id": "qin23b", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nWu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., and Xiao, J. 3d shapenets: A deep representation for volumetric shapes. In CVPR, 2015.\\n\\nXiao, Y., Zhang, T., Liu, S., and Qin, H. Benchmarking the robustness of quantized models. arXiv preprint arXiv:2304.03968, 2023.\\n\\nXu, Y., Han, K., Xu, C., Tang, Y., Xu, C., and Wang, Y. Learning frequency domain approximation for binary neural networks. NeurIPS, 2021a.\\n\\nXu, Z., Hsu, Y., and Huang, J. Training shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks. In ICLR, 2018.\\n\\nXu, Z., Lin, M., Liu, J., Chen, J., Shao, L., Gao, Y., Tian, Y., and Ji, R. Recu: Reviving the dead weights in binary neural networks. In IEEE ICCV, 2021b.\\n\\nYe, S., Xu, K., Liu, S., Cheng, H., Lambrechts, J.-H., Zhang, H., Zhou, A., Ma, K., Wang, Y., and Lin, X. Adversarial robustness vs. model compression, or both? In IEEE ICCV, 2019.\\n\\nYim, J., Joo, D., Bae, J., and Kim, J. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In CVPR, 2017.\\n\\nYin, Z., Wang, J., Ding, Y., Xiao, Y., Guo, J., Tao, R., and Qin, H. Improving generalization of deepfake detection with domain adaptive batch normalization. In ACM MMW, 2021.\\n\\nYuan, C. and Agaian, S. S. A comprehensive review of binary neural network. arXiv preprint arXiv:2110.06804, 2021.\\n\\nZagoruyko, S. and Komodakis, N. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In ICLR, 2017.\\n\\nZhang, C., Zhang, M., Zhang, S., Jin, D., Zhou, Q., Cai, Z., Zhao, H., Liu, X., and Liu, Z. Delving deep into the generalization of vision transformers under distribution shifts. In CVPR, 2022a.\\n\\nZhang, J., Pan, Y., Yao, T., Zhao, H., and Mei, T. dabnn: A super fast inference framework for binary neural networks on arm devices. In ACM MM, 2019.\\n\\nZhang, S., Liu, C., Jiang, H., Wei, S., Dai, L., and Hu, Y. Feedforward sequential memory networks: A new structure to learn long-term dependency. arXiv preprint arXiv:1512.08301, 2015.\\n\\nZhang, S., Lei, M., Yan, Z., and Dai, L. Deep fsmn for large vocabulary continuous speech recognition. In ICASSP, 2018a.\\n\\nZhang, X., Zhou, X., Lin, M., and Sun, J. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In CVPR, 2018b.\\n\\nZhang, X., Qin, H., Ding, Y., Gong, R., Yan, Q., Tao, R., Li, Y., Yu, F., and Liu, X. Diversifying sample generation for accurate data-free quantization. In CVPR, 2021a.\\n\\nZhang, Y., Pan, J., Liu, X., Chen, H., Chen, D., and Zhang, Z. Fracbnn: Accurate and fpga-efficient binary neural networks with fractional activations. In FPGA, 2021b.\\n\\nZhang, Y., Zhang, Z., and Lew, L. Pokebnn: A binary pursuit of lightweight accuracy. In CVPR, 2022b.\\n\\nZhao, M., Dai, S., Zhu, Y., Tang, H., Xie, P., Li, Y., Liu, C., and Zhang, B. Pb-gcn: Progressive binary graph convolutional networks for skeleton-based action recognition. Neurocomputing, 2022a.\\n\\nZhao, R., Song, W., Zhang, W., Xing, T., Lin, J.-H., Sriavastava, M., Gupta, R., and Zhang, Z. Accelerating binarized convolutional neural networks with software-programmable fpgas. In FPGA, 2017.\\n\\nZhao, Z., Xu, S., Zhang, C., Liu, J., and Zhang, J. Bayesian fusion for infrared and visible images. Signal Processing, 2020a.\\n\\nZhao, Z., Xu, S., Zhang, C., Liu, J., Zhang, J., and Li, P. Didfuse: Deep image decomposition for infrared and visible image fusion. In IJCAI, 2020b.\\n\\nZhao, Z., Bai, H., Zhang, J., Zhang, Y., Xu, S., Lin, Z., Timofte, R., and Gool, L. V. Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion. CoRR, abs/2211.14461, 2022b.\\n\\nZhao, Z., Xu, S., Zhang, J., Liang, C., Zhang, C., and Liu, J. Efficient and model-based infrared and visible image fusion via algorithm unrolling. IEEE TCSVT, 2022c.\\n\\nZhao, Z., Zhang, J., Xu, S., Lin, Z., and Pfister, H. Discrete cosine transform network for guided depth map super-resolution. In CVPR, 2022d.\\n\\nZhao, Z., Bai, H., Zhu, Y., Zhang, J., Xu, S., Zhang, Y., Zhang, K., Meng, D., Timofte, R., and Gool, L. V. DDFM: denoising diffusion model for multi-modality image fusion. 2023a.\\n\\nZhao, Z., Zhang, J., Gu, X., Tan, C., Xu, S., Zhang, Y., Timofte, R., and Gool, L. V. Spherical space feature decomposition for guided depth map super-resolution. CoRR, abs/2303.08942, 2023b.\\n\\nZhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.\"}"}
{"id": "qin23b", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Details of Binarization Algorithm\\n\\nModel compression methods including quantization (Xiao et al., 2023; Liu et al., 2021a; Guo et al., 2020a; Qin et al., 2022b; J. Guo, W. Ouyang, and D. Xu, 2020; Qin et al., 2023a; Chen et al., 2022; Qin, 2021; Zhang et al., 2021a; Guo et al., 2023b; Liu et al., 2022a; 2019) have been widely used in various deep learning fields (Wang, 2021; Zhao et al., 2020b; 2023a; 2020a; Guo et al., 2023a; Zhao et al., 2023b; Guo et al., 2021; Liu et al., 2023), including computer vision (Guo et al., 2020b; Wang et al., 2021a; Ding et al., 2022; Zhao et al., 2022d; Wang et al., 2021b; Zhao et al., 2022b; Wang et al., 2022a; Yin et al., 2021; Zhao et al., 2022c), language understanding (Bai et al., 2021; Wang et al., 2022b), speech recognition (Qin et al., 2023b), etc. Previous research has deemed lower bit-width quantization methods as more aggressive (Rusci et al., 2020; Choukroun et al., 2019; Qin et al., 2022a), as they often provide higher compression and faster processing at the cost of lower accuracy. Among all quantization techniques, 1-bit quantization (binarization) is considered the most aggressive (Qin et al., 2022a), as it poses significant accuracy challenges but offers the greatest compression and speed benefits.\\n\\nTraining. When training a binarized model, the sign function is commonly used in the forward pass, and gradient approximations such as STE are applied during the backward pass to enable the model to be trained. Since the parameters are quantized to binary, network binarization methods typically use a simple sign function as the quantizer rather than using the same quantizer as in multi-bit (2-8 bit) quantization (Gong et al., 2019; Gholami et al., 2021). Specifically, as (Gong et al., 2019) describes, for multi-bit uniform quantization, given the bit width $b$ and the floating-point activation/weight $x$ following in the range $(l, u)$, the complete quantization-dequantization process of uniform quantization can be defined as\\n\\n$$Q_U(x) = \\\\text{round}(x / \\\\Delta)$$\\n\\n(10)\\n\\nwhere the original range $(l, u)$ is divided into $2^{b-1}$ intervals $P_i$, $i \\\\in (0, 1, \\\\ldots, 2^b-1)$, and $\\\\Delta = u - l / 2^{b-1}$ is the interval length. When $b = 1$, the $Q_U(x)$ equals the sign function, and the binary function is expressed as\\n\\n$$Q_B(x) = \\\\text{sign}(x).$$\\n\\n(11)\\n\\nTherefore, binarization can be regarded formally as the 1-bit specialization of quantization.\\n\\nDeployment. For efficient deployment on real-world hardware, binarized parameters are grouped in blocks of 32 and processed simultaneously using 32-bit instructions, which is the key principle for achieving acceleration. To compress binary algorithms, instructions such as XNOR (or the combination of EOR and NOT) and popcount are used to enable the deployment of binarized networks on real-world hardware. The XNOR (exclusive-XOR) gate is a combination of an XOR gate and an inverter, and XOR (also known as EOR) is a common instruction that has long been available in assembly instructions for all target platforms. The popcount instruction, or Population Count per byte, counts the number of bits with a specific value in each vector element in the source register, stores the result in a vector and writes it to the destination register (Arm, 2020). This instruction is used to accelerate the inference of binarized networks (Hubara et al., 2016; Rastegari et al., 2016) and is widely supported by various hardware, such as the definitions of popcount in ARM and x86 in (Arm, 2020) and (AMD, 2022), respectively.\\n\\nComparison with other compression techniques. Current network compression technologies primarily focus on reducing the size and computation of full-precision models. Knowledge distillation, for instance, guides the training of small (student) models using the intermediate features and/or soft outputs of large (teacher) models (Hinton et al., 2015; Xu et al., 2018; Chen et al., 2018; Yim et al., 2017; Zagoruyko & Komodakis, 2017). Model pruning (Han et al., 2015; 2016; He et al., 2017) and low-rank decomposition (Denton et al., 2014; Lebedev et al., 2015; Jaderberg et al., 2014; Lebedev & Lempitsky, 2016) also reduce network parameters and computation via pruning and low-rank approximation. Compact model design, on the other hand, creates a compact model from scratch (Howard et al., 2017; Sandler et al., 2018; Zhang et al., 2018b; Ma et al., 2018). While these compression techniques effectively decrease the number of parameters, the compressed models still use 32-bit floating-point numbers, which leaves scope for additional compression using model quantization/binarization techniques. Compared to multi-bit (2-8 bit) model quantization, which compresses parameters to integers (Gong et al., 2014; Wu et al., 2016; Vanhoucke et al., 2011; Gupta et al., 2015), binarization directly applies the sign function to compress the\"}"}
{"id": "qin23b", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BiBench: Benchmarking and Analyzing Network Binarization\\n\\nFigure 4: Timeline of the operator-level binarization algorithms we have considered. The algorithms selected for BiBench are in bold, and the citation is counted till 2023/01/25.\\n\\nSelection Rules:\\n- When creating the BiBench, we considered various binarization algorithms with enhanced operator techniques in binarization research, and the timeline of considered algorithms is Figure 4 and we list their details in Table 5. We follow two general rules when selecting algorithms for our benchmark:\\n  1. The selected algorithms should function on binarized operators that are the fundamental components for binarized networks (as discussed in Section 2.1). We exclude algorithms and techniques that require specific local structures or training pipelines to ensure a fair comparison.\\n  2. The selected algorithms should have an extensive influence to be representative, i.e., selected from widely adopted algorithms or the most advanced ones.\\n\\nSpecifically, we chose algorithms based on the following detailed criteria to ensure representativeness and fairness in evaluations: Operator Techniques (Yes/No), Year, Conference, Citations (up to 2023/01/25), Open source availability (Yes/No), and Specific Structure / Training-pipeline requirements (Yes/No/Optional).\\n\\nWe analyze the techniques proposed in these works. Following the general rules we mentioned, all considered binarization algorithms should have significant contributions to the improvement of the binarization operator (Operator Techniques: Yes) and should not include techniques that are bound to specific architectures and training pipelines to complete well all the evaluations of the learning task, neural architecture, and training consumption tracks in BiBench (Specified Structure / Training-pipeline: No/Optional, Optional means the techniques are included but can be decoupled with binarized operator totally). We also consider the impact and reproducibility of these works. We prioritized the selection of works with more than 100 citations, which means they are more discussed and compared in binarization research and thus have higher impacts. Works in 2021 and later are regarded as the SOTA binarization algorithms and prioritized. Furthermore, we hope the selected works have official open-source implementations for reproducibility.\\n\\nBased on the above selections, eight binarization algorithms, i.e., BNN, XNOR-Net, DoReFa-Net, Bi-Real Net, XNOR-Net++, ReActNet, FDA, and ReCU, stand out and are fully evaluated by our BiBench.\\n\\nAlgorithm Details:\\n- BNN (Courbariaux et al., 2016b): During the training process, BNN uses the straight-through estimator (STE) to calculate gradient $g_x$ which takes into account the saturation effect:\"}"}
{"id": "qin23b", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Hardware          | Arch. | Larq daBNN | FP32 BNN | ReAct | FP32 BNN |\\n|-------------------|-------|------------|----------|-------|----------|\\n| **Apple M1**      |       |            |          |       |          |\\n|                   |       | Apple M1   |          |       |          |\\n|                   |       | 1          |          |       |          |\\n| ResNet18          |       | 44.334     | 8.219    | 8.355 | \u2013         |\\n| ResNet34          |       | 88.334     | 12.505   | 12.771| \u2013         |\\n| VGG-Small         |       | 14.093     | 1.446    | 1.465 | \u2013         |\\n|                   |       | 2          |          |       |          |\\n| ResNet18          |       | 24.775     | 5.037    | 5.194 | \u2013         |\\n| ResNet34          |       | 47.179     | 7.425    | 7.690 | \u2013         |\\n| VGG-Small         |       | 7.398      | 0.829    | 0.854 | \u2013         |\\n|                   |       | 4          |          |       |          |\\n| ResNet18          |       | 18.612     | 3.448    | 3.671 | \u2013         |\\n| ResNet34          |       | 27.515     | 4.965    | 5.254 | \u2013         |\\n| VGG-Small         |       | 4.294      | 0.526    | 0.551 | \u2013         |\\n|                   |       | 8          |          |       |          |\\n| ResNet18          |       | 16.653     | 5.035    | 6.003 | \u2013         |\\n| ResNet34          |       | 27.680     | 6.445    | 6.953 | \u2013         |\\n| VGG-Small         |       | 3.996      | 0.735    | 0.712 | \u2013         |\\n|                   |       | 16         |          |       |          |\\n| ResNet18          |       | 90.323     | 70.697   | 73.729| \u2013         |\\n| ResNet34          |       | 162.057    | 130.907  | 125.362| \u2013        |\\n| VGG-Small         |       | 25.366     | 23.050   | 23.194| \u2013         |\\n| **Apple M1 Max**  |       |            |          |       |          |\\n|                   |       | Apple M1   |          |       |          |\\n|                   |       | 1          |          |       |          |\\n| ResNet18          |       | 46.053     | 8.653    | 8.486 | \u2013         |\\n| ResNet34          |       | 91.861     | 12.593   | 13.039| \u2013         |\\n| VGG-Small         |       | 14.285     | 1.454    | 1.488 | \u2013         |\\n|                   |       | 2          |          |       |          |\\n| ResNet18          |       | 25.039     | 5.450    | 5.361 | \u2013         |\\n| ResNet34          |       | 51.860     | 7.579    | 8.925 | \u2013         |\\n| VGG-Small         |       | 7.657      | 0.855    | 0.896 | \u2013         |\\n|                   |       | 4          |          |       |          |\\n| ResNet18          |       | 14.708     | 3.625    | 3.888 | \u2013         |\\n| ResNet34          |       | 27.933     | 5.266    | 6.021 | \u2013         |\\n| VGG-Small         |       | 4.292      | 0.576    | 0.620 | \u2013         |\\n|                   |       | 8          |          |       |          |\\n| ResNet18          |       | 10.660     | 3.718    | 4.510 | \u2013         |\\n| ResNet34          |       | 18.988     | 4.745    | 5.457 | \u2013         |\\n| VGG-Small         |       | 3.432      | 0.560    | 0.629 | \u2013         |\\n|                   |       | 16         |          |       |          |\\n| ResNet18          |       | 60.500     | 47.727   | 53.900| \u2013         |\\n| ResNet34          |       | 120.449    | 91.464   | 96.356| \u2013         |\\n| VGG-Small         |       | 21.354     | 13.868   | 15.311| \u2013         |\"}"}
{"id": "qin23b", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 21: Accuracy and efficiency comparison among multi-bit quantization (2&8-bits), pruning, and binarization.\\n\\n| Model                      | CIFAR10-Res18 Accuracy | CIFAR10-Res20 Accuracy | FLOPs | Param |\\n|----------------------------|------------------------|------------------------|-------|-------|\\n| FP32                       | 94.82                  | 91.99                  | 13.61 | 11690 |\\n| Binary (overall)           | 91.08 (-3.74)          | 85.74 (-6.25)          | 1.105 (12.32 \u00d7) | 884 (13.22 \u00d7) |\\n| DoReFa-INT2 (Zhou et al., 2016) | 92.71                  | 87.56                  | 1.686 | 1681  |\\n| PACT-INT2 (Choi et al., 2018) | 92.98                  | 88.12                  | 1.686 | 1681  |\\n| LSQ-INT2 (Esser et al., 2019) | 93.11                  | 89.26                  | 1.686 | 1681  |\\n| INT2 (overall)             | 92.93 (-1.89)          | 88.31 (-3.68)          | 1.686 (8.07 \u00d7) | 1681 (6.95 \u00d7) |\\n| DoReFa-INT8 (Zhou et al., 2016) | 94.79                  | 91.83                  | 7.278 | 4067  |\\n| PACT-INT8 (Choi et al., 2018) | 94.80                  | 91.87                  | 7.278 | 4067  |\\n| LSQ-INT8 (Esser et al., 2019) | 94.78                  | 91.95                  | 7.278 | 4067  |\\n| INT8 (overall)             | 94.79 (-0.03)          | 91.88 (-0.11)          | 7.278 (1.87 \u00d7) | 4067 (2.87 \u00d7) |\\n| (Li et al., 2016)          | 94.47                  | 91.32                  | 9.527 | 9936  |\\n| ResRep (Ding et al., 2021) | 94.53                  | 91.76                  | 6.805 | 7247  |\\n| Pruning (overall)          | 94.50 (-0.32)          | 91.54 (-0.45)          | 8.166 (1.67 \u00d7) | 8591 (1.36 \u00d7) |\"}"}
{"id": "qin23b", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Algorithm | Epoch | Optimizer | Learning Rate | Scheduler | Acc. 1 | Acc. 2 | Acc. 3 | Acc. 4 | mean | std |\\n|-----------|-------|-----------|---------------|-----------|--------|--------|--------|--------|------|-----|\\n| XNOR++    | 200   | SGD       | 0.1           | cosine    | 87.82  | 88.42  | 88.12  | 88.55  | 88.19 | 0.28 |\\n|           |       |           |               | step      | 73.55  | 73.11  | 75.06  | 74.05  | 73.78 | 0.73 |\\n|           |       |           | 0.01          | cosine    | 74.03  | 75.06  | 73.64  | 74.53  | 74.71 | 0.56 |\\n|           |       |           |               | step      | 53.55  | 54.16  | 54.01  | 52.91  | 54.36 | 0.58 |\\n|           |       | Adam      | 0.001         | cosine    | 88.77  | 88.65  | 89.10  | 88.61  | 88.81 | 0.19 |\\n|           |       |           |               | step      | 89.18  | 89.05  | 89.27  | 88.93  | 89.00 | 0.14 |\\n|           |       |           | 0.0001        | cosine    | 83.86  | 83.49  | 83.56  | 83.16  | 83.62 | 0.25 |\\n|           |       |           |               | step      | 83.46  | 83.77  | 84.40  | 84.06  | 83.82 | 0.35 |\\n| ReActNet  | 200   | SGD       | 0.1           | cosine    | 88.60  | 88.53  | 88.38  | 88.48  | 88.89 | 0.19 |\\n|           |       |           |               | step      | 88.42  | 88.01  | 88.10  | 88.02  | 88.43 | 0.21 |\\n|           |       |           | 0.01          | cosine    | 87.75  | 87.86  | 88.00  | 87.80  | 88.02 | 0.12 |\\n|           |       |           |               | step      | 83.29  | 82.89  | 83.65  | 83.76  | 83.27 | 0.35 |\\n|           |       | Adam      | 0.001         | cosine    | 89.47  | 89.29  | 89.01  | 89.05  | 89.14 | 0.19 |\\n|           |       |           |               | step      | 89.27  | 89.74  | 89.48  | 89.40  | 89.39 | 0.18 |\\n|           |       |           | 0.0001        | cosine    | 84.65  | 84.93  | 84.48  | 84.65  | 84.67 | 0.16 |\\n|           |       |           |               | step      | 84.69  | 84.55  | 84.93  | 84.94  | 85.38 | 0.32 |\\n| ReCU      | 200   | SGD       | 0.1           | cosine    | 91.72  | 91.94  | 91.68  | 91.69  | 91.81 | 0.11 |\\n|           |       |           |               | step      | 87.73  | 88.14  | 87.81  | 88.02  | 87.91 | 0.16 |\\n|           |       |           | 0.01          | cosine    | 87.32  | 87.28  | 87.53  | 87.48  | 87.32 | 0.11 |\\n|           |       |           |               | step      | 71.86  | 71.72  | 71.78  | 72.26  | 71.59 | 0.25 |\\n|           |       | Adam      | 0.001         | cosine    | 88.24  | 89.98  | 88.26  | 88.48  | 88.13 | 0.77 |\\n|           |       |           |               | step      | 88.36  | 88.48  | 88.55  | 88.42  | 88.63 | 0.11 |\\n|           |       |           | 0.0001        | cosine    | 80.07  | 81.10  | 80.62  | 81.09  | 79.95 | 0.55 |\\n|           |       |           |               | step      | 81.26  | 81.42  | 81.08  | 81.58  | 81.69 | 0.32 |\\n| FDA       | 200   | SGD       | 0.1           | cosine    | 89.69  | 89.59  | 89.56  | 89.53  | 89.65 | 0.07 |\\n|           |       |           |               | step      | 80.38  | 80.34  | 80.83  | 80.52  | 80.52 | 0.19 |\\n|           |       |           | 0.01          | cosine    | 80.72  | 80.93  | 80.89  | 80.70  | 80.79 | 0.10 |\\n|           |       |           |               | step      | 63.41  | 62.85  | 63.04  | 63.04  | 63.14 | 0.20 |\\n|           |       | Adam      | 0.001         | cosine    | 89.70  | 89.57  | 89.57  | 89.80  | 89.76 | 0.11 |\\n|           |       |           |               | step      | 89.84  | 89.85  | 90.10  | 89.79  | 90.01 | 0.13 |\\n|           |       |           | 0.0001        | cosine    | 89.59  | 89.10  | 89.34  | 89.31  | 89.51 | 0.19 |\\n|           |       |           |               | step      | 89.52  | 89.59  | 89.52  | 89.64  | 89.58 | 0.05 |\"}"}
{"id": "qin23b", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Hardware | Arch. | Larq daBNN | ReAct | FP32 BNN | ReAct | FP32 BNN |\\n|----------|-------|-----------|-------|----------|-------|----------|\\n| Kirin 970 |       |           |       |          |       |          |\\n|          | 1     | 1         | 1     | 516.427  | 123.263 | 126.457  |\\n|          |       | 1         | 2     | 1449.67  | 159.615 | 171.227  |\\n|          |       | 1         | 4     | 242.443  | 14.833  | 16.401   |\\n|          |       | 2         | 3     | 191.517  | 42.986  | 47.182   |\\n|          |       | 4         | 7     | 96.937   | 37.457  | 56.017   |\\n| Kirin 980 |       |           |       |          |       |          |\\n|          | 1     | 1         | 1     | 307.624  | 49.009  | 50.018   |\\n|          |       | 1         | 2     | 367.355  | 71.909  | 74.920   |\\n|          |       | 1         | 4     | 83.163   | 7.772   | 8.215    |\\n|          |       | 2         | 3     | 187.494  | 52.057  | 54.285   |\\n|          |       | 4         | 7     | 104.076  | 29.556  | 35.539   |\\n|          |       | 8         | 8     | 60.307   | 45.683  | 56.416   |\\n| Kirin 985 |       |           |       |          |       |          |\\n|          | 1     | 1         | 1     | 173.238  | 27.429  | 30.626   |\\n|          |       | 1         | 2     | 187.494  | 25.672  | 35.477   |\\n|          |       | 1         | 4     | 104.076  | 29.556  | 35.539   |\\n|          |       | 2         | 3     | 60.307   | 45.683  | 56.416   |\\n|          |       | 4         | 7     | 60.307   | 45.683  | 56.416   |\\n|          |       | 8         | 8     | 60.307   | 45.683  | 56.416   |\\n| Kirin 990 |       |           |       |          |       |          |\\n|          | 1     | 1         | 1     | 114.238  | 21.235  | 22.066   |\\n|          |       | 1         | 2     | 59.329   | 13.911  | 14.179   |\\n|          |       | 1         | 4     | 38.403   | 10.280  | 12.208   |\\n|          |       | 2         | 3     | 38.403   | 10.280  | 12.208   |\\n|          |       | 4         | 7     | 38.403   | 10.280  | 12.208   |\\n|          |       | 8         | 8     | 38.403   | 10.280  | 12.208   |\"}"}
{"id": "qin23b", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Hardware      | Arch.  | Larq daBNN | ReAct | FP32 BNN | ReAct | FP32 BNN |\\n|--------------|--------|------------|-------|----------|-------|----------|\\n| Kirin 9000E  |        | 1          |       |          | 1     |          |\\n| ResNet18     | 118.059| 19.865     | 20.547| 129.270  | 24.781|          |\\n| ResNet34     | 236.047| 31.822     | 32.575| 250.680  | 42.134|          |\\n| VGG-Small    | 39.114 | 3.595      | 3.832 | \u2013        | \u2013     | \u2013        |\\n| Dimensity 820|        | 2          |       |          | 2     |          |\\n| ResNet18     | 68.351 | 16.821     | 16.115| \u2013        | \u2013     | \u2013        |\\n| ResNet34     | 133.671| 25.061     | 24.660| \u2013        | \u2013     | \u2013        |\\n| VGG-Small    | 23.018 | 2.684      | 2.598 | \u2013        | \u2013     | \u2013        |\\n| Dimensity 9000|       | 4          |       |          | 4     |          |\\n| ResNet18     | 45.592 | 17.452     | 18.847| \u2013        | \u2013     | \u2013        |\\n| ResNet34     | 91.648 | 23.395     | 28.022| \u2013        | \u2013     | \u2013        |\\n| VGG-Small    | 14.360 | 2.762      | 2.782 | \u2013        | \u2013     | \u2013        |\\n| Snapdragon 855+|      | 8          |       |          | 8     |          |\\n| ResNet18     | 43.363 | 61.263     | 42.328| \u2013        | \u2013     | \u2013        |\\n| ResNet34     | 89.405 | 70.232     | 93.558| \u2013        | \u2013     | \u2013        |\\n| VGG-Small    | 19.070 | 17.351     | 23.825| \u2013        | \u2013     | \u2013        |\"}"}
{"id": "qin23b", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Hardware | ResNet18 | ResNet34 | VGG-Small |\\n|----------|---------|---------|-----------|\\n| Snapdragon 870 |\\n| 1 | 88.145 | 16.527 | 17.020 | 126.762 | 25.240 |\\n| 2 | 63.829 | 18.351 | 19.575 | \u2013 | \u2013 |\\n| 4 | 42.796 | 17.578 | 21.083 | \u2013 | \u2013 |\\n| 8 | 46.798 | 19.192 | 28.579 | \u2013 | \u2013 |\\n| Snapdragon 888 |\\n| 1 | 77.205 | 15.547 | 16.111 | 123.618 | 25.240 |\\n| 2 | 46.297 | 19.309 | 19.321 | \u2013 | \u2013 |\\n| 4 | 33.524 | 13.699 | 14.332 | \u2013 | \u2013 |\\n| 8 | 33.761 | 26.108 | 58.989 | \u2013 | \u2013 |\\n| Raspberrypi 3B+ |\\n| 1 | 740.509 | 158.732 | 175.256 | 1460.723 | 241.713 |\\n| 2 | 667.012 | 143.716 | 106.894 | \u2013 | \u2013 |\\n| 4 | 562.567 | 108.585 | 116.640 | \u2013 | \u2013 |\\n| 8 | 877.026 | 279.660 | 356.239 | \u2013 | \u2013 |\\n| Raspberrypi 4B |\\n| 1 | 448.744 | 80.822 | 82.380 | 688.838 | 120.348 |\\n| 2 | 261.861 | 49.079 | 55.279 | \u2013 | \u2013 |\\n| 4 | 270.191 | 36.331 | 45.903 | \u2013 | \u2013 |\\n| 8 | 466.585 | 168.844 | 226.771 | \u2013 | \u2013 |\"}"}
{"id": "qin23b", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 12: Accuracy on Language and Speech Tasks.\\n\\n| Task | Arch. | FP32 | Binarization Algorithm |\\n|------|-------|------|------------------------|\\n|      |       |      | BNN | XNOR | DoReFa | Bi-Real | XNOR++ | ReActNet | ReCU | FDA |\\n| GLUE |       |      |     |      |       |         |        |          |      |     |\\n| MNLI-m | BERT-Tiny | 4L   | 82.81 | 36.90 | 41.20  | 52.31  | 55.09  | 37.27  | 55.52 | 38.55 |\\n|       | BERT-Tiny | 6L   | 84.76 | 37.01 | 51.17  | 63.09  | 66.81  | 37.98  | 66.47 | 37.95 |\\n|       | BERT-Base | 84.88 | 35.45 | 41.40 | 60.67  | 62.47  | 35.45  | 60.22  | 35.45 | 63.49 |\\n| MNLI-mm | BERT-Tiny | 4L   | 83.08 | 36.54 | 41.55  | 53.01  | 55.57  | 36.07  | 55.89 | 37.62 |\\n|       | BERT-Tiny | 6L   | 84.42 | 36.47 | 50.92  | 63.87  | 66.82  | 38.11  | 67.64 | 36.91 |\\n|       | BERT-Base | 85.45 | 35.22 | 41.18 | 60.96  | 63.17  | 35.22  | 61.19  | 35.22 | 63.72 |\\n| QQP  | BERT-Tiny | 4L   | 90.47 | 66.19 | 73.69  | 75.79  | 77.38  | 64.97  | 76.92 | 67.32 |\\n|       | BERT-Tiny | 6L   | 85.98 | 63.18 | 78.90  | 80.93  | 82.42  | 63.19  | 82.95 | 63.3  |\\n|       | BERT-Base | 91.51 | 63.18 | 71.93 | 77.07  | 80.01  | 63.18  | 81.16  | 63.18 | 83.26 |\\n| SST-2 | BERT-Tiny | 4L   | 92.43 | 52.98 | 79.93  | 82.45  | 84.06  | 54.01  | 84.17 | 54.24 |\\n|       | BERT-Tiny | 6L   | 90.25 | 58.14 | 84.74  | 86.23  | 87.73  | 69.38  | 87.95 | 52.40 |\\n|       | BERT-Base | 93.23 | 52.29 | 78.78 | 86.01  | 86.35  | 53.32  | 84.40  | 52.40 | 87.93 |\\n| CoLA | BERT-Tiny | 4L   | 49.61 | 6.55  | 7.22   | 12.69  | 16.86  | 0      | 14.71 | 6.25  |\\n|       | BERT-Tiny | 6L   | 54.17 | 2.57  | 12.57  | 15.97  | 17.94  | 0      | 15.24 | 2.24  |\\n|       | BERT-Base | 59.71 | 4.63  | 0     | 4.74   | 15.95  | 0      | 4.63   | 0.40  | 4.63  |\\n| STS-B | BERT-Tiny | 4L   | 86.35 | 4.31  | 18.05  | 18.74  | 22.65  | 7.45   | 22.73 | 8.20  |\\n|       | BERT-Tiny | 6L   | 89.79 | 1.04  | 14.72  | 22.31  | 24.59  | 5.70   | 23.40 | 8.22  |\\n|       | BERT-Base | 90.06 | 6.94  | 12.19 | 18.26  | 20.76  | 4.99   | 8.73   | 6.59  | 10.14 |\\n| MRPC | BERT-Tiny | 4L   | 85.50 | 68.30 | 71.74  | 71.99  | 71.74  | 68.30  | 71.74 | 71.25 |\\n|       | BERT-Tiny | 6L   | 87.71 | 68.30 | 70.76  | 71.74  | 71.49  | 68.30  | 71.74 | 69.04 |\\n|       | BERT-Base | 86.24 | 68.30 | 68.30 | 70.02  | 70.27  | 68.30  | 71.25  | 68.30 | 69.04 |\\n| RTE  | BERT-Tiny | 4L   | 65.34 | 56.31 | 53.43  | 56.31  | 55.59  | 54.15  | 57.76 | 61.01 |\\n|       | BERT-Tiny | 6L   | 68.95 | 56.31 | 54.51  | 54.51  | 58.12  | 49.09  | 53.43 | 58.84 |\\n|       | BERT-Base | 72.20 | 53.43 | 57.04 | 55.23  | 54.51  | 54.87  | 55.23  | 55.23 | 55.23 |\\n| Speech Commands | FSMN | 94.89 | 56.45 | 56.45 | 68.65 | 73.60 | 75.04 | 73.80 | 56.45 | 56.45 |\\n|       | D-FSMN  | 97.51 | 88.32 | 92.03 | 78.92 | 85.11 | 56.77 | 83.80 | 92.11 | 93.91 |\"}"}
{"id": "qin23b", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Noise Type | Origin | FPN32 | BNN | XNOR | DoReFa | Bi-Real | XNOR++ | ReActNet | FDA |\\n|------------|--------|-------|-----|------|--------|---------|--------|----------|-----|\\n| Gaussian Noise-1 | 94.82 | 89.69 | 91.40 | 91.55 | 91.20 | 90.04 | 91.55 | 92.79 | 90.42 |\\n| Gaussian Noise-2 | 78.23 | 74.22 | 76.00 | 74.97 | 74.95 | 75.07 | 75.15 | 78.25 | 77.36 |\\n| Gaussian Noise-3 | 56.72 | 56.73 | 62.44 | 55.94 | 58.33 | 57.52 | 55.97 | 61.32 | 60.48 |\\n| Gaussian Noise-4 | 36.93 | 42.69 | 47.58 | 39.56 | 43.47 | 40.79 | 37.99 | 43.32 | 44.26 |\\n| Gaussian Noise-5 | 31.03 | 38.35 | 41.43 | 33.24 | 36.65 | 34.68 | 31.47 | 35.91 | 37.30 |\\n| Impulse Noise-1  | 82.54 | 84.57 | 86.94 | 84.68 | 87.30 | 85.72 | 86.89 | 88.73 | 85.80 |\\n| Impulse Noise-2  | 70.12 | 77.13 | 80.74 | 77.35 | 81.14 | 79.62 | 80.25 | 82.80 | 80.30 |\\n| Impulse Noise-3  | 59.88 | 70.58 | 75.01 | 69.20 | 74.59 | 71.82 | 72.16 | 76.05 | 72.60 |\\n| Impulse Noise-4  | 40.59 | 54.42 | 59.39 | 49.48 | 56.66 | 52.61 | 49.79 | 58.44 | 56.45 |\\n| Impulse Noise-5  | 26.03 | 39.86 | 41.54 | 32.72 | 37.28 | 35.12 | 28.42 | 38.26 | 39.98 |\\n| Shot Noise-1     | 85.75 | 81.51 | 81.31 | 81.84 | 81.58 | 80.66 | 81.88 | 83.98 | 82.42 |\\n| Shot Noise-2     | 76.61 | 72.04 | 74.02 | 72.21 | 72.81 | 73.03 | 72.32 | 76.70 | 75.10 |\\n| Shot Noise-3     | 52.21 | 53.90 | 57.08 | 50.66 | 54.59 | 53.76 | 51.31 | 57.22 | 56.56 |\\n| Shot Noise-4     | 44.13 | 47.58 | 51.29 | 43.59 | 48.36 | 46.64 | 44.21 | 48.78 | 48.91 |\\n| Shot Noise-5     | 32.73 | 39.93 | 40.79 | 33.80 | 38.50 | 36.47 | 31.79 | 36.46 | 37.80 |\\n| Speckle Noise-1  | 86.30 | 81.29 | 81.94 | 80.93 | 80.77 | 81.14 | 82.17 | 84.17 | 82.62 |\\n| Speckle Noise-2  | 71.94 | 68.07 | 70.14 | 67.50 | 69.22 | 69.35 | 68.26 | 72.94 | 71.70 |\\n| Speckle Noise-3  | 64.47 | 62.12 | 64.13 | 60.24 | 63.44 | 62.50 | 61.14 | 66.89 | 64.27 |\\n| Speckle Noise-4  | 49.81 | 51.93 | 53.77 | 47.93 | 52.75 | 50.59 | 48.39 | 54.13 | 52.40 |\\n| Speckle Noise-5  | 38.70 | 44.25 | 43.60 | 38.65 | 43.16 | 42.09 | 37.78 | 42.13 | 42.57 |\\n| Gaussian Blur-1  | 94.17 | 89.03 | 90.50 | 89.33 | 90.56 | 89.00 | 91.05 | 92.16 | 89.33 |\\n| Gaussian Blur-2  | 87.04 | 78.30 | 81.98 | 78.81 | 80.42 | 77.75 | 81.20 | 84.80 | 78.93 |\\n| Gaussian Blur-3  | 75.15 | 67.74 | 68.27 | 67.67 | 68.16 | 64.54 | 67.42 | 73.62 | 66.29 |\\n| Gaussian Blur-4  | 59.50 | 55.17 | 53.63 | 55.74 | 54.08 | 52.44 | 52.72 | 60.32 | 53.37 |\\n| Gaussian Blur-5  | 36.03 | 37.31 | 33.96 | 37.50 | 37.54 | 36.77 | 34.08 | 39.22 | 34.93 |\\n| Defocus Blur-1   | 94.20 | 88.73 | 91.06 | 89.10 | 90.32 | 88.91 | 90.92 | 91.98 | 89.58 |\\n| Defocus Blur-2   | 92.75 | 85.97 | 88.99 | 86.59 | 88.31 | 85.58 | 87.91 | 90.47 | 87.01 |\\n| Defocus Blur-3   | 87.38 | 79.02 | 82.43 | 78.88 | 80.71 | 77.58 | 80.88 | 84.85 | 79.52 |\\n| Defocus Blur-4   | 76.99 | 69.13 | 71.02 | 68.29 | 68.33 | 65.96 | 68.42 | 74.40 | 68.22 |\\n| Defocus Blur-5   | 52.09 | 48.85 | 51.99 | 48.82 | 49.17 | 48.45 | 46.92 | 55.70 | 48.27 |\\n| Glass Blur-1     | 54.93 | 56.57 | 51.72 | 57.94 | 56.78 | 57.29 | 56.27 | 58.82 | 58.92 |\\n| Glass Blur-2     | 56.37 | 57.93 | 53.46 | 60.42 | 59.21 | 59.32 | 58.03 | 60.25 | 60.56 |\\n| Glass Blur-3     | 59.21 | 61.43 | 56.98 | 64.11 | 61.72 | 62.41 | 60.39 | 62.84 | 63.32 |\\n| Glass Blur-4     | 45.65 | 46.50 | 42.72 | 48.48 | 47.19 | 47.83 | 46.88 | 49.09 | 49.23 |\\n| Glass Blur-5     | 49.19 | 49.52 | 46.40 | 52.06 | 49.83 | 50.02 | 49.08 | 51.14 | 51.82 |\\n| Motion Blur-1    | 89.40 | 81.57 | 83.27 | 82.00 | 83.11 | 81.48 | 84.19 | 86.21 | 82.83 |\\n| Motion Blur-2    | 81.95 | 71.52 | 74.71 | 73.38 | 72.48 | 70.99 | 74.35 | 77.75 | 74.09 |\\n| Motion Blur-3    | 72.48 | 61.87 | 66.21 | 63.86 | 63.39 | 61.57 | 63.85 | 68.31 | 64.36 |\\n| Motion Blur-4    | 72.79 | 62.40 | 66.18 | 63.94 | 62.84 | 62.03 | 64.54 | 67.88 | 64.13 |\\n| Motion Blur-5    | 63.91 | 54.14 | 57.98 | 56.07 | 55.90 | 54.35 | 55.60 | 59.71 | 56.65 |\\n| Zoo Blur-1       | 87.36 | 78.25 | 81.31 | 78.56 | 79.45 | 77.20 | 80.22 | 83.69 | 78.55 |\\n| Zoo Blur-2       | 83.89 | 74.84 | 77.73 | 75.39 | 75.88 | 72.83 | 75.74 | 80.46 | 74.72 |\\n| Zoo Blur-3       | 77.73 | 69.00 | 70.98 | 68.81 | 69.03 | 66.56 | 68.34 | 74.33 | 68.21 |\\n| Zoo Blur-4       | 71.39 | 64.12 | 65.21 | 63.79 | 62.81 | 61.01 | 62.47 | 68.67 | 62.58 |\\n| Zoo Blur-5       | 60.60 | 55.15 | 55.83 | 55.40 | 54.38 | 52.66 | 52.24 | 59.51 | 53.94 |\\n| Brightness-1     | 94.31 | 89.29 | 90.84 | 89.53 | 90.80 | 89.30 | 90.97 | 92.06 | 89.74 |\\n| Brightness-2     | 94.03 | 88.25 | 90.42 | 88.71 | 89.66 | 88.50 | 90.64 | 91.64 | 88.77 |\\n| Brightness-3     | 93.53 | 87.40 | 89.38 | 87.38 | 89.17 | 87.31 | 89.63 | 90.72 | 87.84 |\\n| Brightness-4     | 92.74 | 85.45 | 88.27 | 86.12 | 87.78 | 85.44 | 88.16 | 89.58 | 86.39 |\\n| Brightness-5     | 90.36 | 80.95 | 85.22 | 81.65 | 83.79 | 80.99 | 84.45 | 86.53 | 82.04 |\"}"}
{"id": "qin23b", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 14: Results for Robustness Corruption on CIFAR10-C Dataset with Different Binarization Algorithms (2/2).\\n\\n| Noise | FP32 | Binarization Algorithm | Algorithm | Algorithm | Algorithm | Algorithm | Algorithm | Algorithm |\\n|-------|------|------------------------|-----------|-----------|-----------|-----------|-----------|-----------|\\n| fog-1 | 94.04| XNOR                   | 88.17     | 90.89     | 88.84     | 89.91     | 88.51     | 90.84     | 89.43     |\\n| fog-2 | 93.03| DoReFa                 | 84.58     | 88.85     | 85.48     | 87.26     | 84.77     | 88.00     | 86.76     |\\n| fog-3 | 90.69| Bi-Real                | 78.07     | 85.20     | 80.07     | 83.32     | 78.94     | 83.77     | 82.78     |\\n| fog-4 | 86.72| XNOR++                 | 69.56     | 78.92     | 72.27     | 75.89     | 71.01     | 77.96     | 75.62     |\\n| fog-5 | 68.60| ReActNet               | 49.04     | 53.90     | 52.33     | 52.88     | 49.68     | 57.67     | 55.18     |\\n| frost-1|89.97 | FDA                    | 83.66     | 84.70     | 84.07     | 85.76     | 83.64     | 85.51     | 84.85     |\\n| frost-2|84.42 | contrast               | 77.88     | 78.97     | 77.47     | 78.96     | 77.26     | 79.14     | 79.16     |\\n| frost-3|74.85 | jpeg compression        | 67.67     | 69.30     | 67.14     | 68.76     | 66.03     | 69.58     | 72.54     |\\n| frost-4|73.32 | elastic transform       | 65.93     | 67.14     | 65.97     | 68.52     | 65.37     | 67.93     | 69.41     |\\n| frost-5|62.13 | jpeg compression        | 55.02     | 56.67     | 55.62     | 56.77     | 54.26     | 57.69     | 59.88     |\\n| Overall|74.11 |                        | 69.43     | 71.51     | 69.36     | 70.76     | 69.09     | 70.31     | 73.56     | 70.70     |\"}"}
{"id": "qin23b", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 15: Sensitivity to Hyper Parameters in Training (1/2)\\n\\n| Algorithm | Epoch | Optimizer | Learning Rate | Scheduler | Acc. (mean) | Acc. (std) |\\n|-----------|-------|-----------|---------------|-----------|-------------|------------|\\n| FP32 | 200 | SGD | 0.1 | cosine | 94.58 | 0.20 |\\n| | | | | step | 92.63 | 0.20 |\\n| | | | 0.01 | cosine | 92.23 | 0.22 |\\n| | | | | step | 83.94 | 0.53 |\\n| | Adam | 0.001 | cosine | 93.51 | 0.27 |\\n| | | | step | 93.37 | 0.10 |\\n| | | 0.0001 | cosine | 89.97 | 0.03 |\\n| | | | step | 90.57 | 0.25 |\\n| BNN | 200 | SGD | 0.1 | cosine | 87.62 | 0.53 |\\n| | | | step | 70.87 | 1.17 |\\n| | | | 0.01 | cosine | 73.52 | 0.87 |\\n| | | | | step | 52.85 | 0.57 |\\n| | Adam | 0.001 | cosine | 88.76 | 0.12 |\\n| | | | step | 88.85 | 0.22 |\\n| | | 0.0001 | cosine | 83.46 | 0.25 |\\n| | | | step | 84.08 | 0.29 |\\n| XNOR | 200 | SGD | 0.1 | cosine | 91.83 | 0.18 |\\n| | | | step | 90.02 | 0.17 |\\n| | | | 0.01 | cosine | 90.09 | 0.16 |\\n| | | | | step | 86.86 | 0.24 |\\n| | Adam | 0.001 | cosine | 89.39 | 0.20 |\\n| | | | step | 89.92 | 0.16 |\\n| | | 0.0001 | cosine | 86.18 | 0.34 |\\n| | | | step | 86.32 | 0.34 |\\n| DoReFa | 200 | SGD | 0.1 | cosine | 85.64 | 0.15 |\\n| | | | step | 86.95 | 0.17 |\\n| | | | 0.01 | cosine | 86.56 | 0.14 |\\n| | | | | step | 78.76 | 0.76 |\\n| | Adam | 0.001 | cosine | 88.85 | 0.11 |\\n| | | | step | 89.08 | 0.16 |\\n| | | 0.0001 | cosine | 83.56 | 0.20 |\\n| | | | step | 83.70 | 0.26 |\\n| Bi-Real | 200 | SGD | 0.1 | cosine | 87.55 | 0.30 |\\n| | | | step | 87.95 | 0.25 |\\n| | | | 0.01 | cosine | 87.76 | 0.11 |\\n| | | | | step | 83.75 | 0.40 |\\n| | Adam | 0.001 | cosine | 88.78 | 0.16 |\\n| | | | step | 88.89 | 0.13 |\\n| | | 0.0001 | cosine | 83.96 | 0.31 |\\n| | | | step | 84.63 | 0.20 |\"}"}
