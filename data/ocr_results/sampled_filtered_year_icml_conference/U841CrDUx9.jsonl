{"id": "U841CrDUx9", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nDecision-making problems, categorized as single-agent, e.g., Atari, cooperative multi-agent, e.g., Hanabi, competitive multi-agent, e.g., Hold'em poker, and mixed cooperative and competitive, e.g., football, are ubiquitous in the real world. Although various methods have been proposed to address the specific decision-making categories, these methods typically evolve independently and cannot generalize to other categories. Therefore, a fundamental question for decision-making is: Can we develop a single algorithm to tackle ALL categories of decision-making problems?\\n\\nThere are several main challenges to address this question: i) different categories involve different numbers of agents and different relationships between agents, ii) different categories have different solution concepts and evaluation measures, and iii) there lacks a comprehensive benchmark covering all the categories. This work presents a preliminary attempt to address the question with three main contributions. i) We propose the generalized mirror descent (GMD), a generalization of MD variants, which considers multiple historical policies and works with a broader class of Bregman divergences. ii) We propose the configurable mirror descent (CMD) where a meta-controller is introduced to dynamically adjust the hyperparameters in GMD conditional on the evaluation measures. iii) We construct the GAMESBENCH with 15 academic-friendly games across different decision-making categories. Extensive experiments demonstrate that CMD achieves empirically competitive or better outcomes compared to baselines while providing the capability of exploring diverse dimensions of decision making.\\n\\n*Equal contribution\\n1. Introduction\\nDecision-making problems are pervasive in the real world (Sutton & Barto, 2018; Shoham & Leyton-Brown, 2008), which can be generally categorized into single-agent, e.g., Atari (Mnih et al., 2015), cooperative multi-agent, e.g., Hanabi game (Bard et al., 2020), competitive multi-agent, e.g., Hold'em poker (Brown & Sandholm, 2018; 2019), and mixed cooperative and competitive (MCC), e.g., football (Kurach et al., 2020; Liu et al., 2022a). To solve these problems, various methods are proposed where notable examples include PPO (Schulman et al., 2017) for single-agent category, QMIX (Rashid et al., 2018) for cooperative multi-agent category and PSRO (Lanctot et al., 2017) for competitive category. Despite the successes in specific categories, these methods are developed almost independently and cannot generalize to other categories. Therefore, a fundamental question for decision making to answer is: Can we develop a single algorithm to tackle ALL categories of decision-making problems?\\n\\nFigure 1. Overview of the categories of decision making and the four desiderata for the required method to satisfy.\"}"}
{"id": "U841CrDUx9", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Configurable Mirror Descent: Towards a Unification of Decision Making and cooperative multi-agent categories, while for the competitive and MCC multi-agent categories, Nash equilibrium (NE) (Nash, 1951) is the canonical solution concept and other solution concepts, e.g., correlated equilibrium (Aumann, 1987) are also considered. Furthermore, even for one solution concept, e.g., NE, there are different evaluation measures, e.g., NashConv or NashConv with social welfare and fairness.\\n\\nTo summarize the challenges, we propose the four desiderata that the methods should satisfy:\\n\\n- **D1**: Applicable to single- and multi-agent categories\\n- **D2**: Applicable to coop., comp., & MCC categories\\n- **D3**: Applicable to different solution concepts\\n- **D4**: Applicable to different evaluation measures\\n\\nAn overall illustration of the categories of the decision making and the desiderata is displayed in Figure 1. Third, existing benchmarks are typically specialized for specific decision-making categories, while a comprehensive benchmark that satisfies the following two desiderata is lacking.\\n\\n- **D5**: (Comprehensive) It covers all categories\\n- **D6**: (Academic-friendly) It is less resource-intensive\\n\\nIn this work, we make a preliminary attempt to address these challenges and provide three main contributions. i) We propose the generalized mirror descent (GMD), a generalization of existing MD algorithms (Nemirovskij & Yudin, 1983; Beck & Teboulle, 2003), which incorporates multiple historical policies into the policy updating and is able to explore a broader class of Bregman divergence by addressing the Karush\u2013Kuhn\u2013Tucker (KKT) conditions at each iteration. As GMD is adopted by each agent independently, it can be applied to different decision-making categories involving different numbers of agents and different relationships between agents (**D1** and **D2**). ii) We propose the configurable mirror descent (CMD) by introducing a meta-controller to dynamically adjust the hyper-parameters in GMD conditional on the evaluation measures, allowing us to study different solution concepts as well as evaluation measures (**D3** and **D4**), with minimal modifications. iii) We construct the GAMEBENCH consisting of 15 games which cover all the decision-making categories (**D5**), and are deliberately constructed with the principle that running algorithms on these games does not require much computational resource (**D6**), and hence, forming a comprehensive and academic-friendly testbed for researchers to efficiently develop and test novel algorithms. Extensive experiments on the GAMEBENCH show that CMD achieves empirically competitive or better outcomes compared to baselines while offering the ability to investigate diverse dimensions of decision making.\\n\\n---\\n\\n1 This is related to the equilibrium selection problem (Harsanyi et al., 1988) and different measures lead to different equilibria.\\n\\n---\\n\\n2 **A Real-World Motivating Scenario**\\n\\nWe provide an illustrative example to highlight the importance and real-world implications of a unified algorithm framework. Consider that a robotic company is developing and selling generalist domestic robots to users. The user may ask the robot to learn to complete different novel tasks, including single-agent, cooperative, competitive, and MCC categories, by specifying the objective. Therefore, if we can deploy a unified algorithm into the robot, the robot can learn to complete different novel tasks with a single algorithm. Developing and deploying such a unified algorithm would benefit both the development and users. For the development side, as only a single policy learning rule is required, the deployment and user interface design could be largely simplified, which would be more cost-efficient than deploying different specialized algorithms such as MAPPO and PSRO as they may complicate the development pipeline and user interface design. For the user side, the user only needs to configure one set of parameters for different novel tasks, e.g., only needs to specify the optimization objective of the meta-controller in our proposed CMD algorithm.\\n\\n---\\n\\n3 **Related Work**\\n\\nThe related literature is too vast to cover in its entirety. We present an overview below to emphasize our contributions while more related works can be found in Appendix B.\\n\\n**Decision Making.** Substantial progress has been achieved in developing algorithms to address different categories of decision-making problems, e.g., DQN (Mnih et al., 2015) and PPO (Schulman et al., 2017) for single-agent category, QMIX (Rashid et al., 2018) and MAPPO (Yu et al., 2022) for cooperative multi-agent category, self-play (Tesauro et al., 1995) and PSRO (Lanctot et al., 2017) for competitive and MCC categories, to name just a few. Despite the successes in specific categories, these methods often cannot directly generalize to different categories. In this work, we make a preliminary attempt to develop a single algorithm capable of tackling all categories of decision-making problems which typically involve different numbers of agents, different relationships between agents, different solution concepts as well as different evaluation measures.\\n\\n**Mirror Descent.** Mirror descent (MD) (Nemirovskij & Yudin, 1983; Beck & Teboulle, 2003; Vural et al., 2022) has shown effectiveness in learning optimal policies in single-agent RL (Tomar et al., 2022) and proved the last-iterate convergence in learning approximate equilibrium in zero-sum games (Bailey & Piliouras, 2018; Kangarshahi et al., 2018; Wibisono et al., 2022; Kozuno et al., 2021; Lee et al., 2021; Jain et al., 2022; Ao et al., 2023; Liu et al., 2023; Cen et al., 2023; Sokota et al., 2023) and some classes of general-sum games, e.g., polymatrix and potential games (Anagnostides 2023).\"}"}
{"id": "U841CrDUx9", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $\u03c8'_{-1}$ is the inverse function of $\u03c8'$. Substituting the above expression for $\u03c0(a)$ into the constraint (10b), we have:\\n\\n$$X_{a \\\\in A} \u03c0(a) = X_{a \\\\in A} \u03c8'_{-1}(A(a)) - \u03bb_B = 1,$$\\n\\n(13)\\n\\nwhich completes the proof.\\n\\nNumerical Method for Computing $\u03bb$.\\n\\nNotably, Eq. (13) typically cannot be solved analytically. To address this problem, we propose to use a numerical method to compute the value of $\u03bb$, offering the possibility of exploring a broader class of Bregman divergence. Specifically, for any convex function $\u03c8$, we employ the Newton method (Ypma, 1995) to compute the value of $\u03bb$, which is shown in Algorithm 3, where $C$ is the number of iterations.\\n\\n**Algorithm 3**\\n\\n1: Given $\u03c8$, $A$, and $B$. Randomly initialize the value of $\u03bb$\\n2: for $C$ iterations do\\n3: $\u03bb = \u03bb - \\\\frac{g(\u03bb)}{g'(\u03bb)}$\\n4: end for\\n\\nDifferent Bregman Divergences.\\n\\nIn Table 2, we list the convex functions considered in our work. To be more intuitive, we plot these convex functions in Figure 8.\"}"}
{"id": "U841CrDUx9", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Configurable Mirror Descent: Towards a Unification of Decision Making\\n\\n\\\\[ \\\\pi_{k+1} = \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\langle Q(\\\\pi_k), \\\\pi \\\\rangle - \\\\alpha_{k-1} - \\\\frac{1}{M-1} \\\\sum_{\\\\tau=0}^{M-1} B\\\\phi(\\\\pi, \\\\pi_{k-\\\\tau}) - \\\\eta \\\\|\\\\pi - \\\\pi_k\\\\|^2 \\\\]  \\\\hspace{1cm} (14)\\n\\nIn Appendix F.7, we perform an ablation study to show the effectiveness of adding such a magnet policy. Nevertheless, it is worth noting that this particular choice should not be confused with the original MMD even when \\\\(M = 1\\\\) as the policy updating rule is derived via a numerical method, instead of relying on the closed-form solution (Sokota et al., 2023).\\n\\nD.3. Derivation of MMD-EU\\n\\nIn this section, we present the details of the baseline, MMD-EU, used in our experiments. This baseline follows the spirit of MMD-KL (Sokota et al., 2023). Consider the following problem:\\n\\n\\\\[ \\\\pi_{k+1} = \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\langle Q(\\\\pi_k), \\\\pi \\\\rangle - \\\\xi \\\\|\\\\pi - \\\\rho\\\\|^2 - \\\\frac{1}{2} \\\\eta \\\\|\\\\pi - \\\\pi_k\\\\|^2 \\\\]  \\\\hspace{1cm} (15)\\n\\nwhere \\\\(\\\\xi > 0\\\\) is the regularization intensity, \\\\(\\\\eta > 0\\\\) is the step size, and \\\\(\\\\rho\\\\) is the magnet policy. Let \\\\(\\\\phi(\\\\pi) = \\\\frac{1}{2} \\\\sum_{a \\\\in A} \\\\|\\\\pi(a)\\\\|_2\\\\), i.e., the squared Euclidean norm. Then, we need to optimize the following objective:\\n\\n\\\\[ \\\\langle Q(\\\\pi_k), \\\\pi \\\\rangle - \\\\xi \\\\|\\\\pi - \\\\rho\\\\|^2 - \\\\frac{1}{2} \\\\eta \\\\|\\\\pi - \\\\pi_k\\\\|^2 \\\\]  \\\\hspace{1cm} (16)\\n\\nwith the constraint \\\\(\\\\sum_{a \\\\in A} \\\\pi(a) = 1\\\\) and \\\\(\\\\pi(a) > 0\\\\). We can use the Lagrange multiplier to get the following objective:\\n\\n\\\\[ \\\\langle Q(\\\\pi_k), \\\\pi \\\\rangle - \\\\xi \\\\|\\\\pi - \\\\rho\\\\|^2 - \\\\frac{1}{2} \\\\eta \\\\|\\\\pi - \\\\pi_k\\\\|^2 + \\\\lambda \\\\]  \\\\hspace{1cm} (17)\\n\\nTaking the derivative of both \\\\(\\\\pi\\\\) and \\\\(\\\\lambda\\\\), we have:\\n\\n\\\\[ Q(a, \\\\pi_k) - \\\\xi (\\\\pi(a) - \\\\rho(a)) - \\\\eta (\\\\pi(a) - \\\\pi_k(a)) - \\\\lambda = 0 \\\\]  \\\\hspace{1cm} (18)\\n\\n\\\\[ \\\\sum_{a \\\\in A} \\\\pi(a) = 1 \\\\]  \\\\hspace{1cm} (19)\\n\\nTherefore from Eq. (18), we have:\\n\\n\\\\[ \\\\pi(a) = \\\\frac{\\\\xi \\\\rho(a) + 1}{\\\\eta \\\\pi_k(a) + |A|} Q(a, \\\\pi_k) - \\\\lambda \\\\left(\\\\frac{\\\\xi + 1}{\\\\eta}\\\\right) \\\\]  \\\\hspace{1cm} (20)\\n\\nSubstituting the above equation to Eq. (19), we have:\\n\\n\\\\[ \\\\sum_{a \\\\in A} \\\\frac{\\\\xi \\\\rho(a) + 1}{\\\\eta \\\\pi_k(a) + |A|} Q(a, \\\\pi_k) - \\\\lambda \\\\left(\\\\frac{\\\\xi + 1}{\\\\eta}\\\\right) = 1 \\\\]  \\\\hspace{1cm} (21)\\n\\n\\\\[ \\\\Rightarrow \\\\sum_{a \\\\in A} \\\\frac{\\\\xi \\\\rho(a) + 1}{\\\\eta \\\\pi_k(a) + |A|} Q(a, \\\\pi_k) = \\\\left(\\\\frac{\\\\xi + 1}{\\\\eta}\\\\right) + \\\\sum_{a \\\\in A} \\\\lambda \\\\]  \\\\hspace{1cm} (22)\\n\\nNote that \\\\(\\\\sum_{a \\\\in A} \\\\frac{\\\\xi \\\\rho(a)}{\\\\eta \\\\pi_k(a) + |A|} = \\\\frac{\\\\xi}{\\\\eta} + 1\\\\), we have:\\n\\n\\\\[ \\\\lambda = \\\\frac{1}{|A|} \\\\sum_{a \\\\in A} Q(a, \\\\pi_k) \\\\]  \\\\hspace{1cm} (23)\\n\\nThen we can compute the new policy as follows:\\n\\n\\\\[ \\\\pi(a) = \\\\frac{\\\\xi \\\\rho(a) + 1}{\\\\eta \\\\pi_k(a) + |A|} Q(a, \\\\pi_k) - \\\\left(\\\\frac{\\\\xi + 1}{\\\\eta}\\\\right) \\\\sum_{a \\\\in A} Q(a, \\\\pi_k) \\\\]  \\\\hspace{1cm} (24)\"}"}
{"id": "U841CrDUx9", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theoretically, we note that by choosing the suitable values for $\\\\xi$ and $\\\\eta$, we can always ensure that $\\\\pi$ is well-defined, i.e., $\\\\pi(a) \\\\geq 0, \\\\forall a \\\\in A$. In experiments, we can use a projection operation to ensure this condition ($\\\\zeta = 1 + e^{-10}$ is used to avoid division by zero):\\n\\n$$\\\\pi_{k+1}(a) = \\\\max\\\\{0, \\\\pi_k(a)\\\\} + \\\\zeta \\\\max\\\\{0, \\\\pi_k(a')\\\\} + \\\\zeta$$ (25)\\n\\nIn addition, similar to MMD-KL, the magnet policy is updated as (i.e., moving magnet):\\n\\n$$\\\\rho_{k+1}(a) = (1 - \\\\tilde{\\\\eta}) \\\\rho_k(a) + \\\\tilde{\\\\eta} \\\\pi_{k+1}(a),$$ (26)\\n\\nwhere $\\\\tilde{\\\\eta} > 0$ is the learning rate for the magnet policy $\\\\rho$. In practice, the initial magnet policy $\\\\rho_1$ can be set to the initial policy $\\\\pi_1$ which is typically a uniform policy.\\n\\n**D.4. Meta-Controller for Different Measures**\\n\\nGMD generalizes exiting MD algorithms in two aspects: i) it takes multiple previous policies into account and can recover some of the existing MD algorithms by setting the $M$ and $\\\\alpha$, and ii) it can consider a broader class of Bregman divergence by setting $\\\\phi$ to more possible convex functions (Table 2). As a consequence, we argue that when GMD is executed by each agent independently, it could satisfy the first two desiderata $D1$ and $D2$ presented in the Introduction. However, as mentioned in Section 5.2, since there is no explicit objective regarding different evaluation measures (and different solution concepts) arises in this \\\"decentralized\\\" execution process, GMD itself cannot satisfy well the last two desiderata $D3$ and $D4$.\\n\\nTo address the challenges, our solution is the zero-order meta-controller (MC) which dynamically adjusts the hyper-parameters conditional on the evaluation measures (Section 5.2). In this section, we present the details of different MCs.\\n\\n**Direction-Guided Random Search (DRS).** Our DRS method is obtained by applying the direction-guided update (Section 5.2) to the existing RS method presented in (Wang et al., 2022). Specifically, at the iteration $k$,\\n\\n1. Sample $D$ candidate updates $\\\\{u_j\\\\}$ $\\\\forall j \\\\leq D$ from a spherically symmetric distribution $u_j \\\\sim q$.\\n2. Update $\\\\alpha$ as follows:\\n   - $\\\\alpha_j^+ = \\\\text{CLIP}_{1}(\\\\alpha_j + \\\\mu u_j)$,\\n   - $\\\\alpha_j^- = \\\\text{CLIP}_{1}(\\\\alpha_j - \\\\mu u_j)$,\\n   - $1 \\\\leq j \\\\leq D$.\\n3. Update $\\\\pi$ as follows:\\n   - $\\\\pi_j^+ = \\\\text{GMD}(\\\\alpha_j^+)$,\\n   - $\\\\pi_j^- = \\\\text{GMD}(\\\\alpha_j^-)$,\\n   - $1 \\\\leq j \\\\leq D$.\\n4. Update $\\\\delta$ as follows:\\n   - $\\\\delta_j = L(\\\\pi_j^+) - L(\\\\pi_j^-)$,\\n   - $1 \\\\leq j \\\\leq D$.\\n5. Sample $u^*$ as follows:\\n   - $u^* = -\\\\sum_{j=1}^{D} \\\\text{sgn}(\\\\delta_j) u_j$.\\n6. Update $\\\\alpha$ as follows:\\n   - $\\\\alpha \\\\leftarrow \\\\text{CLIP}_{1}(\\\\alpha + u^*)$.\\n\\n**RS.** The vanilla RS which is adapted from (Wang et al., 2022). The only difference from DRS is it updates $\\\\alpha$ directly based on the performance difference $\\\\delta_j$.\\n\\n**GradientLess Descent (GLD).** This method is adapted from (Wang et al., 2022). At the iteration $k$,\\n\\n1. Sample $D$ candidate updates $\\\\{u_j\\\\}$ $\\\\forall j \\\\leq D$.\\n2. Different from RS which samples the candidates from a fixed radius (the smoothing parameter $\\\\mu$), $\\\\delta_j$ is defined as:\\n   - $\\\\delta_j = L(\\\\pi_j^+) - L(\\\\pi_j^-)$,\\n   - $1 \\\\leq j \\\\leq D$.\\n3. Sample $u^*$ as follows:\\n   - $u^* = -\\\\sum_{j=1}^{D} \\\\text{sgn}(\\\\delta_j) u_j$.\\n4. Update $\\\\alpha$ as follows:\\n   - $\\\\alpha \\\\leftarrow \\\\text{CLIP}_{1}(\\\\alpha + u^*)$.\"}"}
{"id": "U841CrDUx9", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In DRS and RS, we independently sample the candidates on spheres with various radiuses uniformly sampled from the interval $[r_L, r_H]$. Then, we update $\\\\alpha$ as follows:\\n\\n$$\\n\\\\alpha_j^{+} = \\\\text{CLIP}(\\\\alpha + u_j),\\n\\\\pi_j^{+} = \\\\text{GMD}(\\\\alpha_j^+),\\n$$\\n\\n$1 \\\\leq j \\\\leq D$.\\n\\n$$\\n\\\\delta_j = L(\\\\pi_j^+) - L(\\\\pi_k),\\n$$\\n\\n$1 \\\\leq j \\\\leq D$.\\n\\n$$\\nu^* = -\\\\sum_{j=1}^{D} \\\\delta_j u_j,$$\\n\\n$$\\n\\\\alpha \\\\leftarrow \\\\text{CLIP}(\\\\alpha + u^*).\\n$$\\n\\nIntuitively, by comparing the performance of $D$ candidates, $\\\\alpha$ is updated by the candidate with the smallest value of $L$.\\n\\nGradientLess Descent with Summation (GLDS). Different from GLD which uses only one of the $D$ samples to update the $\\\\alpha$, we can follow the idea of RS/DRS to take all the candidates into account by summation. Specifically, let $L(\\\\pi_k)$ denote the performance of the current policy $\\\\pi_k$, then we have:\\n\\n$$\\n\\\\alpha_j^{+} = \\\\text{CLIP}(\\\\alpha + u_j),\\n\\\\pi_j^{+} = \\\\text{GMD}(\\\\alpha_j^+),\\n$$\\n\\n$1 \\\\leq j \\\\leq D$.\\n\\n$$\\n\\\\delta_j = L(\\\\pi_j^+) - L(\\\\pi_k),\\n$$\\n\\n$1 \\\\leq j \\\\leq D$.\\n\\n$$\\nu^* = -\\\\sum_{j=1}^{D} \\\\delta_j u_j,$$\\n\\n$$\\n\\\\alpha \\\\leftarrow \\\\text{CLIP}(\\\\alpha + u^*).\\n$$\\n\\nDirection-Guided GLDS (DGLDS). Applying the direction-guided update to the GLDS, we can get this method. Precisely, let $L(\\\\pi_k)$ denote the performance of the current policy $\\\\pi_k$, then we have:\\n\\n$$\\n\\\\alpha_j^{+} = \\\\text{CLIP}(\\\\alpha + u_j),\\n\\\\pi_j^{+} = \\\\text{GMD}(\\\\alpha_j^+),\\n$$\\n\\n$1 \\\\leq j \\\\leq D$.\\n\\n$$\\n\\\\delta_j = L(\\\\pi_j^+) - L(\\\\pi_k),\\n$$\\n\\n$1 \\\\leq j \\\\leq D$.\\n\\n$$\\nu^* = -\\\\sum_{j=1}^{D} Sgn(\\\\delta_j) u_j,$$\\n\\n$$\\n\\\\alpha \\\\leftarrow \\\\text{CLIP}(\\\\alpha + u^*).\\n$$\\n\\nAs the meta-controller needs to evaluate the performance of the candidates, extra computational cost is required. In our experiments, to trade-off between the learning performance and running time, we update $\\\\alpha$ every $\\\\kappa \\\\geq 1$ iteration. In addition, during the first $M - 1$ iterations, i.e., $k < M$, as there are only $k < M$ historical policies, we set $\\\\alpha_\\\\tau = 1/k$ for $0 \\\\leq \\\\tau \\\\leq k - 1$. In other words, MC will start to update $\\\\alpha$ only after $M$ iterations. Algorithm 2 in the main text is the simplified version which shows the primary principle of CMD. In Algorithm 4, we present the full details of CMD.\\n\\n**Algorithm 4** Configurable Mirror Descent (CMD)\\n\\n1: Given $L$, $\\\\psi$, initial (joint) policy $\\\\pi_1$, $M$, $D$, $\\\\epsilon$, $\\\\iota$\\n\\n2: for $k = 1, \\\\ldots, K$\\n\\n3: if $k \\\\leq M$\\n\\n4: $\\\\alpha_\\\\tau = 1/k, \\\\forall 0 \\\\leq \\\\tau \\\\leq k - 1$\\n\\n5: else\\n\\n6: if $k \\\\% \\\\kappa = 0$\\n\\n7: Sample $D$ candidates $\\\\{\\\\alpha_j\\\\}_{j=1}^{D}$\\n\\n8: Derive new joint policies $\\\\{\\\\pi_j = \\\\text{GMD}(\\\\alpha_j)\\\\}_{j=1}^{D}$\\n\\n9: Evaluate new joint policies $\\\\{L(\\\\pi_j)\\\\}_{j=1}^{D}$\\n\\n10: Update $\\\\alpha$ based on $\\\\{L(\\\\pi_j)\\\\}_{j=1}^{D}$\\n\\n11: end if\\n\\n12: end if\\n\\n13: Compute $\\\\pi_{k+1}$ via GMD with the updated $\\\\alpha$\\n\\n14: end for\"}"}
{"id": "U841CrDUx9", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As mentioned in Section D.2, in our experiments, when instantiating CMD, we by default add a magnet policy (the initial \\\\( M \\\\) policy) into the policy updating as it has been demonstrated that adding a magnet policy is powerful in solving two-player zero-sum games (Sokota et al., 2023; Liu et al., 2023). To verify this, we conduct an ablation study where \u201cCMD w/o Mag\u201d denotes the method that only considers the most recent \\\\( \\\\delta \\\\) and \\\\( \\\\rho \\\\) values (instead of the closed-form solution (Sokota et al., 2023)).\\n\\nMCCGoofspiel, CMD finally converges to a lower NashConv without the magnet, it could perform worse in some other three categories, adding the magnet policy is necessary for CMD to work consistently well across all the games; though in the single-agent and cooperative categories where the \\\\( \\\\rho \\\\) values are relatively simpler than the other games (as shown in Table 6, \\\\( \\\\rho \\\\).)\\n\\nFigure 20. From the results, we can see that i) For single-agent and cooperative categories, instead of relying on the closed-form solution (Sokota et al., 2023).\"}"}
{"id": "U841CrDUx9", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"three-player we conduct experiments on the $\\\\alpha$ value of follows a similar pattern to DRS, it performs on par with or better than DGLDS. $\\\\alpha$ among the 5 MCs. From the evolution of different MCs over the learning process. We can get the same conclusion as OptGap/NashConv: DRS is the best choice curves of the performance of different MCs, and in Figure 23, we present the evolution of the value of Different Meta-Controllers. Then, we investigate the effectiveness of different MCs. In Figure 22, we present the learning experiments: $M$ games may require different $M$ policies\\n\\nWe first investigate the influence of the number of previous policies\\n\\nNumber of Historical Policies and Smoothing Parameter.\\n\\ndivergences, and iv) investigating the effectiveness of magnet.\\n\\nNote that in two-player zero-sum games, NE and CCE can be shown to be payoff equivalent (v. Neumann, 1928). Therefore, $\\\\text{CCE}$ and $\\\\text{CCEG}$: i) investigating the combination of $M$, $\\\\mu$ and $\\\\text{CCE}$ under the measure CCEGap.\\n\\n$P_1$ $P_2$ $P_3$ $P_4$ $P_5$\\n\\n$M$, $\\\\mu$, and the results are shown in Figure 21. We can get the same conclusion: different $\\\\mu$ 0.01) for both Kuhn and Goofspiel. All the other hyper-parameter settings are the same as $\\\\text{Kuhn}$ and $\\\\text{Goofspiel}$. We follow the same experimental pipeline of OptGap and $\\\\text{OptGap}$\\n\\nand $\\\\text{OptGap}$, and the results are shown in Figure 21. We can get the same conclusion: different $\\\\mu$. By comparison, we determine their default values which will be fixed in the other $\\\\text{D3}$ $\\\\text{D4}$\"}"}
{"id": "U841CrDUx9", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"still conclude that adding the magnet policy is necessary for our CMD. Nevertheless, we can get the same conclusions as for different convex functions in Table 2, and the results are given in Figure 24. We can get the same conclusions as for the entropy function, e.g.,\\n\\n\\\\[ x = \\\\ln 2 \\\\]\\n\\nNext, we investigate how CMD performs under different Bregman divergences induced by different convex functions. The evolution of the hyper-parameters of different MCs. The y-axis is the value of \\\\( x \\\\), our CMD can converge faster than the SOTA MMD-KL in terms of the number of iterations.\"}"}
{"id": "U841CrDUx9", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"welfare, not the NashConv. In Battleship, while CMD can get a higher (average) social welfare, the final joint policy is\\nthe MC's objective, though could achieve a higher social welfare, cannot converge the Nash equilibrium. In Battleship, CMD with SW as\\nthe MC's objective can converge to the NE faster than that with SW as the MC's objective. In Bargaining and TradeComm, CMD with NashConv as\\nthe MC's objective converges to the approximate NE even though the MC's objective is social\\nwelfare\\nNE with maximum social\\nBargaining and TradeComm, the learning still can converge to the approximate NE even though the MC's objective is social\\nwelfare.\\nFrom the top line of the figure, we can see that CMD/GMD can empirically achieve competitive or better social welfare\\nexperiments on the general-sum games, and the results are shown in Figure 26. In this experiment, we use the default values\\nIn this section, we apply our methods to the evaluation measure \u2013 social welfare (Davis & Whinston, 1962). We conduct\\nwelfare, not the NashConv. In Battleship, while CMD can get a higher (average) social welfare, the final joint policy is\\nthe MC's objective, though could achieve a higher social welfare, cannot converge the Nash equilibrium. In Battleship, CMD with SW as\\nthe MC's objective can converge to the approximate NE faster than that with SW as the MC's objective. In Bargaining and TradeComm, CMD with NashConv as\\nthe MC's objective converges to the approximate NE even though the MC's objective is social\\nwelfare\\nNE with maximum social\\nBargaining and TradeComm, the learning still can converge to the approximate NE even though the MC's objective is social\\nwelfare.\"}"}
{"id": "U841CrDUx9", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Configurable Mirror Descent: Towards a Unification of Decision Making\\n\\nA. Code Repository\\n\\nCode for experiments is available at https://github.com/IpadLi/CMD.\\n\\nB. More Related Works\\n\\nSingle-Agent Category.\\n\\nIn the single-agent category, reinforcement learning (RL) (Sutton & Barto, 2018) has proved successful in many real-world applications. The power of RL is further amplified with the integration of deep neural networks, leading to various deep RL algorithms that have been successfully applied to various application domains such as video games (Mnih et al., 2015), robot navigation (Singh et al., 2022), and financial technology (Sun et al., 2023b).\\n\\nAmong these algorithms, PPO (Schulman et al., 2017) is one of the most commonly used methods to solve single-agent RL problems. Recent works have shown that independent PPO (de Witt et al., 2020; Sun et al., 2023a) can effectively solve single-agent and cooperative multi-agent RL problems. In addition, a variant of PPO is also shown to be effective in solving two-player zero-sum games when both players adopt this algorithm (Sokota et al., 2023). Nevertheless, it remains elusive whether these single-agent algorithms can be applied to solve other categories of decision-making problems which may involve different properties including different numbers of agents, different relationships between agents, different solution concepts, and different evaluation measures. In this work, we aim to develop a single algorithm that, when executed by each agent, provides an effective approach to address different categories of decision-making problems.\\n\\nCooperative Multi-Agent Category.\\n\\nCooperative multi-agent RL (MARL) has been demonstrated successful in solving many real-world cooperative tasks such as traffic signal control (Xu et al., 2021; Su et al., 2022), power management (Wang et al., 2021b), finance (Fang et al., 2023), and multi-robot cooperation (Rizk et al., 2019). In the past decade, a variety of MARL algorithms, e.g., QMIX (Rashid et al., 2018) and its variants (Son et al., 2019; Rashid et al., 2020; Wang et al., 2021a), MADDPG (Lowe et al., 2017), COMA (Foerster et al., 2018), and MAPPO (Yu et al., 2022), to name just a few, have been proposed and achieved significant performance in various multi-agent benchmarks, e.g., SMAC (Samvelyan et al., 2019) and Dota II (Berner et al., 2019). These algorithms typically follow the principle of centralized training and decentralized execution (CTDE) where global information is only available during training. Despite their success, they cannot be directly applied to competitive and mixed cooperative and competitive categories. In this work, our proposed CMD can be applied to different decision-making categories and share similarities with the CTDE paradigm: the meta-controller takes all the agents (i.e., the joint policy) into account to optimize the hyper-parameters conditional on the targeted evaluation measure (a \u201ccentralized\u201d process) while each agent in the environment independently execute the GMD with the given hyper-parameters to update the policy (a \u201cdecentralized\u201d process).\\n\\nCompetitive Multi-Agent Category.\\n\\nThere has long been a history of researchers pursuing artificial intelligence (AI) agents that can achieve human-level or super-human-level performance in solving various competitive multi-agent games such as chess (Campbell et al., 2002), Go (Silver et al., 2017), poker (Brown & Sandholm, 2019), and Stratego (Perolat et al., 2022). Due to the competitive nature, the development of learning algorithms for solving these games is typically largely different from single-agent and cooperative MARL. Among others, counterfactual regret minimization (CFR) (Zinkevich et al., 2007) and policy-space response oracles (PSRO) (Lanctot et al., 2017) are two representative algorithms that have been widely used to solve complex games (Schmid et al., 2023). Another category of algorithm that has drawn increasing attention recently is the mirror descent (MD) (Nemirovskij & Yudin, 1983; Beck & Teboulle, 2003). In contrast to CFR and PSRO which are \u201caverage-iterate\u201d algorithms, MD has proved the \u201clast-iterate\u201d convergence property in solving two-player zero-sum games (Bailey & Piliouras, 2018; Kangarshahi et al., 2018; Wibisono et al., 2022; Kozuno et al., 2021; Lee et al., 2021; Jain et al., 2022; Ao et al., 2023; Liu et al., 2023; Cen et al., 2023; Sokota et al., 2023) and some classes of general-sum games (Anagnostides et al., 2022b). Moreover, MD has also been demonstrated effective in solving single-agent RL problems (Tomar et al., 2022). Despite their success, existing MD algorithms typically focus on some specific Bregman divergences which may not be the optimal choices across different decision-making categories. Our proposed CMD generalizes existing MD algorithms to consider a broader class of Bregman divergence, which could achieve better learning performance in addressing different categories of decision-making problems.\\n\\nMixed Cooperative and Competitive Category.\\n\\nIn some real-world scenarios, the relationship between agents could be neither purely cooperative nor purely competitive. For example, in a football game, the agents belonging to the same team need to cooperate while also competing with the other team (Kurach et al., 2020). In hidden-role games (Carminati et al., 2023), each agent tries to identify their (unknown) teammates and compete with other (unknown) adversaries (Wang & Kaneko, 2018; Serrino et al., 2019; Albrecht et al., 2022). However, in contrast to the other three categories (single-agent, purely cooperative, and purely competitive), mixed cooperative and competitive (MCC) games are largely unstudied (Xu et al., 2021).\"}"}
{"id": "U841CrDUx9", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Furthermore, as MD algorithms typically require updating the policy at each decision point, running them on the current benchmark games such as football (Kurach et al., 2020) could be computationally prohibited. In the present work, we construct 3 MCC games that are academic-friendly \u2013 their numbers of decision points are not too large and hence, running MD algorithms (and other algorithms such as CFR-type (Zinkevich et al., 2007; Tammelin, 2014)) on these games does not require much computational resource (e.g., running time and memory usage).\\n\\nHyper-Parameter Tuning. Existing works typically determine the hyper-parameter values of the MD algorithms depending on the domain knowledge (Sokota et al., 2023; Anagnostides et al., 2022b; Hsieh et al., 2021; Zhou et al., 2018; Mertikopoulos et al., 2019; Bailey & Piliouras, 2019; Golowich et al., 2020), which, though convenient for theoretical analysis, may not be easy to generalize to different games. On the other hand, as the evaluation measures, e.g., NashConv, could be non-differentiable with respect to the hyper-parameters, the gradient-based methods such as STAC (Zahavy et al., 2020) could also be less applicable. In this sense, a more feasible method is the zero-order hyper-parameter optimization which can update the parameters of interest without access to the true gradient, which has been extensively adopted in the adversarial robustness of deep neural networks (Ilyas et al., 2018), meta-learning (Song et al., 2020), transfer learning (Tsai et al., 2020), and neural architecture search (NSA) (Wang et al., 2022). Nevertheless, we found that directly applying existing zero-order methods could be ineffective as when the value of the evaluation measure is too small, they may not be able to derive an effective update for the hyper-parameter. To address this issue, we propose a simple yet effective technique \u2013 direction-guided update \u2013 where the performance difference between two candidates is used to only determine the update direction of the hyper-parameters rather than the update magnitude, which is more effective than existing methods (Wang et al., 2022) when the value of the performance is extremely small.\"}"}
{"id": "U841CrDUx9", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Configurable Mirror Descent: Towards a Unification of Decision Making\\n\\nC. Notation Table\\n\\nTable 3.\\n\\nN\\n\\n\\\\{1, \\\\ldots, N\\\\}, the set of N agents.\\n\\nS\\n\\nthe finite set of states.\\n\\nA\\n\\nA \u00d7 i \u2208 N A i where A i is the finite set of actions of agent i.\\n\\nO\\n\\nO \u00d7 i \u2208 N O i where O i is the finite set of observations of agent i.\\n\\n\u03a9\\n\\n\u03a9 \u00d7 i \u2208 N \u03a9 i where \u03a9 i: S \u00d7 A \u2192 O i is the observation function of agent i.\\n\\nP\\n\\nP: S \u00d7 A \u2192 \u2206(S), the state transition function.\\n\\nR\\n\\nR = \\\\{r i\\\\} i \u2208 N where r i: S \u00d7 A \u2192 R is the reward function of agent i.\\n\\n\u03b3\\n\\n\u03b3 \u2208 [0, 1), the discount factor.\\n\\n\u03bd\\n\\n\u03bd \u2208 \u2206(S), the initial state distribution.\\n\\n\u03c4\\n\\n\u03c4 t i the decision point (action-observation history) of agent i at time t, \u03c4 t i \u2208 T t i.\\n\\nT\\n\\nT t i T t i = (O i \u00d7 A i) t, the space of decision points of agent i at time step t.\\n\\n\u03a0\\n\\n\u03a0 = \u00d7 i \u2208 N \u03a0 i where \u03a0 i is the policy space of agent i.\\n\\n\u03c0\\n\\n\u03c0 = \u03c0 1 \u2299 \u00b7\u00b7\u00b7 \u2299 \u03c0 N, the joint policy, \u03c0 = \u03c0 1 \u00d7 \u00b7\u00b7\u00b7 \u00d7 \u03c0 N, the product policy.\\n\\nV\\n\\nV i(s, \u03c0) = V i(\u03bd, \u03c0) the value functions of agent i, V i(\u03bd, \u03c0) := E s \u223c \u03bd [V i(s, \u03c0)].\\n\\nL\\n\\nL(\u03c0) the evaluation measure of the joint policy \u03c0.\\n\\n\u03c0 k\\n\\nthe single agent\u2019s policy at the iteration k of an algorithm.\\n\\n\u03c0 k\\n\\nthe joint policy at the iteration k of an algorithm.\\n\\nQ\\n\\nQ(\u03c0 k) = (Q(a, \u03c0 k)) a \u2208 A, the action-value vector of a single agent induced by \u03c0 k.\\n\\nB\\n\\nB \u03d5(x; y) = \u03d5(x) \u2212 \u03d5(y) \u2212 \u27e8\u2207 \u03d5(y), x \u2212 y\u27e9, the Bregman divergence with respect to \u03d5.\\n\\nK\\n\\nthe number of iterations of an algorithm.\\n\\nM\\n\\nM \u2265 1, the number of historical policies.\\n\\n\u03b1\\n\\n\u03b1 = (\u03b1 \u03c4 ) 0 \u2264 \u03c4 \u2264 M \u2212 1, \u03b1 \u03c4 is the regularization intensity of \u03c0 k \u2212 \u03c4.\\n\\n\u03f5\\n\\n\u03f5 > 0, the smallest probability of an action.\\n\\n\u03d5\\n\\n\u03d5(\u03c0) = P a \u2208 A \u03c8(\u03c0(a)), \u03c8 is some convex function defined on [0, 1].\\n\\n\u03bb, \u03b2\\n\\n\u03bb, \u03b2 = (\u03b2 a) a \u2208 A, the dual variables.\\n\\nA, B\\n\\nA = Q(\u03c0 k) + P M \u2212 1 \u03c4=0 \u03b1 \u03c4 \u03d5\u2032(\u03c0 k \u2212 \u03c4), B = P M \u2212 1 \u03c4=0 \u03b1 \u03c4, where \u03d5\u2032 is the derivative of \u03d5.\\n\\n\u03c8\u22121\\n\\nthe inverse function of \u03c8\u2032 (the derivative of \u03c8).\\n\\nC\\n\\nC > 0, the number of iterations for the Newton method.\\n\\ng\\n\\ng(\u03bb) = P a \u2208 A \u03c8\u22121(A(a) \u2212 \u03bb)/B)\\n\\n\u22121\\n\\n\u22121\\n\\nthe derivative of \u03c8\u22121.\\n\\nD\\n\\nD the number of sampled candidate \u03b1's.\\n\\n{\u03b1 j} D j=1 candidates by perturbing the current \u03b1.\\n\\n{\u03c0 j} D j=1 {\u03c0 j = GMD(\u03b1 j)} D j=1, D new joint policies derived via GMD.\\n\\n\u00b5\\n\\nthe smoothing parameter in DRS and RS.\\n\\n[r L, r H] the interval of the radiuses of the spheres in DGLDS, GLDS, and GLD.\\n\\n{u j} D j=1 D candidate updates sampled from a spherically symmetric distribution u j \u223c q.\\n\\n\u03b1 j + \u03b1 j \u2212 \u03b1 j + = CLIP 1 \u03b9(\u03b1 j + \u00b5 u j),\\n\\n\u03b1 j \u2212 = CLIP 1 \u03b9(\u03b1 j \u2212 \u00b5 u j), the candidates by perturbing the current \u03b1.\\n\\n\u03c0 j +, \u03c0 j \u2212 = GMD(\u03b1 j +), \u03c0 j \u2212 = GMD(\u03b1 j \u2212), the new joint policies obtained via GMD.\\n\\n\u03b4 j \u03b4 j = L(\u03c0 j +) \u2212 L(\u03c0 j \u2212), the performance difference between \u03c0 j + and \u03c0 j \u2212.\\n\\nu \u2217 the final update.\\n\\nSgn Sgn(z) = 1 if z > 0, Sgn(z) = \u22121 if z < 0, otherwise, Sgn(z) = 0.\\n\\nCLIP 1 \u03b9 CLIP 1 \u03b9(z) = \u03b9 if z < \u03b9, CLIP 1 \u03b9(z) = 1 if z > 1, otherwise, CLIP 1 \u03b9(z) = z, where 0 < \u03b9 < 1.\\n\\n\u03ba \u2265 1, update the \u03b1 every \u03ba iterations.\\n\\n\u03c1 the magnet policy in MMD.\\n\\n\u03be > 0, the regularization intensity of the magnet policy.\\n\\n\u03b7 > 0, the step size in MMD.\\n\\n\u02dc \u03b7 > 0, the step size of the magnet policy in MMD.\"}"}
{"id": "U841CrDUx9", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Configurable Mirror Descent: Towards a Unification of Decision Making\\n\\nD. Configurable Mirror Descent\\n\\nIn this section, we present all the details of our methods. In Section D.1, we present the details of GMD. In Section D.2, we establish some connections between GMD and existing MD algorithms. In Section D.3, we restrict the convex function $\\\\phi$ to the squared Euclidean norm and derive the closed-form solution under the MMD policy updating rule. Finally, in Section D.4, we present the details of different meta-controllers.\\n\\nD.1. Generalized Mirror Descent\\n\\nIn this section, we present the proof of Proposition 5.1, the pseudo-code of Newton's method for computing the value of the dual variable, and the convex functions considered in our work.\\n\\nProof of Proposition 5.1.\\n\\nConsider the optimization problem (3). By the definition of Bregman divergence, we have:\\n\\n$$\\n\\\\pi_{k+1} = \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\langle Q(\\\\pi_k), \\\\pi \\\\rangle - \\\\sum_{\\\\tau=0}^{M-1} \\\\alpha_\\\\tau B_{\\\\phi}(\\\\pi, \\\\pi_k - \\\\tau) \\n$$\\n\\n(5)\\n\\n$$\\n\\\\Rightarrow \\\\pi_{k+1} = \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\langle Q(\\\\pi_k), \\\\pi \\\\rangle - \\\\alpha_\\\\tau \\\\sum_{\\\\tau=0}^{M-1} \\\\phi(\\\\pi) - \\\\phi(\\\\pi_k) - \\\\langle \\\\phi'(\\\\pi_k - \\\\tau), \\\\pi - \\\\pi_k - \\\\tau \\\\rangle\\n$$\\n\\n(6)\\n\\n$$\\n\\\\Rightarrow \\\\pi_{k+1} = \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\langle A, \\\\pi \\\\rangle - B_{\\\\phi}(\\\\pi) + \\\\text{const.}\\n$$\\n\\n(7)\\n\\nwhere \\\"const.\\\" summarizes all terms that are irrelevant to $\\\\pi$. Let $A = Q(\\\\pi_k) + \\\\sum_{\\\\tau=0}^{M-1} \\\\alpha_\\\\tau \\\\phi'(\\\\pi_k - \\\\tau)$ and $B = \\\\sum_{\\\\tau=0}^{M-1} \\\\alpha_\\\\tau$ which are fixed at the current iteration $k$. Then, we can convert Eq. (3) to the following optimization problem:\\n\\n$$\\n\\\\pi_{k+1} = \\\\arg \\\\max_{\\\\pi \\\\in \\\\Pi} \\\\langle A, \\\\pi \\\\rangle - B_{\\\\phi}(\\\\pi) + \\\\text{const.}\\n$$\\n\\ns.t.\\n\\n$$\\n\\\\sum_{a \\\\in A} \\\\pi(a) = 1 \\\\quad \\\\text{and} \\\\quad \\\\pi(a) \\\\geq 0, \\\\quad \\\\forall a \\\\in A.\\n$$\\n\\n(8)\\n\\nTo solve the constrained optimization problem (8), we can apply the Lagrange multiplier, which gives us:\\n\\n$$\\nL(\\\\pi, \\\\lambda, \\\\beta) = \\\\langle A, \\\\pi \\\\rangle - B_{\\\\phi}(\\\\pi) + \\\\text{const.} - \\\\lambda \\\\left( \\\\sum_{a \\\\in A} \\\\pi(a) - 1 \\\\right) + \\\\sum_{a \\\\in A} \\\\beta_a \\\\pi(a)\\n$$\\n\\n(9)\\n\\nwhere $\\\\lambda$ and $\\\\beta = (\\\\beta_a)_{a \\\\in A}$ are the dual variables. For such problems, we can get the Karush\u2013Kuhn\u2013Tucker (KKT) conditions for each component (action) $a \\\\in A$ as follows:\\n\\n$$\\nA(a) + B_{\\\\phi}'(\\\\pi_0(a)) - \\\\lambda + \\\\beta_a = 0\\n$$\\n\\n(10a)\\n\\n$$\\n\\\\sum_{a \\\\in A} \\\\pi(a) = 1\\n$$\\n\\n(10b)\\n\\n$$\\n\\\\beta_a \\\\pi(a) = 0\\n$$\\n\\n(10c)\\n\\n$$\\n\\\\pi(a) \\\\geq 0, \\\\quad \\\\beta_a \\\\geq 0.\\n$$\\n\\n(10d)\\n\\nThen the problem is to find a policy $\\\\pi$ such that it satisfies all the above conditions, which could be difficult owing to two reasons: i) it simultaneously involves the two dual variables $\\\\lambda$ and $\\\\beta_a$, and ii) in Eq. (10a), computing the value $\\\\phi'(\\\\pi_0(a))$ involves all the components (actions) as $\\\\phi$ is defined on the policy $\\\\pi$, not the individual component (action) $a \\\\in A$.\\n\\nTo address the challenges, we apply the two conditions: $\\\\pi(a) \\\\geq \\\\epsilon$ and $\\\\phi(\\\\pi) = \\\\sum_{a \\\\in A} \\\\psi(\\\\pi(a))$. Then, we have $\\\\phi'(\\\\pi_0(a)) = \\\\psi'(\\\\pi_0(a))$. As a result, the problem (10a\u201310d) is simplified to the following problem:\\n\\n$$\\nA(a) + B_{\\\\psi}'(\\\\pi_0(a)) - \\\\lambda = 0\\n$$\\n\\n(11a)\\n\\n$$\\n\\\\sum_{a \\\\in A} \\\\pi(a) = 1\\n$$\\n\\n(11b)\\n\\n$$\\n\\\\pi(a) \\\\geq \\\\epsilon.\\n$$\\n\\n(11c)\\n\\nFrom Eq. (11a), we can get that:\\n\\n$$\\n\\\\forall a \\\\in A, \\\\quad \\\\pi(a) = \\\\psi'(\\\\pi_0(a)) - \\\\frac{1}{B} A(a) - \\\\frac{1}{B} \\\\lambda\\n$$\\n\\n(12)\"}"}
{"id": "U841CrDUx9", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14.\\n\\nFigure 15.\\n\\nThe evolution of the hyper-parameter values of different MCs in the Single-Agent category. The $\\\\alpha$-axis is the number of iterations.\"}"}
{"id": "U841CrDUx9", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 17. The evolution of the hyper-parameter values of different MCs in the category. The $y$-axis is the value of the number of iterations.\"}"}
{"id": "U841CrDUx9", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 18. The evolution of the hyper-parameter values of different MCs in the MCC category. The y-axis is the value of $\\\\alpha$. The x-axis is the number of iterations.\"}"}
{"id": "U841CrDUx9", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 19. Experimental results for different Bregman divergences. The first 6 figures correspond to the single-agent and cooperative games in Figure 17. For all the single-agent games, our CMD could outperform MMD-KL in most games such as MCMKL-A and MCMKL-B with square and logarithm losses, our CMD could also outperform MMD-KL in some games such as MCCKuhn-A and MCCKuhn-B. In this section, we investigate how CMD performs under the different Bregman divergences induced by different convex and concave function pairs. One of the prominent features of our CMD (GMD) is that it is capable of exploring more possible Bregman divergences.\\n\\nFurthermore, in MCCGoofspiel, choice across all the games. Nevertheless, in some games, there exist other convex functions that are better choices. For the remaining games, there is no capacity to explore alternatives. For all the figures, NashConv-length is the $x$-axis and OptGap-length is the $y$-axis. For all the figures, NashConv-length is the $x$-axis and OptGap-length is the $y$-axis.\"}"}
{"id": "U841CrDUx9", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Configurable Mirror Descent: Towards a Unification of Decision Making\\n\\nChen, Y., Song, X., Lee, C., Wang, Z., Zhang, R., Dohan, D., Kawakami, K., Kochanski, G., Doucet, A., Ranzato, M., et al. Towards learning universal hyperparameter optimizers with transformers. In NeurIPS, pp. 32053\u201332068, 2022.\\n\\nCilingir, H. K., Manzelli, R., and Kulis, B. Deep divergence learning. In ICML, pp. 2027\u20132037, 2020.\\n\\nDavis, O. A. and Whinston, A. Externalities, welfare, and the theory of games. Journal of Political Economy, 70(3):241\u2013262, 1962.\\n\\nde Witt, C. S., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P. H., Sun, M., and Whiteson, S. Is independent learning all you need in the StarCraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.\\n\\nFang, Y., Tang, Z., Ren, K., Liu, W., Zhao, L., Bian, J., Li, D., Zhang, W., Yu, Y., and Liu, T.-Y. Learning multi-agent intention-aware communication for optimal multi-order execution in finance. In KDD, pp. 4003\u20134012, 2023.\\n\\nFarina, G., Bianchi, T., and Sandholm, T. Coarse correlation in extensive-form games. In AAAI, pp. 1934\u20131941, 2020.\\n\\nFoerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S. Counterfactual multi-agent policy gradients. In AAAI, pp. 2974\u20132982, 2018.\\n\\nFoerster, J., Song, F., Hughes, E., Burch, N., Dunning, I., Whiteson, S., Botvinick, M., and Bowling, M. Bayesian action decoder for deep multi-agent reinforcement learning. In ICML, pp. 1942\u20131951, 2019.\\n\\nGolowich, N., Pattathil, S., and Daskalakis, C. Tight last-iterate convergence rates for no-regret learning in multiplayer games. In NeurIPS, pp. 20766\u201320778, 2020.\\n\\nHarsanyi, J. C., Selten, R., et al. A General Theory of Equilibrium Selection in Games. The MIT Press, 1988.\\n\\nHong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., Ran, C., Xiao, L., Wu, C., and Schmidhuber, J. MetaGPT: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023.\\n\\nHsieh, Y.-G., Antonakopoulos, K., and Mertikopoulos, P. Adaptive learning in continuous games: Optimal regret bounds and convergence to Nash equilibrium. In COLT, pp. 2388\u20132422, 2021.\\n\\nIlyas, A., Engstrom, L., Athalye, A., and Lin, J. Black-box adversarial attacks with limited queries and information. In ICML, pp. 2137\u20132146, 2018.\\n\\nJain, R., Piliouras, G., and Sim, R. Matrix multiplicative weights updates in quantum zero-sum games: Conservation laws & recurrence. In NeurIPS, pp. 4123\u20134135, 2022.\\n\\nKangarshahi, E. A., Hsieh, Y.-P., Sahin, M. F., and Cevher, V. Let's be honest: An optimal no-regret framework for zero-sum games. In ICML, pp. 2488\u20132496, 2018.\\n\\nKozuno, T., M\u00e9nard, P., Munos, R., and Valko, M. Model-free learning for two-player zero-sum partially observable Markov games with perfect recall. In NeurIPS, pp. 11987\u201311998, 2021.\\n\\nKuhn, H. W. A simplified two-person poker. Contributions to the Theory of Games, 1:97\u2013103, 1950.\\n\\nKurach, K., Raichuk, A., Sta\u0144czyk, P., Zaj\u0105c, M., Bachem, O., Espeholt, L., Riquelme, C., Vincent, D., Michalski, M., Bousquet, O., and Gelly, S. Google research football: A novel reinforcement learning environment. In AAAI, pp. 4501\u20134510, 2020.\\n\\nLanctot, M., Zambaldi, V., Gruslys, A., Lazaridou, A., Tuyls, K., P\u00e9rolat, J., Silver, D., and Graepel, T. A unified game-theoretic approach to multiagent reinforcement learning. In NeurIPS, pp. 4190\u20134203, 2017.\\n\\nLanctot, M., Lockhart, E., Lespiau, J.-B., Zambaldi, V., Upadhyay, S., P\u00e9rolat, J., Srinivasan, S., Timbers, F., Tuyls, K., Omidshafiei, S., Hennes, D., Morrill, D., Muller, P., Ewalds, T., Faulkner, R., Kram\u00e1r, J., De Vylder, B., Saeta, B., Bradbury, J., Ding, D., Borgeaud, S., Lai, M., Schrittwieser, J., Anthony, T., Hughes, E., Danihelka, I., and Ryan-Davis, J. OpenSpiel: A framework for reinforcement learning in games. arXiv preprint arXiv:1908.09453, 2019.\\n\\nLanctot, M., Schultz, J., Burch, N., Smith, M. O., Hennes, D., Anthony, T., and Perolat, J. Population-based evaluation in repeated rock-paper-scissors as a benchmark for multiagent reinforcement learning. TMLR, 2023. ISSN 2835-8856.\\n\\nLee, C.-W., Kroer, C., and Luo, H. Last-iterate convergence in extensive-form games. In NeurIPS, pp. 14293\u201314305, 2021.\\n\\nLewis, M., Yarats, D., Dauphin, Y., Parikh, D., and Batra, D. Deal or no deal? End-to-end learning of negotiation dialogues. In EMNLP, pp. 2443\u20132453, 2017.\\n\\nLindauer, M., Eggensperger, K., Feurer, M., Biedenkapp, A., Deng, D., Benjamins, C., Ruhkopf, T., Sass, R., and Hutter, F. SMAC3: A versatile Bayesian optimization package for hyperparameter optimization. The Journal of Machine Learning Research, 23(1):2475\u20132483, 2022.\"}"}
{"id": "U841CrDUx9", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Configurable Mirror Descent: Towards a Unification of Decision Making\\n\\nLiu, M., Ozdaglar, A. E., Yu, T., and Zhang, K. The power of regularization in solving extensive-form games. In ICLR, 2023.\\n\\nLiu, S., Chen, P.-Y., Kailkhura, B., Zhang, G., Hero III, A. O., and Varshney, P. K. A primer on zeroth-order optimization in signal processing and machine learning: Principles, recent advances, and applications. IEEE Signal Processing Magazine, 37(5):43\u201354, 2020.\\n\\nLiu, S., Lever, G., Wang, Z., Merel, J., Eslami, S. A., Hennes, D., Czarnecki, W. M., Tassa, Y., Omidshafiei, S., Abdolmaleki, A., et al. From motor control to team play in simulated humanoid football. Science Robotics, 7(69):eabo0235, 2022a.\\n\\nLiu, W., Jiang, H., Li, B., and Li, H. Equivalence analysis between counterfactual regret minimization and online mirror descent. In ICML, pp. 13717\u201313745, 2022b.\\n\\nLowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., and Moritz, I. Multi-agent actor-critic for mixed cooperative-competitive environments. In NeurIPS, pp. 6382\u20136393, 2017.\\n\\nLu, F., Raff, E., and Ferraro, F. Neural Bregman divergences for distance learning. In ICLR, 2023.\\n\\nMao, S., Cai, Y., Xia, Y., Wu, W., Wang, X., Wang, F., Ge, T., and Wei, F. Alympics: Language agents meet game theory \u2013 exploring strategic decision-making with AI agents. arXiv preprint arXiv:2311.03220, 2023.\\n\\nMarris, L., Muller, P., Lanctot, M., Tuyls, K., and Graepel, T. Multi-agent training beyond zero-sum with correlated equilibrium meta-solvers. In ICML, pp. 7480\u20137491, 2021.\\n\\nMertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S., Chandrasekhar, V., and Piliouras, G. Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile. In ICLR, 2019.\\n\\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.\\n\\nMoulin, H. and Vial, J. P. Strategically zero-sum games: The class of games whose completely mixed equilibria cannot be improved upon. International Journal of Game Theory, 7:201\u2013221, 1978.\\n\\nNash, J. Non-cooperative games. Annals of Mathematics, 54(2):286\u2013295, 1951.\\n\\nNemirovskij, A. S. and Yudin, D. B. Problem Complexity and Method Efficiency in Optimization. Wiley-Interscience, 1983.\\n\\nOliehoek, F. A. and Amato, C. A Concise Introduction to Decentralized POMDPs. Springer, 2016.\\n\\nPerolat, J., De Vylder, B., Hennes, D., Tarassov, E., Strub, F., de Boer, V., Muller, P., Connor, J. T., Burch, N., Anthony, T., McAleer, S., Elie, R., Cen, S. H., Wang, Z., Gruslys, A., Malysheva, A., Khan, M., Ozair, S., Timbers, F., Pohlen, T., Eccles, T., Rowland, M., Lanctot, M., Lespiau, J.-B., Piot, B., Omidshafiei, S., Lockhart, E., Sifre, L., Beauguerlange, N., Munos, R., Silver, D., Singh, S., Hassabis, D., and Tuyls, K. Mastering the game of Stratego with model-free multiagent reinforcement learning. Science, 378(6623):990\u2013996, 2022.\\n\\nRabin, M. Incorporating fairness into game theory and economics. The American Economic Review, 83(5):1281\u20131302, 1993.\\n\\nRadhakrishnan, A., Belkin, M., and Uhler, C. Linear convergence of generalized mirror descent with time-dependent mirrors. arXiv preprint arXiv:2009.08574, 2020.\\n\\nRashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and Whiteson, S. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In ICML, pp. 4295\u20134304, 2018.\\n\\nRashid, T., Farquhar, G., Peng, B., and Whiteson, S. Weighted QMIX: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning. In NeurIPS, pp. 10199\u201310210, 2020.\\n\\nRizk, Y., Awad, M., and Tunstel, E. W. Cooperative heterogeneous multi-robot systems: A survey. ACM Computing Surveys, 52(2):1\u201331, 2019.\\n\\nRoss, S. M. Goofspiel\u2013the game of pure strategy. Journal of Applied Probability, 8(3):621\u2013625, 1971.\\n\\nSamvelyan, M., Rashid, T., De Witt, C. S., Farquhar, G., Nardelli, N., Rudner, T. G., Hung, C.-M., Torr, P. H., Foerster, J., and Whiteson, S. The StarCraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\\n\\nSchmid, M., Moravcik, M., Burch, N., Kadlec, R., Davidsson, J., Waugh, K., Bard, N., Timbers, F., Lanctot, M., Zacharias Holland, G., Davoodi, E., Christianson, A., and Bowling, M. Student of games: A unified learning algorithm for both perfect and imperfect information games. Science Advances, 9(46):eadg3256, 2023.\\n\\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\"}"}
{"id": "U841CrDUx9", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "U841CrDUx9", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van Hasselt, H., Silver, D., and Singh, S. A self-tuning actor-critic algorithm. In NeurIPS, pp. 20913\u201320924, 2020.\\n\\nZhou, Z., Mertikopoulos, P., Athey, S., Bambos, N., Glynn, P. W., and Ye, Y. Learning in games with lossy feedback. In NeurIPS, pp. 5134\u20135144, 2018.\\n\\nZinkevich, M., Johanson, M., Bowling, M., and Piccione, C. Regret minimization in games with incomplete information. In NeurIPS, pp. 1729\u20131736, 2007.\"}"}
{"id": "U841CrDUx9", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we present the details of GAMEBENCH (see Figure 3 for an overview). In Section E.1, we discuss the motivation and desiderata by briefly reviewing the games that have been employed to test existing MD algorithms. In Section E.2, we present the details of the construction of all 15 games. Finally, in Section E.3, we present the evaluation measures considered in this work.\\n\\nE.1. Motivation and Desiderata\\nAs mentioned in Section 6, existing benchmarks for decision making are typically specialized for some specific categories. Furthermore, running MD algorithms on these benchmarks could be computationally prohibitive as the number of decision points in the environments could be extremely large. On the other hand, though MD algorithms have been demonstrated powerful in single-agent RL (Tomar et al., 2022) and two-player zero-sum games (Wibisono et al., 2022; Kozuno et al., 2021; Lee et al., 2021; Liu et al., 2022b; Jain et al., 2022; Ao et al., 2023; Liu et al., 2023; Cen et al., 2023; Sokota et al., 2023) in recent works, their experiments are typically conducted on a handful of games. It remains elusive how will these MD algorithms perform when applied to other categories of decision-making problems. In Table 5, we briefly review the games that have been used in some recent works.\\n\\nTable 5. The games that have been used in recent works on MD algorithms. Note that this list does not include the games that are used to benchmark deep learning-based algorithms in these references.\\n\\n| Reference          | Game            | Category              |\\n|--------------------|-----------------|-----------------------|\\n| (Sokota et al., 2023) | Kuhn Poker      | Two-Player Zero-Sum   |\\n|                    | Leduc Poker     | Two-Player Zero-Sum   |\\n|                    | 2x2 Abrupt Dark Hex | Two-Player Zero-Sum   |\\n|                    | 4-Sided Liar's Dice | Two-Player Zero-Sum   |\\n| (Liu et al., 2023)  | Kuhn Poker      | Two-Player Zero-Sum   |\\n|                    | Leduc Poker     | Two-Player Zero-Sum   |\\n| (Anagnostides et al., 2022b) | Sheriff       | Two-Player General-Sum |\\n|                    | Battleship      | Two-Player General-Sum |\\n|                    | Goofspiel       | Two-Player General-Sum |\\n|                    | Liar's Dice     | Two-Player Zero-Sum   |\\n| (Anagnostides et al., 2022a) | Sheriff       | Two-Player General-Sum |\\n|                    | Battleship      | Two-Player General-Sum |\\n|                    | Goofspiel       | Two-Player General-Sum |\\n|                    | Liar's Dice     | Two-Player Zero-Sum   |\\n| (Lee et al., 2021)  | Kuhn Poker      | Two-Player Zero-Sum   |\\n|                    | Leduc Poker     | Two-Player Zero-Sum   |\\n|                    | Pursuit-Evasion | Two-Player Zero-Sum   |\\n| (Liu et al., 2022b) | Leduc Poker     | Two-Player Zero-Sum   |\\n|                    | Goofspiel       | Two-Player Zero-Sum   |\\n|                    | Liar's Dice     | Two-Player Zero-Sum   |\\n|                    | Battleship      | Two-Player Zero-Sum   |\\n\\nIn view of the above facts, we aim to construct a novel benchmark which should satisfy two desiderata (D5 and D6 presented in the Introduction): i) it should cover all categories of decision making (comprehensive), and ii) the games are relatively simple and running MD algorithms on these games does not require much computational resource (academic-friendly).\\n\\nE.2. Games\\nIn this section, we present the details of the construction of all 15 games in our GAMEBENCH. All the games are divided into 5 categories: single-agent, cooperative multi-agent, competitive multi-agent zero-sum (zero-sum), competitive multi-agent general-sum (general-sum), and mixed cooperative and competitive (MCC) categories. In Table 6, we give an overview.\"}"}
{"id": "U841CrDUx9", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We curate the GAMEBENCH on top of OpenSpiel (Lanctot et al., 2019). For cooperative, zero-sum, and general-sum categories, we construct the game by passing the configurations to the games implemented in OpenSpiel. The configurations for these games are deliberately selected such that the instances of these games are academic-friendly (i.e., their numbers of decision points are not too large). For single-agent and MCC categories, we obtain the games by modifying the original games in OpenSpiel. In the following, we present the details of each category.\\n\\nTable 6. The games and their statistics in GAMEBENCH. \\\\(N\\\\) is the number of players and \\\"#DP\\\" stands for the number of decision points.\\n\\n| Category       | Name of Game w/ Config. | Shorthand          | \\\\(N\\\\) | #DP | Evaluation Measure          |\\n|----------------|--------------------------|--------------------|-------|-----|-----------------------------|\\n| Single-Agent   | single_agent_kuhn_a      | Kuhn-A             | 1     | 6   | OptGap                      |\\n|                | single_agent_kuhn_b      | Kuhn-B             | 1     | 6   | OptGap                      |\\n|                | single_agent_goofspiel   | Goofspiel-S        | 1     | 8   | OptGap                      |\\n| Cooperative    | tiny_hanabi_game_a       | TinyHanabi-A       | 2     | 8   | OptGap                      |\\n|                | tiny_hanabi_game_b       | TinyHanabi-B       | 2     | 6   | OptGap                      |\\n|                | tiny_hanabi_game_c       | TinyHanabi-C       | 2     | 6   | OptGap                      |\\n| Zero-Sum       | kuhn_poker(players=3)    | Kuhn               | 3     | 48  | NashConv, CCEGap           |\\n|                | leduc_poker(players=2)   | Leduc              | 2     | 936 | NashConv                    |\\n|                | goofspiel(players=3)     | Goofspiel          | 3     | 30  | NashConv, CCEGap           |\\n| General-Sum    | bargaining(max_turns=2)  | Bargaining         | 2     | 178 | NashConv, SW               |\\n|                | trade_comm(num_items=2)  | TradeComm          | 2     | 22  | NashConv, SW               |\\n|                | battleship               | Battleship         | 2     | 210 | NashConv, SW               |\\n| MCC            | mix_kuhn_3p_game_a       | MCCKuhn-A          | 3     | 48  | NashConv                   |\\n|                | mix_kuhn_3p_game_b       | MCCKuhn-B          | 3     | 48  | NashConv                   |\\n|                | mix_goofspiel_3p         | MCCGoofspiel       | 3     | 30  | NashConv                   |\\n\\nSingle-Agent. We construct three single-agent games: Kuhn-A, Kuhn-B, and Goofspiel-S, from the original two-player Kuhn poker and Goofspiel in OpenSpiel. Consider a two-player Kuhn poker game. To obtain a single-agent counterpart, we fix one player's policy as the uniform policy (called the background player) while only updating the other player's policy (called the focal player) at each iteration. In Kuhn-A, player 1 is selected as the focal player while in Kuhn-B, player 2 is chosen as the focal player, as the two players are asymmetric (Kuhn, 1950). Similarly, we can get Goofspiel-S. As the two players are symmetric in Goofspiel (Ross, 1971), we choose player 1 as the focal player without loss of generality.\\n\\nCooperative. For cooperative games, we consider the following three two-player tiny Hanabi games (Foerster et al., 2019; Sokota et al., 2021): TinyHanabi-A, TinyHanabi-B, and TinyHanabi-C. The payoff matrices along with the optimal values of these games are given in Figure 9. These games are easy to obtain in OpenSpiel by setting the three parameters: \\\\(\\\\text{num\\\\_chance}\\\\), \\\\(\\\\text{num\\\\_actions}\\\\), and \\\\(\\\\text{payoff}\\\\). For \\\\(\\\\text{num\\\\_chance}\\\\), they are 2, 2, and 2, respectively. For \\\\(\\\\text{num\\\\_actions}\\\\), they are 3, 2, and 2, respectively.\\n\\nCompetitive Zero-Sum and General-Sum. We consider the following three zero-sum games: three-player Kuhn, two-player Leduc, and three-player Goofspiel, and the following three general-sum games: two-player Battleship (Farina et al., 2020), two-player TradeComm (Sokota et al., 2021), and two-player Bargaining (Lewis et al., 2017), which are implemented in OpenSpiel. The configurations of these games are given in the second column in Table 6. Note that in contrast to most of the existing works which only focus on two-player games, we set the number of players to more than two players in some of the games: Kuhn and Goofspiel are three-player games.\\n\\nMixed Cooperative and Competitive (MCC). We construct the following three-player MCC games: MCCKuhn-A, MCCKuhn-B, and MCCGoofspiel, from the original three-player Kuhn poker and three-player Goofspiel in OpenSpiel. Consider a three-player Kuhn poker game. To obtain an MCC counterpart, we partition the three players into two teams: Team 1 includes two players while Team 2 only consists of one player (i.e., two vs. one). When computing the rewards of the players, in Team 1, each player will get the average reward of the team. Precisely, let \\\\(r_{\\\\text{team}} = r_1 + r_2\\\\) denote the team reward which is the sum of the original rewards of the two team members. Then, the true rewards of the two players are \\\\(\\\\tilde{r}_1 = \\\\tilde{r}_2 = r_{\\\\text{team}} / 2\\\\). In MCCKuhn-A, Team 1 includes players 1 and 2 (i.e., \\\\(\\\\{1, 2\\\\}\\\\) vs. 3), while in MCCKuhn-B, Team 1 includes players 1 and 3 (i.e., \\\\(\\\\{1, 3\\\\}\\\\) vs. 2). Similarly, we can get MCCGoofspiel in the same manner.\"}"}
{"id": "U841CrDUx9", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As shown in Table 6, the number of decision points (#DP) varies across different categories, which shows that GAME includes diverse enough environments as, to some extent, the number of decision points reflects the difficulty of the game.\\n\\nE.3. Evaluation Measures\\n\\nAs shown in Figure 3, we consider multiple evaluation measures in GAME. There are two types of measures: i) the notion of optimality, including OptGap and social welfare, and ii) the notion of equilibrium, including NashConv and CCEGap. In the last column of Table 6, we present the measures employed in each game. In single-agent and cooperative categories, we use OptGap as the measure which captures the distance of the current (joint) policy to the optimal (joint) policy. In the other three categories, the primary measure is NashConv which captures the distance of the current joint policy to the Nash equilibrium. In addition, we also consider other solution concepts and evaluation measures in some of the games. For zero-sum Kuhn and Goofspiel, as there are three players, we also consider the measure CCEGap which captures the distance of the current joint policy to the coarse correlated equilibrium (CCE). For general-sum games, we also consider the social welfare (SW) of all the agents.\\n\\nExcept for the MCC category, all the measures can be easily computed by using the built-in implementation functions in OpenSpiel. However, to compute the NashConv in the MCC games, we need to compute the best response policy of the team, i.e., a joint policy of the team members, rather than the policy of a single agent. This is incompatible with the built-in implementation in OpenSpiel, which only computes the best response policy of a single agent. In other words, if we directly adopt the built-in implementation, the NashConv will correspond to the original three-player game, not the modified game.\\n\\nUnfortunately, computing the exact joint policy of the team members is not easy in practice. Nevertheless, it is worth noting that from our experiments, we found that MMD-KL can effectively solve cooperative decision-making problems. As a result, we can apply MMD-KL to compute the approximate best response of the team as it is a purely cooperative environment from the team's perspective (the other team's policy is fixed when computing the best response of the team). For a team that only has a single player, we use the built-in implementation in OpenSpiel to compute the exact best response policy of the player.\\n\\nIn summary, during the policy learning process, when the evaluation of the current joint policy is needed, we use MMD-KL as a subroutine to compute a team's approximate best response while using built-in implementation to compute a single player's exact best response. In the MMD-KL subroutine, the starting point of the best response is set to the current joint policy of the team members. In experiments, to balance the accuracy of the approximate best response and running time, the number of updates in the MMD-KL subroutine is set to 100 (the returned joint policy can be also called a better response). For example, in MCCKuhn-A, suppose the current joint policy is $\\\\pi = \\\\pi_\\\\text{team} \\\\times \\\\pi_3$ where $\\\\pi_\\\\text{team} = \\\\pi_1 \\\\otimes \\\\pi_2$ is the team's joint policy. The built-in implementation in OpenSpiel can only compute the best response policy for every single agent and hence, the resulting NashConv ($\\\\pi$) = $\\\\sum_{i=1}^{3} [V_i(\\\\nu, \\\\pi_{BR_i} \\\\times \\\\pi_{-i}) - V_i(\\\\nu, \\\\pi)]$ corresponds to the original three-player game. In contrast, in our method, we use MMD-KL to compute the team's best response rather than the single agent's. Therefore, the NashConv of $\\\\pi$ is:\\n\\n$$\\\\text{NashConv}(\\\\pi) = V_{\\\\text{team}}(\\\\nu, \\\\pi_{BR_{\\\\text{team}}} \\\\times \\\\pi_3) - V_{\\\\text{team}}(\\\\nu, \\\\pi) + V_3(\\\\nu, \\\\pi_{\\\\text{team}} \\\\times \\\\pi_{BR_3}) - V_3(\\\\nu, \\\\pi),$$\\n\\n(27)\\n\\nwhere $\\\\pi_{BR_{\\\\text{team}}}$ is the team's BR policy computed via MMD-KL given that player 3 is fixed to $\\\\pi_3$ (that is, player 3 is a part of the environment from the team's perspective). As players 1 and 2 are fully cooperative, they share the same value $V_{\\\\text{team}}$. 25\"}"}
{"id": "U841CrDUx9", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide more experimental details, results, and analysis. We briefly summarize each section below.\\n\\n- **Section F.1.** More details on the experimental setup, including the hyper-parameter settings for different methods.\\n- **Section F.2.** Searching of $M$ (the number of previous policies) and $\\\\mu$ (the smoothing parameter in DRS) ($D_1$ and $D_2$).\\n- **Section F.3.** Investigation of performance w.r.t. the number of joint actions ($D_1$ and $D_2$).\\n- **Section F.4.** Investigation of GMD with different heuristic strategies for adjusting $\\\\alpha$ ($D_1$ and $D_2$).\\n- **Section F.5.** Investigation of different meta-controllers ($D_1$ and $D_2$).\\n- **Section F.6.** Investigation of different Bregman divergences ($D_1$ and $D_2$).\\n- **Section F.7.** Investigation of the effectiveness of adding the magnet policy ($D_1$ and $D_2$).\\n- **Section F.8.** Investigation of different evaluation measures and different solution concepts ($D_3$ and $D_4$).\\n- **Section F.9.** Analysis of the computational complexity for running different algorithms on GAMEBENCH ($D_5$ and $D_6$).\\n\\n### F.1. Experimental Setup\\n\\n**Hyper-parameters.** Table 7 provides the default values of hyper-parameters used in different methods. In the RS-type meta-controllers (RS and DRS), the spherically symmetric distribution $q$ is a standard multivariate normal distribution $N(0, I)$. For CMD/GMD, there are two critical hyper-parameters: the number of previous policies $M \\\\geq 1$ and the smoothing parameter $\\\\mu$ in DRS. In Section F.2, we perform an ablation study to determine their default values (given in Table 7), which will be fixed in other experiments. The specific setups for each experiment will be given in each of the following sections.\\n\\n**Baselines.** We consider the MMD-type (MMD-KL and MMD-EU) and CFR-type (CFR and CFR+) algorithms as the baselines. It is worth noting that CFR-type algorithms can be also applied to single-agent and cooperative categories.\\n\\n**Computational Resources.** Experiments are performed on a machine with a 24-core i9 and NVIDIA A4000. For CMD, the results are obtained with 3 random seeds. For other methods, as there is no randomness, no multiple runs are needed.\\n\\n### Table 7. Default values of the hyper-parameters in different methods.\\n\\n| Game    | $K$ | $\\\\epsilon$ | $C$ | $\\\\iota$ | $M$ | $D$ | $\\\\kappa$ | $\\\\mu$ | $r_L$ | $r_H$ | $\\\\xi$ | $\\\\eta$ | $\\\\tilde{\\\\eta}$ |\\n|---------|-----|-------------|-----|---------|-----|-----|---------|-------|-------|-------|------|------|---------|\\n| Kuhn-A  | 100000 | 1e-10      | 50  | 1e-6    | 1   | 5   | 10      | 0.05  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| Kuhn-B  | 100000 | 1e-10      | 50  | 1e-6    | 1   | 5   | 10      | 0.05  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| Goofspiel-S | 100000 | 1e-10    | 50  | 1e-6    | 1   | 5   | 10      | 0.05  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| TinyHanabi-A | 100000 | 1e-10 | 50  | 1e-6    | 3   | 5   | 10      | 0.05  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| TinyHanabi-B | 100000 | 1e-10 | 50  | 1e-6    | 1   | 5   | 10      | 0.05  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| TinyHanabi-C | 100000 | 1e-10 | 50  | 1e-6    | 1   | 5   | 10      | 0.05  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| Kuhn    | 100000 | 1e-10      | 50  | 1e-6    | 5   | 5   | 10      | 0.01  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| Leduc   | 100000 | 1e-10      | 50  | 1e-6    | 3   | 5   | 10      | 0.05  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| Goofspiel | 100000 | 1e-10    | 50  | 1e-6    | 3   | 5   | 10      | 0.01  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| Bargaining | 100000 | 1e-10   | 50  | 1e-6    | 5   | 5   | 10      | 0.05  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| TradeComm | 100000 | 1e-10 | 50  | 1e-6    | 1   | 5   | 10      | 0.01  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| Battleship | 100000 | 1e-10  | 50  | 1e-6    | 1   | 5   | 10      | 0.05  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| MCCKuhn-A | 100000 | 1e-10 | 50  | 1e-6    | 1   | 5   | 10      | 0.01  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| MCCKuhn-B | 100000 | 1e-10 | 50  | 1e-6    | 1   | 5   | 10      | 0.01  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\\n| MCCGoofspiel | 100000 | 1e-10 | 50  | 1e-6    | 1   | 5   | 10      | 0.01  | 0.01  | 0.05  | 1    | 0.1  | 0.05    |\"}"}
{"id": "U841CrDUx9", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We incorporate different MD algorithms into the baselines, including the CFR (Zinkevich et al., 2007), its exploitative versions (Shoham & Zinkevich, 2007), MMD-KL (van den Herik et al., 2023) where the policy updating rule is induced with KL divergence, and MMD-SCF (van den Herik et al., 2023) where a meta-controller introduced in Section 5.2 is used. As a comparison, we consider the following methods: i) CMD: our method where the hyper-parameters are determined by the results by answering several research questions (explore/exploit dilemma, stability, computational complexity, etc.,), ii) GMD: the algorithm where the hyper-parameters are determined by the AME runs in each best-response (BR) computation (see Section 5.2). In this section, we evaluate our method on the games in Appendix E.2.\\n\\nOptimal Gap and Social Welfare. Roughly speaking, there are two types of evaluation measures. The first type of measure is based on the optimal gap, which is a comparison with the optimal solution, and the second type is based on the social welfare, which is a comparison with the Nash equilibrium. As G is a single-agent game, we can use the optimal gap as a measure for the quality of the solution. In a team setting, the optimal gap is not a good measure because it does not take into account the quality of the solution for the other agents. Instead, we can use the social welfare as a measure for the quality of the solution. The social welfare is defined as the sum of the utilities of all the agents.\\n\\nAs G is a single-agent game, we can use the optimal gap as a measure for the quality of the solution. In a team setting, the optimal gap is not a good measure because it does not take into account the quality of the solution for the other agents. Instead, we can use the social welfare as a measure for the quality of the solution. The social welfare is defined as the sum of the utilities of all the agents.\\n\\nAlgorithms. We first describe the experimental setup. Then, we present the details of the constructions and the statistics of the G games in Appendix E.3. In addition, we also include CFR-type algorithms as the baselines, including the state-of-the-art baselines.\"}"}
{"id": "U841CrDUx9", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RQ1. (Desiderata D1 and D2)\\n\\nIn Figure 6, how does CMD perform under different Bregman divergences?\\n\\nRQ2. (Comparison with Baselines)\\n\\nAs we can see, our proposed MC significantly outperforms others either in comparison, we can obtain the following takeaways.\\n\\nRQ3. (Desiderata D3 and D4)\\n\\nCan CMD effectively tackle more efficiently adjust the hyper-parameters.\\n\\nRQ4. (Different Bregman Divergences)\\n\\nAs shown in Figure 4, by incorporating multiple historical policies and dynamically adjusting the hyper-parameters, our CMD can find the approximate Nash equilibrium (the NashConv converges to an extremely small value).\\n\\nRQ5. (Desiderata D5 and D6)\\n\\nIn Figure 5, we compare DRS and MCCKuhn-B and performs on par with the baselines achieving competitive or better outcomes.\\n\\nRQ6. (Aversity and Robustness)\\n\\nIn Figure 7, we apply CMD to two different measures: CCE-AME and CCE-ENCH.\\n\\nCan CMD generalize to consider different solution concepts and evaluation measures?\\n\\nIs the direction-guided update in the CMD algorithm on G achievable in all categories of decision-making problems?\\n\\n8. Limitations, Future Works, and Conclusions\\n\\nWe found that, although minimal modifications: changing the MC's objective to the Bregman divergence, a prominent feature compared with existing MD methods. See Appendix F.6 for more results.\\n\\n8.1 Configurable Mirror Descent: Towards a Unification of Decision Making\\n\\nThe computational complexity can be found in Appendix F.9. This can be verified by comparing CMD with MMD-KL where CMD converges to a similar OptGap or NashConv value with MMD-KL using fewer iterations. For GMD with different types of MC, does not cause much burden extra operations may be required, running the MD and CFR algorithms on GAME.\\n\\nCollecting results and analysis can be found in Appendix F.8. Notably, for the capability of (empirically) exploring a broader class of strategies, though has been widely adopted, could be not the optimal \u03c8 instantiation with different convex functions. From the results and analysis, more interesting insights can be found by verifying the effectiveness of incorporating multiple histories. More detailed results and analysis can be found in Appendix F.9.\"}"}
{"id": "U841CrDUx9", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As a preliminary attempt, there are still some limitations under some specific Bregman divergence (Sokota et al., 2020; Amos et al., 2017). Moreover, using neural networks to learn the convex function for the given category may not be easy and thus may require new treatment. When selecting the Bregman divergence, there are two main approaches: the regularized optimization problem in each decision point method, rather than depends on the closed-form solution to the convex function\\\\footnote{\\\\psi} of large language models (LLMs) to solve various decision-making problems. iv) Replacing the message passing rule of CMD with the more computationally efficient GMD, a generalization of exiting MD algorithms, which can be applied to different decision-making categories involving different numbers of agents and different relationships between agents (Hong et al., 2023; Bakhtin et al., 2022; Rabin, 1993). iii) We could include more evaluation measures (e.g., fairness) to tackle all categories of decision-making problems. iv) Replacing the message passing rule of CMD with the more computationally efficient GMD, a generalization of exiting MD algorithms, which can be applied to different decision-making categories involving different numbers of agents and different relationships between agents (Hong et al., 2023; Bakhtin et al., 2022; Rabin, 1993). iii) We could include more evaluation measures (e.g., fairness) to tackle all categories of decision-making problems.\"}"}
{"id": "U841CrDUx9", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThis work is supported by the National Research Foundation, Singapore under its Industry Alignment Fund \u2013 Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.\\n\\nHau Chan is supported by the National Institute of General Medical Sciences of the National Institutes of Health [P20GM130461], the Rural Drug Addiction Research Center at the University of Nebraska-Lincoln, and the National Science Foundation under grant IIS:RI #2302999. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies. Shuyue Hu is supported by the Shanghai Artificial Intelligence Laboratory.\\n\\nImpact Statement\\n\\nThis paper presents work whose goal is to advance the field of decision making. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.\\n\\nReferences\\n\\nAlbrecht, J., Fetterman, A., Fogelman, B., Kitanidis, E., Wr\u00f3blewski, B., Seo, N., Rosenthal, M., Knutins, M., Polizzi, Z., Simon, J., and Qiu, K. Avalon: A benchmark for RL generalization using procedurally generated worlds. In NeurIPS Datasets and Benchmarks Track, pp. 12813\u201312825, 2022.\\n\\nAmos, B., Xu, L., and Kolter, J. Z. Input convex neural networks. In ICML, pp. 146\u2013155, 2017.\\n\\nAnagnostides, I., Farina, G., Panageas, I., and Sandholm, T. Optimistic mirror descent either converges to Nash or to strong coarse correlated equilibria in bimatrix games. In NeurIPS, pp. 16439\u201316454, 2022a.\\n\\nAnagnostides, I., Panageas, I., Farina, G., and Sandholm, T. On last-iterate convergence beyond zero-sum games. In ICML, pp. 536\u2013581, 2022b.\\n\\nAo, R., Cen, S., and Chi, Y. Asynchronous gradient play in zero-sum multi-agent games. In ICLR, 2023.\\n\\nAumann, R. J. Correlated equilibrium as an expression of Bayesian rationality. Econometrica: Journal of the Econometric Society, 55(1):1\u201318, 1987.\\n\\nBailey, J. P. and Piliouras, G. Multiplicative weights update in zero-sum games. In EC, pp. 321\u2013338, 2018.\\n\\nBailey, J. P. and Piliouras, G. Fast and furious learning in zero-sum games: Vanishing regret with non-vanishing step sizes. In NeurIPS, pp. 12997\u201313007, 2019.\\n\\nBakhtin, A., Brown, N., Dinan, E., Farina, G., Flaherty, C., Fried, D., Goff, A., Gray, J., Hu, H., Jacob, A. P., Komeili, M., Konath, K., Kwon, M., Lerer, A., Lewis, M., Miller, A. H., Mitts, S., Renduchintala, A., Roller, S., Rowe, D., Shi, W., Spisak, J., Wei, A., Wu, D., Zhang, H., and Zijlstra, M. Human-level play in the game of Diplomacy by combining language models with strategic reasoning. Science, 378(6624):1067\u20131074, 2022.\\n\\nBard, N., Foerster, J. N., Chandar, S., Burch, N., Lanc tot, M., Song, H. F., Parisotto, E., Dumoulin, V., Moitra, S., Hughes, E., Dunning, I., Mourad, S., Larochelle, H., Bellemare, M. G., and Bowling, M. The Hanabi challenge: A new frontier for AI research. Artificial Intelligence, 280:103216, 2020.\\n\\nBeck, A. and Teboulle, M. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167\u2013175, 2003.\\n\\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253\u2013279, 2013.\\n\\nBerner, C., Brockman, G., Chan, B., Cheung, V., D\u02db ebiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., J\u00f3zefowicz, R., Gray, S., Olsson, C., Pachocki, J., Petrov, M., Pinto, H. P. d. O., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski, F., and Zhang, S. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\\n\\nBoyd, S. P. and Vandenberghe, L. Convex Optimization. Cambridge University Press, 2004.\\n\\nBrown, N. and Sandholm, T. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, 359(6374):418\u2013424, 2018.\\n\\nBrown, N. and Sandholm, T. Superhuman AI for multiplayer poker. Science, 365(6456):885\u2013890, 2019.\\n\\nCampbell, M., Hoane Jr, A. J., and Hsu, F.-h. Deep Blue. Artificial Intelligence, 134(1-2):57\u201383, 2002.\\n\\nCarminati, L., Zhang, B. H., Farina, G., Gatti, N., and Sandholm, T. Hidden-role games: Equilibrium concepts and computation. arXiv preprint arXiv:2308.16017, 2023.\\n\\nCen, S., Chi, Y., Du, S. S., and Xiao, L. Faster last-iterate convergence of policy optimization in zero-sum Markov games. In ICLR, 2023.\"}"}
{"id": "U841CrDUx9", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we give some remarks on the computational complexity of different methods. In Section F.9.1, we focus on the running time of different methods, and in Section F.9.2 we present the memory consumption of different methods.\\n\\nNote that i) these numbers are obtained under the default values of hyper-parameters (Table 7), and ii) these numbers are not absolute and depend on the property of the game (see Table 6) and the computational resources used to run the experiments (see Section F.1); they only provide intuition on the computational complexity of different methods to show that our GAMEBENCH can satisfy the desiderata $D5$ and $D6$ mentioned in the Introduction.\\n\\n### F.9.1. Running Time\\n\\nThe running time of different methods in different games is shown in Table 8. From the results, we can see that in most of the games, running all the algorithms does not cause a very long running time (the desiderata $D5$ and $D6$ presented in the Introduction), even for our methods where a set of extra operations is required: GMD requires computing the value of the dual variable via a numerical method and CMD further requires to evaluate multiple candidates of the hyper-parameters.\\n\\nNotably, we emphasize that: i) Our methods (CMD/GMD) provide the capability of exploring more dimensions of decision making, though they require extra computational cost (the major limitation of the current version of our methods); ii) Comparing CMD and GMD, we can see that the major cost comes from evaluating multiple samples. Therefore, as pointed out in Section 8, we view this as a future direction: developing more computationally efficient hyper-parameter value updating methods without sacrificing performance. In this regard, other techniques such as Bayesian optimization (Lindauer et al., 2022) or offline hyper-parameter optimization approaches (Chen et al., 2022) may be required.\\n\\n| Game      | CFR     | CFR+    | MMD     | -KL    | MMD     | -EU     | GMD     | CMD     | DRS | RS | DGLDS | GLDS | GLD |\\n|-----------|---------|---------|---------|--------|---------|---------|---------|---------|-----|----|-------|------|-----|\\n| Kuhn-A    | 0.0004  | 0.0004  | 0.0003  | 0.0003 | 0.0034  | 0.0372  | 0.0372  | 0.0204  | 0.0204| 0.0203|\\n| Kuhn-B    | 0.0004  | 0.0004  | 0.0003  | 0.0003 | 0.0033  | 0.0370  | 0.0365  | 0.0202  | 0.0201| 0.0202|\\n| Goofspiel-S | 0.0007 | 0.0006  | 0.0004  | 0.0004 | 0.0046  | 0.0491  | 0.0489  | 0.0269  | 0.0275| 0.0270|\\n| TinyHanabi-A | 0.0006 | 0.0006  | 0.0004  | 0.0004 | 0.0045  | 0.0474  | 0.0472  | 0.0262  | 0.0268| 0.0260|\\n| TinyHanabi-B | 0.0004 | 0.0004  | 0.0003  | 0.0003 | 0.0033  | 0.0366  | 0.0370  | 0.0198  | 0.0192| 0.0197|\\n| TinyHanabi-C | 0.0004 | 0.0004  | 0.0003  | 0.0003 | 0.0032  | 0.0364  | 0.0359  | 0.0195  | 0.0195| 0.0199|\\n| Kuhn      | 0.0084  | 0.0082  | 0.0022  | 0.0021 | 0.0267  | 0.4102  | 0.4058  | 0.2273  | 0.2282| 0.2288|\\n| Leduc     | 0.0942  | 0.0961  | 0.0422  | 0.0412 | 0.5146  | 6.8897  | 6.9749  | 3.8188  | 3.8116| 3.8744|\\n| Goofspiel | 0.0072  | 0.0073  | 0.0014  | 0.0014 | 0.0167  | 0.2879  | 0.2899  | 0.1636  | 0.1610| 0.1625|\\n| Bargaining| 0.0279  | 0.0273  | 0.0130  | 0.0116 | 0.1093  | 1.5311  | 1.5308  | 0.8428  | 0.8579| 0.8516|\\n| TradeComm | 0.0028  | 0.0029  | 0.0011  | 0.0010 | 0.0121  | 0.1704  | 0.1699  | 0.0957  | 0.0942| 0.0939|\\n| Battleship| 0.0245  | 0.0248  | 0.0097  | 0.0094 | 0.1125  | 1.5570  | 1.5771  | 0.8833  | 0.8774| 0.8835|\\n| MCCKuhn-A | 0.0083  | 0.0084  | 0.0021  | 0.0021 | 0.0264  | 6.9054  | 6.8377  | 4.0453  | 4.1461| 4.1034|\\n| MCCKuhn-B | 0.0080  | 0.0083  | 0.0021  | 0.0021 | 0.0260  | 6.8100  | 6.7847  | 4.1047  | 4.0944| 4.1104|\\n| MCCGoofspiel | 0.0070 | 0.0073  | 0.0014  | 0.0014 | 0.0167  | 5.3541  | 5.3852  | 3.1817  | 3.1945| 3.1836|\"}"}
{"id": "U841CrDUx9", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"The memory usage of different methods in different games is provided in Table 9. From the results, we can see that running all the algorithms does not cause much memory consumption, which shows that our GAMEBENCH is academic-friendly.\\n\\n| Game  | CFR | CFR+ | MMD | MMD -KL | GMD | CMD | DRS | RS | DGLDS | GLDS | GLD |\\n|-------|-----|------|-----|---------|-----|-----|-----|----|-------|------|-----|\\n| Kuhn-A | 0.8750 | 0.9805 | 0.3711 | 0.3750 | 0.9492 | 1.0352 | 1.0859 | 0.3750 | 0.3750 | 0.3750 |\\n| Kuhn-B | 0.8672 | 0.8672 | 0.4297 | 0.3750 | 0.7461 | 1.1289 | 1.0352 | 0.3750 | 0.3750 | 0.3164 |\\n| Goofspiel-S | 1.1875 | 1.2969 | 1.2617 | 1.2539 | 1.2578 | 1.2578 | 1.2578 | 1.6992 | 1.6953 | 1.6992 |\\n| TinyHanabi-A | 1.0352 | 1.0156 | 0.4805 | 0.4258 | 0.4883 | 1.0312 | 1.1836 | 0.4453 | 0.4297 | 0.4883 |\\n| TinyHanabi-B | 0.9922 | 1.0898 | 0.4922 | 0.4844 | 0.4336 | 0.4922 | 0.5430 | 0.4922 | 0.4297 | 0.4922 |\\n| TinyHanabi-C | 0.9805 | 0.9922 | 0.4336 | 0.4297 | 0.4336 | 0.4922 | 0.4336 | 0.4336 | 0.4336 | 0.4336 |\\n| Kuhn | 1.9961 | 2.0078 | 3.0352 | 3.1367 | 3.5586 | 3.7734 | 3.7227 | 3.5156 | 3.5352 | 3.5273 |\\n| Leduc | 26.262 | 26.344 | 51.664 | 52.465 | 58.273 | 59.063 | 58.555 | 52.688 | 51.949 | 51.359 |\\n| Goofspiel | 2.4844 | 2.4297 | 3.5039 | 3.4961 | 4.3555 | 4.3359 | 4.3047 | 4.0391 | 4.0430 | 4.0898 |\\n| Bargaining | 10.633 | 10.578 | 24.816 | 24.852 | 35.129 | 35.422 | 35.020 | 31.941 | 32.344 | 31.781 |\\n| TradeComm | 1.4961 | 1.5430 | 2.0156 | 2.0703 | 2.0664 | 2.0078 | 2.0117 | 2.2461 | 2.2461 | 2.3633 |\\n| Battleship | 6.9102 | 6.9023 | 13.539 | 13.422 | 13.543 | 13.543 | 13.484 | 12.098 | 12.148 | 12.332 |\\n| MCCKuhn-A | 2.5742 | 2.5195 | 2.0312 | 2.0273 | 2.5586 | 2.6719 | 2.7266 | 2.3047 | 2.2930 | 2.2930 |\\n| MCCKuhn-B | 2.4688 | 2.5703 | 2.0273 | 1.9648 | 1.9727 | 2.6562 | 2.7617 | 2.2383 | 2.2305 | 2.1797 |\\n| MCCGoofspiel | 2.7500 | 2.7500 | 3.0078 | 3.0820 | 3.0781 | 3.0195 | 3.1289 | 2.8203 | 2.8047 | 2.8672 |\"}"}
{"id": "U841CrDUx9", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the number of iterations.\\n\\nFor all the figures, the -axis is . The rest figures correspond to other categories where the \\\\( \\\\text{OptGap} \\\\) is common in existing MD algorithms, is not always the optimal choice across different decision-making problems. For example, in the most difficult Leduc poker game, when \\\\( \\\\mu = 1 \\\\), we find that the learning performance. We consider configurations of MD.\"}"}
{"id": "U841CrDUx9", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11.\\n\\nPerformance of different methods w.r.t. the number of joint actions. The first 6 figures correspond to single-agent and baseline MD and CFR-type algorithms, CMD requires evaluating multiple candidates at each iteration. Nevertheless, compared to baseline MD and CFR-type algorithms, CMD provides a feasible way to study different solution concepts and evaluation measures, though, in the current version, it is small in our experiments. The rest figures correspond to other categories where the \\\\( D \\\\) \\\\( x \\\\) \\\\(-axis is the number of iterations. For example, a method that only needs to sample one candidate (in this case, the number of joint actions will be 5), i.e., sample 5 candidate joint policies), the \\\\( D \\\\) \\\\( x \\\\) \\\\(-axis for the CMD by \\\\( \\\\frac{x}{D} \\\\) \\\\( y \\\\) \\\\. The conclusions in terms of the number of iterations presented in the main text still hold in terms of the number of joint actions.\"}"}
{"id": "U841CrDUx9", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12. While it can also be beaten by others in other scenarios.\\n\\nIn this section, we consider two more heuristic strategies: i) \\\"GMD(LD)\\\" denotes that the\\n\u03b1\\n... In the main text, the baseline method GMD employs a fixed strategy \u2013 a uniform distribution \u2013 to determine the value of\\nF.\\n\\nF.4. Different Heuristic Strategies for Adjusting\\n\\nOptGap\\n-axis is decayed with the iteration in the form of inverse square root function\\n\\nThe first 6 figures correspond to single-agent\\n\\nExperimental results for GMD with different heuristic strategies for adjusting\\n\u03b1\\n...\"}"}
{"id": "U841CrDUx9", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"verify this intuition, we visualize the evolution of the hyper-parameter \u03b1. The critical observation that supports our proposed DRS is that the value of the evaluation number of iterations.\\n\\nFigure 13. Evolution of Hyper-Parameters.\\n\\nFrom the results, we can see that DRS is the best choice among different MCs. Particularly, in Leduc and MCCKuhn-B, DRS achieves a significantly better convergent performance than other baseline algorithms. In this section, we investigate the effectiveness of different MCs, and the results are shown in Figure 13.\"}"}
{"id": "U841CrDUx9", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"when the value of the performance is extremely small. We propose a simple yet effective zero-order optimization with respect to the hyper-parameters. To address the issue, \\\\( \\\\Pi \\\\)\\n\\n\\\\[ \\\\pi_t \\\\]\\n\\nis a decision point (i.e., \\\\( \\\\tau \\\\)). The reward function of agent \\\\( i \\\\)\\n\\n\\\\[ R \\\\]\\n\\ndenotes the simplex. The transition function which specifies the probability of transition\\n\\n\\\\[ a \\\\in A \\\\]\\n\\n\\\\[ \\\\Omega = \\\\{ a \\\\} \\\\]\\n\\n\\\\[ \\\\Omega \\\\in \\\\mathbb{N} \\\\]\\n\\n\\\\[ \\\\Omega \\\\in \\\\mathbb{N} \\\\]\\n\\n\\\\[ \\\\Omega \\\\]\\n\\ngives the joint action of agents where \\\\( i \\\\) and observations of agent \\\\( i \\\\)\\n\\n\\\\[ O \\\\]\\n\\n\\\\[ A \\\\]\\n\\n\\\\[ O \\\\in S \\\\]\\n\\n\\\\[ S \\\\]\\n\\n\\\\[ N \\\\]\\n\\n\\\\[ \\\\pi_t \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ a \\\\]\\n\\n\\\\[ \\\\Omega \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\Omega \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n\\\\[ \\\\pi \\\\]\\n\\n"}
{"id": "U841CrDUx9", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1.\\n\\n| Method       | Multiple previous policies | Working on any number of agents | Working on any number of agents | Configurable for any relationship between agents |\\n|--------------|---------------------------|--------------------------------|--------------------------------|-----------------------------------------------|\\n| MD (Nemirovskij & Yudin, 1983) | % | % | % | % |\\n| MMD (Sokota et al., 2023) | ! | ! | ! | ! |\\n| GMD (This work) | ! | ! | ! | ! |\\n| CMD (This work) | ! | ! | ! | ! |\\n\\n5. Configurable Mirror Descent\\n\\nIn this section, we propose a novel algorithm which satisfies the four desiderata (D1\u2013D4) presented in the Introduction. First, we propose the generalized mirror descent (GMD), a generalization of existing MD algorithms, which when independently executed by each agent, can effectively tackle different decision-making categories involving different numbers of agents and different relationships between agents (D1 and D2). Second, we propose the configurable mirror descent (CMD) where a meta-controller is introduced to dynamically adjust the hyper-parameters of GMD conditional on the evaluation measures, which can be configured to account for different solution concepts as well as evaluation measures (D3 and D4), with minimal modifications.\\n\\nCMD shares similarities with the centralized training and decentralized execution (CTDE) (Lowe et al., 2017) since the meta-controller considers all agents to optimize the targeted evaluation measures (\u201ccentralized\u201d training from the controller\u2019s perspective) while GMD is executed by each agent independently (\u201cdecentralized\u201d execution from each agent\u2019s perspective). The overview of CMD is shown in Figure 2 and Table 1 presents a comparison to more clearly position our methods in the context of related literature.\"}"}
{"id": "U841CrDUx9", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\( \\\\psi(x) \\\\) can be converted to solve the following equation:\\n\\n\\\\[ \\\\forall \\\\pi \\\\in \\\\mathcal{A} \\\\text{ have:} \\\\]\\n\\nAlgorithm 3 in Appendix D.1.\\n\\nThis result is obtained via the Karush\u2013Kuhn\u2013Tucker (KKT) conditions of the Lagrange function obtained by applying the Lagrange multiplier \\\\( \\\\lambda \\\\) where \\\\( \\\\mathcal{M} \\\\) is the dual variable, \\\\( \\\\mathcal{A} \\\\) the domain, and \\\\( \\\\mathcal{B} \\\\) the regularization intensity of \\\\( \\\\pi \\\\).\\n\\nThe negative entropy and squared Euclidean norm are two special variants that have been extensively adopted in literature. In addition to taking multiple historical policies into consideration, GMD differentiates Bregman Divergences.\\n\\nProjection Operation.\\n\\nAfter computing the value of \\\\( \\\\pi^k \\\\), we can get the policy \\\\( \\\\pi^k \\\\) of \\\\( \\\\pi^k \\\\) with \\\\( \\\\alpha \\\\), \\\\( \\\\lambda \\\\), and \\\\( \\\\tau \\\\). The full derivation can be found in Appendix D.1.\\n\\nProposition 5.1. Assume that i) the derivative of \\\\( \\\\psi \\\\) is the dual variable; ii) the projection operation to ensure that \\\\( \\\\pi \\\\in \\\\mathcal{A} \\\\). The term GMD is also used in (Radhakrishnan et al., 2020), which is a convex function defined on \\\\( \\\\mathcal{X} \\\\). The sum of convex functions is still a convex function. Furthermore, the negative entropy and the constraints are removed (i.e., the \\\\( \\\\phi \\\\) only in certain settings such as the convex function \\\\( \\\\mathcal{V} \\\\).\\n\\nIn practice, the problem could have a closed-form solution. To address this issue, we typically cannot be solved analytically. To this end, first, we have the following result:\\n\\nNow we need to compute the value of \\\\( \\\\psi \\\\) and let \\\\( \\\\lambda = 1 \\\\). The numerical method to compute the value of \\\\( \\\\lambda \\\\) is the inverse function of \\\\( \\\\psi \\\\).\\n\\nThe pseudo-code can be found in GMD Summary. The term GMD can satisfy the desiderata \\\\( D1 \\\\) while GMD cannot satisfy the last two desiderata \\\\( D2 \\\\). The sum of convex functions is still a convex function. Furthermore, the negative entropy and the constraints are removed (i.e., the \\\\( \\\\phi \\\\) only in certain settings such as the convex function \\\\( \\\\mathcal{V} \\\\).\\n\\nThe term GMD can satisfy the desiderata \\\\( D1 \\\\). The term GMD is a \\\"centralized\\\" process from the meta-controller's perspective as it considers all the agents (i.e., the \\\\( \\\\phi \\\\) which is a \\\"decentralized\\\" execution process (different measures could be better than existing choices such as KL divergence and entropy). More details can be found in Appendix D.1.}\\n\\nThe \\\\( \\\\phi \\\\) parameters in GMD conditional on the evaluation measures may be better than existing choices such as KL divergence and entropy. More details can be found in Appendix D.1.\"}"}
{"id": "U841CrDUx9", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Configurable Mirror Descent (CMD)  \\n\\nAlgorithm 2  \\n\\nThe pseudo-code of CMD is shown in Algorithm 2.\\n\\nLet $\\\\alpha_k$ be the desiderata adapted in the $k$th iteration. At the iteration $k$, the update direction-guided update is defined as follows:\\n\\n$$L_{\\\\alpha_k} = \\\\alpha_k \\\\sum_{i=1}^{N} (V_{\\\\nu,\\\\pi}^{i} - V_{\\\\nu,\\\\pi_{\\\\emptyset}}^{i})$$\\n\\nwhere $V_{\\\\nu,\\\\pi}^{i}$ is the performance of the new joint policy derived from the previous $\\\\pi_{\\\\emptyset}$ and $\\\\pi^{i}$ is the corresponding joint policies sampled by perturbing the current $\\\\pi_{\\\\emptyset}$.\\n\\nTo address this issue, we construct an effective zero-order MC by leveraging a zero-order method to optimize $\\\\alpha_k$ based on the performance difference between the two candidates sampled by perturbing the current $\\\\pi_{\\\\emptyset}$.\\n\\nThe components of GMD require much computational resource (academic-friendly). Existing zero-order methods such as the random search (RS) (Liu et al., 2020) typically update the $\\\\pi$ corresponding joint policies based on the performance difference between the two candidates sampled by perturbing the current $\\\\pi_{\\\\emptyset}$. More details on different MCs can be found in Appendix D.4.\\n\\nIn our experiments, we construct an MC \u2013 direction-guided update (RS) (Wang et al., 2022). More details on different MCs are given below.\\n\\n**Zero-Order Meta-Controller.** As shown in Eq. (3), given $\\\\alpha_k$, the number of historical policies $M$, and the hyper-parameters $\\\\epsilon$, we propose to update $\\\\alpha_k$ via GMD with the updated $\\\\alpha_k$ +1. To address this issue, we construct an effective zero-order MC by leveraging a zero-order method to optimize $\\\\alpha_k$ based on the performance difference between the two candidates sampled by perturbing the current $\\\\pi_{\\\\emptyset}$.\\n\\n**Direction-Guided Update.**\\n\\nDerive new joint policies $\\\\pi^{j}$:\\n\\n1. Given $\\\\alpha_k$, Sample $\\\\{L_{\\\\alpha_k}, D_{j}\\\\}$\\n2. Evaluate new joint policies $\\\\pi^{j}$\\n3. Evaluate $\\\\pi^{j}$\\n4. Derive new joint policies $\\\\pi^{j}$\\n5. Compute $\\\\alpha_k$ based on the performance of these new joint policies.\\n6. Update $\\\\alpha_k$ based on the performance difference between the two candidates sampled by perturbing the current $\\\\pi_{\\\\emptyset}$\\n7. Compute $\\\\alpha_k$ based on the performance of these new joint policies.\\n8. end for\\n\\n**Motivation.**\\n\\nAlthough various benchmarks have been suggested in literature, they are typically specialized for specific decision-making categories, e.g., Atari (Bellemare et al., 2013), Battleship (Brown & Sandholm, 2018; 2019) for competitive category, and footbal (Brown & Sandholm, 2020) for cooperative category, Hold'em poker (Brown & Sandholm, 2018; 2019) for competitive category, and footbal (Brown & Sandholm, 2020) for cooperative category, Hold'em poker (Brown & Sandholm, 2018; 2019) for competitive category, and footbal (Brown & Sandholm, 2020) for cooperative category, and Hanabi (Bard et al., 2014) for single-agent category. In our GAMES, we curate the GAMES which consists of 15 games covering all categories of decision-making problems (comprehensive and data-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive). As MD algorithms require much computational resource (academic-friendly), running them on the existing benchmarks could be resource-intensive. To address this issue, we construct a simple yet effective technique the direction-guided update (RS) (Liu et al., 2020) typically update the $\\\\pi$ corresponding joint policies based on the performance difference between the two candidates sampled by perturbing the current $\\\\pi_{\\\\emptyset}$.\\n\\nMotivated by the above facts, we construct a new benchmark \u2013 GAMES. It satisfies the two desiderata (Nemirovskij & Yudin, 1983) and different algorithms, which is shown in Figure 3.\\n\\n**Desiderata.**\\n\\nBy incorporating the MC into GMD, we can be configured to establish the CMD. Intuitively, CMD can be configured to apply to different evaluation measures and hence, can satisfy the relationship between agents (cooperative, competitive, mixed cooperative and competitive \u2013 as they can involve different zero-sum and general-sum \u2013 as they can involve different solution concepts and evaluation measures (given below).\\n\\n**Games.**\\n\\nWe construct all 15 games under two primary principles: i) these games involve as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g., it is impractical to execute the policy updating at each decision point at each iteration, running them on the existing benchmarks could be resource-intensive as the number of decision points in the games involves as many aspects of decision making as possible, e.g., the number of agents (single or multiple) and the environment could be extremely large (e.g"}
