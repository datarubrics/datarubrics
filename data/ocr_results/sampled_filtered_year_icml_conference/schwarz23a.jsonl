{"id": "schwarz23a", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Table 2. More qualitative examples from the UCF-101 datasets. Shown are full video reconstructions and residuals of various VC-INR models at varying bit-rates.\\n\\nFigure 13. t-SNE (Van der Maaten & Hinton, 2008) visualisation of gating masks after adaptation on CIFAR-10. (a) Full test set results (b) zoomed-in (c) Correlation between gating mask sparsity and performance level. Sparsity calculated as the fraction of sparse weights (i.e. $|G_{low} \\\\odot W_{ij}| < 0.001$) relative to the total number of all weights in the network.\\n\\nExample 1 here here here here\\nExample 2 here here here here\\nExample 3 here here here here\\nExample 4 here here here here\\nExample 5 here here here here\\nExample 6 here here here here\\nExample 7 here here here here\\nExample 8 here here here here\\nExample 9 here here here here\\nExample 10 here here here here\\nExample 11 here here here here\\nExample 12 here here here here\\nExample 13 here here here here\\nExample 14 here here here here\\nExample 15 here here here here\\nExample 16 here here here here\"}"}
{"id": "schwarz23a", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\n(a) Compared with COIN++ (Dupont et al., 2022b).\\n\\n(b) Compared with MSCN (Schwarz & Teh, 2022).\\n\\nFigure 14. More qualitative results from the Kodak dataset. Shown are VC-INR models in comparison with other INR-based techniques at similar bit-rates (3rd column) as well as a high-quality model (last column).\"}"}
{"id": "schwarz23a", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3. Hyperparameters for compression experiments on CIFAR-10.\\n\\n| Parameter Considered | Range | Comment |\\n|----------------------|-------|---------|\\n| INR training         |        |         |\\n| Patching             | {False} |         |\\n| Activation function  | {h(x): sin(\u03c9\u2080x)} (SIREN) |         |\\n| \u03c9\u2080                   | {30}   |         |\\n| Network depth        | {15}   |         |\\n| Network width        | {512}  |         |\\n| Batch size per device| {32, 64} |         |\\n| Num devices          | {8}    |         |\\n| Optimiser            | {Adam} |         |\\n| Outer learning rate  | {3 \u00b7 10\u207b\u2076} |         |\\n| Num inner steps      | {3}    |         |\\n| Meta-learn \u03d5 init.   | {True} |         |\\n| Meta SGD range       | [-5.0, 5.0] (Max./Min. for Meta-SGD LRs) |         |\\n| Meta SGD init range  | [1.0, 1.0] (Uniformly sampled). |         |\\n| \u03d5 \u2192 {G\u2081 low, ..., G\u1d38 low} network dim(\u03d5) | {2048, 3072, 4096} |         |\\n| Use LayerNorm        | {True} |         |\\n| Network width        | {6144} |         |\\n| Residual blocks      | {2}    |         |\\n| Activation function  | {Leaky Relu (Xu et al., 2015)} |         |\\n| Adapt first Layer    | {False} | Apply low-rank gating to 1st layer? |\\n| Quantiser training   | {True} |         |\\n| Normalise \u03d5          | {Per dim.} | \u03d5\u1d62 - \u02c6\u00b5\u1d62 \u02c6\u03c3\u1d62 based on \u03d5 train-set stats. |\\n| \u03bb(L distorm penalty) | {0.33, 0.66, 1.0, 3.33, 6.66} |         |\\n| Analysis transform (g\u2090) Residual blocks | {1} |         |\\n| g\u2090 Network width     | {2048, 4096, 5120} |         |\\n| g\u2090 Activation function | SeLU (Klambauer et al., 2017) |         |\\n| dim(y)               | {1024, 2048, 4096, 5120} |         |\\n| Synthesis transform g\u209b | Same as g\u2090 |         |\\n| Optimiser            | {Adam} |         |\\n| Learning rate        | {1 \u00b7 10\u207b\u2074} |         |\\n| Batch size per device| {32, 64} |         |\\n| Num devices          | {1} | 19         |\"}"}
{"id": "schwarz23a", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Parameter Considered | range | Comment |\\n|----------------------|-------|---------|\\n| INR training         |       | Pre-training on {Div2k} as in (Schwarz & Teh, 2022; Str\\\"umpler et al., 2022) |\\n| Patching             | {$(32 \\\\times 32)$} | Dividing 768 $\\\\times$ 512 images. |\\n| Activation function  | {h($x$) : sin($\\\\omega_0 x$)} (SIREN)} | |\\n| $\\\\omega_0$          | {30} | |\\n| Network depth        | {15} | |\\n| Network width        | {512} | |\\n| Batch size per device| {32} | |\\n| Num devices          | {8} | |\\n| Optimiser            | {Adam} | |\\n| Outer learning rate  | {3$\\\\cdot$10$^{-6}$} | |\\n| Num inner steps      | {3} | |\\n| Meta-learn $\\\\phi$ int. | {True} | |\\n| Meta SGD range       | {[-5.0, 5.0]} (Max./Min. for Meta-SGD LRs) | |\\n| Meta SGD init range  | {[1.0, 1.0]} (Uniformly sampled). | |\\n| $\\\\phi$ $\\\\rightarrow$ | {G(1)$^{low}$, . . . , G($L^{low}$)$^{low}$} network | |\\n| dim($\\\\phi$)          | {512, 1024} | |\\n| Use LayerNorm        | {True} | |\\n| Network width        | {4096} | |\\n| Residual blocks      | {1} | |\\n| Activation function  | {Leaky Relu} (Xu et al., 2015) | |\\n| Adapt first Layer    | {False} | |\\n| Quantiser training   | {True} | Normalise $\\\\phi$ per dim. $\\\\hat{\\\\mu}_i$ $\\\\hat{\\\\sigma}_i$ based on $\\\\phi^{train-set}$ stats. |\\n| $\\\\lambda$ ($L_{distortion}$ penalty) | {0.01, 0.033, 0.1, 0.33, 0.66, 1.0} | |\\n| Analysis transform ($g_a$) Residual blocks | {1} | |\\n| $g_a$ Network width  | {256, 512, 1024} | |\\n| $g_a$ Activation function | SeLU (Klambauer et al., 2017) | |\\n| dim($y$)             | {256, 512, 1024} | |\\n| Synthesis transform  | $g_s$ Same as $g_a$ | |\\n| Optimiser            | {Adam} | |\\n| Learning rate        | {1$\\\\cdot$10$^{-4}$} | |\\n| Batch size per device| {128} | |\\n| Num devices          | {1, 20} | |\"}"}
{"id": "schwarz23a", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Parameter Considered | range | Comment |\\n|----------------------|-------|---------|\\n| INR training         |       |         |\\n| Patching             | {False} |         |\\n| Activation function  | {\\\\( h(x) : \\\\sin(\\\\omega_0 x) \\\\)} (SIREN) |         |\\n| \\\\( \\\\omega_0 \\\\)       | {30}  |         |\\n| Network depth        | {10}  |         |\\n| Network width        | {384} |         |\\n| Batch size per device| {4}   |         |\\n| Num devices          | {4}   |         |\\n| Optimiser            | {Adam} |         |\\n| Outer learning rate  | {3 \\\\cdot 10^{-6}} |         |\\n| Num inner steps      | {3}   |         |\\n| Meta-learn \\\\( \\\\phi \\\\) init. | {True} |         |\\n| Meta SGD range       | {-5.0, 5.0} | (Max./Min. for Meta-SGD LRs) |\\n| Meta SGD init range  | [1.0, 1.0] | (Uniformly sampled). |\\n| \\\\( \\\\phi \\\\to \\\\{ G(1)_{\\\\text{low}}, \\\\ldots, G(L)_{\\\\text{low}} \\\\} \\\\) network dim(\\\\( \\\\phi \\\\)) | {4, 8, 12, 32, 64, 128} |         |\\n| Use LayerNorm        | {True} |         |\\n| Network width        | {512} |         |\\n| Residual blocks      | {2}   |         |\\n| Activation function  | {Leaky Relu} (Xu et al., 2015) |         |\\n| Adapt first Layer    | {False} | Apply low-rank gating to 1st layer? |\\n| Quantiser training   |       |         |\\n| Normalise \\\\( \\\\phi \\\\) | {True} | Per dim. \\\\( \\\\phi_i - \\\\hat{\\\\mu}_i \\\\hat{\\\\sigma}_i \\\\) based on \\\\( \\\\phi \\\\) train-set stats. |\\n| \\\\( \\\\lambda(\\\\text{Ldistortion penalty}) \\\\) | {0.001, 0.01, 0.1} |         |\\n| Analysis transform \\\\( g_a \\\\) Residual blocks | {2} |         |\\n| \\\\( g_a \\\\) Network width | {8, 12, 32, 64, 128} |         |\\n| \\\\( g_a \\\\) Activation function SeLU (Klambauer et al., 2017) |         |\\n| dim(\\\\( y \\\\))         | {8, 12, 32, 64, 128} |         |\\n| Synthesis transform \\\\( g_s \\\\) | Same as \\\\( g_a \\\\) |         |\\n| Optimiser            | {Adam} |         |\\n| Learning rate        | {1 \\\\cdot 10^{-4}} |         |\\n| Batch size per device| {128, 256} |         |\\n| Num devices          | {1}   |         |\"}"}
{"id": "schwarz23a", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 6. Hyperparameters for compression experiments on LibriSpeech.\\n\\n| Parameter Considered | Range | Comment |\\n|----------------------|-------|---------|\\n| INR training         | {200, 400, 800} | Dividing 48k dim. audio signal. |\\n| Activation function  | \\\\{h(x) \\\\colon \\\\sin(\\\\omega_0 x)\\\\} (SIREN) | \\\\(\\\\omega_0\\\\) \\\\{10, 30, 50\\\\} |\\n| Network depth        | {10}  |         |\\n| Network width        | {512} |         |\\n| Batch size per device| {32, 64} |         |\\n| Num devices          | {1}   |         |\\n| Optimiser            | {Adam} |         |\\n| Outer learning rate  | {3 \\\\cdot 10^{-6}} |         |\\n| Num inner steps      | {3}   |         |\\n| Meta-learn \\\\(\\\\phi\\\\) init. | True |         |\\n| Meta SGD range       | [-5.0, 5.0] | (Max./Min. for Meta-SGD LRs) |\\n| Meta SGD init range  | [1.0, 1.0] | (Uniformly sampled). |\\n| \\\\(\\\\phi\\\\rightarrow\\\\) {G(1)\\\\_low, \\\\ldots, G(L)\\\\_low} network | dim(\\\\(\\\\phi\\\\)) \\\\{64, 128, 256, 512, 1024\\\\} |\\n| Use LayerNorm        | True  |         |\\n| Network width        | {512, 512, 768, 1536, 3072} |         |\\n| Residual blocks      | {2}   |         |\\n| Activation function  | {Leaky Relu (Xu et al., 2015)} |         |\\n| Adapt first Layer    | False | Apply low-rank gating to 1st layer? |\\n| Quantiser training   | True  | Per dim. \\\\(\\\\phi_i - \\\\hat{\\\\mu}_i \\\\hat{\\\\sigma}_i\\\\) based on \\\\(\\\\phi\\\\text{train-set stats.}\\\\) |\\n| \\\\(\\\\lambda(\\\\text{distortion penalty})\\\\) | {1.0, 10.0, 100.0} |         |\\n| Analysis transform \\\\(g_a\\\\) Residual blocks | 2 |         |\\n| \\\\(g_a\\\\) Network width | {128, 256, 512, 1024} |         |\\n| \\\\(g_a\\\\) Activation function SeLU (Klambauer et al., 2017) |         |\\n| dim(\\\\(y\\\\)) | {128, 256, 512, 1024} |         |\\n| Synthesis transform \\\\(g_s\\\\) | Same as \\\\(g_a\\\\) |         |\\n| Optimiser            | {Adam} |         |\\n| Learning rate        | {1 \\\\cdot 10^{-4}} |         |\\n| Batch size per device| {128} |         |\\n| Num devices          | {1}   |         |\"}"}
{"id": "schwarz23a", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Parameter Considered | range | Comment |\\n|----------------------|-------|---------|\\n| INR training         |       |         |\\n| Patching             | (4, 8, 8), (8, 8, 8), (4, 16, 16), (8, 16, 16) | Dividing dim. video. |\\n| Activation function  | \\\\{h(x) : \\\\sin(\\\\omega_0 x) (SIREN)\\\\} | \\\\omega_0 \\\\{30\\\\} |\\n| Network depth        | {10}  |         |\\n| Network width        | {256} |         |\\n| Batch size per device| {4}   |         |\\n| Num devices          | {4}   |         |\\n| Optimiser            | {Adam} |         |\\n| Outer learning rate  | {3 \\\\cdot 10^{-6}} |         |\\n| Num inner steps      | {3}   |         |\\n| Meta-learn \\\\(\\\\phi\\\\) init. | \\\\{True\\\\} |         |\\n| Meta SGD range       | \\\\[-5.0, 5.0\\\\] | (Max./Min. for Meta-SGD LRs) |\\n| Meta SGD init range  | \\\\[1.0, 1.0\\\\] | (Uniformly sampled). |\\n| \\\\(\\\\phi\\\\) \\\\rightarrow \\\\{G(1)_{\\\\text{low}}, \\\\ldots, G(L)_{\\\\text{low}}\\\\} network | \\\\{512, 1536, 2048, 2048\\\\} | dim(\\\\(\\\\phi\\\\)) |\\n| Use LayerNorm        | \\\\{True\\\\} |         |\\n| Network width        | {512} |         |\\n| Residual blocks      | {2}   |         |\\n| Activation function  | \\\\{Leaky Relu (Xu et al., 2015)\\\\} |         |\\n| Adapt first Layer    | \\\\{False\\\\} | Apply low-rank gating to 1st layer? |\\n| Quantiser training   | \\\\{True\\\\} | Normalise \\\\(\\\\phi\\\\) per dim. \\\\(\\\\phi_i - \\\\hat{\\\\mu}_i \\\\hat{\\\\sigma}_i\\\\) based on \\\\(\\\\phi\\\\) train-set stats. |\\n| \\\\(\\\\lambda\\\\) (L dist. penalty) | \\\\{0.001, 0.01, 0.1, 1.0, 10.0\\\\} |         |\\n| Analysis transform \\\\((g_a)\\\\) Residual blocks | \\\\{1\\\\} |         |\\n| \\\\(g_a\\\\) Network width | \\\\{256, 512, 1024, 2048\\\\} |         |\\n| \\\\(g_a\\\\) Activation function | SeLU (Klambauer et al., 2017) |         |\\n| dim(\\\\(y\\\\))          | \\\\{256, 512, 1024, 2048\\\\} |         |\\n| Synthesis transform \\\\((g_s)\\\\) | Same as \\\\(g_a\\\\) |         |\\n| Optimiser            | {Adam} |         |\\n| Learning rate        | {1 \\\\cdot 10^{-4}} |         |\\n| Batch size per device| {64}  |         |\\n| Num devices          | {1}   |         |\"}"}
{"id": "schwarz23a", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\nJonathan Richard Schwarz\\n\\nJihoon Tack\\n\\nYee Whye Teh\\n\\nJaeho Lee\\n\\nJinwoo Shin\\n\\nAbstract\\n\\nWe introduce a modality-agnostic neural compression algorithm based on a functional view of data and parameterised as an Implicit Neural Representation (INR). Bridging the gap between latent coding and sparsity, we obtain compact latent representations non-linearly mapped to a soft gating mechanism. This allows the specialisation of a shared INR network to each data item through subnetwork selection. After obtaining a dataset of such latent representations, we directly optimise the rate/distortion trade-off in a modality-agnostic space using neural compression. Variational Compression of Implicit Neural Representations (VC-INR) shows improved performance given the same representational capacity pre quantisation while also outperforming previous quantisation schemes used for other INR techniques.\\n\\nOur experiments demonstrate strong results over a large set of diverse modalities using the same algorithm without any modality-specific induc tive biases. We show results on images, climate data, 3D shapes and scenes as well as audio and video, introducing VC-INR as the first INR-based method to outperform codecs as well-known and diverse as JPEG 2000, MP3 and AV1/HEVC on their respective modalities.\\n\\n1. Introduction\\n\\nData compression has become a critical problem in the modern era, as vast amounts of data is added to and transmitted through computer networks (Clissa, 2022) at previously unimaginable rates. While momentous progress has been made compared to naive representations, custom compression techniques are still developed for each modality at hand, carefully introducing inductive biases into new algorithms. While being an undoubtedly successful approach, it has limited the transfer of algorithmic ideas between techniques designed for different forms of data. More importantly, in certain engineering or scientific problems, vast amounts of data may be collected for which no generally accepted compression technique may be available (e.g. the AR/VR domain (Yang et al., 2022), point clouds, remote sensing or climate data), inhibiting progress in such fields.\\n\\nIn this paper, we join a recent group of researchers (e.g. Dupont et al., 2021; 2022b; Schwarz & Teh, 2022) in arguing for a paradigm shift: Making modality-agnosticism a key guiding principle, we advocate for a single algorithmic workbench on which methods applicable to any type of data represented by a coordinate and feature space are developed. This would allow research effort to be pooled and any jointly developed model or learning improvement to benefit multiple downstream compression applications at once.\\n\\nA promising approach towards realising this idea is the use of Implicit Neural Representations (INRs) or Neural Fields (e.g. Tancik et al., 2020; Sitzmann et al., 2020). An INR relies on a functional interpretation of data, specifically as a mapping from coordinates to features (e.g. \\\\((x, y) \\\\rightarrow (r, g, b)\\\\) for images), which is parameterised a neural network. INRs offer various attractive properties, including upsampling to arbitrary resolution (Chen et al., 2021) or a pathway to new approaches to applications such as generative modeling or classification (Dupont et al., 2022a). For our purpose, the most intriguing property of the INR approach is its inherent modality-agnosticism, as any data point can in theory be represented provided it is expressed as a coordinate to feature mapping and thus learnable. Consequently, a learned INR is simply an encoding of the data point within the weights of a neural network, the efficient storage of which has received much attention at a time of ever increasing model capacity. We can thus state the second guiding principle of the work at hand: Data-as-model-compression.\\n\\nThis second principle distinguishes our ideas from much of the existing work on Neural Compression (e.g. Ball\u00e9 et al., 2017; 2018; Cheng et al., 2020b), which directly encodes a given data point into a codespace, hence relying on carefully designed modality-specific encoding and decoding networks.\"}"}
{"id": "schwarz23a", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations (often called analysis/synthesis transforms). Throughout the manuscript, we will highlight how we overcome this limitation while building on rather than replacing the work from this community.\\n\\nAmong the recent work on compression with INRs on the other hand, various ideas for the efficient storage of INRs have been explored. So far, proposed compression and quantisation algorithms are relatively simple (e.g. Uniform Quantisation) or rely on a separate per-signal optimisation process (Str\u00a8umpler et al., 2022), hence significantly increasing runtime. In addition, much of this work relies on ideas borrowed from Meta-Learning (Finn et al., 2017) to decrease encoding times, which opens up various questions about the best trade-off between compact parameterisation and (Meta-) Learning algorithms. Therefore, despite significant efforts, a substantial gap still exists between INR-based compression and the hand-designed compression methods for certain modalities (e.g. JPEG 2000 for images, MP3 for audio).\\n\\nIn this paper, we improve INR-based compression in a two-fold approach (i) We experiment with advanced conditioning techniques resulting in better signal-reconstruction pre-quantisation (ii) We overcome limitations of previously used quantisation techniques and introduce a learned quantiser, allowing us to maintain significantly higher reconstruction quality at lower file sizes post-quantisation. Both directions of investigation adhere to the guiding principles of modality-agnosticism and the view of data as model compression. This presentation is not accidental, as we can think of the two axes of investigation as orthogonal algorithmic considerations. Indeed, any improvement in (i) increases the upper bound of performance maintained in the quantisation and entropy coding steps in (ii), while any improvement in (ii) reduces the gap between upper bound and actually realised performance.\\n\\nContributions:\\n\\n\u2022 Improved conditioning: We propose a middle ground between recent sparsity and latent coding approaches to compact representations. The proposed technique introduces a non-linear mapping from a latent codes to a low-rank soft gating matrix per layer, selecting a sub-network to represent a data item in an underlying INR. This is shown to learn more efficiently and result in better reconstructions compared to previous approaches. Our interpretation and experimental analysis shines new lights onto related ideas explored in other contexts.\\n\\n\u2022 Improved compression: We introduce a learned compressor pre-trained on compact latent codes representing training data. As such latent codes may be extracted from any modality, our proposed compressor operates fully modality-agnostic while making use of the same algorithmic insights previously only applicable to specific modalities.\\n\\nWe verify VC-INR on various data modalities, including image, voxels, scene, climate, audio, and video datasets. Overall, our experimental results demonstrate strong results, consistently outperforming previous INR-based compression methods and improving on popular compression schemes such as MP3 on audio and A VC/HEVC on video clips. In particular, VC-INR achieves a new state-of-the-art results on modality-agnostic compression with INRs, improving the Peak Signal to Noise Ratio (PSNR) on the same bits-per-pixel (bpp) bit rate by 3.3 dB for CIFAR-10 (Krizhevsky et al., 2009), by 2 dB on Kodak (Images), 3.5 dB for ERA5 (climate data) (Hersbach et al., 2019) and 9.5 dB for Librispeech (audio) (Panayotov et al., 2015) respectively. In addition, we outperform MP3 on Librispeech by 5.6 dB and HEVC on Videos by 8.8 dB.\\n\\nThroughout this paper, we express a given data point $x$ as a set of coordinates $c \\\\in C$ and real-valued features $y \\\\in Y$ and its corresponding INR representation as $\\\\phi \\\\in \\\\mathbb{R}^D$. Whenever appropriate, we distinguish between $N$ data points using superscripts, i.e., $\\\\{(x_i, \\\\phi_i)\\\\}_{i=1}^N$ and individual coordinate/feature pairs using subscripts, i.e. $x_i := \\\\{(c_j, y_j)\\\\}_{j=1}^M$.  \\n\\n2. Related Work\\n\\nINRs are neural networks approximating the functional mapping from coordinate to feature space. INRs are effective methods for modeling complex continuous signals, such as 2D images (Chen et al., 2021), 3D scenes (Park et al., 2019), videos (Kim et al., 2022), and are even applicable for modeling discrete data, e.g. graphs (Grattarola & Vandergheynst, 2022). To this end, several architectures have been proposed to capture high-frequency signal details, examples being sinusoidal activations (Sitzmann et al., 2020), positional encodings (Mildenhall et al., 2020), and Fourier features (Tancik et al., 2020).\\n\\nNeural compression is an end-to-end autoencoder-based lossy compression framework aiming to directly minimise the inherent rate/distortion trade-off. This is based on a transform-coding approach (Goyal, 2001) shown in Figure 1a, where a data item $x$ is transformed into a latent code $z$ through an analysis transform $g_a$. During training, quantisation is simulated through uniform noise ($\\\\mathbb{U}$) resulting in a noisy $e_z$ and a corresponding reconstruction $e_x = g_s(e_z)$ through the synthesis transform $g_s$. At test time, $z$ is quantised (and entropy coded), resulting in codes and reconstructions $\\\\hat{z}, \\\\hat{x}$ respectively. Taking $g_a, g_s$ to be deep neural networks, the neural compression paradigm was introduced in (Ball\u00b4e et al., 2017; Theis et al., 2017), who make theo-\"}"}
{"id": "schwarz23a", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\nFigure 1. Operational diagrams of learned compression models. Inference time paths are shown in blue. (a) Conventional neural compression (e.g. Ball\u00e9 et al., 2018) (b) Modality-agnostic neural compression with INRs (e.g. Dupont et al., 2022b; Schwarz & Teh, 2022) (c) Modality-agnostic variational compression of INRs is built upon the strengths of both techniques.\\n\\nData compression with INRs (Figure 1b), introduced by (Dupont et al., 2021) as a modality agnostic compression method required long optimisation processes and architecture search to find a suitable rate/distortion trade-off. Following the wider INR literature (e.g. Tancik et al., 2021), tabula-rasa learning was quickly replaced by a significantly faster Meta-Learning (Finn et al., 2017) adaptation loop (shown as $O$ in the diagram) while architecture search has been abandoned in favour of compact, instance specific representations $\\\\phi$ on which a deeper, shared INR $f$ is conditioned. The two mainstream approaches have been sparse representations (Lee et al., 2021; Schwarz & Teh, 2022) implementing a close surrogate for the rate loss and/or FiLM-style modulations (Perez et al., 2018; Chan et al., 2021; Mehta et al., 2021) optionally linearly predicted from a compact latent code (Dupont et al., 2022a;b).\\n\\nDiffering from the conventional neural compression workflow, methods following this paradigm either do not feature an explicit quantisation step (Dupont et al., 2021; Lee et al., 2021) beyond default casting to 16-bit representation or rely on simple uniform quantisation based on first and second moment training statistics (Dupont et al., 2022b; Schwarz & Teh, 2022). Recently, Gordon et al. (2023) introduce an alternative quantisation scheme based on K-means clustering, avoiding the likely sub-optimal division of the quantisation space into equally sized regions. Crucially however, subsequent quantisation is not accounted for during training of the previous approaches, forgoing optimisation for deviations in the representations $\\\\hat{\\\\phi}$. This is highlighted by a separate path at inference time in Figure 1b. While advanced quantisation has been introduced (Str\u00f6mpl et al., 2022), this requires additional training stages, thus increasing encoding runtime. Damodaran et al. (2023) is also similar to one aspect of this work by focusing on improving compression of INRs, showing strong improvements over COIN++ albeit only evaluating the method on images. In terms of applications of compression with INRs, Huang & Hoefler (2022) show the large potential gains in climate applications. Damodaran et al. (2023)\\n\\n3. Variational Compression of INRs\\n\\n3.1. Overview\\n\\nIn contrast to the two approaches discussed in the previous section, we now present a computational framework which maintains modality-agnosticism while allowing the use of deep entropy coding. We show a high-level overview in Figure 1c: The method can be best understood as an application of the non-linear transform coding paradigm (Figure 1a) in the compact representation space of the INR approach (Figure 1b).\\n\\nMore concretely, as in other INR techniques, we transform a\"}"}
{"id": "schwarz23a", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\nWe can improve on the relatively simple quantisation techniques in prior works by employing non-linear transforming coding in $\\\\phi$ space, with its analysis and synthesis transforms $g_a$, $g_s$ now operating on a modality-agnostic representation. This conceptionally simple change has the prime advantage of allowing the use of work from the neural compression literature with minimal changes (limited to simplification of $g_a$ and $g_s$), thus elevating conventional neural compression to a modality-agnostic paradigm. Compared to prior INR based compression, this allows the direction optimisation of the rate-distortion trade-off (as opposed to using a surrogate) using a deep entropy model. Moreover, a simple forward pass through $g_a$, subsequent quantisation $Q$ and then $g_s$ is preferable to an iterative technique such as quantisation aware training (Str\u00a8umpler et al., 2022) at inference time due to runtime considerations.\\n\\nThe rest of this section is split into the two axes of algorithmic improvements presented in this work: After giving a brief description of practical INR-learning on large datasets (Section 3.2), we then (i) Present an improved conditioning technique for specialising the shared base INR $f$ on the data-item specific representation $\\\\phi$ (Section 3.3) (ii) give a detailed discussion of the non-linear transform coding approach use (Section 3.4).\\n\\n3.2. INRs with Instance-Specific Modulations\\n\\nAn INR is a function $f(\\\\cdot; \\\\theta) : C \\\\rightarrow Y$ representing a data point through a network with parameters $\\\\theta$. The INR objective is the mean-squared-error of predictions on the data point's coordinates $\\\\{c_j\\\\}$ and the true features $\\\\{y_j\\\\}$:\\n\\n$$\\\\min_{\\\\theta} \\\\frac{1}{M} \\\\sum_{j=1}^{M} f(c_j; \\\\theta) - y_j^2$$\\n\\nIn practice, naive optimisation would require a large number of iterative steps and result in a set of high-dimensional parameter vectors $\\\\{\\\\theta_i\\\\}$ each representing a data point $x_i$, making this an unattractive choice. It is thus attractive to introduce a low dimensional data-item specific parameter $\\\\phi_i$ to model variations in $f$, while the much larger $\\\\theta$ is used to capture structure across a dataset. The shared INR $f(\\\\cdot; \\\\theta)$ is specialised to $x_i$ through $\\\\phi_i$ resulting in $f(\\\\cdot; \\\\theta, \\\\phi_i)$. A reduction in the number of iterative steps per data item is achieved through Meta-Learning (Finn et al., 2017), allowing $\\\\phi^*$ for a test data point $x^*$ to be obtained in a handful of optimisation steps (see Appendix for details on Meta-Learning).\\n\\nCommon ways to condition $f$ on $\\\\phi_i$ are layer-specific modulations $s(l)$ obtained by indexing into $\\\\phi_i$, i.e. $\\\\phi_i = [s(1), \\\\ldots, s(L)]$ (Mehta et al., 2021). These modulations take the form of FiLM-style (Perez et al., 2018) shifts, i.e. $c(l-1) \\\\rightarrow h(W(l)c(l-1) + b(l) + s(l))$, where $W(l)$, $b(l)$ are shared weights and biases and $h$ is the activation function.\\n\\nTo further reduce the size of $\\\\phi_i$, modulations of an $L$-layer INR $s := [s(1), \\\\ldots, s(L)]$ can be predicted from $\\\\phi_i$ using a shared linear mapping as $s = W' \\\\phi + b'$ (Dupont et al., 2022a) or alternatively by pruning dimensions in $\\\\phi_i$ through sparsity (Schwarz & Teh, 2022). Both techniques have their own drawbacks: Predictions of $s$ from $\\\\phi$ have been challenging to train and so far been limited to linear mappings, thus lacking representational capacity. Sparsity techniques on the other hand require approximate inference, introducing additional complexity and various new hyperparameters.\\n\\n3.3. INR Specialisation through Subnetwork Selection\\n\\nInstead, we take inspiration from both perspectives while overcoming their respective limitations. Following the sparsity paradigm, we observe that while a single network may be conditioned on potentially hundreds of distinct tasks through subnetwork selection (Frankle & Carbin, 2018; Schwarz et al., 2021), it is unclear whether this must be done through hard gating (i.e. requiring exact zeros and ones) and thus require approximate inference. Indeed, recent work (He et al., 2019) suggests that soft-gating in the form of the output of a sigmoid $\\\\sigma(x) = 1/(1+e^{-x})$ may be sufficient. In addition, it is clear that the idea of parametric predictions from $\\\\phi$ may in principle be used in conjunction with subnetwork selection, as a compact $\\\\phi$ could then concentrate its capacity on the non-sparse entries of $s$, hence naturally combining both ideas.\\n\\nTo this end, we thus suggest the use of a non-linear prediction network mapping $\\\\phi$ to low-rank soft gating masks taking the same shape as the weight-matrices of each layer (see Figure 2a). The functional form of the soft-gating masks is inspired by (Skorokhodov et al., 2021) and takes the form of a low-rank matrix obtained through the outer product of two vectors non-linearly predicted from $\\\\phi$. This choice is sensible for two reasons: First, low-rank parameterisation is widely used as an effective tool for parameter reductions (Phan et al., 2020) and secondly, such modulation have shown potential in representing complex signals such as high-resolution images (Skorokhodov et al., 2021) and videos (Yu et al., 2022). Formally, given the activations of the preceding layer $c(l-1)$, the transformation of each layer $l$ is $c(l-1) \\\\rightarrow \\\\sin(\\\\omega_0(G(l)_{\\\\text{low}} \\\\circ W(l)c(l-1) + b(l)))$\\n\\n$G(l)_{\\\\text{low}} := \\\\sigma(U(l)V(l)^\\\\top)$\\n\\nwhere $U(l)$, $V(l) \\\\in \\\\mathbb{R}^{m \\\\times d}$ are data specific parameters with $d \\\\ll m$, $\\\\circ$ is element-wise multiplication and $\\\\sigma(\\\\cdot)$ the sigmoid operator. Here, we use sinusoidal activation function with its hyperparameters $\\\\omega_0 \\\\in \\\\mathbb{R}^{+}$ introduced for INRs in (Sitzmann et al., 2020). The central hypothesis 4\"}"}
{"id": "schwarz23a", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"## Figure 8.\\n\\nCompression results on (a) videos and (b) audio.\\n\\n### (a) UCF-101\\n\\n| Bit-rate [bpp] | PSNR [dB] |\\n|---------------|-----------|\\n| 20            | 20.5      |\\n| 25            | 21.2      |\\n| 30            | 21.8      |\\n| 35            | 22.3      |\\n| 40            | 22.8      |\\n| 45            | 23.3      |\\n| 50            | 23.8      |\\n| 55            | 24.3      |\\n| 60            | 24.8      |\\n\\nVC-INR (ours) | H.264/AVC (s) | H.265/HEVC (s)\\n\\n### (b) LibriSpeech\\n\\n| Bit-rate [kbps] | PSNR [dB] |\\n|----------------|-----------|\\n| 40             | 9.5       |\\n| 45             | 9.2       |\\n| 50             | 9.0       |\\n| 55             | 8.8       |\\n| 60             | 8.6       |\\n\\nVC-INR (ours) | COIN++ | MP3 (s)\\n\\n## Acknowledgements\\n\\nWe thank Hyunjik Kim and Danilo J. Rezende for their insightful comments and feedback. We also thank Strumpel et al. (2022) for sharing their results. This work is supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST), No.2021-0-02068, Artificial Intelligence Innovation Hub, and No.2022-0-00713, Meta-learning applicable to real-world problems) and National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (2022R1F1A1075067).\\n\\n## References\\n\\nAgustsson, E. and Timofte, R. Ntire 2017 challenge on single image super-resolution: Dataset and study. In IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2017.\\n\\nAgustsson, E., Tschannen, M., Mentzer, F., Timofte, R., and Gool, L. V. Generative adversarial networks for extreme learned image compression. In IEEE International Conference on Computer Vision, 2019.\\n\\nAgustsson, E., Minnen, D., Johnston, N., Balle, J., Hwang, S. J., and Toderici, G. Scale-space flow for end-to-end optimized video compression. In CVPR, 2020.\\n\\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n\\nBall\u00e9, J., Laparra, V., and Simoncelli, E. P. Density modeling of images using a generalized normalization transformation. arXiv preprint arXiv:1511.06281, 2015.\\n\\nBall\u00e9, J., Laparra, V., and Simoncelli, E. P. End-to-end optimized image compression. In International Conference on Learning Representations, 2017.\\n\\nBall\u00e9, J., Minnen, D., Singh, S., Hwang, S. J., and Johnston, N. Variational image compression with a scale hyperprior. In International Conference on Learning Representations, 2018.\\n\\nBellard, F. Bpg image format. https://bellard.org/bpg/, 2014.\\n\\nBross, B., Wang, Y.-K., Ye, Y., Liu, S., Chen, J., Sullivan, G. J., and Ohm, J.-R. Overview of the versatile video coding (vvc) standard and its applications. IEEE Transactions on Circuits and Systems for Video Technology, 2021.\"}"}
{"id": "schwarz23a", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "schwarz23a", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\nLi, Z., Zhou, F., Chen, F., and Li, H. Meta-sgd: Learning to learn quickly for few-shot learning. arXiv preprint arXiv:1707.09835, 2017.\\n\\nLu, G., Ouyang, W., Xu, D., Zhang, X., Cai, C., and Gao, Z. Dvc: An end-to-end deep video compression framework. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.\\n\\nMaaten, L. v. d. and Hinton, G. Visualizing data using t-sne. Journal of Machine Learning Research, 2008.\\n\\nMehta, I., Gharbi, M., Barnes, C., Shechtman, E., Ramamoorthi, R., and Chandraker, M. Modulated periodic activations for generalizable local functional representations. In IEEE Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nMildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. NeRF: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, 2020.\\n\\nMinnen, D., Ball\u00e9, J., and Toderici, G. Joint autoregressive and hierarchical priors for learned image compression. In Advances in Neural Information Processing Systems, 2018a.\\n\\nMinnen, D., Ball\u00e9, J., and Toderici, G. D. Joint autoregressive and hierarchical priors for learned image compression. In Advances in Neural Information Processing Systems, 2018b.\\n\\nMP3. MP3 codec. https://www.iso.org/standard/22412.html, 1993.\\n\\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: an asr corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2015.\\n\\nPark, J. J., Florence, P., Straub, J., Newcombe, R., and Lovegrove, S. Deepsdf: Learning continuous signed distance functions for shape representation. In IEEE Conference on Computer Vision and Pattern Recognition, 2019.\\n\\nPerez, E., Strub, F., De Vries, H., Dumoulin, V., and Courville, A. Film: Visual reasoning with a general conditioning layer. In AAAI Conference on Artificial Intelligence, 2018.\\n\\nPhan, A.-H., Sobolev, K., Sozykin, K., Ermilov, D., Gusak, J., Tichavsk\u00fd, P., Glukhov, V., Oseledets, I., and Cichocki, A. Stable low-rank tensor decomposition for compression of convolutional neural network. In European Conference on Computer Vision, 2020.\\n\\nSalomon, D. Data compression: the complete reference. Springer Science & Business Media, 2004.\\n\\nSanturkar, S., Tsipras, D., Ilyas, A., and Madry, A. How does batch normalization help optimization? In Advances in Neural Information Processing Systems, 2018.\\n\\nSchwarz, J., Jayakumar, S., Pascanu, R., Latham, P. E., and Teh, Y. Powerpropagation: A sparsity inducing weight reparameterisation. Advances in Neural Information Processing Systems, 34:28889\u201328903, 2021.\\n\\nSchwarz, J. R. and Teh, Y. W. Meta-learning sparse compression networks. Transactions on Machine Learning Research, 2022.\\n\\nSitzmann, V., Zollh\u00f6fer, M., and Wetzstein, G. Scene representation networks: Continuous 3d-structure-aware neural scene representations. In Advances in Neural Information Processing Systems, 2019.\\n\\nSitzmann, V., Martel, J. N. P., Bergman, A. W., Lindell, D. B., and Wetzstein, G. Implicit neural representations with periodic activation functions. In Advances in Neural Information Processing Systems, 2020.\\n\\nSkodras, A., Christopoulos, C., and Ebrahimi, T. The jpeg 2000 still image compression standard. IEEE Signal Processing Magazine, 2001.\\n\\nSkorokhodov, I., Ignatyev, S., and Elhoseiny, M. Adversarial generation of continuous images. In IEEE Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nSoomro, K., Zamir, A. R., and Shah, M. UCF101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\\n\\nStr\u00fcmppler, Y., Postels, J., Yang, R., Van Gool, L., and Tombari, F. Implicit neural representations for image compression. In European Conference on Computer Vision, 2022.\\n\\nTancik, M., Srinivasan, P. P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J. T., and Ng, R. Fourier features let networks learn high frequency functions in low dimensional domains. In Advances in Neural Information Processing Systems, 2020.\\n\\nTancik, M., Mildenhall, B., Wang, T., Schmidt, D., Srinivasan, P. P., Barron, J. T., and Ng, R. Learned initializations for optimizing coordinate-based neural representations. In IEEE Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nTheis, L., Shi, W., Cunningham, A., and Husz\u00e1r, F. Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017.\"}"}
{"id": "schwarz23a", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\nTheis, L., Salimans, T., Hoffman, M. D., and Mentzer, F. Lossy compression with gaussian diffusion. arXiv preprint arXiv:2206.08889, 2022.\\n\\nVan der Maaten, L. and Hinton, G. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\\n\\nWallace, G. K. The jpeg still picture compression standard. IEEE Transactions on Consumer Electronics, 1992.\\n\\nWang, Z., Simoncelli, E. P., and Bovik, A. C. Multiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, volume 2, pp. 1398\u20131402. Ieee, 2003.\\n\\nWiegand, T., Sullivan, G. J., Bjontegaard, G., and Luthra, A. Overview of the h.264/avc video coding standard. IEEE Transactions on circuits and systems for video technology, 2003.\\n\\nXu, B., Wang, N., Chen, T., and Li, M. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853, 2015.\\n\\nXu, J., Sun, X., Zhang, Z., Zhao, G., and Lin, J. Understanding and improving layer normalization. In Advances in Neural Information Processing Systems, 2019.\\n\\nYang, Y., Mandt, S., and Theis, L. An introduction to neural data compression. arXiv preprint arXiv:2202.06533, 2022.\\n\\nYu, S., Tack, J., Mo, S., Kim, H., Kim, J., Ha, J.-W., and Shin, J. Generating videos with dynamics-aware implicit generative adversarial networks. In International Conference on Learning Representations, 2022.\\n\\nZintgraf, L., Shiarli, K., Kurin, V., Hofmann, K., and White-son, S. Fast context adaptation via meta-learning. In International Conference on Machine Learning, 2019.\"}"}
{"id": "schwarz23a", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\n(a) Non-linear projection from latent representation $\\\\phi$ to $G(l)_{low}$.\\n\\nBit stream\\nShared INR\\n\\n(b) Non-linear transform coding in latent representation space.\\n\\nFigure 2. Architectural details of full model. AE/AD: Arithmetic Encoding/Decoding\\n\\nAs before, we can reduce the dimensions of low-rank modulation further, obtaining $[U^{(1)}, V^{(1)}, \\\\ldots, U^{(N)}, V^{(N)}]$ directly from the compact representation $\\\\phi$ by predicting a long vector, subsequently reshaped into the respective matrices. Unlike existing methods utilising a linear mapping (Dupont et al., 2022a;b), we use deep residual networks to increase the expressive power, enabled through various stabilisation techniques:\\n\\n- Stabilisation Techniques\\n  - In line with prior work, we find the direct optimisation of non-linear networks via Meta-Learning to be unstable and under-performing. As low-rank parameterisations are also known to suffer from stability issues, the direct use yield unsatisfactory results. Instead, we suggest three stabilising techniques. (1) First, we propose the normalisation of the modulation $\\\\phi$ with LayerNorm (Ba et al., 2016), i.e., $\\\\phi \\\\rightarrow \\\\text{LayerNorm}(\\\\phi)$ as in Fig 2a. Intuitively, this results in higher order gradient optimisation becoming more stable as a normalisation scheme reduces the sharpness of the gradients (Santurkar et al., 2018; Xu et al., 2019). (2) We find residual connections and increasing layer widths (up to computational limits) to aid gradient propagation and significantly increase the performance of non-linear networks. (3) We hypothesise that the sigmoidal bounding of $G(l)_{low}$ itself has a stabilising effect, preventing the matrix norm from divergence.\\n\\nAt this point it is worth noting that the combination of subnetwork selection techniques and non-linear predictors are not unique to compression and indeed may be beneficial in the wide array of downstream applications made possible through the INR paradigm (Dupont et al., 2022b). Next, we explain the subsequent quantisation of $\\\\phi$.\\n\\n3.4. Variational Compression of Latent Modulations\\n\\nThe key to using non-linear transform coding in a modality-agnostic paradigm is the observation that $\\\\phi$ may be obtained from data of any kind. For a given modulation $\\\\phi$, our goal is now to encode the modulation into a code $z = g_a(\\\\phi)$ with low Shannon cross-entropy (its rate) and a reconstruction $\\\\hat{\\\\phi} = g_s(\\\\hat{z})$ with low distortion from $\\\\phi$ after quantisation $\\\\hat{z} = Q(z) = \\\\text{round}(z)$. Because $\\\\hat{z}$ is discrete, it can be losslessly compressed using entropy coding such as arithmetic or Huffman coding (Salomon, 2004) to obtain a bit stream. Here, we use the deep-factorised prior introduced for images in (Ball\u00e9 et al., 2017) and used as the basis of many follow-up works. The authors establish the interpretation of a relaxed rate-distortion performance as variational autoencoder under a specific generative and inference model, lending the name VC-INR to our method.\\n\\nWe state the compression loss as the sum of (i) the rate of the code and (ii) the distortion of the recovered signal:\\n\\n$$L_{compress}(\\\\pi_a, \\\\pi_s, x, \\\\phi) = L_{rate} + \\\\lambda L_{distortion} = -\\\\log_2[p(\\\\hat{z}|Q(g_a(\\\\phi; \\\\pi_a)))] + \\\\lambda L_{MSE}(g_s(\\\\hat{z}; \\\\pi_s), \\\\phi)$$\\n\\nwith $p(\\\\hat{z}$ the entropy model, $L_{MSE}$ the mean squared error (MSE), and $\\\\pi_a, \\\\pi_s$ parameters of the analysis and synthesis transforms. The reconstruction $\\\\hat{\\\\phi}$ is decoded from the quantised code $\\\\hat{z}$. To optimise this loss, we follow (Ball\u00e9 et al., 2017) by approximating the discrete quantisation with uniform noise $U(-\\\\frac{1}{2}, \\\\frac{1}{2})$ to generate a noisy code $\\\\tilde{z}$ and use the differentiable prior $p(\\\\tilde{z}$ with a non-parametric piecewise linear density model (Ball\u00e9 et al., 2018).\\n\\nWe show architectural details in Figure 2b: Differing from the typical design of $g_a, g_s$, we do not make use of activations with local gain control and find the SeLU activation (Klambauer et al., 2017) sufficient. In addition, as the vectors $\\\\phi$ are flat regardless of modality, we can simplify the design of both networks to residual MLPs, removing an-\"}"}
{"id": "schwarz23a", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\nFinally, we note that the distortion term in Equation (4) is merely a surrogate for the real reconstruction quality of the data \\\\(x\\\\). We thus modify \\\\(L_{\\\\text{distortion}}\\\\) to measure distortion on data directly:\\n\\n\\\\[\\nL_{\\\\text{distortion}} = L_{\\\\text{MSE}}(f(\\\\cdot; \\\\theta, gs(\\\\hat{z}; \\\\pi_s)), y)\\n\\\\]\\n\\nwhich we observe to result in the highest quality reconstructions. At this point we emphasise that advanced techniques (Ball\u00e9 et al., 2018; Cheng et al., 2020a) may be straightforwardly introduced.\\n\\n4. Experiments\\n\\nSo far, we have discussed a two-fold approach: (i) Advanced conditioning to better capture an underlying signal within a fixed representation pre-quantisation. (ii) Variational compression subsequently trained on datasets of such representations. In our empirical evaluation, we will first demonstrate the effectiveness of (i) in isolation (as its results is an upper bound for distortion performance). We then demonstrate the combination of both ideas on a range of compression problems. This will help clearly delineate performance gains as well as provide additional insights into the technique. Throughout the section, we primarily evaluate the performance using the Peak Signal to Noise Ratio (PSNR):\\n\\n\\\\[\\n-10 \\\\log_{10}(\\\\text{MSE}), \\\\text{where MSE is the mean-squared error between the original and the reconstructed signal.}\\n\\\\]\\n\\n4.1. Effectiveness of Advanced Conditioning\\n\\nTable 1. Results for various latent modulation sizes. Shown is voxel accuracy (ShapeNet10) and PSNR (others).\\n\\n| Dataset       | Model | Performance @ dim(\\\\(\\\\phi\\\\)) |\\n|---------------|-------|-----------------------------|\\n| ERA5 (4\u00d7)    | Functa | 43.2 43.7 43.8 44.0 44.1  |\\n|               | MSCN  | 44.6 45.7 46.0 46.6 46.9  |\\n|               | VC-INR | 45.0 46.2 47.6 49.0 50.0   |\\n| CelebA-HQ    | Functa | 21.6 23.5 25.6 28.0 30.7   |\\n|               | MSCN  | 21.8 23.8 25.7 28.1 30.9   |\\n|               | VC-INR | 22.0 23.9 26.0 28.3 30.8   |\\n| SRN Cars     | Functa | 22.4 23.0 23.1 23.2 23.1   |\\n|               | MSCN  | 22.8 24.0 24.3 24.5 24.8   |\\n|               | VC-INR | 23.9 24.0 24.3 25.2 25.5   |\\n| ShapeNet10   | Functa | 99.30 99.40 99.44 99.50 99.55 |\\n|               | MSCN  | 99.43 99.50 99.56 99.63 99.69 |\\n|               | VC-INR | 99.54 99.61 99.64 99.70 99.71 |\\n\\nWe first evaluate our method pre-quantisation on various modalities including images using CelebA-HQ dataset (Karpathy et al., 2018), manifolds using ERA5 (Hersbach et al., 2019), 3d NeRF scenes using the SRN cars (Sitzmann et al., 2019) and 3d voxels using the top 10 classes of ShapeNet (Chang et al., 2015). Following prior work, we train SIREN with 15 layers of 512 units and use MetaSGD (Li et al., 2017) with 3 inner-loop steps as our Meta-Learning method and use the same task batch size for comparable conditions.\\n\\nFor baselines, we compare our technique with latent modulations using Functa (Dupont et al., 2022a) and sparse modulations using MSCN (Schwarz & Teh, 2022). More details in the Appendix.\\n\\nAs illustrated in Table 1, we demonstrates a marked improvement over previous approaches in almost all cases. Particularly noteworthy, VC-INR outperforms MSCN on ERA5 by more than 3.1dB when using a modulation size of 1024. This is particularly significant, as PSNR is based on a logarithmic scale. In addition, we note that the use of more complex latent modulations/mask networks (as opposed to the linear projection of Functa) not only leads to better results, but also exhibits significantly faster learning progress (Figure 3a).\\n\\nA key hypothesis of our proposed conditioning technique is the idea of subnetwork selection. To provide empirical evidence and understand the behaviour of our conditioning method, we analyse the masks \\\\(G_{low}\\\\) after obtaining \\\\(\\\\phi_i\\\\) for test set images. This is shown in Figure 3. First, we note that the product of gating masks and a shared, Meta-Learning matrix does indeed implement moderate sparsity levels (which we define as \\\\(|(G_{low} \\\\odot W)_{ij}| < 0.001\\\\)). Despite avoiding the use of approximate inference (Figure 3b), Remarkably, we observe sparsity levels varying significantly per layer, suggesting VC-INR learns where to learn. This is particularly significant as it is well known that only a fraction of layers typically need to adapted in Meta-Learning (Zintgraf et al., 2019). Indeed, this was a key insight of MSCN (Schwarz & Teh, 2022) which we share despite our use of a much simpler sparsity/gating method.\\n\\nMoreover, further examining our soft gating mechanism, we provide a t-SNE visualization (Maaten & Hinton, 2008) of the adapted masks on CelebA-HQ (Figure 3c). Resulting patterns intriguingly show clear clustering according to image characteristics such as background color, indicating the ability to condition the shared INR based on image statistics.\\n\\n4.2. Data Compression Across Modalities\\n\\nWe now evaluate VC-INR for data compression, the primary focus of our work. To demonstrate the versatility of VC-INR, we examine a range frequently encountered modalities. We measure reconstruction performance measured in terms of PSNR under different levels of compressed data sizes measured in kilobits per second (kbps) for audio and bits-per-pixel (bpp)\u00b2. Baselines are codecs such as JPEG (Wallace, 1992), JPEG 2000 (Skodras et al., 2001), BPG.\u00b2\"}"}
{"id": "schwarz23a", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\nFigure 3. Analysis of the VC-INR on CelebA-HQ (a) Learning curves of Functa, linear & non-linear VC-INR models (b) Sparsity patterns of adapted weights throughout the network (c) t-SNE (Van der Maaten & Hinton, 2008) visualisation of masks.\\n\\nFigure 4. Compression results on image datasets CIFAR-10 (left) & Kodak (right). Modality-specific approaches are shown with a dashed line and marked (s). Conventional neural compression methods (also modality-agnostic) with a dotted line and marked (s, n). BMS is (Ball\u00e9 et al., 2017), Str\u00fcpler is (Str\u00fcpler et al., 2022) and VTM (Bross et al., 2021). We also compare against the modality-specific neural compression scheme BMS (Ball\u00e9 et al., 2018), VTM (Bross et al., 2021) and other INR techniques such as COIN (Dupont et al., 2021), COIN++ (Dupont et al., 2022b), MSCN (Schwarz & Teh, 2022) and the method in (Str\u00fcpler et al., 2022).\\n\\nUniform vs Variational Compression\\nWhile the previous section provides empirical justification for the architectural changes of VC-INR, we now additionally show the effectiveness of the proposed quantisation method. Figure 6 contrasts rate-distortion curves obtained using Uniform Quantisation with the transform coding setup introduced earlier. Results are obtained by compressing the latent vectors obtained from two pre-trained models to varying bit-rates by varying the number of bits for uniform quantisation or the rate-distortion trade-off parameter $\\\\lambda$ for the full VC-INR model. We note that the use of Ball\u00e9 et al. (2017)'s transform coding drastically shifts the rate-distortion curves towards lower bit-rates while maintaining a better reconstruction ratio. Furthermore, we can also see that the improved pre-training effectively increases the ceiling reconstruction performance when comparing uniform quantisation for VC-INR (light blue) with our COIN++ implementation (orange).\\n\\nImages\\nWe show compression performance on the image domain using the CIFAR-10 (Krizhevsky et al., 2009) and Kodak (meta-trained on Div2k (Agustsson & Timofte, 2017)) datasets. In order to handle the large images found in the Kodak dataset, we divide the images into smaller patches as previously established in prior work.\\n\\nFigure 4 shows that VC-INR significantly and consistently outperforms prior INR-based data compression methods (COIN, COIN++, MSCN, Str\u00fcpler) and even certain image codecs (JPEG/JPEG 2000) on the CIFAR-10 dataset. In addition, VC-INR reconstruction continue to improve with higher bitrates, which we demonstrate by almost pixel-perfect reconstruction. This implies that learned entropy coding is a key factor in achieving strong results. Furthermore, VC-INR shows comparable performance to the strongest modality-specific methods at low bitrates, despite not taking advantage of inductive biases. While not fully matching state-of-the-art (SOTA) results on images compared to all compression techniques, we significantly reduce the gap. Note that we provide further results on Kodak using Multiscale structural similarity index measure (MS-SSIM) in the Appendix.\"}"}
{"id": "schwarz23a", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Qualitative results from the Kodak dataset. Shown are VC-INR models in comparison with other INR-based techniques at similar bit-rates (3rd column) as well as a high-quality model (last column).\\n\\nFigure 6. Learned vs uniform quantisation for VC-INR & COIN++ on CIFAR-10.\\n\\nManifolds\\nThis evaluates VC-INR on global temperature measurements from the ERA5 dataset. The dataset consists of temperature measurements (features) at equally spaced latitudes and longitudes (coordinates) on Earth from 1979 to 2020, represented by spherical coordinates. Since no codec or neural compression method has been developed specifically for this modality, we compare VC-INR to COIN++ and image codecs (applied by unrolling the manifold on a rectangular grid) as baselines. As shown in Figure 7, VC-INR, we achieve an improvement of approximately 3.5dB at the same bitrate compared to the SOTA. This highlights the large potential impact modality-agnostic techniques might have for specialised data types.\\n\\nAudios\\nEvaluating VC-INR in the audio domain, we utilise the LibriSpeech dataset (Panayotov et al., 2015), a large speech dataset recorded at a 16kHz sampling rate. We consider the MP3 codec as well as COIN++ as baseline methods. As in COIN++ we use patching to keep the comparison fair. Our results, shown in Figure 8b demonstrate impressive results, showing that VC-INR significantly outperforms both COIN++ as well as the widely used and popular MP3 codec.\\n\\nVideos\\nTurning to the video domain, we compress clips from the UCF-101 action recognition dataset (Soomro et al., 2012), once again using patching. Here, we compare VC-INR to video codecs A VC and HEVC. Impressively, VC-INR significantly outperforms both A VC and HEVC.\\n\\nFigure 7. Compression results on ERA5 (climate data/manifolds). INR outperforms both, raising hopes for the potential of INR-based compression to one day replace hand-designed codecs for video. Qualitative results are available in Figure 9, showing the prediction errors of VC-INR models at varying bit-rates, for all of which we achieve SOTA better results.\\n\\n5. Conclusion\\nWe introduce VC-INR, a modality-agnostic neural compression technique showing strong and consistent improvements over previous INR-based methods. This was achieved by developing algorithmic improvements across the two axes of representational power and advanced quantisation while maintaining modality-agnosticism. Our technique bridges the gap between recent approaches to compact INR representations based on latent codes and sparsity and shows how previously modality-specific algorithms can be elevated to the modality-agnostic setting. Our evaluation shows strong improvement on previous work with INRs (e.g. Dupont et al., 2022b; Schwarz & Teh, 2022) while outperforming certain established algorithms (e.g. JPEG on images, MP3 on audio and A VC/HEVC on videos) while reducing the gap to others (e.g. BPG or BMS on images). We believe that the conceptual advantage of a single algorithm applicable to all modulations will continue to show rapid improvements.\"}"}
{"id": "schwarz23a", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Dataset Description\\n\\n**CelebA-HQ**\\nis a high-quality version of the CelebA dataset, which includes images of celebrities along with corresponding attributes (Karras et al., 2018). By following (Dupont et al., 2022a), we divide the dataset into 27,000 training examples and 3,000 test examples, and pre-processed the pixel coordinates into $[0, 1]_2$ and feature values ranging from 0 to 1.\\n\\n**ShapeNet**\\nis a dataset of 3D shapes of 10 different object categories (Chang et al., 2015). We follow the pre-processing by (Dupont et al., 2022a), and downscale the resolution of $128^3$ to $64^3$ by using `scipy.ndimage.zoom` function with threshold 0.05. To augment the datasets, the authors applied a 50-fold expansion by independently scaling the shapes in the x, y, and z axes using a randomly sampled scale within the range of 0.75 to 1.25. The resulting dataset includes 1,516,750 training examples and 168,850 test examples with voxel coordinates into $[0, 1]^3$ and occupancies in binary $\\\\{0, 1\\\\}$.\\n\\n**ERA5**\\nis a dataset consists of temperature observations from 1979 to 2020 on a global grid of equally spaced latitudes and longitudes (Hersbach et al., 2019). By following (Dupont et al., 2022a), we downsample the grid resolution $721 \\\\times 1044$ to $181 \\\\times 360$. Each time step is treated as a separate data point, and the dataset is split into a training set of 9676 data points and a test set of 2420 data points. As for the input, the given latitudes $c_{\\\\text{lat}}$ and longitudes $c_{\\\\text{long}}$ are transformed into 3D Cartesian coordinates $c = (\\\\cos c_{\\\\text{lat}} \\\\cos c_{\\\\text{long}}, \\\\cos c_{\\\\text{lat}} \\\\sin c_{\\\\text{long}}, \\\\sin c_{\\\\text{lat}})$ where latitudes $c_{\\\\text{lat}}$ are equally spaced in $[-\\\\pi/2, \\\\pi/2]$ and longitudes $c_{\\\\text{long}}$ are equally spaced in $[0, 2\\\\pi(n-1)/n]$ where $n$ the number of distinct values of longitude (360).\\n\\n**SRN Cars**\\nis a dataset of car scenes, with 2458 examples in the training set and 703 examples in the test set (Sitzmann et al., 2019). Each example consists of 50 random views centered on the car in the training set, and 251 views in the test set. The pre-processing of the data was conducted according to the guidelines provided by (Dupont et al., 2022b).\\n\\n**CIFAR-10**\\nis a dataset of 50,000 train and 10,000 test images with a resolution of $32 \\\\times 32$, comprising 10 different object categories (Krizhevsky et al., 2009). We use the same pre-processing as in CelebA-HQ dataset.\\n\\n**Kodak**\\nis a dataset of 24 uncompressed PNG images with a resolution of $768 \\\\times 512$, provided by the Kodak corporation. By following (Schwarz & Teh, 2022), we meta-learn on the high-quality versions of the Div2K dataset (Agustsson & Timofte, 2017), which consists of 900 images (by combining train and validation set). For Meta-Learning, we also train the model on randomly cropped $32 \\\\times 32$ patches and for evaluation, we split the image into non-overlapping patches where each modulations are adapted on each patches. Here, we also use the same pre-processing as in CelebA-HQ dataset.\\n\\n**LibriSpeech**\\nis a collection of read English speech recordings at a 16kHz sampling rate (Panayotov et al., 2015). By following Dupont et al. (2022b), we use the train-clean-100 split, which consists of 28,539 examples, and the test-clean split, which consists 2,620 examples. For the experiments, we use the first 3 seconds of each example (which is 48,000 audio samples) for both training and evaluation. For the pre-processing, we scale the coordinates into $[-5, 5]$.\\n\\n**UCF-101**\\nis a video action dataset comprising 13,320 videos with a resolution of $320 \\\\times 240$, organised into 101 classes (Soomro et al., 2012). In order to standardise the input for the model, we center-crop each video clip to $240 \\\\times 240 \\\\times 24$ and then resized to $128 \\\\times 128 \\\\times 24$.\\n\\nB. Numerical Results\\n\\nFor the sake of reputability, we now state the numerical compression values used to plot the results in Section 4. Note that baseline results have been taken from the code repository for COIN++ (Dupont et al., 2022b):\\n\\n### Cifar-10\\n\\n- vcinr_bpp = [0.29, 0.31, 1.18, 1.18, 2.95, 3.33, 4.88, 6.70, 8.69, 10.56, 12.52]\\n- vcinr_psnr = [22.76, 22.86, 28.86, 28.86, 34.96, 35.95, 40.25, 43.45, 45.70, 47.56, 48.32]\\n\\n### Kodak\\n\\n- vcinr_bpp = [0.08, 0.14, 0.48, 1.09, 1.54, 2.17, 3.09, 3.74, 5.56]\\n- vcinr_psnr = [26.86, 28.33, 32.07, 34.78, 36.59, 38.57, 41.26, 42.12, 42.24]\\n\\n### ERA-5\\n\\n- vcinr_bpp = [0.004, 0.004, 0.005, 0.00758, 0.011, 0.02119, 0.05, 0.07616]\\n- vcinr_psnr = [39.172, 40.766, 45.219, 47.965, 49.612, 51.25, 52.89, 54.25]\\n\\n### LibriSpeech\\n\\n- vcinr_bpp = [7.38, 8.04, 9.06, 14.61, 18.42, 20.06, 34.99, 43.69, 79.54, 120.77]\"}"}
{"id": "schwarz23a", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modality-Agnostic Variational Compression of Implicit Neural Representations\\n\\nvcinr_psnr = [44.10, 45.05, 45.93, 49.10, 50.68, 51.28, 55.61, 57.03, 59.33, 59.40]\\n\\n# UCF-101\\nvcinr_bpp = [0.09, 0.10, 0.26, 0.27, 0.42, 0.99, 1.59, 2.17, 4.00, 4.42]\\nvcinr_psnr = [29.90, 30.37, 34.51, 34.75, 36.83, 41.07, 44.58, 47.86, 55.81, 56.22]\\n\\nC. Meta-Learning Implicit Neural Representations with Latent Modulations\\n\\nIn order to efficiently and effectively encode a given signal into a compact latent representation, we utilise a Gradient-based Meta-Learning approach, such as model-agnostic meta-learning (MAML) (Finn et al., 2017). In our case, MAML aims to find a good initialisation $\\\\phi_0$ and shared INR parameter $\\\\theta$, allowing for the encoding of a given signal $x$ into the modulation $\\\\phi$ within a few gradient steps from $\\\\phi_0$.\\n\\nWriting $L_{MSE}(\\\\theta, \\\\phi_0, x)$ as a shorthand for the INR fitting loss (Equation (1)), a single gradient step adaptation of MAML is computed as:\\n\\n$$\\\\phi = \\\\phi_0 - \\\\alpha \\\\nabla_{\\\\phi_0} L_{MSE}(\\\\theta, \\\\phi_0, x),$$\\n\\n(6)\\n\\nwhere $\\\\alpha$ is the step size used in the inner loop. Note that one can easily iterate the adaptation for multiple steps. The key idea of MAML is to backpropagate through this optimisation process, directly learning an initialisation $\\\\phi_0$ (along with additional shared parameters $\\\\theta$) such that $\\\\phi$ can parameterise a good reconstruction of the signal after adaptation. This is typically computed over the training signal distribution $p(x)$:\\n\\n$$\\\\min_{\\\\theta, \\\\phi_0} \\\\mathbb{E}_{x \\\\sim p(x)} L_{MSE}(\\\\theta, \\\\phi_0, x) = \\\\min_{\\\\theta, \\\\phi_0} \\\\mathbb{E}_{x \\\\sim p(x)} h L_{MSE}(\\\\theta, \\\\phi_0, x),$$\\n\\n(7)\\n\\nHere, we refer each optimisation of MAML, Equation (6) as \u201cinner-loop\u201d, and (7) as \u201couter-loop\u201d, respectively. In practise, we also meta-learn the step size $\\\\alpha$ as another parameter updated in the outer loop, an approach known as MetaSGD (Li et al., 2017). This can be interpreted as a pre-conditioning of the gradient.\\n\\nAlgorithm 1\\n\\nINR Meta-training stage\\n\\nData: Dataset $\\\\{x_i, y_i\\\\}_{i=1}^N$\\n\\n1. Initialise shared network $\\\\theta$ and latent modulation initialisation $\\\\phi_0$.\\n\\n2. while not converged do\\n\\n3. Sample batch of data $B = \\\\{x_j, y_j\\\\}_{j=1}^B$\\n\\n4. for $j \\\\leftarrow 1$ to $B$ do\\n\\n5. $\\\\phi_j \\\\leftarrow \\\\phi_0 - \\\\alpha \\\\nabla_{\\\\phi_0} L_{MSE}(f(x_j, \\\\theta, \\\\phi_0), y_j)$\\n\\n6. $\\\\phi_0 \\\\leftarrow \\\\phi_0 - \\\\beta \\\\mathbb{E}_{j \\\\sim B} \\\\nabla_{\\\\phi_0} L_{MSE}(f(x_j, \\\\theta, \\\\phi_j), y_j)$\\n\\n7. $\\\\theta \\\\leftarrow \\\\theta - \\\\beta \\\\mathbb{E}_{j \\\\sim B} \\\\nabla_{\\\\theta} L_{MSE}(f(x_j, \\\\theta, \\\\phi_j), y_j)$\\n\\nResult: Dataset of latent modulations $\\\\{\\\\phi_i\\\\}_{i=1}^N$\\n\\nAlgorithm 2\\n\\nQuantisation training stage\\n\\nData: Dataset of latent modulations $\\\\{\\\\phi_i\\\\}_{i=1}^N$, $\\\\theta$, $\\\\lambda$\\n\\n8. while not converged do\\n\\n9. Initialise parameters $\\\\pi_a, \\\\pi_s$.\\n\\n10. Sample batch of data $B = \\\\{\\\\phi_j, x_j, y_j\\\\}_{j=1}^B$\\n\\n11. $z \\\\leftarrow g_a(\\\\phi_j; \\\\pi_a)$ // Rounding at inference to obtain $\\\\hat{z}$\\n\\n12. $e_z \\\\leftarrow z + \\\\epsilon; \\\\epsilon \\\\sim U(-\\\\frac{1}{2}, \\\\frac{1}{2})$ // Compute entropy model $p_{\\\\hat{z}}$ and rate\\n\\n13. $\\\\ell_{rate} = -\\\\log_2(p_{\\\\hat{z}}(e_z))$ $e_{\\\\phi_j} \\\\leftarrow g_s(e_z; \\\\pi_s)$\\n\\n14. $\\\\ell_{distortion} = L_{MSE}(f(x_j, \\\\theta, e_{\\\\phi_j}), y_j)$\\n\\n15. $\\\\pi_a \\\\leftarrow \\\\pi_a - \\\\beta \\\\mathbb{E}_{j \\\\sim B} \\\\nabla_{\\\\pi_a} (\\\\ell_{rate} + \\\\lambda \\\\ell_{distortion})$\\n\\n16. $\\\\pi_s \\\\leftarrow \\\\pi_s - \\\\beta \\\\mathbb{E}_{j \\\\sim B} \\\\nabla_{\\\\pi_s} (\\\\ell_{rate} + \\\\lambda \\\\ell_{distortion})$\\n\\nD. VC-INR algorithmic details\\n\\nAlgorithms 1 and 2 show details of the Meta-Learning (introduced in the previous section) and quantisation learning stages. The output of Algorithm 1 directly feeds into the pipeline for quantisation. Hence, the two problems of optimal parameterisation and quantisation can be tackled independently, thus allowing for various combination for future work.\"}"}
{"id": "schwarz23a", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E. Additional experimental results\\n\\nE.1. Stabilising Meta-Learning of Soft Gating mask Modulations with LayerNorm\\n\\nIn Section 3.3, we demonstrate the importance of using LayerNorm (Ba et al., 2016) in the meta-learning of our new parameterisation. In Figure 10b we demonstrate that Meta-Learning becomes highly unstable by default (an effect becoming more severe with larger $\\\\dim(\\\\phi)$) and thus requires extensive hyperparameter search which may still suffer from occasional instability. Instead, we find that LayerNorm largely removes this phenomenon, leading to more stable training and better results. We hypothesise that such a divergence occurs when the norm of the inner loop gradient is large, indicating a sharp loss landscape. LayerNorm addresses this issue by smoothing the loss landscape, as has previously been shown (Santurkar et al., 2018; Xu et al., 2019). Furthermore, it effectively bounds the norm of $\\\\phi$.\\n\\nIn addition, we show that our new parameterisation can effectively make use of increasing network capacity (while Functa shows decreasing performance for non-linear mappings from latent parameters to modulations). Figure 10a shows this effect to be particularly effective for increasing network width, which we recommend for optimal performance during pre-training.\\n\\nE.2. Results using MS-SSIM\\n\\nIn addition to results measured using PSNR, we provide results on Kodak using Multiscale structural similarity index measure (SSIM) (Wang et al., 2003) results in Figure 11 due to its better correlation with perceptual similarity. We observe comparable the results in Figure 4 with VC-INR performing similarly to JPEG & JPEG-2000.\"}"}
{"id": "schwarz23a", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"F. Qualitative Results\\n\\nF.1. Cifar10\\n\\nFigure 12 shows more qualitative results on Cifar10 for various rate/distortions trade-offs.\\n\\n| Original | Pre Quantisation |\\n|----------|-----------------|\\n| BPP: 11.66 | BPP: 6.12 |\\n| PSNR: 47.36 | PSNR: 44.63 |\\n| BPP: 10.74 | BPP: 5.62 |\\n| PSNR: 51.96 | PSNR: 48.24 |\\n| BPP: 11.42 | BPP: 5.80 |\\n| PSNR: 47.29 | PSNR: 45.20 |\\n| BPP: 9.47 | BPP: 4.75 |\\n| PSNR: 55.17 | PSNR: 51.81 |\\n| BPP: 11.81 | BPP: 6.19 |\\n| PSNR: 48.61 | PSNR: 45.72 |\\n| BPP: 11.03 | BPP: 5.85 |\\n| PSNR: 50.93 | PSNR: 47.26 |\\n\\nIn addition, we provide a further analysis of gating masks using a similar t-SNE projection as shown in the main text for Cifar-10 as well as an analysis of sparsity level and reconstruction correlation in Figure 13. With regards to correlation, it is firstly worth noting that there is little variation in the total sparsity level (reaching from 32.5 - 33.2). Secondly, we observe only very weak correlation (Pearson's correlation coefficient: 0.177) suggesting that no straightforward relationship between sparsity and performance exists.\\n\\nF.2. Kodak\\n\\nFigure 14 shows more qualitative results on Kodak in comparison with COIN++ (Dupont et al., 2022b) and MSCN (Schwarz & Teh, 2022).\\n\\nF.3. UCF-101\\n\\nFigure 15 shows more qualitative results on frames from the UCF-101 dataset. We provide links to each of the reconstruction video clips and its residual in comparison with the original video in Table 2.\\n\\nG. Hyperparameters\\n\\nG.1. Compression Experiments\\n\\nWe show hyperparameters for both INR training and subsequent compression training for CIFAR-10 in Table 3, for Kodak in Table 4, for ERA5 in Table 5, for LibriSpeech in Table 6 and for UCF-101 in Table 7.\"}"}
