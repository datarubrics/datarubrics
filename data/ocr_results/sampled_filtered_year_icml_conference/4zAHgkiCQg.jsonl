{"id": "4zAHgkiCQg", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nFigure 10. Examples of hallucinated rules (left) and facts (right) produced by GPT-3.5-Turbo while solving our logical reasoning benchmark.\"}"}
{"id": "4zAHgkiCQg", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11. An example logical reasoning problem with different premise orders. The number emojis are for ease of viewing. The ampersands were originally \u201cand\u201ds in the original prompt. The facts and query have been excluded for brevity.\\n\\nTo examine the effect of such position bias, we conduct ablations on PaLM 2-L with 10 distracting rules, and we compare the performance with relevant rules added in the beginning, middle or the end of the problem description. Table 6 shows that with the same order and number of rules, the variation in performance is very small, whereas changing the order significantly affects the results. Note that the longest inputs in our logical reasoning benchmark, i.e., problems with 12 relevant rules and 10 distracting rules, only contain no more than 300 tokens, which is relatively short compared to the context length limit of LLMs in our evaluation. These results confirm that for our tasks where the input problems (and thus input context) are short, the lost-in-the-middle phenomenon is not the primary cause of the performance difference. In our primary experiments, for all logical reasoning problems, we interleave distracting rules with relevant rules in the input context.\\n\\nE. Full Results for Logical Reasoning\\n\\nTables 7 and 10 present the accuracy numbers for Figures 3 and 5, which are results on different numbers of relevant rules without distracting rules.\\n\\nTables 8 and 11 present the accuracy numbers for Figures 4 and 6 with 5 distracting rules.\\n\\nTables 9 and 12 present the accuracy numbers for Figures 4 and 6 with 10 distracting rules.\"}"}
{"id": "4zAHgkiCQg", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. Logical reasoning results performance of PaLM 2-L, with relevant rules at different positions of the input context.\\n\\nF. Full Results on R-GSM\\n\\nTables 13 and 14 present the accuracy numbers for Figures 7 and 8, which are breakdown results on R-GSM problems with different numbers of reasoning steps and different numbers of sentences in the problem description respectively.\"}"}
{"id": "4zAHgkiCQg", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nFigure 12. R-GSM example where the original problem can be correctly solved by GPT-4 Turbo, but the model fails on the reordered one.\"}"}
{"id": "4zAHgkiCQg", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nFigure 13. R-GSM example where the original problem can be correctly solved by all models, but GPT-4 Turbo and Gemini Pro failed on the reordered one.\"}"}
{"id": "4zAHgkiCQg", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nFigure 14. R-GSM example where both the original and the reordered problems were correctly solved by all LLMs in our evaluation.\"}"}
{"id": "4zAHgkiCQg", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Order Matters in Reasoning with Large Language Models\\n\\nFigure 15. R-GSM example where both the original and the reordered problems were correctly solved by all LLMs in our evaluation.\"}"}
{"id": "4zAHgkiCQg", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Table 7. Result table corresponding to Figure 3. |\\n|-----------------------------------------------|\\n| **Rule Order** | **Accuracy** |\\n|----------------|--------------|\\n| Forward        | Backward     | Shuffled    |\\n|----------------|--------------|\\n| 4              | 99.0%        | 99.0%       | 98.5%       |\\n| 5              | 98.5%        | 99.5%       | 98.2%       |\\n| 6              | 98.5%        | 99.5%       | 98.3%       |\\n| 7              | 98.5%        | 98.0%       | 97.0%       |\\n| 8              | 99.0%        | 98.5%       | 98.3%       |\\n| 9              | 99.0%        | 97.5%       | 93.5%       |\\n| 10             | 98.0%        | 98.5%       | 98.3%       |\\n| 11             | 98.5%        | 98.5%       | 98.3%       |\\n| 12             | 99.0%        | 99.0%       | 98.3%       |\\n\\n(a) GPT-4-turbo.\\n\\n| Rule Order | Accuracy |\\n|------------|----------|\\n| Forward    | 98.5%    | 98.5%    | 98.3%    |\\n| Backward   | 98.5%    | 98.5%    | 98.3%    |\\n| Shuffled   | 98.3%    | 98.3%    | 98.3%    |\\n\\n(b) PaLM 2-L.\\n\\n| Rule Order | Accuracy |\\n|------------|----------|\\n| Forward    | 98.0%    | 93.5%    | 95.3%    |\\n| Backward   | 96.5%    | 89.0%    | 91.2%    |\\n| Shuffled   | 95.3%    | 91.2%    | 91.2%    |\\n\\n(c) Gemini 1.0 Pro.\\n\\n| Rule Order | Accuracy |\\n|------------|----------|\\n| Forward    | 94.0%    | 66.0%    | 78.7%    |\\n| Backward   | 88.0%    | 57.5%    | 66.5%    |\\n| Shuffled   | 85.7%    | 57.5%    | 66.5%    |\\n\\n(d) GPT-3.5-turbo.\"}"}
{"id": "4zAHgkiCQg", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nLarge language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground-truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that even if the model performance is decent on the optimal order, permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark.\\n\\n1. Introduction\\nLarge language models (LLMs) have demonstrated impressive performance across a variety of reasoning tasks (Wei et al., 2022; Cobbe et al., 2021; Hendrycks et al., 2021; Chen et al., 2021; Austin et al., 2021). In particular, recent state-of-the-art LLMs have reached or even surpassed human performance on multiple reasoning benchmarks, including STEM problem-solving and code generation (Bubeck et al., 2023; Gemini, 2023; Li et al., 2022). However, recent works show that LLMs exhibit failure modes that align with human-like cognitive biases (Berglund et al., 2023; Shi et al., 2023; Hagendorff et al., 2023; Jones & Steinhardt, 2022; McCoy et al., 2023). For example, Berglund et al. (2023) revealed the Reversal Curse; i.e., LLMs trained on \u201cA is B\u201d tend to fail to infer that \u201cB is A.\u201d Distractibility is another failure mode (Shi et al., 2023; Jones & Steinhardt, 2022), in which LLM performance drastically decreases when irrelevant context is included in the task description. In this work, we investigate the effect that premise order has on LLM reasoning. Specifically, in deductive reasoning, changing the order of premises alone does not change the conclusion. Consider the following illustrative example:\\n\\n1. If $A$ then $B$.\\n2. If $B$ then $C$.\\n3. $A$ is True.\\n\\nWe can derive that $C$ is True regardless of the order of these 3 premises. While some studies show that humans have a preference on the premise order to facilitate their reasoning (Dekeyser et al., 2000; Girotto et al., 1997), the premise order does not drastically affect human performance, especially for problems that only involve modus ponens (if $P$ then $Q$; $P$; therefore $Q$), which are relatively straightforward for humans.\\n\\nIn contrast to humans, we observe that for LLMs, the premise order has a significant impact on reasoning performance. In particular, LLMs reach the best performance when the premises are arranged in the same order as they appear in the ground-truth proof. Taking the illustrative problem above as an example, we observe two phenomena:\\n\\n1. Presenting \u201cIf $A$ then $B$\u201d before \u201cIf $B$ then $C$\u201d in the prompt generally achieves a higher accuracy compared to the reverse order.\\n2. The performance gap is more significant when the number of premises increases.\\n\\nIntuitively, such a preference on the premise order aligns with human preference (Dekeyser et al., 2000) because in the preferred order, each derivation step can be done on-the-fly while looking at premises one by one, without needing to look back and forth across all premises at each step. We conduct a systematic study on the premise order effect using a variety of LLMs, including GPT-4-turbo, GPT-3.5-turbo, and others, to examine the premise order effect in various reasoning tasks.\"}"}
{"id": "4zAHgkiCQg", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nFigure 1. Premise order affects the reasoning performance: a failure case for logical reasoning. Left: rules are sorted in the same order as the ground-truth proof (forward order with $\\\\tau = 1$ as defined in Section 2.1). Right: the wrong prediction with GPT-4-turbo after shuffling the rule set ($\\\\tau = 0$). Distracting rules are bold and in light blue.\\n\\nturbo (OpenAI, 2023), PaLM 2-L (Google, 2023), and Geminini 1.0 Pro (Gemini, 2023). Our primary focus is deductive reasoning, and we benchmark all LLMs on problems that only involve modus ponens (if $P$ then $Q$; $P$; therefore $Q$), where all LLMs in our evaluation at least achieve decent performance with a small number of premises. We show that the accuracy decrease caused by different ordering can be more than 30%. The ordering effect is further amplified when irrelevant premises (i.e., premises that are not needed to derive a conclusion) are presented in the prompt. Figure 1 illustrates a failure case, where all LLMs fail to generate the proof after changing the order of relevant rules. Interestingly, while all LLMs perform best when the premise order follows the ground-truth proof, they reveal different preferences on other alternative orderings. Specifically, compared to randomly ordering the premises, GPT-4-turbo and GPT-3.5-turbo generally achieve better performance when the premise order is exactly the reverse of the ground-truth proof. On the other hand, PaLM 2-L generally achieves the worst performance with such a reversed order.\\n\\nBesides logical reasoning, we construct R-GSM to further investigate the ordering effect on mathematical reasoning. Specifically, we build R-GSM on top of a subset of the GSM8K benchmark (Cobbe et al., 2021), where we change the order of sentences in the problem description and manually verify that the ground-truth answer remains the same. Despite the fact that multiple LLMs have achieved very high accuracies on the original GSM8K benchmark, our experiments again show that even on such relatively simple reasoning problems, changing the premise order could still cause a notable performance drop for all LLMs, especially on longer problems that require more reasoning steps.\\n\\nOur evaluation highlights that even in reasoning domains where the premise order does not matter, premise order does matter in LLM reasoning. In particular, starting from tasks where LLMs achieve decent performance with the forward order, the performance can significantly degrade with alternative premise orderings. The premise ordering effect indicates that LLMs are more comfortable reasoning via reading left-to-right instead of back-and-forth, which can be attributed to the auto-regressive model design or the reasoning bias learned from the training corpus. We leave proposing new training and modeling techniques to mitigate the premise order effect as future work.\\n\\n2. Benchmarks\\n\\n2.1. Logical Reasoning\\n\\nPrior work has revealed the weaknesses of LLMs in logical reasoning (Han et al., 2022; Xu et al., 2023; Saparov 2023).\"}"}
{"id": "4zAHgkiCQg", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nFigure 2.\\n\\nR-GSM example where the original problem can be correctly solved by all LLMs in our evaluation, but all of them failed on the reordered one. Different calculation steps and their corresponding problem statements are annotated in light blue. Specifically, the reasoning steps of the original problem follows the ordering of problem statements, while the reordered problem does not.\\n\\net al., 2023; Saparov & He, 2022; Wan et al., 2024), especially when the proof is long and requires the knowledge of multiple deduction theorems. To isolate the effect of premise orders, we focus on a confined problem space adapted from SimpleLogic (Zhang et al., 2022), which only includes propositional logic problems with definite clauses. Specifically, each problem includes: (1) a set of facts $A_1, ..., A_n$ that hold true; (2) a set of rules of the form \u201cIf $X$, then $Y$\u201d, \u201cIf $X_0$ and $X_1$, then $Y$\u201d, or \u201cIf $X_0$ and $X_1$ and $X_2$, then $Y$\u201d; and (3) a conclusion \u201c$C$ is True\u201d to be proved.\\n\\nAs opposed to SimpleLogic \u2014 which formulates the problem as a binary classification task (i.e., indicate whether the conclusion is True or False) \u2014 in our benchmark, every problem has a ground-truth label of True, and we consider the prediction to be correct only when the generated proof is completely valid. With these strict criteria, the LLM is required to produce the step-by-step deduction that leads to the conclusion, and any hallucination of nonexistent facts and rules is considered erroneous. In addition, differently from SimpleLogic, which uses English words as predicates (e.g., an example rule is \u201cIf Alice is fast and smart, then Alice is bad\u201d), all predicates are randomly generated pseudowords in our benchmark. This design choice is motivated by our observation that when presented with rules of randomly selected English words, LLMs often question the validity of various rules and tend not to generate a proof. Using pseudowords eliminates this issue; additionally, this design also forces the LLM to generate the proof solely based on the problem statement, without relying on its internal knowledge.\\n\\nThe key characteristic of our benchmark is that for each logical reasoning problem, we synthetically generate variants with different premise orders. Specifically, we denote the order that conforms to the ground-truth proof with forward chaining as the forward order, where the rule applied in each derivation step is sequentially presented in the problem description. Intuitively, presenting premises in the forward order simplifies the problem for humans, as this allows us to write the proof on-the-fly while reading the premises. Conversely, a premise ordering that is more random increases the task difficulty, since carrying out the derivation requires us to repetitively look for premises for each reasoning step. Motivated by this intuition, we categorize different premise orders based on their Kendall tau distance $\\\\tau$ (Cicirello, 2019; Sen, 1968) to the forward order, normalized into the range $[-1, 1]$. Specifically, $\\\\tau = 1$ is the forward order, and we denote the order with $\\\\tau = -1$ as the backward order, which is the reverse of the forward order and aligns with the proof via backward chaining. $\\\\tau \\\\approx 0$ suggests that there is no strong correlation between the premise order in the problem description and the proof. To thoroughly investigate the LLM preference on different premise orders, we evaluate the model performance on $\\\\tau = 0.5$, $0$, and $-0.5$, in addition...\"}"}
{"id": "4zAHgkiCQg", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nto the forward ($\\\\tau = 1$) and backward ($\\\\tau = -1$) orders. We present examples with $\\\\tau = 1$ and 0 in Figure 1, and defer examples with other $\\\\tau$ values to Figure 11 in Appendix B.\\n\\nWe measure the premise order effect by varying the following two factors:\\n\\n\u2022 Number of rules required in the proof. It is expected that the premise order effect is more significant for a proof with more rules. For our benchmark, we generate problems whose numbers of rules in the proof range from 4 to 12.\\n\\n\u2022 Number of distracting rules (i.e., rules that are not useful for the proof) presented in the problem. The presence of distracting rules also complicates the problem, as premise selection itself is challenging (Wang et al., 2017; Ferreira & Freitas, 2020; Irving et al., 2016), and LLMs are shown to be easily distracted by irrelevant context (Shi et al., 2023). We include problem variants with 0, 5 and 10 distracting rules.\\n\\nWe generate 200 problems for each number of required rules. Considering different premise orders and numbers of distracting rules, each problem includes 15 variants, resulting in a total of 27K problems in our benchmark.\\n\\n2.2. R-GSM for Mathematical Reasoning\\n\\nTo further assess the effect of premise orders beyond logical reasoning, we construct the R-GSM dataset based on GSM8K (Cobbe et al., 2021), which is a popular benchmark of grade school math word problems. Specifically, we first select GSM8K test problems with at least 5 sentences in the problem description. Afterward, we filter out those problems where there is no alternative ordering that does not change the ground truth answer. An example of such problems is those with sentences that follow the causal order of an event series. For each of the remaining problem, we keep the last sentence untouched and rewrite the problem description with a different ordering of other sentences. Minor editing on words is allowed to ensure the grammatical correctness of the problem description. To facilitate the annotation process, for each problem, we write a simple function to enumerate all alternative orderings of problem statements until an ordering that causes the LLM prediction failure is discovered. Such enumeration is able to find failure cases on more than 70% problems for GPT-4-turbo and PaLM 2-L. However, as opposed to logical reasoning tasks, simply shuffling sentences in GSM8K problems often results in different ground truth answers or renders the problem invalid. Therefore, a manual rewriting process is done to ensure that the rewritten problems preserve the same ground truth answer and are grammatically correct. In total, our R-GSM benchmark contains 220 pairs of problems, including both the original GSM8K problem description and the manually rewritten one with a different ordering of problem statements.\\n\\nDespite that over 60% of problems in R-GSM only have 5 sentences, and all problems have at most 8 sentences and less than 200 tokens, our evaluation shows that all LLMs still perform considerably worse on rewritten problems. Figure 2 presents an example in R-GSM where all LLMs correctly solve the original problem but not the rewritten one. Specifically, the reasoning steps for the original problem follows the ordering of problem statements, while for the rewritten problem, the second calculation step in the correct solution should refer to the second-to-last sentence instead of the second sentence in the problem description. We provide a more detailed case study in Section 3.3, and present the full dataset statistics in Appendix A.\\n\\n3. Experiments\\n\\n3.1. Experimental Setup\\n\\nWe evaluate the premise ordering effect on GPT-4-turbo, GPT-3.5-turbo, PaLM 2-L and Gemini 1.0 Pro. We perform the greedy decoding with the temperature 0, and apply the zero-shot prompting in all experiments unless otherwise specified. On R-GSM, the model input only contains the problem description without additional instructions. For logical reasoning, as shown in Figure 1, we add an instruction in the prompt to ask for a derivation that specifies which premise is used in each step.\\n\\n3.2. Logical Reasoning\\n\\nFigure 3 presents the results with different numbers of relevant rules included in ground-truth proofs, where the problem does not contain distracting rules, and the shuffled accuracy is the aggregation of results with $\\\\tau = 0, 5, 0, -0.5$. Across different LLMs, the forward order consistently achieves the best performance, which aligns with the human preference. The performance drop caused by alternative orderings becomes more significant when the number of rules increases. Meanwhile, models with weaker reasoning capabilities are also more sensitive to different premise orders. Specifically, while the accuracy decrease of GPT-4-turbo and PaLM 2-L is up to 20\u221230%, with Gemini 1.0 Pro and GPT-3.5-turbo, changing the premise order from the forward order can degrade the accuracy from over 65% to below 25%, with an accuracy decrease of more than 40%.\"}"}
{"id": "4zAHgkiCQg", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Steps | Init Acc | Reorder Acc |\\n|-------|----------|-------------|\\n| 2     | 86.4%    | 79.5%       |\\n| 3     | 78.5%    | 69.4%       |\\n| 4     | 69.4%    | 51.0%       |\\n| 5     | 63.1%    | 51.8%       |\\n| 6     | 77.3%    | 54.5%       |\\n\\nTable 13. Results corresponding to Figure 7.\\n\\n| Steps | Init Acc | Reorder Acc |\\n|-------|----------|-------------|\\n| 2     | 80.5%    | 69.1%       |\\n| 3     | 79.0%    | 60.9%       |\\n| 4     | 77.3%    | 54.5%       |\\n| 5     | 67.3%    | 51.8%       |\\n| 6     | 62.1%    | 46.0%       |\\n\\nTable 14. Results corresponding to Figure 8.\"}"}
{"id": "4zAHgkiCQg", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nFigure 3. Results on logical reasoning without distracting rules. See Table 7 in Appendix E for accuracy numbers.\\n\\nFigure 4. Results on logical reasoning with distracting rules. See Tables 8 and 9 in Appendix E for accuracy numbers.\\n\\nFigure 5. Fine-grained results on different $\\\\tau$ without distracting rules. See Table 10 in Appendix E for accuracy numbers.\\n\\nFigure 6. Fine-grained results on different $\\\\tau$ with distracting rules. See Tables 11 and 12 in Appendix E for accuracy numbers.\"}"}
{"id": "4zAHgkiCQg", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nGPT-4-turbo generally prefers the backward order over other orders, and the overall performance decreases with a smaller absolute value of $\\\\tau$. This observation is also consistent with human reasoning patterns, as backward chaining is another well-established inference method. On the other hand, PaLM 2-L generally performs the worst with the backward order. With the decrease of $\\\\tau$ (i.e., the premise order deviates more from the forward order), the accuracy drops. The preferences of Gemini 1.0 Pro and GPT-3.5-turbo are less consistent; still, they prefer the backward order more often than other non-forward premise orders.\\n\\nEffect of distracting rules.\\nWe assess the effect of distracting rules of GPT-4-turbo and PaLM 2-L, which reach a decent performance without the presence of distracting rules. Figures 4 and 6 show that adding distracting rules further decreases the reasoning performance and magnifies the effect of different premise orders. Still, the overall preferences of both LLMs remain the same as the scenario without distracting rules. Specifically, both LLMs again achieve the best performance with the forward order, and GPT-4-turbo prefers the backward order over other non-forward orders, while PaLM 2-L performance decreases with a smaller $\\\\tau$.\\n\\nError analysis.\\nIn Table 1, we present a breakdown on prediction errors across different premise orders. We consider the following error categories:\\n\\n1. wrong refutation: the LLM wrongly claims that the conclusion can not be proved;\\n2. rule hallucination: the LLM generates rules that do not exist in the problem;\\n3. fact hallucination: the LLM generates facts that do not exist in the problem, or are not proved yet.\\n\\nWe observe that for all LLMs, fact hallucination is typically the most common error pattern, and this error type escalates dramatically with the decrease of $\\\\tau$. The main reason is that LLMs are inclined to use the rules in the sequential order as they present in the problem, so when the next rule in the problem is not yet applicable, LLMs might still hallucinate facts to complete the proof step. Simultaneously, we observe that the percentage of wrong refutation is generally lower for $\\\\tau = -1$ than for $\\\\vert \\\\tau \\\\vert < 1$. We present an example of wrong refutation in Figure 1, and we include more examples of rule and fact hallucination in Figure 10 of Appendix B.\\n\\n3.3. R-GSM for Mathematical Reasoning\\nTable 2 demonstrates the overall results on R-GSM. Again, all LLMs achieve a lower performance on R-GSM, as compared to GSM8K. Note that the original GSM8K problems are not necessarily written in the most preferable way, and thus sometimes the manual rewriting facilitates the reasoning and allows the model to correctly solve the reordered problems.\\n\\nTable 1.\\n| Model            | $\\\\tau$ | Wrong Refutation | Rule Hallucination | Fact Hallucination |\\n|------------------|--------|------------------|-------------------|-------------------|\\n| GPT-4-turbo      | 1      | 96.5%            | 0.5%              | 1.5%              |\\n|                  | 0.5    | 76.0%            | 10.5%             | 2.0%              |\\n|                  | 0      | 82.0%            | 4.5%              | 3.5%              |\\n|                  | -0.5   | 84.5%            | 1.0%              | 4.5%              |\\n|                  | -1     | 84.0%            | 0.0%              | 3.5%              |\\n| GPT-3.5-turbo    | 1      | 30.0%            | 24.5%             | 9.5%              |\\n|                  | 0.5    | 1.0%             | 54.5%             | 9.5%              |\\n|                  | 0      | 0.5%             | 55.0%             | 7.5%              |\\n|                  | -0.5   | 2.0%             | 50.0%             | 8.5%              |\\n|                  | -1     | 1.5%             | 34.5%             | 14.5%             |\\n| PaLM 2-L         | 1      | 88.0%            | 0.5%              | 3.0%              |\\n|                  | 0.5    | 74.5%            | 1.5%              | 9.5%              |\\n|                  | 0      | 65.5%            | 2.0%              | 11.0%             |\\n|                  | -0.5   | 59.5%            | 1.5%              | 10.0%             |\\n|                  | -1     | 57.5%            | 1.0%              | 11.5%             |\\n| Gemini 1.0 Pro   | 1      | 16.5%            | 28.0%             | 5.0%              |\\n|                  | 0.5    | 0.0%             | 59.0%             | 3.5%              |\\n|                  | 0      | 0.0%             | 34.0%             | 9.0%              |\\n|                  | -0.5   | 0.5%             | 24.5%             | 9.5%              |\\n|                  | -1     | 0.5%             | 27.5%             | 11.5%             |\\n\\nTable 2.\\n| Model            | Init Acc | Reorder Acc |\\n|------------------|----------|-------------|\\n| GPT-4-turbo      | 94.1%    | 85.0%       |\\n| PaLM 2-L         | 86.4%    | 79.5%       |\\n| Gemini 1.0 Pro   | 80.5%    | 69.1%       |\\n| GPT-3.5-turbo    | 67.3%    | 51.8%       |\\n\\nTable 3.\\n| Model            | Init Acc | Reorder Acc |\\n|------------------|----------|-------------|\\n| GPT-4-turbo      | 100%     | 89.9%       |\\n| PaLM 2-L         | 100%     | 87.9%       |\\n| Gemini 1.0 Pro   | 100%     | 74.6%       |\\n| GPT-3.5-turbo    | 100%     | 64.9%       |\\n\\nFor each model, the accuracies on the R-GSM subset where the original problems are correctly solved, thus the initial accuracy is 100% for all models.\"}"}
{"id": "4zAHgkiCQg", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nFigure 7. R-GSM results with different numbers of reasoning steps in the ground truth. See Table 13 in Appendix F for accuracy numbers.\\n\\nFigure 8. R-GSM results with different problem lengths. See Table 14 in Appendix F for accuracy numbers.\\n\\n| Model             | Accuracy on Original | Accuracy on Reordered |\\n|-------------------|----------------------|-----------------------|\\n| GPT-4-turbo       | 45.0%                | 15.0%                 |\\n| GPT-3.5-turbo     | 21.6%                | 19.6%                 |\\n| PaLM 2-L          | 34.8%                | 4.3%                  |\\n| Gemini 1.0 Pro    | 29.5%                | 18.2%                 |\\n\\nTable 4. Error analysis on R-GSM. \u201cTemporal\u201d refers to the temporal order, and \u201cUnknown\u201d refers to the unknown variables.\\n\\nGiven the performance degradation with zero-shot prompting, one potential mitigation is to apply few-shot chain-of-thought prompting and include demonstrations of solving reordered problems in the prompt. However, we observe that the few-shot results are mostly similar to or even worse than zero-shot ones. The only improvement we have seen is with GPT-3.5-turbo: using 5 exemplars of reordered problems annotated with ground truth solutions, the accuracy on original problems becomes 79.1%, and the accuracy on reordered problems becomes 66.4%. Despite the performance improvement, there is still a 13% accuracy drop after reordering. These results show that using few-shot demonstrations alone is insufficient for addressing the effect of premise orders.\\n\\nBreakdown of problem complexity. Figures 7 and 8 present the breakdown results on different number of reasoning steps and different number of problem sentences, respectively. Unsurprisingly, across all LLMs, the proof accuracy suffers on problems that require more reasoning steps and contain a greater number of sentences. Overall, the gap between the accuracies on initial and rewritten problems is more significant with more reasoning steps and longer problems for both GPT-4-turbo and Gemini 1.0 Pro, while the gap remains similar across different numbers of reasoning steps and problem lengths for PaLM 2-L and GPT-3.5-turbo.\\n\\nError analysis. To further understand the failure modes, for each LLM, we analyze those error cases where the original problems can be correctly solved but not the reordered ones, and we categorize the common error types in Table 4. Similar to our observation in logical reasoning experiments, the prediction errors in R-GSM are primarily due to the LLMs blindly using numbers in the sequential order of their appearances in the problem. Specifically, the most common error case for all LLMs is their tendency to overlook temporal order. Figure 2 presents such an example, where the prediction failure is because some earlier events are described in the later part of the problem. Another category of errors occurs when some quantities are not specified while...\"}"}
{"id": "4zAHgkiCQg", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Premise Order Matters in Reasoning with Large Language Models\\n\\nFigure 9. R-GSM example where the original problem can be correctly solved by all LLMs, but GPT-3.5-Turbo fails on the reordered version while all the other LLMs still solve it correctly.\\n\\nprocessing the problem in the sequential order, which introduces unknown variables for calculation. Take, for example, the problem in Figure 9. In the original problem, the number of each animal can be directly calculated based on its preceding sentence. However, in the reordered problem, the number of gerbils cannot directly be computed based on the preceding sentences, since the number of fish remains unknown up to that point, and the LLM must read the remaining sentences and calculate the number of fish first. However, the prediction from GPT-3.5-turbo instead uses the number calculated in the previous step (i.e., the number of rabbits) to calculate the number of gerbils, resulting in an error. Such a failure mode is less common with PaLM 2-L, but still constitutes a non-negligible proportion of prediction errors for the other LLMs. We present more examples of model predictions in Appendix C.\\n\\n4. Related Work\\n\\nFailure modes of LLMs. The premise order effect in this work is connected to several failure modes of LLMs in the literature, including the reversal curse (Berglund et al., 2023), distractibility (Shi et al., 2023), position bias (Liu et al., 2024; Wang et al., 2023), and limited capability of logical reasoning (Han et al., 2022; Xu et al., 2023; Saparov et al., 2023; Saparov & He, 2022; Wan et al., 2024; Zhu et al., 2023; Yan et al., 2023). Specifically, Shi et al. (2023) show that including irrelevant context in the problem statement leads to a considerable performance drop on GSM8K and other reasoning benchmarks, revealing that LLMs are distractible. This finding is in-line with our evaluation on logical reasoning, where we observe that adding irrelevant rules not only degrades the overall logical reasoning performance, but also escalates the premise order effect. Similarly, the Reversal Curse (Berglund et al., 2023) unveils another perspective of the order effect, where they show that an LLM that recognizes \u201cA is B\u201d does not necessarily learn that \u201cB is A.\u201d While their work studies the order effect between two entities within a single factual statement, our work focuses on reasoning problems with multiple premises, without restrictions on the number of (or relationship between) entities. In particular, for logical reasoning, we demonstrate that random permutations of premises often result in worse accuracy than the purely backward order. Liu et al. (2024) discover the lost-in-the-middle phenomenon in the long-context scenario: the LLM performance is the best when the relevant information to solve the task is placed at the beginning or the end of the input context, while the performance is the worst when the LLM needs to utilize input context in the middle. In Appendix D, we show that lost-in-the-middle phenomenon does not affect the performance on our tasks, since the length of input problems does not exceed 300 tokens in our benchmark, which is relatively small compared to the context length limit of LLMs in our...\"}"}
{"id": "4zAHgkiCQg", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Premise | Order Matters in Reasoning with Large Language Models |\\n|---------|-----------------------------------------------------|\\n|         | Rules Order Acc                                     |\\n|         | Forward 98.0%                                       |\\n|         | Backward 99.5%                                      |\\n|         | Shuffled 99.0%                                      |\\n|         | Forward 99.5%                                       |\\n|         | Backward 98.5%                                      |\\n|         | Shuffled 98.0%                                      |\\n|         | Forward 97.5%                                       |\\n|         | Backward 97.0%                                      |\\n|         | Shuffled 96.7%                                      |\\n|         | Forward 93.5%                                       |\\n|         | Backward 92.0%                                      |\\n|         | Shuffled 90.2%                                      |\\n|         | Forward 89.5%                                       |\\n|         | Backward 85.5%                                      |\\n|         | Shuffled 82.2%                                      |\\n|         | Forward 88.0%                                       |\\n|         | Backward 84.0%                                      |\\n|         | Shuffled 82.7%                                      |\\n|         | Forward 89.0%                                       |\\n|         | Backward 77.0%                                      |\\n|         | Shuffled 74.2%                                      |\\n|         | Forward 84.5%                                       |\\n|         | Backward 75.5%                                      |\\n|         | Shuffled 71.5%                                      |\\n|         | Forward 80.5%                                       |\\n|         | Backward 72.5%                                      |\\n|         | Shuffled 57.2%                                      |\\n|         | (a) GPT-4-turbo.                                    |\\n|         | Forward 98.5%                                       |\\n|         | Backward 95.5%                                      |\\n|         | Shuffled 94.5%                                      |\\n|         | Forward 97.0%                                       |\\n|         | Backward 93.5%                                      |\\n|         | Shuffled 94.8%                                      |\\n|         | Forward 88.0%                                       |\\n|         | Backward 85.0%                                      |\\n|         | Shuffled 88.5%                                      |\\n|         | Forward 87.5%                                       |\\n|         | Backward 68.0%                                      |\\n|         | Shuffled 75.8%                                      |\\n|         | Forward 84.5%                                       |\\n|         | Backward 63.0%                                      |\\n|         | Shuffled 66.0%                                      |\\n|         | Forward 81.5%                                       |\\n|         | Backward 56.5%                                      |\\n|         | Shuffled 60.8%                                      |\\n|         | Forward 79.5%                                       |\\n|         | Backward 46.5%                                      |\\n|         | Shuffled 55.5%                                      |\\n|         | Forward 73.0%                                       |\\n|         | Backward 43.5%                                      |\\n|         | Shuffled 42.5%                                      |\\n|         | Forward 64.0%                                       |\\n|         | Backward 32.5%                                      |\\n|         | Shuffled 38.2%                                      |\\n|         | (b) PaLM 2-L.                                       |\\n\\nTable 8. Results corresponding to Figure 4 with 5 distracting rules.\"}"}
{"id": "4zAHgkiCQg", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Premise Order Matters in Reasoning with Large Language Models\\n\\n| Method     | Rule Order | Accuracy (Forward) | Accuracy (Backward) | Accuracy (Shuffled) |\\n|------------|------------|--------------------|---------------------|---------------------|\\n| GPT-4-turbo| 4          | 97.0%              | 98.0%               | 97.7%               |\\n|            | 5          | 98.0%              | 96.0%               | 96.5%               |\\n|            | 6          | 92.5%              | 88.5%               | 90.3%               |\\n|            | 7          | 84.5%              | 80.0%               | 76.0%               |\\n|            | 8          | 81.5%              | 76.5%               | 70.5%               |\\n|            | 9          | 73.0%              | 65.0%               | 62.8%               |\\n|            | 10         | 64.5%              | 59.0%               | 53.7%               |\\n|            | 11         | 58.5%              | 53.0%               | 48.7%               |\\n|            | 12         | 57.5%              | 46.5%               | 40.0%               |\\n\\n| PaLM 2-L   | 4          | 94.0%              | 91.0%               | 92.5%               |\\n|            | 5          | 89.0%              | 77.0%               | 79.7%               |\\n|            | 6          | 71.5%              | 55.0%               | 60.7%               |\\n|            | 7          | 68.5%              | 39.5%               | 46.7%               |\\n|            | 8          | 61.5%              | 38.0%               | 42.7%               |\\n|            | 9          | 47.0%              | 29.5%               | 30.7%               |\\n|            | 10         | 46.5%              | 15.5%               | 25.0%               |\\n|            | 11         | 36.5%              | 15.5%               | 18.2%               |\\n|            | 12         |                    |                     |                     |\\n\\n### Table 9.\\nResults corresponding to Figure 4 with 10 distracting rules.\"}"}
{"id": "4zAHgkiCQg", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Rule | Rule | Acc | $\\\\tau$ |\\n|------|------|-----|-------|\\n| 8    |      | 1.0 | 99.0% |\\n|      |      | 0.5 | 95.0% |\\n|      |      | 0.0 | 91.0% |\\n|      |      | -0.5| 94.5% |\\n|      |      | -1.0| 95.5% |\\n| 10   |      | 1.0 | 99.0% |\\n|      |      | 0.5 | 91.0% |\\n|      |      | 0.0 | 82.5% |\\n|      |      | -0.5| 88.5% |\\n|      |      | -1.0| 92.5% |\\n| 11   |      | 1.0 | 98.5% |\\n|      |      | 0.5 | 90.0% |\\n|      |      | 0.0 | 84.5% |\\n|      |      | -0.5| 88.0% |\\n|      |      | -1.0| 91.0% |\\n| 12   |      | 1.0 | 96.5% |\\n|      |      | 0.5 | 76.0% |\\n|      |      | 0.0 | 82.0% |\\n|      |      | -0.5| 84.5% |\\n|      |      | -1.0| 84.0% |\\n\\n(a) GPT-4-turbo.\\n\\n| Rule | Rule | Acc | $\\\\tau$ |\\n|------|------|-----|-------|\\n| 8    |      | 1.0 | 95.5% |\\n|      |      | 0.5 | 89.5% |\\n|      |      | 0.0 | 86.5% |\\n|      |      | -0.5| 87.0% |\\n|      |      | -1.0| 77.0% |\\n| 10   |      | 1.0 | 95.0% |\\n|      |      | 0.5 | 84.0% |\\n|      |      | 0.0 | 83.0% |\\n|      |      | -0.5| 76.0% |\\n|      |      | -1.0| 75.5% |\\n| 11   |      | 1.0 | 94.0% |\\n|      |      | 0.5 | 80.5% |\\n|      |      | 0.0 | 76.5% |\\n|      |      | -0.5| 79.0% |\\n|      |      | -1.0| 66.0% |\\n| 12   |      | 1.0 | 88.0% |\\n|      |      | 0.5 | 74.5% |\\n|      |      | 0.0 | 65.5% |\\n|      |      | -0.5| 59.5% |\\n|      |      | -1.0| 57.5% |\\n\\n(b) PaLM 2-L.\\n\\n| Rule | Rule | Acc | $\\\\tau$ |\\n|------|------|-----|-------|\\n| 6    |      | 1.0 | 87.5% |\\n|      |      | 0.5 | 68.5% |\\n|      |      | 0.0 | 75.5% |\\n|      |      | -0.5| 72.0% |\\n|      |      | -1.0| 77.5% |\\n| 8    |      | 1.0 | 50.0% |\\n|      |      | 0.5 | 10.5% |\\n|      |      | 0.0 | 12.0% |\\n|      |      | -0.5| 15.0% |\\n|      |      | -1.0| 17.5% |\\n| 10   |      | 1.0 | 34.0% |\\n|      |      | 0.5 | 2.0% |\\n|      |      | 0.0 | 3.5% |\\n|      |      | -0.5| 2.0% |\\n|      |      | -1.0| 4.5% |\\n| 12   |      | 1.0 | 16.5% |\\n|      |      | 0.5 | 0.0% |\\n|      |      | 0.0 | 0.0% |\\n|      |      | -0.5| 0.5% |\\n|      |      | -1.0| 0.5% |\\n\\n(c) Gemini 1.0 Pro.\\n\\n| Rule | Rule | Acc | $\\\\tau$ |\\n|------|------|-----|-------|\\n| 6    |      | 1.0 | 87.5% |\\n|      |      | 0.5 | 68.5% |\\n|      |      | 0.0 | 75.5% |\\n|      |      | -0.5| 72.0% |\\n|      |      | -1.0| 77.5% |\\n| 8    |      | 1.0 | 50.0% |\\n|      |      | 0.5 | 10.5% |\\n|      |      | 0.0 | 12.0% |\\n|      |      | -0.5| 15.0% |\\n|      |      | -1.0| 17.5% |\\n| 10   |      | 1.0 | 34.0% |\\n|      |      | 0.5 | 2.0% |\\n|      |      | 0.0 | 3.5% |\\n|      |      | -0.5| 2.0% |\\n|      |      | -1.0| 4.5% |\\n| 12   |      | 1.0 | 16.5% |\\n|      |      | 0.5 | 0.0% |\\n|      |      | 0.0 | 0.0% |\\n|      |      | -0.5| 0.5% |\\n|      |      | -1.0| 0.5% |\\n\\n(d) GPT-3.5-turbo.\\n\\nTable 10. Result table corresponding to Figure 5.\"}"}
{"id": "4zAHgkiCQg", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 11.\\n\\nResults corresponding to Figure 6 with 5 distracting rules.\\n\\n| \u03c4 | Acc  |\\n|---|------|\\n| 1  | 81.5%|\\n| 0.5 | 73.0%|\\n| 0  | 65.5%|\\n| -0.5 | 73.0%|\\n| -1  | 76.5%|\\n| 10 | 64.5%|\\n| 0.5 | 48.5%|\\n| 0  | 50.5%|\\n| -0.5 | 62.0%|\\n| -1  | 59.0%|\\n| 11 | 58.5%|\\n| 0.5 | 54.0%|\\n| 0  | 41.0%|\\n| -0.5 | 51.0%|\\n| -1  | 53.0%|\\n| 12 | 57.5%|\\n| 0.5 | 33.0%|\\n| 0  | 42.0%|\\n| -0.5 | 45.0%|\\n| -1  | 46.5%|\\n\\n(a) GPT-4-turbo.\\n\\n(b) PaLM 2-L.\\n\\n## Table 12.\\n\\nResults corresponding to Figure 6 with 10 distracting rules.\\n\\n| \u03c4 | Acc  |\\n|---|------|\\n| 1  | 68.5%|\\n| 0.5 | 48.5%|\\n| 0  | 45.5%|\\n| -0.5 | 46.0%|\\n| -1  | 39.5%|\\n| 10 | 47.0%|\\n| 0.5 | 35.0%|\\n| 0  | 30.0%|\\n| -0.5 | 27.0%|\\n| -1  | 29.5%|\\n| 11 | 46.5%|\\n| 0.5 | 30.0%|\\n| 0  | 24.5%|\\n| -0.5 | 20.5%|\\n| -1  | 15.5%|\\n| 12 | 36.5%|\\n| 0.5 | 18.0%|\\n| 0  | 19.0%|\\n| -0.5 | 17.5%|\\n| -1  | 15.5%|\\n\\n(a) GPT-4-turbo.\\n\\n(b) PaLM 2-L.\"}"}
{"id": "4zAHgkiCQg", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluation. Yan et al. (2023) present an approach called Concise and Organized Perception for deductive reasoning, which first generates directed graphs by connecting facts and rules in the problem, then prune and reorder the context accordingly before calling the LLM to solve the problem. The improvement achieved by this approach again demonstrates the effect of premise ordering and irrelevant premises on logical reasoning. While such input preprocessing methods can mitigate the ordering effect on certain reasoning tasks, they require task-specific design and do not generalize across domains. We consider developing generic end-to-end reasoning techniques for LLMs to address the premise order effect as future work.\\n\\nOrder effect for human logical reasoning.\\n\\nAlthough the premise order does not matter in deductive reasoning, several studies show that the premise order can impact the human reasoning performance (Dekeyser et al., 2000; Girotto et al., 1997). Dekeyser et al. (2000) described co-reference as a human preference of premise order; i.e., humans prefer the premises to be presented in an order where they can draw immediate conclusions after seeing each one. In this work, we show that LLMs also have such a preference, and they achieve the best performance when the ordering of rules follows the ground-truth proof. Girotto et al. (1997) studied how the premise order affects logical reasoning for humans, and found that the premise order has a significant effect in solving modus tollens problems (i.e., if $P$, then $Q$; not $Q$; therefore, not $P$), but not modus ponens problems (i.e., if $P$, then $Q$; $P$; therefore, $Q$). However, differing from our work, they studied the influence of different ordering between rules and facts, e.g., their experiments on modus tollens problems show that presenting negation statements (not $Q$) before rules (if $P$, then $Q$) improves the performance over the reverse order. On the other hand, our work focuses on modus ponens problems that are easier for both humans and LLMs, and we show that the LLM performance is still quite sensitive to the ordering of the premises.\\n\\nOrder effect of language models.\\n\\nSome prior works show that language models are able to understand permuted texts to some extent, i.e., after a random permutation of words, models usually maintain reasonable performance (Abdou et al., 2022; Sinha et al., 2020). Moreover, Cao et al. (2023) show that even when a large fraction of words are scrambled, GPT-4 still achieves decent performance on several reasoning benchmarks. In contrast to permuted texts in these works that are typically unnatural and nonsensical, our premise order permutations do not alter the semantic meaning and remain syntactically valid (we manually verify this). Nevertheless, we demonstrate that LLM reasoning performance is highly brittle to the ordering of the premises. For long-digit addition, prior works demonstrate that reversing the input numbers is a key to achieve better length generalization performance (Lee et al., 2023; Zhou et al., 2023; 2024). Specifically, by reversing the input numbers so that the least significant digit is presented first, the Transformer learns a simpler way of performing addition, where the model only needs to perform computation with the corresponding digits of operands and the carry-on digit at each step, without the need of looking at other digits. This approach enables the Transformer to better perform addition when trained from scratch, which also aligns with our finding: after reversing the input numbers, the premise order (i.e., orders of digits) follows the right ordering of performing long-digit addition, thus enables Transformers to better learn the task.\\n\\n5. Conclusion\\n\\nIn this work, we show that the premise order significantly affects LLMs' performance on reasoning tasks, even when the premise order does not change the underlying task itself. Our comprehensive evaluation demonstrates that LLM tendencies resemble human preference w.r.t. premise order, i.e., LLMs achieve the best performance when the premise order follows the intermediate reasoning steps to solve the problem. Conversely, LLMs face difficulties when the reasoning problem requires the model to read the problem description back-and-forth, resulting in a performance drop of over 30%. We further extend the study to mathematical reasoning and present the R-GSM benchmark, and again experimentally confirm the ordering effect.\\n\\nWhile humans also have a preference of premise orders for reasoning problems, LLMs are much more susceptible to such ordering effects. We can attempt to ascribe the premise order effect to several candidate factors, such as the auto-regressive model design, training objectives, and training data mixture. However, we leave proposing theoretical explanations of this limitation and developing new techniques towards addressing the premise order effect as future work.\\n\\nImpact Statement\\n\\nOur work presents an empirical study on weaknesses of LLM reasoning. One limitation of our work is that we did not conduct a rigorous human study on our benchmarks, and thus the LLM performance is not directly comparable to humans. This work can inspire future studies on the comparison of human and LLM reasoning.\\n\\nWhile our work has not yet proposed a solution to address the premise order effect of LLMs, this work helps the community better understand the capabilities of existing LLMs, and thus better use them in practice. It may also motivate the community to identify the root cause of these pitfalls and address them, leading to the development of LLMs with stronger reasoning abilities.\"}"}
{"id": "4zAHgkiCQg", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We would like to thank Chen Liang and Dale Schuurmans for helpful discussion and feedback.\\n\\nReferences\\n\\nAbdou, M., Ravishankar, V., Kulmizev, A., and S\u00f8gaard, A. Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6907\u20136919, 2022.\\n\\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\\n\\nBerglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A. C., Korbak, T., and Evans, O. The reversal curse: Llms trained on \\\"a is b\\\" fail to learn \\\"b is a\\\". arXiv preprint arXiv:2309.12288, 2023.\\n\\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\\n\\nCao, Q., Kojima, T., Matsuo, Y., and Iwasawa, Y. Unnatural error correction: Gpt-4 can almost perfectly handle unnatural scrambled text. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 8898\u20138913, 2023.\\n\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\\n\\nCicirello, V. A. Kendall tau sequence distance: Extending kendall tau from ranks to sequences. arXiv preprint arXiv:1905.02752, 2019.\\n\\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\\n\\nDekeyser, M., Schroyens, W., Schaeken, W., Spitaels, O., and d'Ydewalle, G. Preferred premise order in propositional reasoning: Semantic informativeness and coreference. Deductive reasoning and strategies, pp. 73\u201395, 2000.\\n\\nFerreira, D. and Freitas, A. Premise selection in natural language mathematical texts. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7365\u20137374, 2020.\\n\\nGemini. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\n\\nGirotto, V., Mazzocco, A., and Tasso, A. The effect of premise order in conditional reasoning: A test of the mental model theory. Cognition, 63(1):1\u201328, 1997.\\n\\nGoogle. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.\\n\\nHagendorff, T., Fabi, S., and Kosinski, M. Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. Nature Computational Science, 3(10):833\u2013838, 2023.\\n\\nHan, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Benson, L., Sun, L., Zubova, E., Qiao, Y., Burtell, M., et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022.\\n\\nHendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\\n\\nIrving, G., Szegedy, C., Alemi, A. A., \u00c9\u00e9n, N., Chollet, F., and Urban, J. Deepmath-deep sequence models for premise selection. Advances in neural information processing systems, 29, 2016.\\n\\nJones, E. and Steinhardt, J. Capturing failures of large language models via human cognitive biases. Advances in Neural Information Processing Systems, 35:11785\u201311799, 2022.\\n\\nLee, N., Sreenivasan, K., Lee, J. D., Lee, K., and Papailiopoulos, D. Teaching arithmetic to small transformers. arXiv preprint arXiv:2307.03381, 2023.\\n\\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., et al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022.\\n\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157\u2013173, 2024.\\n\\nMcCoy, R. T., Yao, S., Friedman, D., Hardy, M., and Griffiths, T. L. Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638, 2023.\\n\\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\"}"}
{"id": "4zAHgkiCQg", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Order Matters in Reasoning with Large Language Models\\n\\nSaparov, A. and He, H. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.\\n\\nSaparov, A., Pang, R. Y., Padmakumar, V., Joshi, N., Kazemi, S. M., Kim, N., and He, H. Testing the general deductive reasoning capacity of large language models using ood examples. arXiv preprint arXiv:2305.15269, 2023.\\n\\nSen, P. K. Estimates of the regression coefficient based on kendall\u2019s tau. Journal of the American statistical association, 63(324):1379\u20131389, 1968.\\n\\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Sch\u00e4rli, N., and Zhou, D. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 31210\u201331227. PMLR, 2023.\\n\\nSinha, K., Parthasarathi, P., Pineau, J., and Williams, A. Unnatural language inference. arXiv preprint arXiv:2101.00010, 2020.\\n\\nWan, Y., Wang, W., Yang, Y., Yuan, Y., Huang, J.-t., He, P., Jiao, W., and Lyu, M. R. A & b== b & a: Triggering logical reasoning failures in large language models. arXiv preprint arXiv:2401.00757, 2024.\\n\\nWang, M., Tang, Y., Wang, J., and Deng, J. Premise selection for theorem proving by deep graph embedding. Advances in neural information processing systems, 30, 2017.\\n\\nWang, P., Li, L., Chen, L., Cai, Z., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., and Sui, Z. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837, 2022.\\n\\nXu, F., Lin, Q., Han, J., Zhao, T., Liu, J., and Cambria, E. Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. arXiv preprint arXiv:2306.09841, 2023.\\n\\nYan, S., Shen, C., Liu, J., and Ye, J. Concise and organized perception facilitates large language models for deductive reasoning. arXiv preprint arXiv:2310.03309, 2023.\\n\\nZhang, H., Li, L. H., Meng, T., Chang, K.-W., and Broeck, G. V. d. On the paradox of learning to reason from data. arXiv preprint arXiv:2205.11502, 2022.\\n\\nZhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran, P. What algorithms can transformers learn? a study in length generalization. arXiv preprint arXiv:2310.16028, 2023.\\n\\nZhou, Y., Alon, U., Chen, X., Wang, X., Agarwal, R., and Zhou, D. Transformers can achieve length generalization but not robustly. arXiv preprint arXiv:2402.09371, 2024.\\n\\nZhu, Z., Xue, Y., Chen, X., Zhou, D., Tang, J., Schuurmans, D., and Dai, H. Large language models can learn rules. arXiv preprint arXiv:2310.07064, 2023.\"}"}
{"id": "4zAHgkiCQg", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5 presents the statistics of our R-GSM benchmark.\\n\\n| # Steps | # Problems |\\n|---------|------------|\\n| 2       | 20         |\\n| 3       | 43         |\\n| 4       | 65         |\\n| 5       | 43         |\\n| 6       | 23         |\\n| 7       | 15         |\\n| 8       | 11         |\\n\\n(a) # Sentences # Problems\\n| 5       | 133        |\\n| 6       | 65         |\\n| 7       | 19         |\\n| 8       | 3          |\\n\\n(b) Table 5. Statistics of the R-GSM dataset, with 220 problems in total: (a) breakdown on the number of reasoning steps; (b) breakdown on the number of sentences in the questions.\\n\\nB. Logical Reasoning Examples\\n\\nFigure 10 presents common classes of errors \u2014 hallucinated rules and facts \u2014 by LLMs while solving our logical reasoning benchmark.\\n\\nFigure 11 presents a sample logical reasoning problem with premise orders of different \\\\( \\\\tau \\\\) values. We can see that the rules become less ordered when the absolute value of \\\\( \\\\tau \\\\) decreases.\\n\\nC. R-GSM Examples\\n\\nIn this section, we present more examples of LLM predictions on R-GSM problems. Figure 12 presents a failure case of a probability problem, which falls into the \u201cOthers\u201d category in the error analysis (Table 4). Specifically, in the reordered problem, after the LLM reads the sentence about the scenario with a normal teacher coming in, the LLM immediately attempts to compute the probability that Marcus has to turn in his homework, ignoring that the LLM needs to compute the probability that a normal teacher will come in using the next sentence.\\n\\nFigures 13 shows another wrong prediction of GPT-4 Turbo, where the error pattern is analogous to rule hallucination in logical reasoning evaluation. Interestingly, when moving the sentence about yellow cars preceding to the sentence about quantities of blue and green cars, GPT-4 Turbo starts to hallucinate the relationship between the number of yellow cars and the number of blue cars, resulting in insufficient information to correctly solve the problem.\\n\\nFigures 14 and 15 present examples where both the original and reordered problems are correctly solved by LLMs in our evaluation. In both original problems, the succeeding sentences do not strongly depend on the preceding sentences.\\n\\nD. Discussion: Does Logical Reasoning Suffer from the Lost-in-the-middle Issue?\\n\\nLiu et al. (2024) demonstrate that when the input context becomes long, LLMs might suffer from the lost-in-the-middle issue: the model performance significantly degrades when relevant information to solve the task is in the middle of the input, instead of at the beginning or the end. Therefore, when given distracting rules for logical reasoning, another potential factor that might affect the model performance is the position of relevant rules in the model input.\"}"}
