{"id": "QRjTDhCIO8", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge\\n\\nC.1. Data Curation\\n\\nCrystal apo structure for PDBBind time split test set. The APO structures are retrieved from the Protein Data Bank (PDB) (Burley et al., 2021) by first extracting the sequence of HOLO structures and conducting a BLAST (Ye et al., 2006) search against the PDB database. Each hit protein is then structurally aligned to the holo-structure using PyMOL (DeLano et al., 2002), focusing on the superposition of corresponding C\\\\text{\\\\textalpha} atoms of amino acid residues. Post alignment, structures are assessed for quality and relevance similar to ApoBind (Aggarwal et al., 2021): those with a backbone C\\\\text{\\\\textalpha} Root Mean Square Deviation (RMSD) exceeding 15 \\\\text{\\textdegree}A, or those showing less than 80% sequence identity or coverage compared to the full protein sequence, are rejected. Additionally, any hit structure with ligands located within 4 \\\\text{\\textdegree}A of any atoms in the crystal structure pose of the complex is also excluded.\\n\\nCrystal CrossDock structures. The targets are from DUD-E (Mysinger et al., 2012), a dataset designed for the unbiased virtual screening task. Seven targets are selected, AKT1, AMPC, CXCR4, GCR, HIVPR, HIVRT, and KIF11, which are representatives of kinase, other enzymes, G protein-coupled receptors, nuclear receptors, protease, other enzymes, and miscellaneous classes. Within each target, we retrieve different protein-ligand structures from PDB using a similar method and conditions of the construction of crystal apo time split test set.\\n\\nFor search-based method, we use their official software suits and for deep-learning-based baselines, we use their official implementation and weights. To develop diffdock (pocket), we adopt the same pocket truncation method as our RE-DOCK and full-atom graph representation as it used for their confidence model. Other settings keep the same with the original diffdock.\\n\\nD. More details about the bridge\\n\\nThe space of the pocket-ligand poses $R_3(m + n)$ ($m$ and $n$ are the number of pocket sidechain and ligand atoms) is huge and encompasses far more degrees of freedom than are relevant in molecular docking. The complex flexibility lies almost entirely in the torsion angles at rotatable bonds (Corso et al., 2023); thus, we incorporate geometric prior, which is known in advance (e.g., fixed bond lengths, angles, and essentially rigid small rings) with a seed (randomized only in torsion angles) pocket sidechain and ligand conformations $C$ in isolation, and model the pocket-ligand complex pose in an $(m + 6)$-dimensional submanifold $M_C$, where $m$ is the number of rotatable bonds and six additional degrees of freedom come from rototranslations relative to the fixed protein backbone.\\n\\nWhile diffusion generative models have been applied to molecular docking, existing approaches are ill-suited for flexible and realistic docking scenarios, where we need to co-model the flexibility of pocket sidechains and ligands as well as their interactions. To develop Re-Dock, we recognize the dynamic and interactive nature of docking with induced fit; thus, we use the diffusion bridge model to inject physical priors of molecular interactions into the generative process. To simplify the modeling process and incorporate geometric prior (e.g. fixed bond lengths, bond angles between atoms), we follow the successful experience in the field of conformational generation and develop geometry-based generative models.\\n\\nWe construct such a bridge with prior of the atom interaction potentials regarding how the diffusion process should look like for generating each given data point over the product space of geometries: global rotation/translation of ligands and local molecular torsion/residue sidechain angles.\\n\\nAs the induced-fit process is driven by the inherent interaction energies and generative docking needs a confidence model for ranking predicted poses (Corso et al., 2023), we include learnable interaction energy with an additional energy head to co-model the interaction energy and conformation.\\n\\nWe use prior energy terms in Amber (Case et al., 2021), including: 1. The Lennard-Jones (LJ) energy $E_{LJ}(x) = \\\\sum_{i \\\\neq j} e(x_{ri} - x_{rj})$ and $e(\\\\ell) = (\\\\sigma/\\\\ell)^{12} - 2(\\\\sigma/\\\\ell)^{6}$. The parameter $\\\\sigma$ is an approximation for average nucleus distance. 2. The nuclei-nuclei repulsion (Coulomb) electromagnetic potential energy $E_{\\\\text{Coulomb}} = \\\\sum_{ij} \\\\kappa q_{\\\\hat{x}_i} q_{\\\\hat{x}_j} / x_{ri} - x_{rj}$, where $\\\\kappa$ is Coulomb constant and $q_{r}$ denotes the point charge of atom of type $r$, which depends on the number of protons.\\n\\nWe use these energies based on the same threshold with the $E_{\\\\text{clash}}$ in Equation. 10 as the ligand and sidechains are close enough.\\n\\nAs the energy-to-geometry map is derived from rigid-body mechanics, it is naturally equivariant to roto-translations (i.e. rotations and translations). As the calculation of prior drift on torsion manifold $SO(2)$, where $M = m_{\\\\text{lig}} + m_{\\\\text{sc}}$ is the number of rotatable bonds, is complex and requires extensive computations, we omit it for simplicity. The torsion force can be described as the difference between the torques applied on each side of the rotatable bonds.\"}"}
{"id": "QRjTDhCIO8", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We provide generalized results with predicted pocket with p2rank (Kriv\u00e1\u0148 & Hoksza, 2018) in Table 6.\\n\\n| Method            | Top-1 RMSD | Top-5 RMSD | Average |\\n|-------------------|------------|------------|---------|\\n| GNINA             | 34.6       | 62.6       | 3.3     |\\n| SMINA             | 21.5       | 50.1       | 5.0     |\\n| LeDock            | 24.0       | 48.6       | 5.1     |\\n| SurfLex           | 6.1        | 31.9       | 6.7     |\\n| QVina             | 21.3       | 50.9       | 4.9     |\\n| GNINA-Flex        | 29.4       | 63.7       | 3.5     |\\n| SMINA-Flex        | 20.5       | 46.2       | 5.3     |\\n| FlexPose          | 42.0       | -          | -       |\\n| DiffDock(pocket, 40) | 51.8       | 75.3       | 2.0     |\\n| ReDock            | 51.9       | 77.4       | 2.0     |\\n| ReDock (40)       | 53.9       | 80.3       | 1.8     |\\n| ReDock (p2rank, 40) | 40.2       | 65.7       | 3.1     |\\n\\nTable 6. Performance of flexible redocking on the PBDBind test set (bound, holo-crystal proteins) and its corresponding ESMFold predicted Apo structures (unbound). The best metrics are marked by **bold**. In parenthesis, we specify the number of poses sampled from the generative model. It is worth noting that only our Re-Dock has no access to the holo-crystal proteins. p2rank denotes we use the predicted pocket center with p2rank and a pocket radius of 15 \u02daA.\"}"}
{"id": "QRjTDhCIO8", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Performance of sidechain generation on PBDBind test set and its corresponding ESMFold predicted apo structures. The best metrics are marked by bold. In parenthesis, we specify the number of poses sampled from the generative model. The results suggest our superior sidechain generation performance.\\n\\n| Method                  | Top-1 SC-RMSD (%) | Top-5 SC-RMSD (%) |\\n|-------------------------|-------------------|-------------------|\\n| GNINA-Flex 3.3          | 71.9              | 7.7               |\\n| SMINA-Flex 2.0          | 63.8              | 8.3               |\\n| Re-Dock (10)            | 86.8              | 95.4              |\\n| Re-Dock (40)            | 87.5              | 100.0             |\\n\\nTable 3. Ablation study for designed components on flexible re-docking task in PDBBind test set. The best metrics are marked by bold. In parenthesis, we specify the number of poses sampled from the generative model. w/o Sidechain Generation refers to removing the sidechain torsion sampling process in inference. w/o Prior Energy means training a vanilla geometric bridge without prior drift. Interaction prior guided sampling adds classifier guidance to DiffDock(pocket) using Energy-to-Geometry mapping sidechain poses to better understand interactions. Similar to the ligand pose prediction, we formulate this task to assess generated samples. d) Cross-dock. It's an important real-world task for drug discovery (Zhang et al., 2023a). The task involves predicting the ligand poses using pocket structures that are bound with different ligands, which can be biased and misleading.\\n\\nDatasets. We conduct training, flexible redocking, and sidechain pose prediction evaluation on the PDBBind v2020 dataset (Liu et al., 2017) with the time-based dataset split following previous works (Corso et al., 2023; Pei et al., 2023). For apo-docking, we use the ESMFold on the test set of PDBBind to obtain predicted apo structures. We also curate corresponding apo crystal structures with the PDBBind test via searching against Protein Data Bank (Burley et al., 2021). For cross-dock, we collect complex structures bound with different ligands for 60 important drug targets and select 7 representative targets for evaluation, resulting in over 10000 complexes.\\n\\nMetrics. We use Ligand RMSD as the evaluation metric, which calculates the root-mean-square deviation between the predicted and the holo crystal ligand atomic Cartesian coordinates. Following (Corso et al., 2023), we report the percentage of predictions with RMSD below 2 and 5, the median RMSD (Med.) and average runtime per complex. For sidechain pose prediction, we use a similar metric named SC-RMSD and report results with different thresholds of 1 and 2. For generated structures, the perfect match with the crystal structures is unrealistic, as they differ in bond lengths. To compensate for this fact, we use a relative measure that compares the SC-RMSD before and after the prediction.\\n\\nBaselines. We compare Re-Dock with state-of-the-art search-based methods SMINA (Koes et al., 2013), GNINA (McNutt et al., 2021), LeDock (Zhao & Caflisch, 2013), SurfLex (Jain, 2003), and Qvina (Alhossary et al., 2015), and the recent deep learning method DiffDock (Corso et al., 2023). We implemented and retrained DiffDock(pocket) with pockets as input for comparison. We also include flexible docking baselines GNINA-Flex, SMINA-Flex, and FlexPose (Dong et al., 2023). For generative models, we sample various poses per complex and take the most accurate pose out of 1 or 5 highest-ranked predictions (Top-1 or Top-5) according to the confidence model. Extensive details about the experimental setup including data curation and training can be found in the Appendix. C.\"}"}
{"id": "QRjTDhCIO8", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Performance comparison on apo crystal re-docking and a more challenging task, cross-dock. The best metrics are marked by bold and we only generate 10 samples per complex for cross-dock to reduce computational burden. Methods with * only have Top-1 results. In addition to predicted approximated apo-structures, we provide promising results on apo crystal structures which are frequently used in real-world applications. Cross-dock requires the docking methods to predict correct ligand poses based on pocket structure bound with another ligand, which can be misleading and thus challenging. The results demonstrate the effectiveness and advantages of explicit modeling of flexible sidechains and our geometric prior bridge.\\n\\n\\nTable 5. Flexible redocking results on PoseBuster benchmark. PB-Valid means the generated docking samples pass the PoseBuster Valid check and are physically valid (Buttenschoen et al., 2024). The best results among deep learning-based methods are bold. Results demonstrate the effectiveness of Re-Dock for generating realistic (i.e., physically valid) samples.\"}"}
{"id": "QRjTDhCIO8", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Conclusion\\n\\nWe introduce an under-explored docking task, flexible docking, which better aligns with realistic applications. Tailored to this task, we present Re-Dock, a geometric prior bridge generative model with the energy-to-geometry mapping inspired by the Newton-Euler equation in mechanics. We extend the prior bridge to the geometric manifold and propose a novel and general module that bridges the explicit interaction energy with implicit geometries to design geometric prior drifts and co-model the energy and poses. With explicit generative modeling of ligands and pocket sidechain poses, Re-Dock simulates the induced fitting process with a probabilistic one. The results on the proposed thoughtful benchmarks demonstrate our efficiency and effectiveness.\\n\\nLimitations still exist, including insufficient exploration of pocket prediction and end-to-end blind docking.\\n\\nAcknowledgements\\n\\nThis work was supported by the Science & Technology Innovation 2030 Major Program Project No. 2021ZD0150100, National Natural Science Foundation of China Project No. U21A20427, Project No. WU2022A009 from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University, and Project No. WU2023C019 from the Westlake University Industries of the Future Research. Besides, we thank the help of Dr. Tailin Wu, and his great efforts in his deep insights into cutting-edge issues, the guidance provided in the rebuttal, and the funding that supplied us with the necessary equipment for our research. Finally, we thank the Westlake University HPC Center for providing part of the computational resources.\\n\\nImpact Statement\\n\\nUnlike previous methods, Re-Dock is designed and trained for flexible docking and thus better generalizes to realistic applications with real-world drug targets and realistic results. It integrates binding energy prediction and can offer great value for many realistic drug discovery and protein engineering pipelines as well as disease understanding. With a better trade-off of efficiency and effectiveness (for instance, choosing a proper number of samples generated per complex and sampling steps would help), Re-Dock can be applied for both large-scale virtual screening and deeper analysis of protein-ligand interaction. While there exists the potential risk that docking methods could be misused to develop harmful drugs, it's important to note that drug development is subject to stringent oversight globally. This rigorous regulatory environment ensures that such misuses can be effectively managed and controlled.\\n\\nReferences\\n\\nAggarwal, R., Gupta, A., and Priyakumar, U. Apobind: a dataset of ligand unbound protein conformations for machine learning applications in de novo drug design. arXiv preprint arXiv:2108.09926, 2021.\\n\\nAlhossary, A., Handoko, S. D., Mu, Y., and Kwoh, C.-K. Fast, accurate, and reliable molecular docking with quickvina 2. Bioinformatics, 31(13):2214\u20132216, 2015.\\n\\nBrocidiacono, M., Popov, K. I., Koes, D. R., and Tropsha, A. Plantain: Diffusion-inspired pose score minimization for fast and accurate molecular docking. ArXiv, 2023.\\n\\nBurley, S. K., Bhikadiya, C., Bi, C., Bittrich, S., Chen, L., Crichlow, G. V., Christie, C. H., Dalenberg, K., Di Costanzo, L., Duarte, J. M., et al. Rcsb protein data bank: powerful new tools for exploring 3d structures of biological macromolecules for basic and applied research in fundamental biology, biomedicine, biotechnology, bioengineering and energy sciences. Nucleic acids research, 49(D1):D437\u2013D451, 2021.\\n\\nButtenschoen, M., Morris, G. M., and Deane, C. M. Pose-busters: Ai-based docking methods fail to generate physically valid poses or generalise to novel sequences. Chemical Science, 2024.\\n\\nCase, D. A., Aktulga, H. M., Belfon, K., Ben-Shalom, I., Brozell, S. R., Cerutti, D. S., Cheatham III, T. E., Cruzeiro, V. W. D., Darden, T. A., Duke, R. E., et al. Amber 2021. University of California, San Francisco, 2021.\\n\\nChen, R., Li, L., and Weng, Z. Zdock: an initial-stage protein-docking algorithm. Proteins: Structure, Function, and Bioinformatics, 52(1):80\u201387, 2003.\\n\\nClark, J. J., Benson, M. L., Smith, R. D., and Carlson, H. A. Inherent versus induced protein flexibility: comparisons within and between apo and holo structures. PLoS computational biology, 15(1):e1006705, 2019.\\n\\nCorso, G., Stark, H., Jing, B., Barzilay, R., and Jaakkola, T. Diffdock: Diffusion steps, twists, and turns for molecular docking. International Conference on Learning Representations (ICLR), 2023.\\n\\nDe Bortoli, V., Thornton, J., Heng, J., and Doucet, A. Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 17695\u201317709. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/file/9\"}"}
{"id": "QRjTDhCIO8", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge\\n\\nde Veer, S. J., White, A. M., and Craik, D. J. Sunflower trypsin inhibitor-1 (sfti-1): sowing seeds in the fields of chemistry and biology. Angewandte Chemie International Edition, 60(15):8050\u20138071, 2021.\\n\\nDe Vries, S. J., Van Dijk, M., and Bonvin, A. M. The haddock web server for data-driven biomolecular docking. Nature protocols, 5(5):883\u2013897, 2010.\\n\\nDeLano, W. L. et al. Pymol: An open-source molecular graphics tool. CCP4 Newsl. Protein Crystallogr, 40(1):82\u201392, 2002.\\n\\nDhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.\\n\\nDong, T., Yang, Z., Zhou, J., and Chen, C. Y.-C. Equivariant flexible modeling of the protein\u2013ligand binding pose with geometric deep learning. Journal of Chemical Theory and Computation, 19(22):8446\u20138459, 2023.\\n\\nEberhardt, J., Santos-Martins, D., Tillack, A. F., and Forli, S. Autodock vina 1.2.0: New docking methods, expanded force field, and python bindings. Journal of Chemical Information and Modeling, 61(8):3891\u20133898, 2021. doi: 10.1021/acs.jcim.1c00203. URL https://doi.org/10.1021/acs.jcim.1c00203. PMID: 34278794.\\n\\nEvans, R., O'Neill, M., Pritzel, A., Antropova, N., Senior, A., Green, T., \u02c7Z\u00b4\u0131dek, A., Bates, R., Blackwell, S., Yim, J., et al. Protein complex prediction with alphafold-multimer. biorxiv, pp. 2021\u201310, 2021.\\n\\nGallus, A. S. and Hirsh, J. Prevention of venous thromboembolism. In Seminars in thrombosis and hemostasis, volume 2, pp. 232\u2013290. Copyright \u00a9 1976 by Thieme Medical Publishers, Inc., 1976.\\n\\nGanea, O.-E., Huang, X., Bunne, C., Bian, Y., Barzilay, R., Jaakkola, T., and Krause, A. Independent se (3)-equivariant models for end-to-end rigid protein docking. arXiv preprint arXiv:2111.07786, 2021.\\n\\nGeiger, M. and Smidt, T. e3nn: Euclidean neural networks. arXiv preprint arXiv:2207.09453, 2022.\\n\\nGuan, J., Peng, X., Jiang, P., Luo, Y., Peng, J., and Ma, J. Linkernet: Fragment poses and linker co-design with 3d equivariant diffusion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=6EaLIw3W7c.\\n\\nHalgren, T. A., Murphy, R. B., Friesner, R. A., Beard, H. S., Frye, L. L., Pollard, W. T., and Banks, J. L. Glide: a new approach for rapid, accurate docking and scoring. 2. enrichment factors in database screening. Journal of medicinal chemistry, 47(7):1750\u20131759, 2004.\\n\\nHartmann, C., Antes, I., and Lengauer, T. Docking and scoring with alternative side-chain conformations. Proteins: Structure, Function, and Bioinformatics, 74(3):712\u2013726, 2009.\\n\\nHeng, J., Bortoli, V. D., Doucet, A., and Thornton, J. Simulating diffusion bridges with score matching, 2022.\\n\\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.\\n\\nHu, B., Xia, J., Zheng, J., Tan, C., Huang, Y., Xu, Y., and Li, S. Z. Protein language models and structure prediction: Connection and progression, 2022.\\n\\nHu, B., Tan, C., Xia, J., Zheng, J., Huang, Y., Wu, L., Liu, Y., Xu, Y., and Li, S. Z. Learning complete protein representation by deep coupling of sequence and structure. bioRxiv, 2023. doi: 10.1101/2023.07.05.547769. URL https://www.biorxiv.org/content/early/2023/07/07/2023.07.05.547769.\\n\\nHuang, Y., Li, S., Su, J., Wu, L., Zhang, O., Lin, H., Qi, J., Liu, Z., Gao, Z., Liu, Y., et al. Protein 3d graph structure learning for robust structure-based protein property prediction. arXiv preprint arXiv:2310.11466, 2023a.\\n\\nHuang, Y., Wu, L., Lin, H., Zheng, J., Wang, G., and Li, S. Z. Data-efficient protein 3d geometric pretraining via refinement of diffused protein structure decoy, 2023b.\\n\\nJain, A. N. Surflex: fully automatic flexible molecular docking using a molecular similarity-based search engine. Journal of medicinal chemistry, 46(4):499\u2013511, 2003.\\n\\nJin, W., Sarkizova, S., Chen, X., Hacohen, N., and Uhler, C. Unsupervised protein-ligand binding energy prediction via neural euler's rotation equation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=46gYakmj4e.\\n\\nJing, B., Corso, G., Chang, J., Barzilay, R., and Jaakkola, T. Torsional diffusion for molecular conformer generation. arXiv preprint arXiv:2206.01729, 2022.\\n\\nKoes, D. R., Baumgartner, M. P., and Camacho, C. J. Lessons learned in empirical scoring with smina from the csar 2011 benchmarking exercise. Journal of chemical information and modeling, 53(8):1893\u20131904, 2013.\"}"}
{"id": "QRjTDhCIO8", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge\\n\\nKong, X., Huang, W., and Liu, Y. Conditional antibody design as 3d equivariant graph translation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=LFHFQbjxIiP.\\n\\nKrishna, R., Wang, J., Ahern, W., Sturmfels, P., Venkatesh, P., Kalvet, I., Lee, G. R., Morey-Burrows, F. S., Anishchenko, I., Humphreys, I. R., et al. Generalized biomolecular modeling and design with rosettafold all-atom. bioRxiv, pp. 2023\u201310, 2023.\\n\\nKriv\u00e1k, R. and Hoksza, D. P2rank: machine learning based tool for rapid and accurate prediction of ligand binding sites from protein structure. Journal of cheminformatics, 10:1\u201312, 2018.\\n\\nLeach, A., Schmon, S. M., Degiacomi, M. T., and Willcocks, C. G. Denoising diffusion probabilistic models on so (3) for rotational alignment. In ICLR 2022 Workshop on Geometrical and Topological Representation Learning, 2022.\\n\\nLiao, Z., You, R., Huang, X., Yao, X., Huang, T., and Zhu, S. Deepdock: enhancing ligand-protein interaction prediction by a combination of ligand and structure information. In 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pp. 311\u2013317. IEEE, 2019.\\n\\nLin, H., Huang, Y., Liu, M., Li, X., Ji, S., and Li, S. Z. Diffbp: Generative diffusion of 3d molecules for target protein binding. arXiv preprint arXiv:2211.11214, 2022a.\\n\\nLin, H., Huang, Y., Zhang, H., Wu, L., Li, S., Chen, Z., and Li, S. Z. Functional-group-based diffusion for pocket-specific molecule generation and elaboration. arXiv preprint arXiv:2306.13769, 2023a.\\n\\nLin, H., Wu, L., Xu, Y., Huang, Y., Li, S., Zhao, G., and Li, S. Z. Non-equispaced fourier neural solvers for pdes, 2023b.\\n\\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., dos Santos Costa, A., Fazel-Zarandi, M., Sercu, T., Candido, S., et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. BioRxiv, 2022:500902, 2022b.\\n\\nLin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., dos Santos Costa, A., Fazel-Zarandi, M., Sercu, T., Candido, S., et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022c.\\n\\nLiu, X., Wu, L., Ye, M., and Liu, Q. Let us build bridges: Understanding and extending diffusion generative models, 2022.\\n\\nLiu, Z., Su, M., Han, L., Liu, J., Yang, Q., Li, Y., and Wang, R. Forging the basis for developing protein\u2013ligand interaction scoring functions. Accounts of chemical research, 50(2):302\u2013309, 2017.\\n\\nLu, W., Wu, Q., Zhang, J., Rao, J., Li, C., and Zheng, S. Tankbind: Trigonometry-aware neural networks for drug-protein binding structure prediction. bioRxiv, 2022. doi: 10.1101/2022.06.06.495043. URL https://www.biorxiv.org/content/early/2022/10/25/2022.06.06.495043.\\n\\nMcNutt, A. T., Francoeur, P., Aggarwal, R., Masuda, T., Meli, R., Ragoza, M., Sunseri, J., and Koes, D. R. Gnina 1.0: molecular docking with deep learning. Journal of cheminformatics, 13(1):1\u201320, 2021.\\n\\nMcPartlon, M. and Xu, J. Deep learning for flexible and site-specific protein docking and design. bioRxiv, pp. 2023\u201304, 2023.\\n\\nMysinger, M. M., Carchia, M., Irwin, J. J., and Shoichet, B. K. Directory of useful decoys, enhanced (dud-e): better ligands and decoys for better benchmarking. Journal of medicinal chemistry, 55(14):6582\u20136594, 2012.\\n\\nPei, Q., Gao, K., Wu, L., Zhu, J., Xia, Y., Xie, S., Qin, T., He, K., Liu, T.-Y., and Yan, R. FABind: Fast and accurate protein-ligand binding. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=PnWakgg1RL.\\n\\nPeluchetti, S. Non-denoising forward-time diffusions. arxiv, 2021.\\n\\nPlainer, M., Toth, M., Dobers, S., Stark, H., Corso, G., Marquet, C., and Barzilay, R. Diffdock-pocket: Diffusion for pocket-level docking with sidechain flexibility. In NeurIPS 2023 Workshop on New Frontiers of AI for Drug Discovery and Development, 2023.\\n\\nQiao, Z., Nie, W., Vahdat, A., Miller III, T. F., and Anandkumar, A. State-specific protein-ligand complex structure prediction with a multi-scale deep generative model. Preprint at arXiv https://doi.org/10.48550/arXiv, 2209, 2023.\\n\\nSahu, D., Rathor, L. S., Dwivedi, S. D., Shah, K., Chauhan, N. S., Singh, M. R., and Singh, D. A review on molecular docking as an interpretative tool for molecular targets in disease management. ASSAY and Drug Development Technologies, 22(1):40\u201350, 2024.\\n\\nSherman, W., Day, T., Jacobson, M. P., Friesner, R. A., and Farid, R. Novel procedure for modeling ligand/receptor induced fit effects. Journal of medicinal chemistry, 49(2):534\u2013553, 2006.\"}"}
{"id": "QRjTDhCIO8", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "QRjTDhCIO8", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Protein Protein Docking\\n\\nProtein-protein docking aims to predict the combined structure of two proteins from their individual shapes. This process, similar to molecular docking, typically assumes that proteins do not change shape when they bind, limiting their movements to rotations and translations in three dimensions. Searching-based protein-protein docking methods often start with a basic initialization and refine it gradually (Yan et al., 2017; Chen et al., 2003; De Vries et al., 2010). Furthermore, some use template-based modeling to align a target protein with known structures (Vakser, 2014). Deep learning techniques (Wu et al., 2023a; Lin et al., 2023b; Wu et al., 2023b; Hu et al., 2023; Wu et al., 2022c; Zheng et al., 2023a) in this field are divided into two main types: one aims for immediate predictions, while the other focuses on refining predictions step by step. Research like (Ganea et al., 2021) targets predicting the precise adjustments needed for binding. Another study integrates physical principles into an energy-based model to predict the 3D structure of protein complexes (Sverrisson et al., 2022). Techniques that refine their predictions progressively, such as ALPHAFOLD-MULTIMER (Evans et al., 2021), fold multiple proteins together based on their sequences and related proteins. In parallel with this work, DOCKGPT (McPartlon & Xu, 2023) offers a novel approach for precise and adaptable protein docking.\\n\\nFlexible Docking\\n\\nFlexible docking is an open challenge for developing docking methods as it includes much more degrees of freedom than rigid docking. Methods often adopt effective assumptions for restricting the degrees of freedom, such as only considering the flexible sidechain conformations (Alhossary et al., 2015; Taylor et al., 2003; Hartmann et al., 2009). (Zhang et al., 2023b) develop generative models for sidechain packing (that's, without ligands), and (Plainer et al., 2023) explores pocket-specific docking with receptor sidechain flexibility. Different from previous diffusion-based methods for sidechain flexibility, we explore a novel generative framework, incorporate interaction prior for sufficient protein-ligand interaction modeling, and co-model the poses and binding energy. Without sidechain matching and additional training on ESMFold-generated structures, we achieve comparable or better results than (Plainer et al., 2023).\\n\\nB. Model Architecture and Training\\n\\nB.1. Model Architecture\\n\\nNotations.\\n\\nFor each protein-ligand complex, we represent it as a heterogeneous graph with both coarse and fine grain modeling\\n\\n\\\\[ G = (V, E) \\\\]\\n\\nHere, the components \\\\( V \\\\) correspond to the nodes (i.e., the heavy atoms) of the ligand and protein (atom-level). \\\\( V \\\\) represent the nodes (i.e., the residues) of the protein (residue-level); \\\\( E \\\\) separately contain internal edges within each component and external edges across components. To be specific, each node in \\\\( V \\\\), i.e., \\\\( v = (h, x) \\\\) is represented as a node embedding vector \\\\( h \\\\) initialized with trainable type embedding vector and ESM-2 embedding (Lin et al., 2022c) following (Corso et al., 2023), and a coordinate vector \\\\( x \\\\) of the corresponding (heavy) atoms (atom-level) or the alpha carbon of the residue backbone (residue-level). To be aware of the general gesture of ligand for better generation of the binding pose, we add a global node that connects to all atoms in the component coordinated at the mean of all coordinates.\\n\\nEach edge in \\\\( E \\\\), i.e., \\\\( e = (h, x) \\\\) consists of an edge embedding of node \\\\((i, j)\\\\) initialized with radial basis embedding of edge length and edge vector \\\\( x = x_j - x_i \\\\). For explicit and accurate modeling of protein-ligand interactions, we separate internal and external interactions because of their different role in binding and distance scales. Furthermore, to model interactions in a fine-grained manner, we connect nodes using cutoffs dependent on the type of nodes they are connecting and assign the edges distinct edge types corresponding to different graph convolution kernels following (Zhang et al., 2023c; Kong et al., 2023; Corso et al., 2023). We further denote the pocket-ligand subgraph as\\n\\n\\\\[ G_p = (V, E) \\\\]\\n\\nwhich is the input of the docking model.\\n\\nArchitecture.\\n\\nWe construct a heterogeneous graph with atoms and residue levels to reason the pocket-ligand interaction in a more fine-grained manner (Zhang et al., 2023c). The edge constructions have considered various interaction types, including internal interactions (atom-atom interactions, chemical bone interactions, residue-residue spatial and sequential interactions, atom-residue subordination relationship, etc.) and external interactions (pocket-ligand atom interactions, residue-atom interactions, framework-pocket interactions, etc.). We assign different graph convolution kernels to each edge type and model every interaction separately. These relation-aware schemes can enhance the modeling ability for accurate pocket-ligand interactions.\\n\\nThe output of the score model must be in the tangent space \\\\( T^{\\\\mathbb{R}^3} \\\\oplus T^{\\\\mathbb{R}SO(3)} \\\\oplus T^{\\\\mathbb{R}SO(2)} \\\\) and the predicted energy must be in \\\\( SE(3) \\\\)-invariant. The space \\\\( T^{\\\\mathbb{R}^3} \\\\) and \\\\( T^{\\\\mathbb{R}SO(3)} \\\\) represent the translation and rotation (Euler) vectors which are both \\\\( SE(3) \\\\)-equivariant and \\\\( T^{\\\\mathbb{R}SO(2)} \\\\) corresponds to scores on \\\\( SE(3) \\\\)-invariant quantities (torsion angles). To achieve such\"}"}
{"id": "QRjTDhCIO8", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"desire, we apply a SE(3)-equivariant convolution network (6 layers of graph convolutions on the heterogeneous graph implemented with e3nn library (Geiger & Smidt, 2022; Corso et al., 2023)) as the shared encoder with equivariant and invariant pooling as heads as well as pseudo torque convolution (Jing et al., 2022) for predicting torsional updates.\\n\\nB.2. Model training and implementation details\\n\\nFollowing (Corso et al., 2023), we train and evaluate all the models on PDBBind (Liu et al., 2017) based on time-split dataset partition. We also adopt a conformer matching procedure described in (Jing et al., 2022) for eliminating the distributional shift between RDKit-initialization used in inference and ground-truth ligand pose in the dataset. We summarize the training as algorithmic description as in Algorithm 1.\\n\\nAlgorithm 1\\n\\nLearning diffusion generative models\\n\\nInput: Training pairs \\\\{x\u2217, y\\\\} where x\u2217 is the ground truth ligand pose and y is the ground truth protein structure, RDKit predictions \\\\{c\\\\}, variance for each geometry \\\\{\u03c3g\\\\}, prior interaction energy E(x, y) and Qxg the bridge in Eq. 6, and energy-to-geometry mapping \\\\(F\\\\) and a diffusion model \\\\(P_\u03b8\\\\).\\n\\nOutput: Parameters \\\\(\u03b8\\\\) of \\\\(P_\u03b8\\\\).\\n\\n1: Randomly initialize the parameters of \\\\(P_\u03b8\\\\).\\n2: for \\\\(c, x, y \\\\in \\\\{(x\u2217, y, c)\\\\}\\\\) do\\n3: Let \\\\(x_0 \\\\leftarrow \\\\text{conformer align}(x\u2217, c)\\\\);\\n4: Sample \\\\(t \\\\in \\\\text{Uni}([0, 1]), \\\\chi \\\\in \\\\{0, 1, 2, 3\\\\}\\\\);\\n5: Sample \\\\(\\\\{\\\\Delta g\\\\}\\\\) from diffusion kernels \\\\(p_gt(\u00b7|\u03c3g)\\\\);\\n6: Compute \\\\((x_t, y_t) \\\\leftarrow \\\\text{Apply}(\\\\{\\\\Delta g\\\\}, x_0, y, sc)\\\\) and \\\\(f_gt = F(E(x_t, y_t))\\\\);\\n7: Predict scores;\\n8: Take optimization step on the sum of denoising score matching loss Eq. 4 on each geometry;\\n9: end for\\n10: return Diffusion Bridge \\\\(P_\u03b8\\\\).\\n\\nWe use Adam as optimizer with learning rate=0.001 and exponential moving average of the weights during training, which we will use in inference. We update the moving average after every optimization step with a decay factor of 0.999. The batch size is 64. We run inference with 20 denoising steps on 500 validation complexes every 10 epochs and use the set of weights with the highest percentage of RMSDs less than 2 \u00c5 as the final score model. All baselines and our approach are implemented using the PyTorch 1.6.0 library with Intel(R) Xeon(R) Gold 6240R@2.40GHz CPU and NVIDIA A100 GPU. We train our score model for 600 epochs (around 7 days). As we co-model the binding energy and poses, we don't need additional training for the confidence model as in (Corso et al., 2023), but the confidence model can still be adopted in our framework for ranking. For inference, only a single GPU is required but we also implement parallel inference in multi-gpu settings for large scale screening task like cross-dock (around 10000 complexes, 10 poses per complexes and 20 time steps).\\n\\nC. Experimental setup\\n\\nTo evaluate the performance of our method, we design a systematic benchmark mimicking the flexible and realistic setting in drug discovery pipelines, including redocking, unbound (apo)-structure docking (including predicted structures and experimental structures), and cross-dock (docking to bound structure with another ligand, which is very challenging but common in drug discovery) experiments in the PDBBind dataset. Re-Dock has shown competitive results across all tasks in various metrics assessing conformational plausibility. This suggests our model can handle the flexibility of both pockets and ligands and has the potential to be helpful in realistic drug discovery pipelines.\"}"}
{"id": "QRjTDhCIO8", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge\\n\\n**Geometric Prior Drift**\\n\\n- **Pocket apo conformation & Ligand seed conformation**\\n- **Pocket-ligand holo conformation**\\n\\n**Apo Ligand Bind Holo**\\n\\n**Induced Fitting Process Driven by Natural Forces**\\n\\n- **Re-Dock Probabilistic Induced Fitting Process**\\n- **Geometric Prior Bridge over rotations, translations and torsions**\\n\\n**Figure 2.** The illustration of Re-Dock Framework. We aim to simulate the induced fitting process with geometric prior bridges. Our key designs are threefold: 1\u20e3 The pocket sidechains displace the most flexibility for inducing interactions. Thus, we generate the sidechain conformations (the blue and purple sticks are the conformations of two steps before and after, respectively; we omit other sidechains for simplicity) via torsion angle updates while docking. 2\u20e3 We explore a novel generative model, the geometric prior bridge for reflecting the energy-constrained fitting process. Compared with the diffusion processes (the red curves), the prior bridge process (the blue line) is augmented with problem-dependent prior and thus more fast and accurate to generate. 3\u20e3 For explicit modeling of interaction and constructing prior bridges over geometries, we propose an energy-to-geometry mapping module inspired by Newton-Euler equations.\\n\\n**Building and Learning Diffusion Bridges.**\\n\\nThere are infinitive diffusion processes $P_\\\\theta$ that reach the same terminal distribution but differ in distributions of trajectories $Z = \\\\{Z_t: t \\\\in [0, 1]\\\\}$ and $P_\\\\theta_t$ is the marginal distribution of $Z_t$ at time $t$. We aim at learning a generative model with parameter $\\\\theta$ such that the distribution $P_\\\\theta_1$ of the terminal state $Z_1$ equals the data distribution $\\\\Pi^*$. Building and Learning Diffusion Bridges.\\n\\n- **Probabilistic Induced Fitting Process**\\n- **Energy Forces Angular Velocity**\\n\\n**Energy-to-Geometry Mapping**\\n\\n- **Sidechain torsion update**\\n\\n**Parameterization of prior bridge**\\n\\n- **Positive definition covariance coefficient:** $s_\\\\theta t$:\\n  - $R^d \\\\rightarrow R^d$ is parameterized as a neural network with parameter $\\\\theta$, and $\\\\mu_0$ is the prior distribution. Here, $P_\\\\theta$ represents the distribution of the diffusion trajectory $Z = \\\\{Z_t: t \\\\in [0, 1]\\\\}$ and $P_\\\\theta_t$ is the marginal distribution of $Z_t$ at time $t$. We aim at learning a generative model with parameter $\\\\theta$ such that the distribution $P_\\\\theta_1$ of the terminal state $Z_1$ equals the data distribution $\\\\Pi^*$.\\n\\n**Formally, if $Q_x(Z_1 = x)$ = 1, then $Q_x$ or simply an $x$-bridge is a bridge process ending at data point $x$. We can construct physically informed diffusion bridges based on $Q_x$ if we first sample a data point $x \\\\in \\\\Pi^*$ and then draw a bridge $Z \\\\in Q_x$ pinned at $x$. Thus the distribution of trajectories $Z$ is a mixture of $Q_x$: $Q_{\\\\Pi^*} = \\\\int Q_x(dz) \\\\Pi^*(dz)$.\\n\\n**We can learn the diffusion bridge model $P_\\\\theta$ by imitating the trajectories drawn from $Q_{\\\\Pi^*}$ since the crucial property of $Q_{\\\\Pi^*}$ is its end distribution equals the data distribution, i.e., $Q_{\\\\Pi^*} = \\\\Pi^*$. This can be formulated by maximum likelihood or equivalently minimizing the KL divergence:**\\n\\n$$\\\\min_\\\\theta L(\\\\theta) := KL(Q_{\\\\Pi^*} || P_\\\\theta).$$\\n\\nFurthermore, assume that the bridge $Q_x$ is a diffusion model of form:\\n\\n$$Q_x: dZ_t = b_t(Z_t | x) dt + \\\\sigma_t(Z_t) dW_t, \\\\quad Z_0 \\\\sim \\\\mu_0.$$ (2)\\n\\n- $b_t(Z_t | x)$ is an $x$-dependent drift term that needs to be carefully designed to meet the bridge condition and incorporate prior information simultaneously. We adopt a practical and simple family of bridges proposed by (Wu et al., 2022a) in this paper by introducing modifications to Brownian Bridges (Liu et al., 2022):\\n\\n$$Q_x_{bb,f}: dZ_t = \\\\sigma_t f_t(Z_t) + \\\\sigma^2_t x - Z_t \\\\beta_1 - \\\\beta_t dt + \\\\sigma_t dW_t.$$ (3)\\n\\n- $\\\\beta_t = R^0_t \\\\sigma^2_s ds$, and $f_t(Z_t)$ is an extra drift term which reflects physical prior (e.g., the physical force) and $Z_0 \\\\in N(x, \\\\beta_1)$ or $N(0, \\\\beta_1)$ when $\\\\beta_1$ is large enough (Liu et al., 2022). After choosing appropriate $Q_x_{bb,f}$, using Girsanov theorem, the loss function can be reformed into a form of denoising score matching loss of (Heng et al., 2022):\\n\\n$$L(\\\\theta) = E_{Z \\\\sim Q_{\\\\Pi^*}} h_1^2 Z_1 \\\\sigma(Z_t) - 1 (s_\\\\theta t(Z_t) - b_t(Z_t | Z_1))^2 dt + \\\\text{const}.$$ (4)\\n\\nWhere the const term contains the log-likelihood for the initial distribution $\\\\mu_0$, which is a const in our problem.\\n\\n**3.2. Problem Statement**\\n\\nGiven unbounded protein and ligand structures as inputs, flexible docking aims to predict the binding pose (i.e. atom coordinates) of the pocket sidechain and ligand, imitating the process of induced-fit. For simplicity and efficiency, we model the changes the ligand and sidechains undergo during binding following (Corso et al., 2023), instead of...\"}"}
{"id": "QRjTDhCIO8", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $q$ with the Newton-Euler Equation. The overall framework\\nof rigid rotations of the ligand, and the 2D rotation groups\\n$\\\\text{SO}_2$ respectively. A high-level schematic is provided in Fig. 2.\\n\\nIn this section, we formally describe the flexible and\\ntranslations of ligands relative to the fixed protein backbone.\\n\\nRecent advances in Diffusion Bridge (Liu et al., 2022; Wu\\n2022). It states the Brownian bridge\\n$Q$: $d_{bb}$ can be shown to\\nbe the law of:\\n\\n$$\\nQ_{bb} = \\\\sigma_t \\\\cdot |x_t| + \\\\sigma_{\\\\theta_t} \\\\cdot \\\\theta_t + \\\\sigma_{\\\\phi_t} \\\\cdot \\\\phi_t + \\\\sigma_{\\\\psi_t} \\\\cdot \\\\psi_t + \\\\sigma_{\\\\tau_t} \\\\cdot \\\\tau_t + \\\\sigma_{\\\\nu_t} \\\\cdot \\\\nu_t + \\\\sigma_{\\\\kappa_t} \\\\cdot \\\\kappa_t + \\\\sigma_{\\\\lambda_t} \\\\cdot \\\\lambda_t + \\\\sigma_{\\\\mu_t} \\\\cdot \\\\mu_t + \\\\sigma_{\\\\sigma_t} \\\\cdot \\\\sigma_t.\\n$$\\n\\n$\\\\text{SO}_3$ is the product manifold, the forward dif-\\nfusion proceeds independently in each manifold, and the\\nconstruction of the bridge on\\n$G$ is a product space\\n$\\\\text{SO}_2 \\\\oplus \\\\text{SO}_3$. Since\\n$\\\\text{SO}_2$ is a product space\\nof changes in $m$ torsion angles\\n$\\\\theta$, we proceed to\\nde Compressible diffusion bridge (Dong et al., 2022)\\n\\n$$\\n\\\\frac{\\\\partial P}{\\\\partial t} = \\\\text{Grad}(\\\\text{Log}(P)) + \\\\text{Grad}(\\\\text{Log}(Q))\\n$$\\n\\nThis allows us to add noise to specific $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$ $\\\\theta$"}
{"id": "QRjTDhCIO8", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge\\n\\n(a) Sidechain Angles\\n\\nFigure 3. The illustration of sidechain updates. (a) Up to four sidechain $\\\\theta_{sc}$ angles (formally, $\\\\chi$ angles) have a sequential order. (b) Rotating $\\\\theta_{sc1}$ will affect the coordinates of atoms in $\\\\theta_{sc2}$, $\\\\theta_{sc3}$, and $\\\\theta_{sc4}$. It's similar for rotating $\\\\theta_{sc2}$ and $\\\\theta_{sc3}$. The atom groups of the later angles will accumulate noise from the former angles which complicates the latter\u2019s denoising process.\\n\\nTo parameterize the bridge $P_{\\\\theta}$, we use a relation-aware equivariant graph neural network on full atoms of complexes as a shared encoder and the equivariant pooling layer, invariant pooling layer for predicting SE(3)-equivariant (i.e., translations and rotations) and invariant (i.e., binding energy, torsion angles) quantities. Algorithmic description and more details on model architecture, data modeling, and training can be referred in Appendix B.\\n\\n4.2. Injecting Interaction Prior into Geometry Bridges. Although we have defined the prior bridges and score matching loss on $G$, we nevertheless develop the model and integrated interaction prior in 3D coordinates $X$ directly. Providing the full 3D structure, rather than abstract elements of the product space, to the score model allows it to reason about physical interactions using SE(3) equivariant model, and not be dependent on arbitrary definitions of torsion angles (Jin et al., 2023). Further, interaction priors defined on abstract elements of geometries are less informative, and incorporating physical constraints defined on 3D coordinates can provide direct and accurate guidance over interactions within pocket-ligand complexes. Such design can also allow seamless integration of well-defined physical or statistical potential energy priors of protein-ligand interactions, which we choose in this paper.\\n\\nBridging physical energy priors defined in 3D coordinates and prior bridges on geometry product space $G$ is challenging as the prior drift terms in bridges are not directly comparable with the physical energy priors $E(X)$, nor the derivatives, i.e., physical forces $F = \\\\partial E/\\\\partial X$. To fill the gap and enjoy the benefits of both modeling schemes, we propose an Energy-to-Geometry mapping method inspired by rigid body mechanics with Newton-Euler equations.\\n\\nEnergy-to-Geometry Mapping. There are inherent connections between energy and rigid body motions in rigid body mechanics. Given a potential energy $E(X)$, we can calculate the forces $F = \\\\partial E/\\\\partial X$ acting on a rigid body system. Then, the system's combined translational and rotational dynamics can be described in the Newton-Euler equations (Eq. 11). It is natural to take inspiration from rigid body mechanics to convert prior energy defined in 3D coordinates to corresponding geometric updates as our extra drift terms $f_g(t)$ in geometric prior bridges.\\n\\nToo short distances or clashes between atoms of ligands and pockets in the complex conformations generation process will induce abnormal Van der Waals forces. Thus, we incorporate anti-clash potential prior in our bridges. Following (De Bortoli et al., 2021; Corso et al., 2023), we choose\\n\\n$$\\\\{x \\\\in \\\\mathbb{R}^3: S(x) = \\\\gamma\\\\}$$\\n\\nwhere $S(x) = -\\\\sigma \\\\ln \\\\frac{1}{\\\\sum_{j=1}^{NP} \\\\exp \\\\left(-\\\\frac{\\\\|x - x_j\\\\|^2}{\\\\sigma}\\\\right)}$ as the descriptor of the protein surface. $NP$ is the atom number of pockets and $\\\\{x_j\\\\}_{j=1}^{NP}$ is the set of pocket atom coordinate vectors. The anti-clash potential we used can be derived as follows:\\n\\n$$E_{clash}(X) = -N_M \\\\max(0, \\\\gamma - S(x_i)),$$\\n\\n(10)\\n\\nwhere $N_M$ is the atom number of molecules. We also include the Amber physical interaction energy terms as prior potentials. The overall interaction potential energy prior $E(X)$ is a direct sum detailed in the Appendix D.\\n\\nNewton-Euler Equation\\n\\nThe Newton-Euler equations explain a rigid body's combined translational and rotational dynamics. In the Center of Mass (CoM) frame, this can be written in matrix form as:\\n\\n$$F = m I_0 v dt + \\\\omega \\\\times I_c \\\\omega dt +$$\\n\\n$$0 \\\\omega \\\\times I_c \\\\omega$$\\n\\n(11)\\n\\nwhere $F$ and $\\\\tau$ are the total force and torque acting on CoM, $v$ and $\\\\omega$ are the velocity of CoM and the angular velocity around CoM, $m$ and $I_c$ is the mass and inertia matrix of the rigid body, which are constant for a given rigid body. In our Energy-to-Geometry mapping module, the force $f_i(t)$ act on each ligand atom $i$ is defined as the gradient of the interaction energy prior function $f_i(t) = \\\\partial E(X_t)/\\\\partial x_i$. We can then calculate the total force $F_t$ and torque $\\\\tau_t$ for the ligand in the discrete-time step $t$:\\n\\n$$F_t = \\\\sum_{i \\\\in V_L} f_i,$$\\n\\n$$\\\\tau_t = \\\\sum_{i \\\\in V_L} (x_i - p_{ct}) \\\\times f_i.$$\\n\\n(12)\\n\\nwhere $p_{ct}$ is the Center of Mass. We assume the system is stationary at each discrete time step (Guan et al., 2023; Jin et al., 2023). Furthermore, we can specify changes of torsion angles to be disentangled from rotations or translations (Corso et al., 2023) to ensure torsional updates cause no linear or angular momentum, i.e., $\\\\omega_t = 0$ and $v_t = 0$.\\n\\nThus, the Newton-Euler equations (Eq.11) can be simplified as\\n\\n$$F_t = m \\\\frac{dv_t}{dt}$$\\n\\nand\\n\\n$$\\\\tau_t = I_c \\\\frac{d\\\\omega_t}{dt}.$$\\n\\nFor a short enough time period $\\\\Delta t$, we have the velocity and angular velocity of...\"}"}
{"id": "QRjTDhCIO8", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Performance of flexible redocking on the PBDBind test set (bound, holo-crystal proteins) and its corresponding ESMFold predicted Apo structures (unbound). The best metrics are marked by bold. In parenthesis, we specify the number of poses sampled from the generative model. It is worth noting that only our Re-Dock has no access to the holo-crystal proteins. Our Re-Dock surpasses all baselines across different metrics and settings with affordable inference time, which demonstrates our effectiveness and efficiency on flexible redocking as well as apo docking, and the advantages of our geometric prior bridge with sidechain flexibility.\\n\\n\\\\[ \\\\omega_t + \\\\Delta t = I - 1 \\\\mathbf{c}_t \\\\Delta t \\\\]\\n\\n\\\\[ v_t + \\\\Delta t = 1 \\\\mathbf{m}_F \\\\Delta t. \\\\]\\n\\nAssuming each atom in the ligand has the unit mass and setting the time period \\\\( \\\\Delta t \\\\) as hyper-parameter \\\\( \\\\alpha \\\\), the prior drift terms in prior geometric bridges \\\\( Q_g^{bb,f} \\\\) (Eq. 6) can be designed as follows:\\n\\n\\\\[ f_t(Z_t) = \\\\alpha |V_L| F_t, \\\\quad f_R(Z_t) = \\\\alpha I - 1 \\\\mathbf{c}_\\\\tau_t \\\\]\\n\\nwhere \\\\( |V_L| \\\\) denotes the number of atoms in the ligand. More details and analysis (e.g., the equivariance analysis and \\\\( f_\\\\theta_t \\\\)) can be referred to the Appendix.\\n\\n4.3. Learning by Co-Modeling Energy and Poses\\n\\nWe have designed interaction-aware prior bridges on geometries via Eq.6 and Eq.13. Next we will discuss how to better parameterize the learnable drift \\\\( s_\\\\theta_t \\\\) in Eq.1 for learning the bridges. Specifically, we assume the learnable drift has a form of\\n\\n\\\\[ s_\\\\theta_t = \\\\beta \\\\hat{f}_t + \\\\hat{s}_\\\\theta_t \\\\]\\n\\nwhere \\\\( \\\\beta \\\\) can be another learnable parameter, \\\\( \\\\hat{s}_\\\\theta_t \\\\) is direct predicted scores and \\\\( \\\\hat{f}_t \\\\) is also calculated using Energy-to-Geometry mapping with learnable interaction energy \\\\( E_\\\\theta(X,G) \\\\) as in equation 13. This parameterization allows learning of prior drifts and the binding energy via an additional head at the same time. After training, \\\\( E_\\\\theta(X,G) \\\\) can also serve as the confidence model (Corso et al., 2023) in addition to calculating \\\\( s_\\\\theta_t \\\\).\\n\\nThis idea has a connection with the Energy-Based model (EBMs) (Jin et al., 2023), where the likelihood of a data point \\\\( p(X,G) \\\\propto \\\\exp(-E_\\\\theta(X,G)) \\\\). Our training process can be seen as training EBMs using denoising score matching with prior, and then we can interpret the learned energy as protein-ligand interaction energy, naturally including the interaction potential energy prior. Our method combines parameterizing the score directly (leaving the energy function implicit) and explicitly modeling the energy function. Thus, we enjoy both advantages of sampling quality and likelihood estimation to co-model energy and poses for imitating energy-constrained induced-fit process using denoising score matching loss in Eq. 4.\\n\\n5. Experiments\\n\\nIn this section, we justify the advantages of the proposed Re-Dock with comprehensive experiments. The experimental setup is introduced in Section 5.1. We aim to answer six research questions.\\n\\nQ1: How effective is Re-Dock for flexible re-docking and apo-docking tasks?\\nQ2: Can Re-Dock generate accurate sidechain conformations?\\nQ3: Can Re-Dock generalize well to more challenging and realistic tasks (e.g. cross-dock)?\\nQ4: How do key framework designs impact the performance of Re-Dock? Can Re-Dock use fewer sampling steps to further speed up large-scale screening?\\nQ5: Can Re-Dock generate physically valid samples?\\nQ6: What's the quality of generated samples of Re-Dock?\"}"}
{"id": "QRjTDhCIO8", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge\\n\\nYufei Huang,* 1 2\\nOdin Zhang,* 1 3\\nLirong Wu 1 2\\nCheng Tan 1 2\\nHaitao Lin 1 2\\nZhangyang Gao 1 2\\nSiyuan Li 1 2\\nStan. Z. Li \u2020 2\\n\\nAbstract\\n\\nAccurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock demonstrate our model's superior effectiveness and efficiency over current methods.\\n\\n1. Introduction\\n\\nProteins can have their biological functions (Huang et al., 2023b;a; Wu et al., 2022b) altered by the binding of small molecule ligands, such as drugs (Lin et al., 2022a; 2023a; Hu et al., 2022). Molecular docking which reveals this interaction, is critical in drug design and involves predicting the conformation of a protein-ligand complex. A key challenge in molecular docking lies in the induced-fit mechanism (Sherman et al., 2006), where the protein's binding sites (pockets) are flexible, altering their poses in response to ligand binding. Notably, the pocket's sidechain atoms exhibit the most significant flexibility (Clark et al., 2019). Though important, accurately predicting the bound structures is highly challenging. Traditional docking methods (Eberhardt et al., 2021; Alhossary et al., 2015), using empirical scoring functions and optimization algorithms, struggle with the vast search space and complex calculations, often resulting in inaccurate and slow predictions. Recent deep learning approaches (Zhou et al., 2023; Liao et al., 2019) focus on predicting ligand binding poses using bound protein structures (holo-structures), which include ground truth sidechain conformations (i.e., priori leakage). These approaches, by introducing priori leakage (as illustrated in Fig. 1(a)), oversimplify the problem and fail to mimic realistic docking scenarios. Other deep learning methods (Pei et al., 2023; Corso et al., 2023) ignore sidechains for simplicity and implicit flexibility. This neglect often results in unrealistic poses where ligands overlap with side chains, i.e. steric clashes (as shown in Fig.1(b)). More accurate and realistic binding pose predictions require explicit sidechain modeling. Additionally, protein-molecule interaction is the key to model the binding process. Current approaches learn this interaction implicitly with neural networks while fail to incorporate explicit modeling of interaction in 3D coordinates for direct and accurate guidance.\\n\\nTo address these limitations, we introduce an under-explored\"}"}
{"id": "QRjTDhCIO8", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge\\n\\nThe task, named flexible docking (Sahu et al., 2024) to predict poses of ligand and pocket sidechains simultaneously, introduces Re-Dock, a flexible and realistic generalization framework with explicit modeling of pocket sidechain flexibility and integrated interaction prior to steer the generation process.\\n\\nReDock mimics the induced-fit process (Sherman et al., 2006) for realistic docking scenarios and generates physically realistic conformations by extending diffusion bridges (Wu et al., 2022a) to non-Euclidean manifolds of implicit geometries: rotations, translations, and torsion with explicit interaction prior in Euclidean space.\\n\\nIn detail, we construct neural diffusion models to imitate the bridge processes for generating flexible and realistic poses of both ligands and pockets. Diffusion bridges are stochastic processes that guarantee to yield given observations at the fixed terminal time (Liu et al., 2022). Notably, we model sidechain distributions autoregressively for better generation quality concerning their sequential nature (Zhang et al., 2023b).\\n\\nUnlike previous diffusion bridge processes defined over Euclidean space with molecular coordinates (Wu et al., 2022a), we explore bridge processes in geometric space, challenging for its implicit nature of modeling data points. For constructing interaction-informed prior bridges over geometry, we enable Energy-to-Geometry mapping using the Newton-Euler equations inspired by rigid body mechanics.\\n\\nWe benchmark flexible docking in the pocket-aware setting and provide generalized results of Re-Dock with pocket predictions. In many drug discovery pipelines, pockets are identified early (Zhang et al., 2023a) and their sidechains contribute to the majority of flexibility (Clark et al., 2019) during docking. Thus Re-Dock focuses on these flexible pocket sidechains.\\n\\nWe design a new benchmark that reflects realistic scenarios, including apo crystal docking and cross-dock using the PDBBind (Liu et al., 2017) and our curated datasets. Re-Dock shows comparative results in overall benchmark tasks, proving its effectiveness in predicting flexible docking structures. This demonstrates its potential for real-world applications.\\n\\nOur key contributions are:\\n\\n- We introduce the under-explored task of flexible docking and design a rigorous benchmark with new datasets.\\n- We propose Re-Dock, a novel diffusion bridge model extended to non-Euclidean manifolds with energy-to-geometry mapping inspired by mechanics. It enables interaction-aware, \u2018induced\u2019, generative docking processes with co-modeling of binding energy and poses.\\n- Superior benchmark test results including cross-dock suggest our potential for real-world applications.\\n\\n2. Related Works\\n\\nMolecular Docking. This field predicts how proteins and ligands bind together. Traditional approaches, like AutoDock Vina (Eberhardt et al., 2021; Alhossary et al., 2015), SMINA (Koes et al., 2013), and GLIDE (Halgren et al., 2004), utilize energy-based functions. Recently, deep learning, especially Graph Neural Network (Wu et al., 2022d; 2023c; Zheng et al., 2023b) has brought two innovations: Regression-based methods such as Equibind (Stark et al., 2022), Tankbind (Lu et al., 2022), and Fabind (Pei et al., 2023) for predicting the ligand\u2019s docking pose directly and generative docking introduced by DiffDock (Corso et al., 2023; Plainer et al., 2023) approaching docking as the generation of ligand geometries, like rotations.\\n\\nMost methods assume known holo-protein structures, which is often impractical. Traditional approaches and some structure prediction methods (Qiao et al., 2023; Krishna et al., 2023) can account for protein flexibility but require extensive computations. Our work lies in generative docking with pocket flexibility and explicit modeling of interaction, thus can be applied to more realistic tasks, including cross-dock. More related works can be found in the Appendix A.\\n\\n3. Related Works\\n\\nDiffusion Bridge Process. Diffusion-based generative models (Song et al., 2020; Ho et al., 2020) have become popular in AI generation by adding noise to data and then using a reverse process to generate outputs. Efforts to improve these models have led to diffusion bridges. Schrodinger bridges (De Bortoli et al., 2021; Shi et al., 2023) have been proposed for learning entropy-regularized optimal transports of generation paths and guarantee desirable outputs, but these models involve iterative proportional fittings and are computationally costly.\\n\\nResearch works such as (Peluchetti, 2021) and prior bridge (Wu et al., 2022a) directly learn diffusion trajectories with specific ending data points and inject problem-dependent prior into these paths, avoiding the time-reversal technique of (Song et al., 2020). However, applying diffusion bridges to geometric domains (e.g., rotations and torsion) requires designing appropriate bridge processes, which is an area that remains unexplored.\\n\\n3. Backgrounds\\n\\n3.1. Diffusion Bridge for Non-geometric Domains\\n\\nIn this section, we provide an overview of the general diffusion bridge to introduce the necessary notations and concepts based on (Wu et al., 2022a; Liu et al., 2022).\\n\\nDefinition. Diffusion bridges are diffusion processes that are conditioned to initialize and terminate at two given states. We can learn a generative model given a dataset \\\\( \\\\{x_k\\\\}_{k=1}^n \\\\) drawn from an unknown distribution \\\\( \\\\Pi^* \\\\) on \\\\( \\\\mathbb{R}^d \\\\) via building and imitating such a diffusion bridge with prior and data distribution on both sides. A diffusion bridge generative model on the time interval \\\\([0,1] \\\\) is:\\n\\n\\\\[\\nP_\\\\theta: dZ_t = s_\\\\theta t(Z_t) dt + \\\\sigma_t(Z_t) dW_t, \\\\quad \\\\forall t \\\\in [0,1], Z_0 \\\\sim \\\\mu_0. \\\\tag{1}\\n\\\\]\\n\\nwhere \\\\( W_t \\\\) is a standard Wiener process; \\\\( \\\\sigma_t: \\\\mathbb{R}^d \\\\rightarrow \\\\mathbb{R}^{d \\\\times d} \\\\) is\"}"}
