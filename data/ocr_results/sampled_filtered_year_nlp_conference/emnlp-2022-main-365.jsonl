{"id": "emnlp-2022-main-365", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You-En Lin, 1 An-Zi Yen, 2 Hen-Hsen Huang, 3 Hsin-Hsi Chen\\n\\n1 Department of Computer Science and Information Engineering, National Taiwan University, Taiwan\\n2 Department of Computer Science, National Yang Ming Chiao Tung University, Taiwan\\n3 Institute of Information Science, Academia Sinica, Taiwan\\n\\nyelin@nlg.csie.ntu.edu.tw, azyen@nycu.edu.tw, hhhuang@iis.sinica.edu.tw, hhchen@ntu.edu.tw\\n\\nAbstract\\nWhen recalling life experiences, people often forget or confuse life events, which necessitates information recall services. Previous work on information recall focuses on providing such assistance reactively, i.e., by retrieving the life event of a given query. Proactively detecting the need for information recall services is rarely discussed. In this paper, we use a human-annotated life experience retelling dataset to detect the right time to trigger the information recall service. We propose a pilot model\u2014structured event enhancement network (SEEN) that detects life event inconsistency, additional information in life events, and forgotten events. A fusing mechanism is also proposed to incorporate event graphs of stories and enhance the textual representations. To explain the need detection results, SEEN simultaneously provides support evidence by selecting the related nodes from the event graph. Experimental results show that SEEN achieves promising performance in detecting information needs. In addition, the extracted evidence can be served as complementary information to remind users what events they may want to recall.\\n\\n1 Introduction\\nPeople have to deal with many events in their daily life. As time passes, they might forget details about their past experiences. Forgetting the exact name of people or places or things and mixing up life events is a common occurrence. This explains the importance of an information recall system that helps people bring to mind what they are trying to recall. We propose reactive and proactive service modes for an information recall system (Yen et al., 2021a). In reactive mode, users directly ask the system about their life events, whereas in proactive mode, the system attempts to automatically detect whether users need memory recall assistance and then provides the information they seek to recall.\\n\\nFor reactive mode, studies have been done on visual lifelog recall (Gurrin et al., 2016, 2017, 2019, 2020; Chu et al., 2019, 2020), which focuses on the construction of a multimodal retrieval model that enables users to search through photos using textual queries. We propose an information recall system (Yen et al., 2021b) to answer questions about life experiences over a personal knowledge base. In contrast to reactively receiving users\u2019 requests, proactive mode, which detects the right time to trigger the information recall service, is still little explored. In this paper, we further propose a pilot study to proactively detect the user\u2019s need for information recall assistance.\\n\\nOne common use case of memory recall assistance occurs in human conversation. To identify whether people have difficulties in recalling past experiences, Wang et al. (2018) propose a model to detect speech hesitation. Here, we focus on detecting the need for information recall support in people\u2019s narratives. Specifically, we seek to detect the following four situations in narratives to determine whether to trigger the service:\\n\\n1. If the description of the life event is consistent with the user\u2019s past experience, no memory recall assistance is needed.\\n2. Since people cannot remember every detail of their life experiences, we may unconsciously draw on similar but unrelated events to describe an experience that leads to a conflict with the established facts. It is essential to identify the description that is inconsistent with these facts, and retrieve those facts as an explanation to inform the user.\\n3. For the case where the narrative ends without relevant events mentioned, the user may have forgotten the events. The system must remind the user of these forgotten events.\\n4. The user may elaborate on additional events that were not logged before. This additional information could be details about events in lifelogs or they could be previously unlogged events. The system should distinguish\"}"}
{"id": "emnlp-2022-main-365", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"My brother decided to propose to his fianc\u00e9e, Ellie. Afterward, she hosted the party in their backyard. It was so fun! They had a beautifully stocked bar with lots of alcohol. The shots and cocktails were fantastic!\\n\\nIn addition, it was also the first time that my boyfriend met my family.\\n\\nFigure 1: Snippets from two stories in NIR (Left: Pre-Retold, Right: Post-Retold).\\n\\nWhether events are additional or conflict with the facts, and should update the lifelogs with the new information.\\n\\nTo the best of our knowledge, no dataset is available for this purpose. For this reason, we extended the Hippocorpus dataset (Sap et al., 2020) with new life event annotations as cases where users encounter problems and require recall assistance. Sap et al. (2020) invited crowd-workers to write stories about their life experiences, and asked them to write those stories again a few months later. As such, the nature of Hippocorpus meets our requirement. In Hippocorpus, life experiences written the first and the second times are referred to here as pre-retold and post-retold stories, respectively. The need for information recall is detected by comparing the pre-retold and post-retold stories.\\n\\nIn this paper, we propose a model to identify the event types in post-retold and pre-retold stories. The model is referred to as structured event enhancement network (SEEN). A transformer-based language model is used for encoding textual data. To encode the structured information of event description in stories, we construct an event graph by utilizing life event triples. To further capture the relations between events, the results of coreference resolution are incorporated into the event graph. The graph is encoded by the graph attention network (GAT) (Veli\u010dkovi\u0107 et al., 2018; Brody et al., 2022) and fused with the language model for integrating textual and structured information.\\n\\nIn addition, our model will extract the relevant events in a story pair as support evidence to explain the decision of the prediction. In this way, the user will easily recall the forgotten events. In sum, the contributions of our work are threefold: (1) We introduce the task of detecting the need for information recall in a narrative and providing the related information as the support evidence. (2) We present the NIR dataset, a human-annotated life experience retelling dataset for detecting the needs of information recall. (3) To detect information needs, we propose the structured event enhancement network (SEEN). The identified event types and extracted support evidence can assist users in recalling their past experiences and clarifying the confusing events.\\n\\nFrom Hippocorpus to NIR\\nSap et al. (2020) constructed Hippocorpus to investigate the difference in the narrative flow between relating life experiences and telling imaginative stories. In this work, we construct NIR by pruning the imaginative stories in Hippocorpus and retaining those stories about real-life events written by crowd-workers at two different times as pre-retold stories and post-retold stories. Following the four situations mentioned in Section 1, we summarize the following five event types from the story pairs in the dataset: Consistent, Inconsistent, Additional, Forgotten, and Unforgotten. The first three event types occur in the post-retold stories, and the last two event types occur in the pre-retold stories. Figure 1 shows a pair of pre-retold and post-retold stories labeled with these five event types denoted by green, red, blue, gray, and orange boxes, respectively. The numbers in Figure 1 denote the sentences consisting of life events. The details of the five event types are listed as follows:\\n\\nConsistent: The described event matches the user's life experiences. The event in Sentence (5) is Consistent because the event of the brother's fianc\u00e9e hosting the party in the backyard matches the description in Sentence (2). In this case, the event in Sentence (2) is the support evidence.\\n\\nInconsistent: In contrast to Consistent, the description is inconsistent with life events. For example, although the description of the fianc\u00e9e hosting the party in the backyard is consistent with life events, the description of the fianc\u00e9e hosting the party in the backyard is inconsistent with life events. For example, although the description of the fianc\u00e9e hosting the party in the backyard is consistent with life events, the description of the fianc\u00e9e hosting the party in the backyard is inconsistent with life events. For example, although the description of the fianc\u00e9e hosting the party in the backyard is consistent with life events, the description of the fianc\u00e9e hosting the party in the backyard is inconsistent with life events.\\n\\nAdditional: The event is additional to the previous ones.\\n\\nForgotten: The event is forgotten in the pre-retold story.\\n\\nUnforgotten: The event is unforgotten in the pre-retold story.\"}"}
{"id": "emnlp-2022-main-365", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"party matches Sentence (2), her name in the two stories is different. Thus, the event in Sentence (4) is \\\\textit{Inconsistent}. In other words, if the details of the event description in the post-retold story conflict with the facts described in the pre-retold story, it is an \\\\textit{inconsistent} event.\\n\\nAdditional: This is extra information about a life event that is not previously recorded in the collected lifelogs. The event in Sentence (6) is \\\\textit{Additional} due to the lack of similar event in the pre-retold story.\\n\\nForgotten: The life events that have been forgotten, i.e., are not mentioned here. As the event in Sentence (3) does not relate to other events in the post-retold story, it is a \\\\textit{Forgotten} event.\\n\\nUnforgotten: In contrast to \\\\textit{Forgotten}, the life events in the pre-retold story and also mentioned in the post-retold story belong to \\\\textit{Unforgotten} events. As the events in Sentence (2) are also mentioned in the Sentence (4) and Sentence (5) in the post-retold story, they are \\\\textit{Unforgotten} events.\\n\\nIn our dataset, each life event in the pre-retold and post-retold story stories is labeled with one of five event types. The annotation of relevant events within another story of the story pair is also included to denote as support evidence of the event type. That is, we annotate event types and the corresponding support evidence in the pre-retold and post-retold stories. The construction of the dataset is described in Section 3.\\n\\n3 Dataset Construction and Analysis\\n\\n3.1 Life Event Annotation\\n\\nAccording to the definition of LiveKB (Yen et al., 2019, 2020) and ConvLogMiner (Kao et al., 2021), we define a life event as a life experience that is related to specific individuals. Note that a sentence may refer to multiple life events. We follow the work of Yen et al. (2019) to extract life events in the triple form \\\\((subject, predicate, object)\\\\). The predicate is also classified into two types: \\\\textit{explicit} and \\\\textit{implicit} to denote whether the predicate is mentioned in the story. The further details are described in Appendix A. Finally, we collected 60,889 events from 2,520 stories consisting of 44,199 sentences. The distribution of explicit and implicit events was 96.9\\\\% and 3.1\\\\%, respectively.\\n\\n3.2 Event Type Annotation\\n\\nGiven the life event annotation of each sentence, we invited 11 annotators to label the event types of the life events, where the event types are \\\\textit{Consistent}, \\\\textit{Inconsistent}, \\\\textit{Additional}, \\\\textit{Forgotten}, and \\\\textit{Unforgotten}. Given the pairs of pre-retold and post-retold stories, the annotators were invited to first read the stories to understand the author\u2019s experiences. For each story pair, one story is viewed as the reference story, and another story is viewed as the target story. The annotators labeled the event type of each life event in the target story by consulting the reference story. The decision of event type is also based on whether the target story is a pre-retold or post-retold story. In addition, for each story pair, they select the life events in one story that are related to the life events in another story as the support evidence for explaining the event type. Taking Figure 1 as an example, event \\\\((his fianc\u00e9e Ellen, host, it)\\\\) in Sentence (4) is \\\\textit{Inconsistent} since the name of the brother\u2019s fianc\u00e9e conflicts with the event \\\\((my brother, propose to, his fianc\u00e9e Ellie)\\\\) in Sentence (1), although it matches the event \\\\((She, hosted, party)\\\\) in Sentence (2). In other words, to identify \\\\textit{Inconsistent} events, comparing the subtle differences in the descriptions of the pre-retold and post-retold stories is essential.\\n\\nThe examination of the annotation quality proceeds similarly to the method mentioned in Appendix A. We randomly sampled 50 story pairs (2,113 events in total) and assigned them to each annotator. An annotator who majored in linguistics was selected as the supervisor. We measured the agreement of each annotator with the supervisor via the F-score. The average event type agreement was a Cohen\u2019s kappa score of 0.95. Finally, we collected 1,260 story pairs, with an event-type distribution of \\\\textit{Consistent}, \\\\textit{Inconsistent}, \\\\textit{Additional}, \\\\textit{Forgotten}, and \\\\textit{Unforgotten} events of 11,525, 226, 17,661, 18,773, and 12,704, respectively.\\n\\n3.3 Event Type Analysis on Age\\n\\nIn general, older people are assumed to need more memory assistance because of the assumption that they are more likely to forget things than younger people. To examine whether elders are indeed more likely to forget or confuse their past experiences, we calculated the average ratio of the five event types in each story pair over eight age groups. In Hippocorpus, 82, 214, 281, 208, 133, 117, 83, and 133 crowd-workers were 18, 25, 30, 35, 40, 45, 50, and 55 years old, respectively. The ratio of each event type in each age group is shown in Figure 2. For better visualization, the bars are presented...\"}"}
{"id": "emnlp-2022-main-365", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The ratio of each event type in each age group.\\n\\nThe numbers under the bars are the average distribution of each event type. The ratio of the Forgotten events is similar across all age groups, suggesting that both older people and younger people require information recall support. Hereafter, we view people over or equal to 50-years old as the 50-and-above group; those younger than 50-years old are the below-50 group. Comparing the ratio of Inconsistent events between the 50-and-above group and below-50 group, those in the latter group were more likely to confuse life events, where the difference was statistically significant (t-test, $p<0.05$). This suggests that when younger people recall past experiences, they often confuse details. However, when writing post-retold stories, people in the 50-and-above group preferred not to mention events of which they had only vague impressions. The further analyses are described in Appendix B.\\n\\n### 4 Task Formulation\\n\\n**Detection of Information Recall Need:** To detect the need for information recall, we propose a novel task that is aimed at determining the event type by comparing a pair of pre-retold story $U$ and post-retold story $V$. This can be considered a multi-class classification. We regard one story as the reference story $D$ and compare the event triple in another story (i.e., the target story) $D'$ with all sentences in $D$ to identify the event type. Formally, given a pair of $U$ and $V$, the task is to identify the life event type $y_i$ of the $i$-th event triple $e_i$ in $D'$, where $y_i \\\\in \\\\{\\\\text{Consistent}, \\\\text{Inconsistent}, \\\\text{Additional}, \\\\text{Forgotten}, \\\\text{Unforgotten}\\\\}$, and $D'$ denotes $U$ or $V$. On the one hand, for the task of identifying Consistent, Inconsistent, and Additional events, $D = U$ and $D' = V$. On the other hand, for the task of identifying Forgotten and Unforgotten events, $D = V$ and $D' = U$.\\n\\n**Support Evidence Extraction:** To remind the user which event is forgotten or confused in the proactive mode, providing an explanation is beneficial for memory recall. To this end, we also propose an explanation task to extract the events in $D$ that are related to $e_i$ in $D'$ as evidence to explain the decision of event type. The extracted event triple can also help users recall their life experiences.\\n\\n### 5 Structured Event Enhancement Network\\n\\nAlthough the pre-trained language models have shown great success on various NLP tasks, some works (Xiao et al., 2021; Wang et al., 2020; Tang et al., 2020) also suggest that the structured information can enhance token representations. We construct an event graph based on life event triples. The event graph is incorporated into our model for capturing fine-grained information of life event relations within a document. Inspired by GreaseLM (Zhang et al., 2022), which incorporates the language model with the external knowledge graph, we initialize the node representations by using the language model. Specifically, we extract the hidden states from different encoder layers of the language model as the node representations. A GAT model is employed to propagate the structured information of the event graph. Then the updated node representations are used for enhancing the token representations in the language model by our fusion mechanism. Figure 3 shows an overview of our proposed structured event enhancement network (SEEN). The details are described as follows.\\n\\n#### 5.1 Event Graph Construction\\n\\nTo construct an event graph $G_D$, we regard subjects, predicates, and objects of all events in reference story $D$ as the nodes. Since some subjects or objects may refer to other nodes, the nodes which are connected with the coreference links are merged as one node. Here, the coreference links are obtained by utilizing the coreference resolution model (Lee et al., 2018). Then, for each life event triple, we connect the predicate nodes to the subject and object nodes to create $G_D$. To enhance the connectivity of $G_D$, we insert a Super Node $S$ into $G_D$, and connect it to all the other nodes.\"}"}
{"id": "emnlp-2022-main-365", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: Overview of SEEN.\\n\\n5.2 Textual Encoder Layer\\nTo encode textual features of reference story D and the i-th event triple e in target story D', we concatenate D and e with the special tokens [BOS] and [EOS]. The format is [BOS] e D i [EOS] D [EOS]. For example, if the goal is to identify the type of i-th life event in V, the input sequence is [BOS] e V i [EOS] U [EOS], where e V i is the concatenation of the components in e V.\\n\\nThe output of l-th layer is the hidden states H_l = \\\\{h_l[BOS], h_l1, ..., h_li\\\\}, where l = 1,...,L. L is a hyperparameter that denotes the number of transformer layers stacked in the textual encoder layer. l=0 is the initial embedding of the tokens.\\n\\n5.3 Integration Layer\\nTo introduce the structured information of GD into our language model, we stack M integration layers on the textual encoder layer, where M is a hyperparameter.\\n\\nNode Feature Construction:\\nSince different layers in the encoder capture different linguistic information for language understanding (Hoover et al., 2020), we initialize the node representations in GD by using the hidden states of different encoder layers. Each node in GD is a component in the event triple. Hence, a node can be a text span of the given D. To construct the node feature matrix, we first input H_L into the transformer layer in the integration layer. Then, we construct the initial feature matrix of all nodes in D. Specifically, we extract the hidden states of the [BOS] token and the tokens belong to each node. For example, the feature of the j-th node in the m-th integration layer is [h_m[BOS]; \\\\| t \\\\in T j h_m t], where T j is the token set of the j-th node. Afterward, we concatenate the initial features of each node as the initial feature matrix, and fed the matrix into a self-attention layer. Finally, we take the hidden state of the [BOS] token from the self-attention layer's output as the feature of j-th node, which is denote as n_m j.\\n\\nGraph Encoder:\\nAfter initializing the node features, we exploit the GAT layer to encode the event graph. To learn the representation \\\\( \\\\hat{n}_m j \\\\) of the j-th node \\\\( N_j \\\\) from the m-th GAT layer, \\\\( N_j \\\\) receives the messages from its neighbor nodes \\\\( R_j \\\\) and computed its feature as Equation 2, where \\\\( \\\\alpha_{m,j,j} \\\\) and \\\\( \\\\alpha_{m,j,r} \\\\) denote the weights of the j-th node and the r-th neighbor node in m-th GAT layer, respectively. And the attention weight is computed by Equation 3, where \\\\( \\\\alpha_{m,s,d} \\\\) denotes the attention weight of the message between the s-th node and the d-th node. The score \\\\( x_{s,d} \\\\) is computed by Equation 4. The encoded graph is denoted as \\\\( \\\\hat{G}_D,m = \\\\{\\\\hat{n}_m j, \\\\ldots, \\\\hat{n}_m j\\\\} \\\\), where m = 1,...,M.\\n\\n\\\\[ \\\\hat{n}_m j = \\\\alpha_{m,j,j} n_m j + \\\\sum_{r \\\\in R_j} \\\\alpha_{m,j,r} n_m r \\\\] (2)\\n\\n\\\\[ \\\\alpha_{m,s,d} = \\\\frac{\\\\exp(W \\\\tau \\\\text{LeakyRelu}(W \\\\kappa [n_m a; n_m b]))}{\\\\sum_{k \\\\in N_s \\\\cup \\\\{s\\\\}} x_{m,s,k}} \\\\] (3)\\n\\n\\\\[ x_{m,a,b} = \\\\exp(W \\\\tau \\\\text{LeakyRelu}(W \\\\kappa [n_m a; n_m b])) \\\\] (4)\\n\\nFusion Layer:\\nTo enhance the language model with the structured information from GD, we fuse the hidden state of [BOS] token \\\\( h_m[BOS] \\\\) of the m-th transformer layer and the feature of Super Node \\\\( \\\\hat{n}_m S \\\\) in \\\\( \\\\hat{G}_D,m \\\\), where \\\\( \\\\epsilon \\\\) and \\\\( \\\\delta \\\\) are the dimensions of \\\\( h_m[BOS] \\\\) and \\\\( \\\\hat{n}_m S \\\\), respectively. We concatenate and feed the result into a feedforward network to obtain the integrated feature \\\\( z \\\\in \\\\mathbb{R}^{\\\\epsilon + \\\\delta} \\\\). Hence, \\\\( z \\\\) is a feature after the fusion of textual and structured information. Afterward, we split \\\\( z \\\\) into two parts as the updated features \\\\( \\\\tilde{h}_m[BOS] \\\\in \\\\mathbb{R}^\\\\epsilon \\\\).\"}"}
{"id": "emnlp-2022-main-365", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and \\\\( \\\\tilde{n}_m^S \\\\) of the \\\\[BOS\\\\] token and Super Node, respectively.\\n\\n\\\\[\\nz = \\\\text{GeLU}(W([h_m \\\\cdot \\\\text{BOS}; \\\\hat{n}_m^S]) + b) \\\\quad (5)\\n\\\\]\\n\\n5.4 Event Type Classifier\\n\\nAfter updating the features through \\\\( M \\\\) integration layers, the super node's feature \\\\( \\\\tilde{n}_m^M \\\\) and the mean pooling result \\\\( \\\\theta_M \\\\) of graph \\\\( G_{D,M} \\\\) are concatenated with the hidden state of the \\\\[BOS\\\\] token to obtain the feature \\\\( h \\\\) for the event type identification. We use different classifiers to identify the event type of the event triple from different stories. For the events in \\\\( U \\\\), we use the sigmoid function following a feedforward network \\\\( \\\\phi \\\\) to identify whether it is Forgotten or Unforgotten. And the loss is denoted as \\\\( \\\\lambda_U \\\\). Otherwise, we apply the softmax function following another feedforward network \\\\( \\\\psi \\\\) to determine whether the event in \\\\( V \\\\) is Consistent, Inconsistent, or Additional. And the loss is denoted as \\\\( \\\\lambda_V \\\\).\\n\\n\\\\[\\nh = \\\\tilde{h}_M \\\\cdot \\\\text{BOS} \\\\oplus \\\\tilde{n}_m^M \\\\oplus \\\\theta_M \\\\quad (6)\\n\\\\]\\n\\n5.5 Related Node Classifier\\n\\nTo extract the support evidence, we identify whether the node \\\\( N_j \\\\) in \\\\( G_D \\\\) is related to \\\\( e_{D'}^i \\\\). Thus, each node feature is fed into a feedforward network following a sigmoid layer to perform binary classification. Note that Forgotten and Additional are the events only occurring in \\\\( D' \\\\). The related nodes cannot be found in \\\\( D \\\\). Thus, we exclude these two events to train the related node classifier, and the loss is denoted as \\\\( \\\\lambda_G \\\\). Finally, we compute the weighted sum of three losses as shown in Equation 9 to update the model, where \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\) are 0.5 after tuning by the validation set.\\n\\n\\\\[\\ny_{N_j} = \\\\text{Sigmoid}(\\\\hat{n}_m^j \\\\cdot W) \\\\quad (8)\\n\\\\]\\n\\n\\\\[\\n\\\\lambda = \\\\alpha \\\\cdot (\\\\lambda_U + \\\\lambda_V) + \\\\beta \\\\cdot \\\\lambda_G \\\\quad (9)\\n\\\\]\\n\\n6 Comparison with Natural Language Inference\\n\\nTo identify event types, we propose a pilot model to determine the relations between \\\\( e_{D'}^i \\\\) and \\\\( D' \\\\). This is different from simply comparing the relation between two sentences in a natural language inference (NLI) task (Bowman et al., 2015; Williams et al., 2018; Camburu et al., 2018). Identifying the event types in narratives involves two main challenges. Firstly, the event type of the event in \\\\( D' \\\\) must be determined by identifying the event pair relations with all life events in \\\\( D \\\\), since the discourse structures in \\\\( D \\\\) and \\\\( D' \\\\) are often different. Secondly, the granularity of event descriptions between the stories in a pair can differ. Hence, to determine the event type, we must infer the relevant details of the described events in both stories.\\n\\nTo investigate the difference between NLI and event type identification in information recall as assistance, we experiment with the impact of introducing the NLI task into our model. We find that pre-training the language model on the NLI task and fine-tuning the model on our task will improve the performance. However, the label definitions in the NLI task are different from our task. The details are discussed in Appendix D.2.\\n\\n7 Experiments\\n\\n7.1 Baseline Models\\n\\nSince the stories in our dataset are lengthy, we exploit the models that are capable of encoding the whole story as our baseline models.\\n\\n**XLNet** (Yang et al., 2019): XLNet is a sequence-to-sequence autoregressive model that pre-trains with the permutation language modeling task instead of the masked language model task in BERT (Devlin et al., 2018). To determine the event type, we use the hidden state of the last \\\\[EOS\\\\] token as input of the event type classifiers.\\n\\n**GPT-2** (Radford et al., 2019): In addition to the model equipped the autoencoder, we fine-tune an autoregressive model\u2013GPT-2 on our dataset for event type identification.\\n\\n**BART** (Lewis et al., 2019): BART is a sequence-to-sequence model that can encode lengthy documents. Compared with our model only containing the autoencoder, BART consists of an autoencoder and autoregressive decoder.\\n\\n**Longformer** (Beltagy et al., 2020): Since the number of the story tokens exceeds 512, we utilize Longformer which is capable of encoding long-lengthy documents. To fine-tune the model, we concatenate \\\\( e_{D'}^i \\\\) and \\\\( D' \\\\) as the input, and use the classifiers mentioned in Section 5.4.\\n\\n**Longformer with GATs:** To encode the event graphs, we simply stack \\\\( M \\\\) layers of GAT into Longformer. The final hidden states of Longformer and the GAT layer are concatenated and input to the classifiers for identify the event type. The related node classifier is included.\"}"}
{"id": "emnlp-2022-main-365", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Model             | F-score | Precision | Recall |\\n|-------------------|---------|-----------|--------|\\n| XLNet             | 0.6062  | 0.7076    | 0.0238 |\\n| GPT2-large        | 0.6025  | 0.6999    | 0.0000 |\\n| BART-large        | 0.6369  | 0.7582    | 0.0247 |\\n| Longformer-base   | 0.6183  | 0.7340    | 0.0000 |\\n| Longformer-large  | 0.6334  | 0.7462    | 0.0142 |\\n| Longformer-large w/ GATs | 0.6531 | 0.7472    | 0.1095 |\\n| GreaseLM-like (Longformer-large) | 0.6351 | 0.7592    | 0.0000 |\\n| SEEN (BART-large) | 0.6384  | 0.7623    | 0.0000 |\\n| SEEN (Longformer-base) | 0.6341 | 0.7379    | 0.0550 |\\n| SEEN (Longformer-large) | 0.6654 | 0.7633    | 0.1313 |\\n\\nTable 1: Experimental results of detecting information recall needs.\\n\\nTable 2: Results of support evidence extraction task.\\n\\n7.3 Experimental Results\\n\\nThe performance of each model on overall event types are shown in Table 1. We also report the results of each event type. F-score is adopted as the evaluation metric. We calculate McNemar's statistical significance test on the baselines and our models. To verify the effectiveness of the integration layer, we compare the performances of the following three combinations: (1) \\\"BART-large\\\" and \\\"SEEN (BART-large)\\\". (2) \\\"Longformer-base\\\" and \\\"SEEN (Longformer-base)\\\". (3) \\\"Longformer-large\\\" and \\\"SEEN (Longformer-large)\\\". The performances of SEEN in the three combinations outperform the baseline models at \\\\( p < 0.01 \\\\), \\\\( p < 0.05 \\\\), and \\\\( p < 0.01 \\\\), respectively. The results show the adaptability of the integration layer to different language models.\\n\\nWe find that \\\"Longformer-large w/ GATs\\\" significantly outperforms all the other baselines. That means incorporating the event graph is able to encode event relations to improve the performance. In addition, training the task of support evidence extraction simultaneously benefits the performance of event type identification. Moreover, \\\"SEEN (Longformer-large)\\\" outperforms \\\"Longformer-large w/ GATs\\\", suggesting that our proposed fusion mechanism introduces structured information effectively to enhance the language model. Comparing the last three rows, the Longformer-based encoder is better than the BART-based, and \\\"SEEN (Longformer-large)\\\" achieves the highest overall performance. The reason may be that the integration layers are built on the encoder layer. Identifying the event types by exploiting the output of the hidden states from the integration layer connected with the autoencoder is more suitable for our task. While the prediction of \\\"SEEN (BART-large)\\\" is based on the hidden states output from the autoregressive decoder. Note that all the models achieve relatively lower scores on the Inconsistent type because the number of this event is sparse in NIR. Besides, we find that \\\"SEEN (Longformer-large)\\\" usually identifies Inconsistent as Additional. The further error analysis is shown in Appendix D.1.\\n\\nTo verify the impact of the integration layer on the support evidence extraction task, we compare our proposed model SEEN with \\\"Longformer-large w/ GATs\\\" which simply concatenates the hidden states of the language model and the GAT layer. The evaluation metric is macro-averaged F-score. As mentioned in Section 5.5, we only extract the related nodes of Unforgotten, Consistent, and Inconsistent events. In Table 2, SEEN\"}"}
{"id": "emnlp-2022-main-365", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8.1 Ablation Study\\n\\nIn this section, we perform an ablation study to analyze the impact of SEEN with different settings.\\n\\n- **w/o pre-training on NLI**: We introduce the NLI task to strengthen the ability of our language model on capturing semantic features to infer the consistency of event descriptions. Hence, we investigate the influence of pre-training the language model on the Multi-NLI (Williams et al., 2018) dataset.\\n\\n- **w/o Concat**: Instead of concatenating the final hidden state of the super node and the average of node representations, we only use the hidden state of \\\\texttt{[BOS]} as the input of the event type classifier to evaluate the importance of structured features.\\n\\n- **w/o Support Evidence Extraction**: To analyze the impact of extracting support evidence toward the event type identification, we construct a classifier to extract related nodes in the event graph as evidence for explaining the event type predictions.\\n\\n- **w/o Event Graph**: To investigate whether the structured event information is beneficial for capturing the fine-grained relations between life events, we analyze the impact of with or without event graphs on the task of detecting information recall needs. Specifically, \u201cSEEN w/o Event Graph\u201d is the alias of the baseline model \u201cLongformer\u201d, which does not encode the event graph.\\n\\nThe ablation study results are shown in Table 3. We find that the performance degrades the most when the event graph is excluded, suggesting that enhancing the structured event information to the language model benefits the event type identification results. In addition, introducing the subtask of extracting support evidence into SEEN can also assist the model in detecting information recall needs. Furthermore, pre-training on the Multi-NLI dataset and fine-tuning on our NIR dataset is also beneficial for identifying the semantic relatedness between \\\\( D' \\\\) and \\\\( D \\\\). We further perform an experiment to analyze the relevance between the NLI task and the task of detecting information recall needs. Experimental results, reported in Appendix D.2, show that the NLI task is different from our task.\\n\\n8.2 Case Study of Support Evidence Extraction\\n\\nTo investigate the result of the support evidence extraction task, we perform the case study and plot the selected nodes as shown in Figure 4. Case (a) is an *Inconsistent* event since the host of the party described in the event sequence and the event graph (constructed from the reference story) are different. In this case, most of the selected nodes are correct, which are related to the described event and can explain why the event is inconsistent. In contrast to case (a), case (b) fails to select the related nodes and the prediction of the event type is also incorrect. Although the model selects all related nodes in case (c) correctly, the prediction of the event type is wrong. Here, the caller of 911 is the author, not the others, while SEEN classifies the *Inconsistent* event as *Additional*. Note that even though case (c) shows the event type identification is incorrect, SEEN is still capable of reminding the user that the event is forgotten or confused by providing the related nodes. In this way, SEEN can proactively provide information recall assistance.\\n\\nFurthermore, we also verify the relatedness between the tasks of detecting information recall needs and extracting support evidence. Table 4 reports the F-score of support evidence extraction depending on whether the result of detecting information recall needs is correct or wrong.\"}"}
{"id": "emnlp-2022-main-365", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"They had My brother's Engagement party his fianc\u00e9e hosted had in Her backyard\\n\\n(a) Event: Her parents hosted in backyard.\\n\\n(b) Event: we catered for 100 people\\n\\n(c) Event: someone call 911\\n\\nFigure 4: The examples of the support evidence extraction task. The nodes in the circle and square shapes are predicates, and entities (i.e., subjects or objects), respectively. The green nodes are the correct selections, the red nodes are ground truth but not selected, and the orange nodes are selected nodes but not ground truth.\\n\\nInconsistent; Predict: Inconsistent Label: Inconsistent; Predict: Additional Label: Inconsistent; Predict: Consistent\\n\\n9 Related Work\\n\\nRecently, more and more works show their interests in lifelogging. Some works have investigated lifelogging applications on lifestyle understanding (Doherty et al., 2011), diet monitoring (Maekawa, 2013), and contact tracing (Bengio et al., 2020). In addition, several studies have worked on the reactive information recall service. Gurrin et al. (2016, 2017, 2019, 2020) introduce visual lifelog retrieval tasks that aims at querying specific moments in a lifelogger's life. Chu et al. (2019) and Chu et al. (2020) construct a multimodal retrieval model that enables users to search their photos with textual queries. Yen et al. (2021b) propose a system to answer the questions about personal life experiences over personal knowledge base. In this work, we focus on detecting the need for a proactive information recall service along with the support evidences. The structured information, such as dependency parsing results, has proved the effectiveness in capturing the contextual interactions. For instance, the model proposed by Gong et al. (2022), BERT4GCN (Xiao et al., 2021), and SGNET (Zhang et al., 2020) integrate the dependency relations to leverage syntactic information. GreaseLM (Zhang et al., 2022) and LUKE (Yamada et al., 2020) integrate the external knowledge base by fusing token representations and entity representations from the language model and the additional embeddings, respectively. Here, we introduce an event graph into our proposed model to capture the relations of the life events.\\n\\n10 Conclusion\\n\\nInformation recall has attracted much attention in recent years. In contrast to previous studies, we present the task of proactive information recall support and construct NIR, the first human-annotated dataset, to investigate the need for information recall. In this work, we seek to detect event relations between life experiences retold at different times, and identify five event types to determine the time to trigger information recall. To identify the event types for information recall assistance, a pilot model\u2014structured event enhancement network (SEEN) is proposed. We construct an integration layer to fuse the structured information from the event graph into textual representations. In addition, SEEN provides the support evidence to the events by selecting the related nodes in the event graph. Users can consult the explanation to recall their past experiences. However, identifying inconsistent events is still challenging; this is left as future work. We also plan to construct an end-to-end system to extract life events in narratives and provide proactive information recall support. Besides, at the current stage, we utilize the gold event graph of each story in our experiments. In the future, we will explore the method to extract personal life events from document to construct a personal knowledge graph.\\n\\nAcknowledgements\\n\\nThis research was partially supported by National Science and Technology Council, Taiwan, under grants MOST 110-2221-E-002-128-MY3 and MOST 110-2634-F-002-050-.\"}"}
{"id": "emnlp-2022-main-365", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"11 Limitations\\n\\nAs time passes, many events continuously happen in our daily life. Consulting only one document that describes personal life experiences is not enough to identify the need for information recall assistance in the real-world application. However, the dataset that can be applied to investigate the issue of detecting information recall needs is hard to collect. We extend the Hippocorpus dataset, whose nature is in line with our work, to construct the NIR dataset. On the other hand, although our NIR dataset provides two versions of stories of the same events written at different times, we still cannot confirm which story, the previous one or the latter, is correct when contradictory. In this work, we propose a pilot exploration of proactively information recall assistance. To this end, we simply postulate that the story written at the previous time was correct when the user was still deeply impressed by the life events, so the story written at that time was used as a reference story. In addition, the number of inconsistent events is relatively lower in our dataset due to the human writing habit of avoiding uncertain events. In other words, when writing a diary, we always write the ones we exactly remember, which leads to difficulty collecting inconsistent events.\\n\\n12 Ethics Statement\\n\\nConsidering the potential infringement of privacy in the lifelog research, this section is an ethics-related elaboration for our dataset collection and a statement to address the risk of ethics for the methods. Our dataset \\\"NIR\\\" is an extension of an existing public dataset \\\"Hippocorpus\\\". The Hippocorpus dataset is collected from the crowdsourcing that the workers were to write the stories and the summaries twice at different times, and the other workers were to write the imagined version of the stories based on the summaries. The demographic information (age, gender) is optionally reported by the workers. However, the workers' IDs and names are not included in the Hippocorpus dataset. In other words, the dataset does not contain any personally identifiable information that would infringe on someone's privacy. In this work, we will only release the life event annotation and the support evidence of the event types in stories for research purposes. The stories in the Hippocorpus dataset will not be included in NIR. Hippocorpus can be accessed from the website.\\n\\nHowever, in the real world, lifelog applications could suffer from the risk of personal information leakage. The misuse of data and BAD (Broken As Designed) systems may violate the regulation or laws on data protection and privacy (GDPR, etc.). Hence, we leave the investigation of a privacy-aware lifelogging framework as future work.\\n\\nReferences\\n\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv:2004.05150.\\n\\nYoshua Bengio, R. Janda, Y. W. Yu, Daphne Ippolito, Max Jarvie, D. Pilat, Brooke Struck, Sekoul Krastev, and A. Sharma. 2020. The need for privacy with public digital contact tracing during the covid-19 pandemic. The Lancet. Digital Health, 2:e342 \u2013 e344.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.\\n\\nShaked Brody, Uri Alon, and Eran Yahav. 2022. How attentive are graph attention networks? In International Conference on Learning Representations.\\n\\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 9539\u20139549. Curran Associates, Inc.\\n\\nTai-Te Chu, Yi-Ting Liu, Chia-Chung Chang, An-Zi Yen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2020. Nlp301 at the ntcir-15 micro-activity retrieval task: incorporating region of interest features into supervised encoder. In Proceedings of the NTCIR-15 Conference.\\n\\nTzu-Hsuan Chu, Hen-Hsen Huang, and Hsin-Hsi Chen. 2019. Image recall on image-text intertwined lifelogs. In 2019 IEEE/WIC/ACM International Conference on Web Intelligence (WI), pages 398\u2013402. IEEE.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\\n\\nAiden R Doherty, Niamh Caprani, Vaiva Kalnikaite, Cathal Gurrin, Alan F Smeaton, Noel E O'Connor, et al. 2011. Passively recognising human activities.\"}"}
{"id": "emnlp-2022-main-365", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-365", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As Section 3.1 mentioned, we follow the definition in LiveKB (Yen et al., 2019, 2020) and annotate each event with polarity, explicit and implicit. In an explicit event, the predicate can be annotated by directly using the words in the story. In an implicit event, the predicate must be inferred from the context since the action of the event is not mentioned in the story. For implicit predicates, annotators were to choose the proper predicate by consulting FrameNet (Fillmore et al., 2003). For instance, two explicit events \\\\((\\\\text{She, hosted, party})\\\\) and \\\\((\\\\text{She, hosted in, backyard})\\\\) are included in Sentence (2). A single implicit life event \\\\((\\\\text{I, drink, the shots and cocktails})\\\\) is described in Sentence (3).\\n\\nFor the life event annotation, we invited five annotators who majored in linguistics or were English native speakers. Given a story, the annotators were to annotate life events in the story in triple form. To verify the quality of the annotation results, we sampled five stories (i.e., a total of 100 sentences and 129 life events) as reference story and asked a supervisor to label the life events. These stories were also assigned to the other four annotators. Since the three components in the triple were annotated as free text, we joined each component into a sequence.\\n\\nWe measured the agreement of each annotator with the supervisor via the Rouge-L (Lin, 2004) and F-scores for the life event triple and the explicitness of the life event, respectively. Here, the reason for utilizing the Rouge-L score to evaluate the agreement of life event triple annotation is that the components in a triple are text spans. We regard the annotation results of the supervisor as the reference to measure the annotation quality of the other annotators. The resulting average agreement of the life event triple and the explicitness of the life event were 0.87 and 0.80, respectively.\\n\\n**B Event Type Analysis on Ownership**\\n\\nNote that people recall not only their life events but also events involving family, friends, and acquaintances. We further investigated whether people tended to remember their own experiences better than those of others. At the current stage, as events are not labeled to indicate to whom the event belongs, we classified events that do not contain the\"}"}
{"id": "emnlp-2022-main-365", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Hyperparameter of each model.\\n\\n| Model                        | Average training time (hr/epoch) |\\n|------------------------------|----------------------------------|\\n| SEEN (BART-large)            | 0.47                             |\\n| SEEN (Longformer-base)       | 0.36                             |\\n| SEEN (Longformer-large)      | 0.87                             |\\n\\nTable 6: Time consumption to train the models.\\n\\nD Details of Experimental Setup\\n\\nFor each hyperparameter trial, we evaluate it on the validation set, and the one with the highest score on the event type identification task will be chosen. Apart from the hyperparameters, we evaluate our methods on the validation set 10 times in each epoch. The one with the highest score will be treated as the final checkpoint and reported its test set performance. The hyperparameters of each model are reported in Table 5. In addition, we use eight V100 GPUs to train our models and report the average training time in Table 6.\\n\\nD.1 Error Analysis\\n\\nTo investigate the performance of SEEN on each event type, Figure 6 shows the confusion matrix of our model in predicting Consistent, Inconsistent, and Additional. We find that SEEN predicts most Inconsistent events as Additional events. Firstly, although people often mix their experiences, we tend to avoid unclear events while writing, which results in the rareness of the Inconsistent event in our datasets. Apart from the problem of limited training data, this may be because determining that the described event conflicts with established facts require further reasoning on details such as the number of events that occurred, the order of activities, the friend's name, or the object description. Furthermore, since both Inconsistent and Additional cannot be found in the story context, it is more difficult to classify the event between these two types, which may cause misclassifying Inconsistent event as Additional event.\\n\\nD.2 Impact of Pre-training Task\\n\\nTo further compare the event type identification task with the NLI task, we experiment the different.\"}"}
{"id": "emnlp-2022-main-365", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Results of different pre-training task settings.\\n\\n| Method                        | F-score | F-score | F-score | F-score |\\n|-------------------------------|---------|---------|---------|---------|\\n| v 0.4075                      | 0.4715  | 0.0397  | 0.7113  |         |\\n| v 0.5480                      |         |         |         | 0.7556  |\\n| v 0.5572                      | 0.7512  | 0.0837  | 0.8367  |         |\\n\\nAs shown in Table 7, the method only trained on the Multi-NLI dataset does not work well in detecting information recall needs. That means the label definitions between NLI and NIR are marginal different, especially the Consistent events. We speculate the reason is that, in the \u201cEntailment\u201d class, most hypotheses are another way of saying the premises. However, the hypotheses are irrelevant to the premises if the relations are \u201cContradiction\u201d. By contrast, determining the event types of \u201cConsistent\u201d and \u201cInconsistent\u201d requires the ability to recognize subtle differences between the descriptions, such as the sequence of several life events. SEEN trained on both datasets achieves the highest performance. It means pre-training on the NLI task helps the model better capture semantic relatedness between two descriptions.\\n\\nD.3 Number of Integration Layers\\n\\nWe further compare the performance of SEEN with the different numbers of the integration layers. Experimental results shown in Table 8. We find that SEEN with five integration layers ($M = 5$) achieves the highest performance, which is the same as the result of GreaseLM. However, different from GreaseLM, there is no consistency in performance changes while $M$ decreases or increases. We think the reason is that the way SEEN fuses textual and structured features are by iteratively initializing the node representations with the updated token representations in each integration layer (The process is described in Section 5.3). While GreaseLM utilizes additional node embeddings as node representations, and concatenates the parts of hidden states from the language model and the node embeddings without re-initializing the node representations.\\n\\nTable 8: Performance of different number of the integration layer.\\n\\n| $M$ | F-score |\\n|-----|---------|\\n| 3   | 0.6559  |\\n| 4   | 0.6470  |\\n| 5   | 0.6654  |\\n| 6   | 0.6568  |\\n| 7   | 0.6414  |\\n| 8   | 0.6597  |\\n\\nD.4 Contribution of Different Fusion Layers\\n\\nTo investigate the contribution of each fusion layer in SEEN, we compute the distribution of the edge weights between nodes in the GAT layer. We denote the edges connecting to the related node and the unrelated node as $E_{RN}^+$ and $E_{RN}^-$, respectively. To show the difference between the edge weights, we tell whether the edge weights are higher than the threshold (0.5). If the edge weight is higher than the threshold, the edge is denoted as the positive case as \u201ctriggered edges\u201d. In the first GAT layer, the distributions of edge weights are relatively average. That leads to none of the edges is triggered edge. This might be that the first GAT layer attempts to capture structured information of the whole event graph by gathering the messages from the neighbor nodes. In contrast, 6.74% edges are triggered edges in the last GAT layer, which is much more than those in the first layer. We further compare the triggered edge distribution of $E_{RN}^+$ and $E_{RN}^-$, which are 14.93% and 4.92% in the last GAT layer, respectively. That is, compared with the first GAT layer, the last GAT layer in the integration layer aims to focus on the information related to the $eD'_i$. \\n\\n**TABLE 8**\\n\\n| $M$ | F-score |\\n|-----|---------|\\n| 3   | 0.6559  |\\n| 4   | 0.6470  |\\n| 5   | 0.6654  |\\n| 6   | 0.6568  |\\n| 7   | 0.6414  |\\n| 8   | 0.6597  |\"}"}
