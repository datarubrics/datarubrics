{"id": "acl-2024-long-195", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe argue that language-only models don't learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-T\\\\textsuperscript{EST}. These tasks highlight a fundamental gap between human linguistic understanding and the sensory-deprived linguistic understanding of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2\\\\_13B \\\\rightarrow \\\\textit{LLaMA 2 70B}) has no significant effect on H-T\\\\textsuperscript{EST} performance.\\n\\nWe bring in the philosophical case of Mary, who learns about the world in a sensory-deprived environment as a useful conceptual framework to understand how language-only models learn about the world (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limitations of linguistic knowledge acquired in the absence of sensory experience. Our code and data are available at <github.com/brucewlee/h-test>.\\n\\n1 Introduction\\n\\nThe advent of large language models (LLMs) like ChatGPT has revolutionized the field of computational linguistics, offering unprecedented insights into the mechanics of language processing and generation. Yet, a fundamental question persists: How does a language model \\\"experience\\\" and consequently learn language, and how does this differ from human language acquisition? Our aim lies in the systematic exploration of these perceptual-level differences, focusing on the distinction between the acquisition of language through textual data alone versus the multimodal sensory experiences inherent to human learning.\\n\\nHumans experience language through a rich sensory interplay involving visual, auditory, and tactile stimuli (Lieberman, 2002), integrating these sensory inputs to form a comprehensive understanding of language. In contrast, LLMs are trained on vast corpora of text data, quite devoid of sensory context. They process language as a series of tokens, learning patterns and associations between these tokens without any direct experience of the concepts they represent. This sensory deprivation raises critical questions about the completeness and depth of the \\\"understanding\\\" of language that LLMs can achieve through text-based learning alone. Roughly speaking, LLM \\\"perceives without sensing\\\".\\n\\nTo probe these questions, we introduce a series of tasks, collectively referred to as the H-T\\\\textsuperscript{EST}, inspired by philosophical thought experiments. H-T\\\\textsuperscript{EST} is designed to assess the ability of LLMs to learn the physical manifestation of language\u2014that is, how our language looks in our eyes. These tasks challenge the models to demonstrate a form of language understanding that is closer to human cognitive processes.\\n\\nThis exploration is motivated by the philosophical debate on the nature of knowledge and understanding, particularly the thought experiment involving Mary, a scientist who knows everything about the color red but has never experienced it directly. By analogy, can an LLM that has never...\"}"}
{"id": "acl-2024-long-195", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u201cseen\u201d or \u201cheard\u201d truly understand the full linguistic knowledge that derives its significance from visual and auditory experiences?\\n\\n\u2022 **Research Aim 1:** To identify and categorize the types of linguistic information that may remain perpetually out of reach for LLMs trained exclusively on text data, thereby highlighting potential linguistic \u201cblind spots\u201d (Zimmerman et al., 2023) in their learning.\\n\\n\u2022 **Research Aim 2:** To examine the extent to which a language model trained solely on text can achieve a meaningful level of visuospatial ability. Is it possible for these models to conceptualize and employ visual imagery (like $F + _{\u2192}E$) despite their training limitations? Researchers in neuroscience and psychology have asked similar research questions on congenitally blind people, and they report rather mixed results (Likova and Cacciamani, 2018; Ruggiero and Iachini, 2010; Vanlierde and Wanet-Defalque, 2004; Aleman et al., 2001).\\n\\nFigure 1 encapsulates the central philosophical conundrum that guides our inquiry. While there are a number of anecdotal examples of state-of-the-art LLMs struggling with understanding and utilizing these orthographic components of language (ChatGPT doesn\u2019t know if \u201cr\u201d is in the word Blueberry in Appendix H), we quantify and show that this deficiency is rather difficult to solve by commonly pursued LLM research directions. Adding more orthography data will not trivially solve this issue.\\n\\n**2 What Is It Like to Be a ChatGPT?**\\n\\nIn the realm of LLMs, a predominant belief is that scaling up data, model size, and computational power invariably leads to enhanced performance (OpenAI, 2023; Rae et al., 2021), sometimes exceeding expectations (Bubeck et al., 2023; Wei et al., 2022a,b). The recent report on the emergence of world representations within LLMs further emphasizes this point (Li et al., 2023; Nanda, 2023), suggesting a diminishing return on the effort to identify tasks that cannot be addressed by scaling.\\n\\nAn intriguing perspective on this topic, as proposed by Zimmerman et al. (2023), is the idea of comprehending the \u201cexperience\u201d of a ChatGPT-like model. This line of thought is crucial, as it highlights a common oversimplification: the tendency to describe LLMs as being trained in \u201ctext\u201d or \u201clanguage,\u201d as perceived by humans. This tendency to describe things as they seem to us is also discussed as anthropomorphic behavior (Shanahan, 2023). In defense of scientific accuracy, Zimmerman et al. (2023) advances this discussion by distinguishing between two types of information contained within language: Diegetic Information, which encompasses the internal, semantic, and propositional aspects of language, and Supradiegetic Information, which refers to the physical form of language, such as the shape of letters and sounds of syllables. Our interpretation aligns with this framework. Diegetic information in training data roughly corresponds to the semantics of a language. The efficacy of language modeling in teaching this aspect is evident from the consistent improvement in tasks like knowledge-based question answering and other semantics-heavy applications (Biderman et al., 2023; Chowdhery et al., 2022). However, the supradiegetic aspect, which involves the sensory perception of language, remains a less explored territory for language modeling. This is evident from the fragmental evidence of language models\u2019 struggles in downstream tasks with heavy stylistic features, such as automated essay scoring (Mizumoto and Eguchi, 2023; Uto et al., 2020) and readability assessment (Hou et al., 2022; Lee et al., 2021).\\n\\nTo anchor our discussion, we reference the thought experiment by Jackson (1986), featuring \u201cMary,\u201d a scientist confined to a black and white room, learning about the world through monochromatic means. Despite her knowledge of the descriptive aspects of human color vision, her understanding is incomplete until she \u201csees\u201d color firsthand.\\n\\n1. Mary possesses complete descriptive knowledge about human color vision prior to her release.\\n2. Yet, upon seeing color directly, Mary will likely acquire new knowledge.\\n3. Thus, certain knowledge is inherently sensory-dependent.\\n\\nDrawing parallels to our investigation, we posit that while LLMs like ChatGPT are adept at processing the diegetic content of language, their understanding of supradiegetic elements remains rudimentary. The fundamental difference in how humans and machines experience language inevitably leads to some level of incompleteness in learned linguistic knowledge (Driess et al., 2023), and therefore, will not be able to solve certain language tasks, inherently.\"}"}
{"id": "acl-2024-long-195", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1:\\n\\n| Task Criteria | J2 Ultra | Claude V2 | LLaMA 2 70B | Command-Li. | GPT 3.5 | Luminous Sup. |\\n|---------------|---------|-----------|-------------|-------------|---------|---------------|\\n| Uppercase     | 50.5    | 55.5      | 45.5        | 45.5        | 49.5    | 49.9          |\\n| Start Vowel   | 51.5    | 65.5      | 49.5        | 47.5        | 51.5    | 49.0          |\\n| End Punctuation | 48.5   | 52.5      | 47.5        | 54.5        | 52.5    | 52.0          |\\n| Palindrome    | 51.5    | 58.5      | 40.0        | 20.5        | 55.5    | 50.5          |\\n| End Ly        | 58.5    | 75.0      | 46.0        | 61.0        | 46.5    | 51.0          |\\n| Spelled Math  | 51.5    | 69.5      | 45.5        | 40.5        | 63.5    | 50.5          |\\n| Spelled Number | 51.5  | 55.0      | 45.0        | 49.0        | 49.5    | 49.5          |\\n| Rhyme         | 49.5    | 77.5      | 47.0        | 51.0        | 57.0    | 51.5          |\\n| Repeated Word | 49.5    | 53.0      | 32.5        | 54.0        | 50.5    | 51.0          |\\n| Hyphenated Word | 51.5 | 58.0      | 53.5        | 47.0        | 46.5    | 50.0          |\\n| Average       | 51.1    | 60.4      | 44.8        | 47.1        | 52.3    | 50.4          |\\n\\n### Table 2:\\n\\nWe report few-shot (at k = 50) accuracies on A/B classification tasks across six LLM services. We are reporting performances of the language-only models from AI21, Anthropic, Meta, Claude, OpenAI, and Aleph Alpha. Solving these tasks requires a swift understanding and interlocking of how a language looks or sounds. For us humans, such visual and auditory connections with language come naturally, but we argue that sensory-deprived LLMs find these tasks difficult to solve.\\n\\nEach binary classification task has 200 balanced test instances (100 A, 100 B), along with 50 balanced training instances (25 A, 25 B) for each task. This careful design allows for a controlled comparison of LLM performances.\"}"}
{"id": "acl-2024-long-195", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Making Progress on Language-only Modeling Does Not Trivially Solve H-Test: We test weaker models from the same family for models given in Table 2 under the same few-shot (at k = 50) setup. The graphs for the Luminous model family are also shown in magnified versions to show that we are not depicting a flat line.\\n\\nIn this paper, we use the fixed random seed of 12062023, and other experimental setups are detailed in Appendix B.\\n\\nPrecondition. Given each test case and a set of example cases, the goal is to classify whether the test case belongs more to Group A or Group B. Such a task formulation is also often referred to as few-shot prompting, where an LLM classifies the test cases from a number of pre-classified examples provided with the prompt. Here, we use few-shot prompting instead of a direct inquisition like \\\"Does this sentence start with a vowel?\\\" Such prompts linguistically reveal the classification criterion, effectively making the task solvable by step-by-step sequencing of memorized facts like \\\"Elephant starts with E; E is a vowel; therefore, true.\\\"\\n\\nResults. Table 2 reports H-\\\\text{T}EST accuracy on leading proprietary LLMs\\\\textsuperscript{1}. Considering that the random chance accuracy on this binary classification task is 50%, most tested models showed performance that is surprisingly close to this random chance baseline. These models struggled to solve H-\\\\text{T}EST despite being given 50 examples. This strongly suggests the confirmation of our initial conjecture that these sensory properties of language are \\\"blind spots\\\" in most language modeling formulations. Next up, we show that H-\\\\text{T}EST is not trivially solvable through the commonly-pursued LLM research directions: 1. training a stronger, larger LLM with more data (which will probably train more orthographic descriptions of language), 2. serving more few-shot examples (which also serves more orthography-related data), 3. alternative prompting methods to induce multi-step reasoning.\\n\\n\\\\textsuperscript{1}We excluded GPT-4 from Table 2 because GPT-4 is trained with multi-modality in mind (OpenAI, 2023).\"}"}
{"id": "acl-2024-long-195", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Current LLMs Do Not Solve H-T EST Better with More Examples: We test four models from Figure 2 and test with different number of examples (k = {4, 14, 28, 50}). Though we acknowledge that subtask accuracy does vary at different few-shot setups, giving more or fewer examples does not significantly alter the H-T EST performance on average.\\n\\nEver, this initial performance of the GPT-4 API couldn't be replicated when we accessed the API again a few weeks later. Instead, GPT-4o replicated a similar performance. Due to this inconsistency, the initial GPT-4 performance was removed from Table 6 and replaced with the GPT-4o performance. In Section 5, we make an attempt to explain this surprising improvement by comparing multi-modal and mix-of-experts open-source models. However, we fail to explain the improvement in GPT-4o level. Though we are limited by the amount of knowledge on how GPT-4 was trained in a multi-modal way, such a result does hint at a good assumption that H-T EST is indeed solvable when given a well-devised multi-modal system, or possibly, a large enough system (Wei et al., 2022b). However, without a thorough report, no presumption about GPT-4 or GPT-4o model architecture can be made.\\n\\nObservation 2. Number of Examples Has Minimal Impact. Few-shot prompting that we have been using to report the results in Table 2 and Figure 2 is also commonly referred to as in-context learning (ICL) for its property to allow language models to learn tasks given only a few examples in the form of demonstration (Dong et al., 2023; Brown et al., 2020). While ICL can often be as effective as fine-tuning (Duan et al., 2023; Liu et al., 2022), certain studies report that language models internally and temporarily learn these examples to make inferences (Aky\u00fcrek et al., 2023; Dai et al., 2022; Min et al., 2022). Combined with reports of cases where the increasing number of examples can improve performance (Duan et al., 2023; Chen et al., 2023b), one would be able to make a convincing argument against our proposals if performance scaled with the number of examples. In such a case, the few-shot examples given in H-T EST are indeed \\\"learnable.\\\" Figure 3 reports respective performances with a varying number of examples. We only test stronger language-only models in light of the recent reports that ICL itself can be a learned ability (Wei et al., 2023; Olsson et al., 2022). Even though the range of the few-shot setup tested is rather wide (from k = 4 to k = 50), the effect on the average accuracy is minimal. That is, the number of examples given neither increases nor decreases performance significantly, strongly hinting that the LLM is simply not learning from H-T EST examples cases. Such an observation strengthens our initial conjecture that the visual-auditory properties of language that we measure with H-T EST are analogous to the \\\"blindspot\\\" of current language modeling formulations and will be difficult to learn without sensory experience of language. Now, given that the few-shot examples given in H-T EST are not \\\"learnable,\\\" is there an experimental setup that we deem is generally helpful for LLMs but rather misleads them in H-T EST?\"}"}
{"id": "acl-2024-long-195", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Category: Prompt / Response on H-T EST\\n\\nFew-Shot Examples\\n\\nInput: Opera through the telescope School.\\nLabel: A\\n\\nInput: Octopus.\\nLabel: B\\n\\nInput: Elevator in space Lion!\\nLabel: A\\n\\nInput: Giraffe.\\nLabel: B\\n\\nInput: Opera through the telescope Insect.\\nLabel: A\\n\\nInput: Eagle in space Butterfly.\\nLabel: B\\n\\nTest Case\\n\\nInput: Kangaroo in the park Umbrella.\\nLabel: B\\n\\nCoT Response [Wrong]\\n\\nIn the given input sentence, the phrase \u201cin the park\u201d suggests a peaceful and natural setting, which might be more aligned with labels related to nature and tranquility. On the other hand, the word \u201cUmbrella\u201d brings to mind protection from the elements, which could be associated with indoor activities or certain weather conditions. Considering these factors, it is more likely that \u201cKangaroo in the park Umbrella\u201d belongs to label B, which represents a description of animals in their natural habitats or surroundings. Therefore, the final answer is: \u201cKangaroo in the park Umbrella.\u201d\\n\\nTable 3: Misleading CoT\\n\\nWe report an example of a misleading CoT response generated from Command.\\n\\nObservation 3. Deliberate Reasoning Often Decreases Performance.\\n\\nLastly, we prompt LLMs to generate a Chain-of-Thought (CoT) before answering (Wei et al., 2022c). CoT is a widely explored prompting technique that is reported to consistently boost performance in a wide range of complex reasoning tasks, including mathematical, commonsense, symbolic, and logical reasoning (Chu et al., 2023). With the development of Tree-of-Thought, Yao et al. (2023) draws a particular connection to the \u201cSystem 2\u201d thinking of humans. If LLMs benefit from such logical, step-by-step semantic reasoning on H-T EST, this can also imply that H-T EST is fundamentally solvable by developing stronger language-only models. On the other hand, if CoT decreased or did not impact performance, this can strongly cue that H-T EST is well-constructed to test our initial research target of simple, sensory-dependent experiential knowledge.\\n\\nIn Figure 4, we observe that CoT decreases performances in general, confirming our hypothesis. GPT-4, which previously solved H-T EST with over 80% accuracy, also experienced a drop in performance, more strongly confirming our case. To better understand why CoT decreases performance, we analyze the CoT responses generated by the LLMs. Our findings reveal that while LLMs can produce coherent and logical chains of thought, these thought processes often do not align with the sensory aspects required to solve H-T EST as shown in Table 3. For instance, in tasks related to the visual representation of letters, LLMs tend to rely on abstract concepts, which do not translate into a genuine understanding of the visual or auditory characteristics. We discuss further experimental detail in Appendices B, C, and D.\\n\\nSanity Check: Human Performance.\\n\\nTo validate whether the tasks within the H-T EST are indeed as intuitive for humans as we hypothesized. Participants in the study were presented with the few-shot examples of the H-T EST tasks without prior explanation or training. Participants were asked to identify the underlying pattern or rule that distinguished Group A from Group B examples. As shown in Appendix E, human participants achieved perfect scores on H-T EST. In the \u201cPalindrome\u201d task, where one has to identify that Group A words read the same forward and backward, human accuracy was 100%, reflecting our natural ability to visually recognize patterns. Similarly, in the auditory \u201cRhyme\u201d task, participants could effortlessly identify rhyming words. We believe that this result reinforces the notion that human language comprehension is a multimodal process that current LLMs do not replicate.\\n\\n4 Reasoning Challenge: Letter Geometry\\n\\nSo far, we have presented H-T EST and showed that there are certain properties of language that are difficult to learn, as standard sensory-deprived language models do not go through the standard model of perception (stimulus \u2192 sense \u2192 perceive) in learning language. A particular and only exception was GPT-4 among the tested models. As a further challenge that tests not only the visual-auditory understanding of letters but also visuospatial reasoning abilities, we devise a more challenging task: Letter Geometry. As shown in Table 4, this task involves operations like flipping, rotation, addition, and subtraction on letters. We do not include this as a standard part of the H-T EST as performing well on this task likely involves step-\"}"}
{"id": "acl-2024-long-195", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given the letter \\\"Z\\\", what is it most likely to look like when rotated 90 degrees clockwise?\\n\\nN, A, D, T\\n\\nGiven the symbol \\\"3\\\", what does it most likely look like when rotated 180 degrees counterclockwise?\\n\\nd, E, B\\n\\nGiven the letter \\\"u\\\", what is it most likely to look like when flipped along an imaginary vertical line in the middle?\\n\\n5, f, E, u\\n\\nGiven the letters \\\"L\\\" and \\\"I\\\", what is it most likely to look like when subtracted (L - I)?\\n\\n_, A, 0, J\\n\\nGiven the letter \\\"u\\\", rotate it 180 degrees counterclockwise and add it below the original letter. What is it most likely to look like?\\n\\nH, c, J, Y\\n\\nTable 4: Letter Geometry Examples: Five examples of our letter geometry task with answers underlined.\\n\\nby-step reasoning capabilities, which is not what we intended for H-T EST. However, we are still very interested in this letter geometry task as the smallest-sized token that a language model can process is often a letter (Elhage et al., 2021), and the elements of the operations (e.g., 'F' and 'E') have no apparent semantic, phonetic, or logographic correlation that can be derived from the model's training samples (Cheng and Lin, 2013; Lee, 2009). This multiple-choice task encourages the utilization of visual imagery (e.g., 'F' + '_' = 'E') for above random baseline performance of 25%.\\n\\nThe evaluation results are depicted in Figure 5. Most models remain close to a hypothetical baseline, struggling with the sensory-deprived nature of their training. This suggests that the capacity to process visuospatial information in the language is not simply a function of visual-auditory understanding of language but also of how models are trained to reason through sensory-like experiences.\\n\\nWe also assessed the performance improvement across different models from the same research labs. In the case of the Command models, the enhancement was marginal, at 1.11%, which essentially translates to one additional correct response.\\n\\nA standout observation was with OpenAI's models: the shift from GPT-3.5 to GPT-4o marked a substantial increase of around 15%.\\n\\nFigure 5: H-TEST vs. Letter Geometry: We compare the accuracy of seven models on H-T EST and Letter Geometry. The red line represents the linear best fit.\\n\\n5 GPT-4o and Claude 3 Started Solving H-T EST but How?\\n\\nIn Figure 2, we have reported the seemingly unexplainable (jumping) performance improvement on H-T EST from GPT-3.5 (language-only) to GPT-4 (multimodal). We observe a similar improvement from Claude 2 (language-only) to Claude 3 (multimodal) in Table 5. This result is important as it shows that H-T EST is indeed solvable (by a GPT-4-level system), but not through conventionally discussed language-only modeling techniques.\\n\\nAs further detailed in Table 6, GPT-4o reaches human-level performance in palindrome and spelled math tasks, while all other tested models struggle at 50% random baseline performance. But considering that GPT-4o and Claude 3 are multimodal, they lie beyond our initial claims.\\n\\nWe do not understand GPT-4o and Claude 3 Opus's performance on H-T EST. We had three hypotheses aimed at replicating this success:\\n\\n1. Data: training more orthography-specific data can improve H-T EST score,\\n2. Modality: vision-modality can improve H-T EST score,\\n3. Architecture: mixture-of-experts (MoE) architecture (Jiang et al., 2024) can improve H-T EST score. However, we fail at pinpointing what really made solving H-T EST possible.\\n\\nAnalysis 1. Training with more orthography-specific language data does not improve H-T EST score. This is our strongest observation so far that proves LLMs don't naturally learn to solve H-T EST tasks. We produced 1000 training instances...\"}"}
{"id": "acl-2024-long-195", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Different Approaches and Their Impact on H-T EST Performance\\n\\nWe report comparisons across different modeling approaches: scaling (LLaMA 2), multi-modal expansion (N.H. Yi 34B to LLaV A 34B), and proprietary methods (Claude and GPT). The table highlights that while some approaches lead to improvements, none consistently solve the H-T EST per task in H-T EST, and fine-tuned gpt-3.5-turbo-0613 ten different times accordingly. After training for three epochs on each task, we evaluate them on H-T EST at $k = 50$ and observe that no significant performance improvement was achieved (Figure 6; Appendix F). Across all tasks, the performance change (in both directions) was not statistically significant, suggesting that fine-tuning on orthographically rich data does not bridge the sensory experience gap.\\n\\nAnalysis 2. Multi-modality does not automatically improve H-T EST performance. In Table 5, we present a performance comparison between Nous Hermes Yi 34B and LLaV A V1.6 34B (Liu et al., 2023), the latter being a vision-language model based on the former. At the time of writing, LLaV A V1.6 34B is the strongest open-source multi-modal model available. Despite the addition of visual modality, we observe that simply incorporating visual data into the training does not result in a straightforward improvement in H-T EST performance. LLaV A V1.6 showed around random-chance performance, which is clearly not an indication of the model's ability to solve H-T EST.\\n\\nWe initially planned on running additional MoE experiments using instruction-tuned Mistral-7B-Instruct-V2 (Jiang et al., 2023) and Mixtral-8X7B-Instruct-V1 (Jiang et al., 2024) but we skipped this evaluation as the models did not understand that task (at $k = 50$) more than 70% of the test cases. More about this difficulty of evaluating H-T EST on smaller models is discussed in Appendix C. We could not specify a hypothesis that single-handedly and meaningfully improved H-T EST performance. Fortunately, these results strengthen our initial claim on the limitations of language-only models, but we fail to explain how GPT-4o and Claude 3 Opus are solving H-T EST.\\n\\nBackground\\nPhilosophical Case of Experiential Knowledge\\n\\nThe philosophical debate on the nature of knowledge, especially the contrast between experiential and propositional knowledge, is vividly illustrated in Jackson's \\\"Mary's Room\\\" thought experiment. Jackson (1986) contends that propositional knowledge is inadequate without experiential knowledge. This is exemplified by Mary, who, despite her extensive propositional knowledge about color, gains a new type of understanding only upon experiencing color firsthand (Jackson, 1986, 1982). This thought experiment has sparked extensive discourse in philosophy, particularly around the concept of qualia, which refers to individual instances of conscious experience (Dennett, 1993).\"}"}
{"id": "acl-2024-long-195", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: GPT-3.5 Doesn't Solve H-Test Better with Fine-Tuning\\n\\nWe report gpt-3.5-turbo-0613 original vs. fine-tuned performances across H-Test. Appendix F.\\n\\nThomas Nagel, in his paper \u201cWhat is it Like to be a Bat?\u201d, explores the subjective nature of experience, arguing that an objective understanding of the mind must account for the subjective character of experience (Nagel, 1980). This work is pivotal in understanding the limits of propositional knowledge and the importance of experiential understanding. In a similar line of work, Chalmers (1997) discusses the \u2018hard problem\u2019 of consciousness, emphasizing the gap between explanatory physical processes and the qualitative experience of consciousness (Chalmers, 1997). This reinforces that propositional knowledge might not be sufficient in fully explaining conscious experience.\\n\\nDennett (1993), offers a contrasting view, arguing that subjective experiences (qualia) can be explained in terms of physical processes and brain functions (Dennett, 1993). This perspective is crucial in the debate over whether experiential knowledge is any different from propositional knowledge.\\n\\nLarge Language Models and Language Tasks\\n\\nThe development and rise of large language models (LLMs) like GPT-3 have brought about significant advancements in the field of natural language processing. These models, however, process language fundamentally differently from humans, who experience language through sensory inputs (Bender and Koller, 2020; Brown et al., 2020). This difference in processing mechanisms poses challenges in comprehensively modeling human language understanding (Hinton et al., 2015; Russell and Norvig, 2010). Text-only training of LLMs, while effective for many applications, shows limitations in capturing the full essence of human language understanding, especially in tasks requiring sensory and stylistic comprehension (Manning and Schutze, 1999). Such issues has been investigated before but we believe our work is first to pinpoint and quantify the orthographic deficiencies of LLMs (Rust et al., 2022; Belinkov and Bisk, 2018). These limitations underscore the gap in experiential knowledge inherent in current LLMs, as also demonstrated in this research.\\n\\nThe concept of sensory deprivation in LLMs roughly parallels the experiences of visually impaired individuals. Unlike visually impaired persons, who often develop heightened abilities in other senses, LLMs lack such compensatory mechanisms (Likova and Cacciamani, 2018; Amedi et al., 2017). This highlights the unique challenge in AI of developing models capable of processing and understanding language akin to human sensory experiences (Mitchell and Lapata, 2010).\\n\\n7 Conclusion\\n\\nOur research revisits the philosophical case of Mary\u2019s Room, illustrating that LLMs, much like Mary, lack a comprehensive understanding of language due to their sensory-deprived training. We empirically show that scaling model size and data, while beneficial for some aspects of language understanding, does not address the deficiency in orthographic abilities. This finding suggests that there are some fundamental limitations in the current language-modeling paradigm that prevent a holistic understanding of language and that it is still an open research direction.\"}"}
{"id": "acl-2024-long-195", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nIt is important to recognize several limitations inherent to our research methodology and scope:\\n\\n1. The H-T \\\\textit{H-T\\\\textsuperscript{E}ST} emphasizes visual and auditory capabilities and does not consider other sensory inputs like olfactory, gustatory, and tactile perceptions that influence human language understanding. Our conclusions are thus confined to the explored sensory attributes, potentially overlooking the integral role of these unexamined senses in comprehensive language processing.\\n\\n2. Our analysis significantly relies on proprietary data, particularly the performance metrics and training methodologies of language models like GPT-4. The proprietary nature of these models limits our ability to conduct independent verification, introducing a dependency on the accuracy and integrity of third-party reporting.\\n\\n3. The assumption that multi-modal training inherently resolves the sensory gap is also a simplification. The integration of multi-modal data into language model training is a complex task that may introduce new challenges and biases, which our study does not address in detail.\\n\\n4. Our application of philosophical concepts such as qualia to AI and language models operates metaphorically. The subjective nature of qualia and its implications for consciousness in humans do not translate directly to the objective, computational processes of language models and no anthropomorphic generalization should be made.\\n\\n5. It is also crucial to note that the field of AI and language modeling is rapidly evolving. The capabilities of language models are continually advancing, and newer models may exhibit sensory processing abilities not captured by the current iteration of the H-T \\\\textit{H-T\\\\textsuperscript{E}ST}. Our study establishes a dichotomy between human and machine processing of language, not accounting for the possibility that language models may develop distinct, non-human sensory processing methods that are effective in their unique computational context.\\n\\nReferences\\n\\n- Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023. What learning algorithm is in-context learning? investigations with linear models.\\n- Andr\u00e9 Aleman, Laura Van Lee, Mariska HM Mantione, Ilse G Verkoijen, and Edward HF de Haan. 2001. Visual imagery without visual experience: evidence from congenitally totally blind people. Neuroreport, 12(11):2601\u20132604.\\n- Amir Amedi, Shir Hofstetter, Shachar Maidenbaum, and Benedetta Heimler. 2017. Task selectivity as a comprehensive principle for brain organization. Trends in cognitive sciences, 21(5):307\u2013310.\\n- Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic and natural noise both break neural machine translation. In International Conference on Learning Representations.\\n- Emily M Bender and Alexander Koller. 2020. Climbing towards nlu: On meaning, form, and understanding in the age of data. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 5185\u20135198.\\n- Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR.\\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n- S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.\\n- David J Chalmers. 1997. The conscious mind: In search of a fundamental theory. Oxford Paperbacks.\\n- Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. 2023a. When do you need chain-of-thought prompting for chatgpt? arXiv preprint arXiv:2304.03262.\\n- Jiuhai Chen, Lichang Chen, Chen Zhu, and Tianyi Zhou. 2023b. How many demonstrations do you need for in-context learning?\\n- C. M. Cheng and S. Y. Lin. 2013. Chinese orthographic decomposition and logographic structure. Reading and Writing, 26:1111\u20131131.\"}"}
{"id": "acl-2024-long-195", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\\n\\nZheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2023. A survey of chain of thought reasoning: Advances, frontiers and future.\\n\\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559.\\n\\nDaniel C Dennett. 1993. Consciousness explained. Penguin uk.\\n\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey on in-context learning.\\n\\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378.\\n\\nHanyu Duan, Yixuan Tang, Yi Yang, Ahmed Abbasi, and Kar Yan Tam. 2023. Exploring the relationship between in-context learning and instruction tuning.\\n\\nN Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, et al. 2021. A mathematical framework for transformer circuits. Transformer Circuits Thread, 1.\\n\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\\n\\nShudi Hou, Simin Rao, Yu Xia, and Sujian Li. 2022. Promoting pre-trained lm with linguistic features on automatic readability assessment. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, pages 430\u2013436.\\n\\nFrank Jackson. 1982. Epiphenomenal qualia, philosophical quarterly, 32, pp. 127\u201336. (1986). What Mary didn't know, Journal of Philosophy, 83:291\u201395.\\n\\nFrank Jackson. 1986. What mary didn't know. The journal of philosophy, 83(5):291\u2013295.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. arXiv preprint arXiv:2401.04088.\\n\\nBruce W Lee, Yoo Sung Jang, and Jason Hyung-Jong Lee. 2021. Pushing on text readability assessment: A transformer meets handcrafted linguistic features. arXiv preprint arXiv:2109.12258.\\n\\nS. O. Lee. 2009. The korean alphabet: an optimal featural system with graphical ingenuity. Written Language & Literacy, 12(2):202\u2013212.\\n\\nKenneth Li, Aspen K. Hopkins, David Bau, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. 2023. Emergent world representations: Exploring a sequence model trained on a synthetic task.\\n\\nPhilip Lieberman. 2002. On the nature and evolution of the neural bases of human language. American Journal of Physical Anthropology: The Official Publication of the American Association of Physical Anthropologists, 119(S35):36\u201362.\\n\\nLora T Likova and Laura Cacciamani. 2018. Transfer of learning in people who are blind: enhancement of spatial-cognitive abilities through drawing. Journal of visual impairment & blindness, 112(4):385\u2013397.\\n\\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Advances in Neural Information Processing Systems, volume 35, pages 1950\u20131965. Curran Associates, Inc.\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. arXiv preprint arXiv:2304.08485.\\n\\nSheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych. 2023. Are emergent abilities in large language models just in-context learning? arXiv preprint arXiv:2309.01809.\\n\\nChristopher Manning and Hinrich Schutze. 1999. Foundations of statistical natural language processing. MIT press.\\n\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work.\\n\\nJeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive science, 34(8):1388\u20131429.\\n\\nAtsushi Mizumoto and Masaki Eguchi. 2023. Exploring the potential of using an ai language model for automated essay scoring. Research Methods in Applied Linguistics, 2(2):100050.\"}"}
{"id": "acl-2024-long-195", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-195", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task              | J2 Mid | J2 Ultra | Titan Lite | Claude Instant V1 | Claude V1 | Claude V2 | Llama 2 13B | Llama 2 70B | Luminous Base | Luminous Extended | Luminous Supreme |\\n|-------------------|--------|----------|------------|-------------------|-----------|-----------|-------------|-------------|----------------|-------------------|------------------|\\n\\nWe accessed all APIs (\\\"anthropic.claude-instant-v1\\\", \\\"anthropic.claude-v1\\\", \\\"anthropic.claude-v2\\\", \\\"ai21.j2-mid-v1\\\", \\\"ai21.j2-ultra-v1\\\", \\\"command-light\\\", \\\"command\\\", \\\"gpt-3.5-turbo-0613\\\", \\\"gpt-4-0613\\\", \\\"llama 2 13B\\\") between the third week of November and the third week of December 2023. We accessed LLaMA through Replicate, and J2, Claude, and Titan through Amazon Bedrock. All other APIs were accessed through the respective providers. For all experiments, the model temperature was set at 0.7, and the random seed (for generating data) was set at 12062023. The results and exact dataset used in this experiment are reported in Table 5.\\n\\n### Table 5: Experimental Details\\n\\n| Metric                  | J2 Mid | J2 Ultra | Titan Lite | Claude Instant V1 | Claude V1 | Claude V2 | Llama 2 13B | Llama 2 70B | Luminous Base | Luminous Extended | Luminous Supreme |\\n|-------------------------|--------|----------|------------|-------------------|-----------|-----------|-------------|-------------|----------------|-------------------|------------------|\\n\\n*Note:* The table continues with more detailed metrics and values not fully visible in the image.\"}"}
{"id": "acl-2024-long-195", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"research are recoverable by setting the random seed to 12062023. All few-shot tests are conducted with\\nmax_new_tokens parameters set to five. Full code and data at <github.com/brucewlee/h-test>.\\n\\nC On Model Responses and Small Open-Source Models\\n\\nIn our experiments, we observed that major models, including ChatGPT, produced responses in the\\nintended format of a single letter (A or B), as explicitly instructed in our prompts. Indeed, this adherence\\nto the output format was a critical aspect of our experimental design to ensure consistency in the evaluation\\nof model responses. As documented in Appendix C of our paper, each prompt concluded with the directive\\n\\\"(Respond in one letter and nothing else)\\\", which was intended to guide the models toward the desired\\nresponse format.\\n\\nSmaller, open-source models (tested: LLaMA 7B, LLaVA, Mistral, Mixtral, etc.) almost always didn't\\nunderstand the task (didn't respond in A or B). This lack in in-context learning ability is reminiscent of a\\nrecent work (Lu et al., 2023). They showed a tendency to generate responses that were either irrelevant\\nor completely off-topic, which we categorized as \\\"gibberish.\\\" Due to this significant deviation from\\nthe expected task performance, these models were subsequently excluded from our final analysis. This\\ndecision was based on the premise that their responses did not provide meaningful data for evaluating the\\nspecific language understanding capabilities we were investigating, as the response of the language model\\nthat could not process the requirements of the task was irrelevant to our investigation.\\n\\nThe issue of difficult-to-parse response was pronounced in our Chain-of-Thought (CoT) experiment\\nsetup. In this setup, models were prompted to generate a chain of thought before arriving at a conclusion\\n(A or B). We found that this approach sometimes led to responses that did not reach a specific answer.\\nTo account for this discrepancy, we reported two different accuracy metrics in Figure 4 of our paper:\\n\\\"Average Accuracy\\\" and \\\"Adjusted Accuracy\\\". The \\\"Adjusted Accuracy\\\" metric specifically addressed\\ncases where models provided the correct answer but not in the prescribed single-letter format.\\n\\nD More on Parsing CoT Model Responses\\n\\nIn implementing this CoT setup, we follow the conventional CoT evaluation setup where an LLM to be\\ntested generates a lengthy CoT response (which is contrastive to the one-letter response in the previous\\nsetups) and another LLM reads and determines which option (A or B) the lengthy response is pointing to\\n(Chen et al., 2023a). This two-step process inevitably generates cases where the latter LLM cannot clearly\\ninterpret what the former LLM's response pointed to, and we call these cases out-of-context responses.\\nThe gray line in Figure 4 shows adjusted accuracy, excluding these out-of-context responses. Hence, the\\nCoT@14 datapoint's adjusted accuracy (in gray) represents an accuracy of less than 200 test instances per\\nsubtask.\\n\\nE Human Baseline Performance on H-TEST\\n\\n| Task  | Human A | Human B | Human C | Human D |\\n|-------|---------|---------|---------|---------|\\n| k = 4 | 10/10   | -       | -       | -       |\\n| k = 14| -       | 10/10   | -       | -       |\\n| k = 28| -       | -       | 10/10   | -       |\\n| k = 50| -       | -       | -       | 10/10   |\\n\\nTable 7: Human Performance.\\n\\nWe report human performance in identifying the patterns behind the given few-shot\\nelements in A/B classification tasks in H-TEST. The numbers indicate how many correct rules were identified out\\nof ten H-TEST tasks.\\n\\nIn light of the H-TEST's design, which is readily and trivially solvable once the classification pattern\\nbehind the given few-shot examples is understood, our approach to establishing a human baseline focused\\nnot on direct label classification as was done with the language models but on the participants' ability to\\nidentify and verbally articulate these patterns.\\n\\nParticipants: Four English-native undergraduate students from a US university were recruited.\\nProcedure: Participants were individually presented with the same few-shot examples given to LLMs from the\"}"}
{"id": "acl-2024-long-195", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Participants were asked to identify the underlying pattern or rule that distinguished Group A from Group B examples. Upon identifying the pattern, participants were instructed to verbally articulate this rule as precisely as possible.\\n\\nData Collection:\\nWe only considered a participant's answer right when it directly matched the actual classification criteria designed for each task. We consider a response wrong if the participant took more than three minutes. We told the participants that \\\"sentences A and B are different based on a linguistic rule\\\". If the participant asked for clarification, we responded \\\"linguistic rules are patterns like ... (Starts with a certain pattern ...)\\\". No further clarification was provided.\\n\\nTable 8:\\nH-T EST cannot be conveniently solved with more data: Performance of gpt-3.5-turbo-0613 before and after further training with various H-T EST tasks (\u223c1000 train instances each task). All tests were conducted under k = 50 setup. Underscored values represent the cases where the training data task (column) was the same as the test data task (row), which is often conveniently referred to as in-domain.\\n\\nIn our efforts to understand the limits of language-only models in processing tasks that require sensory experience, we conducted a series of fine-tuning experiments on GPT-3.5, specifically targeting its performance on the H-T EST. We selected GPT-3.5 for fine-tuning due to its advanced capabilities and widespread use in both academic and industrial settings. The model was fine-tuned using a custom dataset derived from the H-TEST, comprising 1,000 instances for each task. Fine-tuning was performed over three epochs on the official OpenAI platform. Across all tasks, the improvements were not significant, suggesting that fine-tuning on orthographically rich data does not bridge the sensory experience gap.\"}"}
{"id": "acl-2024-long-195", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Input: \\\"Engine on the wall Octopus...\\\" Label: A\\nInput: \\\"Ice cream ! in the forest Teacher\\\" Label: B\\nInput: \\\"Island in the morning Elevator!\\\" Label: A\\nInput: \\\"Owl with interest ... Friend\\\" Label: B\\nInput: \\\"Penguin with precision River?\\\" Label: A\\nInput: \\\"Iguana all night ... Scientist\\\" Label: B\\nInput: \\\"Penguin to music Owl?\\\" Label: A\\nInput: \\\"Lion with interest ... Penguin\\\" Label: B\\nInput: \\\"Teacher with interest Dog!\\\" Label: A\\nInput: \\\"Cat ! to music Butterfly\\\" Label: B\\nInput: \\\"Cat in the hall Insect...\\\" Label: A\\nInput: \\\"Astronaut on the wall ... Opera\\\" Label: B\\nInput: \\\"Owl in the morning Book!\\\" Label: A\\nInput: \\\"Insect ! in the studio Book\\\" Label: B\\nInput: \\\"Lion in the kitchen House!\\\" Label: A\\nInput: \\\"Insect in the hall ? Flower\\\" Label: B\\nInput: \\\"Friend very quickly House!\\\" Label: A\\nInput: \\\"Butterfly to music ... Penguin\\\" Label: B\\nInput: \\\"Penguin in the forest Astronaut?\\\" Label: A\\nInput: \\\"Apple ? every second Car\\\" Label: B\\nInput: \\\"School in the sky Neighbor...\\\" Label: A\\nInput: \\\"Orchestra on the wall ... School\\\" Label: B\\nInput: \\\"Dog with passion River?\\\" Label: A\\nInput: \\\"Octopus ... in the forest Urchin\\\" Label: B\\nInput: \\\"Elephant in the park Urchin?\\\" Label: A\\nInput: \\\"Teacher to music ... Car\\\" Label: B\\nInput: \\\"Mountain in the forest Car!\\\" Label: A\\nInput: \\\"Albatross in the forest . Butterfly\\\" Label: B\\nInput: \\\"Teacher in the morning Butterfly.\\\" Label: A\\nInput: \\\"Octopus in the morning ! Flower\\\" Label: B\\nInput: \\\"Albatross with precision Butterfly...\\\" Label: A\\nInput: \\\"Penguin in the studio ! Giraffe\\\" Label: B\\nInput: \\\"House in space Butterfly?\\\" Label: A\\nInput: \\\"Dog ? on the wall River\\\" Label: B\\nInput: \\\"Orchestra in the park Neighbor!\\\" Label: A\\nInput: \\\"Teacher in space ... Penguin\\\" Label: B\\nInput: \\\"Unicorn through the telescope Insect...\\\" Label: A\\nInput: \\\"Dog ? in the park Engine\\\" Label: B\\nInput: \\\"Owl to music Owl...\\\" Label: A\\nInput: \\\"Mountain in the sky ? Teacher\\\" Label: B\\nInput: \\\"Neighbor very quickly Urchin.\\\" Label: A\\nInput: \\\"Umbrella ? along the valley Car\\\" Label: B\\nInput: \\\"Kangaroo in the park Umbrella.\\\" Label: A\"}"}
{"id": "acl-2024-long-195", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"H-Test: Uppercase\\n\\nInput: \\\"engine jumps all night accurately.\\\" Label: A\\nInput: \\\"penguin runs all night intently.\\\" Label: B\\nInput: \\\"rivEr drives with precision eagerly.\\\" Label: A\\nInput: \\\"tree sleeps in space quietly.\\\" Label: B\\nInput: \\\"owl obserVes delicious meals slowly.\\\" Label: A\\nInput: \\\"Penguin flows in the studio happily.\\\" Label: B\\nInput: \\\"apple Listens in the studio expertly.\\\" Label: A\\nInput: \\\"airport calculates in the studio enthusiastically.\\\" Label: B\\nInput: \\\"unicorn obServes very quickly fast.\\\" Label: A\\nInput: \\\"owl reads along the valley skillfully.\\\" Label: B\\nInput: \\\"airport travels very quicklY melodiously.\\\" Label: A\\nInput: \\\"Insect dances along the valley carefully.\\\" Label: B\\nInput: \\\"insecT paints in the hall attentively.\\\" Label: A\\nInput: \\\"tree sleeps all night adventurously.\\\" Label: B\\nInput: \\\"friend runs in the morning sSkillfully.\\\" Label: A\\nInput: \\\"book paints in the studio attentively.\\\" Label: B\\nInput: \\\"opera sings in the park eAgerly.\\\" Label: A\\nInput: \\\"Insect paints in the kitchen creatively.\\\" Label: B\\nInput: \\\"opera groWs in the forest brightly.\\\" Label: A\\nInput: \\\"owl grows all night gracefully.\\\" Label: B\\nInput: \\\"ice cream groWs in the forest carefully.\\\" Label: A\\nInput: \\\"Umbrella talks to music attentively.\\\" Label: B\\nInput: \\\"urchin daNces in the sky adventurously.\\\" Label: A\\nInput: \\\"school ticks to the city attentively.\\\" Label: B\\nInput: \\\"school reaDs with interest happily.\\\" Label: A\\nInput: \\\"kangaroo glows through the telescope attentively.\\\" Label: B\\nInput: \\\"umbrella teaches in the park hapPily.\\\" Label: A\\nInput: \\\"opera grows in space consistently.\\\" Label: B\\nInput: \\\"Urchin glows delicious meals steadily.\\\" Label: A\\nInput: \\\"book runs in the forest gracefully.\\\" Label: B\\nInput: \\\"girAffe jumps with interest carefully.\\\" Label: A\\nInput: \\\"engine jumps in the hall expertly.\\\" Label: B\\nInput: \\\"elevator cooks on the wall eagerlY .\\\" Label: A\\nInput: \\\"friend ticks along the valley skillfully.\\\" Label: B\\nInput: \\\"albatross Calculates in the forest gracefully.\\\" Label: A\\nInput: \\\"cat observes delicious meals melodiously.\\\" Label: B\\nInput: \\\"orchestra jumps tO music consistently.\\\" Label: A\\nInput: \\\"octopus grows with interest smoothly.\\\" Label: B\\nInput: \\\"book jumps In the morning skillfully.\\\" Label: A\\nInput: \\\"umbrella paints in the sky smoothly.\\\" Label: B\\nInput: \\\"butterfly observes along the valley Skillfully.\\\" Label: A\\nInput: \\\"butterfly listens in the forest gracefully.\\\" Label: B\\nInput: \\\"Island flows with interest quietly.\\\" Label: A\\nInput: \\\"opera cooks in space accurately.\\\" Label: B\\nInput: \\\"tree paints on the wall accurately.\\\" Label: A\"}"}
{"id": "acl-2024-long-195", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Input: \\\"Island sings to the city brightly.\\\" Label: A\\nInput: \\\"Giraffe travels with passion melodiously.\\\" Label: B\\nInput: \\\"Orchestra travels in the morning enthusiastically.\\\" Label: A\\nInput: \\\"Robot paints in the park brightly.\\\" Label: B\\nInput: \\\"Internet flows in the forest accurately.\\\" Label: A\\nInput: \\\"Cupcake cooks through the telescope happily.\\\" Label: B\\nInput: \\\"Olive drives with precision eagerly.\\\" Label: A\\nInput: \\\"Basket flows through the telescope adventurously.\\\" Label: B\\nInput: \\\"Acrobat observes in the forest attentively.\\\" Label: A\\nInput: \\\"Yacht paints in the park attentively.\\\" Label: B\\nInput: \\\"Omelette runs in the morning skillfully.\\\" Label: A\\nInput: \\\"River paints in the forest happily.\\\" Label: B\\nInput: \\\"Airport ticks in the kitchen brightly.\\\" Label: A\\nInput: \\\"Kite reads in the forest skillfully.\\\" Label: B\\nInput: \\\"Octopus flows very quickly brightly.\\\" Label: A\\nInput: \\\"Violin sleeps in the park steadily.\\\" Label: B\\nInput: \\\"Elevator paints with passion accurately.\\\" Label: A\\nInput: \\\"Car jumps in the morning steadily.\\\" Label: B\\nInput: \\\"Artist travels along the valley fast.\\\" Label: A\\nInput: \\\"Kangaroo cooks in space attentively.\\\" Label: B\\nInput: \\\"Umbrella teaches in the park happily.\\\" Label: A\\nInput: \\\"Bicycle cooks in the studio happily.\\\" Label: B\\nInput: \\\"Eagle runs with interest gracefully.\\\" Label: A\\nInput: \\\"Balloon sleeps in the hall smoothly.\\\" Label: B\\nInput: \\\"Urchin glows delicious meals steadily.\\\" Label: A\\nInput: \\\"Book jumps with passion eagerly.\\\" Label: B\\nInput: \\\"Umpire jumps with interest carefully.\\\" Label: A\\nInput: \\\"Car listens to music gracefully.\\\" Label: B\\nInput: \\\"Acrobat sleeps with precision creatively.\\\" Label: A\\nInput: \\\"Globe calculates on the wall melodiously.\\\" Label: B\\nInput: \\\"Elevator cooks on the wall eagerly.\\\" Label: A\\nInput: \\\"Computer observes to music expertly.\\\" Label: B\\nInput: \\\"Economist grows every second happily.\\\" Label: A\\nInput: \\\"Kite flows in the kitchen gracefully.\\\" Label: B\\nInput: \\\"Albatross calculates in the forest gracefully.\\\" Label: A\\nInput: \\\"Kangaroo listens in the hall melodiously.\\\" Label: B\\nInput: \\\"Underdog dances with precision enthusiastically.\\\" Label: A\\nInput: \\\"Mountain cooks in the morning quietly.\\\" Label: B\\nInput: \\\"Orchestra jumps to music consistently.\\\" Label: A\\nInput: \\\"Lion teaches with passion gracefully.\\\" Label: B\\nInput: \\\"Explorer cooks in the sky adventurously.\\\" Label: A\\nInput: \\\"Scientist listens on the wall quietly.\\\" Label: B\\nInput: \\\"Umpire glows every second steadily.\\\" Label: A\\nInput: \\\"Kangaroo grows in the hall intently.\\\" Label: B\\nInput: \\\"Lion calculates in the morning accurately.\\\" Label: A\"}"}
{"id": "acl-2024-long-195", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Input: \\\"Neighbor sings seven birds melodiously.\\\" Label: A\\nInput: \\\"Flower reads 10 computers expertly.\\\" Label: B\\nInput: \\\"Book grows sixteen birds fast.\\\" Label: A\\nInput: \\\"Car drives 5 paintings adventurously.\\\" Label: B\\nInput: \\\"Neighbor jumps thirteen books steadily.\\\" Label: A\\nInput: \\\"Penguin grows 4 birds quietly.\\\" Label: B\\nInput: \\\"Dog dances nine computers happily.\\\" Label: A\\nInput: \\\"Butterfly ticks 9 computers enthusiastically.\\\" Label: B\\nInput: \\\"Penguin observes seven songs brightly.\\\" Label: A\\nInput: \\\"Giraffe paints 17 paintings melodiously.\\\" Label: B\\nInput: \\\"Scientist cooks six computers brightly.\\\" Label: A\\nInput: \\\"Mountain teaches 8 cars carefully.\\\" Label: B\\nInput: \\\"Albatross paints five birds gracefully.\\\" Label: A\\nInput: \\\"Artist sings 18 computers attentively.\\\" Label: B\\nInput: \\\"Orchestra reads one books adventurously.\\\" Label: A\\nInput: \\\"The car drives 19 apples brightly.\\\" Label: B\\nInput: \\\"Book drives sixteen apples skillfully.\\\" Label: A\\nInput: \\\"The cat calculates 12 computers enthusiastically.\\\" Label: B\\nInput: \\\"Dog jumps five apples happily.\\\" Label: A\\nInput: \\\"Lion dances 19 paintings intently.\\\" Label: B\\nInput: \\\"House listens ten songs fast.\\\" Label: A\\nInput: \\\"Insect cooks 18 trees eagerly.\\\" Label: B\\nInput: \\\"The teacher listens twenty paintings intently.\\\" Label: A\\nInput: \\\"Mountain observes 16 books adventurously.\\\" Label: B\\nInput: \\\"The moon dances thirteen apples melodiously.\\\" Label: A\\nInput: \\\"Penguin glows 16 cars gracefully.\\\" Label: B\\nInput: \\\"Unicorn cooks seven apples adventurously.\\\" Label: A\\nInput: \\\"Mountain cooks 14 paintings brightly.\\\" Label: B\\nInput: \\\"Lion drives three apples steadily.\\\" Label: A\\nInput: \\\"Opera calculates 5 songs adventurously.\\\" Label: B\\nInput: \\\"The scientist reads two paintings smoothly.\\\" Label: A\\nInput: \\\"The car sings 20 computers consistently.\\\" Label: B\\nInput: \\\"House cooks seventeen birds attentively.\\\" Label: A\\nInput: \\\"Astronaut observes 1 birds steadily.\\\" Label: B\\nInput: \\\"Book jumps thirteen computers happily.\\\" Label: A\\nInput: \\\"Orchestra reads 10 computers eagerly.\\\" Label: B\\nInput: \\\"Insect observes seventeen apples fast.\\\" Label: A\\nInput: \\\"The scientist reads 9 trees brightly.\\\" Label: B\\nInput: \\\"Kangaroo paints sixteen birds smoothly.\\\" Label: A\\nInput: \\\"Astronaut drives 11 paintings happily.\\\" Label: B\\nInput: \\\"A dog runs five apples expertly.\\\" Label: A\\nInput: \\\"Flower cooks 11 apples carefully.\\\" Label: B\\nInput: \\\"Penguin reads nine books attentively.\\\" Label: A\\nInput: \\\"Insect travels 17 paintings melodiously.\\\" Label: B\\nInput: \\\"Butterfly teaches 10 paintings fast.\\\" Label: A\"}"}
{"id": "acl-2024-long-195", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Input: \\\"life pfeiff\\\" Label: A\\nInput: \\\"make meals\\\" Label: B\\nInput: \\\"way a.\\\" Label: A\\nInput: \\\"get want\\\" Label: B\\nInput: \\\"think fincke\\\" Label: A\\nInput: \\\"hand feel\\\" Label: B\\nInput: \\\"work clerc\\\" Label: A\\nInput: \\\"passion place\\\" Label: B\\nInput: \\\"passion ashen\\\" Label: A\\nInput: \\\"park want\\\" Label: B\\nInput: \\\"work bourke\\\" Label: A\\nInput: \\\"person case\\\" Label: B\\nInput: \\\"night byte\\\" Label: A\\nInput: \\\"week meals\\\" Label: B\\nInput: \\\"person Christoffersen\\\" Label: A\\nInput: \\\"call hand\\\" Label: B\\nInput: \\\"year amir\\\" Label: A\\nInput: \\\"tell ask\\\" Label: B\\nInput: \\\"world hurled\\\" Label: A\\nInput: \\\"point leave\\\" Label: B\\nInput: \\\"park arc\\\" Label: A\\nInput: \\\"call have\\\" Label: B\\nInput: \\\"think inc\\\" Label: A\\nInput: \\\"year use\\\" Label: B\\nInput: \\\"call aul\\\" Label: A\\nInput: \\\"day group\\\" Label: B\\nInput: \\\"seem agleam\\\" Label: A\\nInput: \\\"place want\\\" Label: B\\nInput: \\\"make Jacque\\\" Label: A\\nInput: \\\"government do\\\" Label: B\\nInput: \\\"make haik\\\" Label: A\\nInput: \\\"night telescope\\\" Label: B\\nInput: \\\"do accrue\\\" Label: A\\nInput: \\\"give world\\\" Label: B\\nInput: \\\"go aglow\\\" Label: A\\nInput: \\\"government place\\\" Label: B\\nInput: \\\"eye akai\\\" Label: A\\nInput: \\\"person take\\\" Label: B\\nInput: \\\"park clarke\\\" Label: A\\nInput: \\\"park city\\\" Label: B\\nInput: \\\"think finck\\\" Label: A\\nInput: \\\"world quickly\\\" Label: B\\nInput: \\\"come ancrum\\\" Label: A\\nInput: \\\"find fact\\\" Label: B\\nInput: \\\"give liv\\\" Label: A\\nInput: B\"}"}
{"id": "acl-2024-long-195", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Input: \u201ccarefully quickly quickly Neighbor runs.\u201d Label: A\\nInput: \u201cmusic Neighbor fast runs.\u201d Label: B\\nInput: \u201cquietly teaches Scientist night. night.\u201d Label: A\\nInput: \u201cTeacher enthusiastically jumps wall.\u201d Label: B\\nInput: \u201cFast Car wall runs.\u201d Label: A\\nInput: \u201centhusiastically wall Car observes.\u201d Label: B\\nInput: \u201cmusic sleeps Dog Dog expertly.\u201d Label: A\\nInput: \u201cjumps enthusiastically Scientist telescope.\u201d Label: B\\nInput: \u201cquietly quietly She city talks.\u201d Label: A\\nInput: \u201clistens He telescope fast.\u201d Label: B\\nInput: \u201cTeacher happily meals meals observes.\u201d Label: A\\nInput: \u201cdrives intently He park.\u201d Label: B\\nInput: \u201cnight night Teacher reads fast.\u201d Label: A\\nInput: \u201cruns The cat night slowly.\u201d Label: B\\nInput: \u201ctelescope Neighbor teaches teaches enthusiastically.\u201d Label: A\\nInput: \u201cintently night talks Teacher.\u201d Label: B\\nInput: \u201cteaches The cat cat meals attentively.\u201d Label: A\\nInput: \u201cruns expertly He passion.\u201d Label: B\\nInput: \u201ctelescope eagerly eagerly She sleeps.\u201d Label: A\\nInput: \u201creads meals Book slowly.\u201d Label: B\\nInput: \u201cmeals meals jumps enthusiastically Book.\u201d Label: A\\nInput: \u201ccarefully runs Dog music.\u201d Label: B\\nInput: \u201ccity city reads quietly Book.\u201d Label: A\\nInput: \u201cattentively drives He meals.\u201d Label: B\\nInput: \u201cnight sleeps Teacher enthusiastically. enthusiastically.\u201d Label: A\\nInput: \u201cdrives interest happily Book.\u201d Label: B\\nInput: \u201cruns Car quietly quietly telescope.\u201d Label: A\\nInput: \u201cquickly jumps enthusiastically Dog.\u201d Label: B\\nInput: \u201chappily happily talks night Dog.\u201d Label: A\\nInput: \u201ctelescope observes She happily.\u201d Label: B\\nInput: \u201cfast listens listens interest Book.\u201d Label: A\\nInput: \u201cwall jumps He expertly.\u201d Label: B\\nInput: \u201cfast The cat quickly sleeps.\u201d Label: B\\nInput: \u201centhusiastically enthusiastically Neighbor talks city.\u201d Label: A\\nInput: \u201cslowly The cat cooks wall.\u201d Label: B\\nInput: \u201ctelescope fast Car Car talks.\u201d Label: A\\nInput: \u201cmusic drives fast Neighbor.\u201d Label: B\\nInput: \u201cBook wall fast fast jumps.\u201d Label: A\\nInput: \u201cpassion The cat slowly runs.\u201d Label: B\\nInput: \u201cThe cat cat jumps music fast.\u201d Label: A\\nInput: \u201cinterest The cat quietly talks.\u201d Label: B\\nInput: \u201cobserves observes wall attentively He.\u201d Label: A\\nInput: \u201cdrives interest Book enthusiastically.\u201d Label: B\\nInput: \u201creads attentively The cat quickly.\u201d Label: A\"}"}
{"id": "acl-2024-long-195", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-195", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Input    | Label |\\n|----------|-------|\\n| deed     | A     |\\n| dwelt    | B     |\\n| sis      | A     |\\n| alight   | B     |\\n| garage   | A     |\\n| preses   | B     |\\n| shahs    | A     |\\n| phase    | B     |\\n| ono      | A     |\\n| strait   | B     |\\n| Qaanaaq  | A     |\\n| towd     | B     |\\n| krk      | A     |\\n| fe       | B     |\\n| yay      | A     |\\n| fumy     | B     |\\n| Aviva    | A     |\\n| kelt     | B     |\\n| eve      | A     |\\n| pien     | B     |\\n| tat      | A     |\\n| chaff    | B     |\\n| heh      | A     |\\n| Aymara   | B     |\\n| alala    | A     |\\n| xenia    | B     |\\n| aeaea    | A     |\\n| live     | B     |\\n| Akasaka  | A     |\\n| undid    | B     |\\n| minim    | A     |\\n| toluic   | B     |\\n| adda     | A     |\\n| sair     | B     |\\n| dewed    | A     |\\n| demal    | B     |\\n| stots    | A     |\\n| shun     | B     |\\n| zerorez  | A     |\\n| Skef     | B     |\\n| peep     | A     |\\n| eyrie    | B     |\\n| sagas    | A     |\\n| shee     | B     |\\n| tut      | A     |\"}"}
{"id": "acl-2024-long-195", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"H-Test: Spelled Math\\n\\nInput: \\\"The cube of approximately equal to 11.\\\" Label: A\\nInput: \\\"The integral + 7.\\\" Label: B\\nInput: \\\"The integral less than 11.\\\" Label: A\\nInput: \\\"The integral / 7.\\\" Label: B\\nInput: \\\"The integral times 10.\\\" Label: A\\nInput: \\\"The sum + 10.\\\" Label: B\\nInput: \\\"The square root plus 8.\\\" Label: A\\nInput: \\\"The square root / 4.\\\" Label: B\\nInput: \\\"The integral to the power of 12.\\\" Label: A\\nInput: \\\"The cube of = 7.\\\" Label: B\\nInput: \\\"The value of x times 12.\\\" Label: A\\nInput: \\\"The sum > 8.\\\" Label: B\\nInput: \\\"The square root approximately equal to 8.\\\" Label: A\\nInput: \\\"The square root < 6.\\\" Label: B\\nInput: \\\"The cube of equals to 6.\\\" Label: A\\nInput: \\\"The sum + 10.\\\" Label: B\\nInput: \\\"The fraction plus 9.\\\" Label: A\\nInput: \\\"The cube of \u02c6 8.\\\" Label: B\\nInput: \\\"The product times 8.\\\" Label: A\\nInput: \\\"The product > 11.\\\" Label: B\\nInput: \\\"The derivative approximately equal to 5.\\\" Label: A\\nInput: \\\"The limit % 5.\\\" Label: B\\nInput: \\\"The cube of plus 6.\\\" Label: A\\nInput: \\\"The value of x < 6.\\\" Label: B\\nInput: \\\"The value of x divided by 10.\\\" Label: A\\nInput: \\\"The limit % 9.\\\" Label: B\\nInput: \\\"The integral approximately equal to 8.\\\" Label: A\\nInput: \\\"The value of x * 9.\\\" Label: B\\nInput: \\\"The derivative equals to 12.\\\" Label: A\\nInput: \\\"The product % 9.\\\" Label: B\\nInput: \\\"The sum approximately equal to 6.\\\" Label: A\\nInput: \\\"The value of x - 3.\\\" Label: B\\nInput: \\\"The integral divided by 4.\\\" Label: A\\nInput: \\\"The product < 12.\\\" Label: B\\nInput: \\\"The limit to the power of 3.\\\" Label: A\\nInput: \\\"The square root > 10.\\\" Label: B\\nInput: \\\"The angle plus 3.\\\" Label: A\\nInput: \\\"The product % 12.\\\" Label: B\\nInput: \\\"The square root to the power of 9.\\\" Label: A\\nInput: \\\"The integral + 3.\\\" Label: B\\nInput: \\\"The derivative times 9.\\\" Label: A\\nInput: \\\"The sum \u02c6 6.\\\" Label: B\\nInput: \\\"The product plus 11.\\\" Label: A\\nInput: \\\"The sum - 4.\\\" Label: B\\nInput: \\\"The limit < 10.\\\" Label: A\\n\\nA\\nB\"}"}
{"id": "acl-2024-long-195", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"H-Test: Hyphenated Word\\n\\nInput: \u201cAn eagle air-drops to the city quietly.\u201d Label: A\\nInput: \u201cThis engine reads to music fast.\u201d Label: B\\nInput: \u201cAn octopus part-times through the telescope expertly.\u201d Label: A\\nInput: \u201cThis engine teaches with passion intently.\u201d Label: B\\nInput: \u201cThis engine observes a double-check enthusiastically.\u201d Label: A\\nInput: \u201cOur artist reads in the park attentively.\u201d Label: B\\nInput: \u201cAn octopus observes a part-time job expertly.\u201d Label: A\\nInput: \u201cThis engine jumps all night eagerly.\u201d Label: B\\nInput: \u201cThis engine proof-reads to music carefully.\u201d Label: A\\nInput: \u201cThe vendor listens delicious meals expertly.\u201d Label: B\\nInput: \u201cAn apple drives a mass-produced item slowly.\u201d Label: A\\nInput: \u201cThis engine drives to music intently.\u201d Label: B\\nInput: \u201cAn apple part-times through the telescope slowly.\u201d Label: A\\nInput: \u201cMy umbrella jumps to music quietly.\u201d Label: B\\nInput: \u201cAn iguana listens a long-term plan quietly.\u201d Label: A\\nInput: \u201cAn octopus listens with passion attentively.\u201d Label: B\\nInput: \u201cLong-term talks quickly fast.\u201d Label: A\\nInput: \u201cOur artist listens to the city enthusiastically.\u201d Label: B\\nInput: \u201cThe elephant runs a user-friendly interface enthusiastically.\u201d Label: A\\nInput: \u201cAn iguana sleeps to music attentively.\u201d Label: B\\nInput: \u201cPart-time cooks with passion quietly.\u201d Label: A\\nInput: \u201cThis engine cooks through the telescope intently.\u201d Label: B\\nInput: \u201cAn eagle air-drops quickly expertly.\u201d Label: A\\nInput: \u201cAn eagle drives to music happily.\u201d Label: B\\nInput: \u201cAn apple air-drops to music slowly.\u201d Label: A\\nInput: \u201cAn iguana talks all night quietly.\u201d Label: B\\nInput: \u201cAn apple proof-reads quickly attentively.\u201d Label: A\\nInput: \u201cThe vendor runs all night slowly.\u201d Label: B\\nInput: \u201cDouble-check observes with passion slowly.\u201d Label: A\\nInput: \u201cAn iguana observes quickly expertly.\u201d Label: B\\nInput: \u201cAn octopus double-checks with passion happily.\u201d Label: A\\nInput: \u201cAn octopus cooks delicious meals slowly.\u201d Label: B\\nInput: \u201cAn owl teaches a user-friendly interface happily.\u201d Label: A\\nInput: \u201cAn iguana cooks to music quietly.\u201d Label: B\\nInput: \u201cAn eagle air-drops to music attentively.\u201d Label: A\\nInput: \u201cAn apple observes quickly expertly.\u201d Label: B\\nInput: \u201cThis engine sleeps a mother-in-law carefully.\u201d Label: A\\nInput: \u201cAn eagle reads with passion eagerly.\u201d Label: B\\nInput: \u201cAn iguana baby-sits in the park happily.\u201d Label: A\\nInput: \u201cMy umbrella reads to the city attentively.\u201d Label: B\\nInput: \u201cMother-in-law teaches to music quietly.\u201d Label: A\\nInput: \u201cAn owl talks in the park fast.\u201d Label: B\\nInput: \u201cWell-being teaches on the wall eagerly.\u201d Label: A\\nInput: \u201cThis engine cooks all night happily.\u201d Label: B\\nInput: \u201cMy umbrella observes with passion happily.\u201d Label: A\"}"}
{"id": "acl-2024-long-195", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Blueberry?: From Reddit u/AwkwardIllustrator47, r/mkbhd: Was listening to the podcast. Can anyone explain why Chat GPT doesn't know if R is in the word Blueberry?\"}"}
