{"id": "lrec-2024-main-1114", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Phonetic Segmentation of the UCLA Phonetics Lab Archive\\n\\nEleanor Chodroff, Bla\u017e Pa\u017eon, Annie Baker, and Steven Moran\\n\\n1 University of Zurich, Department of Computational Linguistics, Andreasstrasse 15, 8050 Z\u00fcrich, Switzerland\\n2 University of Ljubljana, blaz.pazon@gmail.com\\n3 University of Neuch\u00e2tel, steven.moran@unine.ch\\n4 University of Miami\\n\\nAbstract\\n\\nResearch in speech technologies and comparative linguistics depends on access to diverse and accessible speech data. The UCLA Phonetics Lab Archive is one of the earliest multilingual speech corpora, with long-form audio recordings and phonetic transcriptions for 314 languages (Ladefoged et al., 2009). Recently, 95 of these languages were time-aligned with word-level phonetic transcriptions (Li et al., 2021). Here we present VoxAngeles, a corpus of audited phonetic transcriptions and phone-level alignments of the UCLA Phonetics Lab Archive, which uses the 95-language CMU release as our starting point. VoxAngeles also includes word- and phone-level segmentations from the original UCLA corpus, as well as phonetic measurements of word and phone durations, vowel formants, and vowel f0. This corpus enhances the usability of the original data, particularly for quantitative phonetic typology, as demonstrated through a case study of vowel intrinsic f0. We also discuss the utility of the VoxAngeles corpus for general research and pedagogy in crosslinguistic phonetics, as well as for low-resource and multilingual speech technologies. VoxAngeles is free to download and use under a CC-BY-NC 4.0 license.\\n\\nKeywords: crosslinguistic speech corpora, phonetic alignment, phonetic segmentation\\n\\n1. Introduction\\n\\nAdvancements in speech technologies and comparative linguistics research benefit from increased availability of accessible crosslinguistic speech data. Goals ranging from language description and preservation, phonetic theory and typology, to automatic speech recognition or text-to-speech synthesis, require diverse crosslinguistic data. While ''big data'' has allowed incredible development in both speech science and technology for high-resource languages, the quantity of such data simply does not exist for the majority of the world's languages. Moreover, for speech data that might exist, it is unlikely to be in a directly usable format for many research questions in industry or science.\\n\\nThe usability of a speech corpus depends considerably on the research question. The mere existence of speech audio data may suffice for some research questions, whereas for others, metadata may be necessary for downstream analysis. The most useful accompanying metadata is a written orthographic and/or phonetic transcription of the spoken audio which can enable supervised training of speech technology systems, as well as the searchability of the audio for key phrases or segments.\\n\\nAccess to crosslinguistic speech corpora has risen dramatically in recent years, particularly with the release of several massively multilingual speech corpora. This growth spurt has arisen in large part from technological advances in computational power including increased storage and processing capacity, as well as the increased accessibility of decent recording devices through personal smartphones or computers. Two such massively multilingual speech corpora are the CMU Wilderness Corpus with approximately 20 hours of speech per language for over 600 languages (Black, 2019), and the Mozilla Common Voice Dataset with anywhere from 1 to over 3000 hours of speech per language for over 100 languages (Ardila et al., 2020). The direct usability of these corpora for many researchers comes through the availability of utterance-level alignments, as well as the development of automatic word- and phone-level alignments for many of the languages (VoxClamantis for Wilderness: Salesky et al., 2020; VoxCommunis for Common Voice: Ahn and Chodroff, 2022; CommonVoice Utils: Tyers and Meyer, 2021).\"}"}
{"id": "lrec-2024-main-1114", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"These include, but are not limited to the GlobalPhone Corpus with 20 languages (Schultz et al., 2013), Multilingual LibriSpeech with eight languages (Pratap et al., 2020), and the DoReCo Corpus of fieldwork recordings from over 50 languages (Seifart et al., 2022).\\n\\nIndeed, developing usable crosslinguistic speech corpora has long been a goal in the speech community. The UCLA Phonetics Lab Archive is one of the earliest, and to this day, one of the most crosslinguistically diverse collections of speech data (Ladefoged et al., 2009). The corpus contains spoken audio recordings and phonetic transcriptions from 314 languages representing 51 language families, and is freely available to the public in an online repository.\\n\\nThe data were collected in fieldwork sessions from the 1960s to the late 1990s (Ladefoged and Maddieson, 1996a). Between 2006 and 2009, a web page was created for each language, which contain links to the original audio recordings and fieldnotes, along with digitized tables of the phonetic transcriptions, orthographic forms, and in many cases, translations (Ladefoged and Schuh, 2004; Ladefoged et al. 2009).\\n\\nThough the corpus includes phonetic transcriptions of the target language, the transcriptions have not previously been time-aligned to the recordings. In a large and valuable undertaking, Li et al. (2021) obtained time-aligned transcripts for recordings from 95 languages of the original corpus using an automated approach. Specifically, the word-level phonetic transcriptions were scraped from the primary online data base, and the long audio files were segmented into individual recordings of transcribed words. Through this process, utterances in the non-target language (e.g., the language of interaction during the fieldwork session) could be bypassed. The audio segmentations were then validated by a human listener to ensure the alignment between the text and audio was correct. This effort greatly enhanced the usability of the corpus, enabling direct work on multilingual and universal phone recognition (e.g., Li et al., 2022; Liu et al., 2023).\\n\\nIn the present paper, we present VoxAngeles, an updated release of the UCLA Phonetics Lab Archive, that includes manually corrected phone-level alignments using the original, or closely adapted phonetic transcriptions, as well as phonetic measurements of phone and word durations, vowel formants, and vowel f0. In total, the current release of the corpus spans 95 languages from 21 language families (as defined by Glottolog; Hammarstr\u00f6m et al., 2023). In the final portion of the paper, we demonstrate the utility of this corpus for phonetic typology in a case study of a previously posited crosslinguistic phonetic universal, vowel intrinsic f0.\\n\\nIn the present paper, we present VoxAngeles, an updated release of the UCLA Phonetics Lab Archive, that includes manually corrected phone-level alignments using the original, or closely adapted phonetic transcriptions, as well as phonetic measurements of phone and word durations, vowel formants, and vowel f0. In total, the current release of the corpus spans 95 languages from 21 language families (as defined by Glottolog; Hammarstr\u00f6m et al., 2023). In the final portion of the paper, we demonstrate the utility of this corpus for phonetic typology in a case study of a previously posited crosslinguistic phonetic universal, vowel intrinsic f0.\\n\\n2. Methods\\n\\nThe overall procedure for developing the VoxAngeles corpus is depicted in Figure 1. Using the word segmentations and extracted phonetic transcriptions from the 95 languages in the CMU release as input (Li et al., 2021), we automatically force aligned the phonetic transcription to the audio file using the Montreal Forced Aligner (McAuliffe et al., 2017; Section 2.1). The phone alignments were then manually adjusted and audited along several dimensions (Section 2.2), and additional data were segmented from the original UCLA Phonetics Lab Archive where possible (Section 2.3). Finally, acoustic-phonetic measurements were extracted for phonetic analysis (Section 2.4).\\n\\n2.1 Phonetic Forced Alignment\\n\\nThe CMU release of the UCLA Phonetics Archive contains individual audio files from 95 languages, in which each audio file is accompanied with a word-level phonetic transcription scraped from the original corpus website (Li et al. 2021). The phonetic transcriptions were largely based on the IPA, though several discrepancies are outlined in Section 2.2. To obtain phone-level time alignments, the files were prepared for phonetic forced alignment using the Montreal Forced Aligner (MFA). We employed the most diverse and at the time, largest, pretrained acoustic model available through the MFA. This was the \u201cenglish 2.0.0a\u201d acoustic model, which we refer to as the \u201cGlobal English model\u201d (McAuliffe and Sonderegger, 2022). It was trained on 3700+ hours of global English, including recordings from American, British, Nigerian, South African, and Irish varieties of English, among others. This model has been shown to be consistently competitive with or more accurate than small language-specific acoustic models (~1 hour) in forced alignment performance (Chodroff et al., 2024).\\n\\nAs one would expect, the phonetic symbols used in the UCLA Phonetics Lab Archive show remarkable diversity (see also Maddieson, 1984; Moran and Cysouw, 2018). For alignment with the pretrained Global English model, we needed to remap these phones to those in the acoustic model phone set. Thus, we identified phonetically comparable segments for the remapping process, and used the Interlingual MFA toolkit (Dolatian, 2023) to maintain a mapping between the original and English-adapted phonetic transcriptions. The initial alignment was then implemented using the MFA, the Global English acoustic model, and the custom pronunciation lexicon with the remapped UCLA Phonetics Lab Archive transcriptions. Following alignment, the phones were reconverted back to their original transcription using the Interlingual MFA toolkit (Dolatian, 2023). The resulting set of Praat TextGrids with the word- and phone-level alignments were then distributed to two phonetically.\\n\\n2.2 Manual Auditing\\n\\nWe audited the word- and phone-level alignments across 51 of the 95 languages in the VoxAngeles corpus. Although the automatic alignment was trained on English data, the greater phonetic diversity of the corpus provided additional insights into the performance of the Forced Aligner.\\n\\n2.3 Additional Data Segmentation\\n\\nIn addition to the English-accented material, we also segmented data from 23 languages in which the speakers did not have English accents, which provided more diverse material for acoustic-phonetic analysis.\\n\\n2.4 Acoustic-Phonetic Measurements\\n\\nWe extracted several acoustic-phonetic measurements, including vowel formants and f0. These data were analyzed using a variety of methods, including machine learning algorithms.\\n\\nWe are aware that VoxAngeles is not actually Latin.\"}"}
{"id": "lrec-2024-main-1114", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"trained annotators for manual adjustment and auditing.\\n\\n2.2 Manual Alignment and Auditing\\n\\nDuring the process of manual alignment, we adhered to three main guiding principles. Firstly, we aimed to remain faithful to the transcription produced by the linguist who originally compiled and transcribed the word list. Second, we aimed to represent the entire speech signal with the transcription provided, and third, to assign a section of the speech signal to each element of the transcription.\\n\\nDespite the overall intention to stay faithful to the original transcription, in some cases it was considered beneficial to modify those in the MFA output TextGrids. This was the case for transcriptions which featured obsolete IPA symbols, errors occurring at different points of the extraction process, or the use of non-standard phonetic symbols. These modifications were implemented in the interest of maintaining consistency throughout the dataset. Some of the more common issues or obstacles we encountered included:\\n\\nInconsistent representation of suprasegmental features. The primary exception to the fundamental guideline of retaining the original transcription was the marking of tones, syllable boundaries, and stress. Where included, these tended to be marked inconsistently throughout the corpus, which in some cases hindered automatic processing. These were therefore removed.\\n\\nObsolete and nonstandard symbols. As the UCLA Phonetics Lab Archive compiles work by various investigators conducted over several decades, there was an unsurprising lack of uniformity when it came to the employed set of phonetic symbols. Some of these symbols were obsolete IPA symbols, some from the Americanist phonetic notation, and some perhaps were developed to simplify the transcription in handwriting or with a typewriter. We aimed to include only current standard IPA symbols, and to ensure consistency across languages. To standardize the notation, we frequently consulted the scanned copies of the original field notes. Many of the notes included legends that mapped a simplified symbol to a more complex phonetic transcription. First, several of the IPA symbols used in the original transcriptions are now obsolete. Among the more common were <\u0269> (lowercase iota), representing a near-close near-front unrounded vowel, replaced by the current standard <\u026a> (small capital I), and <\u0277> (closed omega), representing a near-close near-back rounded vowel, replaced by the current standard <\u028a> (inverted omega). As of the 1989 Kiel Convention, however, the symbols [\u0269 \u0277] were standardized to [\u026a \u028a], which remains the standard to this day (Roach, 1989). Many of the obsolete symbols were also missing in the extracted TextGrids, likely from rendering issues in the digitized HTML file.\\n\\nCases in which there was no clear one-to-one correspondence between the original symbol and a modern equivalent were addressed by referring to, where available, relevant literature on the language in question. For example, in the case of Angami (njm), certain segments in the transcriptions did not have a clear modern equivalent. In that case, we were able to consult a phonetic description of the grammar in Blankenship et al. (1992), which allowed us to update the MFA output TextGrid transcription from [whe] to [\u028de], and from [mhe] to [m\u0325\u02b0e]. In some cases, nonstandard symbols, frequently but not always from the North American Phonetic Alphabet, were adapted to current standard IPA symbols to ensure consistency and maintain the necessary phonetic contrasts. An example of this is Ladino (lad), which distinguishes an alveolar tap /\u027e/ from an alveolar trill /\u027e/; these were represented in the transcripts respectively by <r> and <\u0155>. The latter is not standard IPA, so we changed all original <r> symbols to <\u027e>, and <\u0155> to <r>. Additional common modifications are listed in the following. Many of these mappings were determined based on the identification of a legend in the original field notes: <\u00fc> replaced by <y>, <\u00e4> replaced by <\u00e6>, <\u0161> replaced by <\u0283>, <\u010d> replaced by <t\u0361\u0283>, <\u017e> replaced by <\u0292>, <\u01ef> replaced by <d\u0361\u0292>, <\u0144> or <\u00f1> replaced by <\u0272>, <\u019b> replaced by <t\u0361\u026c>, and the underdot denoting retroflex consonants (e.g., <\u1e6d>) replaced by the modern retroflex consonant symbols (e.g., <\u0288>).\\n\\nTypographical issues. The symbol <\u0261> used on the UCLA website was in most cases rendered as <q\u0331> (underlined <q>) at some point during the automatic extraction process undertaken by Li et al. (2021). In several cases, the remapping between the original and the English-adapted phonetic transcriptions for the MFA was not entirely successful, even in cases where the phone was not remapped. The reason for this is unclear; however, in each case these were manually corrected during the auditing process. For example, while the symbol <\u0273> was preserved in the word tier, it was replaced by <\u0272> in the phones tier in Scottish Gaelic (gla), Gujarati (guj) and Kannada (kan). This occurred less frequently with other symbols and less consistently across different words. Other examples included: <\u0261> replaced by <k> in Basque (eus) and Kannada (kan), <\u0281\u02b7> replaced by <f> in Chamalal (cji), <\u0257> replaced by <d> in Gujarati (guj), <\u028c> replaced by <\u0259> and <\u027d> replaced by <\u0279> in Kannada (kan).\\n\\nMissing symbols. Some symbols exhibited rendering issues which led to complete disappearance, such as tie bars combining less frequent sound combinations (e.g., /h\u0361w/ in Bassa (bsq), or /p\u0361\u0283/ in Hebrew (heb)), various diacritics (including carons, underdots, modifier letters, symbols denoting nasalized, velarized, rounded and dental phones), and some less common IPA symbols (e.g., <\u02a1> and <\u029c> in Aghul (agx)). A status shared by all missing symbols was that of being in some way removed from the more frequently used and core IPA symbols; they were either non-standard, non-IPA or\"}"}
{"id": "lrec-2024-main-1114", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"symbols which are rarely used, both independently or in certain uncommon combinations. This can be observed for example in the case of the Lithuanian (lit) word kelti ('to lift'): both the field notes and the digitized word list on the UCLA Phonetics Lab Archive webpage provide the transcription [k\u02b2\u00e6l\u0306t\u02b2\u0269], whereas the CMU word list and consequently the MFA output TextGrid have the transcription [k\u02b2\u00e6lt\u02b2], with missing diacritics and the obsolete <\u0269> symbol (discussed above).\\n\\nUnclear segment boundaries. As with the majority of segmentation tasks, there were cases in which boundaries between two segments were difficult to define (see for example Figure 2). In our attempt to remain faithful to the original transcription, we made sure to insert a boundary, even when the acoustics did not strongly indicate a specific point of separation. The absence of clear boundaries is to be expected, considering that speech does not consist of isolated sounds; rather, it manifests as a continuous speech signal shaped by coarticulation.\\n\\nFigure 2: Example of unclear boundaries in a case of adjacent vowels with dynamic formant trajectories from Min Nan (nan).\\n\\nTranscription\u2013audio mismatch. For multiple files the phonetic transcription provided in the MFA output TextGrids did not exactly match the audio. This presented a problem when aligning, and was addressed by consulting the word list in the database, the original recording and field notes. This facilitated the attribution of the transcript\u2013audio mismatch to one of a few possible errors. Usually, the error could be attributed to a mismatch of the transcript and the audio. If the corresponding transcript was found in the word list and we could confirm it to be the correct match for the audio, we simply adapted the transcript accordingly. In some cases, further inspection revealed the cause of the mismatch to be an inconsistency between the original transcript in the field notes and that of the digitized word list. In such cases the digitized word list typically contained an error and we opted for the original transcript. An example of this discrepancy is the transcript of the Danish (dan) word kantate 'cantata', transcribed as /k\u02b0\u025bt\u02b0i\u00f0\u0259/ in the digitized word list, and /k\u02b0\u025bnt\u02b0ei\u00f0\u0259/ in the field notes. The latter was used, as it was not only the original transcript, but it also better represented the speech signal.\\n\\nIn other cases, the audio file itself was problematic. A prime example of this was a string of faulty audio files at the end of the dataset for Armenian (hye), where some audio files were mismatches, while the rest contained only noise from the original recording. Such recordings containing only noise also occurred in Ibibio (ibb), while one recording of Igbo (ibo) contained only the linguist\u2019s speech. Such mismatched files were removed.\\n\\nFigure 3: Example of audio quality differences between Malayalam (mal; top) and Dutch (nld; bottom). The spectrogram settings are the same for both images.\\n\\nAudio quality. The overall quality of the audio recordings varied considerably, largely due to the wide range of years and environments over which the data were collected (see for example Figure 3). Users of the corpus should be aware of this disparity of quality between languages. Occasionally, certain recordings were interrupted by loud noises, such as thuds or vocalizations from another speaker. Rarely, when a word-final consonant was pronounced silently or was a plosive with a long occlusion phase, the recording was cut off prematurely. In even rarer cases, the audio was cut off at the onset of the word. In each of these scenarios, the files were removed, as the information from the sounds was interrupted or incomplete.\\n\\n2.3 Additional Data\\n\\nData from 11 additional languages were manually aligned at the word-level and extracted into individual recordings. These have also been force-aligned using\"}"}
{"id": "lrec-2024-main-1114", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the same procedure as described in Section 2.1, but have not yet gone through manual correction. In total, 1669 recordings were added to the corpus, with a median of 55 new recordings per language and a range of 23 to 692 new recordings:\\n\\nSouth Levantine Arabic (ajp): 92; Western Apache (apw): 40; South Azerbaijani (azb): 55; Bemba (bem): 23; Bengal (ben): 250; Albay Biscolano (bkh): 35; Edo (bin): 45; Bassa (bsq): 44; Czech (ces): 268; Chamorro (cha): 125; Degema (deg): 692.\\n\\nFigure 4: World map of the 95 aligned and audited languages from the UCLA Phonetics Lab Archive. Each color represents a distinct language family.\\n\\n2.4 Acoustic-phonetic Analysis\\n\\nThe current release includes the following pre-extracted acoustic-phonetic measurements: the duration of all phone segments, the f0 at each quartile and decile of corner vowel s, and F1\u2013F3 at each quartile and decile of corner vowel s. Along with each of these measurements, we include information about the preceding and following phones, the corresponding word, file name, the phone start and end times, and the word start and end times.\\n\\nThe repository also includes the corresponding Praat scripts that can be modified to expand the targeted segment set.\\n\\nAll measurements were extracted using Praat. F0 contours were extracted using the To Pitch function, with the floor at 75 Hz, the ceiling at 500 Hz, and a time step of 0.01 s. Formant contours were extracted using the To Formant (burg) function, with parameters informed by the average vowel midpoint f0 of the file. If the average vowel midpoint f0 was over 160 Hz (the mean of our data), the formant ceiling was set to 5500 Hz. Otherwise, it was set to 5000 Hz. In all cases, the window length was 0.025 s, the time step was 0.01 s, the maximum number of formants estimated was 5, and pre-emphasis was added from 50 Hz.\\n\\n3. Results\\n\\nThe manual alignment yielded a total of 54,455 word-level recordings from 95 languages (Figure 4), with a median of 49 recordings and a range of 20 to 162 recordings per language. The languages span 21 language families (Hammarstr\u00f6m et al., 2023; see Table 1). Within these files, a total of 22,825 phone intervals were aligned, with a median of 228 and a range of 46 to 755 phone intervals per language. Within the corpus, 568 distinct phones were present. The number of distinct phone types per language ranged from 13 to 93 with a median of 35. Of the observed phone types, approximately 209 were vowels, 184 were stops, affricates, trills, or taps, 94 were fricatives, 41 were approximants, and 40 were nasal consonants.\\n\\n| Language Family         | # Languages | # Files   |\\n|-------------------------|-------------|-----------|\\n| Atlantic-Congo          | 27          | 922       |\\n| Indo-European           | 21          | 1650      |\\n| Austronesian            | 9           | 419       |\\n| Afro-Asiatic            | 8           | 534       |\\n| Sino-Tibetan            | 7           | 484       |\\n| Dravidian               | 3           | 140       |\\n| Abkhaz-Adyge            | 2           | 175       |\\n| Austroasiatic           | 2           | 115       |\\n| Kru                     | 2           | 99        |\\n| Nakh-Daghestanian       | 2           | 116       |\\n| Uralic                  | 2           | 229       |\\n| Athabaskan-Eyak-Tlingit | 1           | 62        |\\n| Basque                  | 1           | 46        |\\n| Central Sudanic         | 1           | 23        |\\n| Ijoid                   | 1           | 85        |\\n| Kakua-Nukak             | 1           | 63        |\\n| Mande                   | 1           | 69        |\\n| Nuclear Torricelli      | 1           | 70        |\\n| Salishan                | 1           | 46        |\\n| Siouan                  | 1           | 42        |\\n| Turkic                  | 1           | 56        |\\n\\nTable 1: Language families in the audited subset of the corpus along with the total number of languages and word-level files per family.\\n\\nRelative to the CMU release, a total of 64 files were removed from across 28 languages, mostly due to interfering noise (see Section 2.2). In close reference to the original field notes and for reasons outlined in Section 2.2, we also updated phonetic transcriptions for around 1,354 words from 85 languages. This corresponded to about 25% of the dataset. Within these languages, a median of 9 transcriptions were changed per language, with a mean of 1.6 and a maximum of 88.\\n\\nOf the phone intervals retained in the comparison between the original MFA alignment and the corrected versions, the median onset boundary adjustment was 6.9 ms. Approximately 71% of the boundaries were within 20 ms of the gold boundary, 57% within 10 ms, and 45% within 5 ms. About 37% of the boundaries were unchanged. Overall, this\"}"}
{"id": "lrec-2024-main-1114", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"indicates decent performance of the automatic forced alignment.\\n\\n4. Case Study: Intrinsic f0\\n\\nIn its phonetically time-aligned format, the VoxAngeles corpus serves as a foundational resource for investigations in phonetic typology. Several phonetic universals have been previously suggested in the literature, but there has been limited empirical research due to the lack of access to diverse crosslinguistic speech data. Such universals include intrinsic f0 or intrinsic vowel duration, where low vowels (e.g., /a/ or /\u00e6/) tend to have a lower f0 and longer vowel duration than corresponding high vowels (e.g., /i/ or /u/) (Meyer, 1896\u20137; Whalen and Levitt, 1995). These may be due to an automatic biomechanical consequence of the tongue or jaw movement, but with the underlying assumption that the intended phonetic target was uniform for each vowel category. The degree to which these effects are automatic or under speaker control has, however, been extensively debated in the field (Diehl and Kluender, 1989; Whalen and Levitt, 1995; Ting et al., 2023).\\n\\nWhile phonetically robust, these phenomena are rarely phonologized (cf., consonant f0 in which a high f0 after voiceless consonants and low f0 after voiced consonants can lead to tonogenesis; see Ting et al., 2023 for an overview).\\n\\nIntrinsic f0 has had the benefit of being studied across a wide range of languages. Whalen and Levitt (1995) conducted a meta-analysis of the literature, identifying previously reported vowel-specific f0 means from approximately 31 languages across 11 language families. They found a significant difference between the f0 of high and low vowels across languages, with language-specific means largely demonstrating the expected numerical direction. More recently, Ting et al. (2023) investigated the presence and magnitude of this effect in a large-scale crosslinguistic study using 16 languages across 8 language families in the GlobalPhone and Librispeech Corpora (Schultz et al., 2013; Panayatov et al., 2015). They additionally confirmed the presence of the effect in each language, but to varying degrees of magnitude. In this study, we contribute to this literature by investigating whether high vowels such as /i/ and /u/ have a higher f0 than low vowels such as /a/, with increased language diversity in the dataset. Through this process, we can also assess the relative strengths and weaknesses of the VoxAngeles corpus for such investigations.\\n\\n4.1 Methods\\n\\nThe midpoint f0 was extracted from high and low vowels using the extraction methods described in Section 2.4. First, vowel types with fewer than 10 tokens and devoiced vowels were excluded. Vowels were then collapsed into three broad categories: high front vowels [i i\u02d0 i\u032f \u026a \u0268\u0303], high back vowels [u u\u02d0 \u028a \u028a\u0303 \u0289\u026f], and low vowels [\u00e6 \u00e6\u02d0 a a\u02d0 a\u0325 \u0251 \u0251\u02d0 \u0251\u0303]. For simplicity, we will refer to these respectively as /i/, /u/, and /a/.\\n\\nLanguage-specific broad vowels with fewer than 10 tokens were excluded from analysis. Midpoint f0 was converted to ERB (Glasberg and Moore, 1990), a perceptual scale, then averaged by broad category type within each language. As a descriptive analysis, we calculated the numerical difference of the effect on the averages, and whether the direction conformed to the previously posited universal at a descriptive level.\\n\\nInferential analyses were conducted at two levels: the language-specific level and the cross-language level. For the language-specific level, we aimed to test the reliability of an intrinsic f0 effect within a language, and determine the proportion of languages that exhibited the expected relationship. The midpoint f0 in ERB was predicted from the broad vowel category of /i/ vs /a/ or /u/ vs /a/.\\n\\nTwo models were chosen instead of one given that only one of the two high vowel categories passed our filtering criteria for many of the languages. Given the frequently small sample size, unbalanced contexts, and single speaker status, only simple linear regressions could be run on each language. Significance was assessed after applying the conservative Bonferroni correction, in which the alpha of 0.05 was divided by the total number of tested languages.\\n\\nTo investigate how reliably this effect held across languages, we submitted the raw data to two linear mixed-effects model with f0 in ERB as the dependent variable and fixed effects of broad vowel category (/i/ vs /a/ or /u/ vs /a/; treatment-coded), the voicing status of the preceding segment (voiced, voiceless, or silence; treatment-coded against voiced), the voicing status of the following segment (voiced, voiceless, or silence; treatment-coded against voiced), their interactions, the duration of the vowel, and random intercept and slopes for vowel category, preceding segment, and following segment by language. Only one speaker was observed per language so the language-specific random effects doubled as a speaker effect. The first model compared /i/ to /a/ in the broad vowel category, and the second compared /u/ to /a/. Effects with a t-value beyond an absolute value of 2.0 were taken as significant.\\n\\n4.2 Results\\n\\nAfter data exclusion, 53 languages from 17 language families were available for the /i/\u2013/a/ analysis, and 36 languages from 13 families for the /u/\u2013/a/ analysis.\\n\\nFigure 5 illustrates the variability as well as the systematicity in midpoint f0 within a speaker and language for each comparison between the high and low vowels. Each point representing the paired f0 means lies quite close to the dashed line of identity. The correlation of paired mean f0s was significant for each high\u2013low vowel comparison (/i/\u2013/a/: r = 0.95; /u/\u2013/a/: r = 0.94; each p < 0.001), demonstrating a highly predictable relationship between the f0 values of the two categories across languages.\\n\\nFor the /i/\u2013/a/ comparison, 39 of the 53 languages conformed to the expectation with a higher f0 for /i/ than /a/ (74%), whereas 14 of the 53 languages showed the numerically opposite ranking. As can be seen in Figure 3, the differences between /i/ and /a/ f0s were generally small.\"}"}
{"id": "lrec-2024-main-1114", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"correction, only four of the 39 direction-conforming languages reached significance and one of the 14 non-conforming languages. With an unadjusted alpha of 0.05, the number of \u201csignificant\u201d differences reached 19 of the 39 conforming languages, and two of the 14 non-conforming languages.\\n\\nThough most languages indeed demonstrated the expected direction of the effect, the differences were generally small and not significant, at least with these sample sizes.\\n\\nSimilar patterns of findings were observed for the /u/\u2013/a/ comparison: 32 of the 36 languages conformed to the expectation with a higher f0 of /u/ than /a/ (89%), whereas five languages showed the numerically opposite ranking. As before, the differences were generally small between /u/ and /a/, particularly if the f0 difference went in the opposite direction from expectations. With the Bonferroni-adjusted alpha value, a significant difference was observed for only four of the 31 direction-conforming languages and zero of the five non-conforming languages. With an unadjusted alpha of 0.05, 15 of the 31 conforming languages showed a significant difference, along with one of the five non-conforming languages.\\n\\nThe majority of the language-specific results were numerically, but not always significantly in line with the expected intrinsic f0 direction; however, the crosslinguistic analysis revealed a reliable effect of vowel height across languages for each of the /i/\u2013/a/ and /u/\u2013/a/ linear mixed-effects analyses (Table 2).\\n\\n| Vowel Height | Est. \u03b2 (t-val.) | Est. \u03b2 (t-val.) |\\n|--------------|----------------|----------------|\\n| /i/\u2013/a/     | 0.24 (4.33)    | 0.16 (2.15)    |\\n| /u/\u2013/a/     | 0.16 (2.15)    | 0.09 (2.07)    |\\n\\nTable 2: Estimated beta values and t-values for significant effects in the /i/\u2013/a/ or /u/\u2013/a/ mixed-effects linear model of f0 (ERB). (Italicized numbers with reduced font size reflect non-significance in the indicated model.)\\n\\nThe patterns of significance for the main effects were consistent between the /i/\u2013/a/ and /u/\u2013/a/ models (see Table 2). A preceding voiceless segment, a following silence, and a longer vowel duration corresponded to a significantly higher f0 for both the high and low vowels. The presence of a preceding silence or following voiceless segment did not significantly influence overall vowel f0.\\n\\nRegarding the interactions with vowel height, some minor differences emerged between the two models. In the /i/\u2013/a/ model, the difference between /i/ and /a/ f0s significantly decreased following a voiceless segment and before silence. In the /u/\u2013/a/ model, the difference in f0 between /u/ and /a/ following a voiceless segment, as well as before a voiceless segment was significantly larger. In both models, a preceding silence did not significantly influence the intrinsic f0 effect.\"}"}
{"id": "lrec-2024-main-1114", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On a macro-level, the effect of vowel height on f0 was significant, and we confirmed previous findings that final boundaries can diminish the effect (e.g., the interaction of vowel height and following silence; Ladd and Silverman, 1984; Shadle, 1985). More over, we observed highly similar f0s across vowel heights, which is consistent with the idea that f0 should be near-uniform across high and low vowels, despite any automatic or even intentional f0 enhancement effects (automatic effects: Whalen et al., 1998; intentional effects: Diehl and Kluender, 1989; uniformity: Chodroff and Wilson, 2022). Nevertheless, the stability of the effect varied across languages and speakers. A few counterexamples in the direction of the effect were observed for certain speakers and languages: this could reflect inherent speaker- or language-specific variability of the phenomenon, or simply be an artifact of unbalanced phonetic environments or low sample sizes.\\n\\nA few contradictory languages were also found relative to previous findings. In our sample, Finnish, Greek, and Hausa had numerically non-conforming f0 effects for /i/ and /a/ (/a/ f0 > /i/ f0). Whalen and Levitt (1995) found numerically expected differences for each of these languages, even with a low sample size (1\u20132 speakers). Ting et al. (2023) additionally found a significant f0 contrast in the expected direction across 100 speakers of Hausa, though speaker-specific deviations were not reported.\\n\\nOverall, we observe crosslinguistic support for an intrinsic f0 effect, in which the f0 of high vowels tends to be higher than that of low vowels. Nevertheless, the findings were more variable across individual speakers and languages than has been previously reported. It may be that with a larger sample size per language and more favorable prosodic contexts, the effect could stabilize. Alternatively, previous studies could have been limited by a relatively smaller number of languages and language families; the effect could vary depending on the speaker or language. The findings call for further investigation of the effect at scale, across a more diverse set of languages, and with consideration of the effect\u2019s stability from speaker to speaker.\\n\\nThe analysis is indeed limited by shortcomings of the corpus: at the present, only data from one speaker per language is available, and only one word per utterance. These present unfortunate confounds between speaker and language, as well as between the measurement and prosodic position. Our future work aims to extract additional data from the UCLA Phonetics Lab Archive, which will marginally increase the number of speakers per language, as well as the overall sample size. We also hope to extract longer passages of spoken data when available. Finally, further extraction and segmentation from unprocessed languages will further diversify the available data for analysis.\"}"}
{"id": "lrec-2024-main-1114", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the time of recording. Requests for data removal from the speakers, their descendants, or their language communities will be honored.\\n\\n7. Acknowledgments\\n\\nThe authors wish to thank Jian Zhu, Claire Moore-Cantwell, Pat Keating, and Ian Maddieson for helpful discussion. This project was supported by SNF Grants PR00P1_208460 to EC and PCEFP1_186841 to SM, and research funding to BP from Ryan Cotterell.\\n\\n8. Bibliographic References\\n\\nDiehl, R. L. and Kluender, K. R. (1989). On the objects of speech perception. Ecological Psychology, 1(2), 121\u2013144.\\n\\nChodroff, E., Ahn, E., and Dolatian, H. (2024). Comparing language-specific and cross-language acoustic models for low-resource phonetic forced alignment. Language Documentation & Conservation. [Manuscript accepted for publication].\\n\\nChodroff, E. and Wilson, C. (2022). Uniformity in phonetic realization: Evidence from sibilant place of articulation in American English. Language, 98(2), 250\u2013289.\\n\\nGlasberg, B. R. and Moore, B. C. J. (1990). Derivation of auditory filter shapes from notched-noise data. Hearing Research, 47(1-2):103\u2013138.\\n\\nLadd, R. and Silverman, K. E. (1984). Vowel intrinsic pitch in connected speech. Phonetica, 41(1), 31\u201340.\\n\\nLadefoged, P. and Maddieson, I. (1991). Phonetic structures of dying languages. (NSF grant 9107004.) https://www.nsf.gov/awardsearch/showAward?AWD_ID=9107004\\n\\nLadefoged, P. and Maddieson, I. (1994). Phonetic structures of endangered languages (NSF grant 9319705.) https://www.nsf.gov/awardsearch/showAward?AWD_ID=9319705&HistoricalAwards=false\\n\\nLadefoged, P. and Maddieson, I. (1996a). Recording the phonetic structures of endangered languages. UCLA Working Papers in Phonetics, 1\u20137.\\n\\nLadefoged, P. and Maddieson, I. (1996b). The Sounds of the World's Languages. Oxford, UK/Cambridge, MA: Blackwell.\\n\\nLadefoged, P. and Schuh, R. (2004). Broadening access to UCLA phonetic data. (NSF grant 0345465.) https://www.nsf.gov/awardsearch/showAward?AWD_ID=0345465\\n\\nLi, X., Metze, F., Mortensen, D. R., Black, A. W., and Watanabe, S. (2022). Phone inventories and recognition for every language. In Proceedings of the Thirteenth Language Resources and Evaluation Conference (pp. 1061\u20131067).\\n\\nLiu, Q., Gong, Z., Yang, Z., Yang, Y., Li, S., Ding, C., ... and Kurohashi, S. (2023). Hierarchical softmax for end-to-end low-resource multilingual speech recognition. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (pp. 1\u20135).\\n\\nMaddieson, I. (1984). Patterns of Sounds. Cambridge University Press, Cambridge.\\n\\nMeyer, E.A. (1896\u20137). Zur Tonbewegung des Vokals im gesprochenen und gesungenen Einzelwort. Phonetische Studien (Beiblatt zu der Zeitschrift Die Neuren Sprachen), 10, pp. 1\u201321.\\n\\nTing, C., Clayards, M., Sonderegger, M., and McAuliffe, M. (2023, October 24). The cross-linguistic distribution of vowel and consonant intrinsic F0 effects.\\n\\nWhalen, D. H., Gick, B., Kumada, M., and Honda, K. (1999). Cricothyroid activity in high and low vowels: Exploring the automaticity of intrinsic F0. Journal of Phonetics, 27(2), 125\u2013142.\\n\\nWhalen, D. H. and Levitt, A. G. (1995). The universality of intrinsic F0 of vowels. Journal of Phonetics, 23(3), 349\u2013366.\\n\\n9. Language Resource References\\n\\nAhn, E. and Chodroff, E. (2022). VoxCommunis: A corpus for cross-linguistic phonetic analysis. In Proceedings of the 13th Language Resources and Evaluation Conference (pp. 5286\u20135294).\\n\\nArdila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. (2020). Common Voice: A massively-multilingual speech corpus. In Proceedings of the 12th Conference on Language Resources and Evaluation.\\n\\nBlack, A. W. (2019). CMU Wilderness multilingual speech dataset. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (pp. 5971\u20135975). IEEE.\\n\\nDolatian, H. (2023). Interlingual MFA. GitHub. https://github.com/jhdeov/interlingual-MFA\\n\\nHammarstr\u00f6m, H., Forkel, R., Haspelmath, M., and Bank, S. (2023). Glottolog 4.8. Leipzig: Max Planck Institute for Evolutionary Anthropology. https://doi.org/10.5281/zenodo.7398962 (Available online at http://glottolog.org)\\n\\nLadefoged, P., Blankenship, B., Schuh, R. G., Jones, P., Gfroerer, N., Griffiths, E., Harrington, L., Hipp, C., Jones, P., Kaneko, M., Moore-Cantwell, C., Oh, G., Pfister, K., Vaughan, K., Videc, R., Weismuller, S., Weiss, S., White, J., Conlon, S., Lee, W.S.J., and Toribio, R. (2009). The UCLA Phonetics Lab Archive. Los Angeles, CA: UCLA Department of Linguistics. http://archive.phonetics.ucla.edu\\n\\nLi, X., Mortensen, D. R., Metze, F., & Black, A. W. (2021). Multilingual phonetic dataset for low-resource speech recognition. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 6958\u20136962). IEEE. Retrieved July 2023.\\n\\nMcAuliffe, M., Socolof, M., Mihuc, S., Wagner, M., and Sonderegger, M. (2017). Montreal Forced Aligner: Trainable text-speech alignment using Kaldi. In Proceedings of Interspeech 2017 (pp. 498\u2013502).\\n\\nMcAuliffe, M. And Sonderegger, M. (2022). English MFA acoustic model v2.0.0. https://mfa-models.readthedocs.io/acoustic/English/English_MFA_acoustic_model_v2_0_0.html\\n\\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. (2015). Librispeech: An ASR corpus based on\"}"}
{"id": "lrec-2024-main-1114", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"public domain audio books. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (pp. 5206\u20135210). IEEE.\\n\\nPratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert, R. (2020). MLS: A large-scale multilingual dataset for speech research. In Proceedings of Interspeech 2020 (pp. 2757\u20132761).\\n\\nRoach, P. J. (1989). Report on the 1989 Kiel Convention: International Phonetic Association. Journal of the International Phonetic Association, 19(2), 67\u201380.\\n\\nSalesky, E., Chodroff, E., Pimentel, T., Wiesner, M., Cotterell, R., Black, A. W., and Eisner, J. (2020). A corpus for large-scale phonetic typology. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp. 4526\u20134546). Association for Computational Linguistics.\\n\\nSchultz, T., Vu, N. T., and Schlippe, T. (2013). GlobalPhone: A multilingual text & speech database in 20 languages. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (pp. 8126\u20138130). IEEE.\\n\\nSeifart, F., Paschen, L., and Stave, M. (eds.). (2022). Language Documentation Reference Corpus (DoReCo) 1.2. Berlin & Lyon: Leibniz Zentrum Allgemeine Sprachwissenschaft & Laboratoire Dynamique Du Langage (UMR5596, CNRS & Universit\u00e9 Lyon 2).\\n\\nTyers, F. M. and Meyer, J. (2021). What shall we do with an hour of data? Speech recognition for the underserved languages of Common Voice. arXiv preprint arXiv:2105.04674.\"}"}
