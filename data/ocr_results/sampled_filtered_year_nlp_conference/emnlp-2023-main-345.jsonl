{"id": "emnlp-2023-main-345", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Definition: Evaluate the similarity between the two sentences, with respect to the condition. Assign the pair a score between 1 and 5 as follows:\\n\\n1: The two sentences are completely dissimilar with respect to the condition.\\n2: The two sentences are dissimilar, but are on a similar topic with respect to the condition.\\n3: The two sentences are roughly equivalent, but some important information differs or is missing with respect to the condition.\\n4: The two sentences are mostly equivalent, but some unimportant details differ with respect to the condition.\\n5: The two sentences are completely equivalent with respect to the condition.\\n\\nQuery\\nInput: Sentence 1: Elderly man sitting on a blue couch reading a paper.\\nSentence 2: Older man riding public transportation while reading a newspaper.\\nCondition: The location of the man.\\n\\nOutput: Figure 6: The full text input for the zero-shot evaluation with large language models, using 'long' instructions. Emphasis and section titles added for clarity.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We will be hosting more HITs in the future and we invite you to attend those.\\n\\nPlease send any feedback you have to: placeholder@gmail.com\\n\\nTask summary\\n\\nOur goal is to understand the similarity of a sentence pair based on a condition.\\n\\nConcretely, for a sentence pair (S1 and S2), provide one condition (C-High) such that S1 and S2 have high similarity, and one condition (C-Low) such that they have low similarity.\\n\\nAs an example:\\n\\nS1: A large green ball was bouncing on the street.\\n\\nS2: I bought a small green avocado.\\n\\nC-High: The color (High similarity because it is green in both sentences)\\n\\nC-Low: The size (Low similarity because it is large in the first sentence and small in the second sentence)\\n\\nConditions are English phrases which are used to choose a part of the sentences.\\n\\nGuidelines for conditions\\n\\nYou are allowed to use the internet to understand the sentences, but the conditions need to be written by you.\\n\\nThe following guidelines need to be followed.\\n\\n1. Conditions should be grammatically correct English phrases or sentences.\\n\\n2. Avoid conditions which reference information that cannot be inferred from sentences. For example, avoid the following condition, because the color of the animal in S2 is unclear:\\n\\n   a. S1: Brown bears attacked people in the night.\\n   b. S2: Some dogs were barking on the road.\\n   c. C-High: The color of the animal.\\n\\n   But the following is a good condition because it can be inferred that the game is chess:\\n\\n   d. S1: Black ultimately reached a end game two pawns up.\\n   e. S2: Now the white king comes just in time to support his pawn.\\n   f. C-High: The game being played.\\n\\n3. Conditions should reference aspects or attributes of sentences and not the values.\\n\\n   For example, the following (\\\"The color is green\\\") is an incorrect condition because it directly mentions \\\"green\\\", which is the value of the attribute \\\"color\\\":\\n\\n   a. S1: A large green watermelon.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"b. S2: A green avocado in the basket.\\n\\nc. C-High: The color is green.\\n\\nInstead, the same condition can correctly be written as: \\\"The color of the fruit\\\".\\n\\n4. Avoid conditions which explicitly use words like \\\"sentences\\\". For example, instead of saying \\\"the color in the sentence\\\", just say \\\"The color\\\".\\n\\n5. Avoid vague conditions which do not help narrow down a specific aspect of the sentence. For example, avoid conditions which simply say \\\"The activity\\\", which does not help narrow the aspect. Instead use more informative words like \\\"the sport\\\" or \\\"the hobby\\\" as much as possible.\\n\\n6. Whenever possible, try to write conditions which refer to abstract similarity.\\n\\nConsider the following sentences:\\n\\na. Two women are celebrating a goal.\\nb. A couple is eating a tasty meal.\\n\\nA condition which is more abstract is preferred:\\n\\nc. Abstract condition: The sentiment of the people.\\n\\nAlthough a more literal condition is valid, it is less preferred:\\n\\nd. Literal condition: The number of people.\\n\\nExamples: We provide good and bad examples of conditions for sentence pairs, along with the reasoning.\\n\\nGood examples: All the following conditions are valid because they follow our guidelines.\\n\\nSentence 1\\nSentence 2\\nCondition Similarity Explanation\\n\\nThe moon looked incredible!\\nThe car was completely covered in snow.\\n\\nThe color of the object.\\nHigh\\nThe color is white in both cases.\\n\\nThis is a good condition because it references the color of the object without explicitly mentioning it.\\n\\nA group of people wearing helmets and riding on bikes.\\nA group of bikers are gathered together and taking pictures.\\n\\nThe speed of the cyclist.\\nLow\\nThe group of cyclists is moving in the first sentence whereas they are not in the second. Hence their speeds are dissimilar.\\n\\nThree people are holding a ladder while another climbs it.\\nThree people are listening to music in a car.\\n\\nThe number of people.\\nLow\\nThere are four people in the first sentence but only three in the second sentence.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bad examples\\n\\nAll the following conditions are invalid because they ignore one or more of our guidelines.\\n\\n| Sentence 1 | Condition | Reason for invalidity |\\n|------------|-----------|-----------------------|\\n\\nEgyptians appeased gods with offerings and prayers. People in this era put faith in specific gods to protect their lives. The culture involved. It violates guideline 2. The culture in the second sentence cannot be inferred and is missing information.\\n\\nAn adult elephant is playing in the river. A boulder is rolling down the hill. The size of the object is large. It violates guideline 3. The condition should have been \\\"The size of the object,\\\" without explicitly referring to it being \\\"large.\\\"\\n\\nA guitarist is playing on a bench. A man in a green hat is playing the guitar on the road. The instrument in the sentence. It violates guideline 4. The condition would be good if \\\"in the sentence\\\" was removed so that it is just \\\"The instrument.\\\"\\n\\nA middle-aged man is helping construct a grass hut. Three men work on a roof. The activity. This condition is too vague and does not reference a specific aspect. A better condition would be: \\\"The type of construction.\\\"\\n\\nA man on top of a partially completed roof laying down more shingles. A man in a hard hat and safety gear stands in a construction site. The number of people. While this condition is valid, it violates guideline 6, which says that an abstract condition should be considered wherever possible. A better condition would have been, \\\"The occupation of the man,\\\" which is \\\"construction worker\\\" in both cases.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We will be hosting more HITs in the future and we invite you to attempt those. Please send any feedback you have to: placeholder@gmail.com\\n\\nTask summary\\nOur goal is to understand the similarity of a sentence pair based on a condition.\\n\\nConcretely, for a sentence pair \\\\((S1 \\\\text{ and } S2)\\\\) and a condition \\\\(C\\\\), provide a score which indicates the similarity of \\\\(S1\\\\) and \\\\(S2\\\\) with respect to \\\\(C\\\\).\\n\\nAs an example:\\n\\n\\\\(S1\\\\): A large green ball was bouncing on the street.\\n\\n\\\\(S2\\\\): I bought a small green avocado.\\n\\n\\\\(C\\\\): The size of the object\\n\\nSimilarity Score: 1 (Low similarity because it is large in the first and small in the second sentence)\\n\\nGuidelines for annotating similarity\\n\\nPart 1\\nGiven two sentences and a condition, first check if the condition applies to both the sentences. If the condition does not apply even to one of the sentences, please check the box provided to indicate the same.\\n\\nFor example:\\n\\n\\\\(S1\\\\): A small dog happily runs across the street.\\n\\n\\\\(S2\\\\): I bought a small green avocado.\\n\\n\\\\(C\\\\): The sentiment\\n\\nIn the above example, the condition does not make sense for \\\\(S2\\\\) because there is no sentiment that can be inferred from it.\\n\\nPart 2\\nIf the condition makes sense, given two sentences and a condition, please assign a similarity score for the sentences when interpreted with respect to the condition.\\n\\nThe score has to be one of the following numbers: \\\\(\\\\{1, 2, 3, 4, 5\\\\}\\\\).\\n\\nTips:\\n\\n- Please avoid overusing the middle range score (3) as much as possible.\\n- Feel free to use the extreme scores (1 and 5) if they make sense to you.\\n\\nThe following is the meaning of the different scores:\\n\\n1. Score \\\\(= 1\\\\): The two sentences are completely dissimilar with respect to the condition. For example:\"}"}
{"id": "emnlp-2023-main-345", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The gender (man and woman are different with respect to gender) 2.\\n\\nScore = 2: The two sentences are different, but are similar with respect to the condition.\\n\\nFor example:\\nS1: A man plays the guitar.\\nS2: A little girl listens to the violin.\\n\\nC: The instrument (Both are string instruments, similar but different instruments) 3.\\n\\nScore = 3: The two sentences are roughly equivalent, but some important information differs or is missing with respect to the condition.\\n\\nFor example:\\nS1: A small crowd gathered around the injured person.\\nS2: A crowd jumps up and down to the tunes played by an artist.\\n\\nC: Number of people (While both are crowds, it is important and unclear how many people there are.) 4.\\n\\nScore = 4: The two sentences are mostly equivalent, but some unimportant details differ with respect to the condition.\\n\\nFor example:\\nS1: The little girl plays the jazz guitar.\\nS2: The guitar looked nice and shiny.\\n\\nC: The instrument (Guitar in both cases, but the exact type is different and unimportant) 5.\\n\\nScore = 5: The two sentences are completely equivalent as they mean the same thing with respect to the condition.\\n\\nFor example:\\nS1: Three boys play on the playground.\\nS2: There are 3 girls near the fountain.\\n\\nC: The number of people (3 and three are strictly equivalent)\"}"}
{"id": "emnlp-2023-main-345", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nSemantic textual similarity (STS), a cornerstone task in NLP, measures the degree of similarity between a pair of sentences, and has broad application in fields such as information retrieval and natural language understanding. However, sentence similarity can be inherently ambiguous, depending on the specific aspect of interest. We resolve this ambiguity by proposing a novel task called Conditional STS (C-STS) which measures sentences' similarity conditioned on a feature described in natural language (hereon, condition). As an example, the similarity between the sentences \u201cThe NBA player shoots a three-pointer.\u201d and \u201cA man throws a tennis ball into the air to serve.\u201d is higher for the condition \u201cThe motion of the ball\u201d (both upward) and lower for \u201cThe size of the ball\u201d (one large and one small). C-STS\u2019s advantages are two-fold: (1) it reduces the subjectivity and ambiguity of STS and (2) enables fine-grained language model evaluation through diverse natural language conditions. We put several state-of-the-art models to the test, and even those performing well on STS (e.g. SimCSE, Flan-T5, and GPT-4) find CSTS challenging; all yielding Spearman correlation scores below 50. To encourage a more comprehensive evaluation of semantic similarity and natural language understanding, we make nearly 19K C-STS examples and code available for others to train and test their models.\\n\\n1 Introduction\\nOver the years, natural language processing (NLP) has progressed through the co-evolution of model design (e.g. architectures, training methods) and evaluation methods for language tasks (Wang et al., 2018, 2019; Hendrycks et al., 2021). A common task used to evaluate NLP models has been Semantic Textual Similarity (STS) (Agirre et al., 2012), which evaluates the models\u2019 ability to predict the semantic similarity between two sentences. Several diverse STS datasets are popularly used, with prior work expanding the STS task to multiple domains and languages (Agirre et al., 2013, 2014, 2015, 2016; Cer et al., 2017; Abdalla et al., 2021). STS tasks have been a component of the popular GLUE natural language understanding benchmark (Wang et al., 2018) and are a key evaluation tool for sentence-representation learning specifically (Conneau et al., 2017; Cer et al., 2018; Reimers and Gurevych, 2019; Gao et al., 2021, inter alia).\\n\\nDespite its popularity, STS may be inherently ill-defined. The general semantic similarity of two sentences can be highly subjective and vary wildly depending on which aspects one decides to focus on. As observed in several studies, ambiguity in similarity judgements of word or sentence pairs can be reduced with the help of context for both human and machine. Providing conditions reduces ambiguity of the sentence similarity task, and allows evaluation of a grounded and multi-faceted notion of sentence similarity.\\n\\nFigure 1: C-STS: Two sentences are judged by their similarities based on free-form natural language conditions. The two sentences are more similar when judged by the condition \u2018The base of the object\u2019 (highlighted in yellow) as both windsurfing and surfing use a similar board but are dissimilar when judged by the condition \u2018The way the object is propelled\u2019 (highlighted in blue) because one is propelled by waves and the other by wind. Providing conditions reduces ambiguity of the sentence similarity task, and allows evaluation of a grounded and multi-faceted notion of sentence similarity.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sentence Pair Data Collection\\n\\nImage Encoder\\n\\nText Image Embedding\\n\\nCosine(                ,                )\\n\\nSelect top-K\\n\\nText\\n\\nUse Captions\\n\\nCosine(        ,            )\\n\\nCondition Annotation & Verification\\n\\nA large green ball was bouncing on the street.\\nI bought a small green avocado.\\n\\nC-High: The color\\nC-Low: The size\\n\\nB. Candidate Filtering\\n\\nA. Sample Image-Caption Pair\\n\\nText Text\\n\\nText Encoder\\n\\nFinal text pairs\\n\\nThresholding\\n\\nFigure 2: Illustrating the data collection process for C-STS-2023. (Left) We first show the sentence pair collection procedure (\u00a72.2.1). Step A: An image-caption pair is sampled (red) from the dataset and then fed into the image encoder to get the image embedding. The image embedding is compared against all other image embeddings in the dataset (blue) to find the top-k similar images. The original caption is then paired with the corresponding captions of the top-k similar images to generate sentence pairs. Step B: The sentence pairs are filtered based on textual similarity. (Right) We illustrate the condition annotation/verification procedure (\u00a72.2.2). Once the sentence pairs have been collected, they are sent to qualified Mechanical Turkers to get annotations and verify conditions.\\n\\nmans (De Deyne et al., 2016a,b) and models (Veit et al., 2016; Ye et al., 2022a; Lopez-Gazpio et al., 2017; Camburu et al., 2018).\\n\\nConsidering the importance of STS tasks for evaluating sentence representations, we propose a new task called Conditional STS (C-STS), illustrated in Figure 1, which seeks to disambiguate the similarity of two sentences by measuring similarity within the context of a condition sentence.\\n\\nC-STS uses free-form natural language conditions, enabling us to evaluate and probe natural language understanding for myriad fine-grained aspects. Figure 1 illustrates two conditions (\\\"The base of the object\\\" and \\\"The way the object is propelled\\\") which probes language models' conception of similarity for different aspects concerning water sports and physical reasoning. Since conditions themselves are unconstrained sentences, they allow us to evaluate a precise, grounded, and multi-faceted notion of sentence similarity.\\n\\nTo comprehensively test models on C-STS, we create the C-STS-2023 dataset which includes 18,908 instances containing sentence pairs, a condition, and a scalar similarity judgement on the Likert scale (Likert, 1932). We find that even state-of-the-art sentence encoders and large language models perform poorly on our task. Although SimCSE (Gao et al., 2021) and GPT-4 (OpenAI, 2023a) are among the best-performing systems, their relatively poor Spearman correlation of 47.5 and 43.6 respectively, points to significant room for improvement (SimCSE achieves a Spearman correlation of 88.09 on STS-B validation splits for comparison).\\n\\nWe believe that C-STS provides a testbed for potentially novel modeling settings and applications. Toward this end, we propose and evaluate a unique encoding setting (a tri-encoder) and objectives (a quadruplet contrastive loss with hard negatives) that take advantage of C-STS's three-sentence inputs and paired high- and low-similarity instances.\\n\\nOur qualitative analysis shows that models find C-STS challenging when tested on different aspects of the same sentence pair rather than testing an unconditional and ambiguous notion of similarity. We hope that future work evaluates on C-STS in addition to STS tasks to comprehensively benchmark semantic similarity in language models.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"pect(s) of the sentences are being referred to. Formally, consider conditions ($c_i \\\\in C$) that refer to disjoint aspects of the sentences, then the similarity of the two sentences may be represented as:\\n\\n$$|C| \\\\sum_{i=1}^{n} w_i \\\\text{sim}_{c_i}(s_1, s_2)$$\\n\\nsubject to $\\\\sum_{i=1}^{n} w_i = 1$.\\n\\nHere, $w_i$ is the weight assigned by the annotator to the condition $c_i$, and $\\\\text{sim}_{c_i}(s_1, s_2)$ is the similarity of the sentences with respect to the condition.\\n\\nThese weights are latent to the task and each annotator has their own interpretation of them which helps marginalize similarity, thus introducing ambiguity in the task. C-STS seeks to disambiguate the STS task by measuring similarity conditioned by a single aspect specified in natural language.\\n\\n### 2.2 Conditional semantic textual similarity\\n\\nC-STS is a task comprised of quadruplets containing two sentences (a sentence pair), a natural language condition, and a similarity assessment ($\\\\{s_1, s_2, c, y\\\\}$). Crucially, we do not place any strict constraints on $c$, allowing it to be any relevant phrase. This allows us to probe potentially any possible aspect of similarity that may be considered between sentences.\\n\\n#### 2.2.1 Sentence Data Collection\\n\\nThe first stage of making the C-STS dataset is to acquire the sentence pairs that will later be used in eliciting conditioning statements from annotators. We source sentence pairs $\\\\{s_1, s_2\\\\}$ for our dataset from image-captioning datasets through a two-step process: (1) generate candidate text pairs through dense retrieval from the corresponding image representations and (2) filter out candidates that are irrelevant or ineffectual for our purposes.\\n\\n**Image Retrieval**\\n\\nImage-captioning datasets provide a compelling data source because image pair similarity and caption (text) pair similarity encode different semantics (Parekh et al., 2021). Image-representations thus serve as an informative latent variable which can represent their captions in ways that are not captured by text retrievers.\\n\\nSince current sentence representation models overlook aspects of conditional similarity, we utilize both the image and text to retrieve sentence pairs which form the foundation of our dataset. We aim to derive sentence pairs from an image-caption dataset $D$ to aid in creating conditioning statements. To do this, we first generate a store of image pairs, or $P_I$. Each pair, denoted by $I_i, I_j$, is such that $I_j$ is amongst the top-$k$ most similar images to $I_i$, determined by the cosine distance metric of their respective image representations obtained via an image encoder $E_I(\\\\cdot)$. After establishing $P_I$, we convert it into a sentence pair store ($P_S$) by replacing each image in a pair with its corresponding caption. When each image $I_i \\\\in D$ is associated with a set of sentences $\\\\{s\\\\}_i$ we take all sentence pairs from the Cartesian product $\\\\{s\\\\}_i \\\\times \\\\{s\\\\}_j$ for each image pair $I_i, I_j \\\\in P_I$.\\n\\n**Candidate Filtering**\\n\\nAfter acquiring initial sentence pairs through image retrieval, we perform additional filtering to eliminate sentence pairs which are ill-suited for our task. Specifically, we aim to include only pairs of sentences for which the unconditional similarity is somewhat ambiguous, as this incentivizes models to rely on the condition when reasoning about the conditional similarity.\\n\\nTo this end, we avoid high similarity sentence pairs by filtering out those with a high bag-of-words intersection over union and avoid low similarity sentence by choosing sentences with moderate or high cosine similarity of their SimCSE embeddings (Gao et al., 2021). See Appendix A.2 for a full description of all filtering criteria used.\\n\\n**Dataset sources**\\n\\nFor the construction of sentence pairs candidates, we use two image-caption datasets: the train split from the 2014 MS COCO dataset (Lin et al., 2014) containing $\\\\sim 83,000$ images, and Flickr30K (Young et al., 2014) containing $\\\\sim 31,000$ images. Each dataset is processed separately and we do not intermix them during the retrieval stage. We use CLIP-ViT (Radford et al., 2021) to encode images and include the specific filtering criteria in Table 6 of Appendix A.2.\\n\\n#### 2.2.2 Annotation Methodology\\n\\nFor each sentence pair in the store ($P_S$), we wish to collect conditions and semantic similarity annotations for each sentence pair and condition triplet, $\\\\{s_1, s_2, c\\\\}$. As $c$ is a free-form natural language sentence, the annotator is provided with a high-level of control over which aspect to condition on. Human annotations are acquired through Mechanical Turk in a 3-stage process.\\n\\n**Stage 1:** Choosing a high-quality worker pool\\n\\nIn the first stage, we design a qualification test to select workers who excel at our task. Specifically, we test two skills: (1) The quality of conditions they generated and (2) the quality of their annotations of those conditions for the sentence pairs.\\n\\nWe score the conditions and annotations based on their clarity, relevance, and precision. We filter out workers who perform below a certain threshold, ensuring that we only work with high-quality annotators.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"write for a given sentence pair and (2) semantic similarity judgements for a triplet \\\\{s_1, s_2, c\\\\}. We choose a pool of 271 workers who perform well on both tasks and restrict subsequent stages to include only workers from this pool. See Appendices C.1 and C.2 for an example of these tests.\\n\\nStage 2: Condition annotation\\nAfter sourcing sentence pairs \\\\{s_1, s_2\\\\} using the strategy discussed in the Section 2.2.1, we instruct workers to annotate each pair with one condition such that the similarity in its context is high (C-High) and one such that the similarity in its context is low (C-Low). Example:\\n\\n\\\\begin{itemize}\\n  \\\\item \\\\textbf{s}_1: A large green ball was bouncing on the street\\n  \\\\item \\\\textbf{s}_2: I bought a small green avocado\\n  \\\\item \\\\textbf{C-High}: The color of the object\\n  \\\\item \\\\textbf{C-Low}: The size of the object\\n\\\\end{itemize}\\n\\nWe do not place any constraints on the conditions other than that they should be semantically unambiguous phrases and relevant to the sentence pair (Appendix C.1).\\n\\nStage 3: Condition verification and similarity assessment\\nThe output of annotations from the previous stage are triplets \\\\{s_1, s_2, c\\\\} with a binary similarity assessment (high or low). In this stage we ask new annotators to assign a similarity on a Likert scale (Likert, 1932) (as an integer between 1 and 5) as is common with semantic textual similarity tasks (Agirre et al., 2012). In addition to assigning a similarity, we also use this stage to verify if the conditions from the previous stage are pertinent to the sentence pairs, filtering out potentially low quality examples. At the end of this stage, we have \\\\{s_1, s_2, c, y\\\\} quadruplets which have passed a layer of human verification (Appendix C.2).\\n\\n3 Dataset Analysis\\nDataset statistics\\nTo ensure high-quality, faithful, and diverse annotations, we collect a total of 20,000 instances and perform quality assurance (Section 5.3) resulting in a total of 18,908 instances as part of the C-STS-2023 dataset. Following standard practice, we create train, validation, and test splits in a 60 : 15 : 25 ratio. We present the distribution of similarity scores, which are discrete numbers between [1, 5], in Figure 4. We also measure the inter-annotator agreement on a random sample of 100 examples with three independent annotations and find Fleiss' kappa score (Fleiss, 1971) to be 0.61 which implies substantial inter-annotator agreement. Average length of sentences and conditions is 12.6 and 5.3 words.\\n\\nQualitative analysis\\nC-STS allows us to evaluate the generally fuzzy notion of sentence similarity with greater fidelity. We illustrate this in Table 1, where precise and discriminative conditions allow a targeted, fine-grained, and grounded definition of sentence similarity. The following is a representative instance where the conditions tease out nuanced and hidden similarities and differences between the two lexically similar sentences on surfing: Consider \\\\textbf{s}_1: \\\"A windsurfer skims the water...\\\" and \\\\textbf{s}_2: \\\"The surfer is riding a wave...\\\". While the sentences are significantly dissimilar based on the condition \\\"the way the object is propelled\\\" as they talk about wind surfing and surfing respectively (the former uses a sail whereas the latter depends on the wave), they are very similar in context of the condition \\\"the base of the object\\\" as both wind-surfing and surfing use a similar board.\\n\\nOur diverse set of conditions provides broad support over the distribution of conditions and enables a holistic and multi-faceted evaluation of sentence similarity. For example, the conditions for the sentences on Tennis in Table 1 test similarity both on the sport being played (which requires understanding lexical and knowledge artifacts) as well as the number of people (which requires reasoning and commonsense capabilities).\\n\\n4 Baselines\\nWe evaluate our dataset on several baselines which can be categorized into (1) Fine-tuning baselines, which are pre-trained models finetuned on the C-STS training split, and (2) Large language models (LLMs) baselines, which are evaluated using instructions and in-context examples.\\n\\n4.1 Fine-tuning baselines\\nWe evaluate three sentence encoder models RoBERTa (Liu et al., 2019), supervised SimCSE (Gao et al., 2021) and unsupervised DiffCSE (Chuang et al., 2022). SimCSE and DiffCSE represent state-of-the-art sentence encoder models which are particularly strong on STS tasks. For both SimCSE and DiffCSE, we use the RoBERTa pre-trained varieties.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An older man holding a glass of wine while standing between two beautiful ladies.\\n\\nA group of people gather around a table with bottles and glasses of wine.\\n\\nThe people's demeanor: 5\\n\\nThe number of bottles: 1\\n\\nVarious items are spread out on the floor, like a bag has been emptied.\\n\\nA woman with a bag and its contents placed out before her on a bed.\\n\\nThe arrangement of objects: 4\\n\\nThe surface the objects are on: 1\\n\\nA windsurfer skims the water with his outstretched hand.\\n\\nThe surfer is riding a wave with a mountain in the background.\\n\\nThe base of the object: 5\\n\\nThe way the object is propelled: 1\\n\\nFemale tennis player jumping off the ground and swinging racket in front of an audience.\\n\\nA young lady dressed in white playing tennis while the ball girl retrieves a tennis ball behind her.\\n\\nThe sport being played: 5\\n\\nThe number of people: 1\\n\\nTable 1: Four examples from the C-STS validation set. Under different conditions, the same sentence pair can be separated into high similarity and low similarity. Scale from 1 (dissimilar) to 5 (similar).\\n\\nregression finetuning for STS tasks by simply concatenating the sentences and encoding them together before generating a prediction; let us call this type of architecture a cross-encoder. Recent approaches instead opt to encode sentences separately and compare their similarity using a distance metric, such as the cosine distance Reimers and Gurevych (2019); which we will call a bi-encoder.\\n\\nWhile DiffCSE and SimCSE were designed with the bi-encoder setting in mind, we observe that they work well in the cross-encoder setting as well. For our baselines, we evaluate each model in both settings. For the cross-encoder configuration, we encode the triplet containing the sentences and the condition \\\\( \\\\{s_1, s_2, c\\\\} \\\\), and the output is a scalar similarity score \u2013 \\\\( f_\\\\theta(s_1; s_2; c) \\\\). For the bi-encoder configuration (Reimers and Gurevych, 2019), the sentences of a pair are encoded independently along with the condition using a Siamese network and their cosine similarity is computed \u2013 \\\\( \\\\cos(f_\\\\theta(s_1; c), f_\\\\theta(s_2; c)) \\\\).\\n\\nIn addition to the bi- and cross-encoder models, we propose tri-encoder models which encode each sentence and condition separately. This conceptually resembles late-interaction contextualized retrieval approaches, such as Humeau et al. (2020) or Khattab and Zaharia (2020), but our approach is specific to C-STS. For this, we first encode all sentences of the triplet separately, with encoder \\\\( f_\\\\theta(\\\\cdot) \\\\) as \\\\( s_i = f_\\\\theta(s_i) \\\\), where \\\\( s_i \\\\in \\\\mathbb{R}^d \\\\). We then perform an additional transformation \\\\( h: \\\\mathbb{R}^2d \\\\rightarrow \\\\mathbb{R}^d \\\\) that operates on the condition and one each of the sentences. We finally compute the conditional similarity using the cosine similarity as \\\\( \\\\cos(h(c); s_1), h(c); s_2) \\\\).\\n\\nWe experiment with 2 functions for \\\\( h \\\\), an MLP and the Hadamard product.\\n\\nObjectives\\n\\nIn addition to the standard MSE loss for regression, we use a quadruplet contrastive margin loss which we denote Quad. Since each sentence pair in C-STS comes with two conditions (one with higher similarity and one with lower similarity) we represent the conditional encoding of each sentence in the higher-similarity pair as \\\\( p_1 \\\\) and \\\\( p_2 \\\\) and represent the conditional encoding of each sentence in the lower similarity pair as \\\\( n_1 \\\\) and \\\\( n_2 \\\\). The Quad loss is then defined as follows:\\n\\n\\\\[\\n\\\\text{Quad}(p_1, p_2, n_1, n_2) = \\\\max(\\\\lambda + \\\\cos(n_1, n_2) - \\\\cos(p_1, p_2), 0)\\n\\\\]\\n\\nwhere \\\\( \\\\lambda \\\\) is a margin hyperparameter.\\n\\nWe train all of our tasks for regression using, alternatively, mean squared error (MSE), Quad, and a linear combination of the quadruplet loss and MSE (Quad + MSE). Since we require a separate conditional encoding for each sentence, the Quad and (Quad + MSE) objectives apply only the bi-encoder and tri-encoder configurations.\\n\\nHyperparameters\\n\\nWe evaluate the baselines on the test split for C-STS. We perform a hyperparameter sweep to select the best performing configuration and test using models trained with 3 random seeds, with further details in Appendix A.3. As a comparison for our training setting, we perform a similar hyperparameter sweep for the STS-B (Cer et al., 2017) dataset, with the validation split results.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Large language models baselines\\n\\nFor the generative setting, we evaluate two types of models: (1) instruction-finetuned encoder-decoder models, including Flan-T5 (Chung et al., 2022), Flan-UL2 (Tay et al., 2023), and TinyTURKINSTRUCT (Wang et al., 2022) and (2) proprietary autoregressive LLMs including ChatGPT-3.5 (OpenAI, 2022) and GPT-4 (OpenAI, 2023a).\\n\\nFor ChatGPT-3.5 and GPT-4, we use the OpenAI API with versions gpt-3.5-turbo-0301 and gpt-4-0314 respectively.\\n\\nWhen evaluating zero- or few-shot capabilities, each model input is composed of up to three parts: instruction (task definition), k in-context examples, and query. Models are evaluated with 0, 2, or 4 examples and using three different instruction prompts: no instruction, short instruction, which provides only a high-level description of the task, and long instruction, shown in Figure 6, which resembles the annotation guidelines and is similar to the instructions used for the STS-B classification task in Wang et al. (2022).\\n\\nFor few-shot evaluation, we additionally always group a sentence pair's two conditional similarity examples together, so models will always see contrasting pairs in the examples, but won't see a paired example for the query. We provide examples of the formats used for the input and output for more settings in Appendix B. As we did for the finetuned models, we also evaluate these models on the STS-B validation split, shown in Table 12, with instruction finetuned models and ChatGPT achieving strong performance.\\n\\n5 Results\\n\\n5.1 Evaluating sentence encoders on C-STS\\n\\nZero-shot bi-encoder performance\\n\\nAs an initial comparison, we evaluate bi-encoder models without finetuning, on both C-STS and STS-B. As shown in Table 2, we see that strong performance on STS-B does not translate to good performance on C-STS, suggesting that these models fail entirely to incorporate the provided conditioning statement. These results suggest that current approaches to training sentence encoders may be too specialized to existing tasks for evaluation, such as STS-B.\\n\\n| Encoding Model | Spear. | Pearson |\\n|----------------|--------|---------|\\n| RoBERTa BASE   | 28.1\u00b18.5 | 22.3\u00b114.1 |\\n| RoBERTa LARGE  | 27.4\u00b16.2 | 21.3\u00b18.4  |\\n| DiffCSE BASE   | 43.4\u00b10.2 | 43.5\u00b10.2 |\\n| DiffCSE LARGE  | 47.5\u00b10.1 | 47.6\u00b10.1 |\\n| SimCSE BASE    | 44.8\u00b10.3 | 44.9\u00b10.3 |\\n| SimCSE LARGE   | 47.5\u00b10.1 | 47.6\u00b10.1 |\\n\\nTable 3: We report fine-tuned model test split results in Spearman and Pearson correlations for three models (RoBERTa, DiffCSE, and SimCSE) in different encoding settings.\\n\\nFine-tuning baselines\\n\\nWe finetune our sentence encoder baselines on C-STS and show the test performance in Table 3. Again, the best models are SimCSE and DiffCSE in the bi-encoding setting. This is suggests that the sentence representations learned in their contrastive learning phase facilitate learning for C-STS substantially, but still struggle with all Spearman correlation below 50.\\n\\nPerformance on C-STS varies significantly depending on the encoding configurations, with the bi-encoder setting proving to be the most effective, especially for SimCSE and DiffCSE models. Performance of the tri-encoder model, introduced in Section 4.1 was generally poor, with all models performing well below their bi-encoding and cross-encoding counterparts.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Few-shot Spearman correlation on the test split.\\n\\nModels perform much worse than their finetuned counterparts, with GPT-4 being the only evaluated model that achieves comparable performance to some fine-tuned baselines.\\n\\n\u2020: Fine-tuning on the full train set.\\n\\n5.2 Evaluating pre-trained LLMs\\n\\nWe show performance of generative models evaluated on C-STS in various prompting settings in Table 4, with some additional results for smaller Flan-T5 models in Table 11 in the Appendix. Notably, the state-of-the-art language model, GPT-4, performs substantially better than all competing models and systems (UL2, Flan-T5, ChatGPT-3.5) and is competitive with a finetuned SimCSE LARGE model, the best performing sentence-encoder. For example, in most settings, GPT-4 outperforms ChatGPT-3.5 and Flan models by over 10 points. This suggests existing large language benchmarks may correlate with C-STS as GPT-4 has shown to be the most proficient in a wide variety of evaluation settings (OpenAI, 2023b).\\n\\nBetween suites of models of different sizes (viz. Flan-T5, Tk-Instruct), we observe a strong correlation between model scale and performance. We also find that providing instructions improves performance substantially for C-STS and that this performance is robust to different instructions lengths and the number of in-context examples.\\n\\n5.3 Analysis\\n\\nScaling laws for C-STS\\n\\nWe evaluate the effect of the quantity of C-STS data on sentence-embedding methods for SimCSE LARGE (Figure 3). We notice that for all three encoding strategies, performance monotonically increases as we increase the size of the training dataset. For example, for the SimCSE bi-encoder, the Spearman correlation increases from 30 when using a train set of 1,000 examples to 45 for 7,000 examples. There is almost a linear increase in the performance of the models, especially the bi-encoder as we increase the amount of data. This quantitatively enforces the quality of the dataset, but also retroactively makes the point that rather than relying on more data, we require better modeling strategies.\\n\\nQualitative analysis\\n\\nWe present predictions from different models in Table 5 to illustrate systematic pitfalls. For instance, Flan-T5 makes incorrect predictions even for straightforward instances and falsely predicts that both sentences talk about the same dish, even though the sentences clearly talk about sandwiches and pizza respectively. Additionally, ChatGPT-3.5 incorrectly predicts that the two sentences are completely dissimilar when talking about the types of plants, even though both sentences mention flowering plants. Note that our annotation, unlike ChatGPT-3.5, captures the nuance that the first sentence talks about both shrubbery and flowers, while the second sentence talks only about flowers, and therefore assigns a conservative similarity score of 3. The most proficient model on C-STS, GPT-4, is much better at capturing these nuances and accurately predicts, for instance, that the height of the giraffe's head (refer to the fourth example), is high in one sentence and...\"}"}
{"id": "emnlp-2023-main-345", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 5: Examples of model predictions evaluated on C-STS in the in-context setting ($K = 2$ with no instructions).\\n\\nWe choose examples with different levels of accuracy, showcasing different failure cases of model behavior.\\n\\n- **Flan-T5-Base**\\n  - Sentence 1: A man taking a bite out of a sandwich at a table with someone else.\\n  - Sentence 2: A man sitting with a pizza in his hand in front of pizza on the table.\\n  - Condition: Type of dish.\\n  - Output: Pred: 4.5, Label: 1.0\\n\\n- **GPT-3.5**\\n  - Sentence 1: A wooden bench surrounded by shrubbery and flowers on the side of a house.\\n  - Sentence 2: A scene displays a vast array of flower pots in front of a decorated building.\\n  - Condition: The type of plants.\\n  - Output: Pred: 0.0, Label: 3.0\\n\\n- **GPT-4**\\n  - Sentence 1: Football player jumping to catch the ball with an empty stand behind him.\\n  - Sentence 2: A football player preparing a football for a field goal kick, while his teammates can coach watch him.\\n  - Condition: The game being played.\\n  - Output: Pred: 3.0, Label: 5.0\\n\\n- **GPT-4**\\n  - Sentence 1: A giraffe reaches up his head on a ledge high up on a rock.\\n  - Sentence 2: A giraffe in a zoo bending over the fence towards where impalas are grazing.\\n  - Condition: The height of the giraffe's head.\\n  - Output: Pred: 1.0, Label: 1.0\\n\\n---\\n\\n6 Related Work\\n\\nHistorical perspectives of semantic similarities: Measuring semantic similarities is a long-standing problem spanning cognitive science (Miller and Charles, 1991) to psychology (Tversky, 1977) where early attempts are made to quantify the subjective similarity judgements with information theoretical concepts. More recently, interest in semantic similarity has gained popularity in the context of machine learning, with works in computer vision recognizing that the notion of similarity between images varies with conditions (Veit et al., 2016) and can therefore be ambiguous (Ye et al., 2022b).\\n\\nTextual similarity tasks: Capturing textual similarity is also considered a fundamental problem in natural language processing. Works such as Agirre et al. (2012, 2016) define the textual semantic similarity tasks (STS), which is widely used in common benchmarks such as GLUE (Wang et al., 2018). Extensions to the STS setting have been proposed such as making the task broader with multilinguality (Cer et al., 2017) or incorporating relatedness (Abdalla et al., 2021). However, the loose definition of similarity has not been acknowledged as an issue explicitly. In contrast, our work tackles the ambiguity problem by collecting conditions and hence reducing subjectivity. To alleviate ambiguity, explanations play an important role in identifying the differences between the two sentences either in their syntactical structure (Lopez-Gazpio et al., 2017) or in natural language (Camburu et al., 2018), but the post-hoc nature of explanations prevents it from being used prior to the similarity judgement, rendering it a supplemental component as opposed to a paradigm change in the task setup. Beyond STS, works that leverage conditioning to enhance sentence representations obtain improved performance for retrieval (Asai et al., 2023) and embedding qualities (He et al., 2015; Su et al., 2023; Jiang et al., 2022), which corroborates the observation that conditioning as a form of disambiguation benefits similarity measures.\\n\\n7 Conclusion\\n\\nIn this work, we propose conditional semantic textual similarity (C-STS), a novel semantic similarity assessment task that resolves the inherent ambiguity in STS. Given the importance of STS and its importance in sentence representation evaluation we believe that C-STS is a timely and necessary addition to the language model evaluation landscape. Rather than testing unconditional semantic similarity, the diversity of conditions in our dataset allows fine-grained evaluation. The same sentence pairs can be tested on a variety of different aspects represented by conditions, with similarities often varying significantly. C-STS poses a challenging hurdle to both encoder-only and state-of-the-art generative language models which struggle to capture the high-dimensional manifold of similarity.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We believe that a combination of improved modeling and fine-tuning strategies are required to push the boundaries on C-STS and we hope that C-STS can enable innovative future work in language understanding and representation learning.\\n\\nLimitations\\n\\nWe propose the novel task of conditional semantic textual similarity (C-STS). Given that this is a new task, we collect a dataset of over 19,000 instances, but one limitation that this size can be increased to ensure sentence embedding style models have additional data for fine-tuning. Further, we use two different sources to collect our sentence pairs, and future studies, motivated by STS follow-ups, can collect data from other sources.\\n\\nReferences\\n\\nMohamed Abdalla, Krishnapriya Vishnubhotla, and Saif M. Mohammad. 2021. What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study. ArXiv:2110.04845 [cs].\\n\\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, I\u00f1igo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252\u2013263, Denver, Colorado. Association for Computational Linguistics.\\n\\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81\u201391, Dublin, Ireland. Association for Computational Linguistics.\\n\\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 497\u2013511, San Diego, California. Association for Computational Linguistics.\\n\\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012).\\n\\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic textual similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 32\u201343, Atlanta, Georgia, USA. Association for Computational Linguistics.\\n\\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2023. Task-aware retrieval with instructions. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3650\u20133675, Toronto, Canada. Association for Computational Linguistics.\\n\\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. 2017. SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1\u201314, Vancouver, Canada. Association for Computational Linguistics.\\n\\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. 2018. Universal sentence encoder for English. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 169\u2013174, Brussels, Belgium. Association for Computational Linguistics.\\n\\nYung-Sung Chuang, Rumen Dangovski, Hongyin Luo, Yang Zhang, Shiyu Chang, Marin Soljacic, Shang-Wen Li, Scott Yih, Yoon Kim, and James Glass. 2022. DiffCSE: Difference-based contrastive learning for sentence embeddings. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4207\u20134218, Seattle, United States. Association for Computational Linguistics.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670\u2013680, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nSimon De Deyne, Daniel J Navarro, Amy Perfors, and Gert Storms. 2016a. Structure at every scale: A semantic network account of the similarities between unrelated concepts. Journal of Experimental Psychology: General, 145(9):1228.\\n\\nSimon De Deyne, Amy Perfors, and Daniel J Navarro. 2016b. Predicting human similarity judgments with distributional models: The value of word associations. In Proceedings of coling 2016, the 26th international conference on computational linguistics: Technical papers, pages 1861\u20131870.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nJoseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76:378\u2013382.\\n\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nHua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576\u20131586, Lisbon, Portugal. Association for Computational Linguistics.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations.\\n\\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2020. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In International Conference on Learning Representations.\\n\\nTing Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang, Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen Huang, Denvy Deng, and Qi Zhang. 2022. Prompt-BERT: Improving BERT sentence embeddings with prompts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8826\u20138837, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nOmar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201920, page 39\u201348, New York, NY, USA. Association for Computing Machinery.\\n\\nRensis Likert. 1932. A technique for the measurement of attitudes. Archives of psychology.\\n\\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European Conference on Computer Vision.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.\\n\\nI. Lopez-Gazpio, M. Maritxalar, A. Gonzalez-Agirre, G. Rigau, L. Uria, and E. Agirre. 2017. Interpretable semantic textual similarity: Finding and explaining differences between sentences. Knowledge-Based Systems, 119:186\u2013199.\\n\\nGeorge A Miller and Walter G Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes.\\n\\nOpenAI. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt.\\n\\nOpenAI. 2023a. Gpt-4. Accessed: 2023-05-23.\\n\\nOpenAI. 2023b. Gpt-4 technical report.\\n\\nZarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, and Yinfei Yang. 2021. Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO. arXiv:2004.15020 [cs]. ArXiv: 2004.15020.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sarathy, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-345", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nA.1 Distribution of annotated similarity in the dataset\\n\\nThe distribution of similarities is equitably spread out over the Likert scale, as depicted in Figure 4.\\n\\nA.2 Sentence Pair Generation Details\\n\\nHere we include some further details about sourcing sentence pairs from image-caption datasets. As discussed in Section 2, we use a variety of metrics to quantitatively characterize the sentence pairs, and then to filter with the goal of removing pairs with excessively high or low unconditional similarity. The general criteria we consider are defined as follows:\\n\\n- **IOU**: This is computed by taking the intersection over union of the bag of words for each sentence, after stopword removal. It represents the lexical similarity and overlap of a sentence pair.\\n- **d_{text}**: The cosine distance of the pair's SimCSE embeddings. We chose SimCSE due to its ubiquity and effectiveness.\\n- **ratio**: This is the ratio of the shorter sentence's word count to the longer sentence's word count in a given pair.\\n- **length**: This is the character length of the shortest sentence in a pair.\\n\\nUsing these criteria, we filter the sentence pairs based upon thresholds (exact values shown in Table 6) where sentences are rejected if they violate any of these criteria. These thresholds were selected based primarily manual inspection of samples on their margins. Criteria such as ratio and length are used primarily to facilitate comparison. Sentences with very different lengths are more difficult to compare, as are sentences that are very short or contain few details.\\n\\n| Dataset       | IOU       | d_{text}  | ratio  | length |\\n|---------------|-----------|-----------|--------|--------|\\n| COCO Flickr30K | \u2264 0.12    | \u2265 0.4     | \u2265 0.7  | \u2265 48   |\\n\\nTable 6: The list of filters criteria and values used for each dataset. Sentence pairs that violate any criterion are discarded.\\n\\nA.3 Evaluation Details\\n\\nImplementation Details\\n\\nAll models, with the exception of the ChatGPT systems, are trained and evaluated in PyTorch using the Huggingface Transformers library (Wolf et al., 2019) and pre-trained weights repository. We use the STS-B dataset as distributed on https://huggingface.co/docs/datasets as part of the GLUE (Wang et al., 2018) evaluation benchmark.\\n\\nFinetuned Baselines\\n\\nFor evaluation of the fine-tuned baselines on C-STS, we perform a hyperparameter sweep to select the best training settings for each model and encoding method before evaluating on the test split of C-STS. We show the hyperparameter values used in the sweep in Table 7, and the final hyperparameter values chosen in Table 8. We evaluate 3 random seeds using the best validation configuration to evaluate on the test data, with final results reported in Table 3.\\n\\nWe additionally perform an extensive evaluation of our models on STS-B. We perform a comparable validation sweep as shown in Table 7, reporting the best performing hyperparameters and their performance in Table 9.\\n\\nLastly, we perform a data ablation training a RoBERTa BASE model alternatively on only the condition and only the sentence pair. The model trained to predict similarity based on the condition statement alone recovers non-trivial performance, but falls well behind the full-input baseline.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Batch Size {32}\\nEncoding Type {Cross, Bi-, Tri-}\\nEpochs {3}\\nLearning Rate {1e-5, 3e-5}\\nLR Schedule {linear}\\nObjective {MSE, Quad, Quad + MSE}\\nPooler Type {[CLS] w/ MLP, w/o MLP}\\nSeed {42}\\nWarmup Ratio {0.1}\\nWeight Decay {0, 0.1}\\n\\nTable 7: Hyperparameter sweep done for C-STS validation for finetuned models. The same sweep, with the exception of the Encoding Type and Objective hyperparameters are done for STS-B.\\n\\nGenerative Baselines\\nWe report more details of results of the generative baselines for the validation sets of C-STS and STS-B. For comparison to validation performance of other models, we include the validation performance for C-STS in Table 11, which largely mirrors performance on the test set. We notice, expectedly, that models frequently output non-numerical responses in settings where there are no instructions to do so, or no in-context examples to follow.\\n\\nOn STS-B validation performance, models generally perform much better than on C-STS, with some models performing comparably to finetuned models. Since STS-B is included as a task in Natural Instructions v2 (Wang et al., 2022), it is likely to be recognizable to Flan-T5 models, which counts Natural Instructions v2 in its training data. Likewise, STS-B is comprised of long-existing and popular datasets, which plausibly exist in the corpora used to train ChatGPT models.\\n\\nProcessing Prompting Baseline Generations\\nFor parsing prompting model generations, we allow for a maximum of 20 generation tokens. The output is stripped of non-numeric characters and errant punctuation before being cast to a float. For example, the response \\\"The Answer is 2.0.\\\" is processed as 2.0 and counts as a valid prediction. If the cast fails, we mark the answer invalid and replace the predictions by a number $y \\\\sim U[1, 5]$.\\n\\nB Prompt Examples\\n\\nAll prompts for the prompting baselines may consist of instructions, examples, and a query, though we include evaluations for no instructions and no examples in our results. Figure 5 shows an prompt example for the short instructions and $K = 2$ and Figure 6 shows an example for long instructions and zero-shot setup.\\n\\nInstructions\\nOn a scale between 1 and 5, how similar are the following two sentences with respect to the condition provided? Respond only with a score between 1 and 5.\\n\\nExamples\\nInput: Sentence 1: A bunch of blue buses parked in a parking lot in front of a housing community.\\nSentence 2: Two buses, one blue and one red and white, are going to different destinations.\\nCondition: The type of transportation.\\nOutput: 4.0.\\n\\nInput: Sentence 1: A bunch of blue buses parked in a parking lot in front of a housing community.\\nSentence 2: Two buses, one blue and one red and white, are going to different destinations.\\nCondition: The number of buses.\\nOutput: 1.0.\\n\\nQuery\\nInput: Sentence 1: The skater is descending the wooden wall beside the slope.\\nSentence 2: A boy skateboards off a ramp covered in graffiti.\\nCondition: The location.\\nOutput: 3.0.\\n\\nFigure 5: We show the full input for 2-shot setting with short instructions.\\n\\nC Crowdsourcing Guidelines\\nC.1 Condition Annotation\\nWe provide the complete condition annotation guidelines used for Mechanical Turk data collection in Figure 7.\\n\\nC.2 Condition Verification\\nWe provide the complete verification guidelines used for Mechanical Turk data collection in Figure 8.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | Encoding     | Learning Rate | Weight Decay | Transform | Objective | Spearman | Pearson |\\n|-------------|--------------|---------------|--------------|-----------|-----------|----------|---------|\\n| **RoBERTa BASE** | Cross Encoder | 3.0e-05       | 0.10         | True      | MSE       | 28.70    | 27.50   |\\n|             | Bi Encoder   | 3.0e-05       | 0.10         | True      | Quad + MSE Hadamard | 21.82    | 21.46   |\\n| **RoBERTa LARGE** | Cross Encoder | 1.0e-05       | 0.10         | True      | MSE       | 28.70    | 27.50   |\\n|             | Bi Encoder   | 1.0e-05       | 0.10         | True      | Quad + MSE Hadamard | 21.82    | 21.46   |\\n| **DiffCSE BASE** | Cross Encoder | 3.0e-05       | 0.10         | False     | MSE       | 28.70    | 27.50   |\\n|             | Bi Encoder   | 3.0e-05       | 0.10         | False     | MSE       | 28.70    | 27.50   |\\n| **SimCSE BASE** | Cross Encoder | 3.0e-05       | 0.10         | True      | MSE       | 28.70    | 27.50   |\\n|             | Bi Encoder   | 3.0e-05       | 0.10         | False     | MSE       | 28.70    | 27.50   |\\n| **SimCSE LARGE** | Cross Encoder | 1.0e-05       | 0.10         | True      | MSE       | 28.70    | 27.50   |\\n|             | Bi Encoder   | 1.0e-05       | 0.10         | False     | MSE       | 28.70    | 27.50   |\\n\\nTable 8: Fine-tuning models' results over validation split. We show the best performing configuration selected over the validation split which was the final configuration used to report each models' test performance.\\n\\nTable 9: Validation performance of best sweep setting on STS-B.\\n\\n| Data Ablation | Spear. | Pears. |\\n|---------------|--------|--------|\\n| Condition Only | 28.21  | 28.62  |\\n| Sentence Only  | 9.98   | 9.51   |\\n| Baseline       | 40.11  | 40.21  |\\n\\nTable 10: When finetuned only with condition statement, RoBERTa BASE model can recover non-trivial performance, but falls well behind the baseline. Training on only the sentence pairs proves to be even less informative. We report the best validation performance over the same hyperparameter grid described in Section 4.1.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Instruction Model | 0-shot | 2-shot | 4-shot | Invalid Pears | Spear | Invalid Pears | Spear | Invalid Pears | Spear | Invalid Pears | Spear |\\n|-------------------|--------|--------|--------|---------------|-------|---------------|-------|---------------|-------|---------------|-------|\\n| Flan-T5 SMALL     | 91.74  | 3.23   | 2.20   | 35.64         | 7.06  | 8.20          | 24.21 | 7.14          | 6.98  |               |       |\\n| Flan-T5 BASE      | 97.18  | -4.25  | -3.65  | 6.42          | 5.51  | 9.86          | 2.40  | 11.39         | 12.11 |               |       |\\n| Flan-T5 LARGE     | 98.69  | -2.86  | -1.47  | 13.37         | 13.27 | 13.26         | 2.68  | 13.98         | 12.74 |               |       |\\n| Flan-T5 XL        | 86.27  | -0.81  | -0.69  | 8.29          | 11.21 | 12.81         | 0.53  | 18.15         | 17.05 |               |       |\\n| Flan-T5 XXL       | 74.14  | 3.21   | 3.78   | 0.14          | 11.37 | 12.05         | 0.00  | 10.28         | 12.08 |               |       |\\n| Flan-UL2          | 83.87  | 0.53   | 4.39   | 3.03          | 16.32 | 18.97         | 0.28  | 15.32         | 17.69 |               |       |\\n| Tk-Instruct 3B    | 87.33  | -2.06  | -2.05  | 0.67          | 2.12  | 1.70          | 0.00  | 0.26          | 0.32  |               |       |\\n| Tk-Instruct 11B   | 22.37  | 2.36   | 5.58   | 0.21          | 8.00  | 8.43          | 2.65  | 3.15          | 3.76  |               |       |\\n| ChatGPT-3.5       | 65.24  | 5.80   | 11.21  | 17.57         | 3.96  | 3.91          | 2.43  | 6.49          | 6.31  |               |       |\\n| GPT-4             | 59.17  | 9.01   | 16.69  | 4.98          | 16.10 | 15.56         | 0.60  | 26.74         | 26.59 |               |       |\\n\\nTable 11: Validation performance for prompting baselines on C-STS.\"}"}
{"id": "emnlp-2023-main-345", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Instruction Model | 0-shot | 2-shot | 4-shot | None | Flan-T5 SMALL | Flan-T5 BASE | Flan-T5 LARGE | Flan-T5 XL | Flan-T5 XXL | Flan-UL2 | Tk-Instruct 3B | Tk-Instruct 11B | ChatGPT-3.5 | GPT-4 |\\n|------------------|--------|--------|--------|------|-------------|-------------|--------------|----------|------------|----------|----------------|----------------|------------|-------|\\n| 0-shot           |        |        |        |      | 0.07        | 0.00        | 0.00         | 0.00     | 0.00       | 0.00     | 0.07           | 0.00           | 0.00       | 0.00 |\\n| 2-shot           |        |        |        |      | 18.44       | 82.32       | 89.81        | 90.33    | 90.75      | 91.02    | 23.90          | 64.09          | 86.28      | 89.57 |\\n| 4-shot           |        |        |        |      | 18.43       | 82.22       | 89.86        | 90.62    | 90.97      | 91.68    | 26.76          | 65.20          | 86.59      | 89.76 |\\n| None             |        |        |        |      | 19.09       | 82.42       | 89.85        | 90.43    | 91.51      | 91.90    | 66.89          | 67.63          | 86.16      | 90.01 |\\n| Flan-T5 SMALL    |        |        |        |      | 19.21       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 67.63          | 68.06          | 86.90      | 90.65 |\\n| Flan-T5 BASE     |        |        |        |      | 80.98       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 67.63          | 68.06          | 86.90      | 90.65 |\\n| Flan-T5 LARGE    |        |        |        |      | 87.89       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 67.63          | 68.06          | 86.90      | 90.65 |\\n| Flan-T5 XL       |        |        |        |      | 89.76       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 67.63          | 68.06          | 86.90      | 90.65 |\\n| Flan-T5 XXL      |        |        |        |      | 89.79       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 67.63          | 68.06          | 86.90      | 90.65 |\\n| Flan-UL2         |        |        |        |      | 19.97       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 67.63          | 68.06          | 86.90      | 90.65 |\\n| Tk-Instruct 3B   |        |        |        |      | 20.41       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 68.06          | 68.06          | 86.90      | 90.65 |\\n| Tk-Instruct 11B  |        |        |        |      | 81.31       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 68.06          | 68.06          | 86.90      | 90.65 |\\n| ChatGPT-3.5      |        |        |        |      | 74.96       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 68.06          | 68.06          | 86.90      | 90.65 |\\n| GPT-4            |        |        |        |      | 86.15       | 82.49       | 89.85        | 90.66    | 91.51      | 92.05    | 68.06          | 68.06          | 86.90      | 90.65 |\"}"}
