{"id": "lrec-2024-main-1294", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SI-NLI: A Slovene Natural Language Inference Dataset and its Evaluation\\n\\nMatej Klemen, Ale\u0161 \u017dagar, Jaka \u010cibej, Marko Robnik-\u0160ikonja\\nUniversity of Ljubljana, Faculty of Computer and Information Science\\nVe\u010dna pot 113, 1000 Ljubljana, Slovenia\\n{matej.klemen, ales.zagar, jaka.cibej, marko.robnik}@fri.uni-lj.si\\n\\nAbstract\\nNatural language inference (NLI) is an important language understanding benchmark. Two deficiencies of this benchmark are: i) most existing NLI datasets exist for English and a few other well-resourced languages, and ii) most NLI datasets are formed with a narrow set of annotators\u2019 instructions, allowing the prediction models to capture linguistic clues instead of measuring true reasoning capability. We address both issues and introduce SI-NLI, the first dataset for Slovene natural language inference. The dataset is constructed from scratch using knowledgeable annotators with carefully crafted guidelines aiming to avoid commonly encountered problems in existing NLI datasets. We also manually translate the SI-NLI to English to enable cross-lingual model training and evaluation. Using the newly created dataset and its translation, we train and evaluate a variety of large transformer language models in a monolingual and cross-lingual setting. The results indicate that larger models, in general, achieve better performance. The qualitative analysis shows that the SI-NLI dataset is diverse and that there remains plenty of room for improvement even for the largest models.\\n\\nKeywords: natural language inference, Slovene, cross-lingual, transformers\\n\\n1. Introduction\\nNatural language processing (NLP) is a rapidly developing area, with many new ever-more capable language models (LMs) being regularly introduced. While initial development focused primarily on processing a small pool of broadly spoken languages such as English and Chinese, later development started investing effort in a broader set of languages, as well as support for multilinguality and cross-linguality. This enables a more complete evaluation of the models as different languages contain different linguistic phenomena that may have a notable effect on the difficulty of tasks and consequent models\u2019 performance. As the datasets in less-resourced languages are commonly smaller, multilingual evaluation may also reveal performance discrepancies due to the data requirements of the models.\\n\\nThe language coverage varies significantly between NLP tasks: while certain tasks such as POS-tagging and dependency parsing have a wide coverage due to extensive international projects such as Universal Dependencies (Nivre et al., 2020), most tasks cover significantly fewer languages. In our work, we focus on extending the resources for natural language inference (NLI), an important semantic task with low coverage in languages other than English (see Section 2 for examples). NLI is an extension of the textual entailment recognition task (Dagan et al., 2006): given a premise and a hypothesis text, the goal is to determine whether the hypothesis is definitely true given the premise (entailment, E), definitely wrong given the premise (contradiction, C), or the relation is not decisive (neutral, N).\\n\\nTo extend the coverage of resources to less-resourced languages, authors use techniques such as translation of existing datasets for the same task into a new language (Obadi\u0107 et al., 2023), recasting existing datasets (Uppal et al., 2020) for a different task by transforming the target class via a known relation, and construction of a new resource from scratch (Hu et al., 2020). While the translation approach is convenient as it can be done (semi-)automatically, the constructed dataset may not fully represent the complexity of the target language, especially if machine translation is in question. For example, a translation of an English dataset into Slovene would not produce texts involving the dual grammatical number as the English language does not feature this phenomenon.\\n\\nData recasting does not suffer from this issue, but it may produce datasets that do not fully represent the complexity of the task. For example, Uppal et al. (2020) convert a sentiment classification dataset into a natural language inference dataset by converting each sentiment class into a hypothesis via a template, e.g., \u201cThis product got positive reviews\u201d. As a result, the hypothesis in the produced dataset can only be one of a handful of texts, while the hypothesis, in general, can be an arbitrary text.\\n\\nIn our work, we opt for the third option, i.e., we construct a new resource from scratch. We build it using the hypothesis editing protocol described by Bowman et al. (2020) that has shown promise in reducing the amount of unwanted statistical cues enabling models to learn shortcuts\"}"}
{"id": "lrec-2024-main-1294", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"More specifically, we gather candidate premises and hypotheses from publicly available Slovene reference corpus ccKres (Logar et al., 2013), and ask linguist students to edit or rewrite the hypothesis for each of the three NLI relations, following customized annotation guidelines designed to warn against pitfalls of existing NLI datasets.\\n\\nIn summary, we make the following contributions:\\n\\n1. We introduce SI-NLI (Klemen et al., 2022), the first dataset for Slovene natural language inference. The dataset is constructed from scratch and aims to avoid pitfalls commonly encountered in existing NLI datasets for other languages. In addition, we manually translate the dataset to English to enable cross-lingual model training and evaluation.\\n\\n2. Using the SI-NLI dataset, we train and evaluate a variety of language models in a monolingual, as well as cross-lingual (from Slovene to English) setting, setting baseline results and analyzing the model performance.\\n\\nThe remainder of this paper is structured as follows. In Section 2, we overview existing NLI resources. In Section 3, we describe the construction of the SI-NLI dataset and its essential statistics. In Section 4, we analyze the accuracy of language models using our resource. In Section 5, we summarize the findings and suggest possible directions for further work.\\n\\n2. Related Work\\n\\nDesigning systems that can capture the meaning of the text is at the core of artificial intelligence and natural language processing. Motivated by the importance of this aspect, Dagan et al. (2006) proposed textual entailment recognition as an abstract task to compare how well different systems capture the meaning, and released an English dataset. Given two texts, the task is to determine if the meaning of the second text can be inferred from the first text (entailment) or not (non-entailment). In the following years, shared tasks such as the PASCAL Recognising Textual Entailment Challenge (Dagan et al., 2006) and SemEval-2014 Task 1 (Marelli et al., 2014) have contributed to the rapid development of semantic systems and the popularity of the task. Motivated in part by the introduction of data-hungry neural models, Bowman et al. (2015) released the large-scale Stanford NLI dataset. To account for cases where the relation cannot be determined certainly, the authors used a three-class annotation scheme by dividing non-entailment into a contradiction and neutral class, and started the popularization of the modified task of natural language inference. As the dataset only covers one genre of texts, the Multi-Genre NLI (MNLI) dataset (Williams et al., 2018) was released as a generalized option for NLI, covering ten genres instead. Using the same procedure as in MNLI, Conneau et al. (2018) collected additional validation and test examples, and manually translated them into 15 languages, enabling the evaluation of multilingual and cross-lingual systems. NLI datasets were introduced for several other languages such as Turkish (Budure et al., 2020), Korean (Ham et al., 2020), Persian (Khashabi et al., 2021), Croatian (Obadi\u0107 et al., 2023), and Chinese (Hu et al., 2020). The datasets are typically constructed either by translating existing datasets such as XNLI (Obadi\u0107 et al., 2023), recasting data primarily used for other tasks such as sentiment classification (Uppal et al., 2020), or from scratch using a custom annotation procedure (Hu et al., 2020).\\n\\nDespite becoming popular evaluation benchmarks, many NLI datasets suffer from annotation artifacts, which enable performing NLI using shortcuts. For example, Gururangan et al. (2018) mention the issue of keywords that are very indicative of a class, such as negation words for contradiction, and the hypothesis-only bias, due to which models are able to correctly classify text pair relations using only one of the texts. Constructing NLI datasets from scratch, authors have tried to mitigate the number of artifacts, e.g., by using professional annotators, modified annotation guidelines, or an alternative dataset construction protocol (Parrish et al., 2021; Hu et al., 2020; Bowman et al., 2020). In our work, we use all three options to create a quality resource: (1) we use a hypothesis editing dataset construction protocol, which has previously shown promise in reducing some artifacts (Bowman et al., 2020); (2) we use skilled linguist students instead of crowdsourcing; (3) we design annotation guidelines that warn against common artifacts in existing datasets. Our main focus in this work is not the evaluation of our annotation procedure, but an introduction of a NLI dataset for a previously unsupported morphologically rich language (Slovene), as well as the demonstration of the usability of the dataset.\\n\\n3. The SI-NLI Dataset\\n\\nAn outline of the dataset construction process is shown in Figure 1: we construct it by sourcing pairs of sentences with similar meaning from a publicly available Slovene reference corpus ccKres (Logar et al., 2013) as the seed premises and hypotheses, and asking skilled annotators to edit the hypotheses three times, once for each of the three NLI relations, following customized annotation guidelines in the process. By sourcing semantically similar pairs from a reference corpus and asking multiple native...\"}"}
{"id": "lrec-2024-main-1294", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: An overview of the SI-NLI construction process. We start with an open data source, from which we filter out (1) irrelevant sentences with inadequate structures, embed the remaining sentences (2), and cluster them (3). We obtain groups of semantically similar sentences, from which we sample pairs (4). These are handled by human annotators through hypothesis editing and text pair annotation (5) to produce SI-NLI.\\n\\n3.1. Sourcing the pairs\\nTo allow the public distribution of our dataset, we use the 10-million token Slovene reference corpus ccKres 1.0 (Logar et al., 2013) whose license allows free non-commercial use of the data. As the corpus also contains sentences that are not suitable for our task, such as partial sentences (with no verbs) or sentences consisting of only several tokens (signatures, dates, etc.), we keep only sentences with between 10 and 40 tokens that contain a verb and at least one noun, determiner, proper noun, or pronoun.\\n\\nAt this stage, we could sample premises from the filtered data and ask the annotators to construct the hypotheses. However, this unconstrained setting did not work well in previous dataset construction attempts, as annotators resort to a few standard patterns of entailment and contradiction. Therefore, we decided to further guide and diversify the annotation procedure by providing annotators seed premises as well as hypotheses, and asking them to modify the hypotheses according to the desired class.\\n\\nTo construct hypotheses, we embedded the sentences using the Language agnostic BERT Sentence Embeddings (Feng et al., 2022), projected the embeddings to a lower dimensionality (100 dimensions) using PCA (Pearson, 1901), and clustered the embeddings using DBSCAN (Ester et al., 1996) clustering algorithm. Then, we sampled non-overlapping sentence pairs from the obtained clusters. For example, if a cluster contained five sentences, we constructed two pairs, while one remained unused. We obtained a large pool of sentence pairs, from which we drew data for annotation according to our budget.\\n\\n3.2. Annotation Process\\nThe annotation process was divided into several steps: (1) preliminary annotation, which we performed ourselves to produce and refine a set of reliable guidelines that the annotators could follow; (2) introductory and training sessions for annotators; (3) hypothesis editing/formation by annotators; and (4) cross-checking of hypotheses by annotators. We summarize them in the following subsections.\\n\\n3.2.1. Preliminary Annotation and Guidelines\\nThe review of related work and instructions for similar tasks has shown that the guidelines for NLI categorization are somewhat unsatisfactory, with a noticeable lack of examples beyond the basic ones to illustrate, e.g., the use of negation to form a contradictory hypothesis (\\\"John has a book. \u2192 John does not have a book.\\\").\\n\\nWe used the scikit-learn (Pedregosa et al., 2011) implementation of DBSCAN, using parameters $\\\\epsilon = 0.3$, $\\\\text{min}$_{samples} $= 2$. The parameters were determined based on qualitative analysis.\"}"}
{"id": "lrec-2024-main-1294", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We devised a set of detailed guidelines that contain a thorough list of strategies on how to form more complex hypotheses, with explanations. We provided general principles that advise against high overlap between premise and hypothesis, and described each of the three hypothesis classes with both adequate and inadequate examples in order to emphasize good and bad practices. For instance, the use of synonyms, acronyms, metaphorical expressions, active/passive voice conversion, and commonsensical reasoning are all listed as adequate strategies, while simple negation, overlapping and shortening of original premises, or minimal substitutions (e.g., nouns to pronouns) are discouraged.\\n\\nAn example from the guidelines is shown below where \\\\(P\\\\) is the premise, while \\\\(H-\\\\) and \\\\(H+\\\\) are examples of a bad and good hypothesis for entailment, respectively.\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{[P]} & \\\\quad \\\\text{Vse vilice in no\u017ee, ki so jih pobrali iz lijaka, so zlo\u017eili v predale v omari.} \\\\\\\\\\n\\\\text{[H-]} & \\\\quad \\\\text{V predale v omari so zlo\u017eili vse vilice in no\u017ee, ki so jih pobrali iz lijaka.} \\\\\\\\\\n\\\\text{[H+]} & \\\\quad \\\\text{Lijak je bil poln pribora, zato so ga pospravili v omaro.}\\n\\\\end{align*}\\n\\\\]\\n\\nIn \\\\(H-\\\\), the modification is insufficient as only the word order is changed in Slovene. The \\\\(H+\\\\) example, on the other hand, changes the word order and substitutes original expressions with a synonym (\\\\textit{zlo\u017eili} \u201cplaced\u201d - \\\\textit{pospravili} \u201cput away\u201d) and a hypernym (\\\\textit{pribor} \u201ccutlery\u201d).\\n\\nThe guidelines were designed based on a test annotation of approximately 50 premises and hypotheses; for each premise, at least three hypotheses were formed (one for each class) and then discussed to harmonize the decisions, particularly for borderline cases. The guidelines were later, during the annotation campaign, updated with additional examples and explanations. Although the guidelines contain some language-specific strategies that do not necessarily pertain to English (such as elaborations on whether a change in an inflected verb form affects the meaning of the hypothesis with respect to the premise), they can mostly be generalized to other languages. We translated them into English and made them publicly available.\\n\\n3.2.2. Introductory and Training Sessions\\n\\nFor the annotation campaign, a total of 8 annotators were recruited, all of whom were students of translation and linguistics at the Faculty of Arts of the University of Ljubljana. The annotators were selected based on several factors, such as their educational background (field of study and year; with higher years (e.g., MA students) prioritized), availability (at least 8 hours per week), and previous experience with linguistic annotation tasks. During the introductory session (a meeting with the researchers to annotate examples and jointly discuss the guidelines and basic concepts of entailment, contradiction, and neutrality), the annotators were trained for the task, and initial misconceptions were resolved. We first demonstrated the workflow by forming three hypotheses for several premises, and continued with a joint annotation session, during which each annotator handled hypotheses construction (entailment, contradiction, neutrality) for additional premises, and had an opportunity to discuss adequate and inadequate strategies. We emphasized that the guidelines should not be taken as strict instructions or a checklist of strategies that need to be implemented in every single hypothesis, but as suggestions to avoid forming completely inadequate examples. This demands a great deal of creativity with paraphrasing and accuracy in conveying the correct meaning, which is why students of translation were very suitable for this task.\\n\\nAfter the introductory session, each annotator received a separate online spreadsheet with a batch of approximately 30 premises for individual hypothesis formation as part of the training session. Once the first batch was done, we manually checked the formed hypotheses and provided feedback: first to individual annotators (who received examples of good and bad hypotheses along with our suggestions for better examples and the rationale behind them), as well as to the group as a whole - especially in the case of frequent errors and bad practices. For instance, neutral hypotheses proved to be the most difficult to form, and at the beginning, many annotators resorted to forming them by simply adding additional information not present in the premise, which was allowed in the guidelines, but not encouraged as a go-to strategy. A mailing list was created in order to allow annotators to post questions and dilemmas so we could resolve them with the whole group.\\n\\n3.2.3. Hypothesis Formation and Cross-checking\\n\\nBased on the results of the initial annotation, we estimated that 80 premises (which result in 240 hypotheses) take approximately 8 hours of work, and set this as the minimum weekly quota for annotators. From the beginning of the main phase of the annotation campaign, the annotators received an additional 80 premises in their online spreadsheet every week until approximately 6,000 hypotheses...\"}"}
{"id": "lrec-2024-main-1294", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"were formed, which was the maximum number according to our plan based on the annotation budget.\\n\\nIn the second part of the main phase of the campaign, the annotators were tasked with classifying their colleagues' hypotheses in a double-blind setup. They were provided with random premises and one of the hypotheses formed by other annotators and asked to categorize them as either entailment, contradiction, or neutrality. This allowed for some degree of quality control and calculation of inter-annotator agreement.\\n\\n### 3.2.4. Inter-Annotator Agreement\\n\\nTable 1 shows the formed hypotheses by the annotators' agreement: the first categorization indicates the relation the original annotator needed to form, while the second is the one assigned by the cross-checker. In the majority of cases (79.26%), the cross-checker confirmed the hypothesis class, while 18.59% of examples show lesser disagreement, i.e., a tie between neutrality (N) and contradiction (C) or, much more frequently, between neutrality and entailment (E). Only 2% of cases show major disagreement (a tie between contradiction and entailment).\\n\\n| Categorization | Number | %      |\\n|---------------|--------|--------|\\n| C-C           | 1,699  | 29.17  |\\n| E-E           | 1,705  | 29.28  |\\n| N-N           | 1,212  | 20.81  |\\n| C-N           | 159    | 2.73   |\\n| N-C           | 82     | 1.41   |\\n| E-N           | 200    | 3.43   |\\n| N-E           | 642    | 11.02  |\\n| C-E           | 76     | 1.30   |\\n| E-C           | 41     | 0.70   |\\n| Errors        | 8      | 0.15   |\\n| Total         | 5,824  | 100.00 |\\n\\nTo check the inter-annotator agreement, we calculate Cohen's kappa (Table 2) for all annotator pairings that had more than one annotation in common (minimum 6, maximum 644). The average coefficient value is approximately 0.74, which indicates a reasonably high agreement. The lowest agreement values (between 0.21 and 0.60) can all be attributed to a single annotator who left the campaign after finishing the training session. The results show that in general, the annotators were consistent both in forming hypotheses as well as classifying them.\\n\\nTable 2: Inter-annotator agreement statistics.\\n\\n| \u03ba Value | Average | Median | Minimum | Standard Deviation |\\n|---------|---------|--------|---------|--------------------|\\n|         | 0.739   | 0.739  | 0.213   | 0.198              |\\n\\nThe first round of agreement analysis was followed by an additional round of cross-checking. 583 examples of minor disagreement (N-E and E-N) were assigned a third annotation from the more reliable annotators. As before, the annotators only checked examples formed and classified by others. The results are shown in Table 3. Almost 70% of ambiguous examples were classified as neutral, while 30% leaned more towards entailment. An additional five examples were annotated as inadequate.\\n\\nTable 3: Additional cross-checking statistics of annotators' agreement. E stands for Entailment, C for Contradiction, and N for Neutrality.\\n\\n| Categorization | Number | %      |\\n|---------------|--------|--------|\\n| E-N-E         | 74     | 12.69  |\\n| E-N-N         | 56     | 9.61   |\\n| N-E-E         | 103    | 17.67  |\\n| N-E-N         | 345    | 59.18  |\\n| Errors        | 5      | 0.85   |\\n| Total         | 583    | 100.00 |\\n\\nFinally, 117 examples with major disagreement (C-E and E-C) were resolved by ourselves by either assigning the final annotation if the hypothesis was adequate, or by adapting the hypothesis to make it in line with the first annotation (according to which it was formed).\\n\\nIn the dataset, the hypotheses that exhibited complete agreement (e.g., C-C) and those disambiguated in the last phase (e.g., N-E-N) were assigned a final annotation based on the majority vote. However, all annotations and the IDs of their annotators are listed separately to allow for effective filtering and provide more transparency.\\n\\n### 3.3. The SI-NLI 1.0 Dataset\\n\\nThe constructed dataset consists of 5937 examples, split into 4392 training, 547 validation, and 998 test examples. To construct the split, we selected all examples where the first and second annotation disagreed, as well as all examples with the same premise as those examples, and placed them into\"}"}
{"id": "lrec-2024-main-1294", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We grouped the remaining examples based on their premise, and split the groups between the validation and test set so that the test set contains approximately 1000 examples and the validation set contains approximately 500 examples. The remaining examples were put into the training set. We publish the dataset for non-commercial use (Klemen et al., 2022). The public test set does not contain class annotations in order to reduce potential issues with overfitting the test set, and to encourage submissions on the SloBench evaluation portal for Slovene natural language processing tasks.\\n\\nIn total, the construction of the dataset (including cross-checking) required approximately 200 hours of work and cost around \u20ac2,000. Half of the allocated time and budget were used for cross-checking.\\n\\nTo enable cross-lingual analysis, we translated the dataset into English by first automatically translating the examples into English with the DeepL machine translation tool and then manually correcting them. For this paper, 514 pairs were manually checked to get the quality evaluation statistics. 127 (70%) premises and 374 (73%) hypotheses required no further editing. Only 4 premises and 11 hypotheses contained major semantic errors that completely changed their meaning. Other errors included erroneously translated named entities, minor omissions (e.g., omissions of adverbs), and minor grammatical errors (most frequently mistranslations of pronouns, e.g., \u201cshe\u201d instead of \u201cit\u201d). Overall, the manual analysis showed that the machine translations did not require much editing. We release the translated data publicly (Klemen et al., 2024).\\n\\n### 4. Model Evaluation\\n\\nIn this section, we present the results of our NLI experiments. We start by describing the tested language models (Section 4.1) and the experimental settings (Section 4.2). Then, we present the results of two experiments. First, we present the experiments in Slovene using monolingual and multilingual models (Section 4.3); then, we perform cross-lingual experiments using a subset of the Slovene test set translated into English (Section 4.4). For reproducibility of our work, we publish the source code online.\\n\\n#### 4.1. Tested Language Models\\n\\nUsing the created SI-NLI dataset, we trained several classifiers using monolingual, few-lingual, and massively multilingual pretrained transformer language models of three types: encoder-based, decoder-based, and encoder-decoder models. The three types differ based on using only encoder, only decoder, or encoder and decoder transformer layers (Vaswani et al., 2017). The initial motivation for different types of models was their usability for different tasks. For example, decoder-based models are mostly used for (autoregressive) text generation tasks, encoder-based models for text representation and classification tasks, and encoder-decoder models for sequence-to-sequence transformation tasks. However, the distinction in their common use is blurred as tasks can be converted into a common text-to-text format (Raffel et al., 2020) or approached via instruction-augmented text generation (Ouyang et al., 2022), a conversion we also utilize in our experiments.\\n\\nWe next describe the used pretrained models and summarize them in Table 4.\\n\\n**Encoder-based models.** We use the monolingual Slovenian SloBERTa (Ul\u010dar and Robnik-\u0160ikonja, 2021), the trilingual Croatian-Slovene-English CROBERTa (CSE-BERT) (Ul\u010dar and Robnik-\u0160ikonja, 2020), the case-different multilingual BERT model (mBERTc) (Devlin et al., 2019), and the base and large multilingual XLM-RoBERTa (XLM-R) (Conneau et al., 2020). We use the models in a discriminative setting, i.e., we extend the pretrained models with a linear layer, and fine-tune a three-class classifier.\\n\\n**Decoder-based models.** We use the Slovenian gpt-sl-base model, and the multilingual GPT-3.5-turbo instruction-tuned model (OpenAI, 2023). We use gpt-sl-base in a discriminative setting, while we use GPT-3.5-turbo in a generative setting, i.e., we fine-tuning the model to produce the NLI class in its text form (e.g., \u201centailment\u201d).\\n\\n**Encoder-decoder models.** We use the small and large Slovenian T5 (Ul\u010dar and Robnik-\u0160ikonja, 2023), and the small, base, and large multilingual T5 models (Xue et al., 2021). We use the models in a generative setting.\\n\\n#### 4.2. Experimental Settings\\n\\nWe performed the monolingual Slovene NLI experiments using the dataset splits described in Section 3.3, and the cross-lingual NLI experiments using the Slovene training and validation set together with the manually translated English test set. To train the models, we used reasonable hyperparameter values instead of performing thorough hyperparameter tuning as we are interested in general baselines rather than optimal model performance. We trained the autoregressive and masked language model classifiers for up to 10 epochs, and...\"}"}
{"id": "lrec-2024-main-1294", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Summary of the used language models.\\n\\nThe size of the GPT3.5-turbo model is marked with `?` as its size is undisclosed and estimated based on its predecessor.\\n\\n| Model Languages | # parameters |\\n|-----------------|--------------|\\n| gpt-sl-base Slovene | 110M |\\n| SloBERTa Slovene | 110M |\\n| CSE-BERT triling. | 110M |\\n| mBERTc-base multiling. | 110M |\\n| XLM-R-base multiling. | 125M, large multiling. 355M |\\n| T5-sl-small Slovene | 60M |\\n| -large Slovene | 750M |\\n| mT5-small multiling. | 300M |\\n| -base multiling. | 580M |\\n| -large multiling. | 1.2B |\\n| GPT3.5-turbo multiling. | 175B |\\n\\nThe sequence-to-sequence model classifiers for up to 300 epochs, selecting the best model based on the validation set accuracy. We vary the training time as certain Slovene sequence-to-sequence models converged very slowly and required significantly more training, while the autoregressive and masked language models converged significantly faster than in 10 epochs. For GPT3.5-turbo, we used 3 epochs, a setting which was automatically suggested by the OpenAI training platform. We used the AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate $2 \\\\cdot 10^{-5}$. To constrain memory usage, we used a maximum input sequence length equal to the 99th percentile of all training sequence lengths (between 100 and 150 tokens), and truncated sequences beyond this length.\\n\\nWe report results using the mean and standard deviation of the test set accuracy across five runs of the training procedure, except for GPT3.5-turbo, for which we report a single-run accuracy. We assume that the standard deviation for GPT3.5-turbo would be comparable to other models.\\n\\n4.3. Slovene NLI Experiments\\n\\nTable 5 shows the accuracy of the tested models on the Slovene NLI task. In an unconstrained comparison, the best accuracy is achieved by GPT3.5-turbo (0.857), followed by XLM-R-large (0.791). This shows the impressive ability of GPT3.5-turbo to adapt to the Slovene language despite Slovene texts likely not being largely present during its pretraining. In addition, the results suggest that improved accuracy can be obtained using larger models, although the improvement is diminishing: in comparison with the accuracy of the best base-sized model SloBERTa (0.744), the 3-times larger XLM-R-large achieves an absolute improvement of +0.047, while the >1500-times larger GPT3.5-turbo achieves an absolute improvement of +0.113. Surprisingly, the large Slovenian T5 model t5-sl-large achieves lower accuracy (0.590) than its smaller equivalent t5-sl-small (0.653). This confirms observations of Ul\u010dar and Robnik-\u0160ikonja (2023) of the inadequacy of t5-sl-large, likely due to insufficient training data involved in its pretraining.\\n\\nTable 5: Classification accuracy of different models for Slovene NLI experiments.\\n\\n| Model     | Accuracy | Std Dev |\\n|-----------|----------|---------|\\n| majority  |          |         |\\n| gpt-sl-base | 0.479 (0.019) |         |\\n| SloBERTa  | 0.744 (0.008) |         |\\n| CSEBERT   | 0.667 (0.005) |         |\\n| mBERTc-base | 0.595 (0.011) |         |\\n| XLM-R-base | 0.670 (0.011) |         |\\n| XLM-R-large | 0.791 (0.014) |         |\\n| T5-sl-small | 0.653 (0.004) |         |\\n| T5-sl-large | 0.590 (0.007) |         |\\n| mT5-small  | 0.540 (0.012) |         |\\n| mT5-base   | 0.621 (0.007) |         |\\n| mT5-large  | 0.767 (0.005) |         |\\n| GPT3.5-turbo | 0.857 |         |\\n\\nComparing models of the base size (i.e., 110M parameters) in isolation, we see that the models trained for a smaller set of languages regularly outperform the broadly-focused multilingual models, with the monolingual SloBERTa model achieving accuracy 0.744, followed by the trilingual CSE-BERT, and massively multilingual mBERTc-base. A notable exception is the monolingual gpt-sl-base model with a low accuracy 0.479. We hypothesize this is due to suboptimal pre-training of gpt-sl-base and not the architectural differences. To validate this, we ran the same experiment with an English GPT-2 model (Radford et al., 2019) of similar size (124M) on an automatically translated English version of the SI-NLI dataset, and observed the model achieving a significantly higher accuracy (0.588) with a similar standard deviation (0.020).\\n\\nDespite being primarily aimed at generative tasks, the T5 models are able to perform the NLI classification task relatively well, although they commonly lag behind the encoder-based models in terms of accuracy. The best performing T5 model mT5-large achieves accuracy 0.767 and comes close to the accuracy of the best encoder-based model XLM-R-large (0.791), although the significantly larger size of mT5-large likely plays an important role in this. A small drawback of the T5 models we have observed in our experiments and previously mentioned in Section 4.2 is their occasional slow convergence. We attribute this to the models' need to learn how to generate the class in...\"}"}
{"id": "lrec-2024-main-1294", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Table 6, we present four manually selected erroneously classified examples by GPT3.5-turbo on the Slovene NLI task.\\n\\n**Example 1:**\\n\\n**P:** V njej je prebujal ob\u010dutke, ki jih ni poznala. (He made her feel feelings she did not know before.)\\n\\n**H:** Obnjemjeza\u010dutiladoslejneznanaob\u010dutja. (By his side she felt feelings she did not feel thus far.)\\n\\nPredicted: neutral, Correct: entailment\\n\\n**Example 2:**\\n\\n**P:** \u00bbJa?\u00ab sem rekel, ko sem zasli\u0161al trkanje po vratih. (\\\"Yes?\\\" I uttered once I heard the knocking on the door.)\\n\\n**H:** Zadru\u017eniki sem se oglasil na trkanje po vratih. (Due to my curiosity I answered the door knock.)\\n\\nPredicted: neutral, Correct: entailment\\n\\n**Example 3:**\\n\\n**P:** S prijateljem \u0161e ne bova vrgla pu\u0161ke v koruzo. (Me and my friend will not give up yet.)\\n\\n**H:** \\\"Sprijateljemsvasenaveli\u010dalaneuspehov,zato bova odnehala.\\\" (Me and my friend got tired of failure, so we will quit.)\\n\\nPredicted: neutral, Correct: contradiction\\n\\n**Example 4:**\\n\\n**P:** Vabljeni ste na konferenco, ki se bo zgodila v \u010detrtek, 11. 11. 2010 v Ljubljani. (You are invited to the conference happening on Thursday 11. 11. 2010 in Ljubljana.)\\n\\n**H:** Vabimo vas na slavni dogodek, ki bova potekal 11. novembra 2010 v prestolnici. (We invite you to the gala event happening on November 11 in the capital.)\\n\\nPredicted: neutral, Correct: entailment\\n\\nTo observe if the misclassifications are primarily actual errors or a consequence of different annotation perspectives (Plank et al., 2014), we qualitatively observe the errors of the best-performing model GPT3.5-turbo. While it achieves high accuracy and certain misclassified examples could also be labeled differently depending on the assumed background context, the model nonetheless makes multiple unambiguous errors due to not possessing certain capabilities. This is likely the consequence of being pre-trained on a negligible amount of Slovene texts. We show a small manual selection of errors in Table 6.\\n\\nIn Example 1, the model seems to misclassify the example as the pronoun \\\"he\\\" is implicitly present in the verb; this is a special property of the Slovene language.\\n\\nIn Example 2, the model seems to misclassify the example due to the lack of common sense that the utterance \\\"Ja?\\\" (\\\"Yes?\\\") implies a person's curiosity.\\n\\nIn Example 3, the model seems to misclassify the example because it does not understand the idiom \\\"vre\u010di pu\u0161ko v koruzo\\\" (\\\"to give up\\\").\\n\\nIn Example 4, the model seems to misclassify the example as it does not possess background knowledge that Ljubljana is also referred to as \\\"prestolnica\\\" (\\\"the capital\\\").\\n\\nIn general, our qualitative error analysis suggests there is room for improvement despite the already high accuracy achieved by the best model.\\n\\n### 4.4. Cross-lingual NLI Experiments\\n\\nTable 7 shows the accuracy of models on the cross-lingual NLI task, where the training data is Slovene, and the test data is a test set translated into English. We only evaluate models capable of handling English and Slovene, i.e., only a subset of the models evaluated in Section 4.3.\\n\\n**Table 7: Results of cross-lingual experiments (SL \u2192 EN) on a manually translated subset of the test set.**\\n\\n| Model       | Accuracy |\\n|-------------|----------|\\n| majority    | 0.344    |\\n| CSE-BERT    | 0.623    |\\n| (0.011)     |          |\\n| mBERTc-base | 0.489    |\\n| (0.031)     |          |\\n| XLM-R-base  | 0.620    |\\n| (0.038)     |          |\\n| XLM-R-large | 0.765    |\\n| (0.017)     |          |\\n| mT5-small   | 0.428    |\\n| (0.011)     |          |\\n| mT5-base    | 0.579    |\\n| (0.029)     |          |\\n| mT5-large   | 0.746    |\\n| (0.017)     |          |\\n| GPT3.5-turbo| 0.837    |\\n\\nWe observe similar trends in the model performance as on the Slovene test set: the best accuracy is achieved by GPT3.5-turbo (0.837) and XLM-R-large (0.765), indicating the influence of the model size. Similarly as before, more narrowly focused models beat massively multilingual ones when comparing similar sizes of models. Concretely, the trilingual CSE-BERT with accuracy 0.623 performs better than mBERTc-base (0.489) and equivalently to the XLM-R-base model (0.620) containing 15 million more parameters.\\n\\nTo assess the influence of manual translation on the model performance, we test a well-performing model (XLM-R-large) on the automatically translated test set, and compare its accuracy to the accuracy achieved on the manually translated test set. On the automatically translated set, the model achieves the classification accuracy of 0.768 with the standard deviation 0.014, indicating no significant difference. This strengthens the observation...\"}"}
{"id": "lrec-2024-main-1294", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of the high quality of automatic translations previously noted in Section 3.3. Although the manual corrections improved the text legibility, they do not seem to have an influence on the model's prediction accuracy.\\n\\n5. Conclusion\\n\\nWe presented the Slovene NLI dataset SI-NLI and released it publicly for non-commercial use. We described its creation process and set the initial prediction baselines using a diverse LM selection. The quantitative results show that models (particularly the larger ones) can perform the task relatively well, while the qualitative analysis reveals that several deficiencies are still present in the models. Improving models by reducing these deficiencies is a promising direction for further work. Additionally, we translated the dataset into English and performed cross-lingual NLI experiments. These models perform slightly worse. The patterns where models struggle also present a sensible direction for further improving the created resource by introducing more challenging examples using the same underlying linguistic phenomena (e.g., metaphors). This will ensure the relevance of the resource for evaluating the increasingly powerful LLMs.\\n\\nIn addition to improving the models, we see another promising direction in approaching NLI through the lens of data perspectivism (Plank et al., 2014). In our work, we have assumed the existence of a single ground truth label, while the true label might be fuzzy for legitimate reasons instead of annotation errors.\"}"}
{"id": "lrec-2024-main-1294", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Limitations\\n\\nOur work introduces SI-NLI, a NLI dataset for Slovene. Previous work has shown that NLI datasets can contain biases that enable models to learn shortcuts. While we designed the annotation guidelines to mitigate these issues, and performed annotator training and quality checks, we did not extensively evaluate the potential biases in the created dataset. Our goal was the introduction of a new resource for NLI in a previously unsupported language and the initial testing of models, while we see a more thorough bias analysis and use in downstream applications as logical next steps in future work.\\n\\n7. Ethical statement\\n\\nWe aim to maintain the privacy of participants in the annotation process. However, further research on the annotation process could benefit from this metadata. Therefore, we release pseudonymized identifiers of the annotators while their true identity is not shared. We see no further ethical issues in the conducted research.\\n\\nAcknowledgements\\n\\nThe dataset construction campaign was funded by CLARIN.SI. The work was partially supported by the Slovenian Research Agency (ARRS) core research programme P6-0411, young researcher grant, as well as projects J7-3159, CRP V5-2297, and L2-50070.\\n\\n8. Bibliographical References\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642.\\n\\nSamuel R. Bowman, Jennimaria Palomaki, Livio Baldini Soares, and Emily Pitler. 2020. New protocols and negative results for textual entailment data collection. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8203\u20138214.\\n\\nEmrah Budur, R\u0131za \u00d6z\u00e7elik, Tunga Gungor, and Christopher Potts. 2020. Data and representation for Turkish natural language inference. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8253\u20138267.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451.\\n\\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485.\\n\\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment, pages 177\u2013190.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186.\\n\\nMartin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, and Xiaowei Xu. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, KDD\u201996, page 226\u2013231.\\n\\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic BERT sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878\u2013891.\\n\\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. 2020. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673.\"}"}
{"id": "lrec-2024-main-1294", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jiyeon Ham, Yo Joong Choe, Kyubyong Park, Ilji Choi, and Hyungjoon Soh. 2020. KorNLI and KorSTS: New benchmark datasets for Korean natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 422\u2013430.\\n\\nHai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra K\u00fcbler, and Lawrence Moss. 2020. OCNLI: Original Chinese Natural Language Inference. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3512\u20133526, Online.\\n\\nDaniel Khashabi, Arman Cohan, Siamak Shak\u0435\u0440\u0438, Pedram Hosseini, Pouya Pezeshkpour, Malhihe Alikhani, Moin Aminnaseri, Marzieh Bitaab, Faeze Brahman, Sarik Ghazarian, Mozhdeh Gheini, Arman Kabiri, Rabeeh Karimi Mambahagdi, Omid Memarrast, Ahmadreza Mosalanezhad, Erfan Noury, Shahab Raji, Mohammad Sadegh Rasooli, Sepideh Sadeghi, Erfan Sadeqi Azer, Niloofar Safi Samghabadi, Mahsa Shafaei, Saber Sheybani, Ali Tazarv, and Yadollah Yaghoobzadeh. 2021. ParsiNLU: A Suite of Language Understanding Challenges for Persian. Transactions of the Association for Computational Linguistics, 9:1147\u20131162.\\n\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.\\n\\nMarco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 1\u20138.\\n\\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Haji\u010d, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. 2020. Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4034\u20134043.\\n\\nLeo Obadi\u0107, Andrej Jertec, Marko Rajnovi\u0107, and Branimir Dropulji\u0107. 2023. C-XNLI: Croatian extension of XNLI dataset. In Findings of the Association for Computational Linguistics: ACL 2023, pages 2258\u20132267.\\n\\nOpenAI. 2023. GPT-4 technical report. ArXiv preprint 2303.08774.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744.\\n\\nAlicia Parrish, William Huang, Omar Agha, Soohwan Lee, Nikita Nangia, Alexia Warstadt, Karmanya Aggarwal, Emily Allaway, Tal Linzen, and Samuel R. Bowman. 2021. Does putting a linguist in the loop improve NLU data collection? In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4886\u20134901.\\n\\nKarl Pearson. 1901. LIII. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559\u2013572.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830.\\n\\nBarbara Plank, Dirk Hovy, and Anders S\u00f8gaard. 2014. Linguistically debatable or just plain wrong? In Proceedingsofthe52ndAnnualMeeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 507\u2013511.\\n\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8).\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1).\\n\\nMatej Ul\u010dar and Marko Robnik-\u0160ikonja. 2020. FinEst BERT and CroSloEngual BERT: Less is more in multilingual models. In Text, Speech, and Dialogue: 23rd International Conference, TSD 2020, page 104\u2013111.\\n\\nMatej Ul\u010dar and Marko Robnik \u0160ikonja. 2021. SloBERTa: Slovene monolingual large pre-trained masked language model. In Proceedings of SI-KDD within the 24th International Multiconference Information Society 2021, page 17\u201320.\"}"}
{"id": "lrec-2024-main-1294", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Matej Ul\u010dar and Marko Robnik-\u0160ikonja. 2023. Sequence-to-sequence pretraining for a less-resourced Slovenian language. *Frontiers in Artificial Intelligence*, 6.\\n\\nShagun Uppal, Vivek Gupta, Avinash Swaminathan, Haimin Zhang, Debanjan Mahata, Rakesh Gosangi, Rajiv Ratn Shah, and Amanda Stent. 2020. Two-step classification using recast data for low-resource settings. In *Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing*, pages 706\u2013719.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In *Proceedings of the 31st International Conference on Neural Information Processing Systems*, page 6000\u20136010.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1112\u20131122.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 483\u2013498.\\n\\n9. Language Resource References\\n\\nKlemen, Matej and \u017dagar, Ale\u0161 and \u010cibej, Jaka and Robnik-\u0160ikonja, Marko. 2022. Slovene Natural Language Inference Dataset SI-NLI. Slovenian language resource repository CLARIN.SI.\\n\\nKlemen, Matej and \u017dagar, Ale\u0161 and \u010cibej, Jaka and Robnik-\u0160ikonja, Marko. 2024. English translation of the Slovene Natural Language Inference Dataset SI-NLI-en 1.0. Slovenian language resource repository CLARIN.SI.\\n\\nLogar, Nata\u0161a and Erjavec, Toma\u017e and Krek, Simon and Gr\u010dar, Miha and Holozan, Peter. 2013. Written corpus ccKres 1.0. Slovenian language resource repository CLARIN.SI.\"}"}
