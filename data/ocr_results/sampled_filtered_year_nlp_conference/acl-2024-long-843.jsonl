{"id": "acl-2024-long-843", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Introduction\\n\\nBuilding Large Language Models (LLMs) is an inherently data-intensive process requiring a comprehensive set of resources for pre-training (Raffel et al., 2020; Xue et al., 2021; Gao et al., 2021; Penedo et al., 2023; Nguyen et al., 2023a; Abadji et al., 2022) and fine-tuning (Longpre et al., 2023; Conover et al., 2023; K\u00f6pf et al., 2023; Ding et al., 2023a). The last year has seen remarkable progress in building English LLMs, thanks to open-source models (Touvron et al., 2023a,b; Jiang et al., 2023, 2024a; Almazrouei et al., 2023) developed using comprehensive datasets containing such resources. Nonetheless, this progress has largely bypassed low and mid-resource languages due to the lack of data resulting from the lack of open source pipelines for curating data for such languages from diverse sources such as websites (which require crawling and extraction), books (which require OCR) and videos (which require transcription). Further, for instruction fine-tuning, English LLMs now rely on model-generated data such as ShareGPT1, Self-Instruct (Wang et al., 2023a), Evol-Instruct (Xu et al., 2023a), Ultra-Chat (Ding et al., 2023a), etc. However, for low and mid resource languages this option is not available due to lack of high quality LLMs, leading to a chicken and egg problem, further widening the gap between the have and the have-nots.\\n\\nA case in point is that of languages from the Indian sub-continent which collectively are spoken by over 1.4 billion people. We focus on the 22 languages recognised in the 8th schedule of the Indian constitution. These languages, despite their significant number of speakers, receive minimal representation in the training datasets and tokenizers of current open-source LLMs (Touvron et al., 2023b; Jiang et al., 2024a; Almazrouei et al., 2023) leading to a notable exclusion of their rich cultural contexts and nuances. In this work, we address this disparity by making the following contributions:\\n\\n1. SANGRAHA: Pretraining data containing 251B tokens summed up over 22 languages extracted\\n\\n1https://sharegpt.com/\\n2We built a custom tokenizer which supports English and...\"}"}
{"id": "acl-2024-long-843", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overview of the different components present in IndicAlign.\\n\\n2. SETU: Spark-based (Zaharia et al., 2016) distributed pipeline customised for Indian languages for extracting content from websites, PDFs and videos, with in-built stages for cleaning, filtering, toxicity removal and deduplication.\\n\\n3. IndicAlign-INSTRUCT: A diverse collection of 74.7 million prompt-response pairs across 22 languages collected through four methods: aggregating existing Instruction Fine-Tuning (IFT) datasets, translating English datasets into 14 Indian languages using an open-source translation model, creating context-grounded conversations from India-centric Wikipedia articles using open-source LLMs, and establishing a crowdsourcing platform called Anudesh for prompt collection.\\n\\n4. IndicAlign-TOXIC: 123K pairs of toxic prompt and non-toxic responses generated using open source English LLMs and translated to 14 Indian languages for safety alignment of Indic LLMs.\\n\\nWe collectively refer to the above as IndicLLMSuite. We try to balance quality and quantity while acknowledging recent trends of using synthetic data for building powerful LLMs for English (Gunasekar et al., 2023; Li et al., 2023c) as well as low resource languages (Nguyen et al., 2023b; Li et al., 2023b). To ensure quality, we take help from humans to verify websites to flag noisy or machine translated content and to create toxicity lists for Indian languages. On the other hand, to ensure explicit representation of prompt-response pairs grounded in Indian context we take the help of Indian languages and has an average fertility of 1.3 to 2.79 across the 22 languages. We use this tokenizer for all the reported statistics unless mentioned otherwise.\\n\\nWe recognize the need to represent diverse knowledge and alignment information in Indic languages for better performance of LLMs in Indic languages. Hence, we undertake large-scale machine translation of rich English resources like Wikipedia as well as English finetuning datasets into Indian languages using SOTA open-source MT models. We thus balance source original data with translated and LLM-generated data to create the above collection.\\n\\nWe believe that these choices can be replicated across other languages to create LLMSuites. All the code, tools and datasets developed as a part of this work will be publicly released and hopefully advance the development of LLMs for Indian languages. Given that LLM training is an expensive exercise, we plan to undertake community-effort to train LLMs, where multiple groups can pool together computing resources to build a high-quality Indic language LLM.\\n\\n2 Related Works\\n\\nWe organise the Related Work into 3 sections in line with our main contributions.\\n\\nMultilingual Datasets.\\n\\nPrevious works like OSCAR (Abadji et al., 2022), CC100 (Conneau et al., 2020), and mC4 (Raffel et al., 2020) are curated from CommonCrawl dumps through extensive cleaning stages. MADLAD-400 (Kudugunta et al., 2023) extends to 419 languages, incorporating human audits and iterative refinement, along with language family-specific filters. ROOTS (Lauren\u00e7on et al., 2023) and CulturaX (Nguyen et al., 2023a) combined existing datasets and used strict cleaning stages.\"}"}
{"id": "acl-2024-long-843", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce the Sangraha Verified dataset, comprising Web Data, PDF Data, and Speech Transcripts Data. This dataset emphasizes human-verified quality across various stages of its curation. Web Data. Our web data, constituting the majority of Sangraha, diverges from traditional Common Crawl-based approaches by prioritizing data quality. This involves manual verification of each website before scraping. Following Kakwani et al. (2020); Doddapaneni et al. (2023), we identify web sources, primarily news articles, through existing repositories and automated searches. Additionally,\"}"}
{"id": "acl-2024-long-843", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we engage volunteers to select websites in Indian languages and English based on content quality and cultural relevance. To enhance diversity, we manually verify and add a small subset of base URLs extracted from the mC4 corpus. The selection and scraping processes use the webcorpus toolkit. We detail the process further in Appendix A.1.\\n\\nAcknowledging the wealth of Indian language content in undigitized books and documents, we focus on text extraction from digitized PDFs. We download Indian language PDFs from the Internet Archive, selecting high-quality documents through a detailed process described in Appendix A.2. Additionally, we collect documents from different government sources including Parliamentary debates, magazines, textbooks, etc. We list down the sources and their details in Appendix A.2. For OCR, we employ GCP's Vision tool, recognized for good performance across categories (Dilmegani, 2023). Our future work will continue to explore digitization and OCR of new public sources.\\n\\nSpeech Transcripts.\\n\\nWe source movie subtitles from OpenSubtitles, song lyrics, Mann ki Baat transcripts following Siripragada et al. (2020), and NPTEL transcripts, as extended by Bhogale et al. (2023b). These transcripts feature a substantial amount of technical text in Indian languages. Additionally, we transcribe around 80K hours of Hindi videos from YouTube using the Riva Conformer ASR Model. We plan to extend the transcription efforts to all 22 scheduled Indian languages, with pipeline details described in Appendix A.3.\\n\\n3.1.2 Sangraha Synthetic\\n\\nThere is a huge disparity between the information rich digital content and knowledge available in English as compared to Indian languages. To address this disparity, we introduce SANGRAHASYNTHETIC, an initiative aimed at democratizing access to knowledge by translating a knowledge-rich English corpus into Indian languages. Utilizing INDICTRANS2 (Gala et al., 2023), we translated the entirety of English Wikimedia into 14 Indian languages resulting in nearly 90B tokens. Since INDICTRANS2 operates at the sentence-level and does not retain the document level formatting such as newlines, markdowns and other structures, we developed the SETUTRANSLATE pipeline. This pipeline facilitates the translation of documents and conversations while preserving the original document structure.\\n\\nRecognising the prevalent trend of \u201cRomanized\u201d Indic language usage particularly in informal settings and in digital communication, we extend Husain et al. (2024) and transliterate the above-translated content in 14 languages to Roman script using INDICXSLIT (Madhani et al., 2023b) resulting in about 72B tokens. Going forward, we will extend SANGRAHASYNTHETIC to cover all the 22 scheduled languages of India.\\n\\n3.1.3 Sangraha Unverified\\n\\nWe introduce the SANGRAHAVNVERIFIED split to expand the Sangraha corpus while ensuring high quality. This split employs a perplexity filtering pipeline, inspired by CCNet (Wenzek et al., 2020), with the SANGRAHAVERIFIED split serving as the benchmark for data quality. Following this approach, we train 5-gram Kneser-Ney models using KenLM library (Heafield, 2011) for each language on a sample of SANGRAHAVERIFIED data. We then clean the entire Indic splits of CULTURA (Nguyen et al., 2023a) and MADLAD400 (Kudugunta et al., 2023) datasets through the Setu data cleaning pipeline. We consider CULTURA and MADLAD400 as these represent the latest and most comprehensive multilingual collections. We compute the perplexity for all the resultant cleaned documents and retain only those documents whose perplexity is below the threshold chosen for that language. We describe more details of the pipeline and the statistics for various languages in Appendix B.\\n\\n3.2 Setu: A Comprehensive Pipeline for Data Cleaning, Filtering, and Deduplication\\n\\nTo clean, filter, and deduplicate Web, PDF, and Speech data, we create Setu, a pipeline built on Apache Spark which broadly has 4 stages - document preparation, document cleaning and analysis, flagging and filtering, and deduplication. The document preparation stage focuses on extracting the text from our diverse sources and creating text documents for further processing. For Web documents, we use trafilatura (Barbaresi, 2021b) to extract text from HTML, while the PDFs are run through a pipeline that uses a combination of the various bounding box related information to filter...\"}"}
{"id": "acl-2024-long-843", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the cleaning and analysis stage, we perform in-document cleaning to reduce the noise within a single document. We also use a multi-model approach for language identification by combining the outputs from IndicLID (Madhani et al., 2023a), CLD3, and NLLB (Costa-juss\u00e0 et al., 2022). We then perform analysis by computing various statistics like character, word counts, NSFW word count, n-gram repetition ratio, etc. In the flagging and filtering stage, we apply various filters based on the statistics computed like line length filters, NSFW word filters, and repetition filters which remove noisy and toxic documents. In the end, the deduplication stage performs fuzzy deduplication using MinHashLSH implemented in text-dedup repository by employing \\\\( n = 5 \\\\) and threshold = 0.7. A detailed overview and analysis of Setu is in Appendix D.\\n\\n### 3.3 Data Analysis\\n\\nThe final statistics of Sangraha are shown in Table 1.\\n\\n#### Comparison with other Multilingual Corpora:\\n\\nWe compare Sangraha Verified split with other Indic-only corpora - IndicCorp V1 (Kakwani et al., 2020), IndicCorp V2 (Doddapaneni et al., 2023) and Wikipedia. Figure 2 shows the distribution of the number of tokens for different Indic languages. We observe a significant increase in the size for all languages especially in the lower resource languages. Overall Sangraha Verified contains 64.3B tokens and is 2.6 \\\\( \\\\times \\\\) bigger than IndicCorp V2. We show a detailed language-wise comparison in Table 20 in Appendix F.\"}"}
{"id": "acl-2024-long-843", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Number of tokens (in Billions) dropped at each stage in Cultura-X and MADLAD-400 when cleaned using Setu. Notably, the Deduplication stage exhibits the most significant reduction in tokens, which can be attributed to the fact that a lot of web content for Indic Languages comprises news articles with similar content disseminated across various platforms. We show qualitative examples of the content that gets filtered out at each stage in Appendix D. To show how Setu performs on other corpora, we clean the entire Cultura-X and MADLAD-400 datasets through Setu. Figure 5 shows the token drops across the stages for both. The massive drop from Stage-1 to Stage-2 shows that both the corpora had significant amount of noise inside documents like menus, headers, etc despite the claims of them being clean. We show examples of what kind of content is getting removed from Cultura-X and MADLAD-400 in Appendix C.\\n\\nIndicAlign\\n\\nIndicAlign comprises two distinct splits: IndicAlign-Istruct and IndicAlign-Toxic data, each contributing to the robustness and diversity of the dataset. Table 2 encapsulates the overall statistics of IndicAlign.\\n\\n4.1 IndicAlign - Instruct\\n\\nThe IndicAlign-Istruct segment encompasses datasets that can be used to imbibe instruction-following ability in Large Language Models. Firstly we amalgamate different existing Instruction Finetuning (IFT) datasets with prompts authored by humans and responses generated by either humans or open, license-friendly models. To complement this human-centric approach, which is often too expensive and time consuming, we turn to synthetic data generation using existing chat-aligned models following the works of Ding et al. (2023b), Habash et al. (2022), and Xu et al. (2023b). We ensure that our outputs are always from open, license-friendly models and are always grounded in context. Given limited space, the descriptions given below are brief and we point the reader to Appendix E for more details and examples of all the datasets mentioned below.\\n\\nIndic-ShareLlama\\n\\nWe collect the prompts from the first turns of ShareGPT data and prompt LLAMA2-70B C Hat model (Touvron et al., 2023b) for responses. Excluding non-English, coding, and math prompts, we translate and transliterate these prompt-response pairs into 14 languages.\\n\\nDolly-Translated\\n\\nFollowing Gala et al. (2024) and Husain et al. (2024), we translate and transliterate DOLLY-15K (Conover et al., 2023) dataset into 14 Indic languages.\\n\\nOpenAssistant-Translated\\n\\nWe extend the efforts of Gala et al. (2024); Husain et al. (2024) and release the translated and transliterated OPENASSISTANT-TANT (K\u00f6pf et al., 2023) in 14 Indic languages.\\n\\nWikiHow\\n\\nWikihow is an online wiki-style platform that serves as a valuable resource for a diverse array of how-to articles. Gala et al. (2024) curate around 20,400 and 6000 instruction-answer pairs in English and Hindi. The data is formulated as a completion task given either a question or a question along with a few initial steps. We translated it into 14 Indic languages.\\n\\nIndoWordNet\\n\\nTo get grammar and language creativity data we employ IndoWordnet (Bhat-tacharyya (2010), Panjwani et al. (2018)) to construct instruction-answer pairs. Our approach involves identifying a set of 21 distinct intents, such as part-of-speech identification and sentence creation using specific words. For each intent, we create five unique templates, both for prompts and responses, across 18 languages. Then for every WordNet entry, we randomly populate 20 templates, thus creating tailored instruction-answer pairs.\\n\\nAnudesh\\n\\nAnudesh is a crowd-sourced collection of prompts accompanied by responses from LLAMA2-70B C Hat model. The participants are provided with instructions detailing the nature of interaction expected from them. Each instruction has an Intent, a domain, and a language instruction. The intent describes the interaction's goal, such as summarization or recommendation seeking. The domain specifies...\"}"}
{"id": "acl-2024-long-843", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Overall statistics of INDIC ALIGN. Dolly-T represents Dolly Translated, OpenAssistant-T represents OpenAssistant Translated.\\n\\nWiki-Conv\\nWe use Wikipedia passages and Wiki-Infoboxes as contexts to generate conversations. An Infobox is a fixed-format table added to Wikipedia articles that summarizes important facts, statistics, and important points in an easy-to-read format. We use LLAMA 2-70B C HAT (Touvron et al., 2023b) to generate an entire conversation in a user-assistant format in a single generation and subsequently translate and transliterate it to 14 Indic languages using our pipeline.\\n\\nWiki-Chat\\nUnlike WIKI-CONV, here we try to simulate dialogues between two LLMs. We use the Wikipedia context from WIKI-CONV to determine an intent which drives the conversation between a User LLM agent and an Assistant LLM agent. This simulation involves four distinct LLM agents: Intent LLM, Init User LLM, Assistant LLM, and Next User LLM. We use LLAMA 2-70B C HAT (Touvron et al., 2023b) and MISTRAL-7B C HAT (Jiang et al., 2023) to simulate the conversations which are then translated and transliterated to 14 Indic languages.\\n\\nWe again request the reader to check Appendix E for more details and examples (especially for IndoWordNet, Anudesh, Wiki-Conv and Wiki-Chat).\\n\\n4.2 IndicAlign - Toxic\\nAligning chat models to responsibly handle toxic prompts is a crucial aspect of developing ethically responsible models. In this work, we present initial steps towards creating datasets aimed at refining model responses to toxic inputs. We use both human and synthetic data collection strategies and introduce two distinct datasets: HH-RLHF - Translated, comprising human-curated data, and Toxic Matrix, a novel toxic alignment dataset created synthetically.\\n\\nHH-RLHF - Translated\\nWe prompt LLAMA 2-70B C HAT to classify each of the initial user prompts from HH-RLHF (Bai et al., 2022) as either toxic or non-toxic, along with providing the rationale for its decision. From approximately 169K initial prompts, around 32K were identified as toxic. We frame the response for each toxic prompt by including a statement of inability to engage due to the toxic nature of the prompt accompanied by the rationale given by the model. We then translate this into 14 Indic languages followed by transliteration to Roman script.\\n\\nToxic Matrix\\nWe introduce a novel approach to generate toxic alignment data synthetically using a taxonomy with three main axes: Content-Type, Target Group, and Prompt Style. We expand each axis to come up with a comprehensive list of categories. Table 3 shows examples of categories under each of the axes. We then use MISTRAL-7B C HAT (Jiang et al., 2023) model to generate toxic prompts for each combination of our categories. We found MISTRAL-7B C HAT to have minimal safety alignment which allows it to create highly creative toxic content. We then use another model - LLAMA 2-70B C HAT - to respond to these toxic prompts. LLAMA 2-70B C HAT is selected for its...\"}"}
{"id": "acl-2024-long-843", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"strong toxic alignment, meaning it either refuses to engage with the toxic content or provides a non-toxic response. We generate in total about 90K toxic prompt-response pairs, all of which are translated and transliterated to 14 Indic languages. Although previous works have shown different ways to distill instruction following alignment from strong models, we propose this method as one of the ways to distill toxic alignment using a combination of a weakly and a strongly toxic-aligned model. This approach, while still under development, offers a promising direction for improving the ethical alignment of conversational models. It's important to note, however, that this method is part of an ongoing effort and not a definitive solution to ensuring toxic alignment. We propose this taxonomy-based approach as one of the ways of approaching this problem of generating/collecting toxic data and thereby aligning the models. We refer the reader to Appendix F for further details and examples.\\n\\n4.3 Analysis\\n\\nNumber of turns: Our curated dataset exhibits a wide range across various dimensions. Specifically, the range of dialogue turns spans from an average of 9.27 to a minimum of 1, which will result in the trained model's capability to support dialogues of both short and extended lengths. Furthermore, the variation in average instruction and output lengths will underscore the model's proficiency in processing and generating content of diverse lengths.\\n\\nLexical diversity of the data:\\nTo show the lexical diversity of the prompts, following the work of UltraChat (Ding et al., 2023b) we use the Measure of Textual Lexical Diversity (MTLD) score (McCarthy and Jarvis., 2010). As seen in Table 2, the OpenAssistant dataset has the highest lexical diversity, attributable to its sourcing from approximately 13,500 volunteers. Additionally, the lexical diversity of the Wiki-Chat dataset is on par with other human-generated datasets such as Indic ShareLlama and Dolly, indicating that our methodology of using intents to drive conversations, is effective in producing prompts with diversity comparable to those collected from human participants.\\n\\nIntent Diversity Analysis:\\nFigure 6 depicts the distribution of intents within the Wiki-Chat dataset. Notably, since we have used Wikipedia as the context, we understandably see a majority of the interactions revolving around Information Seeking. We also observe diversity of intents centered around various real-world scenarios showing the real-world applicability of our data. We show additional analysis of the data in Appendix E.\\n\\n5 Conclusion\\n\\nIn summary, our work addresses the underrepresentation of low and mid-resource languages, specifically focusing on the 22 constitutionally recognized languages. We introduce INDIC SUITE, a comprehensive framework encompassing SANGRAHA pretraining data, SETU a Spark-based pipeline for data curation, INDICALIGN-INSTRUCT a diverse prompt-response collection, and INDICALIGN-TOXIC containing aligned toxic responses for Indic LLMs. By striking a balance between human-verified content and model-generated data, we aim to provide equitable access to information for diverse linguistic communities. We encourage community collaboration in the costly endeavor of LLM training, advocating for the pooling of resources to build high-quality fully open source Indic language LLMs. Through the public release of\"}"}
{"id": "acl-2024-long-843", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"our tools and datasets, we hope to inspire advancements in LLM development for Indian languages and beyond.\\n\\nLimitations\\nWhile Sangraha leverages publicly available web content, PDFs, and videos as primary data sources, it\u2019s crucial to acknowledge potential biases inherent in this data, which could be inherited by any model trained on the data. We leave the analysis on potential biases and debiasing techniques for future work.\\n\\nWe rely on NSFW word detection for toxic data detection, which does not fully capture or mitigate toxicity and sometimes results in false positives. We call upon the community to create better toxic data detection techniques for all Indian languages.\\n\\nDespite our efforts to remove Personally Identifiable Information (PII) from crowdsourced data, there remains a risk of inadvertent inclusion. The dataset exhibits lower representation from higher age groups, uneven coverage across Indian states, and a lack of comprehensive inclusion for low-resource languages. Additionally, our method of translating toxic prompts into Indic languages may not adequately capture the nuanced variations in Indian contexts.\\n\\nWe again call upon the community to contribute towards enhancing data diversity, improving translation methodologies for better cultural and contextual relevance, and developing more effective tools for debiasing and ensuring ethical use.\\n\\nEthics Statement\\nAll individuals involved in this effort, including annotators and developers, were adequately compensated for their work, adhering to all relevant norms and regulations of our country. The volunteers engaged in the curation of crowd-sourced data were informed about the public release of the data. Both the pretraining and fine-tuning datasets have been checked for offensive content, as necessary.\\n\\nThe released code will carry an MIT License, and all datasets will be released under appropriate open licenses.\\n\\nAcknowledgements\\nWe would like to thank EkStep Foundation and Nilekani Philanthropies for their generous grant towards building datasets, models, tools and other resources for Indian languages. We also thank Google for their valuable grant that facilitated OCR through Google Cloud Vision and for access to free TPUs as part of the TPU Research Cloud (TRC) initiative. We thank the Sarvam AI team for their insightful discussions and constructive comments on this project. We are also immensely grateful to the volunteers from the AI4Bharat team for their motivation and meticulous efforts in conducting manual audits.\\n\\nReferences\\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Beno\u00eet Sagot. 2022. Towards a cleaner document-oriented multilingual crawled corpus. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 4344\u20134355, Marseille, France. European Language Resources Association.\\n\\nAmro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S. Morcos. 2023. Semdedup: Data-efficient learning at web-scale through semantic deduplication. CoRR, abs/2303.09540.\\n\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The falcon series of open language models.\\n\\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback.\\n\\nAdrien Barbaresi. 2021a. Trafilatura: A web scraping library and command-line tool for text discovery and extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL 2021 - System Demonstrations, Online, August 1-6, 2021, pages 122\u2013131. Association for Computational Linguistics.\\n\\nAdrien Barbaresi. 2021b. Trafilatura: A web scraping library and command-line tool for text discovery and extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations.\"}"}
{"id": "acl-2024-long-843", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-843", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-843", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine \u00c7abuk Ball\u0131, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. 2022. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50\u201372.\\n\\nSneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. 2023. Madlad-400: A multilingual and document-level large audited dataset.\\n\\nAnoop Kunchukuttan. 2020. The indicnlp library. https://github.com/anoopkunchukuttan/indic_nlp_library/blob/master/docs/indicnlp.pdf.\\n\\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. 2023. Openassistant conversations \u2013 democratizing large language model alignment.\\n\\nHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, J\u00f6rg Frohberg, Mario \u0160a\u0161ko, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Mu\u00f1oz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Al-mubarak, Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ade-lani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Alexandra Luccioni, and Yacine Jernite. 2023. The bigscience roots corpus: A 1.6tb composite multilingual dataset.\\n\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8424\u20138445. Association for Computational Linguistics.\\n\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. CAMEL: communicative agents for \u201cmind\u201d exploration of large scale language model society. CoRR, abs/2303.17760.\\n\\nHaonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023b. Bactrian-x: Multilingual replicable instruction-following models with low-rank adaptation.\\n\\nYuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023c. Textbooks are all you need ii: phi-1.5 technical report.\\n\\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning.\\n\\nEdward Loper and Steven Bird. 2002. Nltk: The natural language toolkit.\\n\\nYash Madhani, Mitesh M. Khapra, and Anoop Kunchukuttan. 2023a. Bhasa-abhijnaanam: Native-script and romanized language identification for 22 Indic languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 816\u2013826, Toronto, Canada. Association for Computational Linguistics.\\n\\nYash Madhani, Sushane Parthan, Priyanka Bedekar, Gokul Nc, Ruchi Khapra, Anoop Kunchukuttan, Pratyush Kumar, and Mitesh Khapra. 2023b. Aksharantar: Open Indic-language transliteration datasets and models for the next billion users. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 40\u201357, Singapore. Association for Computational Linguistics.\\n\\nPhilip M. McCarthy and Scott Jarvis. 2010. Mltd, vocdd, and hd-d: A validation study of sophisticated approaches to lexical diversity assessment.\\n\\nDipak Narayan, Debasri Chakrabarti, Prabhakar Pande, and Pushpak Bhattacharyya. 2002. An experience in building the indo wordnet-a wordnet for hindi. In First International Conference on Global WordNet, Mysore, India.\\n\\nThuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2023a. Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages.\\n\\nXuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, and Lidong Bing. 2023b. Seallms \u2013 large language models for southeast asia.\\n\\nRitesh Panjwani, Diptesh Kanojia, and Pushpak Bhattacharyya. 2018. pyiwn: A python based API to access Indian language WordNets. In Proceedings of...\"}"}
{"id": "acl-2024-long-843", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data, and web data only. CoRR, abs/2306.01116.\\n\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susan-nah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Mari-beth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Bud-den, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-poukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-tiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis & insights from training gopher. CoRR, abs/2112.11446.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67.\\n\\nScrapinghub. 2021. Article extraction benchmark. GitHub repository.\\n\\nShashank Siripragada, Jerin Philip, Vinay P. Namboodiri, and C V Jawahar. 2020. A multilingual parallel corpora collection effort for Indian languages. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 3743\u20133751, Marseille, France. European Language Resources Association.\\n\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodrigez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.\\n\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023a. Self-instruct: Aligning language models with self-generated instructions.\"}"}
{"id": "acl-2024-long-843", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 13484\u201313508. Association for Computational Linguistics.\\n\\nXiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, and Jun Xie. 2023. Polylm: An open source polyglot large language model. arXiv preprint arXiv:2307.06018.\\n\\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4003\u20134012, Marseille, France. European Language Resources Association.\\n\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a. Wizardlm: Empowering large language models to follow complex instructions. CoRR, abs/2304.12244.\\n\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023b. Wizardlm: Empowering large language models to follow complex instructions.\\n\\nCanwen Xu, Daya Guo, Nan Duan, and Julian J. McAuley. 2023c. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 6268\u20136278. Association for Computational Linguistics.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.\\n\\nMatei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust, Ankur Dave, Xianggrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin, Ali Ghodsi, Joseph Gonzalez, Scott Shenker, and Ion Stoica. 2016. Apache spark: a unified engine for big data processing. Commun. ACM, 59(11):56\u201365.\\n\\nWenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024. (inthe)wildchat: 570k chatGPT interaction logs in the wild. In The Twelfth International Conference on Learning Representations.\"}"}
{"id": "acl-2024-long-843", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Sangraha Verified Website domain statistics\\n\\n| Domain | Number of Websites |\\n|--------|--------------------|\\n| com    | 5926               |\\n| in     | 817                |\\n| org    | 446                |\\n| net    | 250                |\\n| co.in  | 81                 |\\n| tv     | 75                 |\\n| others | 445                |\\n| Total  | 8040               |\\n\\nAppendix A\\n\\nA.1 Curation of Sangraha Verified - Web Data\\n\\nHere we discuss the details about curation of Sangraha Verified - Web data. Table 4 shows the domain-level statistics of the websites in Sangraha Verified.\\n\\nIn this work, we adopt a three-fold strategy to collect a comprehensive collection of websites for scraping. Firstly, we extend the efforts of Kakwani et al. (2020) and Doddapaneni et al. (2023) of discovering web sources using existing repositories and automated web searches, to discover a large list of Indic language websites. But, unlike the previous efforts, we do not restrict ourselves to just news websites. Secondly, we identify various domains such as Indian Culture, Food, Health, and Travel, among others and enlist volunteers to gather websites within these domains, prioritizing those in Indic languages or English but pertinent to the Indian context. Thirdly, we collect the base URLs from MC4 (Xue et al., 2021), focusing on websites with high amount of content and get them verified by volunteers. Additionally, we include all the Indian Government websites which serves as a valuable resource, given their multilingual content.\\n\\nVolunteers review each website collected via automated methods and decide on acceptance or rejection based on the criteria defined. A website can be rejected if either of the below conditions were met:\\n\\n- Website is non-Indic or non-English.\\n- Website is an adult, gambling, or a general toxic website.\\n- Website has content that directly appears to be machine-translated.\\n\\nFigure 7 presents the verification outcomes, highlighting a significant rejection rate due to website inactivity, particularly those sourced from MC4. This means that the information in existing collections if becoming outdated because of defunct websites. We make available the verification portal for further research utilization.\\n\\nA.2 Curation of Sangraha Verified - PDF Data\\n\\nIn this section, we elaborate on the methodology adopted for curating the Sangraha Verified - PDF data. Table 5 shows the detailed source-level PDF statistics. We discuss the details of the curation of data from each source below.\\n\\nInternet Archive\\n\\nUtilizing the official API of the Internet Archive, we collected approximately 921K PDF documents across all Indic languages. This collection spans diverse categories such as religious texts, news articles, fiction, educational materials, and scientific literature. We subsequently filtered out PDFs incompatible with GCP Vision, specifically excluding languages like Bodo, Dogri, Kashmiri, Konkani, Maithili, Manipuri, and Sindhi due to their low-resource status, with plans for future inclusion.\\n\\nTo optimize for quality and manage costs, OCR was performed solely on high-quality PDFs. We\\n\\n14https://archive.org/developers/internetarchive/\\n15https://cloud.google.com/vision/docs/languages\"}"}
{"id": "acl-2024-long-843", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rejected 29.7%\\nAccepted 70.3%\\n(a) Comparison of the number of websites accepted and rejected\\n\\n| Domain    | Count |\\n|-----------|-------|\\n| News      | 0     |\\n| Blog      | 1000  |\\n| Shopping  | 2000  |\\n| Education | 3000  |\\n| Religion/Spirit | 0 |\\n| Government | 1500 |\\n| others    | 2000  |\\n\\n(b) Accepted Websites statistics - Domain information\\n\\n| Reason                  | Count |\\n|-------------------------|-------|\\n| Website not loading     | 0     |\\n| Non Indic Language      | 500   |\\n| Machine Translated      | 1000  |\\n| Content                 | 1500  |\\n| Adult Content           | 2000  |\\n| Not Relevant            | 15846 |\\n\\n(c) Rejected Website statistics - Rejection Reason\\n\\nFigure 7\\n\\nFirst remove all the corrupted and encrypted PDFs. Additionally, resource limitations from GCP Vision necessitated the filtering of PDFs exceeding 2000 pages. We also filter out all the PDFs having less than 25 pages as these are often incoherent documents such as glossaries, comics, bills, and receipts. Quality assurance measures for OCR included filtering out scanned PDFs with a Pixel Per Inch (PPI) rating below 300. Additionally, we analyzed images from 10 consecutive pages of each PDF, focusing on metrics like average image area coverage and brightness. PDFs with images were considered for further analysis if they covered less than 50% of the page area and had a brightness level above 200. Table 6 shows the statistics of PDFs filtered after each filtering stage.\\n\\neGyanKosh, India\u2019s National Digital Repository, serves as a repository for digital learning resources from Open and Distance Learning Institutions, covering subjects such as History, Economics, Political Science, Public Administration, and Sociology, across various Indian languages.\\n\\nIndian Parliament\\nThis source comprises manually compiled summaries of debates and discussions from the Indian Parliament and various State Legislative Assemblies. These form a rich source of local and culturally relevant data. We collect all the publicly available Parliamentary and State Assembly materials. Table 7 shows the statistics of the state-wise collected documents.\\n\\nAIR News\\nAll India Radio (AIR) is the national radio broadcaster of India, a Prasar Bharati division, that streams radio programs in all major Indian languages. Following the approach of (Bhogale et al., 2023a), we collect news bulletins for 12 Indian languages. Table 8 shows the language level statistics of the collected data.\\n\\nGovt. Magazines\\nWe aggregated content from magazines published by governmental agencies, which include annual reports, details on governmental schemes, initiatives, cabinet decisions, and current affairs, published in multiple Indian languages.\"}"}
{"id": "acl-2024-long-843", "page_num": 49, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 39: Wiki How\\n\\nFigure 40: Toxic Matrix\"}"}
{"id": "acl-2024-long-843", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 17: Number of instruction-answer pairs for each language in the Indo WordNet split of INDIALIGN.\\n\\nTable 18 shows some examples of the Intents and Domains. Given LAMA's constraints with Indic languages, we follow the translate-test approach where we first translate prompts into English before processing and then translating the responses back to the respective Indic languages. Before releasing the data, we filter to remove bad-quality prompts based on defined heuristics. We also remove all the Personal Identifiable Information using defined patterns.\\n\\nUser Base Analysis\\nThe demographic analysis of any dataset's contributors is crucial for understanding its representativeness and inclusivity. Each user is prompted first with a declaration - \\\"I consent to release my conversations under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.\\\" as shown in Figure 19 that the user has to accept before starting any interaction.\\n\\nGeographically, the user base is predominantly from Karnataka, Maharashtra, and Tamil Nadu, as shown in Figure 21, with a notable underrepresentation of users from other states, especially the North Eastern states. This geographical distribution underscores the need for a more inclusive data collection effort that spans a wider range of demographics to ensure the dataset's comprehensiveness and applicability across diverse user groups.\\n\\nThe current demographic skew in our dataset highlights a pressing need for inclusivity in data collection methodologies. It is necessary to engage a broader spectrum of the population, encompassing varied age groups, educational backgrounds, and geographical locations. Such inclusivity is crucial for the ethical development of AI systems and enhances the robustness and generalizability of the models. Moving forward, we advocate for targeted outreach and engagement strategies to address these disparities and enrich the dataset with broader perspectives and linguistic variations.\"}"}
{"id": "acl-2024-long-843", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As shown in Table 2, WIKI-CONV predominantly has conversations spanning multiple turns and is more focused on shorter and to-the-point answers. We show the prompt used to generate this data in Figure 29.\\n\\nE.4 Wiki-Chat\\nTo enhance the collection of open-generation conversations, we follow the approaches tried out by ULTRA (Ding et al., 2023a), CAMEL (Li et al., 2023a), and others of simulating interactions between two models. Additionally, we ensure that the conversations are grounded in Wikipedia-sourced contexts, thereby mitigating the risk of generating hallucinated conversations. We show the overview of the entire pipeline in Figure 30.\\n\\nUsing Wikipedia context from WIKI-CONV, we determine an intent to drive the conversation between a User LLM and an Assistant LLM agent. We use LLAMA2-70B CHAT (Touvron et al., 2023b) and MIXTRAL-8X7B-V0.1 (Jiang et al., 2024b) to simulate the conversations, which are then translated and transliterated to 14 Indian languages forming WIKI-CHAT. This simulation broadly involves four different LLM agents:\\n\\n- **Intent LLM**: Utilized to derive potential conversation intents from a given context that can drive the conversations. Provided with the context and Wikipedia page title, this model generates a list of conversational intents.\\n- **Init User LLM**: Responsible for generating the initial user prompt based on the provided context and intent. This step is crucial in setting the conversation's tone, and hence careful curation is undertaken to avoid defaulting to an assistant role, as noted by (Ding et al., 2023a).\\n- **Assistant LLM**: Generates the assistant's response to the user prompt, ensuring relevance and grounding in the provided context and conversation history.\\n- **Next User LLM**: Continues the conversation by acting as the user, using the context and previous conversation history to generate subsequent prompts.\\n\\nThe process starts with the Intent LLM to identify the possible conversation intents in the given context. Following this, the Init User LLM crafts the initial user prompt, which is then addressed by\"}"}
{"id": "acl-2024-long-843", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Age Group Distribution\\n\\n| Age Group | #Participants |\\n|-----------|--------------|\\n| 18-24     | 250          |\\n| 25-34     | 500          |\\n| 35-44     | 750          |\\n| 45-54     | 1000         |\\n| 55-64     | 1250         |\\n| 65+       | 15865        |\\n\\n### Qualification Distribution\\n\\n| Qualification | #Participants |\\n|---------------|--------------|\\n| Masters       | 250          |\\n| Bachelors     | 500          |\\n| Doctoral      | 750          |\\n| Other         | 1000         |\\n\\n### State-wise Distribution\\n\\n| State                  | Percentage |\\n|------------------------|------------|\\n| Madhya Pradesh         | 3.5%       |\\n| Kerala                 | 5.6%       |\\n| National Capital Territory | 6.5%    |\\n| West Bengal             | 6.6%       |\\n| Andhra Pradesh          | 6.7%       |\\n| Telangana               | 8.7%       |\\n| Uttar Pradesh           | 8.8%       |\\n| Karnataka               | 23.0%      |\\n| Maharashtra             | 15.7%      |\\n| Tamil Nadu              | 14.9%      |\\n\\n**Figure 21: User Demographic Analysis of Anudesh**\"}"}
{"id": "acl-2024-long-843", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Understanding the history of Delhi.\\n\\nIntent\\n\\nA: Qutubuddin Aibak was the first ruler of Delhi Sultanate\\n\\nQ: Who was the first ruler of Delhi Sultanate?\\n\\nQ: When was Qutubuddin Aibak born?\\n\\nFigure 22: Overview of the WIKI-C HAT pipeline. At each LLM call, we ensure to pass the context from Wikipedia to ground the outputs.\\n\\nthe Assistant LLM, completing one conversation turn. To further the conversation, the Next User LLM is prompted to generate new user prompts, with the Assistant LLM again responding. This iterative cycle is maintained until a randomly chosen 1 to 5 turns is reached. We show the prompt templates for each LLM agent in Figure 22. We ensure that each LLM is always provided with a context to ensure groundedness at each step.\\n\\nData Cleaning\\n\\nDespite rigorous prompting, some model outputs necessitate cleaning to ensure conversation quality. Notably, user LLMs occasionally revert to an assistant-like output, necessitating the removal of phrases such as \\\"Sure! Here is something a user may ask...\\n\\nAlso, we notice the behavior of asking prompts from a second person point of view like \\\"Ask the assistant the benefits of using Hydrogen Peroxide.\\\" We make sure to explicitly detect and filter out these noisy prompts. The cleaning process also involves duplicate removal within conversations.\\n\\nComparison of LLAMA 2-70B C HAT and M IXTRAL-8X7B-V0.1 models\\n\\nTable 19 shows the statistics of the conversations generated by LLAMA 2-70B C HAT and M IXTRAL-8X7B-V0.1 models. We observe that conversations generated using M IXTRAL-8X7B-V0.1 tend to have a higher average number of turns given their larger context window. Since we pass the context to the model as part of each prompt, LLAMA 2-70B C HAT fails in conversations involving a higher number of turns due to the smaller context window. Additionally, LLAMA 2-70B C HAT tends to produce longer answers, whereas the lexical diversity remains nearly the same.\\n\\nWe also detect the number of times, the models break character and revert to the original assistant forms.\\n\\nF Curation of INDICALIGN-TOXIC HH-RLHF Toxic Classification\\n\\nHH-RLHF (Bai et al., 2022) is a conversation dataset released to train a preference (or reward) models for subsequent RLHF training. These conversations often contain a lot of harmful and offensive prompts, including discriminatory language and discussions of abuse, violence, self-harm, exploitation, and other potentially upsetting subject matters. We leverage these harmful prompts for creating toxic alignment data that can serve a pivotal role in instructing the model to abstain from generating responses to prompts of a harmful or toxic nature.\\n\\nWe first extract the initial user prompts from the dataset. Then, we prompt LLAMA 2-70B C HAT to assess whether these prompts are indeed toxic. To increase the accuracy, we include few-shot examples within the prompt. In addition to identifying toxic prompts, we prompt LLAMA 2-70B C HAT for explanations regarding the rationale behind the...\"}"}
{"id": "acl-2024-long-843", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 23: Comparative Analysis of Noun and Verb Usage Patterns Across Five Datasets\"}"}
{"id": "acl-2024-long-843", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-843", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 19: Analysis of conversations generated using LLAMA 2-70B C HAT and MISTRAL -8 X 7B- V 0.1 to toxic flagging. Figure 28 shows the detailed prompt template. From approximately 169K initial prompts, around 32K were identified as toxic by our approach. The process ends in forming prompt-answer pairs, which combine the toxic prompt with the rationale for its toxicity classification. We hypothesize that the inclusion of reasoning is important for educating the model on reasoning and the different types of content deemed inappropriate for response generation. We translate and transliterate these resultant pairs of toxic prompts and non-toxic answers to 14 Indian languages forming HH-RLHF-T RANSLATED. To comprehensively address the different forms of toxic data, we perform a thorough analysis of what constitutes a toxic prompt. We define a toxic prompt as a prompt that \u201ccan\u201d elicit a potentially toxic response. We note that not all toxic prompts can have a toxic answer. Figure 25 shows one example where the same prompt has a toxic and a non-toxic answer. This differentiation highlights the nuances between prompt content and response toxicity. Building on this foundation, we identify three primary axes of a toxic prompt:\\n\\n- **Content Type**: This dimension identifies the prompt\u2019s core theme or subject matter that imbues it with a toxic quality, such as violent content or hate speech. It essentially captures the underlying intent of the toxic prompt. Various examples of content types are cataloged in Table 3.\\n\\n- **Target Group**: This aspect specifies the individual or group towards whom the toxic prompt is directed. Table 3 lists down a few examples of target groups.\\n\\n- **Prompt Style**: This characteristic describes how the prompt conveys toxicity. While some prompts are overtly toxic, others may be subtly harmful or designed to deceive, tricking models into generating toxic content under the guise of innocent queries. Different prompt styles and their descriptions are listed in Figure 26.\\n\\nWe leverage a combination of a relatively less toxic aligned model - MISTRAL -7B C HAT - and a highly toxic aligned model - LLAMA 2-70B C HAT - to generate toxic prompt and refusal answer pairs. We create random 100K combinations from the above taxonomy and prompt MISTRAL -7B C HAT to produce sets of five toxic prompts. Figure 27 shows the prompt template used for generating toxic prompts. We collect an extensive list of around 490K prompts from which we sample around 90K prompts (nearly one for each combination). We then prompt LLAMA 2-70B C HAT to get a response for that prompt. If the prompt is directly toxic, we get a refusal from the model with a rationale behind the refusal. Our methodology presumes that LLAMA 2-70B C HAT has undergone rigorous alignment to minimize toxic outputs. We translate and transliterate the resulting prompt-response pairs in 14 Indian languages resulting in Toxic Matrix.\\n\\nAlthough previous works have shown different ways to distill instruction following alignment from strong models, we propose this method as one of the ways to distill toxic alignment using a combination of a weakly and a strongly toxic-aligned model. This approach, while still under development, offers a promising direction for improving the ethical alignment of conversational models. However, it\u2019s important to note that this method is part of an ongoing effort and not a definitive solution to ensuring toxic alignment. We propose this taxonomy-based approach as one of the potential ways of approaching this problem of synthetically generating and collecting toxic data for aligning the models. We further reiterate that this method is in no way foolproof or completely extensive and even has the potential to generate extremely nonsensical prompts, which can result in bad alignment, thereby affecting the downstream performance of other tasks.\"}"}
{"id": "acl-2024-long-843", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Some people say that false accusations of sexual harassment are rampant and damaging to ... strong negative emotions.\\n\\nFigure 26: Description of each prompt-style we want to generate using MISTRAL -7B CHAT.\"}"}
{"id": "acl-2024-long-843", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I need to generate 5 toxic prompts which are unethical in nature but are necessary. The figure below shows the prompt template used for generating taxonomical toxic prompt data using MISTRAL-7B CHAT.\"}"}
{"id": "acl-2024-long-843", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Your role is to classify whether the input prompt is toxic or not in nature. The output will be a boolean value indicating whether the input is toxic or not.\\n\\nPrompt Template\\n\\nFigure 29: Prompt template used generating conversations for the Wiki-Conv data.\\n\\nBelow is an article: {wiki_text}\\n\\nI want you to simulate an interesting conversation between a ... and should have 5-6 turns. Generate only a single conversation.\\n\\nWiki-Conv Prompt Template\"}"}
{"id": "acl-2024-long-843", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are a helpful, respectful and honest assistant. You will assist in identifying the ... Wiki-Chat Assistant LLM\\nTemplate\\n(d) Prompt template - Assistant LLM\\nFigure 30: Wiki-Chat Prompt templates\\n(b) Prompt template - Init User LLM\\n(e) Prompt template - Next User LLM\\n lick for next user\"}"}
{"id": "acl-2024-long-843", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | Stage-1 Words | Stage-2 Docs | Stage-3 Words | Stage-4 Docs |\\n|----------|---------------|--------------|---------------|--------------|\\n| asm      | 22M           | 43K          | 16M           | 42K          |\\n| ben      | 4199M         | 11721K       | 3812M         | 11305K       |\\n| brx      | -             | 476          | 77            | 1           |\\n| doi      | -             | 11K          | 10922         | 53           |\\n| eng      | -             | 17M          | 12M           | 33K          |\\n| guj      | 524M          | 1084K        | 462M          | 1049K        |\\n| hin      | 10664M        | 18740K       | 8985M         | 17950K       |\\n| kan      | 436M          | 1225K        | 403M          | 1198K        |\\n| kas      | -             | 50K          | 17811         | 61           |\\n| kok      | 0.16M         | 444          | 0.17M         | 912          |\\n| mai      | 1195          | 47           | 1284          | 46           |\\n| mal      | 698M          | 2480K        | 635M          | 2408K        |\\n| mni      | -             | 0.1M         | 0.06M         | 89           |\\n| mar      | 934M          | 2180K        | 857M          | 2138K        |\\n| nep      | 1154M         | 3047K        | 1082M         | 2983K        |\\n| ori      | 39M           | 124K         | 32M           | 117K         |\\n| pan      | 369M          | 597K         | 284M          | 499K         |\\n| san      | 3M            | 11K          | 1M            | 11K          |\\n| sat      | -             | 536          | -             | 105          |\\n| snd      | 83M           | 91K          | 76M           | 85K          |\\n| tam      | 1607M         | 4295K        | 1485M         | 4166K        |\\n| tel      | 583M          | 1657K        | 546M          | 1599K        |\\n| urd      | 1872M         | 2538K        | 1729M         | 2435K        |\\n| Total    | 23195M        | 49843K       | 20429M        | 48081K       |\\n|          |               |              | 19872M        | 44277K       |\\n\\nTable 21: Statistics of the number of words and documents getting filtered out.\"}"}
{"id": "acl-2024-long-843", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-843", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "acl-2024-long-843", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 36: Wiki-Chat\"}"}
{"id": "acl-2024-long-843", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "acl-2024-long-843", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Statistics of PDFs filtering from Internet Archive\\n\\n| State             | Number of PDFs |\\n|-------------------|----------------|\\n| Andhra Pradesh    | 3383           |\\n| Bihar             | 306            |\\n| Gujarat           | 3241           |\\n| Haryana           | 433            |\\n| Himachal Pradesh  | 2035           |\\n| Jharkhand         | 124            |\\n| Karnataka         | 8405           |\\n| Kerela            | 2039           |\\n| Madhya Pradesh    | 656            |\\n| Maharashtra       | 544            |\\n| Punjab            | 287            |\\n| Rajasthan         | 7             |\\n| Tamil Nadu        | 680            |\\n| Indian Parliament | 14896          |\\n| Total             | 37036          |\\n\\nTable 7: Statistics of the PDFs collected from Indian Parliament\\n\\n| Language | Number of PDFs |\\n|----------|----------------|\\n| Bengali  | 5721           |\\n| Gujarati | 5586           |\\n| Hindi    | 18560          |\\n| Kannada  | 4888           |\\n| Konkani  | 471            |\\n| Malayalam| 5665           |\\n| Marathi  | 8958           |\\n| Nepali   | 1686           |\\n| Odia     | 5769           |\\n| Punjabi  | 885            |\\n| Sanskrit | 730            |\\n| Tamil    | 7002           |\\n| Telugu   | 5555           |\\n| Urdu     | 2877           |\\n| Total    | 74353          |\\n\\nTable 8: Language-wise statistics of PDFs collected from AIR newsonair\\n\\n| Language | Original Count | After Validity Check | After Page Count Check | After Image Filters |\\n|----------|----------------|----------------------|------------------------|---------------------|\\n| Hindi    | 349,365        | 344,454              | 106,112                | 102,164             |\\n| Urdu     | 177,867        | 157,121              | 127,495                | 73,966              |\\n| Sanskrit | 88,238         | 84,804               | 76,401                 | 70,663              |\\n| Bengali  | 59,636         | 55,023               | 50,825                 | 45,272              |\\n| Tamil    | 52,199         | 49,924               | 37,243                 | 29,755              |\\n| Telugu   | 50,320         | 48,919               | 40,860                 | 38,243              |\\n| Gujarati | 43,677         | 42,021               | 34,514                 | 34,038              |\\n| Malayalam| 34,858         | 31,594               | 11,627                 | 4,725               |\\n| Kannada  | 24,446         | 23,589               | 18,661                 | 17,493              |\\n| Punjabi  | 13,898         | 12,932               | 7,397                  | 5,617               |\\n| Marathi  | 9,710          | 9,174                | 7,875                  | 7,478               |\\n| Assamese | 2,424          | 2,408                | 2,205                  | 2,408               |\\n| Nepali   | 1,545          | 1,497                | 836                    | 671                 |\\n| Odia     | 4,972          | 4,733                | 2,439                  | 4,732               |\\n| Total    | 74353          |                      |                        |                     |\"}"}
{"id": "acl-2024-long-843", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: State-wise statistics of School textbooks collected\\n\\n| State                | Number of PDFs |\\n|----------------------|----------------|\\n| Andhra Pradesh       | 126            |\\n| Assam                | 61             |\\n| Bihar                | 426            |\\n| Goa                  | 31             |\\n| Haryana              | 31             |\\n| Himachal Pradesh     | 1909           |\\n| Karnataka            | 502            |\\n| Kerala               | 121            |\\n| Maharashtra          | 76             |\\n| Manipur              | 70             |\\n| Meghalaya            | 293            |\\n| Mizoram              | 40             |\\n| Nagaland             | 681            |\\n| Odisha               | 41             |\\n| Punjab               | 195            |\\n| Rajasthan            | 186            |\\n| Telangana            | 235            |\\n| Tripura              | 365            |\\n| West Bengal          | 125            |\\n| National             | 598            |\\n| Other Books          | 1442           |\\n| Total                | 7554           |\\n\\nThis set includes publicly available textbooks from various Indian states and those published by the National Council of Educational Research and Training (NCERT), providing a rich source of educational content in multiple Indian languages. Table 9 shows the statistics of the books collected from different sources.\\n\\nMiscellaneous\\n\\nIn addition to the categorized sources, we also incorporated a variety of documents from government and public domains, focusing on content either in Indic languages or in English with relevance to India.\\n\\nA.3 Curation of SANGRAHA VERIFIED\\n\\nHere we discuss the details about Speech Data split of SANGRAHA VERIFIED component. Table 10 shows the detailed source-level statistics. We discuss the details of the curation of the data from each source below.\\n\\n| Source                | Number of Instances |\\n|-----------------------|---------------------|\\n| YouTube - Hindi       | 276K videos         |\\n| Open Subtitles        | 14K movies          |\\n| NPTEL - Transcripts   | 1.4K courses        |\\n| Mann Ki Baat          | 1.4K podcasts       |\\n| Others                | 15K                 |\\n| Total                 | 309K                |\\n\\nTable 10: Statistics of the various sources of Speech Data collected\\n\\nYoutube - Hindi\\n\\nFollowing the approach of Bhogale et al. (2024), we collect around 80K hours of audio data from Youtube videos in Hindi language. We then chunk it into smaller segments by detecting silences using WebRTC VAD and get each chunk transcribed using the Hindi Conformer model. Then, we piece together all the transcripts to obtain the transcript for the whole video.\\n\\nOpenSubtitles\\n\\nFollowing Gao et al. (2021), we collect all the Indic Language subtitles from OpenSubtitles. We first process the SRT files using simple regex based patterns to remove the timestamps and extract the text. We then define regex patterns to filter out other noisy content like character cues, continuation ellipses, etc. We then combine the different parts to form a single document per SRT file. Table 11 shows the language-wise statistics of Subtitles.\\n\\nNPTEL - Transcripts\\n\\nThe National Programme on Technology Enhanced Learning (NPTEL) is an Indian e-learning platform for university-level science, technology, engineering and mathematics subjects that is jointly developed by various Indian Institutes. Although the course content developed by NPTEL is primarily in English, a lot of it has been manually transcribed and translated into 11 different Indian Languages and reviewed before being made publicly available. The translated content has been compiled and released as course textbooks. Table 12 shows the statistics of the course transcripts available in different languages.\\n\\n16https://github.com/wiseman/py-webrtcvad\\n17https://www.opensubtitles.org/\\n18https://nptel.ac.in/\"}"}
{"id": "acl-2024-long-843", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | Number of Instances |\\n|----------|---------------------|\\n| Assamese | 2                   |\\n| Bengali  | 2619                |\\n| English  | 1178                |\\n| Hindi    | 2808                |\\n| Kannada  | 7                   |\\n| Malayalam| 7571                |\\n| Odia     | 3                   |\\n| Sindhi   | 30                  |\\n| Tamil    | 223                 |\\n| Telugu   | 20                  |\\n| Urdu     | 129                 |\\n| Total    | 14590               |\\n\\nTable 11: Language wise statistics of subtitles collected from OpenSubtitles\\n\\n| Language | Number of Courses |\\n|----------|-------------------|\\n| Assamese | 1                 |\\n| Bengali  | 91                |\\n| English  | 523               |\\n| Gujarati | 106               |\\n| Hindi    | 184               |\\n| Kannada  | 89                |\\n| Malayalam| 108               |\\n| Marathi  | 85                |\\n| Punjabi  | 1                 |\\n| Tamil    | 150               |\\n| Telugu   | 98                |\\n| Total    | 1436              |\\n\\nTable 12: Language wise statistics of the course transcripts collected from NPTEL\\n\\n| Language | Number of Instances |\\n|----------|---------------------|\\n| Assamese | 63                  |\\n| Bengali  | 91                  |\\n| English  | 410                 |\\n| Gujarati | 92                  |\\n| Hindi    | 89                  |\\n| Kannada  | 78                  |\\n| Malayalam| 89                  |\\n| Marathi  | 90                  |\\n| Manipuri | 65                  |\\n| Odia     | 82                  |\\n| Punjabi  | 81                  |\\n| Tamil    | 85                  |\\n| Telugu   | 89                  |\\n| Urdu     | 64                  |\\n| Total    | 1468                |\\n\\nTable 13: Language-wise Mann Ki Baat transcripts collected\\n\\nMann Ki Baat is an Indian Radio programme hosted by the Indian Prime Minister usually with a frequency of 1 per month. This is transcribed and then manually translated into 13 Indian languages.\\n\\nA.4 Setu Translate\\n\\nMajority of the machine translation systems are trained as sentence-level translators which often struggle to preserve various entities like inter-sentence separators, new-line characters, tab-spaces, markdowns, bullet points, etc. Simple sentence-tokenizers present in the packages like NLTK (Loper and Bird, 2002) and IndicNLP Library (Kunchukuttan, 2020) are not capable of retaining these inter-sentence separators and markdowns. We introduce SETU-TRANSLATE, a robust translation pipeline for mass-translation of both pre-training as well as Instruction fine-tuning data while preserving the structure of the document. Overall, SETU-TRANSLATE focuses on the accurate identification of the parts of the document that must be sent to the translation model and then the replacement of the translated sentences in the overall document thereby preserving the overall structure of the translated document. The five main aspects of SETU-TRANSLATE are described in this section.\"}"}
{"id": "acl-2024-long-843", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Using regex patterns, we identify the parts of the documents we intend to translate. The goal of this stage is to preserve the structure of the document. The regex patterns defined ignore markdown structures, code snippets (enclosed in backticks), bullet points, paragraph indicators, Roman numerals, etc., and extract only the sentences. After performing unicode-normalization and deduplication on the extracted sentences, a global sentence-level dataset is created.\\n\\nInference\\n\\nWe binarize the data first and then utilize INDICTION2 for translating English into Indic languages. We leverage both GPUs and TPUs for large-scale translation. To benefit the community, we open-source the flax port for INDICTION2 for faster TPU inference.\\n\\nReplace\\n\\nOnce we have the translated sentences, we perform a regex-based replacement of the original sentences with the translated ones. This ensures that only sentences are replaced and the other structure of the document is retained as is.\\n\\nA.5 Setu Transliteration\\n\\nSimilar to translation, we also release the Setu Transliteration pipeline. Since transliteration is done at a word level and doesn't consider the context of the remaining words, we follow the normal word replacement strategy. We maintain a continuously updating mapping of Indic words to their Roman counterparts in a prefix-based hierarchical format which we feel is the key to speedup and rapid access to the required word pairs.\\n\\nWord Mapping Dictionary\\n\\nFor the creation of the initial mapping, we use AKSHARANTAR (Madhani et al., 2023b) dataset, which is the largest publicly available transliteration dataset for Indic languages as the starting point. We convert AKSHARANTAR into the said prefix-based hierarchical format. This mapping is continuously updated with the new mappings as we discover new un-romanized words further in our pipeline.\\n\\nWord Replacement\\n\\nWord level replacement has 2 main challenges: (i) identifying words to replace while preserving the entire document structure; and (ii) unordered replacement leading to sub-word replacement instead of the entire word. We address (i) using the same regex-based approach used in SETU-TRANSLATE.\\n\\nTo address (ii), we sort the mapping based on source-language word length in descending order before feeding the mapping to the regex-based 'replace' module.\\n\\nInference\\n\\nDuring the first 'replace' pass, we log the un-romanized words whose mapping is not available in the current word mapping dictionary. In the 'inference' stage, we transliterate these words using INDICTIONX (Madhani et al., 2023b) to get an updated word-mapping dictionary. We then repeat the word-replacement until all the words are properly romanized.\\n\\nB Curation of SANGRAHUVERIFIED\\n\\nWe describe the methodology employed for curating the SANGRAHUVERIFIED split, leveraging a perplexity-based filtering pipeline inspired by CCNET (Wenzek et al., 2020). We first randomly sample 200,000 documents from SANGRAHAVERIFIED split for each language. We then normalize each document by converting text to lowercase, removing accents from characters, normalizing numbers to a uniform representation (specifically converting all digits to \\\"0\\\"), replacing a predefined set of Unicode punctuation with their ASCII counterparts, and removing non-printing characters. We then train a sentencepiece tokenizer and tokenize all of the sampled data. Then, we train a 5-gram Kneser-Ney models using KenLM (Heafield, 2011) library. We binarize these models for quicker inference.\\n\\nFor deciding the language-specific thresholds, we create a validation set by sampling another 100,000 documents from SANGRAHAVERIFIED and calculate the perplexity of each document using the trained n-gram models. We then sort the perplexities and choose the 80th percentile value as the threshold for each language. Table 14 shows the thresholds chosen for each language. To prefer more quality over volume, higher percentile thresholds can be chosen, but that may result in reduced diversity and representativeness of the resultant data.\\n\\nWe clean the entire CULTURA X and MADLAD400 corpora using the Setu Cleaning pipeline and...\"}"}
{"id": "acl-2024-long-843", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 14: Perplexity Statistics of CULTURA X and MADLAD-400 datasets. Perplexity is calculated using n-gram language models trained on data sampled from SANGRAHAVERIFIED. We de-duplicate it with the entire SANGRAHAVERIFIED split. Finally, we calculate the perplexities of each document and filter out those that are above the chosen threshold. Table 14 shows the final number of documents chosen after perplexity filtering.\\n\\nB.1 Perplexity Analysis\\n\\nFigure 8 shows the perplexity distributions of the cleaned CULTURA X and MADLAD-400 data using the n-gram language models trained on SANGRAHAVERIFIED. We observe that certain languages, specifically Hindi, Malayalam, and Marathi, exhibit relatively tight distributions of perplexity values. This indicates a higher degree of similarity in the statistical properties of these language datasets to the SANGRAHAVERIFIED training data. Conversely, we note that some languages, particularly those classified as low-medium resource, show more dispersed perplexity distributions.\\n\\nC Uncleanliness of Existing Corpora\\n\\nIssues with Language Identification\\n\\nThe evolution of Language Identification (LID) models has predominantly focused on European languages, leading to significant challenges in accurately identifying languages from diverse linguistic families, notably Indic languages. Kreutzer et al. (2022) highlights a significant concern regarding the mislabeling of languages in existing multilingual corpora, an issue that undermines the reliability of language identification (LID) models. In this small study, we analyze 200,000 documents per Indic language from the MC4 (Raffel et al., 2020) and OSCAR (Abadji et al., 2022) datasets, employing the INDIC LID model for its superior performance on Indic languages and support for Romanized text (Madhani et al., 2023a).\\n\\nMC4 uses only cld3 model whereas OSCAR defines an even stricter pipeline for identifying the language. It combines sentence-level LID and aggregates them based on certain thresholds to classify a document as multilingual or monolingual.\\n\\nOur analysis uncovers a significant discrepancy in the accuracy of LID across various Indic languages within the MC4 dataset. The languages sharing a common script, such as Hindi, Marathi, and Nepali, experience higher rates of mislabeling. This contrasts with languages with unique scripts showing significantly lower mismatch percentages. Conversely, the application of a more sophisticated LID methodology in the OSCAR dataset markedly diminishes these inaccuracies, showing the effectiveness of a refined approach to language identification. This observation demonstrates the necessity for the development of language family-specific identification models (Madhani et al., 2023a), as well as the incorporation of better LID modules within data-cleaning pipelines.\\n\\nAmount of Noise in Existing Corpora\\n\\nWe clean the entirety of CULTURA X and MADLAD-400 datasets using our Setu cleaning pipeline and show the drop in the number of words and documents across the stages. This helps us identify the type of noise present in these datasets. Figure 5 shows the drop in the number of tokens in these datasets respectively. We see a significant drop in both from Stage-1 to Stage-2 showing that a lot of noise in the form of Menu Items, Index...\"}"}
{"id": "acl-2024-long-843", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Log Perplexity distributions of Cleaned CULTURA and MADLAD-400 using 5-gram language models trained on SANGRAHAVERIFIED.\\n\\nFigure 9: % mismatch of the tagged language and the language predicted by INDIKLID lists, etc. must have crept in despite they being cleaned using their existing cleaning pipelines. We show a few examples of the kind of noisy text being filtered out in Figure 10.\\n\\nTable 21 shows the overall statistics of the CULTURA data filtered out at each stage in Setu.\\n\\nD Setu Data Cleaning Pipeline\\n\\nHere we discuss the inner details for each stage in the SETU. Our main goal for SETU is to open-source a distributed and cloud-agnostic data cleaning pipeline for large-scale datasets so that community is not stuck with any specific cloud provider or compute-scale. Using spark, we are able to achieve all the necessary requirements. Figure 11 shows the overview of the entire pipeline.\\n\\nD.1 Stage - 1: Document Preparation\\n\\nThis stage focuses on the extraction of text from varied data sources, ensuring the retention of main content while eliminating extraneous information and then preparing the notion of a document that is preserved throughout the pipeline. Due to the different modalities of content, this stage is different for each of Web, PDF, and Speech data.\\n\\nWeb Documents\\n\\nPreparation of the document for Web data is quite straightforward. We use trafilatura (Barbaresi, 2021a) to extract the text from the HTML pages that are scraped by webcorpus scraper. Although trafilatura is reportedly the best non-commercial library (Scrapinghub, 2021), we still notice a considerable amount of noise in the outputs, specifically in dynamic webpages. Figure 12 shows an example of noisy content extracted using trafilatura. In Web data, each webpage after text extraction is considered as a document.\"}"}
{"id": "acl-2024-long-843", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u0aac\u0abe\u0ab8\u0aad\u0ac7\u0ab5\u0ac7\u0ab0 \u0a93\u0aab \u0a9c\u0abe\u0aae\u0abe\u0a88 \u0a9c\u0ab2\u0abe \u0a86\u0a89\u0aab \u0ab8\u0ac7\u0aae \u0aa4\u0ac7\u0aa8 \u0aab\u0ab0\u0ac0 \u0aaa\u0abe\u0aa1\u0ac7 \u0a9c\u0abe\u0ab9\u0ac7\u0ab0\u0aa8\u0abe\u0aae\u0abe \u0aa8\u0ac7 \u0ab9\u0acb\u0ab5\u0abe \u0aa8\u0ab9\u0ac0\u0a82 \u0a86\u0ab5\u0aa4\u0abe \u0ab9\u0acb\u0ab5\u0abe\u0aa8\u0ac0 \u0aab\u0ab0\u0aaf\u0abe\u0aa6\u0acb, \u0aad\u0ab0\u0ac2\u0a9a\u0aa8\u0abe \u0a85\u0abf\u0aa7\u0a95 \u0a9c\u0ab2\u0acd\u0ab2\u0abe \u0aae\u0ac7\u0a9c\u0ac0\u0ab8\u0acd\u0a9f\u0acd\u0ab0\u0ac7\u0a9f\u0ac7 \u0aac\u0ab9\u0abe\u0ab0 \u0aaa\u0abe\u0aa1\u0acd\u0aaf\u0abe\u0a82 \u0a9c\u0abe\u0ab9\u0ac7\u0ab0\u0aa8\u0abe\u0aae\u0abe\u0a82.\\n\\n\u0aad\u0ab0\u0ac2\u0a9a\u0aa8\u0abe \u0a85\u0abf\u0aa7\u0a95 \u0ab9\u0abe\u0ab2\u0aae\u0abe\u0a82 \u0a86\u0ab5\u0ac0\u0aa8\u0ac7 \u0a95\u0ab0\u0ac0\u0aa8\u0ac7 \u0ab5\u0ac7\u0aaa\u0abe\u0ab0\u0ac0 \u0aa6\u0acd\u0ab5\u0abe\u0ab0\u0abe \u0ab0\u0abe\u0a9c\u0aa6\u0acd\u0ab0\u0acb\u0ab9\u0aa8\u0acb \u0a97\u0aa3\u0a85\u0abf\u09a7\u0a95\u0abe\u0ab0 \u0aa4\u0ab0\u0ac0\u0a95\u0ac7 \u0ab9\u0acb\u0ab5\u0abe\u0aae\u0abe\u0a82 \u0a86\u0ab5\u0ac0 \u0ab6\u0a95\u0ac7 \u0a9b\u0ac7.\\n\\n\u0ab6\u0ab9\u0ac7\u0ab0\u0ac0 \u0aaa\u0abe\u0ab8\u0ac7\u0aa5\u0ac0 \u0a95\u0acb\u0a88\u0aaa\u0aa3 \u0ab2\u0acb\u0a95\u0acb \u0ab8\u0abe\u0aae\u0ac7 \u0ab6\u0a95\u0ac7 \u0aaa\u0abe\u0aa1\u0aaf \u0a9b\u0ac7 \u0a85\u0aa8\u0ac7 \u0a86\u0aaa\u0ab5\u0abe\u0aae\u0abe\u0a82 \u0ab0\u0ac2\u0aaa\u0aaf\u0abe\u0aa8\u0abe \u0ab0\u0ac0\u0a95\u0ab6\u0abe\u0a9a\u0abe\u0ab2\u0a95\u0acb\u0aa8\u0ac7 \u0aad\u0ab0\u0ac2\u0a9a\u0aa8\u0abe \u0ab0\u0abe\u0a9c\u0aa6\u0acd\u0ab0\u0acb\u0ab9\u0aa8\u0acb \u0a97\u0aa3\u0a85\u0abf\u0aa7\u0a95\u0abe\u0ab0 \u0aa4\u0ab0\u0ac0\u0a95\u0ac7 \u0ab9\u0acb\u0ab5\u0abe\u0aae\u0abe\u0a82 \u0a86\u0ab5\u0ac0 \u0ab6\u0a95\u0ac7 \u0a9b\u0ac7.\\n\\n5 July 2021 10:31 AM GMT\\n\\nCultura-X: Uncleaned\\n\\nCultura-X: cleaned\\n\\nMADLAD: Uncleaned\\n\\nMADLAD: cleaned\\n\\nFigure 10: Examples of noisy content being filtered out using Setu from the already \\\"cleaned\\\" C\"}"}
{"id": "acl-2024-long-843", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sangraha\\nPreparation\\nCleaning & Analysis\\nFilter\\nDeduplication\\nPDF's\\nInternet...\\nthe application of...\\n\\nFigure 12: Example showing noisy content being extracted from the HTML using trafilatura\"}"}
{"id": "acl-2024-long-843", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Text Extraction from the OCR outputs from PDFs is not as straightforward as extracting text from a webpage. When utilizing Google Vision OCR for extracting text from PDF documents, the output is a structured JSON file that contains detailed information about the detected text. This information is organized hierarchically from larger text blocks down to individual characters. This hierarchical structure allows for a nuanced understanding of the document's layout and content. Broadly the bounding boxes are organized in the following hierarchy:\\n\\n- Block, Paragraph, Word, Character.\\n\\nA block is the highest level of structure and is a container for paragraphs grouped to reflect their spatial relationships. Paragraphs are subdivisions of blocks and represent cohesive units of text, typically separated from other units by new lines or indentation. Words are the basic units of text and meaning within a paragraph. Each word is identified and extracted as a separate entity in the OCR output. Characters are the most granular level of text extraction, representing individual letters, numbers, punctuation marks, and other textual symbols.\\n\\nEach category contains information such as the bounding box coordinates, confidence scores, language scores, and the text identified in that box. We observe that directly consuming the text from the OCR is not good as it contains a lot of noise coming in due to incorrect layout parsing. We also observed that due to the skewness and quality of images, we had multiple instances where we had bounding box overlaps, bounding box mismatch/misalignment, text overlaps, and language script mismatches. To resolve these and extract the highest quality text, we develop bounding-box based filters. We list the filters below:\\n\\n- **Bounding Box Suppression**: Here, we perform bounding box suppression, where we try to suppress the smaller bounding boxes that overlap with larger bounding boxes. For each pair of overlapping bounding boxes, we calculate the ratio of the area of intersection over the area of the smaller bounding box. We suppress the smaller bounding box if this ratio exceeds a chosen threshold. Figure 13a shows an example of a page where bounding box suppression is applied.\\n\\n- **Removing Horizontally Sparse Pages**: Here, we identify and remove pages that exhibit a significant lack of content across the horizontal span of the page. If a page has large horizontal gaps with little to no content\u2014indicating that the text or visual elements are spread thinly across the width of the page\u2014it is considered horizontally sparse. Such pages are often less informative or relevant, like index pages and table of contents among others. Figure 13b shows an example of a page flagged as horizontally sparse.\\n\\n- **Removing Vertically Sparse Pages**: Similarly, we also remove pages with insufficient content along the vertical axis. Pages containing large vertical gaps, such as excessive spacing between paragraphs or sections without meaningful content, are deemed vertically sparse. These pages are also less informative, like pages having publisher information, colophons, comic strips, etc. Figure 13c shows an example of a page flagged as vertically sparse.\\n\\n- **Removing Pages with High Overlapping Bounding Boxes**: Here, we remove the pages having a very high bounding box overlap percentage, i.e., greater than a chosen threshold as shown in Figure 13d.\\n\\n- **Removing Sparse Blocked Pages**: Here, we remove the pages having very sparse bounding boxes. A block bounding box is considered sparse if the difference between the total area of the block bounding box and the total area of paragraph bounding boxes enclosed in it is greater than a chosen threshold. By this, we remove pages with tables, large images, and forms among others.\\n\\n- **Removing Pages with Low Script Confidence**: Here, we compute each paragraph's average script confidence score on a given page. Paragraphs with scores below our confidence thresholds are flagged for potential exclusion. Subsequently, the entire page is discarded if the number of flagged paragraphs exceeds an allowable limit. This ensures a balance between rejecting poor-quality OCR output and retaining usable content.\\n\\nAfter filtering, we merge the final text extracted from the pages to form documents. To maintain textual continuity as well as to get as many long-form documents as possible, we concatenate the\"}"}
{"id": "acl-2024-long-843", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 15: Showing average page count of PDF documents after merge operation\\n\\nText of only consecutive batches of pages of a given PDF together. Table 15 shows the average number of pages per language that are merged to form a document.\\n\\nD.2 Document Cleaning and Analysis\\n\\nWe divide this stage into three sub-stages - Document Cleaning, Language Identification, and Analysis.\\n\\nDocument Cleaning\\n\\nAlthough trafilatura and GCP Vision OCR are reportedly the best (Scrapinghub, 2021; Dilmegani, 2023), we still need to mitigate the errors that creep in. We define the below filters that clean a document.\\n\\n\u2022 Code Span Removal: This filter is applied exclusively for Web Crawls where we define regex patterns to detect and remove code spans like improperly rendered HTML or JavaScript code.\\n\\n\u2022 Symbol Heavy Filter: Documents with a high ratio of invalid characters (e.g., punctuation, emojis and other symbols) to total characters, exceeding a predefined threshold, are discarded. Refer to the Figure 14 for an illustrative example.\\n\\n\u2022 Terminal Punctuation Filter: Exclusively for web crawls, it removes text segments lacking valid terminal punctuation, effectively filtering out clickbait text, menus, and incomplete sentences. See Figure 15 for an example of content removed using this filter.\\n\\n\u2022 Symbol Only Chunk Filter: This filter removes all the text chunks that have only the numbers or symbols.\\n\\n\u2022 Repeated Chunk filter: Applied to PDFs to eliminate repeated text chunks, targeting redundant headers and titles.\\n\\n\u2022 Chunk length filter: Specific to PDFs, it removes chunks with a word count below a set threshold.\\n\\nLanguage Identification\\n\\nTo address the issues of accuracy that may occur while relying on a singular model highlighted in Appendix C, we use an ensemble approach using three LID models - INDLID (Madhani et al., 2023a), CLD3, NLLB (Costa-juss\u00e0 et al., 2022). Notably, INDLID, which is specifically trained for Indic languages, is assigned a preference weighting in our ensemble framework. However, if both CLD3 and NLLB agree on a different language and are very confident about it (beyond a chosen threshold), we consider their prediction instead. This methodology aims to leverage the specialized capabilities of INDLID for Indic languages while still incorporating the complementary strengths of CLD3 and NLLB in other languages.\\n\\nDocument Analysis\\n\\nWe compute various document-specific statistics for subsequent filtering. The metrics and their descriptions are outlined in the Table 16.\\n\\nD.3 Flagging and Filtering\\n\\nFollowing the analysis, the documents are filtered based on predefined language-specific thresholds for the computed statistics. This step is essential to eliminate residual noise that might have survived the initial cleaning process. We include filters inspired from various previous works like ROOTS (Lauren\u00e7on et al., 2023), GOPHER (Rae et al., 2022) and C4 (Raffel et al., 2020) among a few.\\n\\n\u2022 NSFW word ratio filter: In an effort to reduce corpus toxicity, documents with a high ratio of NSFW (Not Safe For Work) words...\"}"}
{"id": "acl-2024-long-843", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Bounding Box Suppression: Page in which smaller bounding boxes are suppressed as these can lead to false flagging of pages or misaligned text.\\n\\nHorizontally Sparse: Page filtered out due to less horizontal text coverage, this can be indicative of very small lines, lists, index etc.\\n\\nVertically Sparse: Page filtered out due to less vertical text coverage. This can be indicative of title pages, comics, etc.\\n\\nHigh Bounding Box Overlap: Page filtered out due to high bounding box overlap. This high overlapping can lead to disordered parsing of text, break in continuity, etc.\\n\\nFigure 13: Illustrative examples of pages flagged in various bounding box filters.\"}"}
{"id": "acl-2024-long-843", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Product Type       | Installed Capacity | Production Quantity | Sales Quantity | Sales Value |\\n|-------------------|--------------------|---------------------|---------------|-------------|\\n| Marble & Granite  | -                  | NA                  | NA            | 68.63       |\\n| Sale of services  | -                  | NA                  | NA            | 1.08        |\\n| **Total**         | **69.71**          |                     |               |             |\\n\\nSource: Religare Technotrends\\n\\n---\\n\\n**Figure 14:** Document flagged by symbol heavy filter in the Stage-2.\\n\\n---\\n\\nLyell Immunopharma to use MaxCyte\u2019s Flow Electroporation\u00ae technology and ExPERT\u00ae platform in its T cell product candidates targeting solid tumors.\\n\\nROCKVILLE, Md., July 06, 2023 (GLOBE NEWSWIRE) \u2014 MaxCyte, Inc., (Nasdaq: MXCT; LSE: MXCT), a leading, cell-engineering focused company providing enabling platform technologies to advance the discovery, development and commercialization of next-generation cell-based therapeutics and to support innovative, cell-based research, today announced the signing of a strategic platform license (SPL) with Lyell Immunopharma, Inc., a clinical stage T cell reprogramming company.\\n\\nUnder the terms of the agreement, Lyell Immunopharma obtains non-exclusive clinical and commercial rights to use MaxCyte\u2019s Flow Electroporation\u00ae technology and ExPERT\u00ae platform. In return, MaxCyte is eligible to receive platform licensing fees, clinical milestone payments and sales-based payments.\\n\\n\u201cAt MaxCyte, our goal is to maximize the potential of cells to improve patients\u2019 lives, and it is through collaborations such as this that we can achieve success,\u201d said Doug Doerfler, President and CEO of MaxCyte. \u201cWe look forward to supporting Lyell Immunopharma in its development of solid tumor treatments for patients with unmet needs.\u201d\\n\\nMaxCyte\u2019s ExPERT\u00ae instrument portfolio is the next generation of leading, clinically-validated electroporation technology for complex and scalable cell engineering. By delivering high transfection efficiency, seamless scalability and enhanced functionality, the ExPERT\u00ae platform delivers the high-end performance essential to enabling the next wave of biological and cellular therapeutics. Each of MaxCyte\u2019s strategic partnerships generates pre-commercial milestone revenue and the vast majority include sales-based payments.\\n\\n**About MaxCyte**\\n\\nAt MaxCyte, we pursue cell engineering excellence to maximize the potential of cells to improve patients\u2019 lives. We have spent more than 20 years honing our expertise by building best-in-class platforms, perfecting the art of the transfection workflow, and venturing beyond today\u2019s processes to innovate tomorrow\u2019s solutions. Our ExPERT\u00ae platform, which is based on our Flow Electroporation\u00ae technology, has been designed to support the rapidly expanding cell therapy market and can be utilized across the continuum of the high-growth cell therapy sector, from discovery and development through commercialization of next-generation, cell-based medicines. The ExPERT family of products includes: four instruments, the ATx\u00ae:tm:, STx\u00ae:tm:, GTx\u00ae:tm: and VLx\u00ae:tm:; a portfolio of proprietary related processing assemblies or disposables; and software protocols, all supported by a robust worldwide intellectual property portfolio. By providing our partners with the right technology, as well as technical and regulatory support, we aim to guide them on their journey to transform human health. Learn more at maxcyte.com and follow us on Twitter and LinkedIn.\\n\\nMaxCyte Contacts:\\n\\n**US IR Adviser**\\nGilmartin Group\\nDavid Deuchler, CFA\\n+1 415\u2013937\u20135400\\nir@maxcyte.com\\n\\n**US Media Relations**\\nSpectrum Seismic Collaborative\\nValerie Enes\\n+1 408\u2013497\u20138568\\nvalerie@spectrumscience.com\\n\\n**Nominated Adviser and Joint Corporate Broker**\\nPanmure Gordon\\nEmma Earl / Freddy Crossley\\nCorporate Broking\\nRupert Dearden\\n+44 (0)20 7886 2500\\n\\n**UK IR Adviser**\\nConsilium Strategic Communications\\nMary-Jane Elliott\\nChris Welsh\\n+44 (0)203 709 5700\\nmaxcyte@consilium-comms.com\\n\\n---\\n\\n**Figure 15:** Cleaning performed by \u2018terminal punctuation filter\u2019 in Stage-2.\"}"}
{"id": "acl-2024-long-843", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Metrics Description                  |  |\\n|-------------------------------------|---|\\n| bytes                              | size of the document in terms of bytes, |\\n| word_count                         | no. of words present in a document |\\n| char_count                         | no. of characters present in a document |\\n| lines_count                        | total no. of sentences present in a document |\\n| mean_line_length                   | mean sentence length in terms of words of a document |\\n| min_line_length                    | minimum sentence length in terms of words of a document |\\n| max_line_length                    | maximum sentence length in terms of words of a document |\\n| nsfw_words_count                   | no. of NSFW words present in a document |\\n| non_li_character_count             | no. of non-Latin/non-Indic characters in a document |\\n| 10_gram_characters_repetition_score| score used for filtering documents using 10-gram character repetition filter |\\n| 5_gram_words_repetition_score      | score used for filtering documents using 5-gram word repetition filter |\\n\\nTable 16: Showing all the metrics that are calculated in the analysis stage.\"}"}
{"id": "acl-2024-long-843", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to total words are excluded. This approach aligns with that of INDIC CORP V 2, involving the development of an NSFW word list specifically tailored for Indic languages. This list is made available to the research community to encourage further studies.\\n\\n- Non Latin/Indic character ratio filter: Documents characterized by a significant ratio of non-Latin/Indic characters are removed. This filter eliminates content erroneously classified as Indic by the Language Identification (LID) stage. Figure 17 shows an example of the type of content removed by this filter.\\n\\n- Line count filter: Documents with an exceedingly low number of lines are discarded to remove potentially irrelevant or insufficient content.\\n\\n- Minimum mean line length filter: This filter targets documents with short average line lengths, effectively removing index pages and similar content deemed unsuitable for the corpus.\\n\\n- 5-gram word repetition: Inspired from ROOTS, we create a filter for the repetitions by looking at the occurrences of the 5-gram word sequences. We define the word repetition ratio as the ratio of the sum of the occurrences greater than or equal to the sum of all occurrences, and we discard documents with too high a ratio.\\n\\n- 10-gram character repetition: Similar to the word repetition filter, this criterion focuses on 10-gram character sequences. Documents exhibiting a high ratio of such repetitions are excluded, based on methodology inspired by ROOTS.\\n\\nD.4 Deduplication\\n\\nThe concluding stage of Setu addresses the critical task of deduplication using fuzzy deduplication. Following CULTUREX, we use the Python implementation of MinHashLSH from the text-dedup repository. We efficiently identify and remove duplicate documents within the corpus by utilizing 5-grams and a similarity threshold of 0.7, based on Jaccard similarity. This procedure is executed separately for each language, utilizing a computing node with 256 CPUs.\\n\\nE Curation of INDIC ALIGN - INSTRUCT\\n\\nE.1 Indo WordNet\\n\\nWordNets are a comprehensive lexical database originally designed for English (Fellbaum, 1998) and later extended to Indic Languages (Narayan et al., 2002; Bhattacharyya, 2010). It organizes words into sets of synonyms called synsets, providing short definitions and usage examples. Beyond mere dictionaries, WordNet also captures the various semantic relationships between words. We leverage this rich semantic information to create instruction fine-tuning data to teach the model grammar and language creativity.\\n\\nWe first identify a list of 21 potential intents encompassing tasks such as Part of Speech identification, sentence construction, and synonym discovery. We craft 5 prompt-response templates for each intent, resulting in a repository of 105 distinct templates. Then we iterate through the lexicon in IndoWordNet using pyiwn (Panjwani et al., 2018), randomly sampling 100 templates for each word yielding around 74M pairs for 18 Indic languages. Figure 18 shows some examples of templates. Table 17 shows each language\u2019s final statistics of the prompt-answer pairs.\\n\\nE.2 Anudesh\\n\\nHere, we introduce a novel dataset of real user interactions with conversational models, leveraging open, license-compatible models such as Llama-70B CHat (Touvron et al., 2023b). Recognizing the limitations imposed by OpenAI\u2019s terms of use on existing crowd-sourced model interaction datasets, such as SHAREGPT and WILD-CHAT (Zhao et al., 2024), our dataset aims to provide a resource, free from such constraints, thereby facilitating broader applicability in training diverse conversational models.\\n\\nWe create Anudesh by asking the user to interact with the model while following an instruction displayed on the screen. Occasionally, we allow unrestricted interactions to collect more diverse and creative prompts. Each displayed instruction is based on three axes that guide the user -\\n\\n- Intent - Defines the purpose and goal behind\"}"}
{"id": "acl-2024-long-843", "page_num": 31, "content": "{\"primary_language\":\"hi\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u0906\u0907\u090f \u0926\u094b \u0938\u0902\u0916\u094d\u092f\u093e\u0913\u0902 16 x 77 \u0915\u093e \u0939\u0932 \u0928\u0915\u093e\u0932\u0924\u0947 \u0939\u0948\u0902\u0964 \u0907\u0938 \u091f\u094d\u0930\u093f\u0915 \u092e\u0947\u0902, \u0939\u092e \u092a\u0939\u0932\u093e \u0928\u0902\u092c\u0930 \u0909\u0920\u093e\u0915\u0930 \u0936\u0941\u0930\u0941\u0906\u0924 \u0915\u0930\u0947\u0902\u0917\u0947\u0964 16 \u090f\u0915 \u0938\u092e \u0938\u0902\u0916\u094d\u092f\u093e \u0939\u0948, \u0907\u0938\u0947 \u0906\u0927\u0947 \u092e\u0947\u0902 \u0935\u093f\u092d\u093e\u091c\u093f\u0924 \u0915\u0930\u0947\u0902 \u0914\u0930 \u0939\u092e\u0947\u0902 \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0939\u094b\u0924\u093e \u0939\u0948, \\\\( 16/2 = 8 \\\\)\u0964\\n\\n\u0905\u092c, \u0926\u0942\u0938\u0930\u0940 \u0938\u0902\u0916\u094d\u092f\u093e \u0915\u094b \u0926\u094b\u0917\u0941\u0928\u093e \u0915\u0930\u0947\u0902 \u092f\u093e\u0928\u0940 \\n\\\\[ 77 \\\\times 2 = 154 \\\\]\\n\\n\u0905\u092a\u0928\u0947 \u0905\u0902\u0924\u093f\u092e \u0909\u0924\u094d\u0924\u0930 \u0915\u0947 \u0932\u093f\u090f, \u0906\u092a \u092a\u0930\u0923\u093e\u092e\u0940 \u0938\u0902\u0916\u094d\u092f\u093e\u0913\u0902 \u0915\u094b \u0906\u0938\u093e\u0928\u0940 \u0938\u0947 \u0917\u0941\u0923\u093e \u0938\u0915\u0924\u0947 \u0939\u0948\u0902, \u0905\u0925\u0935\u093e \\n\\\\[ 154 \\\\times 8 = 1232 \\\\]\\n\\n\u092a\u094d\u0930\u0936\u094d\u0928 42 x 49 \u0915\u093e \u0909\u0924\u094d\u0924\u0930 \u0916\u094b\u091c\u0947\u0902\u0964\\n\u0926\u0939\u093e\u0908 \u0915\u093e \u0905\u0902\u0915 \u0932\u0947\u0902 \u0914\u0930 \u0907\u0938\u0947 \u0905\u0917\u0932\u0940 \u0938\u092c\u0938\u0947 \u092c\u0921\u093c\u0940 \u0938\u0902\u0916\u094d\u092f\u093e, \u092f\u093e\u0928\u0940 \\n\\\\[ 4 \\\\times 5 = 30 \\\\]\\n\\n\u0938\u0947 \u0917\u0941\u0923\u093e \u0915\u0930\u0947\u0902\u0964\\n\u0907\u0938\u0915\u0947 \u092c\u093e\u0926 \u0926\u094b\u0928\u094b\u0902 \u0915\u0947 \u090f\u0915 \u0905\u0902\u0915 \u0915\u094b \u0917\u0941\u0923\u093e \u0915\u0930\u0947\u0902\u0964\\n\\\\[ 2 \\\\times 8 = 16 \\\\]\\n\\n\u0906\u0907\u090f \u0926\u094b\u0928\u094b\u0902 \u0905\u0902\u0915\u094b\u0902 \u0915\u094b \u090f\u0915 \u0938\u093e\u0925 \u0930\u0916\u0947\u0902 \u0914\u0930 \u0909\u0924\u094d\u0924\u0930 3016 \u0939\u094b\u0917\u093e\u0964\\n\\n9 \u0938\u0947 \u0935\u093f\u092d\u093e\u091c\u094d\u092f\u0924\u093e \u091c\u093e\u0902\u091a\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u092f\u0926 \u0915\u094b\u0908 \u0938\u0902\u0916\u094d\u092f\u093e \u0915\u0947 \u0938\u092d\u0940 \u0905\u0902\u0915\u094b\u0902 \u0915\u093e \u0915\u0941\u0932 \u092f\u094b\u0917 9 \u0938\u0947 \u0935\u093f\u092d\u093e\u091c\u094d\u092f \u0939\u0948\u0964\\n'4 \u0938\u0947 \u0935\u093f\u092d\u093e\u091c\u094d\u092f\u0924\u093e \u091c\u093e\u0902\u091a\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f' \u092f\u0939 \u0928\u0927\u093e\u0930\u094d\u0917\u0930\u0924 \u0915\u0930\u0928\u0947 \u0915\u0947 \u0932\u093f\u090f \u0915\u094b\u0908 \u0938\u0902\u0916\u094d\u092f\u093e 4 \u0938\u0947 \u0935\u093f\u092d\u093e\u091c\u094d\u092f \u0939\u0948 \u092f\u093e \u0928\u0939\u0940\u0902, \u0939\u092e\u0947\u0902 \u0909\u0938\u0915\u0947 \u0905\u0902\u0924\u093f\u092e 2 \u0905\u0902\u0915\u094b\u0902 \u0915\u093e \u0935\u0948\u0936\u094d\u0932\u0947\u0937\u0923 \u0915\u0930\u0928\u093e \u0939\u094b\u0917\u093e\u0964\\n\u092f\u0926 \u0935\u0947 4 \u0938\u0947 \u0935\u093f\u092d\u093e\u091c\u094d\u092f \u0939\u0948\u0902, \u0924\u094b \u092a\u0942\u0930\u0940 \u0938\u0902\u0916\u094d\u092f\u093e 4 \u0938\u0947 \u0935\u093f\u092d\u093e\u091c\u094d\u092f \u0939\u094b\u0917\u0940\u0964\\n\\n\u0906\u0907\u090f \u0938\u0902\u0916\u094d\u092f\u093e 685 \u0915\u0940 \u0915\u0932\u094d\u092a\u0928\u093e \u0915\u0930\u0947\u0902 \u0914\u0930 \u0939\u092e\u0947\u0902 \u0907\u0938\u0915\u0947 5% \u0915\u0940 \u0917\u0923\u0928\u093e \u0915\u0930\u0928\u0940 \u0939\u0948\u0964\\n\u0924\u094b, \u0939\u092e\u0947\u0902 \u0915\u094d\u092f\u093e \u0915\u0930\u0928\u093e \u0939\u0948, \u0905\u0902\u0915 685 \u0915\u093e \u0926\u0936\u092e\u0932\u0935 685.0 \u091c\u0948\u0938\u093e \u0939\u094b\u0917\u093e \u0906\u0907\u090f \u0926\u0936\u092e\u0932\u0935 \u0915\u094b \u090f\u0915 \u0938\u094d\u0925\u093e\u0928 \u0906\u0917\u0947 \u092c\u0922\u093c\u093e\u090f\u0902,\\n\\n\\\\[ 68.5 \\\\]\\n\\n\u0905\u092c \u0939\u092e\u0947\u0902 \u0938\u0902\u0916\u094d\u092f\u093e 68.5 \u0915\u094b 2 \u0938\u0947 \u0935\u093f\u092d\u093e\u091c\u093f\u0924 \u0915\u0930\u0928\u093e \u0939\u0948, \u0939\u092e\u0947\u0902 \u092e\u0932\u0924\u093e \u0939\u0948, 34.25\u0964\\n\u0907\u0938 \u092a\u094d\u0930\u0915\u093e\u0930, 685 \u0915\u093e 5% 34.25 \u0939\u0948\u0964\"}"}
{"id": "acl-2024-long-843", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Can you explain the common meaning of {word}?\\nThe common meaning of {word} is: {answer}.\"}"}
