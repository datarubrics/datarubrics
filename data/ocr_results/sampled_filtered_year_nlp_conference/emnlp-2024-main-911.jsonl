{"id": "emnlp-2024-main-911", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | TOTAL Errors | Detected Errors | Undetected Errors | % Undetected Errors |\\n|-------------------|--------------|-----------------|------------------|--------------------|\\n| LF COHERENCE      | 91           | 45              | 46               | 0.51               |\\n| COMPREHENSIVENESS | 90           | 0               | 90               | 1.00               |\\n| CONSISTENCY       | 84           | 6               | 78               | 0.93               |\\n| GRAMMAR           | 92           | 16              | 76               | 0.83               |\\n| CHRONOLOGY        | 71           | 0               | 71               | 1.00               |\\n| SPELLING          | 100          | 7               | 93               | 0.93               |\\n| CONTEXTUAL        | 94           | 28              | 66               | 0.70               |\\n| ENTITY            | 87           | 27              | 60               | 0.69               |\\n| INCORRECT FACT     | 68           | 15              | 53               | 0.78               |\\n| NUMBER ERRORS      | 74           | 15              | 59               | 0.80               |\\n| OPPOSITE FACT      | 91           | 28              | 63               | 0.69               |\\n| REMOVE FACT        | 69           | 1               | 68               | 0.99               |\\n| ASSUMPTIONS        | 81           | 2               | 79               | 0.98               |\\n| DOLESS            | 100          | 17              | 83               | 0.83               |\\n| DOMORE            | 50           | 39              | 11               | 0.22               |\\n| IGNORE FORMAT      | 99           | 24              | 75               | 0.76               |\\n| SEQUENCE ERRORS    | 49           | 4               | 45               | 0.92               |\\n| CALCULATIONS       | 149          | 97              | 52               | 0.35               |\\n| COPYING NUMBERS    | 83           | 58              | 25               | 0.30               |\\n| FINAL ERRORS       | 97           | 48              | 49               | 0.51               |\\n| INCORRECT UNITS    | 77           | 44              | 33               | 0.43               |\\n| WRONG FORMULA      | 88           | 63              | 25               | 0.37               |\\n\\nTable 10: Results from evaluating FBI using Axis+Rubrics evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | TOTAL Errors | G | P | Both \u2713 | Both \u2717 | \u0338= | % Undetected Errors |\\n|-------------------|--------------|---|---|--------|--------|----|-------------------|\\n| LF                | 528          | 141| 0 | 318    | 0      | 69 | 0.73              |\\n| COHERENCE         |              | 91 | 73| 0      | 11     | 0  | 7                 |\\n| COMPREHENSIVENESS | 90           | 11 | 0 | 57     | 0      | 22 | 0.88              |\\n| CONSISTENCY       | 84           | 12 | 0 | 59     | 0      | 13 | 0.86              |\\n| GRAMMAR           | 92           | 32 | 0 | 46     | 0      | 14 | 0.65              |\\n| CHRONOLOGY        | 71           | 1  | 0 | 68     | 0      | 2  | 0.99              |\\n| SPELLING          | 100          | 12 | 0 | 77     | 0      | 11 | 0.88              |\\n| TOTAL             | 16296        | 234| 1 | 116    | 0      | 132| 0.52              |\\n| CONTEXTUAL        | 94           | 55 | 0 | 12     | 0      | 27 | 0.41              |\\n| ENTITY            | 87           | 51 | 0 | 16     | 0      | 20 | 0.41              |\\n| INCORRECT FACT     | 68           | 32 | 0 | 12     | 0      | 24 | 0.53              |\\n| NUMBER ERRORS      | 74           | 29 | 1 | 22     | 0      | 22 | 0.61              |\\n| OPPOSITE FACT      | 91           | 55 | 0 | 12     | 0      | 24 | 0.40              |\\n| REMOVE FACT        | 69           | 12 | 0 | 42     | 0      | 15 | 0.83              |\\n| TOTAL             | 16296        | 234| 1 | 116    | 0      | 132| 0.52              |\\n| ASSUMPTIONS       | 81           | 6  | 25| 34     | 0      | 16 | 0.93              |\\n| DOLESS            | 100          | 40 | 0 | 22     | 0      | 38 | 0.60              |\\n| DORESP            | 50           | 7  | 1 | 17     | 0      | 25 | 0.86              |\\n| IGNORE FORMAT     | 99           | 13 | 0 | 56     | 0      | 30 | 0.87              |\\n| SEQUENCE ERRORS   | 49           | 0  | 0 | 49     | 0      | 0  | 1.00              |\\n| TOTAL             | 16296        | 234| 1 | 116    | 0      | 132| 0.52              |\\n| CALCULATIONS      | 149          | 96 | 1 | 18     | 1      | 32 | 0.35              |\\n| COPYING NUMBERS   | 83           | 58 | 0 | 7      | 1      | 17 | 0.30              |\\n| FINAL ERRORS      | 97           | 58 | 1 | 6      | 0      | 32 | 0.40              |\\n| INCORRECT UNITS   | 77           | 48 | 0 | 17     | 1      | 11 | 0.38              |\\n| WRONG FORMULA     | 88           | 56 | 1 | 15     | 3      | 13 | 0.36              |\\n| TOTAL             | 16296        | 234| 1 | 116    | 0      | 132| 0.52              |\\n\\nTable I: Results from evaluating FBI using the Pairwise\u2217 evaluator. An error is said to be detected if the evaluator chooses the Gold Answer. G indicates the number of times the evaluator has chosen the Gold Answer, P for the Perturbed Answer, Both \u2713 when both answers are correct, Both \u2717 when both are incorrect, and \u0338= for verdict inconsistencies.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | TOTAL Errors | G | P | Both \u2713 | Both \u2717 | \u0338= | % Undetected Errors |\\n|-------------------|---------------|---|---|--------|--------|----|---------------------|\\n| LF coherence      | 91 69 0 2 0 18 | 0.22 |   |        |        |    |                     |\\n| Coherence         | 90 25 0 18 0 47 | 0.72 |   |        |        |    |                     |\\n| Consistency       | 84 10 0 40 0 33 | 0.88 |   |        |        |    |                     |\\n| Grammar           | 92 12 0 24 0 54 | 0.87 |   |        |        |    |                     |\\n| Chronology        | 71 0 0 50 0 19 | 1.00 |   |        |        |    |                     |\\n| Spelling          | 100 5 0 56 0 38 | 0.95 |   |        |        |    |                     |\\n| Total             | 528 121 0 190 0 209 | 0.77 |   |        |        |    |                     |\\n| Context           | 94 76 0 5 0 13 | 0.19 |   |        |        |    |                     |\\n| Entity            | 87 44 0 11 0 28 | 0.47 |   |        |        |    |                     |\\n| Incorrect fact    | 68 36 0 3 0 27 | 0.45 |   |        |        |    |                     |\\n| Number errors     | 74 34 0 9 0 28 | 0.52 |   |        |        |    |                     |\\n| Opposite fact     | 91 39 0 3 0 48 | 0.57 |   |        |        |    |                     |\\n| Remove fact       | 69 24 0 16 0 28 | 0.65 |   |        |        |    |                     |\\n| Total             | 483 253 0 47 0 172 | 0.46 |   |        |        |    |                     |\\n| Assumptions       | 81 4 43 3 0 31 | 0.95 |   |        |        |    |                     |\\n| Gloss            | 100 58 0 11 0 30 | 0.41 |   |        |        |    |                     |\\n| More             | 50 24 2 0 0 24 | 0.52 |   |        |        |    |                     |\\n| Ignore format     | 99 35 0 27 0 23 | 0.59 |   |        |        |    |                     |\\n| Sequence errors   | 49 0 0 23 0 26 | 1.00 |   |        |        |    |                     |\\n| Total             | 379 121 45 64 0 134 | 0.67 |   |        |        |    |                     |\\n| Calculations      | 149 77 0 6 1 38 | 0.37 |   |        |        |    |                     |\\n| Copying numbers   | 83 40 0 1 1 18 | 0.33 |   |        |        |    |                     |\\n| Final errors      | 97 59 0 0 0 18 | 0.23 |   |        |        |    |                     |\\n| Incorrect units   | 77 38 0 7 0 20 | 0.42 |   |        |        |    |                     |\\n| Wrong formula     | 88 39 0 4 1 23 | 0.42 |   |        |        |    |                     |\\n| Total             | 494 253 18 3 117 | 0.35 |   |        |        |    |                     |\\n\\nTable 12: Results from evaluating FBI using the Pairwise evaluator. An error is said to be detected if the evaluator chooses the Gold Answer. G indicates the number of times the evaluator has chosen the Gold Answer, P for the Perturbed Answer, Both \u2713 when both answers are correct, Both \u2717 when both are incorrect, and \u0338= for verdict inconsistencies.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | TOTAL | Errors | G | P | Both | \u2713 | Both | \u2717 | \u0338= | % Undetected Errors |\\n|------------------|-------|--------|---|---|------|---|------|---|---|---------------------|\\n| COHERENCE        | 91    | 82     | 0 | 2 | 0    | 7 | 0    | 0 | 0 | 100.00              |\\n| COMPREHENSIVENESS| 90    | 28     | 0 | 25| 0    | 37| 0    | 0 | 0 | 100.00              |\\n| CONSISTENCY      | 84    | 10     | 0 | 46| 0    | 28| 0    | 0 | 0 | 100.00              |\\n| GRAMMAR          | 92    | 8      | 0 | 24| 0    | 60| 0    | 0 | 0 | 100.00              |\\n| CHRONOLOGY       | 71    | 0      | 0 | 51| 0    | 20| 0    | 0 | 0 | 100.00              |\\n| SPELLING         | 100   | 4      | 0 | 48| 0    | 48| 0    | 0 | 0 | 100.00              |\\n| TOTAL            | 528   | 132    | 0 | 196| 0    | 200| 0    | 0 | 0 | 100.00              |\\n| CONTEXTUAL       | 94    | 36     | 0 | 9 | 0    | 48| 0    | 0 | 0 | 100.00              |\\n| ENTITY           | 87    | 37     | 0 | 14| 0    | 34| 0    | 0 | 0 | 100.00              |\\n| INCORRECT FACT    | 68    | 27     | 0 | 4 | 0    | 36| 0    | 0 | 0 | 100.00              |\\n| NUMBER ERRORS     | 74    | 27     | 0 | 13| 0    | 32| 0    | 0 | 0 | 100.00              |\\n| OPPOSITE FACT     | 91    | 32     | 0 | 6 | 0    | 53| 0    | 0 | 0 | 100.00              |\\n| REMOVE FACT       | 69    | 19     | 0 | 18| 0    | 32| 0    | 0 | 0 | 100.00              |\\n| TOTAL            | 483   | 178    | 0 | 64| 0    | 235| 0    | 0 | 0 | 100.00              |\\n| ASSUMPTIONS      | 81    | 3      | 57| 5 | 0    | 16| 0    | 0 | 0 | 100.00              |\\n| DOLESS           | 100   | 60     | 2 | 15| 0    | 23| 0    | 0 | 0 | 100.00              |\\n| DOWHORE          | 50    | 25     | 3 | 0 | 0    | 22| 0    | 0 | 0 | 100.00              |\\n| IGNORE FORMAT    | 99    | 33     | 0 | 29| 0    | 37| 0    | 0 | 0 | 100.00              |\\n| SEQUENCE ERRORS   | 49    | 1      | 0 | 24| 0    | 24| 0    | 0 | 0 | 100.00              |\\n| TOTAL            | 379   | 122    | 62| 73| 0    | 122| 0    | 0 | 0 | 100.00              |\\n| RALCULATIONS     | 149   | 82     | 1 | 12| 0    | 46| 0    | 0 | 0 | 100.00              |\\n| COPYING NUMBERS   | 83    | 55     | 0 | 6 | 0    | 18| 0    | 0 | 0 | 100.00              |\\n| FINAL ERRORS     | 97    | 47     | 1 | 0 | 0    | 42| 0    | 0 | 0 | 100.00              |\\n| INCORRECT UNITS  | 77    | 47     | 0 | 10| 1    | 19| 0    | 0 | 0 | 100.00              |\\n| WRONG FORMULA    | 88    | 46     | 1 | 7 | 0    | 27| 0    | 0 | 0 | 100.00              |\\n| TOTAL            | 494   | 277    | 3 | 35| 1    | 152| 0    | 0 | 0 | 100.00              |\\n\\nTable 13: Results from evaluating FBI using the Rules evaluator. An error is said to be detected if the evaluator chooses the Gold Answer. G indicates the number of times the evaluator has chosen the Gold Answer, P for the Perturbed Answer, Both \u2713 when both answers are correct, Both \u2717 when both are incorrect, and \u0338= for verdict inconsistencies.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | TOTAL Errors | G (\u2713) | P (\u2717) | Both (\u2713) | Both (\u2717) | \u0338= | % Undetected |\\n|------------------|--------------|-------|-------|----------|----------|----|-------------|\\n| COHERENCE        | 91           | 82    | 0     | 1        | 0        | 8  | 0.10        |\\n| COMPREHENSIVENESS| 90           | 49    | 0     | 9        | 0        | 32 | 0.46        |\\n| CONSISTENCY      | 84           | 16    | 0     | 50       | 0        | 18 | 0.81        |\\n| GRAMMAR          | 92           | 34    | 0     | 26       | 0        | 32 | 0.63        |\\n| CHRONOLOGY       | 71           | 0     | 0     | 57       | 0        | 14 | 1.00        |\\n| SPELLING         | 100          | 11    | 0     | 58       | 0        | 31 | 0.89        |\\n| CONTEXTUAL       | 94           | 60    | 0     | 8        | 0        | 26 | 0.36        |\\n| ENTITY           | 87           | 60    | 0     | 11       | 0        | 16 | 0.31        |\\n| INCORRECT FACT   | 68           | 41    | 0     | 4        | 0        | 23 | 0.40        |\\n| NUMBER ERRORS    | 74           | 45    | 0     | 10       | 0        | 19 | 0.39        |\\n| OPPOSITE FACT    | 91           | 61    | 0     | 7        | 0        | 23 | 0.33        |\\n| REMOVE FACT      | 69           | 5     | 0     | 58       | 0        | 6  | 0.93        |\\n| TOTAL            | 483          | 272   | 0     | 98       | 0        | 113 | 0.44      |\\n| ASSUMPTIONS      | 81           | 2     | 62    | 4        | 0        | 13 | 0.98        |\\n| LESS             | 100          | 57    | 0     | 11       | 0        | 32 | 0.43        |\\n| MORE             | 50           | 40    | 2     | 3        | 0        | 5  | 0.20        |\\n| IGNORE FORMAT    | 99           | 53    | 0     | 13       | 0        | 33 | 0.46        |\\n| SEQUENCE ERRORS  | 49           | 5     | 0     | 23       | 0        | 21 | 0.90        |\\n| TOTAL            | 379          | 157   | 64    | 54       | 0        | 104 | 0.59      |\\n| CALCULATIONS     | 149          | 108   | 1     | 16       | 0        | 23 | 0.27        |\\n| COPYING NUMBERS  | 83           | 69    | 1     | 7        | 0        | 6  | 0.17        |\\n| FINAL ERRORS     | 97           | 75    | 1     | 2        | 0        | 19 | 0.23        |\\n| INCORRECT UNITS  | 77           | 42    | 0     | 20       | 0        | 15 | 0.45        |\\n| WRONG FORMULA    | 88           | 64    | 0     | 12       | 0        | 12 | 0.27        |\\n| TOTAL            | 494          | 358   | 3     | 57       | 0        | 75  | 0.27      |\\n\\nTable 14: Results from evaluating FBI using the Axis evaluator. An error is said to be detected if the evaluator chooses the Gold Answer. G indicates the number of times the evaluator has chosen the Gold Answer, P for the Perturbed Answer, Both \u2713 when both answers are correct, Both \u2717 when both are incorrect, and \u0338= for verdict inconsistencies.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 15: Results from evaluating FBI using the Axis+Rules evaluator. An error is said to be detected if the evaluator chooses the Gold Answer. G indicates the number of times the evaluator has chosen the Gold Answer, P for the Perturbed Answer, Both \u2713 when both answers are correct, Both \u2717 when both are incorrect, and \u0338= for verdict inconsistencies.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 16: Results from evaluating FBI using the Reference evaluator. An error is said to be detected if the evaluator gives a perfect score of 10 to the perturbed answer. \\n\\n| Perturbation Type | TOTAL | Errors | 10 | 9 | 8 | <8 | % Undetected Errors |\\n|-------------------|-------|--------|----|---|---|-----|---------------------|\\n| LF                |       |        |    |   |   |     |                     |\\n| COHERENCE         | 91    | 2      | 5  |   | 9 | 75  | 0.02                |\\n| COMPREHENSIVENESS | 90    | 32     | 39 | 6 | 13| 0.36 |                     |\\n| CONSISTENCY       | 84    | 31     | 27 | 4 | 22| 0.37 |                     |\\n| GRAMMAR           | 92    | 10     | 51 | 9 | 22| 0.11 |                     |\\n| CHRONOLOGY        | 71    | 47     | 21 | 2 | 1 | 0.66 |                     |\\n| SPELLING          | 100   | 14     | 72 | 4 | 10| 0.14 |                     |\\n| CONTEXTUAL        | 94    | 1      | 27 | 11| 55| 0.01 |                     |\\n| ENTITY            | 87    | 8      | 16 | 16| 47| 0.09 |                     |\\n| INCORRECT FACT     | 68    | 2      | 15 | 12| 39| 0.03 |                     |\\n| NUMBER ERRORS      | 74    | 6      | 21 | 15| 32| 0.08 |                     |\\n| OPPOSITE FACT      | 91    | 0      | 11 | 4 | 76| 0.00 |                     |\\n| REMOVE FACT        | 69    | 36     | 18 | 10| 5 | 0.52 |                     |\\n| REMOVED ERRORS     | 483   | 53     | 108| 68| 254| 0.11 |                     |\\n| ASSUMPTIONS        | 81    | 50     | 17 | 4 | 10| 0.62 |                     |\\n| DOLESS            | 100   | 32     | 6  | 15| 47| 0.32 |                     |\\n| DOLLERE            | 50    | 22     | 10 | 12| 6 | 0.44 |                     |\\n| IGNORE FORMAT      | 99    | 43     | 18 | 7 | 30| 0.43 |                     |\\n| SEQUENCE ERRORS    | 49    | 39     | 8  | 2 | 0 | 0.80 |                     |\\n| REMOVED ERRORS     | 379   | 186    | 59 | 40| 93 | 0.49 |                     |\\n| CALCULATIONS       | 149   | 6      | 6  | 6 | 131| 0.04 |                     |\\n| COPYING NUMBERS    | 83    | 4      | 4  | 3 | 72 | 0.05 |                     |\\n| FINAL ERRORS       | 97    | 1      | 2  | 4 | 89 | 0.01 |                     |\\n| INCORRECT UNITS    | 77    | 10     | 10 | 4 | 53 | 0.13 |                     |\\n| WRONG FORMULA      | 88    | 1      | 12 | 1 | 74 | 0.01 |                     |\"}"}
{"id": "emnlp-2024-main-911", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | # Errs | 5 | 4 | <4 | % Errors 5 | 4 | <4 | % Errors |\\n|-------------------|-------|---|---|-----|------------|---|----|---------|\\n| LF                |       |   |   |     |            |   |     |         |\\n| COHERENCE         | 91    | 9 | 33| 49 | 0.10      | 17| 18 | 56      |\\n| COMPREHENSIVENESS | 90    | 40| 42| 8  | 0.44      | 40| 46 | 4       |\\n| CONSISTENCY       | 84    | 36| 36| 12 | 0.43      | 50| 26 | 8       |\\n| GRAMMAR           | 92    | 44| 39| 9  | 0.48      | 56| 31 | 5       |\\n| CHRONOLOGY        | 71    | 43| 24| 4  | 0.61      | 42| 23 | 6       |\\n| SPELLING          | 100   | 46| 49| 5  | 0.46      | 65| 28 | 7       |\\n| TOTAL             | 528   | 218|223|87 | 0.41      | 270|172|86      |\\n| FC               |       |   |   |     |            |   |     |         |\\n| CONTEXTUAL        | 94    | 46| 39| 9  | 0.49      | 56| 25 | 13      |\\n| ENTITY            | 87    | 34| 41| 12 | 0.39      | 51| 22 | 14      |\\n| INCORRECT ACT      | 68    | 29| 30| 9  | 0.43      | 45| 18 | 5       |\\n| NUMBER ERRORS     | 74    | 36| 32| 6  | 0.49      | 47| 18 | 9       |\\n| OPPOSITE ACT      | 91    | 37| 41| 13 | 0.41      | 52| 28 | 11      |\\n| REMOVE ACT        | 69    | 41| 27| 1  | 0.59      | 50| 18 | 1       |\\n| TOTAL             | 483   | 223|210|50 | 0.46      | 301|129|53      |\\n| IF               |       |   |   |     |            |   |     |         |\\n| ASSUMPTIONS       | 81    | 38| 41| 2  | 0.47      | 56| 25 | 0       |\\n| DOLESS            | 100   | 53| 44| 3  | 0.53      | 54| 44 | 2       |\\n| DO MORE           | 50    | 17| 24| 9  | 0.34      | 16| 28 | 6       |\\n| IGNORE FORMAT     | 99    | 49| 43| 7  | 0.49      | 53| 35 | 11      |\\n| SEQUENCE ERRORS   | 49    | 18| 28| 3  | 0.37      | 21| 21 | 7       |\\n| TOTAL             | 379   | 175|180|24 | 0.46      | 200|153|26      |\\n| RC               |       |   |   |     |            |   |     |         |\\n| CALCULATIONS      | 149   | 23| 67| 59 | 0.15      | 14| 75 | 60      |\\n| COPYING NUMBERS   | 83    | 11| 38| 34 | 0.13      | 9 | 42 | 32      |\\n| FINAL ERRORS      | 97    | 22| 46| 29 | 0.23      | 10| 54 | 33      |\\n| INCORRECT UNITS   | 77    | 16| 23| 38 | 0.21      | 8 | 34 | 35      |\\n| WRONG FORMULA     | 88    | 17| 48| 23 | 0.19      | 17| 38 | 33      |\\n| TOTAL             | 494   | 89 |222|183|0.18      | 58 |243 |193     |\\n\\nTable 17: Results from evaluating FBI using the Prometheus evaluator. An error is said to be detected if the evaluator gives a perfect score of 5 to the perturbed answer. 5 indicates the number of times the evaluator has given the score of 5, 4 for the score of 4, and <4 for scores less than 4. Generic indicates evaluating with general scoring rubrics and Specific indicates evaluating with task-specific rubrics.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | # Errs | # DE | # UE | % UE | # DE | # UE | % UE | # DE | # UE | % UE |\\n|-------------------|-------|-----|-----|------|------|-----|-----|------|------|-----|------|\\n| LF                | 61    | 21  | 0.29| 18   | 0.20 | 8   | 0.09| 83   | 8    | 0.09| 72   |\\n| coherence         | 90    | 22  | 0.82| 71   | 0.79 | 60  | 0.67| 29   | 60   | 0.67| 19   |\\n| comprehensiveness | 84    | 9   | 1.00| 65   | 0.78 | 59  | 0.65| 29   | 55   | 0.65| 18   |\\n| grammar           | 92    | 8   | 0.95| 80   | 0.87 | 80  | 0.87| 29   | 63   | 0.68| 12   |\\n| chronology        | 71    | 1   | 1.15| 60   | 0.91 | 64  | 0.91| 18   | 53   | 0.75| 6   |\\n| spelling          | 100   | 7   | 1.01| 85   | 1.00 | 80  | 0.92| 18   | 82   | 0.82| 13   |\\n| total             | 528   | 108 | 370 | 0.86 | 140 | 378 | 0.74| 206  | 321  | 0.61| 92   |\\n| contextual        | 94    | 5   | 1.03| 82   | 1.03 | 80  | 0.85| 28   | 66   | 0.70| 14   |\\n| entity            | 87    | 13  | 0.94| 64   | 0.94 | 65  | 0.75| 32   | 55   | 0.63| 22   |\\n| incorrect fact    | 68    | 6   | 0.95| 61   | 0.95 | 55  | 0.85| 15   | 53   | 0.78| 10   |\\n| number errors     | 74    | 6   | 1.00| 61   | 1.00 | 57  | 0.89| 11   | 63   | 0.85| 8   |\\n| opposite fact     | 91    | 10  | 0.96| 74   | 0.96 | 74  | 0.81| 32   | 59   | 0.65| 17   |\\n| remove fact       | 69    | 18  | 0.74| 49   | 0.74 | 61  | 0.88| 13   | 56   | 0.81| 8   |\\n| total             | 483   | 58  | 385 | 0.95 | 79  | 404 | 0.84| 131  | 352  | 0.73| 116  |\\n| assumptions       | 81    | 10  | 0.55| 55   | 0.55 | 71  | 0.88| 25   | 56   | 0.69| 10   |\\n| docless           | 100   | 34  | 60  | 0.68 | 45  | 54  | 0.55| 59   | 41   | 0.41| 34   |\\n| docmore           | 50    | 11  | 0.81| 35   | 0.81 | 39  | 0.78| 26   | 24   | 0.48| 11   |\\n| ignore format     | 99    | 12  | 0.71| 53   | 0.71 | 74  | 0.75| 49   | 49   | 0.51| 25   |\\n| sequence errors   | 49    | 16  | 0.67| 33   | 0.67 | 45  | 0.92| 17   | 32   | 0.65| 4   |\\n| total             | 379   | 83  | 236 | 0.90 | 95  | 283 | 0.75| 176  | 202  | 0.54| 49   |\\n| calculations      | 149   | 55  | 82  | 0.65 | 90  | 59  | 0.40| 81   | 64   | 0.43| 28   |\\n| copying numbers   | 83    | 27  | 47  | 0.71 | 42  | 41  | 0.49| 54   | 28   | 0.34| 52   |\\n| final errors      | 97    | 18  | 70  | 0.88 | 35  | 62  | 0.64| 36   | 60   | 0.63| 35   |\\n| incorrect units   | 77    | 34  | 37  | 0.56 | 50  | 27  | 0.35| 59   | 17   | 0.22| 52   |\\n| wrong formula     | 88    | 25  | 54  | 0.77 | 43  | 44  | 0.51| 55   | 32   | 0.37| 26   |\\n| total             | 494   | 159 | 290 | 0.71 | 260 | 233 | 0.47| 285  | 201  | 0.41| 176  |\\n\\nTable 18: Results from evaluating FBI using Vanilla-Llama-3-70B-Instruct Claude-3-Opus Gemini-1.5-Pro evaluators. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | # Errs | G | P | Both \u2713 | Both \u2717 | \u0338= | % Errs | G | P | Both \u2713 | Both \u2717 | \u0338= | % Errs |\\n|------------------|-------|---|---|-------|-------|-----|--------|---|---|-------|-------|-----|--------|\\n|                  | 16304 |   |   | 100   | 100   | 0   | 100    |   |   | 100   | 100   | 0   | 100    |\\n\\nTable 19: Results from evaluating FBI using the Axis+Rules-Llama-3-70B-Instruct, Calude-3-Opus, and Gemini-1.5-Pro evaluators. An error is said to be detected if the evaluator chooses the Gold Answer. G indicates the number of times the evaluator has chosen the Gold Answer, P for the Perturbed Answer, Both \u2713 when both answers are correct, Both \u2717 when both are incorrect, and \u0338= for verdict inconsistencies.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | # Errs | 10 | 9 | 8 | <8 | % Errs | 10 | 9 | 8 | <8 | % Errs |\\n|-------------------|-------|----|---|---|----|-------|----|---|---|----|-----|-------|\\n| Coherence         | 91    | 1  | 2 | 3 | 4  | 5     | 6  | 7 | 8 | 9  | 10  | 1     |\\n| Comprehensiveness | 90    | 1  | 3 | 3 | 2  | 5     | 7  | 2 | 0 | 1  | 20  | 0.1   |\\n| Consistency       | 84    | 2  | 4 | 1 | 9  | 5     | 2  | 0 | 0 | 1  | 6   | 0.03  |\\n| Grammar           | 92    | 1  | 5 | 5 | 5  | 3     | 8  | 1 | 0 | 2  | 5   | 0.01  |\\n| Chronology        | 71    | 3  | 3 | 4 | 1  | 5     | 2  | 0 | 0 | 1  | 6   | 0.06  |\\n| Spelling          | 100   | 4  | 5 | 9 | 4  | 6     | 5  | 0 | 0 | 1  | 1   | 0.06  |\\n| Contextual        | 94    | 0  | 4 | 9 | 19 | 19    | 0  | 0 | 0 | 1  | 19   | 0.00  |\\n| Entity            | 87    | 0  | 5 | 0 | 17 | 16    | 0  | 0 | 0 | 1  | 16   | 0.00  |\\n| Incorrect Facts   | 68    | 0  | 3 | 8 | 18 | 9     | 2  | 0 | 0 | 1  | 9    | 0.00  |\\n| Number Errors     | 74    | 2  | 5 | 3 | 10 | 4     | 3  | 0 | 0 | 1  | 10   | 0.03  |\\n| Opposite Facts    | 91    | 0  | 3 | 7 | 19 | 30    | 0  | 0 | 0 | 1  | 30   | 0.00  |\\n| Remove Facts      | 69    | 4  | 2 | 9 | 22 | 13    | 13 | 0 | 0 | 1  | 13   | 0.06  |\\n| Total             | 483   | 6  | 2 | 53 | 10 | 4     | 3  | 0 | 0 | 1  | 4    | 0.01  |\\n| Incorrect Units   | 77    | 2  | 2 | 11 | 34 | 0     | 3  | 0 | 0 | 1  | 2    | 0.03  |\\n| Incorrect Formula | 88    | 7  | 2 | 26 | 25 | 25    | 2  | 0 | 0 | 1  | 26   | 0.08  |\\n| Total             | 494   | 21 | 1 | 119 | 156 | 157 | 13 | 0 | 0 | 1  | 13   | 0.05  |\\n\\nTable 20: Results from evaluating FBI using the Reference - Llama-3-70B-Instruct, Calude-3-Open and Gemini-1.5-Pro evaluators. An error is said to be detected if the evaluator gives a perfect score of 10 to the perturbed answer. 10 indicates the number of times the evaluator has given the score of 10, 9 for the score of 9, 8 for the score of 8 and <8 for scores less than 8.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | # Errs Detected | Errors | Undetected Errors | Detected in Explanation | % Undetected Errors |\\n|-------------------|----------------|--------|-------------------|-------------------------|---------------------|\\n| LF                | 16306          | 91     | 82                | 9                       | 0.09                |\\n| CMREHENSIVENESS   | 11240          | 90     | 30                | 60                      | 0.61                |\\n| CONSISTENCY       | 8565           | 84     | 35                | 49                      | 0.50                |\\n| GRAMMAR           | 10000          | 92     | 40                | 52                      | 0.47                |\\n| CHRONOLOGY        | 11325          | 71     | 18                | 53                      | 0.70                |\\n| SPELLING          | 10000          | 100    | 20                | 80                      | 0.69                |\\n| TOTAL             | 52825          | 528    | 225               | 303                     | 0.51                |\\n| CONTEXTUAL        | 9044           | 94     | 45                | 48                      | 0.47                |\\n| ENTITY            | 8545           | 87     | 43                | 44                      | 0.47                |\\n| INCORRECT         | 8171           | 68     | 29                | 38                      | 0.51                |\\n| NUMBER ERRORS     | 8135           | 74     | 30                | 44                      | 0.55                |\\n| OPPOSITE          | 9123           | 91     | 48                | 42                      | 0.41                |\\n| REMOVE            | 9446           | 69     | 25                | 44                      | 0.64                |\\n| TOTAL             | 48327          | 483    | 220               | 260                     | 0.50                |\\n| ASSUMPTIONS       | 6523           | 81     | 12                | 69                      | 0.77                |\\n| DOLESS            | 10000          | 100    | 57                | 43                      | 0.37                |\\n| DOMORE            | 5142           | 50     | 31                | 19                      | 0.14                |\\n| IGNORE FORMAT     | 9935           | 99     | 41                | 57                      | 0.48                |\\n| SEQUENCE ERRORS   | 7698           | 49     | 20                | 29                      | 0.57                |\\n| TOTAL             | 37859          | 379    | 161               | 217                     | 0.48                |\\n| CALCULATIONS      | 14900          | 149    | 112               | 34                      | 0.15                |\\n| COPYING NUMBERS   | 8315           | 83     | 69                | 12                      | 0.13                |\\n| FINAL ERRORS      | 8714           | 97     | 53                | 43                      | 0.29                |\\n| INCORRECT UNITS   | 7796           | 77     | 60                | 16                      | 0.13                |\\n| WRONG FORMULA     | 8746           | 88     | 66                | 19                      | 0.18                |\\n| TOTAL             | 49450          | 494    | 360               | 124                     | 0.18                |\\n\\nTable 21: Results from looking at the explanation of the Vanilla evaluator to determine the presence of the error in the response. Detected in Explanation shows the number of \u201cadditional\u201d errors detected by looking at the explanation in addition to the score.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | # Errs Detected | Errors | Undetected | Errors | Detected in | % Undetected |\\n|-------------------|-----------------|--------|------------|--------|-------------|-------------|\\n| COHERENCE         | 91              | 58     | 33         | 1      | Justification | 0.35        |\\n| COMPREHENSIVENESS | 90              | 1      | 89         | 5      | Justification | 0.93        |\\n| CONSISTENCY       | 84              | 8      | 76         | 3      | Justification | 0.87        |\\n| GRAMMAR           | 92              | 17     | 75         | 7      | Justification | 0.74        |\\n| CHRONOLOGY        | 71              | 0      | 71         | 9      | Justification | 0.87        |\\n| SPELLING          | 100             | 6      | 94         | 3      | Justification | 0.91        |\\n| CONTEXTUAL        | 94              | 29     | 65         | 23     | Justification | 0.45        |\\n| ENTITY            | 87              | 30     | 57         | 15     | Justification | 0.48        |\\n| INCORRECT FACT     | 68              | 17     | 51         | 14     | Justification | 0.54        |\\n| NUMBER ERRORS      | 74              | 18     | 56         | 16     | Justification | 0.54        |\\n| OPPOSITE FACT      | 91              | 32     | 59         | 23     | Justification | 0.40        |\\n| REMOVE FACT        | 69              | 1      | 68         | 20     | Justification | 0.70        |\\n| TOTAL              | 528             | 90     | 438        | 28     | Justification | 0.78        |\\n| ASSUMPTIONS        | 81              | 5      | 76         | 8      | Justification | 0.84        |\\n| DOLESS            | 100             | 20     | 80         | 0      | Justification | 0.80        |\\n| DOKORES            | 50              | 40     | 10         | 6      | Justification | 0.08        |\\n| IGNORE FORMAT      | 99              | 25     | 74         | 12     | Justification | 0.63        |\\n| SEQUENCE ERRORS    | 49              | 5      | 44         | 16     | Justification | 0.57        |\\n| TOTAL              | 379             | 95     | 284        | 42     | Justification | 0.64        |\\n| RACULATIONS        | 149             | 100    | 49         | 9      | Justification | 0.27        |\\n| COPYING NUMBERS    | 83              | 57     | 26         | 9      | Justification | 0.20        |\\n| FINAL ERRORS       | 97              | 46     | 51         | 7      | Justification | 0.45        |\\n| INCORRECT UNITS    | 77              | 42     | 35         | 7      | Justification | 0.36        |\\n| WRONG FORMULA      | 88              | 63     | 25         | 6      | Justification | 0.22        |\\n| TOTAL              | 494             | 308    | 186        | 42     | Justification | 0.30        |\\n\\nTable 22: Results from looking at the explanation of the Axis evaluator to determine the presence of the error in the response. Detected in Explanation shows the number of \\\"additional\\\" errors detected by looking at the explanation in addition to the score.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | # Errs Detected | Errors | Undetected | Errors | Detected in | % Undetected |\\n|-------------------|-----------------|--------|------------|--------|-------------|--------------|\\n| LF               | 91              | 47     | 44         | 2      | 0.46        |              |\\n| COMPREHENSIVENESS | 90              | 2      | 88         | 5      | 0.92        |              |\\n| CONSISTENCY      | 84              | 11     | 73         | 6      | 0.80        |              |\\n| GRAMMAR          | 92              | 15     | 77         | 6      | 0.77        |              |\\n| CHRONOLOGY       | 71              | 0      | 71         | 5      | 0.93        |              |\\n| SPELLING         | 100             | 4      | 96         | 8      | 0.88        |              |\\n| TOTAL            | 528             | 79     | 449        | 32     | 0.79        |              |\\n| CONTEXTUAL       | 94              | 34     | 60         | 3      | 0.61        |              |\\n| ENTITY           | 87              | 29     | 58         | 3      | 0.63        |              |\\n| INCORRECT FACT   | 68              | 18     | 50         | 2      | 0.71        |              |\\n| NUMBER ERRORS    | 74              | 17     | 57         | 7      | 0.68        |              |\\n| OPPOSITE FACT    | 91              | 32     | 59         | 6      | 0.58        |              |\\n| REMOVE FACT      | 69              | 1      | 68         | 10     | 0.84        |              |\\n| TOTAL            | 483             | 131    | 352        | 31     | 0.66        |              |\\n| ASSUMPTIONS      | 81              | 1      | 80         | 1      | 0.98        |              |\\n| DOLESS           | 100             | 8      | 92         | 8      | 0.84        |              |\\n| DOMEMORE         | 50              | 39     | 11         | 2      | 0.18        |              |\\n| IGNORE FORMAT    | 99              | 26     | 73         | 14     | 0.60        |              |\\n| SEQUENCE ERRORS  | 49              | 0      | 49         | 5      | 0.90        |              |\\n| TOTAL            | 379             | 74     | 305        | 30     | 0.73        |              |\\n| CALCULATIONS     | 149             | 102    | 47         | 10     | 0.25        |              |\\n| COPYING NUMBERS  | 83              | 64     | 19         | 3      | 0.19        |              |\\n| FINAL ERRORS     | 97              | 49     | 48         | 9      | 0.40        |              |\\n| INCORRECT UNITS  | 77              | 56     | 21         | 4      | 0.22        |              |\\n| WRONG FORMULA    | 88              | 61     | 27         | 13     | 0.16        |              |\\n| TOTAL            | 494             | 332    | 162        | 39     | 0.25        |              |\\n\\nTable 23: Results from looking at the explanation of the Rubrics evaluator to determine the presence of the error in the response. Detected in Explanation shows the number of \\\"additional\\\" errors detected by looking at the explanation in addition to the score.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type | # Errs Detected | Errors | Undetected Errors | Detected in Justification | % Undetected Errors |\\n|------------------|-----------------|--------|-------------------|--------------------------|---------------------|\\n| LF               | 160             | 45     | 46                | 0                        | 0.51                |\\n| COMPREHENSIVENESS| 0.90            | 0      | 90                | 11                       | 0.88                |\\n| CONSISTENCY      | 0.84            | 6      | 78                | 8                        | 0.83                |\\n| GRAMMAR          | 0.92            | 16     | 76                | 5                        | 0.77                |\\n| CHRONOLOGY       | 0.71            | 0      | 71                | 12                       | 0.83                |\\n| SPELLING         | 1.00            | 7      | 93                | 6                        | 0.87                |\\n| TOTAL            | 528             | 74     | 454               | 42                       | 0.78                |\\n| CONTEXTUAL       | 0.94            | 28     | 66                | 19                       | 0.50                |\\n| ENTITY           | 0.87            | 27     | 60                | 9                        | 0.59                |\\n| INCORRECT FACT    | 0.68            | 15     | 53                | 10                       | 0.63                |\\n| NUMBER ERRORS     | 0.74            | 15     | 59                | 12                       | 0.64                |\\n| OPPOSITE FACT     | 0.91            | 28     | 63                | 12                       | 0.56                |\\n| REMOVE FACT       | 0.69            | 1      | 68                | 16                       | 0.75                |\\n| TOTAL             | 483             | 114    | 369               | 78                       | 0.60                |\\n| ASSUMPTIONS       | 0.81            | 2      | 79                | 6                        | 0.90                |\\n| DOLESS            | 1.00            | 17     | 83                | 9                        | 0.74                |\\n| DO MORE           | 0.50            | 39     | 11                | 1                        | 0.20                |\\n| IGNORE FORMAT     | 0.99            | 24     | 75                | 14                       | 0.62                |\\n| SEQUENCE ERRORS   | 0.82            | 4      | 45                | 5                        | 0.82                |\\n| TOTAL             | 379             | 86     | 293               | 35                       | 0.68                |\\n| CALCULATIONS      | 0.149           | 97     | 52                | 14                       | 0.26                |\\n| COPYING NUMBERS   | 0.83            | 58     | 25                | 7                        | 0.22                |\\n| FINAL ERRORS      | 0.97            | 48     | 49                | 12                       | 0.38                |\\n| INCORRECT UNITS   | 0.77            | 44     | 33                | 7                        | 0.34                |\\n| WRONG FORMULA     | 0.88            | 63     | 25                | 9                        | 0.18                |\\n| TOTAL             | 494             | 310    | 184               | 49                       | 0.27                |\\n\\nTable 24: Results from looking at the explanation of the Axis+Rubrics evaluator to determine the presence of the error in the response. Detected in Explanation shows the number of \\\"additional\\\" errors detected by looking at the explanation in addition to the score.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finding Blind Spots in Evaluator LLMs with Interpretable Checklists\\n\\nSumanth Doddapaneni* 1, 2\\nMohammed Safi Ur Rahman Khan 1, 2\\nSshubam Verma 1\\nMitesh M. Khapra 1, 2\\n\\n1 Nilekani Centre at AI4Bharat\\n2 Indian Institute of Technology, Madras\\n\\nCorrespondence: \\n{sumanthd, miteshk}@cse.iitm.ac.in, safikhan@ai4bharat.org\\n\\nAbstract\\n\\nLarge Language Models (LLMs) are increasingly relied upon to evaluate text outputs of other LLMs, thereby influencing leaderboards and development decisions. However, concerns persist over the accuracy of these assessments and the potential for misleading conclusions. In this work, we investigate the effectiveness of LLMs as evaluators for text generation tasks. We propose FBI, a novel framework designed to examine the proficiency of Evaluator LLMs in assessing four critical abilities in other LLMs: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency. By introducing targeted perturbations in answers generated by LLMs, that clearly impact one of these key capabilities, we test whether an Evaluator LLM can detect these quality drops. By creating a total of 2400 perturbed answers covering 22 perturbation categories, we conduct a comprehensive study using different evaluation strategies on five prominent LLMs commonly used as evaluators in the literature. Our findings reveal significant shortcomings in current Evaluator LLMs, which failed to identify quality drops in over 50% of cases on average. Single-answer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance. These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications.\\n\\n1 Introduction\\n\\nLarge Language Models (LLMs) are gaining widespread acceptance as the gold standard for evaluation in numerous applications, thanks to their efficiency and significant reductions in cost & time compared to human evaluators (Kim et al., 2023, 2024a; Chiang and Lee, 2023; Chen et al., 2023). Furthermore, Evaluator LLMs are increasingly being utilized in the creation and maintenance of leaderboards for benchmarking various AI models (Watts et al., 2024; Zheng et al., 2023). While this reliance on LLMs offers significant advantages, it also presents potential drawbacks that warrant careful consideration. If LLMs are not effective evaluators, the resulting rankings and assessments could be fundamentally flawed, leading to inaccurate conclusions and misguided decisions. Therefore, it is crucial to pause and rigorously assess the evaluation capabilities of LLMs.\\n\\nRecent studies have explored the effectiveness of LLMs as evaluators and have reported strong correlations with human evaluations (Dubois et al., 2023; Zheng et al., 2023). While these findings are promising, accepting LLMs as reliable evaluators necessitates more nuanced assessments (Zeng et al., 2023). As LLMs become integral in a diverse range of tasks, they are expected to demonstrate a wide array of abilities, including factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"racy, instruction following, coherence in long-form writing, and reasoning proficiency. Consequently, it is crucial to determine if Evaluator LLMs can indeed do a fine grained assessment of these varied abilities. Specifically, can they evaluate factual correctness, grammar, spelling, mathematical proficiency, and adherence to instructions in answers generated by other LLMs? (ref. Fig. 1) The necessity for such thorough fine-grained assessments is underscored by the Checklist (Ribeiro et al., 2020) approach, initially applied to BERT (Devlin et al., 2019) and subsequently adapted in studies across various tasks and models (Sai et al., 2021).\\n\\nIn this work, we introduce FBI, a comprehensive framework designed to find blind spots in evaluator LLMs using an interpretable checklist across four fundamental text generation abilities: (a) factual accuracy, (b) instruction following, (c) coherence in long-form writing, and (d) reasoning proficiency. To rigorously assess an Evaluator LLM's ability to grade answers along these dimensions, we introduce perturbations that degrade the quality of the answer in one of these areas, expecting that good Evaluator LLMs will detect these quality drops and adjust their scores accordingly. Additionally, we develop quality-preserving perturbations where an Evaluator LLM should maintain consistent scoring. A detailed description of the 22 perturbation categories that we used is provided in Table 2. Staring with 500 prompts, we first generate long-form responses using GPT-4-TURBO. We then use a human-in-the-loop approach, to systematically perturb these responses, resulting in a dataset of 2400 tuples, where each tuple contains a prompt, response, and perturbed response. Using the generated perturbations, we employed three evaluation paradigms (a) single-answer evaluation, (b) pairwise evaluation, and (c) reference-guided evaluation. Within each paradigm, we try multiple popular strategies of using Evaluator LLMs, such as, providing a rubric, asking for a justification, specifying the axis of evaluation, etc. Using these strategies, we assess the evaluation capabilities of five widely-used Evaluator LLMs. Our findings indicate that LLMs are currently far from being reliable evaluators for text generation tasks. Even with the best models and evaluation strategies, Evaluator LLMs failed to identify errors in over 50% of cases, on average. Interestingly, across all evaluation strategies, we observed that all popular Evaluator LLMs consistently performed poorly. Notably, even basic perturbation categories, such as, fluency perturbations (e.g. spellings and grammar) posed challenges for the evaluators. We also observed cases where Evaluator LLMs did not adjust their scores for perturbed responses despite correctly identifying the perturbations in their explanations. When used for single-answer grading and pairwise evaluation, Evaluator LLMs showed significant limitations, suggesting they are not reliable in these setups. In contrast, when used for reference-based evaluation, they demonstrated relatively better performance. Overall, our experiments uncovered significant blind spots in Evaluator LLMs, warranting caution in their direct application in practical settings.\\n\\n2 Related Work\\nLLMs as Evaluators.\\nLLMs have been increasingly used for automated evaluation for various NLG tasks (Wang et al., 2023a; Chiang and yi Lee, 2023; Kocmi and Federmann, 2023). We broadly classify this into two paradigms - (i) reference-driven evaluations (Fu et al., 2023; Kim et al., 2023), and (ii) reference-free evaluations (Liu et al., 2023; Zheng et al., 2023). The evaluator is either asked for a score (score-based evaluation) (Liu et al., 2023; Zheng et al., 2023; Hada et al., 2023) or to choose the best amongst two given responses (pairwise comparison evaluation) (Zheng et al., 2023; Wang et al., 2023b; Liu et al., 2023). Additionally, various open-source evaluation-specific trained models have also been proposed (Wang et al., 2023d; Kim et al., 2023; Zhu et al., 2023). Further, advanced ensemble approaches include evaluation via multi-agent interactions (Chan et al., 2023; Zhang et al., 2023) or with external agents (Min et al., 2023; Hasanbeig et al., 2023).\\n\\nBiases in Evaluator LLMs.\\nStudies around Evaluator LLMs have highlighted the various biases - position bias (Zheng et al., 2023; Wang et al., 2023c), self preference bias (Panickssery et al., 2024; Liu et al., 2023), verbosity bias (Wu and Aji, 2023; Zeng et al., 2023), etc. Various approaches, including chain-of-thought reasoning (Zheng et al., 2023; Zeng et al., 2023), position-swapping (Zeng et al., 2023), among others, have been suggested to mitigate some of these. Recent studies (Hada et al., 2023; Saha et al., 2023) also show the effectiveness of the evaluators can be increased by evaluating specific axes and providing detailed rubrics/rules (Ye et al., 2023; Kim et al., 2024a).\"}"}
{"id": "emnlp-2024-main-911", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluation of Evaluator LLMs.\\n\\nCritically analysing evaluation metrics and suggesting methods to improve their robustness has always been of interest to the NLP community (Sai B et al., 2023; Mathur et al., 2020). Recent studies have evaluated the efficacy of LLMs as evaluators for specific types of tasks (Hada et al., 2024; Shen et al., 2023) and evaluation paradigms (Wang et al., 2023b,a) by assessing their agreement with human evaluations (Hada et al., 2023; Chiang and Lee, 2023; Zheng et al., 2023). Additionally, the robustness of these evaluators has been tested using adversarial examples (Kamoi et al., 2024; Chen et al., 2024; Wu and Aji, 2023), further showing their strengths and weaknesses.\\n\\nOur proposed framework represents a significant departure from these existing approaches in several key aspects. First, we focus on a broader set of essential abilities: factual understanding, instruction following, long-form writing, and reasoning. Second, all prompts and the 2400 perturbed answers in our framework are carefully crafted and/or validated by humans, ensuring high quality and relevance to the abilities being evaluated. Third, our framework offers finer granularity in perturbation types, allowing us to finely identify and isolate the capabilities and limitations of Evaluator LLMs. This detailed analysis assists in making more knowledgeable choices about when to utilize LLMs as evaluators. Lastly, we focus on three popular evaluation paradigms, viz., reference-less single answer scoring, reference-less pairwise comparison, and reference based scoring, thereby providing a comprehensive toolkit for evaluating LLM performance across different dimensions.\\n\\n3 FBI: Meta-Evaluation Checklist\\n\\nWe introduce FBI, a meta-evaluation benchmark designed to assess the capabilities of Evaluator LLMs in examining the outputs of other LLMs across four distinct task abilities: (i) Factual accuracy, (ii) Reasoning ability, (iii) instruction following, and (iv) proficiency in long-form writing. Each instance within the benchmark comprises a tuple \\\\((I, A_{\\\\text{gold}}, A_{\\\\text{perturb}})\\\\), where \\\\(I\\\\) represents the input instruction or prompt given to the model, \\\\(A_{\\\\text{gold}}\\\\) denotes the correct or gold answer, and \\\\(A_{\\\\text{perturb}}\\\\) signifies a perturbed version of the gold answer.\\n\\nThe perturbed answers, \\\\(A_{\\\\text{perturb}}\\\\), are generated by introducing specific types of errors across each of the four task abilities (Table 2) to evaluate whether LLM evaluators can accurately identify and account for these errors in the perturbed answers.\\n\\nThe perturbations are based on perturbation categories carefully crafted by human annotators, informed by the prevalent failure modes in current LLMs (Min et al., 2023; Wu et al., 2023; Zhou et al., 2023b). These human annotators are graduate students who are well aware of the typical errors made by LLMs. Such human oversight is used throughout the benchmark's development, from prompt selection (\u00a73.1) to defining perturbation categories (\u00a73.2) and creating the perturbations (\u00a73.3). To ensure a high standard of accuracy and reliability, all perturbations within FBI undergo rigorous manual vetting (\u00a73.4). Table 1 presents some statistics about FBI, and the detailed generation process is discussed in the following sub-sections.\\n\\n3.1 Prompt Selection\\n\\nWe selected six test sets containing prompts in English, viz., WizardLM (Xu et al., 2023), MTBench (Zheng et al., 2023), UltraChat (Ding et al., 2023), LIMA (Zhou et al., 2023a), LLMBar (Zeng et al., 2023), and TAPAS (Wang et al., 2023b,a). Each test set contains prompts that are designed to probe specific abilities, for example, factual understanding, instruction following, and long-form writing. These prompts are carefully selected to cover a wide range of difficulty levels and to ensure a comprehensive evaluation of the LLMs.\\n\\nThe selected prompts are used to generate training instances in the FBI benchmark. Each prompt is paired with a correct answer and one or more perturbed versions of the answer, ensuring a diverse and representative set of tasks for the LLMs to evaluate.\\n\\nTable 1: Statistics of perturbations across all the 4 task abilities and each of the perturbation categories.\\n\\n| Category          | Instances | Long Form (LF) | RAMMAR | SPELLING | CONSISTENCY | CHRONOLOGY | COHERENCE | COMPREHENSIVENESS |\\n|-------------------|-----------|----------------|--------|----------|-------------|------------|-----------|------------------|\\n| Factual (F)       | 483       | 94             | 87     | 68       | 74          | 91         | 90        |                  |\\n| Instruction Following (IF) | 379     | 50             | 100    | 99       | 49          | 81         |           |                  |\\n| Reasoning (R)     | 494       | 149            | 83     | 97       | 77          | 88         |           |                  |\\n| Score Invariant (SI) | 516 | 0              |        |          |             |            |           |                  |\\n\\nTotal 2400\\n\\nThe detailed generation process is discussed in the following sub-sections.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al., 2023), and IFEval (Zhou et al., 2023b). These test sets were selected for their recency and because they contain prompts for long-form generation, creativity, and open-ended tasks that require instruction-following. Collectively, these test sets comprise of 1809 prompts. We manually categorized each prompt into one of the 4 task categories: Long Form Writing (LF): These prompts require generating long pieces of text and explore generic topics, often including detailed analysis and storytelling. For example, How can I improve my time management skills?\\n\\nFactual (F): These prompts seek objective information or facts. For example, What is the primary function of a capacitor in an electrical circuit?\\n\\nInstruction Following (IF): These prompts require executing specific steps or guidelines to achieve a particular outcome or answer. For example, Write a poem with four lines and the following words: peace, sky, race, ground.\\n\\nReasoning (R): These prompts necessitate the application of logic, mathematics, and critical thinking to analyze information and draw conclusions. For example, A bat and a ball together cost $1.10. The bat costs $1.00 more than the ball. How much does the ball cost?\\n\\nWe sampled 100 questions from each of the four abilities, supplementing prompts requiring reasoning ability from the GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) benchmarks. Additionally, we created 200 prompts tailored to instruction following to address specific perturbation categories. The gold answers (A\\\\text{gold}) for all prompts were generated using the GPT-4-TURBO model. To ensure the quality and accuracy of A\\\\text{gold}, we conducted manual verification by randomly sampling 25\\\\% instances from each category and found that the gold answers maintain a high level of correctness.\\n\\nImportantly, we emphasize that the quality of gold answers is not critical in our study, as our primary focus is on directional score changes (i.e., we are interested in knowing if a perturbed answer with clear errors scores relatively lower than the original answer which did not have these errors).\\n\\n3.2 Perturbation Categories\\n\\nLLMs exhibit numerous failure modes, encompassing shortcomings in reasoning (Wu et al., 2023; Wei et al., 2022), factuality (Hu et al., 2024; Min et al., 2023), instruction-following (Zhou et al., 2023b; Li et al., 2023), and, in some instances, coherence and consistency (Naismith et al., 2023; Shen et al., 2023) in generated text. Given that we utilize Evaluator LLMs to assess responses in one or more of these abilities, it is imperative for the evaluator to excel in them. Our perturbations across each task ability are crafted keeping these failure modes in mind, as presented in Table 2. While our perturbations are primarily designed to decrease scores, we also develop score-invariant perturbations (\u00a73.5), which are intended not to affect the score relative to the gold answer.\\n\\n3.3 Perturbation Generation\\n\\nTo generate perturbed answers (A\\\\text{perturb}) along each of the defined categories (\u00a73.2), we use GPT-4-TURBO by prompting it with specific instructions tailored to each perturbation category. The model was tasked with producing perturbed answers and explaining the reasoning behind each perturbation. We iteratively refined the instructions by manually reviewing a sample of 25\\\\% of perturbed answers for each category, till we were satisfied with the generated perturbations.\\n\\n3.4 Human-In-The-Loop\\n\\nWhile GPT generally succeeds in generating the expected perturbations, we observed instances where the model (i) deviates from the intended perturbation, (ii) produces the incorrect style of perturbation, or (iii) accurately generates the reasoning but fails to reflect it in A\\\\text{perturb}. To address these inconsistencies, we meticulously vet all generated perturbations through a manual review process. Each perturbed answer produced by GPT-4-TURBO is examined against A\\\\text{gold}, and then categorized as valid, invalid, or score invariant. A perturbation is considered valid only if it should logically result in a scoring penalty as determined by human annotators. The vetting is carried out by students who possess a comprehensive understanding of LLM literature, holding at least a bachelor's or master's degree. To aid in validating perturbations, we developed a tool, the details of which are outlined in Appendix A.\\n\\n3.5 Score-Invariant Perturbations\\n\\nScore-invariant perturbations are those modifications that do not warrant a scoring penalty. These are collected in two ways: (i) human annotators\"}"}
{"id": "emnlp-2024-main-911", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Perturbation categories across each of the task abilities. The green highlights indicate the original text\\nand the red highlights indicated the perturbed text. Complete examples of each perturbation can be found in\\nsupplementary material.\\n\\n4 Strategies for using Evaluator LLMs\\n\\nIn this section, we outline the prompting strategies employed by Evaluator LLMs benchmarked\\non FBI. An Evaluator LLM, $f(\\\\cdot)$, takes the input instruction, LLM generated response and an eval-\\nuation prompt, $P_{eval}$, as input, and is required to\\ngenerate a score and an optional explanation. To\\nmake the evaluation more robust, the evaluator may\\nalso be provided with additional information spec-\\ning the axes of evaluation, rubrics, rules, and\\nother criteria. Our study focuses on 3 evaluation\\nparadigms: (i) Single-answer scoring (\u00a74.1), (ii)\\nPairwise comparison (\u00a74.2), and (iii) Reference-\\nguided evaluation (\u00a74.3). For all the strategies eval-\\nuation prompts $P_{eval}$ are adapted from Zheng et al.\\n(2023); Zeng et al. (2023); Hada et al. (2023).\\n\\n4.1 Single Answer Scoring\\n\\nIn this paradigm, evaluator $f(\\\\cdot)$ is tasked with scor-\\ning a model response based solely on its parameter-\\nized knowledge.\\n\\nVanilla $\\\\star$ (Zheng et al., 2023): In this strategy,\\nthe evaluator $f(\\\\cdot)$ is presented with only the input\\ninstruction $I$ and a model response $A_{model}. The\\nrole of $f(\\\\cdot)$ is to evaluate $A_{model}$ and assign a score,\\ndenoted as $f(P_{eval}, I, A_{model}) \\\\rightarrow (\\\\text{score})$.\\n\\nVanilla (Zheng et al., 2023): This strategy ex-\\ntends \\\"Vanilla $\\\\star\\\"$, where the evaluator $f(\\\\cdot)$ is tasked\\nnot only with scoring the model response $A_{model}$ but also providing an explanation for the score - rep-\\nresented as $f(P_{eval}, I, A_{model}) \\\\rightarrow (\\\\text{exp}, \\\\text{score})$.\\n\\nRubric (Zeng et al., 2023): In this strategy, in\\naddition to the instruction $I$ and the model re-\\nsponse $A_{model}$, we also provide a grading rubric $R$.\\nThe evaluator $f(\\\\cdot)$ is prompted to first generate an\\nexplanation followed by a score - represented as $f(P_{eval}, R, I, A_{model}) \\\\rightarrow (\\\\text{exp}, \\\\text{score})$.\\n\\nAxis (Hada et al., 2023): In this strategy, the\\nevaluator $f(\\\\cdot)$ is prompted to assess the model re-\\nsponse, $A_{model}$, along a designated axis, $Ax$, align-\\ning with the category of the instruction (\u00a73.1).\"}"}
{"id": "emnlp-2024-main-911", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For instance, factual questions are evaluated along the hallucination axis to determine the presence of fabricated content. This process is formally represented as \\\\( f(P_{eval}, Ax, I, A_{model}) \\\\rightarrow (exp, score) \\\\).\\n\\nAxis+Rubric (Hada et al., 2023): In this strategy, the evaluator \\\\( f(\\\\cdot) \\\\) is provided with both a specific evaluation axis \\\\( Ax \\\\) and detailed scoring rubrics \\\\( R \\\\) for that axis. This is formally represented as \\\\( f(P_{eval}, Ax, R, I, A_{model}) \\\\rightarrow (exp, score) \\\\).\\n\\n4.2 Pairwise Comparison\\n\\nIn this paradigm, the evaluator \\\\( f(\\\\cdot) \\\\) is tasked to choose the better response from the two given options by again relying on its parameterized knowledge.\\n\\nPairwise\u2217 (Zheng et al., 2023): The evaluator \\\\( f(\\\\cdot) \\\\) here is given only an instruction \\\\( I \\\\) and two model responses \\\\( A_1 \\\\) and \\\\( A_2 \\\\) and is tasked to determine the better response or mark both as equally valid. This is formally represented as \\\\( f(P_{eval}, I, A_1, A_2) \\\\rightarrow (verdict) \\\\).\\n\\nPairwise (Zheng et al., 2023): This strategy extends \\\"Pairwise\u2217\\\", where the evaluator is tasked not only with choosing the better response but also providing an explanation for the verdict - represented as \\\\( f(P_{eval}, I, A_1, A_2) \\\\rightarrow (exp, verdict) \\\\).\\n\\nRules (Zeng et al., 2023): In this strategy, in addition to the instruction \\\\( I \\\\) and the two model responses \\\\( A_1, A_2 \\\\), the evaluator \\\\( f(\\\\cdot) \\\\) is given detailed rules for evaluation and is asked to generate an explanation followed by the verdict. This process is formally represented as \\\\( f(P_{eval}, R, I, A_1, A_2) \\\\rightarrow (exp, verdict) \\\\).\\n\\nAxis (Hada et al., 2023): Extending the Axis strategy defined in Sec \u00a74.1, the evaluator \\\\( f(\\\\cdot) \\\\) is asked to choose the better response along a designated axis \\\\( Ax \\\\). The evaluator is prompted with the instruction \\\\( I \\\\), two model responses \\\\( A_1, A_2 \\\\), and the description of the axis \\\\( Ax \\\\) - represented as \\\\( f(P_{eval}, Ax, R, I, A_1, A_2) \\\\rightarrow (exp, verdict) \\\\).\\n\\nAxis+Rules (Zeng et al., 2023; Hada et al., 2023): Extending the Axis+Rubric strategy defined in Sec \u00a74.1, this strategy involves choosing the better response along the designated axis \\\\( Ax \\\\). The evaluator is prompted with the instruction \\\\( I \\\\), two model responses \\\\( A_1, A_2 \\\\), details about the axis \\\\( Ax \\\\), and detailed rules for evaluation - represented as \\\\( f(P_{eval}, Ax, R, I, A_1, A_2) \\\\rightarrow (exp, verdict) \\\\).\\n\\n4.3 Reference-guided Single Answer Scoring\\n\\nIn this paradigm, the evaluator \\\\( f(\\\\cdot) \\\\) is tasked to score a response by comparing against a reference. It is important to note that this approach may not be feasible for many open-ended questions.\\n\\nReference (Zheng et al., 2023): In this strategy, given an instruction \\\\( I \\\\), a model response \\\\( A_{model} \\\\), and a ground truth reference answer \\\\( A_{gold} \\\\), the evaluator \\\\( f(\\\\cdot) \\\\) is tasked with scoring the model response, along with giving an explanation. This is formally represented as \\\\( f(P_{eval}, I, A_{gold}, A_{model}) \\\\rightarrow (exp, score) \\\\).\\n\\n5 Experiments\\n\\nWe use GPT-4-TURBO as our primary evaluation model, given its widespread adoption (Zeng et al., 2023; Hada et al., 2023; Min et al., 2023). We also extend our analysis to other proprietary models - GEMINI-1.5-PRO (Team et al., 2024) and CALADE-3-OPUS (Anthropic, 2024), open-source models like LLAMA-3-70B-INSTRUCT (Meta, 2024), and trained evaluator models like ROMETHEUS 2 (Kim et al., 2024b). All evaluations are conducted at a temperature of zero to ensure reproducibility.\\n\\nIn single answer scoring (\u00a74.1) paradigm, we measure the percentage of instances where the score remains unchanged by the perturbation as our metric. Ideally, except for score-invariant perturbations, the evaluator should penalize the score of the perturbed answer. For pairwise comparison paradigm (\u00a74.2), we include our \\\"gold\\\" answer as one of the responses, requiring the evaluator to select the best response between the \\\"gold\\\" and the \\\"perturbed\\\" answer. Here, we measure the percentage of times the evaluator does not choose the gold answer as our metric. To mitigate position bias (Wang et al., 2023c), we conduct each evaluation twice, swapping the order of the gold and perturbed responses.\\n\\nFor reference-guided single answer scoring paradigm (\u00a74.3), the gold answer serves as the reference. Here, we measure the percentage of times the evaluator awards a perfect score to the perturbed answer as our metric.\\n\\n5.1 Is GPT-4-Turbo a good evaluator?\\n\\nReferring to the first section of Table 3, we observe that in the case of single answer scoring, 2 We reuse the axes and rubrics defined in Section \u00a74.1 as the evaluation rubrics for PROMETHEUS 2.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Comparison of different evaluation strategies using GPT-4-TURBO. The numbers indicate the percentage of instances where the score/verdict generated by the LLM evaluator is not affected by the perturbation. Lower values (\u2193) indicate better performance in all categories except SI.\\n\\n* denotes evaluators that only give a score without any justification.\\n\\nGPT-4-TURBO fails to lower its score for the perturbed answer in a majority of the cases, except for Reasoning tasks. Further, the performance of GPT-4-TURBO is better when using simpler strategies, such as, Vanilla* and Vanilla, as compared to the more advanced strategies with explicit rubrics and/or specified axis of evaluation. This could imply that while adding additional rubrics and criteria may increase the overall thoroughness, it may not necessarily enhance the model's ability to detect subtler errors.\\n\\nNow, referring to the second section of Table 3, we observe that in the case of pairwise comparison, GPT-4-TURBO fails to detect the perturbed answer in majority of the cases, except for Reasoning tasks. Further, in contrast to the above, in this case, advanced strategies perform better than the basic strategies. This indicates that for comparative evaluations, having detailed specific rules can help improve the reliability of the models. Lastly, referring to the first row of the last section of Table 3, we observe that when a reference is provided, GPT-4-TURBO performs much better but there are still a notable number of failures. The evaluator, despite being presented with the gold answer marked as a reference answer, fails to recognize the perturbations in many cases, except for reasoning tasks where it performs very well.\\n\\nOur overall verdict is that GPT-4-TURBO is not a good evaluator as it fails to detect perturbations which cause a drop in the quality of the answer.\\n\\nTable 4: Comparison of the performance of different models across the best-observed evaluation strategies. Lower values (\u2193) indicate better performance in all categories except SI.\\n\\n5.2 How do other popular Evaluator LLMs perform?\\n\\nWe extend our evaluation to other models and compare their performance when using the 3 best strategies identified in Table 3. Table 4 shows that GPT-4-TURBO consistently outperforms other models in both the reference-less paradigms. Due to the high API cost of using the CLAUDE-3-OPLUS model, we restrict its evaluation to only the Vanilla strategy, and note that it performed poorly as an Evaluator LLM.\\n\\nIn the reference-based paradigm, LLAMA-3-70B-INSTRUCT model surprisingly outperforms all others. Upon manually reviewing few instances, we observe that LLAMA-3-70B-INSTRUCT is a stringent evaluator and rarely awards perfect scores to even very well-formed answers when presented with a reference answer. While this may suggest that LLAMA-3-70B-INSTRUCT has a high evaluation standard, it also raises concerns about overly relying on the reference answer, which is typically not available in most practical scenarios. To further investigate this, we evaluate all the models on Score Invariant perturbations (Section \u00a73.5) using the Reference evaluation strategy. Consistent with our prior observations, LLAMA-3-70B-INSTRUCT seldom awards perfect scores, doing so only in 13% of the cases as shown in Table 4. Lastly, looking at the last row of Table 4, we observe that even trained Evaluator LLMs like PROMETHEUS are...\"}"}
{"id": "emnlp-2024-main-911", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: Comparison of perturbations detected solely by score analysis versus those identified with explanations. The highlighted region marked with stars denotes perturbations detected in explanations but not reflected in scores. Despite this, a significant proportion of perturbations remain undetected.\\n\\nTable 5: Comparing performance of Rubrics and Axis+Rubrics strategies with score range of 1-3 and 1-5. The numbers indicate the percentage of instances where the score generated by the LLM evaluator is not affected by the perturbation. Lower values (\u2193) indicate better performance in all categories.\\n\\n5.3 Does it help to look beyond scores?\\nIn addition to scoring, our evaluators also generate explanations that provide a justification for each score. We investigate whether these explanations detect the perturbations, even though this is not reflected in the scores. We prompt GPT-3.5-TURBO model with explanations from the instances where the evaluator rated the perturbed answer as equal to the gold answer, asking it to identify if any mistake or error has been reported in the explanation. Figure 2 reveals that explanations are only marginally helpful. Although perturbations are sometimes identified, they are overlooked or not considered significant enough to penalize the score. It is important to note that all the perturbations here were intended to incur a scoring penalty. Thus, while explicitly considering the explanations offers a slight improvement in the evaluator's performance, the overall performance is still poor.\\n\\n5.4 What about score-invariant perturbations?\\nWe evaluate different Evaluator LLMs using score-invariant perturbations (\u00a73.5). Ideally, the evaluator should not reduce its score for these perturbations in score-based evaluations and should deem both responses correct in pairwise evaluations. Referring to Table 3, in reference-less scoring, GPT-4-TURBO performs better when using non-vanilla evaluating strategies, while in pairwise comparison, it performs better when using simpler evaluation strategies. Similarly, as shown in Table 4, we observe that other Evaluator LLMs also perform well in a majority of cases. However, there is still a significant number of responses with score-invariant perturbations that they rate poorly.\\n\\n5.5 Does increasing the range help in scoring?\\nBased on recommendations from Hada et al. (2023), our initial set-up for the Rubrics and Axis+Rubrics evaluators used a scoring range of 1 to 3. To explore whether a wider scoring range could enhance the evaluators' ability to identify and account for the perturbations, we extended the range to 1 to 5. Results presented in Table 5 suggest that this broader range slightly improves the evaluators' performance, perhaps due to the availability of more flexibility in scoring decisions.\\n\\n6 Conclusion\\nWe propose FBI, a novel framework designed to evaluate the proficiency of Evaluator LLMs in assessing four critical abilities: factual accuracy, instruction adherence, coherence in long-form writing, and reasoning proficiency, through targeted perturbations. Our comprehensive study, involving 2400 perturbed answers across 22 categories and using three evaluation paradigms (single-answer, pairwise, and reference-guided evaluation), reveals significant shortcomings in current Evaluator LLMs. Our findings show that even the most advanced models failed to identify quality drops in over 50% of cases on average. While reference-based evaluations performed relatively better, single-answer and pairwise evaluations demonstrated notable limitations. These results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications. We hope that the FBI framework will be further extended and used for continued meta-evaluation of Evaluator LLMs.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\nIn our evaluation setup, detailed in Section 4, we concentrate on three primary evaluation paradigms: single-answer assessment, pairwise comparison, and reference-guided evaluation within a single model context and leave out multi-agent meta-evaluation and for future work. While we have compiled a list of perturbation categories, we believe it is not exhaustive and there is room for further expansion. Our evaluation framework encompasses four fundamental task abilities, with plans to explore more advanced capabilities such as multilingual generation, tool usage, and planning in future work.\\n\\nEthics\\nAll annotations described in Section 3 were done by students from our research group, all of whom hold at least a bachelor\u2019s or master\u2019s degree. This annotation was done as a part of their routine research work. The datasets used in this paper are all available under permissible licenses, and we adhere strictly to their intended usage, maintaining compliance with licensing requirements. Additionally, the code used for our evaluations and perturbation generation will be made publicly available under the MIT License. We only used ChatGPT for assistance purely with the language of the paper, e.g., paraphrasing, spell-checking, or polishing the author\u2019s original content, without suggesting new content.\\n\\nAcknowledgements\\nWe would like to thank EkStep Foundation and Nilekani Philanthropies for their generous grant, which supported this research. We extend our gratitude to Ananth, Devilal, Niharika, Nikhil, Sakshi, Sparsh, and Suhaas, Suriya for their invaluable assistance with manual audits. We also thank Raj Dabre and Anoop Kunchukuttan for their insightful discussions. We thank Google for supporting Sumanth\u2019s work through the Google Ph.D. Fellowship.\\n\\nReferences\\nAnthropic. 2024. Introducing the next generation of claude. https://www.anthropic.com/news/clau...\"}"}
{"id": "emnlp-2024-main-911", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv: 2302.04166.\\n\\nRishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2024. Metal: Towards multilingual meta-evaluation. arXiv preprint arXiv: 2404.01667.\\n\\nRishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, M. Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation? FINDINGS.\\n\\nHosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, and Ida Momennejad. 2023. Allure: Auditing and improving llm-based evaluation of text using iterative in-context-learning. arXiv preprint arXiv: 2309.13701.\\n\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\n\\nXuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, and Zhijiang Guo. 2024. Towards understanding factual knowledge of large language models. In The Twelfth International Conference on Learning Representations.\\n\\nRyo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, and Rui Zhang. 2024. Evaluating llms at detecting errors in llm responses. arXiv preprint arXiv: 2404.03602.\\n\\nSeungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2023. Prometheus: Inducing fine-grained evaluation capability in language models. CoRR, abs/2310.08491.\\n\\nSeungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024a. The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models.\\n\\nSeungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024b. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv: 2405.01535.\\n\\nTom Kocmi and C. Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. European Association for Machine Translation Conferences/Workshops.\\n\\nZekun Li, Baolin Peng, Pengcheng He, and Xifeng Yan. 2023. Evaluating the instruction-following robustness of large language models to prompt injection. arXiv preprint arXiv: 2308.10819.\\n\\nYang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: Nlg evaluation using gpt-4 with better human alignment. Conference on Empirical Methods in Natural Language Processing.\\n\\nAdian Liusie, Potsawee Manakul, and Mark J. F. Gales. 2023. Llm comparative assessment: Zero-shot nlg evaluation through pairwise comparisons using large language models. arXiv preprint arXiv: 2307.07889.\\n\\nNitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984\u20134997, Online. Association for Computational Linguistics.\\n\\nMeta. 2024. Introducing meta llama 3: The most capable openly available llm to date. https://ai.meta.com/blog/meta-llama-3/. Accessed: 2024-06-14.\\n\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv: 2305.14251.\\n\\nBen Naismith, Phoebe Mulcaire, and Jill Burstein. 2023. Automated evaluation of written discourse coherence using GPT-4. In Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 394\u2013403, Toronto, Canada. Association for Computational Linguistics.\\n\\nArjun Panickssery, Samuel R. Bowman, and Shi Feng. 2024. Llm evaluators recognize and favor their own generations. arXiv preprint arXiv: 2404.13076.\\n\\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u20134912, Online. Association for Computational Linguistics.\\n\\nSwarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 2023. Branch-solve-merge improves large language model evaluation and generation. CoRR, abs/2310.15123.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-911", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Steiner, Sayed Hadi Hashemi, Jacob Austin, Anita\\nGergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant, Anders Andreassen, Carlos\\nAraya, Nikhil Sethi, Rakesh Shivanna, Steven Hand, Ankur Bapna, Ali Khodaei, Antoine Miech, Garrett\\nTanzer, Andy Swing, Shantanu Thakoor, Lora Aroyo, Zhufeng Pan, Zachary Nado, Jakub Sygnowski,\\nStephanie Winkler, Dian Yu, Mohammad Saleh, Loren Maggiore, Yamini Bansal, Xavier Garcia,\\nMehran Kazemi, Piyush Patil, Ishita Dasgupta, Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka,\\nAmit Marathe, Vladimir Feinberg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller,\\nLexi Walker, Richard Tanburn, Mukarram Tariq, Disha Shrivastava, Fei Xia, Qingze Wang, Chung-\\nCheng Chiu, Zoe Ashwood, Khuslen Baatarsukh, Sina Samangooei, Rapha\u00ebl Lopez Kaufman, Fred Al-\\ncober, Axel Stjerngren, Paul Komarek, Katerina Tsihnlas, Anudhyan Boral, Ramona Comanescu, Jeremy\\nChen, Ruibo Liu, Chris Welty, Dawn Bloxwich, Charline Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew\\nMauger, Xerxes Dotiwalla, Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Krishna Haridasan,\\nGabe Barth-Maron, Craig Swanson, Dominika Rogozi\u0144ska, Alek Andreev, Paul Kishan Rubenstein,\\nRuoxin Sang, Dan Hurt, Gamaleldin Elsayed, Renshen Wang, Dave Lacey, Anastasija Ili\u0107, Yao Zhao,\\nAdam Iwanicki, Alejandro Lince, Alexander Chen, Christina Lyu, Carl Lebsack, Jordan Griffith, Meenu\\nGaba, Paramjit Sandhu, Phil Chen, Anna Koop, Ravi Rajwar, Soheil Hassas Yeganeh, Solomon Chang,\\nRui Zhu, Soroush Radpour, Elnaz Davoodi, Ving Ian Lei, Yang Xu, Daniel Toyama, Constant Segal, Martin\\nWicke, Hanzhao Lin, Anna Bulanova, Adri\u00e0 Puigdom\u00e8nech Badia, Nemanja Raki\u0107evi\u0107, Pablo Sprech-\\nmann, Angelos Filos, Shaobo Hou, V\u00edctor Campos, Nora Kassner, Devendra Sachan, Meire Fortunato,\\nChimezie Iwuanyanwu, Vitaly Nikolaev, Balaji Lakshminarayanan, Sadegh Jazayeri, Mani Varadarajan,\\nChetan Tekur, Doug Fritz, Misha Khalman, David Reitter, Kingshuk Dasgupta, Shourya Sarcar, Tina\\nOrnduff, Javier Snaider, Fantine Huot, Johnson Jia, Rupert Kemp, Nejc Trdin, Anitha Vijayakumar,\\nLucy Kim, Christof Angermueller, Li Lao, Tianqi Liu, Haibin Zhang, David Engel, Somer Greene,\\nAna\u00efs White, Jessica Austin, Lilly Taylor, Shereen Ashraf, Dangyi Liu, Maria Georgaki, Irene Cai,\\nYana Kulizhskaya, Sonam Goenka, Brennan Saeta, Ying Xu, Christian Frank, Dario de Cesare,\\nBrona Robenek, Harry Richardson, Mahmoud Alnahlawi, Christopher Yew, Priya Ponnapalli,\\nMarco Tagliasacchi, Alex Korchemniy, Yelin Kim, Dinghua Li, Bill Rosgen, Kyle Levin, Jeremy Wiesner,\\nPraseem Banzal, Praveen Srinivasan, Hongkun Yu, \u00c7a\u011flar \u00dcnl\u00fc, David Reid, Zora Tung, Daniel Finchelstein,\\nRavin Kumar, Andre Elisseeff, Jin Huang, Ming Zhang, Ricardo Aguilar, Mai Gim\u00e9nez, Jiawei Xia,\\nOlivier Dousse, Willi Gierke, Damion Yates, Komal Jalan, Lu Li, Eri Latorre-Chimoto, Duc Dung Nguyen,\\nKen Durden, Praveen Kallakuri, Yaxin Liu, Matthew Johnson, Tomy Tsai, Alice Talbert, Jasmine Liu,\\nAlexander Neitz, Chen Elkind, Marco Selvi, Mimi Jasarevic, Livio Baldini Soares, Albert Cui,\\nPidong Wang, Alek Wenjiao Wang, Xinyu Ye, Krystal Kallarackal, Lucia Loher, Hoi Lam, Josef Broder,\\nDan Holtmann-Rice, Nina Martin, Bramandia Ramadhana, Mrinal Shukla, Sujoy Basu, Abhi Mohan,\\nNick Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush Garg, Jane Park, DongHyun Choi,\\nDiane Wu, Sankalp Singh, Zhishuai Zhang, Amir Globerson, Lily Yu, John Carpenter,\\nF\u00e9lix de Chaumont Quitry, Carey Radebaugh, Chu-Cheng Lin, Alex Tudor, Prakash Shroff,\\nDrew Garmon, Dayou Du, Neera Vats, Han Lu, Shariq Iqbal, Alex Yakubovich, Nilesh Tripuraneni,\\nJames Manyika, Haroon Qureshi, Nan Hua, Christel Ngani, Maria Abi Raad, Hannah Forbes,\\nJeff Stanway, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie Li, Balaji Venkatar-\\nman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian Tenney,\\nXihui Wu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach, Zhuyun Xiao,\\nPeter Hawkins, Sid Dalmia, Clement Farabet, Pedro Valenzuela, Quan Yuan, Ananth Agarwal,\\nMia Chen, Wooyeol Kim, Brice Hulse, Nandita Dukkipati, Adam Paszke, Andrew Bolt, Kiam Choo,\\nJennifer Beattie, Jennifer Prendki, Harsha Vashisht, Rebeca Santamaria-Fernandez,\\nLuis C. Cobo, Jarek Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin Ramirez,\\nMatt Harvey, Tyler Liechty, Heiga Zen, Jeff Seibert, Clara Huiyi Hu, Andrey Khorlin, Maigo Le,\\nAsaf Aharoni, Megan Li, Lily Wang, Sandeep Kumar, Norman Casagrande, Jay Hoover,\\nDalia El Badawy, David Soergel, Denis Vnukov, Matt Miecnikowski, Jiri Simsa,\\nPraveen Kumar, Thibault Sellam, Daniel Vlasic, Samira Daruki, Nir Shabat, John Zhang,\\nGuolong Su, Jiageng Zhang, Jeremiah Liu, Yi Sun, Evan Palmer, Alireza Ghaffarkhah, Xi Xiong,\\nVictor Cotruta, Michael Fink, Lucas Dixon, Ashwin Sreevatsa, Adrian Goedeckemeyer,\\nAlek Dimitriev, Mohsen Jafari, Remi Crocker, Nicholas FitzGerald, Aviral Kumar,\\nSanjay Ghemawat, Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina,\\nMarcus Wu, Laura Knight, Marin Georgiev, Hyo Lee, Harry Askham, Abhishek Chakladar,\\nAnnie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn,\\nDenese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim, Damien Vincent,\\nMilad Nasr, Christopher A. Choquette-Choo, Reiko Tojo, Shawn Lu, Diego de Las Casas,\\nYuchung Cheng, Tolga Bolukbasi, Katherine Lee, Saaber Fatehi, Rajagopal Ananthanarayanan,\\nMiteyan Patel, Charbel Kaed, Jing Li, Shreyas Rammohan Belle,\\nZhe Chen, Jaclyn Konzelmann, Siim P\u00f5der, Roopal Garg, Vinod Koverkathu, Adam Brown,\\nChris Dyer, Rosanne Liu, Azade Nova, Jun Xu, Alanna Walton, Alicia Parrish, Mark Epstein,\\nSara McCarthy, Slav Petrov, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean,\\nand Oriol Vinyals. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of\\ntokens of context. arXiv preprint arXiv: 2403.05530.\\n\\nJiaan Wang, Yunlong Liang, Fandong Meng, Zengkui\\nSun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng\\nQu, and Jie Zhou. 2023a. Is chatgpt a good nlg\\nevaluator? a preliminary study. arXiv preprint arXiv:\\n2303.04048.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We engaged 17 graduate student volunteers with a good understanding of Large Language Models to manually verify the perturbations. Each annotator was provided with the instruction, the original gold answer, and the GPT-4-TURBO generated perturbed answer. They were tasked with classifying each perturbation into one of five categories: (i) Valid Perturbation, (ii) Invalid Perturbation, (iii) Score Invariant Perturbation, (iv) Not Relevant, and (v) Not Sure. Additionally, annotators were given explanations of the expected perturbations and the reasons why GPT-4-TURBO considered them valid.\\n\\nTo facilitate this process, we developed a straightforward application, the interface of which is depicted in Figure 3. This tool highlights the differences between the original and perturbed answers to aid easy identification. Annotators were instructed to label an answer as \u201cValid Perturbation\u201d only if they believed the perturbation warranted a score penalty relative to the gold answer. Perturbations not affecting the score were to be labeled \u201cScore Invariant\u201d. If a perturbation was deemed incorrect or not reflected in the perturbed answer, annotators were asked to adjust the perturbation manually. Perturbations irrelevant to the category were to be marked as \u201cNot Relevant\u201d.\\n\\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv preprint arXiv: 2305.17926.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Detailed Results of Single Answer Evaluators can be found in Table 6, 7, 8, 9, 10.\\n\\nDetailed Results of Pairwise Evaluators can be found in Table 11, 12, 13, 14, 15.\\n\\nDetailed results of Reference-guided Evaluators can be found in Table 16, 17.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type          | TOTAL Errors | Detected Errors | Undetected Errors | % Undetected Errors |\\n|---------------------------|--------------|-----------------|-------------------|---------------------|\\n| LF                        | 91           | 78              | 13                | 0.14                |\\n| Coherence                 | 90           | 9               | 82                | 0.91                |\\n| Comprehensiveness         | 84           | 16              | 68                | 0.81                |\\n| Grammar                   | 92           | 25              | 67                | 0.73                |\\n| Chronology                | 71           | 7               | 64                | 0.90                |\\n| Spelling                  | 100          | 11              | 89                | 0.89                |\\n| TOTAL                     | 528          | 146             | 383               | 0.73                |\\n| Contextual                | 94           | 41              | 53                | 0.56                |\\n| Entity                    | 87           | 29              | 58                | 0.67                |\\n| Incorrect Fact            | 68           | 24              | 44                | 0.65                |\\n| Number Errors             | 74           | 22              | 52                | 0.70                |\\n| Opposite Fact             | 91           | 39              | 52                | 0.57                |\\n| Remove Fact               | 69           | 4               | 65                | 0.94                |\\n| TOTAL                     | 483          | 159             | 324               | 0.67                |\\n| Assumptions               | 81           | 4               | 77                | 0.95                |\\n| D0leS                     | 100          | 32              | 68                | 0.68                |\\n| D0more                    | 50           | 34              | 16                | 0.32                |\\n| Ignore Format             | 99           | 36              | 63                | 0.64                |\\n| Sequence Errors           | 49           | 4               | 45                | 0.92                |\\n| TOTAL                     | 379          | 110             | 269               | 0.71                |\\n| Calculations              | 149          | 121             | 28                | 0.19                |\\n| Copying Numbers           | 83           | 69              | 14                | 0.17                |\\n| Final Errors              | 97           | 54              | 43                | 0.44                |\\n| Incorrect Units           | 77           | 66              | 11                | 0.14                |\\n| Wrong Formula             | 88           | 73              | 15                | 0.17                |\\n| TOTAL                     | 494          | 383             | 111               | 0.22                |\\n\\nTable 6: Results from evaluating FBI using Vanilla evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.\\n\\n| Perturbation Type          | TOTAL Errors | Detected Errors | Undetected Errors | % Undetected Errors |\\n|---------------------------|--------------|-----------------|-------------------|---------------------|\\n| LF                        | 91           | 78              | 13                | 0.14                |\\n| Coherence                 | 90           | 9               | 82                | 0.91                |\\n| Comprehensiveness         | 84           | 16              | 68                | 0.81                |\\n| Grammar                   | 92           | 25              | 67                | 0.73                |\\n| Chronology                | 71           | 7               | 64                | 0.90                |\\n| Spelling                  | 100          | 11              | 89                | 0.89                |\\n| TOTAL                     | 528          | 146             | 383               | 0.73                |\\n| Contextual                | 94           | 41              | 53                | 0.56                |\\n| Entity                    | 87           | 29              | 58                | 0.67                |\\n| Incorrect Fact            | 68           | 24              | 44                | 0.65                |\\n| Number Errors             | 74           | 22              | 52                | 0.70                |\\n| Opposite Fact             | 91           | 39              | 52                | 0.57                |\\n| Remove Fact               | 69           | 4               | 65                | 0.94                |\\n| TOTAL                     | 483          | 159             | 324               | 0.67                |\\n| Assumptions               | 81           | 4               | 77                | 0.95                |\\n| D0leS                     | 100          | 32              | 68                | 0.68                |\\n| D0more                    | 50           | 34              | 16                | 0.32                |\\n| Ignore Format             | 99           | 36              | 63                | 0.64                |\\n| Sequence Errors           | 49           | 4               | 45                | 0.92                |\\n| TOTAL                     | 379          | 110             | 269               | 0.71                |\\n| Calculations              | 149          | 121             | 28                | 0.19                |\\n| Copying Numbers           | 83           | 69              | 14                | 0.17                |\\n| Final Errors              | 97           | 54              | 43                | 0.44                |\\n| Incorrect Units           | 77           | 66              | 11                | 0.14                |\\n| Wrong Formula             | 88           | 73              | 15                | 0.17                |\\n| TOTAL                     | 494          | 383             | 111               | 0.22                |\\n\\nTable 7: Results from evaluating FBI using Vanilla evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.\"}"}
{"id": "emnlp-2024-main-911", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Perturbation Type       | TOTAL Errors | Detected Errors | Undetected Errors | % Undetected Errors |\\n|-------------------------|--------------|-----------------|-------------------|---------------------|\\n| LF COHERENCE            | 91           | 58              | 33                | 0.36                |\\n| COMPREHENSIVENESS       | 90           | 1               | 89                | 0.99                |\\n| CONSISTENCY             | 84           | 8               | 76                | 0.90                |\\n| GRAMMAR                 | 92           | 17              | 75                | 0.82                |\\n| CHRONOLOGY              | 71           | 0               | 71                | 1.00                |\\n| SPELLING                | 100          | 6               | 94                | 0.94                |\\n| ________________________ | ____________ | ____________   | ____________      | ____________        |\\n|                        | 528          | 90              | 438               | 0.83                |\\n| CONTEXTUAL              | 94           | 29              | 65                | 0.69                |\\n| ENTITY                  | 87           | 30              | 57                | 0.66                |\\n| INCORRECT FACT           | 68           | 17              | 51                | 0.75                |\\n| NUMBER ERRORS            | 74           | 18              | 56                | 0.76                |\\n| OPPOSITE FACT            | 91           | 32              | 59                | 0.65                |\\n| REMOVE FACT              | 69           | 1               | 68                | 0.99                |\\n| ________________________ | ____________ | ____________   | ____________      | ____________        |\\n|                        | 483          | 127             | 356               | 0.74                |\\n| ASSUMPTIONS             | 81           | 5               | 76                | 0.94                |\\n| DO LESS                  | 100          | 20              | 80                | 0.80                |\\n| DO MORE                  | 50           | 40              | 10                | 0.20                |\\n| IGNORE FORMAT            | 99           | 25              | 74                | 0.75                |\\n| SEQUENCE ERRORS          | 49           | 5               | 44                | 0.90                |\\n| ________________________ | ____________ | ____________   | ____________      | ____________        |\\n|                        | 379          | 95              | 284               | 0.75                |\\n| CALCULATIONS             | 149          | 100             | 49                | 0.53                |\\n| COPYING NUMBERS          | 83           | 57              | 26                | 0.31                |\\n| FINAL ERRORS             | 97           | 46              | 51                | 0.53                |\\n| INCORRECT UNITS          | 77           | 42              | 35                | 0.45                |\\n| WRONG FORMULA            | 88           | 63              | 25                | 0.28                |\\n| ________________________ | ____________ | ____________   | ____________      | ____________        |\\n|                        | 494          | 308             | 186               | 0.43                |\\n\\nTable 8: Results from evaluating FBI using Rubrics evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.\\n\\nTable 9: Results from evaluating FBI using Axis evaluator. An error is said to be detected if the evaluator penalizes the score of the perturbed answer.\"}"}
