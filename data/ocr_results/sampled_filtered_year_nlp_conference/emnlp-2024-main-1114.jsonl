{"id": "emnlp-2024-main-1114", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: confusion matrices for salience prediction across different models.\\n\\n(a) GPT-4-turbo zero-shot vanilla (left), GPT-4-turbo few-shot vanilla (right)\\n(b) Mistral-7B-Instruct (left), Llama-2-7B-chat (right)\\n(c) Flan-t5-base (left), TinyLlama-1.1B-chat (right)\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: Annotation interface for the summary expansion task; the three candidates are ordered via drag-and-drop.\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The International Monetary Fund (IMF) has agreed to provide Ukraine with a $15.6 billion financial package, a significant move as it\u2019s the first loan offered to a country engaged in war. This agreement, structured in two phases, aims to first enhance Ukraine\u2019s fiscal and financial stability, with a subsequent focus on broader reforms. This decision comes after recent changes in the IMF\u2019s policies, now allowing funding for countries experiencing \u2018exceptionally high uncertainty\u2019. The journey to this agreement began last June when Ukraine first initiated discussions with the IMF, leading to a final approval by the IMF executive board on March 20, after an initial round of talks failed and a second was launched in August. Meanwhile, Pakistan faces challenges in securing a vital IMF bailout package. The government revealed that the IMF is seeking financial support for Pakistan from \u2018friendly\u2019 nations before proceeding with a $6.5 billion bailout program. This stance was reiterated with a slight variation in the required amount, mentioning a $6.1 billion tranche. Criticism has been directed at the IMF\u2019s approach to assisting poor countries. Nobel Prize-winning economist Joseph Stiglitz, in his 2002 book \u201cGlobalisation and Its Discontents,\u201d criticized the IMF\u2019s economic reform conditions. He argued that these often counterproductive measures have had devastating effects on the populations of the target countries, highlighting a broader debate on the IMF\u2019s role and policies in global financial stability.\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Which questions should I answer?\\n\\nSalience Prediction of Inquisitive Questions\\n\\nYating Wu*1, Ritika Mangla*2, Alexandros G. Dimakis1,4, Greg Durrett2, Junyi Jessy Li3\\n\\n1 Electrical and Computer Engineering, 2 Computer Science, 3 Linguistics\\nThe University of Texas at Austin, 4 BespokeLabs.ai\\n{yating.wu, jessy}@utexas.edu\\n\\nAbstract\\n\\nInquisitive questions \u2014 open-ended, curiosity-driven questions people ask as they read \u2014 are an integral part of discourse processing (Van Kuppevelt, 1995; Onea, 2016; Kehler and Rohde, 2017) and comprehension (Prince, 2004). Recent work in NLP has taken advantage of question generation capabilities of LLMs to enhance a wide range of applications. But the space of inquisitive questions is vast: many potential questions can be evoked from a given context. So which of those should be prioritized to find answers? Linguistic theories, unfortunately, have not yet provided an answer. This paper presents QSALIENCE, a salience predictor of inquisitive questions. QSALIENCE is instruction-tuned over our dataset of linguist-annotated salience scores of 1,766 (context, question) pairs. A question scores high on salience if answering it would greatly enhance the understanding of the text (Van Rooy, 2003). We show that highly salient questions are empirically more likely to be answered in the same article, bridging potential questions (Onea, 2016) with Questions Under Discussion (Roberts, 2012). We further validate our findings by showing that answering salient questions is an indicator of summarization quality in news.\\n\\n1 Introduction\\n\\nAsking questions is the natural language manifestation of human inquisitiveness: we insist on getting answers for what we are curious about since childhood (Chouinard et al., 2007). Acquired strategies of question generation have a profound impact on education (Davey and McBride, 1986; Prince, 2004). In linguistics, both theoretical and psycholinguistic work argued that readers generate inquisitive questions, seeking information in a conversation or as they read (Van Kuppevelt, 1995; Ginzburg, 1996; Onea, 2016; Kehler and Rohde, 2017). In NLP, pre-trained models have enabled the generation of these open-ended, curiosity-driven, information-seeking questions, leading to a flourish of recent work: identifying information loss between two texts (Trienes et al., 2024; Cole et al., 2023), analyzing the diversity of news perspectives (Laban et al., 2022), generating elaborations or explanations (Wu et al., 2023b; Fok et al., 2023), evaluating summaries (Pratapa et al., 2023), asking follow-up questions (Meng et al., 2023), decontextualization (Newman et al., 2023), and planning (Narayan et al., 2023).\\n\\nHowever, the space of possible inquisitive questions is vast. Prior work (Ko et al., 2020; Westera et al., 2020) showed that many distinct questions can be evoked from a given context, yet not all questions are equally good for an application. In theoretical linguistics, this also brings up a long-standing gap in understanding how discourse progresses (Warstadt, 2020): some of such inquisitive \\\"potential questions\\\" (as named in Onea (2016)) are likely more pertinent than others. Some of these questions may be answered (by the writer) later in the article and thus become Questions Under Discussion (QUDs) (Roberts, 2012). Evidence in psycholinguistics indicate that readers form expectations how a discourse would progress (Kehler and Rohde, 2017), providing a foundation for the predictability of QUDs (Westera et al., 2020). Van Rooy (2003) argues that a question is important if answering it provides high utility. However, there is so far no computational work to predict whether or not those questions should be answered or how salient they are.\\n\\nThis work (Figure 1) seeks to answer these questions by training a salience prediction model for inquisitive questions, using a new linguist-annotated corpus. In line with Van Rooy (2003), a question scores high on salience if answering it would greatly enhance the understanding of the text. First, we collected validity and salience ratings of 1,766 inquisitive questions over English news articles.\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Amid skepticism that Russia's war in Chechnya can be ended across a negotiating table, peace talks were set to resume Wednesday in neighboring Ingushetia.\\n\\nThe scheduled resumption of talks in the town of Sleptsovsk came two days after an agreement on a limited ceasefire, calling for both sides to stop using heavy artillery Tuesday.\\n\\nMany Chechens are fighting independently of the forces loyal to Chechen President Dzhokhar Dudayev, and Dudayev's representative at the peace talks, Aslan Maskhadov, warned that he does not control them.\\n\\n---\\n\\n**Q1** What other progress has been made toward peace recently?\\n\\n**Q2** What is the significance of the limited ceasefire agreement that was reached?\\n\\n**Q3** What is the reason behind the artillery fire in Grozny on Tuesday despite the agreed ceasefire?\\n\\n**Q4** What are the reports of Chechen missile attacks southwest of the Chechen capital?\\n\\n**Q5** What is the source of the Chechen missile attacks?\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that they are limited and presented no formal or empirical validation. Kehler and Rohde (2017)'s psycholinguistic experiments showed that people form expectations of what QUDs are upcoming using contextual cues, and that such expectations affect their interpretation of linguistic forms. This is compelling evidence for the incremental processing of discourse (Altmann and Steedman, 1988; Cristea and Webber, 1997) and why people ask questions. Westera et al. (2020) later studied how likely a potential question is answered, using the TED-Q corpus that annotates both questions and answers in a (limited) moving window of context. Yet this work focuses on the predictability of QUDs, rather than a reader-centric view of salience as in ours. Salience defined in our work is in line with Van Rooy (2003)'s information-theoretic argument that questions are salient when information utility of the answer is high; yet empirical evidence at-scale is yet to be seen.\\n\\nApplications: Generating Inquisitive Questions\\n\\nPrior work developed datasets and models for generating inquisitive questions (defined as open-ended high-level questions targeting discourse understanding) (Ko et al., 2020; Gao et al., 2022), which was later used in a range of applications (Laban et al., 2022; Wu et al., 2023b; Fok et al., 2023; Newman et al., 2023; Trienes et al., 2024). However, this existing work does not explicitly define or model question salience. In QUD parsing, prior work focuses on what makes questions linguistically felicitous QUDs (Riester et al., 2018; Wu et al., 2023a).\\n\\nA question salience model, however, is often necessary in downstream applications. For instance, in elaborative simplification (Wu et al., 2023b), the lack of a salience model means that existing approaches cannot predict which concepts to insert explanations for. Indeed, the over-generation of valid, fine-grained inquisitive questions is undesirable and can easily overwhelm the readers (Trienes et al., 2024). In goal-oriented forums, Rao and Daum\u00e9 III (2018) calculated information utility from the answers to rank clarification questions; however this presupposes an explicit discourse goal to solve a specific task. While domain-specific notions of salience can sometimes be implicitly captured in end-to-end training with a downstream gold-standard (e.g., in summarization planning (Narayan et al., 2023)), it does not apply to most prior work mentioned above, as they are more open-ended.\\n\\n3 Task Definition\\n\\nA (human or machine) reader is reading a document, with established context $C_p$ (preceding context) consisting of sentences $\\\\{S_1, ..., S_{k-1}\\\\}$. The reader generates a potential question (Section 2; Onea (2016)) $Q_{evoked}$ at sentence $S_k$ (also called the \\\"anchor sentence\\\" (Wu et al., 2023a)).\\n\\nThe salience of $Q$ is the measure of the extent to which it is important for a question $Q$ to be answered, in order to gain a fuller understanding of the situation described, after its invocation at sentence $S_k$ (Van Rooy, 2003). Specifically, for all valid questions, we define a Likert scale of 1-5 (full definitions found in Appendix A):\\n\\n- Score = 1: $Q$ is not related to $C_p$.\\n- Score = 2: $Q$ is related to $C_p$ but seems to be a stretch to ask, and answering it is not useful.\\n- Score = 3: $Q$ is related to $C_p$ but whether it is answered does not matter much to the reader.\\n- Score = 4: $Q$ is related to $C_p$ and answering it might clarify some newly introduced concepts, or might expand on an idea already introduced.\\n- Score = 5: $Q$ is related to $C_p$ and it should definitely be answered as it clarifies a concept introduced in $S_k$ or asks for more information about newly introduced humans (or animated) individuals into the discourse.\\n\\nA question is invalid if it contains grammatical or factual errors, or is not anchored in $S_k$. The last criteria follows linguistic constraints in Wu et al. (2023a) reflecting that the content of $Q$ is not grounded in $S_k$, hence should not be evoked at $S_k$.\\n\\nA note on subjectivity. The salience values are, to some degree, subjective. However, prior work has shown compelling evidence that certain QUDs are more predictable than others (Westera et al., 2020) and that linguistic cues in the text play a significant role in readers' expectation (Kehler and Rohde, 2017). Under the assumption that there isn't too much divergence between the authors' intended audience and the background of the actual readers, our work sets out to capture such expectations through a question salience score.\\n\\n4 Data Collection\\n\\nWe first present QSALIENCE-data, a corpus of 1,766 inquisitive questions annotated with salience,\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"plus natural language rationales for their judgments. Although question generation has been used widely, application-independent datasets consisting of human-generated inquisitive questions are scarce. Thus, we generate questions with LLMs, both to obtain a sizable amount of data and also to understand inquisitive question generation capabilities of LLMs. We supplement these questions with a smaller number of human-generated questions from prior work, which allows us to perform deeper analysis (Section 5).\\n\\n4.1 Source Texts and Questions\\nTable 1 summarizes the number of source articles and questions in QSALIENCE-data. We draw these from different existing corpora to support different facets of our experimental analysis. They are from:\\n\\n(1) News texts from DCQA (Ko et al., 2022). We generate questions from DCQA articles, with gradually increasing size of $C_p$. Additionally, the annotated QUDs in DCQA allows us to study the salience of QUDs compared to inquisitive questions in general (Section 5).\\n\\n(2) TED talks from TED-Q (Westera et al., 2020). In addition to LLM-generated questions, we also annotate the salience of one of the 6 excerpts with human generated questions in the TED-Q dataset. This provides data for further analysis on question salience vs. how answerable they are (Section 5).\\n\\n(3) DiverseSumm (Huang et al., 2024) contains a newer set of news articles for which we annotate salience of LLM-generated questions. For convenience, we denote this subset as Div. Article. These are source articles for Section 7, our downstream task. To ensure fair evaluation for the task, the articles we selected were all roughly 1,500 words. Additionally, we annotate question salience on a set of GPT-4 generated short TL;DRs for these articles. We denote this subset Div. TL;DR.\\n\\nMachine Generated Questions\\nGiven the preceding context $C_p$ along with the anchor sentence $S_k$, we prompt LLMs to generate 5 questions about a part of the sentence that a reader may be curious about (settings and prompt in Appendix C.1). Multiple LLMs were used to cover a more diverse set of question styles in the dataset. Specifically, 250 questions were generated from Llama-2-7B-chat, 249 from Mistral-7B-instruct, 100 from GPT-3.5-turbo, and 1,106 from GPT-4-turbo.\\n\\nFor full articles, we begin the question generation process from the 4th sentence until the 16th sentence, maintaining a gap of two sentences between consecutive question generation probes, similar to Westera et al. (2020). For the DiverseSumm TL;DRs which are typically 3 sentences long (50 words), we generate questions per sentence.\\n\\nHuman Generated Questions\\nWe annotate the salience of 61 human generated questions from the above sources, to perform analyses in Section 5. Among those, 36 of them are derived from 2 articles of DCQA and 25 of them are from one article of TED-Q.\\n\\n4.2 Salience Annotation\\nQSALIENCE-data is annotated by three linguistics undergraduate students at our institution who are native English speakers. They have previously been involved in multiple linguistic annotation tasks and have been trained on our specific annotation guideline on 50 questions (25 questions \u00d7 2 articles). The annotation guideline can be found in Appendix A.\\n\\nIn addition to the labels, annotators also provide natural language rationales which we release with QSALIENCE-data. These rationales are used in few-shot Chain-of-Thought prompting (Wei et al., 2024) in Section 6. The annotators were paid at least $15/hr.\\n\\nAgreement\\nThe inter-annotator agreement (IAA) as measured by the Krippendorff's alpha (Krippendorff, 2011) (ordinal, with the \\\"invalid\\\" label set to 0) is 0.719 for the DCQA articles, 0.632 for TED-Q, 0.751 for Div. Article and 0.649 for Div. TL;DR. These values indicate substantial agreement (Artstein and Poesio, 2008), providing evidence to the predictability of reader expectations manifested as inquisitive questions.\\n\\nAggregation\\nFor label aggregation, we take the average salience of all annotations, then round it to the closest integer.\\n\\nAnalysis\\nExamples of the annotated data are shown in Figure 1 and Appendix Table 13. Table 2 provides the label distribution for QSALIENCE-data. Notably, more than 90.8% of the questions generated from LLMs are valid, making them...\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Count of articles and questions, average length and standard deviation of human and machine-generated questions per dataset.\\n\\n| Dataset     | # Articles | # Questions | Average Length | Standard Deviation |\\n|-------------|------------|-------------|----------------|--------------------|\\n| DCQA        | 4          | 260         | 11.97          | 4.76               |\\n| TED-Q       | 1          | 100         | 11.07          | 3.9                |\\n| Div. Article| 27         | 957         | 14.8           | 4.44               |\\n| Div. TL;DR  | 34         | 449         | 16.32          | 3.78               |\\n| All         | 66         | 1766        | 13.99          | 4.57               |\\n\\nTable 2: Validity and salience distribution (in %) of human-annotated labels for the questions in QSALIENCE-data.\\n\\n| Dataset     | 1 | 2 | 3 | 4 | 5 |\\n|-------------|---|---|---|---|---|\\n| DCQA        | 13.4| 2.3|19.2|36.9|20.0|8.0|\\n| TED-Q       | 14.0|0  | 6.0|29.0|35.0|16.0|\\n| Div. Article| 9.9| 0.8|17.2|22.6|31.9|17.6|\\n| Div. TL;DR  | 3.6| 0.2| 4.7|12.0|47.0|32.5|\\n| All         | 9.1| 0.8|13.7|22.4|34.1|19.9|\\n\\nPromising tools for inquisitive question generation. Our qualitative analysis of annotator rationales for invalid questions show that many of them do not have the right anchor sentence (i.e., Q not anchored in S); this was also found in Wu et al. (2023a). A few invalid questions also contain non-factual presuppositions. Among valid questions, those with the lowest score of 1 (question was irrelevant to C) is rare. However, the salience of the questions varies, indicating the potential usefulness of a salience predictor for LLM-generated questions in downstream tasks. We further analyze salience scores stratified by the LLMs that generate the questions in Appendix C.2.\\n\\nIn Appendix H, we show that question types that are more likely to associate with high salience ratings are Consequence, Example, and Procedural. Disjunctive, Concept, and Judgmental.\\n\\nTable 3: Distribution (in %) of human-annotated answerability labels for 311 questions stratified by data source.\\n\\n| Dataset     | 0  | 1 | 2 | 3 |\\n|-------------|----|---|---|---|\\n| DCQA        | 0.28| 0.35| 0.16| 0.21|\\n| TED-Q       | 0.07| 0.22| 0.23| 0.48|\\n\\nTable 4: Spearman rank correlation between salience and answerability annotated by humans and a random baseline. The correlation values that are not statistically significant (p < 0.05) are marked with an *.\\n\\nTable 5: Salience distribution of 36 DCQA questions that are annotated QUDs. Similar to the previous analysis, we also take a random subset of 36 potential questions, averaging their scores over 10 trials and present their salience distribution.\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Salience distribution for 36 human annotated QUDs from DCQA, compared to a random set of inquisitive questions of the same size.\\n\\n6 Salience Prediction\\nWe experiment with a range of models for the prediction of question salience, given valid questions. Our finding is that salience prediction is a discourse task that recovers implicit information not readily grasped by LLMs, while our best instruction-tuned model, QSALIENCE, can achieve moderate agreement with humans. We further present question validity classifiers in Appendix B, which can be used with QSALIENCE in a pipeline fashion.\\n\\n6.1 Models\\nInstruction Tuning\\nWe instruction fine-tune several open-source LLMs with QLoRA (Dettmers et al., 2023):\\n- Mistral (Jiang et al., 2023) (Mistral-7B-Instruct),\\n- Llama 2 (Touvron et al., 2023) (Llama-2-7b-chat),\\n- TinyLlama (Zhang et al., 2024a) (TinyLlama-1.1B-chat),\\n- Flan-T5 (Chung et al., 2024) (flan-t5-base).\\nAdamW (Loshchilov and Hutter, 2018) is used for optimization. Hyperparameters can be found under Table 11 in the Appendix.\\n\\nThe training data is formulated as (input, output) pairs where input consists of \\\\( C_p, S_k, Q \\\\), and output is the salience score. Appendix E shows the instructions for these models. For Flan-T5, since the context span is 512 tokens, we also experiment without using instructions, and truncate the context in the reverse sentence order from \\\\( C_p \\\\) and \\\\( S_k \\\\) until the context length is filled.\\n\\nLLM Zero-/Few-shot Baselines\\nWe perform extensive experiments with various zero-shot and in-context learning scenarios with GPT-4-turbo. We show prompts in Appendix D.\\n\\n(1) Zero-shot (vanilla), where the model is given an instruction similar to that of the annotators.\\n(2) Few-shot (vanilla), where 15 in-context learning examples (3 per label) of \\\\((C_p, C_k, Q, scr)\\\\) are given, where \\\\( scr \\\\) denotes the salience score. We utilize LLMs' recency bias (Liu et al., 2024) to nudge its prediction to better align with our label distribution. Thus we altered the order of in-context demonstrations such that the examples at the end have labels more frequent within our train set.\\n(3) Few-shot (kNN). Performance of LLMs is often sensitive to the selection of the in-context examples (Rubin et al., 2022). Hence we use a kNN-based approach (Liu et al., 2022) to find the closest in-context examples to the current test instance. We encode \\\\( C_p \\\\) and \\\\( S_k \\\\) separately with RoBERTa-large (Liu et al., 2019) and take the average of the CLS tokens of each. We use Euclidean distance and retrieve one closest example for each salience label. These examples are put in-context following a similar ordering as the few-shot (vanilla) setting.\\n(4) Chain-of-Thought (CoT). We experimented with Chain-Of-Thought prompting (Wei et al., 2024). For few-shot CoT, we use 5 in-context examples, with the reasoning taken from the natural language rationales that the annotators gave during salience annotation.\\n\\n6.2 Evaluation\\nData\\nWe create a test set of 235 valid questions. The rest of the dataset is split into training (1,228 valid questions) and validation (143 valid questions). The data splits are stratified by articles. We upsample the training data to balance the label distribution. Our final training set consists of 2,355 examples, where each label has its 471 examples. We do not upsample validation or test sets.\\n\\nEvaluation Metrics\\nWe measure the performance of salience prediction using four metrics:\\n\\n(1) Mean Absolute Error (MAE) between the predicted salience scores and the aggregated human scores;\\n(2) Spearman\u2019s \\\\( \\\\rho \\\\) between the two;\\n(3) macro-averaged F1. These are standard metrics for ordinal classification or regression. In addition, we report (4) Krippendorff\u2019s \\\\( \\\\alpha \\\\) that measures agreement, also used in Section 4.2 between annotators.\\n\\nResults\\nTable 6 shows that the fine-tuned models clearly outperform zero- or few-shot LLMs, even with stronger prompting techniques such as kNN-based in-context example retrieval and Chain-of-Thought. On the contrary, among the fine-tuned smaller models, the best performing Mistral-based...\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Model performance on the salience prediction task, for GPT-4 zero/few-shot baselines (top) and instruction-tuned LLMs (bottom).\\n\\n| Model                                | MAE   | Spearman | Macro F1 | krippendorff's \u03b1 |\\n|--------------------------------------|-------|----------|----------|------------------|\\n| GPT4 zero-shot (vanilla)             | 1.314 | 0.229    | 0.193    | -0.141           |\\n| GPT4 few-shot (vanilla)              | 0.910 | 0.417    | 0.316    | 0.358            |\\n| GPT4 few-shot (kNN)                  | 1.063 | 0.359    | 0.245    | 0.215            |\\n| GPT4 CoT zero-shot                   | 1.144 | 0.366    | 0.197    | 0.058            |\\n| GPT4 CoT few-shot                    | 1.034 | 0.327    | 0.292    | 0.165            |\\n| Quesary (Mistral-7B-instruct)        | 0.579 | 0.623    | 0.417    | 0.615            |\\n| Llama-2-7B-chat                      | 0.626 | 0.566    | 0.413    | 0.557            |\\n| Flan-t5-base                         | 0.706 | 0.542    | 0.370    | 0.526            |\\n| TinyLlama-1.1B-chat                  | 0.664 | 0.522    | 0.402    | 0.496            |\\n\\nBold: top-2 performance; blue shades: best performance for baselines and for fine-tuned models.\\n\\nModel achieves moderate agreement with human annotation with a substantial correlation. Even flan-T5-base with only 250M parameters and a small context window can be fine-tuned for this task to achieve competitive performance. These conclusions indicate that question salience is difficult to elicit from LLM prompting or in-context learning, and that explicit training can successfully capture this notion.\\n\\nAppendix Figure 3 shows the confusion matrix for zero-shot GPT-4, the best-performing GPT-4 setting (few-shot vanilla), and our fine-tuned models.\\n\\n5 Compared to fine-tuned models, GPT-4 tends to give a high score for the question by predicting many 4s and 5s, indicating its inability to distinguish good vs. bad questions irrespective of the Likert scale. By comparison, predictions from fine-tuned models tend to confuse primarily labels closer to each other. This also shows our fine tuned models understand the tasks better than in-context learning with GPT-4.\\n\\n7 Use Case: Do better expanded summaries answer more salient questions?\\n\\nWe demonstrate the usefulness of question salience prediction in a downstream task: summary expansion. Given a document $D$ and a short TL;DR $S_s$, the summary expansion task aims to generate a longer summary $S_l$ that captures a fuller picture of the article, shown in Figure 2. This task captures the situation where after reading the TL;DR, a curious reader often wants to know more in order to decide if they want to read the entire article. A similar task is deployed in Semantic Scholar, though in a very different domain than ours.\\n\\n5 Note that the label 1 is extremely rare (Table 2) and is not present in the test set.\\n\\nGiven findings in Section 4.2, our hypothesis is that reader expectation aligns with QUDs in the expanded summary; namely, a higher-quality summary answers more salient questions.\\n\\n7.1 Data\\n\\nArticles and TL;DRs\\n\\nWe sample 34 articles from DiverseSum as source articles; these articles are roughly 1,500 words long. Given the strong performance of GPT-* in summarization (Goyal et al., 2022; Zhang et al., 2024b), we prompt GPT-4-turbo to produce a 50-word summary TL;DR of the article. All prompts needed in this section are shown in Appendix F.\\n\\nExpanded Summaries\\n\\nFor each TL;DR, we generate three expanded summaries while controlling their lengths to be between 230\u2013250 words:\\n\\n1. GPT-4: given $(D, S_s)$, we prompt GPT-4-turbo for $S_l$.\\n2. Flan-T5: we use flan-t5-large to produce an elaboration in a similar manner as above. Since this model can produce summaries with obvious errors such as repeating sentences and violating the length control, we refine the Flan-T5 outputs to fix these obvious errors, while preserving the summary as much as possible, using GPT-4-turbo.\\n3. GPT-4-Corrupted: We synthetically generate \u201cbad\u201d summaries to serve as a baseline that is missing the most prevalent topics. First, we prompt GPT-4-turbo with $(D, S_s)$ and ask it to identify important topics from $S_s$. Using these topics and $D$, we then prompt the model to generate a long summary within our expected word count that does not include these relevant topics. Since this response can sometimes be incoherent and disobey...\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hong Kong and Singaporean authorities welcome UBS's acquisition of Credit Suisse. Temasek Holdings reportedly shows interest in investing in the banking sector of the Philippines. On the regulative side, CDPQ is reportedly interested in acquiring the Montreal General Insurance Company. This period underscores the region's keen interest in the telecommunication sector to Credit Suisse\u2026 Similarly, the US is set to begin inspections on Hong Kong's financier, underscoring the limited exposure of the local institutions to Credit Suisse\u2026 On the other hand, the US prepares to inspect Hong Kong's ATC, underscoring the risk Credit Suisse's situation poses to the region, the US is set to begin inspections on Hong Kong's ATC, underscoring the risk Credit Suisse's situation poses to the region, while CDPQ is considering purchasing a unit.\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a theoretical idea of which questions are useful for understanding and likely to be answered later in a text, and an empirical notion of what questions are useful. We showed that predicting salience is possible with fine-tuned models, and these approaches outperform GPT-4. Furthermore, we showed in a pilot use case that notions of summary quality align with how many salient questions were answered.\\n\\nLimitations\\nWhile this work takes the first step at empirically connecting prior discourse literature and developing a salience model for inquisitive questions, we have not engaged in the formal semantics of potential questions as in Onea (2016). An additional limitation is that we have not explicitly measured information utility (in information-theoretic terms) given the open-ended nature of the questions, although our notion of salience is consistent with Van Rooy (2003).\\n\\nThis work has considered only English text, sourcing articles from existing datasets that provided groundwork for various analyses in this paper, both theoretical ones and empirical experiments. Thus even though our notion of question salience is application-agnostic, we believe an exciting future direction is to explore question salience in other domains and languages.\\n\\nFinally, when considering the notions of salience for our texts, we assume that the reader backgrounds are not too divergent from what the writer has intended. A large discrepancy between the two could lead to readers having very different salient questions; e.g., when the reading level of the reader is much lower than that of the intended audience (Wu et al., 2023b). Thus our tool and dataset should not be used when reader backgrounds are too different from the writer expectations or among themselves.\\n\\nAcknowledgments\\nSpecial thanks to Kathryn Kazanas, Keziah Reina, Karim Villaescusa F, Akhila Gunturu, Andrea Conde, Jada Li and Melanie Quintero for providing data annotation for this project. This research has been supported by NSF Grants IIS 2145479, IIS 2145280, AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844, a grant from Open Philanthropy, and research gifts by Western Digital, Amazon, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco, and the Stanly P. Finch Centennial Professorship in Engineering.\\n\\nReferences\\nGerry Altmann and Mark Steedman. 1988. Interaction with context during human sentence processing. Cognition, 30(3):191\u2013238.\\nRon Artstein and Massimo Poesio. 2008. Survey article: Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555\u2013596.\\nAnton Benz and Katja Jasinskaja. 2017. Questions under discussion: From sentence to discourse. Discourse Processes, 54(3):177\u2013186.\\nShuyang Cao and Lu Wang. 2021. Controllable open-ended question generation with a new question type ontology. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6424\u20136439, Online. Association for Computational Linguistics.\\nMichelle M Chouinard, Paul L Harris, and Michael P Maratsos. 2007. Children's questions: A mechanism for cognitive development. Monographs of the society for research in child development, pages i\u2013129.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1\u201353.\\nJeremy R. Cole, Palak Jain, Julian Martin Eisenschlos, Michael J.Q. Zhang, Eunsol Choi, and Bhuwan Dhingra. 2023. DiffQG: Generating questions to summarize factual changes. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3088\u20133101, Dubrovnik, Croatia. Association for Computational Linguistics.\\nDan Cristea and Bonnie Webber. 1997. Expectations in incremental discourse processing. In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 88\u201395, Madrid, Spain. Association for Computational Linguistics.\\nBeth Davey and Susan McBride. 1986. Effects of question-generation training on reading comprehension. Journal of Educational Psychology, 78(4):256.\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. QLoRA: Efficient finetuning of quantized LLMs. Advances in Neural Information Processing Systems.\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lingyu Gao, Debanjan Ghosh, and Kevin Gimpel. 2022. \u201cWhat makes a question inquisitive?\u201d a study on type-controlled inquisitive question generation. In Proceedings of the 11th Joint Conference on Lexical and Computational Semantics, pages 240\u2013257, Seattle, Washington. Association for Computational Linguistics.\\n\\nJonathan Ginzburg. 1996. Dynamics and the semantics of dialogue. Logic, Language and Computation, 1:221\u2013237.\\n\\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356.\\n\\nKung-Hsiang Huang, Philippe Laban, Alexander Fabibri, Prafulla Kumar Choubey, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2024. Embrace divergence for richer insights: A multi-document summarization benchmark and a case study on summarizing diverse information from news articles. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 570\u2013593, Mexico City, Mexico. Association for Computational Linguistics.\\n\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Guillem Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.\\n\\nAndrew Kehler and Hannah Rohde. 2017. Evaluating an expectation-driven question-under-discussion model of discourse interpretation. Discourse Processes, 54(3):219\u2013238.\\n\\nWei-Jen Ko, Te-yuan Chen, Yiyan Huang, Greg Durrett, and Junyi Jessy Li. 2020. Inquisitive question generation for high level text comprehension. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6544\u20136555, Online. Association for Computational Linguistics.\\n\\nWei-Jen Ko, Cutter Dalton, Mark Simmons, Eliza Fisher, Greg Durrett, and Junyi Jessy Li. 2022. Discourse comprehension: A question answering framework to represent sentence connections. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11752\u201311764, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nKlaus Krippendorff. 2011. Computing krippendorff's alpha-reliability.\\n\\nPhilippe Laban, Chien-Sheng Wu, Lidiya Murakhovs'ka, Xiang Chen, and Caiming Xiong. 2022. Discord questions: A computational approach to diversity analysis in news coverage. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5180\u20135194, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics.\\n\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157\u2013173.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandal Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nIlya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations.\\n\\nYan Meng, Liangming Pan, Yixin Cao, and Min-Yen Kan. 2023. FollowupQG: Towards information-seeking follow-up question generation. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 252\u2013271, Nusa Dua, Bali. Association for Computational Linguistics.\\n\\nShashi Narayan, Joshua Maynez, Reinald Kim Amplayo, Kuzman Ganchev, Annie Louis, Fantine Huot, Anders Sandholm, Dipanjan Das, and Mirella Lapata. 2023. Conditional generation with a question-answering blueprint. Transactions of the Association for Computational Linguistics, 11:974\u2013996.\\n\\nBenjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. 2023. A question answering framework for decontextualizing user-facing snippets from scientific documents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3194\u20133212, Singapore. Association for Computational Linguistics.\\n\\nEdgar Onea. 2016. Potential questions at the semantics-pragmatics interface. In Potential Questions at the Semantics-Pragmatics Interface. Brill.\\n\\nAdithya Pratapa, Kevin Small, and Markus Dreyer. 2023. Background summarization of event timelines. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8111\u20138136, Singapore. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-1114", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As one reads an article, it is natural to ask curiosity-driven questions to enhance one's understanding of the article. Amongst different potential questions that one might ask while reading the article, to what extent is it important for it to be answered later in the article? Can we perhaps rank these questions? We develop an evaluation schema to do just that!\\n\\n**Task**\\n\\nGiven the prior context, anchor sentence, and a list of potential questions, score the questions on the basis of the following schema.\\n\\n- **Score=0**: These are questions which satisfy at least one of the following criterion:\\n  1. Question has grammar errors\\n  2. Question is not anchored in the given anchor sentence\\n  3. Question contains multiple sub-questions\\n  4. Question misinterprets the context\\n\\n- **Score=1**: The question is not very related to the topic (basically to weed out any odd questions)\\n\\n- **Score=2**: The question is related to the concepts introduced in the prior context and the anchor sentence but asking the question seems like a stretch. Answering the question doesn't seem useful in making the article feel complete. Typically questions that also seem to be completely answered by the prior context and the anchor sentence are given this score.\\n\\n- **Score=3**: The question is related to the prior context and anchor sentence but answering it doesn't matter to me. Answering it may provide additional information which may/may not enhance my understanding of the article.\\n\\n- **Score=4**: Answering the question is somewhat useful because, for example, it might clarify some newly introduced concepts, or might expand on an idea already introduced. It is useful to answer the question because it might influence the narrative. There is a degree of uncertainty here as compared to when you would score a question 5.\\n\\n- **Score=5**: This question should definitely be answered in the subsequent context. Some reasons why the question should definitely be answered:\\n  1. It clarifies a concept introduced in the anchor sentence\\n  2. It asks about surprising or mysterious events/objects\\n  3. It asks for more information about newly introduced humans (or animated) individuals into the discourse\\n  4. Answering this question is essential to understanding the narrative.\\n\\nDo keep in mind that one shouldn't make an inference about other people. For instance, if the question is about defining or explaining a concept, and you don't need that explanation, don't say that answering the question may still be helpful just because you think some other people will find the answer useful.\\n\\n**Validity Classification**\\n\\nPer Table 2, invalid questions accounted for 9.1% of the annotated data. Thus we also experiment with question validity classification, which can be used in a pipeline to first find invalid questions to exclude, before scoring their salience.\\n\\n**LLM Zero-/Few-shot Baselines**\\n\\nSince many invalid questions are caused by anchor issues (Section 4.2), we use the anchor relevance prompt in QUDEval (Wu et al., 2023a) for few-shot prompting using GPT-4 and Mistral-7B-instruct to classify question validity.\\n\\n**Fine-Tuning**\\n\\nWe also fine-tune flan-t5-base and TinyLlama-1.1B-chat on this task. Prompt B.1 list the instruction for TinyLlama-chat.\\n\\nAdamW (Loshchilov and Hutter, 2018) was used as optimizer with a learning rate of 3e-4, trained for 2 epochs. We perform downsampling to balance the data distribution.\\n\\n**Results**\\n\\nTable 8 shows that both prompting and fine-tuned models perform decently well on question validity classification. The fine-tuned models are on-par with prompting LLMs.\\n\\n| Model                     | Macro F1 |\\n|---------------------------|----------|\\n| GPT-4 few-shot            | 0.689    |\\n| Mistral-7B-instruct few-shot | 0.538   |\\n| Flan-t5-base fine tuned   | 0.662    |\\n| TinyLlama-1.1B-chat fine tuned | 0.693  |\\n\\n*Table 8: Question Validity Performance*\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.1 Instruction for in fine-tuned models for question validity classification.\\n\\nSystem: Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\nInstruction: Is the question well-grounded in the anchor sentence? Please evaluate using the following scale:\\n\\n1: The question is fully grounded in the anchor sentence.\\nOr some parts of the question are grounded in the anchor sentence.\\n\\n0: The question is not grounded at all in the anchor sentence.\\n\\nBased on the question and the anchor, please choose one of the above options. If the question refers to the same entity as the anchor, we consider the question to be grounded.\\n\\nInput: question: {{question}}\\nanchor sentence: {{anchor sentence}}\\n\\nResponse: {{score}}\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.2 Zero-shot CoT prompt for salience prediction.\\n\\narticle: {{article context C + anchor sentence S}}\\n\\nquestion: {{question}}\\n\\nsystem:\\nImagine you are a curious reader who is reading the article. You come across a question and you need to determine if it should be answered in the following article or not. You have to give a reason and a score for this input.\\n\\nScore = 1 means the question is completely unrelated to the topic of the article or misinterprets the context of the article. Score = 2 means the question is related to the article but it has already mostly been answered by the article. Score = 3 means the question is related to the article but answering it is not useful as it might expand of an idea that is not very important or central to the context of the article. Score = 4 means the question is related to the article and answering it is somewhat useful in enhancing the understanding of the article. Score = 5 means the question is related to the article and should definitely be answered because it expands on some ideas which are central to the article. Note that the score is given according to the information utility of its answer. If a question is related to the article but doesn't need to be answered or is not central to the article, do NOT give it a high score of 4 or 5, instead give a score of 3 if the question is unanswered by the article and 2 if it has already been answered by the article. To differentiate between a score of 4 vs 5, think of how the article would look like if you don't answer the question - if the article would not feel complete without the answer to the question, give a score of 5, else a 4. A score of 4 is usually given if answering the question will be useful but there might be other questions that are more important to answer as compared to this. A score of 5 is only given to the best and most important questions that MUST be answered so use it carefully and sparingly. Do not be biased towards giving a high score and follow the above instructions carefully. First provide a reasoning for your response and then the score. Now let's think step by step.\\n\\nreason:\\nE Setups for instruction fine-tuning\\nModel Seq Len Learn. Rate Epoch\\nMistral-7B-instruct 4096 0.0003 3\\nLlama-2-7B-chat 4096 0.0001 5\\nFlan-t5-base 512 0.0003 3\\nTinyLlama-1.1B-chat 4096 0.0003 4\\n\\nTable 11: Parameters for fine-tuned Models in salience scoring\\nE.1 Instruction for fine-tuned models for salience prediction.\\n\\nSystem:\\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\nInstruction:\\nGive a score from 1 to 5 for how important it is for the question to be answered later in the article. Score = 1 means the question is completely unrelated to the topic of the article. Score = 2 means the question is related to the article but answering it is not useful in making the article feel complete. Score = 3 means the question is related to the article but answering it might not enhance the understanding of the article. Score = 4 means the question is related to the article and answering it is somewhat useful in enhancing the understanding of the article. Score = 5 means the question is related to the article and should definitely be answered because it might provide explanation for some new concepts.\\n\\nInput:\\narticle: {{article context C + anchor sentence S}}\\nquestion: {{question}}\\nResponse::\\n{{score}}\\n\\nF Prompts for Expanded Summary Generation\\nF.1 Prompt for generating a short TL;DR.\\nContext:\\n{{article}}\\nGenerate a short 50-word summary for the above article. Remember, do not exceed 50 words.\\nSummary:\\nF.2 Prompt for GPT-4 summary expansion.\\narticle: {{article}}\\nshort summary: {{tl;dr}}\\nProduce an elaboration of the short summary by including relevant details from the article within a word count range of 230 to 250 words. Strive for conciseness and clarity in the article.\"}"}
{"id": "emnlp-2024-main-1114", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-1114", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The desperate actions by governments, regulatory authorities, and banks in both the US and Europe have not only failed to stem the growing financial crisis but in some ways are making it worse. In the US, following the failure of the Silvergate bank, Silicon Valley Bank and Signature over the past two weeks, the latter two recording the second- and third-largest banking failures in US history respectively, attention has turned to the travails of the First Republic Bank with growing concerns that it could be the next to go. Last week, a consortium of 11 major banks, under the leadership of JPMorgan Chase CEO Jamie Dimon, with the collaboration of Treasury Secretary Janet Yellen, deposited $30 billion with the struggling bank. It was hoped this show of confidence would stop the outflow of depositors' money, ease the pressure on its share price and stabilise it.\\n\\nIn just a few days, the operation has been revealed as a complete failure. While the outflows are reported to have slowed somewhat, First Republic has lost $70 billion out of the total of $176 billion it held at the start of the year. And despite the injection of cash, the company's shares have continued to plummet.\\n\\nFirst Republic's share price has fallen by 90 percent since the beginning of the month, closing 47 percent down yesterday. Long-term bonds that mature in 2046 were trading at 55 cents on the dollar, down from 75 cents in early March. First Republic took another hit before trading opened yesterday, when the ratings agency S&P Global downgraded its credit rating for the second time in a week.\\n\\n| Question                                                                 | Salience |\\n|--------------------------------------------------------------------------|----------|\\n| Who initiated the act of depositing $30 billion into the struggling First Republic Bank? | 2        |\\n| Why was it thought that this deposit would stem the outflow of depositors' money? | 5        |\\n| What role did Treasury Secretary Janet Yellen play in this financial effort? | 0        |\\n| How has the injection of cash affected the overall financial health of the company? | 3        |\\n| What are the strategies that the company intends to use to stabilize its shares amid the injection of cash? | 4        |\\n| What was First Republic's credit rating prior to these two downgrades by S&P Global? | 3        |\\n| Why did the ratings agency S&P Global downgrade First Republic's credit rating for the second time in a week? | 5        |\"}"}
