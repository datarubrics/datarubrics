{"id": "emnlp-2024-main-1186", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Retrieval-enriched zero-shot image classification in low-resource domains\\nNicola Dall\u2019Asen1,2 Yiming Wang3 Enrico Fini1\\nElisa Ricci1,3\\n1University of Trento\\n2University of Pisa\\n3Fondazione Bruno Kessler\\nCorrespondence: nicola.dallasen@unitn.it\\nProject website: https://fodark.github.io/CoRE\\n\\nAbstract\\nLow-resource domains, characterized by scarce data and annotations, present significant challenges for language and visual understanding tasks, with the latter much under-explored in the literature. Recent advancements in Vision-Language Models (VLM) have shown promising results in high-resource domains but fall short in low-resource concepts that are underrepresented (e.g. only a handful of images per category) in the pre-training set. We tackle the challenging task of zero-shot low-resource image classification from a novel perspective. By leveraging a retrieval-based strategy, we achieve this in a training-free fashion. Specifically, our method, named CORE (Combination of Retrieval Enrichment), enriches the representation of both query images and class prototypes by retrieving relevant textual information from large web-crawled databases. This retrieval-based enrichment significantly boosts classification performance by incorporating the broader contextual information relevant to the specific class. We validate our method on a newly established benchmark covering diverse low-resource domains, including medical imaging, rare plants, and circuits. Our experiments demonstrate that CORE outperforms existing state-of-the-art methods that rely on synthetic data generation and model fine-tuning.\\n\\n1 Introduction\\nLow-resource domains refer to those rare domains where the data or its annotation is truly scarce. Similarly, low-resource languages are those that have significantly less content available online (Magueresse et al., 2020) with respect to other high-resource languages, like English. There exist abundant research on the topic in the context of natural language processing (Ranathunga et al., 2023; Adams et al., 2017; Fadaee et al., 2017; Pan et al., 2017). However, surprisingly, the vision counterpart, i.e. low-resource visual domains, is much under-explored despite the numerous practical applications. In this paper, we focus on classifying images in low resource domains, i.e. where we can find only a handful of images per category. The causes for such limited data can be various: for example, when only certain devices are capable of capturing the visual content, e.g. astronomy or medical imaging; the visual content itself is sensitive or private, e.g. due to privacy issues, or rarely appears in nature, e.g. deep ocean animals, or other long-tailed categories. Their associated annotations can also be limited due to the expertise requires, in particular for niche fields, e.g. electric design or phytology.\\n\\nRecent large vision-language models (VLMs) have fostered a paradigm shift in image classification. Their flexibility and generalization, enabled by web-scale pre-training with text-image pairs,\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"makes them versatile tools in many sub-fields of computer vision. Numerous works have appeared, with the objective of tuning VLMs, e.g. CLIP (Radford et al., 2021) or SigLIP (Zhai et al., 2023), to address image zero-shot (Jia et al., 2022) or few-shot (Chowdhury et al., 2023; da Costa et al., 2023) image classification. However, the images involved in those studies are mostly in high-resource image domains, where there exist thousands of images on the Internet for VLMs to learn from during the web-scale pre-training (Udandarao et al., 2024).\\n\\nHowever, directly performing zero-shot classification in low-resource domains does not yield satisfactory performance due to the data scarcity in pre-training. Even supervised fine-tuning might fall short in learning the underlying data distribution due to the very limited amount of data and annotation. Among the techniques that have been explored in the pioneering work (Zhang et al., 2024), one prominent recipe is to fine-tune the VLMs on data augmented via synthetic generation (e.g. Stable Diffusion (Rombach et al., 2021)). Despite the performance improvements, by analyzing the generated images, we observe that image generation models are also affected by the low-resource nature of the task. The generation quality is largely dependent on the noise injected on the real samples: by injecting limited noise, the synthetic images appear very similar to the original samples, being correct but not diverse, while by injecting more noise, the synthetic images diversify in appearance, but are mostly semantically incorrect and exhibit domain-specific rule violations. This is because the data distribution of rare domains is not well-represented in the generative models latent space (Mokady et al., 2022; Trabucco et al., 2024).\\n\\nInstead of generating synthetic images as data augmentation, we explore the possibility of retrieving relevant information from a textual corpus, crawled from the Internet, to enrich the data representation at inference time, as shown in Fig. 1. It turns out that retrieval is also non-trivial in the low-resource regime as (i) pre-trained models generally under-represent the low-resource domains, thus greatly limiting the retrieval efficacy; (ii) large web-crawled databases can contain noisy or incorrect content, a problem that is more severe in low-resource domains.\\n\\nThus, a careful design is required to leverage the retrieved data. In this work, we propose the first training-free and retrieval-based method, CORE (Combination of Retrieval Enrichment), to tackle low-resource image classification. Following a VLM-based zero-shot classification paradigm, we propose to enrich the representation for both the query image and the class prototypes with textual content retrieved with different encoder backbones from large web-crawled databases. Specifically, for the query image, we employ the pre-trained image encoder from a VLM as our vision retrieval backbone. We perform image-to-text retrieval, obtaining the most relevant captions with respect to the query image.\\n\\nFrom our preliminary analysis, we observe that although the specific category (e.g. \u201cLED\u201d) appears sparsely in the retrieval, its broader category (\u201ccircuit\u201d) does occur frequently. Previous studies have empirically demonstrated that enriching the prompt with the broader concept, together with noise, can significantly boost the zero-shot recognition performance (Roth et al., 2023). We thus enrich the image embedding by combining it with the textual embedding from image-to-text retrieval. Similarly, we construct the enriched class prototypes. For each class, we form its corresponding text prompt and embed it with a pre-trained text encoder to retrieve captions that are most relevant. Then, the retrieved captions are encoded with the VLM text encoder and aggregated together with the textual embedding of the original class prompt. The final categorization is obtained by computing the cosine similarity between the enriched visual representation against the enriched textual class prototypes.\\n\\nTo validate the effectiveness of our proposed method, we also collect a set of datasets that covers diverse low-resource domains, including medical imaging, rare plants, and circuits. CORE can effectively improve the data representation in a training-free fashion, with a noticeable improvement in image classification performance on all the datasets, outperforming the state-of-the-art method that involves synthetic image generation and model fine-tuning.\\n\\nTo summarize, our contributions are:\\n\\n\u2022 We propose the first training-free retrieval-based method CORE for addressing zero-shot low-resource image classification;\\n\u2022 we propose a data representation enrichment strategy for both query image and class prototypes, using the textual content retrieved from the database;\\n\u2022 we establish a benchmark featuring zero-shot\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"low-resource image classification, composed of representative datasets and VLM-based baselines and state-of-the-art methods; our training-free method is effective in classifying low-resource images, outperforming competitors with training and other training-free baselines by a large margin.\\n\\n2 Related work\\n\\nHigh-resource data. The de-facto standard in Deep Learning has become to train larger foundation models with a high volume of data, e.g. ImageNet (Deng et al., 2009) or LAION-5B (Schuhmann et al., 2022) for vision, which contain up to 5 billion images, or FineWeb (Penedo et al., 2024) for text, which contains 15 trillion of tokens. For vision-related tasks, data focuses on natural images, which are plentiful online and can be easily obtained. However, when moving to more specific domains, e.g. medical (Irvin et al., 2019), satellite imaging (Helber et al., 2019), or long-tailed distributed data (Van Horn et al., 2018), data becomes less available. Although these rare domains have been understudied in favor of higher-available data, we follow the study of (Zhang et al., 2024) and investigate domains where the number of available data is in the order of hundreds, and training is an under-performing option.\\n\\nWe propose a novel way to address the challenge, and we exploit the knowledge coming from web-scale image-text pairs datasets through retrieval. Retrieval-based solutions have proved successful for image classification (Liu et al., 2023b; Conti et al., 2023) and NLP tasks (Lewis et al., 2020), and we propose to enrich the data representation in VLMs.\\n\\nMultimodal foundation models. Multimodal Foundation Models, such as CLIP (Radford et al., 2021) or BLIP (Li et al., 2022, 2023) have gained a lot of popularity due to their outstanding zero-shot capabilities, derived from their weak-supervised training on web-crawled data. The most common paradigm is to train on image-text pairs, which can be easily obtained on the web (Changpinyo et al., 2021; Schuhmann et al., 2022), but recent approaches like ImageBind (Girdhar et al., 2023) bridge several modalities, e.g. images, videos, audio, text, and thermal. We focus in particular on Vision-Language Models (VLM).\\n\\nAdaptation of VLM to downstream tasks is performed in several ways, e.g. full finetuning, or Parameter-Efficient FineTuning (PEFT) methods, e.g. LoRA (Hu et al., 2022), Prompt Tuning (Jia et al., 2022), or Textual Prompt Tuning (Zhou et al., 2022). These approaches are not suitable for the rare domain setting as the amount of data is limited, domain-specific, and extremely different from the pre-training data of the original VLM.\\n\\nVLM few-shot learning. Several works have proved the effectiveness of few-shot learning when adapting VLMs to downstream tasks. Some notable works include a combination of PEFT strategy with VLMs, such as APoLLo (Chowdhury et al., 2023) that synthetically augments both the visual and textual branch of CLIP, or DISEF (da Costa et al., 2023) that employs LoRA and synthetic images to fine-tune CLIP. We closely follow the work of (Zhang et al., 2024), where the authors fine-tune ImageBind (Girdhar et al., 2023) with an AdaptFormer (Chen et al., 2022) module on a combination of real and synthetic data for low-resource rare domains. Differently from these works, we do not employ synthetic data and we do not fine-tune, instead, we propose a training-free zero-shot solution through retrieval to adapt both the visual and textual representation in CLIP.\\n\\n3 Background\\n\\nTo establish a foundation in understanding our method, we provide a brief introduction to Vision-Language Models and web-scale databases, the two essential elements in our method design.\\n\\n3.1 Vision-Language Models\\n\\nVision-Language Models (VLM) (Radford et al., 2021; Jia et al., 2021) learn a function $f_{VLM}: \\\\mathbb{V} \\\\times \\\\mathbb{L} \\\\rightarrow \\\\mathbb{R}^{d}$ with the goal to maximize their representation similarities in order to map images in visual space $\\\\mathbb{V}$ and texts in language space $\\\\mathbb{L}$ to the same latent space. In particular, a VLM is composed of a vision encoder $f_{V}^{VLM}: \\\\mathbb{V} \\\\rightarrow \\\\mathbb{R}^{d}$ that maps images to a visual embedding and a language encoder $f_{L}^{VLM}: \\\\mathbb{L} \\\\rightarrow \\\\mathbb{R}^{d}$ that maps the text in natural language (after converting into tokens) to an embedding. VLMs learn to project the two modalities to the same latent space $\\\\mathbb{R}^{d}$ via contrastive learning with millions of web-crawled image-text pairs, enabling image classification using text by evaluating their similarity in this shared space.\\n\\nAt inference, the VLM can be used to assess the similarity between an image sample and a set of prompt texts that are composed from a number of...\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Our CORE enriches both the image embedding $z_q$ and the class prompts $p$ with retrieved captions from a large-scale web-crawled database $D$. We weight the retrieved captions $T$ with their similarity scores $S_T$, which we skew with controllable temperatures $\\\\tau$. By combining the retrieved captions embedding with the original representations $W$ and $q$ through $\\\\alpha$ and $\\\\beta$, we obtain enriched representations $W^+$ and $z^+q$ which we employ for zero-shot classification.\\n\\nGiven a query image $q \\\\in V$, we can obtain its visual embedding via the image encoder as $z_q = f_V VLM(q)$. We can build the class prototypes $W \\\\in \\\\mathbb{R}^{N \\\\times d}$ as the textual embeddings using the VLM text encoder. Specifically, for each class $c_n$, we build the text prompt $p_n = \\\\{\"prefix\"|[CLS]_n\\\\}$, where $\\\\{\"prefix\\\\}$ usually is \\\"a photo of a\\\" and $[CLS]_n$ is the name of $c_n$ in text. Then the class prototypes can be formed as $W = f_{LV LM}(\\\\{p\\\\}_n)$. Finally, we can compute the cosine similarities between the image embedding $z_q$ and the class prototypes $W$ in order to predict the class $\\\\hat{c}$ for the query image $q$:\\n\\n$$\\\\hat{c} = \\\\arg\\\\max_c (z_q \\\\times W^T)$$\\n\\n(1)\\n\\n3.2 Retrieval databases\\n\\nTogether with the VLMs, there is also the emergence of large vision-language databases that can be leveraged for VLM pre-training and retrieval. The community has collected and released several web-scale image-text pairs datasets, such as LAION (Schuhmann et al., 2022), CC12M (Changpinyo et al., 2021), and COYO (Byeon et al., 2022), with respectively 400M/5B, 12M and 700M image-text pairs. Formally, let $D = \\\\{(i,t)\\\\}_M$ be the retrieval database that consists of $M$ items, where each item is paired with an image $i_m$ and its associated textual description $t_m$. We are mostly interested in the textual content of $D$ as it contains rich language-induced semantics (Conti et al., 2023) that might contribute to enriching the knowledge that is specific to low-resource domains.\\n\\nWith millions and billions of data items, it is critical to perform retrieval efficiently. To this purpose, we focus on embedding-based retrieval, whose goal is to retrieve the most similar elements from a database from a query embedding. Such retrieval is supported by off-the-shelf tools, e.g. Faiss (Douze et al., 2024), that enable similarity search and clustering of dense vectors at scale. In particular, we prepare the text-image database $D$ by encoding data items into embeddings using encoders of VLMs. Given an embedding $z$ corresponds to either visual or textual modality, we can retrieve a set of $k$ textual descriptions $T$ that are most similar to the $z$ from the database $D$:\\n\\n$$T = \\\\text{top-k}(\\\\langle z, f_{LV LM}(t) \\\\rangle), \\\\forall t \\\\in D$$\\n\\n(2)\\n\\nwhere $\\\\langle \\\\cdot \\\\rangle$ computes the cosine similarity between two embeddings, and $\\\\text{top-k}(\\\\cdot)$ returns $k$ textual descriptions with the highest cosine similarities.\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We focus on zero-shot low-resource image classification, following the paradigm of VLMs as described in Sec 3.1. Low-resource domains are generally not well represented by VLMs given their limited availability in the pre-training dataset (Udandarao et al., 2024). There is a need to steer the original data representation towards more specialized low-resource domains represented by the query image $q$.\\n\\nTo this end, our training-free method CORE extracts domain-relevant information from a large text-image database $D$, to enrich the representation of both class prototypes and the query image.\\n\\nFor the class prototypes, i.e. textual representation of the classes $\\\\{cn\\\\}_{n=1}^{N}$, we retrieve semantically close captions for each class with the embedding of prompt text regarding the $c_n$. On one hand, the retrieved captions contain rich domain-level information while being less specific to the exact class. On the other hand, the class prompt is only specific to the class of interest without much prior of the domain information. Therefore, we further join the retrieved captions with the prompt class text to obtain the textual class prototypes $W$ enriched with the domain context.\\n\\nA similar rationale is applied to enrich the query image representation via image-to-text retrieval, implemented by the image encoder $f_{\\\\text{VLM}}$ of a pre-trained VLM. With the visual embedding $z_q$ of the query image $q$, we retrieve the set of captions that are the most aligned to $z_q$ in the shared latent space, and use the retrieved captions to obtain an enriched image representation $z_q^+$.\\n\\nThe final class is predicted using $z_q^+$ and $W$ similar to Eq. 1:\\n\\n$$\\\\hat{c} = \\\\arg\\\\max_c (z_q^+ \\\\times W^\\\\top).$$\\n\\n(3)\\n\\nWe describe each retrieval branch in detail in the following sections and we show an overview of our CORE in Fig. 2.\\n\\n### 4.1 Class representation enrichment\\n\\nWe leverage text-to-text retrieval to enrich the class prototypes of the set of predefined classes $\\\\{cn\\\\}_{n=1}^{N}$.\\n\\nFor each class $c_n$, we first build the text prompt $p_n$ following the prompt format as described in Sec. 3.1. Different prompt templates are also experimented in Sec. 5.2.\\n\\nFor the text-to-text retrieval, we leverage the encoder of a LLM to obtain the textual embeddings for both the per-class text prompt, i.e. $l_n = f_{\\\\text{LLM}}(p_n)$ and all textual content in the database $D$.\\n\\nFor each $l_n$, we then retrieve from $D$ a set of $k$ most similar textual descriptions $T_n$ with respect to class $c_n$. Each retrieved text $t_i \\\\in T_n$ has an associated cosine similarity in the range of $[-1, 1]$ w.r.t. the embedding of the prompt text $l_n$, forming a set of $k$ scores $S_{T_n}$.\\n\\nAs the retrieved texts contain rich domain-level context, we further embed them and merge their textual embeddings to obtain a domain-specialized embedding. Since the eventual classification is achieved in the latent space of VLMs, we leverage the text encoder of the VLM to embed the retrieved texts, i.e. $Z_{T_n} = f_{\\\\text{LVM}}(T_n)$.\\n\\nAs the retrieved captions are associated with different similarity scores $S_{T_n}$, we weigh their contribution to form the domain-specialized embedding accordingly. Specifically, we propose to build a probability distribution out of the similarities scores as:\\n\\n$$\\\\sigma_{T_n} = \\\\text{softmax}(S_{T_n^{2}}),$$\\n\\n(4)\\n\\nwhere $\\\\tau_{2}$ is the temperature parameter that controls the skewness of the distribution.\\n\\nFor the class $c_n$, the embedding from retrieved texts $Z_{T_n}$ are then combined as a weighted sum with the weight being $\\\\sigma_{T_n}$, forming the domain-specialized embeddings for all classes $W_{T}$.\\n\\nFinally, we build the retrieval-enriched class prototypes by linearly interpolating prompt-text class prototypes $W$ as described in Sec 3.1 and the retrieved domain-specialized ones $W_{T}$, with an interpolation factor $\\\\alpha$ as:\\n\\n$$W^+ = \\\\alpha W_{T} + (1 - \\\\alpha) W,$$\\n\\n(5)\\n\\nwhere $\\\\alpha$ is a hyperparameter and we shows its impact in Section 5.2.\\n\\n### 4.2 Image query representation enrichment\\n\\nSymmetrically, we want to enrich the query image representation and exploit the rich web-scale semantics of captions to build a more representative image embedding.\\n\\nStarting from a query image $q$, for image-to-text retrieval, we use the visual encoder of the VLM to obtain the image embedding $z_q = f_{\\\\text{VLM}}(q)$, while we use the text encoder $f_{\\\\text{LVM}}$ to embed the database $D$, in this way image and text are mapped in the same space. We retrieve from $D$ the $k$ most similar captions $T$, with their associated cosine similarities $S_{T}$. Similar to the class prototype.\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                      | Circuits iNaturalist2021 (LT100) | HAM10000 |\\n|----------------------------|----------------------------------|----------|\\n|                            | Acc@1  | Acc@5  | Acc@1  | Acc@5  | Acc@1  | Acc@5  |\\n| ImageBind (Zhang et al., 2024) | 24.10   | 49.30  | 31.60  |        |        |        |\\n| \u2020                          | 60.50   |        | 54.60  |        | 96.56  |        |\\n| SigLIP@384px (Zhai et al., 2023) | 19.53   |        | 30.61  |        | 34.50  |        |\\n| \u2020                          |        | 63.50  |        | 54.60  |        | 95.90  |\\n| CLIP ViT-L (Radford et al., 2021) | 7.98    | 29.13  | 8.00   | 22.60  | 45.27  | 90.80  |\\n| CLIP ViT-L@336px (Radford et al., 2021) | 9.09    | 30.33  | 7.60   | 22.70  | 40.97  | 90.27  |\\n| BLIP2-EV A (Li et al., 2023) | 17.63   | N/A    | 1.40   | N/A    | 2.91   | N/A    |\\n| LlaV A 1.6 34B (Liu et al., 2023a) | 29.59   | N/A    | 0.60   | N/A    | 10.59  | N/A    |\\n| ImageBind (Girdhar et al., 2023) | 22.36   | 51.02  | 6.70   | 23.90  | 14.43  | 84.25  |\\n| SigLIP@384px (Zhai et al., 2023) | 35.81   | 58.63  | 19.10  | 45.70  | 57.64  | 96.16  |\\n| CORE (Ours \u2014 CC12M) | 42.94   | 7.13   | 67.71  | 9.08   | 21.40  | 2.30   |\\n| CORE (Ours \u2014 COYO-700M) | 43.88   | 8.07   | 71.99  | 13.36  | 22.10  | 3.00   |\\n\\nTable 1: Top-1 and top-5 accuracy on the proposed benchmark.\\n\\n\u2020 indicates our re-implementation.\\n\\nWe report gain and loss w.r.t. the best training-free solution.\\n\\n5 Experiments\\n\\nWe evaluate our proposed method CORE in comparison with state-of-the-art Vision-Language approaches using three challenging low-resource datasets. We describe the dataset used and the evaluation protocol we follow. We discuss the results w.r.t. the baseline methods, and we ablate the proposed components and architectural choices.\\n\\nDatasets.\\n\\nWe consider three datasets covering different low-resource scenarios for our analysis: we employ the Circuit Diagram Classification dataset (Zhang et al., 2024) that comprises 1,332 circuit diagram images covering 32 different classes, the images are scraped from the web and textbooks. The authors split the data into 154 images for training and the remaining for testing, resulting in an average of \u223c5 samples per class available at training time.\\n\\nThe second dataset we consider is iNaturalist 2021 (Van Horn et al., 2018) which contains 10,000 species with a fine-grained classification. The dataset features many visually similar species, captured in a wide variety of situations. In order to remain in the rare domain setting, we restrict our analysis to the rarest 100 species in terms of available training samples and set the maximum amount of training shots to 5. We test on 10 images for each class, therefore the test set contains 1,000 samples.\\n\\nThe last dataset we employ is HAM10000 (Tschandl et al., 2018), which comprises dermatoscopic images from different populations. The dataset has 7 classes that include a representative collection of all important diagnostic categories in the realm of pigmented lesions. The test set includes 1,511 images.\\n\\nDatabase(s).\\n\\nWe use CC12M (Changpinyo et al., 2021) as our source dataset to build the database D, following previous works (Conti et al., 2023). We also use a subset (10%) of COYO-700M to test the scaling laws of the retrieval database. We focus on their textual corpus as we are interested in retrieving the relevant pieces of text. The retrieval databases are implemented using Faiss (Johnson et al., 2019) on pre-extracted embeddings. We use the SigLIP (Zhai et al., 2023) text encoder for the image-to-text retrieval, while we employ a text-only encoder to obtain a stronger textual semantic in text-to-text retrieval. In particular, we use SFR-Embedding-Mistral (Meng et al., 2024) as fLLM.\\n\\nEvaluation protocol.\\n\\nAs a common practice in image classification tasks, we report the performance.\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We compare against state-of-the-art VLMs and LMMs in the training-free zero-shot scenario. In particular, we compare with CLIP (Radford et al., 2021), using the ViT-L/14 and ViT-L/14@336px vision encoder variants, BLIP-2 (Li et al., 2023) using the ViT-g/14 vision encoder from EV A-CLIP (Fang et al., 2023), LLaV A-NeXT (Liu et al., 2023a) with a 34B LLM, ImageBind (Girdhar et al., 2023) as in (Zhang et al., 2024), and SigLIP (Zhai et al., 2023). For the last two models, we also implement a training-based variant as in (Zhang et al., 2024), which trains an AdaptFormer (Chen et al., 2022) and a linear classification head.\\n\\nDiscussion. We present a quantitative evaluation of CORE and the baselines in Tab. 1. We can see that among the training-free approaches, CORE outperforms the others by a substantial margin (up to 8.07% in top-1 accuracy on Circuits). When comparing to fine-tuned solutions, CORE still outperforms them, even given the substantial gain between zero-shot and fine-tuned ImageBind (up to 40% in HAM10000). Fine-tuning SigLIP seems detrimental to performance, except for iNaturalist, and we deem the lower performance w.r.t. fine-tuned ImageBind to the higher number of trainable parameters, as SigLIP embeddings are bigger than ImageBind ones (1152 vs 1024). We study this behavior further in Section 5.2.\\n\\nLMMs reach satisfactory results on Circuits, while the low performance on iNaturalist and HAM10000 is due to these models answering consistently with the same class name for almost all the samples. On iNaturalist, supervised fine-tuning represents a stronger solution w.r.t. our CORE, and we deem this results to two factors: the nature of the dataset and the availability of the concepts in the retrieval database. Being the images of different plants and animals, the visual overlap is less prominent than in the other two datasets, therefore supervised fine-tuning can effectively separate different classes with a small amount of training data. Secondly, as in iNaturalist class names are provided with their Greek or Latin scientific name, this limits the amount of relevant data that can be retrieved from the database as even in large-scale image-text datasets, such as LAION 400M, this type of data is under-represented (Parashar et al., 2023). We mitigate this effect by using both scientific and common names when retrieving and building the zero-shot weights.\\n\\nThis benchmark allows us to show the training-free strength of our method, but also showcase the viability of training-based solutions given the right assumptions. Nevertheless, CORE remains the strongest training-free solution. We can also observe that employing a bigger retrieval database improves the top-1 accuracy across all the datasets.\\n\\n5.2 Ablation\\n\\nVarious embedding fusion strategies. We ablate the contribution of each of our proposed components in Tab. 2, starting from the zero-shot only and reaching the full CORE. We first add the weighting between zero-shot weights and retrieved weights without the temperature on the similarities, effectively having a na\u00efve average of retrieved embeddings.\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Ablation of proposed components of CORE CC12M, starting from the zero-shot only to the full CORE. As shown in the table, each of the components can bring performance improvement across all the datasets, proving the effectiveness of the designed retrieval strategy.\\n\\nTable 3: Accuracy of CORE CC12M using different models for text-to-text retrieval. The LLM (Mistral) embedding is stronger than vision-language aligned embedding in terms of the unimodal text-to-text retrieval.\\n\\nTable 4: Zeroshot results on Circuits with different prefixes. Numbers denote: (1) a circuit diagram of. (2) a circuit of. (3) an electronic schematic of. (4) a photo of a circuit diagram: a. (5) a picture of a {} circuit. (6) a photo of an electronic circuit: a. (7) a photo of a.\\n\\nPrompting strategy. We study the role of the \u201cprefix\u201d part of the queries when performing retrieval and when building the zero-shot weights. In Tab. 4 we show the effect of using only the zero-shot weights $W$ or only the retrieved captions weights $W_T$ for zero-shot classification. We can see that the best prefix to build zero-shot weights is not the best prefix to retrieve information from the database, where constraining the domain in the prompt becomes fundamental to obtaining good performance. Additionally, using only retrieved captions as class prototypes leads to unsatisfactory results. We then restrict the study to two styles of the prefix: a generic \u201ca photo of a\u201d and a domain-specific \u201ca {domain} of a\u201d.\\n\\nWe study the effect of these two prefixes in Table 5. For iNaturalist, since the images cover very different natural domains, e.g. animals, plants, fungi, etc. we cannot design a domain-specific prompt, and we can only use the domain-agnostic\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Accuracy of our COTRE CC12M with different prompting strategies for zero-shot weights and text-to-text retrieval. Employing a domain-specific prefix for zero-shot and a domain-agnostic prefix for retrieval leads to generally better results across all the benchmarks.\\n\\nOne. We can observe that employing a domain-specific prefix for zero-shot and a domain-agnostic prefix for retrieval leads to generally better results across all the benchmarks.\\n\\n6 Conclusion\\nWe presented COTRE, a training-free retrieval-based zero-shot solution for low-resource image classification. Through the retrieval of semantically relevant textual information for both image representations and class prototypes, COTRE is able to enhance the richness of feature representations and achieves state-of-the-art performance. Remarkably, it does so without the need for additional training or labeled data, outperforming existing training-based methods under extremely low-resource conditions. Moreover, we established a comprehensive benchmark using representative datasets and baseline models, providing a robust testing ground for the low-resource image classification task. The results demonstrate the efficacy and generalization capability of COTRE across various low-resource domains, representing a significant step forward in low-resource image classification.\\n\\n7 Limitations\\nWhile we prove text retrieval to be a strong training-free solution on the proposed benchmark, the performance is still limited by the representation of a domain in the external database. An example of this low coverage is provided by (Liu et al., 2023b), where Patch-Camelyon, a medical dataset, has a limited presence even in LAION 400M. We face the same problem when trying to apply COTRE to Parasitic Egg Classification (Anantrasirichai et al., 2022), where retrieved captions from CC12M only contain the most common egg parasite name, and where supervised fine-tuning becomes the strongest way to adapt VLMs for the setting.\\n\\n8 Ethical considerations\\nWe employ large-scale web-crawled data to enrich our representations, and this type of data is by definition mostly uncurated, and it may reflect bias from the real world. This data might contain harmful or Not Safe For Work (NSFW) content, as demonstrated in previous studies (Thiel, 2023). The scientific community has acted to mitigate these risks, e.g. employing NSFW filters before releasing the image-text pairs (Byeon et al., 2022). Nevertheless, we do not employ the image content of these datasets, and we use the textual part only at the semantic level to enhance the image classification performance. Therefore we never expose users to harmful or undesired content.\\n\\nAcknowledgments\\nWe thank CINECA and the ISCRA initiative for the availability of high-performance computing resources. This work was supported by the EU Horizon ELIAS (No. 101120237) and AI4TRUST (No.101070190) projects, and the FAIR - Future AI Research (PE00000013), funded by NextGeneration EU, and the PRIN LEGO-AI (Prot. 2020TA3K9N) project. This work was carried out in the Vision and Learning joint laboratory of FBK and UNITN.\\n\\nReferences\\nOliver Adams, Adam Makarucha, Graham Neubig, Steven Bird, and Trevor Cohn. 2017. Cross-lingual word embeddings for low-resource language modeling. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL): Volume 1, Long Papers.\\n\\nNantheera Anantrasirichai, Thanarat H Chalidabhongse, Duangdao Palasuwan, Korranat Naruenatthanaset, Thananop Kobchaisawat, Nuntiporn Nunthanasup, Kanyarat Boonpeng, Xudong Ma, and Alin Achim. 2022. Icip 2022 challenge on parasitic egg detection and classification in microscopic images: Dataset, methods and results. In IEEE International Conference on Image Processing (ICIP).\\n\\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso,\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mark Saroufim, Marcos Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala. 2024. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS \u201924).\\n\\nMinwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. 2022. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset.\\n\\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nShoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. 2022. Adaptformer: Adapting vision transformers for scalable visual recognition. In Advances in Neural Information Processing Systems (NeurIPS).\\n\\nSanjoy Chowdhury, Sayan Nag, and Dinesh Manocha. 2023. APoLLo : Unified adapter and prompt learning for vision language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nAlessandro Conti, Enrico Fini, Massimiliano Mancini, Paolo Rota, Yiming Wang, and Elisa Ricci. 2023. Vocabulary-free image classification. In Advances in NeurIPS.\\n\\nVictor G Turrisi da Costa, Nicola Dall'Asen, Yiming Wang, Nicu Sebe, and Elisa Ricci. 2023. Diversified in-domain synthesis with efficient fine-tuning for few-shot classification. arXiv preprint arXiv:2312.03046.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nMatthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar\u00e9, Maria Lomeli, Lucas Hosseini, and Herv\u00e9 J\u00e9gou. 2024. The faiss library. arXiv preprint arXiv:2401.08281.\\n\\nMarzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low-resource neural machine translation. arXiv preprint arXiv:1705.00440.\\n\\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. 2023. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. 2019. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing.\\n\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR).\\n\\nJeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. 2019. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning (ICML).\\n\\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser Nam Lim. 2022. Visual prompt tuning. In Proceedings of the IEEE/CVF European Conference on Computer Vision (ECCV).\\n\\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data.\\n\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Systems (NeurIPS).\\n\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (ICML).\\n\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning (ICML).\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Haotian Liu, Kilho Son, Jianwei Yang, Ce Liu, Jian-feng Gao, Yong Jae Lee, and Chunyuan Li. 2023b. Learning customized visual models with retrieval-augmented knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nAlexandre Magueresse, Vincent Carles, and Evan Heetderks. 2020. Low-resource languages: A review of past work and future challenges. arXiv preprint arXiv:2006.07264.\\n\\nRui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024. Sfr-embedding-mistral: enhance text retrieval with transfer learning. Salesforce AI Research Blog.\\n\\nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022. Null-text inversion for editing real images using guided diffusion models. 2023 IEEE. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Cross-lingual name tagging and linking for 282 languages. In Proceedings of the 55th annual meeting of the association for computational linguistics (ACL): Volume 1, Long Papers.\\n\\nShubham Parashar, Zhiqiu Lin, Yanan Li, and Shu Kong. 2023. Prompting scientific names for zero-shot species recognition. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nGuilherme Penedo, Hynek Kydl\u00ed \u02c7cek, Leandro von Werra, and Thomas Wolf. 2024. Fineweb.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sathy, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning (ICML).\\n\\nSurangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi Shekhar, Mehreen Alam, and Rishemjit Kaur. 2023. Neural machine translation for low-resource languages: A survey. ACM Computing Surveys.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2021. High-resolution image synthesis with latent diffusion models. 2022 IEEE. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nKarsten Roth, Jae Myung Kim, A Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata. 2023. Waffling around for performance: Visual classification with random words and broad concepts. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems (NeurIPS).\\n\\nDavid Thiel. 2023. Identifying and eliminating csam in generative ml training data and models. Technical report, Technical Report. Stanford University, Palo Alto, CA. https://purl.stanford... .\\n\\nBrandon Trabucco, Kyle Doherty, Max A Gurinas, and Ruslan Salakhutdinov. 2024. Effective data augmentation with diffusion models. In International Conference on Learning Representations (ICLR).\\n\\nPhilipp Tschandl, Cliff Rosendahl, and Harald Kittler. 2018. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Scientific data.\\n\\nVishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip HS Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. 2024. No\u201d zero-shot\u201d without exponential data: Pretraining concept frequency determines multimodal model performance. arXiv preprint arXiv:2404.04125.\\n\\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. 2018. The inaturalist species classification and detection dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).\\n\\nYunhua Zhang, Hazel Doughty, and Cees GM Snoek. 2024. Low-resource vision challenges for foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2022. Learning to prompt for vision-language models. International Journal of Computer Vision.\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Baselines implementation details\\n\\nFor VLMs (i.e. CLIP, ImageBind, and SigLIP) we report the zero-shot results with zero-shot weights built starting from the prompt \\\\{prefix\\\\} [CLS].\\n\\nAs \\\\{prefix\\\\} we test both domain-specific (e.g. \u201ca circuit diagram of a\u201d for Circuits dataset) and domain-agnostic sentences (e.g. \u201ca photo of a\u201d as standard practice in CLIP), and we report the best result for each model. For iNaturalist we follow the insights of (Parashar et al., 2023) and try using common names of animals and plants instead of their scientific names. We find that merging the zero-shot weights of common and scientific names improves the overall accuracy, therefore we report these results for this dataset.\\n\\nFor LMM (i.e. BLIP2 and LLaV A) we feed the query image with the textual prompt \u201cQuestion: what\u2019s the name of the object in the image out of [class names]? Answer with the name only. Answer: \u201d, then parse the answer and match the name with the dataset classes.\\n\\nFor ImageBind and SigLIP we follow (Zhang et al., 2024) recipe for fine-tuning, and generate synthetic images using their pipeline. The generation involves using a Stable Diffusion model (Rombach et al., 2021) on top of noised images, and feeding the model with a domain-specific prompt, e.g. \u201ca circuit diagram of a [CLS].\u201d to re-generate the missing part. The amount of noise added from the diffusion schedule depends on the role of the synthetic image, i.e. it is set to 30% of the schedule for samples used as positive in the contrastive loss, and 60% for the negative samples. We maintain the original training hyperparameters (including batch size and learning rate), and we train an AdaptFormer (Chen et al., 2022) module with rank 2 and a linear classification head.\\n\\nOn synthetic data augmentation\\n\\nWe extend the discussion from Sec. 1 on using synthetic data augmentation to address low-data scenarios. We argue that while synthetic data generation can effectively enhance recognition performance, it introduces training images that are either overly similar to the original samples or incorrect, violating domain rules, one example being the case of (Zhang et al., 2024). We showcase examples of both \u201cpositive\u201d and \u201cnegative\u201d types of synthetic augmentation in Fig. 7. We can see that \u201cpositive\u201d samples do not differ significantly from the original sample and do not bring meaningful variation to the original sample. On the other hand, the \u201cnegative\u201d samples differ to the point of changing semantics (first row) or breaking the reference domain rules (third row).\\n\\nOn having an image-to-image retrieval\\n\\nInitially, we also implemented an image-to-image retrieval system leveraging DINOv2 embeddings. We hypothesized that enhancing the original image embedding with features from retrieved images would improve its capability to retrieve relevant textual information. However, this approach did not yield the desired results. Although the retrieved images were from the correct domain, they often lacked the appropriate semantic class. This mismatch caused the embeddings to deviate significantly from the target class, resulting in a text-retrieval performance decline. Quantitative results supporting this observation are presented in Tab. 8.\\n\\nD Interesting failure cases\\n\\nWe extend the discussion started in Section 7 regarding the failure cases of retrieval-based solutions w.r.t. training-based ones. We include the quantitative results on the Parasitic Egg Recognition challenge (Anantrasirichai et al., 2022) in Tab. 9. We can see that in this case, the training-based solutions outperform the training-free solutions by a large margin. We deem this for two reasons: i) the classes differ visually, making a training approach powerful in telling the different classes, and ii) this type of data is under-represented in large-scale datasets (Liu et al., 2023b), making the retrieval less effective which, nevertheless, outperforms the other training-free solutions.\\n\\nE Additional analyses\\n\\nDependency on sample quantity. The experiments in Tab. 1 are conducted with \\\\(\\\\sim 5\\\\) annotated samples per class as in the Circuits dataset of (Zhang et al., 2024), where only 154 images are available for training on 32 classes. We complement this analysis by showing in Tab. 6 the effect of having access to more training data for training-based solutions on HAM10000. We can see that the model performance scales with the amount of annotated samples per class. SigLIP trained with 15 samples achieves better results than CORE. The low amount of training and\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Accuracy of training-based solutions with increasing number of training shots per class on HAM10000.\\n\\n| Number of shots per class | 1  | 5  | 10 | 15 | 20 |\\n|--------------------------|----|----|----|----|----|\\n| Acc@1                    | 10.85 | 68.70 | 54.60 | 96.56 | 55.59 | 96.10 | 46.72 | 97.88 | 57.84 | 98.81 |\\n| SigLIP                   | 25.74 | 90.73 | 54.60 | 95.90 | 46.72 | 95.90 | 62.21 | 98.15 | 61.42 | 98.21 |\\n\\nTable 7: Synthetic image from the baseline of (Zhang et al., 2024). We show original samples, the \u201cpositive\u201d augmentation and the \u201cnegative\u201d augmentation.\\n\\n| Method                  | Circuits | iNaturalist | HAM10000 |\\n|-------------------------|----------|-------------|----------|\\n| DINOv2 img2img          | 23.28    | 7.78        | 59.96    |\\n| CORE (Ours \u2014 CC12M)     | 42.94    | 21.40       | 61.54    |\\n\\nTable 8: Top-1 accuracy on the proposed benchmark using DINOv2 features to retrieve relevant images compared to our CORECC12M.\\n\\nValidation data, on the other hand, makes the selection of the best-trained model noisy, therefore the improvement with more samples is not straightforward to assess.\\n\\nF Accuracy@1 vs Accuracy@5 tradeoff\\n\\nIn our investigation, we observed that modifying the hyperparameters $\\\\alpha$, $\\\\beta$, and $\\\\tau$ resulted in a tradeoff between Accuracy@1 and Accuracy@5. We prioritized Accuracy@1, which is widely regarded as the primary classification metric. This sensitivity was particularly pronounced in the HAM10000 dataset, where we identified several data points that constitute the Pareto frontier when plotting Accuracy@1 against Accuracy@5. Some notable examples from this frontier are presented in Table 10.\\n\\nG Retrieval Efficiency\\n\\nOur method CORE leverages an efficient indexing and retrieval mechanism, implemented with FAISS (Douze et al., 2024), achieving 57.54 ms per image-text retrieval on a CPU. As text-to-text retrieval is performed offline, we do not include it in the runtime analysis. As shown in Table 11,\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Acc@1 vs Acc@5 tradeoff on HAM10000.\\n\\n|                | Acc@1 | Acc@5 |\\n|----------------|-------|-------|\\n|                | 27.73 | 96.10 |\\n|                | 39.58 | 95.96 |\\n|                | 54.80 | 95.63 |\\n|                | 57.97 | 95.50 |\\n|                | 61.95 | 95.30 |\\n|                | 62.21 | 94.51 |\\n\\nPer image inference at runtime, CORE requires on average 110 ms (the sum of SigLIP inference time and retrieval time), while the competitor with the best performance (SigLIP@384px) requires 53 ms.\\n\\nCORE requires 110 ms, which is about one-third of other VLMs (e.g. CLIP ViT-L@336px, BLIP2-EVA) and is of one magnitude less than LLaVA 1.6 (34B).\\n\\nIn terms of memory/storage, in addition to the models, CORE requires the storage of both the metadata (i.e., the filenames and original captions), and the indexes (i.e., the embeddings). For CC12M, the metadata is of 2.7 GB and the index is of 3.4 GB by SigLIP (3.1 GB by SFR-Embedding-Mistral). For our COYO-700M subset, the metadata is of 6.0 GB, and the indices are of 22 GB by SigLIP (23 GB by SFR-Embedding-Mistral). In comparison, other competitors do not require the storage of a database, but the models themselves could be storage-demanding, e.g. LLaVA 1.6 (34B) occupies about 65GB of storage, while CoRE (COYO-700M) in total occupies as little as 9.4GB, as the LLM is optional as shown in the ablation \\\"On the use of LLM for embedding\\\".\\n\\nExamples of retrieved captions\\n\\nWe provide a qualitative showcase of retrieved captions by our CORE for both text-to-text and image-to-text in Tab. 12 and Tab. 13. We use the 10% subset of COYO-700M as the database, and we also show some failure cases (e.g. sixth row of Tab. 13).\\n\\nComputational requirements\\n\\nSynthetic data generation for trained baselines, as the amount of training data is low, has an upper bound of 12 GPU/hours on an NVIDIA A100 for iNaturalist. The subsequent model training has an upper bound of 4 GPU/hours on an NVIDIA A100 for SigLIP.\\n\\nFor our CORE, the most time-consuming task is represented by the external database embedding. The upper bound is 16 GPU/days for the COYO-700M subset. CC12M has been embedded in 3 GPU/days. Retrieval can then be performed without access to any GPU and on any consumer hardware in $\\\\sim 58$ ms.\\n\\nLicenses\\n\\nMost of the datasets used (Circuits (Zhang et al., 2024), iNaturalist (Van Horn et al., 2018), HAM10000 (Tschandl et al., 2018), and Parasitic Egg Classification (Anantrasirichai et al., 2022)) are released under Creative Commons Attribution Non-Commercial 4.0. COYO-700M (Byeon et al., 2022) is released under Creative Commons Attribution 4.0, while CC12M (Changpinyo et al., 2021) is released \\\"as is\\\". LLaVA (Liu et al., 2023a) and SigLIP (Zhai et al., 2023) are released under Apache 2.0. ImageBind (Girdhar et al., 2023) is released under Creative Commons Attribution Non-Commercial ShareAlike 4.0. CLIP (Radford et al., 2021) is released under MIT. BLIP (Li et al., 2023) is released under BSD-3-Clause. SFR-Embedding-Mistral (Meng et al., 2024) is released under Creative Commons Attribution Non-Commercial 4.0. PyTorch (Ansel et al., 2024) is released \\\"as is\\\". Faiss (Douze et al., 2024) is released under MIT.\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                | Inference (ms) | Params Storage (GB) | Method                           | Inference (ms) | Params Storage (GB) |\\n|-----------------------|----------------|---------------------|----------------------------------|----------------|---------------------|\\n| CLIP ViT-L            | 55             | 123M                | **f**T                            | 303M           | V LM                |\\n| CLIP ViT-L@336px      | 368            | 123M                | **f**T                            | 304M           | V LM                |\\n| BLIP2-EV A           | 397            | 2.9B                | **f**L                            | 986M           | V LM                |\\n| LLaV A               | 1.6            | (34B) 4970          | **f**L                            | 303M           | V LM                |\\n| ImageBind             | 52             | 302M                | **f**T                            | 632M           | V LM                |\\n| SigLIP@384px         | 53             | 449M                | **f**T                            | 428M           | V LM                |\\n| COORE                 | 110            | 449M                | **f**T                            | 428M           | V LM                |\\n\\nTable 11: Retrieval efficiency of our COORE and all the baselines in terms of inference time, parameter count, and storage requirements.\\n\\nTable 12: Examples of retrieved captions in text-to-text using the COYO-700M subset.\\n\\n**Input**\\n\\n- A circuit diagram of an amplifier.\\n  - Electrical Diagram\\n  - Amplifier\\n  - Audio amplifier circuit diagram with layout\\n  - Circuit diagram for transistor as audio amplifier\\n\\n- A circuit diagram of a LED.\\n  - Technical Drawing\\n  - LED\\n  - The symbol for a light emitting diode.\\n  - One LED with leads all bent out\\n\\n- A circuit diagram of an audio mixer.\\n  - Photo of an audio mixer board\\n  - 8-Channel Audio Mixer picture 1\\n  - Mixer with volume faders and pan knobs\\n\\n- A skin lesion of Bowen's disease.\\n  - Pre-Cancerous Actinic Keratosis\\n  - A biopsy specimen showing hyperkeratosis, papilloma\\n  - Looking for premalignant skin cancer\\n\\n- A skin lesion of melanoma.\\n  - Melanoma of the Skin, Cut-section\\n  - This picture shows a melanoma lesion with varying colors.\\n  - A mole that turned out to be melanoma skin cancer\\n\\n- A skin lesion of melanocytic nevi.\\n  - This picture shows a melanoma lesion with varying colors.\\n  - Recurrence of melanocytic naevus\\n  - Graphic of a melanoma\\n\\n- A photo of a Lygodium japonicum.\\n  - A picture of a Japanese holly fern\\n  - Japanese painted fern\\n  - Pyrrosia Ferns on moss covered trunk\\n\\n- A photo of a Salvinia minima.\\n  - 12 Water Spangles (Salvinia Minima), Live Aquarium/Aquatic\\n  - Salvinia Minima (Water Spangles) floating aquarium plant\\n  - Water drop salvinia sp trichomes stock image\\n\\n- A photo of a Azolla filiculoides.\\n  - The Azolla fern has leaves floating on the water surface\\n  - Image of Azolla filiculoides(). Click to enlarge parts of image.\\n  - Picture of Fern\"}"}
{"id": "emnlp-2024-main-1186", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hobby Electronics\\n\\nCircuits: AC Powered 220V\\n\\nLED Circuit\\n\\nTransformerless LED Lighting LED Lamp\\n\\nCircuit Electronics\\n\\nAstonishing Christmas Lights\\n\\nPIEZO SOUNDER WITH BUILT-IN CIRCUIT\\n\\nHow to Build A Speaker Circuit With Adjustable Volume\\n\\nWhat do I need for a simple speaker circuit?\\n\\nIC 555, dry-run protection\\n\\nWireless Remote Control Switch\\n\\n230V AC Mains Over Voltage Protection Circuit\\n\\nSkin colored papules centered around hair follicles.\\n\\nDermoscopic image of a porokeratosis of Mibelli lesion.\\n\\nPigmented basal cell carcinoma dermoscopy.\\n\\nDermoscopy. Brown and blue-grey dots/clods.\\n\\nDermoscopic image of a porokeratosis of Mibelli lesion. Dermoscopy. Chaos and clues.\\n\\nBrown blotches are formed. Oil Red Staining. Pigment used as normal pigment pattern.\\n\\nNotogrammitis billardierei (Finger fern) at Wingecarribee Asplenium polyodon (East Maui).\\n\\nThis image is licensed.\\n\\nNot sure what this fern is. I thought maybe Buckler fern. Any ideas?\\n\\nA seed fern frond is prepared for analysis.\\n\\nA tiny plant on a tree fern's trunk. Asplenium polypodon (West Maui).\\n\\nAstrolepis cochisensis Cochise Scaly Cloakfern\\n\\nNotogrammitis billardierei (Finger fern)\\n\\nFerns emerging from charred earth.\"}"}
