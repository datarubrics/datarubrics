{"id": "emnlp-2023-main-33", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: Causal Graph cases of DialogXL and Ours (CAE II).\\n\\nFigure 8: Causal Graph cases of EGAT and Ours (CAE III).\"}"}
{"id": "emnlp-2023-main-33", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9: Causal Graph cases of RGAT and Ours (CAE IV).\\n\\nFigure 10: Causal Graph cases of DECN and Ours (CAE V).\"}"}
{"id": "emnlp-2023-main-33", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 11: Causal Graph cases of DAG-ERC and Ours (CAE VI).\"}"}
{"id": "emnlp-2023-main-33", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning\\n\\nHang Chen and Xinyu Yang and Jing Luo\\nXi'an Jiaotong University\\n{albert2123,luojingl}@stu.xjtu.edu.cn\\nyxyphd@mail.xjtu.edu.cn\\nWenjing Zhu\\nDu Xiao Man Inc.\\nzhuwenjing02@duxiaoman.com\\n\\nAbstract\\nOur investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships. To overcome this limitation, we propose the incorporation of i.i.d. noise terms into the conversation process, thereby constructing a structural causal model (SCM). It explores how distinct causal relationships of fitted embeddings can be discerned through independent conditions. To facilitate the implementation of deep learning, we introduce the cogn frameworks to handle unstructured conversation data, and employ an autoencoder architecture to regard the unobservable noise as learnable \u201cimplicit causes.\u201d Moreover, we curate a synthetic dataset that includes i.i.d. noise. Through comprehensive experiments, we validate the effectiveness and interpretability of our approach. Our code is available in https://github.com/Zodiark-ch/mater-of-our-EMNLP2023-paper.\\n\\n1 Introduction\\nNowadays, numerous conversation recognition tasks (such as Emotion Recognition in Conversation (ERC) task (Pereira et al., 2023; Thakur et al., 2023), Intent Recognition (IR) task (Ye et al., 2023; Ni, 2023) and Dialogue Act Recognition (DAR) task (Arora et al., 2023)) have shown promising performance in specialized supervised and unsupervised methods. Considering the RoBERTa pretrained model (Liu et al., 2019) as the examples, \u201cMy eyelids are fighting\u201d and \u201cI want to sleep,\u201d which have similar semantics but different tokens can be well fitted within embeddings. (i.e., these two embeddings exhibit a strong resemblance via certain metrics such as cosine similarity.) However, when it comes to the relationship between two utterances, denoted as $A$ and $B$, wherein their embeddings can be fitted, various possible relationships exist: $A$ acts as the cause of $B$ ($A \\\\rightarrow B$), $A$ acts as the outcome of $B$ ($A \\\\leftarrow B$), or more complex, $A$ and $B$ are both influenced by a common cause ($A \\\\leftarrow C \\\\rightarrow B$), and so on. Particularly in reasoning tasks (Uymaz and Metin, 2022; Feng et al., 2022), it is crucial for these methods to transcend the mere fitting of embeddings and possess the capacity to discriminate diverse causal relationships. (i.e., the ability of causal discrimination) (Bao et al., 2022; Shirai et al., 2023).\\n\\nTo specifically investigate the causal discrimination capability of existing methods in conversation, we narrow down our research to a particular task: Affective Reasoning in Conversation (ARC), which has included Emotion-Cause Pair Extraction (ECPE) Xia and Ding (2019) and Emotion-Cause Span Recognition (ECSR) Poria et al. (2021).\\n\\nWe begin with conducting tests to evaluate the causal discrimination of existing methods including the large language models (LLMs) (Kasneci et al., 2023). One typical evaluation involves the causal reversal test: for emotion-cause utterance pairs with true labels ($A$, $B$) representing a causal relationship of $B \\\\rightarrow A$, we scrutinize the predictions generated by the existing methods using both positive pairs ($A$, $B$) and negative pairs ($B$, $A$). The results reveal that all the examined methods performed similarly across the two sample types. As we are concerned, they lacked causal discriminality. (Details are shown in Section 2.3)\\n\\nIn order to discriminate different causal relationships between two similar embeddings, we construct the dialogue process as a Structural Causal Model (SCM). Many endeavors (Cheng et al., 2022; Nogueira et al., 2022) supporting that i.i.d. noise of SCM could facilitate the discrimination of causal relationships when fitting two variables. Under the presence of noise, each utterance is not only explicitly influenced by the other utterances but also implicitly influenced by the i.i.d. exogenous factors.\"}"}
{"id": "emnlp-2023-main-33", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"nous noise. Consequently, this framework ensures that two fitted embeddings result in diverse causal relationships, which are determined by corresponding independent conditions between the residual terms and embeddings. For simplicity, we refer to other utterances as explicit causes and exogenous noise as implicit causes.\\n\\nFurthermore, to enable the learnability of such causal discrimination within embeddings, we propose a common skeleton, named \\\\textit{cogn} skeleton for each utterance derived from some broadly accepted prior hypotheses. It can address the challenges arising from variable-length and unstructured dialogue samples. Subsequently, we develop an autoencoder architecture to learn the unobservable implicit causes. Specifically, we consider the implicit causes as latent variables and utilize a graph attention network (GAT) (Veli\u010dkovi\u0107 et al., 2017) to encode its representation. Additionally, the decoder leverages the inverse matrix of the causal strength, ensuring an accurate retrieval of the causal relationships.\\n\\nFinally, we conduct extensive experimental evaluations: 1) our approach significantly outperforms existing methods including prominent LLMs (GPT-3.5 and GPT-4) in two affective reasoning tasks (ECPE and ECSR) and one emotion recognition task (ERC), demonstrating its effectiveness in affective reasoning. 2) our method exhibits a significant reduction in false predictions for negative samples across three causal discrimination scenarios. 3) we curate a synthetic dataset with implicit causes to visualize the latent variable in our implementation.\\n\\nOur contribution is four-fold:\\n\u2022 We formulated the dialogue process as an SCM and analyzed the causal relationships represented by different independent conditions.\\n\u2022 We devised the \\\\textit{cogn} skeleton to address the problems of variable-length and unstructured dialogue samples.\\n\u2022 We adopted an autoencoder architecture to overcome the unobservability of implicit causes and make it learnable.\\n\u2022 We constructed a synthetic dataset with implicit causes and conducted extensive evaluations of our proposed method.\\n\\n2 Related Works and Challenges\\n\\n2.1 Task Definition\\nFor notational consistency, we use the following terminology. The target utterance \\\\(U_t\\\\) is the \\\\(t\\\\)th utterances of a conversation \\\\(D = (U_1, U_2, U_3, ..., U_N)\\\\) where \\\\(N\\\\) is the maximum number of utterances in this conversation and \\\\(0 < t \\\\leq N\\\\). The emotion label \\\\(\\\\text{Emo}^t\\\\) denotes the emotion type of \\\\(U_t\\\\).\\n\\nThe emotion-cause pair (ECP) is a pair \\\\((U_t, U_i)\\\\), where \\\\(U_i\\\\) is the \\\\(i\\\\)th utterance of this conversation. In the ECP, \\\\(U_t\\\\) represents the emotion utterance and \\\\(U_i\\\\) is the corresponding cause utterance. Moreover, the cause label \\\\(C_{t,i}\\\\) denotes the cause span type of the ECP \\\\((U_t, U_i)\\\\).\\n\\nThus, in a given text,\\\\(\\\\text{ERC}\\\\) is the task of identifying all \\\\(\\\\text{Emo}^t\\\\). Moreover, \\\\(\\\\text{ECPE}\\\\) aims to extract a set of ECPs and \\\\(\\\\text{ECSR}\\\\) aims to identify all \\\\(C_{t,i}\\\\).\\n\\n2.2 Affective Reasoning in Conversation\\nChen et al. (2018) introduced the pioneering work on \\\\(\\\\text{ERC}\\\\) due to the growing availability of public conversational data. Building upon this, Xia and Ding (2019) further advanced the field by proposing the \\\\(\\\\text{ECPE}\\\\) that jointly identifies both emotions and their corresponding causes. Moreover, Poria et al. (2021) has extended \\\\(\\\\text{ECPE}\\\\) into conversations and proposed a novel \\\\(\\\\text{ECSR}\\\\) task, specifically designed to identify ECP spans within conversation contexts. More recently, increasing works have indicated the crucial role played by accurate inference models in facilitating complex reasoning within these tasks, such as the assumption about interlocutors (Zhang et al., 2019; Lian et al., 2021; Shen et al., 2021) and context (Ghosal et al., 2019; Shen et al., 2022; Chen et al., 2023).\\n\\n2.3 Challenge of Affective Reasoning\\nWe examined the performance of a range of methods for addressing affective reasoning in conversations, including both unsupervised approaches (large language models (LLMs), BERT-based pretrained models) and supervised approaches (task-related approaches).\\n\\nOverall, all the methods demonstrated a lack of discriminability on two types of challenges:\\n\u2022 Samples where emotional utterances and causal utterances are interchanged. For a dialogue instance, if the ECP is \\\\((U_1, U_2)\\\\) (\\\\(U_2\\\\) is the cause of \\\\(U_1\\\\)), the prediction results obtained by the existing methods tend to include both \\\\((U_1, U_2)\\\\) and \\\\((U_2, U_1)\\\\).\"}"}
{"id": "emnlp-2023-main-33", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Methods\\n\\nChallenge 1\\n\\n- \\\\((U_a, U_b)\\\\)\\n- \\\\((U_b, U_a)\\\\)\\n- \\\\((U_a, U_b)\\\\)\\n- \\\\((U_b, U_c)\\\\)\\n- \\\\((U_a, U_c)\\\\)\\n\\nChallenge 2\\n\\n- \\\\((U_{a1}, U_{a2})\\\\)\\n- \\\\((U_{b1}, U_{b2})\\\\)\\n- \\\\((U_{c1}, U_{c2})\\\\)\\n\\nTable 1: For the two challenges mentioned in Section 2.3, we conducted tests on a subset of 200 samples from the RECCON dataset. We recorded the number of samples identified by above methods. In the second row of Challenge 1, we showed the count of samples where these methods extracted the negative pairs in reverse cause order. Similarly, in the third row of Challenge 2, we showed the count of samples where these methods extracted negative indirect pairs.\\n\\n- Samples with indirect connections. For example, if the ECPs in a conversation are \\\\((U_1, U_2)\\\\) and \\\\((U_2, U_3)\\\\), the prediction results obtained by the methods often include an additional pair \\\\((U_1, U_3)\\\\).\\n\\nWe evaluated the performance of existing methods on these two challenges, and the detailed results are shown in Table 1. All evaluated methods extracted a nearly equal number of negative samples as positive samples. Considering their performance in broad research domains, both unsupervised and supervised methods could demonstrate a desirable fitting ability to capture the semantic similarity between two utterances. This often apparently results in satisfactory performance in most tasks. However, when it comes to more specific causal relationships within semantically similar sentences (such as reasoning tasks), they may not exhibit the same level of \u201cintelligence\u201d and output some \u201cpseudo-correlation\u201d.\\n\\nIn the area of causal discovery, Causal Markov and Faithfulness Assumptions (Spirtes et al., 2000; Colombo et al., 2012; Ogarrio et al., 2016), provide insights into capturing more specific causal relationships in the situation of the above challenges. Considering two similar variables: \\\\(A\\\\) and \\\\(B\\\\) that can be fitted, the independence tests enable the determination of specific causal relationships, such as \u201c\\\\(A \\\\rightarrow B\\\\)\u201d, \u201c\\\\(B \\\\rightarrow A\\\\)\u201d, or \u201c\\\\(A \\\\rightarrow L \\\\rightarrow B\\\\)\u201d. More recently, the Structural Causal Model (SCM) (Shimizu et al., 2006; Shimizu and Bollen, 2014; Sanchez-Romero et al., 2019) built upon the independent noise assumptions has emerged as an effective approach to the limitation of Markov equivalence classes in distinguishing causal relationships.\\n\\n![Figure 1: The conversation case with five utterances.](image)\\n\\nIn the SCM, we assume that each utterance \\\\(U_i\\\\) has a corresponding implicit cause \\\\(E_i\\\\), and has several explicit causes. i.e., \\\\(U_4\\\\) has an implicit cause \\\\(E_4\\\\) and two explicit causes \\\\(U_3\\\\) and \\\\(U_2\\\\). In the lower part of the figure, SCM adopts \\\\(U_t = \\\\sum \\\\alpha_{it} U_i + E_t\\\\) to denote these relationships and formalize the conversation as a DAG.\\n\\nThe noise terms (also called exogenous variables) for each variable, enables methods such as Independent Component Analysis (ICA) to identify more comprehensive causal relationships between the two fitted variables.\"}"}
{"id": "emnlp-2023-main-33", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{fi\\\\}_{i=1}^{N} are functions that determine U with U_i = fi(relUi) + E_i, where relUt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in the number of utterances. Note that each Ei is independent in the SCM. Structural equations F = \\\\{"}
{"id": "emnlp-2023-main-33", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Processing of our approaches, with a six-utterances conversation case as the input. Causal skeleton indicates which utterances (nodes) are used for aggregation. For each layer $\\\\ell$, we collect representations $H_\\\\ell$ for all utterances where each row represents one utterance. Causal Encoder yields the implicit causes $E_\\\\ell$, the input for Decoder learning the causal representation. In all matrices, light gray nodes represent the masked part.\\n\\nSCM, as well as the implicit cause towards the utterance $U_t$ in conversation. Furthermore, we denote the word embedding of $U$ by $H = [h_1, h_2, \\\\ldots, h_N]$, and the relationships between utterances in rows can also be written as: $H = A^T H + E$, where $A_{i,t} \\\\neq 0$ stands for a directed edge from $U_i$ to $U_t$ in SCM. Thus we can define the Graph $G = (V, E)$ with adjacency matrix $A_{i,i} = 0$ for all $i$.\\n\\nHowever, in this equation, only $H$ is known. The unknown variable $A$ is typically the target of inference, while the unknown variable $E$ represents exogenous variables that implicitly influence each utterance, such as the speaker's memory, experiences, or desires. (Krych-Appelbaum et al., 2007; Sidera et al., 2018) These factors are typically missing in existing conversation resources. Therefore, determining $A$ based on completely unknown $E$ is another problem we aim to address.\\n\\nHence, we treat $A^T$ as an autoregression matrix of the $G$, and then $E$ can be yielded by an autoencoder model. The whole process reads:\\n\\n$$E_\\\\ell = f((I - A^T)H)(2)$$\\n\\n$$\\\\hat{H}_\\\\ell = g((I - A^T)^{-1}E)(3)$$\\n\\nwhere $f(\\\\cdot)$ and $g(\\\\cdot)$ represent the encoder and decoder neural networks respectively. Encoder aims to generate an implicit cause $E$, and Decoder devotes to yielding a causal representation $\\\\hat{H}$. From Equation 1, causal representation $\\\\hat{H}_t$ reasons about the fusion relations of heterogeneous explicit causes $\\\\sum_{i \\\\in rel} H_i$ and implicit cause $E_t$. The details of this process are shown in Figure 3.\\n\\nEncoder. We use the graph attention mechanism to learn the adjacency matrix $A$ and construct a hierarchical GNN to instantiate the $f(\\\\cdot)$.\\n\\nThus, for each utterance at the $\\\\ell$-th layer, the $A_{i,t}$ computed by attention mechanism is a weighted combination of $h_\\\\ell t$ for each directly related utterance $U_i$ ($i \\\\in rel$):\\n\\n$$A_{i,t} = \\\\text{LeakyReLU}(e_{i,t}) \\\\sum_{j \\\\in rel} \\\\text{LeakyReLU}(e_{j,t})(4)$$\\n\\n$$e_{i,t} = - \\\\rightarrow h_i W_{i}^{\\\\ell}(\\\\text{row}) + (- \\\\rightarrow h_t W_{t}^{\\\\ell}(\\\\text{col}))^T(5)$$\\n\\nwhere $W_{row}^{\\\\ell} \\\\in \\\\mathbb{R}^{N \\\\times 1}$ and $W_{col}^{\\\\ell} \\\\in \\\\mathbb{R}^{N \\\\times 1}$ are the learnable parameters in the graph attention. Moreover, the GNN aggregates the information from the neighbor utterances as following:\\n\\n$$H_{\\\\ell+1} = \\\\text{eLU}((I - (A_{\\\\ell})^T)H_{\\\\ell} W_{\\\\ell})(6)$$\\n\\nwhere $W_{\\\\ell}$ stands for parameters in the corresponding layer. From the final layer of the evaluation process, by extracting $A_{L-1}$ computed in Equation 4, the marginal or conditional \u201cdistribution\u201d of $H$ is obtained, showing how to discover Causal Graph $G$ from $D$. Besides, by extracting $H_L$ in Equation 6, we can obtain the independent embedding for the implicit causes $E = \\\\text{MLP}(H_L)$.\\n\\nDecoder. We utilize the $A$ and $E$ computed from Encoder to generate the causal representation $\\\\hat{H}$. With a fixed adjacency matrix $A$, the GNN aggregates the information of implicit causes from neighbor nodes as follows:\\n\\n$$\\\\hat{E}_{\\\\ell+1} = \\\\text{eLU}((I - (A_{L})^T) - 1 E_{\\\\ell} M_{\\\\ell})(7)$$\\n\\nwhere $M_{\\\\ell}$ is parameters in the corresponding layer. As the same architecture as the encoder, $\\\\hat{H} = \\\\text{MLP}(\\\\hat{E}_L)$. As the same architecture as the encoder, $\\\\hat{H} =$...\"}"}
{"id": "emnlp-2023-main-33", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this approach, \\\\( \\\\hat{E} \\\\) and \\\\( E \\\\) acts identically under the linear SCM model. Similarly, \\\\( \\\\hat{H} \\\\) should be aligned with \\\\( H \\\\) in emotion dimensions under the non-linear SCM model. In short, we adopt an auxiliary loss measuring the Kullback-Leibler (KL) divergence (Joyce, 2011) of \\\\( \\\\hat{H} \\\\) and \\\\( H \\\\) mapped into the exact emotion dimensions. Moreover, implicit causes \\\\( E \\\\) is one of the crucial influence factors on \\\\( \\\\hat{H} \\\\), so that the loss aims to impose the constraint that \\\\( \\\\hat{H} \\\\) is the embedding of our need to ensure generating correct \\\\( E \\\\).\\n\\nLoss \\\\( KL = \\\\sum_t \\\\sum_{e \\\\in \\\\text{Emo}} p_e(\\\\hat{U}_t) \\\\log p_e(\\\\hat{U}_t) / p_e(U_t) \\\\) (9)\\n\\nwhere \\\\( e \\\\) is any emotion type in \\\\( \\\\text{Emo} \\\\), \\\\( p_e \\\\) denotes the probability labeled with emotion \\\\( e \\\\). In the whole process of ARC tasks, we followed (Wei et al., 2020; Poria et al., 2021) to add several losses of ECPE and ECSR respectively.\\n\\nFurthermore, we would like to explain the difference between our approach and Variational Autoencoder (VAE) (Kingma and Welling, 2014). The output of the encoder in VAE is \\\\( q_{\\\\phi}(Z) \\\\). With this estimation of \\\\( \\\\hat{q}_{\\\\phi}(Z) \\\\), we can measure the variation \\\\( \\\\xi(q_{\\\\phi}(Z)) \\\\) (also called \\\\( \\\\nabla_{\\\\phi} \\\\text{ELBO}(\\\\hat{q}_{\\\\phi}) \\\\)) to obtain the approximation estimation of \\\\( \\\\text{ELBO}(q) \\\\). In contrast, our output is \\\\( E \\\\), a fixed matrix rather than a distribution. In other words, the VAE depends on the prior distribution over the latent variables, whereas our approach has a dependency on the consistency of \\\\( H \\\\) and \\\\( \\\\hat{H} \\\\), which is non-sampling and non-distributive.\\n\\n### 4 Experiments\\n\\nIn this section, we conduct extensive experiments to answer the 3 research questions:\\n\\n- **RQ1**: How effective is our method in affective reasoning tasks?\\n- **RQ2**: How do we justify the causal discriminability of our method?\\n- **RQ3**: How do we gauge the difference between the latent variable \\\\( E \\\\) and designed implicit causes?\\n\\n#### 4.1 Datasets, Implementation and Baselines\\n\\nWe use six real datasets for three affective reasoning tasks and one synthetic dataset for justifying \\\\( E \\\\) in our model. The statistics of them are shown in Table 2. Appendix C depicts the detailed introductions of each dataset.\\n\\nWe adopt the consistent benchmarks of the SOTA methods in three tasks, including the pre-training language model, hyper-parameters, \\\\( t \\\\)-tests, and metrics. The implementation details are shown in Appendix D.\\n\\nAccording to the hypotheses of these baselines, for each cogn skeleton, we choose one recent SOTA work: II: DialogXL (Shen et al., 2022). III: EGAT (Chen et al., 2023). IV: RGAT (Ishiwatari et al., 2021). V: DECN (Lian et al., 2021). VI: DAG-ERC (Shen et al., 2021).\\n\\n#### 4.2 Overall Performance (RQ1)\\n\\nTable 3 reports the results of ECPE and ECSR, with \\\\( p < 0.01 \\\\) in the \\\\( t \\\\)-test, where the best improvement and best performance both concentrate on VI. With the visualization of Appendix F, we infer that the upper triangular adjacency matrix of DAG-ERC, not restricted by the backpropagation, benefits from Hypothesis 6. Moreover, II lags farthest behind in the ECPE while achieving the second best in the ECSR, showing that the reliance on a hypothesis is not equal in different tasks. Furthermore, without Hypotheses 1 and 6, III, IV, and V are far from the best performance since Hypothesis 1 has the maximum identifying space, and Hypothesis 6 supplies the highest number of information passing. Finally, it is worth noting that three skeleton-agnostic baselines and unsupervised methods perform poorly in the RECCON-IE dataset, indicating that our models have stronger representation learning capabilities as well as highlighting the continued research value of affective reasoning tasks.\\n\\nWe further conducted six sets of ablation experiments to study the effects of different modules. In Table 4, we summarized results under the following cases: replacing Loss \\\\( KL \\\\) with BCE loss function...\"}"}
{"id": "emnlp-2023-main-33", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Overall performance in ECPE and ECSR tasks. We additionally compare four unsupervised approaches and two baselines not belonging to any skeleton: RANK-CP (Wei et al., 2020), ECPE-2D (Ding et al., 2020). The RoBERTa+ represents the large RoBERTa version (1024 dimensions). The DD and IE are two subsets (see Appendix C).\\n\\nTable 4: Ablation results (BCE); removing the Loss KL (w/o Loss KL); replacing Decoder module with a Linear layer (w/o Decoder); removing the RNN module (w/o Hypo 6); adding the edges from successors to predecessors (w/o Hypo 5); reducing the speaker types to one (w/o Hypo 4).\\n\\nAs shown in Table 4, BCE loss performs similarly to Loss KL; thus, we empirically demonstrate that our auxiliary loss is essentially different from Loss KL in VAE. The F1 score decreases heavily without auxiliary loss or decoder, these two are necessary ingredients for building complete processing to learn the causal representation via $E$.\\n\\nBesides, Hypotheses 4, 5, and 6 are all critical but removing Hypothesis 4 leads to the highlight degradation in 3 skeletons. This result corroborates the theory of Lian et al. (2021) and Shen et al. (2021), who state that speaker identity is the strong inductive bias in conversation. Finally, it is expected to see that skeleton with Hypotheses 4, 5, and 6 should be the closest to perfection while the DAG-ERC + Ours indeed achieves the SOTA.\\n\\nTable 5: Results of causal discriminability. The calculation results = count of predicted results that matched the samples / total number of samples * 100. The three causal models are: Reversal Model: Positive samples $(i, j)$ and negative samples $(j, i)$; Chain Model: Positive samples $(i, k)$ and $(k, j)$, and negative samples $(i, j)$; Common Cause Model: Positive samples $(k, i)$ and $(k, j)$, and negative samples $(i, j)$ or $(j, i)$. The metric \u201cPos\u201d represents the percentage of positive samples, indicating the extraction capability. Higher Pos samples imply a better extraction capability. The metric \u201cNeg\u201d represents the percentage of negative samples. A smaller difference between Pos and Neg indicates a weaker causal discriminability.\\n\\nFurthermore, Appendix E reports the results of ERC task and sensitivity experiments to analyze how our model performs in different $L$ and $k$.\\n\\n4.3 Relationship analysis (RQ2) We are also concerned about the causal discriminability for similar utterances. Table 5 demonstrates that in all three different causal models, none of the methods were able to distinguish between negative and positive samples. Because both negative and positive samples can be fit within these three causal models, solely from an embedding similarity perspective. However, our method significantly decreases the percentage of negative samples indicating the effectiveness of incorporating implicit cause noise to enhance causal discriminative ability.\\n\\nAdditionally, we show the adjacent matrices of our model and current SOTA methods in Appendix F. which indicates that our model can more freely explore the relationship between different utterances via adjacent matrices shifting rather than being limited to a fixed structure (e.g., attention module).\"}"}
{"id": "emnlp-2023-main-33", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.4 Implicit Causes Study (RQ3)\\n\\nThe latent variable $E$ is intended to represent the mentioned implicit causes. Therefore, the global distribution of the latent variable $E$ should be approximately equal to the one of implicit causes. Although human evaluation labels are better for proving reasonable performance, it is intractable to annotate implicit causes due to their unobservability. We thus trained our model in a synthetic dataset given a set of fixed i.i.d. implicit causes to observe how the $E$ is similar to the ground truth implicit causes distributions. Figure 4 (a-b) shows the projection of $E$ and implicit causes, respectively, using t-SNE (Knyazev et al., 2019). We observe that $E$ and implicit causes are both similarly clustered into three parts through the distribution properties. $E$ is consistent with the implicit causes in the samples with or without noise indicating that $E$ successfully learns the implicit causes.\\n\\nMoreover, in Appendix G, we first prove the approximate emotion consistency between utterance $U_t$ and its implicit causes when $U_t$ and $U_i$ in the emotion-cause pair $(U_t, U_i)$ do not belong to the same emotion category. Then, we demonstrate through the ERC task that by replacing $\\\\hat{H}$ with $E$, the emotion consistency provided by implicit causes is preserved.\\n\\n4.5 Limitations\\n\\nIn our model, our method can distinguish between $U_i \\\\rightarrow U_j$ and $U_i \\\\leftarrow U_k \\\\rightarrow U_j$. However, our method is unable to distinguish between $U_i \\\\rightarrow U_j$ and $U_i \\\\leftarrow L \\\\rightarrow U_j$, where $L$ represents a unobserved variable, called common causes or confounders. In Tables 3, 7, and 8, skeletons II, III, and IV generally lag far behind V and VI. This unsatisfactory performance of these skeletons indicates that excessive adding-edge leads to serious confounders. Therefore, we proposed a theoretical design for testifying the existing of latent confounders:\\n\\nConfounding between Non-adjacent Nodes:\\n\\nConsider two utterances $U_i$ and $U_j$ being non-adjacent utterances. Let $Pa$ be the union of the parents of $U_i$ and $U_j$:\\n\\n$$Pa = U_i \\\\cup U_j$$\\n\\nIf we perform an intervention on $Pa$ (i.e., $do(Pa) = pa$), we thus have $U_i \\\\perp \\\\perp U_j$ if and only if there is a latent confounder $L$ such that $U_i \\\\leftarrow L \\\\rightarrow U_j$.\\n\\nConfounding between Adjacent Nodes:\\n\\nConsider two utterances $U_i$ and $U_j$ being adjacent utterances: $U_i \\\\rightarrow U_j$. If there are no latent confounders, we have $P(U_j | U_i) = P(U_j | do(U_i = u_i))$.\\n\\nIndeed, implementing intervention operations on conversation data poses a significant challenge. Therefore, in our new work, we have proposed general intervention writing:\\n\\n$$do(X) := Pa(X) = \\\\emptyset$$\\n\\nwhere $Pa(X)$ denotes the parent set. Moreover, the most significant obstacle to further research is the lack of a high-quality dataset with complete causal relationship labels. Hence, we have constructed a simulated dialogue dataset via GPT-4 and plan to make it open soon.\\n\\n5 Conclusion\\n\\nThe results of testing prevalent approaches on the ARC task have demonstrated that almost all approaches are unable to determine the specific causal relationship that leads to the association of two well-fitted embeddings. In order to enhance the causal discrimination of existing methods, we constructed a SCM with i.i.d. noise terms, and analyzed the independent conditions that can identify the causal relationships between two fitted utterances. Moreover, we proposed the cogn framework to address the unstructured nature of conversation data, designed an autoencoder implementation to make implicit cause learnable, and created a synthetic dataset with noise labels for comprehensive experimental evaluation. While our method still has some limitations, such as confounders and the inability to scale to all methods, we hope that our theory, design, and model can provide valuable insights for the broader exploration of this problem to demonstrate that our work is de facto need for identifying causal relationships.\"}"}
{"id": "emnlp-2023-main-33", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nRaj Agrawal, Chandler Squires, Neha Prasad, and Caroline Uhler. 2021. The decamfounder: Non-linear causal discovery in the presence of hidden variables. arXiv preprint arXiv:2102.07921.\\n\\nSiddhant Arora, Hayato Futami, Emiru Tsunoo, Brian Yan, and Shinji Watanabe. 2023. Joint modelling of spoken language understanding tasks with integrated dialog history. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE.\\n\\nYinan Bao, Qianwen Ma, Lingwei Wei, Wei Zhou, and Songlin Hu. 2022. Multi-granularity semantic aware graph model for reducing position bias in emotion-cause pair extraction. arXiv preprint arXiv:2205.02132.\\n\\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. 2008. Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation, 42(4):335\u2013359.\\n\\nHang Chen, Xinyu Yang, and Chenguang Li. 2023. Learning a general clause-to-clause relationships for enhancing emotion-cause pair extraction.\\n\\nSheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo, Lun-Wei Ku, et al. 2018. Emotionlines: An emotion corpus of multi-party conversations. arXiv preprint arXiv:1802.08379.\\n\\nLu Cheng, Ruocheng Guo, Raha Moraffah, Paras Sheth, Kasim Selcuk Candan, and Huan Liu. 2022. Evaluation methods and measures for causal learning algorithms. IEEE Transactions on Artificial Intelligence.\\n\\nDiego Colombo, Marloes H Maathuis, Markus Kalisch, and Thomas S Richardson. 2012. Learning high-dimensional directed acyclic graphs with latent and selection variables. The Annals of Statistics, pages 294\u2013321.\\n\\nZixiang Ding, Rui Xia, and Jianfei Yu. 2020. ECPE-2D: Emotion-cause pair extraction based on joint two-dimensional representation, interaction and prediction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3161\u20133170, Online. Association for Computational Linguistics.\\n\\nShutong Feng, Nurul Lubis, Christian Geishauser, Hsien-chin Lin, Michael Heck, Carel van Niekerk, and Milica Ga\u0161ic. 2022. Emowoz: A large-scale corpus and labelling scheme for emotion recognition in task-oriented dialogue systems.\\n\\nDeepanway Ghosal, Navonil Majumder, Soujanya Poria, Niyati Chhaya, and Alexander Gelbukh. 2019. DialogueGCN: A graph convolutional neural network for emotion recognition in conversation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 154\u2013164, Hong Kong, China. Association for Computational Linguistics.\\n\\nTaichi Ishiwatari, Yuki Yasuda, Taro Miyazaki, and Jun Goto. 2021. Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7360\u20137370, Online. Association for Computational Linguistics.\\n\\nJames M Joyce. 2011. Kullback-leibler divergence. In International encyclopedia of statistical science, pages 720\u2013722. Springer.\\n\\nEnkelejda Kasneci, Kathrin Se\u00dfler, Stefan K\u00fcchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan G\u00fcnnemann, Eyke H\u00fcllermeier, et al. 2023. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and Individual Differences, 103:102274.\\n\\nD. P. Kingma and M. Welling. 2014. Auto-encoding variational bayes. arXiv.org.\\n\\nBoris Knyazev, Graham W Taylor, and Mohamed Amer. 2019. Understanding attention and generalization in graph neural networks. Advances in neural information processing systems, 32.\\n\\nMeredyth Krych-Appelbaum, Julie Banzon Law, Dayna Jones, Allyson Barnacz, Amanda Johnson, and Julian Paul Keenan. 2007. \u201ci think i know what you mean\u201d: The role of theory of mind in collaborative communication. Interaction Studies, 8(2):267\u2013280.\\n\\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. DailyDialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 986\u2013995, Taipei, Taiwan. Asian Federation of Natural Language Processing.\\n\\nZheng Lian, Bin Liu, and Jianhua Tao. 2021. Decn: Dialogical emotion correction network for conversational emotion recognition. Neurocomputing, 454:483\u2013495.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Madaan Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nNavonil Majumder, Soujanya Poria, Devamanyu Hazarika, Rada Mihalcea, Alexander Gelbukh, and Erik Cambria. 2019. Dialoguernn: An attentive rnn for emotion detection in conversations. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 6818\u20136825.\"}"}
{"id": "emnlp-2023-main-33", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jinjie Ni. 2023. Attention mechanism optimization for sub-symbolic-based and neural-symbolic-based natural language processing.\\n\\nAna Rita Nogueira, Andrea Pugnana, Salvatore Ruggeri, Dino Pedreschi, and Jo\u00e3o Gama. 2022. Methods and tools for causal discovery and causal inference. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 12(2):e1449.\\n\\nJuan Miguel Ogarrio, Peter Spirtes, and Joe Ramsey. 2016. A hybrid causal search algorithm for latent variable models. In Conference on probabilistic graphical models, pages 368\u2013379. PMLR.\\n\\nDesmond C Ong, Jamil Zaki, and Noah D Goodman. 2015. Affective cognition: Exploring lay theories of emotion. Cognition, 143:141\u2013162.\\n\\nDesmond C Ong, Jamil Zaki, and Noah D Goodman. 2019. Computational models of emotion inference in theory of mind: A review and roadmap. Topics in cognitive science, 11(2):338\u2013357.\\n\\nPatr\u00edcia Pereira, Helena Moniz, and Joao Paulo Carvalho. 2023. Deep emotion recognition in textual conversations: A survey. arXiv preprint arXiv:2211.09172.\\n\\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2019. MELD: A multimodal multi-party dataset for emotion recognition in conversations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 527\u2013536, Florence, Italy. Association for Computational Linguistics.\\n\\nSoujanya Poria, Navonil Majumder, Devamanyu Hazarika, Deepanway Ghosal, Rishabh Bhardwaj, Samuel Yu Bai Jian, Pengfei Hong, Romila Ghosh, Abhinaba Roy, Niyati Chhaya, et al. 2021. Recognizing emotion cause in conversations. Cognitive Computation, 13(5):1317\u20131332.\\n\\nRuben Sanchez-Romero, Joseph D Ramsey, Kun Zhang, Madelyn RK Glymour, Biwei Huang, and Clark Glymour. 2019. Estimating feedforward and feedback effective connections from fmri time series: Assessments of statistical methods. Network Neuroscience, 3(2):274\u2013306.\\n\\nWeizhou Shen, Junqing Chen, Xiaojun Quan, and Zhixiang Xie. 2022. Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition. In AAAI.\\n\\nWeizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun Quan. 2021. Directed acyclic graph network for conversational emotion recognition. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1551\u20131560, Online. Association for Computational Linguistics.\\n\\nShohei Shimizu and Kenneth Bollen. 2014. Bayesian estimation of causal direction in acyclic structural equation models with individual-specific confounder variables and non-gaussian distributions. J. Mach. Learn. Res., 15(1):2629\u20132652.\\n\\nShohei Shimizu, Patrik O Hoyer, Aapo Hyv\u00e4rinen, Antti Kerminen, and Michael Jordan. 2006. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10).\\n\\nKeisuke Shirai, Hirotaka Kameko, and Shinsuke Mori. 2023. Towards flow graph prediction of open-domain procedural texts. arXiv preprint arXiv:2305.19497.\\n\\nFrancesc Sidera, Georgina Perpi\u00f1\u00e0, J\u00e8ssica Serrano, and Carles Rostan. 2018. Why is theory of mind important for referential communication? Current Psychology, 37:82\u201397.\\n\\nPater Spirtes, Clark Glymour, Richard Scheines, Stuart Kauffman, Valerio Aimale, and Frank Wimberly. 2000. Constructing bayesian network models of gene expression networks from microarray data.\\n\\nChandler Squires, Annie Yun, Eshaan Nichani, Raj Agrawal, and Caroline Uhler. 2022. Causal structure discovery between clusters of nodes induced by latent factors. In Conference on Causal Learning and Reasoning, pages 669\u2013687. PMLR.\\n\\nPalash Thakur, Ronit Shahu, and Vikas Gupta. 2023. Audio and text-based emotion recognition system using deep learning. In Advances in Signal Processing, Embedded Systems and IoT: Proceedings of Seventh ICMEET-2022, pages 447\u2013459. Springer.\\n\\nVeronika Thost and Jie Chen. 2021. Directed acyclic graph neural networks. arXiv preprint arXiv:2101.07965.\\n\\nHande Aka Uymaz and Senem Kumova Metin. 2022. Vector based sentiment and emotion analysis from text: A survey. Engineering Applications of Artificial Intelligence, 113:104922.\\n\\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903.\\n\\nPenghui Wei, Jiahao Zhao, and Wenji Mao. 2020. Effective inter-clause modeling for end-to-end emotion-cause pair extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3171\u20133181, Online. Association for Computational Linguistics.\\n\\nRui Xia and Zixiang Ding. 2019. Emotion-cause pair extraction: A new task to emotion analysis in texts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1003\u20131012, Florence, Italy. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-33", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sayyed Zahiri and Jinho D. Choi. 2018. Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks. In Proceedings of the AAAI Workshop on Affective Content Analysis, AFFCON\u201918, pages 44\u201351, New Orleans, LA.\\n\\nDong Zhang, Liangqing Wu, Changlong Sun, Shoushan Li, Qiaoming Zhu, and Guodong Zhou. 2019. Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations. In IJCAI, pages 5415\u20135421.\\n\\nA Proof of Definition 2\\n\\nLet $X$ and $Y$ be two variables in an SCM, with their respective noise terms denoted as $E_X$ and $E_Y$ (where $E_X$ and $E_Y$ are mutually independent).\\n\\nLet $\\\\hat{X}$ and $\\\\hat{Y}$ represent the fitted values of $X$ and $Y$ w.r.t. each other: $\\\\hat{X} = \\\\lambda Y$ and $\\\\hat{Y} = \\\\frac{1}{\\\\lambda} X$.\\n\\nThe residual terms between the fitted values and the true values are denoted as $\\\\Sigma X = X - \\\\hat{X}$ and $\\\\Sigma Y = Y - \\\\hat{Y}$. The true strength of $Y \\\\rightarrow X$ is $k$.\\n\\nHence, if the SCM only contains two variables writing:\\n\\n$$Y = E_Y$$\\n\\n$$X = kY + E_X$$\\n\\nThe residual terms could write:\\n\\n$$\\\\Sigma X = X - \\\\lambda \\\\left(kX - E_X\\\\right)$$\\n\\n$$\\\\Sigma Y = Y - \\\\frac{1}{\\\\lambda} \\\\left(kY + E_X\\\\right)$$\\n\\nThen, if the true causal relationship is from $Y$ to $X$, $\\\\lambda = k$.\\n\\n$\\\\Sigma X$ does not contain the term of $E_Y$ while $\\\\Sigma Y$ contains the term of $E_X$. We could obtain the independence of residual terms writing:\\n\\n$$\\\\Sigma X = \\\\lambda E_X \\\\perp \\\\perp Y$$\\n\\n$$\\\\Sigma Y = \\\\frac{1}{\\\\lambda} E_X \\\\not\\\\perp \\\\not\\\\perp X$$\\n\\nand vice versa. Therefore, we could obtain the independence condition:\\n\\n\u2022 $\\\\Sigma X \\\\perp \\\\perp Y$, $\\\\Sigma Y \\\\not\\\\perp \\\\not\\\\perp X \\\\Rightarrow Y \\\\rightarrow X$\\n\\n\u2022 $\\\\Sigma X \\\\not\\\\perp \\\\not\\\\perp Y$, $\\\\Sigma Y \\\\perp \\\\perp X \\\\Rightarrow X \\\\rightarrow Y$\\n\\nFurthermore, there may exist a set of independence:\\n\\n$$\\\\Sigma X \\\\not\\\\perp \\\\not\\\\perp Y$$\\n\\n$$\\\\Sigma Y \\\\not\\\\perp \\\\not\\\\perp X$$\\n\\nWe would like to assume that there is a latent variable $L$, for this situation, constructing two relationships $L \\\\rightarrow X$ and $L \\\\rightarrow Y$.\\n\\nThen we obtain:\\n\\n$$\\\\Sigma L \\\\not\\\\perp \\\\not\\\\perp X$$\\n\\n$$\\\\Sigma L \\\\not\\\\perp \\\\not\\\\perp Y$$\\n\\nBy utilizing the transitivity of conditional independence, we can establish $X \\\\not\\\\perp \\\\not\\\\perp Y$, and finally achieve the situation:\\n\\n$$\\\\Sigma X \\\\not\\\\perp \\\\not\\\\perp Y$$\\n\\n$$\\\\Sigma Y \\\\not\\\\perp \\\\not\\\\perp X$$\\n\\nWe likewise assume a latent variable $L$ establishing $X \\\\rightarrow L$ and $Y \\\\rightarrow L$ for the opposite situation where $\\\\Sigma X \\\\perp \\\\perp Y$, $\\\\Sigma Y \\\\perp \\\\perp X$, and $X$, $Y$ are two isolated variables in SCM. From the above independence conditions, we could obtain:\\n\\n$$\\\\Sigma L \\\\perp \\\\perp X$$\\n\\n$$\\\\Sigma L \\\\perp \\\\perp Y$$\\n\\nDue to the graph structure of SCM, we could obtain:\\n\\n$$\\\\Sigma X \\\\perp \\\\perp Y$$\\n\\n$$\\\\Sigma Y \\\\perp \\\\perp X$$\\n\\n\u21d2 $X \\\\perp \\\\perp Y$.\\n\\nConsidering the residual terms, we finally obtain:\\n\\n$$X \\\\not\\\\perp \\\\not\\\\perp \\\\Sigma X$$\\n\\nand\\n\\n$$X \\\\perp \\\\perp Y \\\\Rightarrow \\\\Sigma X \\\\perp \\\\perp Y$$\\n\\n$$Y \\\\not\\\\perp \\\\not\\\\perp \\\\Sigma Y$$\\n\\nand\\n\\n$$Y \\\\perp \\\\perp X \\\\Rightarrow \\\\Sigma Y \\\\perp \\\\perp X$$.\\n\\nHence, we could obtain additional two independence conditions:\\n\\n\u2022 $\\\\Sigma X \\\\not\\\\perp \\\\not\\\\perp Y$, $\\\\Sigma Y \\\\not\\\\perp \\\\not\\\\perp X \\\\Rightarrow L \\\\rightarrow X, L \\\\rightarrow Y$\\n\\n\u2022 $\\\\Sigma X \\\\perp \\\\perp Y$, $\\\\Sigma Y \\\\perp \\\\perp X \\\\Rightarrow X \\\\rightarrow L, Y \\\\rightarrow L$\\n\\nBased on the independence conditions of 2-variables SCM, we could extend it to the general SCM including more than 2 variables. Given any two variables in a SCM, we could testify to the independence condition and finally orientate via the whole SCM.\\n\\nB Hypotheses and Algorithms for Skeletons\\n\\nHypothesis 0. $\\\\forall U_i \\\\in D$, it has the same causal skeleton as other utterances.\\n\\nBy regarding Hypothesis 0 as the prior knowledge, a common causal skeleton containing a target variable and a fixed number of related variables can reason about the relations between the target utterance and other considered utterances. We denote this skeleton of $U_t$ by $S(U_t)$.\\n\\nThere are $\\\\forall U_i, U_j \\\\in D$, $S(U_i) = S(U_j)$.\\n\\nAdditionally, there are some other empirical hypotheses from the above approaches. These hypotheses can be divided into two categories: one is about the \u201corder\u201d of utterances ($Hypotheses 1, 2, 3$), and the other is about intermingling dynamics among the interlocutors ($Hypotheses 4, 5, 6$).\\n\\nHypothesis 1. (Majumder et al., 2019) Under the sequential order, the target utterance receives information only from the previous utterance.\"}"}
{"id": "emnlp-2023-main-33", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Statistics of 6 cogn skeletons. We detailed the hypotheses each cogn skeleton adopted and the original works from which we designed them.\\n\\nHypothesis 2. (Wei et al., 2020) Under the graph order, the target utterance receives information from all other utterances.\\n\\nHypothesis 3. (Ghosal et al., 2019) Under the local graph order, target utterance receives local information from \\\\( k \\\\) surround utterances.\\n\\nHypothesis 4. (Zhang et al., 2019) The influence between two utterances can be discriminated by whether the two utterances belong to the same speaker identity.\\n\\nHypothesis 5. (Lian et al., 2021) Target utterance only receives information from the predecessor utterances.\\n\\nHypothesis 6. (Shen et al., 2021) Between two utterances both related to the target utterance, there is also information passing, often dubbed as a partial order.\\n\\nA cogn skeleton is denoted by \\\\( H = (V, E, M) \\\\).\\n\\nThe \\\\( V = U_1, U_2, U_3, ..., U_N \\\\) represents a set of utterances in a conversation, and the edge \\\\((i, j, m) \u2208 E\\\\) denotes the influence from \\\\( U_i \\\\) to \\\\( U_j \\\\), where \\\\( m_{i,j} \u2208 M \\\\) is the type of the edge depending on whether \\\\( U_i \\\\) and \\\\( U_j \\\\) belong to one and the same speaker. Thus \\\\( M = 0, 1 \\\\), where 1 for that they are the same speaker and 0 for different. Then we denote the speaker type of \\\\( U_i \\\\) by a function \\\\( p(U_i) \\\\).\\n\\nAt last, we show the process of building 6 cogn skeletons in Algorithms 1\u22126.\\n\\nAlgorithm 1: Building I cogn skeleton\\nInput: \\\\( D, p(\u00b7) \\\\), \\\\( k \\\\)\\nOutput: \\\\( H = (V, E) \\\\)\\n\\n1. \\\\( V \u2190 U_1, U_2, U_3, ..., U_N \\\\)\\n2. \\\\( E \u2190 \u2205 \\\\)\\n3. \\\\( \u2200i \u2208 2, 3, ..., N\u22121 \\\\) do\\n   4. \\\\( E \u2190 E \u222a (i, i+1) \\\\)\\n4. return \\\\( H = (V, E) \\\\)\\n\\nFinally, in Figure 5, we show the adjacency matrix of each cogn skeleton by inputting a binary alternating conversation case with 6 utterances. But Algorithm 2:\\n\\nAlgorithm 2: Building II cogn skeleton\\nInput: \\\\( D, p(\u00b7) \\\\), \\\\( k \\\\)\\nOutput: \\\\( H = (V, E) \\\\)\\n\\n1. \\\\( V \u2190 U_1, U_2, U_3, ..., U_N \\\\)\\n2. \\\\( E \u2190 \u2205 \\\\)\\n3. \\\\( \u2200i \u2208 2, 3, ..., N \\\\) do\\n   4. \\\\( \u2200j \u2208 2, 3, ..., N \\\\) do\\n      5. if \\\\( i \\\\neq j \\\\) then\\n         6. \\\\( E \u2190 E \u222a (j, i) \\\\)\\n      7. else\\n         8. continue\\n8. return \\\\( H = (V, E) \\\\)\\n\\nAlgorithm 3: Building III cogn skeleton\\nInput: \\\\( D, p(\u00b7) \\\\), \\\\( k \\\\)\\nOutput: \\\\( H = (V, E, M) \\\\)\\n\\n1. \\\\( V \u2190 U_1, U_2, U_3, ..., U_N \\\\)\\n2. \\\\( E \u2190 \u2205 \\\\)\\n3. \\\\( M \u2190 0, 1 \\\\)\\n4. \\\\( \u2200i \u2208 2, 3, ..., N \\\\) do\\n   5. \\\\( \u2200j \u2208 2, 3, ..., N \\\\) do\\n      6. if \\\\( p(U_j) = p(U_i) \\\\) and \\\\( i \\\\neq j \\\\) then\\n         7. \\\\( E \u2190 E \u222a (j, i, 1) \\\\)\\n      8. else if \\\\( p(U_j) \\\\neq p(U_i) \\\\) and \\\\( i \\\\neq j \\\\) then\\n         9. \\\\( E \u2190 E \u222a (j, i, 0) \\\\)\\n      10. else\\n         11. continue\\n12. return \\\\( H = (V, E, M) \\\\)\"}"}
{"id": "emnlp-2023-main-33", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Adjacency matrices towards 6 cogn skeletons when $k = 2$. \\n\\n$((i,j) \\\\neq \\\\text{None})$ represents that $U_i$ is influenced by $U_j$.\\n\\nAlgorithm 4: Building IV cogn skeleton\\n\\nInput: $D, p(\\\\cdot)$\\n\\nOutput: $H = (V, E, M)$\\n\\n1. $V \\\\leftarrow U_1, U_2, U_3, ..., U_N$\\n2. $E \\\\leftarrow \\\\emptyset$\\n3. $M \\\\leftarrow 0, 1$\\n\\n4. forall $i \\\\in 2, 3, ..., N$ do\\n\\n5. forall $j \\\\in 2, 3, ..., N$ do\\n\\n6. if $p(U_j) = p(U_i)$ and $0 < |i - j| < k$ then\\n\\n7. $E \\\\leftarrow E \\\\cup (j, i, 1)$\\n\\n8. else if $p(U_j) \\\\neq p(U_i)$ and $0 < |i - j| < k$ then\\n\\n9. $E \\\\leftarrow E \\\\cup (j, i, 0)$\\n\\n10. else\\n\\n11. Continue\\n\\n12. return $H = (V, E, M)$\\n\\nAlgorithm 5: Building V cogn skeleton\\n\\nInput: $D, p(\\\\cdot)$\\n\\nOutput: $H = (V, E, M)$\\n\\n1. $V \\\\leftarrow U_1, U_2, U_3, ..., U_N$\\n2. $E \\\\leftarrow \\\\emptyset$\\n3. $M \\\\leftarrow 0, 1$\\n\\n4. forall $i \\\\in 2, 3, ..., N$ do\\n\\n5. $\\\\gamma \\\\leftarrow i - 1$\\n\\n6. if $p(U_\\\\gamma) = p(U_i)$ then\\n\\n7. $E \\\\leftarrow E \\\\cup (\\\\gamma, i, 1)$\\n\\n8. else\\n\\n9. $E \\\\leftarrow E \\\\cup (\\\\gamma, i, 0)$\\n\\n10. $\\\\gamma \\\\leftarrow \\\\gamma - 1$\\n\\n11. return $H = (V, E, M)$\\n\\nAlgorithm 6: Building VI cogn skeleton\\n\\nInput: $D, p(\\\\cdot)$\\n\\nOutput: $H = (V, E, M)$\\n\\n1. $V \\\\leftarrow U_1, U_2, U_3, ..., U_N$\\n2. $E \\\\leftarrow \\\\emptyset$\\n3. $M \\\\leftarrow 0, 1$\\n\\n4. forall $i \\\\in 2, 3, ..., N$ do\\n\\n5. $c \\\\leftarrow 0$\\n\\n6. $\\\\gamma \\\\leftarrow i - 1$\\n\\n7. while $\\\\gamma > 0$ and $c < k$ do\\n\\n8. if $p(U_\\\\gamma) = p(U_i)$ then\\n\\n9. $E \\\\leftarrow E \\\\cup (\\\\gamma, i, 1)$\\n\\n10. $c \\\\leftarrow c + 1$\\n\\n11. else\\n\\n12. $E \\\\leftarrow E \\\\cup (\\\\gamma, i, 0)$\\n\\n13. $\\\\gamma \\\\leftarrow \\\\gamma - 1$\\n\\n14. return $H = (V, E, M)$\\n\\nNote that adjacency can not indicate all the differences among these skeletons, for example, Hypothesis 6 takes effect when the model learns the relationship based on the VI's skeleton.\\n\\nC Datasets\\n\\nDailyDialog (Li et al., 2017): A Human-written dialogs dataset with 7 emotion labels (neutral, happiness, surprise, sadness, anger, disgust, and fear). We follow Shen et al. (2021) to regard utterance turns as speaker turns.\\n\\nMELD (Poria et al., 2019): A multimodel ERC dataset with 7 emotion labels as the same as DailyDialog.\\n\\nEmoryNLP (Zahiri and Choi, 2018): A TV show scripts dataset with 7 emotion labels (neutral, sad, mad, scared, powerful, peaceful, joyful).\\n\\nIEMOCAP (Busso et al., 2008): A multimodel ERC dataset with 9 emotion labels (neutral, happy, sad, angry, frustrated, excited, surprised, disappointed, and fear). However, models in ERC field...\"}"}
{"id": "emnlp-2023-main-33", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are often evaluated on samples with first six emotions due to the too few samples of latter three emotions. 20 dialogues for validation set is following Shen et al. (2021).\\n\\n**RECCON** (Poria et al., 2021): The first dataset for emotion cause recognition of conversation including RECCON-DD and RECCON-IE (a subset emulating an out-of-distribution generalization test). RECCON-DD includes 5380 labeled ECPs and 5 cause spans (no-context, interpersonal, self-contagion, hybrid, and latent).\\n\\n**Synthetic dataset**: We create a synthetic dataset by following the benchmark of the causal discovery field (Agrawal et al., 2021; Squires et al., 2022). To minimize sample bias, we did not randomly draw causal graphs as samples. Inversely, the number of samples in the synthetic dataset and the number of utterances and labels per sample are restricted to be consistent with RECCON. We use Causal Additive Models (CAM), specifically SCM structure for our datasets. As shown in Algorithm 7, first, we assume that each i.i.d. implicit causes\\n\\n$E \\\\sim N(1, 1)$ if it is an emotion utterance in the original dataset, and $E \\\\sim N(-1, 1)$ if it is not. Then, we update each utterance via speaker turns $S$:\\n\\nIf there is an emotion-cause pair $(U_i, U_j) \\\\in L$, then\\n\\n$U_i = \\\\alpha_{j,i} U_j + E_i (\\\\alpha_{j,i} \\\\sim \\\\text{Uniform}(0.7, 1))$, and for those pairs without emotion-cause label, $\\\\alpha_{j,i} \\\\sim \\\\text{Uniform}(0, 0.3)$. Finally, we randomly select a noise $\\\\xi \\\\sim \\\\text{Uniform}(-0.25, 0.25)$ for each utterance $U_i = U_i + \\\\xi_i$.\\n\\n**D Implementation Details**\\n\\nIn the word embedding, we adopt the affect-based pre-trained features proposed by Shen et al. (2021) for all baselines and models. Although there are different pre-trained models in these skeleton baselines, the SOTA work DAG-ERC and EGAT have investigated their performances in a consistent pre-trained model. Therefore, for a fair and direct comparison, we continue this benchmark using the pre-trained embedding published by DAG-ERC for three tasks.\\n\\nIn the hyper-parameters, we follow the setting of Shen et al. (2021) in the ERC task. Moreover, in the ECPE and ECSR, the learning rate is set to $3e-5$, batch size is set to 32, and epoch is set to 60. Further in our approach, we set $L$ to 1, and implicit cause size is set to 192, hidden size of GNN is set to 1.\\n\\n[Algorithm 7: Creating Non-noise Synthetic dataset]\\n\\n**E Other Experiments in Affective Reasoning**\\n\\nIn Table 7, our approach performs better than the corresponding baseline under all skeletons in four datasets. Hence, using a causal auto-encoder to find the implicit causes benefits this task. Besides, our approach improves significantly under skeletons II, III, and IV. From Figure 2, these three skeletons have more relevant nodes than others, so there are more redundant edges to be corrected.\"}"}
{"id": "emnlp-2023-main-33", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Skt Model | DailyDialog | MELD | EmoryNLP | IEMOCAP |\\n|-----------|-------------|------|----------|---------|\\n| II        |             |      |          |         |\\n| DialogXL  | 54.93       | 62.41| 34.73    | 65.94   |\\n| Ours      | 59.51       | 63.62| 39.16    | 66.47   |\\n| III       |             |      |          |         |\\n| EGAT\u2020     | 59.23       | 63.51| 38.77    | 66.76   |\\n| Ours      | 59.68       | 63.71| 39.62    | 68.18   |\\n| IV        |             |      |          |         |\\n| RGAT      | 54.31       | 60.91| 34.42    | 65.22   |\\n| Ours      | 59.65       | 63.69| 39.22    | 67.65   |\\n| V         |             |      |          |         |\\n| DECN\u2020     | 59.08       | 63.78| 39.44    | 67.41   |\\n| Ours      | 59.28       | 63.91| 40.11    | 67.61   |\\n| VI        |             |      |          |         |\\n| DAG-ERC   | 59.33       | 63.65| 39.02    | 68.03   |\\n| Ours      | 59.53       | 63.81| 39.54    | 69.17   |\\n\\nTable 7: Overall performance in ERC task. \u2020 denotes the results implemented in this paper. The better scores in the same skeleton are in bold, and the best of all skeletons is in red.\\n\\nFigure 6: Further layers $L$ and related node number $K$ with VI skeleton model in ECPE task.\\n\\nThen, we investigate how the number of layers and the variants of causal skeletons would affect the performance of our approach. So we further conducted several contrasts with $k$ up to 5 and $L$ up to 6, as shown in Figure 6. One observation is that the best performance occurs at either $k = 1$, 2, or 3, which indicates that $k \\\\geq 4$ offers no advantage and even leads to confounding. Moreover, $L = 1$ achieves the best performance under all $k$ values.\\n\\nIn other words, one layer is sufficient to yield the most effective implicit causes.\\n\\nF Visualization of Causal Graph\\n\\nIn the Figure 7 to 11, we showed the Visualization of the adjacency matrix $(I - AT)^{-1}$. When the auxiliary loss $\\\\text{Loss}_{KL}$ achieves the lower bound, $(I - AT)^{-1}$ represents the relationship matrix between utterances and implicit causes.\\n\\nIn the ECPE task, we extracted 10 samples from test sets in different folds. To facilitate comparison and contrasting, we selected five 7-utterances cases and five 8-utterances cases. The IDs are as follows:\\n\\n- 7-utterances cases: 110, 170, 224, 372, 500.\\n- 8-utterances cases: 62, 74, 104, 177, 584.\\n\\nTo obtain the non-negative value, we adopted the $T = \\\\text{sigmoid}(\\\\cdot - 0.05)$ to process the original tensors $(I - AT)^{-1}$ outputted from the encoder. We follow a common practice: set the threshold as 0.05 to delete some unimportant edges. And to highlight which implicit cause contributes the each utterance best, we adopted the $\\\\text{softmax}(\\\\cdot)$ to process columns afterward and labeled the block with value $> 0$.\\n\\nIt is expected that: (i) when skeletons construct overage edges, our model is able to degrade the influences of some negligible utterances by deleting the corresponding edges from their implicit causes. (ii) when skeletons construct insufficient edges, our model can add some edges to obtain more information.\\n\\nG Proof of emotion consistency of implicit causes and utterances\\n\\nWe would like to explain why implicit causes and utterances are consistent in emotion from both theory and equation, in the condition where emotional utterance and cause utterance possess different emotion types.\\n\\nWe define the implicit causes as the unobservable emotional desire and the utterances as the observable emotional expression. This definition is proposed in Ong et al. (2019, 2015), which also argues that emotional expression is affected by desires and event outcomes. Moreover, for emotion utterances that are not influenced by explicit cause factors, the source of their emotions should originate from implicit causes. The desire and the expression generally belong to the same emotion because the outcomes often have little effect on emotional expression. Our paper can also deduce this conclusion from the SCM (Equation 1). Considering there is a linear map $f(\\\\cdot)$ from representation space to emotion space. Then we can obtain the...\"}"}
{"id": "emnlp-2023-main-33", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[(I-A)U = f(E) \\\\quad (16)\\\\]\\n\\n\\\\[(I-A)f(U) = f(E) \\\\quad (17)\\\\]\\n\\n\\\\[f(U) = WTf(E) \\\\quad (18)\\\\]\\n\\nNote that \\\\(W = (I-A)\\\\) and \\\\(A_{i,i} = 0\\\\). So in \\\\(W\\\\), the value of the elements on the diagonal is constant at 1 and is a constant maximum of each column.\\n\\nNaturally, \\\\(f(E)\\\\) is an approximate estimate of \\\\(f(U)\\\\) especially since \\\\(U_t\\\\) and \\\\(U_i\\\\) in the ECP \\\\((U_t, U_i)\\\\) do not belong to the same emotion category, which is why we think implicit causes are reasonable when the F1 score of Table 6 is high.\\n\\nTherefore, we test the F1 scores in ERC task by replacing \\\\(\\\\hat{H}\\\\) with \\\\(E\\\\) from a consensus that implicit causes should be aligned with utterances in the emotion types.\\n\\nIn Table 8, we reported the overall results of \\\\(E\\\\) in ERC task. Note that we only examine the sample of ECP with different emotion types. Among five skeletons and four datasets, almost all results achieve 90% scores of corresponding performances of \\\\(\\\\hat{H}\\\\), which indicates that \\\\(E\\\\) is practically aligned with \\\\(\\\\hat{H}\\\\) in the affective dimension.\"}"}
