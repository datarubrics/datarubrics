{"id": "emnlp-2022-main-733", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In retrieval-based dialogue systems, a response selection model acts as a ranker to select the most appropriate response among several candidates. However, such selection models tend to rely on context-response content similarity, which makes models vulnerable to adversarial responses that are semantically similar but not relevant to the dialogue context. Recent studies have shown that leveraging these adversarial responses as negative training samples is useful for improving the discriminating power of the selection model. Nevertheless, collecting human-written adversarial responses is expensive, and existing synthesizing methods often have limited scalability. To overcome these limitations, this paper proposes a simple but efficient method for generating adversarial negative responses leveraging a large-scale language model. Experimental results on dialogue selection tasks show that our method outperforms other methods of synthesizing adversarial negative responses. These results suggest that our method can be an effective alternative to human annotators in generating adversarial responses. Our dataset and generation code is available at https://github.com/leenw23/generating-negatives-by-gpt3.\\n\\n1 Introduction\\n\\nIn retrieval-based dialogue systems, the response selection model aims to predict the most appropriate response among multiple candidates retrieved for a given conversation context (Zhou et al., 2018; Wu et al., 2019; Chen et al., 2022). The selection model is generally trained to distinguish a related (positive) response from randomly sampled negative responses on training datasets, but such a model generally poses the following problems. First, randomly selected negatives are often too easy to distinguish because they are totally irrelevant to the dialogue context (Li et al., 2019; Lin et al., 2020). In this case, the model is more likely to predict the response only by relying on the superficial content similarity of the context-response pairs (Yuan et al., 2019; Sai et al., 2020; Whang et al., 2021). These models are vulnerable to adversarial responses with high content similarity to the dialogue context, and fail to distinguish subtle differences in various contexts in real-world scenarios. Second, random sampling causes sampling bias, such as containing false negatives for a given dialogue context (Zhou et al., 2022). These biases inherent in the training datasets hinder the accurate prediction of the selection model, resulting in performance degradation.\\n\\nTo mitigate this problem, recent studies have proposed various methods to synthesize and leverage adversarial negative training samples so that the selection model can learn features beyond content similarity (Srivastava et al., 2020; Kaushik et al., 2021). However, existing methods for synthesizing adversarial negative responses (Ebrahimi et al., 2018; Alzantot et al., 2018; Zhang et al., 2019; Qiu et al., 2021; Gupta et al., 2021) still have limitations in creating human-like responses. The most reliable method is to collect human-written adversarial negatives (Sai et al., 2020), but it is not scalable because it is expensive and time consuming.\"}"}
{"id": "emnlp-2022-main-733", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"To overcome these limitations, we note that large-scale language models such as GPT3 can be utilized as a low-cost data labeler (Wang et al., 2021). In this paper, we present PNEG, a Prompt-based NEgative response Generation method leveraging a large-scale language model (Figure 1). In P-NEG, we can cheaply collect human-like adversarial negatives by using an in-context learning-based data augmentation method with human-written samples as demonstration examples.\\n\\nExperimental results on the dialogue response selection task show that the selection model that trained the negative samples from PNEG has better performance than other baselines. We then conduct quality evaluation and ablation studies to analyze the validity of PNEG. Consequently, we argue that PNEG can be an efficient alternative to human annotators in generating adversarial responses.\\n\\nOur contributions are as follows:\\n\\n\u2022 We propose PNEG, a Prompt-based NEgative response Generation method.\\n\u2022 Our method can generate high-quality adversarial negative responses only with a few human-written examples.\\n\u2022 We show that our method outperforms other baselines across multiple model architectures on the response selection task.\\n\\n2 PNEG: Prompt-based NEgative response Generation\\n\\nLarge-scale language models such as GPT-3 (Brown et al., 2020) can augment fluent text training samples using natural language prompts and in-context examples (Yoo et al., 2021; Schick and Sch\u00fctze, 2021; Bae et al., 2022; Liu et al., 2022). By extending such direction to the response selection task, we propose PNEG, a Prompt-based NEgative response Generation method for robust response selection models. Our method consists of three steps: (1) selecting demonstration examples, (2) constructing a prompt containing examples and target dialogue context, and (3) generating adversarial negative responses by inputting the prompt into GPT-3. The generated negative responses are used as training samples for response selection task.\\n\\n2.1 Step 1: Example Selection\\n\\nWe first sample a total $k$ demonstration examples from an example set $E$ to construct a prompt for creating five irrelevant responses containing keywords of the given dialogue context:\\n\\n### Dialogue context:\\n\\n\"\"\"A: Yes, it's the same every day \u2026\\nB: I'll look forward to that, I can't stand\u2026\\n\"\"\"  \\n\\nCreate five irrelevant responses containing keywords of the given dialogue context:\\n\\n1. I thought that the flight ticket ...\\n2. Actually I was looking forward to...\\n3. Yes, I bought all the document \u2026\\n4. I use my mobile banking app \u2026\\n5. I thought there might be a quite ...\\n\\n2.2 Step 2: Prompt Construction\\n\\nPrompt Design\\n\\nWe first design a prompt to use as an input to GPT-3. Large-scale language models are generally pre-trained to generate responses with high coherence for a given context, but our method aims to generate adversarial responses with high content similarity but low coherence. In particular, natural language prompt-based reasoning is sensitive to prompt changes (Jin et al., 2022), so we have to carefully design prompts to generate accurate negative responses. We devise several prompts to evaluate generative quality, inspired by related works (Yoo et al., 2021; Schick and Sch\u00fctze, 2021; Min et al., 2022). We evaluate the devised prompts by the response selection task, and then select the finest prompt template based on the results of Appendix B.\\n\\nPrompt Construction\\n\\nOur prompt is constructed with the chosen prompt template, $k$ number of examples, and the target dialogue context that we aim to generate multiple negative responses. The\\n\\n2.3 Step 3: Generation\\n\\nWe then input the prompt into GPT-3 to generate five negative responses containing keywords of the given dialogue context:\\n\\n### Dialogue context:\\n\\n\"\"\"A: We have been over this a hundred ...\\nB: Why not? Just a cute little puppy ...\\n\"\"\"  \\n\\nCreate five irrelevant responses containing keywords of the given dialogue context:\\n\\n1. I'm seriously worried about my \u2026\\n2. I like my boss. She takes all effort \u2026\\n3. I had a great weekend with \u2026\\n4. I got a new puppy on Christmas Eve ..\\n5. I want to get a dog as birthday gift \u2026\\n\\nFigure 2: Overall pipeline of PNEG.\"}"}
{"id": "emnlp-2022-main-733", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"prompt template consists of three components to clarify the role of each example and target context: (1) a task instruction written in natural language, (2) an enumerator to receive each utterance from examples and the target context, and (3) a separator to separate each example or dialogue context in the prompt. The details of each component in the prompt template are shown in Figure 2.\\n\\n2.3 Step 3: Negative Response Generation\\nNow, GPT-3 can generate augmented negative responses following our input prompt (C in Figure 2). The examples within the prompt encourage the language model to generate negative responses of similar patterns to the human-written negative responses. The task instruction directly guides the model to understand the target task and the relationship between a dialogue context and corresponding negative responses in the examples.\\n\\n3 Experimental Setup\\n3.1 Dialogue Response Selection Task\\nWe evaluate our method on the dialogue response selection task. We train the selection model with 11 response candidates consisting of 1 positive response, 5 random and 5 adversarial negative responses per context. We report the R@1 and mean reciprocal rank (MRR) score. For evaluation, we use random and adversarial test datasets which consist of the 6 candidates for each context.\\n\\n3.2 Datasets\\n\\nDailyDialog++\\nWe use the DailyDialog++ dataset (Sai et al., 2020) for our overall experiments. This dataset consists of 16,900, 1028, and 1142 dialogue contexts in training, validation, and test datasets, respectively. Since only the subset of 9259 contexts in the training dataset contains adversarial responses, we use them as our training dataset. Each context has five adversarially curated negative responses written by human annotators.\\n\\nPersonaChat\\nWe also use the PersonaChat dataset (Zhang et al., 2018) on the response selection task. This dataset consists of 8938, 1000, and 968 dialogue conversations in training, validation, and test datasets, respectively. We use 8938 contexts for training, and concatenate the persona sentences in front of the context. Since there are no human-written adversarial negative responses in this dataset, we create an adversarial test dataset by sampling one response from the context and including it in the candidate responses following Gupta et al. (2021) and Whang et al. (2021).\\n\\n3.3 Baselines\\nWe compare our method with the following baselines.\\n\\nRandom\\nRandomly sampled responses.\\n\\nHuman (Sai et al., 2020)\\nHuman-written adversarial responses in DailyDialog++ dataset.\\n\\nBM25 (Karpukhin et al., 2020)\\nRetrieved responses from BM25 (Robertson and Zaragoza, 2009).\\n\\nSemi-hard (Li et al., 2019)\\nRetrieved responses from training dataset based on their similarity between positive response with a margin of $\\\\alpha$.\\n\\nWe perform a static sampling using sentence-BERT (Reimers and Gurevych, 2019) with $\\\\alpha$ as 0.07 following Gupta et al. (2021).\\n\\nMask-and-fill (Gupta et al., 2021)\\nThis method first randomly masks the words in an answer response, and infills them using masked language modeling conditioned on a random context.\\n\\nKey-sem (Gupta et al., 2021)\\nThis method generates new responses conditioned on keywords in the context using GPT-2 (Radford et al., 2019).\\n\\n$P_{NEG}$ (Ours)\\nGPT-3 generated adversarial negative responses by using our method, $P_{NEG}$.\\n\\n3.4 Models\\nWe train dialogue selection models with different negative responses described in \u00a73.3. The models are based on cross-encoder architecture, and three different pre-trained language models are used in experiments: 1) BERT (Devlin et al., 2019), 2) RoBERTa (Liu et al., 2019), and 3) ELECTRA (Clark et al., 2019). The implementation details are provided in Appendix A.\\n\\n4 Experiments\\n4.1 Performance on Response Selection Task\\nWe compare our method with the baselines for the response selection task (Table 1). On the adversarial test dataset, we notice that $P_{NEG}$ consistently outperforms other baselines across different model architectures. The $P_{NEG}$ especially shows the most similar performance to the human baseline, which\\n\\n1The negative responses samples generated by each method are provided in Table 10.\"}"}
{"id": "emnlp-2022-main-733", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"suggests that our method can be an effective alternative to human annotators for collecting adversarial negative samples. In the random test dataset, Semi-hard performs better than other baselines including \\\\( P_{\\\\text{NEG}} \\\\) and even the human baseline. This tendency indicates that the robustness of the models to the adversarial test dataset does not always lead to the random test dataset. We speculate that these results are due to data distribution shifts according to different negative response sampling strategies (Penha and Hauff, 2021). Retrieval-based methods like BM25 and Semi-hard would suffer from finding appropriate negative responses when the dialogue corpus for response retrieval is limited.\\n\\nWe also compare our method with the baselines for the response selection task on the PersonaChat (Table 2). Although \\\\( P_{\\\\text{NEG}} \\\\) generates negative responses using human-written examples from DailyDialog++, it shows better performance than other baselines in the adversarial test dataset. Such results prove the scalability of \\\\( P_{\\\\text{NEG}} \\\\) across multiple dialogue datasets, because our method can automatically generate adversarial negative samples with only a few human-written samples. It means that our method can easily transfer to other domain conversations only with minimal human effect. Since we used fewer samples for PersonaChat experiments, the response selection models overall show lower performance than the DailyDialog++ experiment.\\n\\n### 4.2 Synthetic Dataset Quality\\n\\nWe evaluate the quality of adversarial responses with predictive scores of the response selection model and context-response similarity model. We assume that the higher the prediction score of the selection model for the adversarial negative response, the more effective it is for the robust training of the model. The evaluation results are shown in Figure 3. In both models, the prediction score for negative responses generated by \\\\( P_{\\\\text{NEG}} \\\\) is higher on average than other responses. The difference is more significant in the score of the selection model, suggesting that \\\\( P_{\\\\text{NEG}} \\\\) can produce more effective adversarial responses that are confused with the positive response. Although Semi-hard samples negative responses using similarity scores from Sentence-BERT, the responses have lower scores than other methods because the sampling pool is limited. We additionally provide human evaluation.\\n\\n2 The experiment details are provided in Appendix C.\\n\\n| Model   | Test Set   | Mean R@1 | Mean MRR |\\n|---------|------------|----------|----------|\\n|         | Random     | Adversarial |\\n| BERT    | Random     | 0.865    | 0.923    |\\n|         | Adversarial| 0.674    | 0.806    |\\n| BM25    | Random     | 0.845    | 0.911    |\\n|         | Adversarial| 0.857    | 0.915    |\\n| Semi-hard | Random     | 0.881    | 0.934    |\\n|         | Adversarial| 0.672    | 0.804    |\\n| Key-sem | Random     | 0.864    | 0.923    |\\n|         | Adversarial| 0.842    | 0.909    |\\n| Mask&fill | Random     | 0.869    | 0.926    |\\n|         | Adversarial| 0.856    | 0.916    |\\n| P_{\\\\text{NEG}} | Random     | 0.867    | 0.924    |\\n| Human   | Random     | 0.870    | 0.926    |\\n|         | Adversarial| 0.937    | 0.964    |\\n| RoBERTa | Random     | 0.879    | 0.932    |\\n|         | Adversarial| 0.658    | 0.797    |\\n| BM25    | Random     | 0.879    | 0.932    |\\n|         | Adversarial| 0.865    | 0.920    |\\n| Semi-hard | Random     | 0.892    | 0.937    |\\n|         | Adversarial| 0.660    | 0.797    |\\n| Key-sem | Random     | 0.889    | 0.937    |\\n|         | Adversarial| 0.868    | 0.924    |\\n| Mask&fill | Random     | 0.873    | 0.927    |\\n|         | Adversarial| 0.868    | 0.922    |\\n| P_{\\\\text{NEG}} | Random     | 0.882    | 0.933    |\\n| Human   | Random     | 0.891    | 0.938    |\\n|         | Adversarial| 0.942    | 0.967    |\\n\\nTable 1: Performance in the dialogue response selection task on Random and Adversarial test datasets based on the DailyDialog++. Among the methods except for human baseline, the best result is shown in bold, and the second-highest result is underlined.\\n\\n| Approach | Test Set   | Mean R@1 | Mean MRR |\\n|----------|------------|----------|----------|\\n|         | Random     | Adversarial |\\n| Rand + Adv. | Random     | 0.815    | 0.316    |\\n| Semi-hard | Random     | 0.814    | 0.338    |\\n| BM25    | Random     | 0.718    | 0.637    |\\n| P_{\\\\text{NEG}} (Ours) | Random     | 0.774    | 0.684    |\\n\\nTable 2: Performance in the dialogue response selection task on Random and Adversarial test datasets based on the PersonaChat. Among the methods except for human baseline, the best result is shown in bold, and the second-highest result is underlined.\\n\\nresults for synthetic quality in Appendix D.\\n\\n### 4.3 Varying the Size of Example Set\\n\\nWe study the effect of the size of the example set \\\\( E \\\\) containing examples for prompt construction on the performance of the response selection task. As shown in Table 3, even if the size of \\\\( E \\\\) becomes extremely small (e.g., 0.1%), the performance on the adversarial test dataset hardly decreases. These results show that our method can efficiently generate high-quality adversarial responses by collecting only a few demo examples. To increase the diversity of examples, we further try +REUSE, which continuously adds the negative responses generated by \\\\( P_{\\\\text{NEG}} \\\\) to \\\\( E \\\\). However, the 0.1%+REUSE has a\"}"}
{"id": "emnlp-2022-main-733", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance on the size changes of \\\\( E \\\\) containing examples used to construct prompts of our method.\\n\\n| Sub. (%) | Test Set (R@1) | Mean | 0.1 | 0.852 | 0.899 | 0.876 |\\n|---------|----------------|------|-----|-------|-------|-------|\\n|         | Random Adv Rand + Adv | 0.1 | 9 | 0.843 | 0.938 | 0.891 |\\n|         | 1 | 9 | 0.845 | 0.936 | 0.891 |\\n|         | 10 | 9 | 0.852 | 0.936 | 0.894 |\\n|         | 100 (P\\\\( \\\\text{NEG} \\\\)) | 9259 | 0.877 | 0.941 | 0.909 |\\n\\nFigure 3: Box plot of prediction scores (blue) and similarity score (orange) for each type of response. The prediction scores are linearly normalized into the [0,1].\\n\\nConclusion\\n\\nIn this paper, we present P\\\\( \\\\text{NEG} \\\\), a prompt-based adversarial negative response generation method for response selection tasks. P\\\\( \\\\text{NEG} \\\\) can collect high-quality adversarial negative responses at a lower cost than human annotators. Our experiments on dialogue response selection tasks show that negative responses generated by P\\\\( \\\\text{NEG} \\\\) can improve the discriminating power of the selection models. In future work, we are interested in designing more confused adversarial responses and how to effectively utilize them for training the selection models.\\n\\nLimitations\\n\\nIn this section, we present two major limitations of our work. First, P\\\\( \\\\text{NEG} \\\\) has not yet been tested on a larger dataset. We expect our method can produce effective adversarial training samples in large-scale datasets at a reasonable cost. However, since the performance of sampling-based methods can improve on sufficiently large training datasets, it is necessary to compare such methods with P\\\\( \\\\text{NEG} \\\\).\\n\\nSecond, there is a limitation to leveraging the adversarial negative response for training the selection model in the dialogue response selection task. The selection model that learns the adversarial negative response improves performance in the adversarial test dataset with the same distribution of candidates, but decreases performance in the random test dataset. This is not the result we were hoping for, and we need a close analysis to solve this phenomenon.\\n\\nEthics Statement\\n\\nWe clarified compliance with the ACL Ethics Policy in this study. In particular, in the human evaluation experiment, we recruited participants fairly, designed experiments that require minimal effort, and provided adequate rewards to participants.\\n\\nAcknowledgements\\n\\nThis work was supported by Institute for Information & Communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No. 2020-0-00368, A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques, No. 2013-2-00131, Development of Knowledge Evolutionary WiseQA Platform Technology for Human Knowledge Augmented Services, and No. 2019-0-00075, Artificial Intelligence Graduate School Program(KAIST)).\\n\\nReferences\\n\\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. 2018. Generating natural language adversarial examples. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2890\u20132896, Brussels, Belgium. Association for Computational Linguistics.\\n\\nSanghwan Bae, Donghyun Kwak, Sungdong Kim, Donghoon Ham, Soyoung Kang, Sang-Woo Lee, and Woomyoung Park. 2022. Building a role specified open-domain dialogue system leveraging large-scale language models. arXiv preprint arXiv:2205.00176.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec...\"}"}
{"id": "emnlp-2022-main-733", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-733", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics.\\n\\nStephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc.\\n\\nAnanya B. Sai, Akash Kumar Mohankumar, Siddhartha Arora, and Mitesh M. Khapra. 2020. Improving dialog evaluation with a multi-reference adversarial dataset and large scale pretraining. Transactions of the Association for Computational Linguistics, 8:810\u2013827.\\n\\nTimo Schick and Hinrich Sch\u00fctze. 2021. Generating datasets with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6943\u20136951, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nMegha Srivastava, Tatsunori Hashimoto, and Percy Liang. 2020. Robustness to spurious correlations via human annotations. In International Conference on Machine Learning, pages 9109\u20139119. PMLR.\\n\\nShuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. 2021. Want to reduce labeling cost? GPT-3 can help. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4195\u20134205, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nJason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6382\u20136388, Hong Kong, China. Association for Computational Linguistics.\\n\\nTaesun Whang, Dongyub Lee, Dongsuk Oh, Chanhee Lee, Kijong Han, Dong-hun Lee, and Saebyeok Lee. 2021. Do response selection models really know what's next? utterance manipulation strategies for multi-turn response selection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14041\u201314049.\\n\\nThomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. 2018. Transfertransfo: A transfer learning approach for neural network based conversational agents. In NeurIPS Workshop on Conversational AI.\\n\\nYu Wu, Wei Wu, Zhoujun Li, and Ming Zhou. 2018. Learning matching models with weak supervision for response selection in retrieval-based chatbots. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 420\u2013425, Melbourne, Australia. Association for Computational Linguistics.\\n\\nYu Wu, Wei Wu, Chen Xing, Can Xu, Zhoujun Li, and Ming Zhou. 2019. A sequential matching framework for multi-turn response selection in retrieval-based chatbots. Computational Linguistics, 45(1):163\u2013197.\\n\\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, and Woomyoung Park. 2021. GPT3Mix: Leveraging large-scale language models for text augmentation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2225\u20132239, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nChunyuan Yuan, Wei Zhou, Mingming Li, Shangwen Lv, Fuqing Zhu, Jizhong Han, and Songlin Hu. 2019. Multi-hop selector network for multi-turn response selection in retrieval-based chatbots. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 111\u2013120, Hong Kong, China. Association for Computational Linguistics.\\n\\nHuangzhao Zhang, Hao Zhou, Ning Miao, and Lei Li. 2019. Generating fluent adversarial examples for natural languages. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5564\u20135569, Florence, Italy. Association for Computational Linguistics.\\n\\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2204\u20132213.\\n\\nKun Zhou, Beichen Zhang, Xin Zhao, and Ji-Rong Wen. 2022. Debiased contrastive learning of unsupervised sentence representations. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6120\u20136130, Dublin, Ireland. Association for Computational Linguistics.\\n\\nXiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu, and Hua Wu. 2018. Multi-turn response selection for chatbots with deep attention matching network. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1118\u20131127, Melbourne, Australia. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-733", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Ablation study on the number of examples $k$ in the prompts of our method. ($k = 0, 1, \\\\text{and} 2$)\\n\\n| $k$ | Test Set Mean |\\n|-----|---------------|\\n|     | Random Adv Rand + Adv |\\n| 0   | 0.799 0.841 0.820 |\\n| 1   | 0.856 0.893 0.875 |\\n| 2   | 0.859 0.928 0.894 |\\n\\nB.1 Number of Examples ($k$)\\nWe analyze the effect of the number of examples $k$ in the prompts of our method on the response selection model. The results are in Table 4. Our method has the highest performance when using two examples ($k=2$), but using one example ($k=1$) can also be a reasonable alternative. The performance of prompts without examples ($k=0$) is rapidly degraded due to frequent occurrence or false-negative generation. These results show that it is important to provide an adequate number of examples to minimize the occurrence of false-negative responses.\\n\\nWe also measured the Jaccard similarity and length correlation between generated responses and each example in the prompt to qualitatively analyze the effect of the example on the generated responses. As shown in Table 5, the Jaccard similarity and length correlation coefficient are measured higher when $k=1$ than when $k=2$, and the generated responses are more affected by the closer example. Such contamination effect can increase the effectiveness of the in-context example as guidance of the task, but it can also limit the diversity.\\n\\nB.2 Task Instruction Type ($I$)\\nWe compare the performance of $P_{\\\\text{NEG}}$ according to changes in the task instruction. We design the following three types: (1) direct instruction ($I_{\\\\text{dir}}$), (2) direct instruction with a positive response ($I_{\\\\text{pos}}$), and (3) implicit instruction ($I_{\\\\text{imp}}$). We expect that $I_{\\\\text{pos}}$ can generate more challenging negatives by referring to the positive response, and $I_{\\\\text{imp}}$ can generate diverse responses due to the reduced constraints in the prompt. As shown in Table 6, $I_{\\\\text{pos}}$ shows lower performance than $I_{\\\\text{dir}}$ in the random test dataset. Since $I_{\\\\text{imp}}$ is vulnerable to false-negative generation, it has the lowest performance in both random and adversarial test datasets.\"}"}
{"id": "emnlp-2022-main-733", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Approach       | Pred. Score | Similarity |\\n|---------------|-------------|------------|\\n| Random        | \u22122.749      | 2.48       |\\n| Semi-hard     | \u22122.051      | 2.83       |\\n| Mask-and-fill | \u22121.925      | 3.26       |\\n| Key-sem       | \u22121.956      | 3.34       |\\n| NEG (Ours)    | \u22120.598      | 3.53       |\\n| Human         | \u22120.279      | 3.13       |\\n\\nTable 7: Automatic evaluation results for response quality. Pred. Score and Similarity indicate the predicted score of each response by selection model and the similarity score between each response and the context measured by Sentence-BERT, respectively. The mean and standard deviation of each score are reported in the mean std. format.\\n\\n| Approach       | Rand neg | Adv Rand | Rand + Adv Rand |\\n|---------------|----------|----------|-----------------|\\n| Mask-and-fill | 56.6%    |          |                 |\\n| P NEG (Ours)  | 43.6%    |          |                 |\\n| Human         | 47.4%    |          |                 |\\n\\nTable 8: Human evaluation results to verify the quality of synthetic adversarial negative responses.\\n\\nAutomatic Evaluation Results\\nTable 7 shows statistics on the scores of each model for automatic evaluation in \u00a74.2. For the evaluation, we first divide the training dataset of DailyDialog++ by 8:2 and use it as a training and test dataset, respectively. Then we train the selection model using BERT with randomly sampled negatives. For the context-response similarity model, we use a pre-trained Sentence-BERT. Among the negative responses, human-written responses and our responses usually get the high predictive score than other negative responses. In terms of similarity score, our negative responses show high similarity with dialogue contexts. We speculate that the higher similarity of our responses with the dialogue contexts can improve the robustness of response selection models by encouraging them to learn the features beyond superficial context-response similarity.\\n\\nHuman Evaluation\\nFor human evaluation, we randomly sampled 100 data consisting of a dialogue context and 5 negative responses from three different method (Mask-and-fill, P NEG, and Human). Each response is evaluated by three human annotators. We recruited a total of 9 human annotators (6 males and 3 females) for the human evaluation. Human annotators classify the type of each negative response as random, hard, false negative according to the review criteria described in the DailyDialog++. The type of each data is basically determined by a majority, and if the evaluation result is a tie, such data is determined to be a random negative type. Table 8 shows the human evaluation results. Our P NEG has a slightly higher false negative ratio than Mask-and-fill, but shows the highest hard negative ratio. In future work, we may consider soft labeling (Wu et al., 2018; Chen et al., 2020) or label smoothing (M\u00fcller et al., 2019) techniques to alleviate this problem.\\n\\nData Augmentation\\nWe conduct data augmentation experiments by synthesizing adversarial negative responses to the additional datasets. For the experiment, we use the dialogue contexts in the original DailyDialog dataset that has no duplication with the contexts in DailyDialog++. The results are shown in Table 9. Data augmentation using our method generally leads to improved performance. However, if the training dataset is already large enough, the model can properly generalize it (Wei and Zou, 2019). In our experiments, the performance of the selection model is saturated, if the dataset is augmented by more than 10,000 (about 100%).\\n\\nFrequent Error Types in GPT-3 Generation\\nDuring our experiments, we often observed the weird generation results of GPT-3. The frequent error types in generated results of GPT-3 can be roughly categorized as follows: (1) n-gram or word repetition, (2) containing too many \"__\" or \"_ _\", (3) out of numbering rules. We generate negative responses with GPT-3 for the given context until there is no error response that is aforementioned. Note that false negative is a semantic error type that needs to be evaluated by humans.\"}"}
{"id": "emnlp-2022-main-733", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The PNEG prompt is as follows:\\n\\n###\\n\\nDialogue context:\\n```\\nA: How about taking the damaged portion at a lower price?\\nB: What kind of price did you want?\\nA: I was thinking of 30% off.\\n```\\n\\nCreate five irrelevant responses containing keywords of the given dialogue context:\\n\\n1. I have not completed the portions of the children, ...\\n2. Shall I inquire about the price of the plane tickets ...\\n3. I have been thinking up new ways of supplying money ...\\n4. My car roof was not damaged in the accident.\\n5. I purchased a different kind of dress in the shopping mall ...\\n\\n###\\n\\nDialogue context:\\n```\\nA: No, but that was a random change of subject.\\nB: It may have been random, but have you?\\nA: I haven't lately.\\n```\\n\\nCreate five irrelevant responses containing keywords of the given dialogue context:\\n\\n1. Yeah, Our society is annoying. They keep on changing ...\\n2. I am not sure which subject I am going to take. Lately, ...\\n3. I don't know that day Prof. Lesley was randomly picking up ...\\n4. Today In college some random guy came and started talking ...\\n5. Have you seen Tina lately? I am feeling weird as ...\\n\\n###\\n\\nDialogue context:\\n```\\nA: Paul, a company called me for an interview.\\nB: That's great! You need to prepare for it.\\nA: How?\\n```\\n\\nCreate five irrelevant responses containing keywords of the given dialogue context:\\n\\n1. Then, GPT-3 generate following negative responses:\\n   I don't know how to handle an interview.\\n2. How could I prepare for the interview?\\n3. To be honest man, I don't know what to say.\\n4. You are probably thinking that I could not do well in the interview, right?\\n5. Did you remember to register for the interview?\\n\\nThis prompt is particularly sensitive to the task instruction. For example, GPT-3 generates emotionally negative responses when the word \\\"irrelevant\\\" in the task description is changed to \\\"negative.\\\" In addition, if we remove \\\"containing keywords of -\\\" from the task instruction, the generation quality of GPT-3 is degraded.\"}"}
{"id": "emnlp-2022-main-733", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A: I am sorry to tell you that you failed in the job interview in our company.\\n\\nContext B: It is tough to accept it, I mean. But can you tell me why?\\n\\nA: It is hard to tell. Maybe it is because you're not resourceful enough to be a manager.\\n\\nWell what can I say! I am speechless, anyways thanks for calling.\\n\\nThe interview was quite good, I answered everything I was asked for, and so what seems to be...\\n\\nPositive\\n\\nWell I was confident! It is really shocking that I couldn't clear it.\\n\\nBut why! What is the matter? My performance was good.\\n\\nAnyways never mind, I will keep on trying, thank you for your information.\\n\\n1. Yogurt can be incredibly healthy. Unfortunately, most yogurts found in the grocery store are...\\n2. Well! I was really scared; maybe I need few more lessons.\\n3. I too don't have any idea what they mean.\\n4. Okay, you can have a day off.\\n5. Well! Again it depends on family to family as I said to you earlier.\\n\\n1. It is tough to accept it, i mean. But can you tell me why?\\n2. I am indeed. What can you tell me?\\n\\nBM25\\n\\n3. I am sorry, sir. Can you tell me your room number?\\n4. Can you tell me why you replied to our advertisement?\\n5. What's the matter? Please tell me. Maybe i can help you.\\n\\n1. It's hard to say, but it sure was. You did a good job as a probationary employee, didn't you?\\n2. No problem. The company will give me the job interview.\\n\\nKey-sem\\n\\n3. No, that's better. I think we'd better do that at the least convenient time.\\n4. I don't know exactly. I've never had any difficulty in starting a business. Why did you ask me to come in to talk about your business?\\n5. Sorry, i'm not very resourceful enough to go there.\\n\\n1. Are you sorry for yourself for not being resourceful?\\n2. It is tough to accept that I should attend the meeting without having lunch in the afternoon.\\n\\nP NEG\\n\\n3. I have tried to keep track of cross-cultural communication skills through the internet.\\n4. You are so resourceful that you can work with people from all over the world.\\n5. My partner has no resources to go over the financial reports. I am confused why she wants to be a part of this company again.\\n\\n1. Are you sorry for yourself for not being resourceful?\\n2. It is tough to accept that I should attend the meeting without having lunch in the afternoon.\\n\\nHuman\\n\\n3. Tomorrow there is a job interview in Titan company.\\n4. She marched into the office and demanded to speak to the manager but she failed to talk.\\n5. They were resourceful and talented people in our school.\"}"}
{"id": "emnlp-2022-main-733", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A: I am sorry to tell you that you failed in the job interview in our company.\\n\\nContext B: It is tough to accept it, I mean. But can you tell me why?\\n\\nA: It is hard to tell. Maybe it is because you're not resourceful enough to be a manager.\\n\\n1. If you spend most of your time on the phone, then you are certainly not unemployed.\\n\\n2. It is hard to tell the difference between our company's products and those of my friend's company.\\n\\n3. I have tried to keep track of cross-cultural communication skills through the internet.\\n\\n4. You are so resourceful that you can work with people from all over the world.\\n\\n5. My partner has no resources to go over the financial reports. I am confused why she wants to be a part of this company again.\\n\\nE = 100%\\n\\nVarying E with {10%, 1%, 0.1%, and 0.1%}, k = 2, I = Ipneg\\n\\n1. We must consider the ways to reduce our expenditure.\\n\\n2. We simply have to make more of an effort if we are going to succeed in the future.\\n\\n3. I already know that I want that job - what is your final offer?\\n\\n4. I am scared that our luck has run out and we will never win again.\\n\\n5. This is my personal opinion .\\n\\n1. I don't know how to thank you for the kind of lesson I received from you.\\n\\n2. My sister would like to study in your school next year, but she doesn't want to leave the place.\\n\\nE = 1% \\n\\n3. Our new teacher is rather young and pretty. What a pity that she's married.\\n\\n4. We will go on a trip this Saturday as my parent promised me.\\n\\n5. I am going on a date with my boyfriend tonight, but it doesn't seem too good as he told me that he would ask for a break up tomorrow night.\\n\\nE = 0.1%\\n\\n1. I am the best and I know it. You can't beat me.\\n\\n2. I got a very good score in my English class. What's the big deal?\\n\\n3. I will be back to take revenge.\\n\\n4. The interview was held in the manager's room, who is currently out of town for a business trip.\\n\\n5. Tough to accept it, but I'm here to accept it with an open heart and welcome the next round\\n\\n1. Please give me a raise in salary, please.\\n\\n2. I think you're an interesting manager.\\n\\nE = 0.1%\\n\\n3. Here is the money I owe you.\\n\\n4. I really appreciate your kindness.\\n\\n5. Your answer is totally off my expectation! That's so amazing.\\n\\nVarying k with {0, 1}, E = 100%, I = Ipneg\\n\\n1. Would you like to drink a cup of coffee together?\\n\\n2. I learned the recipe for a new dish yesterday and would like to try making it.\\n\\nk = 1\\n\\n3. Would you please give me some money now?\\n\\n4. You look tired; I'll take your job and do it myself then.\\n\\n5. Do you think you could go shopping with me later? I want to know what kind of houses are ...\\n\\n1. I don't know why I failed to win the race. Maybe it is because my legs were too short. \\n\\n2. I am sorry to tell you that you failed in the job interview in our company.\\n\\n3. Well! I am stunned. That's it? Thanks for telling me at least a reason.\\n\\n4. Sir, I am sorry to tell you that you failed in the job interview in our company.\\n\\n5. It is hard to tell . Maybe it is because you're not resourceful enough to be a manager.\\n\\nVarying (I, k) with {(Ipos, 2), (Imp, 0)}, E = 100%\\n\\n1. Thank God, otherwise it would have been worse.\\n\\n2. What? I am not able to listen because the person next to me is shouting.\\n\\nk = 2\\n\\n3. Oh no! can I reduce my cell phone bill by getting to hear this news?\\n\\nIpos\\n\\n4. I need the electric blanket in my room to survive this winter season.\\n\\n5. MHe's probably referring to the lack of managerial skills that I have established till now.\\n\\n1. I want you to think again.\\n\\n2. I already have the new pair of glasses.\\n\\nk = 0\\n\\n3. I used to be a soccer player when I was in high school.\\n\\nImp\\n\\n4. What a coincidence! How many first-class stamps do you have on hand?\\n\\n5. I have never been encouraged enough to have a good start.\"}"}
