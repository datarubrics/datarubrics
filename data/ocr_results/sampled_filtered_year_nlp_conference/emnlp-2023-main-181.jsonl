{"id": "emnlp-2023-main-181", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Overview of new operators (change, add and remove) added to the DSL.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The object discriminator is a neural network with input dimension as 256 and a single 300 dimensional hidden layer with ReLU activation function. This discriminator is trained using the standard GAN objective $\\\\ell_{\\\\text{objGAN}}$. Note, $\\\\ell_{\\\\text{objGAN}}$ has 2 parts \u2013 i) the loss for the generated (fake) object embedding using the add network, and ii) the loss for the real objects (all the unchanged object embeddings of the image). The former is unscaled but the latter one is scaled by a factor of $1/(\\\\text{num_objects})$.\\n\\nThe edge discriminator is a neural network with input dimension as $(256 \\\\times 3)$ and a single 300 dimensional hidden layer with ReLU activation function. As input to this discriminator network, we pass the concatenation of the two objects and the edge connecting them. This discriminator is trained using the standard GAN objective $\\\\ell_{\\\\text{edgeGAN}}$. See Fig 5b for an overview of the add operator.\\n\\n### Additional Results\\n\\n#### D.1 Detailed Performance for Zero-Shot Generalization on Larger Scenes\\n\\nTable 11 below is a detailed version of the Table 3 in the main paper. This table compares the performance of N\\\\_EUROSIM with baseline methods TIM-GAN, GeNeVA and IP2P for the zero-shot generalization to larger scenes (with $\\\\geq 10$ objects), while the models were trained on images with $3-8$ objects. Relative to the main paper's table 3, this table offers separate performance numbers for each of the add, remove and change instructions.\\n\\n#### D.2 Image Retrieval Task\\n\\nA task that is closely related to the image manipulation task is the task of Text Guided Image Retrieval, proposed by (Vo et al., 2019). Through this experiment, our is to demonstrate that N\\\\_EUROSIM is highly effective in solving this task as well. In what follows, we provide details about this task, baselines, evaluation metric, how we adapted N\\\\_EUROSIM for this task, and finally performance results in Table 12. This table is a detailed version of the Table 4 in the main paper.\\n\\n**Task Definition:**\\nGiven an image $I$, a text instruction $T$, and a database of images $D$, the task is to retrieve an image from the database that is semantically as close to the ground truth manipulated image as possible.\\n\\nNote, for each such $(I, T)$ pair, some image from the database, say $\\\\tilde{I} \\\\in D$, is assumed to be the ideal image that should ideally be retrieved at rank-1. This, so called desired gold retrieval image might even be an image which is the ideal manipulated version of the original images $I$ in terms of satisfying the instruction $T$ perfectly. Or, image $\\\\tilde{I}$ may not be such an ideal manipulated image but it still may be the image in whole corpus $D$ that comes closest to the ideal manipulated image.\\n\\nIn practice, while measuring the performance of any such system for this task, the gold manipulated image for $(I, T)$ pair is typically inserted into the database $D$ and such an image then serves as the desired gold retrieval image $\\\\tilde{I}$.\\n\\n**Baselines:**\\nOur baselines includes popular supervised learning systems designed for this task. The first baseline is TIRG proposed by Vo et al. (2019) where they combine image and text to get a joint embedding and train their model in a supervised manner using embedding of the desired retrieved image as supervision. For completeness, we also include comparison with other baselines \u2013 Concat, Image-Only, and Text-Only \u2013 that were introduced by Vo et al. (2019).\\n\\nA recent model proposed by Chen et al. (2020) uses symbolic scene graphs (instead of embeddings) to retrieve images from the database. Motivated by this, we also retrieve images via the scene graph that is generated by the manipulation module of N\\\\_EUROSIM. However, unlike Chen et al. (2020), the nodes and edges in our scene graph have associated vectors and make a novel use of them while retrieving. We do not compare our performance with (Chen et al., 2020) since its code is unavailable and we haven't been able to reproduce their numbers on datasets used in their paper. Moreover, (Chen et al., 2020) uses full supervision of the desired output image (which is converted to a symbolic scene graph), while we do not.\\n\\n**Evaluation Metric:**\\nWe use Recall@$k$ (and report results for $k=1, 3$) for evaluating the performance of text guided image retrieval algorithms which is standard in the literature.\\n\\n**Retrieval using Scene Graphs:**\\nWe use the scene graph generated by N\\\\_EUROSIM as the latent representation to retrieve images from the database. We introduce a novel yet simple method to retrieve images using scene graph representation. For converting an image into the scene graph, we use the vi-\"}"}
{"id": "emnlp-2023-main-181", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Detailed performance scores for N\\\\textsc{EuroSim}, TIM-GAN, GeNeV A and IP2P for zero-shot generalization to larger scenes (with $\\\\geq 10$ objects) from CIM-NLI-LARGE dataset, while models are trained on images with 3\u22128 objects. Table has separate performance numbers for add, remove, and change instructions. Along with each method, we have also written the number of data points from CIM-NLI dataset that were used for training. \\n\\nR\\\\textsubscript{1} and R\\\\textsubscript{3} correspond to Recall\\\\textsubscript{@1} and Recall\\\\textsubscript{@3}, respectively.\\n\\nGiven the scene graph $G$ for the input image $I$ and the manipulation instruction text $T$, N\\\\textsc{EuroSim} converts the scene graph into the changed scene graph $G_{\\\\tilde{I}}$, as described in Section C in Appendix. Now, we use this graph $G_{\\\\tilde{I}}$ as a query to retrieve images from the database $D$. For retrieval, we use the novel graph edit distance (GED) between $G_{\\\\tilde{I}}$ and the scene graph representation of the database images. The scene graph for each database image is also obtained using the visual representation network of N\\\\textsc{EuroSim}. The graph edit distance is given below.\\n\\n$$\\\\text{GED}(G_{\\\\tilde{I}}, G_D) = \\\\begin{cases} \\\\infty & |N_{\\\\tilde{I}}| \\\\neq |N_D| \\\\\\\\ \\\\min_{\\\\pi \\\\in \\\\Pi} \\\\sum_{\\\\forall i \\\\in \\\\{1, \\\\cdots, |N_{\\\\tilde{I}}|\\\\}} c(n_i, y_i) & \\\\text{otherwise.} \\\\end{cases}$$\\n\\nwhere, $G_{\\\\tilde{I}} = (N_{\\\\tilde{I}}, V_{\\\\tilde{I}})$ and $G_D = (N_D, V_D)$. $n_i$ and $y_i$ are the node embeddings of the query graph $G_{\\\\tilde{I}}$ and scene graph $G_D$ of an image from the database. $c(a, b)$ is the cosine similarities between embeddings $a$ and $b$. This GED is much simpler than that defined in (Chen et al., 2020), since it does not need any hand designed cost for change, removal, or addition of nodes, or different attribute values. It can simply rely on the cosine similarities between node embeddings. We use the Hungarian algorithm (Kuhn, 1955) for calculating the optimal matching $\\\\pi$ of the nodes, among all possible matching $\\\\Pi$. We use the negative of the cosine similarity scores between nodes to create the cost matrix for the Hungarian algorithm to process. This simple yet highly effective approach (See Table 4 in the main paper and Table 12 in the appendix), can be improved by more sophisticated techniques that include distance between edge embeddings and including notion of subgraphs in the GED. We leave this as future work. This result shows that our manipulation network edits the scene graph in a desirable manner, as per the input instruction. \\n\\nD.3 Detailed Multi-hop Reasoning\\n\\nTable 14 below provides a detailed split of the performance numbers reported in Table 3 of the main paper across i) number of hops ($0\u22123$ hops) and ii) type of instructions (add/remove/change). We observe that for change and remove instructions, N\\\\textsc{EuroSim} improves over TIM-GAN, GeNeV A and IP2P trained on 5.4K CIM-NLI data points by a significant margin ($\\\\sim 20\\\\%$ on 3-hop change/remove instructions). However, N\\\\textsc{EuroSim} lags behind TIM-GAN when the entire CIM-NLI labeled data is used to train TIM-GAN. We also observe that all the models perform poorly on the add instructions, as compared to change and remove instructions. \\n\\nD.4 Detailed Performance for Different Cost Ratios $\\\\beta$\\n\\nTable 2 in Section 4 of the main paper showed the performance of N\\\\textsc{EuroSim} compared with TIM-GAN and GeNeV A for various values of $\\\\beta$, where\"}"}
{"id": "emnlp-2023-main-181", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Performance scores (Recall@1) on the Image Retrieval task, comparing N\\\\textsc{EuroSim} with TIM-GAN and GeNeVA with increase in reasoning hops, for add, remove, and change instructions. Along with each method, number of data points from CIM-NLI used for training are written.\\n\\n$\\\\beta$ is the ratio of the number of annotated (with output image supervision) image manipulation examples required by the supervised baselines, to the number of annotated VQA examples required to train N\\\\textsc{EuroSim}. In Table 13, we show a detailed split of the performance, for the add, change, and remove operators, across the same values of $\\\\beta$ as taken before.\\n\\nWe find that for the change operator, N\\\\textsc{EuroSim} performs better than TIM-GAN by a margin of $\\\\sim 8\\\\%$ (considering Recall@1) for $\\\\beta \\\\leq 0.1$. For the remove operator, N\\\\textsc{EuroSim} performs better than TIM-GAN by a margin of $\\\\sim 4\\\\%$ (considering Recall@1) for $\\\\beta \\\\leq 0.2$. Overall, N\\\\textsc{EuroSim} performs similar to TIM-GAN, for $\\\\beta = 0.2$, for remove and change operators. All models perform poorly on the add operator as compared to the change and remove operators. We find that having full output image supervision allows TIM-GAN to reconstruct (copy) the unchanged objects from the input to the output for all the operators. This results in a higher recall in general but its effect is most pronounced in the Recall@3. N\\\\textsc{EuroSim}, on the other hand, suffers from rendering errors which makes the overall recall score (especially Recall@3) lower. We believe that improving image rendering quality would significantly improve the performance of N\\\\textsc{EuroSim} and we leave this as future work.\\n\\nD.5 Results on Datasets from different domains\\n\\nD.5.1 Minecraft Dataset\\n\\nDataset Creation: We create a new dataset having (Image, instruction) by building over the Minecraft dataset used in (Yi et al., 2018). Specifically, we create zero and one hop remove instructions and one hop add instructions similar to the creation of CIM-NLI. This dataset contains scenes and objects from the Minecraft video game and is used in prior works for testing Neuro-Symbolic VQA systems like NSCL (Mao et al., 2019) and NS-VQA (Yi et al., 2018). The setting of the Minecraft worlds dataset is significantly different from CLEVR in terms of concepts and attributes of objects and visual appearance.\\n\\nExperiment: We use the above dataset for testing the addition and removal of objects using NeuroSIM (See Fig 6). We train NeuroSIM\u2019s decoder to generate images from scene graphs of the minecraft dataset. We assume access to a parser that gives us programs for an instruction. For removal, we use the same remove network as described above, while for addition, we assume access to the features of object to be added, which is added to the scene graph of the image and the decoder decodes the final image. See Figure 6 for a set of successful examples on the Minecraft dataset. We see that using our method, one can add and remove objects from the scene successfully, without using any output image as supervision during training. Though we have assumed the availability of a parser in the above set-up, training it jointly with other modules should be straightforward, and can be achieved using our general approach described in Section 3 of the main paper.\\n\\nE End-to-end Training\\n\\nThe main objective of this work is to make use of weakly supervised VQA data for the image manipu...\"}"}
{"id": "emnlp-2023-main-181", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Given the significant increase in performance of N\\\\textsuperscript{EURO}SIM when using supervised data, we also test its generalization capability (Analogous to Section 4.2, 4.3), and quality of scene graph retrieval (Analogous to Section 4.5). From Table 16, we see that N\\\\textsuperscript{EURO}SIM (e2e) shows improved zero-shot generalization to larger scenes. Even when trained on just 5.4k CIM-NLI data, N\\\\textsuperscript{EURO}SIM (e2e) improves over TIM-GAN-54k by 3.9 R@1 points. A 5.3 point improvement over TIM-GAN is observed when full CIM-NLI data is used.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 14: Performance scores (Recall@1) for N\\\\textsubscript{EURO}SIM with TIM-GAN, GeNeV A and IP2P with increase in reasoning hops, for add, remove, and change instructions. Along with each method, number of data points from CIM-NLI used for training are written.\\n\\nFigure 6: Results for addition and removal of objects from images of the minecraft dataset.\\n\\nSupervised training significantly improves the scene graph quality, thus improving retrieval performance. Supervised training improves retrieval by 7.3 R@1 points over weakly supervised N\\\\textsubscript{EURO}SIM baseline. These findings suggest that N\\\\textsubscript{EURO}SIM(e2e) significantly outperforms other supervised approaches in almost all settings. One can fine-tune the image decoder and the visual representation network to further enhance the findings, which should greatly enhance the outcomes.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 15: Performance comparison of N\\\\_EURO\\\\_SIM(e2e) with baselines using Recall@1. N\\\\_EURO\\\\_SIM(e2e) refers to N\\\\_EURO\\\\_SIM trained end-to-end by utilizing ground truth manipulated images as the supervision for N\\\\_EURO\\\\_SIM modules.\\n\\n| Model          | Train Data Size | R1  | R3  |\\n|----------------|-----------------|-----|-----|\\n| TIM-GAN        | 5.4K            | 30.2| 80.7|\\n| N\\\\_EURO\\\\_SIM  | 5.4K            | 63.7| 89.1|\\n| N\\\\_EURO\\\\_SIM(e2e) | 5.4K         | 70.2| 92.6|\\n| N\\\\_EURO\\\\_SIM(e2e) | 54K        | 71.6| 91.7|\\n\\n### Table 16: Zero-shot generalization to larger scenes (Extension of Table 3 of main paper).\\n\\n| Model          | Train Data Size | Acc  |\\n|----------------|-----------------|------|\\n| TIM-GAN        | 5.4K            | 80.7 |\\n| N\\\\_EURO\\\\_SIM  | 5.4K            | 89.1 |\\n| N\\\\_EURO\\\\_SIM(e2e) | 5.4K         | 92.6 |\\n| N\\\\_EURO\\\\_SIM(e2e) | 54K        | 91.7 |\\n\\nWe also tested the semantic parsing ability of Large Language Models (LLMs), specifically GPT-4 for our task. The task of semantic parsing is given manipulation instruction text in natural language, generated the symbolic program by parsing the input text. To provide GPT-4 with context, we designed an extensive prompt that begins with our DSL followed by six different in-context examples representing various instruction types for few-shot learning. This prompt is then followed with the instruction text that we want to parse. We tested GPT-4 on a randomly sampled subset of our test dataset. For evaluation, we measured the accuracy of semantic parsing using an exact match between the generated symbolic program and the ground-truth symbolic program.\\n\\nThe detailed results are given in Table 19. Interestingly, we observed that GPT-4 performed poorly on Add instructions, achieving less than 10% parsing accuracy. To address this, we prompted GPT-4 separately with additional few-shot examples for Add instructions, which led to the results displayed in the table. Even with the additional examples, GPT-4's performance did not significantly improve.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 17: Performance with increasing reasoning hops (Extension of Table 3 of main paper).\\n\\n| Model               | R1 | R3 |\\n|---------------------|----|----|\\n| Text-Only           | 0.2| 0.4|\\n| Image-Only          | 34.1| 83.6|\\n| Concat              | 39.5| 86.9|\\n| TIRG                | 34.8| 84.6|\\n| N EURO SIM          | 85.8| 92.9|\\n| N EURO SIM(e2e)     | 93.1| 96.7|\\n\\nTable 18: Quality of scene graph measured via retrieval (Extension of Table 4 of main paper).\\n\\nInstruction type | Hops | Total  |\\n-----------------|------|--------|\\n| Add             | 18.9 | 30.5   | 37.5 | 29.5 |\\n| Remove          | 91.5 | 85.1   | 80.0 | 84.9 | 85.6 |\\n| Change          | 70.7 | 81.7   | 76.2 | 70.2 | 74.6 |\\n| Total           | 81.5 | 55.6   | 55.9 | 58.1 | 60.5 |\\n\\nTable 19: Few-shot parsing results of GPT-4 guidance. Add instructions remained significantly lower in accuracy compared to other instruction types. This analysis demonstrates that our reinforcement learning-based instruction parser outperforms GPT-4, at least on this dataset. It also highlights the need for more careful prompt engineering before LLMs like GPT-4 can be readily applied in our specific setting.\\n\\nG Computational Resources\\n\\nWe trained all our models and baselines on 1 Nvidia Volta V100 GPU with 32GB memory and 512GB system RAM except IP2P which was trained on 8-A100 80 GB GPUs. Our image decoder training takes about 4 days of training time. Training of the VQA task takes 5\u22127 days of training time and training the Manipulation networks take 4\u22125 hours of training time.\\n\\nH Hyperparameters and Validation\\n\\nH.1 Training for VQA Task\\n\\nThe hyperparameters for the VQA task are kept same as default values coming from the prior work (Mao et al., 2019). We refer the readers to (Mao et al., 2019) for more details. We obtained a question answering accuracy of 99.3% after training on the VQA task.\\n\\nH.2 Training Semantic Parser\\n\\nThe semantic parser is trained to parse instructions. Learning of this module happens using the REINFORCE algorithm as described in Section C of this appendix. During REINFORCE algorithm, we search for positive rewards from the set {7, 8, 10}, and negative rewards from the set {0, 2, 3}. We finally choose a positive reward of 8 and negative reward of 2. For making this decision, we first train the semantic parser for 20 epochs and then calculate its accuracy by running it on the quantized scenes from the validation set. For a particular output program, we say it is correct if it leads to an object being selected (see Section C of the appendix for more information) and this is how the accuracy of the semantic parser is calculated.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"accuracy is a proxy for the real accuracy. An alternative is to use annotated ground truth programs for calculating accuracy and then selecting hyperparameters. However, we do not use ground truth programs. All other hyperparameters are kept the same as used by (Mao et al., 2019) to train the parser on VQA task. We obtain a validation accuracy of 95.64% after training the semantic parser for manipulation instructions.\\n\\nH.3 Training Manipulation Networks\\nThe architecture details of the manipulation network are present in Section C of this appendix. We use batch size of 32, learning rate of $10^{-3}$, and optimize using AdamW (Loshchilov and Hutter, 2019) with weight decay of $10^{-4}$. Rest of the hyperparameters are kept the same as used in (Mao et al., 2019). During training, at every 5th epochs, we calculate the manipulation accuracy by using the query networks that were trained while training the N-EURO SIM on VQA data. This serves as a proxy to the validation accuracy.\\n\\n- For the change network training, we use the query accuracy of whether the attribute that was supposed to change for a particular object, has changed correctly or not. Also, whether any other attribute has changed or not.\\n- For the add network training, we use the query accuracy of whether the attributes of the added object are correct or not. Also, whether the added object is in a correct relation with reference object or not.\\n\\nWe obtained a validation accuracy (based on query-ing) of 95.9% for the add network and an accuracy of 99.1% for the change network.\\n\\nH.4 Image Decoder Training\\nThe architecture of the image decoder is similar to (Johnson et al., 2018) but our input scene graph (having embeddings for nodes and edges) is directly processed by the graph neural network. We use a batch size of 16, learning rate of $10^{-5}$, and optimize using Adam (Kingma and Ba, 2015) optimizer. The rest of the hyperparameters are same as (Johnson et al., 2018). We train the image decoder for a fixed set of 1000K iterations.\\n\\nI Qualitative Analysis\\nFigures 7, 8, 9 compare the images generated by N-EURO SIM, TIM-GAN, and GeNeVA on add, change and remove instructions respectively. N-EURO SIM\u2019s advantage lies in semantic correctness of manipulated images. For example, see Figure 7 row #3,4; Figure 8 row #2; 9 all images. In these images, N-EURO SIM was able to achieve semantically correct changes, while TIM-GAN, GeNeVA faced problems like blurry, smudged objects while adding them to the scene, removing incorrect objects from the scene, or not changing/partially changing the object to be changed. Images generated by TIM-GAN are better in quality as compared to N-EURO SIM. We believe the reason for this is that TIM-GAN, being fully supervised, only changes a small portion of the image and has learned to copy a significant portion of the input image directly to the output. However, this doesn\u2019t ensure the semantic correctness of TIM-GAN\u2019s manipulation, as described above with examples where it makes errors. The images generated by N-EURO SIM look slightly worse since the entire image is generated from object based embeddings in the scene graph. Improving neural image rendering from scene graphs can be a promising step to improve N-EURO SIM.\\n\\nJ Errors\\nFigure 10 captures the images generated by our model where it has made errors. The kind of errors that N-EURO SIM makes can be broadly classified into three categories.\\n\\n- **[Rendering Errors]** This set includes images generated by our model which are semantically correct but suffer from rendering errors. The common rendering errors include malformed cubes, partial cubes, change in position of objects, and different lighting.\\n- **[Logical Errors]** This set includes images generated by our model which have logical errors. That is, manipulation instruction has been interpreted incorrectly and a different manipulation has been performed. This happens mainly due to an incorrect parse of the input instruction into the program, or manipulation network not trained to perfection. For example, change network changing attributes which were supposed to remain unchanged.\\n- **[VQA Errors]** The query networks are not ideal and have errors after they are trained on the VQA task. This in turn causes errors in supervision (obtained from query networks) while training the model.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There is a shiny thing that is on the right side of the shiny block, add a big gray metallic ball in front of it.\\n\\nThere is a rubber thing behind the matte thing in front of the tiny rubber object, add a tiny blue shiny sphere behind it.\\n\\nAdd a small gray rubber cylinder that is in front of the big cube.\\n\\nThere is a shiny thing that is on the right side of the shiny block, add a big gray metallic cylinder that is in front of the small rubber object behind the tiny green matte cylinder.\\n\\nThere is a purple shiny object in front of the purple metal ball, add a large red matte ball to the left of it.\\n\\nThere is a rubber thing in front of the red matte ball; change the shape of it to cylinder.\\n\\nChange material of the rubber object in front of the small rubber thing that is left of the tiny gray matte sphere that is in front of the yellow block to shiny.\\n\\nThere is a small matte thing; change the color of it to purple.\\n\\nThere is a cylinder that is behind the small metallic cylinder; change the size of it to tiny.\\n\\nThere is a tiny cylinder that is to the left of the small blue thing to the left of the big green metallic cylinder; change the material of it to matte.\\n\\nFigure 7: Visual comparison of N EURO SIM with TIM-GAN and GeNeVA for the add operator. The red bounding boxes in the ground truth output image indicate the objects required to add to the input image.\\n\\nFigure 8: Visual comparison of N EURO SIM with TIM-GAN and GeNeVA for the change operator. The red bounding boxes in the input and ground truth output image indicate the objects required to be changed.\\n\\nAblations\\n\\nTable 20 shows the performance of N EURO SIM when certain loss terms are removed while learning of the networks. This depicts the importance of loss terms that we have considered. In particular we test the performance of the network by removing loss terms.\\n\\nmanipulation networks and leads to a less than optimally trained manipulation network. Also, during inference, object embeddings may not be perfect due to the imperfections in the visual representation network and that leads to incorrect rendering.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There is a large metal object left of the metallic object that is to the right of the large metallic thing in front of the sphere, remove it.\\n\\nThere is a big sphere in front of the big ball behind the blue thing, remove it.\\n\\nThere is a metallic thing in front of the small gray rubber thing, remove it.\\n\\nThere is a shiny cube, remove it.\\n\\nFigure 9: Visual comparison of NEUROSIM with TIM-GAN and GeNeVA for the remove operator. The red bounding boxes in the input image indicate objects required to be removed.\\n\\nFigure 10: Types of errors in NEUROSIM.\\n\\nL Interpretability of NEUROSIM:\\n\\nNEUROSIM allows for interpretable image manipulation through programs which are generated as an intermediate representation of the input instruction. This is one of the major strengths of NEUROSIM, since it allows humans to detect where NEUROSIM failed. This is not possible with purely neural models, that behave as a black box. Knowing about the\"}"}
{"id": "emnlp-2023-main-181", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Loss | R1 | R3 |\\n---|---|---|\\n\u2113  | 45.3 | 65.5 |\\n\u2113\u2212 | \u2113add | \u2113add |\\nedgeGAN | 43.7 | 66.0 |\\nobjGAN | 44.3 | 60.2 |\\nobjSup | \u2212\u2113add | \u2113add |\\nedgeSup | 44.1 | 57.9 |\\n\\nTable 20: Ablations conducted by removing some loss terms. \u2113 is the total loss before any ablation. For each loss term being removed, the superscript denotes which network it belongs to (add or change). Ablations are conducted for the setting where $\\\\beta = 0.054$ (see main paper Section 4 for the definition of $\\\\beta$).\\n\\nFailure cases of NEUROSIM also mean that it can be selectively trained to improve certain parts of the network (for eg individually training on change instructions to improve the change command, if the model is performing poorly on change instructions). We now assess the correctness of intermediate programs using randomly selected qualitative examples present in Figure 11. Since no wrong program was obtained in the randomly selected set, we find 2 more data points manually, to show some wrong examples.\\n\\nM Human Evaluation Details\\nSee Table 21 for the questions (paraphrased) asked to the evaluators. Detailed instructions and an example of the questions provided to the evaluators can be found in Figure 12. A total of 10 evaluators, consisting of a mix of undergraduate and post-graduate students, were involved in the study. The same set of 30 random images were given to each evaluator. They were compensated at a rate three times the average hourly salary in the country of origin. Each evaluator was given up to 24 hours to complete the task.\\n\\nSimplifying Multi-Hop Instructions using NeuroSIM Modules\\nIn this section, we provide details on our method of utilizing the trained semantic parser to convert the complex multi-hop instruction into a simplified 0 or 1 hop instruction. We generate three simplified templates one for each edit operation.\\n\\n1. Change the [attribute] of [size] [color] [material] [shape] to [attribute']\\n2. Remove the [size] [color] [material] [shape]\\n3. Add a [size] [color] [material] [shape] to the [relation] of [shape']\\n\\nNext, given a multi-hop instruction we parse it using our semantic parser which gives us the object's embedding on which either an operation is to be executed (in case of change and remove operations) or a new object has to be inserted in relation to it (in case of add operation). The trained query-networks predicts the symbolic values of the concepts in the placeholders. Example, if the MH instruction is \\\"Change the size of the big thing that is behind the metallic cylinder behind the purple object that is to the right of the big brown shiny object to tiny\\\", we find the placeholder attributes to be operation=change, attribute=size, color=yellow, shape=cube, size=large, material=rubber, attribute'=tiny. Hence the simplified instruction becomes, \\\"Change the size of the large yellow rubber cube to tiny\\\". Add and Remove instructions follow similarly.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Change the shape of the big gray thing to cube.\\n\\nRemove the gray rubber thing in front of the gray matte sphere behind the large gray matte sphere.\\n\\nRemove the brown metal object that is left of the blue matte block that is left of the brown thing on the right side of the large cyan metal cube.\\n\\nThere is a matte block that is in front of the big gray rubber object; change the material of it to shiny.\\n\\nAdd a tiny purple metal ball that is in front of the blue object that is behind the matte ball.\\n\\nscene() - filter(['rubber', 'sphere']) - relate(['behind']) - filter(['blue']) - add(rel_concept=['front'], concept_set=['small', 'purple', 'metal', 'sphere'])\\n\\nscene() - filter(['large', 'cyan', 'metal', 'cube']) - relate(['right']) - filter(['brown']) - relate(['left']) - filter(['blue', 'rubber', 'cube']) - relate(['left']) - filter(['brown', 'metal']) - remove()\\n\\nscene() - filter(['large', 'gray']) - change(attr=shape, concept=['cube'])\\n\\nscene() - filter(['large', 'gray', 'rubber', 'sphere']) - relate(['behind']) - filter(['gray', 'rubber', 'sphere']) - relate(['front']) - filter(['gray', 'rubber']) - remove()\\n\\nscene() - filter(['large', 'gray']) - relate(['front']) - filter(['rubber', 'cube']) - change(attr=material, concept=['rubber'])\\n\\nscene() - filter(['purple', 'metal']) - relate(['front']) - filter(['large']) - relate(['right']) - filter(['large', 'sphere']) - remove()\\n\\nscene() - filter(['brown', 'metal', 'cube']) - relate(['front']) - filter(['small', 'metal', 'cube']) - change(attr=material, concept=['rubber'])\\n\\nscene() - filter(['small', 'red', 'rubber']) - relate(['front']) - filter(['rubber', 'cylinder']) - relate(['right']) - filter(['cylinder']) - remove()\\n\\nscene() - filter(['small', 'gray']) - relate(['behind']) - filter(['metal', 'cube']) - add(rel_concept=['behind'], concept_set=['large', 'purple', 'metal', 'sphere'])\\n\\nscene() - filter(['small', 'yellow', 'sphere']) - relate(['front']) - filter(['large']) - relate(['right']) - filter(['large', 'sphere']) - remove()\\n\\nscene() - filter(['large', 'brown', 'cube']) - relate(['behind']) - filter(['brown']) - relate(['right']) - relate(['left']) - filter(['brown']) - remove()\"}"}
{"id": "emnlp-2023-main-181", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question 1:\\n[Change] Are all the attributes (color, shape, size, material, and relative position) of the changed object mentioned in the instructions identical between the ground truth image and the system-generated image?\\n\\n[Add] Are all the attributes (color, shape, size, material, and relative position) of the added object mentioned in the instructions identical between the ground truth image and the system-generated image?\\n\\n[Remove] Are same objects removed in ground truth image and the system-generated image?\\n\\nQuestion 2:\\n[Change] Are all the attributes (color, shape, size, material, and relative position) of the remaining objects identical between the ground truth image and the system-generated image?\\n\\n[Add] Are all the attributes (color, shape, size, material, and relative position) of the remaining objects identical between the ground truth image and the system-generated image?\\n\\n[Remove] Are all the attributes (color, shape, size, material, and relative position) of the remaining objects identical between the ground truth image and the system-generated image?\\n\\nTable 21: Questions asked to human evaluators for evaluating N-EUROSIM and TIM-GAN. Note that there are some variations in the questions for Change, Add, and Remove instructions due to different semantic nature of the instructions.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 31, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-181", "page_num": 32, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-181", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We are interested in image manipulation via natural language text\u2014a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL) (Mao et al., 2019), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NEUROSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NEUROSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NEUROSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation.\\n\\n1 Introduction\\nThe last decade has seen significant growth in the application of neural models to a variety of tasks including those in computer vision (Chen et al., 2017; Krizhevsky et al., 2012), NLP (Wu et al., 2016), robotics and speech (Yu and Deng, 2016). It has been observed that these models often lack interpretability (Fan et al., 2021), and may not always be well suited to handle complex reasoning tasks (Dai et al., 2019). On the other hand, classical AI systems can seamlessly perform complex reasoning in an interpretable manner due to their symbolic representation (Pham et al., 2007; Cai and Su, 2012). But these models often lack in their ability to handle low-level representations and be robust to noise.\\n\\nNeuro-Symbolic models (Dong et al., 2019; Mao et al., 2019; Han et al., 2019) overcome these limitations by combining the power of (purely) neural with (purely) symbolic representations. Studies (Andreas et al., 2016; Hu et al., 2017; Johnson et al., 2017a; Mao et al., 2019) have shown that neuro-symbolic models have several desirable properties such as modularity, interpretability, and improved generalizability.\\n\\nOur aim in this work is to build neuro-symbolic models for the task of weakly supervised manipulation of images comprising multiple objects, via complex multi-hop natural language instructions. Specifically, we are interested in weak supervision that only uses the data annotated for VQA tasks, avoiding the high cost of getting supervised annotations in the form of target manipulated images. Our key intuition here is that this task can be solved simply by querying the manipulated representation without ever explicitly looking at the target image. The prior work includes weakly supervised approaches (Nam et al., 2018; Li et al., 2020) that require textual descriptions of images during training and are limited to very simple scenes (or instructions). (See Section 2 for a survey).\\n\\nOur solution builds on Neuro-Symbolic Concept Learner (NSCL) proposed by (Mao et al., 2019) for solving VQA. We extend this work to incorporate the notion of manipulation operations such as change, add, and remove objects in a given image.\\n\\nAs one of our main contributions, we design novel neural modules and a training strategy that just uses VQA annotations as weakly supervised data for the task of image manipulation. The neural modules are trained with the help of novel loss functions that measure the faithfulness of the manipulated scene and object representations by accessing a separate set of query networks, interchangeably referred to as quantization networks, trained just using VQA data. The manipulation takes place through interpretable programs created using primitive neural and symbolic operations from a Domain Specific Language.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neuro-symbolic Image Manipulator (NEUROSIM)\\n\\nChange the size of the thing behind the large ball to big.\\n\\n```\\nscene()\\n\\nfilter(O, large)\\n\\nrelate(O, behind)\\n\\nchange_size(O, large)\\n```\\n\\n**Input:** Source image \\\\( \\\\mathcal{I} \\\\)\\n\\n**Output:** Manipulated image \\\\( \\\\mathcal{I} \\\\)\\n\\n**Input:** Instruction Text \\\\( \\\\mathcal{T} \\\\)\\n\\n**Output:** Manipulation program \\\\( \\\\mathcal{P} \\\\)\\n\\nFigure 1: The problem setup. See Section 1 for more details.\\n\\nLanguage (DSL). Separately, a network is trained to render the image from a scene graph representation using a combination of \\\\( L_1 \\\\) and adversarial losses as done by (Johnson et al., 2018). The entire pipeline is trained without any intermediate supervision. We refer to our system as Neuro-Symbolic Image Manipulator (NEUROSIM). Figure 1 shows an example of I/O pair for our approach. Contributions of our work are as follows:\\n\\n1. We create NEUROSIM, the first neuro-symbolic, weakly supervised, and interpretable model for the task of text-guided image manipulation, that does not require output images for training.\\n2. We extend CLEVR (Johnson et al., 2017b), a benchmark dataset for VQA, to incorporate manipulation instructions and create a new dataset called as Complex Image Manipulation via Natural Language Instructions (CIM-NLI). We also create CIM-NLI-LARGE dataset to test zero-shot generalization.\\n3. We provide extensive quantitative experiments on newly created CIM-NLI, CIM-NLI-LARGE datasets along with qualitative experiments on Minecraft (Yi et al., 2018). Despite being weakly supervised, NEUROSIM is highly competitive to supervised SOTA approaches including a recently proposed diffusion based model (Brooks et al., 2023). NEUROSIM also performs well on instructions requiring multi-hop reasoning, all while being interpretable. We publicly release our code and data.\\n\\n**2 Related Work**\\n\\nTable 1 categorizes the related work across three broad dimensions - problem setting, task complexity, and approach. The problem setting comprises two sub-dimensions: i) supervision type - self, directive, or weak, ii) instruction format - text or UI-based. The task complexity comprises of following sub-dimensions: ii) scene complexity \u2013 single or multiple objects, ii) instruction complexity - zero or multi-hop instructions, iii) kinds of manipulations allowed - add, remove, or change. Finally, the approach consists of the following sub-dimensions: i) model \u2013 neural or neuro-symbolic and ii) whether a symbolic program is generated on the way or not. Dong et al. (2017), TAGAN (Nam et al., 2018), and ManiGAN (Li et al., 2020) are close to us in terms of the problem setting. These manipulate the source image using a GAN-based encoder-decoder architecture. Their weak supervision differs from ours \u2013 We need VQA annotation, they need captions or textual descriptions. The complexity of their natural language instructions is restricted to 0-hop. Most of their experimentation is limited to single (salient) object scenes.\\n\\nIn terms of task complexity, the closest to us are approaches such as TIM-GAN (Zhang et al., 2021), GeNeVA (El-Nouby et al., 2019), which build an encoder-decoder architecture and work with a latent representation of the image as well as the manipulation instruction. They require a large number of manipulated images as explicit annotations for training.\\n\\nIn terms of technique, the closest to our work are neuro-symbolic approaches for VQA such as NSVQA (Yi et al., 2018), NSCL (Mao et al., 2019), Neural Module Networks (Andreas et al., 2016) and its extensions (Hu et al., 2017; Johnson et al., 2017a). Clearly, while the modeling approach is similar and consists of constructing latent programs, the desired tasks are different in the two cases. Our work extends the NSCL approach for the task of automated image manipulation.\\n\\nJiang et al. (2021), Shi et al. (2021) deal with editing global features, such as brightness, contrast, etc., instead of object-level manipulations like in our case. Recent models such as InstructPix2Pix (Brooks et al., 2023), DALL-E (Ramesh et al., 2022) and Imagen (Saharia et al., 2022) on text-to-image generation using diffusion models are capable of editing images but require captions for input images; preliminary studies (Marcus et al., 2022) highlight their shortcomings in composi-\"}"}
{"id": "emnlp-2023-main-181", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 1: Comparison of Prior Work\\n\\n| Supervision Type | Instruction Format | SC: Scene Complexity | IC: Instruction Complexity | Operations | Model | Program |\\n|------------------|--------------------|----------------------|---------------------------|------------|-------|---------|\\n| SIMSG            | Self Supervision   | N/A                  | MO                        | N/A        | N/A   | N       |\\n|                  |                    |                      |                           |            |       | \u2713       |\\n| PGIM             | Direct Supervision | N/A                  | MO                        | N/A        | N/A   | \u2713       |\\n| GeNeV            | Direct Supervision | Text                 | MO                        | Multi-Hop  | N/A   | \u2713       |\\n| TIM-GAN          | Direct Supervision | Text                 | MO                        | Zero-Hop   | N/A   | \u2713       |\\n| Dong et. al      | Weak Supervision   | Text                 | SO                        | Zero-Hop   | N/A   | \u2713       |\\n| TAGAN            | Weak Supervision   | Text                 | SO                        | Zero-Hop   | N/A   | \u2713       |\\n| ManiGAN          | Weak Supervision   | Text                 | SO                        | Zero-Hop   | N/A   | \u2713       |\\n| InstructPix2Pix  | Pre-training +     | Text                 | MO                        | Multi-Hop  | N/A   | \u2713       |\\n|                 | Supervision        |                      |                           |            |       |         |\\n| EURO SIM (ours)  | Weak Supervision   | Text                 | MO                        | Multi-Hop  | N/A   | \u2713       |\\n\\n### 3.1 Motivation and Architecture Overview\\n\\nThe key motivation behind our approach comes from the following hypothesis: consider a learner $L$ (e.g., a neural network or the student in Fig 2) with sufficient capacity trying to achieve the task of manipulation over images $I$. Further, let each image be represented in terms of its properties, or properties of its constituents (e.g., objects like apple, leaf, tree, etc. in Fig 2), where each property comes from a finite set $S$ (e.g., attributes of objects in an image). Let the learner be provided with the prior knowledge (for e.g. through Question Answering as in Fig 2) about properties (e.g., color) and their possible values (e.g., red). Then, in order to learn the task of manipulation, it suffices to provide the learner with a query network, which given a manipulated image $\\\\tilde{I}$ constructed by the learner via command $C$, can correctly answer questions (i.e. query) about the desired state of various properties of the constituents of the image $\\\\tilde{I}$. The query network can be internal to the learner (e.g., the student in Fig 2 can query himself for checking the color of apples in the manipulated image). The learner can query repeatedly until it learns to perform the manipulation task correctly. Note, the learner does not have access to the supervised data corresponding to triplets of the form $(I_s, C, I_f)$, where $I_s$ is the starting image, $C$ is the manipulation command, and $I_f$ is the target manipulated image. Inspired by this, we set out to test this hypothesis by building a model capable of manipulating images, without target images as supervision.\\n\\nFigure 3 captures a high-level architecture of the proposed EURO SIM pipeline. EURO SIM allows manipulating images containing multiple objects, via complex natural language instructions. Similar to Mao et al. (2019), EURO SIM assumes the availability of a domain-specific language (DSL) for parsing the instruction text $T$ into an executable program $P$. EURO SIM is capable of handling addition, removal, and change operations over image objects. It reasons over the image for locating where the manipulation needs to take place followed by carrying out the manipulation operation. The first three modules, namely i) visual representation network, ii) semantic parser, and iii) concept quantization network are suitably customized from the NSCL and trained as required for our purpose. In what follows, we describe the design and training mechanism of EURO SIM.\\n\\n### 3.2 Modules Inherited from NSCL\\n\\n1. **Visual Representation Network:** Given input image $I$, this network converts it into a scene graph $G = (N, E)$. The nodes $N$ of this scene graph are object embeddings and the edges $E$ are embeddings capturing the relationship between pair of objects (nodes). Node embeddings are obtained by passing the bounding box of each object (along with the full image) through a ResNet-34 (He et al., 2016). Edge embeddings are obtained by concatenating the corresponding object embeddings.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is the color of the apple on the tree?\\n\\nPaint the red apples on the tree with blue color.\\n\\nTry Learning knowledge of concepts, attributes, relational-concepts eg color, red, apple, tree, on the via many QnA pairs.\\n\\nRed \u2714\\nGreen \u2716\\n\\nLearning to manipulate an image via self correction using knowledge of concepts and attributes acquired before \u2716\\n\\nAre the apples blue? No\\n\\nRetry \u2714\\n\\nFigure 2: Motivating example N EUROSIM. Best viewed under magnification. See Section 3.1 for more details.\\n\\nSemantic Parsing Module\\n\\nProgram \ud835\udc43\\n\\nVisual Representation Network\\n\\nScene graph \ud835\udc46\\n\\nScene graph \ud835\udc46\\n\\nSource image \ud835\udc46\\n\\nTarget image \ud835\udc46\\n\\nRendering Network\\n\\nManipulation Network\\n\\nProgram Executor\\n\\n3.3 Novel Modules and Training N EUROSIM\\n\\nN EUROSIM training starts with three sub-modules trained on the VQA task as described in Section 3.2. Next, we extend the original DSL to include three additional functional sub-modules within the semantic parsing module, namely add, remove, and change. Refer to appendix section A for details on the DSL. We now reset the semantic parsing module and train it again from scratch for generating programs corresponding to image manipulation instruction text \ud835\udc47. Such a program is subsequently used by the downstream pipeline to reason over the scene graph \ud835\udc46 and manipulate the image. In this step, the semantic parser is trained using an off-policy program search based REINFORCE (Williams, 1992) algorithm. Unlike the training of semantic parser for the VQA task, in this step, we do not have any final answer like reward supervision for training. Hence, we resort to a weaker form of supervision. In particular, consider an input instruction text \ud835\udc47 and set of all possible manipulation program templates \ud835\udc43 from which one can create any actual program \ud835\udc43 that is executable over the scene graph of the input image. For a program \ud835\udc43 \u2208 \ud835\udc43, our reward is positive if this program \ud835\udc43 selects any object (or part of the scene graph) to be sent to the manipulation networks (change/add/remove). See Appendix C for more details. Once the semantic parser is retrained, we clamp the first three modules and continue using them for the purpose of parsing.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"instructions and converting images into their scene graph representations. Scene graphs are manipulated using our novel module called manipulation network which is described next.\\n\\n4] Manipulation Network: This is our key module responsible for carrying out the manipulation operations. We allow three kinds of manipulation operations \u2013 add, remove, and change. Each of these operations is a composition of a quasi-symbolic and symbolic operation. A symbolic operation corresponds to a function that performs the required structural changes (i.e. addition/deletion of a node or an edge) in the scene graph $G$ against a given instruction. A quasi-symbolic operation is a dedicated neural network that takes the relevant part of $G$ as input and computes new representations of nodes and edges that are compatible with the changes described in the parsed instruction.\\n\\n(a) Change Network: For each visual attribute $a \\\\in A$ (e.g. shape, size, ...), we have a separate change neural network that takes the pair of (object embedding, embedding of the changed concept) as input and outputs the embedding of the changed object. This is the quasi-symbolic part of the change function, while the symbolic part is identity mapping. For e.g., let $g_{\\\\text{color}}: R^d_{\\\\text{obj}} + d_{\\\\text{attr}} \\\\rightarrow R^d_{\\\\text{obj}}$ represent the neural network that changes the color of an object. Consider $o \\\\in R^d_{\\\\text{obj}}$ as the object embedding and $c_{\\\\text{red}} \\\\in R^d_{\\\\text{attr}}$ as the concept embedding for the red color, then $\\\\tilde{o} = g_{\\\\text{color}}(o; c_{\\\\text{red}})$ represents the changed object embedding, whose color would be red. After applying the change neural network, we obtain the changed representation of the object $\\\\tilde{o} = g_{a}(o; cs^*a)$, where $s^*a$ is the desired changed value for the attribute $a$. This network is trained using the following losses.\\n\\n$$\\\\ell_a = -\\\\sum_{\\\\forall s \\\\in S_a} s^*a \\\\log [p(h_a(\\\\tilde{o}) = s)] \\\\quad (1)$$\\n\\n$$\\\\ell_a = -\\\\sum_{\\\\forall a' \\\\in A, a' \\\\neq a} \\\\sum_{\\\\forall s \\\\in S_a} p(h_{a'}(o) = s) \\\\cdot \\\\log [p(h_{a'}(\\\\tilde{o}) = s)] \\\\quad (2)$$\\n\\nwhere, $h_a(x)$ gives the concept value of the attribute $a$ (in symbolic form $s \\\\in S_a$) for the object $x$. The quantity $p(h_a(x) = s)$ denotes the probability that the concept value of the attribute $a$ for the object $x$ is equal to $s$ and is given as follows $p(h_a(x) = s) = \\\\exp \\\\text{dist}(f_a(x), cs) / \\\\sum_{\\\\tilde{s} \\\\in S_a} \\\\exp \\\\text{dist}(f_a(x), \\\\tilde{s})$ where, $\\\\text{dist}(a,b) = (a^\\\\top b - t^2) / t_1$ is the shifted and scaled cosine similarity, $t_1, t_2$ being constants.\\n\\nThe first loss term $\\\\ell_a$ penalizes the model if the (symbolic) value of the attribute $a$ for the manipulated object is different from the desired value $s^*a$ in terms of probabilities. The second term $\\\\ell_a$, on the other hand, penalizes the model if the values of any of the other attributes $a'$, deviate from their original values. Apart from these losses, we also include following additional losses.\\n\\n$$\\\\ell_{\\\\text{cycle}} = \\\\|o - g_{a}(\\\\tilde{o}; c_{\\\\text{old}})\\\\|^2 \\\\quad (3)$$\\n\\n$$\\\\ell_{\\\\text{consistency}} = \\\\|o - g_{a}(o; c_{\\\\text{old}})\\\\|^2 \\\\quad (4)$$\\n\\n$$\\\\ell_{\\\\text{objGAN}} = -\\\\sum_{o' \\\\in O} \\\\log D((o')) + \\\\log (1 - D(g_{a}(o'; c_{\\\\text{old}}))) \\\\quad (5)$$\\n\\nwhere $c_{\\\\text{old}}$ is the original value of the attribute $a$ of object $o$, before undergoing change. Intuitively the first loss term $\\\\ell_{\\\\text{cycle}}$ says that, changing an object and then changing it back should result in the same object. The second loss term $\\\\ell_{\\\\text{consistency}}$ intuitively means that changing an object $o$ that has value $c_{\\\\text{old}}$ for attribute $a$, into a new object with the same value $c_{\\\\text{old}}$, should not result in any change. These additional losses prevent the change network from changing attributes which are not explicitly taken care of in earlier losses (1) and (2). For e.g., rotation or location attributes of the objects that are not part of our DSL. We also impose an adversarial loss $\\\\ell_{\\\\text{objGAN}}$ to ensure that the new object embedding $\\\\tilde{o}$ is from the same distribution as real object embeddings. See Appendix C for more details.\\n\\n(b) Remove Network: This network takes the scene graph $G_I$ of the input image and removes the subgraph from $G_I$ that contains the nodes (and incident edges) corresponding to the object(s) that need to be removed, and returns a new scene graph $G_{\\\\tilde{I}}$ which is reduced in size. The quasi-symbolic function for the remove network is identity.\\n\\n(c) Add Network: For adding a new object into the scene, add network requires the symbolic values of different attributes, say $\\\\{s_{a1}, s_{a2}, ..., s_{ak}\\\\}$, for the new object, e.g., $\\\\{\\\\text{red}, \\\\text{cylinder}, ...\\\\}$. It also requires the spatial relation $r$ (e.g. RightOf) of the new object with respect to an existing object in the scene. The add function first predicts the object (node) embedding $\\\\tilde{o}_{\\\\text{new}}$ for the object to be added, followed by predicting edge embeddings for new edges incident on the new node. New object embedding is obtained as follows:\\n\\n$$\\\\tilde{o}_{\\\\text{new}} = g_{\\\\text{addObj}}(\\\\{cs_{a1}, cs_{a2}, ..., cs_{ak}\\\\}, o_{rel}, c_r)$$\\n\\nwhere, $o_{rel}$ is the object embedding of an existing object, relative to which the new object's position $r$ is specified. For each existing objects $o_i$ in the scene, an ...\"}"}
{"id": "emnlp-2023-main-181", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"edge \\\\( \\\\tilde{e} \\\\) is predicted between the newly added object \\\\( \\\\tilde{o}_{\\\\text{new}} \\\\) and existing object \\\\( o_i \\\\) in following manner:\\n\\n\\\\[\\n\\\\tilde{e}_{\\\\text{new},i} = g_{\\\\text{addEdge}}(\\\\tilde{o}_{\\\\text{new}}, o_i)\\n\\\\]\\n\\nFunctions \\\\( g_{\\\\text{addObj}}(\\\\cdot) \\\\) and \\\\( g_{\\\\text{addEdge}}(\\\\cdot) \\\\) are quasi-symbolic operations. Symbolic operations in add network comprise adding the above node and the incident edges into the scene graph.\\n\\nThe add network is trained in a self-supervised manner. For this, we pick a training image and create its scene graph. Next, we randomly select an object \\\\( o \\\\) from this image and quantize its concepts, along with a relation with any other object \\\\( o_i \\\\) in the same image. We then use our remove network to remove this object \\\\( o \\\\) from the scene. Finally, we use the quantized concepts and the relation that were gathered above and add this object \\\\( o \\\\) back into the scene graph using \\\\( g_{\\\\text{addObj}}(\\\\cdot) \\\\) and \\\\( g_{\\\\text{addEdge}}(\\\\cdot) \\\\). Let the embedding of the object after adding it back is \\\\( \\\\tilde{o}_{\\\\text{new}} \\\\). The training losses are as follows:\\n\\n\\\\[\\n\\\\ell_{\\\\text{concepts}} = - \\\\sum_{j=1}^{k} \\\\log(p(h_a(\\\\tilde{o}_{\\\\text{new}}) = s_a_j)) \\\\quad (6)\\n\\\\]\\n\\n\\\\[\\n\\\\ell_{\\\\text{relation}} = - \\\\log(p(h_r(\\\\tilde{o}_{\\\\text{new}}, o_i) = r)) \\\\quad (7)\\n\\\\]\\n\\n\\\\[\\n\\\\ell_{\\\\text{objSup}} = \\\\| o - \\\\tilde{o}_{\\\\text{new}} \\\\|_2 \\\\quad (8)\\n\\\\]\\n\\n\\\\[\\n\\\\ell_{\\\\text{edgeSup}} = \\\\sum_{i \\\\in \\\\mathcal{O}} \\\\| e_{\\\\text{old},i} - \\\\tilde{e}_{\\\\text{new},i} \\\\|_2 \\\\quad (9)\\n\\\\]\\n\\n\\\\[\\n\\\\ell_{\\\\text{edgeGAN}} = - \\\\sum_{i \\\\in \\\\mathcal{O}} \\\\left[ \\\\log(D(\\\\{o; e_{\\\\text{old},i}; o_i\\\\}) + \\\\log(1 - D(\\\\{\\\\tilde{o}_{\\\\text{new}}; \\\\tilde{e}_{\\\\text{new},i}; o_i\\\\})) \\\\right] \\\\quad (10)\\n\\\\]\\n\\nwhere \\\\( s_a_j \\\\) is the required (symbolic) value of the attribute \\\\( a_j \\\\) for the original object \\\\( o \\\\), and \\\\( r \\\\) is the required relational concept. \\\\( \\\\mathcal{O} \\\\) is the set of the objects in the image, \\\\( e_{\\\\text{old},i} \\\\) is the edge embedding for the edge between original object \\\\( o \\\\) and its neighboring object \\\\( o_i \\\\). Similarly, \\\\( \\\\tilde{e}_{\\\\text{new},i} \\\\) is the corresponding embedding of the same edge but after when we have (removed + added back) the original object. The loss terms \\\\( \\\\ell_{\\\\text{concepts}} \\\\) and \\\\( \\\\ell_{\\\\text{relation}} \\\\) ensure that the added object comprises desired values of attributes and relation, respectively. Since we had first removed and then added the object back, we already have the original edge and object representation, and hence we use them in loss terms given in equation 9. We use adversarial loss equation 10 for generating real \\\\((o, e, o)\\\\) triples and also a loss similar to equation 5 for generating real objects.\\n\\n3.4 Image Rendering from Scene Graph\\n\\nFollowing Johnson et al. (2018), the scene graph for an image is first generated using the visual representation network, which is processed by a GCN and passed through a mask regression network followed by a box regression network to generate a coarse 2-dimensional structure (scene layout). A Cascaded Refinement Network (Chen and Koltun, 2017) is then employed to generate an image from the scene layout. A min-max adversarial training procedure is used to generate realistic images, using a patch-based and object-based discriminator.\\n\\n4 Experiments\\n\\nDatasets:\\n\\nAmong the existing datasets, CSS (Vo et al., 2019) contains simple 0-hop instructions and is primarily designed for the text-guided image retrieval task. Other datasets such as i-CLEVR (El-Nouby et al., 2019) and CoDraw are designed for iterative image editing. i-CLEVR contains only \u201cadd\u201d instructions and CoDraw doesn\u2019t contain multi-hop instructions. Hence we created our own multi-object multi-hop instruction based image manipulation dataset, referred to as CIM-NLI. This dataset was generated with the help of CLEVR toolkit (Johnson et al., 2017b). CIM-NLI consists of \\\\((\\\\text{Source image } I, \\\\text{Instruction text } T, \\\\text{Target image } \\\\tilde{I})\\\\) triplets. The dataset contains a total of 18K, 5K, 5K unique images and 54K, 14K, 14K instructions in the train, validation and test splits respectively. Refer to Appendix B for more details about the dataset generation and dataset splits.\\n\\nBaselines:\\n\\nWe compare our model with purely supervised approaches such as TIM-GAN (Zhang et al., 2021), GeNeVA (El-Nouby et al., 2019) and InstructPix2Pix (Brooks et al., 2023). In order to make a fair and meaningful comparison between the two kinds (supervised and our, weakly-supervised) approaches, we carve out the following set-up. Assume the cost required to create one single annotated example for image manipulation task be \\\\( \\\\alpha_m \\\\) while the corresponding cost for the VQA task be \\\\( \\\\alpha_v \\\\). Let \\\\( \\\\alpha = \\\\alpha_m / \\\\alpha_v \\\\). Let \\\\( \\\\beta_m \\\\) be the number of annotated examples required by a supervised baseline for reaching a performance level of \\\\( \\\\eta_m \\\\) on the image manipulation task. Similarly, let \\\\( \\\\beta_v \\\\) be the number of annotated VQA examples required to train \\\\( \\\\text{EURO SIM} \\\\) to reach the performance level of \\\\( \\\\eta_v \\\\). Let \\\\( \\\\beta = \\\\beta_m / \\\\beta_v \\\\). We are interested in figuring out the range of \\\\( \\\\beta \\\\) for which performance of our system \\\\( \\\\eta_v \\\\) is at least as good as the baseline \\\\( \\\\eta_m \\\\). Correspondingly we can compute the ratio of the labeling effort required, i.e., \\\\( \\\\alpha \\\\beta \\\\), to reach these performance levels.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If $\\\\alpha \\\\beta > 1$, our system achieves the same or better performance, with lower annotation cost.\\n\\nWeakly supervised models (Li et al., 2020; Nam et al., 2018) are designed for a problem setting different from ours \u2013 single salient object scenes, simple 0-hop instructions (Refer Section 2 for details). Further, they require paired images and their textual descriptions as annotations. We, therefore, do not compare with them in our experiments. See Appendix G, H for computational resources and hyperparameters respectively.\\n\\nEvaluation Metrics:\\nFor evaluation on image manipulation task, we use three metrics - i) FID, ii) Recall@k, and iii) Relational-similarity (rsim). FID (Heusel et al., 2017) measures the realism of the generated images. We use the implementation proposed in Parmar et al. (2022) to compute FID. Recall@k measures the semantic similarity of gold manipulated image $\\\\tilde{I}$ and system produced manipulated image $\\\\tilde{I}$. For computing Recall@k, we follow Zhang et al. (2021), i.e. we use $\\\\tilde{I}$ as a query and retrieve images from a corpus comprising the entire test set. rsim measures how many of the ground truth relations between the objects are present in the generated image. We follow (El-Nouby et al., 2019) to implement rsim metric that uses predictions from a trained object-detector (Faster-RCNN) to perform relation matching between the scene-graphs of ground-truth and generated images.\\n\\n4.1 Performance with varying Dataset Size\\nTable 2 compares the performance of NEURO SIM other SoTA methods two level of $\\\\beta$.\\n\\n| Method         | $\\\\beta = 0.054$ | $\\\\beta = 0.54$ |\\n|----------------|-----------------|----------------|\\n| FID            |                 |                |\\n| R@1            |                 |                |\\n| R@3            |                 |                |\\n| rsim           |                 |                |\\n| GeNeVA         | 42.6            | 66.6           |\\n| TIM-GAN        | 24.2            | 31.9           |\\n| IP2P           | 3.4             | 40.6           |\\n| NEURO SIM      | 35.0            | 45.3           |\\n\\nTable 2: Performance comparison of NEURO SIM with TIM-GAN and GeNeVA, and InstructPix2Pix (IP2P) with 10% data ($\\\\beta = 0.054$) and full data ($\\\\beta = 0.54$).\\n\\nWe always use 100K VQA examples (5K Images, 20 questions per image) for our weakly supervised training. $R@1$, $R@3$ correspond to Recall@1,3 respectively. FID: lower is better; Recall/rsim: higher is better. See Section 4.1 for more details.\\n\\nBased IP2P in full data setting, using the $R@1$ performance metric. This clearly demonstrates the strength of our approach in learning to manipulate while only making use of VQA annotations. We hypothesize that, in most cases, NEURO SIM will be preferable since we expect the cost of annotating an output image for manipulation to be significantly higher than the cost of annotating a VQA example. To reach the performance of the NEURO SIM in a low data regime, TIM-GAN requires a larger number of expensive annotated examples (ref. Table 13 in Appendix). The FID metric shows similar trend across dataset sizes and across models. The FID scores for NEURO SIM could potentially be improved by jointly training VQA module along with image decoder and is a future direction.\\n\\nWe evaluate InstructPix2Pix (IP2P) (Brooks et al., 2023), a state-of-the-art pre-trained diffusion model for image editing, in a zero-shot manner on the CIM-NLI dataset. Considering its extensive pre-training, we expect IP2P to have learned the concepts present in the CIM-NLI dataset. In this setting IP2P achieves a FID score of 33.07 and $R@1$ score of 7.48 illustrating the limitations of...\"}"}
{"id": "emnlp-2023-main-181", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"large-scale models in effectively executing complex instruction-based editing tasks without full dataset fine-tuning. Table 2 contains the results obtained by IP2P after fine-tuning for 16k iterations on CIM-NLI dataset.\\n\\n| Method       | Larger Scenes | Hops | $R_1$ | $R_3$ |\\n|--------------|---------------|------|-------|-------|\\n| GeNeV A      | 54 K          |      | 0.65  | 0.26  |\\n| TIM-GAN      | 54 K          |      | 0.7  | 0.16  |\\n| IP2P         | 54 K          |      | 0.99  | 0.72  |\\n| N EURO SIM   | 54 K          |      | 0.89  | 0.64  |\\n\\nTable 3: (Left) Performance on generalization to Larger Scenes. (Right) $R_1$ results for 0-hop (ZH) vs multi-hop (MH) instruction-guided image manipulation. See Sections 4.2 and 4.3 for more details.\\n\\n4.2 Performance versus Reasoning Hops\\n\\nTable 3 (right) compares baselines with N EURO SIM for performance over instructions requiring zero-hop (ZH) versus multi-hop (1\u22123 hops) (MH) reasoning. Since there are no Add instructions with ZH, we exclude them from this experiment for the comparison to be meaningful. GeNeV A performs abysmally on both ZH as well as MH. We see a significant drop in the performance of both TIM-GAN and IP2P when going from ZH to MH instructions, both for training on 54K, as well as, 5.4K data-points. In contrast, N EURO SIM trained on 10% data, sees a performance drop of only 0.5 points showing its robustness for complex reasoning tasks.\\n\\n4.3 Zero-shot Generalization to Larger Scenes\\n\\nWe developed another dataset called CIM-NLI-LARGE, consisting of scenes having 10\u221213 objects (See Appendix B for details). We study the combinatorial generalization ability of N EURO SIM and the baselines when the models are trained on CIM-NLI containing scenes with 3\u22128 objects only and evaluated on CIM-NLI-LARGE. Table 3 captures such a comparison. N EURO SIM does significantly better, i.e., 33 pts ($R_1$) than TIM-GAN and is competitive with IP2P when trained on 10% (5.4K data points) of CIM-NLI. We do see a drop in performance relative to baselines when they are trained on full (54K) data, but this is expected as effect of supervision takes over, and ours is a weakly supervised model. Nevertheless, this experiment demonstrates the effectiveness of our model for zero-shot generalization, despite being weakly supervised.\\n\\n4.4 Qualitative Analysis and Interpretability\\n\\nFigure 4 shows anecdotal examples for visually comparing N EURO SIM with baselines. Note, GeNeV A either performs the wrong operation on the image (row #1, 2, 3) or simply copies the input image to output without any modifications. TIM-GAN often makes semantic errors which show its lack of reasoning (row #3) or make partial edits (row #1). IP2P also suffers from this where it edits incorrect object (row #1,2). Compared to baselines, N EURO SIM produces semantically more meaningful image manipulation. N EURO SIM can also easily recover occluded objects (row #4). For more results, see Appendix I, J. N EURO SIM produces interpretable output programs, showing the steps taken by the model to edit the images, which also helps in detecting errors (ref. Appendix L).\\n\\n4.5 Evaluating Manipulated Scene Graph\\n\\nWe strongly believe image rendering module of N EURO SIM pipeline and encoder modules used for computing Recall add some amount of inefficiencies resulting in lower $R_1$ and $R_3$ scores for us. Therefore, we decide to assess the quality of manipulated scene graph $\\\\tilde{G}_I$. Table 4: $G_\\\\tilde{I}$ Quality via image retrieval. For this, we consider the text guided image retrieval task proposed by (Vo et al., 2019). In this task, an image from the database has to be retrieved which would be the closest match to the desired manipulated image. Therefore, we use our manipulated scene graph $\\\\tilde{G}_I$ as the latent representation of the input instruction and image for image retrieval. We retrieve images from the database based on a novel graph edit distance between N EURO SIM generated $\\\\tilde{G}_I$ of the desired manipulated images, and scene graphs of the images in the database. This distance is defined using the Hungarian algorithm (Kuhn, 1955) with a simple cost defined between any 2 nodes of the graph (ref. Appendix D for details). Table 4 captures the performance of N EURO SIM and other popular baselines for the image retrieval task. N EURO SIM significantly outperforms supervised learning baselines by a margin of $\\\\sim50\\\\%$ without using output image supervision, demonstrating that N EURO SIM\"}"}
{"id": "emnlp-2023-main-181", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"meaningfully edits the scene graph. Refer to Sec-\\ntion 4.7 for human evaluation results and Appendix\\nSection D-E, K, for more results including results \\non Minecraft dataset and ablations.\\n\\n4.6 A Hybrid Approach using N\\\\textsubscript{EURO}SIM\\n\\nFrom Table 3, we observe that both TIM-GAN and IP2P suffer a significant drop in performance when moving from ZH to MH instructions, whereas N\\\\textsubscript{EURO}SIM is fairly robust to this change. Further, we note that the manipulation instructions in our dataset are multi-hop in terms of reasoning, but once an object of interest is identified, the actual manipulation operation can be seen as single hop. We use this observation to design a hybrid supervised baseline that utilizes the superior reasoning capability of N\\\\textsubscript{EURO}SIM and high quality editing and generation capabilities of IP2P.\\n\\nWe take the CIM-NLI test set and parse the text-instructions through our trained semantic-parser to obtain the object embeddings over which the manipulation operation is to be performed. We utilize our trained query networks to obtain the symbolic attributes such as color, shape, size and material of the identified object. Using these attributes we simplify a complex multi-hop instruction into a simple instruction with 0 or 1 hops using a simple template based approach (see Appendix Section N for details). These simplified instructions are fed to the fine-tuned IP2P model to generate the edited images. We refer to our hybrid approach as IP2P-NS where NS refers to Neuro-Symbolic. Table 5 presents the results. We find that there is a clear advantage of using a hybrid neuro-symbolic model integrating N\\\\textsubscript{EURO}SIM with IP2P. We see a significant gain on FID, recall, rsim when we use the hybrid approach, especially in the low resource setting ($\\\\beta = 0.054$). Compared to IP2P, the hybrid neuro-symbolic approach results in better FID, recall and rsim scores, except a small drop in R1 for $\\\\beta = 0.54$ setting. This opens up the possibility of further exploring such hybrid models in future for improved performance (in the supervised setting).\\n\\n**Table 5**: Comparison between IP2P-NS and IP2P.\\n\\n| Method       | FID  | R1   | R3   | rsim |\\n|--------------|------|------|------|------|\\n| IP2P         | 3.4  | 40.6 | 3.77 | 0.88 |\\n| N\\\\textsubscript{EURO}SIM | 3.5 | 45.3 | 3.65 | 0.91 |\\n| IP2P-NS      | 1.96 | 45.5 | 3.83 | 0.94 |\\n\\n**Table 6**: Human evaluation comparing various models.\\n\\n| Qn.   | N\\\\textsubscript{EURO}SIM 5.4K | TIM-GAN 54K | IP2P 54K |\\n|-------|-----------------------------|-------------|----------|\\n| Q1    | 0.41                        | 0.27        | 0.25     |\\n| Q2    | 0.33                        | 0.84        | 0.78     |\\n\\nFor the human evaluation study, we presented 10 evaluators with a set of five images each, including: The input image, the ground-truth image and manipulated images generated by N\\\\textsubscript{EURO}SIM 5.4K, TIM-GAN 54K, and IP2P 54K. Images generated by the candidate models were randomly shuffled to prevent any bias. Evaluators were asked two binary questions, each requiring a 'yes' (1) or 'no' (0) response, to assess the models: (Q1) Does the model perform the desired change mentioned in the input instruction?, (Q2) Does the model not introduce any undesired change elsewhere in the image? Refer to Appendix Section M for more details about exact questions and the human evaluation process.\\n\\nThe average scores from the evaluators across different questions can be found in Table 6. The study achieved a high average Fleiss' kappa score (Fleiss et al., 2013) of 0.646, indicating strong inter-evaluator agreement. Notably, N\\\\textsubscript{EURO}SIM (5.4K) outperforms TIM-GAN and IP2P (54K) in Q1 suggesting its superior ability to do reasoning, and identify the relevant object as well as affect the desired change. In contrast, TIM-GAN and IP2P score significantly better in Q2, demonstrating their ability not to introduce unwanted changes elsewhere in the image, possibly due to better generation quality compared to N\\\\textsubscript{EURO}SIM.\\n\\n5 Conclusion\\n\\nWe present a neuro-symbolic, interpretable approach N\\\\textsubscript{EURO}SIM to solve image manipulation task using weak supervision in the form of VQA annotations. Our approach can handle multi-object scenes with complex instructions requiring multi-hop reasoning, and solve the task without any output image supervision. We also curate a dataset of image manipulation and demonstrate the potential of our approach compared to supervised baselines. Future work includes understanding the nature of errors made by N\\\\textsubscript{EURO}SIM, having a human in the loop to provide feedback to the system for correction, and experimenting with real image datasets.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6 Ethics Statement\\nAll the datasets used in this paper were synthetically generated and do not contain any personally identifiable information or offensive content. The ideas and techniques proposed in this paper are useful in designing interpretable natural language-guided tools for image editing, computer-aided design, and video games. One of the possible adverse impacts of AI-based image manipulation is the creation of deepfakes (Vaccari and Chadwick, 2020) (using deep learning to create fake images). To counter deepfakes, several researchers (Dolhansky et al., 2020; Mirsky and Lee, 2021) have also looked into the problem of detecting real vs. fake images.\\n\\n7 Limitations\\nA limitation of our approach is that when transferring to a new domain, having different visual concepts requires not only learning new visual concepts but also the DSL needs to be redefined. Automatic learning of DSL from data has been explored in some prior works (Ellis et al., 2021, 2018), and improving our model using these techniques are future work for us. We can also use more powerful graph decoders for image generation, for improved image quality, which would naturally result in stronger results on image manipulation.\\n\\nAcknowledgements\\nWe thank anonymous reviewers for their insightful suggestions that helped in greatly improving our paper. We also thank Rushil Gupta and other members of IIT Delhi DAIR group for their helpful comments and suggestions on this work. This work was supported by an IBM AI Horizons Network (AIHN) grant. We thank IIT Delhi HPC facility, IBM cloud facility, and IBM Cognitive Computing Cluster (CCC) for computational resources. Ashish Goswami is a PhD student at Yardi-ScAI@IIT-Delhi and is supported by Yardi School of AI Publication Grant. Parag Singla was supported by the DARPA Explainable Artificial Intelligence (XAI) Program with number N66001-17-2-4032, IBM AI Horizon Networks (AIHN) grant and IBM SUR awards. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of the funding agencies.\\n\\nReferences\\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Learning to compose neural networks for question answering. In Proceedings of NAACL-HLT, pages 1545\u20131554.\\n\\nTim Brooks, Aleksander Holynski, and Alexei A. Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of CVPR, pages 18392\u201318402.\\n\\nShaowei Cai and Kaile Su. 2012. Configuration checking with aspiration in local search for SAT. In Proceedings of AAAI, pages 434\u2013440.\\n\\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. 2017. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848.\\n\\nLichang Chen, Guosheng Lin, Shijie Wang, and Qingyao Wu. 2020. Graph edit distance reward: Learning to edit scene graph. In Proceedings of ECCV, pages 539\u2013554.\\n\\nQifeng Chen and Vladlen Koltun. 2017. Photographic image synthesis with cascaded refinement networks. In Proceedings of ICCV, pages 1520\u20131529.\\n\\nBlender Online Community. 2018. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam.\\n\\nWang-Zhou Dai, Qiu-Ling Xu, Yang Yu, and Zhi-Hua Zhou. 2019. Bridging machine learning and logical reasoning by abductive learning. In Proceedings of NeurIPS, pages 2811\u20132822.\\n\\nBrian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Cantion Ferrer. 2020. The deepfake detection challenge (dfdc) dataset. ArXiv preprint, abs/2006.07397.\\n\\nHao Dong, Simiao Yu, Chao Wu, and Yike Guo. 2017. Semantic image synthesis via adversarial learning. In Proceedings of ICCV, pages 5707\u20135715.\\n\\nHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural logic machines. In Proceedings of ICLR.\\n\\nAlaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, R. Devon Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua Bengio, and Graham W. Taylor. 2019. Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction. In Proceedings of ICCV, pages 10303\u201310311.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-181", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cristian Vaccari and Andrew Chadwick. 2020. Deep-fakes and disinformation: Exploring the impact of synthetic political video on deception, uncertainty, and trust in news. Social Media+ Society, 6(1).\\n\\nNam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. 2019. Composing text and image for image retrieval - an empirical odyssey. In Proceedings of CVPR, pages 6439\u20136448.\\n\\nRonald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229\u2013256.\\n\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. ArXiv preprint, abs/1609.08144.\\n\\nKexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. 2018. Neural-symbolic VQA: disentangling reasoning from vision and language understanding. In Proceedings of NeurIPS, pages 1039\u20131050.\\n\\nDong Yu and Li Deng. 2016. Automatic speech recognition, volume 1. Springer.\\n\\nTianhao Zhang, Hung-Yu Tseng, Lu Jiang, Weilong Yang, Honglak Lee, and Irfan Essa. 2021. Text as neural operator: Image manipulation by text instruction. In Proceedings of ACM MM, pages 1893\u20131902.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA Domain Specific Language (DSL)\\n\\nTable 7 captures the DSL used by our NEURO SIM pipeline. The first 5 constructs in this table are common with the DSL used in Mao et al. (2019). The last 3 operations (Change, Add, and Remove) were added by us to allow for the manipulation of objects. Table 8 shows the type system used by the DSL in this work. The first 5 types are inherited from (Mao et al., 2019) while the last one is an extension of the type system for handling the inputs to the Add operator.\\n\\nB Dataset Details\\n\\nWe use CLEVR dataset and CLEVR toolkit (code to generate the dataset). These are public and are under CC and BSD licenses respectively, and are used by many works, including ours, for research purposes. We now give details of the datasets we create, building upon CLEVR.\\n\\nB.1 CIM-NLI Dataset\\n\\nThis dataset was generated with the help of CLEVR toolkit (Johnson et al., 2017b) by using following recipe.\\n\\n1. First, we create a source image $I$ and the corresponding scene data by using Blender (Community, 2018) software.\\n2. For each source image $I$ created above, we generate multiple instruction texts $T$'s using its scene data. These are generated using templates, similar to question templates proposed by (Johnson et al., 2017b).\\n3. For each such $(I, T)$ pair, we attach a corresponding symbolic program $P$ (not used by NEURO SIM though) as well as scene data for the corresponding changed image.\\n4. Finally, for each $(I, T)$ pair, we generate the target gold image $\\\\tilde{I}$ using Blender software and its scene data from the previous step.\\n\\nBelow are some of the important characteristics of the CIM-NLI dataset.\\n\\n- Each source image $I$ comprises several objects and each object comprises four visual attributes - color, shape, size, and material.\\n- Each instructions text $T$ comprises one of the following three kinds of manipulation operations - add, remove, and change.\\n- An add instruction specifies color, shape, size, and material of the object that needs to be added. It also specifies a direct (or indirect) relation with one or more existing objects (called reference object(s)). The number of relations that are required to traverse for nailing down the target object is referred to as # of reasoning hops and we have allowed instructions with up to 3-hops reasoning. We do not generate any 0-hop instruction for add due to ambiguity of where to place the object inside the scene.\\n- A change instruction first specifies zero or more attributes to uniquely identify the object that needs to be changed. It may also specify a direct (or indirect) relation with one or more existing reference objects. Lastly, it specifies the target values of an attribute for the identified object which needs to be changed.\\n- A remove instruction specifies zero or more attributes of the object(s) to be removed. Additionally, it may specify a direct (or indirect) relation with one or more existing reference objects.\\n\\nTable 9 captures the fine grained statistics about the CIM-NLI dataset. Specifically, it further splits each of the train, validation, and test set across the instruction types - add, remove, and change.\\n\\nB.2 CIM-NLI-LARGE Dataset\\n\\nWe created another dataset called CIM-NLI-LARGE to test the generalization ability of NEURO SIM on images containing more number of objects than training images. CIM-NLI-LARGE tests the zero-shot transfer ability of both NEURO SIM and baselines on scenes containing more objects.\\n\\nEach image in CIM-NLI-LARGE dataset comprises of $10^{-13}$ objects as opposed to $3^{-8}$ objects in CIM-NLI dataset which was used to train NEURO SIM. The CIM-NLI-LARGE dataset consists of $1K$ unique input images. We have created 3 instructions for each image resulting in a total of $3K$ instructions. The number of add instructions is significantly less since there is very little free space available in the scene to add new objects. To create scenes with 12 and 13 objects, we made all objects as small size and the minimum distance between...\"}"}
{"id": "emnlp-2023-main-181", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Extended Domain Specific Language (DSL) used by N\\\\EURO SIM.\\n\\nB.3 Multi-hop Instructions\\n\\nIn what follows, we have given examples of the instructions that require multi-hop reasoning to nail down the location/object to be manipulated in the image.\\n\\n\u2022 Remove the tiny green rubber ball.\\n\\n\u2022 There is a block right of the tiny green rubber ball, remove it.\\n\\n\u2022 Remove the shiny cube left of the block in front of the gray thing.\\n\\n\u2022 Remove the small thing that is left of the brown matte object behind the tiny cylinder that is behind the big yellow metal block.\\n\\nC Model Details\\n\\nC.1 Semantic Parser\\n\\nC.1.1 Details on Parsing\\n\\nWe begin by extending the type system of (Mao et al., 2019) and add ConceptSet because our add operation takes as input a set of concepts depicting attribute values of the new object being added (refer Table 8 for the details). Next, in a manner similar to (Mao et al., 2019), we use a rule based system for extracting concept words from the input text. We, however, add an extra rule for extracting ConceptSet from the input sentence. Rest of the semantic parsing methodology remains the same as given in (Mao et al., 2019), with the difference being that our training is weakly supervised (refer Section 3.3 of the main paper).\\n\\nC.1.2 Training\\n\\nAs explained in Section 3.3 of the main paper, for training with weaker form of supervision, we use an off-policy program search based REINFORCE (Williams, 1992) algorithm for calculating the exact gradient. For this, we define a set of all possible program templates \\\\( P_t \\\\). For a given input instruction...\"}"}
{"id": "emnlp-2023-main-181", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Type Remarks\\n\\nObjConcept Concepts for any given object, such as blue, cylinder, etc.\\n\\nAttribute Attributes for any given object, such as color, shape, etc.\\n\\nRelConcept Relational concepts for any given object pair, such as RightOf, LeftOf, etc.\\n\\nObjectDepicts a single object\\n\\nObjectSetDepicts multiple objects\\n\\nConceptSetA set of elements of ObjConcept type\\n\\nTable 8: Extended type system for the DSL used by NEUROSIM.\\n\\n| Operation       | Split # | # reasoning hops | # objects |\\n|-----------------|---------|------------------|-----------|\\n|                 | train   | min  | mean | max | min  | mean | max |\\n| Add valid       | 4459    | 1    | 2.00 | 3   | 3    | 5.50 | 8   |\\n| Add valid       | 4464    | 1    | 2.00 | 3   | 3    | 5.45 | 8   |\\n| Remove valid    | 5000    | 0    | 1.50 | 3   | 3    | 5.50 | 8   |\\n| Remove valid    | 5000    | 0    | 1.50 | 3   | 3    | 5.48 | 8   |\\n| Change valid    | 4996    | 0    | 1.50 | 3   | 3    | 5.56 | 8   |\\n| Change valid    | 4998    | 0    | 1.50 | 3   | 3    | 5.52 | 8   |\\n\\nTable 9: Statistics of CIM-NLI dataset introduced in this paper.\\n\\nAfter a hyperparameter search for the reward (refer Section H of the appendix), we assign a reward of +8 if the reasoning() part of the program leads to an object being selected for change/remove instruction or a related object being selected for add instruction. If no such object is selected, we give a reward of +2. Reward values were decided on the basis of validation set accuracy. We find that with this training strategy, we achieve the validation set accuracy of 95.64%, where this accuracy is calculated based on whether a program lead to an object being selected or not. Note, this is a proxy to the actual accuracy. For finding the actual accuracy, we would need a validation set of (instruction, ground truth output program) pairs, but we do not use this supervised data for training or validation.\"}"}
{"id": "emnlp-2023-main-181", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Statistics of CIM-NLI-LARGE dataset.\\n\\nC.2 Manipulation Network\\nIn what follows, we provide finer details of manipulation network components.\\n\\nChange Network:\\nAs described in Section 3.3 of the main paper, we have a change neural network for each attribute. For changing the current attribute value of a given object \\\\( o \\\\), we use the following neural network:\\n\\n\\\\[ \\\\tilde{\\\\text{g}}_a(\\\\cdot) = \\\\text{g}_a(o; c_{s^a}), \\\\]\\n\\nwhere \\\\( s^a \\\\) is the desired changed value for the attribute \\\\( a \\\\). \\\\( \\\\tilde{o} \\\\) is the new representation of the object. We model \\\\( \\\\text{g}_a(\\\\cdot) \\\\) by a single layer neural network without having any non-linearity. The input dimension of this neural network is \\\\( (256 + 64) \\\\) because we concatenate the object representation \\\\( o \\\\in \\\\mathbb{R}^{256} \\\\) with the desired concept representation \\\\( d \\\\in \\\\mathbb{R}^{64} \\\\). We pass this concatenated vector through \\\\( \\\\text{g}_a(\\\\cdot) \\\\) to get the revised representation of the object: \\\\( \\\\tilde{o} \\\\in \\\\mathbb{R}^{256} \\\\).\\n\\nThe loss used to train the weights of the change network is a weighted sum of losses equation 1 to equation 5 given in the main paper. This leads to the overall loss function given below.\\n\\n\\\\[ L_{overall\\\\_change} = \\\\lambda_1\\\\ell_a + \\\\lambda_2\\\\ell_a + \\\\lambda_3\\\\ell_{cycle} + \\\\lambda_4\\\\ell_{consistency} + \\\\lambda_5\\\\ell_{objGAN} \\\\]\\n\\nwhere, \\\\( \\\\ell_{objGAN} \\\\) above is the modified GAN loss (Goodfellow et al., 2014). Here \\\\( \\\\lambda_1 = 1 \\\\), \\\\( \\\\lambda_2 = \\\\frac{1}{(\\\\text{num.attrs} - 1) \\\\times \\\\text{num.concepts}} \\\\), \\\\( \\\\lambda_3 = \\\\lambda_4 = 10^3 \\\\), and \\\\( \\\\lambda_5 = \\\\frac{1}{\\\\text{num.objects}} \\\\). Here, \\\\( \\\\text{num.objects} \\\\) is the number of objects in input image, \\\\( \\\\text{num.attrs} \\\\) is the total number of attributes for each object, and \\\\( \\\\text{num.concepts} \\\\) are the total number of concepts in the NSCL (Mao et al., 2019) framework.\\n\\nThe object discriminator is a neural network with input dimension \\\\( 256 \\\\) and a single 300 dimensional hidden layer with ReLU activation function. This discriminator is trained using standard GAN objective \\\\( \\\\ell_{objGAN} \\\\). See Fig 5a for an overview of the change operator.\\n\\nRemove Network:\\nThe remove network is a symbolic operation as described in Section 3.3 of the main paper. That is, given an input set of objects, the remove operation deletes the subgraph of the scene graph that contains the nodes corresponding to removed objects and the edges incident on those nodes. See Fig 5c for an overview of the remove operator.\\n\\nAdd Network:\\nThe neural operation in the add operator comprises of predicting the object representation for the newly added object using a function \\\\( \\\\text{g}_\\\\text{addObj}(\\\\cdot) \\\\). This function is modeled as a single layer neural network without any activation. The input to this network is a concatenated vector \\\\( [c_{s_1}, c_{s_2}, \\\\ldots, c_{s_k}, o_{rel}, c_r] \\\\), where \\\\( [c_{s_1}, c_{s_2}, \\\\ldots, c_{s_k}] \\\\) represents the concatenation of all the concept vectors of the desired new objects. The vector \\\\( o_{rel} \\\\) is the representation of the object with whom the relation (i.e. position) of the new object has been specified and \\\\( c_r \\\\) is the concept vector for that relationship. The input dimension of \\\\( \\\\text{g}_\\\\text{addObj}(\\\\cdot) \\\\) is \\\\( (k \\\\times 64 + 256 + 64) \\\\) and the output dimension is 256. For predicting representation of newly added edges in the scene graph, we use edge predictor \\\\( \\\\text{g}_\\\\text{addEdge}(\\\\cdot) \\\\). The input to this edge predictor function is the concatenated representation of the objects which are linked by the edge. The input dimension of \\\\( \\\\text{g}_\\\\text{addEdge}(\\\\cdot) \\\\) is \\\\( (256 + 256) \\\\) and the output dimension is 256.\\n\\nThe loss used to train the add network weights is a weighted sum of losses equation 6 to equation 10 along with an object discriminator loss. The overall loss is given by the following expression.\\n\\n\\\\[ L_{overall\\\\_add} = \\\\lambda_1\\\\ell_{concepts} + \\\\lambda_2\\\\ell_{relation} + \\\\lambda_3\\\\ell_{objSup} + \\\\lambda_4\\\\ell_{edgeSup} + \\\\lambda_5\\\\ell_{edgeGAN} + \\\\lambda_6\\\\ell_{objGAN} \\\\]\\n\\nwhere, \\\\( \\\\ell_{objGAN} \\\\) and \\\\( \\\\ell_{edgeGAN} \\\\) above denotes the modified GAN loss (Goodfellow et al., 2014). Here...\"}"}
{"id": "emnlp-2023-main-181", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: Screenshots for human evaluation study. See Section 4.7 for more details.\"}"}
