{"id": "emnlp-2023-main-76", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Researchers at Switzerland's University of Basel and ETH Zurich have established the theoretical foundations for 100% secure communication, ensuring protection not just against quantum computers but also in instances where the communication devices' operational mechanisms and trustworthiness are unknown. The protocol adds artificial noise to information about the cryptographic key, guaranteeing eavesdroppers receive so little actual data as to render the protocol unbreakable. Nicolas Sangouard from the University of Basel said, \\\"Since the first small-scale quantum computers are now available, we urgently need new solutions for protecting privacy. Our work represents a significant step toward the next milestone in secure communications.\\\"\\n\\nResearchers at the University of Basel in Switzerland say they have developed a method for generating a secure quantum key via photonic conversion. The method adds artificial noise to the initial measurement data, which cannot be known or controlled by an adversary, to guarantee that the key will be obtained by both Alice and Bob, who are connected by a quantum channel. By adding the noise, the researchers were able to increase the threshold at which they can guarantee the key is obtained. Although the researchers say their method is not perfect, they believe it is still effective and could lead to a new generation of highly secure and provable quantum key distribution. They note that the new method provides security even when assumptions about the security of QKD are not fulfilled.\\n\\nResearchers at the University of Basel in Germany have developed a method for generating a key for a quantum computer in which both Alice and Bob are connected by a quantum channel. The method adds artificial noise to the initial measurement data to increase the probability that the signals sent over the quantum channel are successfully received. However, in practice, only a small amount of information about the key can be obtained. In a fully device-independent QKD, the researchers assume that any information contained in the measurement device must be known and controlled by an external party. Unlike traditional quantum key distribution, Eve does not directly control the measurement outcome, and in the absence of such information, the key is sent to an untrusted source who also distributes the information to the other user. By adding artificial noise, Eve can not ensure that the information in the raw key is real. \\\"Our method provides a way to significantly relax the threshold in such a way as to provide provable security,\\\" says Basel Professor Sangouard.\\n\\nResearchers at the University of Basel in Switzerland say they have developed a method for generating a secure quantum key via photonic conversion. The method involves adding artificial noise to the initial measurement data, which cannot be known or controlled by an adversary, to increase the probability that the key is obtained. Although the researchers say the method is not fully device-independent, they say it provides security even when assumptions are made about the security of the quantum key distribution. \\\"There is a fundamental obstacle in the development of QKD, i.e. the requirement that an adversary cannot fully control the quantum channel,\\\" says Basel Professor Sangouard Bancal. However, he says the method provides sufficient bounds on the minimum required global detection efficiency to ensure that the information sent over the qubit channel is good and accurate.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"'Don\u2019t Get Too Technical with Me':\\nA Discourse Structure-Based Framework for Science Journalism\\nRonald Cardenas1, Bingsheng Yao2, Dakuo Wang3, and Yufang Hou4\\n1University of Edinburgh,\\n2Rensselaer Polytechnic Institute\\n3Northeastern University,\\n4IBM Research Europe, Ireland\\nronald.cardenas@ed.ac.uk, yaob@rpi.edu\\nd.wang@northeastern.edu, yhou@ie.ibm.com\\n\\nAbstract\\nScience journalism refers to the task of reporting technical findings of a scientific paper as a less technical news article to the general public. We aim to design an automated system to support this real-world task (i.e., automatic science journalism) by 1) introducing a newly-constructed and real-world dataset (SCI TECH NEWS), with tuples of a publicly-available scientific paper, its corresponding news article, and an expert-written short summary snippet; 2) proposing a novel technical framework that integrates a paper\u2019s discourse structure with its metadata to guide generation; and, 3) demonstrating with extensive automatic and human experiments that our framework outperforms other baseline methods (e.g. Alpaca and ChatGPT) in elaborating a content plan meaningful for the target audience, simplifying the information selected, and producing a coherent final report in a layman\u2019s style.\\n\\n1 Introduction\\nScience journalism refers to producing journalistic content that covers topics related to different areas of scientific research (Angler, 2017). It plays an important role in fostering public understanding of science and its impact. However, the sheer volume of scientific literature makes it challenging for journalists to report on every significant discovery, potentially leaving many overlooked. For instance, in the year 2022 alone, 185,692 papers were submitted to the preprint repository arXiv.org spanning highly diverse scientific domains such as biomedical research, social and political sciences, engineering research and a multitude of others.1 To this date, PubMed contains around 345,332 scientific publications about the novel coronavirus Covid-19, nearly 1.6 times as many as those produced in 200 years of work on influenza.2\\n\\nThe enormous quantity of scientific literature and the huge amount of manual effort required to produce high-quality science journalistic content inspired recent interest in tasks such as generating blog titles or slides for scientific papers (Vadapalli et al., 2018; Sun et al., 2021), extracting structured knowledge from scientific literature (Hou et al., 2019; Mondal et al., 2021; Zhang et al., 2022), simplifying technical health manuals for the general public (Cao et al., 2020), and creating plain language summaries for scientific literature (Dangovski et al., 2021; Goldsack et al., 2022).\\n\\nOur work focuses on generating simplified journalistic summaries of scientific papers for the non-technical general audience. To achieve this goal, we introduce a new dataset, SCI TECH NEWS, which pairs full scientific papers with their corresponding press release articles and newswire snippets as published in ACM TechNews. We further carry out in-depth analysis to understand the journalists\u2019 summarization strategies from different dimensions (Section 3.2). Then, we explore novel model designs to generate short journalistic summaries for scientific papers. Unlike previous studies that model this problem as a \u201cflat\u201d sequence-to-sequence task and ignore crucial metadata information of scientific papers (Dangovski et al., 2021; Goldsack et al., 2022), we propose a technical framework that integrates author and affiliation data as they are important information in scientific news articles. Furthermore, we encode each sentence with its corresponding discourse rhetoric role (e.g., background or methods) and apply a hierarchical decoding strategy to generate summaries. As illustrated in Figure 1, our trained decoding model first generates a content plan, which is then employed to guide the model in producing summaries that adhere to the plan\u2019s structure.\\n\\nIn summary, our main contributions are two-fold. First, we construct a new open-access high-quality dataset for automatic science journalism\"}"}
{"id": "emnlp-2023-main-76", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Computer scientists at Israel\u2019s Tel Aviv University (TAU) say they have developed a \u201cmaster face\u201d method for circumventing a large number of facial recognition systems, by applying artificial intelligence to generate a facial template. The researchers say the technique exploits such systems\u2019 usage of broad sets of markers to identify specific people; producing facial templates that match many such markers essentially creates an omni-face that can bypass numerous safeguards. The researchers created the master face by plugging an algorithm into a generative adversarial network that builds digital images of artificial human faces. The TAU team said testing showed the template was able to unlock over 20% of the identities in an open-source database of 13,000 facial images operated by the University of Massachusetts.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We collect archived TechNews snippets between 1999 and 2021 and link them with their respective press release articles. Then, we parse each news article for links to the scientific article it reports about. We discard samples where we find more than one link to scientific articles in the press release. Finally, the scientific articles are retrieved in PDF format and processed using Grobid. Following collection strategies of previous scientific summarization datasets (Cohan et al., 2018), section heading names are retrieved, and the article text is divided into sections. We also extract the title and all author names and affiliations.\\n\\nTable 1 presents statistics of our dataset in comparison with datasets for lay, newswire, and scientific article summarization. Tokenization and sentence splitting was done using spaCy (Honnibal et al., 2020). In total, we gathered 29,069 press release summaries, from which 18,933 were linked to their corresponding press release articles. From these, 2,431 instances\u2014aligned rows in Table 1\u2014were linked to their corresponding scientific articles. In this final subset, all instances have press release metadata (e.g., date of publication, author), press release summary and article, scientific article metadata (e.g., author names and affiliations), and scientific article body and abstract. We refer to this subset as SCITECHNEWS-ALIGNED, divide it into validation (1,431) and test set (1,000), and leave the rest of the unaligned data as non-parallel training data. Figure 6 in the appendix showcases a complete example of the aligned dataset. The train-test division was made according to the source and availability of each instance's corresponding scientific article, i.e., whether it is open access or not. The test set consists of only open-access scientific articles, whereas the validation set contains open-access as well as articles accessible only through institutional credentials. For this reason, we release the curated test set to the research community but instead provide download instructions for the validation set.\\n\\n3.2 Dataset Analysis\\n\\nWe conduct an in-depth analysis of our dataset and report the knowledge domains covered and the variation in content type and writing style between scientific abstracts and press summaries. SCITECHNEWS gathers scientific articles from a diverse pool of knowledge domains, including Computer Science, Physics, Engineering, and Biomedical, as shown in Table 2. Sources include journals in Nature, ACM, APS, as well as conference-style articles from arXiv, IEEE, BioArxiv, among others. Note that a sizable chunk of articles was obtained from the authors' personal websites, as shown by the category 'author'.\\n\\nTable 2: Most frequent sources of scientific articles in the validation and test set of SCITECHNEWS. The 'author' category refers to papers obtained from authors' personal websites.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Table 3: Differences between scientific abstracts (Sci) and press release (PR) summaries in SciTechNews, in terms of text readability (\u2193, the lower, the more readable) and percentage of novel ngrams \u2013 as a proxy for abstractiveness (\u2191, the higher, the more abstractive).\\n\\n| Metric          | Sci | PR |\\n|-----------------|-----|----|\\n| FKGL            | 14.81 | 14.79 |\\n| CLI             | 15.17 | 14.23 |\\n| DCRS            | 11.08 | 11.13 |\\n| Gunning         | 16.33 | 16.75 |\\n| Average         | 14.35 | 14.23 |\\n| Abstractivity (%)|     |    |\\n| Novel unigrams  | 14.07 | 32.12 |\\n| Novel bigrams   | 47.32 | 72.50 |\\n| Novel trigrams  | 70.38 | 90.21 |\\n\\n5 These metrics aim to measure the simplicity or readability of a text by applying experimental formulas that consider the number of characters, words, and sentences in a text. For all these metrics, the lower the score, the more readable or simpler a text is. As shown in Table 3, the readability of abstracts and press summaries are on comparable levels (small gaps in scores), in line with observations in previous work in text simplification (Devaraj et al., 2021) and lay summarization (Goldsack et al., 2022). Nevertheless, all differences are statistically significant by means of the Wincoxin-Mann-Whitney test.\\n\\n### Summarization Strategies.\\nWe examined and quantified the differences in summarization strategies required in our dataset.\\n\\nFirst, we assessed the degree of text overlap between the source document (i.e., the scientific article body) and either the abstract or the press summary as the reference summary, as shown in Figure 2. Specifically, we examine the extractiveness level of dataset samples in terms of extractive fragment coverage and density (Grusky et al., 2018).\\n\\nWhen the reference summary is of non-scientific style (Fig. 2a), our dataset shows lower density than PLOS (Goldsack et al., 2022), a recent benchmark for lay summarization. This indicates that the task of science journalism, as exemplified by our dataset, requires following a less extractive strategy, i.e., shorter fragments are required to be copied verbatim from the source document. Similarly, when the reference summary is of scientific style (Fig. 2b), our dataset shows far lower density levels compared to arXiv and a more concentrated distribution in terms of coverage. Such features indicate that SciTechNews is much less extractive than arXiv and constitutes a more challenging dataset for scientific article summarization, as we corroborated with preliminary experiments.\\n\\nSecond, we examined the amount of information in the reference summary not mentioned verbatim in the source document, a proxy for abstractiveness. Table 3 (second row) presents the percentage of novel n-grams in scientific abstracts (Sci) and press release summaries (PR). PR summaries show a significantly higher level of abstractivity than abstracts, indicating the heavy presence of information fusion and rephrasing strategies during summarization.\\n\\n### Distribution of Named Entities.\\nWhat type of named entities are reported in a summary can be indicative of the writing style, more precisely of the intended audience and communicative goal of said summary. We quantify this difference by comparing the distribution of named entities in scientific abstracts against that of press summaries.\\n\\nAs shown in Figure 3, press summaries show a distribution of named entities extracted using the spaCy library.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Average frequency of named entities in press release (PR) summaries and scientific abstracts (Sci).\\n\\nHigh presence of organization and person entities, whereas scientific abstracts report mostly number entities. It is worth noting the low, however noticeable, presence of organization and person entities in the scientific abstracts. Upon closer inspection, these entities referred to scientific instruments and constants named after real-life scientists, e.g., the Hubble telescope. In contrast, person entities in press summaries most often referred to author names, whereas the organization entity referred to their affiliations.\\n\\nDiscourse Structure\\n\\nNext, we examine the difference in scientific discourse structure between abstracts and press release summaries. We employ the model proposed by Li et al. (2021) trained on the PubMed-RCT dataset (Dernoncourt and Lee, 2017), and label each sentence in a summary with its rhetorical role, e.g., background, conclusion, method, among others. Figure 4 presents the presence of rhetorical roles along with their relative positions in the summaries. Scientific abstracts tend to start with background information, then present methods, followed by results, and finish with conclusions. In contrast, press release summaries tend to emphasize conclusions way sooner than abstracts, taking the spotlight away from results and, to a lesser degree, from methods. Surprisingly, the relative presence of background information seems to be similar in both abstracts and press release summaries, in contrast with its emphasized presence in lay summaries, as reported in previous work (Goldsack et al., 2022).\\n\\n7 Li et al. (2021) report F-scores of 0.95 and 0.84 for scientific discourse tagging on two datasets from the biomedical domain.\\n\\nProblem Formulation and Modelling\\n\\nWe cast the problem of science journalism as an encoder-decoder generative task and propose a model that performs content planning and style transferring while summarizing the content. Given a scientific article text $D$, enriched with metadata information $M$, the task proceeds in two steps. First, a plan $s$ is generated conditioned on the input document, $p(s|D, M)$, followed by the summary $y$, conditioned on both the document and the plan, $p(y|s, D, M)$. Following Narayan et al. (2021), we train an encoder-decoder model that encodes an input document and learns to generate the concatenated plan and summary as a single sequence. Let $D = \\\\langle x_0, ..., x_N \\\\rangle$ be a scientific article text, modeled as a sequence of sentences, let $M$ be the set of author-affiliation pairs associated with the said article, and let $Y$ be the target summary. We define $D' = \\\\langle m, m_0, .., m|M|, t_0, x_0, .., t_N, x_N \\\\rangle$ as the input to the encoder, where $m$ is a special token indicating the beginning of metadata information, $m_i \\\\in M$ is an author name concatenated to the corresponding affiliation, and $t_j$ is a label indicating the scientific rhetorical role of sentence $x_i$.\\n\\nGiven the encoder states, the decoder proceeds to generate plan $s$ conditioned on $D'$, $p(s|D'; \\\\Theta)$, where $\\\\Theta$ are the model parameters. The plan is defined as $s = \\\\langle [\\\\text{PLAN}]s_0, ..., s|y| \\\\rangle$ where $s_k$ is a label indicating the rhetorical role sentence $y_k$ in summary $y$ should cover. Figure 1 shows an example of the annotated document and content plan. We use a Bart encoder-decoder architecture (Lewis et al., 2020) and train it with $D'$ as the source and $[s; y]$ (plan and summary concatenated) as the target. We call this model Bart plan.\\n\\nExperimental Setup\\n\\nIn this section, we elaborate on the baselines used and evaluation methods, both using automatic metrics and eliciting human judgments. Following previous work (Goldsack et al., 2022), we use the abstract followed by the introduction as the article body and prepend the metadata information as described in the previous section.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"maximizing the ROUGE score (rouge 1 + rouge 2)\\n\\nFor unsupervised baselines, we compare against LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004), two extractive systems that model the document as a graph of sentences and score them using node centrality measures. For supervised systems, we choose BART (Lewis et al., 2020) as our encoder-decoder architecture and use the pretrained checkpoints for BART-LARGE available at the HuggingFace library (Wolf et al., 2020). The following BART-based systems are compared: Bartarx, finetuned on the ARXIV dataset (Cohan et al., 2018); BartSciT, finetuned on SCITECH-NEWS with only the abstract and introduction text as input, without metadata information or scientific rhetoric labels, and tasked to generate only the target summary without plan; and finally, Bartmeta, trained with metadata and article as input and summary without plan as the target.\\n\\nFinally, we benchmark on recently proposed large language models (LLM) fine-tuned on the instruction-following paradigm: GPT-3.5-Turbo 8, We used model gpt-3.5-turbo-0301 in https://platform.openai.com/docs/models based on GPT3 (Brown et al., 2020); FlanT5-Large (Chung et al., 2022), fine-tuned on T5-3B (Raffel et al., 2020); and Alpaca 7B (Taori et al., 2023), an instruction-finetuned version of LLaMA (Touvron et al., 2023). We employ the same instruction prompt followed by the abstract and introduction for all systems, \u201cWrite a report of this paper in journalistic style.\u201d\\n\\n5.2 Evaluation Measures\\n\\nGiven the nature of our task, we evaluate the intrinsic performance of our models across the axes of summarization and style transfer.\\n\\nSummarization. The informativeness, relevance, and fluency of the generated summaries are evaluated using ROUGE 1, 2, and L, respectively (Lin, 2004). Semantic relevance is evaluated with BertScore (Zhang et al.) using RoBERTa-large as base model (Liu et al., 2019) and in-domain importance weighting. All evaluations were made over the summary text after stripping away the generated content plan.\\n\\nStyle Transfer To distinguish between press release style and scientific style, we train a binary sentence classifier using press release summary sentences from the unaligned split of SCITECH-NEWS as positive samples, and an equal amount of sentences from scientific abstracts from arXiv (Cohan et al., 2018) as negative examples. We\\n\\n9 As calculated by the rouge-score library.\\n\\n10 BertScore has been proven a reliable metric when equipped with importance weighting in highly technical domains such as medical texts (Miura et al., 2021; Hossain et al., 2020).\"}"}
{"id": "emnlp-2023-main-76", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"use the RoBERTa-base model as implemented in the huggingface library in a sequence classification setup. Then, the style score \\\\( s_y \\\\) of summary \\\\( S \\\\) is defined as the probability of the positive class, averaged over all sentences in \\\\( S \\\\).\\n\\nFaithfulness. Factual consistency of generated summaries with respect to their source document is quantified using QuestEval (Scialom et al., 2021).\\n\\nHuman Evaluation. We take a random sample of 30 items from the test set and conduct a study using Best-Worst Scaling (Louviere et al., 2015), a method that measures the maximum difference between options and has been shown to produce more robust results than rating scales (Kiritchenko and Mohammad, 2017). Human subjects were shown the source document (abstract, introduction, and metadata) along with the output of three systems. They were asked to choose the best and the worst according to the following dimensions: (1) Informativeness \u2013 how well the summary captures important information from the document; (2) Factuality \u2013 whether named entities were supported by the source document; (3) Non-Redundancy \u2013 if the summary presents less repeated information; (4) Readability \u2013 if the summary is written in simple terms; (5) Style \u2013 whether the summary text follows a journalistic writing style; and finally, (6) Usefulness \u2013 how useful would the summary be as a first draft when writing a press release summary of a scientific article. Systems are ranked across a dimension by assigning them a score between \\\\(-1\\\\) and \\\\(1\\\\), calculated as the difference between the proportion of times it was selected as best and selected as worst. See Appendix D for more details.\\n\\n6 Results and Discussion\\n\\nIn this section, we present and discuss the results of our automatic and human evaluations, provide a comprehensive analysis of the factuality errors our systems incur, and finish with a demonstration of controlled generation with custom user plans.\\n\\n6.1 Automatic Metrics\\n\\nInformativeness and Fluency. Table 4 presents the performance of the compared systems in terms of ROUGE and BertScore. We notice that the extractive upper-bounds, A\\\\(\\\\text{BSTRACT} \\\\) and E\\\\(\\\\text{XT-ORACLE} \\\\), obtain relatively lower scores compared to previously reported extractive upper-bounds in lay summarization (Goldsack et al., 2022). This further confirms that the kind of content covered in press release summaries and scientific abstracts are fundamentally different, as explored in Section 3.2. For the abstractive systems, we notice that Bart\\\\(\\\\text{arx} \\\\) significantly improves over Bart\\\\(\\\\text{SciT} \\\\), highlighting the critical importance of adding metadata information to the source document. Generating a scientific rhetorical plan as part of the output further improves informativeness (Rouge-1) and fluency (Rouge-L), as well as semantic relevance (BertScore) of the produced content. It is worth noting that the assessed LLMs, Alpaca, FlanT5, and GPT-3.5, underperform the proposed models, indicating that the task poses a significant challenge to them under zero-shot conditions.\\n\\nReadability, Faithfulness, and Style. First, we find that adding metadata and plan information reduces syntactic and lexical complexity and improves faithfulness, as shown in Table 5. Inter-\"}"}
{"id": "emnlp-2023-main-76", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: System ranking according to human judgements, along (Inf)ormativeness, (Non-Red)undancy, (Fact)uality, (Read)ability, Press Release (Sty)le, and (Use)fulness. Best system is shown in bold.\\n\\nTable 6 shows the results of our human evaluation study, comparing models effective at encoding metadata (Bart meta), generating a plan (Bart plan), and a strong LLM baseline (GPT-3.5). Inter-annotator agreement \u2013 Krippendorff\u2019s alpha (Krippendorff, 2007) \u2013 was found to be 0.57. Pairwise statistical significance was tested using a one-way ANOVA with posthoc Tukey-HSD tests and 95% confidence interval. The difference between preferences across dimensions was found to be significant (p < 0.01) for the following pairs: expert-written gold PR summaries vs. all systems, in all dimensions; for Non-Redundancy, Bart plan and GPT3.5 against Bart meta; for Factuality, Bart meta vs GPT-3.5; for Readability, Bart plan vs all systems and Bart meta vs GPT-3.5; and for Style and Usefulness, all pairs combinations were significant.\\n\\nThe results indicate the following. First, scores for PR summaries are higher than machine-generated text, further confirming the difficulty of the task and showing ample room for improvement. Second, Bart meta\u2019s rather low scores in Non-Redundancy and Style can be due to its memorization of highly frequent patterns in the dataset, e.g., \u2018researchers at the university of ... \u2019. In contrast, Bart plan generates more diverse and stylish text. Third, whereas GPT-3.5\u2019s high Factuality score can be attributed to the difference in the number of architecture parameters, its low Readability and Style scores indicate that the simplification and stylization of complex knowledge still pose a significant challenge. Finally, in terms of Usefulness, users preferred Bart plan as a starting draft for writing a press release summary, demonstrating the model\u2019s effectiveness for this task.\\n\\n6.3 Factuality Error Analysis\\nWe further analyzed the types of factuality errors our systems incurred on. We uniformly sampled 30 instances from the test set and manually annotated their respective reference summary and summaries generated by Bart plan and GPT3.5-Turbo. We adapt the error taxonomy employed in Goyal and Durrett (2021) and consider three categories at the span level: (i) Entity-related, when the span is a named entity (same entity categories considered in Section 3.2); (ii) Noun Phrase-related, when the span is an NP modifier; and (iii) Other Errors, such as repetitions and grammatical errors. Each error category except \u2018Other\u2019 is further divided into sub-categories: Intrinsic, Extrinsic, and World Knowledge, depending on where the supporting information is found (Cao and Wang, 2021). Intrinsic errors are caused when phrases or entities found in the input are generated in the wrong place. In contrast, extrinsic errors happen when the generated span cannot be supported by the input or any external source (e.g., Wikipedia). Finally, word knowledge errors are caused when the span cannot be verified with the input but it can be verified using external knowledge, e.g. author X being the director of the C.S. department at university Y.\\n\\nTable 7 presents the proportion of error categories found in the inspected summaries, along with the total number of error spans found for each system. It is worth noting that the total number of errors follows the ranking trend in Table 6, with PR summaries having the least number of errors, followed by GPT3.5, and then Bart plan. First, we observe that reference summaries exhibit only Entity and NP-related errors of type World Knowledge. The event-related category is not considered here since the source documents in SCIENCE NEWS do not contain events.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Researchers at the University of Basel in Switzerland say they have developed a method for generating a secure quantum key via photonic conversion. The method adds artificial noise to the initial measurement data, which cannot be known or controlled by an adversary, to guarantee that the key will be obtained by both Alice and Bob, who are connected by a quantum channel. By adding the noise, the researchers were able to increase the threshold at which they can guarantee the key is obtained. Although the researchers say their method is not perfect, they believe it is still effective and could lead to a new generation of highly secure and provable quantum key distribution. They note that the new method provides security even when assumptions about the security of QKD are not fulfilled.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The introduced dataset is in English, as a result, our models and results are limited to English only. Future work can focus on the creation of datasets and the adaptation of science journalism to other languages. Also of relevance is the limited size of our dataset, and the potential lack of balanced coverage on the reported knowledge domains. Finally, LLMs results suggest that a more extensive prompt engineering might be critical to induce generation with adequate press release journalistic style.\\n\\nAnother limitation of our approach is the usage of only author and affiliation metadata as additional input information. We decide to only consider this metadata for the following reason. Considering the distribution of named entities found in Press Release reference summaries (analyzed in Section 3.2 and depicted in Figure 3), it is worth noting that entities of type Organization and Person are the most frequent entities \u2013 after numbers and miscellaneous. Hence, we decided to restrict the usage of metadata in our framework to author's names and affiliations. However, other metadata information was collected, both from the scientific article (e.g. publication venue and year, title) and press release articles (e.g. title, PR publication date, journalistic organization), as detailed in Section 3.1. We include the complete metadata in the released dataset so that future investigations can leverage them.\\n\\n9 Ethics Statements\\n\\nThe task of automatic science journalism is intended to support journalists or the researchers themselves in writing high-quality journalistic content more efficiently and coping with information overload. For instance, a journalist could use the summaries generated by our systems as an initial draft and edit it for factual inconsistencies and add context if needed. Although we do not foresee the negative societal impact of the task or the accompanying data itself, we point at the general challenges related to factuality and bias in machine-generated texts, and call the potential users and developers of science journalism applications to exert caution and follow up-to-date ethical policies.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ashwin Devaraj, Byron C Wallace, Iain J Marshall, and Junyi Jessy Li. 2021. Paragraph-level simplification of medical texts. In Proceedings of the 2021 Conference on NAACL-HLT, volume 2021, page 4972.\\n\\nG\u00fcnes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. *Journal of artificial intelligence research*, 22:457\u2013479.\\n\\nTomas Goldsack, Zhihao Zhang, Chenghua Lin, and Carolina Scarton. 2022. Making science simple: Corpora for the lay summarisation of scientific literature. In Proceedings of the 2022 Conference on EMNLP, pages 10589\u201310604. Association for Computational Linguistics.\\n\\nTanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference on NAACL-HLT, pages 1449\u20131462.\\n\\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference on NAACL-HLT, pages 708\u2013719.\\n\\nRobert Gunning. 1968. *The technique of clear writing*, rev. ed edition. McGraw-Hill.\\n\\nKarl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Proceedings of the 28th International Conference on NeuRIPS, pages 1693\u20131701.\\n\\nMatthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python.\\n\\nTamanna Hossain, Robert L Logan IV, Arjuna Ugarte, Yoshitomo Matsubara, Sean Young, and Sameer Singh. 2020. Covidlies: Detecting covid-19 misinformation on social media. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020.\\n\\nYufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, and Debasis Ganguly. 2019. Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction. In Proceedings of the 57th Annual Meeting of the ACL, pages 5203\u20135213. Association for Computational Linguistics.\\n\\nJ Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. 1975. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Technical report, Naval Technical Training Command Millington TN Research Branch.\\n\\nSvetlana Kiritchenko and Saif Mohammad. 2017. Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation. In Proceedings of the 55th Annual Meeting of the ACL, pages 465\u2013470.\\n\\nKlaus Krippendorff. 2007. Computing krippendorff's alpha-reliability. *Departmental Papers (ASC), UPenn*.\\n\\nPhilippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. 2022. Summac: Re-visiting nli-based models for inconsistency detection in summarization. *Transactions of the ACL*, 10:163\u2013177.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the ACL, pages 7871\u20137880.\\n\\nXiangci Li, Gully Burns, and Nanyun Peng. 2021. Scientific discourse tagging for evidence extraction. In Proceedings of the 16th Conference of the EACL, pages 2550\u20132562.\\n\\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In *Text summarization branches out*, pages 74\u201381.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mananth Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. *CoRR*, abs/1907.11692.\\n\\nIlya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations.\\n\\nJordan J Louviere, Terry N Flynn, and Anthony Alfred John Marley. 2015. *Best-worst scaling: Theory, methods and applications*. Cambridge University Press.\\n\\nRada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proceedings of the 2004 conference on EMNLP, pages 404\u2013411.\\n\\nYasuhide Miura, Yuhao Zhang, Emily Tsai, Curtis Laneglotz, and Dan Jurafsky. 2021. Improving factual completeness and consistency of image-to-text radiology report generation. In Proceedings of the 2021 Conference on NAACL-HLT, pages 5288\u20135304.\\n\\nIshani Mondal, Yufang Hou, and Charles Jochim. 2021. End-to-end construction of NLP knowledge graph. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1885\u20131895. Association for Computational Linguistics.\\n\\nShashi Narayan, Shay B Cohen, and Mirella Lapata. 2019. What is this article about? extreme summarization with topic-aware convolutional neural networks. *Journal of Artificial Intelligence Research*, 66:243\u2013278.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shashi Narayan, Yao Zhao, Joshua Maynez, Gon\u00e7alo Sim\u00f5es, Vitaly Nikolaev, and Ryan McDonald. 2021. Planning with learned entity prompts for abstractive summarization. Transactions of the ACL, 9:1475\u20131492.\\n\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the ACL, pages 4885\u20134901.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551.\\n\\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. Questeval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on EMNLP, pages 6594\u20136604.\\n\\nEdward Sun, Yufang Hou, Dakuo Wang, Yunfeng Zhang, and Nancy X. R. Wang. 2021. D2S: Document-to-slide generation via query-based text summarization. In Proceedings of the 2021 Conference on NAACL-HLT, pages 1405\u20131418. Association for Computational Linguistics.\\n\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\\n\\nRaghuram Vadapalli, Bakhtiyar Syed, Nishant Prabhu, Balaji Vasan Srinivasan, and Vasudeva Varma. 2018. When science journalism meets artificial intelligence: An interactive demonstration. In Proceedings of the 2018 Conference on EMNLP: System Demonstrations, pages 163\u2013168. Association for Computational Linguistics.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtoowicz, et al. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on EMNLP: System demonstrations, pages 38\u201345.\\n\\nBingsheng Yao, Dakuo Wang, Tongshuang Wu, Zheng Zhang, Toby Li, Mo Yu, and Ying Xu. 2022. It is ai\u2019s turn to ask humans a question: Question-answer pair generation for children\u2019s story books. In Proceedings of the 60th Annual Meeting of the ACL, pages 731\u2013744.\\n\\nFarooq Zaman, Matthew Shardlow, Saeed-Ul Hassan, Naif Radi Aljohani, and Raheel Nawaz. 2020. Htss: A novel hybrid text summarisation and simplification architecture. Information Processing & Management, 57(6):102351.\\n\\nShao Zhang, Hui Xu, Yuting Jia, Ying Wen, Dakuo Wang, Luoyi Fu, Xinbing Wang, and Chenghu Zhou. 2022. Geodeepshovel: A platform for building scientific database from geoscience literature with ai assistance. Geoscience Data Journal.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Researchers Say They've Found a Wildly Successful Bypass for Face Recognition Tech\\n\\nComputer scientists at Israel's Tel Aviv University (TAU) say they have developed a \\\"master face\\\" method for circumventing a large number of facial recognition systems, by applying artificial intelligence to generate a facial template. The researchers say the technique exploits such systems' usage of broad sets of markers to identify specific people; producing facial templates that match many such markers essentially creates an omni-face that can bypass numerous safeguards.\\n\\nIn addition to helping police arrest the wrong person or monitor how often you visit the Gap, facial recognition is increasingly used by companies as a routine security procedure: it's a way to unlock your phone or log into social media, for example. This practice comes with an exchange of privacy for the promise of comfort and security but, according to a recent study, . . .\\n\\nGenerating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution\\n\\nAbstract\\n\\nA master face is a face image that passes face-based identity-authentication for a large portion of the population. These faces can be used to impersonate, with a high probability of success, any user, without having access to any user-information. We optimize these faces, by using an evolutionary algorithm in the latent embedding space of the StyleGAN face generator. Multiple evolutionary strategies are compared, . . .\"}"}
{"id": "emnlp-2023-main-76", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| System     | Readability | Faithfulness |\\n|------------|-------------|--------------|\\n| Bartarx    | 15.21       | 15.33        | 11.71 | 17.27 | 47.90 | 80.12 | 69.95 |\\n| BartSciT   | 15.40       | 13.70        | 10.74 | 17.36 | 36.54 | 28.62 | 22.77 |\\n| Bartmeta   | 15.22       | 13.43        | 10.66 | 17.21 | 36.91 | 28.33 | 25.30 |\\n| Bartplan   | 15.35       | 13.55        | 11.03 | 17.59 | 38.16 | 28.54 | 28.96 |\\n| Alpaca     | 12.21       | 13.82        | 11.00 | 14.04 | 38.00 | 67.97 | 48.76 |\\n| FlanT5-large | 15.12     | 16.36        | 11.92 | 16.97 | 44.36 | 73.76 | 63.26 |\\n| GPT-3.5-Turbo | 14.68    | 16.52        | 11.29 | 16.03 | 46.51 | 55.02 | 49.82 |\\n| PR Summary | 15.16       | 14.61        | 11.51 | 17.25 | 33.95 | 27.09 | 31.10 |\\n\\nTable 8: Supplementary performance results of systems in terms of readability (the lower the better) and faithfulness (the higher the better). QEval: QuestEval; Sumc: SummaC; ANLI: Adversarial NLI.\\n\\nFigure 7: Web interface used for the human study introduction in Section 4.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"university of illinois at urbana - champaign ( uiuc ) scientists have created a computer model of a simple brain network based on that of a sea slug, which was trained to find food. the asimov simulated slug was placed in a confined setting to randomly encounter food pellets that were alternately delicious or noxious, emitting a characteristic odor to either attract or repulse the slug. asimov was programmed to experience satiation as well as reward, and the addition of a rewarding but nutritionally empty pellet enticed the slug to pursue it to the exclusion of all else. although consuming this pellet caused satiation, that sense of fullness dissipated over time, with a homeostatic plasticity mechanism causing asimov to experience withdrawal. uiuc\u2019s rhanor gillette said, \u201cby watching how this brain makes sense of its environment, we expect to learn more about how real-world brains work.\u201d\\n\\nBart meta researchers at the university of illinois at urbana - champaign ( uiuc ) have developed an artificial intelligence (ai) system that simulates the addictive process, incorporating a sense of taste and disgust as well as learning mechanisms that drive addiction. the team used a virtual reality to create cyberslug, an agent that forages based on value and probability, with the artificial intelligence system learning to approach or avoid prey based on information such as nutritional value, comfort, and the potential of infection. asimov was exposed to stimuli that it did not like, such as choking, and would eventually feel the effects of withdrawal. uiuc\u2019s rhanor gillette said, \u201cthrough that simulation, we can see that ai has a very similar aesthetic sense to addiction and that the feelings that people with addiction have may be similar to those that other people have.\u201d\\n\\nBart plan a university of illinois at urbana - champaign (uiuc) scientist has developed an artificial intelligence (ai) system that simulates a drug-like addiction, with the goal of making it easy to visualize the effects of such addiction on a computer. the program gives the ai visual cues to expect to receive a high reward, but also warns the agent of the potential unpleasant feelings that may follow after consuming that reward. ekaterina gribkova\u2019s lab developed the artificial intelligence to replicate the neural circuitry involved in addiction, and showed that ai can be programmed to watch movies, listen to music, and act on visual cues. said gribkov, \u201cif you want to understand what addiction is like, you can start with the analogs and find out how the addictive processes are connected to each other.\u201d\\n\\nAesthetic decision-making in primitive generalist foragers, shedding light on the coevolutionary origins of aesthetics and addiction. According to a recent study published in the journal PLOS Biology, a rudimentary aesthetic sense can be found in the stimulus valuations and cost-benefit decisions made by primitive generalist foragers. This aesthetic sense is based on factors governing personal economic decisions such as incentive, appetite, and learning. The study found that the addictive process is an extreme expression of aesthetic dynamics. The researchers used an interactive, agent-based model called Asimov to reproduce a simple aesthetic sense from known neural relations of cost-benefit decision-making in foraging. In the presence of very high reward, an addiction-like process emerges. A drug-like prey provides extreme reward with no nutritive value, initiating high selectivity and prolonged cravings for the drug through reward learning.\"}"}
{"id": "emnlp-2023-main-76", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Researchers from MIT, Arizona State University, and the University of Massachusetts at Amherst have developed a new control system for legged robots. The system enhances the speed and agility of robots as they leap across gaps. The control algorithm processes real-time video input and translates it into instructions for bodily movement. The researchers combined elements of controllers that do not incorporate vision into a separate vision-handling module, trained the controller through reinforcement learning, and tested it in MIT's mini cheetah robot. The system outperformed other systems using a single controller, enabling successful crossing of 90% of physical terrains.\\n\\nResearchers at MIT have developed a quadruped robot that can successfully jump through wide gaps and across uneven terrain. This is a significant milestone toward the development of \\\"blind walking,\\\" which relies on the robot navigating without vision. The robot navigates by anticipating ground behavior and executing novel maneuvers, such as jumping over large gaps by planning beforehand. MIT's Pulkit Agrawal says, \\\"we're trying to create a system that can be adaptable to any type of environment, even though it has visual impairment.\\\" The robot uses a control scheme called depth-based impulse control (DIC), which provides flexibility in controlling the system near unseen obstacles.\\n\\nA quadruped robot designed by Tao Chen at MIT can successfully jump across small gaps. However, the robot would not know if it is safe to continue moving forward. The robot uses a depth-based impulse control system that avoids the traditional control scheme of arranging the robot in a straight line along the edge of the obstacle. Instead, the robot jumps depending on its momentum. As the robot approaches the edge, it senses an obstacle getting closer and uses light to predict how far it is from the robot and whether it can jump across the gap. To control the robot, Chen says, \\\"we need to provide a large range of possible control schemes that the robot can use to produce dynamic behavior, such as jumping, using reinforcement learning.\\\"\"}"}
