{"id": "emnlp-2023-main-24", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | Macro Avg. | N | SECOS | S1 | T5 | FLAN | mT5 | ByT5 |\\n|---------|------------|---|-------|----|----|------|------|------|\\n| pl      | 57.8       | - | 49.5  | 51.6| 63.6| 53.6 | 50.8 | -    |\\n| pt      | 62.1       | - | 55.7  | 71.3| 43.9| 53.7 | 56.8 | 49.3 |\\n| ro      | 64.2       | - | 66.0  | 73.4| 46.1| 53.7 | 58.1 | 38.5 |\\n| ru      | 62.3       | - | 48.5  | 71.1| 49.2| 59.3 | 62.3 | 43.5 |\\n| sk      | 70.7       | - | 60.8  | 73.9| 48.9| 63.0 | 61.0 | 51.2 |\\n| sq      | 87.6       | - | 71.1  | 79.7| 72.8| 74.1 | 74.6 | 38.6 |\\n| sv      | 86.8       | - | 71.1  | 81.2| 72.7| 79.6 | 72.5 | 38.6 |\\n| ta      | 90.7       | - | 66.0  | 80.7| 88.1| 81.5 | 74.2 | 88.9 |\\n| te      | 94.9       | - | 75.3  | 85.0| 95.1| 85.2 | 78.4 | 93.8 |\\n| th      | 95.9       | - | 93.0  | 96.9| 95.8| 96.6 | 97.6 | 93.4 |\\n| tr      | 57.8       | - | 50.8  | 50.8| 57.8| 50.8 | -    | -    |\\n| uk      | 62.1       | - | 55.7  | 71.3| 43.9| 53.7 | 56.8 | 49.3 |\\n| yi      | 64.2       | - | 66.0  | 73.4| 46.1| 53.7 | 58.1 | 38.5 |\\n| yo      | 62.3       | - | 48.5  | 71.1| 49.2| 59.3 | 62.3 | 43.5 |\\n\\nTable 10: Accuracy on languages is-pa.\\n\\n| Language | Macro Avg. | N | SECOS | S1 | T5 | FLAN | mT5 | ByT5 |\\n|---------|------------|---|-------|----|----|------|------|------|\\n| pl      | 32.5       | - | 5.0   | 5.3| 13.9| 46.8 | -    | -    |\\n| pt      | 41.4       | - | 42.6  | 0.0| 16.9| 15.4 | 29.0 | 21.1 |\\n| ro      | 45.4       | - | 48.5  | 0.0| 17.6| 12.8 | 33.0 | 28.9 |\\n| ru      | 26.4       | - | 26.8  | 21.5| 45.0| 30.8 | 21.9 | 11.8 |\\n| sk      | 65.9       | - | 56.1  | 61.7| 75.9| 64.1 | 33.6 | 25.0 |\\n| sq      | 78.7       | - | 68.6  | 0.0| 18.3| 23.1 | 59.3 | 65.8 |\\n| sv      | 82.9       | - | 68.9  | 82.6| 86.7| 79.5 | 61.9 | 60.5 |\\n| ta      | 90.5       | - | 81.2  | 83.9| 91.7| 84.6 | 73.5 | 80.3 |\\n| te      | 80.1       | - | 91.3  | 99.3| 74.0| 77.3 | 80.3 | 81.9 |\\n| th      | 90.0       | - | 92.1  | 97.2| 88.0| 95.5 | 84.5 | 85.5 |\\n| tr      | 82.1       | - | 83.6  | 88.8| 72.1| 86.4 | 56.0 | 71.1 |\\n| uk      | 78.7       | - | 68.9  | 100| 91.4| 83.7 | 100 | 84.6 |\\n| yi      | 90.0       | - | 96.9  | 97.1| 91.9| 91.9 | 91.9 | 91.9 |\\n| yo      | 90.0       | - | 96.4  | 98.1| 82.2| 96.7 | 98.3 | 96.7 |\\n\\nTable 11: Accuracy on languages pl-yo.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models\\n\\nBenjamin Minixhofer 1 Jonas Pfeiffer\u2020 2 Ivan Vuli\u0107\u2020 1\\n\\n1 University of Cambridge\\n2 Google DeepMind\\n\\nAbstract\\n\\nWhile many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and non-compound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the first stage, while the second, supervised learning stage optionally fine-tunes the model on the annotated Wiktionary data. Our self-supervised models outperform the prior best unsupervised decompounding models by 13.9% accuracy on average. Our fine-tuned models outperform all prior (language-specific) decompounding tools. Furthermore, we use our models to leverage decompounding during the creation of a subword tokenizer, which we refer to as CompoundPiece. CompoundPiece tokenizes compound words more favorably on average, leading to improved performance on decompounding over an otherwise equivalent model using SentencePiece tokenization.\\n\\n1 Introduction\\n\\nDecompounding is the task of separating compound words into their single word constituents. Decompounding is used in user-facing tools such as dictionaries and morphological analyzers (Altinok, 2018). Historically, it has also been widely used as a preprocessing step for other NLP tasks, e.g. for information retrieval (Monz and De Rijke, 2002; Braschler and Ripplinger, 2004), automatic speech recognition (Adda-Decker and Adda, 2000) and machine translation (Koehn and Knight, 2003).\\n\\nDecompounding can come in two similar yet different task formats: (i) compound segmentation and (ii) compound normalization (Ziering and van der Plas, 2016). Compound segmentation is the task of segmenting a word into its compound constituents, while preserving its surface form (e.g. bridesmaid \u2192 brides + maid). Compound normalization is the task of recovering the base form of each compound constituent (e.g. bridesmaid \u2192 bride + maid).\\n\\nMost prior work on decompounding has focused on the few languages with excessively productive...\"}"}
{"id": "emnlp-2023-main-24", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"compound formation such as Finnish, German and Swedish (Koehn and Knight, 2003; Shapiro, 2016; Riedl and Biemann, 2016). However, compound words occur in a large, diverse number of languages (Vogel and Scalise, 2010). Yet, datasets which annotate compounds with their segmented or normalized form sparsely exist, even in languages with high compound usage. As the first contribution of this work, we aim to address this issue by introducing a dataset of 255k compound words and their normalized form as well as non-compound words covering 56 languages obtained from Wiktionary (www.wiktionary.org).\\n\\nUsing our dataset, we then find that large language models (LLMs), which typically rely on subword-based tokenization (Sennrich et al., 2016; Kudo and Richardson, 2018), struggle with decompounding, as illustrated in Figure 1. Performance is especially low for compounds where subword boundaries do not coincide with compound constituent boundaries; we term compounds with this property \u2018hard\u2019 compounds (Figure 2).\\n\\nIn order to create a more effective decompounding model, we then formulate compound segmentation and normalization as a sequence-to-sequence learning task (Sutskever et al., 2014) and train a byte-level ByT5 model (Xue et al., 2022) using a two-stage framework. In the first stage, we use a novel self-supervised hyphen-prediction objective to learn compound segmentation without any labeled data. In the second stage, we turn the model into a compound normalization model via supervised training on our Wiktionary data. In addition, we introduce a procedure to predict the segmentation of any compound word based on its normalized form, effectively making compound segmentation a subtask of normalization. Finally, we demonstrate that decompounding has real-world applications by investigating compound segmentation for language model tokenization. We apply compound segmentation as pretokenization during training of a SentencePiece tokenizer (Kudo and Richardson, 2018), which results in fewer hard compounds while incurring no extra cost during training and inference of the language model (i.e. the only extra cost occurs during creation of the tokenizer).\\n\\nOur Stage 1 models outperform the best prior unsupervised models by 13.9% accuracy on average, while our (supervised) Stage 2 models outperform all prior language-specific decompounding tools. Furthermore, a model trained with a CompoundPiece tokenizer achieves a 5.5% improved performance on compound normalization over an otherwise equivalent SentencePiece model.\\n\\n**Contributions.**\\n\\n1) We introduce a dataset for decompounding of 255k words across 56 languages obtained from Wiktionary.\\n\\n2) We show that a byte-level language model can efficiently decompound words via a two-stage training framework, whereas current subword-based LLMs fall short.\\n\\n3) We present a way to improve subword tokenization by performing compound segmentation during creation of the tokenizer.\\n\\n4) We make our code, models and dataset publicly available at github.com/bminixhofer/compoundpiece.\\n\\n**2 Related Work**\\n\\n**Decompounding.** Early work in decompounding used word frequency lists along with manually specified suffixes (e.g., a connective -s-) to segment and normalize German compounds (Langer, 1998; Koehn and Knight, 2003). Subsequently, multiple submissions to the Morpho Challenge in morphological segmentation (Kurimo et al., 2010) explicitly or implicitly made use of compound segmentation (Lignos, 2010; Virpioja et al., 2011). Later work replaced the fixed list of suffixes used in Koehn and Knight (2003) by learned morphological operations from parallel corpora (Macherey et al., 2011) or from pre-lemmatized corpora of non-compound words (Ziering and van der Plas, 2016). Another branch of work added more linguistic knowledge in the form of black- and white-lists to the paradigm of Koehn and Knight (2003), resulting in JWordSplitter (German) and nl-splitter (Dutch); this has only been done for a couple of languages due to its knowledge-intensive nature. CharSplit (Tuggener, 2016) achieves high performance for German by relying on the frequency of character n-grams appearing within the compound. While the approaches above use (at most) light supervision, there exist supervised approaches which learn directly from an annotated corpus of compounds and their constituents, along with optional auxiliary signals (Biemann et al., 2008; Alfonseca et al., 2008). In contrast, SECOS (Riedl and Biemann, 2016) is a fully unsupervised and language-agnostic method achieving competitive performance by using word embeddings along with word frequencies for semantic compound segmentation.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"**Figure 3:** Number of positive and negative examples across languages in the Wiktionary dataset.\\n\\n| Word Constituents | Language |\\n|-------------------|----------|\\n| \ufedf\ufbff\ufe8e \u06be\ufee2 (sibling) \u06be\ufee2 same) + \ufedf\ufbff\ufe8e (ancestor) | Persian |\\n| akiratis (horizon) akis (eye) + ratas (circle) | Lithuanian |\\n| \u0634\u06a9\u0631\u0627 (border) \u0634\u06a9 (limit) + apa (distance) | Kazakh |\\n| \ufe91\ufeae \u064d\u06a9\u064e\u062f (cashflow) \ufe91\ufeae (cash) + \ufe91\u064e\u064d\u0648 (stream) | Gujarati |\\n\\n**Figure 4:** Example words in the Wiktionary dataset.\\n\\n- **Relation to Morphological Segmentation.** Decompounding can be seen as a special case of morphological segmentation (Batsuren et al., 2022a). However, a large amount of work in morphological segmentation focuses on derivational and inflectional morphology (Cotterell et al., 2016; Faruqui et al., 2016; Cotterell et al., 2018; McCarthy et al., 2019; Goldman et al., 2022), which is reflected by datasets such as UniMorph (Batsuren et al., 2022b) and MorphyNet (Batsuren et al., 2021) annotating inflectional and derivational affixes, but not compound constituents. The SIGMORPHON-2022 Shared Task (Batsuren et al., 2022a, SMST 2022) breaks this pattern by providing a dataset for segmentation into compound constituents in addition to inflectional and derivational affixes. We improve on the SMST 2022 dataset by broadening coverage from 9 to 56 languages, as well as handling negatives (i.e., non-compounds) more carefully (\u00a73.1).\\n\\n**Decompounding Datasets.** Besides the SMST 2022 dataset, datasets for decompounding include AuCoPro (van Zaanen et al., 2014) for Dutch and Afrikaans, and the GermaNet dataset for German (Henrich and Hinrichs, 2011). Although there is a significant amount of work studying compound terms in languages with highly productive compound formation beyond German and Dutch, such as Finnish and Greek (Pollatsek et al., 2000; Lind\u00e9n and Pirinen, 2009; Koliopoulou, 2014; Shapiro, 2016; Virkkunen et al., 2018), to the best of our knowledge there exist no public datasets for decompounding in these languages (and beyond).\\n\\n**Linguistically Informed Tokenization.** Various studies have tried augmenting or replacing the 'linguistically uninformed' subword-tokenizers used in contemporary LMs (Devlin et al., 2019; Rafail et al., 2020, inter alia) such as SentencePiece (Kudo and Richardson, 2018) and BPE (Sennrich et al., 2016) with linguistic knowledge. Using manually constructed morphological analyzers before applying BPE (Pan et al., 2020) or after generation (Matthews et al., 2018) has led to improvements, but is limited by the availability (and quality) of morphological analyzers across many languages. Unsupervised morphological segmentation has not shown consistent improvements (Zhou, 2018; Salavera and Lignos, 2021; Domingo et al., 2023); see Mielke et al. (2021) for additional discussion.\\n\\n### 3 Methodology\\n\\n**3.1 Dataset Construction**\\n\\nWe use words categorized as compound terms on Wiktionary to create a dataset for decompounding. The information on Wiktionary allows associating compound terms with their corresponding normalized constituents. Since Wiktionary only annotates the top-level split,\\n\\nwe recursively split constituents into their smallest parts by checking if the top-level constituents are themselves compound words. Many prior decompounding tools do not evaluate performance on negative examples (i.e., non-compound words; Koehn and Knight, 2003; Riedl and Biemann, 2016; Tuggener, 2016) since most prior datasets do not contain any (Henrich\\n\\nFor instance, highwayman is segmented into highway + man instead of high + way + man.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It is not trivial to obtain negative examples from Wiktionary since a large amount of compound words are not categorized as such, leading to many false negatives. We solve this issue by using all normalized compound constituents as negative examples, since by definition the compound constituents can also appear on their own as non-compound words. Note that this way of obtaining negative examples is biased against words which never occur inside compounds; however, we found this to be a rather weak bias (Appendix E). We include every language with at least 100 words, leading to a dataset which covers 56 languages. The number of training examples is shown in Figure 3, example words in Figure 4. We select up to 1,000 words (but at most 50% of total words) in every language as evaluation data. See Appendix A for further details concerning the dataset.\\n\\n3.2 Two-Stage Training\\n\\nTo overcome the problem of data scarcity in low-resource languages, we introduce a two-stage training procedure for creating dedicated decompounding models. In Stage 1, we train on the self-supervised objective of restoring hyphenation in words extracted from a large-scale Web corpus, leading to a self-supervised compound segmentation model. In Stage 2, we fine-tune the model on compounds and their normalized constituents from an annotated corpus in a supervised fashion, turning it into a compound normalization model.\\n\\nStage 1: Self-Supervised Compound Segmentation.\\nThis stage is motivated by the fact that hyphen characters can be seen as a high-precision, low-recall indicator of compound constituent boundaries, in the same way that newline characters are a high-precision, low-recall indicator of sentence boundaries (Minixhofer et al., 2023). We use this natural segmentation into compound constituents to create a compound segmentation model without requiring any labeled data. First, we obtain all words containing a hyphen plus an equivalent amount of non-hyphenated words from a corpus of unannotated text. Hyphens primarily have two uses: (1) as a compound boundary and (2) to indicate the word continues on the next line. We only want to retain hyphens when they function as compound boundaries, so we filter the instances of (2) by discarding all words where the hyphenated form of the word occurs $x \\\\leq e^{-6}$ times less frequent.\\n\\nWe strip all words of hyphens and train a seq2seq LM to predict the original (hyphenated) form of each word. We introduce a logit bias $b$ added to the logit of the token representing a hyphen to skew generation towards or away from hyphenation at inference time. Training on this data enables effective compound segmentation without relying on human annotations, as demonstrated later in \u00a75.\\n\\nStage 2: Supervised Compound Normalization.\\nIn the second stage, we improve upon the Stage 1 model by additional training on labeled data, where the inputs are individual compounds, and the target is to predict the normalized constituents of each compound, separated by a hyphen. Training exclusively on compound normalization allows using data from the collected Wiktionary dataset, which contains compound terms along with their normalized constituents across many languages, but does not contain compound segmentation annotations.\\n\\n3.3 Turning Normalization into Segmentation\\n\\nConsidering the scarcity of annotated compound segmentation data, it is infeasible to train a multilingual model directly on segmentation. Thus, we introduce a method to predict a segmentation given the normalized constituents. Let $x$ be a word of length $n$. In addition, we have $k$ normalized constituents of length $m$. Let $s$ be a sequence of length $n$.\\n\\n$$\\n\\\\ell(s, c) = \\\\min \\\\{ d(s, s'), d(s', c) \\\\}\\n$$\\n\\nInput\\nFind optimal segmentation\\nOutput\\nSegmentation : \\\\{aki, ratis\\\\}\\n\\nFigure 5: Turning compound normalization into segmentation by minimizing edit distance (\u00a73.3).\"}"}
{"id": "emnlp-2023-main-24", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"pound constituents $c = \\\\{c_1, ..., c_k\\\\}$ (e.g. predicted by the Stage 2 model). Our aim is to find boundaries $r = \\\\{r_0, ..., r_k\\\\}$, $r_0 = 0$, $r_k = n$ giving rise to the segmentation $s = \\\\{x[r_0: r_1], ..., x[r_k-1: r_k]\\\\}$. We approach this problem by minimizing the edit distance of each segment to its corresponding normalized constituent. This leads to an optimization problem where the cost $C(s)$ indicates the total edits needed to turn all segments into their corresponding normalized constituents:\\n\\n$$C(s) = k \\\\sum_{i=1}^{L} L(s_i, c_i).$$\\n\\nHere, $L$ is an edit distance metric such as Levenshtein distance (Levenshtein et al., 1966). The optimal segmentation $s^*$ is the segmentation with the minimal cost:\\n\\n$$s^* = \\\\arg\\\\min_s C(s).$$\\n\\nIn case of ties, we prefer segmentations with higher edit cost for segments with lower indices due to the preference for languages in our training set for suffixation over prefixation (Hammarstr\u00f6m, 2021).\\n\\nThere is a total of $\\\\binom{n}{k-1}$ possible segmentations, so solving the optimization problem via enumeration of all solutions is only feasible for short words (Figure 5). We introduce a more efficient search algorithm which is capable of quickly finding the optimal segmentation of long words by enumerating candidates in order of a lower bound on the edit distance in Appendix B. This method can be used to turn the normalization predictions of a model into segmentation. We also use it on the ground-truth normalization from Wiktionary, making it possible to approximate compound segmentation performance by comparing the segmentation corresponding to the ground-truth normalization to the segmentation produced by the model normalization predictions.\\n\\n### 3.4 Reducing Hard Compounds\\n\\nWe define hard compounds relative to a particular tokenizer as compound words where the constituent boundaries do not coincide with token boundaries set by the tokenizer. More formally, a compound word made up of $k$ constituents and $l$ subwords is hard if the constituent boundaries $r = \\\\{r_0, ..., r_k\\\\}$ are not a subset of the token boundaries $t = \\\\{t_0, ..., t_l\\\\}$, i.e. $r \\\\not\\\\subset t$.\\n\\nE.g., given $x = \\\\text{bridesmaid}$, $c = \\\\{\\\\text{bride}, \\\\text{maid}\\\\}$, we prefer the segmentation $\\\\{\\\\text{brides}, \\\\text{maid}\\\\}$ over $\\\\{\\\\text{bride}, \\\\text{smaid}\\\\}$, although their cost is equal.\\n\\nWe hypothesize that hard compounds may impair language model performance due to the non-trivial relation of subwords to the compound word. In contrast, in easy compounds the word is naturally decomposed into its constituents. The increased difficulty of hard compounds is apparent on the sequence-to-sequence compound segmentation task: for an easy compound, all tokens can be copied to the output (only the special separator tokens must be inserted). On the other hand, for hard compounds, the tokens change, requiring knowledge of the characters within each token.\\n\\nTokenizers where every possible constituent boundary is a token boundary trivially do not give rise to any hard compounds. This includes character-level (Clark et al., 2022; Tay et al., 2022b) as well as byte-level tokenizers (Xue et al., 2022). However, many contemporary language models use subword-based tokenizers to increase efficiency (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020). We propose a modification to subword tokenization to reduce the number of hard compounds while keeping the efficiency advantages.\\n\\nSubword tokenizers typically segment text into pre-tokens (e.g. by splitting on whitespace) before applying their subword tokenization algorithm (Mielke et al., 2021). We propose modifying pretokenization by applying compound segmentation in addition to splitting on whitespace. This modification is only done during creation of the tokenizer, thus incurring no additional cost once the tokenizer has been created. We refer to tokenizers created in this way as CompoundPiece tokenizers. The modified pretokenization tries to create more subwords which do not span compound constituent boundaries, thus decreasing the fraction of hard compounds (Figure 6). It aims to turn the dual-route model for computing the meaning of complex (compound) words proposed by Hofmann et al. (2021) into a single-route model which always computes the meaning of compounds from their constituent subwords, and never stores a compound word as a single subword.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Experimental Setup\\n\\n4.1 Data\\nWe obtain Stage 1 data by selecting all words containing a hyphen from a subset of the mC4 corpus (Xue et al., 2021) which results in \\\\( \\\\tilde{t} \\\\) 25M hyphenated words. As negative examples, we choose the \\\\( n \\\\) most common words from mC4 such that there is an equivalent amount of non-hyphenated and hyphenated words in every language. Regarding the Stage 2 data, see Section \u00a73.1 before.\\n\\n4.2 Training\\nWe train a decompounding model using a two-stage framework (\u00a73) covering 56 languages. We use ByT5 (Xue et al., 2022) as our main pretrained model and the main starting point since it directly ingests Unicode bytes instead of using subword tokenization, leading to zero hard compounds. We compare our approach against the subword-based T5 (Raffel et al., 2020), Flan-T5 (Chung et al., 2022) and mT5 (Xue et al., 2021) trained with the same two-stage framework. We use \\\\( t_{5x} \\\\) (Roberts et al., 2022) for training with a batch size of 512 and a maximum sequence length of 64 tokens, otherwise matching T5 pretraining (Raffel et al., 2020). The setup is the same for Stage 1 and Stage 2.\\n\\n4.3 Evaluation\\n\\nMetric.\\nWe measure performance via averaged accuracy, i.e., the ratio of examples which are entirely correctly segmented or normalized.\\n\\nDatasets.\\nBesides our new Wiktionary evaluation subset, we use the established datasets for particular languages: GermaNet (Henrich and Hinrichs, 2011), AuCoPro for Dutch (van Zaanen et al., 2014) as well the subset containing compound-only words across 6 languages from the SIGMORPHON 2022 Shared Task (Batsuren et al., 2022a).\\n\\nBaselines.\\nWe use SECOS as the main unsupervised baseline, as well as CharSplit, JWS and nl-splitter as baselines using different amounts of supervision. For the SIGMORPHON 2022 Shared Task dataset, we compare against the task winner, DeepSPIN-3 (Peters and Martins, 2022).\\n\\n7 We do not include words containing derivational or inflectional affixes since the type of morpheme is not specified, so it is not possible to distinguish between derivational/inflectional affixes and compound constituents. We also do not include root words since we found from manual inspection that >10% of root words are mislabeled, likely due to the difficulty of obtaining negative examples from Wiktionary (\u00a73.1).\\n\\nLanguages.\\nFor clarity of presentation, we present results on Danish, German, English, Spanish, Estonian, Greek, Persian, Finnish, Hungarian, Kazakh, Latvian, Dutch, Polish and Swedish as a linguistically diverse subset of languages with productive compound formation in the main paper. For the full evaluation across all languages, see Appendix C.\\n\\n5 Results and Discussion\\nMain compound segmentation results are shown in Table 1. For the self-supervised models, we choose the logit bias \\\\( b = 3 \\\\) to bias generation towards hyphenated words.\\n\\n8 ByT5 outperforms subword-based models by a large margin with an absolute 8.9% improvement over the best subword-based model after Stage 1 training, and a 3.7% improvement after Stage 2 training. Comparing models not trained on any annotated data, the self-supervised ByT5 outperforms SECOS on 13 out of 14 languages, and by 13.9% on average.\\n\\nWe further compare against language-specific and supervised methods in Table 2. Our ByT5-based model outperforms all prior methods on every dataset. Since GermaNet tests compound head segmentation (i.e., even if a word contains multiple constituents, it is only split into a head and a modifier) we count an example as correctly segmented if either the first constituent matches the modifier or the last constituent matches the head.\\n\\nEvaluating LLMs on Decompounding.\\nWe also evaluate in-context learning performance of multiple LLMs on compound segmentation. We use T5 models with 770M, 3B and 11B parameters (Raffel et al., 2020) as well as the UL2 model with 20B parameters (Tay et al., 2022a) since all of them use the same tokenizer, enabling performance comparisons on hard compounds across LLMs. We use the model versions fine-tuned on the Flan dataset collection (Chung et al., 2022), matching our prompt to the style of instructions in the Flan collection (Appendix D). Zero- to 16-shot results are shown in Figure 7. Although the LLMs perform non-trivially well on easy compounds, performance is close to zero (<3%) on hard compounds. Intriguingly, UL2 20B performs worse than Flan T5 XXL (11B), reversing the trend seen on other tasks (Tay et al., 2022a). All the LLMs perform considerably worse than our ByT5-based model; see Figure 1.\\n\\n8 Chosen among the set \\\\{0, 1, 2, 3, 4\\\\} to maximize performance on the English validation data.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model     | SECOS | S1 | Flan-T5 | mT5 | ByT5 | S1+S2 | T5 | Flan-T5 | mT5 | ByT5 | S1+S2 | T5 | ByT5 | S1+S2 |\\n|-----------|-------|----|---------|-----|------|-------|----|---------|-----|------|-------|----|------|-------|\\n|           | 30.0  | 55.3| 58.4    | 25.8| 75.6 | 86.3  | 58.4| 86.6    | 87.1| 92.2 | 93.3  | 89.0| 95.2 | 93.3  |\\n| Macro Avg.| 66.5  | 56.1| 58.5    | 38.8| 76.0 | 96.0  | 56.1| 95.3    | 94.1| 96.2 | 94.5  | 96.2| 98.3 | 96.2  |\\n| P         | 41.2  | 85.9| 89.1    | 79.7| 91.3 | 95.4  | 85.9| 95.5    | 97.4| 98.8 | 98.3  | 98.3| 99.7 | 98.8  |\\n| N         | 29.0  | 29.0| 37.0    | 18.6| 51.6 | 77.8  | 29.0| 80.9    | 83.2| 92.6 | 95.1  | 95.1| 100  | 100   |\\n|           | 5.3   | 0.0 | 0.0     | 2.1 | 40.9 | 0.0   | 0.0| 59.0    | 62.8| 86.1 | 95.4  | 95.4| 100  | 100   |\\n|           | 1.4   | 31.6| 33.0    | 3.9 | 20.9 | 59.0  | 31.6| 59.0    | 62.8| 86.1 | 95.4  | 95.4| 100  | 100   |\\n|           | 53.1  | 48.6| 53.4    | 24.1| 52.7 | 98.2  | 48.6| 87.3    | 98.3| 98.8 | 98.3  | 98.3| 100  | 100   |\\n|           | 38.8  | 16.9| 17.6    | 18.8| 70.0 | 89.1  | 16.9| 68.2    | 92.4| 98.6 | 98.3  | 98.3| 100  | 100   |\\n|           | 5.0   | 29.6| 41.7    | 45.0| 75.9 | 75.9  | 29.6| 68.2    | 92.4| 98.6 | 98.3  | 98.3| 100  | 100   |\\n|           | 13.9  | 44.9| 44.8    | 20.2| 41.7 | 75.9  | 44.9| 93.6    | 97.5| 98.6 | 98.3  | 98.3| 100  | 100   |\\n|           | 46.8  | 36.1| 40.3    | 32.9| 57.2 | 84.1  | 46.8| 77.4    | 94.3| 97.5 | 98.3  | 98.3| 100  | 100   |\\n|           | 22.2  | 53.1| 56.5    | 21.9| 51.8 | 90.0  | 22.2| 89.4    | 97.3| 98.6 | 98.3  | 98.3| 100  | 100   |\\n|           | 32.2  | 39.8| 42.9    | 30.9| 64.8 | 85.4  | 32.2| 69.5    | 97.9| 98.6 | 98.3  | 98.3| 100  | 100   |\\n\\nTable 1: Accuracy on compounds (Positives=P), non-compound words (Negatives=N) and across all examples. We report scores of SECOS as baseline, as well as Stage 1 training only (S1) and Stage 1 plus Stage 2 training (S1+S2).\\n\\nFigure 7: Few-shot in-context learning performance of LLMs on easy positives, hard positives, negatives and across all examples. Hard negatives are the same across all LLMs since they use the same tokenizer.\\n\\nReducing Hard Compounds via Compound-Piece.\\n\\nTo evaluate our method of reducing the number of hard compounds in subword-based language models (\u00a73.4), we train Compound-Piece models in two configurations: (i) multilingual tokenizers across all 56 languages and (ii) separate monolingual tokenizers for every language. For the multilingual tokenizers, we sample languages with \\\\( p(L) \\\\propto |L|^\\\\alpha \\\\) where \\\\( p(L) \\\\) is the probability of sampling text from a language \\\\( L \\\\) with \\\\( |L| \\\\) texts as in prior work (Conneau et al., 2020). We use a subsample of 10M texts from the mC4 corpus (Xue et al., 2021) with \\\\( \\\\alpha = 0.2 \\\\). The vocabulary size is 250k for the multilingual and 32k for the monolingual tokenizers, following prior work (Rust et al., 2021; Conneau et al., 2020).\\n\\nWe use our fine-tuned ByT5 model for train-time pretokenization into compound constituents and SentencePiece (Kudo and Richardson, 2018) with Unigram LM (Kudo, 2018) as the subword tokenization applied after pretokenization. As a baseline, we train SentencePiece tokenizers with pretokenization into words (split by whitespace) on the same data. Table 3 shows the percentage of hard compounds for every tokenizer. Compound-Piece reduces the number of hard compounds from 27.1% \u2192 9.7% on average in the monolingual case. In the multilingual case, there is a less marked improvement.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison against supervised and rule-based baseline models. We use the subset of compound-only words from the Sigmorphon Shared Task (SMST) 2022 data which covers 7 languages (Batsuren et al., 2022a).\\n\\n| Language | Multilingual | Monolingual |\\n|----------|--------------|-------------|\\n| Danish   | 15.5         | 12.4        |\\n| German   | 9.9          | 8.2         |\\n| English  | 7.5          | 4.6         |\\n| Spanish  | 29.0         | 18.7        |\\n| Estonian | 25.5         | 15.2        |\\n| Greek    | 39.9         | 23.1        |\\n| Persian  | 38.6         | 37.2        |\\n| Finnish  | 25.1         | 20.3        |\\n| Hungarian| 13.8         | 10.1        |\\n| Kazakh   | 14.4         | 9.0         |\\n| Latvian  | 20.2         | 16.1        |\\n| Dutch    | 12.8         | 10.2        |\\n| Polish   | 45.7         | 33.1        |\\n| Swedish  | 13.9         | 12.5        |\\n\\nMacro Avg. | 22.3 | 16.5 |\\n\\nTable 3: Percentage of hard compounds after segmentation with different tokenizers. SentencePiece (SPM) and CompoundPiece (CPM) tokenizers are trained on text in all 56 languages (Multilingual) and for every language separately (Monolingual). The improvement of 23.2% \u2192 16.5% may be because tokens from different languages interfere with the segmentation of any given word. We test this hypothesis by computing plausible token origins for tokens in the multilingual tokenizer. This is done by checking which monolingual tokenizers also contain the token in their vocabulary, and ordering the result by unigram token probability. Examples are shown in Table 4.\\n\\nTable 4: Example compound words which are easy for the monolingual but hard for the multilingual CompoundPiece tokenizer. \\\"_\\\" indicates whitespace.\\n\\n| Word | Segmentation | Origin Language |\\n|------|--------------|-----------------|\\n| tugboat | _tug , boat _tu , gbo , at | es, sk, it |\\n| mindstate | _mind , state _mindst | da, it, et, en |\\n| coatrack | _coat , rack _coa , track _coa | gl, ro |\\n\\nTable 5: Accuracy of our multilingual T5 models trained with SentencePiece (SPM-T5) and CompoundPiece (CPM-T5) on segmentation and normalization.\\n\\n| Language | Segmentation Normalization |\\n|----------|----------------------------|\\n| Danish   | SPM-T5 77.8, CPM-T5 65.5 |\\n| German   | SPM-T5 81.0, CPM-T5 61.5 |\\n| English  | SPM-T5 84.9, CPM-T5 82.9 |\\n| Spanish  | SPM-T5 75.2, CPM-T5 50.1 |\\n| Estonian | SPM-T5 78.6, CPM-T5 55.1 |\\n| Greek    | SPM-T5 70.6, CPM-T5 47.1 |\\n| Persian  | SPM-T5 58.2, CPM-T5 46.6 |\\n| Finnish  | SPM-T5 72.8, CPM-T5 59.0 |\\n| Hungarian| SPM-T5 76.2, CPM-T5 73.3 |\\n| Kazakh   | SPM-T5 72.9, CPM-T5 59.0 |\\n| Latvian  | SPM-T5 75.2, CPM-T5 53.5 |\\n| Dutch    | SPM-T5 78.2, CPM-T5 60.9 |\\n| Polish   | SPM-T5 65.8, CPM-T5 42.6 |\\n| Swedish  | SPM-T5 76.2, CPM-T5 61.0 |\\n\\nMacro Avg. | SPM-T5 74.6, CPM-T5 58.4 |\\n\\nTable 6: Ablation studies on not filtering hyphens-as-newline-indicator and on skipping Stage 1 training.\\n\\n| | Segmentation Normalization | \\n|--------|---------------------------|\\n| ByT5 (S1) | P N All | P N All |\\n| 50.8 | 82.5 | 66.6 |\\n| 53.8 | 62.3 | 58.9 |\\n| ByT5 (S1+S2) | 80.9 | 98.0 | 89.8 |\\n| S1 | 79.3 | 97.3 | 88.6 |\\n\\nTo more thoroughly evaluate our tokenization, we train multilingual T5 models using SentencePiece and CompoundPiece. We use the same sampling ratio ($\\\\alpha = 0.2$) of mC4 as for creating the tokenizer, but instead use a subset of 500M texts. We match the architecture and the pretraining setup of the mT5-base model, but train for a total of 350 million steps.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We evaluate the model on the decompounding task. Results are shown in Table 5.\\n\\nAblation Studies. We quantify the impact of the most significant design choices of our model in Table 6. Although filtering hyphens-as-newline-indicator (\u00a74.1) removes only 300k words (<1%) from the pretraining data, it increases performance on negatives by a large margin. Removing Stage 1 training (i.e., fine-tuning directly on the Wiktionary data instead) consistently decreases performance.\\n\\nConclusion\\n\\nWe systematically investigated word decompounding tasks of compound segmentation and normalization on a wide scale and in multilingual contexts. To this end, we introduced a dataset of 255k words including compounds and non-compounds across 56 languages from Wiktionary, which allowed us to evaluate performance of LLMs on decompounding. We found that current LLMs' performance is limited due to hard compounds which arise when subword token boundaries do not coincide with compound constituent boundaries. We then introduced dedicated models for decompounding which use byte-level tokenization to entirely avoid hard compounds. Finally, we used our decompounding models to create novel CompoundPiece tokenizers, keeping the efficiency advantages of subword tokenization while strongly decreasing the amount of hard compounds; this increases the performance of CompoundPiece models over comparable SentencePiece models on the decompounding tasks.\\n\\nLimitations\\n\\nAlthough self-supervised training in Stage 1 allows for decompounding without any annotated training data, Stage 2 training is limited to languages with sufficient entries in Wiktionary: this excludes extremely low-resource languages. Furthermore, due to computational constraints we have not trained larger models using CompoundPiece tokenization; hence we are unable to report on its benefits at larger scales and on tasks besides decompounding.\\n\\nAcknowledgements\\n\\nIvan Vuli\u0107 is supported by a personal Royal Society University Research Fellowship \u2018Inclusive and Sustainable Language Technology for a Truly Multilingual World\u2019 (no 221137; 2022\u2013). Research supported with Cloud TPUs from Google\u2019s TPU Research Cloud (TRC). We thank Sebastian Ruder and Srini Narayanan for helpful feedback on a draft of this paper.\\n\\nReferences\\n\\nMartine Adda-Decker and Gilles Adda. 2000. Morphological decomposition for asr in german. In Workshop on Phonetics and Phonology in ASR, Saarbr\u00fccken, Germany, pages 129\u2013143.\\n\\nEnrique Alfonseca, Slaven Bilac, and Stefan Pharies. 2008. Decompounding query keywords from compounding languages. In Proceedings of ACL-08: HLT, Short Papers, pages 253\u2013256, Columbus, Ohio. Association for Computational Linguistics.\\n\\nDuygu Altinok. 2018. Demorphy, german language morphological analyzer. arXiv preprint arXiv:1803.00902.\\n\\nKhuyagbaatar Batsuren, G\u00e1bor Bella, Aryaman Arora, Viktor Martinovic, Kyle Gorman, Zden\u02c7ek \u017dabokrtsk\u00fd, Amarsanaa Ganbold, \u0160\u00e1rka Dohnalov\u00e1, Magda \u0160ev\u02c7c\u00edkov\u00e1, Kate\u02c7rina Pelegrinov\u00e1, Fausto Giunchiglia, Ryan Cotterell, and Ekaterina Vylomova. 2022a. The SIGMORPHON 2022 shared task on morpheme segmentation. In Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 103\u2013116, Seattle, Washington. Association for Computational Linguistics.\\n\\nKhuyagbaatar Batsuren, G\u00e1bor Bella, and Fausto Giunchiglia. 2021. MorphyNet: a large multilingual database of derivational and inflectional morphology. In Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 39\u201348, Online. Association for Computational Linguistics.\\n\\nKhuyagbaatar Batsuren, Omer Goldman, Salam Khalifa, Nizar Habash, Witold Kiera \u00b4s, G\u00e1bor Bella, Brian Leonard, Garrett Nicolai, Kyle Gorman, Yustinus Ghanggo Ate, Maria Ryskina, Sabrina Mielke, Elena Budianskaya, Charbel El-Khaissi, Tiago Pimentel, Michael Gasser, William Abbott Lane, Mohit Raj, Matt Coler, Jaime Rafael Montoya Samame, Delio Siticonatzi Camaiteri, Esa\u00fa Zu-umaeta Rojas, Didier L\u00f3pez Francis, Arturo Once-vay, Juan L\u00f3pez Bautista, Gema Celeste Silva Villegas, Lucas Torroba Hennigen, Adam Ek, David Guriel, Peter Dirix, Jean-Philippe Bernardy, Andrey Scherbakov, Aziyana Bayyr-ool, Antonios Anastasopoulos, Roberto Zariquiey, Karina Sheifer, Sofya Ganieva, Hilaria Cruz, Ritv\u00e1n Karah\u00f3 \u02c7ga, Stella Markantonatou, George Pavlidis, Matvey Plugaryov, Elena Klyachko, Ali Salehi, Candy Angulo, Jatayu Baxi, Andrew Krizhanovsky, Natalia...\"}"}
{"id": "emnlp-2023-main-24", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Krizhanovskaya, Elizabeth Salesky, Clara Vania, Sarndana Ivanova, Jennifer White, Rowan Hall Maudslay, Josef Valvoda, Ran Zmigrod, Paula Czarnowska, Irene Nikkarinen, Aelita Salchak, Brijesh Bhatt, Christopher Straughn, Zoey Liu, Jonathan North Washington, Yuval Pinter, Duygu Ataman, Marcin Wolinski, Totok Suhardijanto, Anna Yablonskaya, Niklas Stoehr, Hossep Dolatian, Zahroh Nuriah, Shyam Ratan, Francis M. Tyers, Edoardo M. Ponti, Grant Aiton, Aryaman Arora, Richard J. Hatcher, Ritesh Kumar, Jeremiah Young, Daria Rodionova, Anastasia Yemelina, Taras Andrushko, Igor Marchenko, Polina Mashkovtseva, Alexandra Serova, Emily Prud'hommeaux, Maria Nepomniaschaya, Fausto Giunchiglia, Eleanor Chodroff, Mans Hulden, Miikka Silfverberg, Arya D. McCarthy, David Yarowsky, Ryan Cotterell, Reut Tsarfaty, and Ekaterina Vylomova. 2022b. UniMorph 4.0: Universal Morphology. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 840\u2013855, Marseille, France. European Language Resources Association.\\n\\nChris Biemann, Uwe Quasthoff, Gerhard Heyer, and Florian Holz. 2008. ASV toolbox: a modular collection of language exploration tools. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08), Marrakech, Morocco. European Language Resources Association (ELRA).\\n\\nMartin Braschler and B\u00e4rbel Ripplinger. 2004. How effective is stemming and decompounding for german text retrieval? Information Retrieval, 7:291\u2013316.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\\n\\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2022. Canine: Pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics, 10:73\u201391.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\\n\\nRyan Cotterell, Christo Kirov, John Sylak-Glassman, G\u00e9raldine Walther, Ekaterina Vylomova, Arya D. McCarthy, Katharina Kann, Sabrina J. Mielke, Garrett Nicolai, Miikka Silfverberg, David Yarowsky, Jonathan Eisner, and Mans Hulden. 2018. The CoNLL\u2013SIGMORPHON 2018 shared task: Universal morphological reinflection. In Proceedings of the CoNLL\u2013SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection, pages 1\u201327, Brussels. Association for Computational Linguistics.\\n\\nRyan Cotterell, Tim Vieira, and Hinrich Sch\u00fctze. 2016. A joint model of orthography and morphological segmentation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 664\u2013669, San Diego, California. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nMiguel Domingo, Mercedes Garc\u00eda-Mart\u00ednez, Alexandre Helle, Francisco Casacuberta, and Manuel Hermann. 2023. How much does tokenization affect neural machine translation? In Computational Linguistics and Intelligent Text Processing: 20th International Conference, CICLing 2019, La Rochelle, France, April 7\u201313, 2019, Revised Selected Papers, Part I, pages 545\u2013554. Springer.\\n\\nManaal Faruqui, Yulia Tsvetkov, Graham Neubig, and Chris Dyer. 2016. Morphological inflection generation using character sequence to sequence learning. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 634\u2013643, San Diego, California. Association for Computational Linguistics.\\n\\nOmer Goldman, David Guriel, and Reut Tsarfaty. 2022. (un)solving morphological inflection: Lemma overlap artificially inflates models' performance. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 864\u2013870, Dublin, Ireland. Association for Computational Linguistics.\\n\\nHarald Hammarstr\u00f6m. 2021. Measuring prefixation and suffixation in the languages of the world. In.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-24", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Christof Monz and Maarten De Rijke. 2002. Shallow morphological analysis in monolingual information retrieval for Dutch, German, and Italian. In Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001 Darmstadt, Germany, September 3\u20134, 2001 Revised Papers, pages 262\u2013277. Springer.\\n\\nYirong Pan, Xiao Li, Yating Yang, and Rui Dong. 2020. Morphological word segmentation on agglutinative languages for neural machine translation. arXiv preprint arXiv:2001.01589.\\n\\nBen Peters and Andre F. T. Martins. 2022. Beyond characters: Subword-level morpheme segmentation. In Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 131\u2013138, Seattle, Washington. Association for Computational Linguistics.\\n\\nAlexander Pollatsek, Jukka Hy\u00f6n\u00e4, and Raymond Bertram. 2000. The role of morphological constituents in reading Finnish compound words. Journal of Experimental Psychology: Human Perception and Performance, 26(2):820.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nMartin Riedl and Chris Biemann. 2016. Unsupervised compound splitting with distributional semantics rivals supervised methods. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 617\u2013622, San Diego, California. Association for Computational Linguistics.\\n\\nAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. 2022. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189.\\n\\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli\u0107, Sebastian Ruder, and Iryna Gurevych. 2021. How good is your tokenizer? on the monolingual performance of multilingual language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3118\u20133135, Online. Association for Computational Linguistics.\\n\\nJonne Saleva and Constantine Lignos. 2021. The effectiveness of morphology-aware segmentation in low-resource neural machine translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 164\u2013174, Online. Association for Computational Linguistics.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.\\n\\nNaomi Tachikawa Shapiro. 2016. Splitting compounds with ngrams. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 630\u2013640, Osaka, Japan. The COLING 2016 Organizing Committee.\\n\\nRobyn Speer. 2022. rspeer/wordfreq: v3.0.\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc.\\n\\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022a. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131.\\n\\nYi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2022b. Charformer: Fast character transformers via gradient-based subword tokenization. In International Conference on Learning Representations.\\n\\nDon Tuggener. 2016. Incremental coreference resolution for German. Ph.D. thesis, University of Zurich.\\n\\nMenno van Zaanen, Gerhard van Huyssteen, Suzanne Aussems, Chris Emmery, and Roald Eiselen. 2014. The development of Dutch and Afrikaans language resources for compound boundary analysis. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), pages 1056\u20131062, Reykjavik, Iceland. European Language Resources Association (ELRA).\\n\\nP\u00e4ivi Johanna Virkkunen, Juraj Simko, Heini Henriikka Kallio, and Martti Tapani Vainio. 2018. Prosodic features of Finnish compound words. In Proceedings of the 9th International Conference on Speech Prosody 2018. International Speech Communications Association.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Dataset Statistics\\n\\nStatistics for the training and validation splits of the Wiktionary dataset are shown in Table 7.\\n\\nB Efficient Segmentation Algorithm\\n\\nPseudocode of the brute-force algorithm to turn normalization into segmentation is shown in Algorithm 1. Since enumerating all possible segmentations is only feasible for short words (\u00a73.3) we introduce a more efficient algorithm (Algorithm 2) where candidate segmentations are ordered such that segmentations with constituents closest in length to the corresponding normalized constituents appear first. Assuming insertions and deletions both have a cost of one (as is the case in standard Levenshtein distance), constituents are thus sorted in increasing order of a lower bound on edit distance. The procedure can stop once the lower bound on edit distance reaches the cost of the best solution found so far since by that point it is impossible for a better solution to be found.\\n\\nNote that the normalization-to-segmentation problem is related to sequence partitioning (Manne and Sorevik, 1995; Han et al., 1992) where the aim is to find a partition of a sequence such that the maximum cost across partitions of some cost function is minimized. However, since our goal is to find the partitioning with the minimum aggregated cost, algorithms for conventional sequence partitioning are not applicable.\\n\\nC Results for All Languages\\n\\nSegmentation accuracy for all languages is shown in Tables 8-11.\\n\\nD LLM Prompts\\n\\nThe prompt used for LLM evaluations (\u00a75) is shown in Figure 8. The prompt was chosen among 10 prompts to maximize performance on Flan-T5 Large. For 2- to 16-shot results, we provide 50% positive (compound) and 50% negative (non-compound) examples in a random order.\\n\\nE Quantifying Negative Collection Bias\\n\\nWe conduct an experiment to measure the extent of the bias against words which do not occur inside compounds in our data collection methodology (\u00a73.1). In particular, we quantify the bias against long non-compound words, which usually would not occur inside compounds. We took a...\"}"}
{"id": "emnlp-2023-main-24", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Prompts used to evaluate LLM in-context learning compound segmentation performance.\\n\\nrandom sample of 500 words each from word frequency lists in English and German (Speer, 2022), manually removed compound words, and compared the length statistics of this (unbiased) sample of non-compounds to our non-compound dataset.\\n\\nWhile words in our non-compound dataset are indeed shorter on average (6.0 vs. 6.7 chars for English, 6.7 vs. 7.1 chars for German), with less than one character length difference on average, there is only a weak length bias in data collection.\\n\\nWe also found qualitatively that our non-compound dataset contains a wide variety of words since compounding is typically a process that can occur for many different root words.\\n\\nData:\\n\\n\\\\[ x, c \\\\]\\n\\nResult:\\n\\n\\\\[ s^* \\\\]\\n\\n\\\\[ k \\\\leftarrow |c|, n \\\\leftarrow |x|, r_0 \\\\leftarrow 0, r_n \\\\leftarrow n \\\\]\\n\\nbest_cost \\\\leftarrow \\\\infty\\n\\nfor \\\\( r_1, \\\\ldots, r_{n-1} \\\\) \\\\in \\\\([k-1]\\\\)\\n\\ndo\\n\\nCompute \\\\( s, C(s) \\\\) /* see \u00a73.3 */\\n\\nif \\\\( C(s) < \\\\text{best}_\\\\text{cost} \\\\)\\n\\n\\\\( s_{\\\\text{best}} \\\\leftarrow s \\\\)\\n\\n\\\\( \\\\text{best}_\\\\text{cost} \\\\leftarrow C(s) \\\\)\\n\\nend\\n\\nend\\n\\n\\\\( s^* \\\\leftarrow s_{\\\\text{best}} \\\\)\\n\\nAlgorithm 2: Segmentation by enumerating candidates in order of increased lower bound on edit distance.\\n\\n\\\\[ \\\\Delta = n - \\\\sum_i |c_i| \\\\]\\n\\nlower_bound \\\\leftarrow |\\\\Delta|\\n\\nwhile lower_bound < \\\\text{best}_\\\\text{cost}\\n\\ndo\\n\\noffsets = \\\\{ \\\\}\\n\\n\\\\[ x_i = k, \\\\sum_i |x_i| = \\\\text{lower}_\\\\text{bound}, \\\\sum_i x_i = \\\\Delta \\\\}\\n\\nlower_bound \\\\leftarrow lower_bound + 1\\n\\nfor \\\\( o_1, \\\\ldots, o_k \\\\in \\\\text{offsets} \\\\)\\n\\ndo\\n\\n\\\\( r_1, \\\\ldots, r_k - 1 = |c_1| + o_1, \\\\ldots, \\\\sum_{i=1}^{n-1} |c_i| + o_i \\\\]\\n\\nCompute \\\\( s, C(s) \\\\) /* see \u00a73.3 */\\n\\nif \\\\( C(s) < \\\\text{best}_\\\\text{cost} \\\\)\\n\\n\\\\( s_{\\\\text{best}} \\\\leftarrow s \\\\)\\n\\n\\\\( \\\\text{best}_\\\\text{cost} \\\\leftarrow C(s) \\\\)\\n\\nend\\n\\nend\\n\\nend\\n\\n\\\\( s^* \\\\leftarrow s_{\\\\text{best}} \\\\)\\n\\nAlgorithm 2: Segmentation by enumerating candidates in order of increased lower bound on edit distance.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language   | iso | #Positive | #Negative | Total | #Positive | #Negative | Total |\\n|------------|-----|-----------|-----------|-------|-----------|-----------|-------|\\n| Afrikaans  | af  | 326       | 193       | 519   | 322       | 197       | 519   |\\n| Azerbaijani| az  | 78        | 97        | 175   | 85        | 89        | 174   |\\n| Belarusian | be  | 32        | 47        | 79    | 40        | 38        | 78    |\\n| Bulgarian  | bg  | 71        | 89        | 160   | 68        | 92        | 160   |\\n| Bengali    | bn  | 301       | 334       | 635   | 304       | 331       | 635   |\\n| Catalan    | ca  | 220       | 218       | 438   | 219       | 218       | 437   |\\n| Czech      | cs  | 388       | 358       | 746   | 392       | 354       | 746   |\\n| Welsh      | cy  | 308       | 273       | 581   | 299       | 281       | 580   |\\n| Danish     | da  | 2145      | 1298      | 3443  | 644       | 356       | 1000  |\\n| German     | de  | 20743     | 7846      | 28589 | 708       | 292       | 1000  |\\n| Greek      | el  | 216       | 292       | 508   | 208       | 299       | 507   |\\n| English    | en  | 22896     | 6480      | 29376 | 759       | 241       | 1000  |\\n| Esperanto  | eo  | 1097      | 849       | 1946  | 559       | 441       | 1000  |\\n| Spanish    | es  | 433       | 401       | 834   | 417       | 417       | 834   |\\n| Estonian   | et  | 349       | 315       | 664   | 376       | 288       | 664   |\\n| Basque     | eu  | 102       | 98        | 200   | 98        | 101       | 199   |\\n| Persian    | fa  | 268       | 314       | 582   | 282       | 300       | 582   |\\n| Finnish    | fi  | 69948     | 13314     | 83262 | 848       | 152       | 1000  |\\n| French     | fr  | 149       | 135       | 284   | 135       | 148       | 283   |\\n| Dutch      | fy  | 92        | 85        | 177   | 90        | 86        | 176   |\\n| Irish      | ga  | 332       | 322       | 654   | 328       | 325       | 653   |\\n| Galician   | gl  | 70        | 79        | 149   | 80        | 69        | 149   |\\n| Gujarati   | gu  | 227       | 279       | 506   | 221       | 285       | 506   |\\n| Hebrew     | he  | 29        | 34        | 63    | 18        | 44        | 62    |\\n| Hindi      | hi  | 472       | 569       | 1041  | 478       | 522       | 1000  |\\n| Hungarian  | hu  | 5238      | 3162      | 8400  | 644       | 356       | 1000  |\\n| Armenian  | hy  | 872       | 745       | 1617  | 509       | 491       | 1000  |\\n| Indonesian | id  | 26        | 45        | 71    | 32        | 38        | 70    |\\n| Icelandic | is  | 2333      | 1603      | 3936  | 592       | 408       | 1000  |\\n| Italian    | it  | 452       | 352       | 804   | 437       | 366       | 803   |\\n| Georgian   | ka  | 137       | 156       | 293   | 149       | 143       | 292   |\\n| Kazakh     | kk  | 244       | 292       | 536   | 278       | 258       | 536   |\\n| Kirghiz    | ky  | 39        | 45        | 84    | 39        | 44        | 83    |\\n| Latin      | la  | 450       | 410       | 860   | 452       | 407       | 859   |\\n| Lithuanian | lt  | 65        | 94        | 159   | 76        | 83        | 159   |\\n| Latvian    | lv  | 244       | 249       | 493   | 223       | 269       | 492   |\\n| Malagasy   | mg  | 35        | 42        | 77    | 32        | 45        | 77    |\\n| Macedonian | mk  | 75        | 94        | 169   | 79        | 90        | 169   |\\n| Malayalam | ml  | 318       | 435       | 753   | 331       | 421       | 752   |\\n| Maltese    | mt  | 35        | 36        | 71    | 36        | 35        | 71    |\\n| Dutch      | nl  | 15184     | 5258      | 20442 | 761       | 239       | 1000  |\\n| Panjabi    | pa  | 24        | 34        | 58    | 19        | 39        | 58    |\\n| Polish     | pl  | 628       | 556       | 1184  | 523       | 477       | 1000  |\\n| Portuguese | pt  | 40        | 57        | 97    | 53        | 44        | 97    |\\n| Romanian   | ro  | 272       | 261       | 533   | 268       | 265       | 533   |\\n| Russian    | ru  | 753       | 718       | 1471  | 507       | 493       | 1000  |\\n| Slovak     | sk  | 26        | 28        | 54    | 25        | 29        | 54    |\\n| Albanian   | sq  | 124       | 113       | 237   | 109       | 127       | 236   |\\n| Swedish    | sv  | 8883      | 4172      | 13055 | 671       | 329       | 1000  |\\n| Tamil      | ta  | 656       | 710       | 1366  | 484       | 516       | 1000  |\\n| Telugu     | te  | 894       | 909       | 1803  | 507       | 493       | 1000  |\\n| Thai       | th  | 4287      | 2754      | 7041  | 614       | 386       | 1000  |\\n| Turkish    | tr  | 295       | 287       | 582   | 310       | 271       | 581   |\\n| Ukrainian  | uk  | 281       | 291       | 572   | 277       | 295       | 572   |\\n| Yiddish    | yi  | 162       | 218       | 380   | 176       | 203       | 379   |\\n| Yoruba     | yo  | 349       | 312       | 661   | 348       | 312       | 660   |\\n\\n| Total      |     | 164713    | 58757     | 223470 | 17539     | 13938     | 31477 |\\n\\nTable 7: Statistics of the Wiktionary dataset.\"}"}
{"id": "emnlp-2023-main-24", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 8: Accuracy on languages af-es.\\n\\n|     | SECOS | S1    | FLAN  | mT5   | ByT5  | S1+S2 |\\n|-----|-------|-------|-------|-------|-------|-------|\\n| N   | 7.4   | 4.1   | 20.2  | -     | -     | -     |\\n|     |       |       |       |       |       |       |\\n|     | 47.5  | 64.7  | 20.0  | 14.7  | 0.0   | 61.2  |\\n|     | 52.8  | 69.4  | 17.5  | 16.2  | 0.0   | 59.8  |\\n|     | 22.7  | 34.1  | 20.0  | 10.3  | 14.5  | 50.7  |\\n|     | 64.9  | 70.6  | 45.0  | 29.4  | 34.5  | 68.5  |\\n| SECOS| 83.9  | 75.3  | 22.5  | 35.3  | 0.0   | 70.8  |\\n| S1  | 84.8  | 74.1  | 22.5  | 33.8  | 0.0   | 75.3  |\\n| FLAN| 83.5  | 85.9  | 70.0  | 76.5  | 79.6  | 71.7  |\\n| mT5 | 90.4  | 89.4  | 80.0  | 79.4  | 91.1  | 81.7  |\\n| ByT5| 91.9  | 97.8  | 94.7  | 96.7  | 100   | 97.2  |\\n|     | 100   | 96.1  | 86.6  | 99.7  | 93.8  | -     |\\n|     | 96.4  | 91.7  | 83.9  | 88.7  | 85.5  | -     |\\n|     | 92.2  | 79.9  | 74.4  | 88.5  | 92.1  | -     |\\n|     | 95.4  | 78.2  | 82.3  | 79.4  | -     | -     |\\n|     | 90.9  | 77.6  | 83.2  | 61.1  | -     | -     |\\n|     | 90.2  | 87.1  | 81.5  | 83.0  | -     | -     |\\n\\nTable 9: Accuracy on languages et-id.\"}"}
