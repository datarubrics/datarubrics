{"id": "acl-2022-long-449", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A SPECT NEWS: Aspect-Oriented Summarization of News Documents\\n\\nOjas Ahuja\u00b9, Jiacheng Xu\u00b9, Akshay Gupta\u00b9, Kevin Horecka\u00b2, Greg Durrett\u00b9\\n\\n\u00b9The University of Texas at Austin\\n\u00b2Walmart NexTech\\n{ojas, jcxu}@utexas.edu, gdurrett@cs.utexas.edu\\n\\nAbstract\\n\\nGeneric summaries try to cover an entire document and query-based summaries try to answer document-specific questions. But real users' needs often fall in between these extremes and correspond to aspects, high-level topics discussed among similar types of documents. In this paper, we collect a dataset of realistic aspect-oriented summaries, A SPECT NEWS, which covers different subtopics about articles in news sub-domains. We annotate data across two domains of articles, earthquakes and fraud investigations, where each article is annotated with two distinct summaries focusing on different aspects for each domain. A system producing a single generic summary cannot concisely satisfy both aspects. Our focus in evaluation is how well existing techniques can generalize to these domains without seeing in-domain training data, so we turn to techniques to construct synthetic training data that have been used in query-focused summarization work. We compare several training schemes that differ in how strongly keywords are used and how oracle summaries are extracted. Our evaluation shows that our final approach yields (a) focused summaries, better than those from a generic summarization system or from keyword matching; (b) a system sensitive to the choice of keywords.\\n\\n1 Introduction\\n\\nRecent progress in text summarization (See et al., 2017; Liu and Lapata, 2019; Zhang et al., 2020a; Lewis et al., 2020) has been supported by the availability of large amounts of supervised data, such as the CNN/Daily Mail and XSum datasets (Hermann et al., 2015; Narayan et al., 2018), which provide a single, generic, topic-agnostic summary. However, a document often contains different aspects (Titov and McDonald, 2008; Woodsend and Lapata, 2012) that might be relevant to different users. For example, a political science researcher studying responses to earthquakes may want a summary with information about government-led recovery efforts and broader social impacts, not a high-level generic summary of what happened. Systems should be able to produce summaries tailored to the diverse information needs of different users. Crucially, these systems should be usable in realistic settings where a user is interested in vague aspects of the document, instead of a highly focused query.\\n\\nIn this work, we present a new dataset for evaluating single-document aspect-oriented extractive summarization which we call A SPECT NEWS. We derive subsets of examples from CNN/Daily Mail following certain topics, namely earthquakes and fraud reports. These domains are special in that the articles within them have several aspects which are repeatedly mentioned across articles and form coherent topics, e.g., impact on human lives of an earthquake. We ask annotators to select sentences relevant to such information needs, which correspond to imagined use cases. Interannotator agreement on full summaries is low due to the inherent subjectivity of the task, so rather than coming up with a consensus summary, we instead primarily evaluate against soft labels based on the fraction of annotators selecting a given sentence.\\n\\nTo benchmark performance on this dataset, we build a system that can summarize a document conditioned on certain aspect-level keywords without assuming annotated training data for those aspects. Since there are no large-scale supervised training sets suitable for this purpose, we explore methods to generate aspect-oriented training data from generic summaries. We compare these with past approaches (Frermann and Klementiev, 2019) on their ability to adapt to our aspect-oriented setting, which requires taking aspectual keyword inputs (as opposed to specific entities or queries) and being appropriately sensitive to these keywords.\\n\\nOur experiments on our A SPECT NEWS dataset\"}"}
{"id": "acl-2022-long-449", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. At least 42 people have died with hundreds more injured after a 6.2-magnitude earthquake hit Indonesia's Sulawesi island early Friday, according to Indonesia's Disaster Management Agency.\\n\\n2. The epicenter of the quake, which struck at 1:28 a.m. Jakarta time, was 6 kilometers (3.7 miles) northeast of the city of Majene, at a depth of 10 kilometers (6.2 miles), according to Indonesia's Meteorology, Climatology and Geophysics Agency.\\n\\n3. Thirty-four people died in the city of Mamuju, to the north of the epicenter, while another eight died in Majene.\\n\\n4. In Majene, at least 637 were injured and 15,000 residents have been displaced, according to\u2026\\n\\n7. Many people are still trapped under collapsed buildings, according to local search and rescue teams.\\n\\n8. Rescuers search for survivors at a collapsed building in Mamuju city in Indonesia.\\n\\n9. \u201cOur priority is saving victims who are still buried under the buildings,\u201d Safaruddin Sanusi, head of West Sulawesi's Communications and Information Department, told CNN Friday\u2026\\n\\n12. \u201cMost\u2026of the people in Mamuju city are now displaced. They are afraid to stay at their houses.\u201d\\n\\n15. \u201cWe need more extrication equipment and more personnel to work fast on saving victims trapped under the building.\u201d\\n\\nFigure 1: Examples of an earthquake-related article paired with extractive summaries from the CNN/DM dataset.\\n\\n\u201cGeneric\u201d represents the selection of a general purpose summarization model. \u201cGeo(graphy)\u201d (colored in green) and \u201cRecovery\u201d (colored in orange) indicate our aspects of interest for the summary. We highlight aspect-relevant phrases in the document.\"}"}
{"id": "acl-2022-long-449", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Prompts and keywords used for each of our two domains: Earthquake and Fraud. These represent prominent topics that users might be interested in.\\n\\n3.1 Target Domains\\nWe draw our datasets from the English-language CNN/Daily Mail summarization dataset (Hermann et al., 2015). We manually identified two domains, earthquakes and fraud, based on inspecting clusters of articles in these domains. These two domains are ideal for two reasons. First, they contain a significant number of on-topic articles (over 200) after careful filtering. Second, the articles in these domains are reasonably homogeneous: each article would often feature at least broadly similar information about an event, making aspect-based summarization well-defined in these cases.\\n\\nAlthough not completely universal, most earthquake articles refer to some information about each of two aspects here: geography (GEO) and recovery (RECV). Figure 1 shows an example of an earthquake-related article. Similarly, most fraud articles include information about the penalty (PEN) imposed for the fraud, and the nature (NATURE) of the fraud.\\n\\nTo retrieve our examples from these two domains, we first encode each article in CNN/DM corpus with a text encoder $E$. We adopt the Universal Sentence Encoder (Cer et al., 2018) for its efficiency and robustness. We create an exemplar sentence for each domain to serve as the target to retrieve the most relevant content. We describe the choice of exemplar sentences in Section A.2.\\n\\nWe measure the similarity of each candidate article $c$ and the exemplar sentence $s$ as the average of the cosine similarity between each of the candidate article's sentences $c_i$ and the exemplar, $\\\\text{sim}(c, s) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\cos(E(c_i), E(s))$.\\n\\nWe found this procedure to be more robust than simple keyword matching for retrieving articles with coherent aspects; for example, keyword matching for \u201cearthquakes\u201d resulted in returning articles primarily about tsunamis due to the imbalanced data distribution. By contrast, other domains like legislation were too heterogeneous: articles about passing a bill may focus on different aspects of a bill\u2019s journey, comments or quotes by elected officials, impact of the legislation, or other factors. We could not come up with a plausible unified information need for the sorts of articles available in this dataset, although our eventual system can be applied to such documents if given appropriate guidance.\"}"}
{"id": "acl-2022-long-449", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Specifying User Intents\\n\\nWith these two domains, we examine our dataset to derive aspects that simulate realistic information needs of users.\\n\\nTable 1 describes the domain, aspect, annotation prompt and keywords used for evaluation. For each domain, we establish two aspects. Each aspect must be well-represented in the corpus and easy to understand by both readers and annotators. The authors annotated these aspects based on inspection of the articles and brainstorming about user intents based on scenarios. For example, the penalty scenario was motivated by a real use case derived from the authors' colleagues investigating reporting of wrongdoing in news articles at scale, where summarization can be used to triage information.\\n\\n3.3 Crowdsourcing\\n\\nFinally, to construct actual extractive summaries for evaluation in these domains, we presented the user intents to annotators on Amazon Mechanical Turk. An annotator is shown a description of intent from Table 1 along with an article and is asked to identify a few sentences from the article that constitute a summary. They can rate each sentence on a scale from 0 to 3 to account for some sentences being more relevant than others. Their final summary, which they are shown to confirm before submitting, consists of all sentences rated with a score of at least 1. The exact prompt is shown in the Appendix.\\n\\nEach article was truncated to 10 sentences for ease of annotation. This assumption was reasonable for the two domains we considered, and the truncation approach has been used in See et al. (2017) without much performance degradation. We found that annotators were unlikely to read a full length article due to the inherent lead bias in news articles, so this also helped simplify the task. In order to maintain a high quality of annotations, we discard annotations that do not have at least a single selected sentence in common with at least a single other annotator on that sample. In practice, this only discards a handful of isolated annotations.\\n\\n3.4 Data Analysis & Annotator Agreement\\n\\nIn Table 2, we show the basic statistics of the collected dataset. We show the distribution of the number of sentences agreed upon by the annotators in Table 3. We see that annotators somewhat agree in most cases, but relatively few sentences are uniformly agreed upon by all annotators. Our initial pilot studies also showed that annotators are often unsure where the cutoff is for information to be notable enough to include in a summary. We therefore view this disagreement as inherent to the task, and preserve these disagreements in evaluation rather than computing a consensus summary.\\n\\nWe also compare the overlap between aspect-oriented annotation and generic extractive oracle derived from reference summaries from CNN/DM. In Table 4, the similarity and exact match between generic oracle summaries and the top 3 annotated sentences are fairly low, which means the annotated aspect driven summaries significantly differ from the standard extractive oracle.\\n\\n4 Building an Aspect-Oriented System\\n\\nOur aspect-oriented data collection works well to create labeled evaluation data, but it is difficult to scale to produce a large training set. Identifying suitable domains and specifying user intents requires significant human effort, and collecting real test cases at scale would require a more involved user study.\\n\\nWe build an aspect-oriented model without gold-labeled aspect-oriented training data. We do this by generating keywords for each article in CNN/DM, and training the model to learn the relationship between these keywords and a summary. Our system follows broadly similar principles to He et al. (2020), but in an extractive setting.\\n\\n4 The number of annotated examples for each aspect is 100, so the EM is an integer.\"}"}
{"id": "acl-2022-long-449", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Comparison of annotation labels and the non-query focused extractive oracle derived from reference summaries. We take the top-3 most common selected sentences from each aspect-oriented dataset and compute Jaccard similarity between the sets and the percentage of exact matches (EM).\\n\\n| Article | Summary |\\n|---------|---------|\\n| 1. Justine Greening has called for a major shake-up in the EU aid budget \u2013 as it emerged more than half the cash is squandered on relatively rich countries. | \\n| 2. The International Development Secretary challenged the basis of the \u00a310-billion-a-year budget, which channels cash to countries such as Turkey, Iceland and Brazil. | \\n| 3. She is pressing for a major shift in policy to target resources at the poorest countries. | \\n| 4. International Development Secretary Justine Greening today insisted aid money [\u2026] | \\n| 5. Miss Greening held talks with ministers from [\u2026] | \\n| 7. Miss Greening said: 'I don\u2019t think it\u2019s right that the EU still gives money to those countries higher up the [\u2026] | \\n| 9. Her intervention comes amid mounting concern about the EU aid budget, which [\u2026] total aid budget. [\u2026] |\"}"}
{"id": "acl-2022-long-449", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mixed Training\\n\\nWe explore a variant of training where we include training data with multiple variants of each original document from the dataset. Each document in the original dataset is mapped to two training samples, (1) a document without keywords and an unmodified oracle extractive summary, (2) a document with keywords and an oracle extractive summary using our modification procedure.\\n\\n4.2 Aspect-Oriented Model\\n\\nOur model is trained to predict a summary $S$ from a document-keywords pair $(D, K)$. Following BERT-SUM (Liu and Lapata, 2019), we fine-tune BERT (Devlin et al., 2019) for extractive summarization using our modified CNN/Daily Mail dataset with keywords. During training, we prepend a special token followed by the keywords to the original document, and use the modified oracle extractive summary as the gold outputs. During inference, the keywords are user-defined. This scheme is similar to He et al. (2020), but differs in that it is extractive.\\n\\nWe refer to this model, trained on our BERTScore references with the mixed training scheme, as AOSUMM.\\n\\n5 Experiments\\n\\nWe evaluate our model on the ASPECT NEWS dataset, comparing performance on aspect-oriented summarization to several baselines. We additionally experiment on the SPACE multi-document dataset (Angelidis et al., 2021) to provide a point of comparison on a prior dataset and show that our aspect-oriented method is competitive with other systems.\\n\\n5.1 Metrics\\n\\nOn ASPECT NEWS, we evaluate our model against the annotations using F1 score and ROUGE scores. It is impossible to achieve 100 F1 on this task due to inherent disagreement between annotators. One downside of F1 is that the model may be penalized even when the predicted sentence is very similar to the annotation, for this reason we also calculate ROUGE-1, -2, and -L scores (Lin, 2004).\\n\\nOn the SPACE dataset, the gold summaries are abstractive, so we only calculate ROUGE scores.\\n\\n5.2 Baselines & Competitor Models\\n\\nOn the SPACE corpus, we primarily focus on comparisons to quantized transformer (QT) (Angelidis et al., 2021) and CTRLSUM (He et al., 2020). For the ASPECT NEWS dataset, we benchmark our system against several other models and baselines which we now describe.\\n\\nHeuristic and QA Baselines\\n\\nKEYWORD takes the keywords described in Table 1 and greedily finds the first occurrence of each keyword in the input document. STR stands for the extractive oracle given the original reference summaries from CNN/DM. QA uses an ELMo-BiDAF question answering model (Seo et al., 2017; Peters et al., 2018) to find answers to synthetic questions \u201dWhat is {keyword}?\u201d for each keyword in the article. We select the sentence where the selected span is located as a sentence to extract. Each of these three technique is an extractive baseline where top sentences are selected.\\n\\nSummarization Baselines\\n\\nWe also compare our AOSUMM model against text summarization models, and query-focused models from previous work (retrained or off-the-shelf). (i) BERTSUM is a bert-base-cased extractive summarization model fine-tuned on CNN/DM (Liu and Lapata, 2019). (ii) BERT-FK shares the similar model architecture as BERTSUM but the training data comes from Frermann and Klementiev (2019). This data is constructed by interleaving several articles from the CNN/DM dataset together, extracting a coarse aspect from the original URL of one of the article, and setting the new gold summary to match that article. (iii) CTRLSUM is an off-the-shelf abstractive summarization model with the capability of conditioning on certain queries or prompts (He et al., 2020). (iv) Our model AOSUMM is based on BERTSUM and trained with techniques described in Section 4.\\n\\n5.3 Results\\n\\nASPECT NEWS\\n\\nThe experimental results on ASPECT NEWS are shown in Table 6. We find that our model outperforms our baselines across F1, ROUGE-1, ROUGE-2, and ROUGE-L scores. Significantly, our model generally outperforms key-word matching, demonstrating that semantic match information from training with the BERTScore oracle may be more useful than training with a ROUGE oracle in terms of reproducing annotators' judgments; recall that our model has not been trained on any ASPECT NEWS data and only on our synthetic data.\"}"}
{"id": "acl-2022-long-449", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Performance comparison of our model (AOS\\\\textsubscript{UMM}) versus baselines on the A\\\\textsubscript{SPECT}N\\\\textsubscript{EWS} dataset in both the earthquakes and fraud domains, using our geography (G\\\\textsubscript{EO\\\\textsubscript{ANNOT}}) and recovery (R\\\\textsubscript{ECV\\\\textsubscript{ANNOT}}) aspects for the former and penalty (P\\\\textsubscript{EN\\\\textsubscript{ANNOT}}), and nature (N\\\\textsubscript{ATURE\\\\textsubscript{ANNOT}}) aspects for the latter. The last row displays the maximum possible F\\\\textsubscript{1} score due to the disagreement of annotation.\\n\\nTable 7: ROUGE-L scores on the SPACE dataset of our model, AOS\\\\textsubscript{UMM}, versus BERT\\\\textsubscript{SUM}, CTRLS\\\\textsubscript{UM}, and quantized transformer (QT). Despite being an extractive model, our approach is competitive with strong query-focused or aspect-based models.\\n\\nWe note that our model's performance falls behind keyword matching some baselines in the geography aspect; this may be because the aspect is relatively homogeneous and can be easily approximated by keyword matching.\\n\\nThe results on all the aspects of the SPACE dataset are shown in Table 7. All of the aspect-oriented models exceed the performance of the generic summaries produced by BERT\\\\textsubscript{SUM}.\\n\\nWe also find that our model performs competitively with the quantized transformer (QT) (Angelidis et al., 2021) and CTRLS\\\\textsubscript{UM} (He et al., 2020) methods in this dataset. This is a surprising result: the AOS\\\\textsubscript{UMM} model is trained only with out-of-domain synthetic data, without access to the aspects prior to keywords specified at test time. Additionally, this is an abstractive task that we are applying an extractive model to.\\n\\n5.4 Ablations and Analysis\\n\\nKeyword Sensitivity\\n\\nWe evaluate the sensitivity of the model to different keywords. There is some overlap between the summaries returned by different keyword sets, as shown by the Jaccard similarity: some sentences may fit under both G\\\\textsubscript{EO\\\\textsubscript{ANNOT}} and R\\\\textsubscript{ECV\\\\textsubscript{ANNOT}}, or both P\\\\textsubscript{EN\\\\textsubscript{ANNOT}} and N\\\\textsubscript{ATURE\\\\textsubscript{ANNOT}}. Table 9 shows statistics of this, with the Fraud keyword sets yielding more similar summaries than those in Earthquake. We also confirm that using the keywords \u201cmatched\u201d to our setting outperforms using other sets of keywords in that domain (Table 8) suggesting that our model is picking summaries in a keyword-driven fashion.\\n\\nKeyword Intensity\\n\\nWe can vary the parameter $k$ controlling the number of times we append the keywords to the reference summary in order to generate the oracle extractive summary. We experiment with different level of intensity and show the result in Table 10. For most cases, $r = 1$ works well among all the datasets.\"}"}
{"id": "acl-2022-long-449", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Comparison of various levels of keyword intensity. We experiment with different level of keyword intensity for different oracle and train our AOS UMM model on these setting. We show the F1 of model's prediction and human annotation. The larger the r, the more keywords will be concatenated.\\n\\n6 Qualitative Evaluation & Comparison\\n\\nExtractive vs. Abstractive Comparison\\n\\nIt is difficult to directly compare the quality of summaries produced by an extractive model to those produced by an abstractive model. Abstractive models do not extract individual sentences from a summary so direct F1 evaluations cannot be compared in the manner of Table 6. ROUGE scores are a misleading comparison given that an extractive model will be better matched to our extractive ground truths. Therefore, we perform a qualitative analysis to determine the models' relative responsiveness to keywords and relative advantages and disadvantages.\\n\\n5 Keyword Sensitivity Comparison\\n\\nAlthough both CTRLS UMM and AOS UMM are sensitive to the choice of keywords and alter their summary in response to different keywords, CTRLS UMM often either hallucinates false information (Maynez et al., 2020) or simply rewords the prompt in the generated summary. We found that just under the GEO keywords in the earthquakes domain, out of 100 sample articles the bigram \\\"not known\\\" appears 27 times in relation to describing the location of the earthquake and \\\"not immediately known\\\" appears another 24 times. The CTRLS UMM model frequently rephrases the prompt rather than synthesizing information in the document related to the keywords into a cogent summary.\\n\\nComparison of Factuality of Output\\n\\nTable 11 shows one example of CTRLS UMM hallucination in the GEO case. Here, the model also rewords the prompt and inserts it into the summary without adding new information. Although such behavior may possibly perform well on automated metrics, it does not serve the purpose of query-focused summarization.\\n\\nExtractive summaries Table 11 shows that our model is able to successfully extract relevant parts of the document for our aspects under consideration. There are some features which may make these summaries hard to process in isolation, such as the quake in the first sentence; our method could be extended with prior techniques to account for anaphora resolution (Durrett et al., 2016).\\n\\n7 Conclusion\\n\\nIn this paper, we present a new dataset for aspect-oriented summarization of news articles called ASPECT NEWS. Unlike query-focused summarization datasets which are often driven by document specific facts or knowledge, this aspect-oriented task is designed to mimic common user intents in domain-specific settings. We present a keyword-controllable system trained on synthetic data and show that it can perform well on ASPECT NEWS without training on the target domains, performing...\"}"}
{"id": "acl-2022-long-449", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"better than a range of strong baseline methods.\\n\\nAcknowledgments\\nThis work was chiefly supported by funding from Walmart Labs and partially supported by NSF Grant IIS-1814522, a gift from Amazon, and a gift from Salesforce Inc. Opinions expressed in this paper do not necessarily reflect the views of these sponsors. Thanks to Ido Dagan for helpful discussion and suggestions about this paper, as well to the anonymous reviewers for their thoughtful comments.\\n\\nReferences\\nStefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021. Extractive opinion summarization in quantized transformer spaces. Transactions of the Association for Computational Linguistics (TACL), 9:277\u2013293.\\n\\nStefanos Angelidis and Mirella Lapata. 2018. Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3675\u20133686, Brussels, Belgium. Association for Computational Linguistics.\\n\\nTal Baumel, Raphael Cohen, and Michael Elhadad. 2014. Query-chain focused summarization. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 913\u2013922, Baltimore, Maryland. Association for Computational Linguistics.\\n\\nDaniel Matthew Cer, Yinfei Yang, Shengyi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, C. Tar, Yun-Hsuan Sung, B. Strope, and R. Kurzweil. 2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175.\\n\\nJanara Christensen, Stephen Soderland, Gagan Bansal, and Mausam. 2014. Hierarchical summarization: Scaling up multi-document summarization. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 902\u2013912, Baltimore, Maryland. Association for Computational Linguistics.\\n\\nHoa Trang Dang. 2005. Overview of duc 2005.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186.\\n\\nLi Dong, Shaohan Huang, Furu Wei, Mirella Lapata, Ming Zhou, and Ke Xu. 2017. Learning to generate product reviews from attributes. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 623\u2013632, Valencia, Spain. Association for Computational Linguistics.\\n\\nGreg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein. 2016. Learning-based single-document summarization with compression and anaphoricity constraints. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1998\u20132008, Berlin, Germany. Association for Computational Linguistics.\\n\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558\u20133567, Florence, Italy. Association for Computational Linguistics.\\n\\nLea Frermann and Alexandre Klementiev. 2019. Introducing document structure for aspect-based summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6263\u20136273, Florence, Italy. Association for Computational Linguistics.\\n\\nDan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing, pages 10\u201318, Boulder, Colorado. Association for Computational Linguistics.\\n\\nJunxian He, Wojciech Kry\u015bci\u0144ski, Bryan McCann, Nazneen Rajani, and Caiming Xiong. 2020. Ctrlsum: Towards generic controllable text summarization. arXiv preprint arXiv:2012.04281.\\n\\nKarl Moritz Hermann, Tom\u00e1s Ko\u010disk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and Comprehend. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS).\\n\\nKaren Sparck Jones. 1998. Automatic summarising: Factors and directions. In Advances in Automatic Text Summarization, pages 1\u201312. MIT Press.\\n\\nKundan Krishna and Balaji Vasan Srinivasan. 2018. Generating topic-oriented summaries using neural attention. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1697\u20131705, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\"}"}
{"id": "acl-2022-long-449", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\\n\\nYang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3730\u20133740, Hong Kong, China. Association for Computational Linguistics.\\n\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan Thomas Mcdonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of The 58th Annual Meeting of the Association for Computational Linguistics (ACL).\\n\\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).\\n\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium.\\n\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nAbigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u20131083, Vancouver, Canada. Association for Computational Linguistics.\\n\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In Proceedings of the International Conference on Machine Learning (ICML).\\n\\nOri Shapira, Ramakanth Pasunuru, Hadar Ronen, Mohit Bansal, Yael Amsterdamer, and Ido Dagan. 2021. Extending multi-document summarization evaluation to the interactive setting. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 657\u2013677, Online. Association for Computational Linguistics.\\n\\nBowen Tan, Lianhui Qin, Eric Xing, and Zhiting Hu. 2020. Summarizing text on any aspects: A knowledge-informed weakly-supervised approach. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6301\u20136309, Online. Association for Computational Linguistics.\\n\\nIvan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of ACL-08: HLT, pages 308\u2013316, Columbus, Ohio. Association for Computational Linguistics.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS).\\n\\nFuru Wei, Wenjie Li, Q. Lu, and Y. He. 2008. Query-sensitive mutual reinforcement chain and its application in query-oriented multi-document summarization. In Proceedings of the Special Interest Group on Information Retrieval (SIGIR).\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierrick Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's Transformers: State-of-the-art Natural Language Processing. arXiv preprint arXiv:1910.03771.\\n\\nKristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 233\u2013243, Jeju Island, Korea. Association for Computational Linguistics.\\n\\nYumo Xu and Mirella Lapata. 2020a. Abstractive query focused summarization with query-free resources. arXiv preprint arXiv:2012.14774.\\n\\nYumo Xu and Mirella Lapata. 2020b. Coarse-to-fine query focused multi-document summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3632\u20133645, Online. Association for Computational Linguistics.\\n\\nOuyang You, Wenjie Li, Sujian Li, and Qin Lu. 2011. Applying regression models to query-focused multi-document summarization. Information Processing & Management, 47:227\u2013237.\"}"}
{"id": "acl-2022-long-449", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020a. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. In Proceedings of the International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b. BERTScore: Evaluating text generation with BERT. In Proceedings of the International Conference on Learning Representations (ICLR).\"}"}
{"id": "acl-2022-long-449", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Training Details\\nFor all models, we split CNN/Daily Mail set into the standard 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs following See et al. (2017). We follow the training procedure for BERTSUM (Liu and Lapata, 2019) with modifications. We use the cased variant of bert-base-cased available through HuggingFace (Wolf et al., 2019) instead of uncased and do not lowercase the dataset during preparation. Our learning rate schedule follows Vaswani et al. (2017) with \\\\( lr = 2 \\\\times 10^{-3} \\\\cdot \\\\min(step - 0.5, step \\\\cdot warmup - 1.5) \\\\) where warmup = 10000.\\n\\nFor fine-tuning AOSUMM on the modified CNN/DM dataset, the training completes in 8 hours on a single NVIDIA Quadro RTX 8000.\\n\\nA.2 Exemplar Sentences\\nIn order to generate earthquake and fraud domain data we filter the CNN/DM dataset using similarity between latent representations of Universal Sentence Encoder (USE) (Cer et al., 2018). To find domain-related articles, we need to generate a sentence that is vague enough to match most in-domain articles but specific enough to exclude articles outside the domain. For earthquakes we found the sentence \\\"An earthquake occurred.\\\" to work well. We embedded this sentence with USE, and calculated distance in latent space to articles in CNN/DM. For the fraud dataset we use the similar sentence \\\"A fraud occurred.\\\" After inspecting the matches, we manually exclude articles that are outside the domain.\\n\\nA.3 Crowdsourcing\\nTo improve the quality of the data collected, we educate annotators with detailed instruction and user-friendly interface shown in Figure 2. We also manually sample and check the collected data.\\n\\nA.4 Oracle Derivation: BERTScore vs. ROUGE\\nIn Table 12 we show the performance improvement from replacing ROUGE-derived oracle labels with their BERTScore-derived counterparts. Using BERTScore (Zhang et al., 2020b) to obtain oracle extractive summaries for training data produces models that are significantly stronger than models trained on sentences selected by maximizing ROUGE score. We hypothesize this is because ROUGE score maximization essentially limits what the model learns to lexical matching, while BERTScore can score based on more abstract, semantic criteria.\\n\\nA.5 Mixed vs. Non-Mixed\\nWe compare models trained using the mixed technique against models trained without any augmentation, and find that the mixed technique generally provides some benefit, but inconsistently. In Table 13, the Mixed technique is effective on GEO, PEN, and NATURE, but not RECV. The small performance advantage of the mixed technique on PEN, NATURE and GEO can be attributed to this domain including both earthquake and fraud domain articles. On RECV, we use the ratio of fraud to earthquake articles, and set it to 1/1, which means that both types of articles are equally important. This setting allows the model to learn more robust features for both domains. However, we do not provide a direct comparison between ROUGE and BERTScore in this experiment, as we focus on the mixed vs. non-mixed technique. In future work, we will investigate the effect of using BERTScore-based or ROUGE-based oracle summaries on the mixed technique.\"}"}
{"id": "acl-2022-long-449", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"formance improvement from Mixed training may result from the model more easily learning the relationship between the keywords and the aspect-oriented summaries due to mixed examples. Another benefit of this technique is that a single model is capable of producing both generic and aspect-oriented summaries.\\n\\nA.6 SPACE Evaluation Details\\n\\nSeveral adjustments were made in order to run our model on the SPACE dataset. Since there are multiple input documents per summary, we first concatenated all documents together and treated the result as a single article. In order to process this large \\\"article\\\" with our model, we processed it in 512-token chunks using BERT in order to obtain representations from the [CLS] token, and then concatenated those representations together before passing them through the classification layer. This allowed selection of any sentence from any part of the input. The following keywords were used for each of the aspects in the dataset: (i) service, customer, staff, employee, assistance; (ii) location, room, region, hotel, place; (iii) food, dining, restaurant, dinner, meal; (iv) building, establishment, room, property, site; (v) cleanliness, sanitary, polished, clean, washed; (vi) rooms, chair, table, bed, wall.\"}"}
