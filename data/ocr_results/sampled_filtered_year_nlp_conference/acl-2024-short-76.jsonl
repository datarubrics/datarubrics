{"id": "acl-2024-short-76", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Getting Serious about Humor:\\nCrafting Humor Datasets with Unfunny Large Language Models\\n\\nZachary Horvitz1,*, Jingru Chen1,*, Rahul Aditya1, Harshvardhan Srivastava1, Robert West2, Zhou Yu1, Kathleen McKeown1\\n\\n1Columbia University, 2EPFL\\n{zfh2000, jc5898, ra3261, hs3447, zy2461}@columbia.edu, robert.west@epfl.ch, kathy@cs.columbia.edu\\n\\nAbstract\\nHumor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. We investigate whether large language models (LLMs) can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to \u201cunfun\u201d jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset where we find that GPT-4\u2019s synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.\\n\\n1 Introduction\\nDespite their success on natural language tasks, large language models (LLMs) struggle to reliably detect and explain humor (Baranov et al., 2023; G\u00f3es et al.; Hessel et al., 2023), and generate novel jokes (Jentzsch and Kersting, 2023). Notably, humans also struggle to write jokes; even at satirical newspapers like The Onion, less than 3% of proposed headlines are printed (West and Horvitz, 2019; Glass, 2008). In contrast, humans are able to consistently edit jokes to unfun them, an insight which motivated West and Horvitz (2019) to host a game where internet users competed to edit satirical headlines to make them serious. The resulting dataset, the Unfun Corpus (West and Horvitz, 2019), has been a valuable tool for advancing computational humor research. The dataset has been used to study properties of both humor and transformer architectures (West and Horvitz, 2019; Peyrard et al., 2021) and even to generate novel satire (Horvitz et al., 2020). Additionally, recent work has found that despite the relatively small size of the original dataset, humor detection models trained on Unfun data generalize remarkably well to other datasets, while models trained on other humor datasets perform poorly at classifying Unfun-edited data (Baranov et al., 2023).\\n\\nWhile useful contributions, Unfun and other aligned humor datasets (Hossain et al., 2019, 2020) are limited in both size and scope, due to their reliance on human annotation. We investigate the alternative of using LLMs to create datasets of aligned humorous and non-humorous texts. Previous work (Jentzsch and Kersting, 2023; Li et al., 2023; Veselovsky et al., 2023) has found that LLMs are limited in their ability to create synthetic humor. We take a new approach, exploiting the asymmetrical difficulty (Josifoski et al., 2023) of synthetic humor generation. Rather than only testing whether LLMs can generate humor, we explore their ability to edit away humor in existing jokes. Validating and harnessing this capability could provide large\\n\\n1Our code and datasets are available at https://github.com/zacharyhorvitz/Getting-Serious-With-LLMs.\"}"}
{"id": "acl-2024-short-76", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"paired datasets and support future work on improving humor detection and even generation. Our contributions include benchmarking against human-curated data in the Unfun corpus, where we find that LLMs like GPT-4 and GPT-3.5 (OpenAI, 2023, 2022) can (1) outperform humans at removing humor from texts and that (2) this ability can be harnessed to generate high quality synthetic data for training humor classifiers. While these models can also be prompted to modify unfunny headlines to craft satire, we find that this ability is more inconsistent and lags behind satirical writers. Finally, we consider a code-mixed English-Hindi humor dataset to evaluate whether GPT-4's \\\"unfunning\\\" ability generalizes to other domains and languages. We find that the resulting synthetic unfunny dataset is rated highly by bilingual annotators and poses challenging adversarial data for models trained on the original corpus.\\n\\n2 Getting Serious with Language Models\\n\\nWe first revisit the Unfun task and resulting dataset, but with language models as players.\\n\\n2.1 Unfun Dataset\\n\\nIn the original Unfun game (West and Horvitz, 2019), players were tasked with editing existing satirical headlines from The Onion to transform the original satire into corresponding serious headlines. For example (removing \\\"Delicious\\\"): \\\"Scientists Discover Delicious New Species\\\". Players were rewarded for preserving token-level similarity with the original satire and for crafting convincingly serious headlines that other players rated as real. The resulting dataset includes approximately 11K unfunned headlines, with a subset rated by players. We leverage Unfun pairs, of satirical headlines and their unfunned counterparts, to benchmark the performance of LLMs at editing humorous texts against humans. We include additional details on data preparation in Appendix A.1.1.\\n\\n2.2 Unfun Generation\\n\\nWe consider a few-shot setting (Brown et al., 2020), and provide LLMs with a short task description, along with a set of input-output exemplar pairs: (humorous text, serious text). Following Veselovsky et al. (2023), we encourage diversity in our synthetic data by sampling these exemplars from a subset of the existing pairs rated as high-quality by the original human players. For the unfunning task, we consider four popular LLMs: GPT-4 (OpenAI, 2023) and GPT-3.5-TURBO, along with MISTRAL-7B-INSTRUCT and MISTRAL-7B (Jiang et al., 2023). We also consider a lightweight alternative approach, ROBERTA-SWAP, that replaces low probability tokens using predictions from a ROBERTA masked language model (Liu et al., 2019). This approach is motivated by the Incongruity Theory of Humor (Hutcheson, 1750; Morreall, 2023), which associates humor with surprise, and previous work that has found humorous headlines to have higher perplexities (Peyrard et al., 2021).\\n\\nROBERTA-SWAP edits satirical headlines by iteratively performing token swaps at positions. At each selected position, the original token is replaced with the highest probability token predicted by the model at that masked time-step. The k swap positions are selected using the ratio between the probability of the original token and the probability assigned to the language model's prediction. Additional details on unfun generation are included in Appendix A.2.1.\\n\\n3 Unfun Evaluation\\n\\n3.1 Experimental Setup\\n\\nThe existing Unfun data enables comparison of human and LLM players, via both automatic and human evaluations. We first evaluate the quality of synthetically generated data through automated evaluation on the downstream task of Unfun detection, and then follow this with a human evaluation.\\n\\n3.1.1 Automatic Evaluations\\n\\nFirst, following recent work on synthetic data (Li et al., 2023; Veselovsky et al., 2023) we evaluate the data quality of outputs from LLMs by testing whether binary humor classifiers trained on the synthetic outputs can differentiate between actual humorous and unfunned headlines from the original Unfun dataset. We compare training on data from human players and actual satirical headlines to two configurations of synthetic data: [Synthetic unfun; Original satire] [Human unfun; Synthetic satire]. These two configurations enable comparing the \\\"unfunning\\\" and joke writing capabilities of LLMs. Additionally, we consider the alternative of using actual unrelated news headlines as non-humorous examples. Using data from each approach, we\"}"}
{"id": "acl-2024-short-76", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Automatic evaluations of synthetic Unfun data. We consider the two directions of editing away (Unfun) and editing in humor (Humor). We report median accuracies (and standard error) on a balanced holdout set (n = 750) over 5 seeds when fine-tuning MISTRAL (Jiang et al., 2023) and RoBERTA (Liu et al., 2019) humor classifiers.\\n\\n| Direction | Source   | Slightly Funny / Funny | Grammatical Coherence |\\n|-----------|----------|------------------------|-----------------------|\\n| Unfun     | ROBERTA- SWAP | 30% / 15%              | 93% / 86%             |\\n|           | MISTRAL  | 21% / 50%              | 100% / 96%            |\\n|           | INSTRUCT | 21% / 14%              | 100% / 96%            |\\n|           | GPT- -3.5| 51% / 23%              | 100% / 98%            |\\n|           | GPT- -4  | 49% / 21%              | 100% / 99%            |\\n| News Headlines | | 81% / 2%               | 99% / 93%             |\\n| Human Players | | 33% / 21%             | 94% / 92%             |\\n\\nTable 2: Human evaluations of synthetic Unfun data. We consider n = 100 samples per approach. We collect three annotations per example and assign labels by majority agreement.\\n\\n3.1.2 Human evaluations\\nTo perform our human evaluations, we recruited 10 university students as annotators, all of whom were American and native English speakers. Annotators were tasked with rating headlines as real/satire/neither. In the case of the \u201csatire\u201d label, we also task the annotators with rating funniness ([0 = not funny, 1 = slightly humorous, 2 = funny]). If the annotator selects \u201cneither\u201d, we ask them to rate the headline\u2019s grammaticality ({0, 1}) and coherence ({0, 1}). We gather three annotations for each sample and assign labels based on majority vote.\\n\\nWe include additional information on our human evaluations and annotation scheme in Appendix A.3 and C.1.\\n\\n3.2 Results\\nAutomatic Evaluations\\nTable 1 contains the automatic evaluations on the Unfun corpus. Notably, when validated on human data, humor classifiers trained on GPT-4\u2019s synthetic unfun data are very performant, incurring the smallest accuracy drop relative to human-edited training data (\u0394Mistral = -3.8% and \u0394RoBERTa = -2.8%). In contrast, classifiers trained with real news headlines incur the largest drop in accuracy (\u0394Mistral = -7.0% and \u0394RoBERTa = -5.0%).\"}"}
{"id": "acl-2024-short-76", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Human evaluations and edit distance of original and synthetic English-Hindi Tweet data (Khandelwal et al., 2018).\\n\\n| Source       | Edit Dist | Humor | Coherence |\\n|--------------|-----------|-------|-----------|\\n| Non-Humor    | 16.8%     | 92.8% |           |\\n| GPT-4 Unfuns | 6.6%      | 16.0% | 93.6%     |\\n| + GPT-4 Filter | 6.9%  | 3.6%  | 89.3%     |\\n\\nHuman Evaluations\\n\\nTable 2 displays the results from our human evaluations. All approaches for generating synthetic humor significantly underperform on Onion headlines on funniness and realness ratings ($p<0.05$). Notably, we do not observe a significant improvement between GPT-3.5 and GPT-4. In contrast, synthetic unfuns from both GPT-3.5 and GPT-4 were significantly more likely than human unfuns to be rated as real news headlines. They were also rated as similarly unfunny and more grammatical and coherent. Surprisingly, our simple ROBERTA-SWAP approach also performed comparably with Unfun players on funniness and real headline metrics, but underperformed on coherence. Together, these results indicate that current LM-based methods underperform satirical writers on humor generation, but can outperform human crowd-workers at editing away humor in satire to craft aligned datasets.\\n\\n4 Extending Unfun to Other Languages\\n\\nRecent work has found that GPT-4 exhibits strong multilingual capabilities (M\u00f8ller et al., 2023; Jiao et al., 2023; Ahuja et al., 2023). Motivated by these findings, we investigate whether its ability to edit away humor generalizes to other languages and forms of joke.\\n\\n4.1 Experimental Setup\\n\\nWe consider an existing corpus of code-mixed English-Hindi tweets, previously annotated as humorous or non-humorous (Khandelwal et al., 2018). Here, we prompt GPT-4 to unfun humorous tweets. To remove low quality results, we secondarily filter outputs that GPT-4 still classifies as humorous.\\n\\nWe provide additional details on dataset preparation in Appendix A.1.2 and English-Hindi unfun generation in A.2.\\n\\nWe perform a human evaluation with bilingual annotators who rated these unfunned outputs from GPT-4 alongside samples from the original dataset. We also run an automatic evaluation, testing the performance of humor classifiers trained with different proportions of synthetic non-humorous data. We evaluate on holdout synthetic data rated by the annotators as coherent and successfully non-humorous. For the humor classifier, we fine-tune an XLM-ROBERTA model (Conneau et al., 2020) previously fine-tuned on English-Hindi Twitter data (Nayak and Joshi, 2022).\\n\\n4.2 Results\\n\\nTables 3 and 4 contain the human evaluations and automatic results for English-Hindi data. GPT-4 edited texts were rated comparably to non-humorous human tweets despite being derived from humorous tweets, which were rated as humorous by our annotators (48%). Filtering with GPT-4 yielded a smaller sample (56/125) that was rated as much less humorous (3.6%). These results demonstrate that GPT-4 is able to reliably unfun English-Hindi tweets, but with more edits than American satirical headlines (6.6 vs 3.8). Additionally, unfunned data can provide a challenging adversarial dataset. In Table 4 we evaluate the performance of humor classifiers on human-vetted unfunned data. When trained on the original dataset, the classifier fails to generalize to the unfunned samples and performs poorly (23% accuracy). Incorporating synthetic training data improves this metric at a cost to accuracy on humorous examples in the original dataset. Together, these results provide evidence that the humor classifier relies on superficial features to identify humorous text, and that, even with fine-tuning, the model struggles to recognize synthetic unfunny data.\"}"}
{"id": "acl-2024-short-76", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Automatic evaluations with English-Hindi synthetic data. We report median accuracies (and standard error) on a holdout set from the original dataset ($n = 591$) and the human-vetted unfuns ($n = 97$). We also report median class-level accuracies for the original dataset.\\n\\n5 Discussion\\n\\nOur results indicate that current LLMs struggle to generate humor, but can outperform crowd-workers at editing away (or unfunning) humor. We hypothesize that maximum likelihood training, combined with autoregressive sampling techniques, does not endow models with the creative spark required for joke writing, and instead lends itself to making high probability, reasonable substitutions to replace incongruous twists. Our evaluations on code-mixed English-Hindi Twitter data indicate that, for GPT-4, this ability can impressively generalize to other languages and settings to create novel Unfun-like datasets. We are excited for future work that harnesses this capability and resulting data to improve humor detection and generation systems, and also to demystify fundamental properties of humor.\\n\\n6 Limitations\\n\\nWe consider two settings, English satirical headlines and code-mixed English-Hindi tweets. Humor practices and references vary by culture (Alden et al., 1993; Jiang et al., 2019), and we leave investigating cultural impacts on LLMs and humor to future work. In both of our evaluations, the subjectivity of humor presents a challenge for our evaluations (Warren et al., 2021). We see evidence of this in Table 3, where only 48% of tweets previously annotated as humorous were also rated as humorous by our annotators, and where 16% of non-humorous tweets were rated as humorous. This likely reflects differences in background knowledge and context between annotators. Additionally, we note that human Unfun players were incentivized to perform minimal edits, which may have affected their human evaluation metrics and lowered edit distances. On average, however, GPT-4 performs less than one additional word edit, and several approaches, including ROBERTA-REV, were performant with lower edit distances than human players. Another concern is data contamination (Sainz et al., 2023), and that a portion of the text from the Unfun corpus could have been trained on and memorized by the LLMs we evaluated. We investigate this concern in Appendix A.6. We note that our results on English-Hindi data show that GPT-4\u2019s abilities generalize to a dataset where these pairs do not already exist on the internet.\\n\\n7 Ethical Statement\\n\\nHumor brings joy to people and plays a critical role in building and maintaining social relationships (Basso, 1979). However, its importance presents a double-edged sword; offensive and hurtful humor can cause real harms, and reinforce prejudice (Benatar, 1999). As a result, with their widespread adoption, it will be paramount for AI systems to be more capable of identifying and appropriately navigating jokes. We believe that our work on benchmarking LLM humor abilities and building challenging detection datasets is an important step in this direction. However, one possible concern is that malicious actors could leverage our unfunning approach to circumvent existing safeguards. In our experimentation, we found numerous settings where GPT-4 refused to generate jokes for offensive topics, but had no trouble editing texts to remove humor and offensiveness. This could enable building large parallel datasets of (offensive-text, non-offensive counterparts) that could then be used to train models for offensive joke generation.\\n\\nAcknowledgements\\n\\nWe would like to thank Eric Horvitz for guidance that helped shape the direction of this work. We are also grateful to Nicholas Deas, Debasmita Bhattatcharya, and Maximillian Chen for their feedback. Additionally, we would like to extend our gratitude to Amith Ananthram, Samir Gadre, Fei-Tzin Lee, Matthew Toles, Elsbeth Turcan, Melanie Sub...\"}"}
{"id": "acl-2024-short-76", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nKabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Krithika Ramesh, Samuel C. Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, and Sunayana Sitaram. 2023. Mega: Multilingual evaluation of generative ai. ArXiv, abs/2303.12528.\\n\\nDana L. Alden, Wayne D. Hoyer, and Chol Lee. 1993. Identifying global and culture-specific dimensions of humor in advertising: A multinational analysis. Journal of Marketing, 57:64 \u2013 75.\\n\\nAlexander Baranov, Vladimir Kniazhevsky, and Pavel Braslavski. 2023. You told me that joke twice: A systematic investigation of transferability and robustness of humor detection models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13701\u201313715, Singapore. Association for Computational Linguistics.\\n\\nK.H. Basso. 1979. Portraits of \u2018the Whiteman\u2019: Linguistic Play and Cultural Symbols among the Western Apache. Cambridge University Press.\\n\\nDavid Benatar. 1999. Prejudice in jest: When racial and gender humor harms. Public Affairs Quarterly, 13(2):191\u2013203.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer. 2020. Unsupervised cross-lingual representation learning at scale.\\n\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms.\\n\\nIra Glass. 2008. Tough room.\\n\\nFabr\u00edcio G\u00f3es, Piotr Sawicki, Marek Grzes, Daniel Brown, and Marco Volpe. Is gpt-4 good enough to evaluate jokes?\\n\\nJack Hessel, Ana Marasovic, Jena D. Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. 2023. Do androids laugh at electric sheep? humor \u201cunderstanding\u201d benchmarks from the new yorker caption contest. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 688\u2013714, Toronto, Canada. Association for Computational Linguistics.\\n\\nZachary Horvitz, Nam Do, and Michael L. Littman. 2020. Context-driven satirical news generation. In Proceedings of the Second Workshop on Figurative Language Processing, pages 40\u201350, Online. Association for Computational Linguistics.\\n\\nNabil Hossain, John Krumm, and Michael Gamon. 2019. \u201cpresident vows to cut <taxes> hair\u201d: Dataset and analysis of creative text editing for humorous headlines. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 133\u2013142, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nNabil Hossain, John Krumm, Tanvir Sajed, and Henry Kautz. 2020. Stimulating creativity with funlines: A case study of humor generation in headlines.\\n\\nF. Hutcheson. 1750. Reflections Upon Laughter: And Remarks Upon the Fable of the Bees. Garland Publishing.\\n\\nSophie Jentzsch and Kristian Kersting. 2023. Chatgpt is fun, but it is not funny! humor is still challenging large language models.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b.\\n\\nTonglin Jiang, Hao Li, and Yubo Hou. 2019. Cultural differences in humor perception, usage, and implications. Frontiers in Psychology, 10.\\n\\nWenxiang Jiao, Wenxuan Wang, Jentse Huang, Xing Wang, and Zhaopeng Tu. 2023. Is chatgpt a good translator? yes with gpt-4 as the engine.\\n\\nMartin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. 2023. Exploiting asymmetry for synthetic training data generation: SynthIE and the case of information extraction. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1555\u20131574, Singapore. Association for Computational Linguistics.\\n\\nAnkush Khandelwal, Sahil Swami, Syed S. Akhtar, and Manish Shrivastava. 2018. Humor detection in english-hindi code-mixed social media content: Corpus and baseline system.\"}"}
{"id": "acl-2024-short-76", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nA.1 Data Preparation\\n\\nA.1.1 Unfun Corpus\\n\\nWe use the February 2, 2023 Unfun (West and Horvitz, 2019) database backup, and consider all valid unfunned headlines (i.e. not None). This results in 11831 pairs. A subset of these have ratings from other players. We use these to curate a high quality evaluation subset of pairs where:\\n\\n\u2022 There is at least one annotation.\\n\u2022 The satirical headline has a funniness rating $\\\\geq 0$.\\n\u2022 The unfunned headline has a funniness rating $\\\\leq 0$.\\n\\nThe resulting 867 pairs were split among prompt examples (10%), dev (30%), and test (60%) shards.\\n\\nFor our training set, we consider the remaining headlines, again ensuring that there is no overlap with other shards. The resulting dataset has many instances where there are multiple unfunned counterparts for each satirical headline. As an additional step, we randomly filter our training, dev, and test shards so that there is only one unfunned headline per satirical headline. This results in a training set of 3882 unfuns, a dev set of 186 unfuns, and a test set of 375 unfuns, in each case, these are included alongside their corresponding satirical headlines.\\n\\nFor an additional training data baseline, we also retrieve an equal number of real news headlines included in the Unfun database.\\n\\nA.1.2 Code-Mixed English-Hindi Humor\\n\\nWe use the version of the English-Hindi Humor dataset by Khandelwal et al. (2018) hosted on GitHub. We use the provided labels for the available data. Notably, a portion of annotated samples appear to be unavailable. We divide the available dataset (n = 2951) into training, dev, and test shards (60%, 20%, 20%). Additionally, we filter tweets containing links.\\n\\nA.2 Data Generation Details\\n\\nWe include our full prompts in Appendix B. For decoding hyperparameters, we use top-$p = 0$ and $\\\\tau = 1.0$ for all LLMs.\\n\\n3https://github.com/epfl-dlab/unfun\\n4https://github.com/Ankh2295/humor-detection-corpus\"}"}
{"id": "acl-2024-short-76", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2.1 Unfun Data Generation\\nTo generate synthetic Unfun for each LLM approach, we prompt each model with randomly sampled in-context pairs from examples from our high quality subset that was set aside for prompting. For our ROBERTA-SWAP baseline, we replace tokens in the original satirical headline using a ROBERTA-BASE model. To select each replacement, we iterate over and individually mask each token in the headline, and then predict the masked token:\\n\\n$$\\\\hat{x}_i = \\\\arg \\\\max_x P(x | x \\\\neq i, \\\\theta_{RoBERTa})$$\\n\\nThe position with the largest ratio between the predicted token and the original token probabilities is selected as the swap position:\\n\\n$$\\\\text{swap position} = \\\\arg \\\\max_i \\\\left[ \\\\frac{P(\\\\hat{x}_i | x \\\\neq i, \\\\theta_{RoBERTa})}{P(x_i | x \\\\neq i, \\\\theta_{RoBERTa})} \\\\right]$$\\n\\nWe then replace $$x_i$$ with $$\\\\hat{x}_i$$, and repeat this procedure $$k$$ times. We set $$k = 3$$ in our experiments.\\n\\nA.2.2 Hindi-English Data Generation\\nUnlike for Unfun, we do not have existing pairs of (un-humorous, humorous) English Hindi tweets. To remedy this, we first generated 50 examples in a zero-shot setting on our training set, and then selected nine high quality results to serve as our prompt. We additionally prompt GPT-4 with humorous and non-humourous texts to classify the resulting unfunned tweets as humorous or non-humorous. We filter unfunned tweets if they are still classified as humorous.\\n\\nA.3 Human Evaluations\\nWe recruited 10 university students as annotators for the Unfun task. All annotators were American and native English speakers. For the English-Hindi dataset, we worked with three bilingual (Hindi and English) speakers. For both evaluations, we gathered three unique annotations per example, and assigned labels based on majority votes. Our Unfun evaluation assumes that any headline labeled as satirical or as real headline is grammatical and coherent. In contrast, we do not consider the grammatical label for English-Hindi data, due to the varied syntactic styles of tweets.\\n\\nIn Table 2, headlines are only rated \u201cReal\u201d if a majority of annotators rated the headline as \u201cReal\u201d (not \u201cSatire\u201d or \u201cNeither\u201d). Headlines are rated \u201cSlightly Funny\u201d if a majority of annotators assigned the headline funniness $$\\\\geq 1$$, and \u201cFunny\u201d with funniness = 2. Our full instructions for both human evaluations are included in Appendix C.1.\\n\\nTables 5 and 6 display inter-annotator agreement statistics.\\n\\n| Human Label | Krippendorff \u03b1 |\\n|-------------|----------------|\\n| Real        | 0.507          |\\n| Funny       | 0.333          |\\n| Very Funny  | 0.214          |\\n| Grammar     | 0.271          |\\n| Coherence   | 0.214          |\\n\\nTable 5: Krippendorff\u2019s \u03b1 results on Unfun dataset.\\n\\n| Human Label | Krippendorff \u03b1 |\\n|-------------|----------------|\\n| Coherence   | 0.206          |\\n| Humorous    | 0.377          |\\n\\nTable 6: Krippendorff\u2019s \u03b1 results on English-Hindi dataset.\\n\\nA.4 Automatic Evaluations\\nOn the Unfun dataset, for each synthetic Unfun approach, we generate data using the corresponding original 3882 training examples as inputs. We then evaluate classifiers trained on each dataset on the filtered high quality holdout data. To generate humor, we provide the unfunned example as input. To edit away humor, we provide the original satirical headline. We also provide in-context pairs drawn from the high quality prompt examples (See A.1.1). For our Real News baseline, we randomly select 3882 real news headlines to serve as non-humorous examples.\\n\\nOn the English-Hindi dataset, we compare training on the original dataset to training on data where (25%) and (50%) of non-humorous examples have been replaced by GPT-4 Filtered unfunned data. We evaluate classifiers on a holdout set from original dataset (n = 591), and also set of Unfuns (n = 97), derived from humorous examples in our holdout set and rated by our annotators as both coherent and non-humorous. All results for both datasets are computed over 5 seeds.\"}"}
{"id": "acl-2024-short-76", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.5 Humor Classifier Training\\n\\nFor the Unfun task, we fine-tune MISTRAL (Jiang et al., 2023) and ROBERTA (Liu et al., 2019) models. For Hindi-English, we consider HING-ROBERTA (Nayak and Joshi, 2022). All models are trained with the AdamW optimizer (Loshchilov and Hutter, 2019) and a constant learning rate. Due to the class imbalance in the available English-Hindi dataset (39% non-humorous, 61% humorous), we weight the loss by the inverse proportion of class frequency.\\n\\nWe fine-tune our MISTRAL classifier with 4-bit quantized LoRA (Dettmers et al., 2023) and the addition of a classification head. For all classifiers, we first perform hyperparameter tuning on the original human authored datasets.\\n\\nFor the Unfun dataset we consider:\\n\\n- **Learning Rates** \u2208 {5e\u22125, 2.5e\u22125, 1.25e\u22125, 6.25e\u22126, 3.125e\u22126, 1.5625e\u22126}\\n- **Batch Size** \u2208 {32, 64, 128, 256}\\n\\nAfter selecting the highest performing configuration, we run each experiment with 5 seeds ([1234, 2345, 3456, 4567, 5678]). We include the most performant hyperparameters in Table 7. All model trains use a single NVIDIA A100 GPU. We estimate the total compute budget to be 200 hours.\\n\\nA.6 Considering Memorization\\n\\nWe investigate whether data contamination and memorization is affecting our results by testing how often synthetic unfuns or humor appear in the original Unfun corpus. We find that only a small fraction of outputs appear to match human-unfunned text or satire headlines. We include results in Table 8. Of these, the majority represent simple edits, indicating that the models may have rediscovered trivial unfuns. For example:\\n\\n\\\"Egypt plunges into state of Middle East crisis\\\"\\n\\nhttps://huggingface.co/mistralai/Mistral-7B-v0.1\\n\\nhttps://huggingface.co/FacebookAI/roberta-base\\n\\nhttps://huggingface.co/l3cube-pune/hing-roberta\\n\\n---\\n\\nB Prompts\\n\\nB.1 Unfun Task Prompts\\n\\nB.1.1 Humor Generation\\n\\nChat Models\\n\\n\\\"You are a helpful assistant that edits realistic headlines to make them humorous.\\\"\\n\\n{\"role\": \\\"user\\\", \\\"content\\\": \\\"<Unfunned Headline>\\\", \\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"<Satire Headline>\\\"}\\n\\nCompletion Models\\n\\n\\\"The following realistic headlines can be edited to be humorous:\\\"\\n\\n\\\"<Unfunned Headline> -> <Satire Headline>\\\"\\n\\nB.1.2 Unfun Generation\\n\\nChat Models\\n\\n\\\"You are a helpful assistant that edits humorous headlines to make them realistic.\\\"\\n\\n{\"role\": \\\"user\\\", \\\"content\\\": \\\"<Satire Headline>\\\", \\\"role\\\": \\\"assistant\\\", \\\"content\\\": \\\"<Unfunned Headline>\\\"}, ...\\n\\nCompletion Models\\n\\n\\\"The following humorous headlines can be edited to be realistic:\\\"\\n\\n\\\"<Satire Headline> -> <Unfunned Headline>\\\"\\n\\nB.2 English-Hindi Task Prompts\\n\\nB.2.1 Unfun Generation\\n\\nChat Models\\n\\n\\\"Kya ye diye hue tweet ka humor wala part hata kar use normal bana sakti ho? Aur jitna ho sake utna punctuation use same rakhe ki koshish karna\\\" [Can you remove the humorous part of the given tweets and make them normal? And try to keep the punctuation as much the same as possible.]\\n\\n9\"}"}
{"id": "acl-2024-short-76", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The training configurations for our automatic evaluations, after hyperparameter tuning.\\n\\n| Model           | Learning Rate | Batch Size |\\n|-----------------|---------------|------------|\\n| MISTRAL (QLoRA) | 6.25e-06      | 32         |\\n| ROBERTA         | 1.25e-05      | 32         |\\n| HING -ROBERTA   | 1.5625e-06    | 8          |\\n\\nTable 8: The number of overlapping samples between human-curated headlines and synthetic headlines in our test examples ($n = 200$).\\n\\n| Model           | Unfun Satire |\\n|-----------------|--------------|\\n| GPT-3           | 3/200        |\\n| GPT-4           | 7/200        |\\n| MISTRAL         | 2/200        |\\n| MISTRAL -INSTRUCT | 2/200   |\\n| ROBERTA -SWAP   | 0/200        |\\n\\nB.2.2 Unfun Filtering\\nChat Models\\n\\n\"You are a pattern-following assistant used to rigorously determine whether a Hindi tweet is intended to be humorous. Given a Hindi tweet, respond only with either of Yes or No. Yes if it is humorous and No if it is not humorous.\\n\\nC. Human Evaluation Instructions\\nC.1 Unfun Task Instructions\\nEach annotator has been assigned a series of text samples to review. First, you are asked to evaluate whether the text sounds like a\\n\\n\u2022 r) real news headline (like from a non-humorous news website)\\n\u2022 OR s) satirical news headline (like from a humorous newspaper like The Onion.)\\n\u2022 OR n) neither (text that would not appear in either setting, because it is ungrammatical, or incoherent.\\n\\nIf you rate a headline as n (neither), you will be further prompted to rate it as a grammatical [no=0,yes=1 (for a news headline) and coherent [no=0,yes=1].\\n\\nIf you rate a headline as s (satire), you will be prompted to subjectively rate the quality of humor:\\n\\n\u2022 0 - not funny\\n\u2022 1 - slightly humorous / there is some identifiable joke\\n\u2022 2 - funny\\n\\nContent Warning: Several headlines may contain references to upsetting content.\\n\\nEXAMPLES:\\n\\nSatirical Headlines\\n\\n\u2022 nhl not quite sure why it has a presea-\\n\\n\u2022 america's sweetheart dumps u.s. for some douchebag\\n\\n\u2022 apple: new iphone good\\n\\n\u2022 cat general says war on string may be unwinnable\\n\\n\u2022 fire chief grants fireman 3-day exten-\\n\\nNews Headlines\\n\\n\u2022 the word 'doofuses' may cost ex-yahoo\\n\\n\u2022 2 meteorites hit connecticut\\n\\n\u2022 the word 'doofuses' may cost ex-yahoo\\n\\n\u2022 2 meteorites hit connecticut\"}"}
{"id": "acl-2024-short-76", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"world outraged by north korea's latest nuke test\\npoverty rate hits 17-year high\\nphilippines: 5 foreign terror suspects in south\"}"}
{"id": "acl-2024-short-76", "page_num": 12, "content": "{\"primary_language\":\"hi\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Bhaari sankhya mein vote karein, aapke TL par wph hi nazar aayega\\n\\nVote in large numbers, wph will only appear in your TL.\\n\\nD Reference Examples\\n\\nTables 9, 10, and 11 include reference samples for English synthetic unfun outputs, English satire outputs, and English-Hindi unfun outputs respectively.\"}"}
{"id": "acl-2024-short-76", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Satire\\n\\n- Tom Petty to play some new stuff he's been working on at Super Bowl.\\n- Jaguars offensive line not sure they can open big enough hole for Maurice Jones Drew.\\n- Obama takes surprise caller during weekly radio address.\\n\\nTable 9: Unfun model output reference samples.\"}"}
{"id": "acl-2024-short-76", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Obama, congress must strike a deal on budget by March 1.\\n\\nESPN sports segment reveals science behind Tony Romo's precision pass.\\n\\nJimmie Johnson disqualified after pre-race inspection reveals car to be violating rules.\\n\\n---\\n\\nTable 10: Satire model output reference samples.\\n\\nOriginal: Ab ki baar.. #MaaBetaFarar.. Ab ki baar.. yeh log farar hain..\\nUnfuns: Patidev ko samarpit aur lambe samay tak saath dena chahiye.\\n\\nOriginal: O naadan parindey ghar aaja. Parinda: naadan tera baap.\\nUnfuns: O naadan parindey ghar aaja. Parinda: Mujhe ghar aane do.\\n\\nOriginal: Neend aaja nahi to kal se tujhe KRK bulaunga\\nUnfuns: Neend aaja nahi to kal se tujhe alag naam se bulaunga\\n\\nOriginal: Bhai ab itne velle bhi nahi hai ki #Ind-VsBan test match dekhenge\\nUnfuns: Bhai ab itne samay nahi hai ki #IndVs-Ban test match dekhenge\\n\\nOriginal: Asli toofan andar hai, jail ke andar.\\nUnfuns: Asli samasya jail ke andar hai. #SalmanVerdict\\n\\nOriginal: Vodafone use karne se acha to ek ka-bootar pal lo.\\nUnfuns: Vodafone use karne se acha to kisi aur network provider ka use karo.\\n\\n---\\n\\nTable 11: GPT-4 English-Hindi unfunned reference samples. See Table 12 for English translations.\"}"}
{"id": "acl-2024-short-76", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This time.. #MotherSonGone.. This time.. these people are gone..\\n\\nHusbands should be like Vim bar, less talk and more work.\\n\\nHusbands should be dedicated and support for a long time.\\n\\nOh naive bird, come home. Bird: Your dad is naive.\\n\\nOh naive bird, come home. Bird: Let me come home.\\n\\nIf sleep doesn\u2019t come, from tomorrow I will call you KRK.\\n\\nIf sleep doesn\u2019t come, from tomorrow I will call you by a different name.\\n\\nBro, we\u2019re not that free to watch the #IndVsBan test match.\\n\\nBro, we don\u2019t have that much time to watch the #IndVsBan test match.\\n\\nThe real storm is inside, inside the jail. #SalmanVerdict\\n\\nIt\u2019s better to raise a pigeon than to use Vodafone.\\n\\nIt\u2019s better to use another network provider than Vodafone.\\n\\nTable 12: Translation of GPT-4 English-Hindi unfunned reference samples.\"}"}
