{"id": "acl-2024-long-862", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nAs the reach of large language models (LMs) expand globally, their ability to cater to diverse cultural contexts becomes crucial. Despite advancements in multilingual capabilities, models are not designed with appropriate cultural nuances. In this paper, we show that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. We introduce CAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities spanning eight types that contrast Arab and Western cultures. CAMeL provides a foundation for measuring cultural biases in LMs through both extrinsic and intrinsic evaluations. Using CAMeL, we examine the cross-cultural performance in Arabic of 16 different LMs on tasks such as story generation, NER, and sentiment analysis, where we find concerning cases of stereotyping and cultural unfairness. We further test their text-infilling performance, revealing the incapability of appropriate adaptation to Arab cultural contexts. Finally, we analyze 6 Arabic pre-training corpora and find that commonly used sources such as Wikipedia may not be best suited to build culturally aware LMs, if used as they are without adjustment.\"}"}
{"id": "acl-2024-long-862", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tion by LMs in their own languages. This leads to\\nthe question: do LMs exhibit bias towards Western\\nentities in non-English, non-Western languages?\\n\\nWhile considerable effort has gone into exploring\\nbiases in LMs with regards to groups of different\\ndemographic or social dimensions (Sheng et al.,\\n2021) such as religion (Abid et al., 2021a,b), race\\n(An et al., 2023; Ahn and Oh, 2021), and national-\\nities (Cao et al., 2022b), much less work (\u00a72) has\\nexamined the cultural appropriateness of LMs in\\nthe non-Western and non-English environments. In\\norder to address this gap, we center our study on\\nculturally relevant entities, as they are important\\naspects of cultural heritage (Montanari, 2006; Tajud-\\ndin, 2018) and can symbolize regional identities\\n(G\u00f3mez-Bantel, 2018). To the best of our knowl-\\nedge, there is no resource readily available for doing\\nso, especially one that can contrast Arab vs. West-\\nern cultural differences. We thus construct a new\\nbenchmark, CAMeL (Cultural Appropriateness Me-\\nasure Set for LMs), which consists of an ex-\\ntensive list of 20,368 Arab and Western entities\\ncovering eight entity types (i.e., person names, food\\ndishes, beverages, clothing items, locations, au-\\nthors, religious places of worship, and sports clubs),\\nand an associated set of 628 naturally occurring\\nprompts as contexts for those entities (\u00a73).\\n\\nWe show that CAMeL entities and prompts en-\\nable cross-cultural testing of LMs in versatile exper-\\nimental setups, including story generation, NER, sen-\\ntiment analysis, and text infilling (\u00a74). We bench-\\nmark 16 LMs pre-trained with Arabic data (\u00a74.1).\\n\\nOur results reveal concerning cases of cultural\\nstereotypes in LM-generated stories, such as the as-\\nsociation of Arab names with poverty/traditionalism\\n(\u00a74.2), and cultural unfairness, such as better NER\\ntagging performance of Western entities and higher\\nassociation of Arab entities with negative sentiment\\n(\u00a74.3). We further show that LMs exhibit high\\nlevels of preference towards Western-associated\\nentities even when prompted by contexts uniquely\\nsuited for Arab culture-associated entities (\u00a74.4).\\n\\nLastly, we discuss that the prevalence of Western\\ncontent in Arabic corpora may be a key contributor\\nto the observed biases in LMs. We analyze the\\ncultural relevance of 6 Arabic pre-training corpora\\nby training n-gram LMs on each corpus and com-\\nparing their text-infilling performance on\\nCAMeL.\\n\\nWe find that sources such as Wikipedia may not be\\nideal for building culturally-aware LMs (\u00a75).\\n\\n2 Related Work\\n\\nThere have been several recent efforts on examining\\nthe cultural alignment of LMs. One line of work\\nexplored the moral knowledge (e.g., judgment of\\nright and wrong actions) encoded in LMs (Fraser\\net al., 2022; Schramowski et al., 2022; H\u00e4mmerl\\net al., 2022; Xu et al., 2023), probing their ability\\nto infer moral variation on topics with cultural di-\\nvergence of opinions (Ramezani and Xu, 2023). It\\nhas been found that LMs can be biased towards the\\nmoral values of certain societies (e.g., American\\n(Johnson et al., 2022)) and political ideologies (e.g.,\\nliberalism (Abdulhai et al., 2023)). Similar works\\nstudied LMs\u2019 understanding of cross-cultural differ-\\nences in values and beliefs (e.g., attitude towards\\nindividualism) (Cao et al., 2023; Arora et al., 2023),\\nand what opinions they hold on political (Hartmann\\net al., 2023; Fenget al., 2023) or other global topics\\n(Santurkar et al., 2023; Durmus et al., 2023).\\n\\nThese past studies have quantified the alignment\\nof LMs through their responses to cultural surveys\\n(Hofstede, 1984; Haerpfer et al., 2021; Graham\\net al., 2011; Guerra and Giner-Sorolla, 2010), where\\nLMs were probed using survey type of questions in a\\nQA setting (e.g., \u2018Is sex before marriage acceptable\\nin China?\u2019), or cloze-style questions reformulated\\nfrom these surveys (e.g., \u2018In China, sex before\\nmarriage is [acceptable/unacceptable]\u2019).\\n\\nWang et al. (2023b) and Masoud et al. (2023) have shown\\nthat LMs reflect values and opinions aligned with\\nWestern culture when probed with such surveys,\\nwhich persists across multiple languages.\\n\\nAnother line of work explored how well LMs\\nstore culture-related commonsense knowledge by\\nprobing for their ability to answer geo-diverse facts\\n(e.g., \u2018The color of the bridal dress in China is\\n[red/white]\u2019) (Nguyen et al., 2023; Yin et al., 2022;\\nKelegandMagdy, 2023). Other studies probe LMs\\nfor cultural norms such as culinary customs (Palta\\nand Rudinger, 2023) and time expressions (Shwartz,\\n2022). Huang and Yang (2023) studied social norm\\nreasoning as an entailment classification task.\\n\\nDifferent from existing work, we study how LMs\\nbehave with entities that exhibit cultural variation\\n(e.g., people names, food dishes, etc.). We ex-\\ntract and annotate an extensive list of cultural enti-\\nties from Wikidata and CommonCrawl, which in\\nturn enables the evaluation of LMs using naturally-\\noccurring prompts that we collect from social media,\\ninstead of the artificial prompts used in survey-\\nbased studies. Our dataset provides a foundation\"}"}
{"id": "acl-2024-long-862", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Arab drink [MASK] late at night is great to calm your nerves.\\n\\n\u0627\u0644\u0639\u0631\u0628\u064a \u0641\u064a \u0622\u062e\u0631 \u0627\u0644\u0644\u064a\u0644 \u0645\u0641\u064a\u062f \u062c\u062f\u0627 \u0644\u0647\u062f\u0648\u0621 \u0627\u0644\u0623\u0639\u0635\u0627\u0628.\\n\\nI suspect the Arab drink [MASK] has a lot of harms.\\n\\n\uc6b8\ub784 \ucde8\uc8fc\uba38\ub2c8\\n\\nEntity Extraction from Wikidata.\\n\\nFor each entity type, we manually identified relevant Wikidata classes under which common entities are grouped in the knowledge base (e.g., \\\"food\\\", \\\"city\\\", \\\"drink\\\", etc.). We then extract all entities registered under those classes that have a label in Arabic language. For Location, Authors, and Sports Club entities, it was possible to extract all entities per each country of the Arab world or the Western world (Western Europe and North America), as they are linked to either a country of origin or a nationality label in the knowledge base. However, for other entity types, we had to manually classify them into Arab and Western lists due to the lack of such demographic labels (see Appendix B.1 for details). Wikidata's coverage of entities in Arabic was extensive for locations, sports clubs, and authors (see Figure 3), but more limited for the other entity types.\\n\\nEntity Extraction from Web Crawls.\\n\\nTo expand on entities collected from Wikidata for entity types where coverage was limited, we perform pattern-based entity extraction on the Arabic subset of the CommonCrawl corpus. Pattern-matching is a simple yet effective method (Chiticariu et al., 2013; Freitag et al., 2022); and importantly, it avoids using any LMs in the construction of the dataset that will be used for evaluating LMs. For each entity type, we manually design 5 to 10 generic patterns composed of nouns or noun-verb expressions typically followed by a specific entity. For example, the pattern \\\"/charfa/charab/char59/char10/char4b/char10/chare9/char10/charae/char4a/char0a/char10/charae/char11/char83\\\" (sister named) is likely to be followed by a female name. We used multiple patterns for each entity type to capture the diversity of expressions used in the natural language data.\"}"}
{"id": "acl-2024-long-862", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Food and Clothing\\n\\nAuthors\\n\\nArab (Wikidata)\\n\\nWestern (Wikidata)\\n\\nArab (CommonCrawl)\\n\\nWestern (CommonCrawl)\\n\\nArabic location entities extracted from Wikidata are about 8.5k North American, 2.8k European, and 1k Arab World.\\n\\nArabic verb conjugations of the same pattern to reflect number and gender. Using such patterns, we perform pattern matching and extract up to two words that appear after a detected pattern. We avoid using more specific and longer patterns to ensure wider coverage of entities (i.e., higher recall lower precision). This process returns between 5k and 10k unique extractions for each entity type, which are then manually filtered and annotated to achieve high precision. We split name and clothing entities into male/female categories to match Arabic's gendered grammar, without intending to exclude other gender identities (Stanczak and Augenstein, 2021). More details are in Appendix B.2.\\n\\nHuman Annotation. We hired two undergraduate students who are native Arabic speakers and paid them at the rate of $18 per hour to classify the extractions into: Arab culture, Western culture, other foreign culture, not culture-specific, or non-entities. For example, when annotating clothing items, we consider Arab entities as traditional/ethnic wear within the Arab world (e.g., Jellabiya, Dishdasha, etc.), and Western entities as terms that refer to specific styles/types of clothing prevalent in the Western world (e.g., Khaki, Hoodie, etc.). The inter-annotator agreement is 0.927 by Cohen\u2019s Kappa. The small number of cases of disagreements were discussed between the annotators.\\n\\nIn Arabic, verbs are conjugated to reflect gender (male or female) and number (singular, dual, or plural) of the subject.\\n\\nFigure 3: Number of cultural entities in CAMeL for each entity type stratified by association with Arab or Western cultures and source of collection (i.e., Wikidata or CommonCrawl). The breakdown of Arabic location entities extracted from Wikidata are about 8.5k North American, 2.8k European, and 1k Arab World.\\n\\nFigure 4: Log counts in the Arabic CommonCrawl vs. frequency rank of Arab and Western name, food, and beverage entities in CAMeL. We capture both very frequent and long-tail entities. All entities are in Arabic (English translations are shown in the figure). Annotation required \u223c60 minutes per 1k extractions. About 15-20% of entities extracted from CommonCrawl overlap with those in Wikidata. CAMeL covers both frequently encountered and less frequent entities (Figure 4).\\n\\n3.2 Collecting Naturally Occurring Prompts\\n\\nOne of our primary objectives is to assess whether LMs can appropriately distinguish between Arab and Western entities when prompted by culturally specific contexts. To achieve this, we create prompts that embed an Arab cultural reference, ensuring they provide contexts uniquely suited for Arab entities. This allows to gauge the LM\u2019s cultural adaptation ability. Additionally, we create prompts with neutral contexts, enabling us to determine the default cultural leanings of LMs. Hence, CAMeL prompts are split across two types: culturally-contextualized prompts (CAMeL-Co) and culturally-agnostic prompts (CAMeL-Ag). Table 1 offers contrasting examples from each.\\n\\nRetrieving Natural Contexts. To ensure we evaluate LMs in scenarios that mirror actual language uses, we construct our prompts from natural contexts that we retrieve from Twitter/X, rather than crowdsourcing prompts (Nadeem et al., 2021a; Nangia et al., 2020a). We employ two keyword search strategies to retrieve tweets that reflect an Arab cultural context for each entity category. First, we use 20 randomly sampled Arab entities from our...\"}"}
{"id": "acl-2024-long-862", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Culturally Contextualized Prompts (Co) Culturally Agnostic Prompts (Ag)\\n\\nTable 1: Examples of naturally occurring Arabic prompts in CAMeL. Original culture-specific entities (e.g., food items or religious places of worship) mentioned by the Twitter/X users are replaced by a [MASK] token.\\n\\nWe search for tweets over the period of 8/1/2023 to 9/30/2023 to avoid overlap with the data LMs may have been pre-trained on. Retrieved tweets are manually inspected to select ones with suitable Arab cultural contexts. From these, we created 250 CAMeL-Co prompts by replacing the original context entities with a [MASK] token. Similarly, we constructed 378 prompts for CAMeL-Ag using generic patterns as search queries that do not contain any cultural reference (see Appendix C).\\n\\nSentiment Annotation. To support fairness evaluation of LMs on sentiment analysis, the prompts were labeled by the annotators for positive, negative, or neutral sentiment. The inter-annotator agreement is 0.954 as measured by Cohen\u2019s Kappa. More details and statistics are provided in Appendix C.3.\\n\\n4 Measuring Cultural Bias in LMs\\n\\nUsing CAMeL, we measure cultural biases of several monolingual and multilingual LMs (\u00a74.1). First, we analyze stereotypes in LM-generated stories (\u00a74.2). We then examine cross-cultural fairness of LMs on the NER and Sentiment Analysis tasks (\u00a74.3). Finally, we benchmark the capability of LMs on culturally appropriate text-infilling (\u00a74.4).\\n\\n4.1 Language Models\\n\\nWe consider LMs that have been intentionally trained for Arabic. For monolingual LMs, we use AraBERT (Antoun et al., 2020), ARBERT (Abdul-Mageed et al., 2021), and CAMeLBERT (Inoue et al., 2021); we compare CAMeLBERT to its variants trained exclusively on Dialectal Arabic (CAMeLBERT-DA) or Modern Standard Arabic (CAMeLBERT-MSA). Additionally, we use models trained on Arabic tweets such as MARBERT (Abdul-Mageed et al., 2021) and AraBERT-T. We also include AraGPT2 (Antoun et al., 2021). For multilingual LMs, besides mBERT, XLM-R (Conneau et al., 2020), BLOOM (Scao et al., 2022), GPT-3.5 and GPT-4, we use Arabic-English bilingual JAIS (Sengupta et al., 2023), GigaBERT and GigaBERT-CS (Lan et al., 2020), which was further trained on synthetic Arabic-English Code-Switched data. We also use AceGPT (Huang et al., 2023), an instruction-tuned version of Llama2 (Touvron et al., 2023) on localized Arabic instructions. Lastly, we use mT5_XXX (Xue et al., 2021) and its recent instruction-tuned counterpart Aya (\u00dcst\u00fcn et al., 2024). We use the base (B) and large (L) versions whenever available. More details about all the LMs used can be found in Appendix D.\\n\\n4.2 Cultural Stereotypes in Story Generation\\n\\nWe examine the potential of GPT-type LMs to reflect stereotypes in their generations when portraying Arab and Western entities. Specifically, we analyze their lexical choices in stories generated about characters with Arab and Western names.\\n\\nSetup. For each of the male and female names in CAMeL, we prompt LMs in Arabic to \\\"Generate a story about a character named [PERSON NAME]\\\". Then, we analyze the frequency of adjective usage by LMs in the stories featuring Arab or Western names. To do so, we extract all adjectives from stories using the Farasa POS tagger (Abdelali et al., 2016) and compute their Odds Ratio (OR) (Wan et al., 2023) (see Appendix F.1 for the formula). A large OR indicates more odds for an adjective of appearing in Western stories, while a small OR indicates more odds of appearing in Arab ones. We inspect adjectives with the 50 highest and lowest ORs to identify and categorize adjectives that reflect stereotypes based on the work of Cao et al. (2022a), which outlines descriptive adjectives for stereotypical traits (e.g., poor, likeable, etc.) using the Agency-Belief...\"}"}
{"id": "acl-2024-long-862", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Odds Ratio of adjectives associated with stereotypical traits in LM generated stories about male characters with Arab and Western names. LMs associate Arab male names with poverty and traditionalism. More analysis on female names can be found in Appendix F.1.\\n\\nCommunion (ABC) framework (Koch et al., 2016).\\n\\nResults. Figure 5 displays the identified adjectives, revealing multiple stereotypical associations. Stories about Arab characters more often cover a theme of poverty with adjectives such as \u201cpoor\u201d persistently used across LMs. On the other hand, the adjective \u201cwealthy\u201d was more likely to appear in Western stories. LMs also tend to use adjectives describing Traditionalism, Dominance (for male names) and Benevolence (for female names) in Arab stories, while using adjectives that reflect Likeability and High-Status in Western stories. We manually inspected stories containing those adjectives, where we found a consistent opening narrative of Arab characters being \u201cborn into a poor and modest family\u201d. This was less prevalent in Western stories where LMs often portrayed positive attributes about the character (see examples in Table 2).\\n\\n4.3 Fairness in NER and Sentiment Analysis\\n\\nTo examine whether LMs treat Arab and Western entities fairly, we analyze their cross-cultural performance on the tasks of NER and sentiment analysis. We perform this analysis using evaluation sentences that include either Arab or Western entities.\\n\\nSetup. We leverage culturally-contextualized prompts (CAMeL-Co) which have been manually labelled for sentiment (\u00a73.2) to create the test data. Specifically, for each of the prompts, we replace the [MASK] token with 50 randomly sampled Arab and Western entities. This generates two distinct culturally-contrasting evaluation sets (one Arab, one Western) for the sentiment analysis experiment, each comprising around 12k sentences. For NER evaluation, we use the subset of 5.7k sentences that contain either person names or locations. We create models capable of performing Arabic NER and sentiment prediction by fine-tuning LMs on datasets commonly used in Arabic NLU benchmarks (Elmadany et al., 2023; Abdul-Mageed et al., 2021). We use the ANER Corp (Benajiba et al., 2007) dataset for NER (name and location tags were used only) and HARD dataset (Elnagar et al., 2018) for sentiment analysis. For GPT-type LMs, we perform in-context learning with 5-shot examples (see prompts in Appendix F.2).\\n\\nNER Results. Figure 6 shows the F1 scores achieved by LMs on recognizing Arab and Western related entities. We find that most LMs perform better when tagging Western person names and locations. Larger discrepancies are observed on locations, reaching up to 20 F1 points of difference. The gap was smaller for tagging of male and female names, where differences were around 5 F1 points.\\n\\nSentiment Analysis Results. Following past work on fairness of sentiment classifiers (Czarnowska et al., 2021), we examine differences in false positive and false negative predictions between sentences containing Arab vs. Western entities. This enables closer analysis of whether LMs show more association of Arab or Western entities with positive or negative sentiments, as opposed to comparing F1 scores which had minimal differences. The results are shown in Figure 7. We observe that nearly all LMs achieve higher false negatives on sentences containing Arab entities, shown in Figure 7. We observe that nearly all LMs achieve higher false negatives on sentences containing Arab entities,\"}"}
{"id": "acl-2024-long-862", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: F1 score achieved by LMs on named entity recognition of Arab vs. Western name (male and female) and location entities. LMs are better at tagging Western entities than Arab ones. Results are averaged across 5 runs.\\n\\nFigure 7: Difference in False Negative (FN) and False Positive (FP) sentiment predictions on prompts filled with Arab and Western entities. Shaded regions show 95% confidence intervals. LMs show higher association of Arab entities with negative sentiment suggesting more false association of Arab entities with negative sentiment. On the other hand, no clear trend of stronger positive sentiment association towards Arab or Western entities is observed.\\n\\n4.4 Culturally-Appropriate Text Infilling\\nTo test the ability of LMs at adaptation to cultural contexts, we use a likelihood-based score that compares model preference of Western vs. Arab entities as fillings of [MASK] tokens in CAMeL prompts.\\n\\nCultural Bias Score. Inspired by the likelihood scoring metric of Nadeem et al. (2021a), we define a Cultural Bias Score (CBS) to measure the level of Western bias in a model LM $\\\\theta$. The CBS computes the percentage of a model\u2019s preference of Western entities over Arab ones. Consider an entity type $D$ and two type-specific sets of Arab entities $A = \\\\{a_i\\\\}_{i=1}^N$ and Western entities $B = \\\\{b_j\\\\}_{j=1}^M$. For a prompt $t_k$, we compute $CBS_D(LM, A, B, t_k)$ as:\\n\\n$$\\\\frac{1}{N \\\\times M} \\\\sum_{i=1}^M \\\\sum_{j=1}^N \\\\left[ \\\\frac{P[\\\\text{MASK}|t_k](b_j)}{P[\\\\text{MASK}|t_k](a_i)} \\\\right],$$\\n\\nwhere $P[\\\\text{MASK}]$ is the LM\u2019s probability of an entity filling the masked token. We evaluate LMs with BERT-type architecture using the full prompts with a [MASK] token for text-infilling and GPT-type/T5-type LMs using only the portion of the prompt appearing before the [MASK]. We take the average over all the sub-words for entities tokenized into sub-words. For a set of prompts $T = \\\\{t_k\\\\}_{k=1}^K$, the CBS per entity type for an LM is computed by averaging over all $t_k \\\\in T$. LMs are considered more Western-biased as its CBS gets closer to 100%.\\n\\nPrompt Adaption. In addition to using the vanilla prompts, we also experiment with two prompt-adaption techniques that may help in localizing LMs to the relevant Arab culture: (1) CultureToken, where the special token \\\\texttt{\\\\char5b/charfa/char0a/char47/char2e/char51/charab/char5d [Arab]} is prepended to prompts, and (2) N-shot demos, where randomly sampled Arab entities are prepended to prompts as demonstrations. We make sure the entity being evaluated is not in the demonstrations.\\n\\nResults. Figure 8 shows the average CBS across all entity types on culturally-contextualized prompts (CAMeL-Co). We provide CBS per each entity type and additional results on CAMeL-Ag in Appendix F.3. We observe the following key findings:\"}"}
{"id": "acl-2024-long-862", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cultural Bias Score (CBS)\\n\\n| Model                | CBS Range |\\n|----------------------|-----------|\\n| ARBERT               | 40% to 60%|\\n| MARBERT              | 40% to 60%|\\n| AraBERT              | 40% to 60%|\\n| AraBERT-T            | 40% to 60%|\\n| CAMeLBERT            | 40% to 60%|\\n| CAMeLBERT-MSA        | 40% to 60%|\\n| CAMeLBERT-DA         | 40% to 60%|\\n| AraGPT2              | 40% to 60%|\\n| mBERT                | 40% to 60%|\\n| GigaBERT             | 40% to 60%|\\n| GigaBERT-CS          | 40% to 60%|\\n| XLMR                 | 40% to 60%|\\n| XLMR                  | 40% to 60%|\\n| mT5 XXL              | 40% to 60%|\\n| Aya                  | 40% to 60%|\\n| AceGPT               | 40% to 60%|\\n| JAIS                 | 40% to 60%|\\n| Bloom                | 40% to 60%|\\n| GPT-3.5              | 40% to 60%|\\n\\nDespite cultural contextualization, high CBS is observed for all LMs (40% to 65%) indicating inability to localize to the relevant culture. LMs prefer Western entities despite Arab cultural contexts. Since CAMeL-Co prompts explicitly refer to Arab culture, an ideal LM is expected to (nearly) always score higher likelihood to Arab entities over Western ones, i.e., with CBS close to 0. However, existing LMs show high average CBS (40-60%), which is on par with their performance on CAMeL-Ag prompts where contexts are neutral. This indicates a struggle in localizing to the appropriate culture in context, and a noticeable preference for Western entities.\\n\\nEven monolingual Arabic-specific LMs exhibit Western bias. Surprisingly, although monolingual LMs are trained on Arabic-only data, they still obtain high CBS scores. The reason may be that part of the pre-training data (more in \u00a75), even if solely in Arabic, often discusses Western topics. Multilingual LMs show stronger Western bias. Most multilingual LMs showed a higher CBS compared with monolingual LMs. This implies that multilingual training could impact cultural relevance of LMs in non-Western languages. We find that embeddings of Arab and Western entities are grouped into distinct clusters by monolingual LMs while mixed up in multilingual LMs (see Appendix G.1).\\n\\nFigure 9: Average CBS achieved by 4-gram LMs trained on Arabic pre-training corpora. Wikipedia, international news, and web-crawls are the most Western-centric. Culturally-relevant demonstrations help with adaptation. Prompt-adaption techniques can potentially help in localizing LMs to the relevant culture. In particular, prepending Arab demonstrations reduced CBS for most LMs. However, introducing a special culture token had little effect.\\n\\n5 Analyzing Arabic Pre-training Data\\n\\nOne main contributor to the observed failures of LMs in inappropriate cultural adaptation could be the prevalence of Western content in the Arabic pre-training corpora. To gain more insight, we analyze six Arabic corpora that are commonly used in pre-training LMs, comparing their cultural relevance.\\n\\nSetup.\\nWe use two local Arabic news corpora (1.5B corpus by El-Khair (2016)) and Assafir news (Antoun et al., 2020)), an international news corpus (OSIAN by Zeroual et al. (2019)), the Arabic portion of CommonCrawl (from OSCAR by Su\u00e1rez et al. (2019)), Arabic Wikipedia, and the 60M Arabic tweets corpus used in training AraBERT-T (Antoun et al., 2020). We train 4-gram LMs using OpenGRM (Roark et al., 2012) without smoothing on each corpus, leveraging their frequency count-based nature to directly compare prevalence of cultural contexts and entities across corpora. We then use the trained 4-grams to compute the average CBS for each corpus using CAMeL-Co for analysis.\\n\\nResults.\\nFigure 9 shows the average CBS of 4-gram LMs trained on each corpus. The results suggest that (Arabic) Wikipedia is the most Western-centric among all corpora, despite being often considered as one of the highest-quality sources for pre-training data. This is mostly because a large portion of Arabic Wikipedia articles discuss Western content. International news had the second highest CBS. Interestingly, web-crawled data was the third most Western-centric source. A recent analysis of CommonCrawl by Thompson et al.\"}"}
{"id": "acl-2024-long-862", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(2024) has shown that a large fraction of the total web content is machine-translated. This could explain the prevalence of Western content as it may get translated into Arabic from languages such as English. We also find that an English-like grammatical structure of Arabic sentences can incite more Western bias in LMs (see Appendix G.2). Local news and Twitter/X corpora had the lowest CBS, suggesting that future work may consider these sources for training more culturally adapted LMs.\\n\\n6 Conclusion\\n\\nWe introduced CAMeL, a novel dataset of naturally occurring prompts and culturally-relevant entities as prompt completions across eight entity types. We showed that when operating in Arabic, LMs exhibit bias towards Western entities, failing in appropriate cultural adaptation. LMs also show cultural unfairness on tasks such as NER and sentiment analysis, and stereotypes in generated stories. By releasing CAMeL, we hope to enable the evaluation and development of culturally-aware LMs.\\n\\nLimitations\\n\\nWe focused on assessing the overall ability of LMs to adapt to Arab cultural contexts and exploring their biases towards Western entities. The entities in CAMeL are therefore primarily categorized as being associated with Arab or Western cultures. However, entities belonging to certain categories, such as food dishes or locations, can be further divided into specific regions and countries within the Arab and Western worlds. This finer-grained categorization could enable analysis of LMs' ability to distinguish between entities belonging to sub-groups of a particular culture. We leave such detailed factual knowledge exploration of sub-cultural distinctions in LMs for future studies.\\n\\nCAMeL only covers the Arabic language and enables the evaluation of model biases with respect to Western vs. Arab cultural entities. The works of Wang et al. (2023b) and Masoud et al. (2023) have shown that when probed using cultural surveys in Chinese, Korean, or Slovak, LMs tend to respond with answers reflecting Western values. CAMeL can be extended in the future to such languages by adopting our approach for entity extraction and prompt construction.\\n\\nWe limited the scope of our experiments on stereotypes in generated stories to only the analysis of lexical terms, specifically adjectives. Future work can leverage CAMeL entities to analyze further variations beyond lexical content, such as stylistic features of the generations. We believe that the release of CAMeL entities will be a valuable asset to the research community for exploring biases in generation tasks beyond only story generation.\\n\\nOur analysis of pre-training corpora was limited to examining the relevance of their cultural content, particularly to understand why LMs fail at adapting to Arab cultural contexts. However, to gain deeper insights into the manifested issues of stereotyping and unfairness, more analyses would be necessary. This involves quantifying the co-occurrences of Arab and Western entities with specific themes (e.g., poverty, negativity, etc.) within the corpora. Further, fine-tuning datasets could also play an additional role in amplifying fairness problems. Future research can leverage CAMeL to examine these issues, building on our initial findings.\\n\\nEthics Statement\\n\\nWhile LMs must adapt to Arab entities when prompts are specifically grounded in an Arab cultural context, the question of what culture they should default to in neutral contexts is more nuanced. This largely depends on the preferences and backgrounds of users. For instance, Arabic speakers residing in non-Arab countries might prefer Arabic LMs to align with the local culture they identify with. However, current LMs default to Western culture in neutral contexts. The neutral prompts we provide in CAMeL-Ag can serve as a valuable testbed for future studies that aim to align LMs to meet the unique cultural preferences of their users.\\n\\nOur prompts were derived from naturally occurring social media contexts obtained from Twitter/X. We do not share the original raw tweets but rather modified versions where original entities mentioned by users have been replaced by [MASK] tokens. The prompts are, therefore, anonymized and do not contain any personally identifiable information. The release of CAMeL prompts is exclusively for research purposes, particularly for evaluating the cultural adaptation of LMs. When constructing our prompts, we have carefully selected contexts that do not contain toxic or offensive language.\\n\\nArabic is a grammatically gendered language where verbs must be conjugated for either male or female genders in the second and third persons. This linguistic restriction affects how we construct...\"}"}
{"id": "acl-2024-long-862", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"prompts for categories such as names and clothing, leading us to separate these prompts into male and female groups. This follows the approach taken by past work on social biases in languages with grammatical gender distinctions (Levy et al., 2023). It's important to clarify that this categorization by gender does not aim to define or differentiate gender identities (Stanczak and Augenstein, 2021) but is done to reflect the language's structure accurately. We also note that the aim of our study is to investigate biases in LMs toward Western entities and not the examination of gender biases.\\n\\nAcknowledgements\\n\\nThe author would like to thank Youssef Naous and Nour Allah El Senary for their help in data annotation. The author also thanks Wissam Antoun for sharing data that facilitated our analysis on pre-training corpora. This research is supported in part by the NSF awards IIS-2144493 and IIS-2052498, ODNI and IARPA via the HIATUS program (contract 2022-22072200004). The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of NSF, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.\\n\\nReferences\\n\\nAhmed Abdelali, Kareem Darwish, Nadir Durrani, and Hamdy Mubarak. 2016. Farasa: A fast and furious segmenter for Arabic. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Demonstrations, pages 11\u201316.\\n\\nMuhammad Abdul-Mageed, AbdelRahim Elmadany, et al. 2021. ARBERT & MARBERT: Deep bidirectional transformers for Arabic. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7088\u20137105.\\n\\nMarwa Abdulhai, Gregory Serapio-Garcia, Cl\u00e9ment Crepy, Daria Valter, John Canny, and Natasha Jaques. 2023. Moral foundations of large language models. arXiv preprint arXiv:2310.15337.\\n\\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021a. Large language models associate Muslims with violence. Nature Machine Intelligence, 3(6):461\u2013463.\\n\\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021b. Persistent anti-Muslim bias in large language models. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES '21, page 298\u2013306, New York, NY, USA. Association for Computing Machinery.\\n\\nJaimeen Ahn and Alice Oh. 2021. Mitigating language-dependent ethnic bias in BERT. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 533\u2013549, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nKabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, et al. 2023. Mega: Multilingual evaluation of generative AI. arXiv preprint arXiv:2303.12528.\\n\\nHaozhe An, Zongxia Li, Jieyu Zhao, and Rachel Rudinger. 2023. SODAPOP: Open-ended discovery of social biases in social commonsense reasoning models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1565\u20131588.\\n\\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020. AraBERT: Transformer-based model for Arabic language understanding. In LREC Workshop Language Resources and Evaluation Conference 11\u201316 May 2020, page 9.\\n\\nWissam Antoun, Fady Baly, and Hazem Hajj. 2021. AraGPT2: Pre-trained transformer for Arabic language generation. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 196\u2013207.\\n\\nArnav Arora, Lucie-Aim\u00e9e Kaffee, and Isabelle Augenstein. 2023. Probing pre-trained language models for cross-cultural differences in values. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 114\u2013130.\\n\\nParishad Behnam Ghader and Aristides Milios. 2022. An analysis of social biases present in BERT variants across multiple languages. In Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022.\\n\\nYassine Benajiba, Paolo Rosso, and Jos\u00e9 Miguel Bened\u00edruiz. 2007. Anersys: An Arabic named entity recognition system based on maximum entropy. In Computational Linguistics and Intelligent Text Processing: 8th International Conference, CICLing 2007, Mexico City, Mexico, February 18-24, 2007. Proceedings 8, pages 143\u2013153. Springer.\\n\\nJayadev Bhaskaran and Isha Bhallamudi. 2019. Good secretaries, bad truck drivers? occupational gender stereotypes in sentiment analysis. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 62\u201368, Florence, Italy. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-862", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, and Vinodkumar Prabhakaran. 2022. Re-contextualizing fairness in NLP: The case of India. In Proceedings of the 2nd Conference of the Asian-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 727\u2013740.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nAylin Caliskan, Joanna B. Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183\u2013186.\\n\\nYang Cao, Anna Sotnikova, Hal Daum\u00e9 III, Rachel Rudinger, and Linda Zou. 2022a. Theory-grounded measurement of U.S. social stereotypes in English language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1276\u20131295.\\n\\nYang Trista Cao, Anna Sotnikova, Hal Daum\u00e9 III, Rachel Rudinger, and Linda Zou. 2022b. Theory-grounded measurement of U.S. social stereotypes in English language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1276\u20131295, Seattle, United States.\\n\\nYong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel Hershcovich. 2023. Assessing cross-cultural alignment between ChatGPT and human societies: An empirical study. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 53\u201367.\\n\\nLaura Chiticariu, Yunyao Li, and Frederick R. Reiss. 2013. Rule-based information extraction is dead! Long live rule-based information extraction systems! In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 827\u2013832, Seattle, Washington, USA. Association for Computational Linguistics.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, \u00c9douard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451.\\n\\nPaula Czarnowska, Yogarshi Vyas, and Kashif Shah. 2021. Quantifying social biases in NLP: A generalization and empirical comparison of extrinsic fairness metrics. Transactions of the Association for Computational Linguistics, 9:1249\u20131267.\\n\\nDipto Das, Shion Guha, and Bryan Semaan. 2023. Toward cultural bias evaluation datasets: The case of Bengali gender, religious, and national identity. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 68\u201383.\\n\\nDavid L. Davies and Donald W. Bouldin. 1979. A cluster separation measure. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-1(2):224\u2013227.\\n\\nSunipa Dev, Tao Li, Jeff M. Phillips, and Vivek Srikrishnakumar. 2021. OSCaR: Orthogonal subspace correction and rectification of biases in word embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5034\u20135050.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186.\\n\\nEsin Durmu\u015f, Karina Nyugen, Thomas I. Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. 2023. Towards measuring the representation of subjective global opinions in language models. arXiv preprint arXiv:2306.16388.\\n\\nIbrahim Abu El-Khair. 2016. 1.5 billion words Arabic corpus. arXiv preprint arXiv:1611.04033.\\n\\nAbdel Rahim Elmadany, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2023. ORCA: A challenging benchmark for Arabic language understanding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 9559\u20139586, Toronto, Canada. Association for Computational Linguistics.\\n\\nAshraf Elnagar, Yasmin S. Khalifa, and Anas Einea. 2018. Hotel Arabic-reviews dataset construction for sentiment analysis applications. Intelligent natural language processing: Trends and applications, pages 35\u201352.\\n\\nShangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models. arXiv preprint arXiv:2305.08283.\\n\\nKathleen C. Fraser, Svetlana Kiritchenko, and Esma Balkir. 2022. Does moral code have a moral code? Probing Delphi\u2019s moral philosophy. In Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022), pages 26\u201342.\\n\\nDayne Freitag, John Cadigan, Robert Sasseen, and Paul Kalmar. 2022. Valet: Rule-based information extraction for rapid deployment. In Proceedings of the 2022 Conference of the Asian-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 727\u2013740.\"}"}
{"id": "acl-2024-long-862", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Derinoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2023. Bias and fairness in large language models: A survey. \\n\\narXiv preprint arXiv:2309.00770.\\n\\nAdriano G\u00f3mez-Bantel. 2018. Football clubs as symbols of regional identities. In Football, Community and Sustainability, pages 32\u201342. Routledge.\\n\\nJesse Graham, Brian A Nosek, Jonathan Haidt, Ravi Iyer, Spassena Koleva, and Peter H Ditto. 2011. Mapping the moral domain. Journal of personality and social psychology, 101(2):366.\\n\\nValeschka M Guerra and Roger Giner-Sorolla. 2010. The community, autonomy, and divinity scale (cads): A new tool for the cross-cultural study of morality. Journal of cross-cultural psychology, 41(1):35\u201350.\\n\\nWei Guo and Aylin Caliskan. 2021. Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 122\u2013133.\\n\\nChristian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya Kizilova, Jaime Diez-Medrano, Marta Lagos, Pippa Norris, E Ponarin, and B Puranen. 2021. World values survey: Round seven. JD Systems Institute & WVSA Secretariat. Data File Version, 2(0).\\n\\nKatharina H\u00e4mmerl, Bj\u00f6rn Deiseroth, Patrick Schramowski, Jind\u0159ich Libovick\u00fd, Constantin A Rothkopf, Alexander Fraser, and Kristian Kersting. 2022. Speaking multiple languages affects the moral bias of language models. arXiv preprint arXiv:2211.07733.\\n\\nJochen Hartmann, Jasper Schwenzow, and Maximilian Witte. 2023. The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation. arXiv preprint arXiv:2301.01768.\\n\\nBabak Hemmatian, Razan Baltaji, and Lav R Varshney. 2023. Muslim-violence bias persists in debiased GPT models. arXiv preprint arXiv:2310.18368.\\n\\nDaniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, et al. 2022. Challenges and strategies in cross-cultural NLP. In 60th Annual Meeting of the Association for Computational Linguistics (ACL), May 22-27, 2022, Dublin, Ireland, pages 6997\u20137013. Association for Computational Linguistics.\\n\\nGeert Hofstede. 1984. Culture's consequences: International differences in work-related values, volume 5. Sage.\\n\\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.\\n\\nHuang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Ziche Liu, et al. 2023. AceGPT, localizing large language models in Arabic. arXiv preprint arXiv:2309.12053.\\n\\nJing Huang and Diyi Yang. 2023. Culturally-aware natural language inference. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7591\u20137609.\\n\\nGo Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda Bouamor, and Nizar Habash. 2021. The interplay of variant, size, and task type in Arabic pre-trained language models. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 92\u2013104.\\n\\nRebecca L Johnson, Giada Pistilli, Natalia Men\u00e9dez-Gonz\u00e1lez, Leslye Denisse Dias Duran, Enrico Panai, Julija Kalpokiene, and Donald Jay Bertulfo. 2022. The Ghost in the Machine has an American accent: Value conflict in GPT-3. arXiv preprint arXiv:2203.07785.\\n\\nMasahiro Kaneko and Danushka Bollegala. 2022. Unmasking the mask\u2013evaluating social biases in masked language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11954\u201311962.\\n\\nMasahiro Kaneko, Aizhan Imankulova, Danushka Bollegala, and Naoaki Okazaki. 2022. Gender bias in masked language models for multiple languages. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2740\u20132750, Seattle, United States. Association for Computational Linguistics.\\n\\nAmr Keleg and Walid Magdy. 2023. DLAMA: A framework for curating culturally diverse facts for probing the knowledge of pretrained language models. arXiv preprint arXiv:2306.05076.\\n\\nAlex Koch, Roland Imhoff, Ron Dotsch, Christian Unkelbach, and Hans Alves. 2016. The abc of stereotypes about groups: Agency/socioeconomic success, conservative\u2013progressive beliefs, and communion. Journal of personality and social psychology, 110(5):675.\\n\\nMascha Kurpicz-Briki. 2020. Cultural differences in bias? Origin and gender bias in pre-trained German and French word embeddings.\\n\\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. 2020. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4727\u20134734.\"}"}
{"id": "acl-2024-long-862", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Anne Lauscher and Goran Glava\u0161. 2019. Are we consistently biased? multidimensional analysis of biases in distributional word vectors. In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics, pages 85\u201391, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nNayeon Lee, Chani Jung, and Alice Oh. 2023. Hate speech classifiers are culturally insensitive. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), pages 35\u201346.\\n\\nSharon Levy, Neha Anna John, Ling Liu, Yogarshi Vyas, Jie Ma, Yoshinari Fujinuma, Miguel Ballesteros, Vittorio Castelli, and Dan Roth. 2023. Comparing biases and the impact of multilingual training across multiple languages. arXiv preprint arXiv:2305.11242.\\n\\nYingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. 2023. A survey on fairness in large language models. arXiv preprint arXiv:2308.10149.\\n\\nWeicheng Ma, Brian Chiang, Tong Wu, Lili Wang, and Soroush Vosoughi. 2023a. Intersectional stereotypes in large language models: Dataset and analysis. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8589\u20138597.\\n\\nWeicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun, Andrew Koulogeorge, Lili Wang, Diyi Yang, and Soroush Vosoughi. 2023b. Deciphering stereotypes in pre-trained language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11328\u201311345.\\n\\nMarta Marchiori Manerba, Karolina Sta\u0144czak, Riccardo Guidotti, and Isabelle Augenstein. 2023. Social bias probing: Fairness benchmarking for language models. arXiv preprint arXiv:2311.09090.\\n\\nReem I Masoud, Ziquan Liu, Martin Ferianc, Philip Treleaven, and Miguel Rodrigues. 2023. Cultural alignment in large language models: An explanatory analysis based on Hofstede's cultural dimensions. arXiv preprint arXiv:2309.12342.\\n\\nChandler May, Alex Wang, Shikha Bordia, Samuel Bowman, and Rachel Rudinger. 2019. On measuring social biases in sentence encoders. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 622\u2013628.\\n\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064.\\n\\nMassimo Montanari. 2006. Food is culture. Columbia University Press.\\n\\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021a. Stereoset: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356\u20135371.\\n\\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021b. Stereoset: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356\u20135371, Online. Association for Computational Linguistics.\\n\\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020a. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953\u20131967, Online. Association for Computational Linguistics.\\n\\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020b. Crows-pairs: A challenge dataset for measuring social biases in masked language models. arXiv preprint arXiv:2010.00133.\\n\\nAur\u00e9lie N\u00e9v\u00e9ol, Yoann Dupont, Julien Bezan\u00e7on, and Kar\u00ebn Fort. 2022. French CrowS-pairs: Extending a challenge dataset for measuring social bias in masked language models to a language other than English. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8521\u20138531, Dublin, Ireland. Association for Computational Linguistics.\\n\\nTuan-Phong Nguyen, Simon Razniewski, Aparna Varde, and Gerhard Weikum. 2023. Extracting cultural commonsense knowledge at scale. In Proceedings of the ACM Web Conference 2023, pages 1907\u20131917.\\n\\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2021. HONEST: Measuring hurtful sentence completion in language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2398\u20132406, Online. Association for Computational Linguistics.\\n\\nDebora Nozza, Federico Bianchi, Anne Lauscher, and Dirk Hovy. 2022. Measuring harmful sentence completion in language models for LGBTQIA+ individuals. In Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion, pages 26\u201334, Dublin, Ireland. Association for Computational Linguistics.\\n\\nShramay Palta and Rachel Rudinger. 2023. FORK: A bite-sized test set for probing culinary cultural biases in commonsense reasoning models. In Findings of the Association for Computational Linguistics: ACL 2023, pages 9952\u20139962.\"}"}
{"id": "acl-2024-long-862", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-862", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-862", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Additional Background\\n\\nCulture-related Biases in LMs. Various studies have explored biases in English LMs with regards to groups from different cultural backgrounds. For example, Abid et al. (2021a) studied stereotypical associations in LMs towards different religious groups by probing LMs with templates such as \\\"[MASK]areviolent\\\". They show that LMs such as GPT-3 associates Muslims with violence more often than other religious groups, which has been found by Hemmatian et al. (2023) to persist even after LMs go through debiasing procedures. Similar template probing studies have explored such social biases in LMs towards races (e.g., \\\"Asians are good at math\\\") (Ma et al., 2023b,a; Cao et al., 2022b; Ross et al., 2021; Nadeem et al., 2021a), nationalities (e.g., \\\"A person from Iraq is an enemy\\\") (Venkit et al., 2023; Manerba et al., 2023; Ahn and Oh, 2021) and more attributes (Nangia et al., 2020b).\\n\\nThis line of research has primarily explored the extent to which LMs reflect human biased associations about specific social or cultural groups present in their pre-training data. While they touch on certain aspects related to culture (e.g., religion), they do not study the LMs's adaptation capability to diverse world cultures. Further, these works are English-centered. In contrast, our work explores how LMs handle entities that associate with different cultures. We show that multilingual and Arabic monolingual LMs exhibit bias towards Western-associated entities, failing at appropriate cultural adaptation to Arab cultural contexts. We also show how LMs demonstrate upsetting stereotypes and unfairness on the NER and sentiment analysis tasks when presented with Arab culture-associated entities as opposed to Western entities.\\n\\nBiases in non-English languages. Various works have explored biases in non-English languages. One line of work translates English datasets into other languages (Levy et al., 2023; N\u00e9v\u00e9ol et al., 2022; Lee et al., 2023; Kurpicz-Briki, 2020; Lauscher and Glava\u0161, 2019). We argue that this is not an effective strategy, as the translated evaluation data lacks the relevant cultural identity (Talat et al., 2022). Most studies focus primarily on gender biases (Das et al., 2023; Vashishtha et al., 2023; Touileb et al., 2022; Kaneko et al., 2022) or social biases (N\u00e9v\u00e9ol et al., 2022; Bhatt et al., 2022; BehnamGhader and Milios, 2022; Nozza et al., 2021). In this paper, we study a more subtle and understudied yet very important problem \u2013 cultural appropriateness of LMs in non-English and non-Western environments. We focus on culture-specific entities and analyze cross-cultural performance of LMs on such entities. We construct CAMeL, a novel dataset of naturally-occurring Arabic prompts obtained from Twitter/X, and an extensive list of entities associated with Arab and Western culture across eight entity types that exhibit cultural variation.\\n\\nMeasuring Biases in LMs. Early work on measuring biases examined vector space distances between static word embeddings of neutral attributes (e.g., professions) and social attributes (e.g., genders, races) (Caliskan et al., 2017; Dev et al., 2021). Embedding-based methods were then adapted to contextualized embeddings of LMs learned from the context of sentences, where neutral and social attributes are placed in sentence templates (e.g., \\\"This is Katie\\\", \\\"This is a friend\\\") (May et al., 2019; Guo and Caliskan, 2021; Tan and Celis, 2019). More recent works adopt probability-based approaches, where LMs are prompted using masked templates and their assigned token probabilities for different groups are compared given the same context (Nozza et al., 2022; Kaneko and Bollegala, 2022; Nadeem et al., 2021b; Nozza et al., 2021; Nangia et al., 2020b; Salazar et al., 2020).\\n\\nIn contrast to the aforementioned \\\"intrinsic\\\" approaches that focus on examining embeddings and probabilities, another line of research adopts \\\"extrinsic\\\" approaches, where the focus is analyzing fairness of LMs towards different groups (e.g., races, nationalities, etc.) on downstream tasks (Czarnowska et al., 2021). In this setting, groups are slotted inside sentence templates that are used for downstream evaluation, allowing comparison of model behavior when groups are switched. Such approaches have been used to explore gender biases in co-reference resolution (Zhao et al., 2018), social biases in sentiment analysis (Bhaskaran and Bhallamudi, 2019), lexical/dialect biases in toxic language detection (Zhou et al., 2021), and other classification tasks (Li et al., 2023).\\n\\nOur dataset enables measurement of cultural biases through both intrinsic and extrinsic approaches (\u00a74). CAMeL prompts and entities support fairness evaluation for several tasks including text classification (sentiment analysis \u00a7 4.3), token-level classification (NER \u00a7 4.3), and text generation (\u00a7 4.2) tasks. CAMeL also supports intrinsic measurements through text infilling tests (\u00a7 4.4).\"}"}
{"id": "acl-2024-long-862", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Entity statistics per source.\\n\\nB Collecting Arab and Western Entities\\nWe provide additional details of the collected Arab and Western entities for each entity type in CAMeL. For religious places of worship, we focus on the two dominant religions in both cultures and hence collect lists of mosques as Arab entities and churches as Western entities. For sports clubs, we specifically collect football clubs as entities. Statistics of CAMeL entities per source are shown in Table 3.\\n\\nGiven that Arabic is a grammatically gendered language, requiring verbs to be conjugated according to male or female genders in both the second and third persons, it is necessary to categorize both names and clothing entities based on gender. This categorization ensures that such entities align grammatically with the verbs in the prompts we create (\u00a7 3.2), which are conjugated according to gender.\\n\\nB.1 Entity Extraction from Wikidata.\\nWe report the Wikidata classes from which entities were extracted in Table 4. A Wikidata class groups together entities that share common characteristics. For example, entities that are considered a food item such as \\\"spaghetti\\\" or \\\"shawarma\\\" are registered under the \\\"food\\\" class in Wikidata. Wikidata classes can also be linked to sub-classes which cover a more specific subset of entities. For example, \\\"Street food\\\" and \\\"Dessert\\\" are sub-classes of the \\\"food\\\" class. We selected classes that are generic and cover a large number of sub-classes to ensure wide coverage of entities.\\n\\nEntities registered in Wikidata may have labels in multiple languages (i.e., their equivalent terms in each of those languages), as they are tied to Wikipedia articles about the entity that may exist in multiple language versions. For example, the Arabic label for the entity \\\"shawarma\\\" is \\\"/char41/chard3/char50/charf0/char41/char11/char83\\\".\\n\\nWe extract all entities under the selected classes and use their Arabic labels when available. Note that not all Wikidata entities have labels in Arabic.\\n\\nB.2 Entity Extraction from Web Crawls\\nWe use the Arabic subset of CommonCrawl from OSCAR (Su\u00e1rez et al., 2019), which partitions the CommonCrawl dumps by language. The Arabic patterns designed to extract entities from the corpus are reported in Table 5. We defined multiple versions of the same pattern, where we used different tenses and gender/number conjugations of the same verb, helping expand extractions. Verb conjugations that reflect gender were specifically helpful in collecting male-specific and female-specific entities (such as names and clothing items). Pattern-based extraction significantly boosted the number of entities obtained from Wikidata (e.g.; a 171% increase in female name entities from 354 to 961). We do not perform the pattern-based extraction process for authors, locations, and sports clubs, since Wikidata...\"}"}
{"id": "acl-2024-long-862", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Constructing Natural Prompts\\n\\nC.1 CAMeL-Co: Details\\n\\nThe patterns used in our query-based search for retrieving culturally-contextualized tweets are reported in Table 6. The number of tweets returned by pattern-based queries was often larger than searching directly with Arab entities, which depended on entity popularity (popular entities returned more tweets). Most queries returned 100 to 500 tweets. For queries that return a larger number, we randomly sample 500 tweets. \u223c15% of tweets were found suitable contexts. 68.8% of the prompts were in Arabic dialects, while 31.2% were in Modern Standard Arabic. Example prompts for each entity type are shown in Table 8.\\n\\nContextualization for GPT-type models. For proper evaluation of GPT-type models in text-infilling tests, we provide a version of the prompts where some prompts were slightly re-written to ensure reference to Arab culture appears before the [MASK] token, as the conditional probability of these models relies only on previous tokens.\\n\\nC.2 CAMeL-Ag: Details\\n\\nThe search patterns used to construct the culturally-agnostic prompts of CAMeL-Ag are reported in Table 7. In this setting, we search for tweets that have neutral contexts; where either Arab or Western entities would be appropriate fillings. Patterns are thus defined to be generic with no specific cultural reference. CAMeL-Ag prompts were obtained from the two-month span of 3/1/2023 to 4/30/2023. For most entity types, we structure the queries in a Pronoun-Verb format to facilitate our analysis on grammatical structure influence (Appendix \u00a7G.2).\\n\\nWe also provide a version of CAMeL-Ag where certain prompts are re-written to be suitable contexts for GPT-type models.\\n\\nC.3 Sentiment Annotation\\n\\nPrompt statistics and sentiment distribution is shown in Table 9. We re-wrote some prompts when possible in the opposite sentiment to obtain balance in sentiments. The small cases of differences in annotation were resolved via discussions between annotators to decide on the final label. For ethical considerations, we do not provide sentiment labels for prompts referring to religious places of worship, and ensure that none of those prompts express negativity towards a religious entity.\\n\\n| Entity Type  | English Pattern | Word Variations |\\n|--------------|-----------------|-----------------|\\n| Beverage     | (drinking the)  | /charc8/char40/char48/char2e/char51/chare5/char11/char85 |\\n| Clothing (F) | (I wear the)    | /charc8/char40/char09/chare1/char4b/char0a/char59/char10/char4b/char51/char10/char4b |\\n| Clothing (M) | (he wears the)  | /charc8/char40/char09/chare0/charf1/char82/char1c/char2e/charca/char10/char4b/char2f/char09/chare0/charf1/char82/char1c/char2e/charca/char4b/char0a |\\n| Food         | (the cooked dish) | /charc8/char40/char10/chare9/char09/char6a/char4a/char2e/chara3 |\\n| Names (M)    | (son of)        | /char09/chare1/char4b/char2e/char40/charf1/chareb |\\n| Names (F)    | (his wife)      | /chare9/char10/char4a/char6b/char2e/charf0/char09/char50/charf0 |\\n\\nTable 5: Patterns used to extract entities from CommonCrawl. We use different word variations and verb conjugations. English translations are provided, though many words do not have direct English equivalents.\"}"}
{"id": "acl-2024-long-862", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Entity Type | Arabic Pattern | Translation |\\n|-------------|----------------|-------------|\\n| Authors     | /char49/char2e/char10/char4b/char41/charbe/charca/charcb | (Book by the author) |\\n| Beverages   | /char48/char2e/char51/chare5/char11/char85/char0d/char40 | (I drink) |\\n|             | /char10/char49/char4b/char2e/char51/chare5/char11/char85 | (I drank) |\\n| Clothing (F)| /char81/char1c/char2e/charcb/char0d/char40 | (I wear) |\\n|             | /char81/char1d/char2e/char42 | (I am wearing) |\\n| Clothing (M)| /char81/char1c/char2e/charcb/char0d/char40 | (I wear) |\\n|             | /char81/char1d/char2e/char42 | (I am wearing) |\\n| Foods       | /char10/char49/charca/charbf/char0d/char40 | (I ate) |\\n|             | /char10/char49/char09/char6a/char4a/char2e/chara3 | (I cooked) |\\n|             | /char10/char49/charca/charbf/char0d/char40/charf1/char4a/charcb/char40 | (Today I ate) |\\n| Locations   | /char10/chare9/char09/char4a/char4b/char0a/char59/chard3/char09/chare1/chard3 | (I am from the city of) |\\n|             | /char10/chare9/char09/char4a/char4b/char0a/char59/chard3/charfa/char0a/char09/charaf | (I am in the city of) |\\n|             | /char10/chare9/char09/char4a/char4b/char0a/char59/chard3/char10/char48/char50/char09/char50 | (I visited the city of) |\\n| Names       | /charf9/char0a/chard6/charde/char85/char40/char0d/char41/char09/char4b/char0d/char40 | (I am named) |\\n| Religious   | /chara9/chard3/char41/char67/char2e | (Jami') |\\n|             | /char59/char6a/char2e/char82/chard3 | (Masjid) |\\n| Sports Clubs| /chara9/char6d/char2e/char19/char11/char85/char0d/char40 | (I support) |\\n\\nTable 6: Arabic patterns (with English translations) used to query Twitter/X for certain entity types in CAMeL-Co. Feminine and masculine Arabic verb conjugations are highlighted.\\n\\nTable 7: Arabic patterns (with English translations) used to query Twitter/X for collecting culturally-agnostic prompts of CAMeL-Ag. Feminine and masculine verb conjugations are highlighted.\\n\\n- **ARBERT** (Abdul-Mageed et al., 2021): trained on 61GB of text in Modern Standard Arabic (MSA) and uses additional pre-training corpora than AraBERT such as public books from Hindawi, the Arabic Gigaword corpus, and the OSIAN corpus. Available in base architecture only.\\n- **MARBERT** (Abdul-Mageed et al., 2021): a BERT model trained only on 1B Arabic tweets; designed to work better on dialects. Available in base architecture only.\\n- **CAMeLBERT** (Inoue et al., 2021): a BERT model trained on a variety of corpora that include Modern Standard Arabic, Dialectal Arabic, and...\"}"}
{"id": "acl-2024-long-862", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 8: Examples of naturally occurring Arabic prompts from CAMeL-Co for multiple types of entities (with English translations). As Arabic is grammatically gendered, we separate Female (F) and Male (M) prompts for Names and Clothing. Feminine and masculine Arabic verb conjugations are highlighted.\\n\\n| Entity Type | #Prompts (pos/neg/neut) | #Prompts (pos/neg/neut) |\\n|-------------|-------------------------|-------------------------|\\n| Authors     | 22                      | 9/9/4                   |\\n| Beverages   | 22                      | 13/7/2                  |\\n| Clothing (F)| 15                      | 5/6/4                   |\\n| Clothing (M)| 15                      | 5/6/4                   |\\n| Food        | 23                      | 9/6/8                   |\\n| Location    | 37                      | 15/15/7                 |\\n| Names (F)   | 40                      | 18/15/7                 |\\n| Names (M)   | 37                      | 13/14/10                |\\n| Religious   | 11                      | \u2014                       |\\n| Sports Clubs| 28                      | 12/13/3                 |\\n\\n**Total** 250 99/91/49 378 123/107/146\\n\\n### Table 9: Number of prompts and sentiment label distribution in CAMeL-Co and CAMeL-Ag.\\n\\n|                  | CAMeL-Co | CAMeL-Ag |\\n|------------------|----------|----------|\\n| Classical Arabic | \u2014        | \u2014        |\\n| CAMeLBERT-DA     | \u2014        | \u2014        |\\n| CAMeLBERT-MSA    | \u2014        | \u2014        |\\n| AraGPT2          | \u2014        | \u2014        |\\n| mBERT            | \u2014        | \u2014        |\\n| XLM-RoBERTa      | \u2014        | \u2014        |\\n| GigaBERT         | \u2014        | \u2014        |\\n\\nAraGPT2 was trained using the same pre-training corpora as AraBERT. We experiment with the base and large versions of the model.\\n\\nmBERT (Devlin et al., 2019): a multilingual version of the BERT model trained solely on Wikipedia and available only in the base architecture.\\n\\nXLM-RoBERTa (Conneau et al., 2020): a multilingual model trained on CommonCrawl and outperforms mBERT on various cross-lingual benchmarks. Available in both base and large architectures.\\n\\nGigaBERT (Lan et al., 2020): a bilingual English-Arabic BERT model that outperforms other multilingual models in zero-shot transfer from English to Arabic. GigaBERT is trained on the Arabic and English Gigaword corpora, Arabic and English Wikipedia, and the OSCAR corpus. We also use a version of GigaBERT, referred to as GigaBERT-CS, which is further pre-trained on Code-Switched data. Both models are in the base architecture.\"}"}
{"id": "acl-2024-long-862", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BLOOM (Scao et al., 2022): a 176 billion parameter multilingual LLM trained on 46 natural languages and 13 programming languages. The language-specific training data largely came from the OSCAR corpus (Su\u00e1rez et al., 2019). We used the HuggingFace inference API to prompt BLOOM which returns token log probabilities when using the details:true parameter.\\n\\nJAIS (Sengupta et al., 2023): a 13 billion parameter bilingual LM trained on English and Arabic. The model is available as JAIS and JAIS-Chat where the later is optimized for dialogue.\\n\\nAceGPT (Huang et al., 2023): A 13 billion parameter LM built by further pre-training Llama2-13b (Touvron et al., 2023) on Arabic corpora and instruction tuning using Arabic Quora questions.\\n\\nmT5 \\\\_\\\\_L (Xue et al., 2021): A 13 billion parameter text-to-text transformer trained on 101 languages. The model is based on the architecture of the original English T5 model (Raffel et al., 2020).\\n\\nAYA (\u00dcst\u00fcn et al., 2024): A 13 billion parameter model that performs instruction fine-tuning of mT5 \\\\_\\\\_L in 101 languages to expand language coverage and improve performance in low-resource languages.\\n\\nGPT-3.5: a 175 billion parameter LM. We experiment with OpenAI\u2019s text-davinci-003 model which has been instruction fine-tuned. The data used to train the GPT-3.5 model has not been publicly disclosed. We retrieve token log probabilities using the OpenAI completions API endpoint with the logprobs:1 parameter.\\n\\nGPT-4: We experiment with OpenAI\u2019s gpt-4-1106-preview model. Data and technical details of the model have not been publicly released. Given that computing the CBS requires access to a language model\u2019s log probabilities, we could not compute CBS scores for GPT4, for which log probabilities for arbitrary inputs are not obtainable through the OpenAI API.\\n\\n**E Pre-training Corpora Details**\\n\\nWe provide details about the Arabic pre-training corpora analyzed in \u00a7 5:\\n\\n11 https://huggingface.co/inference-api\\n12 https://platform.openai.com/docs/api-reference/completions\\n\\nArabic Wikipedia: We use the September 2020 dump of Arabic Wikipedia used in training AraBERT models (Antoun et al., 2020).\\n\\nOSIAN: The Open Source International Arabic News Corpus (Zeroual et al., 2019) consists of 3.5M news articles from 31 news sources. Almost half of this dataset (1.5M articles) is obtained from non-local international news sources (un.org, europa.eu, reuters.com, sputniknews.com, msnbcnewsnetwork.com) or news sources in non-Arab countries such as the UK (bbc.com), USA (cnn.com), Germany (dw.com), in addition to several others. Despite these sources providing news articles written in Arabic, it is highly likely that they contain a larger number of references to Western content.\\n\\n1.5B Corpus: The 1.5 billion words Arabic Corpus (El-Khair, 2016) consists of 5M news articles collected from 10 local news sources in 8 Arab countries.\\n\\nAssafir: News articles from the Lebanese Assafir newspaper used in training AraGPT2 (Antoun et al., 2021) and AraBERTv2 (Antoun et al., 2020).\\n\\nOSCAR: The Open Super-large Crawled Alien coaxR Corpus (Su\u00e1rez et al., 2019) is a multilingual partition of CommonCrawl. We use the Arabic subset of the corpus.\\n\\nTwitter/X: A corpus of 60M Arabic tweets used in training AraBERT-T (Antoun et al., 2020).\\n\\n**F Additional Results**\\n\\nF.1 Stereotypes in LM Generations\\n\\nWe give a description of the Odds Ratio computed for adjectives in LM-generated stories about Arab and Western characters in \u00a7 4.2. We also provide additional results on female names.\\n\\nOdds Ratio. Let $\\\\mathbf{x}_w = [x_{w1}, x_{w2}, \\\\ldots, x_{wW}]$ and $\\\\mathbf{x}_a = [x_{a1}, x_{a2}, \\\\ldots, x_{aA}]$ be the set of adjectives extracted from stories about characters with Arab and Western names respectively. The Odds Ratio (Wan et al., 2023; Szumilas, 2010) of an adjective $x_n$ is calculated as the odds of it appearing in stories with Western-named characters over its odds of appearing in stories with Arab-named characters.\\n\\n1.5 https://dumps.wikimedia.org/\\n14 https://en.wikipedia.org/wiki/As-Safir\\n15 https://commoncrawl.org/\"}"}
{"id": "acl-2024-long-862", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Odds Ratio of adjectives associated with stereotypical traits in LM-generated stories about female characters with Arab and Western names.\\n\\n\\\\[\\n\\\\frac{E(w(x_n))}{\\\\sqrt{\\\\sum_{i=1}^{W} i x_w_i}} / \\\\frac{E(a(x_n))}{\\\\sqrt{\\\\sum_{i=1}^{A} i x_a_i}}\\n\\\\]\\n\\nwhere\\n\\n\\\\[E(w(x_n))\\\\] is the count of the adjective \\\\(x_n\\\\) in stories with Western-named characters, and \\\\(E(a(x_n))\\\\) is its count in ones with Arab-named characters. A larger Odds Ratio reflects more likelihood for an adjective to appear in stories with Western-named characters, while a smaller ratio reflects higher likelihood of appearing in stories with Arab-named characters.\\n\\nResults on Female Characters.\\n\\nThe identified adjectives with stereotypical traits in stories with Arab and Western female names are shown in Figure 10. We notice the association of Arab-named female characters with Traditionalism and Poverty, similar to what was observed with male names (4.2). The adjective \\\"generous\\\" appeared frequently in Arab stories as well, reflecting a Benevolent trait. On the other hand, adjectives that were salient in stories about Western-named characters reflect a Likeable and High-Status trait. However, unlike the case of male characters, adjectives describing a Wealthy trait do not appear frequently for stories with female Western-named characters.\\n\\nF.2 Fairness in NER and Sentiment Analysis\\n\\nF.2.1 Additional Results\\n\\nThe performance of all fine-tuned BERT-type models on NER tagging of Arab vs. Western entities is shown in Figure 11. We also report results on recognizing author names, where LMs show better performance on recognizing Western authors compared to Arab authors.\\n\\nF.2.2 Experimental Details\\n\\nWe used a learning rate of \\\\(5e^{-5}\\\\) and the AdamW Optimizer. We fine-tuned models for 5 epochs and set the batch size to 8. Fine-tuning was performed on 1 NVIDIA A100 GPU. We fine-tuned Aya and mT5 \\\\(X\\\\) using LoRA (Hu et al., 2021) and 4-bit quantization. We set LoRa hyper-parameters as follows: rank=8, alpha=16, dropout=0.05. Since the HARD (Elnagar et al., 2018) dataset for Arabic sentiment analysis is originally imbalanced in terms of sentiment labels, we took a random sample of 30k sentences from the dataset balanced across positive/negative/neutral sentiments for our experiment.\\n\\nF.2.3 Prompts for GPT-type models\\n\\nWe perform Sentiment Analysis and NER for GPT-type models via in-context learning (Brown et al., 2020; Min et al., 2022), where models are prompted with 5 randomly sampled demonstrations (5-shots). In the following, we describe how prompting was performed for each task.\\n\\nSentiment Analysis. The prompt used to predict sentiment with GPT-type models is shown in Table 14, where the model is given an instruction to classify the sentiment of a test sentence, a key mapping labels to sentiments, and 5-shot demonstrations that were randomly sampled from the HARD dataset (Elnagar et al., 2018) for each test sentence.\\n\\nNER. We use the recent approach of Wang et al. (2023a) for NER with GPT models, where models are prompted to mark entities using the special tokens @@ and ## (in the format: @@[entity]##). We prompt models with 5 randomly sampled demonstrations from ANERCorp (Benajiba et al., 2007), where entities were marked with the special tokens. The prompt used is shown in Table 15. The results are reported in Figure 12 for the three entity types of Names, Location, and Authors. The most noticeable discrepancy is observed in location tagging, where models show superior performance on Western location entities. We found JAIS not to perform well on this task, with an F1 score below 10, and hence do not report its results.\"}"}
{"id": "acl-2024-long-862", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: F1 score achieved by BERT-type LMs on named entity recognition of Arab vs. Western person names, author names, and location entities.\\n\\nF.3 Text Infilling\\nF.3.1 Results per entity type\\nWe report the CBS scores achieved by LMs for each entity types on the culturally-contextualized prompts from CAMeL-Co in Table 10.\\n\\nF.3.2 Results on CAMeL-Ag\\nWe report the CBS scores achieved by the models on culturally-agnostic prompts from CAMeL-Ag in Table 11. We observe similar trends to what is seen in the main results of \u00a7 4.4. Without any cultural contextualization, models show high CBS scores across entity types, reaching up to 70-80%. Most multilingual models also show higher CBS than monolingual models.\\n\\nG Additional Analyses\\nG.1 Analyzing Entity Encodings\\nTo compare how LMs encode Arab and Western entities, we compute the contextualized embeddings of 50 randomly sampled entities from each entity type, when placed in prompts from CAMeL-Ag. For entities that get tokenized into multiple tokens, we take the average of their embeddings. To obtain a final encoding for each entity, we average its contextualized embeddings across all prompts.\\n\\nVisualization. We visualize entity embeddings by projecting them into a 2-dimensionalspace using t-SNE (Van der Maaten and Hinton, 2008). The results are shown for BERT-type LMs in Figure 13. It appears that most monolingual models (ARBERT, MARBERT, AraBERT, AraBERT-Twi) separate Arab and Western entities into distinctive clusters. In contrast, such distinction is not observed for...\"}"}
{"id": "acl-2024-long-862", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | #Para./#Voc. | Nam (F) | Nam (M) | Food Clo (M) | Clo (F) | Loc Auth | Bev Rel | Spo | Avg  |\\n|------------------------|--------------|---------|---------|--------------|---------|----------|---------|-----|------|\\n| Monolingual LMs (BERT architecture) |              |         |         |              |         |          |         |     |      |\\n| ARBERT 163m/100k       |              | 34.72   | 32.01   | 37.99        | 61.22   | 62.09    | 47.36   |     | 36.07 |\\n| MARBERT 163m/100k      |              | 50.41   | 47.56   | 40.55        | 57.03   | 62.78    | 44.98   | 43.15| 50.86 |\\n| AraBERT \ud835\udc35 136m/60k     |              | 42.01   | 42.31   | 39.22        | 69.10   | 63.83    | 41.32   |     | 42.62 |\\n| AraBERT \ud835\udc3f 371m/60k    |              | 37.78   | 39.65   | 38.55        | 65.05   | 58.96    | 44.25   | 40.68| 48.04 |\\n| AraBERT-T \ud835\udc35 136m/60k  |              | 50.62   | 49.88   | 36.71        | 62.64   | 59.86    | 47.69   | 48.40| 41.26 |\\n| AraBERT-T \ud835\udc3f 371m/60k  |              | 39.60   | 34.55   | 33.94        | 57.35   | 56.58    | 47.21   | 44.36| 61.79 |\\n| CAMeLBERT 109m/30k     |              | 57.77   | 76.38   | 48.59        | 52.44   | 48.17    | 49.50   | 73.09| 48.59 |\\n| CAMeLBERT-MSA 109m/30k |              | 53.31   | 76.15   | 49.07        | 53.26   | 56.19    | 46.08   | 67.14| 58.07 |\\n| CAMeLBERT-DA 109m/30k  |              | 51.99   | 73.97   | 49.14        | 56.36   | 48.86    | 46.95   | 70.23| 52.65 |\\n| Multilingual LMs (BERT architecture) |          |         |         |              |         |          |         |     |      |\\n| mBERT 110m/5k          |              | 44.97   | 40.31   | 47.75        | 47.38   | 48.05    | 50.05   | 48.58| 79.76 |\\n| GigaBERT 125m/26k     |              | 47.07   | 53.45   | 41.67        | 74.85   | 64.12    | 45.32   | 48.26| 74.66 |\\n| GigaBERT-CS 125m/26k  |              | 50.16   | 55.16   | 43.89        | 76.02   | 64.75    | 50.32   | 58.26| 75.96 |\\n| XLM-R \ud835\udc35 270m/14k      |              | 36.41   | 43.55   | 46.51        | 64.11   | 59.14    | 43.14   | 45.64| 80.92 |\\n| XLM-R \ud835\udc3f 550m/14k      |              | 38.93   | 47.76   | 45.77        | 70.99   | 65.75    | 45.78   | 50.35| 84.45 |\\n| Monolingual LMs (GPT architecture) |        |         |         |              |         |          |         |     |      |\\n| AraGPT2 \ud835\udc35 135m/64k    |              | 41.76   | 48.38   | 55.46        | 64.08   | 62.81    | 50.80   | 58.65| 46.99 |\\n| AraGPT2 \ud835\udc3f 792m/64k    |              | 49.42   | 44.96   | 49.53        | 30.52   | 36.60    | 51.86   | 43.01| 62.88 |\\n| Multilingual LMs (GPT architecture) |      |         |         |              |         |          |         |     |      |\\n| BLOOM 176b/20k         |              | 62.24   | 61.84   | 58.60        | 64.54   | 60.79    | 66.01   | 60.41| 76.07 |\\n| AceGPT 13b/54         |              | 73.24   | 76.89   | 55.68        | 45.98   | 46.37    | 69.62   | 85.12| 51.33 |\\n| JAIS 13b/43k          |              | 45.88   | 41.30   | 54.27        | 59.92   | 61.99    | 48.16   | 53.79| 55.77 |\\n| GPT-3.5 \u2014            |              | 68.67   | 60.14   | 63.82        | 63.10   | 68.06    | 67.90   | 43.62| 66.19 |\\n| Multilingual LMs (T5 architecture) |      |         |         |              |         |          |         |     |      |\\n| mT5 \ud835\udc4b\ud835\udc4b\ud835\udc3f 112b/7.5k     |              | 46.79   | 47.49   | 50.81        | 48.35   | 47.94    | 45.02   | 50.24| 48.75 |\\n| AYA 13b/7.5k          |              | 51.23   | 55.80   | 43.92        | 62.60   | 49.22    | 49.29   | 44.06| 51.26 |\\n| Table 10: Cultural Bias Scores (CBS) of different monolingual (Arabic) and multilingual LMs on prompts from CAMeL-Co that are contextualized to Arab culture (only Arab entities are appropriate fillings). Despite cultural contextualization, high CBS is observed for all models, showing high percentages (30% to 80%) of Western entity preference over the relevant Arab entities and indicating inability to localize to the relevant culture. Standard deviations range between 0% to 5%. #Voc. is the number of Arabic word pieces in the LM\u2019s vocabulary. Most multilingual models, especially for XLM-R and mBERT which are trained a wide variety of languages. On the other hand, distinct clusters can still be recognized for the bilingual GigaBERT models which are trained only on English and Arabic. These observations may indicate that multilingual training with a large variety of languages makes it more challenging for LMs to capture distinctions between entities in a specific language. Measuring Clustering Quality. To verify these observations, we treat Arab and Western entity embeddings for a particular entity type as two distinct clusters in high dimensional space and measure the cluster quality using the Davies-Bouldin Index (DBI) (Davies and Bouldin, 1979). The DBI measures (1) how close items within the same cluster are and (2) how far apart distinct clusters are. Ideally, a good clustering will have tight internal cluster distances and far separation between clusters. Such clustering achieves a DBI closer to 0. Average DBIs across cultural categories for each model are reported in Table 12. The average DBIs of multilingual models are generally higher than monolingual models, with XLM-R achieving the worst clustering quality, supporting the observations in our visualizations. These findings suggest that as models become more capable at multilingual modeling, they could simultaneously lose the cultural distinctiveness of their representations. G.2 Does English-like grammatical structure incite more Western bias? We study the effect of having an English-like grammatical structure of the Arabic prompts on the amplification of bias towards Western entities in LMs. In Arabic, subject pronouns can be and are often dropped, as they can be inferred from verb conjugation. In contrast, subject pronouns are typically necessary to convey the subject of a sentence in English; null subjects are rarely allowed.\"}"}
{"id": "acl-2024-long-862", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Type              | Model Name | #Para./#Voc. | Nam (F) | Nam (M) | Food Clo (M) | Clo (F) | Loc Auth | Bev Rel | Spo | Avg |\\n|-------------------------|------------|--------------|---------|---------|--------------|---------|----------|---------|-----|-----|-------|\\n| Monolingual LMs (BERT architecture) | ARBERT | 163m/100k | 42.70   | 29.78   | 37.38        | 62.68   | 66.15    | 41.99   | 37.79| 70.15| 48.22 |\\n|                         | MARBERT | 163m/100k | 55.53   | 37.61   | 36.73        | 65.74   | 70.10    | 45.79   | 42.05| 56.25| 50.21 |\\n|                         | AraBERT | 136m/60k  | 45.51   | 41.55   | 39.78        | 71.00   | 66.18    | 40.55   | 44.02| 70.47| 50.89 |\\n|                         | AraBERT | 371m/60k  | 45.72   | 35.70   | 37.34        | 71.05   | 62.75    | 40.95   | 42.25| 67.91| 49.32 |\\n|                         | AraBERT-T | 136m/60k | 53.99   | 50.98   | 38.33        | 64.16   | 63.95    | 45.86   | 48.16| 65.13| 52.40 |\\n|                         | AraBERT-T | 371m/60k | 47.43   | 37.34   | 33.22        | 63.94   | 62.21    | 46.96   | 47.97| 65.96| 49.13 |\\n| Multilingual LMs (BERT architecture) | CAMeLBERT | 109m/30k | 58.38   | 75.61   | 49.96        | 53.86   | 52.52    | 53.28   | 75.66| 51.74| 60.32 |\\n|                         | CAMeLBERT-MSA | 109m/30k | 54.09   | 76.63   | 51.31        | 51.00   | 53.08    | 51.68   | 71.38| 60.49| 59.37 |\\n|                         | CAMeLBERT-DA | 109m/30k | 55.74   | 68.18   | 48.96        | 58.15   | 50.83    | 53.96   | 73.52| 49.13| 57.80 |\\n| Monolingual LMs (GPT architecture) | AraGPT2 | 135m/64k | 51.36   | 58.00   | 47.47        | 59.08   | 52.63    | 61.76   | 59.45| 70.35| 55.92 |\\n|                         | AraGPT2 | 792m/64k | 50.18   | 46.11   | 43.40        | 28.45   | 38.96    | 48.38   | 41.53| 68.86| 46.10 |\\n| Multilingual LMs (GPT architecture) | BLOOM | 176b/20k | 60.64   | 57.86   | 59.35        | 63.99   | 58.04    | 65.71   | 57.97| 62.49| 61.68 |\\n|                         | AceGPT | 13b/54 | 67.44   | 66.78   | 49.26        | 44.18   | 46.75    | 67.68   | 79.73| 52.76| 58.96 |\\n|                         | JAIS | 13b/43k | 49.28   | 47.43   | 49.15        | 53.88   | 55.97    | 50.68   | 51.30| 67.18| 51.82 |\\n|                         | GPT-3.5 | \u2014 | 63.40   | 62.20   | 64.45        | 63.42   | 69.29    | 67.80   | 43.91| 67.05| 53.72 |\\n| Multilingual LMs (T5 architecture) | mT5 | 112b/7.5k | 43.04   | 42.51   | 53.17        | 45.59   | 46.60    | 47.51   | 49.77| 39.04| 46.29 |\\n|                         | AYA | 112b/7.5k | 50.26   | 55.09   | 45.82        | 60.87   | 49.13    | 46.60   | 47.20| 50.10| 50.41 |\\n\\nTable 11: CBS scores achieved by models on CAMeL-Ag, where prompts are not general and not contextualized to Arab culture, hence both Arab and Western entities would be appropriate infills. Results are based on 5 runs with 50 randomly sampled Arab and Western entities per entity type. Standard deviations range between 0% and 5%.\"}"}
{"id": "acl-2024-long-862", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: t-SNE visualization of Arab and Western entity embeddings per entity type for all BERT-type LMs. Monolingual models appear to separate Arab and Western entities into distinct clusters while entities are mixed up in most multilingual models.\"}"}
{"id": "acl-2024-long-862", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | Avg DBI | Monolingual LMs | Multilingual LMs |\\n|-------------|---------|-----------------|------------------|\\n| ARBERT      | 3.61    |                 |                  |\\n| MARBERT     | 3.59    |                 |                  |\\n| AraBERT     | 3.73    | 3.61            |                  |\\n| AraBERT-T   | 4.37    | 4.29            |                  |\\n| mBERT       | 4.11    |                 |                  |\\n| GigaBERT    | 3.97    |                 |                  |\\n| GigaBERT-CS | 3.85    |                 |                  |\\n| XLM-R       | 7.00    | 7.00            |                  |\\n| XLM-R       | 6.71    | 6.71            |                  |\\n\\nTable 12: Average Davies-Bouldin index (DBI) across all entity types for several models. Lower scores are better. Multilingual LMs tend to have higher DBIs suggesting a greater mixture of Arab and Western entity embeddings than Monolingual LMs.\\n\\n| Model       | Avg DBI | Monolingual LMs | Multilingual LMs |\\n|-------------|---------|-----------------|------------------|\\n| ARBERT      | 49.38   | 48.67           |                  |\\n| MARBERT     | 51.11   | 52.40           | -1.29            |\\n| AraBERT     | 51.66   | 50.11           | 1.55             |\\n| AraBERT-T   | 50.10   | 50.37           | -0.27            |\\n| CAMeLBERT   | 52.87   | 52.50           | 0.37             |\\n| CAMeLBERT-MSA | 58.61  | 57.62           | 0.99             |\\n| CAMeLBERT-DA | 58.04  | 56.98           | 1.06             |\\n| AraGPT2     | 56.06   | 55.15           | 0.91             |\\n| mBERT       | 55.92   | 55.32           | 0.60             |\\n| GigaBERT    | 49.70   | 49.23           | 0.47             |\\n| GigaBERT-CS | 56.40   | 55.91           | 0.49             |\\n| XLM-R       | 51.75   | 51.34           | 0.41             |\\n| JAIS        | 51.82   | 51.81           | 0.01             |\\n| BLOOM       | 58.61   | 57.62           | 0.99             |\\n| mT5         | 45.90   | 48.53           | -2.63            |\\n\\nTable 13: Effect of dropping pronouns in Arabic prompts on CBS of different LMs (\u0394 = CBS English-like - CBS ProDrop). Most models achieve higher CBS when prompted with Arabic sentences that have an English-like structure.\"}"}
{"id": "acl-2024-long-862", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Classify the sentiment in this sentence based on the following key:\\n0 = neutral\\n1 = positive\\n2 = negative\\n\\nEXAMPLES:\\nSentence: \u201c[EXAMPLE 1]\u201d\\nGiven the above key, the sentiment of this sentence is (0-2): [EXAMPLE 1 SENTIMENT]\\nSentence: \u201c[EXAMPLE 2]\u201d\\nGiven the above key, the sentiment of this sentence is (0-2): [EXAMPLE 2 SENTIMENT]\\n...\\nSentence: \u201c[EXAMPLE N]\u201d\\nGiven the above key, the sentiment of this sentence is (0-2): [EXAMPLE N SENTIMENT]\\nSentence: \u201c[SENTENCE]\u201d\\nGiven the above key, the sentiment of this sentence is (0-2): [SENTENCE SENTIMENT]\\n\\nTable 14: Prompt provided to JAIS, BLOOM, GPT3.5, and GPT-4 models for sentiment analysis.\\nPerform Named Entity Recognition on the following sentence.\\nThe task is to label [Location/Name] entities in the format: @@ entity ##\\nBelow are some examples.\\nEXAMPLES:\\nINPUT: \u201c[EXAMPLE 1]\u201d\\nOUTPUT: [EXAMPLE 1 with entities formatted as @@ entity ##]\\nINPUT: \u201c[EXAMPLE 2]\u201d\\nOUTPUT: [EXAMPLE 2 with entities formatted as @@ entity ##]\\n...\\nINPUT: \u201c[EXAMPLE N]\u201d\\nOUTPUT: [EXAMPLE N with entities formatted as @@ entity ##]\\nINPUT: \u201c[SENTENCE]\u201d\\nOUTPUT: [SENTENCE with entities formatted as @@ entity ##]\\n\\nTable 15: Prompt provided to BLOOM, GPT3.5, and GPT-4 models for Named Entity Recognition.\"}"}
