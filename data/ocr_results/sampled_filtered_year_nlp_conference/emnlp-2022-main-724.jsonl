{"id": "emnlp-2022-main-724", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Making Science Simple: Corpora for the Lay Summarisation of Scientific Literature\\n\\nTomas Goldsack1, Zhihao Zhang2, ..., Carolina Scarton1\\n\\n1Department of Computer Science, University of Sheffield, UK\\n2School of Economics and Management, Beihang University, China\\n{tgoldsack1, c.lin, c.scarton}@sheffield.ac.uk\\nzhhzhang@buaa.edu.cn\\n\\nAbstract\\nLay summarisation aims to jointly summarise and simplify a given text, thus making its content more comprehensible to non-experts. Automatic approaches for lay summarisation can provide significant value in broadening access to scientific literature, enabling a greater degree of both interdisciplinary knowledge sharing and public understanding when it comes to research findings. However, current corpora for this task are limited in their size and scope, hindering the development of broadly applicable data-driven approaches. Aiming to rectify these issues, we present two novel lay summarisation datasets, PLOS (large-scale) and eLife (medium-scale), each of which contains biomedical journal articles alongside expert-written lay summaries. We provide a thorough characterisation of our lay summaries, highlighting differing levels of readability and abstractiveness between datasets that can be leveraged to support the needs of different applications. Finally, we benchmark our datasets using mainstream summarisation approaches and perform a manual evaluation with domain experts, demonstrating their utility and casting light on the key challenges of this task. Our code and datasets are available at https://github.com/TGoldsack1/Corpora_for_Lay_Summarisation.\\n\\n1 Introduction\\nScientific publications contain information that is essential for the preservation and progression of our understanding across all scientific disciplines. Typically being highly technical in nature, such articles tend to assume a degree of background knowledge and make use of domain-specific language, making them difficult to comprehend for one lacking the required expertise (i.e., a lay person). These factors often limit the impact of research to only its direct community (Albert et al., 2015, 2022) and, more dangerously, can cause readers (members of the public, journalists, etc.) to misinterpret research findings (Kuehne and Olden, 2015).\\n\\nThis latter point is especially important for biomedical research which, in addition to having particularly dynamic and confusing terminology (Smith, 2006; Peng et al., 2021), has the potential to directly impact people's decision-making regarding health-related issues, with a pertinent example of this being the widespread misinformation seen during the COVID-19 pandemic (Islam et al., 2020). Aiming to address these challenges, some academic journals choose to publish lay summaries that clearly and concisely explain the context and significance of an article using non-specialist language. Figure 1 illustrates how simplifying journal articles can make them more accessible to a non-specialist audience.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gon (e.g., \u201cSARS-CoV-2\u201d \u2192 \u201cthe virus that causes COVID-19\u201d) and focusing on background information allows a reader to better understand a complex scientific topic. However, in addition to placing an extra burden on authors, lay summaries are not yet ubiquitous and focus only on newly published articles.\\n\\nAutomatic text summarisation can provide significant value in the generation of scientific lay summaries. Although previous use of summarisation techniques for scientific articles has largely focused on generating a technical summary (e.g., the abstract), only a few have addressed the task of lay summarisation and introduced datasets to facilitate its study (Chandrasekaran et al., 2020; Guo et al., 2021; Zaman et al., 2020). However, compared to datasets ordinarily used for training supervised summarisation models, these resources are relatively small (ranging from 572 to 6,695 articles), presenting a significant barrier to the deployment of large data-driven approaches that require training on large amounts of parallel data. Furthermore, these resources are somewhat fragmented in terms of their framing of the task, making use of article and summary formats that limit their applicability to broader biomedical literature. These factors hinder the progression of the field and the development of usable models that can be used to make scientific content accessible to a wider audience.\\n\\nTo help alleviate these issues, we introduce two new datasets derived from different academic journals within the biomedical domain\u2014PLOS and eLife (\u00a73). Both datasets use the full journal article as the source, enabling the training of models which can be broadly applied to wider literature. PLOS is significantly larger than currently available datasets and makes use of short author-written lay summaries (150-200 words), whereas eLife\u2019s summaries are approximately twice as long and written by expert editors who are well-practiced in the simplification of scientific content. Given these differences in authorship and length, we expect the lay summaries of eLife to simplify content to a greater extent, meaning our datasets are able to cater to different audiences and applications (e.g., personalised lay summarisation). We confirm this via an in-depth characterisation of the lay summaries within each dataset, quantifying ways in which they differ from the technical abstract and from each other (\u00a74). Finally, we benchmark our datasets with popular summarisation approaches using automatic metrics and conduct an expert-based manual evaluation, highlighting the utility of our datasets and key challenges for the task of lay summarisation (\u00a75). This paper also presents a literature review (\u00a72), conclusions (\u00a76), and a discussion on its limitations (\u00a77).\"}"}
{"id": "emnlp-2022-main-724", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of lay summarisation datasets, with ours given in bold. Words and sentences (sents) are average values.\\n\\n| Dataset          | # Docs | # words | # words | # sents |\\n|------------------|--------|---------|---------|---------|\\n| LaySumm          | 572    | 4,426.1 | 82.15   | 3.8     |\\n| Eureka-Alert     | 5,204  | 5,027.0 | 635.6   | 24.3    |\\n| CDSR             | 6,695  | 576.0   | 338.2   | 16.1    |\\n| PLOS             | 27,525 | 5,366.7 | 175.6   | 7.8     |\\n| eLife            | 4,828  | 7,806.1 | 347.6   | 15.7    |\\n\\nOur work is the first to provide two datasets with different levels of readability, thus supporting the needs of different audiences and applications. Through each of these factors, we hope to enable the creation of more usable lay summarisation models.\\n\\n3 Our Datasets\\n\\nWe introduce two datasets from different biomedical journals (PLOS and eLife), each containing full scientific articles paired with manually-created lay summaries. For each data source, articles were retrieved in XML format and parsed using Python to retrieve the lay summary, abstract, and article text.\\n\\nIn line with previous datasets for scientific summarisation (Cohan et al., 2018), the article text is separated into sections, and the heading of each section is also retrieved. Sentences are segmented using the PySBD rule-based parser (Sadvilkar and Neumann, 2020), which we empirically found to outperform neural alternatives. We separate our datasets into training, validation, and testing splits at a ratio of 90%/5%/5%.\\n\\nStatistics describing the contents of our datasets and that of past lay summarisation datasets are given in Table 1.\\n\\nPLOS\\n\\nThe Public Library of Science (PLOS) is an open-access publisher that hosts influential peer-reviewed journals across all areas of science and medicine. Several of these journals require authors to submit an author summary alongside their work, defined as a 150-200 word non-technical summary aimed at making the findings of a paper accessible to a wider audience, including non-scientists.\\n\\nThe journals in question focus specifically on Biology, Genetics, Pathogens, and Neglected Tropical Diseases.\\n\\n3.1 Readability\\n\\nWe assess the readability of our lay summaries and abstracts using several established metrics. Specifically, we employ Flesch-Kincaid Grade Level (FKGL), Coleman-Liau Index (CLI), Dale-Chall Readability Score (DCRS), and WordRank score.\\n\\nFKGL, CLI, and DCRS provide an approximation of the (US) grade level of education required to read a given text. The formula for FKGL surrounds the total number of sentences, words, and syllables present within the text, whereas CLI is based on the number of sentences, words, and characters. Alternatively, DCRS measures readability using the average sentence length and the number of familiar words.\\n\\nTable 2: Mean readability scores for abstracts and lay summaries from our datasets. For all metrics, a lower score indicates greater readability.\\n\\n| Metric       | Abstract | Lay Summary |\\n|--------------|----------|-------------|\\n| FKGL         | \u2193        | \u2193           |\\n| CLI          | \u2193        | \u2193           |\\n| DCRS         | \u2193        | \u2193           |\\n| WordRank     | \u2193        | \u2193           |\\n\\n4 Dataset Analysis\\n\\nWe carry out several analyses comparing the lay summaries of our datasets to the respective technical abstracts. Through these analyses, we seek to highlight and quantify the key differences between these two different types of summary, as well as those present between the lay summaries of our two datasets. Specifically, we focus on readability (\u00a74.1), rhetorical structure (\u00a74.2), vocabulary sharing (\u00a74.3), and abstractiveness (\u00a74.4).\\n\\n4.1 Readability\\n\\nWe assess the readability of our lay summaries and abstracts using several established metrics. Specifically, we employ Flesch-Kincaid Grade Level (FKGL), Coleman-Liau Index (CLI), Dale-Chall Readability Score (DCRS), and WordRank score.\\n\\nFKGL, CLI, and DCRS provide an approximation of the (US) grade level of education required to read a given text. The formula for FKGL surrounds the total number of sentences, words, and syllables present within the text, whereas CLI is based on the number of sentences, words, and characters. Alternatively, DCRS measures readability using the average sentence length and the number of familiar words.\\n\\nTable 2: Mean readability scores for abstracts and lay summaries from our datasets. For all metrics, a lower score indicates greater readability.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Barplot visualising the rhetorical class distributions in our abstracts and lay summaries.\\n\\nWords present, using a lookup table of the 3,000 most commonly used English words. Similarly, WordRank estimates the lexical complexity of a text based on how common the language is, using a frequency table derived from English Wikipedia.\\n\\nThe scores given in Table 2 show that the lay summaries of both datasets are consistently more readable than their respective abstracts across all metrics. Although these differences are small in some cases, in line with the findings of previous works (Devaraj et al., 2021), we find them all to be statistically significant by way of Mann\u2013Whitney U tests ($p < 0.05$). These results indicate that lay summaries are more readable than technical abstracts in terms of both syntactic structure and lexical intelligibility. Additionally, the lay summaries from eLife obtain lower readability scores than those of PLOS across all metrics, confirming our expectation that they are suitable for less technical audiences.\\n\\n### 4.2 Rhetorical Structure\\n\\nRhetoric is another important factor when assessing the comprehensibility of a text. Specifically, a lay person will require a much larger focus on the background of a scientific article than an expert in order to understand the significance of its findings (King et al., 2017), thus we would expect lay summaries to focus more on such aspects.\\n\\nTo provide further insight into the structural differences between abstracts and lay summaries, we classify all sentences within each based on their rhetorical status. To do this, we make use of PubMed RTC (Dernoncourt and Lee, 2017), a dataset containing the 20,000 biomedical abstracts retrieved from PubMed, with each sentence labelled according to its rhetorical role (roles: Background, Objective, Methods, Results, Conclusions).\\n\\nWe use PubMed RTC to train the BERT-based sequential classifier introduced by Cohan et al. (2019) due to its strong reported performance (92.9 micro F1-score), before applying this model to lay summary and abstract sentences from our datasets.\\n\\nTable 3: Mean percentage of each rhetorical label within our abstracts and lay summaries.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Stacked barplot showing how regularly (on average) abstract content words are shared with the respective lay summaries (as a % of all words of that type), separated by number of abstract occurrences.\\n\\nThe frequency of each rhetorical class changes according to the sentence position within our summaries. For each sub-graph, observing the pattern of most frequent labels (tallest bars) across all positions allows us to get an idea of the dominant rhetorical structure. In Table 3, we further quantify the difference in structure by giving the average percentage of each label present in the different summaries.\\n\\nFor both datasets, we see a similar pattern when comparing abstract and lay summary distributions. Specifically, a much greater portion of lay summary sentences is dedicated to explaining the relevant background information (\u201cBackground\u201d). This is unsurprising, as such information is essential to understanding the motivation and significance of any work and, thus, would be of great value to a non-expert. This additional focus on \u201cBackground\u201d comes at the expense of sentences focusing on \u201cResults\u201d and (to a lesser extent) \u201cMethods\u201d, which are less frequent within lay summaries. Again, this is to be expected, as these details are less meaningful to an audience without domain expertise.\\n\\n4.3 Content Words\\n\\nAiming to determine what terminology is shared between summary types, we analyse the frequency at which content words occur simultaneously within abstracts and lay summaries. We treat nouns, proper nouns, verbs, and numbers as content words, and we extract these from the summaries using ScispaCy (Neumann et al., 2019), a library that specialises in the processing of biomedical texts.\\n\\nFigure 3 shows the results of our analysis, visualising the average rate at which different types of content word from the abstracts are shared with lay summaries. We divide our analysis based on the number of abstracts content words occur in, allowing us to observe how the ubiquity of a word within the corpus affects the rate at which it is shared.\\n\\nGenerally, we observe similar patterns for content words between datasets. Firstly, regardless of word type or number of abstract occurrences, we find that abstract content words are rarely shared with lay summaries (i.e., \u2018shared\u2019 % < \u2018not shared\u2019 % for all bars). This is indicative of a clear shift in content and/or vocabulary when it comes to the creation of a lay summary. For each dataset, we can also see that the vast majority of content words of all types, except verbs, occur in 10 or fewer abstracts (> 90% on average for both datasets), with most of these occurring within a single abstract.\\n\\nWe found that the majority of these low-frequency content words are highly specific to the topic of a single article or small group of articles. In the case of nouns and proper nouns, we empirically observed these instances to often be highly technical terms (e.g., specific chemicals, lesser-known diseases, etc.), whereas numbers are typically exact numerical figures. It is understandable, therefore, that single-use content words of these types (noun, proper noun, and number) are rarely included in the lay summary, as they will likely be meaningless to a lay reader. The pattern exhibited by verbs differs significantly from that of other word types, as they E.g., For nouns within PLOS, 15.3% occur in a single abstract and are shared with lay summaries, and 48.2% occur in a single abstract and are not shared (i.e., total percentage of nouns that occur in a single abstract = 15.3 + 48.2 = 63.5%).\"}"}
{"id": "emnlp-2022-main-724", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Barplot showing the percentage of novel n-grams for each summary type.\\n\\n8 Typically occur in a greater number of abstracts (most commonly being present within 2-10). For content words of all types, we observe that the ratio of 'shared' to 'not shared' generally increases in line with the number of abstract occurrences.\\n\\n4.4 Abstractiveness\\n\\nWe follow the example of prior works (Sharma et al., 2019; See et al., 2017) by calculating the abstractiveness of our summaries using n-gram novelty, thus providing a measurement of the degree to which the summary uses different language to describe the content of the article. Specifically, for both abstracts and lay summaries, we compute the percentage of summary n-grams which are absent from their respective article. The results of this analysis are presented in Figure 4, where we can observe that lay summaries consistently contain more novel n-grams than abstracts across both datasets. However, the lay summaries of eLife, in addition to being approximately twice as long as those of PLOS (Table 1), appear to be significantly more abstractive. Alongside differences in readability (highlighted in \u00a74.1), we believe these to be important distinctions that should be considered in determining a suitable dataset for a particular use case or application.\\n\\n5 Experiments and Results\\n\\nTo help facilitate future work, we benchmark our datasets using popular heuristics-based, unsupervised, and supervised summarisation approaches (\u00a75.1). Additionally, we provide further insight into these results via a detailed discussion (\u00a75.2) and an expert-based manual evaluation (\u00a75.3).\\n\\n5.1 Baseline Approaches\\n\\nFor our heuristics-based approaches, we include the widely-used L_3EAD baseline which simply uses the first three sentences of the main body of the text. As our lay summaries typically consist of more than three sentences, we also include L_3EAD_K, with k being equal to the average lay summary length for each dataset (Table 1). Additionally, we include the scores obtained by the technical abstracts (ABSTRACT) and ORACLE_EXTRACT, a greedy extractive oracle (Nallapati et al., 2017) that provides an upper bound for the expected performance of extractive models.\\n\\nWe benchmark four unsupervised extractive approaches: LSA (Steinberger and Jezek, 2004), LEX_RANK (Erkan and Radev, 2004), TEXT_RANK (Mihalcea and Tarau, 2004), and HIPERANK (Dong et al., 2021). For supervised models, we use the transformer-based BART base model (Lewis et al., 2020), which we fine-tune on our own datasets. To assess how the use of additional data in various forms can benefit performance, we include several other variants of this model which are described in the remainder of this subsection.\\n\\nAdditional training\\n\\nAs our datasets remain smaller than those used in other forms of summarisation, we experiment with BART_PUB, which is previously trained on the PubMed abstract generation dataset (Cohan et al., 2018) and fine-tuned on our own datasets. Aiming to assess how well models trained on PLOS can generalise to eLife and vice versa, we also include BART_CROSS, which is initially trained on the opposite dataset to that which it is eventually fine-tuned and evaluated on.\\n\\nScaffolding\\n\\nWe also experiment with artificially enlarging our training data by way of a scaffold task. Inspired by CATTS (Cachola et al., 2020), we remove the article's abstract from the input text and train the model to generate the abstract as a scaffold task to lay summarisation. In addition to showing whether training for abstract generation can benefit lay summarisation, results for this model will provide an indication of the baseline BART model's reliance on the abstract content. Specifically, we include two copies of every article within our training data - one using the abstract as the reference summary and the other using the lay summary.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Performance of summarisation models on the test splits of each dataset (R = average ROUGE F1-score). The best non-heuristic scores for each metric are given in bold.\\n\\ndistinguish between the two by prepending the input document with the control tokens \u27e8|ABSTRACT|\u27e9 or \u27e8|SUMMARY|\u27e9. Documents within the validation and test splits are prepended the \u27e8|SUMMARY|\u27e9 code to induce lay summary generation. This model is denoted by BART Scaffold.\\n\\n5.2 Discussion\\nTable 4 presents the performance of the aforementioned approaches on the PLOS and eLife test splits using automatic metrics. In line with common practice for summarisation, we report the F1-scores of ROUGE-1, 2, and L (Lin, 2004). Additionally, we include FKGL and DCRS scores of the generated output (see \u00a74.1), providing an assessment of the syntactic and lexical complexity, respectively.\\n\\nThe importance of the abstract\\nBased on the ROUGE scores obtained by the ABSTRACT baseline, we can safely assume that the lay summaries of PLOS are much closer in resemblance to their respective abstracts than those of eLife. The importance of the abstract for lay summary generation is further highlighted by the ROUGE scores of the BART Scaffold model, which performs notably worse than the standard BART model on PLOS and slightly worse on eLife. These results suggest that having the abstract included within the model input provides significantly more benefit than using abstract generation as an auxiliary training signal.\\n\\nExtractive vs abstractive\\nIn general, we would expect abstractive methods to have greater application for the task of lay summarisation due to their ability to transform (and thus, simplify) an input text. However, abstractive approaches have a tendency to generate hallucinations, resulting in factual inconsistencies between the source and output that damages their usability (Maynez et al., 2020). Therefore, extractive approaches may still have utility for the task, especially if the comprehensibility of selected sentences is directly considered.\\n\\nFor ROUGE scores, we find that extractive baselines (i.e., all unsupervised and heuristic approaches) perform significantly better on PLOS than on eLife, aligning with our previous analysis (\u00a74.4) which identified PLOS as the less abstractive dataset. Interestingly, readability scores achieved by extractive models on PLOS match and sometimes exceed those of abstractive BART models, although they are inconsistent. For eLife, abstractive methods (i.e., BART models) generally obtain superior scores for both ROUGE and readability metrics. In fact, the ROUGE scores achieved by BART exceed those obtained by ORACLE_EXTRACT, further indicating that abstractive methods have greater potential for this dataset.\\n\\nUse of additional data\\nAs previously mentioned, artificially creating more data via an abstract-generation scaffold task results in a decrease in ROUGE scores for both datasets, indicating a reliance on the abstract content for lay summarisation. We also find that pretraining BART on Pubmed (BART_Pubmed) does very little to affect the performance, suggesting that habits learned for abstract generation do not transfer well to lay summarisation. Similarly, BART_Cross achieves a performance close to that of the standard BART model. Overall, these results indicate that additional out-of-domain training does provide much benefit for...\"}"}
{"id": "emnlp-2022-main-724", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.3 Human evaluation\\nTo further assess the usability of the generated abstractive summaries, we perform an additional human evaluation of our standard BART baseline model using two domain experts.\\n\\nOur evaluation uses a random sample of 10 articles from the test split of each dataset. Alongside each model-generated summary, judges are presented with both the abstract and reference lay summary of the given article. Using a 1-5 Likert scale, the annotators are asked to rate the model output based on three criteria: (1) Comprehensiveness \u2013 to what extent does the model output contain all information that might be necessary for a non-expert to understand the high-level topic of the article and the significance of the research; (2) Layness \u2013 to what extent is the content of the model output comprehensible (or readable) to a non-expert, in terms of both structure and language; (3) Factuality \u2013 to what extent is the model output factually consistent with the two other provided summaries. We choose not to provide judges with the full article text in an effort to minimise the complexity of the evaluation and the cognitive burden placed upon them.\\n\\nTable 5 presents the average ratings from our manual evaluation. We calculate Krippendorff\u2019s \u03b1 to measure inter-rater reliability, where we obtain values of 0.78 and 0.54 for PLOS and eLife, respectively. In addition to providing ratings, evaluators also provided comments on the general performance on each criterion for both datasets, providing further insights into model performance.\\n\\nComprehensiveness\\nWe can see from Table 5 that model outputs on PLOS are judged to be more comprehensive than on eLife. From evaluators\u2019 comments, we understand that this largely results from extensive use of abstract content for PLOS, which is sometimes copied directly (or with minor edits) to the lay summary. For eLife, it was observed that new information (i.e., not contained in the reference abstract or lay summary) was often introduced which was irrelevant or confusing, potentially affecting the understanding of a lay reader.\\n\\nLayness\\nInterestingly, given the previously highlighted differences in readability, the average layness of the model output is judged to be equal for both datasets (3.0), suggesting a reasonable degree of content simplification. However, evaluators\u2019 comments indicate that model outputs for each dataset were penalised for different reasons. For PLOS, the aforementioned use of abstract content often resulted in the inclusion of jargon terms that a lay reader would struggle to interpret. Alternatively, the language of eLife outputs was observed to be better suited to a lay audience but was sometimes simplified to a point that it could be misconstrued and mislead a reader, occasionally containing grammatical errors, typos, and repeated content.\\n\\nFactuality\\nAgain, we find an equal average score of 3.0 given for factuality, suggesting the model struggles to produce factually correct outputs for both datasets. In fact, we found no output from either dataset was given a perfect score by both annotators, indicating that simplifying technical content accurately is a consistent problem. Evaluators\u2019 comments highlight contradictions, unclear phrasing, and misrepresentation of entities as key contributing factors to factual inconsistencies. We regard this as an integral obstacle to overcome in the development of usable lay summarisation models and an essential focus for future research.\\n\\n6 Conclusion\\nIn this work, we have introduced PLOS and eLife, two new datasets for the lay summarisation of biomedical research articles. Compared to currently available resources, these datasets possess source article and summary formats that are more broadly applicable to wider literature, with PLOS also being larger by a significant margin. A thorough analysis of our lay summaries highlights key differences between datasets, enabling them to cater to the needs of different audiences and applications. Specifically, in addition to being approximately twice as long as those of PLOS, we find eLife summaries to be both more readable and abstractive, thus better suited to a less technical audience.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To facilitate future research, we benchmark our datasets with popular summarisation models using automatic metrics and conduct an expert-based human evaluation, providing further insight into the intricacies of model performance on our datasets and highlighting key challenges for the task of lay summarisation.\\n\\n7 Limitations\\n\\nAlthough we introduce the largest dataset available to the task of lay summarisation, our datasets remain smaller than those available for other forms of summarisation (e.g., abstract generation), where there exists datasets containing 100,000+ articles. This is largely due to the fact that lay summaries are less ubiquitous than other forms of summary (e.g., the abstract), only being used in a relatively small number of journals, of which only some are open-access and available to be utilised for such purposes as dataset creation.\\n\\nOn a related note, another potential limitation of our datasets is the fact they only cover a single broad domain - biomedicine. Again, this comes down to the availability of data, and the fact that the use of lay summaries is much less common in other scientific domains (e.g., Computer Science).\\n\\nThere is, however, a reason for this disparity in the adoption of lay summaries between domains, as it is generally considered more important that the public have an awareness and understanding of research breakthroughs in health-related areas such as biomedicine. Therefore, we believe it is in these domains that automatic lay summarisation can provide the greatest benefit, although we also hope to address the lay summarisation of other domains in future work.\\n\\n8 Acknowledgements\\n\\nThis work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1].\\n\\nReferences\\n\\nMathieu Albert, Elise Paradis, and Ayelet Kuper. 2015. Interdisciplinary promises versus practices in medicine: The decoupled experiences of social sciences and humanities scholars. Social Science and Medicine, 126:17\u201325.\\n\\nMathieu Albert, Paula Rowland, Farah Friesen, and Suzanne Laberge. 2022. Barriers to cross-disciplinary knowledge flow: The case of medical education research. Perspect. Med. Educ., 11(3):149\u2013155.\\n\\nFernando Alva-Manchego, Louis Martin, Carolina Scarlton, and Lucia Specia. 2019. EASSE: Easier automatic sentence simplification evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pages 49\u201354, Hong Kong, China. Association for Computational Linguistics.\\n\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615\u20133620, Hong Kong, China. Association for Computational Linguistics.\\n\\nIsabel Cachola, Kyle Lo, Arman Cohan, and Daniel Weld. 2020. TLDR: Extreme summarization of scientific documents. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4766\u20134777, Online. Association for Computational Linguistics.\\n\\nMuthu Kumar Chandrasekaran, Guy Feigenblat, Eduard Hovy, and Anita de Waard. 2020. Overview and insights from the shared tasks at scholarly document processing 2020: CL-SciSumm, LaySumm and. In Proceedings of the First Workshop on Scholarly Document Processing, pages 214\u2013224. unknown.\\n\\nArman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, and Dan Weld. 2019. Pretrained language models for sequential sentence classification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3693\u20133699, Hong Kong, China. Association for Computational Linguistics.\\n\\nArman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615\u2013621, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 308\u2013313, Taipei, Taiwan. Asian Federation of Natural Language Processing.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ashwin Devaraj, Iain Marshall, Byron Wallace, and Junyi Jessy Li. 2021. Paragraph-level simplification of medical texts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4972\u20134984, Online. Association for Computational Linguistics.\\n\\nYue Dong, Andrei Mircea, and Jackie Chi Kit Cheung. 2021. Discourse-Aware unsupervised summarization for long scientific documents. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1089\u20131102, Online. Association for Computational Linguistics.\\n\\nG. Erkan and D. R. Radev. 2004. LexRank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, 22:457\u2013479.\\n\\nYue Guo, Wei Qiu, Yizhong Wang, and Trevor Cohen. 2021. Automated Lay Language Summarization of Biomedical Scientific Reviews. Proceedings of the AAAI Conference on Artificial Intelligence, 35(1):160\u2013168.\\n\\nMd Saiful Islam, Tonmoy Sarkar, Sazzad Hossain Khan, Abu-Hena Mostofa Kamal, S M Murshid Hasan, Alamgir Kabir, Dalia Yeasmin, Mohammad Ariful Islam, Kamal Ibne Amin Chowdhury, Kazi Selim Anwar, Abrar Ahmad Chughtai, and Holly Seale. 2020. COVID-19\u2013Related Infodemic and Its Impact on Public Health: A Global Social Media Analysis. The American Journal of Tropical Medicine and Hygiene, 103(4):1621\u20131629.\\n\\nStuart R F King, Emma Pewsey, and Sarah Shailes. 2017. Plain-language Summaries of Research: An inside guide to eLife digests. eLife, 6:e25410.\\n\\nLauren M Kuehne and Julian D Olden. 2015. Lay summaries needed to enhance science communication. Proceedings of the National Academy of Sciences, 112(12):3585\u20133586.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\\n\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. 7th International Conference on Learning Representations.\\n\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan Thomas Mcdonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of The 58th Annual Meeting of the Association for Computational Linguistics (ACL).\\n\\nRada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404\u2013411, Barcelona, Spain. Association for Computational Linguistics.\\n\\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI'17, page 3075\u20133081. AAAI Press.\\n\\nMark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 319\u2013327, Florence, Italy. Association for Computational Linguistics.\\n\\nKeqin Peng, Chuantao Yin, Wenge Rong, Chenghua Lin, Deyu Zhou, and Zhang Xiong. 2021. Named entity aware transfer learning for biomedical factoid question answering. IEEE/ACM Transactions on Computational Biology and Bioinformatics.\\n\\nNipun Sadvilkar and Mark Neumann. 2020. PySBD: Pragmatic sentence boundary disambiguation. In Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS), pages 110\u2013114, Online. Association for Computational Linguistics.\\n\\nAbigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u20131083, Vancouver, Canada. Association for Computational Linguistics.\\n\\nEva Sharma, Chen Li, and Lu Wang. 2019. BIG-PATENT: A Large-Scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204\u20132213, Florence, Italy. Association for Computational Linguistics.\\n\\nBarry Smith. 2006. From concepts to clinical reality: An essay on the benchmarking of biomedical terminologies. Journal of Biomedical Informatics, 39(3):288\u2013298.\\n\\nJosef Steinberger and Karel Jezek. 2004. Using latent semantic analysis in text summarization and summary evaluation. pages 93\u2013100. 7th International Conference ISIM.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,\"}"}
{"id": "emnlp-2022-main-724", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Availability of data sources\\n\\nBoth PLOS and eLife are open-access journals, with an emphasis on making scientific research accessible to a wide audience. PLOS articles are available to be mined, reused, and shared by anyone, as per their data mining policy.\\n\\neLife articles are available under the permissive CC-BY 4.0 license, and thus also available to be retrieved and shared for these purposes.\\n\\nThe data for PLOS and eLife was retrieved on 7/03/22 and 11/03/22, respectively. All articles and summaries are in English only. Our datasets are made available to the community to facilitate future research.\\n\\nAdditional data processing details\\n\\nHere we provide some additional details regarding the dataset creation process, building on the description given in \u00a73. For eLife, the XML files retrieved were found to include multiple versions of the same article, identifiable by the article id which includes a version number. For these, we removed duplicates and kept only the most recent versions.\\n\\nFor both datasets, prior to extraction, we remove all Tables, Figures, and sections marked with the tag \\\"supplementary-material\\\". We also do not extract sections with the heading \\\"acknowledgments\\\".\\n\\nDuring sentence segmentation, we use a regular expression to identify and temporarily replace all \\\"et al.\\\" occurrences with unique placeholder tokens, which are then replaced following segmentation.\\n\\nFollowing segmentation, we again use a regular expression to identify and remove all sentences which began with \\\"DOI:\\\" followed by a URL from both abstracts and lay summaries, as these were found to commonly occur at the end of both.\\n\\nComparison to previous datasets\\n\\nWe provide a comparison of the readability and abstractiveness of lay summaries for all lay summarisation datasets in Table 6 and Figure 5, respectively.\\n\\nBaseline model details\\n\\nHere we provide additional experimental details for our baselines approaches (Table 4). ORACLE_EXT is a greedy oracle, which means it repeatedly extracts the next article sentence that will maximise the mean ROUGE scores (1, 2, and L) of the extracted summary, up to the maximum length (equal to the average lay summary length for a given dataset - Table 1).\\n\\nFor all BART models, we make use of the huggingface library (Wolf et al., 2019). Specifically, we use the \\\"facebook/bart-base\\\" model for baselines BART, BART Cross, and BART Scaffold, and we use the \\\"mse30/bart-base-finetuned-pubmed\\\" model for BART PubMed.\\n\\nTraining was run (using 4x NVIDIA Tesla V100 SXM2 GPUs) for all models with AdamW optimisation (Loshchilov and Hutter, 2019) and an early stopping patience of 25 epochs, with the best model being selected by performance on the validation set (ROUGE-2).\\n\\nAll unsupervised baselines were run with default configurations.\\n\\nAutomatic evaluation\\n\\nFor the calculation of ROUGE scores, we use the rouge-score Python package.\\n\\nFor FKGL and DCSR, we use the textstat Python package.\\n\\nHuman evaluation comments\\n\\nComments on the general model performance for each criterion provided by each annotator for our human evaluation are given in Figures 6 and 7 for PLOS and eLife, respectively.\\n\\nLay summary examples\\n\\nFull examples of lay summaries and their respective technical abstracts are given in Figures 8 and 9 for PLOS and eLife, respectively.\\n\\n13https://pypi.org/project/rouge-score/\\n14https://github.com/shivam5992/textstat\"}"}
{"id": "emnlp-2022-main-724", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 6: Comparison of the lay summary readability scores for all lay summarisation datasets.\\n\\n| Dataset          | LAYSUMM 14.81\u00b12.91 | EUREKA -ALERT 13.16\u00b12.07 | CDSR 12.72\u00b12.16 | PLOS 14.76\u00b12.33 | ELFIFE 10.91\u00b11.44 |\\n|------------------|---------------------|--------------------------|-----------------|-----------------|------------------|\\n\\n### Table 7: Statistics for bars in Figure 3. For each table cell, the overall percentage is split into '% that are shared with lay summaries' / '% that are not shared with lay summaries'.\\n\\n| Word type | PLOS | ELFIFE | Noun | Proper noun | Number | Verb |\\n|-----------|------|--------|------|-------------|--------|------|\\n| 1-2 | 15.3 / 48.2 | 14.9 / 46.2 | 8.7 / 18.1 | 8.6 / 20.3 | 6.3 / 67.6 | 4.2 / 28.9 |\\n| 3-10 | 8.7 / 18.1 | 8.6 / 20.3 | 2.5 / 4.9 | 2.5 / 6.2 | 2.4 / 18.6 | 4.9 / 29.7 |\\n| 11-100 | 2.5 / 4.9 | 2.5 / 6.2 | 0.8 / 1.5 | 0.5 / 0.9 | 0.5 / 3.8 | 3.4 / 18.9 |\\n| 100+ | 0.8 / 1.5 | 0.5 / 0.9 | 0.2 / 0.8 | 0.2 / 0.3 | 0.2 / 0.6 | 1.8 / 8.3 |\\n\\n### Figure 5: Comparison of lay summary n-gram novelty for all lay summarisation datasets.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Comprehensiveness\\nAnnotator 1: The model outputs summarised the important information, however it also cut out quite a lot of background info which is key for understanding the science. Overall, the comprehensiveness was enough for a non-expert to grasp the overall gist of the studies.\\n\\nAnnotator 2: The model seems to use parts of the abstract, and therefore seems quite comprehensive. It also does a decent job of a final \\\"summary\\\" sentence to the lay summary to summarize/put into context.\\n\\nLayness\\nAnnotator 1: Some abstracts contained a lot of jargon which would be confusing/off putting to a non-expert. Although I know some scientific words cannot be substituted, it would be good to have an explanation of the more complex words in brackets, for example.\\n\\nAnnotator 2: Due to the overlap with the reference abstract, the output is comprehensive but probably confusing to a lay audience, in some cases there is no introduction/background on scientific jargon (e.g. we cannot expect a lay audience to understand complex scientific techniques, cellular or molecular machinery). Use of Genus species nomenclature is also likely to confuse lay audiences, where a common name could be used instead, not as well as (e.g. 'Egyptian mosquito' instead of 'Aedes aegypti', also known as the Egyptian mosquito').\\n\\nFactuality\\nAnnotator 1: The majority of the statements were factually correct, although sometimes the meaning of the simplified language could be misinterpreted, which would result in a similar outcome to factually wrong statements.\\n\\nAnnotator 2: Some minor factual errors throughout, and mixups between gene symbols (e.g. where one letter will be changed, PMN to PMA), there are also come cases where it will pull out a % but mix up what gene / condition it is related to, ultimately leading to the formation of a sentence which is factually incorrect.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Comprehensiveness\\nAnnotator 1: Overall the information contained within the model-generated summaries effectively conveyed the information in the references. However, there was a few occasions where new elements/concepts were introduced that could confuse the reader and affect their understanding (these were sometimes factual statements, sometimes seemingly made up). The model abstract did provide enough information for a general understanding of the topic and would be sufficient as a brief overview.\\n\\nAnnotator 2: The model usually picks up on the core points of the abstract but can often introduce extra information which is either off-topic or factually incorrect. The model seems to start off by introducing the topic well but struggles to hit the \\\"what is the significance of this research?\\\" question.\\n\\nLayness\\nAnnotator 1: The language was, in the majority, well suited to a lay person and terminology was adapted accordingly. However, at times the information was simplified to a point where it could be misconstrued which, with scientific information, is a potential risk. At times, jargon still remained and I could imagine some people being confused by this. There were a few grammatical errors and poor sentence structure, typos, repetition etc.\\n\\nAnnotator 2: Sometimes the model introduces extra information which is not suitable for a lay audience, for example: references to genome sequencing, progenitor cells, endoplasmid reticulum. There are also instances where misinterpretation by the model may mislead the lay audience, for example there was an output where Norepinephrine was said to be \\\"a.k.a. dopamine\\\", which is not factually correct.\\n\\nFactuality\\nAnnotator 1: There were a few summaries which contained incorrect information, things that are well-known in the scientific community were poorly conveyed. At times, new information was introduced which contradicted earlier statements, and those of the reference abstracts/lay summaries. Of course, some information was correct. I would be concerned about the level of misinformation which could arise from these summaries, if used to educate a lay audience.\\n\\nAnnotator 2: This seemed to vary based on the abstract and how well the output started, for example if the model introduced the topic well, it would lead to more factual points. However, there were some generated summaries which were factually incorrect from the start and this lead to more errors.\\n\\nFigure 7: Human evaluation comments for eLife.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Technical Abstract\\n\\nRabies is a uniformly fatal disease, but preventable by timely and correct use of post exposure prophylaxis (PEP). Unfortunately, many health care facilities in Pakistan do not carry modern life-saving vaccines and rabies immunoglobulin (RIG), assuming them to be prohibitively expensive and unsafe. Consequently, Emergency Department (ED) health care professionals remain untrained in its application and refer patients out to other hospitals. The conventional Essen regimen requires five vials of cell culture vaccine (CCV) per patient, whereas Thai Red Cross intradermal (TRC-id) regimen requires only one vial per patient, and gives equal seroconversion as compared with Essen regimen. This study documents the cost savings in using the Thai Red Cross intradermal regimen with cell culture vaccine instead of the customary 5-dose Essen intramuscular regimen for eligible bite victims. All patients presenting to the Indus Hospital ED between July 2013 to June 2014 with animal bites received WHO recommended PEP. WHO Category 2 bites received intradermal vaccine alone, while Category 3 victims received vaccine plus wound infiltration with Equine RIG. Patients were counseled, and subsequent doses of the vaccine administered on days 3, 7 and 28. Throughput of cases, consumption utilization of vaccine and ERIG and the cost per patient were recorded. Government hospitals in Pakistan are generally underfinanced and cannot afford treatment of the enormous burden of dog bite victims. Hence, patients are either not treated at all, or asked to purchase their own vaccine, which most cannot afford, resulting in neglect and high incidence of rabies deaths. TRC-id regimen reduced the cost of vaccine to 1/5th of Essen regimen and is strongly recommended for institutions with large throughput. Training ED staff would save lives through a safe, effective and affordable technique.\\n\\nLay Summary\\n\\nRabies is a killer disease caused by the rabies virus that is present in the saliva of rabid animals, mainly the dog. Once symptoms become apparent, death is inevitable. However, rabies can be prevented if correct post exposure prophylaxis (PEP) is instituted as soon as possible after a dog bite and before symptoms of rabies begin. In Pakistan, government hospitals treat 50-70 new bite victims each day. Many still dispense the free but poor quality nerve tissue vaccine that is often ineffective and fraught with serious adverse reactions. Hospital administrators consider PEP too expensive to be administered free of cost. The Indus Hospital (TIH), Karachi is a private teaching hospital which provides free medical care to all. From July 2013-June 2014, 2,983 new bites were seen in the ED, and rather than use the Essen regimen of five full vial intramuscular doses per patient over 28 days, we administered the WHO-approved Thai Red Cross-intradermal (TRC-id) 4-dose regimen. The use of the TRC-id regimen resulted in 80% cost savings over the Essen regimen. In resource-poor settings, we advocate training of ED personnel in TRC-id regimen, which, ultimately, will result in less vaccine consumption, more patient compliance and complete treatment, resulting in more lives being saved.\\n\\nFigure 8: PLOS lay summary example.\"}"}
{"id": "emnlp-2022-main-724", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Technical Abstract\\n\\nAdult stem cells are responsible for life-long tissue maintenance. They reside in and interact with specialized tissue microenvironments (niches). Using murine hair follicle as a model, we show that when junctional perturbations in the niche disrupt barrier function, adjacent stem cells dramatically change their transcriptome independent of bacterial invasion and become capable of directly signaling to and recruiting immune cells. Additionally, these stem cells elevate cell cycle transcripts which reduce their quiescence threshold, enabling them to selectively proliferate within this microenvironment of immune distress cues. However, rather than mobilizing to fuel new tissue regeneration, these ectopically proliferative stem cells remain within their niche to contain the breach. Together, our findings expose a potential communication relay system that operates from the niche to the stem cells to the immune system and back. The repurposing of proliferation by these stem cells patch the breached barrier, stoke the immune response and restore niche integrity.\\n\\nLay Summary\\n\\nMost, if not all, tissues of an adult animal contain stem cells. These stem cells regenerate and repair damaged tissues and organs for the entire lifetime of an animal, contributing to a healthy life. They divide to make daughter cells that become either new stem cells or specialized cells of that organ. Adult stem cells exist in specific areas within tissues known as niches, where they interact with surrounding cells and molecules that inform their behavior. For example, cells and molecules within these niches can signal stem cells to remain in a \u2018dormant\u2019 state, but upon injury, they can mobilize stem cells to form new tissue and repair the wound. So far, it has been unclear how stem cells sense damage and stress and direct their efforts away from their normal duties towards repair. Here, Lay et al. studied the stem cells in the mouse skin that are responsible to regenerate hair. Every hair follicle contains a niche (the \u2018bulge\u2019), where these stem cells live and share their environment with cells that anchor the hair. The niche tethers to the stem cells through specific adhesion molecules that also help the niche to form a tight seal to prevent bacteria from entering. Lay et al. removed one of the adhesion molecules called E-cadherin, which caused a breach in the niche\u2019s barrier. The stem cells sensed their damaged niche, prepared to multiply, and sent out stress signals to the immune system. The immune cells then arrived at the niche and sent signals back to the stem cells, prodding them to multiply and patch the barrier, while at the same time, keeping the inflammation in check. This remarkable ability of the stem cells to recruit immune cells and initiate a dialogue with them enabled the stem cells to divert their attention from regenerating hair and instead directing it towards the site of the tissue damage. Other stem cells, such as those in the lung or gut, may have similar mechanisms to detect and respond to physical damage. It will be interesting to investigate the underlying mechanism of how immune cells are involved in balancing stem cell regenerative capacity and response to physical damage. A better knowledge of these processes could help to regenerate tissues or even entire organs.\\n\\nFigure 9: eLife lay summary example.\"}"}
