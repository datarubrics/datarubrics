{"id": "lrec-2022-1-44", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"KazNERD: Kazakh Named Entity Recognition Dataset\\n\\nRustem Yeshpanov, Yerbolat Khassanov, Huseyin Atakan Varol\\n\\nInstitute of Smart Systems and Artificial Intelligence, Nazarbayev University, Nur-Sultan, Kazakhstan\\n{rustem.yeshpanov, yerbolat.khassanov, ahvarol}@nu.edu.kz\\n\\nAbstract\\n\\nWe present the development of a dataset for Kazakh named entity recognition. The dataset was built as there is a clear need for publicly available annotated corpora in Kazakh, as well as annotation guidelines containing straightforward\u2014but rigorous\u2014rules and examples. The dataset annotation, based on the IOB2 scheme, was carried out on television news text by two native Kazakh speakers under the supervision of the first author. The resulting dataset contains 112,702 sentences and 136,333 annotations for 25 entity classes.\\n\\nState-of-the-art machine learning models to automatise Kazakh named entity recognition were also built, with the best-performing model achieving an exact match F1-score of 97.22% on the test set. The annotated dataset, guidelines, and codes used to train the models are freely available for download under the CC BY 4.0 licence from https://github.com/IS2AI/KazNERD.\\n\\nKeywords: Named Entity Recognition, NER, Kazakh, Dataset, Annotation Guidelines, CRF, BiLSTM, BERT\\n\\n1. Introduction\\n\\nNamed entity recognition (NER) refers to a subtask of information extraction aimed at identifying named entities (NEs) in semi- or unstructured text and classifying them into pre-specified types (Nadeau and Sekine, 2007). NEs, in turn, generally refer to (proper) names of persons, organisations, and geographical locations (Sang and Meulder, 2003), as well as numerical and temporal expressions, including quantities, monetary units, percentages, dates, or durations (Chinchor, 1998). Widely used in natural language processing applications, including automatic text understanding (Cheng and Erk, 2020), machine translation (Babych and Hartley, 2003), question answering (Aliod et al., 2006), and knowledge base development (Etzioni et al., 2005) to name a few, NER has been of interest not only to scientific research, but also to business (Sch\u00f6n et al., 2019) and defence (Han et al., 2020) ever since 1995, when the term was coined (Grishman and Sundheim, 1996).\\n\\nBy virtue of most of the early works in information extraction being launched as part of United States Government initiatives (e.g., ACE, MUC, TIPSTER) (Maynard et al., 2003), a great deal of research in NER concerns English. Nonetheless, an equally large proportion of NER research has been dedicated to different well-resourced languages, such as Spanish, French, German, Japanese, Chinese, Russian (see Nadeau and Sekine (2007), for a detailed overview), as well as to less resourced ones, such as Sindhi (Ali et al., 2020), Romanian (Dumitrescu and Avram, 2020), and Icelandic (Ing\u00f3lfsd\u00f3ttir et al., 2019). Likewise low-resourced, the language of interest of this paper\u2014Kazakh\u2014has only latterly appeared on the radar of NER researchers. Underrepresented and lexically underdeveloped because overshadowed by Russian, which was promoted as a lingua franca during the Soviet era (Dave, 2007), the earliest NER research in this agglutinative Turkic language dates back as recently as 2016. Although there is evidence for annotated corpus construction as part of Kazakh NER research (Akhmed-Zaki et al., 2020; Tolegen et al., 2016), to our knowledge, neither of the corpora is publicly available. In addition, none of the studies into Kazakh NER appears to have developed annotation guidelines\u2014or at least adapted those existing in other languages\u2014to take into account cases characteristic of the Kazakh language.\\n\\nGiven this relatively nascent stage of Kazakh NER accompanied by the digital underrepresentation of the language and the lack of freely accessible annotated corpora, it is hoped that our research will fill the existing gaps in the field and thus contribute to its further development. Particularly, we built a dataset consisting of 112,702 sentences from television news, of which 86,246 are unique sentences and 26,456 are their various representations. All sentences in the dataset were manually annotated by two native Kazakh-speaking linguists, supervised by the first author. This resulted in the largest Kazakh NE annotated corpus. To assist the annotators in making the right choices when presented with expressions potentially matching NEs, annotation guidelines in Kazakh were developed. The guidelines contain rules for annotating 25 NE types, as well as relatable examples of Kazakh NEs. Finally, we built four state-of-the-art machine learning models to automatise Kazakh NER, with the highest exact match F1-score reaching 97.22% on the test set.\\n\\nThe remainder of the paper proceeds as follows: Section 2 reviews existing research on Kazakh NER. Section 3 discusses data collection and preparation, the development of the guidelines and dataset. Section 4 provides the annotated dataset specifications, including the description of NEs, as well as the dataset structure and statistics. Section 5 offers the details of the implemented NER models, the experimental setup, and the evaluation criteria and results. Section 6 discusses the results of the experiment. Section 7 concludes the paper.\\n\\n2. Related Work\\n\\nAs mentioned earlier, Kazakh is a digitally low-resourced language, with a small number of (annotated) corpora freely available. That said, recently, there have been progressive efforts made to address such\"}"}
{"id": "lrec-2022-1-44", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Underrepresentation. Khassanov et al. (2021) have built a crowdsourced freely accessible Kazakh speech corpus (KSC) containing 332 hours of transcribed audio. In another work, Mussakhojayeva et al. (2021a) have constructed the first publicly available large-scale Kazakh text-to-speech synthesis dataset consisting of approximately 93 hours of transcribed audio recordings spoken by male and female professional narrators.\\n\\nWhile Kazakh speech processing research has been gathering momentum, thanks to the recent development of publicly available datasets, Kazakh NER research can hardly boast of commensurable progress, which appears to be chiefly due to a lack of such resources. One of the earliest studies into Kazakh NER was conducted by Sadykova and Ivanov (2016). To build a manually-annotated Kazakh NE corpus, two experts were tasked with labelling 1,000 news articles with a set of seven NEs\u2014namely, (1) person, (2) organisation, (3) location, (4) geopolitical entity (GPE), (5) event, (6) award, and (7) tender\u2014using the brat rapid annotation tool (BRAT) (Stenetorp et al., 2012). Approximately 3,000 NEs are reported to have been tagged, of which 1,084 were persons, 974 locations, and 973 organisations. However, no breakdown of the remaining NEs is provided in the paper, nor is reference made to the metric applied to achieve an inter-annotator agreement (IAA) score of 0.86\u20130.89 (Artstein and Poesio, 2008). Another criticism is that, while the annotation guidelines are reported to have been developed specifically for the task, there is no mention of how to access them or the resulting annotated corpus.\\n\\nTolegen et al. (2016) created a Kazakh NE corpus, annotated according to the IOB (Inside, Outside, Beginning) scheme, from 2,500 general news media articles. The corpus is reported to consist of 18,054 sentences and 270,306 words. Annotation was performed using a self-developed web-based tool, with two native Kazakh speakers using the MUC-7 NE task definition (Chinchor, 1998) as a guide. More than 14,000 NEs were labelled in three categories: 4,292 persons, 7,391 locations, and 2,560 organisations. The IAA measured with Fleiss' kappa ranged from 0.93 to 0.98 (Fleiss, 1971). Furthermore, the scholars conducted an extensive analysis of Kazakh morphological and word type features and were the first to apply a statistical model to Kazakh NER based on conditional random fields (CRFs) (Lafferty et al., 2001), achieving an $F_1$-score of 89.81%.\\n\\nThe same model was used as a baseline in Tolegen et al. (2020), where the researchers approached the Kazakh NER task by comparing (1) a bidirectional long short-term memory (BiLSTM) model (Hochreiter and Schmidhuber, 1997), (2) BiLSTM with CRF (BiLSTM-CRF), and (3) a tensor layer-based deep neural network (DNN) model. While the performance of the BiLSTM model yielded a result significantly lower than that of the baseline model (78.76%), the performance of the BiLSTM-CRF model varied depending on whether or not character embedding was used, 86.45% and 80.28%, respectively. The DNN model outperformed the other models, producing an $F_1$-score of 90.49%. Although the three models were trained on the annotated corpus built in Tolegen et al. (2016), neither of the studies provides information on access to it.\\n\\nIn Kozhirbayev and Yessenbayev (2020), an annotated NE corpus comprising 29,629 sentences was constructed in the IOB format, with the names of persons, organisations, and locations tagged along with Other, a category for NEs of interest that presumably fall outside the three said categories. Four methods to address the Kazakh NER task were applied\u2014specifically, (1) the random forest classifier (Ho, 1995), (2) the Naive Bayes classifier (Friedman et al., 1997), (3) CRFs, and (4) a hybrid method of BiLSTM and CRF. The results show that, while the first two methods achieved an $F_1$-score in the range of 81% to 89%, the hybrid method was notably outperformed by the CRFs, 88% versus 99%, in turn. However, the study included no information on what guidelines were followed to build the corpus, the quantities of NEs in the corpus, and how, if any, annotation accuracy checks were performed.\\n\\nKuralbayev et al. (2020) compared four NER models\u2014(1) CRFs, (2) LSTM with character embedding, (3) LSTM-CRF, and (4) bidirectional encoder representations from transformers (BERT) (Devlin et al., 2019)\u2014to anonymise 40,000 court decisions in Kazakh and Russian. The names of persons, organisations, locations and addresses were tagged using a self-built annotation tool. The scholars note that the BERT model, which was run without fine-tuning, reached an $F_1$-score of 87%, with the results of the other models peaking at 82%. Nevertheless, some notes of caution are warranted here, because, although the model is reported to have achieved high accuracy for both Kazakh and Russian, it was trained exclusively on Russian data. Furthermore, surprisingly, no mention is made of the guidelines used or the IAA assessment, considering that the annotation was carried out by over 150 local university students recruited for the task. Nor is it stated how many NEs were anonymised as a result.\\n\\nThe last study on Kazakh NER we discuss in this paper is by Akhmed-Zaki et al. (2020), who applied the BiLSTM, CRF, and BERT methods to a dataset collected from Kazakh online news portals. The dataset was manually annotated using the IOB scheme with four NEs\u2014(1) persons, (2) locations, (3) organisations, and (4) other. In this study, too, the BERT model performed the best with an $F_1$-score of 97.99%, followed by CRF (94.27%) and BiLSTM (85.31%). While the study provides clear information on the parameters of the BERT model and formulae for the precision, recall, and $F_1$-scores computed, it is still limited by the lack of clarity on the volume of the data. Although the dataset built is claimed to consist of 7,153 sentences, the scholars explicitly state that it was split into 6,507, 2,531, and 3,015 sentences for training, validation, and test sets, respectively, which is 12,053 sentences in the aggregate. It is also unclear whether the category Other was used for NEs that were not names of persons, locations, and organisations, but were still of interest (see, e.g., Kozhirbayev and Yessenbayev (2020)), or whether it simply referred to a category of words that are not annotated as NEs and are labelled as O in the IOB scheme. Much like in the previous studies, no reference is made to the annotation guidelines adhered, the annotators and their backgrounds, the measurement of IAA, and the means of accessing the annotated dataset.\"}"}
{"id": "lrec-2022-1-44", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Details of sentence representations, including designations, sentence count of each representation variant, and an example sentence translated as 'Dow Jones has depreciated by 5.86%.'\\n\\n| Designation | Description | Sentence Count | Example Sentence |\\n|-------------|-------------|----------------|-----------------|\\n| AID         | All sentence elements recorded in Cyrillic script | 809 sentences | Dow Jones |\"}"}
{"id": "lrec-2022-1-44", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.4. Annotation Guidelines\\n\\nConsidering that none of the studies on Kazakh NER provided Kazakh annotation guidelines that our study could rely on to embark on the task, we decided to create such a set of rules. First, we studied some of the most referenced annotation guidelines for NER\u2014particularly, Chinchor (1998), Brunstein (2002), Raytheon BBN Technologies (2004), Linguistic Data Consortium (2008), and Weischedel et al. (2012). Next, the first author experimentally annotated a random sample of 2,000 sentences to see what NEs could actually be extracted from the data on hand. Twenty-two NEs described in the guidelines studied were found in the sample. The first draft of the annotation guidelines containing the definition of an NE, information on the valid boundaries of NEs, rules for NE classification, and related examples was prepared in Kazakh.\\n\\nLater, as a result of the annotator training task, it was decided to tag three more NEs whose examples were found in the news reports annotated. The NEs under consideration were NON_HUMAN, MISCELLANEOUS, and ADAGE. While the first two had been previously mentioned in the existing annotation guidelines for NER, the decision to tag ADAGE rested upon the relatively frequent use of Kazakh proverbs and sayings in the training sentences. Due adjustments were made to the guidelines, with some rules clarified and supported by comprehensible examples.\\n\\nIt is also worth mentioning at this point that the guidelines were iteratively amended as annotation proceeded. This was partly due to subsequent encounters with cases unconsidered while drafting the guidelines and partly as a result of daily discussions of questions posed by the annotators hired for the task. For a complete list of the 25 NEs and their brief descriptions, see Table 3. The final annotation guidelines (in Kazakh) are available for download from our GitHub repository.\\n\\n3.5. Annotation Workflow\\n\\nTwo native Kazakh-speaking linguists received training in NER for two weeks under the supervision of the first author. As part of training, 3,500 sentences from the Khabar agency's official website were annotated, by following the developed guidelines. The annotation was carried out using the Webanno web-based tool (Yimam et al., 2013) (see (Neves and Seva, 2021) for an extensive review of various tools for annotation). The annotators worked independently on the same version of a text file, which was subsequently reviewed by the first author for annotation divergences and inconsistencies. The final version of the file contained text with annotations approved or modified as appropriate by the first author. During the training period, the IAA score, computed by Webanno, reached a Fleiss' kappa of 0.94.\\n\\nThe annotation process proceeded for six months, with the annotators labelling 1,500 sentences per day and the first author inspecting these once they were marked complete. During the period, the IAA score was in the range of 0.95 to 0.97 Fleiss' kappa. Table 3 provides the statistics for annotated NEs.\\n\\n4. KazNERD Specifications\\n\\n4.1. Named Entity Descriptions\\n\\nThe resulting annotated Kazakh NER dataset (hereafter KazNERD) contains 136,333 NEs. As can be seen from Table 3, the top three NEs in KazNERD are CARDINAL, DATE, and GPE. None of the previous Kazakh NER studies has labelled the first two classes. The latter class, embracing names of geopolitical entities, has often been conflated with names of geographical locations under the class LOCATION.\\n\\nSince news reports are normally preceded by (or at least contain) the day or time when a particular event occurred, the frequent use of dates in the dataset was expectable\u2014a total of 25,446 DATE NEs. What is indeed remarkable is the use of numbers in KazNERD. The two classes denoting numbers, CARDINAL (29,260) and ORDINAL (3,870), comprise practically a quarter of the total quantity of NEs in the dataset, a hefty 24.3%.\\n\\nInterestingly, in KazNERD, the triad of NEs most commonly labelled in Kazakh NER research\u2014locations (2,175), persons (13,577), and organisations (7,587)\u2014ranks only third through fifth, even when GPE (17,543) is combined with LOCATION. Also worthy of note is the class ADAGE. The class deriving purely from our observations of Kazakh news and hardly fitting the conventional profile of an NE per se (but rather labelled out of scholarly interest) numbered 196 entities in total. This is higher or comparable in size to the classes MISCELLANEOUS, CONTACT, and NON_HUMAN, previously described as relatively frequent in the NER literature.\\n\\nThere are only eight instances of the class NON_HUMAN, which includes names of creatures other than humans. Such a scarcity of the NEs in the dataset was expected, given that the source data came from television news, which generally reports real-life events. Nevertheless, it was decided to label the NEs as a separate class for consistency with the existing annotation guidelines for NER.\\n\\nAs regards MISCELLANEOUS, the class embraces names of school and university subjects, types of computer networks and technologies, livestock breeds, and other entities that we had difficulty in categorising or deemed superfluous to label as separate classes.\\n\\nThe remaining NEs in KazNERD have been commonly annotated in the existing NER literature and guidelines. The relatively high ranking of the POSITION class (seventh overall, with 6,141 NEs) can be attributed to the domain of television news, which frequently reports on resolutions and activities of individuals holding official titles and occupational positions. The same applies to news reports on the economy, finance, trade, legal frameworks, business and political objectives, and technology in the country in particular and in the world in general, resulting in NEs annotated for the classes MONEY, PERCENTAGE, QUANTITY, PROJECT, PRODUCT, and LAW, accounting for a total of 11.29% of all NEs found in KazNERD.\\n\\nThe names of national and international cultural and political events, as well as the times and venues at which these were held; geographical, ethnic, and religious origins of persons participating in the events among other things;\"}"}
{"id": "lrec-2022-1-44", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: A list of 25 NEs, their short description and statistics\\n\\n| No. | NE Class | Definition | Size | %  |\\n|-----|----------|------------|------|----|\\n| 1   | (ADA)GE | Well-known Kazakh proverbs and sayings | 196  | 0.14 |\\n| 2   | ART     | Titles of books, songs, television programmes, etc. | 2,407 | 1.77 |\\n| 3   | (CAR)DINAL | Cardinal numbers, including whole numbers, fractions, and decimals | 29,260 | 21.46 |\\n| 4   | (CON)TACT | Addresses, emails, phone numbers, URLs | 198  | 0.15 |\\n| 5   | DATE    | Dates or periods of 24 hours or more | 25,446 | 18.66 |\\n| 6   | (DIS)EASE | Diseases or medical conditions | 1,272 | 0.93 |\\n| 7   | (EVE)NT | Named events and phenomena | 1,658 | 1.22 |\\n| 8   | (FAC)ILITY | Names of man-made structures | 2,145 | 1.57 |\\n| 9   | GPE     | Names of geopolitical entities | 17,543 | 12.87 |\\n| 10  | (LAN)GUAGE | Named languages | 443  | 0.32 |\\n| 11  | LAW     | Named legal documents | 533  | 0.39 |\\n| 12  | (LOC)ATION | Names of geographical locations other than GPEs | 2,175 | 1.60 |\\n| 13  | (MIS)CELLANEOUS | Entities of interest but hard to assign a proper tag to | 244  | 0.18 |\\n| 14  | (MON)EY | Monetary values | 4,560 | 3.34 |\\n| 15  | (NON)_HUMAN | Names of pets, animals or non-human creatures | 8    | 0.01 |\\n| 16  | NORP    | Adjectival forms of GPE and LOCATION; named religions, etc. | 3,714 | 2.72 |\\n| 17  | (ORD)INAL | Ordinal numbers, including adverbials | 3,870 | 2.84 |\\n| 18  | (ORG)ANISATION | Names of companies, government agencies, etc. | 7,587 | 5.57 |\\n| 19  | (PERC)ENTAGE | Percentages | 4,283 | 3.14 |\\n| 20  | (PER)SON | Names of persons | 13,577 | 9.96 |\\n| 21  | (POS)ITION | Names of posts and job titles | 6,141 | 4.50 |\\n| 22  | (PROD)UCT | Names of products | 738  | 0.54 |\\n| 23  | (PROJ)ECT | Names of projects, policies, plans, etc. | 2,111 | 1.55 |\\n| 24  | (QUA)NTITY | Length, distance, etc. measurements | 3,908 | 2.87 |\\n| 25  | TIME    | Times of day and time duration less than 24 hours | 2,316 | 1.70 |\\n\\nTotal number of named entities: 136,333 (100%)\\n\\nNote. The parenthesised NE classes will thus be referenced in the tables hereafter.\\n\\nWorks of art and the languages in which these were produced, are reflected in labelling the classes EVENTS (1,658), NORP (3,714), TIME (2,316), FACILITY (2,145), ART (2,407), and LANGUAGE (443).\\n\\nLastly, the comparatively frequent instances of the class DISEASE (1,272 NEs) in KazNERD may be explained by two interrelated factors. First, at the time of conducting the present study, the coronavirus disease 2019 (COVID-19) pandemic received massive public attention, which led to the source data often reflecting information on the outbreak of the disease across the country and worldwide. Second, the national media regularly discussed symptoms of various diseases similar to those observed in individuals infected with COVID-19, which resulted in the names of the diseases appearing in the source news reports.\\n\\n4.2. Structure and Statistics\\n\\nTo allow reproducibility of the NER experiment between different research groups, KazNERD was split into three sets: training (80%), validation (10%), and test (10%). Table 4 provides statistical information on the number of tokens, sentences, and NEs in the dataset and per set. An evenly proportional distribution of sentence representations and NEs across the sets was ensured. We also saw to it that a sentence and its representations were only assigned to the same set. More detailed information on the numbers of NEs and sentence representations across the three sets can be found in Tables 5 and 6.\\n\\nFurthermore, we extracted all unique NEs from KazNERD and computed the intersection between the training, validation, and test sets (see Figure 1). The total numbers of unique NEs in the training, validation, and test sets are 33,177, 6,547, and 6,742, respectively. We found that 42% of the unique NEs in the test set do not appear in the training and validation sets, which confirms its suitability for evaluating the generalisation capability of the NER models.\\n\\nThe three sets are stored in separate files, in the CoNLL-2002 format (Tjong Kim Sang, 2002)\u2014that is, all files contain one token and the corresponding NE tag per line, with blank new lines representing sentence boundaries (see Table 2). Tokens and IOB2 tags are separated by a single space. Additionally, we provide variants of the training, validation, and test sets. The number of sentences is 90,228, 11,167, and 11,307 for the training, validation, and test sets, respectively. The number of tokens is 1,043,305, 129,223, and 129,824, respectively. The number of NEs is 109,342, 13,483, and 13,508, respectively. The proportion of sentences, tokens, and NEs is 80.06%, 9.91%, 10.03% for the training set; 80.11%, 9.92%, 9.97% for the validation set; and 80.20%, 9.89%, 9.91% for the test set.\\n\\nTable 4: The statistics for the training, validation, and test sets of KazNERD\"}"}
{"id": "lrec-2022-1-44", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"files containing identifiers heading each sentence, to allow for more nuanced studies requiring representation- and sentence-level detail. The sentence identifiers are formed by combining representation designations (i.e., AID, BID, CID, DID, EID, and FID) with a unique six-digit sentence number, for example, AID123456. Sentences with multiple representations have the same six-digit number but different designations, for example, AID111111 and BID111111.\\n\\nFigure 1: A Venn diagram depicting the intersection of unique NEs between the training, validation, and test sets of KazNERD\\n\\n| Entity class | Train # | Train % | Valid # | Valid % | Test # | Test % |\\n|--------------|---------|---------|---------|---------|--------|--------|\\n| ADA          | 159     | 0.15    | 18      | 0.13    | 19     | 0.14   |\\n| ART          | 1,953   | 1.79    | 225     | 1.67    | 229    | 1.70   |\\n| CAR          | 23,550  | 21.54   | 2,886   | 21.40   | 2,824  | 20.91  |\\n| CON          | 160     | 0.15    | 18      | 0.13    | 20     | 0.15   |\\n| DATE         | 20,226  | 18.50   | 2,609   | 19.35   | 2,611  | 19.33  |\\n| DIS          | 1,031   | 0.94    | 118     | 0.88    | 123    | 0.91   |\\n| EVE          | 1,352   | 1.24    | 150     | 1.11    | 156    | 1.15   |\\n| FAC          | 1,752   | 1.60    | 195     | 1.45    | 198    | 1.47   |\\n| GPE          | 14,108  | 12.90   | 1,693   | 12.56   | 1,742  | 12.90  |\\n| LAN          | 352     | 0.32    | 45      | 0.33    | 46     | 0.34   |\\n| LAW          | 424     | 0.39    | 54      | 0.40    | 55     | 0.41   |\\n| LOC          | 1,759   | 1.61    | 204     | 1.51    | 212    | 1.57   |\\n| MIS          | 194     | 0.18    | 24      | 0.18    | 26     | 0.19   |\\n| MON          | 3,678   | 3.36    | 441     | 3.27    | 441    | 3.26   |\\n| NORP         | 2,972   | 2.72    | 370     | 2.74    | 372    | 2.75   |\\n| ORD          | 3,105   | 2.84    | 379     | 2.81    | 386    | 2.86   |\\n| ORG          | 6,093   | 5.57    | 759     | 5.63    | 735    | 5.44   |\\n| PERC         | 3,384   | 3.09    | 443     | 3.29    | 456    | 3.38   |\\n| PER          | 10,893  | 9.96    | 1,352   | 10.03   | 1,332  | 9.86   |\\n| POS          | 4,937   | 4.52    | 601     | 4.46    | 603    | 4.46   |\\n| PROD         | 592     | 0.54    | 73      | 0.54    | 73     | 0.54   |\\n| PROJ         | 1,694   | 1.55    | 206     | 1.53    | 211    | 1.56   |\\n| QUA          | 3,094   | 2.83    | 407     | 3.02    | 407    | 3.01   |\\n| TIME         | 1,874   | 1.71    | 212     | 1.57    | 230    | 1.70   |\\n| Total        | 109,342 | 100     | 13,483  | 100     | 13,508 | 100    |\\n\\nTable 5: The distribution of NEs across the training, validation, and test sets of KazNERD\\n\\n| Representation | Train # | Train % | Valid # | Valid % | Test # | Test % |\\n|---------------|---------|---------|---------|---------|--------|--------|\\n| AID           | 69,017  | 76.49   | 8,549   | 76.56   | 8,680  | 76.77  |\\n| BID           | 19,236  | 21.32   | 2,368   | 21.21   | 2,365  | 20.92  |\\n| CID           | 1,059   | 1.17    | 140     | 1.25    | 141    | 1.25   |\\n| DID           | 644     | 0.71    | 81      | 0.73    | 84     | 0.74   |\\n| EID           | 263     | 0.29    | 28      | 0.25    | 35     | 0.31   |\\n| FID           | 9       | 0.01    | 1       | 0.01    | 2      | 0.02   |\\n| Total         | 90,228  | 100     | 11,167  | 100     | 11,307 | 100    |\\n\\nTable 6: The distribution of sentence representations across the training, validation, and test sets of KazNERD\\n\\n5. NER Experiment\\n\\n5.1. NER Methods\\n\\nWe applied several state-of-the-art machine learning methods to evaluate the KazNERD corpus. Detailed information on the NER model implementation and feature construction can be found in our GitHub repository.\\n\\nCRF\\n\\nWe applied the CRF models implemented by the CRFsuite toolkit (Okazaki, 2007). Specifically, we used the features derived from the surface forms of tokens, including target and context token prefixes, suffixes, and shape features. We note that the CRF models do not incorporate external linguistic resources, such as gazetteers, lookup tables, or word vector features.\\n\\nBiLSTM-CNN-CRF\\n\\nWe used the PyTorch implementation of a BiLSTM-CNN-CRF model (Ma and Hovy, 2016). The model combines the word embeddings with the character-level representations extracted using the CNN and feeds them into the BiLSTM module with the CRF output layer. Word embeddings are usually pre-trained on large unlabelled corpora, but, in the present study, we used randomly initialized embeddings.\\n\\nBERT\\n\\nA pre-trained BERT model can be readily applied to the NER task, by reinitializing the output layer size to match the NE labels and fine-tuning the model on the NER data. We used the case-sensitive version of the multilingual BERT model within the Hugging Face Transformers framework (Wolf et al., 2020). The model consists of around 110M parameters and was pre-trained on 104 languages with the largest Wikipedia content, which includes the Kazakh language as well.\\n\\nXLM-RoBERTa\\n\\nWe also applied the XLM-RoBERTa model (Conneau et al., 2020), a multilingual version of RoBERTa (Liu et al., 2019), within the Hugging Face Transformers framework. Similar to BERT, it was adapted for the NER task, by reinitializing the output layer and fine-tuning. The rationale behind choosing the model lies in the fact that it has over five times as many parameters as BERT does (560M) and was pre-trained on CommonCrawl data containing 100 languages, Kazakh included.\\n\\n5.2. Experimental Setup\\n\\nThe four NER models were trained on the training set. The hyperparameters were tuned on the validation set. The final, best-performing, model was evaluated on the test set. The deep learning-based models utilised a single V100 GPU on an NVIDIA DGX-2 machine.\"}"}
{"id": "lrec-2022-1-44", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Experiment results of four NER models on the validation and test sets of KazNERD\\n\\n| Model          | Precision | Recall  | F1-score | Precision | Recall  | F1-score |\\n|----------------|-----------|---------|----------|-----------|---------|----------|\\n| CRF            | 93.62     | 91.93   | 92.77    | 93.20     | 91.63   | 92.41    |\\n| BiLSTM-CNN-CRF | 94.51     | 93.72   | 94.11    | 93.84     | 93.18   | 93.51    |\\n| BERT           | 96.30     | 96.07   | 96.19    | 96.14     | 96.34   | 96.24    |\\n| XLM-RoBERTa    | 97.20     | 97.18   | 97.19    | 97.09     | 97.35   | 97.22    |\\n\\nThe CRF model was run for 550 iterations using the L-BFGS training algorithm, with the $L_1$ and $L_2$ regularisation terms set to 0.1 and 0.01, respectively. The other hyperparameters were left at their default values of the CRFsuite toolkit.\\n\\nFor the BiLSTM-CNN-CRF model, we used a single BiLSTM layer with 256 hidden units and a CNN layer with 30 filters of size 3. The word and character embedding sizes were set to 100 and 30, respectively. We chose an initial learning rate of 0.005 and a batch size of 1,024. To prevent overfitting, a dropout rate of 0.5 was applied. The model was trained for 1,000 epochs using the Adam optimizer and the early stopping criteria based on the validation set, which yielded the highest score on epoch 432.\\n\\nThe BERT model was fine-tuned for 8 epochs, with the initial learning rate set to $5 \\\\times 10^{-5}$ and the weight decay rate set to $10^{-4}$. We set the batch size to 128 and applied 3,000 warmup steps. Likewise, the XLM-RoBERTa model was fine-tuned for 10 epochs, with the initial learning rate set to $10^{-5}$ and the weight decay rate set to $10^{-3}$. We set the batch size to 64 and applied 800 warmup steps.\\n\\n### 5.3. Evaluation Criteria\\n\\nWe evaluate NER performance in terms of exact match using precision, recall and F1-score (Nadeau and Sekine, 2007) and the standard seqeval script (Nakayama, 2018), requiring that both the type and span of predicted NEs match the gold standard mention.\\n\\n### 5.4. Experiment Results\\n\\nTable 7 presents the performance of the NER models on the validation and test sets of KazNERD, measured by micro-averaging (Yang, 2001). The highest results were achieved by XLM-RoBERTa, followed by BERT, BiLSTM-CNN-CRF and CRF. Specifically, XLM-RoBERTa achieved relative improvements of 1%, 4%, and 5% over BERT, BiLSTM-CNN-CRF, and CRF, respectively. In general, all the NER models performed well, achieving precision, recall, and F1-scores of above 90%, highlighting the utility of our annotated dataset for the Kazakh NER task. The results of the XLM-RoBERTa model for different NEs are shown in Table 8 and will be discussed in the following section.\\n\\n### 6. Discussion\\n\\nThe performance of XLM-RoBERTa was above 95% for 14 NE classes and in the range of 85% to 95% for eight classes. Only three classes were predicted with an F1-score below 85%. The model yielded almost perfect results for MONEY (99.89%) and PERSON (99.36%). This could be explained by the composition of these classes. The extent of MONEY NEs includes a monetary value and an explicit monetary unit (e.g., 50 dollars). This must have made it easier for the model to recognize the class, for monetary units in KazNERD are not very diverse, with \u201ctenge\u201d (the local currency), \u201cdollar\u201d, and \u201ceuro\u201d making frequent appearances. Likewise, in Kazakh, PERSON NEs often appear as a combination of first and last names, with both capitalised and the latter normally ending in \u2013\u043e\u0432(\u0430), \u2013\u0435\u0432(\u0430), \u2013\u0438\u043d(\u0430)\u201d. These features presumably enabled the model to achieve high prediction accuracy for the class.\\n\\nThe low F1-scores for NON_HUMAN (0%) and ADAGE (64.52%) on the test set could be due to the apparent insufficiency of instances of the former in the dataset and the form variability of the latter. Increasing the number of NON_HUMAN NEs in the training sample, by expanding...\"}"}
{"id": "lrec-2022-1-44", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the dataset to embrace domains where the use of names of non-humans is expected (e.g., science fiction, children's stories, or animal fantasies) will likely improve the accuracy of the model. As for ADAGE NEs, they are generally easy to recognise in context thanks to their form fixedness (e.g., No smoke without fire). Lexical and grammatical variations of proverbs and sayings are possible (e.g., There is no smoke without fire or Where there is smoke, there is fire), but still unlikely to preclude humans from continuing to identify these: such phrases bear greater psychological and social significance than do other set expressions (Norrick, 2015). However, this can hardly apply to a machine learning model, which will struggle to decide whether a given expression is a pre-existent variation of a known adage, its nonce restructuring, or not an adage at all, especially if there is inadequacy of data to make inferences from. As mentioned earlier, the class ADAGE was labelled as a result of our scientific curiosity, and further review and investigation as to the worth of this class for the NER task is required.\\n\\nSince the present study was, to the best of our knowledge, the first to develop a publicly available annotated corpus as well as guidelines in Kazakh for 25 NE classes, it was subject to several challenges. Firstly, although NER generally implies the recognition of proper nouns in text, which are expected to be capitalised given their designation of names of persons, places, organisations and so forth, some Kazakh nouns assigned to certain NE classes in our dataset do not seem to meet this criterion. For example, the NEs \u0434/uni04AF\u0439\u0441\u0435\u043d\u0431i \u201cMonday\u201d (DATE), \u0445\u0440\u0438\u0441\u0442\u0438\u0430\u043d\u0434\u0430\u0440 \u201cChristians\u201d (NORP), or \u0430/uni0493\u044b\u043b\u0448\u044b\u043d \u0442i\u043bi \u201cEnglish\u201d (LANGUAGE) to name a few, are normally lower-cased in Kazakh, unless they appear at the beginning of a sentence. Further studies on Kazakh NER taking such cases into account need to be undertaken.\\n\\nNE coordination posed another problem. The challenge concerns whether two (or more) coordinated NEs, for example, \u041e\u043b\u0436\u0430\u0441 \u043f\u0435\u043d \u0410\u0438\u043d\u0430 /uni049A\u043e\u0440/uni0493\u0430\u043d\u0431\u0435\u043a \u201cOlzhas and Aina Qorganbek\u201d (the names of a husband and a wife followed by their family name; PERSON) or \u0411\u0430\u0439\u0442/uni04B1\u0440\u0441\u044b\u043d\u043e\u0432 \u043f\u0435\u043d /uni049A\u043e\u043d\u0430-\u043a/uni04E9\u0448\u0435\u043b\u0435\u0440i\u043d\u0434\u0435 \u201con Baitursynov and Qonayev Streets\u201d (the names of two local streets followed by the word \u201cstreets\u201d; FACILITY) ought to be labelled as a single NE or two separate NEs. Although MUC-6 (Grishman and Sundheim, 1996) originally advocated the separate use of annotations, in KazNERD, it was decided to label coordinated NEs as a single entity in accordance with the recommendations of MUC-7 (Chinchor, 1998), promoting joint annotation.\\n\\nAnother similar issue was related to nested entities: for example, should the expression /uni049A\u0430\u0437\u0430/uni049B\u0441\u0442\u0430\u043d \u041f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442i \u201cThe President of Kazakhstan\u201d be considered two entities /uni049A\u0430\u0437\u0430/uni049B\u0441\u0442\u0430\u043d (Kazakhstan, GPE) and \u041f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442i (President, POSITION) or a single entity /uni049A\u0430\u0437\u0430/uni049B\u0441\u0442\u0430\u043d \u041f\u0440\u0435-\u0437\u0438\u0434\u0435\u043d\u0442i (The President of Kazakhstan, POSITION)? Here again, our decision was guided by MUC-7, encouraging the annotation of such expressions as a single NE. Thus, while developing KazNERD, we chose not to decompose compound entities and not to label subentities. However, future research into Kazakh NER should consider these challenges, with the decision as to which of the approaches is more likely to cover the needs of application areas left to the discretion of those concerned.\\n\\nAs regards challenges related to metonymy (i.e., the use of the name of something to refer to that of something else that is closely associated with it, as in Downing Street to refer to the British Prime Minister), consistent with the MUC recommendations, KazNERD generally retains the semantics of common NEs, unless otherwise specified in the developed annotated guidelines. Thus, in \u0410\u0431\u0430\u0439\u0434\u044b \u0442\u0430\u043d\u0443 \u201ccognising Abai\u201d (the name of a great Kazakh poet), the NE \u0410\u0431\u0430\u0439\u0434\u044b is tagged as PERSON, despite the contextual reference to the person's literary works (the NE class ART). This should certainly be borne in mind by enthusiasts willing to make use of KazNERD.\\n\\nSimilarly, challenges presented by the ambiguity between the classes ORGANISATION and FACILITY may presumably account for the comparatively low F1-score for the latter. In the annotation guidelines, we recommend that, in cases of confusion, preference should be given to ORGANISATION when actions normally characteristic of persons (e.g., say, state, report etc.) are used with names of institutions or if a building houses an institution of the same name, unless explicitly referring to the physical structure alone in a locative manner. Yet, in cases where the distinction is still not clear-cut, such as \u041f\u0440\u0435\u0437\u0438\u0434\u0435\u043d\u0442 ... \u0410/uni049B\u043e\u0440\u0434\u0430\u0434\u0430 \u0430\u0440\u043d\u0430\u0439\u044b \u043a\u0435/uni04A3\u0435\u0441 /uni04E9\u0442\u043ai\u0437\u0434i \u201cPresident ... held a special meeting in Akorda\u201d (the official workplace of the President of Kazakhstan), we annotated \u0410/uni049B\u043e\u0440\u0434\u0430\u0434\u0430 as ORGANISATION in line with the existing guidelines tagging White House or Kremlin as ORGANISATION, in spite of the contextual reference to the facility.\\n\\n7. Conclusion\\nThe present study set out to develop the first publicly available annotated dataset for Kazakh NER. The resulting dataset, KazNERD, contains 112,702 sentences from the television news domain and 136,333 annotations for 25 entity classes. All NEs were labelled using the IOB2 scheme by two native Kazakh speakers under the supervision of the first author, in accordance with the annotation guidelines specially designed in and for the Kazakh language. To automate Kazakh NER, state-of-the-art machine learning models were built, with the best-performing model yielding an exact match F1-score of 97.22% on the test set. In the future, we aim to focus on developing fine-grained and domain-independent NER models to ensure their external validity. To this end, we intend to train the models on a version of KazNERD supplemented with annotated data from different domains and genres, including transcribed conversations from television and radio shows, podcasts, phone talks, fiction, and senate speeches.\\n\\nThe annotated dataset, guidelines, and codes used in training the models can be freely downloaded under the CC BY 4.0 licence from https://github.com/IS2AI/KazNERD.\\n\\n8. Acknowledgements\\nOur very special thanks go to Aigerim Kabduluakhitova and Aizhan Seipanova\u2014the two annotators, who demonstrated their expertise, diligence, and continued patience throughout the whole process of developing KazNERD.\"}"}
{"id": "lrec-2022-1-44", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bibliographical References\\n\\nAkhmed-Zaki, D., Mansurova, M., Barakhnin, V., Kubis, M., Chikibayeva, D., and Kyrgyzbayeva, M. (2020). Development of Kazakh named entity recognition models. In International Conference on Computational Collective Intelligence (ICCCI), volume 12496 of Lecture Notes in Computer Science, pages 697\u2013708. Springer.\\n\\nAli, W., Lu, J., and Xu, Z. (2020). SiNER: A large dataset for Sindhi named entity recognition. In Proceedings of the Language Resources and Evaluation Conference (LREC), pages 2953\u20132961. European Language Resources Association.\\n\\nAliod, D. M., van Zaanen, M., and Smith, D. (2006). Named entity recognition for question answering. In Proceedings of the Australasian Language Technology Workshop (ALTA), pages 51\u201358. ALTA.\\n\\nArtstein, R. and Poesio, M. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555\u2013596.\\n\\nBabych, B. and Hartley, A. (2003). Improving machine translation quality with automatic named entity recognition. In Proceedings of the International EAMT Workshop on MT and Other Language Technology Tools, pages 1\u20138.\\n\\nBrunstein, A. (2002). Annotation guidelines for answer types. LDC2005T33, Linguistic Data Consortium, Philadelphia.\\n\\nCheng, P. and Erk, K. (2020). Attending to entities for better text understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7554\u20137561.\\n\\nChinchor, N. A. (1998). Overview of MUC-7. In Proceedings of the Message Understanding Conference (MUC). ACL.\\n\\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). Unsupervised cross-lingual representation learning at scale. In Proceedings of the Association for Computational Linguistics (ACL), pages 8440\u20138451. Association for Computational Linguistics.\\n\\nDave, B. (2007). Kazakhstan: Ethnicity, language and power. Routledge.\\n\\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 4171\u20134186. ACL.\\n\\nDumitrescu, S. D. and Avram, A. (2020). Introducing RONEC - the Romanian named entity corpus. In Proceedings of the Language Resources and Evaluation Conference (LREC), pages 4436\u20134443. European Language Resources Association.\\n\\nEtzioni, O., Cafarella, M., Downey, D., Popescu, A.-M., Shaked, T., Soderland, S., Weld, D. S., and Yates, A. (2005). Unsupervised named-entity extraction from the Web: An experimental study. Artificial Intelligence, 165(1):91\u2013134.\\n\\nFleiss, J. L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378\u2013382.\\n\\nFriedman, N., Geiger, D., and Goldszmidt, M. (1997). Bayesian Network Classifiers. Machine Learning, 29(2-3):131\u2013163.\\n\\nGrishman, R. and Sundheim, B. (1996). Message understanding conference-6: A brief history. In Proceedings of the International Conference on Computational Linguistics (COLING), pages 466\u2013471.\\n\\nHan, X., Ben, K., and Zhang, X. (2020). Research on Named Entity Recognition Technology in Military Software Testing. Journal of Frontiers of Computer Science and Technology, 14(5):740\u2013748.\\n\\nHo, T. K. (1995). Random decision forests. In Proceedings of the International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 278\u2013282. IEEE.\\n\\nHochreiter, S. and Schmidhuber, J. (1997). Long Short-Term Memory. Neural Computation, 9(8):1735\u20131780.\\n\\nIng\u00f3lfsd\u00f3ttir, S. L., Porsteinsson, S., and Loftsson, H. (2019). Towards high accuracy named entity recognition for Icelandic. In Proceedings of the Nordic Conference on Computational Linguistics, pages 363\u2013369. Link\u00f6ping University Electronic Press.\\n\\nKhassanov, Y., Mussakhojayeva, S., Mirzakhmetov, A., Adiyev, A., Nurpeiissov, M., and Varol, H. A. (2021). A crowdsourced open-source Kazakh speech corpus and initial speech recognition baseline. In Proceedings of the European Chapter of the Association for Computational Linguistics (EACL), pages 697\u2013706. ACL.\\n\\nKozhirbayev, Z. M. and Yessenbayev, Z. A. (2020). \u0420\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u0435 \u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0434\u043b\u044f \u043a\u0430\u0437\u0430\u0445\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430. Journal of Mathematics, Mechanics and Computer Science, 107(3):57\u201366.\\n\\nKuralbayev, A., Mukhsimbayev, B., Bekbaganbetov, A., and Fuad, H. (2020). Named Entity Recognition Algorithms Comparison for Judicial Text Data. In Proceedings of the IEEE International Conference on Application of Information and Communication Technologies (AICT), pages 1\u20135. IEEE.\\n\\nLafferty, J., McCallum, A., and Pereira, F. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the International Conference on Machine Learning (ICML), pages 282\u2013289, San Francisco, CA, USA. Morgan Kaufmann Publishers.\\n\\nLinguistic Data Consortium. (2008). ACE (Automatic Content Extraction) English Annotation Guidelines for Entities Version 6.6 2008.06.13.\\n\\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nMa, X. and Hovy, E. H. (2016). End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In Proceedings of the Association for Computational Linguistics (ACL). ACL.\\n\\nMaynard, D., Bontcheva, K., and Cunningham, H. (2003). Towards a Semantic Extraction of Named Entities.\"}"}
{"id": "lrec-2022-1-44", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proceedings of Recent Advances in Natural Language Processing (RANLP), pages 255\u2013261.\\n\\nMussakhojayeva, S., Janaliyeva, A., Mirzakhmetov, A., Khassanov, Y., and Varol, H. A. (2021a). KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset. In Interspeech, pages 2786\u20132790.\\n\\nMussakhojayeva, S., Khassanov, Y., and Varol, H. A. (2021b). A study of multilingual end-to-end speech recognition for Kazakh, Russian, and English. In International Conference on Speech and Computer (SPECOM), volume 12997 of Lecture Notes in Computer Science, pages 448\u2013459. Springer.\\n\\nNadeau, D. and Sekine, S. (2007). A survey of named entity recognition and classification. Lingvisticae Investigationes, 30(1):3\u201326.\\n\\nNakayama, H. (2018). seqeval: A python framework for sequence labeling evaluation. Software available from https://github.com/chakki-works/seqeval.\\n\\nNeves, M. and Seva, J. (2021). An extensive review of tools for manual annotation of documents. Briefings in Bioinformatics, 22(1):146\u2013163.\\n\\nNorrick, N. R., (2015). Subject Area, Terminology, Proverb Definitions, Proverb Features, pages 7\u201327. De Gruyter Open Poland.\\n\\nOkazaki, N. (2007). CRFsuite: a fast implementation of Conditional Random Fields. Software Package.\\n\\nPavlenko, A. (2008). Russian in post-Soviet countries. Russian Linguistics, 32(1):59\u201380.\\n\\nRaytheon BBN Technologies. (2004). OntoNotes Named Entity Guidelines Version 14.0.\\n\\nSadykova, Z. N. and Ivanov, V. V. (2016). \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043a\u043e\u0440\u043f\u0443\u0441\u0430 \u0441 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u043e\u0439 \u0441\u0443\u0449\u043d\u043e\u0441\u0442\u0435\u0439 \u0432 \u043d\u043e\u0432\u043e\u0441\u0442\u043d\u044b\u0445 \u043c\u0435\u0434\u0438\u0430 \u0440\u0435\u0441\u0443\u0440\u0441\u0430\u0445 \u0434\u043b\u044f \u043a\u0430\u0437\u0430\u0445\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 [The formation of a corpus with the markup of entities in news media resources for the Kazakh language]. In TEL-2016, pages 137\u2013141.\\n\\nSang, E. F. T. K. and Meulder, F. D. (2003). Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of the Conference on Natural Language Learning (CoNLL) at HLT-NAACL, pages 142\u2013147. ACL.\\n\\nSang, E. F. T. K. and Veenstra, J. (1999). Representing text chunks. In Proceedings of the European Chapter of the Association for Computational Linguistics (EACL), pages 173\u2013179. ACL.\\n\\nSch\u00f6n, S., Mironova, V., Gabryszak, A., and Hennig, L. (2019). A corpus study and annotation schema for named entity recognition and relation extraction of business products. Proceedings of the Language Resources and Evaluation (LREC), pages 4445\u20134451.\\n\\nStenetorp, P., Pyysalo, S., Topic, G., Ohta, T., Ananiadou, S., and Tsujii, J. (2012). BRAT: A web-based tool for nlp-assisted text annotation. In Proceedings of the European Chapter of the Association for Computational Linguistics (EACL), pages 102\u2013107. ACL.\\n\\nTjong Kim Sang, E. F. (2002). Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. In Proceedings of the Conference on Natural Language Learning (CoNLL).\\n\\nTolegen, G., Toleu, A., and Xiaoqing, Z. (2016). Named entity recognition for Kazakh using conditional random fields. In Proceedings of the International Conference on Computer Processing of Turkic Languages, pages 1\u20138.\\n\\nTolegen, G., Toleu, A., Mamyrbayev, O., and Mussabayev, R. (2020). Neural Named Entity Recognition for Kazakh. arXiv preprint arXiv:2007.13626.\\n\\nWeischedel, R., Palmer, M., Marcus, M., Hovy, E., Pradhan, S., Ramshaw, L., Xue, N., Taylor, A., Kaufman, J., and Franchini, M. (2012). OntoNotes release 5.0 with OntoNotes DB Tool v0.999 beta.\\n\\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP), pages 38\u201345. ACL.\\n\\nYang, Y. (2001). A study of thresholding strategies for text categorization. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 137\u2013145.\"}"}
