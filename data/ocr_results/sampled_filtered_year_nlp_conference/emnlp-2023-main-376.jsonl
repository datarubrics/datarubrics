{"id": "emnlp-2023-main-376", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstractive Open Information Extraction\\nKevin Pei1, Ishan Jindal2, Kevin Chen-Chuan Chang1\\n1University of Illinois at Urbana-Champaign,\\n2IBM Research\\n1{kspei2, kcchang}@illinois.edu,\\n2ishan.jindal@ibm.com\\n\\nAbstract\\nOpen Information Extraction (OpenIE) is a traditional NLP task that extracts structured information from unstructured text to be used for other downstream applications. Traditionally, OpenIE focuses on extracting the surface forms of relations as they appear in the raw text, which we term extractive OpenIE. One of the main drawbacks of this approach is that implicit semantic relations (inferred relations) cannot be extracted, compromising the performance of downstream applications. In this paper, we broaden the scope of OpenIE relations from merely the surface form of relations to include inferred relations, which we term abstractive OpenIE. This new task calls for the development of a new abstractive OpenIE training dataset and a baseline neural model that can extract those inferred relations. We also demonstrate the necessity for a new semantics-based metric for evaluating abstractive OpenIE extractions. Via a case study on Complex QA, we demonstrate the effectiveness of abstractive OpenIE.\\n\\n1 Introduction\\nOpen Information Extraction (OpenIE) is the task of extracting relation tuples from unstructured text (Etzioni et al., 2008; Mausam et al., 2012; Angeli et al., 2015). Unlike traditional information extraction, OpenIE is open domain, intended to be easy to deploy in different domains without fine-tuning. These relations can then be used in downstream applications like summarization (Zhang et al., 2021), question-answering (Lu et al., 2019), and knowledge base population (Kroll et al., 2021). In order to support these applications, OpenIE needs to extract as many different types of relations as possible. One particular relation type of interest is \u201cInferred Relations.\u201d We define an \u201cInferred Relation\u201d to be a relation where the predicate contains words that are not in the original sentence. For example, given the sentence \\\"Albert Einstein (14 March 1879 - 18 April 1955) was a German-born theoretical physicist,\\\" the relation (Albert Einstein, died on, 18 April 1955) can be inferred even though \u201cdied on\u201d is not in the original sentence. Extracting inferred relations increases recall, which is explicitly desired by various downstream tasks including question-answering, slot filling, event schema induction, summarization, and knowledge base population (Pei et al., 2022). Based on the number of inferred relations in the manually annotated dataset WiRe57, extracting inferred relations could increase the total number of relations extracted by 50% (L\u00e9chelle et al., 2018). Existing neural OpenIE models struggle to extract these inferred relations, with only one previous model, OpenIE6, including hand-written rules to extract only some cases of inferred relations (Kolluru et al., 2020a). Table 1 has an example of an inferred relation.\\n\\nAnother problem is that the extraction is very dependent on the sentence\u2019s syntax. For downstream applications like question-answering, slot filling, event schema induction, summarization, and knowledge base population, it is necessary to be able to extract inferred relations. For example, given the sentence \\\"Tokyo, officially Tokyo Metropolis, is the capital city of Japan and one of its 47 prefectures,\\\" the sentence contains the entities \\\"Tokyo\\\" and \\\"Japan\\\" and has the relation (Tokyo, is, the capital city of Japan). However, the sentence also contains the entity \\\"officially Tokyo Metropolis\\\" which has no predicate but still has a relation with the noun \\\"Tokyo.\\\" In the last abstractive relation, \\\"one of its 47 prefectures\\\" is meaningless without the context of the rest of the sentence. It would be more useful to replace the object with \\\"a prefecture\\\" or \\\"one of Japan\u2019s 47 prefectures,\\\" neither of which appear in the sentence. Preexisting OpenIE models cannot extract these abstractive relations.\\n\\n| Relation | Extractive OpenIE Extractions | Abstractive OpenIE Extractions |\\n|----------|-------------------------------|-------------------------------|\\n| Tokyo; is; the capital city of Japan | {Tokyo; is; one of its 47 prefectures} | {Tokyo; is officially; Tokyo Metropolis} |\\n| Tokyo; is; a prefecture | {Tokyo; is; one of Japan\u2019s 47 prefectures} |\\n\\nTable 1: Examples of relations that extractive OpenIE models cannot extract. In this sentence, the apposition \\\"officially Tokyo Metropolis\\\" has no predicate but still has a relation with the noun \\\"Tokyo.\\\" In the last abstractive relation, \\\"one of its 47 prefectures\\\" is meaningless without the context of the rest of the sentence. It would be more useful to replace the object with \\\"a prefecture\\\" or \\\"one of Japan\u2019s 47 prefectures,\\\" neither of which appear in the sentence. Preexisting OpenIE models cannot extract these abstractive relations.\"}"}
{"id": "emnlp-2023-main-376", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"applications using OpenIE, it is important to be able to extract either different surface forms of a relation or its canonical form. The surface form refers to how it appears within the text, while the canonical form refers to the semantic meaning. In question answering (QA), several methods repeatedly paraphrase the questions so that the surface forms of extracted relations match at least one of the question paraphrases, indicating that extracting more surface forms of relation would answer more questions (Fader et al., 2013, 2014; Yin et al., 2015). In addition, the more complex a sentence\u2019s syntax is, such as having more clauses, the more difficult it is to extract high-quality relations. An illustrative example of how being limited to extracting surface forms can be found in Table 1.\\n\\nBy design, all existing neural OpenIE models are unable to extract these abstractive relations, which could be utilized by the downstream application. Therefore, in this work, we propose an abstractive Open Information Extraction (abstractive OpenIE) task. The purpose of this task is to extract relation tuples that are far beyond the reach of any existing OpenIE tasks. We define abstractive OpenIE as a task that given an input sentence generates ordered tuples in the form of (subject, predicate, object) for all possible relations (inferred or non-inferred) within the sentence.\\n\\nAlthough not explicitly defined as such, existing neural models often treat OpenIE as a labeling problem, where tokens are labeled as being part of the subject, predicate, or object of a relation (Kolluru et al., 2020a; Vasilkovsky et al., 2022). Even in cases where OpenIE is defined as a generative problem, the generated relations don\u2019t contain words outside the vocabulary of the original sentence (Kolluru et al., 2020b) (Han and Wang, 2021). Due to the labeling problem definition, prior neural OpenIE models struggle to extract relations with predicates that don\u2019t appear in the original sentence. We refer to all preexisting neural OpenIE models as extractive OpenIE methods, because they can only generate relations by extracting tokens from the original sentence.\\n\\nOne such attempt to go beyond extractive OpenIE is the OpenIE6 model Kolluru et al. (2020a). It explicitly concatenates manually defined out-of-vocabulary tokens at the end of each sentence to allow for the extraction of specific inferred relations. However, obtaining such a list is non-trivial and can not scale to every domain. We differ from OpenIE6 in the sense that abstractive OpenIE models trained on abstractive OpenIE training datasets generate this inferred relation on the fly and do not require defining a list of out-of-vocabulary tokens. Therefore, in this paper, we derive abstractive OpenIE training datasets from existing information extraction datasets and train a baseline machine-learning model that extracts abstractive relations. Further, we also develop an abstractive OpenIE evaluation metric to evaluate the quality of abstractive OpenIE models. Our problem warrants a new evaluation metric because all the existing OpenIE evaluation metrics are lexical and evaluated based on the token overlap between the predicted relations and the gold standard relation. These lexical metrics are undesirable for the proposed task as the relations extracted using the abstractive OpenIE model do not have to use the tokens present in the input sentence. Therefore, we propose a semantics-based metric for evaluating abstractive OpenIE models.\\n\\nIn summary, our contributions are as follows:\\n\\n\u2022 We propose an abstractive OpenIE task to expand the scope of OpenIE extractions compared to prior extractive OpenIE models.\\n\u2022 We derive an abstractive OpenIE training dataset and develop an initial abstractive OpenIE model as a baseline.\\n\u2022 We propose a general-purpose semantics-based evaluation metric for evaluating any OpenIE model.\\n\u2022 We perform a comprehensive comparison between abstractive and extractive OpenIE models.\\n\\n2 Related Work\\nOpenIE Datasets:\\n\\nGiven how data-hungry deep learning models are and how costly it is to manually label OpenIE datasets, most OpenIE training sets are weakly labeled using high-confidence extractions from prior OpenIE models to get \u201csilver-standard\u201d labels. For example, the CopyAttention (Cui et al., 2018), SpanOIE (Zhan and Zhao, 2020), and OIE4 (Kolluru et al., 2020b) training sets are created from high-confidence OpenIE4 extractions from Wikipedia. LSOIE (Solawetz and Larson, 2021) is instead created from examples from the QA-SRL 2.0 dataset. Because traditional OpenIE is...\"}"}
{"id": "emnlp-2023-main-376", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison of the attributes of different datasets. SuRE is the relation extraction model we use to obtain additional inferred relations for training (Lu et al., 2022).\\n\\nTable 3: An example of paraphrasing via back translation. The sentence is from the OIE4 training set.\\n\\nSample Sentence: The purse contains the seal of Order of the Garter.\\n\\nBack Translated Sentence: In the handbag is the seal of the Order of the Garter.\\n\\nRelations: {The purse; contains; the seal of Order of the Garter}\"}"}
{"id": "emnlp-2023-main-376", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: An example of data augmentation via relation extraction. The method used for relation extraction is SuRE (Lu et al., 2022). The sentence is from the OIE4 training set.\\n\\nbetween the predicted relations and the gold standard relations. In particular, OIE2016 is based on tuple-level matching, treating relations extraction as a binary classification problem where a gold standard relation is extracted if a predicted relation contains a majority of tokens in the gold standard relation (Stanovsky and Dagan, 2016). WiRE57 and CaRB use token-level matching, where predicted relations are evaluated based on the token overlap between the best matches between the predicted and gold standard relations (L\u00e9chelle et al., 2018) (Bhardwaj et al., 2019). Because the abstractive relations extracted using abstractive OpenIE do not have to use the original sentence's tokens, evaluating them using lexical metrics is undesirable.\\n\\nThere has been previous interest in semantics-based metrics for evaluating abstractive summarization and machine translation. BERTScore is a popular metric that calculates the cosine similarity between the BERT contextual embeddings of each token in the document and each token in the summary. The highest total similarity score possible from the mapping of tokens in the document to tokens in the summary is then chosen as the BERTScore (Zhang et al., 2019). In theory, this metric would take into account the context of each word, which would capture the semantics of each word. However, it has been found that BERTScore may still be insufficient in cases where individual tokens like negations significantly change the meaning of the sentence, even if it is marginally better than lexical methods like BLEU, ROUGE, and METEOR (Saadany and Orasan, 2021).\\n\\n3 Abstractive OpenIE\\n\\nAbstractive OpenIE is defined as a task that generates ordered tuples in the form of (subject, predicate, object) for all possible relations (inferred or non-inferred) within a given sentence. In this section, we will describe all the pieces required to accomplish this task.\\n\\n3.1 Training Sets\\n\\nAlthough there are existing OpenIE training sets, they do not fit our goals because they are purely extractive. The training set needs to contain inferred relations so that trained models can extract inferred relations. To address this problem, we use two methods to derive abstractive OpenIE training sets from OIE4, a preexisting OpenIE training set:\\n\\nParaphrasing Via Back Translation\\n\\nBack translation is the translation of a text into a different language, then translation back into the original language (Edunov et al., 2018). The resulting text should retain the same semantic meaning, but may differ in the specific words or syntax used. To generate abstractive OpenIE training data, we generate back translations of the sentences but retain the gold standard relations. Because the back translated sentences use different words and syntax, the gold standard relations may no longer consist of only words from the original sentence, thus becoming inferred relations. We provide an example in Table 3.\\n\\nWhen generating paraphrases, we need to make sure that the paraphrased sentence has the same semantic meaning as the original sentence and contains the same relations. Thus, we perform a validation step where we use entailment to measure the quality of the paraphrase. During this step, we use three measures to ensure the quality of the paraphrase. We measure whether the original sentence entails the paraphrase to ensure the paraphrase doesn't contain extraneous information not in the original sentence. We measure whether the paraphrase entails the original sentence to ensure the paraphrase contains all information present in the original sentence. Finally, we measure whether the paraphrased sentence entails all of the gold standard relations to ensure that the relations are the same for the original sentence and the paraphrase. If any of these hypotheses does not have an entailment confidence above a certain threshold, then we do not use the paraphrase in the training data.\\n\\nData Augmentation Via Relation Extraction\\n\\nAlthough paraphrasing can create inferred relations in the training data.\"}"}
{"id": "emnlp-2023-main-376", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sample Sentence Sharon had been in a coma since suffering a stroke in January 2006.\\nSample Relations {Sharon; had been; in a coma}\\nSample Predicate predicates: Sharon had been in a coma since suffering a stroke in January 2006.\\nSample Argument 2006. [pred] had been [pred] suffering [arg1] Sharon [arg2] a stroke in January 2006\\n\\nTable 5: Illustrative training example. For each sentence, there is one predicate prediction example and a number of argument prediction examples equal to the number of gold standard relations. The model first extracts all predicates, then for each predicate extracts the arguments.\\n\\ntions in that the words used may not match the sentence exactly, the relations remain fundamentally the same. The inferred relations that the benchmarks such as WiRe57 contain are not derived from paraphrases of the sentence, so creating paraphrases as training data for them is not appropriate. Instead, we augment the data with additional inferred relations derived using relation extraction (RE). We provide an example in Table 4.\\n\\nRE also aims to extract relations from unstructured text, but instead of being completely open domain, RE is limited to extracting a specific set of relations that must be defined beforehand (Bach and Badaskar, 2007). However, those relations may take a variety of surface forms. For instance, the relation \\\"country_of_birth\\\" could take the form \\\"Einstein was born in Ulm\\\", \\\"Einstein (born 14 March 1879 in Ulm)\\\", other forms. We thus use RE models to extract additional inferred relations for abstractive OpenIE training. To ensure quality and prevent redundancy, we only keep extracted relations above a certain level of confidence and which are not entailed by or entail preexisting OpenIE gold standard relations.\\n\\n3.2 Benchmarks\\nIn contrast to existing OpenIE training dataset, there are several OpenIE benchmarks which contain inferred relations because they were manually annotated or used crowdsourcing for annotation. For evaluation, we use WiRe57, CaRB, Re-OIE2016, and LSOIE test sets. Each of these benchmarks contains a different proportion of inferred relations, in Table 2. In particular, the manual annotation of WiRe57 makes prior extractive OpenIE methods perform poorly compared to their performance on other OpenIE benchmarks. Unlike the other benchmarks, LSOIE contains no inferred relations at all, meaning in theory extractive OpenIE methods should be able to extract all relations. Thus, we can use performance on LSOIE to directly compare abstractive OpenIE and extractive OpenIE models on the extractive OpenIE task. Statistics for the derived training sets and benchmarks is available in Table 2.\\n\\n3.3 Abstractive Tuple Generator\\nPrior OpenIE models are not suited for the proposed task because all existing models are extractive models. As a result, we use generative models to generate relations for a given sentence. We choose to fine-tune T5, a text-to-text transformer model, to generate relations from a sentence (Raffel et al., 2020).\\n\\nInspired by Multi2OIE, we perform relation generation in two stages, a predicate and an argument stage (Ro et al., 2020). In the predicate stage, all predicates are extracted from the sentence at once. The input for this stage is the sentence, while the gold standard is the predicates of all gold standard relations separated by the special \\\"[pred]\\\" token. Although the order of relations in our output doesn't matter, we need to enforce a strict order for the model to learn. Thus, we order the predicates by their position within the sentence.\\n\\nFor the argument prediction stage, for each predicate the model predicts the arguments for the relation with that predicate. Because multiple relations may have the same predicate, we specify the predicate by including all predicates before it in the sentence. For each relation, we assume there are two arguments, which the model extracts simultaneously. The input for this stage is the sentence with the predicate concatenated to the end, separated by\"}"}
{"id": "emnlp-2023-main-376", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.4 Semantic-based Evaluation Metrics\\n\\nCaRB is a popular metric for evaluating OpenIE models, but it requires the predicates of the prediction and gold standard to match to score a given prediction. Although it serves as a good proxy for a semantic metric in extractive OpenIE, it is significantly less useful for abstractive OpenIE where the space of all possible predicates is much larger than just the tokens in the sentence.\\n\\nTo evaluate abstractive OpenIE, we require a semantics-based metric rather than a lexical metric based on token matching. Although previous semantics-based evaluation metrics like BERTScore exist, we do not find them to be appropriate for our use case. Previous semantics-based evaluation metrics do not work well for cases where a single token can dramatically change the semantics of a statement, such as negations like \\\"not\\\" (Saadany and Orasan, 2021). Thus, we introduce a set of 3 evaluation metrics based on entailment for more accurate semantic evaluation. Each of these metrics measures semantic coherence at different granularities, and which granularity is most important will depend on the application and properties of the datasets. We demonstrate this necessity with an example in Table 6.\\n\\nWhen calculating the entailment score for a relation, we remove special characters so that it resembles a sentence. For instance, for the relation triple {Sharon; had been; in a coma}, we form the statement \\\"Sharon had been in a coma.\\\"\\n\\nSentence-tuple entailment\\nThe first metric we propose is sentence-tuple entailment. For recall, we combine all the relations together and see if the combined relations entail the sentence. If the combined relations do not entail the sentence, that means the sentence contains information not in any relation and thus the extracted relations as a whole have poor recall. For precision, we take the average of the entailment score obtained when seeing if the sentence entails an individual relation for all extracted relations. If the relation is not entailed, that means it contains information not in the sentence and thus has poor precision.\\n\\nCombined tuple-tuple entailment\\nThe second metric we propose is combined tuple-tuple entailment. This metric is inspired by a metric proposed by (Du\u0161ek and Kasner, 2020). For this metric, we use the gold standard relations to evaluate the extracted tuples. The combined tuple in this case refers to the combination of all gold standard relations. For recall, we combine all the predicted relations together and see if the combined relations entail the combined gold relations. If the combined predictions do not entail the combined gold, that means the gold relations contain information not in any prediction and thus the extracted relations as a whole have poor recall. For precision, we take the average of the entailment score obtained when seeing if the combined gold entails an individual relation for all extracted relations. If the prediction is not entailed, that means it contains information not in any gold relation and thus has poor precision.\\n\\nCompared to the sentence-tuple entailment metric, this one excludes any extraneous information in the sentence not in the gold standard relations from evaluation.\\n\\nTuple-tuple entailment\\nThe third metric we propose is tuple-tuple entailment. This metric is based on the OpenIE metric CaRB (Bhardwaj et al., 2019). For recall, for each gold standard relation we calculate the entailment for each extracted relation if the gold standard entails that prediction. Then, for each gold standard relation its recall is equal to the highest entailment score achieved by any of the predictions. The recall for the sentence is the average of the recall of its relations. Note that the highest recall for multiple gold standard relations can be achieved by the same predicted relation if the predicted relation contains all of those gold standard relations. For precision, for each gold standard relation we calculate the entailment for each extracted relation if the prediction entails that gold standard relation. Then, we find the optimal matching of gold standard relations to extracted relations that results in the highest average precision. Unlike recall, when calculating precision a predicted relation can only entail a single gold standard relation. This is because we want the number of predictions to match the number of gold relations.\\n\\n4 Experimental Setup\\n\\nDatasets and Metrics\\n\\nWe evaluate the trained abstractive OpenIE model on four benchmarks: WiRe57, CaRB, Re-OIE2016, and LSOIE-wiki with respectively decreasing proportion of inferred relations.\"}"}
{"id": "emnlp-2023-main-376", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rival political factions were unable to resolve disagreements.\\n\\nTable 6: Comparison of different evaluation metrics on an example from the training set. CaRB is a popular lexical metric used to evaluate OpenIE (Bhardwaj et al., 2019). ROUGE-1 is a popular lexical metric to evaluate summarization (Lin, 2004). BERTScore is a previous semantics-based metric used to evaluate summarization (Zhang et al., 2019). Tuple-Tuple Entailment is a new semantics-based metric we propose.\\n\\nSince OIE4 trained OpenIE models showed superior F1 performance on all these benchmarks as compared to other OpenIE training sets we derive abstractive training data from this dataset. We generate four different versions of OIE4 using the methods we describe in Section 3.1. The first version is the original extractive dataset, the second version uses backtranslation for paraphrasing, the third version is augmented by relation extraction, and the fourth uses both backtranslation and relation extraction for augmentation. For backtranslation we use Facebook-FAIR\u2019s WMT\u201919 German-English and English-German models (Ng et al., 2019) and retain only those back translated sentences whose entailment confidence is above 80%. For relation extraction, we use a pretrained SuRE model, a state-of-the-art relation extraction model (Lu et al., 2022) without any additional fine-tuning and keep all relations with confidence above 80%. These confidence thresholds are hyperparameters that may be adjusted.\\n\\nWe compare performance using the preexisting CaRB metric, as well as our own introduced semantics-based metrics of tuple-tuple entailment, combined tuple-tuple entailment, and sentence-tuple entailment. The entailment model we use for our datasets and evaluation metrics is a BERT-based encoder model trained on MNLI, SNLI, and the Hans dataset (Gao et al., 2021).\\n\\nModels and Hyperparameters\\nWe fine-tune the T5-base model for our experiments. We fine-tuned T5 for 5 epochs with an initial learning rate of 2e-5 and batch size of 12. We validate T5 on a subset of the OIE4 training set using the tuple-tuple entailment metric. We also compare our model with MultiOIE, a state-of-the-art neural extractive OpenIE model (Ro et al., 2020). We train MultiOIE on the original OIE4 dataset with no paraphrasing. We use the default hyperparameters of MultiOIE.\"}"}
{"id": "emnlp-2023-main-376", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We first compare performance on all relations in Figure 1. In general, abstractive OpenIE leads to better performance the higher the proportion of inferred relations in the test set. This is expected because Multi2OIE can not extract inferred relations at all. When considering the full benchmarks, of the data augmentation methods we use, SuRE augmentation works the best. Training on back-translated OIE4 degrades the performance compared to the base extractive OIE4 data. This may be because back translation reduces the amount of training data. Additionally, back translation often just replaces the gold standard predicate with a synonym instead of changing the syntax of the sentence, which does not help in the extraction of inferred relations.\\n\\nTo demonstrate the complementary nature of abstractive OpenIE to extractive OpenIE, we combine their extractions. When combining their extractions, we remove redundant relations by removing relations that are entailed by any other relations. If two relations entail each other, then we keep the longer one. A comparison of combined models can be found in Figure 2. When combining model predictions, we observe that back translation actually helps more than SuRE augmentation. This suggests that SuRE augmentation helps extractive OpenIE relations, while back translation is more useful for increasing the recall to inferred relations that could not be extracted by Multi2OIE. The more inferred relations in the benchmark, the more beneficial merging extractions are.\\n\\nWe also evaluate our abstractive OpenIE models on only the inferred relations within each benchmark. To do this, we remove non-inferred relations from the gold standards. We can only measure the resulting recall of the models because the models are trained to generate both inferred and non-inferred relations and the metrics we use penalize the precision when there are too many predicted relations for a given sentence, which would be the case for any sentence that had non-inferred relations. Figure 3 shows the results of these experiments. As before, the more inferred relations in the benchmark, the better suited an abstractive OpenIE model is for the task.\\n\\nUpon a manual examination of the generated relations of each model, we observe that fine-tuning T5 on SuRE-augmented data results in generated relations replacing some of its predicates with the predicates from SuRE. Table 8 demonstrates one example of a model generating a predicate that does not exist within the sentence but is a common predicate among the SuRE-augmented relations.\"}"}
{"id": "emnlp-2023-main-376", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Comparison of Sentence-Tuple Entailment Recall of different combinations of OpenIE models on only the inferred relations in the benchmarks. All models are trained on OIE4.\\n\\nFormerly known as Edo, it has been the de facto seat of government since 1603 when Shogun Tokugawa Ieyasu made the city his headquarters.\\n\\nTable 8: A demonstration that T5 fine-tuned on OIE4 augmented with SuRE extractions generates predicates from the SuRE extractions rather than the sentence. This sentence is from the WiRE57 test set.\\n\\nCase Study\\nTo further test the applicability of abstractive OpenIE, we evaluate its performance on QUEST, a downstream Complex QA task that uses OpenIE in its pipeline (Lu et al., 2019). QUEST specifically desires higher recall from its OpenIE model, which can be achieved by extracting inferred relations. We show the results in Table 7. The results show that augmenting the training data improves downstream performance, indicating that including more inferred relations in the training data is helpful for this task.\\n\\n6 Conclusion\\nIn this paper, we introduce abstractive OpenIE, an alternative to what we call extractive OpenIE, the paradigm all current OpenIE models currently follow, in order to address the problems of inferred relations and surface form extraction. We find that existing OpenIE datasets and metrics are ill-suited for this task. As a result, we introduce abstractive training set, model, and metrics. We then compare our models trained on different abstractive training sets and the state-of-the-art extractive OpenIE model using preexisting OpenIE benchmarks. Overall, we find that our models achieve higher performance on inferred relations, which extractive OpenIE models have previously struggled with. We believe abstractive OpenIE has potential as a task that will greatly benefit downstream applications that use OpenIE in their pipeline.\\n\\n7 Limitations\\nIn this work, we used a relatively smaller T5-base model. A model with more parameters may have led to improved performance. Further, the corpora we chose are all limited to English. As a result, our results are not generalizable to any downstream task that relies on different languages.\\n\\nEthics Statement\\nWe did not create any of the models, datasets, or applications covered in this paper. Any ethical issues with the preexisting OpenIE datasets we use in this paper will reflect on this work.\\n\\nAcknowledgements\\nThis material is based upon work supported by the National Science Foundation IIS 16-19302 and IIS 16-33755, Zhejiang University ZJU Research 083650, Futurewei Technologies HF2017060011 and 094013, IBM-Illinois Center for Cognitive Computing Systems Research (C3SR) and IBM-Illinois Discovery Accelerator Institute (IIDAI), grants from eBay and Microsoft Azure, UIUC OVCR CCIL Planning Grant 434S34, UIUC CSBS Small Grant 434C8U, and UIUC New Frontiers Initiative. Any opinions, findings, conclusions, or recommendations expressed in this publication are...\"}"}
{"id": "emnlp-2023-main-376", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. 2015. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344\u2013354.\\n\\nNguyen Bach and Sameer Badaskar. 2007. A review of relation extraction. Literature review for Language and Statistics II, 2:1\u201315.\\n\\nSangnie Bhardwaj, Samarth Aggarwal, and Mausam Mausam. 2019. Carb: A crowdsourced benchmark for open ie. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6262\u20136267.\\n\\nLei Cui, Furu Wei, and Ming Zhou. 2018. Neurual open information extraction. arXiv preprint arXiv:1805.04270.\\n\\nOnd\u02c7rej Du\u0161ek and Zden \u02c7ek Kasner. 2020. Evaluating semantic accuracy of data-to-text generation with natural language inference. arXiv preprint arXiv:2011.10819.\\n\\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale. arXiv preprint arXiv:1808.09381.\\n\\nOren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from the web. Communications of the ACM, 51(12):68\u201374.\\n\\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1608\u20131618.\\n\\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1156\u20131165.\\n\\nYang Gao, Nicolo Colombo, and Wei Wang. 2021. Adapting by pruning: A case study on bert. arXiv preprint arXiv:2105.03343.\\n\\nJiabao Han and Hongzhi Wang. 2021. Generative adversarial networks for open information extraction. Advances in Computational Intelligence, 1(4):1\u201311.\\n\\nMartin Josifoski, Nicola De Cao, Maxime Peyrard, and Robert West. 2021. Genie: generative information extraction. arXiv preprint arXiv:2112.08340.\\n\\nKeshav Kolluru, Vaibhav Adlakha, Samarth Aggarwal, Soumen Chakrabarti, et al. 2020a. Openie6: Iterative grid labeling and coordination analysis for open information extraction. arXiv preprint arXiv:2010.03147.\\n\\nKeshav Kolluru, Samarth Aggarwal, Vipul Rathore, Soumen Chakrabarti, et al. 2020b. Imojie: Iterative memory-based joint open information extraction. arXiv preprint arXiv:2005.08178.\\n\\nKeshav Kolluru, Muqeeth Mohammed, Shubham Mittal, Soumen Chakrabarti, et al. 2022. Alignment-augmented consistent translation for multilingual open information extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2502\u20132517.\\n\\nHermann Kroll, Jan Pirklbauer, and Wolf-Tilo Balke. 2021. A toolbox for the nearly-unsupervised construction of digital library knowledge graphs. In Proceedings of the ACM/IEEE Joint Conference on Digital Libraries.\\n\\nWilliam L\u00e9chelle, Fabrizio Gotti, and Philippe Langlais. 2018. Wire57: A fine-grained benchmark for open information extraction. arXiv preprint arXiv:1809.08962.\\n\\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381.\\n\\nKeming Lu, I Hsu, Wenxuan Zhou, Mingyu Derek Ma, Muhao Chen, et al. 2022. Summarization as indirect supervision for relation extraction. arXiv preprint arXiv:2205.09837.\\n\\nXiaolu Lu, Soumajit Pramanik, Rishiraj Saha Roy, Abdalghani Abujabal, Yafang Wang, and Gerhard Weikum. 2019. Answering complex questions by joining multi-document evidence with quasi knowledge graphs. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 105\u2013114.\\n\\nMausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning for information extraction. In Proceedings of the 2012Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 523\u2013534, Jeju Island, Korea. Association for Computational Linguistics.\\n\\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, and Sergey Edunov. 2019. Facebook fair\u2019s wmt19 news translation task submission. arXiv preprint arXiv:1907.06616.\\n\\nKevin Pei, Ishan Jindal, Kevin Chen-Chuan Chang, Chengxiang Zhai, and Yunyao Li. 2022. When to use what: An in-depth comparative empirical analysis of openie systems for downstream applications. arXiv preprint arXiv:2211.08228.\"}"}
{"id": "emnlp-2023-main-376", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551.\\n\\nYoungbin Ro, Yukyung Lee, and Pilsung Kang. 2020. Multi2oie: Multilingual open information extraction based on multi-head attention with bert. arXiv preprint arXiv:2009.08128.\\n\\nHadeel Saadany and Constantin Orasan. 2021. Bleu, meteor, bertscore: Evaluation of metrics performance in assessing critical translation errors in sentiment-oriented text. arXiv preprint arXiv:2109.14250.\\n\\nJacob Solawetz and Stefan Larson. 2021. Lsoie: A large-scale dataset for supervised open information extraction. arXiv preprint arXiv:2101.11177.\\n\\nGabriel Stanovsky and Ido Dagan. 2016. Creating a large benchmark for open information extraction. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2300\u20132305.\\n\\nMichael Vasilkovsky, Anton Alekseev, Valentin Malych, Ilya Shenbin, Elena Tutubalina, Dmitriy Salikhov, Mikhail Stepnov, Andrey Chertok, and Sergey Nikolenko. 2022. Detie: Multilingual open information extraction inspired by object detection. In Proceedings of the 36th AAAI Conference on Artificial Intelligence.\\n\\nPengcheng Yin, Nan Duan, Ben Kao, Junwei Bao, and Ming Zhou. 2015. Answering questions with complex semantic constraints on open knowledge bases. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1301\u20131310.\\n\\nJunlang Zhan and Hai Zhao. 2020. Span model for open information extraction on accurate corpus. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9523\u20139530.\\n\\nMengli Zhang, Gang Zhou, Wanting Yu, and Wenfen Liu. 2021. Far-ass: Fact-aware reinforced abstractive sentence summarization. Information Processing & Management, 58(3):102478.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.\\n\\nWe present our empirical results in tables 9, 10, and 11.\"}"}
{"id": "emnlp-2023-main-376", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: Empirical results of different models on different benchmarks. Differences in the number of inferred relations in each of the benchmarks influences the relative performance of each model. The benchmarks are listed from lowest to highest proportion of relations with inferred predicates.\\n\\n| Model Training Set | Benchmark | CaRB Score | Sentence-Tuple Entailment | Combined Tuple-Tuple Entailment |\\n|---------------------|-----------|------------|--------------------------|---------------------------------|\\n|                     |           |            | P  | R  | F1  | P  | R  | F1  | P  | R  | F1  | P  | R  | F1  |\\n| Multi 2             | OIE OIE4  | LSOIE-wiki | 0.396 | 0.318 | 0.353 | 0.953 | 0.381 | 0.545 | 0.595 | 0.488 | 0.536 | 0.591 | 0.467 | 0.522 |\\n|                     | Abstractive T5 OIE4 | Back Translated LSOIE-wiki | 0.496 | 0.369 | 0.423 | 0.964 | 0.432 | 0.596 | 0.614 | 0.525 | 0.566 | 0.608 | 0.499 | 0.548 |\\n|                     | Abstractive T5 OIE4 | Back Translated | 0.5 | 0.483 | 0.491 | 0.961 | 0.439 | 0.603 | 0.627 | 0.546 | 0.584 | 0.640 | 0.510 | 0.568 |\\n|                     | Abstractive T5 OIE4 | With SuRE Relations LSOIE-wiki | 0.518 | 0.49 | 0.504 | 0.963 | 0.436 | 0.601 | 0.632 | 0.565 | 0.597 | 0.645 | 0.511 | 0.570 |\\n|                     | Abstractive T5 OIE4 | Back Translated | With SuRE Relations | 0.538 | 0.527 | 0.532 | 0.974 | 0.571 | 0.720 | 0.645 | 0.670 | 0.657 | 0.660 | 0.611 | 0.634 |\\n|                     | Multi 2   | OIE OIE4  | LSOIE-wiki | 0.565 | 0.373 | 0.449 | 0.939 | 0.351 | 0.511 | 0.835 | 0.504 | 0.629 | 0.763 | 0.477 | 0.587 |\\n|                     | Abstractive T5 OIE4 | ReOIE2016 | 0.733 | 0.449 | 0.557 | 0.953 | 0.425 | 0.588 | 0.861 | 0.580 | 0.693 | 0.779 | 0.543 | 0.640 |\\n|                     | Abstractive T5 OIE4 | Back Translated ReOIE2016 | 0.706 | 0.565 | 0.628 | 0.948 | 0.418 | 0.580 | 0.855 | 0.582 | 0.693 | 0.806 | 0.531 | 0.640 |\\n|                     | Abstractive T5 OIE4 | With SuRE Relations ReOIE2016 | 0.757 | 0.572 | 0.652 | 0.953 | 0.424 | 0.587 | 0.871 | 0.602 | 0.712 | 0.814 | 0.531 | 0.643 |\\n|                     | Abstractive T5 OIE4 | Back Translated | With SuRE Relations | ReOIE2016 | 0.813 | 0.647 | 0.720 | 0.976 | 0.574 | 0.723 | 0.894 | 0.736 | 0.808 | 0.823 | 0.684 | 0.747 |\\n| Multi 2             | OIE OIE4  | LSOIE-wiki | 0.525 | 0.309 | 0.389 | 0.935 | 0.357 | 0.517 | 0.856 | 0.538 | 0.661 | 0.682 | 0.487 | 0.568 |\\n|                     | Abstractive T5 OIE4 | CaRB | 0.619 | 0.336 | 0.436 | 0.949 | 0.431 | 0.593 | 0.882 | 0.592 | 0.709 | 0.694 | 0.526 | 0.599 |\\n|                     | Abstractive T5 OIE4 | Back Translated CaRB | 0.592 | 0.394 | 0.473 | 0.945 | 0.422 | 0.583 | 0.843 | 0.578 | 0.686 | 0.682 | 0.491 | 0.571 |\\n|                     | Abstractive T5 OIE4 | With SuRE Relations CaRB | 0.619 | 0.389 | 0.478 | 0.951 | 0.428 | 0.591 | 0.862 | 0.584 | 0.697 | 0.701 | 0.495 | 0.580 |\\n|                     | Abstractive T5 OIE4 | Back Translated | With SuRE Relations | CaRB | 0.647 | 0.442 | 0.525 | 0.975 | 0.572 | 0.721 | 0.884 | 0.707 | 0.786 | 0.702 | 0.619 | 0.658 |\\n| Multi 2             | OIE OIE4  | ReOIE2016 | 0.45 | 0.343 | 0.389 | 0.960 | 0.362 | 0.526 | 0.668 | 0.572 | 0.617 | 0.378 | 0.574 | 0.456 |\\n|                     | Abstractive T5 OIE4 | WiRe57 | 0.519 | 0.357 | 0.423 | 0.988 | 0.355 | 0.523 | 0.665 | 0.613 | 0.638 | 0.361 | 0.586 | 0.447 |\\n|                     | Abstractive T5 OIE4 | Back Translated WiRe57 | 0.502 | 0.399 | 0.445 | 0.946 | 0.475 | 0.632 | 0.642 | 0.675 | 0.658 | 0.290 | 0.661 | 0.403 |\\n|                     | Abstractive T5 OIE4 | With SuRE Relations WiRe57 | 0.506 | 0.391 | 0.441 | 0.981 | 0.469 | 0.635 | 0.633 | 0.670 | 0.651 | 0.284 | 0.678 | 0.401 |\\n|                     | Abstractive T5 OIE4 | Back Translated | With SuRE Relations | WiRe57 | 0.537 | 0.37 | 0.439 | 0.990 | 0.371 | 0.539 | 0.665 | 0.611 | 0.637 | 0.377 | 0.556 | 0.449 |\\n\\nTable 10: Empirical results where the relations extracted by Multi 2 OIE and abstractive OpenIE are combined. Redundant relations are removed after the combination of extractions. Redundant relations are relations that are entailed by at least one other relation in the same sentence. If two relations entail each other, the shorter one is removed.\"}"}
{"id": "emnlp-2023-main-376", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Training Set | Benchmark | CaRB Score | Sentence-Tuple Entailment | Combined Tuple-Tuple Entailment |\\n|--------------------|-----------|------------|---------------------------|---------------------------------|\\n| Multi OIE OIE4 ReOIE2016 Inferred | Predicates or Args | 0.231 | 0.304 | 0.452 | 0.411 |\\n| Abstractive T5 OIE4 Back Translated ReOIE2016 Inferred | Predicates or Args | 0.231 | 0.219 | 0.394 | 0.343 |\\n| Abstractive T5 OIE4 with SuRE Relations ReOIE2016 Inferred | Predicates or Args | 0.152 | 0.236 | 0.424 | 0.380 |\\n| Abstractive T5 OIE4 CaRB Inferred | Predicates or Args | 0.223 | 0.236 | 0.386 | 0.408 |\\n| Abstractive T5 OIE4 Back Translated with SuRE Relations CaRB Inferred | Predicates or Args | 0.087 | 0.189 | 0.481 | 0.475 |\\n| Multi OIE OIE4 WiRe57 Inferred | Predicates or Args | 0.116 | 0.520 | 0.641 | 0.605 |\\n| Abstractive T5 OIE4 WiRe57 Inferred | Predicates or Args | 0.109 | 0.362 | 0.522 | 0.482 |\\n| Abstractive T5 OIE4 Back Translated WiRe57 Inferred | Predicates or Args | 0.082 | 0.346 | 0.509 | 0.495 |\\n| Abstractive T5 OIE4 with SuRE Relations WiRe57 Inferred | Predicates or Args | 0.128 | 0.360 | 0.511 | 0.490 |\\n| Abstractive T5 OIE4 Back Translated with SuRE Relations WiRe57 Inferred | Predicates or Args | 0.099 | 0.289 | 0.532 | 0.498 |\\n\\nTable 11: Empirical results of models where the gold standard consists only of relations with inferred predicates or arguments. We only measure recall in this case because relations are extracted per-sentence, so relations that do not have inferred predicates will also be extracted, which will lower the precision.\"}"}
