{"id": "acl-2022-long-514", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings\\n\\nKalpesh Krishna\u2660\u2217 Deepak Nathani\u2666 Xavier Garcia\u2666 Bidisha Samanta\u2666 Partha Talukdar\u2666\\nUniversity of Massachusetts Amherst, \u2666Google Research\\nkalpesh@cs.umass.edu {xgarcia, dnathani, bidishasamanta, partha}@google.com\\n\\nAbstract\\nStyle transfer is the task of rewriting a sentence into a target style while approximately preserving content. While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al., 2021) has attempted \u201cfew-shot\u201d style transfer using just 3-10 sentences at inference for style extraction. In this work, we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available. We notice that existing few-shot methods perform this task poorly, often copying inputs verbatim. We push the state-of-the-art for few-shot style transfer with a new method modeling the stylistic difference between paraphrases. When compared to prior work, our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages. Moreover, our method is better at controlling the style transfer magnitude using an input scalar knob. We report promising qualitative results for several attribute transfer tasks (sentiment transfer, simplification, gender neutralization, text anonymization) all without retraining the model.\\n\\n1 Introduction\\nStyle transfer is a natural language generation task in which input sentences need to be re-written into a target style, while preserving semantics. It has many applications such as writing assistance (Heidorn, 2000), controlling generation for attributes like simplicity, formality or persuasion (Xu et al., 2015; Smith et al., 2020; Niu and Carpuat, 2020), data augmentation (Xie et al., 2020; Lee et al., 2021), and author obfuscation (Shetty et al., 2018).\\n\\nMost prior work either assumes access to supervised data with parallel sentences between the two styles (Jhamtani et al., 2017), or access to a large corpus of unpaired sentences with style labels (Prabhumoye et al., 2018; Subramanian et al., 2019). Models built are style-specific and cannot generalize to new styles during inference, which is needed for applications like real-time adaptation to a user\u2019s style in a dialog or writing application. Moreover, access to a large unpaired corpus with style labels is a strong assumption. Most standard \u201cunpaired\u201d style transfer datasets have been carefully curated (Shen et al., 2017) or were originally parallel (Xu et al., 2012; Rao and Tetreault, 2018). This is especially relevant in settings outside English.\"}"}
{"id": "acl-2022-long-514", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"English, where NLP tools and labelled datasets are largely underdeveloped (Joshi et al., 2020). In this work, we take the first steps studying style transfer in seven languages with nearly 1.5 billion speakers in total. Since no training data exists for these languages, we analyzed the current state-of-the-art in few-shot multilingual style transfer, the Universal Rewriter (UR) from Garcia et al. (2021). Unfortunately, we find it often copies the inputs verbatim (Section 3.1), without changing their style.\\n\\nWe propose a simple inference-time trick of style-controlled translation through English, which improves the UR output diversity (Section 4.1). To further boost performance we propose DIFFUR, a novel algorithm using the recent finding that paraphrasing leads to stylistic changes (Krishna et al., 2020). DIFFUR extracts edit vectors from paraphrase pairs, which are used to condition and train the model (Figure 2). On formality transfer and code-mixing addition, our best performing DIF-FUR variant significantly outperforms UR across all languages (by 2-3x) using automatic & human evaluation. Besides better rewriting, our system is better able to control the style transfer magnitude (Figure 1). A scalar knob ($\\\\lambda$) can be adjusted to make the output text reflect the target style (provided by exemplars) more or less. We also observe promising qualitative results in several attribute transfer directions (Section 6.2) including sentiment transfer, simplification, gender neutralization and text anonymization, all without retraining the model and using just 3-10 examples at inference.\\n\\nFinally, we found it hard to precisely evaluate models due to the lack of evaluation datasets and style classifiers (often used as metrics) for many languages. To facilitate further research in Indic formality transfer, we crowdsource formality annotations for 4000 sentence pairs in four Indic languages (Section 5.1), and use this dataset to design the automatic evaluation suite (Section 5).\\n\\nIn summary, our contributions provide an end-to-end recipe for developing and evaluating style transfer models and evaluation in a low-resource setting.\"}"}
{"id": "acl-2022-long-514", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"two non-overlapping spans. Style extracted from one span ($x_1$) is used to denoise the other ($x_2$), $\\\\bar{x}_2 = f_{\\\\text{noise}}(x_2) f_{\\\\text{style}}(x_1)$.\\n\\n$L_{\\\\text{denoise}} = L_{\\\\text{CE}}(\\\\bar{x}_2, x_2)$\\n\\nwhere $L_{\\\\text{CE}}$ is the standard next-word prediction cross entropy loss function and $\\\\text{noise}(\\\\cdot)$ refers to 20-60% random token dropping and token replacement. This objective is used on the mC4 dataset (Xue et al., 2021b) with 101 languages.\\n\\nTo build a general-purpose rewriter which can do translation as well as style transfer, the model is additionally trained on two objectives: (1) supervised machine translation using the OPUS-100 parallel dataset (Zhang et al., 2020), and (2) a self-supervised objective to learn effective style-controlled translation; more details in Appendix C.\\n\\nDuring inference (Figure 1), consider an input sentence $x$ and a transformation from style $A$ to $B$ (say informal to formal). Let $S_A, S_B$ to be example sentences in each of the styles (typically 3-10 sentences). The output $y$ is computed as,\\n\\n$$s_A = \\\\frac{1}{|S_A|} \\\\sum_{y \\\\in S_A} f_{\\\\text{style}}(y)$$\\n$$s_B = \\\\frac{1}{|S_B|} \\\\sum_{y \\\\in S_B} f_{\\\\text{style}}(y)$$\\n\\n$$y = f_{\\\\text{ur}}(x, \\\\lambda(s_B - s_A))$$\\n\\nwhere $\\\\lambda$ acts as a control knob to determine the magnitude of style transfer, and the vector subtraction helps remove confounding style information.\\n\\n3.1 Shortcomings of the Universal Rewriter\\n\\nWe experimented with the UR model on Hindi formality transfer, and noticed poor performance. We noticed that UR has a strong tendency to copy sentences verbatim \u2014 45.5% outputs were copied exactly from the input (and hence not style transferred) for the best performing value of $\\\\lambda$. The copying increase for smaller $\\\\lambda$, making magnitude control harder. We identify the following issues:\\n\\n1. Random token noise leads to unnatural inputs & transformations: The Universal Rewriter uses 20-60% uniformly random token dropping or replacement to noise inputs, which leads to ungrammatical inputs during training. We hypothesize models tend to learn grammatical error correction, which encourages verbatim copying during inference where fluent inputs are used and no error correction is needed. Moreover, token-level noise does not differentiate between content or function words, and cannot do syntactic changes like content reordering (Goyal and Durrett, 2020). Too much noise could distort semantics and encourage hallucination, whereas too little will encourage copying.\\n\\n2. Style vectors may not capture the precise style transformation: The Universal Rewriter extracts the style vector from a single sentence during training, which is a mismatch from the inference where a difference between vectors is taken. Without taking vector differences at inference, we observe semantic preservation and overall performance of the UR model is much lower.\\n\\n3. mC4 is noisy: On reading training data samples, we noticed noisy samples with severe language identification errors in the Hindi subset of mC4. This has also been observed recently in Kreutzer et al. (2022), who audit 100 sentences in each language, and report 50% sentences in Marathi and 20% sentences in Hindi have the wrong language.\\n\\n4. No translation data for several languages: We notice worse performance for languages which did not get parallel translation data (for the translation objective in Section 3). In Table 1 we see UR gets a score of 30.4 for Hindi and Bengali, languages for which it got translation data. However, the scores are lower for Kannada, Telugu & Gujarati (25.5, 22.8, 23.7), for which no translation data was used. We hypothesize translation data encourages learning language-agnostic semantic representations needed for translation from the given language, which in-turn improves style transfer.\\n\\n4 Our Models\\n\\n4.1 Style-Controlled Backtranslation (+BT)\\n\\nWhile the Universal Rewriter model has a strong tendency to exactly copy input sentences while rewriting sentences in the same language (Section 3.1), we found it is an effective style-controlled translation system. This motivates a simple inference-time trick to improve model outputs and reduce copying \u2014 translate sentences to English ($en$) in a style-agnostic manner with a zero style 6. This difference possibly helps remove confounding information (like semantic properties, other styles) and focus on the specific style transformation. Since two spans in the same document will share aspects like article topic / subject along with style, we expect these semantic properties will confound the style vector space obtained after the UR training.\\n\\nUsing the r-AGG style transfer metric from Section 5.5.\"}"}
{"id": "acl-2022-long-514", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fix #1: Use paraphrases as \\\"noise\\\" function instead of random token dropping / replacement.\\n\\nFix #3: Use cleaner sentences from Samanantar instead of noisy mC4.\\n\\nFigure 2: The DIFFUR approach (Section 4.2), with fixes to the shortcomings of the Universal Rewriter approach (Section 3.1) shown. Sentences are noised using paraphrasing, the style vector difference between the paraphrase & original sentence (\\\"edit vector\\\") is used to control denoising. See Figure 1 for the inference-time process.\\n\\nvector 0, and translate back into the source language (lx) with stylistic control.\\n\\n\\\\[ x = \\\\text{fur}(en \\\\oplus x, 0) \\\\]\\n\\n\\\\[ \\\\bar{x} = \\\\text{fur}(lx \\\\oplus x_{\\\\text{en}}, \\\\lambda(s_B - s_A)) \\\\]\\n\\nwhere \\\\( x \\\\) is the input sentence, \\\\( s_A \\\\), \\\\( s_B \\\\) are the styles vectors we want to transfer between, \\\\( en \\\\), \\\\( lx \\\\) are language codes prepended to indicate the output language (Appendix C). Prior work has shown that backtranslation is effective for paraphrasing (Wieting and Gimpel, 2018; Iyyer et al., 2018) and style transfer (Prabhumoye et al., 2018).\\n\\n4.2 Using Paraphrase Vector Differences for Style Transfer (DIFFUR)\\n\\nWhile style-controlled backtranslation is an effective strategy, it needs two translation steps. This is 2x slower than UR, and semantic errors increase with successive translations. To learn effective style transfer systems needing only a single generation step we develop DIFFUR, a new few-shot style transfer training objective (overview in Figure 2). DIFFUR tackles the issues discussed in Section 3.1 using paraphrases and style vector differences.\\n\\nParaphrases as a \\\"noise\\\" function: Instead of using random token-level noise (Issue #1 in Section 3.1), we paraphrase sentences to \\\"noise\\\" them during training. Paraphrasing modifies the lexical & syntactic properties of sentences, while preserving fluency and input semantics. Prior work (Krishna et al., 2020) has shown that paraphrasing leads to stylistic changes, and denoising can be considered a style re-insertion process.\\n\\nTo create paraphrases, we backtranslate sentences from the UR model with no style control (zero vectors used as style vectors). To increase diversity, we use random sampling in both translation steps, pooling generations obtained using temperature values [0.4, 0.6, 0.8, 1.0]. Finally, we discard paraphrase pairs from the training data where the semantic similarity score is outside the range [0.7, 0.98]. This removes backtranslation errors (score < 0.7), and exact copies (score > 0.98). In Appendix K we confirm that our backtranslated paraphrases are lexically diverse from the input.\\n\\nUsing style vector differences for control: To fix the training / inference mismatch for style extraction (Issue #2 in Section 3.1), we propose using style vector differences between the output and input as the stylistic control. Concretely, let \\\\( x \\\\) be an input sentence and \\\\( x_{\\\\text{para}} \\\\) its paraphrase.\\n\\n\\\\[ s_{\\\\text{diff}} = f_{\\\\text{style}}(x) - f_{\\\\text{style}}(x_{\\\\text{para}}) \\\\]\\n\\n\\\\[ \\\\bar{x} = \\\\text{fur}(x_{\\\\text{para}}, \\\\text{stop-grad}(s_{\\\\text{diff}})) \\\\]\\n\\n\\\\[ L = L_{\\\\text{CE}}(\\\\bar{x}, x) \\\\]\\n\\nwhere \\\\( \\\\text{stop-grad}(\\\\cdot) \\\\) stops gradient flow through \\\\( s_{\\\\text{diff}} \\\\), preventing the model from learning to copy \\\\( x \\\\) exactly. To ensure \\\\( f_{\\\\text{style}} \\\\) extracts meaningful style representations, we fine-tune a trained UR model.\\n\\nVector differences have many advantages, 1. Subtracting style vectors between a sentence...\"}"}
{"id": "acl-2022-long-514", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and its paraphrase removes confounding features (like semantics) present in the vectors.\\n\\n2. The vector difference focuses on the precise transformation that is needed to reconstruct the input from its paraphrase.\\n\\n3. The length of $s_{diff}$ acts as a proxy for the amount of style transfer, which is controlled using $\\\\lambda$ during inference (Section 3).\\n\\nDIFFUR is related to neural editor models (Guu et al., 2018; He et al., 2020), where language models are decomposed into a probabilistic space of edit vectors over prototype sentences. We justify the DIFFUR design with ablations in Appendix G.1.\\n\\n4.3 Indic Models ($UR$-INDIC, DIFFUR-INDIC)\\n\\nTo address the issue of no translation data (Issue #4 in Section 3.1), we train Indic variants of our models. We replace the OPUS translation data used for training the Universal Rewriter (Section 3) with Samanantar (Ramesh et al., 2021), which is the largest publicly available parallel translation corpus for 11 Indic languages. We call these variants $UR$-INDIC and DIFFUR-INDIC. This process significantly up-samples the parallel data seen between English / Indic languages, and gives us better performance (Table 1) and lower copy rates, especially for languages with no OPUS translation data.\\n\\n4.4 Multitask Learning (DIFFUR-MLT)\\n\\nOne issue with our DIFFUR-INDIC setup is usage of a stop-grad($\\\\cdot$) to avoid verbatim copying from the input. This prevents gradient flow into the style extractor $f_{style}$, and as we see in Appendix H, a degradation of the style vector space. To prevent this we simply multi-task between the exemplar-driven denoising $UR$ objective (Section 3) and the DIFFUR objective. We initialize the model with the $UR$-INDIC checkpoint, and fine-tune it on these two losses together, giving each loss equal weight.\\n\\n5 Evaluation\\n\\nAutomatic evaluation of style transfer is challenging (Pang, 2019; Mir et al., 2019; Tikhonov et al., 2019), and the lack of resources (such as evaluation datasets, style classifiers) make evaluation trickier for Indic languages. To tackle this issue, we first collect a small dataset of formality and semantic similarity annotations in four Indic languages (Section 5.1). We use this dataset to guide the design of an evaluation suite (Section 5.2-5.6). Since automatic metrics in generation are imperfect (Celikyilmaz et al., 2020), we complement our results with human evaluation (Section 5.7).\\n\\n5.1 Indic Formality Transfer Dataset\\n\\nSince no public datasets exist for formality transfer in Indic languages, it is hard to measure the extent to which automatic metrics (such as style classifiers) are effective. To tackle this issue, we build a dataset of 1000 sentence pairs in each of four Indic languages (Hindi, Bengali, Kannada, Telugu) with formality and semantic similarity annotations. We first style transfer held-out Samanantar sentences using our UR-INDIC + BT model (Section 4.1, 4.3) to create sentence pairs with different formality. We then asked three crowdworkers to 1) label the more formal sentence in each pair; 2) rate semantic similarity on a 3-point scale.\\n\\nOur crowdsourcing is conducted on Task Mate, where we hired native speakers from India with at least a high school education and 90% approval rating on the platform. To ensure crowdworkers understood \u201cformality\u201d, we provided instructions following advice from professional Indian linguists, and asked two qualification questions in their native language. More details (agreement, compensation, instructions) are provided in Appendix E.4.\\n\\n5.2 Transfer Accuracy (r-ACC, a-ACC)\\n\\nOur first metric checks whether the output sentence reflects the target style. This is measured by an external classifier\u2019s predictions on system outputs. We use two variants of transfer accuracy: (1) Relative Accuracy (r-ACC): does the target style classifier score the output sentence higher than the input sentence? (2) Absolute Accuracy (a-ACC): does the classifier score the output sentence higher than 0.5?\\n\\nBuilding multilingual classifiers: Unfortunately, no large style classification datasets exist for most languages, preventing us from building classifiers from scratch. We resort to zero-shot cross lingual transfer techniques (Conneau and Lample, 2019), where large multilingual pretrained models are first fine-tuned on English classification data, and then applied to other languages at inference. We experiment with three such techniques, and find MAD-X classifiers with language adapters (Pfeiffer et al., 2020b) have the highest accuracy of 81% on our Hindi data from Section 5.1. However, MAD-X classifiers were only available for Hindi, so we use https://taskmate.google.com\"}"}
{"id": "acl-2022-long-514", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the next best XLM RoBERTa-base (Conneau et al., 2020) for other languages, which has 75%-82% accuracy on annotated data; details in Appendix E.1.\\n\\n5.3 Semantic Similarity (SIM)\\n\\nOur second evaluation criteria is semantic similarity between the input and output. Following recent recommendations (Marie et al., 2021; Krishna et al., 2020), we avoid n-gram overlap metrics like BLEU (Papineni et al., 2002). Instead, we use LaBSE (Feng et al., 2020), a language-agnostic semantic similarity model based on multilingual BERT (Devlin et al., 2019). LaBSE supports 109 languages, and is the only similarity model we found supporting all the Indic languages in this work. We also observed LaBSE had greater correlation with our annotated data (Section 5.1) compared to alternatives; details in Appendix E.2.\\n\\nQualitatively, we found that sentence pairs with LaBSE scores lower than 0.6 were almost never paraphrases. To avoid rewarding partial credit for low LaBSE scores, we use a hard threshold \\\\( L = 0.75 \\\\) to determine whether pairs are paraphrases, \\\\( \\\\text{SIM}(x,y') = 1 \\\\) if \\\\( \\\\{ \\\\text{LaBSE}(x,y') > L \\\\} \\\\) else 0.\\n\\n5.4 Other Metrics (LANG, COPY, 1-g)\\n\\nAdditionally, we measure whether the input and output sentences are in the same language (LANG), the fraction of outputs copied verbatim from the input (COPY), and the 1-gram overlap between input / output (1-g). High LANG and low COPY / 1-g (more diversity) is better; details in Appendix E.6.\\n\\n5.5 Aggregated Score (r-AGG, a-AGG)\\n\\nTo get a sense of overall system performance, we combine individual metrics into one score. Similar to Krishna et al. (2020) we aggregate metrics as,\\n\\n\\\\[\\n\\\\text{AGG}(x,y') = \\\\text{ACC}(x,y') \\\\cdot \\\\text{SIM}(x,y') \\\\cdot \\\\text{LANG}(y')\\n\\\\]\\n\\n\\\\[\\n\\\\text{AGG}(D) = \\\\frac{1}{|D|} \\\\sum_{x,y' \\\\in D} \\\\text{AGG}(x,y')\\n\\\\]\\n\\nWhere \\\\((x,y')\\\\) are input-output pairs, and \\\\(D\\\\) is the test corpus. Since each of our individual metrics can only take values 0 or 1 at an instance level, our aggregation acts like a Boolean AND operation. In other words, we are measuring the fraction of outputs which simultaneously transfer style, have the same language as the input. Depending on the variant of \\\\(\\\\text{ACC}\\\\) (relative / absolute), we can derive r-AGG / a-AGG.\\n\\n5.6 Evaluating Control (CALIB)\\n\\nAn ideal system should not only be able to style transfer sentences, but also control the magnitude of style transfer using the scalar input \\\\(\\\\lambda\\\\). To evaluate this, for every system we first determine a \\\\(\\\\lambda_{\\\\text{max}}\\\\) value and let \\\\([0, \\\\lambda_{\\\\text{max}}]\\\\) be the range of control values. While in our setup \\\\(\\\\lambda\\\\) is an unbounded scalar, we noticed high values of \\\\(\\\\lambda\\\\) significantly perturb semantics (also noted in Garcia et al., 2021), with systems outputting style-specific n-grams unfaithful to the output. We choose \\\\(\\\\lambda_{\\\\text{max}}\\\\) to be the largest \\\\(\\\\lambda\\\\) from the list \\\\([0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\\\\) whose outputs have an average semantic similarity score (SIM, Section 5.3) of at least 0.75 with the validation set inputs. For each system we take three evenly spaced \\\\(\\\\lambda\\\\) values in its control range, denoted as \\\\(\\\\Lambda = [1/3 \\\\lambda_{\\\\text{max}}, 2/3 \\\\lambda_{\\\\text{max}}, \\\\lambda_{\\\\text{max}}]\\\\). We then compute the style calibration to \\\\(\\\\lambda\\\\), or how often does increasing \\\\(\\\\lambda\\\\) lead to a style score increase?\\n\\nWe measure this with a statistic similar to Kendall's \\\\(\\\\tau\\\\) (Kendall, 1938), counting concordant pairs in \\\\(\\\\Lambda\\\\),\\n\\n\\\\[\\n\\\\text{CALIB}(x) = \\\\frac{1}{n} \\\\sum \\\\frac{1}{\\\\lambda_b > \\\\lambda_a} \\\\{ \\\\text{style}(y_{\\\\lambda_b}) > \\\\text{style}(y_{\\\\lambda_a}) \\\\}\\n\\\\]\\n\\nwhere \\\\(x\\\\) is input, \\\\(\\\\text{CALIB}(x)\\\\) is the average over all possible \\\\(n = 3\\\\) pairs of \\\\(\\\\lambda\\\\) values \\\\((\\\\lambda_a, \\\\lambda_b)\\\\) in \\\\(\\\\Lambda\\\\).\\n\\n5.7 Human Evaluation\\n\\nAutomatic metrics are usually insufficient for style transfer evaluation \u2014 according to Briakou et al. (2021a), 69 / 97 surveyed style transfer papers used human evaluation. We adopt the crowd-sourcing setup from Section 5.1, which was used to build our formality evaluation datasets. We presented 200 generations from each model and the corresponding inputs in a random order, and asked three crowdworkers two questions about each pair of sentences: (1) which sentence is more formal/code-mixed? (2) how similar are the two sentences in meaning? This lets us evaluate r-ACC, SIM, r-AGG, CALIB with respect to human annotations instead of classifier predictions. More experiment details (inter-annotator agreement, compensation, instructions) are provided in Appendix E.4.\"}"}
{"id": "acl-2022-long-514", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Automatic evaluation of formality transfer in Indic languages. Note each proposed method (*-INDIC, +BT) improves performance (\\\\textsc{aggr} defined in Section 5.5), with a combination (\\\\textsc{diffur} - MLT) doing best.\\n\\n| Model    | Hindi | Bengali | Kannada | Telugu | Gujarati |\\n|----------|-------|---------|---------|--------|----------|\\n| UR (2021) | 30.4  | 10.4    | 30.4    | 7.2    | 25.5     |\\n| UR - INDIC | 58.3  | 18.6    | 65.5    | 22.3   | 61.3     |\\n| UR + BT   | 54.2  | 17.8    | 55.6    | 16.9   | 39.8     |\\n| UR - INDIC + BT | 60.0 | 22.2  | 61.1 | 22.0 | 59.2 |\\n| \\\\textsc{diffur} | 71.1  | 22.9    | 72.7    | 25.2   | 69.2     |\\n| \\\\textsc{diffur} - INDIC | 72.6 | 24.0 | 75.4 | 24.3 | 73.1 |\\n| \\\\textsc{diffur} - MLT | 78.1 | 32.2 | 80.0 | 35.0 | 80.4 |\\n\\nTable 2: Performance by individual metrics for Hindi formality transfer.\\n\\n\\\\textsc{diffur} - MLT gives best overall performance (\\\\textsc{aggr} / \\\\textsc{acc}), with a good trade-off between style accuracy (\\\\textsc{acc}), semantic similarity (\\\\textsc{sim}), langID score (\\\\textsc{lang}), and low input copy rates (\\\\textsc{copy}); metrics defined in Section 5, other language results in Appendix I.\\n\\n6 Main Experiments\\n\\n6.1 Experimental Setup\\n\\nIn our experiments, we compare the following models (training details are provided Appendix A):\\n\\n- UR: the Universal Rewriter (Garcia et al., 2021), which is our main baseline (Section 3);\\n- \\\\textsc{diffur}: our model with paraphrase vector differences (Section 4.2);\\n- UR - INDIC, \\\\textsc{diffur} - INDIC: Indic variants of UR and \\\\textsc{diffur} models (Section 4.3);\\n- \\\\textsc{diffur} - MLT: Multitask training between UR - INDIC and \\\\textsc{diffur} - INDIC (Section 4.4);\\n- + BT: models with style-controlled backtranslation at inference time (Section 4.1).\\n\\nOur models are evaluated on (1) formality transfer (Rao and Tetreault, 2018); (2) code-mixing addition, a task where systems attempt to use English words in non-English sentences, while preserving the original script.\\n\\n13 Since we do not have access to any formality evaluation dataset, we hold out 22K sentences from Samanantar in each Indic language. 14 Hinglish is common in India, examples in Figure 5.\\n\\nSince we do not have access to any formality evaluation dataset, we hold out 22K sentences from Samanantar in each Indic language. We do not use GYAFC (Rao and Tetreault, 2018) and XFORMAL (Briakou et al., 2021b) due to reasons in footnote 4. Our dataset from Section 5.1 has already been used for classifier selection, and has machine generated sentences. For validation / testing. For Swahili / Spanish, we use mC4 / WMT2018 sentences. These sets have similar number of formal / informal sentences, as marked by our formality classifiers (Section 5.2), and are transferred to the opposite formality. We re-use the hi/bn formality transfer splits for code-mixing addition, evaluating unidirectional transfer.\\n\\nSeven languages with varying scripts and morphological richness are used for evaluation (hi, es, sw, bn, kn, te, gu). The UR model only saw translation data for hi, es, bn, whereas UR - INDIC sees translation data for all Indic languages (Section 4.3). To test the generalization capability of the \\\\textsc{diffur}, no Gujarati paraphrase training data for is used. Note that no paired/unpaired data with style labels is used during training: models determine the target style at inference using 3-10 exemplars sentences. For few-shot formality transfer, we use the English exemplars from Garcia et al. (2021). We follow their setup and use English exemplars to guide non-English transfer zero-shot. For code-mixing addition, we use Hindi/English code-mixed exemplars in Devanagari (shown in Appendix D).\\n\\n6.2 Main Results\\n\\nEach proposed method improves over prior work, \\\\textsc{diffur} - MLT works best. We present our...\"}"}
{"id": "acl-2022-long-514", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Variation in Kannada formality transfer with $\\\\lambda$. In the left plot, we see DIFFUR-* models have consistently good overall performance with change in $\\\\lambda$. In the right plot, we see the tradeoff between average style change and content similarity as $\\\\lambda$ is varied. Plots (such as DIFFUR-*) which stretch the Y-axis range, closer to the ideal system ($x=1$) and away from the naive system ($x+y=1$, akin to naive model in Krishna et al., 2020) are better.\\n\\nTable 3: Automatic evaluation of formality transfer in Swahili and Spanish. DIFFUR-MLT performs best.\\n\\n| Model           | ACC SIM AGG |\\n|-----------------|-------------|\\n| UR (2021)       | 19.9 / 4.8  |\\n| UR, BT          | 13.7 / 3.4  |\\n| DIFFUR-MLT      | 32.2 / 7.2  |\\n\\nTable 4: Human evaluation on Hindi formality transfer, measuring style accuracy (ACC), input similarity (SIM), overall score (AGG) and control with $\\\\lambda$ (CALIB, C-IN). Like Table 1, DIFFUR-MLT performs best.\\n\\n| Model           | CALIB |\\n|-----------------|-------|\\n| UR (2021)       | 29.5  |\\n| UR-INDIC        | 46.5  |\\n| UR + BT         | 57.5  |\\n| UR-INDIC + BT   | 65.0  |\\n| DIFFUR          | 64.5  |\\n| DIFFUR-INDIC    | 62.0  |\\n| DIFFUR-MLT      | 70.0  |\\n\\nTable 5: Human evaluation on code-mixing addition. DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC). Due to verbatim copying, UR SIM score is nearly 100, but ACC score close to 0.\\n\\n| Model           | CALIB |\\n|-----------------|-------|\\n| UR (2021)       | 29.2  |\\n| DIFFUR          | 64.9  |\\n| UR-INDIC        | 60.7  |\\n| DIFFUR-INDIC    | 69.6  |\\n| UR + BT         | 43.4  |\\n| DIFFUR-MLT      | 69.0  |\\n\\nTable 6: Evaluation of Hindi formality transfer magnitude control using $\\\\lambda$. We find that DIFFUR-* are best at calibrating style change (CALIB) to input $\\\\lambda$ (metrics details in Section 5.6, more results in Appendix F).\\n\\n| Model           | CALIB |\\n|-----------------|-------|\\n| UR (2021)       | 29.2  |\\n| DIFFUR          | 64.9  |\\n| UR-INDIC        | 60.7  |\\n| DIFFUR-INDIC    | 69.6  |\\n| UR + BT         | 43.4  |\\n| DIFFUR-MLT      | 69.0  |\"}"}
{"id": "acl-2022-long-514", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As sentences get more formal, the English word \\\"job\\\" (\u091c\u0949\u092c) is converted to Persian (\u0928\u094c\u0915\u0930\u0940) / high Sanskrit (\u0928\u092f\u0941\u093f\u0924) and honorifics are used (\u0906\u092a\u0915\u0940, \u092c\u0924\u093e\u090f\u0902).\\n\\nFormal: \\n\u0939\u0902\u0938\u093e \u092e\u0947\u0902 \u0926\u094b \u0932\u094b\u0917\u094b\u0902 \u0915\u0940 \u092e\u094c\u0924 \u0939\u0941\u0908 \u0914\u0930 \u0932\u0917\u092d\u0917 150 \u0918\u093e\u092f\u0932 \u0939\u0941\u090f\u0964 (two people died in the violence and 150 were injured)\\n\\nInformal: \\n\u0939\u0902\u0938\u093e \u092e\u0947\u0902 \u0926\u094b \u0932\u094b\u0917 \u092e\u093e\u0930\u0947 \u0917\u090f \u0914\u0930 150 \u0915\u0947 \u0915\u0930\u0940\u092c \u0932\u094b\u0917 \u0918\u093e\u092f\u0932 \u0917\u090f\u0964\\n\\nAs sentences get more informal, besides lexical changes, sentence shortening is common, while roughly conveying same meaning.\\n\\nPositive Sentiment: \\n\u092e\u0941\u091d\u0947 \u092f\u0939 \u092b\u093c\u093f\u0932\u094d\u092e \u092c\u0939\u0941\u0924 \u092a\u0938\u0902\u0926 \u0906\u0908\u0964\\n\\nSimple: \\n\u092d\u093e\u091c\u092a\u093e \u092e\u091c\u093e\u0915 \u0915\u0930\u0924\u0940 \u0926\u0916\u0932\u0940 \u0939\u0948\u0964\\n\\nComplex: \\n\u092d\u093e\u091c\u092a\u093e \u0935\u094d\u092f\u0902\u0917\u094d\u092f \u0915\u0930\u0924\u0940 \u0939\u0948\u0964 \u0915\u0920\u0928 \u092a\u0930\u0936\u094d\u0930\u092e \u0915\u0930 \u0938\u0915\u0924\u093e \u0939\u0948\u0964\\n\\nGendered: \\n\u0930\u092f\u094b \u0913\u0932\u0902\u092a\u0915: \u092c\u0948\u0921\u092e\u0902\u091f\u0928 \u092e\u0947\u0902 \u092d\u093e\u0930\u0924\u0940\u092f \u092e\u0939\u0932\u093e\u0913\u0902 \u0928\u0947 \u0915\u092f\u093e \u0928\u0930\u093e\u0936, \u0939\u093e\u0930 \u0938\u0947 \u0939\u0941\u0908 \u0936\u0941\u0930\u0942\u0906\u0924\u0964\\n\\nGender Neutral: \\n\u0930\u092f\u094b \u0913\u0932\u0902\u092a\u0915: \u092c\u0948\u0921\u092e\u0902\u091f\u0928 \u092e\u0947\u0902 \u092d\u093e\u0930\u0924\u0940\u092f \u0916\u0932\u093e\u0921\u093c\u092f\u094b\u0902 \u0928\u0947 \u0915\u090f \u0928\u0930\u093e\u0936, \u0939\u093e\u0930 \u0938\u0947 \u0939\u0941\u0908 \u0936\u0941\u0930\u0942\u0906\u0924\u0964\\n\\nNegations and word antonyms are common as sentiment changes.\\n\\nLexical substitutions (\u0935\u094d\u092f\u0902\u0917\u094d\u092f \u2192 \u092e\u091c\u093e\u0915, \u0915\u0920\u0928 \u2192 \u0915\u0921\u093c\u0940) to use more commonly spoken words.\\n\\nWith code-mixing, several English words are introduced (\u0924\u0925\u093e \u2192 \u0921\u0947\u091f / date, \u0905\u0925\u093e\u0930\u094d\u0935\u093f\u0924 \u2192 i.e., \u0938\u0932\u093e\u0939\u0915\u093e\u0930\u0940 \u0938\u0947\u0935\u093e\u090f\u0902 \u2192 \u0915\u093e\u0909\u0902\u0938\u0932\u0902\u0917 \u0938\u0935\u0930\u094d\u0935\u093f\u0938\u091c\u093c / counseling services).\\n\\nEntities are replaced with PII (Personal Identifiable Information) tags, to anonymize text.\\n\\nGendered words (\u092e\u0939\u0932\u093e\u0913\u0902) are replaced with their neutral equivalents (\u0916\u0932\u093e\u0921\u093c\u092f\u094b\u0902).\\n\\nEntities (\u0905\u0926\u0924 \u0930\u093e\u0935 \u0939\u0948\u0926\u0930\u0940, \u0907\u0938\u092e\u093e\u0908\u0932 \u0914\u0930 \u0907\u0932\u093e\u092e\u0938\u0940 \u0914\u0930 \u0932\u0942\u0924) \u0915\u094b \u092d\u0940\u0964\\n\\nIn Appendix G we show ablations studies justifying the DIFFUR design, decoding scheme, etc.\\n\\nIn Appendix I we show a breakdown by individual metrics for other languages and plot variations with $\\\\lambda$. We also analyze the style encoder $f_{\\\\text{style}}$ in Appendix H, finding it is an effective style classifier.\\n\\nWe analyze several qualitative outputs from DIFFUR-MLT in Figure 4. Besides formality transfer and code-mixing addition, we transfer several other attributes: sentiment (Li et al., 2018), simplicity (Xu et al., 2015), anonymity (Anandan et al., 2012) and gender neutrality (Reddy and Knight, 2016). More outputs are provided in Appendix J.\"}"}
{"id": "acl-2022-long-514", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nWe are very grateful to the Task Mate team (especially Auric Bonifacio Quintana) for their support and helping us crowdsource data and evaluate models on their platform. We thank John Wieting, Timothy Dozat, Manish Gupta, Rajesh Bhatt, Esha Banerjee, Yixiao Song, Marzena Karpinska, Aravindan Raghuveer, Noah Constant, Parker Riley, Andrea Schioppa, Artem Sokolov, Mohit Iyyer and Slav Petrov for several useful discussions during the course of this project. We are also grateful to Rajiv Teja Nagipogu, Shachi Dave, Bhuthesh R, Parth Kothari, Bhanu Teja Gullapalli and Simran Khanuja for helping us annotate model outputs in several Indian languages during pilot experiments.\\n\\nThis work was mostly done during Kalpesh Krishna (KK)'s internship at Google Research India, hosted by Bidisha Samanta and Partha Talukdar. KK was partly supported by a Google PhD Fellowship.\\n\\nEthical Considerations\\n\\nRecent work has highlighted issues of stylistic bias in text generation systems, specifically machine translation systems (Hovy et al., 2020). We acknowledge these issues, and consider style transfer and style-controlled generation technology as an opportunity to work towards fixing them (for instance, gender neutralization as presented in Section 6.2). Note that it is important to tread down this path carefully \u2014 In Chapter 9, Blodgett (2021) argue that style is inseparable from social meaning (as originally noted by Eckert, 2008), and humans may perceive automatically generated text very differently compared to automatic style classifiers.\\n\\nOur models were trained on 32 Google Cloud TPUs. As discussed in Appendix A, the UR and UR-INDIC model take roughly 18 hours to train. The DIFFUR-* and DIFFUR-MLT models are much cheaper to train (2 hours) since we finetune the pretrained UR-* models. The Google 2020 environment report mentions, 15 \\\"TPUs are highly efficient chips which have been specifically designed for machine learning applications\\\". These accelerators run on Google Cloud, which is carbon neutral today, and is aiming to \\\"run on carbon-free energy, 24/7, at all of Google's data centers by 2030\\\" (https://cloud.google.com/sustainability).\\n\\nReferences\\n\\nRama Kant Agnihotri. 2013. Hindi: An essential grammar. Routledge.\\n\\nNader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6470\u20136484, Online. Association for Computational Linguistics.\\n\\nBalamurugan Anandan, Chris Clifton, Wei Jiang, Mummoorthy Murugesan, Pedro Pastrana-Camacho, and Luo Si. 2012. t-plausibility: Generalizing words to desensitize text. Transactions on Data Privacy, 5(3):505\u2013534.\\n\\nKalika Bali, Jatin Sharma, Monojit Choudhury, and Yogesh Vyas. 2014. \\\"i am borrowing ya mixing?\\\" an analysis of english-hindi code mixing in facebook. In Proceedings of the First Workshop on Computational Approaches to Code Switching, pages 116\u2013126.\\n\\nSu Lin Blodgett. 2021. Sociolinguistically driven approaches for just natural language processing. UMass Amherst Doctoral Dissertations. 2092.\\n\\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. Github.\\n\\nEleftheria Briakou, Sweta Agrawal, Ke Zhang, Joel Tetreault, and Marine Carpuat. 2021a. A review of human evaluation for style transfer. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), Online.\\n\\nEleftheria Briakou, Di Lu, Ke Zhang, and Joel Tetreault. 2021b. Ol\u00e1, bonjour, salve! XFORMAL: A benchmark for multilingual formality style transfer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3199\u20133216, Online. Association for Computational Linguistics.\\n\\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. arXiv preprint arXiv:2006.14799.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1\u201314, Vancouver, Canada. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-514", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-514", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-514", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-514", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices for \u201cFew-shot Controllable Style Transfer for Low-Resource Multilingual Settings\u201d\\n\\nA Model training details\\n\\nTo train the UR-INDIC model, we use mC4 (Xue et al., 2021b) for the self-supervised objectives and Samanantar (Ramesh et al., 2021) for the supervised translation. For creating paraphrase data for training our DIFFUR models (Section 4.2), we again leverage Indic language side of Samanantar sentence pairs. Our models are implemented in JAX (Bradbury et al., 2018) using the T5X library.\\n\\nWe re-use the UR checkpoint from Garcia et al. (2021). To train the UR-INDIC model, we follow the setup in Garcia et al. (2021) and initialize the model with mT5-XL (Xue et al., 2021b), which has 3.7B parameters. We fine-tune the model for 25K steps with a batch size of 512 inputs and a learning rate of 1e-3, using the objectives in Section 3. Training was done on 32 Google Cloud TPUs which took a total of 17.5 hours. To train the DIFFUR and DIFFUR-INDIC models, we further fine-tune UR and UR-INDIC for a total of 4K steps using the objective from Section 4.2, taking 2 hours.\\n\\nB More Related Work\\n\\nMultilingual style transfer is mostly unexplored in prior work: a 35 paper survey by Briakou et al. (2021b) found only one work in Chinese, Russian, Latvian, Estonian, French (Shang et al., 2019; Tikhonov and Yamshchikov, 2018; Korotkova et al., 2019; Niu et al., 2018). Briakou et al. (2021b) further introduced XFORMAL, the first formality transfer evaluation dataset in French, Brazilian Portuguese and Italian. Hindi formality has been studied in linguistics, focusing on politeness (Kachru, 2006; Agnihotri, 2013; Kumar, 2014) and code-mixing (Bali et al., 2014). Due to its prevalence in India, English-Hindi code-mixing has seen work in language modeling (Pratapa et al., 2018; Samanta et al., 2019) and core NLP tasks (Khanuja et al., 2020). To the best of our knowledge, we are the first to study style transfer for Indic languages.\\n\\nA few prior works build models which can control the degree of style transfer using a scalar input (Wang et al., 2019; Samanta et al., 2021). These models are style-specific and require large unpaired style corpora during training. We adopt the inference-time control method used by Garcia et al. (2021) and notice much better controllability after our proposed fixes in Section 4.2.\\n\\nC More details on the translation-specific Universal Rewriter objectives\\n\\nIn this section we describe the details of the supervised translation objective and the style-controlled translation objective used in the Universal Rewriter model. See Section 3 for details on the exemplar-based denoising objective.\\n\\nLearning translation via direct supervision: This objective is the standard supervised translation setup, using zero vectors for style. The output language code is prepended to the input. Consider a pair of parallel sentences \\\\((x, y)\\\\) in languages with codes \\\\(l_x, l_y\\\\) (prepended to the input string), \\\\(\\\\bar{y} = \\\\text{fur}(l_y \\\\oplus x, 0)\\\\) \\\\(L_{\\\\text{translate}} = L_{\\\\text{CE}}(\\\\bar{y}, y)\\\\)\\n\\nThe Universal Rewriter is trained on English-centric translation data from the high-resource languages in OPUS-100 (Zhang et al., 2020).\\n\\nLearning style-controlled translation: This objective emulates \u201cstyle-controlled translation\u201d in a self-supervised manner, via backtranslation through English. Consider \\\\(x_1\\\\) and \\\\(x_2\\\\) to be two non-overlapping spans in mC4 in language \\\\(l_x\\\\), \\\\(x_{\\\\text{en}}_2 = \\\\text{fur}(\\\\text{en} \\\\oplus x_2, -f_{\\\\text{style}}(x_1))\\\\) \\\\(\\\\bar{x}_2 = \\\\text{fur}(l_x \\\\oplus x_{\\\\text{en}}_2, f_{\\\\text{style}}(x_1))\\\\) \\\\(L_{\\\\text{BT}} = L_{\\\\text{CE}}(\\\\bar{x}_2, x_2)\\\\)\\n\\nD Choice of Exemplars\\n\\nFormal exemplars\\n1. This was a remarkably thought-provoking read.\\n2. It is certainly amongst my favorites.\\n3. We humbly request your presence at our gala in the coming week.\\n\\nInformal exemplars\\n1. reading this rly makes u think\\n2. Its def one of my favs\\n3. come swing by our bbq next week if ya can make it\\n\\nComplex exemplars\"}"}
{"id": "acl-2022-long-514", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. \u0917\u0941\u0921 \u092e\u0949\u0928\u0930\u094d\u0928\u093f\u0902\u0917, \u092d\u093e\u0930\u0924\\n2. \u0905\u0917\u0930 \u0906\u092a \u0907\u0938\u0947 \u092b\u094d\u0930\u0940\u091c \u0915\u0930\u0928\u093e \u091a\u093e\u0939\u0924\u0947 \u0939\u0948\u0902, \u0924\u094b \u0906\u092a\u0915\u094b \u091f\u0947\u0902\u092a\u0947\u0930\u0947\u091a\u0930 \u0915\u092e \u0915\u0930\u0928\u093e \u091a\u093e\u0939\u090f\\n3. \u0939\u093e\u092f \u092e\u0941\u091d\u0947 \u091c\u0949\u092c \u091a\u093e\u0939\u090f\\n4. \u0939\u0949\u0932\u0940\u0935\u0941\u0921 \u090f\u0915\u094d\u091f\u094d\u0930\u0947\u0938 \u090f\u0902\u091c\u0947\u0932\u0928\u093e \u091c\u0949\u0932\u0940 \u090f\u0915 \u090f\u0928\u092e\u0947\u0936\u0928 \u091a\u0932\u091a\u093f\u0924\u094d\u0930 \u092a\u094d\u0930\u094b\u0921\u094d\u092f\u0942\u0938 \u0915\u0930 \u0930\u0939\u0940 \u0939\u0948\u0902\u0964\\n5. \u0907\u0938 \u091f\u0942\u0928\u0930\u094d\u0927\u093e\u092e\u0947\u0902 \u091b\u0939 \u091f\u0940\u092e\u0947\u0902 \u091f\u093e\u0907\u091f\u0932 \u0915\u0947 \u0932\u090f \u0915\u092e\u094d\u092a\u0940\u091f\u094d\u0915\u0930\u0947\u0902\u0917\u0940\u0964\\n\\nE.1 Multilingual Classifier Selection\\n\\nDue to the absence of a style classification dataset in Indic languages, we built our multilingual classifier drawing inspiration from recent research in zero-shot cross-lingual transfer (Conneau et al., 2018; Conneau and Lample, 2019; Pfeiffer et al., 2020b). We experimented with three zero-shot transfer techniques while selecting our classifiers for evaluating multilingual style transfer.\\n\\nTRANSLATE TRAIN: The first technique uses the hypothesis that style is preserved across translation.\"}"}
{"id": "acl-2022-long-514", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We classify the style of English sentences in the Samantar translation dataset (Ramesh et al., 2021) using a style classifier trained on English formality data from Krishna et al. (2020). We use the human translated Indic languages sentences as training data. This training data is used to fine-tune a large-scale multilingual language model.\\n\\n**ZERO-SHOT:** The second technique fine-tunes large-scale multilingual language models on an English style transfer dataset, and applies it zero-shot on multilingual data during inference.\\n\\n**MAD-X:** Introduced by Pfeiffer et al. (2020b), this technique is similar to ZERO-SHOT but additionally uses language-specific parameters (\u201cadapters\u201d) during inference. These language-specific adapters have been originally trained using masked language modeling on the desired language data.\\n\\n**Dataset for evaluating classifiers**\\n\\nWe conduct our experiments on Hindi formality classification, leveraging our evaluation datasets from Section 5.1. We removed pairs which did not have full agreement across the three annotators and those pairs which had the consensus rating of \u201cEqual\u201d formality. This filtering process leaves us with 316 pairs in Hindi (out of 1000). In our experiments, we check whether the classifiers give a higher score to the more formal sentence in the pair.\\n\\n**Models**\\n\\nWe leverage the multilingual classifiers open-sourced by Krishna et al. (2020). These models have been trained on the English GYAFC formality classification dataset (Rao and Tetreault, 2018), and have been shown to be effective on the XFORMAL dataset (Briakou et al., 2021b) for formality classification in Italian, French and Brazilian Portuguese.\\n\\nThese classifiers were trained on preprocessed data which had trailing punctuation stripped and English sentences lower-cased, encouraging the models to focus on lexical and syntactic choices. As base multilingual language models, we use (1) mBERT-base from Devlin et al. (2019); (2) XLM-RoBERTa-base from Conneau et al. (2020).\\n\\n**Results**\\n\\n| Model   | Hindi Formality Classification Accuracy (%) |\\n|---------|---------------------------------------------|\\n| TRANSLATE TRAIN |                                            |\\n| mBERT   | 66%                                         |\\n| ZERO-SHOT mBERT | 72%                                         |\\n| XLM-R   | 76%                                         |\\n| MAD-X   | 81%                                         |\\n\\nTable 7: Hindi formality classification accuracy on our crowdsourced dataset (Section 5.1) using different cross-lingual transfer methods. Our results indicate that MAD-X is the most effective method, and XLM-R is a better pretrained model than mBERT.\\n\\n| Language | mBERT | XLM-R |\\n|----------|-------|-------|\\n| bn       | 65.3% | 82.2% |\\n| kn       | 76.3% | 76.9% |\\n| te       | 72.6% | 74.6% |\\n\\nTable 8: Formality classification on our crowdsourced Bengali, Kannada and Telugu dataset (Section 5.1) using the ZERO-SHOT technique described in Appendix E.1. Results confirm the efficacy of the XLM-R classifier. See Table 7 for Hindi results.\\n\\n**E.2 Semantic Similarity Model Selection**\\n\\nWe considered three models for evaluating semantic similarity between the input and output: (1) LaBSE (Feng et al., 2020); (2) m-USE (Yang et al., 2020); (3) multilingual Sentence-BERT (Reimers and Gurevych, 2020), the knowledge-distilled variant paraphrase-xlm-r-multilingual-v1. Among these models, only LaBSE has support for all the Indic languages we were interested in. No Indic language is supported by m-USE, and...\"}"}
{"id": "acl-2022-long-514", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"multilingual Sentence-BERT has been trained on parallel data only for Hindi, Gujarati and Marathi among our Indic languages. However, in terms of Semantic Textual Similarity (STS) benchmarks (Cer et al., 2017) for English, Arabic & Spanish, m-USE and Sentence-BERT outperform LaBSE (Table 1 in Reimers and Gurevych, 2020). LaBSE correlates better than Sentence-BERT with our human-annotated formality dataset:\\n\\nWe measured the Spearman's rank correlation between the semantic similarity annotations on our human-annotated formality datasets (Section 5.1). We discarded 10% sentence pairs which had no agreement among three annotators and took the majority vote for the other sentence pairs. We assigned \u201cDifferent Meaning\u201d a score of 0, \u201cSlight Difference in Meaning\u201d a score of 1 and \u201cApproximately Same Meaning\u201d a score of 2 before measuring Spearman's rank correlation. In Table 9 we see a stronger correlation of human annotations with LaBSE compared to Sentence-BERT, especially for languages like Bengali, Kannada for which Sentence-BERT did not see parallel data.\\n\\n| Threshold | Hindi | Gujarati | Marathi | Average |\\n|-----------|-------|----------|---------|---------|\\n| 0.65      | 97.4  | 96.1     | 94.6    | 91.3    |\\n| 0.75      | 83.9  | 76.1     | 68.4    | 75.5    |\\n| 0.85      | 75.1  | 62.7     | 50.5    | 64.0    |\\n\\nTable 10: Percentage of human annotated semantically similar pairs which have a LaBSE score of at least $L$.\\n\\nAs we increase the threshold $L$, we see this percentage substantially reduces, indicating our chosen thresholds are within the range of variation in LaBSE scores for semantically similar sentences.\\n\\nE.4 More Crowdsourcing Details\\n\\nIn Figure 17, we show screenshots of our crowdsourcing interface along with all the instructions shown to crowdworkers. The instructions were written after consulting professional Indian linguists. Each crowdworker was allowed to annotate a maximum of 50 different sentence pairs per language, paying them $0.05 per pair. For formality classification, we showed crowdworkers two sentences and asked them to choose which one is more formal. Crowdworkers were allowed to mark ties using an \u201dEqual\u201d option. For semantic similarity annotation, we showed crowdworkers the sentence pair and provided three options \u2014 \u201dapproximately same meaning\u201d, \u201dslight difference in meaning\u201d, \u201ddifferent meaning\u201d, to emulate a 3-point Likert scale. While performing our human evaluation (Section 5.7), we use a 0.5 $SIM$ score for \u201dslight difference in meaning\u201d and a 1.0 $SIM$ score for \u201dapproximately same meaning\u201d annotations. For every system considered, we analyzed the same set of 200 input sentences for style transfer performance, and 100 of those sentences for evaluating controllability. We removed sentences which were exact copies of the input (after removing trailing punctuation) or were in the wrong language to save annotator time and cost. When outputs were exact copies of the input, we discarded those.\"}"}
{"id": "acl-2022-long-514", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"input, we assigned SIM = 100, ACC = 0, AGG = 0.\\n\\nIn Table 11 and Table 12 we show the inter-annotator agreement statistics. We measure Fleiss Kappa (Fleiss, 1971), Randolph Kappa (Randolph, 2005; Warrens, 2010), the fraction of sentence pairs with total agreement between the three annotators and the fraction of sentence pairs with no agreement.\\n\\n19\\n\\nIn the table we can see all agreement statistics are well away from a uniform random annotation baseline, indicating good agreement.\\n\\n|            | F-\u03ba | R-\u03ba | all agree | none agree |\\n|------------|-----|-----|-----------|------------|\\n| Random     | 0.00| 0.00| 11.1%     | 22.2%      |\\n| hi         | 0.21| 0.28| 32.8%     | 10.2%      |\\n| bn         | 0.33| 0.40| 43.8%     | 7.2%       |\\n| kn         | 0.22| 0.31| 35.0%     | 7.7%       |\\n| te         | 0.21| 0.31| 36.0%     | 9.3%       |\\n\\nTable 11: Fleiss kappa (F-\u03ba), Randolph kappa (R-\u03ba), and agreement scores of crowdsourcing for formality classification. All \u03ba scores are well above a random annotation baseline, indicating fair agreement.\\n\\n|            | F-\u03ba | R-\u03ba | all agree | none agree |\\n|------------|-----|-----|-----------|------------|\\n| Random     | 0.00| 0.00| 11.1%     | 22.2%      |\\n| hi         | 0.10| 0.27| 32.6%     | 11.8%      |\\n| bn         | 0.24| 0.34| 38.7%     | 10.2%      |\\n| kn         | 0.13| 0.25| 30.8%     | 11.3%      |\\n| te         | 0.10| 0.31| 36.1%     | 9.7%       |\\n\\nTable 12: Fleiss kappa (F-\u03ba), Randolph kappa (R-\u03ba), and agreement scores of crowdsourcing for semantic similarity. All \u03ba scores are well above a random annotation baseline, indicating fair agreement.\\n\\nE.5 Fluency Evaluation\\n\\nUnlike some prior works, we avoid evaluation of output fluency due to the following reasons:\\n\\n1. lack of fluency evaluation tools for Indic languages;\\n2. fluency evaluation often discriminates against styles which are out-of-distribution for the fluency classifier, as discussed in Appendix A.8 of Krishna et al. (2020);\\n3. several prior works (Pang, 2019; Mir et al., 2019; Krishna et al., 2020) have recommended against using perplexity of style language models for fluency evaluation since it is unbounded and favours unnatural sentences with common words;\\n4. large language models are known to produce fluent text as perceived by humans (Ippolito et al., 2020; Akoury et al., 2020), reducing the need for this evaluation.\\n\\nE.6 Details of other individual metrics\\n\\nLanguage Consistency (LANG): Since our semantic similarity metric LaBSE is language-agnostic, it tends to ignore accidental translations, which are common errors in large multilingual transformers (Xue et al., 2021a,b), especially the Universal Rewriter (Section 3.1). Hence, we check whether the output sentence is in the same language as the input, using langdetect.\\n\\nOutput Diversity (COPY, 1-g): As discussed in Section 3.1, the Universal Rewriter has a strong tendency to copy the input verbatim. We build two metrics to measure output diversity compared to the input, which have been previously used for extrACTIVE question answering evaluation (Rajpurkar et al., 2016). The first metric COPY measures the fraction of outputs which were copied verbatim from the input. This is done after removing trailing punctuation, to penalize models generations which solely modify punctuation. A second metric 1-g measures the unigram overlap F1 score between the input and output. A diverse style transfer system should minimize both COPY and 1-g.\\n\\nF More Controllability Evaluations\\n\\nWe follow the setup in Section 5.6 to first compute a \u03bb max per system. We then compute the following,\\n\\n1. Style Transfer Performance (r-AGG): An ideal system should have good overall performance (Section 5.5) across different values in the range \u039b.\\n\\n2. Average Style Score Increase (INCR): As our control value increases, we want the classifier's target style score (compared to the input) to increase. Additionally, we want the style score increase of \u03bb max to be as high as possible, indicating the system can span the range of classifier scores.\\n\\n3. Style Calibration to \u03bb (CALIB, C-IN): As defined in Section 5.6. We additionally also measure calibration by including the input sentence x in the CALIB calculation, treating it as the output for \u03bb = 0 (no style transfer). Here, calibration is averaged over a total of n= 6 (\u03bb1,\u03bb2) pairs. We call this metric C-IN.\\n\\n21This package is the Python port of Nakatani (2010).\"}"}
{"id": "acl-2022-long-514", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A detailed breakdown of performance by different metrics for every model is shown in Table 15.\\n\\nG Ablation Studies\\n\\nG.1 Ablation Study for DIFFUR\\n\\nThis section describes the ablation experiments conducted for the DIFFUR modeling choices in Section 4.2. We ablate a DIFFUR-INDIC model trained on Hindi paraphrase data only, and present results for Hindi formality transfer in Table 16.\\n\\n- no paraphrase: We replaced the paraphrase noise function with the random token dropping/replacing noise used in the denoising objective of UR model (Section 3), and continued to use vector differences. As seen in Table 16, this significantly increases the copy rate, which lowers the style transfer performance.\\n\\n- no paraphrase semantic filtering: We keep a setup identical to Section 4.2, but avoid the LaBSE filtering done (discarding pairs having a LaBSE score outside [0.7, 0.98]) to remove noisy paraphrases or exact copies. As seen in Table 16, this decreases the semantic similarity score of the generations, lowering the overall performance.\\n\\n- no vector differences: Instead of using vector differences for DIFFUR-INDIC, we simply set $s_{\\\\text{diff}} = f_{\\\\text{style}}(x)$, or the style of the target sentence. In Table 16, we see this significantly decreases SIM scores, and LANG scores for $\\\\lambda = 2$.0. We hypothesize that this training encourages the model to rely more heavily on the style vectors, ignoring the paraphrase input. This could happen since the style vectors are solely constructed from the output sentence itself, and semantic information/confounding style is not subtracted out. In other words, the model is behaving more like an autoencoder (through the style vector) instead of a denoising autoencoder with stylistic supervision.\\n\\n- mC4 instead of Samanantar: Instead of creating pseudo-parallel data with Samanantar, we leverage the mC4 dataset itself which was used to train the UR model. We backtranslate spans of text from the Hindi split of mC4 on-the-fly using the UR translation capabilities, and use it as the \\\"paraphrase noise function\\\". To ensure translation performance does not deteriorate during training, 50% mini-batches are supervised translation between Hindi and English. In Table 16, we see decent overall performance, but the LANG score is 6% lower than DIFFUR-INDIC. Qualitatively we found that the model often translates a few Hindi words to English while making text informal. Due to sparsity of English tokens, it often escapes penalization from LANG.\\n\\n- mC4 + exemplar instead of target: This setting is similar to the previous one, but in addition to the mC4 dataset we utilize the vector difference between the style vector of the exemplar span (instead of target span), and the \\\"paraphrase noised\\\" input. Results in Table 16 show this method is not effective, and it's important for the vector difference to model the precise transformation needed.\\n\\nG.2 Choice of Decoding Scheme\\n\\nWe experiment with five decoding schemes on the Hindi formality validation set \u2014 beam search with beam size 1, 4 and top-\\\\(p\\\\) sampling (Holtzman et al., 2020) with \\\\(p = 0.6, 0.75, 0.9\\\\). In Table 17, we present results at a constant style transfer magnitude (\\\\(\\\\lambda = 3.0\\\\)). Consistent with Krishna et al. (2020), we find that top-\\\\(p\\\\) decoding usually gets higher style accuracy (r-ACC, a-ACC) and output diversity (1-g, COPY) scores, but lower semantic similarity (SIM) scores. Overall beam search triumphs since the loss in semantic similarity leads to a worse performing model. In Figure 10, we see a consistent trend across different magnitudes of style transfer (\\\\(\\\\lambda\\\\)). In all our main experiments, we use beam search with beam size 4 to obtain our generations.\\n\\nG.3 Number of Training Steps\\n\\nIn Figure 11, we present the variation in style transfer performance with number of training steps for our best model, the DIFFUR-MLT model. We find that with more training steps performance generally improves, but improvements saturate after 8k steps. We also see the peak of the graphs (best style transfer performance) shift rightwards, indicating a preference for higher \\\\(\\\\lambda\\\\) values.\\n\\nH Analysis Experiments\\n\\nH.1 Style vectors from $f_{\\\\text{style}}$ as style classifiers\\n\\nThe Universal Rewriter models succeed in learning an effective style space, useful for few-shot style transfer. But can this metric space also act as a\"}"}
{"id": "acl-2022-long-514", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 13: style vector as a classifier, measuring the cosine similarity with informal exemplar vectors.\\n\\nTo explore this, we measure the cosine distance between the mean style vector of our informal exemplars and the style vectors derived by passing human-annotated formal/informal pairs (from our dataset of Section 5.1) through $f_{\\\\text{style}}$. We only consider pairs which had complete agreement among annotators. In Table 13 we see good agreement (68.2%-80.7%) between human annotations and the classifier derived from the metric space of the UR-INDIC model. Agreement is lower (67.0%-74.3%) for the DIFFUR-INDIC model, likely due to the stop gradient used in Section 4.2. With DIFFUR-MLT, agreement jumps back up to 75%-81.7% since gradients flow into the style extractor as well.\\n\\n### H.2 Style Vector Analysis with Formal Exemplars\\n\\nIn Appendix H.1, we saw that the metric vector space derived from the style encoder $f_{\\\\text{style}}$ of various models is an effective style classifier, using the informal exemplar vectors. In Table 14, we present a corresponding analysis using formal exemplar vectors. Most accuracy scores are close to 50%, implying this setup is not a very effective style classifier.\\n\\nTable 14: style vector as a classifier, measuring the cosine similarity with formal exemplar vectors.\\n\\nSee Appendix D for the exemplar sentences. We found the informal exemplars more effective than formal exemplars for style classification; Appendix H.2 has a comparison.\"}"}
{"id": "acl-2022-long-514", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"They ignored the court orders.\\n\\nNarendra Modi is a Hindi speaking prime minister who has popularized Hindi across the world.\\n\\nThe police arrested 5 people in Delhi.\\n\\nHe/She is the most senior judge in the Bombay High Court.\\n\\nI've worked closely with them.\\n\\nHe/She plays an important role in the film industry.\\n\\nIt rained in several states.\\n\\nThere's no difference between Shiv Sena and the BJP.\\n\\nPrashant Kishore has started working for the 2019 Lok Sabha elections.\\n\\nAfter this, Indira Gandhi ordered an attack on the Golden Temple.\\n\\nNiranjan is misled by a dancer, Mallika & Amirchand, who are after his wealth.\"}"}
{"id": "acl-2022-long-514", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 9: Lexical overlap between paraphrases used in our DIFFUR training strategy for six different languages (Hindi, Bengali, Kannada, Telugu, Swahili and Spanish). The wide spread of the histogram and sufficient percentage of low overlap pairs confirm the lexical diversity of the paraphrases used. The lexical overlap is measured using the unigram F1 score, using the implementation from the SQuAD evaluation script (Rajpurkar et al., 2016).\"}"}
{"id": "acl-2022-long-514", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 15: Evaluation of extent to which the magnitude of Hindi formality transfer can be controlled with $\\\\lambda$. We find that $\\\\text{DIFFUR-INDIC}$, $\\\\text{DIFFUR-MLT}$ are best at calibrating style change to input $\\\\lambda$ ($\\\\text{CALIB C -IN}$), giving the higher style score increase ($\\\\text{INCR}$) at $\\\\lambda = \\\\lambda_{\\\\text{max}}$ (details of evaluation setup and metrics in Section 5.6, Appendix F).\\n\\nTable 16: Ablation study on Hindi formality transfer validation set using beam size of 4 and $\\\\lambda = 2.0$ unless the optimal hyperparameters were different (marked by **). As shown by the overall a-$\\\\text{AGG}$ scores, removing any component of our design leads to an overall performance drop, sometimes significantly. For a detailed description of analysis and results, see Appendix G.1. For detailed metric descriptions, see Section 5.\\n\\nTable 17: Automatic evaluation of different decoding algorithms (top-$p$ sampling and beam search) on the $\\\\text{DIFFUR-MLT}$ model for Hindi formality transfer (validation set) using $\\\\lambda = 3.0$. As expected, output diversity (1-g, COPY) and style accuracy ($\\\\text{r-ACC}$, a-$\\\\text{ACC}$) improves as we move down the table, but compromise semantic preservation (SIM), bringing the overall performance ($\\\\text{r-AGG}$, a-$\\\\text{AGG}$) down. Also see Figure 10 for a comparison across $\\\\lambda$ values, and Section 5 for detailed metric descriptions.\\n\\nTable 18: Test set performance across languages for a smaller LaBSE semantic similarity threshold of 0.65. Due to the more relaxed threshold, absolute numbers compared to Table 1 are higher. Trends remain similar, with the DIFFUR and INDIC variants outperforming other competing methods.\"}"}
{"id": "acl-2022-long-514", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Variation in Hindi formality transfer (validation set) performance vs $\\\\lambda$ with change in decoding scheme, for the DIFFUR-MLT model. The plots show overall style transfer performance, using the r-AGG (left) and a-AGG (right) metrics from Section 5.5. Beam search with beam size 4 performs best, see Table 17 for an individual metric breakdown while keeping $\\\\lambda = 3$.\\n\\nFigure 11: Variation in Hindi formality transfer validation set performance with change in number of training steps for the DIFFUR-MLT model. The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5. With more training steps performance seems to improve and the peak of the graph shifts towards the right (a preference towards higher scale values). We also see more training steps leads to better controllability (bottom plot, closer to Y-axis is better), but only marginal gains after 6k steps.\"}"}
{"id": "acl-2022-long-514", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 19: Test set performance across languages for a larger LaBSE semantic similarity threshold of 0.85. Due to the stricter threshold, absolute numbers compared to Table 1 are lower, however trends are similar, with the \\\\textit{DIFFUR} and \\\\textit{INDIC} variants outperforming other competing methods.\\n\\n| Model | \u03bb | COPY | \u2193 | 1-g | \u2193 | LANG SIM | r-ACC | a-ACC | r-AGG | a-AGG |\\n|-------|---|------|---|-----|---|----------|-------|-------|-------|-------|\\n| UR (Garcia et al., 2021) | 1.5 | 45.4 | 77.5 | 98.0 | 84.8 | 45.8 | 22.9 | 30.4 | 10.4 |\\n| UR - INDIC | 1.0 | 1.0 | 70.7 | 95.0 | 93.8 | 67.2 | 23.3 | 58.3 | 18.6 |\\n| UR + BT | 0.5 | 0.5 | 44.2 | 92.9 | 85.2 | 72.3 | 27.8 | 54.2 | 17.8 |\\n| UR - INDIC + BT | 1.0 | 1.0 | 49.5 | 95.9 | 85.1 | 76.3 | 33.1 | 60.0 | 22.2 |\\n| DIFFUR | 1.0 | 4.7 | 61.6 | 97.7 | 89.7 | 82.4 | 31.0 | 71.1 | 22.9 |\\n| DIFFUR - INDIC | 1.5 | 5.3 | 63.7 | 98.0 | 91.9 | 81.6 | 30.5 | 72.5 | 23.7 |\\n| DIFFUR - MLT | 2.0 | 3.4 | 57.5 | 98.3 | 84.8 | 86.4 | 36.8 | 70.6 | 24.0 |\\n| DIFFUR - MLT | 2.5 | 4.4 | 61.9 | 97.2 | 89.7 | 89.7 | 34.0 | 78.1 | 27.5 |\\n| DIFFUR - MLT | 3.0 | 2.0 | 52.5 | 95.9 | 72.1 | 94.1 | 51.9 | 64.8 | 32.2 |\\n\\nTable 20: Performance breakdown of Hindi formality transfer by individual metrics described in Section 5.\\n\\nFigure 12: Variation in Hindi formality transfer test set performance & control for different models (see Table 20 for a individual metric breakdown of the models at the best performing $\\\\lambda$). The plots show overall style transfer performance, using the $r$-AGG (top-left) and $a$-AGG (top-right) metrics from Section 5.5. We see the \\\\textit{DIFFUR} models outperform other systems across the $\\\\lambda$ range, and get best performance with the \\\\textit{DIFFUR-MLT} variant. We also see that \\\\textit{DIFFUR} models, especially with \\\\textit{DIFFUR-MLT}, lead to better style transfer control (bottom plot, closer to $x=1$ is better), giving large style variation with $\\\\lambda$ without loss in semantics (X-axis).\"}"}
{"id": "acl-2022-long-514", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"\\\\[ \\\\text{Model} \\\\lambda \\\\ \\\\text{COPY} \\\\ (\\\\downarrow) \\\\ 1-g (\\\\downarrow) \\\\ \\\\text{LANG} \\\\ \\\\text{SIM} \\\\ r-\\\\text{ACC} \\\\ a-\\\\text{ACC} \\\\ r-\\\\text{AGG} \\\\ a-\\\\text{AGG} \\\\]\\n\\nUR (Garcia et al., 2021) 1.5 21.5 69.1 99.9 87.3 42.4 15.6 30.4 7.2\\n\\nUR-Indic 1.0 4.4 58.9 99.0 95.7 69.8 19.5 65.5 17.3\\n\\nUR + BT 1.5 2.4 47.5 97.6 79.8 80.0 37.4 59.6 22.3\\n\\nUR-Indic + BT 1.0 0.4 34.9 99.8 80.6 78.3 31.4 61.1 22.0\\n\\nDiffUR 1.0 2.1 50.6 99.9 91.6 80.8 25.2 72.7 20.9\\n\\nDiffUR-Indic 1.5 1.1 40.6 99.9 75.8 89.1 39.7 65.8 25.2\\n\\nDiffUR-MLT 2.5 0.9 41.4 99.9 75.6 86.1 29.6 53.5 16.9\\n\\nDiffUR-Indic + BT 1.0 0.4 34.9 99.8 80.6 78.3 31.4 61.1 22.0\\n\\nTable 21: Performance breakdown of Bengali formality transfer by individual metrics described in Section 5.\\n\\nFigure 13: Variation in Bengali formality transfer test set performance & control for different models (see Table 21 for a individual metric breakdown of the models at the best performing \\\\( \\\\lambda \\\\)). The plots show overall style transfer performance, using the \\\\( r-\\\\text{AGG} \\\\) (top-left) and \\\\( a-\\\\text{AGG} \\\\) (top-right) metrics from Section 5.5. We see the DiffUR models outperform other systems across the \\\\( \\\\lambda \\\\) range, and get best performance with the DiffUR-MLT variant. We also see that DiffUR models, especially with DiffUR-MLT, lead to better style transfer control (bottom plot, closer to \\\\( x = 1 \\\\) is better), giving large style variation with \\\\( \\\\lambda \\\\) without loss in semantics (X-axis).\"}"}
{"id": "acl-2022-long-514", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Table 22: Performance breakdown of Kannada formality transfer by individual metrics described in Section 5.\\n\\n| Transfer Amount | R-AGG | A-AGG |\\n|-----------------|-------|-------|\\n| UR (Garcia et al., 2021) | 1.5 | 52.0 | 86.8 |\\n| UR-Indic | 1.0 | 8.6 | 62.9 |\\n| UR + BT | 0.5 | 0.3 | 26.0 |\\n| UR-Indic + BT | 1.0 | 1.6 | 40.6 |\\n| DiffUR | 1.0 | 3.0 | 47.4 |\\n| DiffUR-Indic | 1.5 | 2.9 | 50.3 |\\n| DiffUR-MLT | 2.0 | 5.4 | 59.6 |\\n| DiffUR-Indic | 2.0 | 2.3 | 45.2 |\\n\\n### Figure 14: Variation in Kannada formality transfer test set performance & control for different models (see Table 22 for a individual metric breakdown of the models at the best performing \u03bb). The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5. We see the DIFFUR models outperform other systems across the \u03bb range, and get best performance with the DIFFUR-MLT variant. We also see that DIFFUR models, especially with DIFFUR-MLT, lead to better style transfer control (bottom plot, closer to x = 1 is better), giving large style variation with \u03bb without loss in semantics (X-axis).\"}"}
{"id": "acl-2022-long-514", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | \u03bb   | Copy | 1-g | Lang Sim | R-ACC | A-ACC | R-AGG | A-AGG |\\n|---------------|-----|------|-----|----------|-------|-------|-------|-------|\\n| UR (2021)     | 1.5 |      |     |          | 51.3  | 87.0  | 100   | 96.3  |\\n|               | 2.0 |      |     |          | 35.0  | 68.2  | 99.9  | 73.0  |\\n|               | 0.5 |      |     |          | 26.3  | 10.1  | 22.8  | 7.5   |\\n|               | 1.0 |      |     |          | 35.0  | 68.2  | 99.9  | 73.0  |\\n| UR-Indic      | 1.0 |      |     |          | 10.4  | 64.5  | 98.8  | 94.3  |\\n|               | 1.5 |      |     |          | 5.9   | 53.5  | 97.3  | 80.0  |\\n| UR + BT       | 0.5 |      |     |          | 0.2   | 26.3  | 82.4  | 73.4  |\\n|               | 1.0 |      |     |          | 0.1   | 19.8  | 74.9  | 64.7  |\\n| UR-Indic + BT | 0.5 |      |     |          | 0.6   | 39.2  | 99.9  | 79.6  |\\n|               | 1.0 |      |     |          | 0.5   | 36.1  | 99.7  | 74.0  |\\n| DiffUR        | 1.0 |      |     |          | 1.7   | 46.0  | 99.9  | 87.9  |\\n|               | 2.5 |      |     |          | 0.9   | 36.0  | 99.8  | 68.4  |\\n| DiffUR-Indic  | 1.0 |      |     |          | 2.4   | 50.1  | 99.9  | 91.7  |\\n|               | 1.5 |      |     |          | 1.4   | 44.6  | 99.9  | 83.6  |\\n| DiffUR-MLT    | 2.0 |      |     |          | 3.8   | 55.8  | 99.9  | 95.7  |\\n|               | 2.5 |      |     |          | 1.8   | 47.0  | 99.5  | 85.8  |\\n\\nTable 23: Performance breakdown of Telugu formality transfer by individual metrics described in Section 5.\"}"}
{"id": "acl-2022-long-514", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Transfer Amount (\u03bb) | UR (Garcia 2021) | UR-Indic | UR + BT | UR-Indic + BT | DiffUR | DiffUR-Indic | DiffUR-MLT |\\n|---------------------|------------------|----------|--------|--------------|--------|-------------|-----------|\\n| 0.5                 | 0.5              | 0.5      | 0.3    | 0.5          | 0.5    | 0.3         | 0.5       |\\n| 1.0                 | 0.75             | 0.75     | 0.80   | 0.75         | 0.75   | 0.80        | 0.75      |\\n| 1.5                 | 0.85             | 0.85     | 0.90   | 0.85         | 0.85   | 0.90        | 0.85      |\\n| 2.0                 | 0.90             | 0.90     | 0.95   | 0.90         | 0.90   | 0.95        | 0.90      |\\n| 2.5                 | 0.95             | 0.95     | 1.00   | 0.95         | 0.95   | 1.00        | 0.95      |\\n\\n**Figure 16:** Variation in Gujarati formality transfer test set performance & control for different models (see Table 24 for individual metric breakdown of the models at the best performing \u03bb). The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5. Note that Gujarati is a zero-shot language for DIFFUR models \u2014 no Gujarati paraphrase data was seen during training. We see that while the vanilla DIFFUR model performs poorly, the DIFFUR-INDIC is competitive with baselines and the DIFFUR-MLT variant significantly outperforms other systems. We also see that the DIFFUR-MLT variant lead to better style transfer control (bottom plot, closer to x = 1 is better), giving style variation with \u03bb without loss in semantics (X-axis).\"}"}
{"id": "acl-2022-long-514", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 17: Our crowdsourcing interface on Task Mate, used to build our formality evaluation datasets (Section 5.1) and conduct human evaluations (Section 5.7). The first row shows our landing page and instruction set derived from our conversations with professional linguists. The second row shows our qualification questions for formality classification, and the third row shows templates for the two questions asked to crowdworkers per pair.\"}"}
