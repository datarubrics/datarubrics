{"id": "lrec-2024-main-5", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstractive Multi-Video Captioning: Benchmark Dataset Construction and Extensive Evaluation\\n\\nRikito Takahashi, Hirokazu Kiyomaru, Chenhui Chu* and Sadao Kurohashi\\nKyoto University\\n{r-takahashi, kiyomaru, chu, kuro}@nlp.ist.i.kyoto-u.ac.jp\\n\\nAbstract\\nThis paper introduces a new task, abstractive multi-video captioning, which focuses on abstracting multiple videos with natural language. Unlike conventional video captioning tasks generating a specific caption for a video, our task generates an abstract caption of the shared content in a video group containing multiple videos. To address our task, models must learn to understand each video in detail and have strong abstraction abilities to find commonalities among videos.\\n\\nWe construct a benchmark dataset for abstractive multi-video captioning named AbstrActs. AbstrActs contains 13.5k video groups and corresponding abstract captions. AbstrActs is available at https://github.com/ku-nlp/AbstrActs. As abstractive multi-video captioning models, we explore two approaches: end-to-end and cascade. For evaluation, we proposed a new metric, CocoA, which can evaluate the model performance based on the abstractness of the generated captions. In experiments, we report the impact of the way of combining multiple video features, the overall model architecture, and the number of input videos.\\n\\nKeywords: video captioning, multi-video, abstraction\\n\\n1. Introduction\\nVideo captioning has attracted much attention as a fundamental task in vision-and-language research (Li et al., 2019; Aafaq et al., 2019b; Krishna et al., 2017). In the standard setting, models are presented with a video and generate a sentence that describes its content (Venugopalan et al., 2015b). In another popular setting, called dense videocaptioning (Krishna et al., 2017), models extract clips from the video, each representing a distinct event, and generate a caption for each clip. In both settings, the focus is on describing a given single video in detail.\\n\\nIn this paper, we shed light on another important aspect of video comprehension: abstractive video understanding. To elaborate, we examine the two videos in Fig. 1. We can, for example, describe the left video as \\\"adults in sportswear are dancing in front of a mirror\\\" and the right video as \\\"elementary school girls are dancing in a gym.\\\" However, we can also abstractly comprehend both videos and collectively describe them as \\\"a group of people is dancing in a gym.\\\" Such abstractive understanding is the key to identifying commonalities among videos.\\n\\nAbstractive video understanding can help analyze large amounts of video data. One effective way to analyze large amounts of video data is video clustering (Jain, 2010). Auto-labeling to video clusters is a direct application of abstractive video understanding. Auto-labeling is necessary because it is difficult to know the common content of videos in each cluster and the tendency of a large number of clusters. Abstractive video understanding can identify and describe the shared information in videos, i.e., it can generate a label from a video cluster, which can promote video data analysis.\\n\\nUnfortunately, abstractive video understanding is difficult to learn through conventional video captioning. This is because conventional video captioning focuses on providing diverse and concrete descriptions of a single video (Aafaq et al., 2019b; Li et al., 2019). Therefore, in order to learn to abstract video content appropriately, we need to consider a task focusing on information abstraction.\\n\\nIn this paper, we propose a new task, abstractive multi-video captioning, for abstractive video understanding. This task requires that models describe information shared by multiple videos as much as possible. Although generating captions for each video with conventional models and then summarizing them can be another way, we show that abstractive video captioning performs significantly better.\"}"}
{"id": "lrec-2024-main-5", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to solve this task, models need not only to understand each video in detail but also to have strong abstraction abilities to find commonalities among videos. We construct AbstrActs, a dataset for abstractive multi-video captioning. AbstrActs consists of video groups, their corresponding abstract captions, abstractness scores of the abstract captions, and scores representing the degree of agreement between the video and the caption. We explore model variants for the task and evaluate their performance on AbstrActs. Specifically, we investigate the impact of the way of combining multiple video features, the overall model architecture (end-to-end and cascade), and the number of input videos.\\n\\nFor evaluation, we propose a new metric, CocoA, which evaluates the model performance based on the correlation coefficient of abstractness scores between abstract captions and generated captions.\\n\\n2. Related Work\\n\\nWe discuss video captioning datasets, video captioning models, and multi-image vision-and-language tasks.\\n\\n2.1. Video Captioning Datasets\\n\\nOver the past few years, several datasets have been constructed for video captioning. The most widely-used datasets include HowTo100M (Miech et al., 2019), VATEX (Wang et al., 2019), MSR-VTT (Xu et al., 2016), and MSVD (Chen and Dolan, 2011). HowTo100M is by far the largest video-caption dataset, containing $136,000$ video-caption pairs and $23,000$ types of actions. HowTo100M is used for pretraining vision-and-language models. VATEX, which is the source data for AbstrActs presented in this paper, is characterized by its large number of captions. VATEX has ten English and ten Chinese captions for each video. VATEX has $41,250$ videos and $412,500$ English captions in total. ActivityNet Captions (Krishna et al., 2017) and YouCook2 (Zhou et al., 2018) are datasets for dense video captioning with multiple captions corresponding to video clips, each of which corresponds to a single event happening in the video. YouCook2 is a dataset with captions for long, unsegmented videos restricted to the cooking domain. YouCook2 contains $2,000$ videos describing $89$ different cooking recipes for $176$ hours.\\n\\n2.2. Video Captioning Models\\n\\nVideo captioning models are given a video and generate the caption. First, video captioning models extract video features using pretrained video encoders, typically constructed as either a CNN-based model (Tran et al., 2015; Carreira and Zisserman, 2017; Xie et al., 2018) or Transformer-based model (Dosovitskiy et al., 2021; Arnabet al., 2021; Luo et al., 2021; Liu et al., 2022; Bertasius et al., 2021). Most previous studies rely on CNN-based video encoders (Wang et al., 2018; Aafaq et al., 2019a; Zhang and Peng, 2019), but Transformer-based models are becoming popular in recent studies (Luo et al., 2020; Tang et al., 2021; Lin et al., 2022). Then, extracted video features are processed to generate captions. To this end, while early studies rely on LSTM-based models (Gao et al., 2017; Pan et al., 2017; Yan et al., 2019), recent studies increasingly use Transformer-based models (Zhou et al., 2018; Wang et al., 2018).\\n\\nNote that the main focus of existing video captioning models, as well as datasets, is to make the specific content of a single video recognizable and to generate accurate captions. In this paper, we rather focus on understanding video information abstractly.\\n\\n2.3. Multi-image Vision-and-Language Tasks\\n\\nThere are a handful of studies on vision-and-language tasks considering multiple images. ISVQA (Bansal et al., 2020) is a task of visual question answering from multiple images. The model answers natural language questions given multiple images showing the same location from different views. Context-aware group captioning is a task of image group captioning (Li et al., 2020b). The model generates a caption that summarizes multiple target images in the context of another group of related reference images. Our task focuses on generating a caption from multiple videos and thus can be viewed as a temporal extension of previous studies on multi-image language generation tasks.\\n\\n3. Task Definition\\n\\nAbstractive multi-video captioning is a task to generate an abstract caption for multiple videos. The input is a video group $G$ with $n$ videos. The output is an abstract caption $y$ that describes the shared content of the video group $G$ as much as possible. Our goal is to learn an abstractive multi-video captioning model $p_\\\\theta(y | G)$ from training data, where $\\\\theta$ is the set of model parameters. Fig. 1 shows an example of abstractive video captioning. The caption \\\"A group of people is dancing in a gym.\\\" is a good abstract caption for the video group; the phrase \\\"a group of people\\\" appropriately abstracts \\\"adults in sportswear are dancing\\\" shown in the left video and \\\"elementary schoolers\\\" shown in the right one.\"}"}
{"id": "lrec-2024-main-5", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Sample of AbstrActs. AbstrActs is built from VATEX (Wang et al., 2019).\\n\\nWedefineawell-abstractedcaptionasasentence that explains the shared information of multiple videos as much as possible. Note that we do not allow over-abstraction. This is because highly abstract captions such as \u201cPeople are doing something.\u201d are not helpful. However, if we consider abstractive single-video captioning, it is difficult to determine if a caption is overly abstract because the clear criterion is hard to establish. This is why we consider abstractive multi-video captioning; we consider the minimum level of abstraction at which commonalities among given videos can be found to be the criterion for over-abstraction. Thus, in our task, the caption \u201cPeople are doing something.\u201d for the video group in Fig. 1 is regarded as an overly abstract caption as it violates the requirement that the shared content must be described as much as possible.\\n\\n4. Dataset Construction\\n\\nWe construct a new dataset for abstractive video captioning named AbstrActs. Fig. 2 shows an example in AbstrActs. An example consists of a video group, a human caption, abstractness, and TER scores. A video group is a collection of videos that have commonalities. A human caption is a manually assigned abstract caption for the video group. We use human captions as gold labels of abstractive multi-video captioning. Abstractness is a score that indicates the degree of abstraction of an abstract caption. A TER score indicates the content agreement between each video in the video group and the abstract caption, calculated using an off-the-shelf textual entailment recognition (TER) model. We use TER scores to evaluate the quality of abstract captions and filter out noisy data.\\n\\nAbstrActs is built from VATEX (Wang et al., 2019). First, we construct video groups by performing a similarity search on videos in VATEX. Then, we use crowdsourcing to assign human captions to the video groups. Next, we define and calculate the abstractness of each abstract caption. Finally, we calculate TER scores using a TER model.\\n\\n4.1. Video Group\\n\\nWe collect video groups, each of which contains videos with shared information, by performing video retrieval on the videos in VATEX. First, we extract video features of all videos using a pre-trained video encoder. In this study, we employed the CLIP4Clip (Luo et al., 2021) model. Then, we group videos by performing a k-nearest neighbor search on video features. We retrieve the top six similar videos for every video to form a group. Considering that the number of videos should be greater than the minimum problem set at two, we decided on six as the number of videos in the video group. We used Faiss (Johnson et al., 2019) to perform a k-nearest neighbor search.\\n\\n4.2. Abstract Caption\\n\\nWe use crowdsourcing to annotate video groups with abstract captions. In order to increase the reliability and uniformity of abstract captions, we instruct crowdworkers to keep the following rules: (1) Do not write captions that describe the content that does not appear in the videos. (2) Do not write captions that simply list the events in each video. (3) Do not write captions that sum up the number of people or objects for each video. The instruction part of the crowdsourcing interface is shown in Appendix A.\\n\\nTo prevent over-abstraction in abstract captions, we also instructed crowdworkers to write a well-abstracted caption that explains the shared information of multiple videos as much as possible. We assigned one annotator to each video group, and the annotators varied by video group. After repeating annotations with 50 video groups and instructions improvements, we confirmed that the annotated captions correctly describe the shared video content. Then, we started the annotation of the remaining video groups.\\n\\n4.3. Abstractness Score\\n\\nWe define the abstractness score for investigating the abstractness of abstract captions in AbstrActs. Each video in VATEX has ten captions annotated. The abstractness score is calculated between the abstract caption annotated by crowdworkers in our work and the original video captions in VATEX in a video group. Each video group only has one abstractness score.\\n\\nWe use WordNet (Miller, 1995) to calculate an abstractness score. WordNet is an lexical database of English. Words are grouped into sets of cognitive synonyms (synsets). Super-subordinate relations among synsets are organized as a tree structure. We use the distance on the tree for calculating an abstractness score.\"}"}
{"id": "lrec-2024-main-5", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The similarity between two words, $w_1$ and $w_2$, is defined as:\\n\\n$$\\\\text{similarity}(w_1, w_2) = 1 + \\\\text{path}(\\\\text{syn}(w_1), \\\\text{syn}(w_2))$$\\n\\nwhere $\\\\text{path}(\\\\text{syn}(w_1), \\\\text{syn}(w_2))$ is the number of edges of the shortest path between two synsets on the tree.\\n\\nWe calculate an abstractness score, $a$, by following the equations:\\n\\n$$a = 1 - f(W_a, V)$$\\n\\nwhere\\n\\n$$f(W_a, V) = \\\\frac{\\\\sum_{W_v \\\\in V} g(W_a, W_v)}{|V|}$$\\n\\nand\\n\\n$$g(W_a, W_v) = \\\\frac{\\\\sum_{w_v \\\\in W_v} h(W_a, w_v)}{|W_v|}$$\\n\\nand\\n\\n$$h(W_a, w_v) = \\\\max_{w_a \\\\in W_a} \\\\text{similarity}(w_a, w_v)$$\\n\\n$W_a$ is a set of nouns and verbs that appears in an abstract caption.\\n\\n$V$ is a set of captions for a video in a video group.\\n\\n$W_v$ is a set of nouns and verbs that appears in a caption of a video.\\n\\n$w_v$ is a word in $W_v$.\\n\\n$w_a$ is a word in $W_a$.\\n\\nThe abstractness score is a real number that ranges from 0 to 1. A higher abstractness score means a greater semantic difference between an abstract caption and a video caption.\\n\\n4.4. TER Score\\n\\nTextual entailment recognition (TER) is the task of determining the entailment relationship between two sentences. Given a premise sentence and a hypothesis sentence, models determine whether the hypothesis sentence is true (entailment), false (contradiction), or undetermined (neutral), supposing that the premise sentence is true (Storks et al., 2019).\\n\\nWe use TER to evaluate the quality of abstract captions. We suppose that if many of the original captions of a video are entailed by the abstract caption, the abstract caption should appropriately abstract the shared content and thus be of high quality. Such an automatic quality evaluation is helpful in filtering out noisy examples, which are inevitably included as we rely on crowdsourcing or heuristics for data collection.\\n\\nWe assign a TER score to each pair of a video and an abstract caption. We perform TER regarding each of the original captions as a premise and the abstract caption as a hypothesis. We regard the number of original captions that entail the abstract caption as the TER score and assign it to the pair.\\n\\nWe performed TER with SemBERT (Zhang et al., 2020a), a state-of-the-art TER model. We first excluded videos with a TER score of zero from a video group (making the number of videos of a video group possibly fewer than two), and then excluded video groups with fewer than two videos.\\n\\n| Training | Validation | Test |\\n|----------|------------|------|\\n| Videos   | 38,514     | 2,452| 5,157 |\\n| Unique Videos | 16,732     | 1,475| 2,848 |\\n| Videos per Group (avg.) | 3.5        | 3.0  | 3.1  |\\n\\nTable 1: The statistics of AbstrActs. Videos indicate the total number of videos in each video group, including duplicates. Unique videos indicate the number of unique videos in each split.\"}"}
{"id": "lrec-2024-main-5", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.1. Combination Methods for Multiple Features\\n\\nAs Transformer-based models take a single sequence of features as input, multiple video features must be processed into the form to feed them into the Transformer. We explore two methods for inputting multiple video features: concatenation and soft alignment.\\n\\n**Concatenation** is a naive implementation to combine multiple video features, which performs a frame-wise concatenation of video features. This method does not consider differences between the contents of different frames in each video.\\n\\n**Soft alignment** combines multiple video features by focusing on one video and collecting similar frames from the other videos, which shares the same idea as the attention mechanism (Bahdanau et al., 2014). Figure 5 shows an overview of soft alignment. The frame-wise similarity between multiple video features is calculated first, and then the similarity weights the video features before being combined. By applying frame-wise soft alignment to $n$ types of video features $V_1, V_2, ..., V_n$, one video features $V_{align}$ is obtained by the following equation:\\n\\n$$V_{align}(t) = \\\\text{concat}(V_1(t), V'_2(t), ..., V'_n(t))$$\\n\\n(6)\\n\\nwhere $V'_i = W_i \\\\cdot V_i$. (7)\\n\\nHere, $W_i$ is the similarity matrix for the video features $V_1$ and the video features $V_i$. Let $V_1 \\\\in \\\\mathbb{R}^{T_1 \\\\cdot M}$ and $V_i \\\\in \\\\mathbb{R}^{T_i \\\\cdot M}$, where $T_1$ and $T_i$ denote the number of frames in $V_1$ and $V_i$, respectively, and $M$ is feature dimension for each frame. Then $W_i$ is a matrix $\\\\in \\\\mathbb{R}^{T_1 \\\\cdot T_i}$. The similarity between each frame of the two videos is computed as follows:\\n\\n$$W_i(t_1, t_i) = \\\\frac{|V_1(t_1) \\\\cdot V_T(i, t_i)|}{||V_1(t_1)|| \\\\cdot ||V_i(t_i)||}$$\\n\\n(8)\\n\\nThe feature $V_{align}(t)$ at a frame $t$ is the result of collecting features similar to the feature $V_1(t)$ from other video features $V_i$ and then combining them by weighting by similarity. The sequence length $l$ of the features $V_{align}$ is equal to that of the video features $V_1$.\\n\\nThe dimensions of the soft alignment feature increase in proportion to the number of input videos. For instance, the dimensionality of video features when using six videos as input is three times larger than when using two videos as input. When using more than three videos as input, we change the dimension of the input layer of the model corresponding to the dimensionality of the input video features, and we padded zero to the portion of the features corresponding to the missing videos during soft alignment for video groups including fewer videos.\\n\\n5.2. End-to-End Model\\n\\nThe end-to-end model learns to directly generate an abstract caption from an input video group. The end-to-end model has two advantages compared to the cascade model described next. First, it is free from error propagation. Second, the model can take full advantage of the information in the given videos.\\n\\nFigure 6 shows an overview of the end-to-end model. First, $n$ input video features are obtained by a pretrained video encoder. Then, the sequence of features $V_{multi}$ is obtained using either the concatenation method or the soft alignment method described in Sec. 5.1. The transformed features $V_{multi}$ is input to the Transformer-based video encoder to obtain the sequence of features $z = f_{enc}(V_{multi}) = (z_1, z_2, ..., z_l)$. The $l$ is the length of the combined features $V_{multi}$.\\n\\nFinally, the abstract caption $y$ is generated using a Transformer-based language decoder. The word $y_t = f_{dec}(y, z)$ at decoding step $t$ is generated based on the previously generated word sequence $y = (y_1, y_2, ..., y_{t-1})$ and the feature sequence $z$.\\n\\n5.3. Cascade Model\\n\\nThe cascade model combines a single-video captioning module and a multi-sentence abstraction module. The single-video captioning module takes a single video as input and generates the\"}"}
{"id": "lrec-2024-main-5", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Two models for abstractive captioning. The end-to-end model directly generates an abstract caption from multiple videos. In the cascade model, the single-video captioning module first generates concrete captions for each video, and then the multi-sentence abstraction module generates an abstract caption from them.\\n\\nCaption. The multi-sentence abstraction module takes multiple captions as input and generates the abstract caption.\\n\\nFig. 6b shows an overview of the cascade model. First, the single-video captioning module generates a caption for each video using the Transformer model. The resulting video captions are then encoded into sequences of word embeddings with a pretrained word embedding model. The resulting multiple caption features are converted into a single sequence of features using a soft alignment method similar to the one presented in Sec. 5.1. The difference is that instead of conducting soft alignment on video features, we do it on word embeddings of multiple captions. Finally, the transformed features are input to the Transformer to generate the abstract caption.\\n\\nThe advantage of the cascade model is its reusability. The performance of the cascade model can be improved by replacing the single-video captioning module with pretrained models. The disadvantage is that there is a possibility of error propagation due to the nature of solving each subtask independently and sequentially. If single-video captioning produces poor captions, the abstract caption produced by the multi-sentence abstraction module will also be of poor quality. Even if the performance of the single-video captioning module is adequate, there is a problem with missing information due to the conversion of video information into text. Spatio-temporal information in a video is difficult to explain entirely with text. Therefore, the caption generated by the single-video captioning module can lose some shared information in multiple videos, restricting the multi-sentence abstraction module from generating an abstract caption that describes the shared information as much as possible.\\n\\n5.4. Cascade (Gold) Model\\nWe consider the cascade (gold) model as the upper-bound setting of the cascade model. This model does not use the single-video captioning module in the cascade model; instead, this model feeds the gold captions of each video to the multi-sentence abstraction module. As the gold caption should fully describe the content of the video, the gold caption can be regarded as a caption generated by a single-video captioning module with perfect performance. In both training and inference of the cascade (gold) model, the input to the model is the gold captions of multiple videos.\\n\\n6. Experiments\\nWe conduct experiments on abstractive captioning using the models described in Sec. 5.\\n\\n6.1. CocoA: Correlation Coefficient of Abstractness Scores\\nBLEU-4 (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), and ROUGE-L (Lin, 2004) are de facto standards in existing video captioning tasks. However, abstractive captioning is a task to generate abstract captions, whose purpose is different from existing video captioning tasks. Therefore, the criteria for a good caption differs between the abstractive multi-video captioning task and the existing video captioning task. For example, in the case of abstractive multi-videocaptioning, not only the word agreement between the correct label and the generated caption but also the abstractness of each word can be a criterion for evaluation. In addition, abstract captions generated by abstractive multi-video captioning tend to have fewer words than single-video captioning. This short caption is incompatible with the evaluation criteria that use n-grams in the sentence.\\n\\nMotivated by the above, we propose a new metric, CocoA, which can evaluate model performance based on the abstractness score mentioned in Sec. 4.3. The value of CocoA is the Pearson correlation coefficient among the abstractness scores of generated captions \\\\( A_Y \\\\) and that of gold captions \\\\( A_Y' \\\\).\\n\\n\\\\[\\n\\\\text{CocoA}(A_Y, A_Y') = \\\\frac{\\\\text{cov}(A_Y, A_Y')}{\\\\sigma_{A_Y} \\\\sigma_{A_Y'}}\\n\\\\]\\n\\nwhere \\\\( \\\\text{cov} \\\\) is the covariance. \\\\( \\\\sigma_{A_Y} \\\\) and \\\\( \\\\sigma_{A_Y'} \\\\) are the standard deviations of \\\\( A_Y \\\\) and \\\\( A_Y' \\\\). We hypothesize that a strong correlation exists between abstractness scores when a model correctly generates an abstract caption.\"}"}
{"id": "lrec-2024-main-5", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Performance comparison of different feature combination methods with the end-to-end model. \\\"Concat\\\" and \\\"Soft\\\" denote for \\\"Concatenation\\\" and \\\"Soft Alignment,\\\" respectively.\\n\\n| Method   | CocoA | BLEU | CIDEr | METEOR | ROUGE-L |\\n|----------|-------|------|-------|--------|---------|\\n| Concat   | 0.27  | 16.0 | 84.5  | 18.2   | 43.6    |\\n| Soft     | 0.34  | 18.6 | 130.1 | 21.5   | 47.3    |\\n\\nTable 3: Performance comparison of different model structures. The T5 model is for reference. \\\"E2E,\\\" \\\"Cas,\\\" and \\\"Cas (G)\\\" denote for \\\"End-to-End,\\\" \\\"Cascade,\\\" and \\\"Cascade (Gold),\\\" respectively.\\n\\n| Model     | CocoA | BLEU | CIDEr | METEOR | ROUGE-L |\\n|-----------|-------|------|-------|--------|---------|\\n| E2E       | 0.34  | 18.6 | 130.1 | 21.5   | 47.3    |\\n| Cas       | 0.25  | 14.8 | 65.4  | 17.4   | 40.9    |\\n| Cas (G)   | 0.33  | 17.5 | 103.5 | 19.8   | 44.1    |\\n| T5        | 0.50  | 20.3 | 154.6 | 23.2   | 49.1    |\\n\\n6.3. Feature Combination Methods\\n\\nWe compared concatenation and soft alignment, the two methods for combining multiple features into a single feature described in Sec. 5.1. We conducted experiments with the end-to-end model described in Sec. 5.2 comparing two types of multi-input methods. For extracting video features, we used the CLIP4Clip model. Tab. 2 shows the results. The soft-aligned method consistently outperformed the naive concatenation method in all the metrics regardless of the video encoder. Soft alignment is different from the naive concatenation method in that soft alignment considers the similarity of the contents between the different frames of the videos. This difference helps the models find the shared content in multiple videos. We conclude from the experiments that it is better to use the soft-aligned video features as input to the end-to-end model, and thus, we adopt this setting for the subsequent experiments.\\n\\n6.4. Model Architectures\\n\\nWe compared abstractive multi-video captioning with the end-to-end and cascade models. For extracting video features, we used the CLIP4Clip model. We used a pretrained word embedding model from fastText (Bojanowski et al., 2017) for feature extraction of the caption obtained from the single-video captioning module. Training of the cascade model was performed independently for each module. For training the single-video captioning module, we used VATEX. For training the multi-sentence abstraction module, we used both AbstrActs and VATEX. Each video in AbstrActs contains up to six videos, we fixed the number of videos to use to two except in Sec. 6.5 to simplify the experiment settings. We used two videos in each video group with the highest similarity scores in video retrieval when composing the video group. We used VATEX for training the cascade models and for inference in the cascade (gold) models. For evaluation metrics, we used CocoA together with BLEU-4, CIDEr, METEOR, and ROUGE-L.\\n\\nTable 3 shows the result. The end-to-end model outperformed the cascade and cascade (gold) models in all evaluation metrics. For reference, we also provide the score of the T5 (Raffel et al., 2020) model trained to abstract the gold captions of each video in a video group. Note that we concatenate gold captions and input them into the T5 model. One reason for the better performance of the end-to-end model is that it does not have the error propagation problem that can occur in the cascade model. We manually investigated 50 inference results on the test set. We found that in 7 cases, the cascade model generated a poor abstract caption due to the generation error in the single-captioning module. Fig. 7a shows an example that the end-to-end model describes the children in the two videos, while the cascade model describes it as a \\\"person.\\\" Fig. 7b shows an example of the generation of the cascade model. A child is in the right video, but the single-video captioning module describes him in a more abstract word \\\"man.\\\" This generation error propagates to the multi-sentence abstraction module, which generates the overly abstract word \\\"person\\\" instead of the expected words such as \\\"child\\\" or \\\"kid.\\\" The end-to-end model uses the video features directly, eliminating the risk of error propagation and producing the preferred word \\\"kid.\\\"\\n\\nWe investigated the generated captions related to their abstractness scores. Samples with different abstractness scores may also have different difficulties in abstraction. Fig. 8a shows the example with a low abstractness score. The two videos have almost the same content: a person is swimming in a pool. In this example, models correctly generated an abstract caption from the videos. Fig. 8b shows the example with a high abstractness score. The left video shows a male hairstylist combing and cutting a woman's hair, and the right video shows a female hairstylist washing and cutting a man's hair. The shared information in the two videos is that a hairstylist works on a customer. Models generated captions with improper abstraction from these videos. It would be interesting to evaluate and investigate the generated...\"}"}
{"id": "lrec-2024-main-5", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"## Videos CocoA BLEU CIDEr METEOR ROUGE-L\\n\\n| Number of Videos | BLEU | CIDEr | METEOR | ROUGE-L |\\n|------------------|------|-------|--------|---------|\\n| One              | 0.32 | 18.1  | 119.2  | 21.1    |\\n| Two              | 0.34 | 18.6  | 130.1  | 21.5    |\\n| Six              | 0.32 | 17.6  | 120.8  | 20.9    |\\n\\n### Table 4: Performance comparisons of different numbers of video inputs for the end-to-end model.\\n\\nWe further investigated the impact of the number of input videos. In previous experiments, two videos were fed to the models. We compared the number of input videos with one, two, and six, using the end-to-end model as it consistently outperformed the cascade model. For the one video setting, we randomly selected one video from a video group.\\n\\nTable 4 shows the result. We can see that using one video performs worse than using two videos, indicating the importance of understanding multiple videos for the proposed task. Using the input features from six videos did not have a positive impact; in all evaluation metrics, the two-video setting outperformed the six-video setting. One possible reason is that inputting many videos into the model degrades each video's information. We fixed the number of parameters in the Transformer of the end-to-end model. Even if the number of input videos increases to six, the dimension of features processed by the Transformer's attention mechanism remains the same. As the number of input videos increases, the dimensions of the feature values used per video decrease. In the model where six videos are input, the information for each video is compressed, leading to performance degradation. Another possible reason for performance degradation is zero-padding in the soft alignment. We used zero-padding to shape the features when performing soft alignment in a video group with fewer than six videos. For example, if only two videos are in a video group, all the values in the matrix for the remaining four videos are padded with zeros. This zero-padding maybe come noise and prevent the model from learning. Inference examples with different number of input videos can be found in Appendix B.\\n\\n### Conclusion\\n\\nWe introduced a new task, abstractive multi-video captioning, aiming at training models to find and describe commonalities among videos. We constructed the benchmark dataset AbstrActs, containing 13,5k video groups, 27k abstract captions, abstractness scores, and scores representing the degree of agreement between the video and the...\"}"}
{"id": "lrec-2024-main-5", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"abstract caption. We proposed a new metric named CocoA, which evaluates the model performance in terms of the abstractness of captions. We extensively explored model variants to see what improvements can be effective for abstract caption generation on AbstrActs. We hope the dataset, the metric, and insights into models presented in this study facilitate future research on abstractive multi-video captioning. In the future, we plan to investigate the applicability of the models for auto-labeling to video clusters.\\n\\n8. Limitations\\n\\nActions in AbstrActs. In order to simplify the problem setup, we built AbstrActs from VATEX (Wang et al., 2019), in which each video includes just one representative action. However, a video usually includes multiple actions and events. To extend the present work, we need to expand the dataset domain with videos that contain multiple actions.\\n\\nConstructing video groups. When collecting video groups by video retrieval, we used the video features extracted by CLIP4Clip (Luo et al., 2021). This may lead to some bias if we use other video encoders such as (Bain et al., 2021; Li et al., 2020a) in experiments because the similarity of videos depends on the video encoders being used for extracting video features. Our future work is evaluating the effect of the biases and considering another way of collecting video groups.\\n\\nApplicability of the models. In this paper, we did not address the applicability of the models for auto-labeling to video clusters. To simplify the experiment settings, we fixed the number of videos to two except in Sec. 6.5. Besides, the result in Sec. 6.5 indicates that using six videos as input did not positively impact. Given the above, it is worth considering if we can apply models to generate the label of a large video cluster. Applying our model to each of the two video pairs in a cluster to generate abstract captions and then further generate more abstractive captions from them may address the problem of many videos, which we leave as future work.\\n\\n9. Ethics Statement\\n\\nWe used Amazon Mechanical Turk to recruit the crowdworkers at a price of 10 US dollars per hour on average. By agreeing to work on the annotation, crowdworkers have agreed to give the right to use the annotation for research purposes. Videos were shown to crowdworkers for annotation. The videos were from VATEX, which does not contain any harmful ones.\\n\\n10. Acknowledgements\\n\\nThis work was supported by JSPS KAKENHI Grant Number JP23H03454 and Fujitsu.\\n\\n11. Bibliographical References\\n\\nNayyer Aafaq, Naveed Akhtar, Wei Liu, Syed Zulqarnain Gilani, and Ajmal Mian. 2019a. Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12487\u201312496.\\n\\nNayyer Aafaq, Ajmal Mian, Wei Liu, Syed Zulqarnain Gilani, and Mubarak Shah. 2019b. Video description: A survey of methods, datasets, and evaluation metrics. ACM Computing Surveys (CSUR), 52(6):1\u201337.\\n\\nNaomi S Altman. 1992. An introduction to kernel and nearest-neighbor nonparametric regression. The American Statistician, 46(3):175\u2013185.\\n\\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu\u010di\u0107, and Cordelia Schmid. 2021. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836\u20136846.\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.\\n\\nMax Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738.\\n\\nSatanjeev Banerjee and Alon Lavie. 2005. MeToren: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372.\\n\\nAnkan Bansal, Yuting Zhang, and Rama Chellappa. 2020. Visual question answering on image sets. In Proceedings of the European Conference on Computer Vision (ECCV).\\n\\nGedas Bertasius, Heng Wang, and Lorenzo Torresani. 2021. Is space-time attention all you need for video understanding? arXiv preprint arXiv:2102.05095.\"}"}
{"id": "lrec-2024-main-5", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. *Transactions of the association for computational linguistics*, 5:135\u2013146.\\n\\nJo\u00e3o Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? A new model and the kinetics dataset. In *proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 6299\u20136308.\\n\\nPaul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for YouTube recommendations. In *Proceedings of the 10th ACM conference on recommender systems*, pages 191\u2013198.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. *ICLR*.\\n\\nLianli Gao, Zhao Guo, Hanwang Zhang, Xing Xu, and Heng Tao Shen. 2017. Video captioning with attention-based LSTM and semantic consistency. *IEEE Transactions on Multimedia*, 19(9):2045\u20132055.\\n\\nAndrew Gilbert and Richard Bowden. 2011. Igroup: Weakly supervised image and video grouping. In *2011 International Conference on Computer Vision*, pages 2166\u20132173. IEEE.\\n\\nSimon Ging, Mohammadreza Zolfaghari, Hamed Pirsiavash, and Thomas Brox. 2020. Coot: Cooperative hierarchical transformer for video-text representation learning. *arXiv preprint arXiv:2011.00597*.\\n\\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. *Neural computation*, 9(8):1735\u20131780.\\n\\nVladimir Iashin and Esa Rahtu. 2020. Multi-modal dense video captioning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops*, pages 958\u2013959.\\n\\nAnil K Jain. 2010. Data clustering: 50 years beyond k-means. *Pattern recognition letters*, 31(8):651\u2013666.\\n\\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019. Billion-scale similarity search with GPUs. *IEEE Transactions on Big Data*, 7(3):535\u2013547.\\n\\nLinjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. 2020a. Hero: Hierarchical encoder for video + language omni-representation pre-training. In *EMNLP*.\\n\\nSheng Li, Zhiqiang Tao, Kang Li, and Yun Fu. 2019. Visual to text: Survey of image and video captioning. *IEEE Transactions on Emerging Topics in Computational Intelligence*, 3(4):297\u2013312.\\n\\nZhuowan Li, Quan Tran, Long Mai, Zhe Lin, and Alan L Yuille. 2020b. Context-aware group captioning via self-attention and contrastive features. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3440\u20133450.\\n\\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In *Text summarization branches out*, pages 74\u201381.\\n\\nKevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang. 2022. Swinbert: End-to-end transformers with sparse attention for video captioning. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 17949\u201317958.\\n\\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. 2022. Video Swin Transformer. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3202\u20133211.\\n\\nHuaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. 2020. Univl: A unified video and language pre-training model for multimodal understanding and generation. *arXiv preprint arXiv:2002.06353*.\\n\\nHuaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. 2021. Clip4clip: An empirical study of clip for end-to-end video clip retrieval. *arXiv preprint arXiv:2104.08860*.\\n\\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In *ICCV*.\\n\\nYingwei Pan, Ting Yao, Houqiang Li, and Tao Mei. 2017. Video captioning with transferred semantic attributes. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 6504\u20136512.\"}"}
{"id": "lrec-2024-main-5", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551.\\n\\nMike Schuster and Kuldip KPaliwal. 1997. Bidirectional recurrent neural networks. IEEE transactions on Signal Processing, 45(11):2673\u20132681.\\n\\nShikhar Sharma, Layla El Asri, Hannes Schulz, and Jeremie Zumer. 2017. Relevance of unsupervised metrics in task-oriented dialogue for evaluating natural language generation. CoRR, abs/1706.09799.\\n\\nAlok Singh, Thoudam Doren Singh, and Sivaji Bandyopadhyay. 2020. Nits-vcsystem for vatex video captioning challenge 2020. arXiv preprint arXiv:2006.04058.\\n\\nShane Storks, Qiaozi Gao, and Joyce Y Chai. 2019. Recent advances in natural language inference: A survey of benchmarks, resources, and approaches. arXiv preprint arXiv:1904.01172.\\n\\nMingkang Tang, Zhanyu Wang, Zhenhua Liu, Fengyun Rao, Dian Li, and Xiu Li. 2021. Clip4caption: Clip for video caption. In Proceedings of the 29th ACM International Conference on Multimedia, pages 4858\u20134862.\\n\\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. 2015. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 4489\u20134497.\\n\\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575.\\n\\nSubhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2015a. Sequence to sequence-video to text. In Proceedings of the IEEE international conference on computer vision, pages 4534\u20134542.\\n\\nSubhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. 2014. Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729.\\n\\nSubhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. 2015b. Translating videos to natural language using deep recurrent neural networks. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1494\u20131504, Denver, Colorado. Association for Computational Linguistics.\\n\\nBairui Wang, Lin Ma, Wei Zhang, and Wei Liu. 2018. Reconstruction network for video captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7622\u20137631.\\n\\nZheshen Wang, Ming Zhao, Yang Song, Sanjiv Kumar, and Baoxin Li. 2010. Youtubecat: Learning to categorize wild web videos. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 879\u2013886. IEEE.\\n\\nSaining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. 2018. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European conference on computer vision (ECCV), pages 305\u2013321.\\n\\nChenggang Yan, Yunbin Tu, Xingzheng Wang, Yongbing Zhang, Xinhong Hao, Yongdong Zhang, and Qionghai Dai. 2019. Stat: Spatial-temporal attention mechanism for video captioning. IEEE transactions on multimedia, 22(1):229\u2013241.\\n\\nJunchao Zhang and Yuxin Peng. 2019. Object-aware aggregation with bidirectional temporal graph for video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8327\u20138336.\\n\\nZhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, and Xiang Zhou.\"}"}
{"id": "lrec-2024-main-5", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Crowdsourcing Interface\\n\\nFig. 9 shows the instruction part of the crowdsourcing interface that we used to collect human captions. We presented the task instructions with an annotation example to the crowdworker. We showed multiple videos in a video group to crowdworkers and asked them to write an abstract caption describing the shared content in the videos.\\n\\nB. Inference Examples With Different Number of Input Videos\\n\\nFig. 10 shows the generated captions in the experiments with the different number of input videos. In the six-video setting, the model generated an over-abstract caption, while the model generated an appropriate caption in the two-video setting.\"}"}
{"id": "lrec-2024-main-5", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Instruction part of the crowdsourcing interface for collecting human captions.\\n\\nGround Truth: a person is trying to walk on a rope\\n\\nTwo Videos: a person is walking on a rope\\nSix Videos: a person is doing gymnastics\\n\\nFigure 10: Inference examples with different numbers of input videos. We used the top two videos in this figure in the two-video setting.\"}"}
