{"id": "acl-2022-long-253", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ConditionalQA: A Complex Reading Comprehension Dataset with Conditional Answers\\n\\nHaitian Sun\\nSchool of Computer Science\\nCarnegie Mellon University\\nhaitians@cs.cmu.edu\\n\\nWilliam W. Cohen\\nGoogle Research\\nwcohen@google.com\\n\\nRuslan Salakhutdinov\\nSchool of Computer Science\\nCarnegie Mellon University\\nrsalakhu@cs.cmu.edu\\n\\nAbstract\\nWe describe a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e., the answers are only applicable when certain conditions apply. Answering the questions requires compositional logical reasoning across complex context. We call this dataset ConditionalQA. In addition to conditional answers, the dataset also features: (1) long context documents with information that is related in logically complex ways; (2) multi-hop questions that require compositional logical reasoning; (3) a combination of extractive questions, yes/no questions, questions with multiple answers, and not-answerable questions; (4) questions asked without knowing the answers. We show that ConditionalQA is challenging for many of the existing QA models, especially in selecting answer conditions. We believe that this dataset will motivate further research in understanding complex documents to answer hard questions.\\n\\nIntroduction\\nMany reading comprehension (RC) datasets have been recently proposed (Rajpurkar et al., 2016, 2018; Kwiatkowski et al., 2019; Yang et al., 2018; Dasigi et al., 2021; Ferguson et al., 2020). In a reading comprehension task, models are provided with a document and a question and asked to find the answers. Questions in existing reading comprehension datasets generally have a unique answer or a list of answers that are equally correct, e.g., \u201cWho was the president of the US?\u201d with the answers \u201cGeorge Washington\u201d, \u201cThomas Jefferson\u201d, etc. We say that these questions have deterministic answers. However, questions in the real world do not always have deterministic answers, i.e., answers to the questions are different under different conditions. For example, in Figure 1, the document discusses \u201cFuneral Expense Payment\u201d and a question asks an applicant\u2019s eligibility. This question cannot be deterministically answered: the answer is \u201cyes\u201d only if \u201cyou\u2019re arranging a funeral in the UK\u201d, while the answer is \u201cno\u201d, if \u201c... another close relative of the deceased is in work\u201d is true. We call answers that are different under different conditions conditional answers.\\n\\nA conditional answer consists of an answer and a list of conditions. An answer is only true if its conditions apply. In the example above, \u201cyou are arranging a funeral in the UK\u201d is the condition for the answer \u201cyes\u201d. An answer can have multiple conditions. Conditional answers are commonly seen when the context so complex so asking a complete question with a deterministic answer is impractical; for example, when a person asks a question with some prior knowledge in mind but cannot enumerate all necessary details. A practical way to answer incomplete questions is to find all possible answers to the question \u2013 and if some answers are only true...\"}"}
{"id": "acl-2022-long-253", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"under certain conditions, the conditions should be output as well. Answering such questions generally requires the models to understand the complex logic in the context and perform extensive reasoning to identify the answers and conditions.\\n\\nWe present the ConditionalQA dataset, which contains questions with conditional answers. We take documents from the UK government website as our corpus. Documents in this corpus discuss public policies in the UK and were first used in the ShARC dataset (Saeidi et al., 2018). It is particularly interesting for constructing the ConditionalQA dataset because it contains complex contents with complex internal logic such as conjunction, disjunction, and exception (see the example in Figure 1). Questions in ConditionalQA are asked by human annotators. Each example contains a question, a scenario when the question is asked, and a document that discusses the policy that the question asks about. The task is to find all possible answers to the questions that apply to the user's scenario. If an answer is only true under certain conditions, the model should return the list of conditions along with the answer. Answers and conditions are annotated by human annotators with the exact input, i.e. the question, the scenario, and the associated document. We provide supporting evidences labeled by human annotators as additional supervision.\\n\\nIn addition to having conditional answers, ConditionalQA also features the following properties. First, the documents in ConditionalQA have complex structure. As opposed to Wikipedia pages, where most sentences or paragraphs contain standalone information, documents in ConditionalQA usually have complex internal logic that is crucial for answering the questions. Second, many questions in the dataset are naturally multi-hop, as illustrated in the example on Figure 1, e.g. being \\\"the partner of the deceased\\\" satisfied the requirement on \\\"your relationship with the deceased\\\" which is one of high-level requirements to obtain the benefit. Answering those questions requires models that understand the internal logic within the document and reason over it to find correct answers. Third, we decouple the asking and answering process when annotating questions, as suggested by Ferguson et al. (2020); Dasigi et al. (2021); Clark et al. (2020), so questions are asked without knowing the answers. Forth, ConditionalQA contains various types of questions including yes/no questions and extractive questions. Questions can have one or multiple answers, or can be not answerable, as a result of the decoupled annotation process.\\n\\nWe experimented with several strong baseline models on ConditionalQA (Ainslie et al., 2020; Sun et al., 2021; Izacard and Grave, 2021). The best performing model achieves only 64.9% accuracy on yes/no questions, marginally better than the majority baseline (62.2% if always predicting \\\"yes\\\"), and 25.2% exact match (EM) on extractive answers. We further measure the accuracy of jointly predicting answers and conditions, in which the accuracy drops to 49.1% and 22.5%. The best metrics with conditions are obtained if no condition is predicted, showing how challenging it is for existing models to predict conditional answers.\\n\\n2 Related Works\\n\\nMany question answering datasets have been proposed in the past few years (Rajpurkar et al., 2016, 2018; Yang et al., 2018; Dasigi et al., 2021; Ferguson et al., 2020; Kwiatkowski et al., 2019) and research on these has significantly boosted the performance of QA models. As large pretrained language models (Devlin et al., 2019; Liu et al., 2019; Ainslie et al., 2020; Beltagy et al., 2020; Guu et al., 2020; Verga et al., 2020) achieved better performance on traditional reading comprehension and question answering tasks, efforts have been made to make the questions more complex. Several multi-hop QA datasets were released (Yang et al., 2018; Ferguson et al., 2020; Talmor and Berant, 2018; Welbl et al., 2018) to test models' ability to solve complex questions. However, most questions in these datasets are answerable by focusing on a small piece of evidence at a time, e.g. a sentence or a short passage, leaving reasoning through long and complex contents a challenging but unsolved problem.\\n\\nSome datasets have been recently proposed for question answering over long documents. QASPER (Dasigi et al., 2021) contains questions asked from academic papers, e.g. \\\"What are the datasets experimented in this paper?\\\". To answer those questions, the model should read several sections and collect relevant information. NarrativeQA (Mou et al., 2021) requires reading entire books or movie scripts to answer questions about their characters or plots. Other datasets, e.g. HybridQA (Chen et al., 2021b), can also be viewed as question answering over long documents if tables with hyper-linked text from the cells are flattened into...\"}"}
{"id": "acl-2022-long-253", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a hierarchical document. ShARC (Saeidi et al., 2018) is a conversational QA dataset that also uses UK government websites as its corpus. However, the ShARC dataset only contains yes/no questions and the conversation history is generated by annotators with the original rule text in hand, making the conversation artificial. The length of context in ShARC is usually short, such as a few sentences or a short paragraph. While using the same corpus, ConditionalQA contains completely different questions and new types of answers. It focuses on a new problem that has not been previously studied.\\n\\nMost of the existing datasets, including the ones discussed above, contain questions with unique answers. Answers are unique because questions are well specified, e.g. \u201cWho is the president of the US in 2010?\u201d However, questions can be ambiguous if not all information is provided in the question, e.g. \u201cWhen was the Harry Potter movie released?\u201d does not specify which Harry Potter movie. AmbigQA (Min et al., 2020) contains questions that are ambiguous, and requires the model to find all possible answers of an ambiguous question and rewrite the question to make it well specified. Similar datasets Temp-LAMA (Dhingra et al., 2021), TimeQA (Chen et al., 2021a) and SituatedQA (Zhang and Choi, 2021) have been proposed that include questions that require resolving temporal or geographic ambiguity in the context to find the answers. They are similar to ConditionalQA in that questions are incomplete, but ConditionalQA focuses on understanding documents with complex logic and answering questions with conditions. It\u2019s usually not possible to disambiguate questions in ConditionalQA as rewriting the questions (or scenarios) to reflect all conditions of answers to make the questions deterministic is impractical.\\n\\nWe create ConditionalQA in the public policy domain. There are some existing domain specific datasets, including PubMedQA and BioAsq (Nenitidis et al., 2018; Jin et al., 2019) in medical domain, UDC (Lowe et al., 2016) in computer software domain, QASPER (Dasigi et al., 2021) in academic paper domain, PrivacyQA and PolicyQA (Ahmad et al., 2020; Ravichander et al., 2019) in legal domain, etc. PrivacyQA and PolicyQA have similar context as ConditionalQA, but the questions do not require compositional reasoning and the answers are short text spans. We use a corpus in the public policy domain because it is easy to understand by non-experts while being complex enough to support challenging questions. ConditionalQA is not designed to be a domain specific dataset.\\n\\n3 The Task\\n\\nIn our task, the model is provided with a long document that describes a public policy, a question about this document, and a user scenario. The model is asked to read the document and find all answers and their conditions if any.\\n\\n3.1 Corpus\\n\\nDocuments in ConditionalQA describe public policies in the UK, e.g. \u201cApply for Visitor Visa\u201d or \u201cPunishment of Driving Violations\u201d. Each document covers a unique topic and the contents are grouped into sections and subsections. Contents in the same section are closely related but may also be referred in other sections. We create ConditionalQA in this domain because these documents are rather complex with internal logic, yet annotators are familiar with the content so they can ask natural yet challenging questions, compared to formal legal or financial documents with more sophisticated terms and language.\\n\\n3.2 Input\\n\\nThe input to a reading comprehension model consists of a document, a question, and a user scenario:\\n\\n- A document describes a public policy in the UK. Content of a document is coherent and hierarchical, structured into sections and subsections. Documents are crawled from the website and processed by serializing the DOM trees of the web pages into lists of HTML elements with tags, such as <h1>, <p>, <li>, and <tr>. Please see more information in \u00a74.1.\\n- A question asks about a specific aspect of the document, such as eligibility or other aspects with \u201chow\u201d, \u201cwhen\u201d, \u201cwhat\u201d, \u201cwho\u201d, \u201cwhere\u201d, etc. Questions are relevant to the content in the document, even though they may be \u201cnot answerable\u201d.\\n- A user scenario provides background information for the question. Some information will be used to restrict the answers that can be possibly correct. Not all information in the user scenario is relevant because they are written by crowd source workers without seeing the full document or knowing the answers. Information in the scenario is also likely to be incomplete. This setup simulates the real\"}"}
{"id": "acl-2022-long-253", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3 Output\\nA reading comprehension model is asked to predict the answers and the list of conditions if there is any.\\n\\n\u2022 An answer to the question has three different types: (1) \u201cyes\u201d or \u201cno\u201d for questions such as \u201cCan I get this benefit?\u201d; (2) an extracted text span for questions asking \u201chow\u201d, \u201cwhen\u201d, \u201cwhat\u201d, etc.; (3) \u201cnot answerable\u201d if an answer does not exist in the document. Since the information to get a definite answer is sometimes incomplete, besides predicting the answers, the model is asked to identify their conditions.\\n\\n\u2022 A condition contains information that must be satisfied in order to make the answer correct but is not mentioned in the user scenario. In ConditionalQA, we restrict a condition to be one of the HTML elements in the document instead of the exact extracted text span.\\n\\nSelected conditions are then evaluated as a retrieval task with F1 at the element level, i.e. the model should retrieve all HTML elements with unsatisfied information to get a perfect F1 score. If no condition is required, the model must return an empty list. Please see \u00a73.4 for more details on evaluation.\\n\\n3.4 Evaluation\\nWe evaluate performance of models on the ConditionalQA dataset as a reading comprehension (RC) task. Answers are measured with exact match (EM) and F1. Some questions have multiple answers. The model should correctly predict all possible answers to get the full score. Since the order of answers does not matter, to compute the metrics, we compare all possible permutations of the predicted answers to the list of correct answers. We take the best result among all permutations as the result for this example. Let \\\\( \\\\{\\\\hat{a}_1, \\\\ldots, \\\\hat{a}_m\\\\} \\\\) be the list of predicted answer and \\\\( \\\\{a_1, \\\\ldots, a_n\\\\} \\\\) the reference answers. The EM of the predicted answers is\\n\\n\\\\[\\n\\\\text{EM} = \\\\max \\\\left\\\\{ \\\\tilde{a}_1, \\\\ldots, \\\\tilde{a}_m \\\\right\\\\} \\\\frac{1}{n} \\\\sum_{i=1}^{\\\\min(m,n)} s_{\\\\text{em}}(\\\\tilde{a}_i, a_i) \\\\gamma_{m,n}\\n\\\\]\\n\\nWe argue that selecting HTML elements as conditions is already very challenging (see experimental results in \u00a75.2) and leave extracting the exact text spans as future work.\\n\\n\\\\[\\n\\\\gamma_{m,n} = \\\\begin{cases} \\n  e^{1 - m/n} & \\\\text{if } m > n \\\\\\\\\\n  1 & \\\\text{if } m \\\\leq n\\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\( \\\\{\\\\tilde{a}_1, \\\\ldots, \\\\tilde{a}_m\\\\} \\\\) is a permutation of the predicted answers \\\\( \\\\{\\\\hat{a}_1, \\\\ldots, \\\\hat{a}_m\\\\} \\\\) and \\\\( s_{\\\\text{em}}(\\\\cdot, \\\\cdot) \\\\) is the scoring function that measures EM between two text spans. \\\\( \\\\gamma_{m,n} \\\\) is a penalty term that is smaller than 1 if more answers than the reference answers are predicted, i.e. \\\\( m > n \\\\). We compute token-level F1 in the similar way using the scoring function \\\\( s_{\\\\text{f1}}(\\\\cdot, \\\\cdot) \\\\) on the extracted answer spans. For not answerable questions, EM and F1 are 1.0 if and only if no answer is predicted.\\n\\nWe additionally measure the performance of answers with conditions. We adopt the same permutation strategy as above, except that the scoring function will also take into account the accuracy of predicted conditions. Let \\\\( \\\\hat{C}_i \\\\) be the set of predicted conditions for the predicted answer \\\\( \\\\hat{a}_i \\\\) and \\\\( C_i \\\\) be the oracle conditions for the answer \\\\( a_i \\\\). The new scoring function for the predicted answer with conditions is\\n\\n\\\\[\\n\\\\text{s}_{\\\\text{em}+c}(\\\\tilde{a}_i, \\\\tilde{C}_i, a_i, C_i) = \\\\text{s}_{\\\\text{em}}(\\\\tilde{a}_i, a_i) \\\\cdot \\\\text{F1}(\\\\hat{C}_i, C_i)\\n\\\\]\\n\\nwhere \\\\( \\\\text{F1}(\\\\cdot, \\\\cdot) \\\\) measures the accuracy of the set of predicted conditions at HTML element level. Recall that conditions are restricted to select from HTML elements in the document. \\\\( \\\\text{F1}(\\\\hat{C}_i, C_i) \\\\) equals to 1 if and only if all required conditions are selected. This is different from \\\\( s_{\\\\text{f1}}(\\\\cdot, \\\\cdot) \\\\) that measures token level F1 of the extracted answers. If the answer does not require any conditions, the model should predict an empty set. We simply replace the scoring function \\\\( s_{\\\\text{em}}(\\\\cdot, \\\\cdot) \\\\) in Eq. 1 with \\\\( s_{\\\\text{em}+c}(\\\\cdot, \\\\cdot) \\\\) to compute EM with conditions.\\n\\n4 Data Collection\\n4.1 Documents\\nDocuments are originally presented on the UK government website in the HTML format. We crawled the pages from the website and processed it to only keep the crucial tags, that include:\\n\\n\u2022 Headings <h1, h2, h3, h4>: We keep headings at different levels. This can be used to identify the hierarchical structure in the documents.\\n\\n\u2022 Text <p>: This tag is used for general contents. We replace descriptive tags, e.g. <strong>, with the plain tag <p> for simplicity.\\n\\n\u2022 List <li>: We keep the tags for list items, but drop their parent tags <ul> or <ol>. We observe that very few ordered lists (<ol>) have\"}"}
{"id": "acl-2022-long-253", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 Dataset\\n\\nWe have used the dataset of the \\\"Policy Microsimulation in the UK\\\" by (Palmer et al., 2014) to construct our corpus. \\n\\nA processed document contains a list of strings that starts with a tag, follows with its content, and ends with the tag, e.g. \\n\\n> [\"<h1> Overview </h1>\", \"<p> You can apply for ... </p>\"]\\n\\nWe drop some common sections that do not contain any crucial information, e.g. \\\"How to Apply\\\", to make sure that questions are specific to the topic of the documents. We further require that the document should contain at least 3 sections. We end up with 652 documents as our corpus. The max length of the documents is 9230 words (16154 sub-words in T5 (Raffel et al., 2020)).\\n\\n4.2 Questions\\n\\nWe collect questions from crowd source workers on Amazon Mechanical Turk. To encourage workers asking questions not be restricted to a specific piece of text, we hide the full document but instead provide a snippet of the document to the workers. A snippet includes a table of content that contains section and subsection titles (from <h1> and <h2> tags), and the very first subsection in the document that usually provides a high level overview of the topic. The snippet lets workers get familiar with the topic of this document so they can ask closely relevant questions. We observe that restricting the geographic location of workers to the UK can significantly improve the quality of questions because local residents are more familiar with their policies.\\n\\nWe ask the workers to perform three sub-tasks when coming up with the questions. First, we ask the workers to provide three attributes that can identify the group of people who may benefit from or be regulated by the policy discussed in the document. Second, they are asked to come up with a scenario when they will want to read this document and a question about what they would like to know. Third, workers are asked to mark which attributes have been mentioned in their question and scenario. When assessing the annotation quality, we find that asking workers to provide attributes makes the questions and scenarios much more specific, significantly improving the quality of the dataset. We assign 3 workers to documents with four or more sections and 2 workers to documents with three sections. Each worker is asked to give two questions and the two questions have to be diverse. We collect 3617 questions in this stage.\\n\\n4.3 Find Answers\\n\\nWe hire another group of workers to work on the answer portion of this task. Finding answers is very challenging to crowd source workers because it requires the workers to read the full document carefully to understand every piece of information in the document. We provide one-on-one training for the workers to teach them how to select supporting evidences, answers, and conditions.\\n\\nWorkers are asked to perform three sub-tasks. The first step is to select supporting evidences from the document. Supporting evidences are HTML elements that are closely related to the questions, including elements that have content that directly justify the answers and the ones that will be selected as conditions in the next step. In the second step, workers are asked to type answers and select associated conditions. Workers can input as many answers as possible or mark the question as \\\"not answerable\\\". For each answer, they can select one or more supporting evidences as the answer's conditions if needed. Workers are asked not to select conditions if there is sufficient information in the scenario to answer the question. We give workers permission to slightly modify the questions or scenarios if the questions are not clearly stated, or they can mark it as a bad question (different from not answerable) so we will drop it from the dataset.\\n\\nWe additionally perform a revise step to improve the annotation quality. We provide the union of selected evidences and answers from multiple annotations of a question to an additional group of annotators and let them deselect unrelated evidences and merge answers. As the amount of information provided to workers at this step is significantly less than in the previous answer selection stage, the annotation quality improves significantly. We end up with 3102 questions with annotated answers.\\n\\n4.4 Move Conditions to Scenario\\n\\nTo encourage the model of learning subtle difference in user scenarios that affects the answers and conditions, we create new questions by modifying existing questions with conditional answers by moving one of the conditions to their scenarios.\"}"}
{"id": "acl-2022-long-253", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Scenario: \u201cMy father has recently appealed for a traffic ticket.\u201d\\nQuestion: \u201cHow long will it take to get a decision?\u201d\\n\u2022 \u201c4 weeks\u201d\\n\\nScenario: \u201cI applied to cut down a tree on my land but it was rejected 20 days ago.\u201d\\nQuestion: \u201cAm I still able to appeal against it?\u201d\\n\u2022 \u201cyes\u201d\\n\\nScenario: \u201cI will get my first paycheck tomorrow.\u201d\\nQuestion: \u201cWhat information should be on my pay split?\u201d\\n\u2022 \u201cearnings before and after any deductions\u201d\\n\u2022 \u201cthe amount of any deductions\u201d\\n\u2022 \u201cthe number of hours you worked\u201d\\n\\nScenario: \u201cI am looking at buying a new build home. I am 26 and a first-time buyer.\u201d\\nQuestion: \u201cAm I eligible to get an Equity Loan?\u201d\\n\u2022 \u201cyes\u201d\\n  - \u201cable to afford fees and interest\u201d\\n  - \u201csold by an eligible homebuilder\u201d\\n\u2022 \u201cno\u201d\\n  - \u201cYou can not apply if you had any form of sharia mortgage finance\u201d\\n\\nScenario: \u201cI always walk my labrador in open spaces. I forgot to clean up his mess yesterday.\u201d\\nQuestion: \u201cHow much can I be fined for this?\u201d\\n\u2022 \u201c$100\u201d\\n  - \u201c$100 on the spot\u201d\\n\u2022 \u201cup to $1000\u201d\\n  - \u201cup to $1,000 if it goes to court\u201d\\n\\nScenario: \u201cI am about to apply for a Parent of a Child Student Visa to stay with my child for a year in the UK.\u201d\\nQuestion: \u201cWhat documents are needed to apply for this visa?\u201d\\n\u2022 \u201ca current passport or other travel document\u201d\\n\u2022 \u201cproof that you have enough fund\u201d\\n\u2022 \u201cyour tuberculosis (TB) test results\u201d\\n  - \u201cyour tuberculosis (TB) test results if you are from a country where you have to take the TB test\u201d\\n\\nTable 1: Example of questions in ConditionalQA. Text pieces that follow the answers in brackets are conditions. Some answers are deterministically correct so they are not followed by conditions. Specifically, we show the workers the original questions, scenarios, and the annotated answers and conditions. Evidences are also provided for workers to get them familiar with the background of the questions and reasoning performed to get the original answers. Workers are asked to pick one of the conditions and modify the original scenario to reflect this condition. The modified questions and scenarios are sent back to the answering stage to get their annotations. We randomly select a small portion of the questions that have conditional answers as inputs to this stage so as to not affect the original distribution of the dataset. We collected 325 additional examples from this stage.\\n\\n4.5 Train / Dev / Test Splits\\nWe partition the dataset by documents to prevent leaking information between questions from the same document. The dataset contains 436 documents and 2338 questions in the training set, 59 documents and 285 questions in the development set, and 136 documents and 804 questions in the test set. Please see Appendix A for more statistics on ConditionalQA.\\n\\n5 Evaluation\\n5.1 Baselines\\nEvaluating existing models on ConditionalQA is challenging. In addition to predicting answers to questions, the ConditionalQA task also asks the model to find the answers\u2019 conditions if any of them applies. To the best of our knowledge, no existing model fits the purpose of this task. We modified three competitive QA models as baselines to the ConditionalQA dataset. In addition to the new form of answers, traditional reading comprehension models also face the challenge that the context of questions in ConditionalQA is too long to fit into the memory of many Transformer-based models like BERT (Devlin et al., 2019) and even ETC (Ainslie et al., 2020). The baseline models we implemented are described below.\\n\\nETC: ETC (Ainslie et al., 2020) is a pretrained Transformer-based language model that is designed for longer inputs (up to 4096 tokens). ETC achieved the state-of-the-art on several challenging tasks, e.g. HotpotQA and WikiHop (Yang et al., 2018; Welbl et al., 2018). Since ETC cannot fit the entire document (with up to 16154 tokens) into its memory, we cannot let ETC to jointly predict answers and conditions, we designed a two stage...\"}"}
{"id": "acl-2022-long-253", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Experiment results on ConditionalQA (EM / F1). Numbers are obtained by re-running the open-sourced codes of the baselines. \u201cmajority\u201d reflects the accuracy of always predicting \u201cyes\u201d without conditions. *See discussion in text.\\n\\n| Method | Yes/No | Extractive |\\n|--------|--------|------------|\\n| DocHopper | 64.9%  | 49.1% |\\n| FiD | 64.2%  | 25.2% |\\n| Human | 91.4%  | 82.3% |\\n\\nThe ConditionalQA task is very challenging\u2014the performance of the best model on yes/no questions is 64.9% (marginally higher than always predicting the majority answer \u201cyes\u201d), and the performance on extractive questions is 25.2% EM. FiD has the best performance on extractive questions because FiD can predict multiple answers while ETC-pipeline and DocHopper only predict one. The performance drops significantly if answers and conditions are jointly evaluated. The best performance on jointly evaluating answers and conditions (\u201cw/ conditions\u201d) in Table 2 is only 49.1% for yes/no questions and 22.5% EM for extractive questions. Even worse, this best result is obtained when no condition is selected, i.e. the threshold...\"}"}
{"id": "acl-2022-long-253", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Error analysis on the predictions of the best performed model (FiD). The percentage is the fraction of errors made in that category over all errors.\\n\\nThe difficulty of selecting conditions is more obvious if we focus on the subset of questions that have at least one conditional answer. The accuracy drops by 90% if answers and conditions are jointly evaluated.\\n\\nWe also study how the threshold on the confidence scores of selecting conditions affects the evaluation results. Results are shown in Figure 2. As we decrease the threshold for selecting conditions, the EM with conditions on the subset of questions that have conditional answers slightly improves, but the overall EM with conditions drops dramatically due to the false positive conditions.\\n\\nFiD is a generative model so we can not evaluate it in the same way. In our evaluation, predictions from the best performing FiD checkpoint also do not select any conditions.\\n\\nTable 4 shows the best results on the subset of questions that have conditional answers. Hyperparameters are tuned on the subset of questions. We could possibly get better results on questions with conditional answers with threshold $\\\\epsilon < 0.0$, but the improvement is still marginal.\\n\\n5.3 Error Analysis\\nWe manually check 200 examples in the prediction of the best performed model FiD and label the type of errors made. The numbers are shown in Table 3. The most errors are made when only a subset of correct answers is predicted. This is due to the fact that the model (FiD) has a tendency to predict one answer for each question. The second most common errors are made by predicting answers with the correct type but wrong value. Such errors are commonly made by reading comprehension models in many tasks. The model made a lot of errors in yes/no questions because they consist of around 50% of the questions. The model is good at distinguishing yes/no questions and extractive question as producing the wrong kind of answer only makes up of 4.2% of the errors.\\n\\n6 Conclusion\\nWe propose a challenging dataset ConditionalQA that contains questions with conditional answers.\"}"}
{"id": "acl-2022-long-253", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: EM/F1 w/ conditions on the subset of questions with conditional answers. \u201cBest Overall\u201d uses the best checkpoints/hyper-parameters on the full dataset, while \u201cBest Conditional\u201d uses the best ones on the subset of questions. The dataset requires models to understand complex logic in a document in order to find correct answers and conditions to the questions. Experiments on state-of-the-art QA models show that their overall performance on ConditionalQA is relatively poor. This also suggests that current QA models lack the reasoning ability to understand complex documents and answer hard questions with answers beyond single span extraction. We hope that this dataset will stimulate further research in building NLP models with better reasoning abilities.\\n\\n7 Ethics Statements\\nThis dataset should be ONLY used for NLP research purpose. Questions are artificial and do not contain any personal information. Answers are NOT provided by legal professionals and should NOT be used for any legal purposes.\\n\\n8 Acknowledgement\\nThis work was supported in part by the NSF IIS1763562, ONR Grant N000141812861, Google Research. We would also like to thank Vijay A. Saraswat <Vijay.Saraswat@gs.com> for valuable feedback.\\n\\nReferences\\nWasi Ahmad, Jianfeng Chi, Yuan Tian, and Kai-Wei Chang. 2020. PolicyQA: A reading comprehension dataset for privacy policies. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 743\u2013749, Online. Association for Computational Linguistics.\\n\\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. Etc: Encoding long and structured inputs in transformers.\\n\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.\\n\\nWenhu Chen, Xinyi Wang, and William Yang Wang. 2021a. A dataset for answering time-sensitive questions.\\n\\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang. 2021b. Hybridqa: A dataset of multi-hop question answering over tabular and textual data.\\n\\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages.\\n\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding.\\n\\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. 2021. Time-aware language models as temporal knowledge bases.\\n\\nJames Ferguson, Matt Gardner, Hannaneh Hajishirzi, Tushar Khot, and Pradeep Dasigi. 2020. Iirc: A dataset of incomplete information reading comprehension questions.\\n\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasuapat, and Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre-training.\\n\\nGautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering.\\n\\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. 2019. Pubmedqa: A dataset for biomedical research question answering.\\n\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.\\n\\nRyan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2016. The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems.\\n\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. Ambigqa: Answering ambiguous open-domain questions.\"}"}
{"id": "acl-2022-long-253", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xiangyang Mou, Chenghao Yang, Mo Yu, Bingsheng Yao, Xiaoxiao Guo, Saloni Potdar, and Hui Su. 2021. Narrative question answering with cutting-edge open-domain qa techniques: A comprehensive study.\\n\\nAnastasios Nentidis, Anastasia Krithara, Konstantinos Bougiatiotis, Georgios Paliouras, and Ioannis Kakadiaris. 2018. Results of the sixth edition of the BioASQ challenge. In Proceedings of the 6th BioASQ Workshop A challenge on large-scale biomedical semantic indexing and question answering, pages 1\u201310, Brussels, Belgium. Association for Computational Linguistics.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.\\n\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable questions for squad.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.\\n\\nAbhilasha Ravichander, Alan W Black, Shomir Wilson, Thomas Norton, and Norman Sadeh. 2019. Question answering for privacy policies: Combining computational and legal perspectives. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4947\u20134958, Hong Kong, China. Association for Computational Linguistics.\\n\\nMarzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rockt\u00e4schel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018. Interpretation of natural language rules in conversational machine reading.\\n\\nHaitian Sun, William W. Cohen, and Ruslan Salakhutdinov. 2021. End-to-end multihop retrieval for compositional question answering over long documents.\\n\\nAlon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions.\\n\\nPat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen. 2020. Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge.\\n\\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering.\\n\\nMichael J. Q. Zhang and Eunsol Choi. 2021. Situatedqa: Incorporating extra-linguistic contexts into qa.\"}"}
{"id": "acl-2022-long-253", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The dataset consists of yes/no questions and extractive questions. Questions may contain one or more answers, with or without conditions. The statistics of the questions are shown in Table 5.\\n\\n**Answer type**\\n\\nAmong all the answerable questions, 1751 questions have yes/no answers while the other 1527 questions have extractive answers. 1161 of the yes/no questions have the answer \u201cyes\u201d, 712 questions have answer \u201cno\u201d, and 122 questions have both answers \u201cyes\u201d and \u201cno\u201d under different conditions. Please see the example in Table 1. The average length of the extract answers is 6.36 tokens.\\n\\n**Condition type**\\n\\n803 questions have conditional answers. 390 out of the 803 questions have one answer, but this answer is only correct if the conditions are satisfied. 173 questions have multiple answers, each have their own conditions, i.e. the answers are different if different conditions apply. The rest 240 questions also have multiple answers, but some of the answers require conditions while other don\u2019t. See examples in Table 1. A total of 1090 answers from 803 questions have conditions, among which 672 answers have only one condition and 418 answers have multiple conditions.\\n\\n**Number of answers**\\n\\nBesides questions that have different answers under different conditions, 339 questions have multiple deterministic answers.\"}"}
