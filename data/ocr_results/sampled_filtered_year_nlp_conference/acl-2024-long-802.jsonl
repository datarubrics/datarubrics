{"id": "acl-2024-long-802", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators\\n\\nIndraneil Paul1, Goran Glava\u01612, and Iryna Gurevych1\\n\\n1 Ubiquitous Knowledge Processing Lab (UKP Lab)\\nDepartment of Computer Science and Hessian Center for AI (hessian.AI)\\nTechnische Universit\u00e4t Darmstadt\\n\\n2 CAIDAS, University of W\u00fcrzburg\\n\\nwww.ukp.tu-darmstadt.de\\n\\nAbstract\\n\\nCode generation has fast become one of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs, such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside the exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations (IR)\u2014shared across programming languages\u2014to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer. To this end, we first compile SLTrans,1,2 a parallel dataset consisting of nearly 4M self-contained source code files coupled with their respective intermediate representations. Next, starting from various base Code-LMs (ranging from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across various code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.\\n\\n1 Introduction\\n\\nLanguage models for code generation (Code-LMs) are some of the most promising tools for enhancing the productivity of software developers. They have proliferated into automating several parts of the traditional software development lifecycle, including code infilling, comment generation, refactoring, and build error prediction (Fr\u00f6mmgen et al., 2024; Dunay et al., 2024), inter alia. Despite a strong demand for such capabilities across all programming languages, the benchmarking of Code-LMs has largely been dominated by the most resourced languages. For instance, popular benchmarks such as HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and APPS (Hendrycks et al., 2021) all test Code-LMs' competence only in Python: this can result in misleading conclusions on the global utility of Code-LMs. More recent transpilation-oriented benchmarks like Multipl-E (Cassano et al., 2022) and BabelCode (Orlanski et al., 2023)\u2014that test competence on several languages\u2014have laid bare the gaps in Code-LMs' performance across different programming languages. For instance, the state-of-the-art DeepSeekCoder's (Guo et al., 2024) code completion performance in Bash, the most popular shell scripting language, lags its Python pass@1 performance by 30%+ points.\\n\\nThe problem is further exacerbated by the fact that the distribution of programming languages in code corpora is far more skewed than that of natural languages in standard multilingual text corpora. As an example, Ukrainian, considered to be a moderate-to-low-resource natural language (Tracey et al., 2019), comprises a higher proportion of the massively multilingual mC4 corpus (Xue et al., 2021) than Rust (the 13th most popular programming language).\"}"}
{"id": "acl-2024-long-802", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The aforementioned limitations and properties of multilingual code corpora, i.e., skewed and rapidly changing distribution over programming languages, warrant a departure from the conventional approach of pre-training on ever-larger file-level source-code corpora. Indeed, recent evidence points to tangible downstream gains from the adoption of smaller but curated or synthesized data (Gunasekar et al., 2023) as well as from grounding code generation using metadata from language toolchains (Chen et al., 2023a; Gong et al., 2024).\\n\\nThe latter, in principle, allows one to tap into more than half a century of research on programming languages and compilers and utilize views of the source code that often contain additional or more explicitly laid out information. This makes intuitive sense: skewed and fast-evolving distribution of programming languages implies that truly robust multilingual models cannot be obtained from heterogeneous source code alone; instead, some type of code interlingua should be leveraged to facilitate cross-lingual transfer from high- to low(er)-resource languages.\\n\\nIn this work, we propose compiler intermediate representations (IR) to be this interlingua for grounding source code understanding across heterogeneous languages, allowing for the creation of all-around stronger models. The IR are the artifacts of transformations performed by the compiler in three sequential phases: frontend, middle-end, and backend transformations. In popular cross-language compiler frameworks, the frontend IR contains language-specific constructs, whereas the backend IR contains the target platform-specific execution constructs. The middle-end IR, however, is agnostic to the source programming language and target execution platform and thus represents, we argue, an ideal shared representation for positive knowledge transfer in multilingual Code-LMs, offering both (1) a way to better semantically align constructs from heterogeneous programming languages and (2) an alternative (and possibly more informative) view of the source code.\\n\\nContributions and Research Questions.\\nOur work makes the following contributions:\\n\\n1) We create SLTrans, a parallel dataset consisting of nearly 4M pairs of self-contained source code with corresponding IR;\\n2) We conduct a systematic investigation of the benefits of grounding Code-LMs in IR, demonstrating sizeable and consistent empirical gains across a broad range of tasks and programming languages;\\n3) We create and publicly release a suite of base and instruction-tuned Code-LMs dubbed IRCoder (ranging in size from 1.1B to 7.3B parameters), the result of continued pre-training of state-of-the-art Code-LMs on a mixture of parallel data from SLTrans and monolingual data.\\n\\nWe test whether grounding on IR leads to more effective Code-LMs, aiming to provide answers to the following research questions:\\n\\nRQ1: Does training with explicit grounding via parallel source code-IR corpora provide benefits over continued pre-training on (unpaired) source code or IR alone?\\nRQ2: Does grounding on IR improve robustness to prompt perturbations common in human inputs?\\nRQ3: Does training on parallel source-IR data improve multilingual performance on code completion and understanding, with IR driving the positive knowledge transfer?\\nRQ4: What effect does pre-training on IR have on multilingual instruction following?\"}"}
{"id": "acl-2024-long-802", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Curated Data For Multilingual Generalization.\\n\\nCurating high-quality and domain-specific data with instructional value leads to more sample-efficient LM pretraining: Phi-1 (Gunasekar et al., 2023), for example, trained on as few as 7B tokens, performs on a par with models trained on hundreds of times more uncurated data. As Cassano et al. (2023a) shows, curation alone does not suffice for multilingual Code-LMs to generalize to underrepresented and unseen programming languages. Instead, the authors resort to using a bare-bones test case transpiler to translate synthetic test cases to the target language, validating the quality of the synthetic target language data generated this way. This finding aligns with results from (Rozi\u00e8re et al., 2022), where the benefits of such verification have been demonstrated for code translation.\\n\\nThe compiler IR that we leverage in this work is the result of several sequentially executed transformations that\u2014inter alia\u2014eliminate dead code, unroll loops, combine expressions, and inline subroutines and thus offer significant instructional value without the need for generating unit tests, as the transformations are guaranteed to preserve the correctness of the source code.\\n\\nGrounding in Toolchain Metadata.\\n\\nThere exists an extensive body of work that leverages the structure of the code as well as information originating from artifacts of various stages of compilation to ground code generation. Starting with compiler frontend artifacts, attempts have been made to leverage Abstract Syntax Trees (ASTs) for grounding source code understanding by linearizing them and encoding with LSTMs (Jiang et al., 2022), GNNs (Zhang et al., 2022), CNNs (Mou et al., 2016), Transformers (Guo et al., 2022), or some combination thereof (Sun et al., 2020). Other modes of reliance on ASTs include (i) using them as a search prior for graph-based decoding (Brockschmidt et al., 2019), (ii) predicting (heuristically selected) paths from the tree as an auxiliary pre-training objective (Tipirneni et al., 2024) and, (iii) leveraging them for data augmentation: heuristic generation of meaning-preserving transformations, leveraged for contrastive learning (Jain et al., 2021; Quiring et al., 2019; Bahrami et al., 2021). Other compiler frontend artifacts such as Data Flow Graphs (DFGs) (Brauckmann et al., 2020) and Control Flow Graphs (CFGs) (Nair et al., 2020) have also been employed in grounding program understanding. Finally, there is work (Shojaee et al., 2023; Le et al., 2022) that derives the reward that guides program generation via reinforcement learning (RL) from AST, CFG, and DFG matches between the generated and reference code.\\n\\nOn the opposite end, compiler backend outputs have also been employed to ground Code-LMs, with compilation feedback in text form being favored by several recent efforts (Jiang et al., 2023; Chen et al., 2023b; Gou et al., 2023) to guide refining of tentative program generations. Concurrent work (Liu et al., 2023) proposed creating an RL reward to guide generation based on the kind and severity of compilation error outputs. Finally, several existing efforts also leverage the IR produced by the compiler middle-end during its optimization passes, with LLVM being the most frequent choice of IR. IRGen (Li et al., 2022) performs an exploratory study into using the IR itself as a meaning-preserving augmentation to perform contrastive learning on C source code. MulCS (Ma et al., 2023) reports improvements to multilingual code snippet search when the GNN encoder utilizes a custom semantic graph derived from the IR. In the work most closely related to ours, Szafraniec et al. (2023) addresses code translation between four languages, pre-training the translation model using various objectives, including source code to IR translation. Their effort, however, is limited to code translation (i.e., they do not consider any other task) and parallel source-to-IR data only at the function level (i.e., short context). In this work, in contrast, we investigate the general utility (i.e., for a wide range of downstream tasks) of pre-training multilingual Code-LMs using parallel source-to-IR data, scaling additionally up the data collection effort to (i) 12 programming languages and, importantly, (ii) self-contained file-level programs, which, intuitively, allows for grounding of many more source-code concepts (e.g., those instantiated with longer code spans) in IR than function-level alignment. Crucially, we demonstrate that standard LM training on parallel source-to-IR data alone improves the robustness and multilingual ability of Code-LMs, without any architectural interventions and training auxiliary objectives.\\n\\nCross-lingual Transfer and Alignment.\\n\\nMost mainstream code generation models (Li et al., 2023; Guo et al., 2024; Nijkamp et al., 2023; Roziere et al., 2023; Chai et al., 2023), due to being pre-trained on GitHub code, are multilingual by default.\"}"}
{"id": "acl-2024-long-802", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hence, they are subject to the curse of multilinguality, i.e., the degradation of model performance on high-resource languages when the number of training languages or the proportion of low-resource data in the pre-training corpus of a multilingual model is scaled up. This is usually caused by negative interference between unrelated languages to which the model can only allocate a fixed capacity (Lauscher et al., 2020; Wu and Dredze, 2020) and is a well-documented phenomenon in natural language models (Arivazhagan et al., 2019; Conneau et al., 2020). Attempts to circumvent it without scaling up the model to impractical sizes have resorted to sparsity (Ansell et al., 2022; Lee and Hwang, 2023), modularity (Pfeiffer et al., 2022) and model merging (Blevins et al., 2024). While the presence of similar phenomena has been verified in multilingual Code-LMs (Orlanski et al., 2023; Athiwaratkun et al., 2023), research into cross-lingual transfer and alignment across programming languages has been rather sparse, exploring a limited set of tasks and languages (Chen et al., 2022) or introducing task-specific architectural interventions (Yuan et al., 2022; Pian et al., 2023), which are hard to scale.\\n\\nWang et al. (2020) indicate that separation of model parameters into language-agnostic and language-specific subsets can result in language-specific parameters being the cause of negative interference. We believe this presents an opportunity to minimize such interference by means of a shared intermediate representation rather than language-specific parameters. While it is unclear what such representation would be in the case of natural languages, intermediate compiler representations make an obvious choice for programming languages. Grounding Code-LM pretraining on IR data, we believe, should also improve generalization (including to languages unseen in pretraining) and consequently facilitate cross-lingual transfer in downstream tasks, akin to cross-lingual transfer between non-English languages by models trained on English-centric bi-texts (Gao et al., 2023; Artetxe and Schwenk, 2019). Our experiments on a large array of tasks and programming languages show that this is indeed the case.\\n\\n3 SLTrans: A Source Code to LLVM IR Translation Pairs Dataset\\n\\nIn order to test the hypotheses we posit in Section 1, we seek to acquire parallel source-IR data for a mixture of low-, medium-, and high-resource programming languages.\\n\\n### Intermediate Code Representation.\\n\\nWe utilize LLVM (Lattner and Adve, 2004) as the intermediate representation of our choice because it possesses many of the qualities we deem beneficial for our objectives. LLVM is the most prevalent IR in existing code corpora (Kocetkov et al., 2023) and one of the few frameworks that maintain a well-developed human-readable IR standard, rendering its syntax and semantics learnable via language modelling. Additionally, LLVM is adopted as the target IR of many compiler frontends across several programming languages, mainly due to the ease with which its tooling infrastructure enables upstart languages to attain general availability.\\n\\n| Language | Frontend | Avg. Len. | Multiplier | No. Samples | Opt-Level -Oz | Opt-Level -O3 |\\n|----------|----------|-----------|------------|-------------|---------------|---------------|\\n| C++      | clang    | 5.08x     | 2,956,611  | 2,897,477   |               |               |\\n| C        | clang    | 3.26x     | 419,227    | 411,332     |               |               |\\n| Python   | codon    | 11.43x    | 291,011    | 284,676     |               |               |\\n| Rust     | rustc    | 21.36x    | 82,667     | 74,689      |               |               |\\n| Haskell  | ghc      | 16.58x    | 61,483     | 59,378      |               |               |\\n| Go       | gollvm   | 13.87x    | 55,578     | 42,241      |               |               |\\n| Fortran  | flang    | 4.59x     | 35,288     | 31,299      |               |               |\\n| D        | ldc      | 26.11x    | 18,111     | 6,125       |               |               |\\n| Ruby     | crystal  | 6.78x     | 13,949     | 5,787       |               |               |\\n| Nim      | nlvm     | 18.84x    | 2,865      |             |               |               |\\n| Swift    | swiftc   | 8.79x     | 2,179      | 1,354       |               |               |\\n| Obj-C    | clang    | 3.88x     | 403        | 261         |               |               |\\n\\nTable 1: Breakdown of SLTrans across programming languages (with respective compiler frontends).\\n\\n### SLTrans Creation.\\n\\nExtracting LLVM IR from free-form source code in GitHub demands compilable and complete code units, the collection of which comes with several challenges. The proportion of compilable code units in free-form code is abysmally low due to the need for tracking dependencies. Many languages such as C and C++ do not have mature package management systems, which makes following dependency paths across repository boundaries virtually impossible. The problem is further exacerbated by the difficulty of reliably following within-repository file-level dependencies due to aggressive de-duplication of source files during curation of language modelling.\\n\\n7 https://llvm.org/docs/LangRef.html\\n8 https://llvm.org/ProjectsWithLLVM/\\n9 We source Python data via a Codon, which implements a statically typed subset of the Python language specification.\\n10 We source Ruby samples via Crystal \u2014 a statically typed and compiled derivative of the language.\"}"}
{"id": "acl-2024-long-802", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: A high-level overview of our parallel data sourcing and training objective. Each source file is compiled via a corresponding LLVM frontend to obtain human-readable IR. The source and IR are then concatenated, and the model is required to auto-regressively predict the tokens of one with the other in context, thus aligning the constructs in the respective programming language with their analogues in LLVM.\\n\\ncode corpora for performance reasons (Allamanis, 2019; Lee et al., 2022): this mangles the repository structure. Additionally, there are also obstacles to obtaining complete compilation units. Languages such as Rust, Go or Swift simply cannot be compiled at the file level (unless the files are self-contained) as their respective LLVM frontends operate on module or package-level compilation units. As of this writing, multi-file repository-level code language modelling is unsupported by most mainstream code models (Roziere et al., 2023; Li et al., 2023; Nijkamp et al., 2023). As a result, prior attempts at extracting parallel source-IR data have been stymied by the need for language-specific code dependency tracking for successful compilation (Grossman et al., 2023) and thus restricted to function/snippet level code (Szafraniec et al., 2023), which is very limiting in terms of coverage of language constructs (Li et al., 2022).\\n\\nWe sidestep the above issues by sourcing self-contained compilation units from accepted solutions to programming contest problems (Rosetta Code, 2023; Mirzayanov, 2020; Puri et al., 2021; Caballero and Sutskever, 2021), which typically do not have cross-file dependencies. We then compile these source files into two IR flavours: size-optimized (\\\\(-Oz\\\\) opt-level equivalent) IR and performance-optimized (\\\\(-O3\\\\) opt-level equivalent) IR. We further filter only samples with IR shorter than 2500 code lines. The size-optimized IR allows for larger context windows in LLM inference and is also more uniform across languages; being used for deployment, the performance-optimized IR is more prevalent in open-domain code corpora. Collecting both enables fine-grained trading-off between the two during language modelling. Finally, given the abundance of near-duplicates in programming contest solutions, we perform MinHash-based (Broder, 1997) de-duplication. The final dataset, dubbed SLTrans, consists of ca. 4M samples across 12 programming languages, totalling 26.2B tokens. A breakdown of SLTrans is given in Table 1.\\n\\n4 Experimental Setup\\nData Preparation. We leverage LLVM IR to ground matching constructs across heterogeneous languages and facilitate cross-lingual transfer: as header data (along with superfluous platform, vendor, and memory layout information) does not contribute to this goal, we remove it from IR before pairing with source code. We choose the size-optimized IR 80% of the time and performance-optimized IR for 20% of the training samples. Given our computational budget, we could afford to perform continued pretraining on IR-grounded code on approximately 1.5B tokens. Given that (i) this is substantially smaller than the overall size of SLTrans and (ii) acknowledging the skewed language distribution of the dataset, we sub-sample the training corpora using token-level UniMax-1 sampling (Chung et al., 2023), based on the StarCoderBase tokenizer (Li et al., 2023). We select a token budget of 600M tokens this way. We next source 200M tokens of unpaired open-domain IR based on the StarCoderBase tokenizer.\"}"}
{"id": "acl-2024-long-802", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Token counts for the paired, unpaired, and codetext pre-training dataset (StarCoderBase tokenizer).\\n\\n| Dataset            | Code Text | Unpaired | Paired |\\n|--------------------|-----------|----------|--------|\\n| OpenWebMath        | 300M      | 200M     | 100M   |\\n| PeS2o              | 500M      | 300M     | 200M   |\\n| Git Commits        | 200M      | 150M     | 100M   |\\n| TheStack           | 500M      | 400M     | 300M   |\\n| Unpaired IR        |           | 450M     | 200M   |\\n| Source-IR Pairs    |           |          | 600M   |\\n| Total              | 1.5B      | 1.5B     | 1.5B   |\\n\\nThe breakdown of the final dataset, to which we refer with Paired, is given in Table 2.\\n\\nModel Training.\\n\\nAiming for robust findings, we test the effects of IR grounding on six different Code-LMs from three different providers, ranging in size from 1.1B to 7.3B parameters: StarCoderBase (Li et al., 2023) 1.1B, 3.1B, and 7.3B; DeepSeekCoder (Guo et al., 2024) 1.3B and 5.7B; and CodeLlama (Roziere et al., 2023) 6.7B.\\n\\nWe perform continued LM training for each of these models on the Paired dataset built from SLTrans. We introduce two new sentinel tokens\u2014<s2l> and <l2s>\u2014into the models\u2019 vocabulary and use them, respectively, for two possible directions of grounding (each sampled for 50% of training instances):\\n\\n- source_code <s2l> llvm_ir <|EOS|>\\n- llvm_ir <l2s> source_code <|EOS|>\\n\\nWe randomly initialize the sentinel tokens\u2019 embeddings from a Gaussian distribution with the mean set to the average of all pre-trained vocabulary embeddings and retain the variance from the models\u2019 initializer configurations.\\n\\nWe rely on LoRA (Hu et al., 2022) for parameter-efficient continued pre-training (we set r to 256 and an \u03b1 to 128), while keeping the embedding layers trainable. We resort to DeepSpeed (Rasley et al., 2020) Zero Stage-2 to accelerate our training jobs. We train with a maximum sequence length of 4096 tokens using the Adam (Kingma and Ba, 2015) optimizer (\u03b2 values of (0.95, 0.99)) with a base learning rate of $1 \\\\times 10^{-4}$ for the LoRA modules and $4 \\\\times 10^{-5}$ for the embedding layers, employing a cosine schedule (culminates at 10% of the base).\\n\\nTable 3: RQ1: Multipl-E pass@1 all language average performance comparison between different continued pre-training settings.\"}"}
{"id": "acl-2024-long-802", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E runs. While the more common standard is to choose $N = 200$, in the interest of efficiency, we follow existing work (Li et al., 2023) that shows that one can obtain reliable pass@k estimates in as few as 20 generations. We always use nucleus sampling with $p$ of 0.9. Our estimates of pass@1 emulate usage scenarios where correctness is paramount. Hence, we utilize a low temperature of 0.2. In contrast, for our pass@10 and pass@25, we mimic scenarios where creativity and diversity of generations are more important and hence use a higher temperature of 0.8. This practice keeps us in line with prior work (Roziere et al., 2023).\\n\\nComparison of models' performance, displayed in Table 3, brings the key insight: while adding unpaired IR data can bring some performance gains (compare Unpaired against Base and CodeText), these gains are much less pronounced than the gains we obtain by adding paired source-IR data to the training mix (Paired vs. Unpaired). These results suggest that the grounding of heterogeneous source code languages in the same IR\u2014rather than mere exposure to (unaligned) IR\u2014drives most of the performance gains. Comparing CodeText and Base, we see that continued training on the data distribution that is similar to that of original pre-training can hurt performance: most models additionally trained on CodeText exhibit small drops in performance compared to respective Base variants. This observation is in line with prior findings (Cassano et al., 2023a) and is likely the result of degradations caused by repeating data in language modeling (Allamanis, 2019).\\n\\nRQ2: Grounding in IR improves robustness to prompt perturbations.\\n\\nWe next investigate how our source-IR grounding affects the perturbation robustness of Code-LMs. Such robustness is critical, as malformed and adversarial prompts have been shown to successfully lead to the generation of incorrect (Zhou et al., 2022) and insecure code (Dinh et al., 2023; Wu et al., 2023). Our intuition is that grounding in IR should reduce the vulnerability of Code-LMs to such perturbations, as IR is the result of several transformations that tend to remove the effects of minor semantic variances or even mistakes in the source code. We test our hypothesis using 5 differently seeded ReCode (Wang et al., 2023) transformations of HumanEval to measure robustness to three classes of perturbations in Python: code formatting, syntactic variation, and function name mangling. We run three of the four ReCode evaluations using HumanEval as the base dataset. The Format sub-task scrutinizes how robust these models are to source formatting variations, such as turning docstrings into comments and randomly inserting newlines. The Syntax sub-task tests models' susceptibility to syntactic variation patterns common in human-written code (Chakraborty et al., 2022) such as dead-code blocks and renamed variables. Finally, the Function sub-task tests models' robustness to conventional variations in function names such as inflectional variations or synonym substitutions. We follow the benchmark authors' guidance and estimate pass@1 from one greedily sampled output of at most 1024 tokens per prompt. As evidenced by the detailed results in Table 4, IRCoder displays gains across the board, with particularly significant gains in robustness against syntactic variations that are typical for human-written prompts. Interestingly, the gains for robustness to function header mangling are substantially smaller. We believe that this is the artifact of the benchmark, abundant with prompts that include headers and docstrings, which may underestimate the functional robustness of IR grounding \u201cin the wild\u201d.\\n\\nRQ3: IR grounding improves multilingual code understanding.\\n\\nWe next test the multilingual code completion and understanding capabilities of the models after IR grounding, both in zero-shot and fine-tuning setups. For completion, we reproduce the results of Li et al. (2023) and follow the same guidelines as in previous experiments.\"}"}
{"id": "acl-2024-long-802", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: RQ3 and RQ4: All language average performance comparison between IRCoder and the corresponding base models on multilingual tasks. For detailed language-wise breakdowns in Multipl-E results refer to Tables 9 to 11, CodeXGLUE code to text results refer to Table 12, Commit Chronicle results refer to Tables 13 and 14, and HumanEvalFixDocs results refer to Tables 15 and 16 in the Appendix.\\n\\nPort performance on Multipl-E in terms of pass@1, pass@10, and pass@25. We test zero-shot code understanding on CodeXGLUE (Lu et al., 2021) docstring generation task, which requires models to generate a docstring description given the function code as the prompt. We greedily sample continuations capped at 512 tokens and measure the performance with Smoothed BLEU-4 (Lin and Och, 2004) scores w.r.t. the reference docstrings for the languages present in SLTrans: Python, Ruby, and Go.\\n\\nRegarding fine-tuning, we benchmark on the Commit Chronicle (Eliseeva et al., 2023) commit message generation task. For the 8 languages present in SLTrans\u2014C, C++, Go, Objective-C, Python, Ruby, Rust, and Swift\u2014we fine-tune the IR-grounded Code-LLMs and report the performance in terms of ROUGE-2 and ROUGE-L against the reference commit messages. We randomly partition 80%, 10% and 10% of the data into train, validation, and test splits for the 8 languages present in SLTrans\u2014C, C++, Go, Objective-C, Python, Ruby, Rust, and Swift. For languages with a lot of diff samples, we cap the train split at 25,000 samples. We train for 3 epochs with a maximum sequence length of 2048 tokens, using LoRA tuning with an r of 32, \u03b1 of 16, and a batch size of 16. We use the ADAM optimizer with \u03b21 of (0.95, 0.99) and a base learning rate of 3e-4. We adopt a cosine scheduler that finishes at 10% of the base learning rate. Unlike continued pre-training, losses are only backpropagated for the continuations in this phase.\\n\\nResults in Table 5 show that IRCoder significantly and consistently outperforms the base LLMs on all multilingual benchmarks. The language-level breakdown of results (in Appendix A), suggests that grounding in IR facilitates cross-lingual transfer since we observe substantial improvements for low-resource languages.\\n\\nThe results warrant one further point of discussion. Our findings are in contrast with the findings of Orlanski et al. (2023) who show a trade-off between the performance on high and low-resource languages: we, instead, observe gains across the board with no evidence of interference even between typologically disparate programming languages. We find that the IR-grounding also substantially boosts performance on high-resource languages like C++ and Python for which the Code-LLMs have seen hundreds of billions of tokens in pre-training. This contributes to the hypothesis that, despite their large-scale pre-training, Code-LMs gain a limited understanding of higher-level concepts such as control and data flow (Hooda et al., 2024), instead resorting to superficial attributes such as identifier names for anchoring representations across languages (Ahmed and Devanbu, 2022). IR, instead, quite intuitively, does have the potential to align code representations over such concepts. For example, the single-static assignment (SSA) form used by LLVM alongside trans-\"}"}
{"id": "acl-2024-long-802", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"formations such as loop vectorization and register\\nallocation specifies the data flow explicitly; other\\nmodifications captured by IR, such as loop simpli-\\nfication, also aid in simplifying the control flow of\\nsource code, thus aiding code understanding.\\n\\nRQ4: Grounding in IR improves multilingual\\ninstruction following. Finally, we test if the\\nimprovements from IR grounding extend to in-\\nstruction following. To this end, we perform 3\\nepochs of instruction tuning on 23.5k instruction-\\noutput pairs. We collate 18k instruction-output\\npairs from EditPackFT (Cassano et al., 2023b), de-\\nrived by re-formatting the file contents and commit\\nmessages of single-file edit GitHub commits. In\\nthe interest of preserving natural language abil-\\nity, we also source a further 5.5k code-adjacent\\nnatural language instruction-output pairs from the\\nOASST (K\u00f6pf et al., 2023) and OpenOrca (Lian\\net al., 2023) collections. We perform 3 epochs\\nof instruction tuning on all the base and\\nIRCoder models with a maximum sequence length of 2048\\nand backpropagate losses on only the continuations.\\nWe leverage LoRA tuning with an\\n$r$ of 32, $\\\\alpha$ of 16, and a batch size of 16. We use the ADAM op-\\ntimizer with $\\\\beta$ of (0.95, 0.99) and a base learning\\nrate of 3e-4. We employ a cosine scheduler that\\nfinishes at 10% of the base learning rate.\\n\\nWe evaluate instruction following on the Hu-\\nmanEvalFixDocs (Muennighoff et al., 2023) task.\\nThe task instructs the model to fix buggy code snip-\\npets given the docstring of the correct sub-routine\\nand tests the models' ability to correct faults such as\\nidentifier and operator misuse as well as missing or\\nexcess logic. We evaluate for\\nSLTrans languages: C++, Go, Python, and Rust. We benchmark the\\ninstruction following ability of our models using\\npass@1 at temperature 0.2 and\\npass@10 at temper-\\nature 0.8 by sampling 20 continuations of at most\\n1024 tokens. Again, the first setting is designed\\nto mimic factual generations, and the second is to\\nrecreate more creative settings. The task consists\\nof the buggy code followed by the correct docstring\\nand an accompanying instruction to fix the code\\nsnippet. Table 5 shows again that IR grounding\\nbrings performance gains, with the largest improve-\\nments observed for the strongest Code-LMs. This\\nis consistent with existing work which shows that\\nthe benefits of instruction tuning are most apparent\\nfor strong base models (Muennighoff et al., 2023;\\nLongpre et al., 2023).\\n\\n6 Conclusion\\nIn this work, we investigated the effects of ground-\\ning heterogeneous source-code to a shared inter-\\nmediate representation (IR) on code understanding\\nand generation abilities of Code-LMs. To this end,\\nwe first create SLTrans, a 26.2B token source code-\\nIR parallel dataset containing nearly 4M training\\nexamples. We then perform continued pretraining\\non the corpus that includes parallel source code-IR\\ndata from SLTrans for 6 established Code-LMs,\\ndemonstrating that IR grounding brings substantial\\nperformance gains in prompt robustness, multilin-\\ngual code completion, code understanding, and\\ninstruction following, all while training on data\\norders of magnitude smaller than Code-LM pre-\\ntraining corpora. We hope that our encouraging\\nresults catalyze broader research efforts on the in-\\nclusion of intermediate code representations in pre-\\ntraining and in the post-hoc adaptation of code\\nlanguage models.\\n\\nLimitations\\nWe show that compiler IR is a powerful source of\\ncross-lingual alignment that allows for the struc-\\ntures in various languages to be anchored in com-\\nmon IR constructs. However, this is by no means a\\nperfect process. Different frontends make disparate\\nchoices regarding how source code must be trans-\\nformed to IR leading to several 'dialects' of IR that\\nare all valid but may slightly differ. While this does\\nnot seem to get in the way of our gains, it might\\nhave an effect when our approach is extended to\\nnewer languages with less mature toolchains.\\nAdditionally, while the middle-end LLVM IR is\\nintended to be a target-platform agnostic represen-\\ntation, this constraint can sometimes be violated\\ndue to the presence of platform-specific constants,\\napplication binary interface code, and linker logic.\\nFor our purposes, this was worked around by some\\ndata cleaning and by sourcing the IR consistently\\nfrom the same platform.\\nThirdly, there is a risk that the IR may not be\\nable to anchor all the constructs of a language.\\nWhile in some languages like C and C++, there is a\\nstrong mapping between the language constructs\\nand LLVM ones, in others the association might be\\nless tight. For instance, in Rust, the source code is\\nfirst transformed to the language's own IR\\nbefore\\nthe LLVM framework is used. Our results indicate\\n16https://rustc-dev-guide.rust-lang.org/mir/\\nindex.html\"}"}
{"id": "acl-2024-long-802", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that this hasn't gotten in the way so far.\\n\\nFinally, due to the IR being, on average, several times longer than the source code, there arise constraints on the types of models to which our approach can be applied. Most competitive Code-LMs have a context window of at least 4096 tokens making this largely a non-issue. However, it might pose problems in applying this method to older Code-LMs.\\n\\nEthical Risks\\n\\nOur work does not directly interface with any human annotators, with our data collection and evaluation being completely automated. However, the risk of our improved model being more competent at generating malicious code cannot be ruled out. This is a prospect we haven't explicitly evaluated for. We take mitigating steps by releasing the Docker containers used in our training and evaluation jobs, to minimize the risks to downstream users employing our models and methods.\\n\\nAcknowledgements\\n\\nThis work has been funded by Huawei Technologies (Ireland) Co., Ltd. Additionally, it has also been supported by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE.\\n\\nReferences\\n\\nToufique Ahmed and Premkumar Devanbu. 2022. Multilingual training for software engineering. In Proceedings of the 44th International Conference on Software Engineering, pages 1443\u20131455.\\n\\nMiltiadis Allamanis. 2019. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, pages 143\u2013153.\\n\\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vuli\u0107. 2022. Composable sparse fine-tuning for cross-lingual transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1778\u20131796, Dublin, Ireland. Association for Computational Linguistics.\\n\\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. 2019. Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv preprint 1907.05019.\\n\\nMikel Artetxe and Holger Schwenk. 2019. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics, 7:597\u2013610.\\n\\nBen Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, and Ramesh Nallapati. 2023. Multilingual evaluation of code generation models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint 2108.07732.\\n\\nMehdi Bahrami, NC Shrikanth, Yuji Mizobuchi, Lei Liu, Masahiro Fukuyori, Wei-Peng Chen, and Kazuki Munakata. 2021. Augmentedcode: Examining the effects of natural language resources in code retrieval models. arXiv preprint 2110.08512.\\n\\nTerra Blevins, Tomasz Limisiewicz, Suchin Gururangan, Margaret Li, Hila Gonen, Noah A Smith, and Luke Zettlemoyer. 2024. Breaking the curse of multilinguality with cross-lingual expert language models. arXiv preprint 2401.10440.\\n\\nAlexander Brauckmann, Andr\u00e9s Goens, Sebastian Ertel, and Jeronimo Castrillon. 2020. Compiler-based graph representations for deep learning models of code. In Proceedings of the 29th International Conference on Compiler Construction, pages 201\u2013211.\\n\\nMarc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr Polozov. 2019. Generative code modeling with graphs. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\\n\\nAndrei Z Broder. 1997. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pages 21\u201329. IEEE.\\n\\nEthan Caballero and Ilya Sutskever. 2021. Description2code dataset.\\n\\nFederico Cassano, John Gouwar, Francesca Lucchetti, Claire Schlesinger, Carolyn Jane Anderson, Michael Greenberg, Abhinav Jangda, and Arjun Guha. 2023a. Knowledge transfer from high-resource to low-resource programming languages for code llms. Federico Cassano, John Gouwar, Daniel Nguyen, Syndney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson.\"}"}
{"id": "acl-2024-long-802", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-802", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-802", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Loganesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muh-tasham Oblokulov, Zhiruo Wang, Rudra Murthy, Janson T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, and Harm de Vries. 2023. Starcoder: may the source be with you! Transactions on Machine Learning Research. Reproducibility Certification.\\n\\nZongjie Li, Pingchuan Ma, Huaijin Wang, Shuai Wang, Qiyi Tang, Sen Nie, and Shi Wu. 2022. Unleashing the power of compiler intermediate representation to enhance neural program embeddings. In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022, pages 2253\u20132265. ACM.\\n\\nWing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Von Wong, and \u201cTeknium\u201d. 2023. Openorca: An open dataset of gpt augmented flan reasoning traces. https://huggingface.co/Open-Orca/OpenOrca.\\n\\nChin-Yew Lin and Franz Josef Och. 2004. ORANGE: a method for evaluating automatic evaluation metrics for machine translation. In COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 501\u2013507, Geneva, Switzerland. COLING.\\n\\nJiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023. Rltf: Reinforcement learning from unit test feedback. arXiv preprint 2307.04349.\\n\\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 22631\u201322648. PMLR.\\n\\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\n\\nYingwei Ma, Yue Yu, Shanshan Li, Zhouyang Jia, Jun Ma, Rulin Xu, Wei Dong, and Xiangke Liao. 2023. Mulcs: Towards a unified deep representation for multilingual code search. In IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2023, Taipa, Macao, March 21-24, 2023, pages 120\u2013131. IEEE.\\n\\nMike Mirzayanov. 2020. Codeforces: Results of 2020 [annual report]. https://codeforces.com/blog/entry/89502.\\n\\nLili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional neural networks over tree structures for programming language processing. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pages 1287\u20131293. AAAI Press.\\n\\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. 2023. Octopack: Instruction tuning code large language models. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following.\\n\\nAravind Ashok Nair, Avijit Roy, and Karl Meinke. 2020. funcgnn: A graph neural network approach to program similarity. In ESEM \u201920: ACM / IEEE International Symposium on Empirical Software Engineering and Measurement, Bari, Italy, October 5-7, 2020, pages 10:1\u201310:11. ACM.\\n\\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\n\\nGabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui, Joshua Howland, Jonathan Malmaud, Jacob Austin, Rishabh Singh, and Michele Catasta. 2023. Measuring the impact of programming language distribution. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 26619\u201326645. PMLR.\\n\\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2023. Openwebmath: An open dataset of high-quality mathematical web text. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23.\\n\\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. 2022. Lifting the curse of multilinguality by pre-training modular transformers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3479\u20133495, Seattle, United States. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-802", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Weiguo Pian, Hanyu Peng, Xunzhu Tang, Tiezhu Sun, Haoye Tian, Andrew Habib, Jacques Klein, and Tegawend\u00e9 F. Bissyand\u00e9. 2023. Metatptrans: A meta-learning approach for multilingual code representation learning. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 5239\u20135247. AAAI Press.\\n\\nRuchir Puri, David S. Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir R. Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss. 2021. Codenet: A large-scale AI for code dataset for learning a diversity of coding tasks. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\n\\nErwin Quiring, Alwin Maier, and Konrad Rieck. 2019. Misleading authorship attribution of source code using adversarial learning. In 28th USENIX Security Symposium, USENIX Security 2019, Santa Clara, CA, USA, August 14-16, 2019, pages 479\u2013496. USENIX Association.\\n\\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 3505\u20133506. ACM.\\n\\nRosetta Code. 2023. Rosetta code.\\n\\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint 2308.12950.\\n\\nBaptiste Rozi\u00e8re, Jie Zhang, Fran\u00e7ois Charton, Mark Harman, Gabriel Synnaeve, and Guillaume Lample. 2022. Leveraging automated unit tests for unsupervised code translation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\n\\nParshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K. Reddy. 2023. Execution-based code generation using deep reinforcement learning. Transactions on Machine Learning Research.\\n\\nLuca Soldaini and Kyle Lo. 2023. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI. ODC-By, https://github.com/allenai/pes2o.\\n\\nZeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. 2020. Treegen: A tree-based transformer architecture for code generation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8984\u20138991. AAAI Press.\\n\\nMarc Szafraniec, Baptiste Rozi\u00e8re, Hugh Leather, Patrick Labatut, Fran\u00e7ois Charton, and Gabriel Synnaeve. 2023. Code translation with compiler representations. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\n\\nSindhu Tipirneni, Ming Zhu, and Chandan K. Reddy. 2024. Structcoder: Structure-aware transformer for code generation. ACM Trans. Knowl. Discov. Data, 18(3):70:1\u201370:20.\\n\\nJennifer Tracey, Stephanie Strassel, Ann Bies, Zhiyi Song, Michael Arrigo, Kira Griffitt, Dana Delgado, Dave Graff, Seth Kulick, Justin Mott, and Neil Kuster. 2019. Corpus building for low resource languages in the DARPA LORELEI program. In Proceedings of the 2nd Workshop on Technologies for MT of Low Resource Languages, pages 48\u201355, Dublin, Ireland. European Association for Machine Translation.\\n\\nShiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali Krishna Ramanathan, Dan Roth, and Bing Xiang. 2023. ReCode: Robustness evaluation of code generation models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13818\u201313843, Toronto, Canada. Association for Computational Linguistics.\\n\\nZirui Wang, Zachary C. Lipton, and Yulia Tsvetkov. 2020. On negative interference in multilingual models: Findings and a meta-learning treatment. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4438\u20134450, Online. Association for Computational Linguistics.\\n\\nFangzhou Wu, Xiaogeng Liu, and Chaowei Xiao. 2023. Deceptprompt: Exploiting llm-driven code generation via adversarial natural language instructions. arXiv preprint 2312.04730.\\n\\nShijie Wu and Mark Dredze. 2020. Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120\u2013130, Online. Association for Computational Linguistics.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-802", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For completeness, we detail the split and language-wise performance of the models on all tasks discussed in Section 5.\"}"}
{"id": "acl-2024-long-802", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 6: ReCode Format\\n\\n| Model                   | StarCoderBase 1.1B | DeepSeekCoder 1.3B | StarCoderBase 3.1B | DeepSeekCoder 5.7B | CodeLlama 6.7B | StarCoderBase 7.3B |\\n|-------------------------|--------------------|--------------------|--------------------|--------------------|---------------|--------------------|\\n| pass@1                  | 23.40              | 45.43              | 34.78              | 50.12              | 50.11         | 40.11              |\\n| comparison              | 29.87              | 50.98              | 39.63              | 66.38              | 56.09         | 48.59              |\\n| between               | 31.70              | 50.60              | 40.85              | 64.63              | 55.87         | 47.88              |\\n| IRCoder                | 27.42              | 44.18              | 37.80              | 64.67              | 54.43         | 44.89              |\\n| and its               | 27.43              | 53.04              | 40.11              | 66.98              | 52.58         | 47.53              |\\n| corresponding          | 28.65              |                    |                    |                    |               |                    |\\n| base models.             |                    |                    |                    |                    |               |                    |\\n\\n### Table 7: ReCode Syntax\\n\\n| Model                   | StarCoderBase 1.1B | DeepSeekCoder 1.3B | StarCoderBase 3.1B | DeepSeekCoder 5.7B | CodeLlama 6.7B | StarCoderBase 7.3B |\\n|-------------------------|--------------------|--------------------|--------------------|--------------------|---------------|--------------------|\\n| pass@1                  | 8.53               | 17.64              | 14.02              | 23.48              | 16.94         | 14.96              |\\n| comparison              | 32.93              | 52.43              | 38.41              | 64.35              | 52.95         | 50.31              |\\n| between               | 29.14              | 50.52              | 39.63              | 61.63              | 51.97         | 50.31              |\\n| IRCoder                | 31.70              | 50.52              | 38.11              | 64.02              | 54.26         | 45.63              |\\n| and its               | 29.87              | 50.60              | 37.81              | 60.52              | 51.78         | 45.12              |\\n| corresponding          | 26.18              | 47.56              | 31.78              | 58.56              | 43.47         | 42.68              |\\n| base models.             |                    |                    |                    |                    |               |                    |\\n\\n### Table 8: ReCode Function\\n\\n| Model                   | StarCoderBase 1.1B | DeepSeekCoder 1.3B | StarCoderBase 3.1B | DeepSeekCoder 5.7B | CodeLlama 6.7B | StarCoderBase 7.3B |\\n|-------------------------|--------------------|--------------------|--------------------|--------------------|---------------|--------------------|\\n| pass@1                  | 10.24              | 26.99              | 20.87              | 40.24              | 27.44         | 26.33              |\\n| comparison              | 11.44              | 23.93              | 18.02              | 34.32              | 25.11         | 25.02              |\\n| between               | 13.14              | 26.12              | 19.42              | 38.47              | 24.41         | 22.17              |\\n| IRCoder                | 10.33              | 19.65              | 17.07              | 30.26              | 21.08         | 19.68              |\\n| and its               | 12.44              | 25.67              | 20.32              | 40.33              | 23.79         | 25.39              |\\n| corresponding          | 10.26              | 28.41              | 18.56              | 36.76              | 25.09         | 22.56              |\\n| base models.             |                    |                    |                    |                    |               |                    |\\n\\n### Table 15038\\n\\n| Model | Doc to Newline |\\n|-------|----------------|\\n|       |                |\"}"}
{"id": "acl-2024-long-802", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model              | C++  | D    | Go   | Python | Ruby  | Rust  | Swift |\\n|--------------------|------|------|------|--------|-------|-------|-------|\\n| StarCoderBase 1.1B| 10.22| 3.87 | 12.79| 14.26  | 4.46  | 9.21  | 3.64  |\\n| DeepSeekCoder 1.3B| 28.21| 9.77 | 15.87| 27.91  | 21.21 | 16.46 | 8.94  |\\n| StarCoderBase 3.1B| 16.64| 4.89 | 15.63| 21.51  | 4.52  | 16.31 | 9.98  |\\n| DeepSeekCoder 5.7B| 43.44| 13.65| 24.64| 42.67  | 33.43 | 31.79 | 23.79 |\\n| CodeLlama 6.7B    | 26.72| 9.67 | 18.69| 31.13  | 25.28 | 21.43 | 19.87 |\\n| StarCoderBase 7.3B| 23.19| 7.62 | 16.76| 27.88  | 16.96 | 18.81 | 14.38 |\\n| IRCoder 1.1B     | 11.10| 4.65 | 11.78| 14.29  | 6.34  | 9.62  | 3.76  |\\n| IRCoder 1.3B     | 31.79| 10.57| 16.17| 30.61  | 24.35 | 20.91 | 9.14  |\\n| IRCoder 3.1B     | 16.87| 5.67 | 17.78| 21.98  | 11.46 | 16.78 | 9.96  |\\n| IRCoder 5.7B     | 45.61| 15.96| 23.77| 42.92  | 34.60 | 33.94 | 21.17 |\\n| IRCoder 6.7B     | 29.12| 13.02| 19.10| 31.11  | 26.28 | 24.37 | 25.45 |\\n| IRCoder 7.3B     | 23.06| 11.97| 16.81| 25.24  | 19.52 | 19.63 | 12.99 |\\n\\nTable 9: Multipl-E pass@1 comparison between IRCoder and its corresponding base models.\\n\\n| Model              | C++  | D    | Go   | Python | Ruby  | Rust  | Swift |\\n|--------------------|------|------|------|--------|-------|-------|-------|\\n| StarCoderBase 1.1B| 28.19| 12.78| 19.22| 23.04  | 7.65  | 13.45 | 10.69 |\\n| DeepSeekCoder 1.3B| 38.44| 20.71| 22.79| 50.68  | 43.26 | 20.13 | 23.49 |\\n| StarCoderBase 3.1B| 27.38| 13.98| 20.87| 33.89  | 14.59 | 24.97 | 22.68 |\\n| DeepSeekCoder 5.7B| 58.59| 28.32| 31.11| 69.14  | 53.76 | 50.84 | 44.55 |\\n| CodeLlama 6.7B    | 57.65| 22.86| 25.73| 62.43  | 55.96 | 31.95 | 40.93 |\\n| StarCoderBase 7.3B| 40.28| 20.93| 22.14| 53.44  | 44.85 | 30.80 | 26.41 |\\n| IRCoder 1.1B     | 29.79| 13.45| 20.02| 28.43  | 18.79 | 14.31 | 10.43 |\\n| IRCoder 1.3B     | 40.55| 31.89| 23.47| 57.84  | 52.46 | 28.63 | 30.44 |\\n| IRCoder 3.1B     | 35.18| 14.77| 23.59| 46.19  | 28.72 | 24.12 | 23.55 |\\n| IRCoder 5.7B     | 62.16| 33.79| 33.86| 73.41  | 55.08 | 51.69 | 49.04 |\\n| IRCoder 6.7B     | 60.08| 35.27| 26.31| 67.49  | 59.88 | 35.39 | 44.77 |\\n| IRCoder 7.3B     | 47.87| 27.67| 21.87| 56.47  | 49.35 | 30.92 | 32.12 |\\n\\nTable 10: Multipl-E pass@10 comparison between IRCoder and its corresponding base models.\\n\\n| Model              | C++  | D    | Go   | Python | Ruby  | Rust  | Swift |\\n|--------------------|------|------|------|--------|-------|-------|-------|\\n| StarCoderBase 1.1B| 28.19| 12.78| 19.22| 23.04  | 7.65  | 13.45 | 10.69 |\\n| DeepSeekCoder 1.3B| 38.44| 20.71| 22.79| 50.68  | 43.26 | 20.13 | 23.49 |\\n| StarCoderBase 3.1B| 27.38| 13.98| 20.87| 33.89  | 14.59 | 24.97 | 22.68 |\\n| DeepSeekCoder 5.7B| 58.59| 28.32| 31.11| 69.14  | 53.76 | 50.84 | 44.55 |\\n| CodeLlama 6.7B    | 57.65| 22.86| 25.73| 62.43  | 55.96 | 31.95 | 40.93 |\\n| StarCoderBase 7.3B| 40.28| 20.93| 22.14| 53.44  | 44.85 | 30.80 | 26.41 |\\n| IRCoder 1.1B     | 29.79| 13.45| 20.02| 28.43  | 18.79 | 14.31 | 10.43 |\\n| IRCoder 1.3B     | 40.55| 31.89| 23.47| 57.84  | 52.46 | 28.63 | 30.44 |\\n| IRCoder 3.1B     | 35.18| 14.77| 23.59| 46.19  | 28.72 | 24.12 | 23.55 |\\n| IRCoder 5.7B     | 62.16| 33.79| 33.86| 73.41  | 55.08 | 51.69 | 49.04 |\\n| IRCoder 6.7B     | 60.08| 35.27| 26.31| 67.49  | 59.88 | 35.39 | 44.77 |\\n| IRCoder 7.3B     | 47.87| 27.67| 21.87| 56.47  | 49.35 | 30.92 | 32.12 |\\n\\nTable 11: Multipl-E pass@25 comparison between IRCoder and its corresponding base models.\"}"}
{"id": "acl-2024-long-802", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model               | C   | C++ | Go   | Obj-C | Python | Ruby | Rust | Swift |\\n|---------------------|-----|-----|------|-------|--------|------|------|-------|\\n| StarCoderBase 1.1B  | 11.76 | 32.56 | 32.32 |\\n| DeepSeekCoder 1.3B  | 31.87 | 33.98 | 30.77 |\\n| StarCoderBase 3.1B  | 32.76 | 36.73 | 33.06 |\\n| DeepSeekCoder 5.7B  | 32.61 | 35.48 | 33.16 |\\n| CodeLlama 6.7B      | 35.02 | 36.82 | 33.83 |\\n| StarCoderBase 7.3B  | 35.93 | 37.24 | 35.09 |\\n| IRCoder 1.1B       | 32.67 | 34.79 | 33.63 |\\n| IRCoder 1.3B       | 33.30 | 35.11 | 32.14 |\\n| IRCoder 3.1B       | 35.29 | 36.90 | 33.76 |\\n| IRCoder 5.7B       | 36.15 | 37.44 | 34.33 |\\n| IRCoder 6.7B       | 35.94 | 38.11 | 41.17 |\\n| IRCoder 7.3B       | 37.52 | 38.96 | 35.82 |\\n\\nTable 12: CodeXGLUE code-to-text smoothed BLEU-4 comparison between IRCoder and its corresponding base models.\\n\\n| Model               | C   | C++ | Go   | Obj-C | Python | Ruby | Rust | Swift |\\n|---------------------|-----|-----|------|-------|--------|------|------|-------|\\n| StarCoderBase 1.1B  | 29.78 | 35.63 | 32.32 |\\n| DeepSeekCoder 1.3B  | 31.87 | 33.98 | 30.77 |\\n| StarCoderBase 3.1B  | 32.76 | 36.73 | 33.06 |\\n| DeepSeekCoder 5.7B  | 32.61 | 35.48 | 33.16 |\\n| CodeLlama 6.7B      | 35.02 | 36.82 | 33.83 |\\n| StarCoderBase 7.3B  | 35.93 | 37.24 | 35.09 |\\n| IRCoder 1.1B       | 32.67 | 34.79 | 33.63 |\\n| IRCoder 1.3B       | 33.30 | 35.11 | 32.14 |\\n| IRCoder 3.1B       | 35.29 | 36.90 | 33.76 |\\n| IRCoder 5.7B       | 36.15 | 37.44 | 34.33 |\\n| IRCoder 6.7B       | 35.94 | 38.11 | 41.17 |\\n| IRCoder 7.3B       | 37.52 | 38.96 | 35.82 |\\n\\nTable 13: CommitChronicle ROUGE-2 comparison between IRCoder and its corresponding base models.\\n\\n| Model               | C   | C++ | Go   | Obj-C | Python | Ruby | Rust | Swift |\\n|---------------------|-----|-----|------|-------|--------|------|------|-------|\\n| StarCoderBase 1.1B  | 29.78 | 35.63 | 32.32 |\\n| DeepSeekCoder 1.3B  | 31.87 | 33.98 | 30.77 |\\n| StarCoderBase 3.1B  | 32.76 | 36.73 | 33.06 |\\n| DeepSeekCoder 5.7B  | 32.61 | 35.48 | 33.16 |\\n| CodeLlama 6.7B      | 35.02 | 36.82 | 33.83 |\\n| StarCoderBase 7.3B  | 35.93 | 37.24 | 35.09 |\\n| IRCoder 1.1B       | 32.67 | 34.79 | 33.63 |\\n| IRCoder 1.3B       | 33.30 | 35.11 | 32.14 |\\n| IRCoder 3.1B       | 35.29 | 36.90 | 33.76 |\\n| IRCoder 5.7B       | 36.15 | 37.44 | 34.33 |\\n| IRCoder 6.7B       | 35.94 | 38.11 | 41.17 |\\n| IRCoder 7.3B       | 37.52 | 38.96 | 35.82 |\\n\\nTable 14: CommitChronicle ROUGE-L comparison between IRCoder and its corresponding base models.\"}"}
{"id": "acl-2024-long-802", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model             | C++   | Go   | Python | Rust  |\\n|-------------------|-------|------|--------|-------|\\n| StarCoderBase 1.1B| 16.91 | 14.48| 26.67  | 12.74 |\\n| DeepSeekCoder 1.3B| 35.49 | 40.78| 42.94  | 25.68 |\\n| StarCoderBase 3.1B| 45.39 | 44.11| 54.67  | 27.16 |\\n| DeepSeekCoder 5.7B| 64.10 | 60.44| 70.59  | 49.07 |\\n| CodeLlama 6.7B    | 61.32 | 58.73| 61.67  | 45.44 |\\n| StarCoderBase 7.3B| 55.94 | 58.36| 59.56  | 47.22 |\\n| IRCoder 1.1B     | 18.16 | 13.99| 26.67  | 13.18 |\\n| IRCoder 1.3B     | 38.41 | 43.11| 43.15  | 27.11 |\\n| IRCoder 3.1B     | 42.06 | 45.17| 55.06  | 25.74 |\\n| IRCoder 5.7B     | 68.22 | 63.47| 73.42  | 56.27 |\\n| IRCoder 6.7B     | 61.87 | 60.59| 62.04  | 50.44 |\\n| IRCoder 7.3B     | 57.35 | 59.33| 60.11  | 52.71 |\\n\\nTable 15: HumanEvalFixDocs pass@1 comparison between IRCoder and its corresponding base models.\\n\\nTable 16: HumanEvalFixDocs pass@10 comparison between IRCoder and its corresponding base models.\"}"}
