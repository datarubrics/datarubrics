{"id": "lrec-2024-main-1147", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Project MOSLA:\\nRecording Every Moment of Second Language Acquisition\\n\\nMasato Hagiwara\\nJoshua Tanner\\nOctanove Labs & Earth Species Project Mantra Inc.\\nSeattle, WA, USA Tokyo, Japan\\nmasato@octanove.com josh@mantra.co.jp\\n\\nAbstract\\nSecond language acquisition (SLA) is a complex and dynamic process. Many SLA studies that have attempted to record and analyze this process have typically focused on a single modality (e.g., textual output of learners), covered only a short period of time, and/or lacked control (e.g., failed to capture every aspect of the learning process). In Project MOSLA (Moments of Second Language Acquisition), we have created a longitudinal, multimodal, multilingual, and controlled dataset by inviting participants to learn one of three target languages (Arabic, Spanish, and Chinese) from scratch over a span of two years, exclusively through online instruction, and recording every lesson using Zoom. The dataset is semi-automatically annotated with speaker/language IDs and transcripts by both human annotators and fine-tuned state-of-the-art speech models. Our experiments reveal linguistic insights into learners' proficiency development over time, as well as the potential for automatically detecting the areas of focus on the screen purely from the unannotated multimodal data. Our dataset is freely available for research purposes and can serve as a valuable resource for a wide range of applications, including but not limited to SLA, proficiency assessment, language and speech processing, pedagogy, and multimodal learning analytics.\\n\\nKeywords: second language acquisition, multimodal learning analytics, speech processing\\n\\n1. Introduction\\nThe acquisition of a second language is a complex and dynamic process characterized by various milestones and challenges that learners encounter along their journey. Many studies have attempted to record the learning process, although most studies are unimodal (e.g., capturing only the textual output of learners, Geertzen et al., 2014), cover only a short period (e.g., containing snapshots of learner's progress, Settles et al., 2018), and/or are limited in control (e.g., not capturing every aspect of the learning process, Stasaski et al., 2020). It has long been recognized that multimodal, longitudinal interaction is a crucial factor in SLA (Hampel and Stickler, 2012).\\n\\nIn order to shed light on the complex and dynamic nature of the SLA process, in Project MOSLA (Moments of Second Language Acquisition), we created a longitudinal, multimodal, multilingual, and controlled dataset by inviting participants to learn a new language from scratch solely through online instruction over a span of two years and documenting every lesson using Zoom. This dataset, comprising over 250 hours of recorded lessons, captures the rich and nuanced aspects of language learning, including verbal and non-verbal communication, the use of teaching materials, student-teacher interactions, and the evolving proficiency of learners. Notably, the MOSLA dataset encompasses a diverse set of target languages\u2014Arabic, Spanish, and Chinese\u2014including two languages that employ non-Latin alphabets, highlighting the dataset's unique cross-linguistic scope.\\n\\n* Longitudinal (~2 years)\\n* Multimodal (video, screen, and audio)\\n* Multilingual (Arabic, Spanish, and Chinese)\\n* Controlled (no external exposure)\\n\\nData Collection\\n\\n| lang: eng | spkr: t | text: Which city is | \u0b67\u10ff |\\n| lang: eng | spkr: s | text: New York |\\n\\nData Annotation\\n\\nMultimodal Analysis\\n\\n* Speaker diarization\\n* Speaker identification\\n* Language identification\\n* Automated speech recognition\\n\\nFigure 1: Overview of Project MOSLA\\n\\nTo enhance the dataset's utility, we semi-automatically annotated all the utterances in the...\"}"}
{"id": "lrec-2024-main-1147", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"recorded audio with start and end offsets, speaker and language IDs, and transcripts. This annotation was accomplished using human annotators and state-of-the-art machine learning models for speaker diarization, speaker and language identification, and automatic speech recognition. The resulting metadata offers valuable insights into the distribution of speech and speaker identities throughout the learning process, as well as transcriptions of spoken content.\\n\\nIn this paper, we provide an overview of the creation, annotation, analysis, and applications of the MOSLA dataset. We begin by describing our data collection method and then discuss the process of human and machine annotation. We empirically demonstrate that fine-tuning state-of-the-art speech models with a small amount of human-annotated data results in substantial improvements in speaker and language identification, as well as speech recognition performance. Additionally, we show that our data can reveal linguistic insights into the learners' acquisition process of the target language, such as the percentage of non-English utterances and lexical diversity. Furthermore, we demonstrate that, through the use of deep neural network models, we can determine where on the screen the teacher and the learner are focusing, solely from the unannotated multimodal video and audio data. The MOSLA dataset represents a significant contribution to the field of SLA research, providing a rich source of data for investigating the factors influencing language learning outcomes, the role of multimodal cues in the acquisition process, and the development of innovative educational tools.\\n\\nThe MOSLA dataset is freely available for research and non-commercial purposes, ensuring that it can benefit the broader academic community and contribute to advancements in the field of second language acquisition. It can be accessed here: https://www.octanove.com/mosla.html.\\n\\n2. Related Work\\n\\nIn the field of SLA, there have been many studies that aimed to record and analyze the learning process, providing valuable insights into language learning. However, many of these studies have limited temporal coverage, typically spanning only several months (Vercellotti, 2015; Saito and Akiyama, 2017). Duolingo publishes the Second Language Acquisition Modeling (SLAM) dataset (Settles et al., 2018), which contains learner production in their target language. However, the data covers only a 30-day period, offering a relatively short-term perspective on language acquisition. The CIMA dataset (Stasaski et al., 2020) contains tutor-learner interaction data during language learning, but it lacks multimodal and longitudinal characteristics. Similarly, the Teacher-Student Chatroom Corpus (Caines et al., 2020) collected textual interactions between teachers and students during online English teaching but also lacks multimodal and longitudinal aspects.\\n\\nIn the realm of grammatical error correction (GEC), there is a substantial body of research (Bryant et al., 2023) but relatively few GEC corpora focus on longitudinal learning. One noteworthy exception is the EFCamDat corpus (Geertzen et al., 2014), one of the largest GEC corpora, with a collection period spanning a few years. However, only a few of its users participated over the entire duration, with many starting or ending their learning outside the collection period.\\n\\nIn other domains of learning analytics, Kubat et al. (2007) collected two years' worth of multimodal data on first language development through the Human Speechome Project (HSP) (Roy et al., 2006), primarily focusing on first language acquisition and involving data from a single individual. Demszky and Hill (2023) collected and analyzed transcripts of teacher-student discourse in elementary math classrooms.\\n\\nMOSLA is closely related to the field of multimodal learning analytics (MMLA) (Mu et al., 2020) and web-based language learning (WBLL) (Cong-Lem, 2018). For example, Donnelly et al. (2017) analyzed classroom audio recordings, and Monkaresi et al. (2017) examined facial expressions as part of the learning analytics process.\\n\\nThe MOSLA dataset holds the potential to be valuable for various applications, including assessment (Settles et al., 2020), proficiency estimation (Vajjala and Rama, 2018), knowledge tracing (Piech et al., 2015), grammatical error correction (Bryant et al., 2023), automated assessment of speaking proficiency (Fan and Yan, 2020), and optimization of pedagogical approaches (Lepper and Woolverton, 2002), among others. Its longitudinal, multimodal nature makes it a unique resource for studying the complexities of the SLA process.\\n\\n3. Data Collection\\n\\nData collection took place between February 2021 and February 2023. The teacher and the learner had weekly language instruction over zoom. Specifically,\\n\\n\u2022 A learner (a complete beginner) and a teacher have a private lesson per week online (e.g., on Zoom) for at least two years.\\n\u2022 Every lesson is recorded (video, audio, and screen share).\\n\u2022 The learner is not allowed to learn the target language outside of these lessons.\"}"}
{"id": "lrec-2024-main-1147", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All the materials the learner is exposed to are recorded (e.g., via screen share). All the learners in this study were already proficient in two or more languages (their L1 and L2, typically English) before the study started and are generally highly motivated individuals. Below is additional information on the individual courses:\\n\\n- **ara**: Arabic (Modern Standard Arabic)\\n  - Teacher L1: Levantine Arabic\\n  - Learner L1: Japanese\\n  - Learner L2s: English, Mandarin Chinese\\n  - Learner Age: 35-44\\n  - Learner Gender: Male\\n\\n- **spa**: Spanish (Latin American)\\n  - Teacher L1: Spanish (Latin American)\\n  - Learner L1: Mandarin Chinese\\n  - Learner L2s: English, Japanese\\n  - Learner Age: 35-44\\n  - Learner Gender: Female\\n\\n- **zho**: Mandarin Chinese\\n  - Teacher L1: Mandarin Chinese\\n  - Learner L1: Spanish (Latin American)\\n  - Learner L2s: English, Italian, German\\n  - Learner Age: 25-34\\n  - Learner Gender: Female\\n\\nAll the teachers have a minimum of five years of professional experience teaching the target language. Additionally, all the participants are fluent in English, and the teaching instructions were conducted in English, at least initially. The study did not impose restrictions on the teaching methods employed by the instructors; they were free to use their preferred approaches. However, instructors were advised not to use copyrighted materials, such as textbooks and online courses, as-is, unless used in a supplementary capacity. As mentioned earlier, learners were prohibited from learning the target language outside of this study and were not assigned any explicit tasks beyond the classroom. Nevertheless, they were encouraged to review the recorded lesson videos for self-assessment purposes.\\n\\nAll the teaching was conducted via Zoom, and the video and audio were recorded using its standard recording functionality under the default settings. Participants used their own preferred devices for recording audio and video, which means that there was no quality control in regard to the devices.\\n\\n### 4. Data Annotation\\n\\nIn addition to the video data for each lesson, MOSLA includes two sets of annotations containing information about the speech of the student and teacher: a smaller human-annotated set, and a complete machine-annotated set. Data from the human-annotated set is used to train machine learning models as shown in Figure 3, which generate a complete set of data for all lessons. We release all models trained this way for use in future research.\\n\\n#### 4.1. Human Annotation\\n\\nWe employ a bilingual annotator for each language pair, such that the annotator speaks both English and the language being learned. Annotation is done on five minute samples, which are selected as follows: we perform an independent random trial with a 5% chance to succeed for each possible sample, and keep up to one sample per lesson.\\n\\nThe first and last segment of each file are excluded from possible selection, as these often consist of technical setup or greetings instead of language education content.\\n\\nWe use Hachiue (Hayashibe, 2021) for annotation, as it provides an easy to use web interface which allows annotators to mark arbitrary sections of the file as utterances and attach data to them. Annotators were instructed to create segments for distinct utterances from each speaker which include a speaker label (teacher, student or other), a label for the dominant language of the utterance (there are a number of code-switched utterances containing multiple languages), and a literal transcription of the speech. An example of segment annotations is shown in Figure 2.\\n\\n#### 4.2. Machine Annotation\\n\\nUsing the human annotation data, we train and evaluate machine learning models to perform each task necessary for annotation: diarization, speaker and language classification, and automatic speech recognition. These models are then combined into a machine annotation pipeline (Figure 3) that we release all models trained this way for use in future research.\\n\\nAs an exception to this, a small number of Chinese lessons are slightly over-annotated.\"}"}
{"id": "lrec-2024-main-1147", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Annotation Data\\n\\n| Language | Duration | Count | Target | Utterance | Language | Student |\\n|----------|----------|-------|--------|----------|----------|---------|\\n| Arabic   | 3.0 hrs  | 2,330 | 82%    | Human    | Arabic   | Machine |\\n|          | 2.6 hrs  |       |        | Machine  | Spanish  |         |\\n| Spanish  | 2.5 hrs  | 1,006 | 85%    | Human    | Spanish  | Machine |\\n|          | 2.2 hrs  |       |        | Machine  | Chinese  |         |\\n| Chinese  | 4.0 hrs  | 4,375 | 66%    | Human    | Chinese  | Machine |\\n|          | 3.3 hrs  |       |        | Machine  |         |         |\\n\\nFigure 2: Example Annotation\\n\\nFigure 3: Overview of the annotation pipeline\\n\\n4.2.1 Diarization\\n\\nWe use Pyannote (Bredin, 2023; Plaquet and Bredin, 2023) for speaker diarization. We experiment with both fine-tuning the speaker embeddings and segmentation model and with fine-tuning the diarization hyperparameters, but ultimately find the default pipeline to be the most performant. Note that because we perform speaker classification as a separate supervised task, we do not actually use the speaker clustering labels from the diarization pipeline, and care only about speech segmentation. This means that we could perform only voice activity detection (VAD) instead of full speaker diarization, but we found that diarization outperformed plain VAD for speech segmentation. Diarization also has the benefit of allowing us to process overlapping utterances from different speakers.\\n\\nSegmentation F1-score for our system can be seen in Table 3. While Pyannote\u2019s diarization model performed well enough to produce useful results, it is also the weakest component of our pipeline, likely due to background noise and fluctuating audio quality in the lesson recordings. We experimented with other diarization models such as those from the NeMo toolkit (Harper et al., 2019), but were unable to find any which outperformed Pyannote.\\n\\n| Language | F1-score VAD Only | F1-score Diarization |\\n|----------|-------------------|----------------------|\\n| Arabic   | 79.6              | 79.0                 |\\n| Spanish  | 69.4              | 66.6                 |\\n| Chinese  | 86.1              | 84.6                 |\\n\\nTable 3: Segmentation F1 score, computed as the harmonic mean of purity and coverage\\n\\nFor details on purity, coverage and Segmentation F1 computation, see the Pyannote metrics documentation.\"}"}
{"id": "lrec-2024-main-1147", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2.2. Utterance Classification\\n\\nWe treat language and speaker identification as supervised classification tasks, where the input is the audio of a single utterance and the output is a label representing the dominant language in the utterance or the speaker of the utterance, respectively.\\n\\nWe primarily experiment with Whisper (Radford et al., 2022) for these tasks, as it is known to perform well not only on automated speech recognition (ASR) but also on related speech and sound detection tasks (Gong et al., 2023). We found that Whisper (the whisper-large-v2 model) performs quite well when fine-tuned on our data. For speaker classification, we remove Whisper\u2019s autoregressive decoder component and replace it with a simple linear classification head with one hidden layer, such that the output of Whisper\u2019s encoder is fed directly into the classifier. As can be seen in Table 4 and the classification row for Table 5, this configuration performs fairly well on our data.\\n\\n| Language | Classification |\\n|----------|----------------|\\n| Arabic   | 90%            |\\n| Spanish  | 92%            |\\n| Chinese  | 95%            |\\n\\nTable 4: Speaker identification accuracy\\n\\nWe try the same classifier configuration for language identification, but find that our best performance comes from using the standard Whisper architecture, including decoder, fine-tuned on our data. That is, we use our ASR model for language identification by taking a single decoding step and selecting the most likely language token representing either English or the target language. We hypothesize that this performance gap exists because Whisper is already trained to output language tokens, and consequently has learned how to perform language identification using parameters in its decoder.\\n\\n| Language | Classification |\\n|----------|----------------|\\n| Arabic   | 46%            |\\n| Spanish  | 59%            |\\n| Chinese  | 76%            |\\n\\n| Language | whisper-large-v2 | ASR fine-tuned |\\n|----------|------------------|----------------|\\n| Arabic   | 95%              | 95%            |\\n| Spanish  | 95%              | 92%            |\\| Chinese  | 92%              | 90%            |\\n\\nTable 5: Language identification accuracy\\n\\n4.2.3. Automatic Speech Recognition\\n\\nWe also use Whisper for automatic speech recognition (ASR), finding once again that fine-tuning on our annotated data substantially improves performance. For both training and evaluation, the input in all cases is a single utterance as annotated by our human annotators, with the annotated speech as gold output labels. Note that we also provide the language of the utterance to the model by forcing the first decoded token to be the language token representing the utterance\u2019s dominant language. We use human-annotated gold language labels when training and evaluating our ASR models. Both classification and ASR models were fine-tuned for three epochs with a batch size of eight and a learning rate of $1 \\\\times 10^{-6}$, using the cross-entropy loss. We measure ASR performance with character error rate (CER), in part because there is no standard way to calculate word error rate (WER) for languages without spaces like Chinese. Character error rate can be thought of as a measurement of the edit distance between the output of the model and the reference transcription. That is, given a reference of length $N$ characters and model output which can be transformed into this reference with $S$ substitutions, $D$ deletions and $I$ insertions, CER is computed as:\\n\\n$$\\\\text{CER} = S + D + I / N \\\\quad (1)$$\\n\\nASR model performance can be seen in Table 6. Fine-tuning the model improves performance on all languages, but most dramatically for Arabic and Chinese, where the error rates after fine-tuning are nearly half of the original. We speculate that Whisper may have benefited more from fine-tuning in these two languages because it was weaker in them to begin with: Whisper\u2019s reported ASR performance on Arabic and Chinese was substantially worse than Spanish in the original work (Radford et al., 2022).\\n\\n| Language | whisper-large-v2 | ASR fine-tuned |\\n|----------|------------------|----------------|\\n| Arabic   | 60%              | 25%            |\\n| Spanish  | 33%              | 28%            |\\n| Chinese  | 32%              | 17%            |\\n\\nTable 6: CER on each language for ASR models. Punctuation and Arabic diacritics are excluded for all CER computation.\\n\\n4.2.4. Pipeline Scoring & Error Propagation\\n\\nScores in the previous sections are computed by comparing model outputs to human outputs for each human-annotated utterance. However, when running the machine annotation pipeline there is no guarantee that output from diarization or other steps will be correct, and consequently we can expect some degree of error propagation to later tasks in the pipeline. In particular, errors in speech segmentation are potentially damaging to all other tasks, and errors in language classification could lead to worse ASR output because the utterance language is used to bias the ASR model\u2019s output.\"}"}
{"id": "lrec-2024-main-1147", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Because diarization output will not line up perfectly with human annotated utterances, we compute metrics per five minute human-annotated segment instead of per utterance in order to accurately gauge the performance of our pipeline. CER is computed by concatenating the speech in all utterances output by the pipeline and comparing it to the concatenation of all human-annotated utterance text. For speaker and language identification, we compute the identification error rate (IER) for each. IER can be thought of as a measurement of the percentage of the total duration that is classified incorrectly in some way, and is calculated as:\\n\\n$$\\\\text{IER} = \\\\text{f} + \\\\text{m} + \\\\text{c}$$\\n\\nWhere \\\\(f\\\\) is the duration of false positives (non-speech incorrectly identified as speech), \\\\(m\\\\) is the duration of missed speech (speech incorrectly identified as non-speech), \\\\(c\\\\) is the duration of correctly identified speech assigned the wrong classification label, and \\\\(t\\\\) is the total duration. Note that because \\\\(f\\\\) and \\\\(m\\\\) both depend exclusively on the performance of the model identifying speech, IER is particularly sensitive to diarization performance.\\n\\n### Table 7: Error rates for pipeline components: CER for ASR and IER for classification\\n\\n|           | Gold Seg 5% | 10% | 4% |\\n|-----------|-------------|-----|-----|\\n| ASR       | 23%         | 27% | 16% |\\n| Pipeline  | 24%         | 27% | 17% |\\n| Lang ID   | 4%          | 3%  | 5%  |\\n| Pipeline  | 24%         | 23% | 21% |\\n\\nTable 7: Error rates for pipeline components: CER for ASR and IER for classification.\\n\\nIn Table 7, we present the performance of our pipeline components using human-annotated gold speech segmentation, and pipeline diarization. We also include ASR with gold speech segmentation but pipeline language identification. As we can see from these results, errors in diarization have a substantial effect on the performance of downstream tasks. Precise start and end times for utterances are arguably not necessary for downstream analysis focused on speech content, suggesting that the increase in IER for classification tasks may not matter in some cases, but errors in diarization solely lead to an average increase in CER of approximately 10% for ASR. We leave speech segmentation approaches which are more resilient to issues such as variable audio quality to future work.\\n\\n### 5. Experiments\\n\\n#### 5.1. Linguistic Analysis\\n\\nTo demonstrate the kind of analysis that our data can be used for, we compute summary statistics to track changes in the learner and teacher's speech over time. We use a mix of human and machine-annotated data for this, using human data where available and machine-annotated data otherwise.\\n\\nWe begin by examining the percentage of utterances made in the target language by both the teacher and student in each lesson. This is important both because listening and speaking practice are critical to language acquisition, and because for students the degree of target language use in a learning context has been linked to proficiency in that language (Turnbull and Dailey-O'Cain, 2009; Carranza, 1995). We find that the percentage of target language utterances consistently increases over time for both the student and teacher: Spearman's \\\\(\\\\rho\\\\) for the correlation between lesson number and % of target language utterances ranged from 0.32 to 0.73, with all \\\\(p\\\\) values < 0.01. Data for the Spanish student can be seen in Figure 5. All figures presented in this section are linear regressions with 95% confidence intervals.\\n\\n![Student Spanish utterance %](image)\\n\\nFigure 5: Student Spanish utterance %\\n\\nNext, we look at metrics designed to measure lexical diversity in speech, as they have been shown to correlate with assessments of learner ability (Engber, 1995) and can grow over time for language learners (Hsieh, 2016). For computing these metrics, we tokenize Spanish and Chinese using spaCy (which internally uses pkuseg for Chinese) (Honnibal et al., 2020; Luo et al., 2019), and Arabic with CAMeL tools (Obeid et al., 2020). Token data is then cleaned by removing tokens consisting of punctuation, numbers, whitespace and stop words.\\n\\nToken-type ratio (TTR) is one measure of lexical diversity which is commonly used in linguistics research (Thomas, 2005). TTR is calculated as...\"}"}
{"id": "lrec-2024-main-1147", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Guiraud\u2019s index for the Chinese student and teacher\\n\\nThe number of unique tokens (words) divided by the number of total tokens, and ideally would be expected to increase over time as the learner\u2019s vocabulary expands and the teacher moves on to using more complex language. However, TTR has been shown to be unstable in some circumstances, such as when there is substantial variance in the total number of tokens (Van Hout and Vermeer, 2007). This instability is mirrored in our results: while TTR grows over time for some students and teachers in some languages, correlations are often weak or have p values substantially higher than 0.05 suggesting no correlation at all. Some alternatives to TTR have been proposed to address its shortcomings. In particular, we look at Guiraud\u2019s index (Guiraud, 1954), which mitigates the influence of total number of tokens by using the square root of the total token count as the denominator. We present standard TTR and Guiraud\u2019s index below as\\n\\n\\\\[\\n\\\\text{TTR} = \\\\frac{V}{N}\\n\\\\]\\n\\n\\\\[\\n\\\\text{TTR}_{\\\\text{guiraud}} = \\\\frac{V}{\\\\sqrt{N}}\\n\\\\]\\n\\n\\n\\n| Metric | Arabic | Spanish | Chinese |\\n|--------|--------|---------|---------|\\n| Target Lang % | Student | 0.58 | 0.48 | 0.46 |\\n| | Teacher | 0.72 | 0.32 | 0.73 |\\n| Guiraud\u2019s Index | Student | 0.32 | 0.38 | 0.30 |\\n| | Teacher | 0.37 | 0.55 | 0.53 |\\n\\nTable 8: Spearman\u2019s \\\\(\\\\rho\\\\) for correlation between summary statistics and lesson number. All correlations have \\\\(p < 0.01\\\\).\\n\\nWe find that Guiraud\u2019s, like % of target language utterances, consistently increases over time (i.e. correlates with lesson number) for both students and teachers as can be seen in Table 8. Spearman\u2019s \\\\(\\\\rho\\\\) ranges from 0.30 to 0.55 with all \\\\(p\\\\) values < 0.01. Interestingly, this effect is measurably stronger for teachers, who had a mean \\\\(\\\\rho\\\\) of 0.48 as opposed to students\u2019 0.33. A comparison of change in Guiraud\u2019s index for the Chinese student and teacher can be seen in Figure 4. Assuming that students made measurable progress over the course of their lessons and that teachers gradually increased the difficulty of lesson content, these results show that this progression is reflected in our data, and also speak to the suitability of Guiraud\u2019s index as a metric.\\n\\n5.2. Multimodal Analysis\\n\\nWe also illustrate how the rich multimodal data in the MOSLA dataset can be harnessed to gain insights into teacher and student behaviors using modern machine learning techniques. Our objective here is to use machine learning techniques to determine the area of focus for both the teacher and the student on the screen, based solely on unannotated raw audio and video data. Specifically, we use the Matchmap method, as described in (Harwath et al., 2018), to align the raw audio and the image in an unsupervised manner. The underlying principle of this method is that when parts of the input image and the audio co-occur frequently, it results in a high similarity score for that combination. The Matchmap method, as shown in Figure 6, encodes an image and an audio clip using separate encoders, producing a grid or sequence of latent representations for each modality. Let \\\\(a_t\\\\) be the \\\\(u\\\\)-th element of the audio representation vector \\\\(a\\\\) at time \\\\(t\\\\), and \\\\(i_{x,y,u}\\\\) be the \\\\(u\\\\)-th element of the image representation vector \\\\(i_{x,y}\\\\) at position \\\\((x, y)\\\\). After applying a linear projection layer (\\\\(f_a\\\\) and \\\\(f_i\\\\),...\"}"}
{"id": "lrec-2024-main-1147", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"respectively) to each modality, the method computes a three-dimensional matrix called Matchmap as:\\n\\n$$M_{x,y,t} = f_i(i)_{T_{x,y}} f_a(a)_{t}, (4)$$\\n\\nwhich quantifies the degree of \u201ccompatibility\u201d between the image at position $$(x, y)$$ and the audio at time $$t$$. Finally, the Matchmap matrix is aggregated to determine the overall similarity (referred to as SISA\u2014Sum Image, Sum Audio) between a given image $$I$$ and audio $$A$$ instances using a simple arithmetic mean:\\n\\n$$S(I, A) = \\\\frac{1}{N_x N_y N_t} X_{x,y,t} M_{x,y,t} (5)$$\\n\\nwhere $$N_x$$, $$N_y$$, $$N_t$$ denote the width and height of the encoded image, and the length of the encoded audio sequence, respectively.\\n\\nTo learn the Matchmap matrix without the need for labels, we adopt a contrastive learning approach. This approach maximizes the similarity between true image-audio pairs $$(I_i, A_{i})$$ while minimizing the similarity between randomly chosen \u201cimposter\u201d images $$I_{imp_i}$$ and audio $$A_{imp_i}$$. Specifically, the Matchmap method uses the following loss function as the learning objective:\\n\\n$$L = \\\\sum_{i=1}^{N_b} \\\\max(0, S(I_i, A_{imp_i}) - S(I_i, A_i) + \\\\eta) + \\\\max(0, S(I_{imp_i}, A_i) - S(I_i, A_i) + \\\\eta)$$\\n\\n(6)\\n\\nwhere $$N_b$$ is the number of instance per batch. Imposter images and audio were created by randomly permutating the instances within each batch. We set $$\\\\eta = 1$$ in our experiments.\\n\\nWe initially extracted 100 random 10-second chunks from each Arabic lesson video. Images were generated by calculating the average of all the frames within each chunk. In this experiment, we used the pretrained VGG16 model (Simonyan and Zisserman, 2015) before the last pooling layer for encoding images and the Whisper (Radford et al., 2022) encoder (basemodel) for audio. Audio representations were downsampled by averaging every 10-frame window, resulting in $$7 \\\\times 7$$ feature maps for images and 50 frames for audio. Both image and audio representations were then transformed using a 512-dimensionallinear layer before computing the matchmap. The entire network, including the encoders and linear layers, was optimized using Adam with a learning rate of $$1 \\\\times 10^{-4}$$ for 30 epochs, with a batch size of 32.\\n\\nFigure 7 displays some examples of visualized matchmaps. These images were generated by slicing the computed matchmap at time $$t$$ when discourse is taking place, whether initiated by the teacher or student, and then overlaying it as a heatmap onto the original image. As can be seen in the figure, the matchmap highlights relevant parts of the input image, such as the speaker (a) and/or the learning content (b). While we have not conducted a formal evaluation of this model, these results suggest that similar multimodal analytics approaches may prove effective for tasks such as speaker diarization, automated speech recognition, and facial expression analysis.\\n\\n6. Conclusion\\n\\nIn Project MOSLA (Moments of Second Language Acquisition), we address the complexity of SLA by creating a longitudinal, multimodal, multilingual, and controlled dataset that captures every moment of SLA learners\u2019 experience through online instruction. With human and machine annotations gen-\"}"}
{"id": "lrec-2024-main-1147", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"generated using state-of-the-art speech models, the MOSLA dataset provides insights into the distribution of spoken language, speaker identities, and the content of spoken discourse. Our experiments highlight the potential of this resource in revealing target language usage and lexical development, as well as in identifying the areas of focus for both learners and educators during interactions. By offering open access to the MOSLA dataset for research and non-commercial purposes, we hope to inspire a wide array of studies, fostering a deeper understanding of the multifaceted nature of SLA and facilitating the development of more effective pedagogical approaches for second language learners.\\n\\n7. Ethical Considerations\\n\\nAs it is difficult to imagine possible harms as a result of further research or technology built on a dataset about language acquisition, our primary ethical concerns relate to the fairness of compensation and exposure to risk for participants in the study. In regards to compensation: all participants\u2014students, teachers, and annotators\u2014were paid well above the minimum hourly wage in the country in which this research was conducted.\\n\\nWe view risk to participants as consisting broadly of two categories: possible exposure of personally identifiable information (PII) relating to teachers or students, and possible appropriation of teaching material. Our primary mitigation against these risks is that access to MOSLA data will require consenting to a terms of use document which explicitly prohibits attempts to extract PII, appropriate teaching materials, redistribute the data, or otherwise use it for anything other than research. There is no explicit PII included anywhere in the data; our concern is only preventing the possibility of PII being inferred from conversation content in lessons. Furthermore, all participants knew from before their first lesson that they were being recorded with the intent of eventually publishing the data, had the option to withdraw at any point, and had and continue to have the right to request removal of any data, at any time, for any reason.\\n\\nOne other possible area of concern is copyrighted materials. In order to address this, teachers were asked to refrain from using copyrighted materials except in a supplementary capacity, and we are confident that any such usage included in the MOSLA lessons falls under fair use for teaching and research.\\n\\nFinally, in place of an IRB or equivalent institutional review board which we did not have access to, we had a third-party ethics review conducted by an external researcher with an extensive background in AI and data ethics.\\n\\n8. Acknowledgements\\n\\nWe would like to thank Paulino Brener and Abdulwahed Danou for their participation as teachers and Xiuling Mo for her participation as both teacher and learner. Participants are thanked in the acknowledgements at their request, and had the option to remain anonymous.\\n\\nWe would also like to thank Keisuke Sakaguchi and Kasumi Yamazaki for their valuable advice, which informed this research. Finally, we would like to thank Marius Miron for providing a third-party ethics review.\\n\\nBibliographical References\\n\\nHerv\u00e9 Bredin. 2023. pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe. In Proc. INTERSPEECH 2023.\\n\\nChristopher Bryant, Zheng Yuan, Muhammad Reza Qorib, Hannan Cao, Hwee Tou Ng, and Ted Briscoe. 2023. Grammatical Error Correction: A Survey of the State of the Art. Computational Linguistics, pages 1\u201359.\\n\\nAndrew Caines, Helen Yannakoudakis, Helena Edmondson, Helen Allen, Pascual P\u00e9rez-Paredes, Bill Byrne, and Paula Buttery. 2020. The teacher-student chatroom corpus. In Proceedings of the 9th Workshop on NLP for Computer Assisted Language Learning, pages 10\u201320, Gothenburg, Sweden. LiU Electronic Press.\\n\\nIsolda Carranza. 1995. Multilevel analysis of two-way immersion classroom discourse. George-town University round table on languages and linguistics, pages 169\u2013187.\\n\\nNgo Cong-Lem. 2018. Web-based language learning (wbll) for enhancing L2 speaking performance: A review. Advances in Language and Literary Studies.\\n\\nDorottya Demszky and Heather Hill. 2023. The NCTE transcripts: A dataset of elementary math classroom transcripts. In Proceedingsof the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 528\u2013538, Toronto, Canada. Association for Computational Linguistics.\\n\\nPatrick J. Donnelly, Nathaniel Blanchard, Andrew M. Olney, Sean Kelly, Martin Nystrand, and Sidney K. D\u2019Mello. 2017. Words matter: Automatic detection of teacher questions in live classroom discourse using linguistics, acoustics, and context. In Proceedings of the Seventh International\"}"}
{"id": "lrec-2024-main-1147", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cheryl A Engber. 1995. The relationship of lexical proficiency to the quality of ESL compositions. Journal of second language writing, 4(2):139\u2013155.\\n\\nJason Fan and Xun Yan. 2020. Assessing speaking proficiency: A narrative review of speaking assessment research with in the argument-based validation framework. Frontiers in Psychology, 11.\\n\\nJeroen Geertzen, Theodora Alexopoulou, and Anna Korhonen. 2014. Automatic linguistic annotation of large scale L2 databases: The eFCam database (eFCamDat).\\n\\nYuan Gong, Sameer Khurana, Leonid Karlinsky, and James Glass. 2023. Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers. In Proc. INTERSPEECH 2023, pages 2798\u20132802.\\n\\nPierre Guiraud. 1954. Les caract\u00e8res statistiques du vocabulaire: essai de m\u00e9thodologie.\\n\\nRegine Hampel and Ursula Stickler. 2012. The use of videoconferencing to support multimodal interaction in an online language classroom. ReCALL, 24(2):116\u2013137.\\n\\nEric Harper, Somshubra Majumdar, Oleksii Kuchaiev, Li Jason, Yang Zhang, Evelina Bakhaturina, Vahid Noroozi, Sandeep Subramanian, Koluguri Nithin, Huang Jocelyn, Fei Jia, Jagadeesh Balam, Xuesong Yang, Micha Livne, Yi Dong, Sean Naren, and Boris Ginsburg. 2019. NeMo: a toolkit for Conversational AI and Large Language Models.\\n\\nDavid Harwath, Adri\u00e0 Recasens, D\u00eddac Sur\u00eds, Galen Chuang, Antonio Torralba, and James Glass. 2018. Jointly discovering visual objects and spoken words from raw sensory input. In Computer Vision \u2013 ECCV 2018: 15th European Conference, Munich, Germany, September 8\u201314, 2018, Proceedings, Part VI, pages 659\u2013677, Berlin, Heidelberg. Springer-Verlag.\\n\\nYuta Hayashibe. 2021. Hachiue. https://github.com/koniwa/hachiue.\\n\\nMatthew Honnibal, Ines Montani, Sofie Van Laendeghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python.\\n\\nYufen Hsieh. 2016. An exploratory study on Singaporean primary school students\u2019 development in Chinese writing. The Asia-Pacific Education Researcher, 25:541\u2013548.\\n\\nRony Kubat, Philip DeCamp, and Brandon Roy. 2007. Totalrecall: Visualization and semi-automatic annotation of very large audio-visual corpora. In Proceedings of the 9th International Conference on Multimodal Interfaces, ICMI \u201907, pages 208\u2013215, New York, NY, USA. Association for Computing Machinery.\\n\\nMark R. Lepper and Maria Woolverton. 2002. Chapter 7 - the wisdom of practice: Lessons learned from the study of highly effective tutors. In Joshua Aronson, editor, Improving Academic Achievement, Educational Psychology, pages 135\u2013158. Academic Press, San Diego.\\n\\nRuixuan Luo, Jingjing Xu, Yi Zhang, Zhiyuan Zhang, Xuancheng Ren, and Xu Sun. 2019. Pkuseg: A toolkit for multi-domain Chinese word segmentation. CoRR, abs/1906.11455.\\n\\nHamed Monkaresi, Nigel Bosch, Rafael A. Calvo, and Sidney K. D\u2019Mello. 2017. Automated detection of engagement using video-based estimation of facial expressions and heart rate. IEEE Transactions on Affective Computing, 8(1):15\u201328.\\n\\nSu Mu, Meng Cui, and Xiaodi Huang. 2020. Multimodal data fusion in learning analytics: A systematic review. Sensors, 20(23).\\n\\nOssama Obeid, Nasser Zalmout, Salam Khalifa, Dima Taji, Mai Oudah, Bashar Alhafni, Go Inoue, Fadhl Eryani, Alexander Erdmann, and Nizar Habash. 2020. CAMeL tools: An open source Python toolkit for Arabic natural language processing. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 7022\u20137032, Marseille, France. European Language Resources Association.\\n\\nChris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J Guibas, and Jascha Sohl-Dickstein. 2015. Deep knowledge tracing. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.\\n\\nAlexis Plaquet and Herv\u00e9 Bredin. 2023. Powerset multi-class cross entropy loss for neural speaker diarization. In Proc. INTERSPEECH 2023.\\n\\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale weak supervision.\"}"}
{"id": "lrec-2024-main-1147", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deb Roy, Rupal Patel, Philip DeCamp, Rony Kubat, Michael Fleischman, Brandon Roy, Nikolaos Mavridis, Stefanie Tellex, Alexia Salata, Jethran Guinness, Michael Levit, and Peter Gorgniak. 2006. The human speechome project. In Symbol Grounding and Beyond, pages 192\u2013196, Berlin, Heidelberg. Springer Berlin Heidelberg.\\n\\nKazuya Saito and Yuka Akiyama. 2017. Video-based interaction, negotiation for comprehensibility, and second language speech learning: A longitudinal study. Language Learning, 67(1):43\u201374.\\n\\nBurr Settles, Chris Brust, Erin Gustafson, Masato Hagiwara, and Nitin Madnani. 2018. Second language acquisition modeling. In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 56\u201365, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nBurr Settles, Geoffrey T. LaFlair, and Masato Hagiwara. 2020. Machine Learning\u2013Driven Language Assessment. Transactions of the Association for Computational Linguistics, 8:247\u2013263.\\n\\nKaren Simonyan and Andrew Zisserman. 2015. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations.\\n\\nKatherine Stasaski, Kimberly Kao, and Marti A. Hearst. 2020. CIMA: A large open access dialogue dataset for tutoring. In Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52\u201364, Seattle, WA, USA. Association for Computational Linguistics.\\n\\nDax Thomas. 2005. Type-token ratios in one teacher's classroom talk: An investigation of lexical complexity.\\n\\nMiles Turnbull and Jennifer Dailey-O'Cain. 2009. First language use in second and foreign language learning. Multilingual Matters.\\n\\nSowmya Vajjala and Taraka Rama. 2018. Experiments with universal CEFR classification. In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 147\u2013153, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nRoeland Van Hout and Anne Vermeer. 2007. Comparing measures of lexical richness. Modelling and assessing vocabulary knowledge, 93:115.\\n\\nMary Lou Vercellotti. 2015. The Development of Complexity, Accuracy, and Fluency in Second Language Performance: A Longitudinal Study. Applied Linguistics, 38(1):90\u2013111.\"}"}
