{"id": "acl-2024-short-1", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: GPT-3.5 - Difference prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.\"}"}
{"id": "acl-2024-short-1", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Can Language Models Serve as Text-Based World Simulators?\\nRuoyao Wang\u2020, Graham Todd\u2021, Ziang Xiao\u2660, Xingdi Yuan\u2662, Marc-Alexandre C\u00f4t\u00e9\u2662, Peter Clark\u2663, Peter Jansen\u2020\u2663\\n\\n\u2020University of Arizona\\n\u2662Microsoft Research Montr\u00e9al\\n\u2021New York University\\n\u2660Johns Hopkins University\\n\u2663Allen Institute for AI\\n\\n{ruoyaowang,pajansen}@arizona.edu gdrtodd@nyu.edu ziang.xiao@jhu.edu {eric.yuan,macote}@microsoft.com PeterC@allenai.org\\n\\nAbstract\\nVirtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called BYTESIZED32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM\u2019s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.\\n\\n1 Introduction and Related Work\\nSimulating the world is crucial for studying and understanding it. In many cases, however, the breadth and depth of available simulations are limited by the fact that their implementation requires extensive work from a team of human experts over weeks or months. Recent advances in large language models (LLMs) have pointed towards an alternate approach by leveraging the huge amount of knowledge contained in their pre-training datasets. But are they ready to be used directly as simulators? We examine this question in the domain of text-based games, which naturally express the environment and its dynamics in natural language and have long been used as part of advances in decision making processes (C\u00f4t\u00e9 et al., 2018; Fan et al., 2020; Urbanek et al., 2019; Shridhar et al., 2020; Hausknecht et al., 2020; Jansen, 2022; Wang et al., 2023), information extraction (Ammanabrolu and Hausknecht, 2020; Adhikari et al., 2020), and artificial reasoning (Wang et al., 2022).\\n\\nBroadly speaking, there are two ways to leverage LLMs in the context of world modeling and simulation. The first is neurosymbolic: a number of efforts use language models to generate code in a symbolic representation that allows for formal planning or inference (Liu et al., 2023; Nottingham et al., 2023; Wong et al., 2023; Tang et al., 2024). REASONING VIA PLANNING (RAP) (Hao et al., 2023) is one such approach \u2013 it constructs a world model using LLM priors and then uses a...\"}"}
{"id": "acl-2024-short-1", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"dedicated planning algorithm to decide on agent policies (LLMs themselves continue to struggle to act directly as planners (Valmeekam et al., 2023)). Similarly, BYTE-SIZED32 (Wang et al., 2023) tasks LLMs with instantiating simulations of scientific reasoning concepts in the form of large PYTHON programs. These efforts are in contrast to the second, and comparatively less studied, approach of direct simulation. For instance, AI-DUNGEON represents a game world purely through the generated output of a language model, with inconsistent results (Walton, 2020). In this work, we provide the first quantitative analysis of the abilities of LLMs to directly simulate virtual environments. We make use of structured representations in the JSON schema as a scaffold that both improves simulation accuracy and allows for us to directly probe the LLM's abilities across a variety of conditions. In a systematic analysis of GPT-4 (Achiam et al., 2023), we find that LLMs broadly fail to capture state transitions not directly related to agent actions, as well as transitions that require arithmetic, common-sense, or scientific reasoning. Across a variety of conditions, model accuracy does not exceed 59.9% for transitions in which a non-trivial change in the world state occurs. These results suggest that, while promising and useful for downstream tasks, LLMs are not yet ready to act as reliable world simulators without further innovation.\\n\\n1 Methodology\\nWe examine the abilities of LLMs to serve as world simulators in text-based virtual environments, in which an agent receives observations and proposes actions in natural language in order to complete certain objectives. Each text environment can be formally represented as a goal-conditioned partially observable Markov decision process (POMDP) (Kaelbling et al., 1998) with the 7-tuple \\\\((S, A, T, O, R, C, D)\\\\), where \\\\(S\\\\) denotes the state space, \\\\(A\\\\) denotes the action space, \\\\(T: S \\\\times A \\\\rightarrow S\\\\) denotes the transition function, \\\\(O\\\\) denotes the observation function, \\\\(R: S \\\\times A \\\\rightarrow \\\\mathbb{R}\\\\) denotes the reward function, \\\\(C\\\\) denotes a natural language \\\"context message\\\" that describes the goal and action semantics, and \\\\(D: S \\\\times A \\\\rightarrow \\\\{0, 1\\\\}\\\\) denotes the binary completion indicator function.\\n\\nCode and data are available at https://github.com/cognitiveailab/GPT-simulator.\\n\\n| States (avg. per game) | 2463.5 |\\n|------------------------|--------|\\n| Action verbs (avg. per game) | 7.4 |\\n| Object types (avg. per game) | 5.5 |\\n| Object instances (avg. per state) | 10.4 |\\n| Total games | 31 |\\n| Total transitions | 76,369 |\\n\\nTable 1: Corpus statistics of BYTE-SIZED32-SP.\\n\\n2.1 LLM-Sim Task\\nWe propose a prediction task, which we call LLM-as-a-Simulator (LLM-Sim), as a way of quantitatively evaluating the capacity of language models to serve as reliable simulators. The LLM-Sim task is defined as implementing a function \\\\(F: C \\\\times S \\\\times A \\\\rightarrow S \\\\times R \\\\times \\\\{0, 1\\\\}\\\\) as a world simulator that maps from a given context, state, and action (i.e. \\\\(c, s, a\\\\)) to the subsequent state, reward, and game completion status (i.e. \\\\(s_{t+1}, r_{t+1}, d_{t+1}\\\\)). In practice, the whole state transition simulator \\\\(F\\\\) should consider two types of state transitions: action-driven transitions and environment-driven transitions. For the example in Figure 1, the action-driven transition is that the sink is turned on (\\\\(\\\\text{isOn=true}\\\\)) after taking the action turn on sink, and the environment-driven transition is that water fills up the cup in the sink when the sink is on. To better understand LLM's ability to model each of these transitions, we further decompose the simulator function \\\\(F\\\\) into three steps:\\n\\n1. Action-driven transition simulator \\\\(F_{\\\\text{act}}: C \\\\times S \\\\times A \\\\rightarrow S\\\\) predicts \\\\(s_{t+1}\\\\) given \\\\(c, s, a\\\\), where \\\\(s_{t+1}\\\\) represents the direct state change caused by actions.\\n2. Environment-driven transition simulator \\\\(F_{\\\\text{env}}: C \\\\times S_{\\\\text{act}} \\\\rightarrow S\\\\) predicts \\\\(s_{t+1}\\\\) given \\\\(c, s_{t+1}\\\\), where \\\\(s_{t+1}\\\\) is the state that results after any environment-driven transitions.\\n3. Game progress simulator \\\\(F_{R}: C \\\\times S \\\\times A \\\\rightarrow R \\\\times \\\\{0, 1\\\\}\\\\) predicts the reward \\\\(r_{t+1}\\\\) and the game completion status \\\\(d_{t+1}\\\\) given \\\\(c, s_{t+1}, a\\\\).\"}"}
{"id": "acl-2024-short-1", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Full State Prediction: The LLM outputs the complete state. For example, when functioning as F, given c, st and at, the model generates the full game state st+1 alongside rt+1 and dt+1.\\n\\nState Difference Prediction: The LLM outputs only the difference between the input and output states. For example, when functioning as F, given c, st and at, the model generates only the difference between the current and subsequent game states, \u2206((st, rt, dt), (st+1, rt+1, dt+1)), as a way to reduce the need to generate redundant or unchanging information. We do not apply state difference prediction to the game progress simulator FR as its output (rt and dt) is not complex.\\n\\n2.2 Data\\nTo facilitate evaluation on the LLM-Sim task, we introduce a novel dataset of text game state transitions. Our dataset, BYTE-SIZED 32-State-Prediction (BYTE-SIZED 32-SP), consists of 76,369 transitions represented as (c, st, rt, dt, at, st+1, rt+1, dt+1) tuples collected from 31 distinct text games. Additional corpus statistics are summarized in Table 1.\\n\\nTable 2: Average accuracy per game of GPT-4 predicting the whole state transitions (F) as well as action-driven transitions (F act) and environment-driven transitions (F env). We report settings that use LLM generated rules, human written rules, or no rules. Dynamic and static denote whether the game object properties and game progress should be changed; Full and diff denote whether the prediction outcome is the full game state or state differences. Numbers are shown in percentage.\\n\\nTable 3: GPT-4 game progress prediction results versions of the context, one where the rules are written by a human expert (one of the game authors), and one where they are produced by an LLM with access to the game code, and one where no rules are provided. See Appendix C for additional details.\\n\\n2.3 Evaluation\\nPerformance on LLM-Sim is determined by the model\u2019s prediction accuracy w.r.t. the ground truth labels over a dataset of test samples. Depending on the experimental condition, the LLM must model object properties (when simulating F act, F env, or FR) and/or game progress (when simulating FR or FR), defined as:\\n\\nObject Properties: a list of all objects in the game, along with each object\u2019s properties (e.g., temperature, size) and relationships to other objects (e.g., being within or on top of another object).\\n\\nGame Progress: the status of the agent w.r.t. the overall goal, consisting of the current accumulated reward, whether the game has terminated, and whether the overall goal has been achieved.\\n\\nWe note that in each case the LLM is provided with the ground truth previous state (when functions as F env the previous state is st+1) as well as the overall task context. That is to say, the LLM always performs a single-step prediction.\\n\\n3 Experiments\\nFigure 1 demonstrates how we evaluate the performance of a model on the LLM-Sim task using...\"}"}
{"id": "acl-2024-short-1", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Comparison between accuracy of human annotators and GPT-4 on a subset of the BYTE-SIZED 32-SP dataset. Transitions were sampled to normalize GPT-4 performance at 50% (if possible) and annotators were tasked with modeling the complete transition function $F$ and outputting the full state. We evaluate the accuracy of GPT-4 in both the Full State and State Difference prediction regimes. The model receives the previous state (encoded as a JSON object), previous action, and context message, it produces the subsequent state (either as a complete JSON object or as a diff). See Appendix A for details.\\n\\nWe note that the transition dynamics between states depend primarily on the verb used in the action (e.g., take, put, cook, ...). In addition, some state-action pairs do not result in any changes to object properties or game progress. To ensure balance across these conditions (and increase the tractability of our experiments), we sub-sample a dataset $D$ from the full BYTE-SIZED 32-SP set. Formally, let $s_{in}$ be the input state of a simulator function and $s_{out}$ be the output state of the simulator function (e.g. $s_{in} = s_{t}$ and $s_{out} = s_{act + 1}$ for $F_{act}$). We call any transition in which $s_{out} = s_{in}$ (according to the ground-truth) static and call each other transition dynamic. Note that the environment-driven transition following a dynamic action-driven transition is not necessarily dynamic. For example, a state in which the agent takes an apple while the remaining objects in the environment remain the same is a dynamic action-driven transition and a static environment-driven transition. We construct $D$ by randomly sampling 10 dynamic transitions and 10 static transitions from BYTE-SIZED 32-SP for each possible action verb (taking as many as possible if fewer than 10 exist) w.r.t action-driven transitions. The resulting experimental dataset consists of 2954 transition tuples.\\n\\n4 Results\\n\\nTable 2 presents the accuracy of GPT-4 simulating the whole state transitions as well as its accuracy of simulating action-driven transitions and environment-driven transitions alone. We report\\n\\n2 See Appendix E for the results of GPT-3.5.\\n\\nsome major observations below:\\n\\nPredicting action-driven transitions is easier than predicting environment-driven transitions: At best, GPT-4 is able to simulate 77.1% of dynamic action-driven transitions correctly. In contrast, GPT-4 simulates at most 49.7% of dynamic environment-driven transitions correctly. This indicates that the most challenging part of the LLM-Sim task is likely simulating the underlying environmental dynamics.\\n\\nPredicting static transitions is easier than dynamic transitions: Unsurprisingly, modeling a static transition is substantially easier than a dynamic transition across most conditions. While the LLM needs to determine whether a given initial state and action will result in a state change in either case, dynamic transitions also require simulating the dynamics in exactly the same way as the underlying game engine by leveraging the information in the context message.\\n\\nPredicting full game states is easier for dynamic states, whereas predicting state difference is easier for static states: Predicting the state difference for dynamic state significantly improves the performance (>10%) of simulating static transitions, while decreases the performance when simulating dynamic transitions. This may be because state difference prediction is aimed at reducing potential format errors. However, GPT-4 is able to get the response format correct in most cases, while introducing the state difference increases the complexity of the output format of the task.\\n\\nGame rules matter, and LLMs are able to generate good enough game rules: Performance of GPT-4 on all three simulation tasks drops in most conditions when game rules are not provided in the context message. However, we fail to find obvious performance differences between game rules generated by human experts and by LLMs themselves.\\n\\nGPT-4 can predict game progress in most cases: Table 3 presents the results of GPT-4 predicting game progress. With game rules information in the context, GPT-4 can predict the game progress correctly in 92.1% test cases. The presence of these rules in context is crucial: without them, GPT-4's prediction accuracy drops to 61.5%.\\n\\nHumans outperform GPT-4 on the LLM-Sim task: We provide a preliminary human study on the LLM-Sim task. In particular, we take the 5 games...\"}"}
{"id": "acl-2024-short-1", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5 Conclusion\\n\\nWe propose BYTEIZED 32-State-Prediction, a benchmark of 76,369 virtual text environment state transitions for testing LLMs as simulators. We evaluate GPT-4 on this world modeling task. Across models and conditions, the best recorded performance is 59.9% on accurately simulating state transitions that involve non-trivial changes. Because simulation errors accumulate across steps, a simulator with modest single-step accuracy has limited utility in practice \u2013 for example, after 10 steps, average simulation accuracy would reduce to less than 1%. Our results indicate that LLMs are not yet able to reliably act as text world simulators. Further error analysis shows that while LLMs are better at simulating the results of user actions, it is difficult for LLMs to handle environment-driven transitions and transitions that require arithmetic, common sense, or scientific knowledge.\"}"}
{"id": "acl-2024-short-1", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6 Limitations and Ethical Concerns\\n\\n6.1 Limitations\\nThis work considers two strong in-context learning LLMs, GPT-3.5 and GPT-4, in their ability to act as explicit formal simulators. We adopt these models because they are generally the most performant off-the-shelf models across a variety of benchmarks. While we observe that even GPT-3.5 and GPT-4 achieve a modest score at the proposed task, we acknowledge that we did not exhaustively evaluate a large selection of large language models, and other models may perform better. We provide this work as a benchmark to evaluate the performance of existing and future models on the task of accurately simulating state space transitions.\\n\\nIn this work, we propose two representational formalisms for representing state spaces, one that includes full state space, while the other focuses on state difference, both represented using JSON objects. We have chosen these representations based on their popularity and compatibility with the input and output formats of most LLM pretraining data (e.g. Fakhoury et al., 2023), as well as being able to directly compare against gold standard simulator output for evaluation, though it is possible that other representational formats may be more performant at the simulation task.\\n\\nFinally, the state spaces produced in this work are focused around the domain of common-sense and early (elementary) scientific reasoning. These tasks, such as opening containers or activating devices, were chosen because the results of these actions are common knowledge, and models are likely to be most performant in simulating these actions. While this work does address a selection of less frequent actions and properties, it does not address using LLMs as simulators for highly domain-specific areas, such as physical or medical simulation. A long term goal of this work is to facilitate using language models as simulators for high-impact domains, and we view this work as a stepping-stone to developing progressively more capable language model simulators.\\n\\n6.2 Ethical Concerns\\nWe do not foresee an immediate ethical or societal impact resulting from our work. However, we acknowledge that as an LLM application, the proposed LLM-Sim task could be affected in some way by misinformation and hallucinations introduced by the specific LLM selected by the user. Our work highlights the issue with using LLMs as text-based world simulators. In downstream tasks, such as game simulation, LLMs may generate misleading or non-factual information. For example, if the simulator suggests burning a house to boil water, our work does not prevent this, nor do we evaluate the ethical implications of such potentially dangerous suggestions. As a result, we believe such applications are neither suitable nor safe to be deployed to a setting where they directly interact with humans, especially children, e.g., in an educational setting. We urge researchers and practitioners to use our proposed task and dataset in a mindful manner.\\n\\nAcknowledgements\\nWe wish to thank the three anonymous reviewers for their helpful comments on an earlier draft of this paper.\\n\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nAshutosh Adhikari, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Mikul\u00e1\u0161 Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and Will Hamilton. 2020. Learning dynamic belief graphs to generalize on text-based games. Advances in Neural Information Processing Systems, 33:3045\u20133057.\\n\\nPrithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. arXiv preprint arXiv:2001.08837.\\n\\nMarc-Alexandre C\u00f4t\u00e9, \u00c1kos K\u00e1d\u00e1r, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. Textworld: A learning environment for text-based games. CoRR, abs/1806.11532.\\n\\nSarah Fakhoury, Saikat Chakraborty, Madan Musuvathi, and Shuvendu K Lahiri. 2023. Towards generating functionally correct code edits from natural language issue descriptions. arXiv preprint arXiv:2304.03816.\\n\\nAngela Fan, Jack Urbanek, Pratik Ringshia, Emily Dinan, Emma Qian, Siddharth Karamcheti, Shrimai Prabhumoye, Douwe Kiela, Tim Rocktaschel, Arthur Szlam, and Jason Weston. 2020. Generating interactive worlds with text. Proceedings of the AAAI Conference on Artificial Intelligence, 34(02):1693\u20131700.\"}"}
{"id": "acl-2024-short-1", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8154\u20138173.\\n\\nMatthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C\u00f4t\u00e9, and Xingdi Yuan. 2020. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903\u20137910.\\n\\nPeter Jansen. 2022. A systematic survey of text worlds as embodied natural language environments. In Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022), pages 1\u201315.\\n\\nLeslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99\u2013134.\\n\\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. Llm+p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.\\n\\nKolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. 2023. Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling. In International Conference on Machine Learning, pages 26311\u201326325. PMLR.\\n\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768.\\n\\nHao Tang, Darren Key, and Kevin Ellis. 2024. Worldcoder, a model-based llm agent: Building world models by writing code and interacting with the environment. arXiv preprint arXiv:2402.12275.\\n\\nJack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rockt\u00e4schel, Douwe Kiela, Arthur Szlam, and Joshua Weston. 2019. Learning to speak and act in a fantasy text adventure game.\\n\\nKarthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2023. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36:75993\u201376005.\\n\\nNick Walton. 2020. How we scaled AI Dungeon 2 to support over 1,000,000 users.\\n\\nRuoyao Wang, Peter Jansen, Marc-Alexandre C\u00f4t\u00e9, and Prithviraj Ammanabrolu. 2022. Scienceworld: Is your agent smarter than a 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11279\u201311298.\\n\\nRuoyao Wang, Graham Todd, Xingdi Yuan, Ziang Xiao, Marc-Alexandre C\u00f4t\u00e9, and Peter Jansen. 2023. Byte-Sized32: A corpus and challenge task for generating task-specific world models expressed as text games. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13455\u201313471, Singapore. Association for Computational Linguistics.\\n\\nLionel Wong, Gabriel Grand, Alexander K Lew, Noah D Goodman, Vikash K Mansinghka, Jacob Andreas, and Joshua B Tenenbaum. 2023. From word models to world models: Translating from natural language to the probabilistic language of thought. arXiv preprint arXiv:2306.12672.\"}"}
{"id": "acl-2024-short-1", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Model details\\n\\nFor the GPT-3.5 model, we use the gpt-3.5-turbo-0125 model. For the GPT-4 model, we use the gpt-4-0125-preview model. For both models, the temperature is set to 0 to get deterministic results. We also turn on the JSON mode of both models, which ensures that the model gives a valid JSON response.\\n\\nOur experiments cost approximately $5,000 for OpenAI API usage.\\n\\nB Game transition examples\\n\\nWe manually pick the wash-clothes game in BYTE-SIZED 32 as the example game as it contains both state transitions driven by actions and game's underlying dynamics. In tasks where the model predicts action transition, environment-driven transitions, or the game progress alone, we provide one corresponding in-context example. In the task that requires the model to predict everything, we offer two in-context examples in the prompt. The two examples are manually picked such that in one example the game state is changed directly by the action taken while in the other example the game state is changed by the game's underlying dynamics.\\n\\nC Game rules generation\\n\\nC.1 LLM generated rules\\n\\nFor LLM generated rules, we manually check all of them to avoid misinformation and offensive content. We prompt GPT-4 (gpt-4-0125-preview) with the code of each object class to acquire the rules of each object. We also provide one in-context example. We ask GPT-4 to describe the meaning of each critical property (i.e. properties that do not inherit from parent) of the object and the tick function of the object (i.e. a function that defines how object properties may change at each time step regardless of the action taken). Below is an example of our prompt of object rule generation:\\n\\nObject Rule Generation Prompt\\n\\nYou will be given a Python class which defines an object in a text game. List the classes inherited by this class and explain the properties of the object based on your understanding of the code. The properties you need to explain are commented as critical properties in the init function. If the class contains a tick method function, you should also describe how the object properties will be changed at each game tick. Otherwise, do not explain any property. Your response should follow the format of the example below:\\n\\nHere is the code for the example:\\n\\n```python\\n{OBJECT_CLASS_CODE}\\n```\\n\\nThe expected output is:\\n\\nObject: Stove\\nInherits: Container, Device\\nProperties:\\nmaxTemperature: the maximum temperature of the stove in degrees Celsius\\ntempIncreasePerTick: the temperature increases per tick for objects on the stove if the stove is on.\\n\\nNow here is another object class that needs you to explain:\\n\\n```python\\n{OBJECT_CLASS_CODE}\\n```\\n\\nFor action rules generation, we prompt GPT-4 (gpt-4-0125-preview) with the code of the whole game, but unlike object rules, we do not offer any in-context example. We ask GPT-4 to describe each of the actions in the game. Below is an example of our prompt for action rule generation:\\n\\nAction Rule Generation Prompt\\n\\nYou will be given a Python program which defines an a text game. Describe the all actions based on your understanding of the code. You can find all actions listed in the comments at the beginning of the program. You should describe all constraints of each action and how game states will be changed by taking each action.\\n\\nHere is the code of the game:\\n\\n```python\\n{GAME_CODE}\\n```\\n\\nSimilar to action rules, we generate score rules by prompting GPT-4 (gpt-4-0125-preview) with the code of the game and ask GPT-4 to describe how the game can be won or lose and how rewards can be earned. Below is an example of our prompt for score rule generation:\\n\\nScore Rule Generation Prompt\\n\\nYou will be given a Python program which defines an a text game. Describe how the game can be won or lose, and how game scores can be earned based on your understanding of the calculateScore function in the TextGame class. Here is the code of the game. Do not describe the main function.\\n\\n```python\\n{GAME_CODE}\\n```\"}"}
{"id": "acl-2024-short-1", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Rules is an unordered list of rule descriptions that describe the constraints of the action when interacting with different objects (e.g., At least one of the objects should be a wire or a multimeter) or how the rule might function under different conditions (e.g., Disconnect terminal if the terminal is already connected to other objects). To ensure accuracy, the annotator plays through the game and checks if the written object rules were correctly reflected in the gameplay.\\n\\nC.3 Human-Written Object Rules\\n\\nThe object rules describe the meaning of each object property (e.g., temperature, size, weight, etc.) and how they will be changed at each time step. The expert annotators read the game description and source code for each game. They went through the object classes in the code script and wrote the object rules. Each object rule has three main parts: Object, Description, and Properties. The Object specifies the name of the object. The Description explains the general purpose of the object (e.g., GarbageCan is a container that can hold garbage). In the Description, the inheritance of the object class has been noted. The Properties is an unordered list of property descriptions that describe each property of that object (e.g., A Mold has its shape.) and their default value (e.g., By default, a GameObject is not combustible.) if the object is an abstract class. For objects with tick function, there is another property describing how an object may change under each tick. To ensure accuracy, the annotator plays through the game and checks if the written object rules were correctly reflected in the gameplay.\\n\\nC.4 Human-Written Score Rules\\n\\nScore rules describe the conditions to win or lose the game and how rewards can be earned. An expert annotator (one of the BYTESIZED game authors) creates the rules by reading the game description and the code of the score function.\\n\\nD Prompts\\n\\nThe prompts introduced in this section includes game rules that can either be human written rules or LLM generated rules. For experiments without game rules, we simply remove the rules from the corresponding prompts.\\n\\nD.1 Prompt Example:\\n\\nD.1.1 Full State Prediction\\n\\nYou are a simulator of a text game. Read the task description of a text game. Given the current game state in JSON, you need to decide the new game state after taking an action. Your response should be in the same JSON format as the given game state.\\n\\nExample game task description:\\n\\nYour task is to wash the dirty dishes.\\n\\nHere are the descriptions of all game objects properties in the example game:\\n\\n{OBJECT_RULES}\\n\\nHere are the descriptions of all game actions in the example game:\\n\\n{ACTION_RULES}\\n\\nHere is the game state:\\n\\n{GAME_STATE}\\n\\nThe action to take is put plate (ID: 5) in dirty cup (ID: 4)\\n\\nThe expected response is:\\n\\n{GAME_STATE}\\n\\nHere is the game that you need to simulate:\\n\\nTask Description:\\n\\nYour task is to figure out the weight of the cube. Use the answer action to give your answer.\\n\\nHere are the descriptions of all game objects properties:\\n\\n{OBJECT_RULES}\\n\\nHere are the descriptions of all game actions:\\n\\n{ACTION_RULES}\\n\\nHere is the game state:\\n\\n{GAME_STATE}\\n\\nThe action to take is:\\n\\nlook\\n\\nD.1.2 State Difference Prediction\\n\\nYou are a simulator of a text game. Read the task description of a text game. Given the current game state in JSON, you need to decide the new game state after taking an action. Your response should be in the JSON format. It should have two keys: 'modified' and 'removed'. The 'modified' key stores a list of all the object states that are added or changed after taking the action. Keep it an empty list if no object is added or modified. The 'removed' key stores a list of uuids of the objects that are removed. Keep it an empty list if no object is removed.\\n\\nExample game task description:\\n\\nYour task is to wash the dirty dishes.\\n\\nHere are the descriptions of all game objects properties in the example game:\\n\\n{OBJECT_RULES}\\n\\nHere are the descriptions of all game actions in the example game:\\n\\n{ACTION_RULES}\\n\\nHere is the game state:\\n\\n{GAME_STATE}\\n\\nThe action to take is put plate (ID: 5) in dirty cup (ID: 4)\\n\\nThe expected response is:\\n\\n{GAME_STATE_DIFFERENCE}\\n\\nHere is the game that you need to simulate:\\n\\nTask Description:\\n\\nYour task is to figure out the weight of the cube. Use the answer action to give your answer.\\n\\nHere are the descriptions of all game objects properties:\\n\\n{OBJECT_RULES}\\n\\nHere are the descriptions of all game actions:\\n\\n{ACTION_RULES}\\n\\nHere is the game state:\\n\\n{GAME_STATE}\\n\\nThe action to take is:\\n\\nlook\"}"}
{"id": "acl-2024-short-1", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.2 Prompt Example:\\n\\n**Fenv**\\n\\n**D.2.1 Full State Prediction**\\n\\nFull State Prediction Prompt (Fenv)\\n\\nYou are a simulator of a text game. Read the task description. Given the current game state in JSON, you need to decide how the game state changes in the next time step (without considering the agent actions). Rules for such changes are described as the tick function of each object.\\n\\nYour response should be in the same JSON format as the given game state.\\n\\nHere is an example:\\n\\nExample game task description:\\n\\nYour task is to wash the dirty dishes.\\n\\nHere are the descriptions of all game objects properties in the example game:\\n\\n```json\\n\\n```\\n\\nHere is the game state:\\n\\n```json\\n\\n```\\n\\nThe expected response is:\\n\\n```json\\n\\n```\\n\\nHere is the game that you need to simulate:\\n\\nTask Description:\\n\\nYour task is to figure out the weight of the cube. Use the answer action to give your answer.\\n\\nHere are the descriptions of all game objects properties:\\n\\n```json\\n\\n```\\n\\nHere is the game state:\\n\\n```json\\n\\n```\\n\\n**D.2.2 State Difference Prediction**\\n\\nState Difference Prediction Prompt (Fenv)\\n\\nYou are a simulator of a text game. Read the task description. Given the current game state in JSON, you need to decide how the game state changes in the next time step (without considering the agent actions). Rules for such changes are described as the tick function of each object.\\n\\nYour response should be in the JSON format. It should have two keys: 'modified' and 'removed'. The 'modified' key stores a list of all the object states that are added or changed after taking the action. Keep it an empty list if no object is added or modified. The 'removed' key stores a list of uuids of the objects that are removed. Keep it an empty list if no object is removed.\\n\\nHere is an example:\\n\\n**D.3 Prompt Example:**\\n\\n**FR**\\n\\nGame Progress Prediction Prompt (FR)\\n\\nYou are a simulator of a text game. Read the task description of a text game. Given the current game state in JSON, you need to predict the current game score, whether the game is over, and whether the agent wins the game.\\n\\nYour response should be a JSON with three keys: 'score', 'gameOver', and 'gameWon'. 'score' stores the current game score, 'gameOver' stores a bool value on whether the game is over, and 'gameWon' stores a bool value on whether the game is won.\\n\\nHere is an example:\\n\\nExample game task description:\\n\\nYour task is to wash the dirty dishes.\\n\\nHere are the descriptions of all game objects properties in the example game:\\n\\n```json\\n\\n```\\n\\nHere is a description of the game score function:\\n\\n```json\\n\\n```\\n\\nHere is the previous game state:\\n\\n```json\\n\\n```\\n\\nThe game score of the previous state is:\\n\\n```json\\n\\n```\\n\\nThe action to take is use dish soap (ID: 12) on glass (ID: 8)\\n\\n```json\\n\\n```\\n\\nThe expected response is:\\n\\n```json\\n\\n```\\n\\nHere is the game that you need to simulate:\\n\\nTask Description:\\n\\nYour task is to figure out the weight of the cube. Use the answer action to give your answer.\\n\\nHere are the descriptions of all game objects properties:\\n\\n```json\\n\\n```\\n\\nHere is a description of the game score function:\\n\\n```json\\n\\n```\\n\\nHere is the previous game state:\\n\\n```json\\n\\n```\\n\\nThe game score of the previous state is:\\n\\n```json\\n\\n```\\n\\nThe action to take is:\\n\\nlook\\n\\nHere is the current game state after taking the action:\\n\\n```json\\n\\n```\"}"}
{"id": "acl-2024-short-1", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.4 Prompt Example:\\nF\\nD.4.1 Full State Prediction\\nFull State Prediction Prompt (F)\\nYou are a simulator of a text game. Read the task description of a text game. Given the current game state in JSON, you need to decide the new game state after taking an action including the game score.\\nYou may need to create new objects when you predict the new game state. You should assign the uuid of new objects starting from the UUID base given in the instructions. Your response should be in the same JSON format as the given game state.\\nNote that while game states can be changed by actions, some game states may change over the time, which is described in the tick function of each object class.\\nHere are two examples of both cases. Both examples are from the same example game.\\nExample game task description:\\nYour task is to wash the dirty dishes.\\nHere are the descriptions of all game objects properties in the example game:\\n{\\nOBJECT_RULES\\n}  \\nHere are the descriptions of all game actions in the example game:\\n{\\nACTION_RULES\\n}  \\nHere is a description of the score function of the example game:\\n{\\nSCORE_RULES\\n}  \\nIn the first example, the game state is changed by an action:\\nHere is the game state:\\n{\\nGAME_STATE\\n}\\nThe current game UUID base is 12\\nThe action to take is: put plate (ID: 5) in dirty cup (ID: 4)\\nThe expected response is:\\n{\\nGAME_STATE\\n}\\nIn the second example from the same example game, the game state is changed over the time. Note that while in this example the game state is changed by time only, it is possible that a game state is changed by both an action and time.\\nHere is the game state:\\n{\\nGAME_STATE\\n}\\nThe current game UUID base is 13\\nThe action to take is: eat dishwasher (ID: 2) with dirty plate (ID: 5)\\nThe expected response is:\\n{\\nGAME_STATE\\n}\\n\\nD.4.2 State Difference Prediction\\nState Difference Prediction Prompt (F)\\nYou are a simulator of a text game. Read the task description and the current environment observation description. Given the current game state in JSON, you need to decide the new game state after taking an action.\\nYour response should be in the JSON format. It should have three keys: 'modified', 'removed', and 'score'. The 'modified' key stores a list of all the object states that are added or changed after taking the action. Keep it an empty list if no object is added or modified. The 'removed' key stores a list of uuids of the objects that are removed. Keep it an empty list if no object is removed.\\nThe 'score' key stores a dictionary with three keys: 'score' is the current game score, 'gameOver' is a boolean of whether the game is over, and 'gameWon' is a boolean of whether the agent won the game. If a player earns a score or wins/loses the game, you should reflect that change in the dictionary saved under the 'score' key. Otherwise, you should set value of the 'score' key to an empty dictionary.\\nNote that while game states can be changed by actions, some game states may change over the time, which is described in the tick function of each object class.\\nHere are two examples of both cases. Both examples are from the same example game.\\nExample game task description:\\nYour task is to wash the dirty dishes.\\nHere are the descriptions of all game objects properties in the example game:\\n{\\nOBJECT_RULES\\n}  \\nHere are descriptions of all game actions in the example game:\\n{\\nACTION_RULES\\n}  \\nHere is a description of the score function of the example game:\\n{\\nSCORE_RULES\\n}  \\nIn the first example, the game state is changed by an action:\\nCurrent observation:\\n{\\nGAME_OBSERVATION\\n}\\nHere is the game state:\\n{\\nGAME_STATE\\n}\\nThe action to take is put dirty plate (ID: 5) in mug (ID: 6)\\nThe expected response is:\\n{\\nGAME_STATE_DIFFERENCE\\n}  \\nIn the second example from the same example game, the game state is changed over the time. Note that while in this example the game state is changed by time only, it is possible that a game state is changed by both an action and time.\\nCurrent observation:\\n{\\nExample_2 observation\\n}\\nHere is the game state:\\n{\\nGAME_STATE\\n}\\nThe action to take is eat dishwasher (ID: 2) with dirty plate (ID: 5)\\nThe expected response is:\\n{\\nGAME_STATE_DIFFERENCE\\n}\\nHere is the game that you need to simulate:\\nTask Description:\\nYour task is to boil water.\\nHere are the descriptions of all game objects properties:\\n{\\nOBJECT_RULES\\n}  \\nHere are the descriptions of all game actions:\\n{\\nACTION_RULES\\n}  \\nHere is a description of the score function of the game:\\n{\\nSCORE_RULES\\n}  \\nCurrent observation:\\n{\\nGAME_OBSERVATION\\n}\\nHere is the game state:\\n{\\nGAME_STATE\\n}\\nThe current game UUID base is 12\\nThe action to take is:\\nlook\"}"}
{"id": "acl-2024-short-1", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.5 Other Examples\\n\\nBelow is an example of the rule of an action:\\n\\nAction Rule Example\\nput:\\nDescription: put an object into a target container\\nRules:\\n1. The target must be a container (Container)\\n2. The target container must be open\\n3. The object must be in the inventory\\n4. The object must be moveable (isMoveable)\\n\\nBelow is an example of the rule of an object:\\n\\nObject Rule Example\\nObject: Container\\nDescription: Abstract class for things that can be considered 'containers' (e.g. a drawer, a box, a table, a shelf, etc.)\\nProperties:\\n\u2212 A Container is a container.\\n\u2212 A Container could be opened (e.g., e.g. a drawer, a door, a box, etc.), or is it always 'open' (e.g. a table, a shelf, etc.).\\n\u2212 A Container has a property indicating if it is opened.\\n\u2212 A Container has a property indicating the prefix to use when referring to the container (e.g. \\\"in the drawer\\\", \\\"on the table\\\", etc.). By default, the prefix is 'in'\\n\\nBelow is an example of the score rule:\\n\\nScore Rule Example\\nThe player wins the game by getting all dishes clean.\\nThe player gets one point for each dish that is cleaned.\\nThe player loses one point for each dish that is made dirty.\\n\\nBelow is an example of a game state:\\n\\nGame State Example\\n\\n```json\\n{\\n  \\\"game_state\\\": [\\n    {\\n      \\\"name\\\": \\\"agent (ID: 0)\\\",\\n      \\\"uuid\\\": 0,\\n      \\\"type\\\": \\\"Agent\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": true,\\n        \\\"isOpenable\\\": false,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"in\\\"\\n      },\\n      \\\"contains\\\": [\\n        \\\"plate (ID: 5)\\\",\\n        \\\"mug (ID: 6)\\\",\\n        \\\"knife (ID: 7)\\\"\\n      ]\\n    },\\n    {\\n      \\\"name\\\": \\\"plate (ID: 5)\\\",\\n      \\\"uuid\\\": 5,\\n      \\\"type\\\": \\\"Dish\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": true,\\n        \\\"isOpenable\\\": false,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"on\\\",\\n        \\\"dishType\\\": \\\"plate\\\",\\n        \\\"isDirty\\\": true,\\n        \\\"foodMessName\\\": \\\"orange\\\"\\n      },\\n      \\\"contains\\\": []\\n    },\\n    {\\n      \\\"name\\\": \\\"mug (ID: 6)\\\",\\n      \\\"uuid\\\": 6,\\n      \\\"type\\\": \\\"Dish\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": true,\\n        \\\"isOpenable\\\": false,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"in\\\",\\n        \\\"dishType\\\": \\\"mug\\\",\\n        \\\"isDirty\\\": true,\\n        \\\"foodMessName\\\": \\\"sandwhich\\\"\\n      },\\n      \\\"contains\\\": []\\n    },\\n    {\\n      \\\"name\\\": \\\"knife (ID: 7)\\\",\\n      \\\"uuid\\\": 7,\\n      \\\"type\\\": \\\"Dish\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": true,\\n        \\\"isOpenable\\\": false,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"in\\\",\\n        \\\"dishType\\\": \\\"knife\\\",\\n        \\\"isDirty\\\": true,\\n        \\\"foodMessName\\\": \\\"apple (ID: 11)\\\"\\n      },\\n      \\\"contains\\\": []\\n    },\\n    {\\n      \\\"name\\\": \\\"dishwasher (ID: 2)\\\",\\n      \\\"uuid\\\": 2,\\n      \\\"type\\\": \\\"DishWasher\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": false,\\n        \\\"isOpenable\\\": true,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"in\\\",\\n        \\\"isDevice\\\": true,\\n        \\\"isActivatable\\\": true,\\n        \\\"isOn\\\": false,\\n        \\\"cycleStage\\\": 0,\\n        \\\"finishedCycle\\\": false\\n      },\\n      \\\"contains\\\": [\\n        \\\"cup (ID: 4)\\\"\\n      ]\\n    },\\n    {\\n      \\\"name\\\": \\\"cup (ID: 4)\\\",\\n      \\\"uuid\\\": 4,\\n      \\\"type\\\": \\\"Dish\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": true,\\n        \\\"isOpenable\\\": false,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"in\\\",\\n        \\\"dishType\\\": \\\"cup\\\",\\n        \\\"isDirty\\\": true,\\n        \\\"foodMessName\\\": \\\"peanut butter\\\"\\n      },\\n      \\\"contains\\\": []\\n    },\\n    {\\n      \\\"name\\\": \\\"bottle of dish soap (ID: 3)\\\",\\n      \\\"uuid\\\": 3,\\n      \\\"type\\\": \\\"DishSoapBottle\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": false,\\n        \\\"isMoveable\\\": true,\\n        \\\"isDevice\\\": true,\\n        \\\"isActivatable\\\": true,\\n        \\\"isOn\\\": false\\n      },\\n      \\\"contains\\\": []\\n    },\\n    {\\n      \\\"name\\\": \\\"glass (ID: 8)\\\",\\n      \\\"uuid\\\": 8,\\n      \\\"type\\\": \\\"Dish\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": true,\\n        \\\"isOpenable\\\": false,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"in\\\",\\n        \\\"dishType\\\": \\\"glass\\\",\\n        \\\"isDirty\\\": false\\n      },\\n      \\\"contains\\\": []\\n    },\\n    {\\n      \\\"name\\\": \\\"bowl (ID: 9)\\\",\\n      \\\"uuid\\\": 9,\\n      \\\"type\\\": \\\"Dish\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": true,\\n        \\\"isOpenable\\\": false,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"in\\\",\\n        \\\"dishType\\\": \\\"bowl\\\",\\n        \\\"isDirty\\\": false\\n      },\\n      \\\"contains\\\": []\\n    },\\n    {\\n      \\\"name\\\": \\\"banana (ID: 10)\\\",\\n      \\\"uuid\\\": 10,\\n      \\\"type\\\": \\\"Food\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": false,\\n        \\\"isMoveable\\\": true,\\n        \\\"isFood\\\": true\\n      },\\n      \\\"contains\\\": []\\n    },\\n    {\\n      \\\"score\\\": -1,\\n      \\\"gameOver\\\": false,\\n      \\\"gameWon\\\": false\\n    }\\n  ]\\n}\\n```\\n\\nTable 5:\\nAverage accuracy per game of GPT-3.5 predicting the whole state transitions (F) as well as action-driven transitions (F_{act}) and environment-driven transitions (F_{env}). We report settings that use LLM generated rules, human written rules, or no rules. Dynamic and static denote whether the game object properties and game progress should be changed; Full and diff denote whether the prediction outcome is the full game state or state differences. Numbers shown in percentage.\\n\\n| Rules          | Game Progress |\\n|----------------|---------------|\\n| LLM            | 73.9          |\\n| Human          | 63.3          |\\n| No rule        | 64.2          |\\n\\nTable 6:\\nGPT-3.5 game progress prediction results\\n\\nBelow is an example of JSON that describes the difference of two game states:\\n\\nGame State Difference Example\\n\\n```json\\n{\\n  \\\"modified\\\": [\\n    {\\n      \\\"name\\\": \\\"agent (ID: 0)\\\",\\n      \\\"uuid\\\": 0,\\n      \\\"type\\\": \\\"Agent\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": true,\\n        \\\"isOpenable\\\": false,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"in\\\"\\n      },\\n      \\\"contains\\\": [\\n        \\\"mug (ID: 6)\\\",\\n        \\\"knife (ID: 7)\\\"\\n      ]\\n    },\\n    {\\n      \\\"name\\\": \\\"mug (ID: 6)\\\",\\n      \\\"uuid\\\": 6,\\n      \\\"type\\\": \\\"Dish\\\",\\n      \\\"properties\\\": {\\n        \\\"isContainer\\\": true,\\n        \\\"isMoveable\\\": true,\\n        \\\"isOpenable\\\": false,\\n        \\\"isOpen\\\": true,\\n        \\\"containerPrefix\\\": \\\"in\\\",\\n        \\\"dishType\\\": \\\"mug\\\",\\n        \\\"isDirty\\\": true,\\n        \\\"foodMessName\\\": \\\"sandwhich\\\"\\n      },\\n      \\\"contains\\\": [\\n        \\\"plate (ID: 5)\\\"\\n      ]\\n    }\\n  ],\\n  \\\"removed\\\": [],\\n  \\\"score\\\": {}|\\n```\\n\\nE GPT-3.5 results\\n\\nTable 5 and Table 6 shows the performance of a GPT-3.5 simulator predicting objects properties and game progress respectively. There is a huge gap between the GPT-4 performance and GPT-3.5 performance, providing yet another example of how fast LLM develops in the two years. It is also worth notices that the performance difference is larger when no rules is provided, indicating that GPT-3.5 is especially weak at applying common sense knowledge to this few-shot world simulation task.\"}"}
{"id": "acl-2024-short-1", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Property Name                  | Description                                                                 |\\n|-------------------------------|-----------------------------------------------------------------------------|\\n| buried                        | Objects buried in the room                                                  |\\n| combustionTimeRemaining       | Number of time steps remaining to combust of a combusting object             |\\n| connects                      | Electrical objects connecting to the current object                         |\\n| contains                      | Objects in the current object                                               |\\n| cook                          | How an ingredient is cooked                                                 |\\n| current_aperture              | Current aperture of a camera                                                |\\n| current_focus                 | The object that the camera is currently focusing on                         |\\n| current_iso                   | Current ISO of a camera                                                     |\\n| current_shutter_speed         | Current shutter speed of a camera                                           |\\n| cut                           | How an ingredient is cut                                                    |\\n| cycleStage                    | The current stage of the washing machine\u2019s cycle (running/washing/finished).|\\n| durability                    | Number of times left for a shovel to dig something                          |\\n| finishedCycle                 | A boolean indicator of whether the washing machine has finished             |\\n| food                          | The food level of a young bird. Reduce 1 if the young bird is not fed at each time step.|\\n| grow                          | Number of time steps that a young bird has grown                            |\\n| hatch                         | Number of time steps that an egg is hatched                                 |\\n| isAboveMaxTemp                | Whether the temperature of the current food is above its maximum preservation temperature |\\n| isActivated                   | Whether a device is activated                                               |\\n| isChoppable                   | Whether an object is choppable                                              |\\n| isCombusting                  | Whether an object is combusting                                             |\\n| isDirty                       | Whether a dish is dirty                                                     |\\n| isMoveable                    | Whether the current object is moveable                                       |\\n| isOn                          | Whether a device is turned on                                               |\\n| isOpen                        | Whether a container is open                                                 |\\n| isWet                         | Whether a clothes is wet                                                    |\\n| is_open                       | Whether a door is open                                                      |\\n| liquid                        | Whether there is liquid in a container                                      |\\n| mode                          | Mode of a multimeter                                                       |\\n| objects                       | Record of the number of time steps that each object is on the inclined plane |\\n| on                            | Whether a light bulb is on                                                  |\\n| photo                         | The object that the camera has taken a picture of                           |\\n| prefix                        | Prefix abstract to describe the object. E.g., a tree and some firewood      |\\n| stage                         | Life stage of a bird                                                       |\\n| stateOfMatter                 | State of matter of a substance                                             |\\n| sunburn                       | Whether the player's skin is burnt by the sun                               |\\n| temperature                   | Object temperature                                                         |\\n| tick                          | Number of ticks that an object is placed on an inclined plane               |\\n| timeAboveMaxTemp              | Number of time steps that a food is above its maximum preservation temperature |\\n| use_sunscreen                 | Whether the player has used the sunscreen                                   |\\n| volume                        | Volume of an object                                                         |\\n| warm                          | The warmth received by an egg during its hatching stage                     |\\n| wearSpaceSuit                 | Whether the agent wears the spacesuit                                       |\\n\\n2. In Figure 4, we show detailed experimental results on the state difference prediction task performed by GPT-4.\\n\\n3. In Figure 5, we show detailed experimental results on the full state prediction task performed by GPT-3.5.\\n\\n4. In Figure 6, we show detailed experimental results on the state difference prediction task performed by GPT-3.5.\"}"}
{"id": "acl-2024-short-1", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Human-generated rules.\\n(b) LLM-generated rules.\\n(c) No rules.\\n\\nFigure 3: GPT-4 - Full State prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.\"}"}
{"id": "acl-2024-short-1", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: GPT-4 - Difference prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.\"}"}
{"id": "acl-2024-short-1", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: GPT-3.5 - Full State prediction from a) Human-generated rules, b) LLM-generated rules, and c) No rules.\"}"}
