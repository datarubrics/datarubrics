{"id": "emnlp-2022-main-557", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information\\n\\nYijia Shao 1\u2217 ... question \\\"Full Name). When editing the third choice question block, the Options Recommendation suggests \\\"Yes\\\" and \\\"No\\\"\"}"}
{"id": "emnlp-2022-main-557", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"as candidate options. Finally, if the user types\\n\\\"How happy are you with your current job?\\\" for the\\nfourth block but hasn't selected a block type yet,\\nthe Block Type Suggestion predicts it as a rating\\nblock.\\n\\nThe above tasks require a specifically designed\\nmodel to understand semi-structured forms, where\\nnatural language (NL) text is organized by prede-\\ndefined structures. A form is composed of a title, a de-\\nscription, and a series of blocks. For each block, its\\nsubcomponents also follow unique structures. For\\nexample, a Choice block contains a list of options\\nwhich serve as candidate answers to the question\\ndisplayed in the block title. Existing pre-trained\\nlanguage models (PLMs) focus on general-purpose\\nfree-form NL text (Devlin et al., 2019; Yang et al.,\\n2019). They may provide a good starting point\\nto model the rich semantic information within NL\\ncontents of a form. However, they cannot directly\\nhandle the extra structural information of the form.\\n\\nIs it possible to infuse a PLM with structural infor-\\nmation of online forms?\\n\\nIn this paper, we propose FormLM to model\\nboth the semantic and structural information of on-\\nline forms. As we will discuss in \u00a74, there are three\\nkey parts of FormLM. First, the form serialization\\nprocedure, which represents a form as a tree and\\nconverts it into a token sequence without informa-\\ntion loss. Second, inheriting existing PLM with a\\nsmall number of additional parameters: FormLM\\ninherits the parameters of BART (Lewis et al.,\\n2020) to leverage its language modelling capabili-\\nties. Also, by adding extra biases to the attention\\nlayers, FormLM explicitly handles the structural\\ninformation. Third, continual pre-training with col-\\nllected online forms: for better downstream appli-\\ncation: We propose two structure-aware objectives \u2013 Span Masked Language Model and Block Title\\nPermutation \u2013 to continually pre-train FormLM on\\ntop of the inherited and additional parameters.\\n\\nWe evaluate FormLM on Form Creation Ideas\\ntasks using our OOF (Open Online Forms) dataset.\\nThis dataset (see \u00a72.2) is created by crawling and\\nparsing public forms on the Web. Comparing\\n\\nto PLMs such as BART, FormLM improves the\\nROUGE-1 score from 32.82 to 37.53 on Question\\nRecommendation, and the Macro-F1 score from\\n73.3 to 83.9 on Block Type Suggestion.\\n\\nIn summary, our main contributions are:\\n\u2022 We put forward the problem of online form\\nmodeling and formally define a group of tasks\\non Form Creation Ideas. To the best of our\\nknowledge, these problems have not been sys-\\ntematically studied before.\\n\u2022 FormLM is proposed by us to model both\\nthe semantic and structural information by en-\\nhancing PLM with form serialization, struc-\\ntural attention and continual pre-training.\\n\u2022 The public OOF dataset with 62k forms is con-\\nstructed by us. To the best of our knowledge,\\nthis is the first public online form dataset.\\n\u2022 Comprehensive experiments \u2013 especially base-\\nline comparisons, ablation studies, design\\nchoices and empirical studies \u2013 are designed\\nand run by us to evaluate the effectiveness of\\nFormLM on the tasks of Form Creation Ideas\\nwith the form dataset.\\n\\n2 Preliminaries\\n\\nIn this section, we further elaborate the predefined\\nstructure in online forms, and introduce our col-\\nllected dataset.\\n\\n2.1 Online Form Structure\\n\\nModern online form services usually allow users\\nto create a form by piling up different types of\\nblocks. There are eight common block types:\\nText Field, Choice, Time, Date, Likert, Rating, Upload,\\nand Description. Each block type has a predefined\\nstructure (e.g., the options of a choice block) and\\ncorresponds to a specific layout shown in the user\\ninterface (e.g., bullet points or checkboxes of the\\noptions). The order of the blocks in a form usually\\nmatters because they are designed to organize ques-\\ntions in an easy-to-understand way, and to collect\\ndata from various related aspects. For example, in\\nFigure 1, easier profile / fact questions are asked\\nbefore the preference / opinion questions.\\n\\nAs shown at the top of Figure 3, an online\\nform can be viewed as an ordered tree. The\\nroot node \\\\( T \\\\) represents the form title, and its\\nchildren nodes \\\\( Ch(T) = (Desc, B_1, ..., B_N) \\\\)\\nrepresent the form description and a series of\\nblocks. The subtree structure of \\\\( B_i \\\\) depends\\non its type. For Choice and Rating blocks,\\n\\\\( Ch(B_i) = (Type_i, Title_i, Desc_i, C(1)_i, ..., C(n_i)_i) \\\\)\\nwhere \\\\( C(k)_i \\\\) are the options or scores; For\\n\"}"}
{"id": "emnlp-2022-main-557", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Distribution of Block Types in Online Forms.\\n\\n2.2 Online Form Dataset\\nSince there is no existing dataset for online forms, we construct our own OOF (Open Online Forms) dataset by crawling public online forms created on a popular online form website. We filter out forms with low quality and only consider English forms in this work. In total, 62K public forms are collected across different domains, e.g., education, finance, medical, community activities, etc. Due to the semi-structured nature of online forms, we further parsed the crawled HTML pages into JSON format by extracting valid contents and associating each block with its type. Figure 2 shows the distribution of block types in our collected dataset. More details of the dataset construction and its statistics can be found in Appendix A.\\n\\n3 Form Creation Ideas\\nAs illustrated in Figure 1, when adding a new block, one needs to specify its type and title in the first step. Then, other required components \u2013 such as a list of options for a Choice block \u2013 are added according to the block type. In this paper, we focus on the following three tasks which provide Form Creation Ideas to users in the first and later steps.\\n\\nQuestion Recommendation\\nThe Question Recommendation aims at providing users with a recommended question based on the selected block type and the previous context. Formally, the model needs to predict Title\\\\(_i\\\\) based on T, Desc, B\\\\(_1\\\\), ..., B\\\\(_{i-1}\\\\) and Type\\\\(_i\\\\). For example, in Figure 1, it is desirable that the model could recommend \\\"Employee ID\\\" when the form designer creates a Text Field block after the first block.\\n\\nBlock Type Suggestion\\nDifferent from the scenario of Question Recommendation, sometimes form designers may first come up with a block title without clearly specifying its block type. The Block Type Suggestion helps users select a suitable type in this situation. For example, for the last block of Figure 1, the model will predict it as a Rating block and suggest adding candidate rating scores if the form designer has not appointed the block type himself / herself. Formally, given Title\\\\(_i\\\\) and the available context (T, Desc, B\\\\(_1\\\\), ..., B\\\\(_{i-1}\\\\)), the model should predict Type\\\\(_i\\\\) in this task.\\n\\nOptions Recommendation\\nAs Figure 2 shows, Choice blocks are frequently used in online forms. When creating a Choice block, one should additionally provide a set of options, and the Options Recommendation helps in this case. Given the previous context (T, Desc, B\\\\(_1\\\\), ..., B\\\\(_{i-1}\\\\)) and Title\\\\(_i\\\\), the model predicts \\\\(C^{(1)}_i, ..., C^{(n_i)}_i\\\\) if Type\\\\(_i\\\\) = Choice. In this work, we expect the model to recommend a set of possible options at the same time, so the desired output of this task is \\\\(C^{(1)}_i, ..., C^{(n_i)}_i\\\\) concatenated with a vertical bar. For example, in Figure 1, the model may output \\\"Yes | No\\\" to recommend options for the third block.\\n\\n4 Methodology\\nAs discussed in \u00a71, we propose FormLM to model forms for creation ideas. We select BART as the backbone model of FormLM because it is widely used in NL-related tasks and supports both generation and classification tasks. In the rest of this section, we will describe the design and training details of FormLM as demonstrated in Figure 3.\\n\\n4.1 Form Serialization\\nAs discussed in \u00a72.1, an online form could be viewed as an ordered tree. In FormLM we serialize the tree into a token sequence which is compatible with the input format of common PLMs. Figure 3(A) depicts the serialization process which utilizes special tokens and separators. First, a special token is introduced for each block type to explicitly encode Type\\\\(_i\\\\). Second, the vertical bar \\\"|\\\" is used to concatenate a list of related items within a block \u2013 options / scores \\\\(C^{(k_i)}\\\\) of a Choice / Rating block, and rows \\\\(R^{(j_i)}\\\\) or columns \\\\(C^{(k_i)}\\\\) of a Likert block. Finally, multiple subcomponents of \\\\(B_i\\\\) are concatenated using <sep>. Note that there is no information loss in the serialization process, i.e., the hierarchical tree structure of an online form can be reconstructed from the flattened sequence.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Full Name\\nYour answer\\nText Field\\nEmployee ID\\nYour answer\\nText Field\\nYes\\nNo\\nChoice\\nDo you feel you and your manager get along?\\n\\nTeam Building Questionnaire\\nPlease fill out this form to allow us to better understand our employees' interests, strengths, and real feelings.\\n\\nFigure 3: The Overview of FormLM Methodology.\\n\\n(A) Form Serialization\\n\u00a74.1 serializes an online form by adding block type tokens and separate tokens to preserve the tree structure.\\n\\n(B) Structural Attention\\n\u00a74.2 encodes the token type and block-level distance by adding structural biases to each attention layer. Different colors in the attention bias matrix denote different items in the lookup table and the number inside each circle represents the block-level distance of a token pair.\\n\\n(C) Continual Pre-training\\n\u00a74.3 requires the model to recover the input sequence corrupted by SpanMLM and BTP. We use the cross-entropy loss between the decoder's output and the uncorrupted sequence for model optimization.\\n\\n4.2 Structural Attention\\nBeyond adding structural information into the input sequence, in FormLM we further enhance its backbone PLM with specially designed Structural Attention (StructAttn). Our intuition is that the attention calculation among tokens should consider their different roles and locations in a form. E.g., tokens within a question title seldom correlates with the tokens of an option from another question; tokens in nearby blocks (or even the same block) are usually stronger correlated with each other than those from distant blocks.\\n\\nAs illustrated in Figure 3(B), StructAttn encodes the structural information of an online form by adding two bias terms based on the token type (i.e., the role that a token plays in the flattened sequence) and the block-level position. For each attention head, given the query matrix \\\\( Q = [q_1, \\\\cdots, q_n]^{\\\\top} \\\\in \\\\mathbb{R}^{n \\\\times d_k} \\\\), the key matrix \\\\( K = [k_1, \\\\cdots, k_m]^{\\\\top} \\\\in \\\\mathbb{R}^{m \\\\times d_k} \\\\), and the value matrix \\\\( V = [v_1, \\\\cdots, v_m]^{\\\\top} \\\\in \\\\mathbb{R}^{m \\\\times d_v} \\\\), the original output is calculated by\\n\\n\\\\[\\n\\\\hat{A} = QK^{\\\\top}\\\\sqrt{d_k}, \\\\quad \\\\text{Attn}(H) = \\\\text{softmax}(\\\\hat{A})V(1)\\n\\\\]\\n\\nIn FormLM, we add two biases to \\\\( \\\\hat{A} \\\\) and the attention head output of StructAttn is calculated by\\n\\n\\\\[\\nA_{ij} = \\\\hat{A}_{ij} + \\\\sum_{\\\\text{shift}} L[\\\\text{type}(q_i), \\\\text{type}(k_j)] + \\\\mu_e - \\\\lambda d(q_i, k_j)\\n\\\\]\\n\\nAttn(\\\\( H \\\\)) = \\\\text{softmax}(A)V(2)\\n\\nIn Equation (2), the token type bias is calculated based on a learnable lookup table \\\\( L[\\\\cdot, \\\\cdot] \\\\) in each attention layer, and the lookup key \\\\( \\\\text{type}(\\\\cdot) \\\\) is the type of the corresponding token within the form structure. Specifically, in our work, \\\\( \\\\text{type}(\\\\cdot) \\\\) is chosen from 9 token types: FormTitle, FormDesc, BlockTitle, BlockDesc, Option, LikertRow, LikertColumn, BlockType, SepToken. If \\\\( Q \\\\) or \\\\( K \\\\) corresponds to the flattened sequence given by form serialization, \\\\( \\\\text{type}(\\\\cdot) \\\\) can be directly obtained from the original form tree; otherwise, in generation tasks, \\\\( Q \\\\) or \\\\( K \\\\) may correspond to the target, and we set \\\\( \\\\text{type}(\\\\cdot) \\\\) as the expected output token \\\\( \\\\text{type} \\\\), i.e., BlockTitle when generating the question and Option when generating the options. Another bias term in Equation (2) is calculated by an exponential decay function to model the relative block-level position, where \\\\( d(q_i, k_j) \\\\) is the block-level distance between the corresponding tokens.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tokens of $q_i$ and $k_j$ on the form tree. To make $d(q_i, k_j)$ well-defined for each token pair, we set $\\\\text{Desc}$ as the 0-th block ($B_0$) and specify $d(q_i, k_j)$ as 0 if $\\\\text{type}(q_i)$ or $\\\\text{type}(k_j)$ is equal to $\\\\text{FormTitle}$.\\n\\nNote that there are two parameters $\\\\lambda, \\\\mu$ in this term. We make them trainable and constrain their values to be positive to ensure tokens in neighboring blocks give more attention to each other.\\n\\nWe apply StructAttn to three parts of FormLM, self attentions of FormLM encoder, self attentions and cross attentions of FormLM decoder. $Q, K, V$ of encoder self attentions and $K, V$ of decoder cross attentions correspond to the source sequence; while $Q, K, V$ of decoder self attentions and $Q$ of decoder cross attentions correspond to the target sequence. In classification, both the source and the target are the flattened form; while in generation, the target is the recommended question or options.\\n\\nIn \u00a75.5, we will prove the effectiveness of StructAttn through ablation studies and comparing alternative design choices of StructAttn.\\n\\n4.3 Continual Pre-training\\n\\nNote that it is difficult to train a model for online forms from scratch due to the limited data. To effectively adapt FormLM to online forms, we conduct continual pre-training on the training set of our collected dataset (see \u00a72.2) with the following two structure-aware objectives.\\n\\nSpan Masked Language Model (SpanMLM)\\n\\nWe adapt the masked language model (MLM) to forms by randomly selecting and masking some nodes on the form tree within the masking budget. Compared to SpanBERT (Joshi et al., 2020) which improves the MLM objective by masking a sequence of complete words, we do the masking in a higher level of granularity based on the form structure. Our technique masks a block title, option, etc., instead of arbitrarily masking subword tokens. The latter was proven suboptimal in Joshi et al. (2020); Zhang et al. (2019). Specifically, we use a masking budget of 15% and replacing 80% of the masked tokens with $<$MASK$>$, 10% with random tokens and 10% with the original tokens.\\n\\nBlock Title Permutation (BTP)\\n\\nAs discussed in \u00a72.1, each block can be viewed as a subtree. We introduce the block title permutation objective by permuting block titles in a form and requiring the model to recover the original sequence with the intuition that the model needs to understand the semantic relationship between $B_i$ and $\\\\text{Ch}(B_i)$ to solve this challenge. We randomly shuffle all the block titles to construct the corrupted sequence. Following the pre-training process of BART, we unify these two objectives by optimizing a reconstruction loss, i.e., we input the sequence corrupted by SpanMLM and BTP and optimize the cross-entropy loss between the decoder's output and the original intact sequence.\\n\\n5 Experiments\\n\\n5.1 Evaluation Data and Metrics\\n\\nWe evaluate FormLM and other models on the three tasks of Form Creation Ideas (\u00a73) with our OOF dataset (\u00a72.2). The 62k public forms are split into 49,904 for training, 6,238 for validation, and 6,238 for testing. For each task, random sampling is further performed to construct an experiment dataset. Specifically, for each task, we randomly select no more than 5 samples from a single form to avoid sample bias introduced by those lengthy forms. For Question Recommendation and Block Type Suggestion, each sample corresponds to a block and its previous context (see \u00a73). 239,544, 29,558 and 29,466 samples are selected for training, validation and testing, respectively. For Options Recommendation, each sample corresponds to a Choice block with context. 124,994, 15,640 and 15,867 samples are selected for training, validation and testing. For Question and Options Recommendations, following the common practice in natural language generation research, we adopt ROUGE$_1$ (Lin, 2004) scores with the questions/options composed by human as the ground truth. During option recommendation, because the model is expected to recommend a list of options at once, we concatenate options with a vertical bar (described in \u00a74.1) for the comparison of generated results and ground truths. Since it is difficult to have a thorough evaluation of the recommendation quality through the automatic metric, we further include a qualitative study in Appendix D and conduct human evaluations for these two generation tasks (details in Appendix E). For Block Type Suggestion, both accuracy and Macro-F1 are reported to take account of the class imbalance issue.\\n\\n5.2 Baselines\\n\\nAs there was no existing system or model specifically designed for forms, we compare\\n\\n1We use the Hugging Face implementation to calculate the ROUGE score, https://huggingface.co/metrics/rouge.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Results of FormLM and the Baseline Models on the Tasks of Form Creation Ideas. Note that RoBERTa and MarkupLM are encoder-only models, thus cannot be directly applied to generation tasks. We leave their results blank for Question and Options Recommendations where ROUGE scores (R1, R2, RL) are used to evaluate these two generation tasks. Both the averaged metric and its standard deviation (as subscript) are reported for each result over 3 runs. The two gray rows (with up arrow \u2191) show the improvement of FormLM over its backbone model.\\n\\n| Model     | R1 \u00b1 R1 SE | R2 \u00b1 R2 SE | RL \u00b1 RL SE | Macro-F1 \u00b1 Macro-F1 SE | Accuracy \u00b1 Accuracy SE |\\n|-----------|------------|------------|------------|-------------------------|------------------------|\\n| RoBERTa   | -          | -          | -          | 73.7 \u00b1 0.02             | 85.8 \u00b1 0.46            |\\n| GPT-2     | 22.82 \u00b1 0.22 | 9.71 \u00b1 0.04 | 22.37 \u00b1 0.20 | 17.84 \u00b1 0.10 | 11.38 \u00b1 0.05 |\\n| MarkupLM  | -          | -          | -          | 79.8 \u00b1 0.27             | 88.6 \u00b1 0.13            |\\n| BART      | 31.48 \u00b1 0.16 | 15.89 \u00b1 0.18 | 30.91 \u00b1 0.16 | 43.53 \u00b1 0.32 | 31.81 \u00b1 0.21 |\\n| BART      | 32.82 \u00b1 0.05 | 17.06 \u00b1 0.20 | 32.18 \u00b1 0.05 | 46.12 \u00b1 0.12 | 33.74 \u00b1 0.08 |\\n| FormLM    | 35.9 \u00b1 0.08 | 18.27 \u00b1 0.10 | 35.23 \u00b1 0.04 | 44.14 \u00b1 0.06 | 32.39 \u00b1 0.16 |\\n| FormLM    | 37.53 \u00b1 0.07 | 19.70 \u00b1 0.15 | 36.78 \u00b1 0.12 | 47.24 \u00b1 0.02 | 34.65 \u00b1 0.14 |\\n\\n5.3 FormLM Implementation\\n\\nWe implement FormLM using the Transformers library (Wolf et al., 2020). FormLM and FormLM BASE are based on the architecture and parameters of BART (Lewis et al., 2020) and BART BASE, respectively. To construct inputs for these PLMs, we concatenate NL sentences in the available context (see \u00a7 3). MarkupLM (Li et al., 2022), a recent model for web page modeling, is also chosen as a baseline since forms can be displayed as HTML pages on the Internet. To keep accordance with the original inputs of MarkUpLM, we remove the tags without NL text (e.g., <script>, <style>) in the HTML file in OOF dataset. The number of parameters of each model can be found in Appendix B.\\n\\n5.4 Main Results\\n\\nFor FormLM and the baseline models (see \u00a7 5.2), Table 1 shows the results on the Form Creation Ideas tasks. FormLM significantly outperforms the baselines on all tasks. Compared to its backbone BART model (well-known for conditional generation tasks), FormLM further improves the ROUGE-1 scores by 4.71 and 1.12 on Question and Options Recommendations. Human evaluation results in Appendix E also confirm the superiority of FormLM over other baseline models in these two generation tasks. Figure 4 shows questions recommended by BART and FormLM on an example form from the test set. FormLM\u2019s recommendations (e.g., \u201cDestination\u201d, \u201cDeparture Date\u201d) are more specific and more relevant to the topic of this form, while BART\u2019s recommendations (e.g., \u201cName\u201d, \u201cSpecial Requests\u201d) are rather general. Also, after users create B1, B2, B3, B4 and select B5 as a Date type block, FormLM recommends \u201cDeparture Date\u201d while BART recommends \u201cName\u201d which is obviously not suitable to B5.\\n\\nOn Block Type Suggestion, FormLM improves the Macro-F1 score by 10.6. The improvement of FormLM over BART (\u2191 rows in Table 1) shows that our method is highly effective. We will further analyze this in \u00a7 5.5.\\n\\nNote that MarkupLM is a very strong baseline.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Travel Purpose & Special Interests\\n- Business\\n- School\\n- Educational\\n- Cruise\\n- Others\\n\\nDeparture\\nPlease let us know your departure city or airport.\\n\\nAmount of Travelers\\nPlease let us know how many travelers are there in your group.\\n\\nReturn Date\\nPlease let us know your specific return date, so that we can help you plan your trip.\\n\\nFlight\\nPlease let us know your preference of the flight.\\n- I prefer non-stop flights\\n- Other\\n\\nDestination\\nPlease let us know your preference for the hotel.\\n\\nAccommodation, Part 1.\\n- 3 Stars Hotel\\n- Other\\n\\nAccommodation, Part 2.\\n- 4 Stars Hotel\\n- Other\\n\\nFigure 4: Sample Outputs by FormLM and BART for Question Recommendation. FormLM's recommended questions are more relevant to the topic and more suitable to the selected block type.\\n\\nFormLM enhances its backbone PLM with StructAttn. As the row \\\"- Encoder StructAttn\\\" of Table 2 shows, when we ablate StructAttn from FormLM, the Macro-F1 score of Block Type Suggestion drops from 83.9 to 77.9 and the performance on the generation tasks also drops. In FormLM, we apply StructAttn to both encoder and decoder parts. We compare it with the setting without modifying the decoder (row \\\"- Decoder StructAttn\\\") and find applying StructAttn to both the encoder and decoder yields uniformly better results, which may be due to better alignment between the encoder and decoder.\\n\\nTable 2: Ablation Studies on Form Serialization and Structural Attention. \\\"-\\\" means the corresponding component is sequentially removed from FormLM. \\\"- Previous Context\\\" means that the closest block title is the only input.\\n\\nTable 3: Performance of FormLM \\\"w/\\\" and \\\"w/o\\\" Incorporating the Block Type Information.\\n\\nTo further investigate the effectiveness of the design choices in FormLM, we conduct ablation studies and controlled experiments (which are fine-tuned under the same settings as described in \u00a75.3) on the following aspects.\\n\\nForm Serialization\\nFor Form Creation Ideas, it is important to model the complete form context (defined in \u00a73). Row \\\"- Previous Context\\\" of Table 2 shows that there is a large performance drop on all the tasks if block title is the only input. Therefore, we also study the effect of form serialization (see \u00a74.1) which flattens the form context while preserving its tree structure. A naive way of serialization is directly concatenating all available text as NL inputs. Results in this setting (row \\\"- Form Serialization\\\" of Table 2) are much worse than the results of FormLM with form serialization technique. On Block Type Suggestion, the gap is as large as 8.4 on Macro-F1.\\n\\nBlock Type Information\\nA unique characteristic of online forms is the existence of block type (see \u00a72.1). To examine whether FormLM can leverage the important block type information, we run a controlled experiment where block type tokens are replaced by with a placeholder token <type> during form serialization (while other tokens are untouched). As shown in Table 3, removing block type tokens hurts the model performance on all three tasks, which suggests that FormLM can effectively exploit such information.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There are alternative design choices of StructAttn for us to experiment. As Equation (2) shows, there are two bias terms to model the token type and the block-level distance. We compare this design choice (\u201cHybrid\u201d in Figure 5) with adding only the token type bias (\u201cType\u201d) and only the distance bias (\u201cDist\u201d). Note that \u201cHybrid\u201d encodes block-level distance through the exponential decay function, we also compare it with another intuitive design (\u201cHybrid*\u201d) where we use a learnable bias to indicate whether two tokens are within the same block. Besides adding biases, another common practice of modifying attentions is masking. We experiment this design choice (\u201cMask\u201d) by restricting attentions to those tokens in the same node or parent and grandparent nodes within the tree structure. The comparison results are demonstrated in Figure 5.\\n\\n\u201cMask\u201d performs uniformly worse than adding biases. Among the rest of design choices, \u201cHybrid\u201d shows slightly better performance on Options Recommendation and Block Type Suggestion.\\n\\nContinual Pre-training Objectives\\nWe design two objectives (\u00a7 4.3), SpanMLM and BTP, to continually pre-train FormLM on OOF dataset for better domain adaptation. Table 4 shows the ablation results of different objectives. We find FormLM trained with both SpanMLM and BTP performs the best. This suggests SpanMLM which focuses more on the recovery of a single node on the tree and BTP which focuses more on the relationship between different nodes can complement each other.\\n\\n6 Related Work\\n(Semi-)Structured Data Modeling\\nIn this paper, we mainly focus on modelling parsed form data. They follow well-defined structure and are usually created by software such as online services mentioned in \u00a7 1. Existing works (Wang et al., 2022a; Xu et al., 2021; Li et al., 2021; Appalaraju et al., 2021; Aggarwal et al., 2020; He et al., 2017) focus on another type of forms, scanned forms (e.g., photos and scanned PDF files of receipts or surveys), and process multi-modal inputs (text, image). This type of forms requires digitization and parsing before passing to any downstream tasks, which are very different from forms studied in this paper. To the best of our knowledge, the modelling of parsed forms has not been studied before. Existing (semi-)structured data modelling works mainly focus on tables (Yin et al., 2020; Wang et al., 2021), documents (Wan et al., 2021; Liu and Lapata, 2019; Wang et al., 2019), etc. Some works represent the (semi-)structured data as a graph and use graph neural network (GNN) for structural encoding (Wang et al., 2020; Cai et al., 2021). Some other works convert (semi-)structured data into NL inputs to directly use PLMs (Gong et al., 2020) or modify a certain part of transformer models \u2013 e.g., embedding layers (Herzig et al., 2020), attention layers (Eisenbach et al., 2021; Yang et al., 2022), the encoder architecture (Iida et al., 2021). Although it is possible to convert online forms to HTML pages to use models like MarkupLM (Li et al., 2022), the results are suboptimal as shown in \u00a7 5.4 because the unique structural information of online forms are not fully utilized.\\n\\nIntermediate Pre-training\\nIn \u00a7 4.3 we discussed in FormLM how we adapt a general PLM to the form domain through continual pre-training. Intermediate pre-training of a PLM on the target data (usually in a self-supervised way) has been shown efficient on bridging the gap between PLMs and target tasks (Gururangan et al., 2020; Rongali et al., 2020).\"}"}
{"id": "emnlp-2022-main-557", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Many domain specific models (Xu et al., 2019; Chakrabarty et al., 2019; Lee et al., 2020), including those for (semi-)structured data (Yin et al., 2020; Liu et al., 2022), are built with this technique. Following the previous approaches, we design form-specific structure-aware training objectives for the continual pre-training process.\\n\\n7 Conclusion\\nIn this paper, we present FormLM for online form modeling. FormLM jointly considers the semantic and structural information by leveraging the PLM and designing form serialization and structural attention. Furthermore, we continually pre-train FormLM on our collected data with structure-aware objectives for better domain adaptation. An extensive set of experiments show that FormLM outperforms baselines on Form Creation Ideas tasks which assist users in the form creation stage.\\n\\nLimitations\\nIn this work, we conduct research on online form modeling for the first time. While effective in the proposed tasks of Form Creation Ideas, FormLM has some limitations. First, FormLM is designed to assist form designers by recommending questions/options and suggesting the block type. We believe there are more to explore in recommending creation ideas and we plan to design more tasks for Form Creation Ideas, like recommending a whole block, auto-completion, etc., to fully exploit FormLM in the form creation stage. Also, since FormLM performs exceptionally well on Block Type Suggestion, it is worthwhile to consider more fine-grained block types. Second, FormLM only models the form content and leaves out the collected responses. Although form content itself is very informative, it is an important research direction to jointly model online forms and their collected responses for they are useful to other stages of the online form life cycle, especially the form analyzing stage. Furthermore, our collected OOF dataset is limited to English forms and doesn\u2019t have manual labels. We hope to enlarge our dataset with non-English forms and investigate the possibility of adding supervised labels to this dataset in the future to further facilitate the study of online forms.\\n\\nEthics Statement\\nDatasets\\nIn this work, we collect the public OOF dataset for the research community to facilitate future study of online forms. We believe there is no privacy issue related to this dataset. First, the data sources are public available on the Internet, and are anonymously accessible. We complied with the Robots Exclusion Standard during the data collection stage. Second, our dataset only contains form contents and there are no responses or personal information involved. A checklist has been completed at the researchers\u2019 institution to ensure the collected dataset does not have ethical issues.\\n\\nRisks and Limitations\\nOur work proposes FormLM to model online forms and recommend creation ideas to users in the form designing stage. FormLM uses a pre-trained language model, BART, as the backbone. PLMs have a number of ethical concerns in general, like generating biased or discriminative text (Weidinger et al., 2021) and involving lots of computing power in pre-training or fine-tuning (Strubell et al., 2019). The primary risk of our work is that we formulated Question Recommendation and Options Recommendation as generation tasks, but did not include the post-processing of the generated texts in our pipeline. We suggest post-processing the outputs of FormLM to sift out biased or discriminative text before recommending them to the users when applying our technique to online form services. Designing good post-processing techniques is also an interesting avenue for future work.\\n\\nAnother limitation we see from an ethical point of view is that we only consider online forms which use English as the primary language. We are trying to collect online forms in other languages and leave it as a future work to provide a multilingual version of FormLM to assist more users in different parts of the world.\\n\\nComputational Resources\\nThe experiments in our paper require computational resources. However, compared with other LMs pretrained from scratch, FormLM inherits the parameters of its backbone and is continually pre-trained with only 50K online forms. It takes around 8 hours to complete the continual pre-training with 8 NVIDIA V100 GPUs. Despite this, we recognize that not all researchers have access to this resource level, and these computational resources require energy. Notably, all GPU clusters within our organization are shared, and their carbon footprints are monitored in real-time. Our organization is also consistently upgrading our data centers in order to reduce the energy use.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Maisaarah Abd Halim, Cik Feresa Mohd Foozy, Isredza Rahmi, and Aida Mustapha. 2018. A review of live survey application: Surveymonkey and surveygizmo. JOIV: International Journal on Informatics Visualization, 2(4-2):309\u2013312.\\n\\nMilan Aggarwal, Hiresh Gupta, Mausoom Sarkar, and Balaji Krishnamurthy. 2020. Form2Seq: A framework for higher-order form structure extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3830\u20133840, Online. Association for Computational Linguistics.\\n\\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R Manmatha. 2021. Docformer: End-to-end transformer for document understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 993\u20131003.\\n\\nRuichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao. 2021. Sadga: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems, 34:7664\u20137676.\\n\\nTuhin Chakrabarty, Christopher Hidey, and Kathy McKeeown. 2019. IMHO fine-tuning improves claim detection. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 558\u2013563, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nJulian Eisenschlos, Maharshi Gor, Thomas M\u00fcller, and William Cohen. 2021. MATE: Multi-view attention for table transformer efficiency. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7606\u20137619, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nHeng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu. 2020. TableGPT: Few-shot table-to-text generation with table structure reconstruction and content matching. In Proceedings of the 28th International Conference on Computational Linguistics, pages 1978\u20131988, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nSuchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don\u2019t stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342\u20138360, Online. Association for Computational Linguistics.\\n\\nDafang He, Scott Cohen, Brian Price, Daniel Kifer, and C Lee Giles. 2017. Multi-scale multi-task fcn for semantic page segmentation and table detection. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 254\u2013261. IEEE.\\n\\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4320\u20134333, Online. Association for Computational Linguistics.\\n\\nHiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit Iyyer. 2021. TABBIE: Pretrained representations of tabular data. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3446\u20133456, Online. Association for Computational Linguistics.\\n\\nJanet Ilieva, Steve Baron, and Nigel M Healey. 2002. Online surveys in marketing research. International Journal of Market Research, 44(3):1\u201314.\\n\\nRob Johns. 2010. Likert items and scales. Survey question bank: Methods fact sheet, 1(1):11.\\n\\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64\u201377.\\n\\nJon A Krosnick. 2018. Questionnaire design. In The Palgrave handbook of survey research, pages 439\u2013455. Springer.\\n\\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\\n\\nChenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo Si. 2021. StructuralLM: Structural pre-training for form understanding. In\"}"}
{"id": "emnlp-2022-main-557", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-557", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierre Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nLauren Wood, Arnaud Le Hors, Vidur Apparao, Steve Byrne, Mike Champion, Scott Isaacs, Ian Jacobs, Gavin Nicol, Jonathan Robie, Robert Sutor, et al. 1998. Document object model (dom) level 1 specification. W3C recommendation, 1.\\n\\nRobert F Woolson. 2007. Wilcoxon signed-rank test. Wiley encyclopedia of clinical trials, pages 1\u20133.\\n\\nHu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019. BERT post-training for review reading comprehension and aspect-based sentiment analysis. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2324\u20132335, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, and Lidong Zhou. 2021. LayoutLMv2: Multi-modal pre-training for visually-rich document understanding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2579\u20132591, Online. Association for Computational Linguistics.\\n\\nJingfeng Yang, Aditya Gupta, Shyam Upadhyay, Luheng He, Rahul Goel, and Shachi Paul. 2022. TableFormer: Robust transformer modeling for table-text encoding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 528\u2013537, Dublin, Ireland. Association for Computational Linguistics.\\n\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32.\\n\\nOlga Yarmak. 2017. Online surveys in sociology: Opportunities, drawbacks and limitations. In 11th International Conference on Computer Science and Information Technologies CSIT, volume 4, pages 476\u2013477.\\n\\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. 2020. TaBERT: Pretraining for joint understanding of textual and tabular data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413\u20138426, Online. Association for Computational Linguistics.\\n\\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced language representation with informative entities. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1441\u20131451, Florence, Italy. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Details of Open Online Forms Dataset\\n\\nFigure 6: Frequent Words Among Titles of Forms in OOF Dataset.\\n\\nOOF (Open Online Forms) dataset consists of 62K public forms collected on the Web, covering a wide range of domains and purposes. Figure 6 shows some frequent words among titles of the collected data.\\n\\nA.1 Dataset Preprocessing\\n\\nWe crawled 232,758 forms created by a popular online form service on the Internet and filter the crawled data using the following constraints: (1) have at least one question block; (2) have no duplicate question blocks; (3) detected as \u201cen\u201d by Language Detection API of Azure Cognitive Service. Finally, 62,380 forms meet all constraints. We randomly split them into 49,904 for training, 6,238 for validation and 6,238 for training.\\n\\nAs introduced in \u00a72.2, we parsed the crawled HTML pages into JSON format according to the online form structure. Specifically, each JSON file contains keys of \u201ctitle\u201d, \u201cdescription\u201d and \u201cbody\u201d which correspond to form title (T), form description (Desc), and an array of blocks (\\\\{B_1, \\\\ldots, B_n\\\\}). Each block contains keys of \u201ctitle\u201d, \u201cdescription\u201d and \u201ctype\u201d. For Choice type blocks and Rating type blocks, they further contain the key of \u201coptions\u201d; for Likert type blocks, they further contain keys of \u201crows\u201d and \u201ccolumns\u201d. For Description block, we only keep the plain NL text and remove possible information of other modalities (i.e., image, video) because only around 0.1% of Description blocks contain video and 2.0% contain image. When parsing the HTML pages into JSON format, we also remove non-ASCII characters within the form.\\n\\n5 https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes\\n6 https://docs.microsoft.com/en-us/azure/cognitive-services/language-service/language-detection/overview\\n\\nA.2 Form Length Distribution\\n\\nWe define the length of an online form as the number of blocks within it. Around 80% of collected forms have a form length no greater than 20. The detailed distribution of form length is shown in Figure 7. As we have discussed in \u00a75.1, we further perform random sampling to construct our experiment dataset to avoid sample biases introduced by those lengthy forms.\\n\\nFigure 7: Form Length Distribution of Forms in OOF Dataset.\\n\\nB Model Configurations\\n\\nWe compare FormLM with four baseline models, RoBERTa, GPT-2, MarkupLM, and BART. FormLM adds a small number of additional parameters to its backbone model (278K for FormLM and 208K for FormLM\\\\_BASE) to encode structural information in attention layers (\u00a74.2). Table 5 shows model configurations of FormLM and baselines in our experiments.\\n\\n| Model       | #Params | #Layers |\\n|-------------|---------|---------|\\n| RoBERTa     | 124M    | 12      |\\n| GPT-2       | 124M    | 12      |\\n| MarkupLM    | 135M    | 12      |\\n| BART\\\\_BASE  | 139M    | 6+6     |\\n| BART        | 406M    | 12+12   |\\n| FormLM\\\\_BASE| 139M    | 6+6     |\\n| FormLM      | 406M    | 12+12   |\\n\\nTable 5: Model Configurations of FormLM and baselines.\\n\\nC More Implementation Details\\n\\nContinual Pre-training Details\\n\\nWe conduct continual pre-training on the training set of the OOF dataset using SpanMLM and BTP objectives (\u00a74.3). We adopt a masking budget of 15% in SpanMLM and do BTP on all training samples. We train...\"}"}
{"id": "emnlp-2022-main-557", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FormLM for 15K steps on 8 NVIDIA V100 GPUs with 32G GPU memory. We set the total batch size as 32 and the max sequence length as 512. We use AdamW optimizer (Loshchilov and Hutter, 2019) with $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.999$ and the learning rate of $5e^{-5}$. It takes around 8 hours to complete the continual pre-training on our machine.\\n\\nFine-tuning Details\\nAmong our downstream tasks, Next Question Recommendation and Options Recommendation are formulated as conditional generation tasks. We use the form serialization procedure (\u00a74.1) to convert the available context into model inputs. We fine-tune FormLM for 5 epochs with the total batch size of 32, the max source sequence length of 512, and the max target sequence length of 64. We load the best model which has the highest ROUGE-2 score on the validation set in the training process. During generation, we do beam search and set the beam size as 5. Block Type Classification is formulated as a sequence classification task. We follow the original implementation of BART by feeding the same input into the encoder and decoder and passing the final hidden state of the last decoded token into a multi-class linear classifier for classification. We fine-tune FormLM with 5 epochs with the total batch size as 32 and load the best model which has the highest Macro-F1 score on the validation set during the fine-tuning process.\\n\\nD Qualitative Study\\nOnline forms, as a special format of questionnaires, are mainly used to collect information, i.e., demographic information, needs, preferences, etc. (Krosnick, 2018). As shown in Figure 6, the online forms in the OOF dataset are more about objective topics like \u201cApplication\u201d and \u201cRegistration\u201d because these information collection scenarios prevail in the daily usage. To collect information effectively, a good questionnaire should include questions related to the topic and these questions must be logically connected with each other. Also, for those close-ended questions (the majority of them are Choice type questions), they are expected to offer all possible answers for respondents to choose from but not include off-topic options which may cause confusion (Reja et al., 2003). These criteria of good questionnaires restrict the searching space of online form composition, thus making the automatic recommendation of creation ideas conceptually possible.\\n\\nIn \u00a75.4, Figure 4 shows some questions recommended by FormLM. FormLM is able to recommend questions like \u201cDestination\u201d, \u201cDeparture Date\u201d, \u201cType of Accommodation\u201d which are highly related to the topic of travelling and can help collect meaningful information for the travel agency. For Options Recommendation, FormLM can accurately identify polar questions and recommend \u201cYes\u201d, \u201cNo\u201d as candidate options. Also, since FormLM is continually pre-trained on a large amount of online forms, it has no difficulty recommending options for those frequently asked questions, e.g., \u201cGender\u201d, \u201cCurrent Educational Qualifications\u201d, etc.\\n\\nMore interestingly, we notice that FormLM can provide accurate recommendation for questions which are related to their previous contexts. Figure 8 gives two sample outputs by FormLM for Options Recommendation. In the left sample, FormLM gives concrete suggestions which are based on the form title; in the right sample, the recommended locations are all related to school, and they accord well with the domain of this form. We assume that such good performance can be attributed to the effective understanding of form structure and context.\\n\\nE Human Evaluation\\nApart from reporting automatic evaluation results using ROUGE scores, we further conduct human evaluations for Question Recommendation and Options Recommendation. We randomly choose 50 samples from the test sets of the two tasks and collect the recommended question / options from 5 models (GPT-2, BART BASE, BART, FormLM BASE, FormLM). We use an HTML website (actually an online form service) to collect the manual labels. Human evaluation instructions are shown in Figure 9 and Figure 10. Eight experts familiar with online form software products participate in the experiment. For each sample of a task, we construct a Likert question containing the 5 outputs (randomly shuffled and anonymized) of the models. For each sample, three experts compare the 5 outputs using a rating scale of 1 to 5 (the higher, the better) at the same time to achieve better comparison and annotation consistency across different outputs. So in total, we collect 150 expert ratings for each model on each task.\\n\\nThe evaluation results are shown in Table 6 and Table 7. We can see FormLM and FormLM BASE outperform all baseline models on both Question and Options Recommendation when manually evaluated.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I am a ... Choice\\n\\nSuggested options:\\n\\n| Classroom | Hallway | Cafeteria |\\n|------------|---------|-----------|\\n| Restroom   | Bus     | Online    |\\n\\nOnline Bully Report\\nChoosing to help someone in need is very brave. If you see this happening again, please report it. Together we can stop bullying.\\n\\nName of victim(s)\\nName of Student(s) bullying\\n\\nSelect a School\\nBay High School\\nBay - Waveland Middle School\\n\\nDate of this incident (as close as possible)\\n\\nFigure 8: Sample Outputs by FormLM for Options Recommendation. The suggested options are highlighted in blue.\\n\\nTable 6: Summary of Human Evaluation Ratings for Question Recommendation.\\n\\n|           | 5 | 4 | 3 | 2 | 1 | Avg. |\\n|-----------|---|---|---|---|---|------|\\n| GPT-2     | 16| 22| 23| 20| 69| 2.31 |\\n| BART BASE | 28| 21| 12| 23| 66| 2.48 |\\n| BART      | 26| 23| 25| 18| 58| 2.61 |\\n| FormLM BASE | 63| 47| 13| 15| 12| 3.89 |\\n| FormLM    | 72| 41| 16| 9 | 12| 4.01 |\\n\\nTable 7: Summary of Human Evaluation Ratings for Options Recommendation.\\n\\n|           | 5 | 4 | 3 | 2 | 1 | Avg. |\\n|-----------|---|---|---|---|---|------|\\n| GPT-2     | 16| 10| 6 | 9 | 109| 1.77 |\\n| BART BASE | 63| 28| 17| 14| 28 | 3.56 |\\n| BART      | 68| 30| 23| 9 | 20 | 3.78 |\\n| FormLM BASE | 71| 35| 18| 9 | 17| 3.89 |\\n| FormLM    | 89| 29| 14| 7 | 11| 4.19 |\\n\\nWe further conduct Wilcoxon signed-rank test (Woolson, 2007) which is a non-parametric hypothesis test for the matched-pair data to check statistical significance of the comparison between FormLM, FormLM BASE and their backbone PLMs. At 95% confidence level, when comparing FormLM with BART and comparing FormLM BASE with BART BASE, both p-values from Wilcoxon test are less than 0.005. These results show that our models have better performance on these two generation tasks than their backbone PLMs which are well-known for conditional generation.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Online forms are widely used to collect data in everyday scenarios and many software products provide services to help users create online forms which consist of multiple blocks. However, for each form question, form designers need to write an informative title, specify its type, and provide other required components. Such a process is time-consuming. Therefore, we want to design a model to recommend creation ideas and suggestions to online form designers.\\n\\nQuestion Recommendation\\nQuestion Recommendation aims at providing users with a recommended question based on the selected block type and the previous context (form title, form description, previous blocks).\\n\\nIn this study, you will evaluate 10 sets of questions recommended by 5 different models. (Model outputs have been randomly shuffled.) The evaluation interface is as follows:\\n\\nFor each sample, you need to:\\n\\nStep 1: Click the link behind \u201ccontext:\u201d to see the previous context of the form.\\n\\nStep 2: Check the block type marked in bold black.\\n\\nStep 3: Score the recommendations. Each row in the Likert table refers to a model output. You can score each output with the relative score ranging from 1 to 5 (higher score indicates better recommended question).\\n\\nNote that your score should consider three parts:\\n\\n\u2022 Whether the question has clear meaning.\\n\u2022 Whether the question is suitable to the form context (relevant to the form title, non-overlap with previous questions, logically coherent with previous questions, etc.).\\n\u2022 Whether the question suits the selected block type.\"}"}
{"id": "emnlp-2022-main-557", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Options Recommendation\\n\\nChoice blocks are frequently used in online forms. When creating a Choice block, one should additionally provide a set of options. Options Recommendation aims recommending a set of options to users based on the current block title and all the previous context (form title, form description, previous blocks).\\n\\nIn this study, you will evaluate 10 sets of questions recommended by 5 different models. (Model outputs have been randomly shuffled.) The evaluation interface is as follows:\\n\\nFor each sample, you need to\\n\\nStep 1: Click the link behind \\\"context:\\\" to see the previous context of the form and the choice block title that models will make recommendations for.\\n\\nStep 2: Score the recommendations. Each row in the Likert table refers to a model output. Note that we expect models to recommend a set of options, and we concatenate the options with a vertical bar \\\"|\\\". You can score each output with the relative score ranging from 1 to 5 (higher score indicates better recommended options).\\n\\nNote that your score should consider three parts:\\n\\n\u2022 Whether each option has clear meaning and whether it is a suitable answer to the Choice block title.\\n\\n\u2022 Whether this set of options are logically related to each other and non-overlapped.\\n\\n\u2022 Whether this set of options are reasonable when considering the previous form context.\\n\\nFigure 10: Human Evaluation Instructions. (Page 2 / 2)\"}"}
