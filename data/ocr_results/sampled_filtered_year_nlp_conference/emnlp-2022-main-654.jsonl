{"id": "emnlp-2022-main-654", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Screenshot of the annotation interface on AMT.\\n\\ndetermining the $\\\\alpha$ from \\\\{0.90, 0.95, 0.99, 0.995, 0.998, 0.999, 1.0\\\\}, where we find 0.998 to perform the best. We then tune the weight of CLIP-BERTS CORE from \\\\{1, 2, 5\\\\} and find that 2 performs the best for both datasets.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating and Improving Factuality in Multimodal Abstractive Summarization\\n\\nDavid Wan and Mohit Bansal\\nUniversity of North Carolina at Chapel Hill\\n{davidwan,mbansal}@cs.unc.edu\\n\\nAbstract\\nCurrent metrics for evaluating factuality for abstractive document summarization have achieved high correlations with human judgment, but they do not account for the vision modality and thus are not adequate for vision-and-language summarization. We propose CLIPBERTS CORE, a simple weighted combination of CLIPScore (Hessel et al., 2021) and BERTScore (Zhang* et al., 2020) to leverage the robustness and strong factuality detection performance between image-summary and document-summary, respectively. Next, due to the lack of meta-evaluation benchmarks to evaluate the quality of multimodal factuality metrics, we collect human judgments of factuality with respect to documents and images. We show that this simple combination of two metrics in the zero-shot setting achieves higher correlations than existing factuality metrics for document summarization, outperforms an existing multimodal summarization metric, and performs competitively with strong multimodal factuality metrics specifically fine-tuned for the task. Our thorough analysis demonstrates the robustness and high correlation of CLIPBERTS CORE and its components on four factuality metric-evaluation benchmarks. Finally, we demonstrate two practical downstream applications of our CLIPBERTS CORE metric: for selecting important images to focus on during training, and as a reward for reinforcement learning to improve factuality of multimodal summary generation w.r.t automatic and human evaluation.\\n\\n1 Introduction\\nMultimodal abstractive summarization is the task of generating an abridged text that contains the most important information of the source inputs from various modalities. This challenging task builds upon the success of document summarization, where the input is only text documents. For document summarization, there has been tremendous progress in improving the quality of the summaries with the help of large pre-trained models (Lewis et al., 2020; Zhang et al., 2020; Raffel et al., 2020). However, one crucial problem for such models is hallucination, where the model generates contents that are not present or entailed by the document (Maynez et al., 2020; Falke et al., 2019). While there have been significant advancements in developing metrics that correlate highly with the human judgment of factuality (Kryscinski et al., 2020; Durmus et al., 2020; Goyal and Durrett, 2021; Scialom et al., 2021), these metrics only measure factuality between the document and the summary. The lack of judgment between other modalities, such as vision, and the summary makes such metrics not suitable for multimodal settings.\\n\\nWe demonstrate this with the example in Figure 1. The given summary captures less relevant information (cutting the nail) from the document, but it is still considered factual to the document. However, the image shows the main point of the document (finding the place where the nail separates from the quick), making the summary not factual with respect to the image. Current factuality metrics do not account for the image and thus cannot correctly assess factuality for multimodal summaries.\\n\\nIn this work, we introduce a metric that judges factuality of the summary with respect to each input modality. Focusing on the vision-and-language summarization, we propose CLIPBERTS CORE, a simple and robust automatic factuality evaluation metric for multimodal summaries that combines two successful metrics: CLIPScore (Hessel et al., 2021), which shows strong performance in detecting hallucinations between image and text, and BERTScore (Zhang* et al., 2020), which correlates well with the human judgment of factuality for document summarization (Pagnoni et al., 2021).\\n\\nNext, due to the lack of corpora containing ground-truth human factuality judgments to evaluate the quality of multimodal factuality metrics, we collect human judgments of factuality with respect to documents and images. We show that this simple combination of two metrics in the zero-shot setting achieves higher correlations than existing factuality metrics for document summarization, outperforms an existing multimodal summarization metric, and performs competitively with strong multimodal factuality metrics specifically fine-tuned for the task. Our thorough analysis demonstrates the robustness and high correlation of CLIPBERTS CORE and its components on four factuality metric-evaluation benchmarks. Finally, we demonstrate two practical downstream applications of our CLIPBERTS CORE metric: for selecting important images to focus on during training, and as a reward for reinforcement learning to improve factuality of multimodal summary generation w.r.t automatic and human evaluation.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We propose a simple and robust factuality metric for multimodal summarization based on a combination of CLIPScore and BERTScore. We create MUFAE, a meta-evaluation for the quality of multimodal factuality metrics. We present a detailed study of our metric and its components on various factuality metric-evaluation benchmarks and present strong empirical evidence of its robustness. We demonstrate two useful downstream applications of our metric to improve the factuality of multimodal abstractive summarization models.\\n\\nCLIPBERTS CORE consists of two parts that tackle the image-summary and document-summary factuality judgments, respectively. We show an illustration of the computation in Figure 1.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thus, it serves as a fitting candidate for factuality evaluation between the image and the summary. We use CLIP-S, which calculates the cosine similarity between the image embedding $v$ and the text embedding of the summary sentence $t$. To adapt to multimodal summarization, where we have multiple images and multi-sentence summaries, we take the average of the scores of all image and sentence pairs. Formally, given a list of image embeddings $V$ and summary sentence embeddings $T$ from CLIP's image and text encoder, respectively:\\n\\n$$CLIP-S(V, T) = \\\\frac{1}{|V||T|} \\\\sum_{i=1}^{||V||} \\\\sum_{j=1}^{||T||} \\\\text{cossim}(v_i, t_j)$$\\n\\nTo better detect hallucinations present in the summary with respect to the document, we use the precision variant of BERTScore (Zhang* et al., 2020), which achieves high correlations with human judgments of factuality for document summarization (Pagnoni et al., 2021). See Section 4.4 for a detailed discussion and comparison against other text-based factuality metrics. Formally, given the contextual embeddings of each token in the document $D$ and summary $S$, it calculates the pairwise cosine similarity between each document and summary token embeddings:\\n\\n$$BERT-S = \\\\frac{1}{|S|} \\\\max_{d \\\\in D} \\\\sum_{s \\\\in S} \\\\text{cossim}(d, s)$$\\n\\nFull Metric. The final score is a combination of the factuality score for image-summary with CLIP-S and that for document-summary with BERT-S:\\n\\n$$CLIPBERTS = \\\\alpha \\\\text{CLIP-S} + (1 - \\\\alpha) \\\\text{BERT-S},$$\\n\\nwhere $\\\\alpha$ is a tunable parameter. Please see Section 3.4 for other ways to learn this combination.\\n\\n3 Metric Meta-Evaluations\\n\\nNext, after defining the multimodal factuality metric $CLIPBERTS$, we want to evaluate the quality of this new metric by checking whether it correlates with human judgments, similar to what has been done for textual factuality metrics (Wang et al., 2020; Kryscinski et al., 2020; Durmus et al., 2020; Scialom et al., 2021). As there is no human annotations of factuality for multimodal summarization, we first propose a multimodal factuality (MFA) benchmark derived from WikiHow to test the correlations of $CLIPBERTS$ with human judgments of factuality.\\n\\n3.1 MFA Benchmark\\n\\nDataset. We construct an English multimodal WikiHow summarization dataset (Koupaee and Wang, 2018) for the human evaluation, as this dataset has been extensively studied for document summarization (Koupaee and Wang, 2018; Ladhak et al., 2020), and the images associated with the how-to-articles are relevant to the text. We use a recent WikiHow collection effort by Yang et al. (2021) containing images. We generate the step-level multimodal WikiHow dataset by breaking each article into steps and following the construction described in Koupaee and Wang (2018): We consider the first sentence of a step as the summary and the rest of the paragraph as the document, and add the corresponding image. We randomly select 6,000 articles as the validation and test set each, and break each example into steps. Statistics of the dataset can be found in Table 16 of the Appendix. For annotation, we randomly sample 50 articles from the test set, and evaluate the generated summaries for all the corresponding steps. Similar to Kryscinski et al. (2020), we split the 50 articles into 10 articles as the validation and 40 as the test set, resulting in 52 and 193 step-level examples for the validation and test set, respectively.\\n\\nSummarization Systems. Following Pagnoni et al. (2021), we include model summaries from summarization models with varying factuality capabilities. We train four abstractive summarization systems on the multimodal WikiHow dataset, including two document summarization models, T5 (Raffel et al., 2020) and PEGASUS (Zhang et al., 2020), and two multimodal summarization models, CLIP-BART (see section 5), and MOF (Zhu et al., 2018). Details of the models are provided in Appendix A.2. We additionally include the reference summaries, resulting in a total of 260 and 965 examples for the validation and test set, respectively.\\n\\nWe initially attempted to crawl and align images to the original summarization dataset, but many of the links to the articles are no longer valid or the contents have changed since the original construction.\\n\\nAlthough we are primarily interested in the step-level summarization setup for annotation purpose, this creation process also allows future works to experiment with the article-level summarization task by concatenating all the summaries, documents and images of an article.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We conduct the annotations on Amazon Mechanical Turk (AMT) platform. For each HIT, we provide the document and the image and ask the workers to read the five summaries. The workers then need to choose whether each summary is faithful to the document and the image separately. An example of the annotation page can be seen in Appendix A.3. For high-quality annotations, we first conduct a qualification test, where we compare the annotations from the workers against annotations by the authors. Only the workers who have the same annotations on the selected example can perform the actual annotation task. We further select workers from the United States, who have more than 10,000 HITs approved and an approval rate greater than 98%. We pay 0.18 USD per task to ensure a $12/hourly rate. Each task consists of three unique workers, and we take the majority class for the document and image factuality judgments, similar to Pagnoni et al. (2021). We consider the summary to be faithful only if it is considered faithful to both document and image. We also experiment beyond binary judgment by taking the average over the two factuality judgment to indicate a summary may be partially faithful to one of the source, which is shown in Appendix B.\\n\\nInter-Annotator Agreement. We report Fleiss $\\\\kappa$ (Fleiss, 1971) and percentage $p$ of annotators agreement on the majority class, similar to Durmus et al. (2020). We obtain $\\\\kappa = 0.50$, with $p = 88.5\\\\%$, indicating a moderate agreement (Landis and Koch, 1977).\\n\\n### 3.2 Experimental Setup\\n\\n**CLIPBERTS CORE.** For CLIP-S, we use the RN50x64 visual backbone instead of the ViT-B/32 version used in the original metric, as the larger backbone shows a higher correlation on factuality benchmarks. For BERT-S, we choose RoBERTa-large-mnli to compute the contextualized embeddings instead of RoBERTa-large for the same reason. We refer readers to Section 4 for more details. We use the validation set of MUFA ME to tune $\\\\alpha$, where we find that $\\\\alpha = 0.25$ achieves the best correlations on the combined judgment. We use this parameter for all experiments (See Section 3.4 for other ways to learn this combination).\\n\\n### Table 1: Pearson correlation coefficients between automatic metrics and human judgments of factuality with respect to the document, image, and combined.\\n\\n| Metric          | Document | Image | Combined |\\n|-----------------|----------|-------|----------|\\n| FactCC          | 0.01     | -0.00 |          |\\n| DAE             | 0.50     | 0.38  |          |\\n| QuestEval       | 0.41     | 0.32  |          |\\n| CLIPText-S      | 0.19     | 0.14  |          |\\n| BERTScore       | 0.54     | 0.40  |          |\\n| BERT-S          | 0.58     | 0.43  |          |\\n| CLIP-S          | 0.22     | 0.21  |          |\\n| IMMax           | -0.10    | 0.07  |          |\\n| Triplet Net     | 0.25     | 0.25  |          |\\n| MMAE            | 0.21     | 0.26  | 0.22     |\\n| RefCLIP-S       | 0.20     | 0.26  | 0.25     |\\n| CLIP-S + DAE    | 0.53     | 0.33  | 0.41     |\\n| Triplet Net + BERT-S | 0.58 | 0.44  | 0.47     |\\n\\nBaseline Metrics. Having separate judgments for document-summary, image-summary, and multimodal settings allows us to evaluate the metrics\u2019 performance with different modality combinations. For document-summary, we compare against existing factuality metrics, including FactCC (Kryscinski et al., 2020), DAE (Goyal and Durrett, 2021), QuestEval (Scialom et al., 2021), and the original BERTScore (Zhang* et al., 2020). We also measure the performance of the text-matching component of CLIPScore, which we refer to as CLIPText-S. For image-summary evaluation, we compare our CLIP-S against Triplet Network, as described in Yang et al. (2021). We train this metric on the multimodal WikiHow dataset, allowing comparisons of correlations between CLIP-S in the zero-shot setting and a fine-tuned metric for this task. For multimodal factuality metrics, we experiment with several weighted combinations of document-summary and image-summary metrics by tuning the weights on the validation set, including combinations of DAE with CLIP-S, Triplet Network with BERT-S, and RefCLIP-S. We also compare to MMAE (Zhu et al., 2018) developed for evaluating the summary quality of multimodal summarization. As the metric is originally designed for a different dataset, we similarly use the multimodal WikiHow to train its image-summary component IMMax. We refer the readers to Appendix A.1 for details of the metrics.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3 Meta-Evaluation Results\\n\\nTable 1 shows the Pearson correlation of the automatic metrics. We first note that the combined judgments require taking both modalities into consideration. Metrics that only consider the document correlate less with the combined judgment than with the document-only judgment, indicating the importance of capturing the vision modality for evaluating factuality for multimodal summarization. Multimodal factuality metrics, on the other hand, show positive transfers, as they correlate higher on all three settings than its components.\\n\\nNext, for the document-summary factuality judgments, BERT-S achieves the highest correlation, outperforming DAE by 8 points and the original BERTScore by 4 points. Compared to MMAE, which is developed for evaluating the quality of multimodal summarization, CLIPBERTS\u663e\u8457\u5730 outperforms on all three categories, showing the importance of targeting the factuality aspect. While Triplet-Net achieves better correlations on image, CLIPBERTS\u5b9e\u9645\u4e0a outperforms the fine-tuned variants for the document case and provides the same correlations on the combined case. We thus stress the simplicity of CLIPBERTS of only requiring the use of two off-the-shelf metrics in the zero-shot setting without the need for extra training to compare competitively with fine-tuned method.\\n\\n3.4 Comparison of Combination Strategies\\n\\nWhile CLIPBERTS uses $\\\\alpha$ to decide the weights for CLIP-S and BERT-S, we also explore using logistic regression (logis) and multi-layer perceptron (MLP) to output a final score given the two modules, following Zhu et al. (2018).\\n\\nSimilar to the $\\\\alpha$ parameter, we tune the two methods by fitting the metric on the combined human judgment scores and selecting the parameters that would achieve the highest Pearson correlation on the development set of MUFAME meta-evaluation dataset.\\n\\nThe result is presented in Table 2. While logistic regression performs the worst, using MLP for combining the two modules provides similar correlations as CLIPBERTS that uses the $\\\\alpha$ parameter. Specifically, MLP provides a point increase in correlations with respect to the document but provides the same correlations on the combined judgment. The weight combination strategies can be chosen based on preference, but we advocate for the simplicity with the $\\\\alpha$ parameter.\\n\\n4 Additional Factuality Metric-Evaluation Benchmarks\\n\\nWe evaluate CLIPBERTS and its components on additional factuality metric-evaluation benchmarks, focusing on how robust the metrics performs across a variety of tasks and domains.\\n\\n4.1 WikiHow Factuality\\n\\nWe propose the WikiHow Factuality (WikiHowFact) task that evaluates how well the metric can choose the correct summaries over incorrect ones. We derive this task from WikiHow VGSI (Yang et al., 2021) to evaluate the text-image matching performance as a ranking experiment, which has been explored for factuality metric evaluation (Falke et al., 2019). An example can be seen in Figure 2. Each example uses the correct document and image as the prompt and includes four choices consisting of the correct summary and three negative choices.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: WikhowFact ranking accuracy given different input modalities. CLIPBERTS CORE shows the largest positive transfers when combined, outperforming RefCLIP-S on all settings and Triplet Net + BERT-S on random and category settings.\\n\\nSummaries generated by random, category, and similarity sampling strategies described in Yang et al. (2021). We note that this setup is actually a more challenging task than the original VGSI task. See Appendix C.1 for more details. Similar to the meta-evaluation in Section 3.2, we consider the document, image and combined settings depending on the choice of the prompt, and evaluate using the same sets of metrics. We further compare CLIP-S to that using the smaller ViT-B/32 visual backbone. We compute the ranking accuracy of assigning a higher score to the correct summary. See Appendix C.1 for more details.\\n\\nResults. We present the WikiHowFact result in Table 3. First, for the image-summary setting, we observe the power of larger visual backbone at improving factuality, as CLIP-S achieves a 3, 5, and 6.3 point increase compared to CLIP-S ViT-B/32 for the random, category, and similarity split, respectively. For document-summary setting, CLIPText-S and BERT-S achieve high accuracy across the sampling strategies. Interestingly, CLIPText-S performs better than BERT-S, but this does not apply to the multimodal case: CLIPBERTS CORE actually outperforms RefCLIP-S, showing the better positive transfer between CLIP-S and BERT-S. Similar to the meta-evaluation results, the Triplet Network outperforms CLIP-S for the image-summary setting, but the difference on random and category splits is only around 1 point. CLIPBERTS CORE still outperforms Triplet Network + BERT-S on the random and category splits, indicating the strong performance of combining the two metrics for evaluating factuality.\\n\\nTable 4: FOIL accuracy. * indicates results taken from Hessel et al. (2021). Top section represents correlations of factuality metrics for the document-summary setting, while the bottom section show that for the image-summary setting (no-ref) and multimodal setting.\\n\\n4.2 Hallucination in Image Captioning\\n\\nThe FOIL (Shekhar et al., 2017) dataset measures how well the metric can differentiate correct MSCOCO captions from hallucinated ones generated by adversarially swapping out noun phrases. We follow Hessel et al. (2021) and evaluate metrics on the paired setting. We compute the ranking accuracy by giving only the image (no-ref), and with 1 or 4 additional reference captions. We compare CLIPBERTS CORE and its components with CLIPScore variants using the ViT-B/32 backbone. We refer the readers to Appendix C.2 for more details and results on all visual backbones. We present the results in Table 4. BERT-S achieves the highest accuracy in terms of the text-matching performance. Especially when more text (4 references) is provided, it outperforms CLIPText-S by 1.6 points. For image-text matching, we observe similar improvement with larger visual backbones. CLIPBERTS CORE showcases its strength at positive transfer of its two components: we observe improvement over RefCLIP-S RN50x64 of 0.7 points for 1-ref and 1.2 points for 4-ref.\\n\\n4.3 Fine-grained Visual Grounding\\n\\nBISON (Hu et al., 2019) measures the ability of the metric to select the correct MSCOCO image from a semantically similar image, requiring more fine-grained visual grounding to achieve high accuracy. We compare the image-summary metrics, and refer the readers to Appendix C.3 for results on all CLIP-S variants. Table 5 shows that CLIP-S actually outperforms the fully fine-tuned SOTA metric, SCAN t2i (Lee et al., 2018), indicating its robustness and strong text-image detection performance in the zero-shot setting. Triplet Network on\"}"}
{"id": "emnlp-2022-main-654", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: BISON accuracy. * indicates result copied from Hu et al. (2019).\\n\\nThe other hand is not robust to this task, achieving much lower accuracy than all other metrics.\\n\\n4.4 Document Summarization Factuality Evaluation\\n\\nWe compare how BERT-S and CLIPText-S correlate on FRANK, a factuality benchmark evaluation for document abstractive summarization containing 2,250 annotations for generated summaries on XSum (Narayan et al., 2018) and CNN/DM (Hermann et al., 2015). We report Pearson and Spearman correlations, using the official evaluation script.\\n\\nThe result is shown in Table 13 in the Appendix. CLIPText-S does not perform well for detecting faithful summaries for summarization, as Pearson and Spearman coefficients are around 0.10 for all datasets and 0.05 for XSum Spearman. In contrast, BERT-S that uses RoBERTa (Liu et al., 2019) model finetuned on the MNLI (Williams et al., 2018) correlates higher than the original BERTScore on Pearson and Spearman across both datasets. It is thus useful to treat factuality as an entailment problem and use the appropriate model.\\n\\n5 Downstream Applications\\n\\nFinally, we present two useful downstream applications for improving factuality of multimodal summarization models: first by using the metric as a reference image selection to guide the model in attending important images, and second by using it as a reward for self-critical sequence training. For both applications, we train strong baseline models by adapting CLIP-BART (Sung et al., 2022) for multimodal summarization. Specifically, we extract visual features with CLIP and use a projection layer to transform the dimension of the visual representation to the correct dimension of BART (Lewis et al., 2020). Then, the projected features are concatenated with the text features from the original encoder as the joint input representation for BART. See Appendix D for more details.\\n\\n5.1 Multimodal Visual Guidance\\n\\nOne of the well-known tasks is multimodal summarization with multimodal output (Zhu et al., 2020, MSMO), which incorporates the associated images with the CNN/DM articles. The authors show that previous models suffer from modality bias, as the cross-entropy loss is only based on the text modality. To help the model also attend to the vision modality, the authors propose to create visual references by ranking and selecting the most relevant input images. While the authors show improved performance by ranking the images by the ROUGE score between the corresponding caption and the reference summary, such reference does not explicitly guide the model to generate summaries faithful with respect to the images. We thus propose to use CLIPBERTS to select reference images. To incorporate the visual guidance into the training, we add a guidance loss by minimizing the cross-entropy loss, where we consider the selected images by the reference as correct, and the remaining images as incorrect. We use each image's hidden representation from the encoder to produce a binary prediction using a linear layer.\\n\\nWe compare against the model using ROUGE as the visual guidance. Following Zhu et al. (2018), we report the performance of the models on ROUGE, and image precision (IP) of the model's recommended images and human-annotated relevant images. We additionally evaluate factuality using BertScore, FactCC, DAE, and QuestE-val. The result is shown in Table 6. We observe a correlation between the guidance metric and the metric score, as the model with ROUGE guidance achieves higher ROUGE scores, and the model with CLIPBERTS guidance improves all factuality metrics and IP. Though the gain is relatively small, the improvement on factuality metrics is greater than the negligible drop in ROUGE.\\n\\n5.2 Self-Critical Sequence Training with CLIPBERTS Reward\\n\\nA more generalized application to improve factuality is to use CLIPBERTS as a reward for the self-critical sequence training (Rennie et al., 2017, SCST), which optimizes the model using the REINFORCE algorithm (Williams, 1992). Formally, given document d, images v, and the summary y, the self critical loss is defined as:\\n\\n$$L_{SCST} = -\\\\sum_{t=1}^{N} \\\\log P(y_{st}|y_{s1},...,y_{st-1},d,v)$$\\n\\nwhere$$r(y)$$ is the reward function for the summary y.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: MSMO result with different guidance strategies. DAE: lower is better (\u2193). For reference, the SOTA model UniMS (Zhang et al., 2022) achieves 42.94 for R1, 20.50 for R2, and 69.38 for image precision (IP).\\n\\nCLIPBERTS CORE as a guidance improves IP and all factuality metrics with a minor decrease in ROUGE.\\n\\nTable 7: SCST result on MMSS and MSMO. DAE: lower is better (\u2193). We train a CLIP-BART model as the base model for MMSS, and use CLIP-BART CLIPBERTS CORE guidance as the base model for MSMO. We observe consistent improvement on all metrics with SCST over the base model on MMSS, and consistent improvement on all factuality metrics on MSMO. For reference, the SOTA model on MMMS by Li et al. (2020b) achieves 48.19/25.64/45.27 for ROUGE.\\n\\nTable 8: Human evaluation results on MMSS. Model with SCST training are statistically significantly more factual with respect to document, image, and both. * indicates $p<0.05$ and ** indicates $p<0.01$.\\n\\nwhere $r(\u00b7)$ is a reward function, $y_s$ is the sampled summary, and $\\\\hat{y}$ is the summary obtained by greedy decoding. We follow previous works (Pasunuru and Bansal, 2018; Li et al., 2019; Parnell et al., 2021) and train on the combined loss of cross-entropy $L_{XE}$ and the self-critical loss:\\n\\n$$L = \\\\alpha L_{RL} + (1 - \\\\alpha) L_{XE},$$\\n\\nwhere we set $\\\\alpha = 0.998$. We use CLIPBERTS CORE and ROUGE-2 as the rewards, so as to improve factually while maintaining informativeness. Following Pasunuru and Bansal (2018), we alternate the rewards for each step during the training. We upweight CLIP-BERTS CORE by 2x (tuned on the validation set).\"}"}
{"id": "emnlp-2022-main-654", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Related Work\\n\\nMultimodal Summarization. The task of multimodal summarization takes additional inputs from multiple modalities apart from the input text document, including images (Li et al., 2018; Zhu et al., 2020; Li et al., 2020a) and videos (Li et al., 2020c; Palaskar et al., 2019). To incorporate multiple modalities, many works have developed models with multimodal attention (Zhu et al., 2020). When multiple images are present, the rich information present in the images may distract and thus hurt the model's performance. To this end, approaches such as selective gating (Li et al., 2020b), visual guidance (Zhu et al., 2020), and knowledge distillation (Zhang et al., 2022) have been proposed. While these methods have demonstrated improvement in ROUGE, to the best of our knowledge, factuality for such tasks has not been studied. We aim to provide an evaluation benchmark for evaluating factuality, and demonstrate methods to improve factuality for the multimodal summarization task.\\n\\nFaithfulness and Factuality Metrics.\\n\\nMany metrics have been proposed to evaluate the factuality of generated summaries. The metrics can be roughly categorized into entailment-based and question generation and question answering (QGQA) metrics. Entailment-based metrics (Kryscinski et al., 2020; Goyal and Durrett, 2021) train metrics to predict entailment between the document and summary units, such as sentences or dependency arcs. QGQA approaches (Durmus et al., 2020; Wang et al., 2020; Scialom et al., 2021; Fabbri et al., 2022) generates questions from one source using a question generation model and then in turn uses a question answering model to answer the generated questions given the other source. Additionally, counterfactual estimation (Xie et al., 2021) and embedding-based metrics (Zhang et al., 2020) have been explored. While significant progress has been made, the proposed metrics rely only on the document to detect hallucinations and ignore the other modalities. We thus propose CLIPBERTS CORE that addresses the missing modalities while maintaining similar or higher correlations with human judgment of factuality for the document and multimodal summarization task. Meta-evaluations have also been proposed to evaluate such metrics for text summarization that differ in sizes and datasets (Fabbri et al., 2021; Maynez et al., 2020; Wang et al., 2020; Kryscinski et al., 2020; Pagnoni et al., 2021). Our MUFAME is a similar effort but is the first meta-evaluation proposed for the multi-modal summarization task.\\n\\nConclusion\\n\\nIn this work, we present CLIPBERTS CORE, an automatic metric for evaluating factuality for multimodal abstractive summarization. Through meta-evaluation with MUFAME and additional factuality benchmarks, we show CLIPBERTS CORE and its modules correlate well with the human judgment of factuality with respect to the document, image and combined. CLIPBERTS CORE is robust across the different image and text domains and achieves competitive correlation in the zero-shot setting with more complex metrics. We hope this work provides a meta-evaluation for evaluating future multimodal factuality metrics with MUFAME, a strong baseline metric CLIPBERTS CORE to compare against, and two methods to improve the factuality of multimodal abstractive summarization models.\\n\\nLimitations\\n\\nWe limit our work to the task that only contains the vision modality through images and the text modality. However, we note that multimodal summarization also contains video and audio, which we leave for future works. Furthermore, similar to all pre-training models, CLIPScore and BERTScore are also known for reflecting biases of the pre-training data (Hessel et al., 2021; Agarwal et al., 2021), leading to some incorrect predictions. Our work is also focused for datasets in English. Ladhak et al. (2020) proposed a multi-lingual WikiHow by aligning the steps from various languages with the image, and thus our work could be extended to include other languages by including the images to that dataset.\\n\\nAcknowledgment\\n\\nWe thank the reviewers for their helpful comments. We also thank Shiyue Zhang for useful discussions and comments on the paper. This work was supported by NSF-CAREER Award 1846185, ARO Award W911NF2110220, and NSF-AI Engage Institute DRL-211263. The views, opinions, and/or findings contained in this article are those of the authors and not of the funding agency.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-654", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Haoran Li, Junnan Zhu, Tianshang Liu, Jiajun Zhang, and Chengqing Zong. 2018. Multi-modal sentence summarization with modality attention and image filtering. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages 4152\u20134158. International Joint Conferences on Artificial Intelligence Organization.\\n\\nHaoran Li, Junnan Zhu, Jiajun Zhang, Xiaodong He, and Chengqing Zong. 2020b. Multimodal sentence summarization via multimodal selective encoding. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5655\u20135667, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nMingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao, and Rui Yan. 2020c. VMSMO: Learning to generate multimodal summary for video-based news articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9360\u20139369, Online. Association for Computational Linguistics.\\n\\nSiyao Li, Deren Lei, Pengda Qin, and William Yang Wang. 2019. Deep reinforcement learning with distributional semantic rewards for abstractive summarization. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6038\u20136044, Hong Kong, China. Association for Computational Linguistics.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Madaar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.\\n\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.\\n\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, Online. Association for Computational Linguistics.\\n\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Brussels, Belgium. Association for Computational Linguistics.\\n\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding.\\n\\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812\u20134829, Online. Association for Computational Linguistics.\\n\\nShruti Palaskar, Jind \u02c7rich Libovick\u00fd, Spandana Gella, and Florian Metze. 2019. Multimodal abstractive summarization for how2 videos. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6587\u20136596, Florence, Italy. Association for Computational Linguistics.\\n\\nJacob Parnell, Inigo Jauregi Unanue, and Massimo Piccardi. 2021. RewardsOfSum: Exploring reinforcement learning rewards for summarisation. In Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021), pages 1\u201311, Online. Association for Computational Linguistics.\\n\\nRamakanth Pasunuru and Mohit Bansal. 2018. Multi-reward reinforced summarization with saliency and entailment. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 646\u2013653, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sarathy, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. CoRR, abs/2103.00020.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1179\u20131195.\\n\\nAlexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 379\u2013389, Lisbon, Portugal. Association for Computational Linguistics.\\n\\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594\u20136604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ravi Shekhar, Sandro Pezzelle, Yauhen Klimovich, Aur\u00e9lie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017. FOIL it! find one mismatch between image and language caption. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 255\u2013265, Vancouver, Canada. Association for Computational Linguistics.\\n\\nKaren Simonyan and Andrew Zisserman. 2015. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations.\\n\\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022. Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In CVPR.\\n\\nDaniel Ponsa Vassileios Balntas, Edgar Riba and Krystian Mikolajczyk. 2016. Learning local feature descriptors with triplets and shallow convolutional neural networks. In Proceedings of the British Machine Vision Conference (BMVC), pages 119.1\u2013119.11. BMVA Press.\\n\\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135020, Online. Association for Computational Linguistics.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nRonald J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 8(3\u20134):229\u2013256.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nYijun Xiao and William Yang Wang. 2021. On hallucination and predictive uncertainty in conditional language generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2734\u20132744, Online. Association for Computational Linguistics.\\n\\nYuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, and Bolin Ding. 2021. Factual consistency evaluation for text summarization via counterfactual estimation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 100\u2013110, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nYue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, and Chris Callison-Burch. 2021. Visual goal-step inference using wikiHow. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2167\u20132179, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11328\u201311339. PMLR.\\n\\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.\\n\\nZhengkun Zhang, Xiaojun Meng, Yasheng Wang, Xin Jiang, Qun Liu, and Zhenglu Yang. 2022. Unims: A unified framework for multimodal summarization with knowledge distillation. Proceedings of the AAAI Conference on Artificial Intelligence.\\n\\nJunnan Zhu, Haoran Li, Tianshang Liu, Yu Zhou, Jiajun Zhang, and Chengqing Zong. 2018. MSMO: Multimodal summarization with multimodal output. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4154\u20134164, Brussels, Belgium. Association for Computational Linguistics.\\n\\nJunnan Zhu, Yu Zhou, Jiajun Zhang, Haoran Li, Chengqing Zong, and Changliang Li. 2020. Multimodal summarization with guidance of multimodal reference. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):9749\u20139756.\\n\\n---\\n\\nA Meta-Evaluation Details\\n\\nA.1 Metrics Details\\n\\nWe describe the metrics we use for computing correlations. We use the official scripts from the respective repository.\\n\\nFactCC. FactCC (Kryscinski et al., 2020) is an entailment-based metric that uses BERT to output a binary prediction of factuality given the concatenation of the document and a summary sentence as input. The final score is the average factuality score of all summary sentences.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Model Optimizer Learning rate Label Smoothing Num steps Batch size\\nT5 AdamW 5e-5 0.1 15,000 256\\nPEGASUS AdaFactor 8e-5 0.1 15,000 256\\nCLIP-BART AdamW 5e-5 0.1 15,000 256\\nMOF Adam 1e-3 0.0 50,000 512\\nTable 9: Hyper-parameters of the models trained on the multimodal WikiHow summarization task.\\n\\nDAE. DAE (Goyal and Durrett, 2021) is an entailment-based metric that evaluates factuality on dependency arc level instead of on sentence level. We report the sentence-level error. The sentence is considered to contain an error if any of its arcs are predicted to be non-factual, and the final score is the average of all sentence predictions. 0 indicates a sentence contains no error, and 1 indicates the sentence contains an error.\\n\\nQuestEval. A QGQA metric, Scialom et al. (2021) generates questions using a question generation model from both the document and the summary. Then, a question-answering model answers the question generated using the document with the summary, and answers the question generated using the summary with the document. The final score is the harmonic mean of the accuracy of the predicted answers to the true answers from the question generation model.\\n\\nCLIPText-S. CLIPScore provides a variant of the metric that takes in references for the image captioning, and calculates the cosine similarity between the text embeddings $T$ and that of the references $R$. The final score is calculated by taking the average over the maximum reference cosine similarity:\\n\\n$$\\\\text{CLIPText-S}(T,R) = \\\\frac{1}{|T|} \\\\sum_{i=1}^{\\\\max \\\\in R} \\\\text{cossim}(v_i,r)$$\\n\\nBERTScore/BERT-S. The original BERTScore uses roberta-large by default. For BERT-S, we use roberta-large-mnli up to the 11th layer after tuning on FRANK's validation set.\\n\\nTriplet Network. This network maps image and summary embeddings into the same space and minimize the distance between the positive pair and maximize that between the pair of image and negative summaries with the Triplet loss (Vassileios Balntas and Mikolajczyk, 2016). Specifically, a triplet Network takes in a triplet $(V,S_{pos},S_{neg})$, the representation of an image $V$, and that of a positive summary and negative image. We then map the representation to the same space and normalize the embeddings. We then use the triplet loss with a margin of 0.2. To generate the negative summaries, we use the similarity split of VGSI and take the summaries for the three negative choices. We use the CLIP RN50x64 visual backbone to generate the visual representation and use BERT to generate the summary representation. We modify the example training code provided by Transformers, and train for 10 epochs with a learning rate of 5e-5. We use the other default settings.\\n\\nMMAE. MMAE (Zhu et al., 2018) is initially developed for evaluating the summary quality on MSMO, which we adapt to our task. The metric consists of three submodules: image precision (IP), $IM_{MAX}$, and ROUGE-L. For $MUFAE$, since we only have a single image, IP is just 1. $IM_{MAX}$ is trained on the multimodal WikiHow dataset, where the negative image-summary pair is from the same batch. We use the same hyper-parameters of the original MMAE metric. To combine the three scores, we use the MLP variant tuned on the validation set of $MUFAE$.\\n\\nCombined Metrics. We tune the combined metrics on the validation set of $MUFAE$. We use $\\\\alpha = 0.45$ for CLIP-S + DAE, $\\\\alpha = 0.10$ for Triplet Net + BERT-S, and $0.25$ for CLIPBERTS CORE.\\n\\nA.2 Model Details\\nWe train the models on the proposed multimodal WikiHow dataset. The hyper-parameters are shown in Table 9. The pre-trained models and the training scripts for the transformer-based models are taken from HuggingFace's transformers library (Wolf et al., 2020). We set the maximum input length to 256 and output length to 32 for all models.\\n\\nT5. T5 (Raffel et al., 2020) is an encoder-decoder model pre-trained on a collection of tasks in a text-to-text format. We use the t5-small model and fine-tune as a document summarization tasks, ignoring the images. The total number of parameters is 9644.\"}"}
{"id": "emnlp-2022-main-654", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is around 60 million. We use mixed precision, and training was performed on 2 NVIDIA RTX A6000 GPUs for approximately 6 hours. PEGASUS. PEGASUS (Zhang et al., 2020) is another encoder-decoder model specifically designed for the abstractive summarization task by imitating the summarization setup during pre-training. We use PEGASUS-large checkpoint and fine-tune without the images. The total number of parameters is around 571 million. Training was performed on a single NVIDIA RTX A6000 GPU for approximately 28 hours. CLIP-BART. The architecture of CLIP-BART is described in Section 5. The total number of parameters is around 140 million. We fine-tune the model starting from the BART-base checkpoint, and use the CLIP RN50x64 visual encoder to extract image features. We use mixed precision, and the training was performed on a single NVIDIA RTX A6000 GPU for approximately 6 hours. MOF. MOF is based on Zhu et al. (2018), a multimodal summarization model with multimodal attention (Li et al., 2018). The model consists of a single-layer unidirectional LSTM (Hochreiter and Schmidhuber, 1997) with the embedding dimension of 256 and hidden dimension of 512 for the text encoder and text decoder. The multimodal attention is computed by concatenating the textual attention layer and visual attention layer over the visual features, extracted from the global fc7 layers from VGG19 (Simonyan and Zisserman, 2015). The total number of parameters is around 83M. Training was performed on a single NVIDIA RTX A6000 GPU for approximately 40 hours.\\n\\nA.3 Annotation Details\\n\\nFigure 3 shows a screenshot of the annotation task on AMT.\\n\\nB Meta-Evaluation with Continuous Labels\\n\\nWe also experiment with combining the two judgments in a continuous way, by taking the average of the two judgments so that a score of 0.5 indicates that the summary is faithful to only one modality. The combined judgment is shown in Table 10. While the correlations are higher overall for all metrics, the trend is similar to the Table 1, where CLIPBERTS CORE can match the correlations of\\n\\n| Metric | Cont. Combined |\\n|--------|----------------|\\n| FactCC | 0.01           |\\n| DAE    | 0.43           |\\n| QuestEval | 0.39      |\\n| CLIPText-S | 0.19    |\\n| BERTScore | 0.50      |\\n| BERT-S | 0.54           |\\n| CLIP-S | 0.23           |\\n| IM     | 0.07           |\\n| MMAE   | 0.27           |\\n| MLP    | 0.26           |\\n| RefCLIP-S | 0.26     |\\n| CLIP-S + DAE | 0.47   |\\n| Triplet Net + BERT-S | 0.56 |\\n\\nTable 10: Pearson correlation coefficients between automatic metrics and human judgments of factuality with respect to the continuous combined judgment.\\n\\n| Metric | Random | Category | Similarity |\\n|--------|--------|----------|------------|\\n| Random | 25.00  | 25.00    | 25.00      |\\n| Triplet Net | 78.48 | 74.65    | 66.07      |\\n| CLIP-S ViT-B/32 | 83.05 | 75.42    | 62.86      |\\n| CLIP-S | 87.79  | 81.37    | 70.94      |\\n| Human* | 92.00  | 89.20    | 86.00      |\\n\\nTable 11: Original Wikihow VGSI. Results with * indicates results taken from the original paper.\\n\\n| Metric | Acc | SCAN t2i* | Triplet Net | CLIP-S ViT-B/32 | CLIP-S ViT-B/16 | CLIP-S ViT-L/14 | CLIP-S RN50 | CLIP-S RN101 | CLIP-S RN50x4 | CLIP-S RN50x16 | CLIP-S RN50x64 |\\n|--------|-----|-----------|-------------|-----------------|----------------|----------------|-------------|-------------|--------------|----------------|----------------|\\n| SCAN t2i* | 85.89 |            |             |     63.16       |               |               |             |             |              |                 |                |\\n| Triplet Net | 83.85 |            |             | 85.36           |               |               |             |             |              |                 |                |\\n| CLIP-S ViT-B/32 | 83.36 |            |             | 85.89           |               |               |             |             |              |                 |                |\\n| CLIP-S ViT-B/16 | 85.36 |            |             | 85.89           |               |               |             |             |              |                 |                |\\n| CLIP-S ViT-L/14 | 85.89 |            |             | 85.89           |               |               |             |             |              |                 |                |\\n| CLIP-S RN50 | 83.50 |            |             | 83.50           |               |               |             |             |              |                 |                |\\n| CLIP-S RN101 | 83.97 |            |             | 83.97           |               |               |             |             |              |                 |                |\\n| CLIP-S RN50x4 | 84.95 |            |             | 84.95           |               |               |             |             |              |                 |                |\\n| CLIP-S RN50x16 | 85.22 |            |             | 85.22           |               |               |             |             |              |                 |                |\\n| CLIP-S RN50x64 | 86.03 |            |             | 86.03           |               |               |             |             |              |                 |                |\\n\\nTable 12: BISON accuracy. * indicates result copied from Hu et al. (2019).\"}"}
{"id": "emnlp-2022-main-654", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 13: Correlation with human judgment of factuality on FRANK. BERT-S achieves overall higher correlations than its original variant and achieves the highest Pearson correlation on all data.\\n\\nTable 14: FOIL accuracy. * indicates results copied from Hessel et al. (2021).\"}"}
{"id": "emnlp-2022-main-654", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 15: Hyper-parameters of the models trained for downstream applications.\\n\\nEvaluation of different modality combinations similar to the multimodal summarization setting. We consider three settings, where we show no reference (evaluating only on the image-text setting), as well as providing 1 or 4 additional reference captions (excluding the true caption being evaluated). We concatenate the references and consider them as documents. We compare CLIPBERTS CORE and its components primarily against the CLIPScore variants, including CLIPText-S and RefCLIP-S.\\n\\nFor the image-text hallucination detection, we focus on how the different CLIP backbones affects factuality detection between image and the summary. This includes ViT-B/32, ViT-B/16, ViT-L/14, RN50, RN101, RN50x4, RN50x16, and RN50x64.\\n\\nWe present the results in Table 14. We observe a clear trend that larger visual backbones improve accuracy when considering only the visual performance for the no-ref case. Interestingly, ViT-based models outperform the RN-based ones for this task.\\n\\nC.3 BISON\\n\\nBISON (Hu et al., 2019) measures the ability of the metric to select the correct image from two semantically similar images, and thus assesses whether the metric is able to detect fine-grained information present in the text and image. We compare the image-summary metrics, including all CLIP-S variants, similar to FOIL (Appendix C.2).\\n\\nTable 12 shows the result. We observe a similar improvement in accuracy with larger visual backbones as observed with the FOIL dataset. While we similarly observe improvement as the model size grows, CLIP-S RN50x64 is the only backbone that outperforms the fully fine-tuned SOTA metric, SCAN t2i (Lee et al., 2018).\\n\\nC.4 FRANK\\n\\nWe show the result in Table 13. As described in Section 4.4, we compare existing factuality metrics with CLIPText-S and BERT-S. We also include QuestEval, which does not correlate better than BERTScore variants. CLIPText-S does not perform well for detecting faithful summaries for summarization, as Pearson and Spearman coefficients are around 0.10 for all datasets and 0.05 for XSum Spearman. In contrast, BERT-S that uses RoBERTa (Liu et al., 2019) model finetuned on the MNLI (Williams et al., 2018) correlates higher than the original BERTScore on Pearson and Spearman across both datasets. It is thus useful to treat factuality as an entailment problem and use the appropriate model.\\n\\nTable 16: Dataset Statistics\\n\\nFor both experiments, we use the CLIP RN50x64 visual encoder to extract the visual features and we limit the maximum number of images to 10. We train the models with transformers library. For all models, we train the models with mixed precision and AdamW (Loshchilov and Hutter, 2019). Other hyper-parameters are found in Table 15.\\n\\nAll CLIP-BART models are trained with 4 NVIDIA RTX A6000 GPUs. The training took approximately an hour for MMSS, and approximately 19 hours for both MSMO baseline models. For SCST training, we train the models on a single NVIDIA RTX A6000 GPU. Training took approximately 7 hours for MMSS and approximately 70 hours for MSMO. We perform a hyper-parameter search manually by evaluating the models on the validation set of the corresponding datasets and select the best-performing parameter according to BERTScore and ROUGE-2 (since these are the scores we optimize for). We first\"}"}
