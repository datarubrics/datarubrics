{"id": "acl-2022-long-318", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Identifying Moments of Change from Longitudinal User Text\\nAdam Tsakalidis1,2, Federico Nanni2, Anthony Hills1, Jenny Chim1, Jiayu Song1, Maria Liakata1,2,3\\n\\n1Queen Mary University of London, London, United Kingdom\\n2The Alan Turing Institute, London, United Kingdom\\n3University of Warwick, Coventry, United Kingdom\\na.tsakalidis;m.liakata@qmul.ac.uk\\n\\nAbstract\\nIdentifying changes in individuals\u2019 behaviour and mood, as observed via content shared on online platforms, is increasingly gaining importance. Most research to-date on this topic focuses on either: (a) identifying individuals at risk or with a certain mental health condition given a batch of posts or (b) providing equivalent labels at the post level. A disadvantage of such work is the lack of a strong temporal component and the inability to make longitudinal assessments following an individual\u2019s trajectory and allowing timely interventions. Here we define a new task, that of identifying moments of change in individuals on the basis of their shared content online. The changes we consider are sudden shifts in mood (switches) or gradual mood progression (escalations). We have created detailed guidelines for capturing moments of change and a corpus of 500 manually annotated user timelines (18.7K posts). We have developed a variety of baseline models drawing inspiration from related tasks and show that the best performance is obtained through context aware sequential modelling. We also introduce new metrics for capturing rare events in temporal windows.\\n\\n1 Introduction\\nLinguistic and other content from social media data has been used in a number of different studies to obtain biomarkers for mental health. This is gaining importance given the global increase in mental health disorders, the limited access to support services and the prioritisation of mental health as an area by the World Health Organization (2019). Studies using linguistic data for mental health focus on recognising specific conditions related to mental health (e.g., depression, bipolar disorder) (Husseini Orabi et al., 2018), or identifying self-harm ideation in user posts (Yates et al., 2017; Zirikly et al., 2019). However, none of these works, even when incorporating a notion of time (Lynn et al., 2018; Losada et al., 2020), identify how an individual\u2019s mental health changes over time. Yet being able to make assessments on a longitudinal level from linguistic and other digital content is important for clinical outcomes, and especially in mental health (Velupillai et al., 2018). The ability to detect changes in individual\u2019s mental health over time is also important in enabling platform moderators to prioritise interventions for vulnerable individuals (Wadden et al., 2021). Users who currently engage with platforms and apps for mental health support (Neary and Schueller, 2018) would also benefit from being able to monitor their well-being in a longitudinal manner.\\n\\nMotivated by the lack of longitudinal approaches we introduce the task of identifying \u2018Moments of Change\u2019 (MoC) from individuals\u2019 shared online content. We focus in particular on two types of changes: Switches \u2013 mood shifts from positive to negative, or vice versa \u2013 and Escalations \u2013 gradual mood progression (see Fig. 1, detailed in \u00a7 3). Specifically we make the following contributions:\\n\\n\u2022 We present the novel task of identifying moments of change in an individual\u2019s mood by analysing linguistic content shared online over time, along with a longitudinal dataset of 500 user timelines (18.7K posts, English language) from 500 users of an online platform.\\n\u2022 We propose a number of baseline models for identifying moments of change.\\n\u2022 We introduce new metrics for capturing rare events in temporal windows.\\n\\nFigure 1: Example of an Escalation (with a darker \u201cpeak\u201d) and a Switch within a user\u2019s timeline.\"}"}
{"id": "acl-2022-long-318", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"automatically capturing Switches/Escalations, inspired by sentence- and sequence-level state-of-the-art NLP approaches in related tasks.\\n\\n\u2022 We introduce a range of temporally sensitive evaluation metrics for longitudinal NLP tasks adapted from the fields of change point detection (van den Burg and Williams, 2020) and image segmentation (Arbelaez et al., 2010).\\n\\n\u2022 We provide a thorough qualitative linguistic analysis of model performance.\\n\\n2 Related Work\\n\\nSocial Media and Mental Health\\n\\nOnline user-generated content provides a rich resource for computational modelling of wellbeing at both population and individual levels. Research has examined mental health conditions by analysing data from platforms such as Twitter and Reddit (De Choudhury et al., 2013; Coppersmith et al., 2014; Cohan et al., 2018) as well as peer-support networks such as TalkLife (Pruksachatkun et al., 2019). Most such work relies on proxy signals for annotations (e.g., self-disclosure of diagnoses, posts on support networks) and is characterised by a lack of standardisation in terms of annotation and reporting practices (Chancellor and De Choudhury, 2020).\\n\\nWe have provided thorough annotation guidelines for Moments of Change that can aid mental health monitoring over time irrespective of the underlying condition.\\n\\nMoments of Change (MoC)\\n\\nLittle work has specifically focused on automatically capturing changes in user behaviour based on their social media posts. Within the health domain, Guntuku et al. (2020) showed that a user's language on Facebook becomes more depressed and less informal prior to their visit to an emergency department. With respect to mental health, De Choudhury et al. (2016) proposed to identify shifts to suicide ideation by predicting (or not) a transition from posting on a regular forum to a forum for suicide support. Pruksachatkun et al. (2019) examined moments of affective change in TalkLife users by identifying positive changes in sentiment at post-level with respect to a distressing topic earlier in a user's thread. In both cases MoC are overly specific and modelled through binary classification without any notion of temporal modelling.\\n\\nNLP for Mental Health\\n\\nMore advanced NLP methods have been used for predicting psychiatric conditions from textual data, including self-harm, suicide ideation, eating disorders, and depression (Benton et al., 2017; Kshirsagar et al., 2017; Yates et al., 2017; Husseini Orabi et al., 2018; Jiang et al., 2020; Shing et al., 2020). Researchers are increasingly adopting sequential modelling to capture temporal dynamics of language use and mental health. For example, Cao et al. (2019) encode microblog posts using suicide-oriented embeddings fed to an LSTM network to assess the suicidality risk at post level. Sawhney et al. (2020b, 2021) improves further on predicting suicidality at post-level by jointly considering an emotion-oriented post representation and the user's emotional state as reflected through their posting history with temporally aware models. The recent shared tasks in eRisk also consider sequences of user posts in order to classify a user as a \u201cpositive\u201d (wrt self-harm or pathological gambling) or \u201ccontrol\u201d case (Losada et al., 2020; Parapar et al., 2021). While such work still operates at the post- or user-level it highlights the importance of temporally aware modelling.\\n\\nRelated Temporal NLP Tasks\\n\\nSemantic change detection (SCD) aims to identify words whose meaning has changed over time. Given a set of word representations in two time periods, the dominant approach is to learn the optimal transformation using Orthogonal Procrustes (Sch\u00f6nemann, 1966) and measure the level of semantic change of each word via the cosine distance of the resulting vectors (Hamilton et al., 2016). A drawback of this is the lack of connection between consecutive windows. Tsakalidis and Liakata (2020) addressed this through sequential modeling by encoding word embeddings in consecutive time windows and taking the cosine distance between future predicted and actual word vectors. Both approaches are considered as baselines for our task.\\n\\nFirst story detection (FSD) aims to detect new events reported in streams of textual data. Having emerged in the Information Retrieval community (Allan et al., 1998), FSD has been applied to streams of social media posts (Petrovi\u0107 et al., 2010). FSD methods assume that a drastic change in the textual content of a document compared to previous documents signals the appearance of a new story. A baseline from FSD is considered in \u00a74.2.\\n\\n3 Dataset creation\\n\\nWe describe the creation of a dataset of individuals\u2019 timelines annotated with Moments of Change.\"}"}
{"id": "acl-2022-long-318", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"P\\\\(\\\\{(u)\\\\}\\\\)s\\\\(e\\\\) is a subset of their history, a series of posts \\\\([p_0, ..., p_n]\\\\) shared by user \\\\(u\\\\) between dates \\\\(s\\\\) and \\\\(e\\\\). A \u201cMoment of Change\u201d (MoC) is a particular point or period (range of time points) within \\\\([s, e]\\\\) where the behaviour or mental health status of an individual changes. While MoC can have different definitions in various settings, in this paper we are particularly interested in capturing MoC pertaining to an individual\u2019s mood. Other types of MoC can include life events, the onset of symptoms or turning points (e.g., moments of improvement, difficult moments or moments of intervention within therapy sessions).\\n\\n1 We address two types of Moments of Change: Switches (sudden mood shifts from positive to negative, or vice versa) and Escalations (gradual mood progression from neutral or positive to more positive or neutral or negative to more negative). Capturing both sudden and gradual changes in individuals\u2019 mood over time is recognised as important for monitoring mental health conditions (Lutz et al., 2013; Shalom and Aderka, 2020) and is one of the dimensions to measure in psychotherapy (Barkham et al., 2021).\\n\\n3.1 Data Acquisition\\n\\nIndividual\u2019s timelines are extracted from Talklife, a peer-to-peer network for mental health support. Talklife incorporates all the common features of social networks \u2013 post sharing, reacting, commenting, etc. Importantly, it provides a rich resource for computational analysis of mental health (Pruksachatkun et al., 2019; Sharma et al., 2020; Saha and Sharma, 2020) given that content posted by its users focuses on their daily lives and well-being. A complete collection between Aug\u201911\u2013Aug\u201920 (12.3M posts, 1.1M users) was anonymised and provided to our research team in a secure environment upon signing a License Agreement. In this environment, 500 user timelines were extracted (\u00a73.2) and an additional anonymisation step was performed to ensure that usernames were properly hashed when present in the text. The 500 timelines were subsequently annotated using our bespoke annotation tool (\u00a73.3) to derive the resulting longitudinal dataset (\u00a73.4).\\n\\n1 A limitation of our work stems from the fact that MoC are revealed to us by the user\u2019s shared content (i.e., we cannot identify changes in a user\u2019s well-being unless these are expressed online). We provide details on the limitations of our work in the Ethics Statement (\u00a77).\\n\\n2 https://www.talklife.com/\"}"}
{"id": "acl-2022-long-318", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"do not bias (or limit) our analysis on extremely active users. Finally, to ensure linguistic diversity in our dataset, 500 timelines extracted in this way were chosen for annotation at random, each corresponding to a different individual. The resulting dataset consists of 18,702 posts ($\\\\mu = 35$, $SD = 22$ per timeline; range of timeline length $= [10, 124]$, see Fig. 2(a)).\\n\\n3.3 Annotations of MoC\\n\\nAnnotation Interface\\n\\nAn annotation interface was developed to allow efficient viewing and annotation of a timeline (see snippet in Fig. 3). Each post in a timeline was accompanied by its timestamp, the user\u2019s self-assigned emotion and any associated comments (color-coded, to highlight recurrent users involved within the same timeline). Given the context of the entire timeline, annotations for MoC are performed at post level: if an annotator marks a post as a MoC, then they specify whether it is (a) the beginning of a Switch or (b) the peak of an Escalation (i.e., the most positive/negative post of the Escalation). Finally, the range of posts pertaining to a MoC (i.e., all posts in the Switch/Escalation) need to be specified.\\n\\n| Label       | None (O) | Switch (IS) | Escalation (IE) |\\n|-------------|----------|-------------|-----------------|\\n| IAA         | 0.69     | 0.08        | 0.19            |\\n| Majority    | 0.89     | 0.30        | 0.50            |\\n\\nTable 1: Inter Annotator Agreement (IAA).\\n\\nData annotation\\n\\nAfter a round of annotations for guideline development with PhD students within the research group (co-authors of the paper), we recruited three external annotators to manually label the 500 timelines. They all have University degrees in humanities disciplines and come from three different countries; one of them is an English native speaker. Annotators were provided with a set of annotation guidelines containing specific examples, which were enriched and extended during iterative rounds of annotation.\\n\\n3 Annotators completed 2 hands-on training sessions with a separate set of 10 timelines, where they were able to ask questions and discuss opinions to address cases of disagreement. Following the initial training phase, we performed spot checks to provide feedback and answer any questions while they labelled the full dataset ($n = 500$ timelines). Annotators were encouraged to take breaks whenever needed, due to the nature of the content. On average, each annotator spent about 5 minutes on annotating a single timeline.\\n\\n3.4 Deriving the final gold standard\\n\\nThe annotation of MoC is akin to assessment of anomaly detection methods since MoC (Switches and Escalations) are rare, with the majority of posts not being annotated (label \u2018None\u2019). Measuring the agreement in such settings is therefore complex, as established metrics such as Krippendorff\u2019s Alpha and Fleiss\u2019 Kappa would generally yield a low score. This is due to the unrealistically high expected chance agreement (Feinstein and Cicchetti, 1990), which cannot be mitigated by the fact that annotators do agree on the majority of the annotations (especially on the \u2018None\u2019 class). For this reason, we have used as the main indicator the per label positive agreement computed as the ratio of the number of universally agreed-upon instances (the intersection of posts associated with that label) over the total number of instances (the union of posts associated with that label). As highlighted\\n\\nGuidelines are available at https://github.com/Maria-Liakata-NLP-Group/Annotation-guidelines.\"}"}
{"id": "acl-2022-long-318", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in Table 1, while perfect agreement for 'None' is at 69%, perfect agreement on Escalations and Switches is at 19% and 8%, respectively. However, if instead of perfect agreement we consider majority agreement (where two out of three annotators agree), these numbers drastically increase (30% for Switches and 50% for Escalations). Moreover, by examining the systematic annotation preferences of our annotators we have observed that the native speaker marked almost double the amount of Switches compared to the other two annotators, in particular by spotting very subtle cases of mood change. We have thus decided to generate a gold standard based on majority decisions, comprising only cases where at least two out of three annotators agree with the presence of a MoC. The rare cases of complete disagreement have been labelled as 'None'. We thus have 2,018 Escalations and 885 Switches from an overall of 18,702 posts (see Fig. 2(b) for the associated lengths in #posts). In future work we plan to consider aggregation methods based on all annotations or approaches for learning from multiple noisy annotations (Paun and Simpson, 2021).\\n\\n4 Models & Experiment Design\\n\\nOur aim is to detect and characterise the types of MoC based on a user's posting activity. We therefore treat this problem as a supervised classification task (both at post level and in a sequential/timeline-sensitive manner, as presented in \u00a74.2) rather than an unsupervised task, even though we also consider effectively baselines with unsupervised components (FSD, SCD in \u00a74.2). Contrary to traditional sentence or document-level NLP tasks, we incorporate timeline-sensitive evaluation metrics that account for the sequential nature of our model predictions (\u00a74.1).\\n\\nGiven a user's timeline, the aim is to classify each post within it as belonging to a \\\"Switch\\\" (IS), an \\\"Escalation\\\" (IE), or \\\"None\\\" (O). At this point we don't distinguish between beginnings of switches/peaks of escalations and other posts in the respective ranges. While the task is sequential by definition, we train models operating both at the post level in isolation and sequential models at the timeline-level (i.e., accounting for user's posts over time), as detailed in \u00a74.2. We contrast model performance using common post-level classification metrics as well as novel timeline-level evaluation approaches (\u00a74.1). This allows us to investigate the impact of (a) accounting for severe class imbalance and (b) longitudinal modelling. We have randomly divided the annotated dataset into 5 folds (each containing posts from 100 timelines) to allow reporting results on all of the data through cross-validation.\\n\\n4.1 Evaluation Settings\\n\\nPost-level\\n\\nWe first assess model performance on the basis of standard evaluation metrics at the post level (Precision, Recall, F1 score). These are obtained per class and macro-averaged, to better emphasize performance in the two minority class labels (IS & IE). However, post-level metrics are unable to show: (a) the expected accuracy at the timeline level (see example in Fig. 4) and (b) model suitability in predicting regions of change. These aspects are particularly important since we aim to build models capturing MoC over time.\\n\\nTimeline-level\\n\\nOur first set of timeline-level evaluation metrics are inspired from work in change-point detection (van den Burg and Williams, 2020) and mirror the post-level ones, albeit operating on a window and timeline basis. Specifically, working on each timeline and label type independently, we calculate Recall $R(l)_w$ (Precision $P(l)_w$) by counting as \\\"correct\\\" a model prediction for label $l$ if the prediction falls within a window of $w$ posts around post labelled $l$ in the gold standard. Formally:\\n\\n$$R(l)_w = \\\\frac{|TP_w(M(l), GS(l))|}{|GS(l)|}, P(l)_w = \\\\frac{|TP_w(M(l), GS(l))|}{|M(l)|},$$\\n\\nwhere $TP_w$ denotes the true positives that fall within a range of $w$ posts and $M(l)/GS(l)$ are the predicted/actual labels for $l$, respectively. Note that each prediction can only be counted once as \\\"correct\\\". $R(l)_w$ and $P(l)_w$ are calculated on every timeline and are then macro-averaged.\\n\\nThe second set of our timeline-level evaluation metrics is adapted from the field of image segmentation (Arbelaez et al., 2010). Here we aim at evaluating model performance based on its ability to capture regions of change (e.g., in Fig 4, 'GS' shows a timeline with three (two) such regions of Escalations (Switches)). For each such true region $R(l)_GS$, we define its overlap $O(R(l)_GS, R(l)_M)$ with each predicted region $R(l)_M$ as the intersection over union between the two sets. This way, we can get recall and precision oriented coverage metrics as follows:\\n\\n$$C(l)_r(M \\\\rightarrow GS) = \\\\frac{1}{P(R(l)_GS)} \\\\frac{|R(l)_GS|}{|R(l)_GS| \\\\cdot \\\\max_{R(l)_M} \\\\{O(R(l)_GS, R(l)_M)\\\\)}.$$\"}"}
{"id": "acl-2022-long-318", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Actual (GS, shown twice) vs Predicted labels for each post (square) of a single timeline, by two models (M1, M2). Although M2 provides a more faithful 'reconstruction' of the user's mood over time (the predictions are identical but shifted slightly in time), all post-level evaluation metrics for M1 are greater or equal to those obtained by M2 for the two minority classes (IE and IS).\\n\\n$$C(\\\\text{cl} : M \\\\rightarrow \\\\text{GS}) = \\\\max \\\\{ P_{\\\\text{R}}(l \\\\mid M) \\\\cdot R_{\\\\text{R}}(l \\\\mid M) \\\\}.$$  \\n\\nThe coverage metrics are calculated on the time-line basis and macro-averaged similarly to $R_{\\\\text{w}}$ and $P_{\\\\text{w}}$. Using a set of evaluation metrics, each capturing a different aspect of the task, ensures assessment of model performance from many different angles.\\n\\n4.2 Baseline Models\\n\\nWe have considered different approaches to addressing our task:\\n\\n(i) Na\u00efve methods, specifically a Majority classifier (predicting always \\\"None\\\") and a \\\"Random\\\" predictor, picking a label based on the overall label distribution in the dataset. It has been shown that comparisons against such simple baselines is essential to assess performance in computational approaches to mental health (Tsakalidis et al., 2018).\\n\\n(ii) Post-level supervised models operating on posts in isolation (i.e., ignoring post sequence in a user's timeline): (a) Random Forest (Breiman, 2001) on tfidf post representations ($\\\\text{RF-tfidf}$); (b) BiLSTM (Huang et al., 2015) operating on sequences of word embeddings ($\\\\text{BiLSTM-we}$); (c) BERT(ce) (Devlin et al., 2019) using the cross-entropy loss; and (d) BERT(f) trained using the alpha-weighted focal loss (Lin et al., 2017), which is more appropriate for imbalanced datasets.\\n\\n(iii) Emotion Classification\\n\\nWe used DeepMoji (EM-DM) (Felbo et al., 2017) and Twitter-roBERTa-base (EM-TR) from TweetEval '20 (Barbieri et al., 2020) operating on the post-level, to generate softmax probabilities for each emotion (64 for EM-DM, 4 for EM-TR). These provide meta-features to a BiLSTM to obtain timeline-sensitive models for identifying MoC.\\n\\n(iv) First Story Detection (FSD). We have used two common approaches for comparing a post to the $n$ previous ones: representing the previous posts as (i) a single centroid or (ii) the nearest neighbour to the current post among them (Allan et al., 1998; Petrovi\u0107 et al., 2010). In both cases, we calculate the cosine similarity of the current and previous posts. The scores are then fed into a BiLSTM as meta-features for a sequential model. Results are reported for the best method only.\\n\\n(v) Semantic Change Detection (SCD). Instead of the standard task of comparing word representations in consecutive time windows, we consider a user being represented via their posts at particular points in time. We follow two approaches. The first is an Orthogonal Procrustes approach (Sch\u00f6nenmann, 1966) operating on post vectors ($\\\\text{SCD-OP}$). Our aim here is to find the optimal transformation across consecutive representations, with higher errors being indicative of a change in the user's behaviour. In the second approach ($\\\\text{SCD-FP}$) a BiLSTM is trained on the user's $k$ previous posts in order to predict the next one (Tsakalidis and Liakata, 2020). Errors in prediction are taken to signal changes in the user. In both cases, we calculate the dimension-wise difference between the actual and the transformed/predicted representations (post vectors) and use this as a meta-feature to a BiLSTM to obtain a time-sensitive model.\\n\\n(vi) Timeline-sensitive. From our (ii) post-level classifiers, BERT(f) tackles the problem of imbalanced data but fails to model the task in a longitudinal manner. To remedy this, we employ BiLSTM-bert, which treats a timeline as a sequence of posts to be modelled, each being represented via the $[\\\\text{CLS}]$ representation of BERT(f).\\n\\nTo convert the post-level scores/representations from (iii)-(v) above into time-sensitive models we used the same BiLSTM from (vi), operating at the timeline-level. Details for each model and associated hyperparameters are in the Appendix.\\n\\n5 Results & Discussion\\n\\n5.1 Quantitative Comparison\\n\\nModel Comparison\\n\\nTable 2 summarises the results of all models; Fig. 5 further shows the $P_{\\\\text{w}}/R_{\\\\text{w}}$ metrics for IE/IS for the best-performing models. BiLSTM-bert confidently outperforms all competitors.\"}"}
{"id": "acl-2022-long-318", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Post-level Evaluation\\n\\n| Model   | IS P | IS R | IS F1 | IE P | IE R | IE F1 | O P  | O R  | O F1 | Mac. Avg P | Mac. Avg R | Mac. Avg F1 |\\n|---------|------|------|-------|------|------|-------|------|------|------|-----------|-----------|------------|\\n| Na\u00efve   |      |      |       |      |      |       |      |      |      |           |           |            |\\n| Majority | 0.000| 0.000| 0.000 | 0.000| 0.000| 0.000 | 0.000| 0.000| 0.000|           |           |            |\\n| Random  | 0.047| 0.047| 0.047 | 0.047| 0.047| 0.047 | 0.047| 0.047| 0.047|           |           |            |\\n| RF-tfidf | 0.011| 0.011| 0.011 | 0.011| 0.011| 0.011 | 0.011| 0.011| 0.011|           |           |            |\\n| BiLSTM-we | 0.151 | 0.151 | 0.151 | 0.151 | 0.151 | 0.151 | 0.151 | 0.151 | 0.151 |           |           |            |\\n| BERT(ce) | 0.361 | 0.361 | 0.361 | 0.361 | 0.361 | 0.361 | 0.361 | 0.361 | 0.361 |           |           |            |\\n| BERT(f)  | 0.360 | 0.360 | 0.360 | 0.360 | 0.360 | 0.360 | 0.360 | 0.360 | 0.360 |           |           |            |\\n\\n### Timeline-level Evaluation\\n\\n| Model   | IS P | IS R | IS F1 | IE P | IE R | IE F1 | O P  | O R  | O F1 | Mac. Avg P | Mac. Avg R | Mac. Avg F1 |\\n|---------|------|------|-------|------|------|-------|------|------|------|-----------|-----------|------------|\\n| FSD     |      |      |       |      |      |       |      |      |      |           |           |            |\\n| EM-TR   | 0.036| 0.036| 0.036 | 0.248| 0.248| 0.248 | 0.909| 0.909| 0.909|           |           |            |\\n| EM-DM   | 0.118| 0.118| 0.118 | 0.479| 0.479| 0.479 | 0.948| 0.948| 0.948|           |           |            |\\n| SCD-OP  | 0.005| 0.005| 0.005 | 0.408| 0.408| 0.408 | 0.913| 0.913| 0.913|           |           |            |\\n| SCD-FP  | 0.082| 0.082| 0.082 | 0.503| 0.503| 0.503 | 0.944| 0.944| 0.944|           |           |            |\\n| BiLSTM-bert | 0.264 | 0.264 | 0.264 | 0.568| 0.568| 0.568 | 0.917| 0.917| 0.917|           |           |            |\\n\\n**Post-level** The BERT variants perform better than the rest in all metrics. Their coverage metrics though suggest that while they manage to predict better the regions compared to most timeline-level methods (i.e., high $C_r$), they tend to predict more regions than needed (i.e., low $C_p$) \u2013 partially due to their lack of contextual (temporal-wise) information. Finally, as expected, BERT(f) achieves much higher recall for the minority classes (IE/IS), in exchange for a drop in precision compared to BERT(ce) and in recall for the majority class (O).\\n\\n**Models from Related Tasks**\\n\\nEM-DM achieves very high precision ($P_w, P_w$) for the minority classes, showing a clear link between the tasks of emotion recognition and detecting changes in a user's mood \u2013 indeed, emotionally informed models have been successfully applied to post-level classification tasks in mental health (Sawhney et al., 2020a); however, both EM models achieve low recall ($R_w, R_w$) for IE/IS compared to the rest. For the SCD inspired models, SCD-FP outperforms SCD-OP on most metrics. This is largely due to the fact that the former uses the previous $k=3$ posts to predict the next post in a user's timeline (instead of aligning it based on the previous post only. Thus SCD-FP benefits from its longitudinal component \u2013 a finding consistent with work in semantic change detection (Tsakalidis and Liakata, 2020).\\n\\n**Representation vs Fine-tuning vs Focal Loss**\\n\\nWhile BiLSTM-bert yields the highest macro-F1 and the most robust performance across all metrics, it is not clear which of its components contributes the most to our task. To answer this, we perform a comparison against the exact same BiLSTM, albeit fed with different input types: (a) average word embeddings as in BiLSTM-we, (b) Sentence-BERT representations (Reimers and Gurevych, 2019) and (c) fine-tuned representations from BERT(ce). As shown in Table 3, fine-tuning with BERT(ce) outperforms Sentence-BERT representations. While the contextual nature of all of the BERT-based models offers a clear improvement over the static word embeddings, it becomes evident that the use of the focal loss during training...\"}"}
{"id": "acl-2022-long-318", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Macro-avg performance of timeline-level BiLSTM operating on different input representations (see Representation vs Fine-tuning vs Focal Loss in \u00a75.1).\\n\\nFigure 6: Gains/losses in performance (%) when incorporating a longitudinal component for each model (see Timeline-vs Post-level Modelling in \u00a75.1).\\n\\nThe importance of longitudinal modelling is shown via the difference between the BERT and BiLSTM variants when operating on single posts vs on the timeline-level (e.g., see the post-level results of BERT(ce)/Word emb. in Table 3 vs BERT(ce)/BiLSTM-we in Table 2, respectively).\\n\\nWe further examine the role of longitudinal modelling in the rest of our best-performing models from Table 2. In particular, we replace the timeline-level BiLSTM in EM-DM and SCD-FP with a two-layer feed-forward network, operating on post-level input representations \u2013 treating each post in isolation. The differences across all pairwise combinations with and without the longitudinal component are shown in Fig. 6. Timeline-level models achieve much higher precision (6.1%/6.9%/11.1% for P/P1/Cp, respectively) in return for a small sacrifice in the timeline-level recall-oriented metrics (-2.8%/1.9%/2.3% for R/R1/Cr), further highlighting the longitudinal nature of the task.\\n\\nFigure 7: Histogram of positive emotion scores in True Positive & False Negative distributions, for the Switch label.\\n\\n| Emotion   | TP   | FP   | FN   |\\n|-----------|------|------|------|\\n| angry     | 0.03 | 0.06 | 0.13 |\\n| joy       | 0.76 | 0.60 | 0.44 |\\n| optim      | 0.14 | 0.19 | 0.18 |\\n| sad        | 0.07 | 0.15 | 0.25 |\\n\\nTable 4: Average probability of each emotion per classification case on 'Switches' (see Switches in \u00a75.2).\\n\\n5.2 Qualitative Analysis\\n\\nHere we analyse the cases of Switches/Escalations identified or missed by our best performing model (BiLSTM-bert).\\n\\nSwitches (IS) are the most challenging to identify, largely due to being the smallest class with the lowest inter-annotator agreement. However, the EM-based models achieve high levels of precision on Switches, even during post-level evaluation (see Table 2). We therefore employ EM-TR (Barbieri et al., 2020), assigning probability scores for anger/joy/optimism/sadness to each post, and use them to characterise the predictions made by BiLSTM-bert. Fig. 7 and Table 4 show that our model predicts more often (in most cases, correctly) a 'Switch' when the associated posts express positive emotions (joy/optimism), but misses the vast majority of cases when these emotions are absent. The reason for this is that TalkLife users discuss issues around their well-being, with a negative mood prevailing. Therefore, BiLSTM-bert learns that the negative tone forms the users' baseline and thus deviations from this constitute cases of 'Switches' (see example in Table 5). We plan to address this in the future by incorporating transfer learning approaches to our model (Ruder et al., 2019).\\n\\nEscalations (IE) are better captured by our models. Here we examine more closely the cases of 'Peaks' in the escalations (i.e., the posts indicating the most...\"}"}
{"id": "acl-2022-long-318", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Oh, forgot :) Stay safe you lovely people all around the world! Hope you are all having a good night! Stay safe! :D\\n\\nDon't wanna deal with anyone.. Hope school finishes so I can go home soon. Tired of my leg hurting so badly today. I really can't do any training :(\\n\\nHope you're all great! <3 Love you all!\\n\\nTable 5: Example of a Switch in part of a user's (paraphrased) timeline, missed by BiLSTM-bert.\\n\\nFigure 8: Recall for IE cases per cumulative length of Escalation (see Escalations in \u00a75.2).\\n\\nnegative/positive state of the user within an escalation \u2013 see \u00a73.3). As expected, the post-level recall of BiLSTM-bert in these cases is much higher than its recall for the rest of IE cases (.557 vs .408).\\n\\nIn Fig. 8 we analyse the recall of our model in capturing posts denoting escalations, in relation to the length of escalations. We can see that our model is more effective in capturing longer escalations. As opposed to the Switch class, we found no important differences in the expressed emotion between TP and FN cases. By carefully examining the cases of Peaks in isolation, we found that the majority of them express very negative emotions, very often including indication of self-harm. A Logistic Regression trained on bigrams at the post-level to distinguish between identified vs missed cases of Peaks showed that the most positively correlated features for the identified cases were directly linked to self-harm (e.g., \\\"kill myself\\\", \\\"to die\\\", \\\"kill me\\\"). However, this was not necessarily the case with missed cases. Nevertheless, there were several cases of self-harm ideation that were missed by BiLSTM-bert, as well as misses due to the model \\\"ignoring\\\" the user's baseline, as is the case with Switches (see Table 6). Transfer learning and domain adaptation strategies as well as self-harm detection models operating at the post level could help in mitigating this problem.\\n\\nTable 6: Examples of Peaks of Escalations (isolated paraphrased posts) missed by BiLSTM-bert.\\n\\n6 Conclusion and Future Work\\n\\nWe present a novel longitudinal dataset and associated models for personalised monitoring of a user's well-being over time based on linguistic online content. Our dataset contains annotations for: (a) sudden shifts in a user's mood (switches) and (b) gradual mood progression (escalations). Proposed methods are inspired by state-of-the-art contextual models and longitudinal NLP tasks. Importantly, we have introduced temporally sensitive evaluation metrics, adapted from the fields of change-point detection and image segmentation. Our results highlight the importance of considering the temporal aspect of the task and the rarity of mood changes.\\n\\nFuture work could follow four main directions: (a) integrating longitudinal models of detecting changes, with post-level models for emotion and self-harm detection (see \u00a75.2); (b) incorporating transfer learning methods (Ruder et al., 2019) to adapt more effectively to unseen users' timelines; (c) adjusting our models to learn from multiple (noisy) annotators (Paun and Simpson, 2021) and (d) calibrating the parameters of focal loss and testing other loss functions suited to heavily imbalanced classification tasks (Jadon, 2020).\\n\\n7 Ethics Statement\\n\\nEthics institutional review board (IRB) approval was obtained from the corresponding ethics board of the University of Warwick prior to engaging in this research study. Our work involves ethical considerations around the analysis of user generated content shared on a peer support network (TalkLife). A license was obtained to work with the user data from TalkLife and a project proposal was submitted to them in order to embark on the project. The current paper focuses on the identification of moments of change (MoC) on the basis of content shared by individuals. These changes involve recognising sudden shifts in mood (switches or escalations) and gradual mood progression (escalations).\\n\\nWhen my parents go out, I am gonna cut. I feel so horrible. I really don't want to be here anymore. Someone please text me... I swear I am about to harm myself... Please, anyone!' Had an awesome day with my gf and she tagged me! I am not alone! :)\\n\\nHave not cut for the past year!! Yay!!\\n\\nTable 6: Examples of Peaks of Escalations (isolated paraphrased posts) missed by BiLSTM-bert.\"}"}
{"id": "acl-2022-long-318", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Annotators were given contracts and paid fairly in line with University payscales. They were alerted about potentially encountering disturbing content and were advised to take breaks. The annotations are used to train and evaluate natural language processing models for recognising moments of change as described in our detailed guidelines. Working with datasets such as TalkLife and data on online platforms where individuals disclose personal information involves ethical considerations (Mao et al., 2011; Kek\u00fcll\u00fco\u02d8glu et al., 2020). Such considerations include careful analysis and data sharing policies to protect sensitive personal information. The data has been de-identified both at the time of sharing by TalkLife but also by the research team to make sure that no user handles and names are visible. Any examples used in the paper are either paraphrased or artificial. Potential risks from the application of our work in being able to identify moments of change in individuals\u2019 timelines are akin to those in earlier work on personal event identification from social media and the detection of suicidal ideation. Potential mitigation strategies include restricting access to the code base and annotation labels used for evaluation.\\n\\nLimitations\\nOur work in this paper considers moments of change as changes in an individual\u2019s mood judged on the basis of their self-disclosure of their well-being. This is faced by two limiting factors: (a) users may not be self-disclosing important aspects of their daily lives and (b) other types of changes related to their mental health (other than their mood/emotions, such as important life events, symptoms etc.) may be taking place. Though our models could be tested in cases of non-self-disclosure (given the appropriate ground truth labels), the analysis and results presented in this work should not be used to infer any conclusion on such cases. The same also holds for other types of \u2018moments of change\u2019 mentioned in \u00a72 (e.g., transition to suicidal thoughts), as well as other types of changes, such as changes in an individual in terms of discussing more about the future, studied in Althoff et al. (2016), or changes in their self-focus (Pyszczynski and Greenberg, 1987) over time, which we do not examine in this current work.\\n\\nAcknowledgements\\nThis work was supported by a UKRI/EPSRC Turing AI Fellowship to Maria Liakata (grant EP/V030302/1) and the Alan Turing Institute (grant EP/N510129/1). The authors would like to thank Dana Atzil-Slonim, Elena Kochkina, the anonymous reviewers and the meta-reviewer for their valuable feedback on our work, as well as the three annotators for their invaluable efforts in generating the longitudinal dataset.\\n\\nReferences\\nRyan Prescott Adams and David J. C. MacKay. 2007. Bayesian Online Changepoint Detection. arXiv:0710.3742 [stat]. ArXiv: 0710.3742.\\nJames Allan, Jaime G Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study final report.\\nTim Althoff, Kevin Clark, and Jure Leskovec. 2016. Large-scale analysis of counseling conversations: An application of natural language processing to mental health. Transactions of the Association for Computational Linguistics, 4:463\u2013476.\\nPablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. 2010. Contour detection and hierarchical image segmentation. IEEE transactions on pattern analysis and machine intelligence, 33(5):898\u2013916.\\nFrancesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. 2020. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1644\u20131650, Online. Association for Computational Linguistics.\\nMichael Barkham, Wolfgang Lutz, and Louis G Castonguay. 2021. Bergin and Garfield's handbook of psychotherapy and behavior change. John Wiley & Sons.\\nAdrian Benton, Margaret Mitchell, and Dirk Hovy. 2017. Multitask learning for mental health conditions with limited social media data. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 152\u2013162, Valencia, Spain. Association for Computational Linguistics.\\nLeo Breiman. 2001. Random forests. Machine learning, 45(1):5\u201332.\\nLei Cao, Huijun Zhang, Ling Feng, Zihan Wei, Xin Wang, Ningyun Li, and Xiaohao He. 2019. Latent suicide risk detection on microblog via suicide-oriented word embeddings and layered attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1718\u20131728, Hong Kong, China. Association for Computational Linguistics.\\nStevie Chancellor and Munmun De Choudhury. 2020. Methods in predictive techniques for mental health status on social media: a critical review. NPJ digital medicine, 3(1):1\u201311.\"}"}
{"id": "acl-2022-long-318", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Arman Cohan, Bart Desmet, Andrew Yates, Luca Sol-daini, Sean MacAvaney, and Nazli Goharian. 2018.\\nSMHD: a large-scale resource for exploring online\\nlanguage usage for multiple mental health condi-\\ntions. In Proceedings of the 27th International Con-\\nfERENCE ON COMPUTATIONAL LINGUISTICS, pages 1485\u2013\\n1497, Santa Fe, New Mexico, USA. Association for\\nComputational Linguistics.\\n\\nGlen Coppersmith, Mark Dredze, and Craig Harman.\\n2014. Quantifying mental health signals in twitter.\\nIn Proceedings of the workshop on computational\\nlinguistics and clinical psychology: From linguistic\\nsignal to clinical reality, pages 51\u201360.\\n\\nMunmun De Choudhury, Michael Gamon, Scott Counts,\\nand Eric Horvitz. 2013. Predicting depression via so-\\ncial media. In Proceedings of the International AAAI\\nConference on Web and Social Media, volume 7.\\n\\nMunmun De Choudhury, Emre Kiciman, Mark Dredze,\\nGlen Coppersmith, and Mrinal Kumar. 2016. Discov-\\nering shifts to suicidal ideation from mental health\\ncontent in social media. In Proceedings of the 2016\\nCHI Conference on Human Factors in Computing\\nSystems, pages 2098\u20132110, San Jose California USA.\\nACM.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In Proceedings of the 2019 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages\\n4171\u20134186, Minneapolis, Minnesota. Association for\\nComputational Linguistics.\\n\\nAlvan R Feinstein and Domenic V Cicchetti. 1990.\\nHigh agreement but low kappa: I. the problems of\\nagreement. Journal of clinical epidemiology,\\n43(6):543\u2013549.\\n\\nBjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rah-\\nwan, and Sune Lehmann. 2017. Using millions of\\nemoji occurrences to learn any-domain representa-\\ntions for detecting sentiment, emotion and sarcasm.\\nIn Conference on Empirical Methods in Natural Lan-\\nguage Processing (EMNLP).\\n\\nSharath Chandra Guntuku, H Andrew Schwartz, Adarsh\\nKashyap, Jessica S Gaulton, Daniel C Stokes,\\nDavid A Asch, Lyle H Ungar, and Raina M Merchant.\\n2020. Variability in language used on social media\\nprior to hospital visits. Scientific reports, 10(1):1\u20139.\\n\\nWilliam L Hamilton, Jure Leskovec, and Dan Jurafsky.\\n2016. Diachronic word embeddings reveal statistical\\nlaws of semantic change. In Proceedings of the 54th\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 1489\u2013\\n1501.\\n\\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-\\ntional lstm-crf models for sequence tagging.\\narXiv preprint arXiv:1508.01991.\\n\\nAhmed Husseini Orabi, Prasadith Buddhitha, Mahmoud\\nHusseini Orabi, and Diana Inkpen. 2018. Deep learn-\\ning for depression detection of twitter users. In\\nProceedings of the Fifth Workshop on Computational\\nLinguistics and Clinical Psychology: From Keyboard\\nto Clinic, pages 88\u201397, New Orleans, LA. Associa-\\ntion for Computational Linguistics.\\n\\nShruti Jadon. 2020. A survey of loss functions for se-\\nmantic segmentation. In 2020 IEEE Conference on\\nComputational Intelligence in Bioinformatics and\\nComputational Biology (CIBCB), pages 1\u20137. IEEE.\\n\\nZhengping Jiang, Sarah Ita Levitan, Jonathan Zomick,\\nand Julia Hirschberg. 2020. Detection of mental\\nhealth from reddit via deep contextualized represen-\\ntations. In LOUHI@EMNLP.\\n\\nDilara Kek\u00fcll\u00fco\u02d8glu, Walid Magdy, and Kami Vaniea.\\n2020. Analysing privacy leakage of life events on\\ntwitter. In Proceedings of the 12th ACM Conference\\non Web Science.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\\nmethod for stochastic optimization. In 3rd Inter-\\nnational Conference on Learning Representations,\\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\\nConference Track Proceedings.\\n\\nRohan Kshirsagar, Robert W Morris, and Samuel Bow-\\nman. 2017. Detecting and explaining crisis. In\\nProceedings of the Fourth Workshop on Computational\\nLinguistics and Clinical Psychology\u2014From Linguis-\\ntic Signal to Clinical Reality, pages 66\u201373.\\n\\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He,\\nand Piotr Doll\u00e1r. 2017. Focal loss for dense object\\ndetection. In Proceedings of the IEEE international\\nconference on computer vision, pages 2980\u20132988.\\n\\nDavid E Losada, Fabio Crestani, and Javier Parapar.\\n2020. Overview of erisk at clef 2020: Early risk\\nprediction on the internet (extended overview).\\n\\nWolfgang Lutz, Torsten Ehrlich, Julian A. Rubel, Nora\\nHallwachs, Marie-Anna R\u00f6ttger, Christine Jorasz,\\nSarah Mocanu, Silja V ocks, Dietmar Schulte, and Ar-\\nmita Tschitsaz-Stucki. 2013. The ups and downs of\\npsychotherapy: Sudden gains and sudden losses iden-\\ntified with session reports. Psychotherapy Research,\\n23:14 \u2013 24.\\n\\nVeronica Lynn, Alissa Goodman, Kate Niederhoffer,\\nKate Loveys, Philip Resnik, and H Andrew Schwartz.\\n2018. Clpsych 2018 shared task: Predicting cur-\\ntent and future psychological health from childhood\\nessays. In Proceedings of the Fifth Workshop on\\nComputational Linguistics and Clinical Psychology:\\nFrom Keyboard to Clinic, pages 37\u201346.\\n\\nHuina Mao, Xin Shuai, and Apu Kapadia. 2011. Loose\\ntweets: An analysis of privacy leaks on twitter.\\nWPES \u201911, page 1\u201312, New York, NY , USA. As-\\nsociation for Computing Machinery.\\n\\nRohan Mishra, Pradyumn Prakhar Sinha, Ramit Sawh-\\nney, Debanjan Mahata, Puneet Mathur, and Rajiv\\nRatn Shah. 2019. SNAP-BATNET: Cascading Au-\\nthor Profiling and Social Network Graphs for Suicide\\nIdeation Detection on Social Media. In Proceedings\\nof the 2019 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nStudent Research Workshop, pages 147\u2013156, Min-\\nneapolis, Minnesota. Association for Computational\\nLinguistics.\\n\\nJishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stu-\\nart Golodetz, Philip Torr, and Puneet Dokania. 2020.\\nCalibrating deep neural networks using focal loss.\\n\\n[-0x-]\"}"}
{"id": "acl-2022-long-318", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Advances in Neural Information Processing Systems, 33.\\nMartha Neary and Stephen M Schueller. 2018. State of the field of mental health apps. Cognitive and Behavioral Practice, 25(4):531\u2013537.\\nAlexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre K\u0131c\u0131man. 2019. Social data: Biases, methodological pitfalls, and ethical boundaries. Frontiers in Big Data, 2:13.\\nJavier Parapar, Patricia Mart\u00edn-Rodilla, David E Losada, and Fabio Crestani. 2021. Overview of erisk at clef 2021: Early risk prediction on the internet (extended overview).\\nSilviu Paun and Edwin Simpson. 2021. Aggregating and learning from multiple annotators. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts, pages 6\u20139.\\nSa\u0161a Petrovi\u0107, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to twitter. In Human language technologies: The 2010 annual conference of the north american chapter of the association for computational linguistics, pages 181\u2013189.\\nYada Pruksachatkun, Sachin R. Pendse, and Amit Sharma. 2019. Moments of change: Analyzing peer-based cognitive support in online mental health forums. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI '19, page 1\u201313, New York, NY, USA. Association for Computing Machinery.\\nTom Pyszczynski and Jeff Greenberg. 1987. Self-regulatory perseveration and the depressive self-focusing style: a self-awareness theory of reactive depression. Psychological bulletin, 102(1):122.\\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks.\\nSebastian Ruder, Matthew E Peters, Swabha Swayamdipta, and Thomas Wolf. 2019. Transfer learning in natural language processing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials, pages 15\u201318.\\nKoustuv Saha and Amit Sharma. 2020. Causal factors of effective psychosocial outcomes in online mental health communities. In Proceedings of the International AAAI Conference on Web and Social Media, volume 14, pages 590\u2013601.\\nRamit Sawhney, Shivam Agarwal, Arnav Wadhwa, and Rajiv Ratn Shah. 2020a. Deep attentive learning for stock movement prediction from social media text and company correlations. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8415\u20138426, Online. Association for Computational Linguistics.\\nRamit Sawhney, Harshit Joshi, Lucie Flek, and Rajiv Shah. 2021. Phase: Learning emotional phase-aware representations for suicide ideation detection on social media. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2415\u20132428.\\nRamit Sawhney, Harshit Joshi, Saumya Gandhi, and Rajiv Ratn Shah. 2020b. A time-aware transformer based model for suicide ideation detection on social media. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7685\u20137697, Online. Association for Computational Linguistics.\\nPeter H Sch\u00f6nemann. 1966. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):1\u201310.\\nJonathan G. Shalom and Idan M. Aderka. 2020. A meta-analysis of sudden gains in psychotherapy: Outcome and moderators. Clinical Psychology Review, 76:101827.\\nAshish Sharma, Adam Miner, David Atkins, and Tim Althoff. 2020. A computational approach to understanding empathy expressed in text-based mental health support. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5263\u20135276.\\nHan-Chin Shing, Philip Resnik, and Douglas Oard. 2020. A prioritization model for suicidality risk assessment. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8124\u20138137, Online. Association for Computational Linguistics.\\nAdam Tsakalidis and Maria Liakata. 2020. Sequential modelling of the evolution of word representations for semantic change detection. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8485\u20138497, Online. Association for Computational Linguistics.\\nAdam Tsakalidis, Maria Liakata, Theo Damoulas, and Alexandra I Cristea. 2018. Can we assess mental health through social media and smart devices? addressing bias in methodology and evaluation. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 407\u2013423. Springer.\\nGerrit JJ van den Burg and Christopher KI Williams. 2020. An evaluation of change point detection algorithms. arXiv preprint arXiv:2003.06222.\\nSumithra Velupillai, Hanna Suominen, Maria Liakata, Angus Roberts, Anoop D Shah, Katherine Morley, David Osborn, Joseph Hayes, Robert Stewart, Johnny Downs, et al. 2018. Using clinical natural language processing for health outcomes research: overview and actionable suggestions for future advances. Journal of biomedical informatics, 88:11\u201319.\\nDavid Wadden, Tal August, Qisheng Li, and Tim Althoff. 2021. The effect of moderation on online mental health conversations. In Proceedings of the International AAAI Conference on Web and Social Media, volume 15, pages 751\u2013763.\\nThe World Health Organization. 2019. The WHO special initiative for mental health (2019-2023): Universal health coverage for mental health.\\nAndrew Yates, Arman Cohan, and Nazli Goharian. 2017. Depression and self-harm risk assessment in online forums. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,\"}"}
{"id": "acl-2022-long-318", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Hyperparameters\\n\\nHere we provide details on the hyperparameters used by each of our models, presented in \u00a74.2:\\n\\n\u2022 **RF**: Number of trees: [50, 100, 250, 500]\\n\\n\u2022 **BiLSTM-we**: Two hidden layers ([64, 128, 256] units), each followed by a drop-out layer (rate: [.25, .5, .75]) and a final dense layer for the prediction. Trained for 100 epochs (early stopping if no improvement on 5 consecutive epochs) using Adam optimizer (lr: [0.001, 0.0001]) optimising the Cross-Entropy loss with batches of size [128, 256], limited to modelling the first 35 words of each post.\\n\\n\u2022 **BiLSTM-bert**: Two hidden layers ([64, 128, 256] and [124] units, respectively), each followed by a drop-out layer (rate: [.25, .5, .75]) and a final dense layer on each timestep for the prediction. Trained for 100 epochs (early stopping if no improvement on 5 consecutive epochs) using Adam optimizer (Kingma and Ba, 2015) (lr: [0.001, 0.0001]) optimising the Cross-Entropy loss with batches of size [16, 32, 64].\\n\\n\u2022 **EM-DM & EM-TR**: Same architecture as BiLSTM-bert, albeit operating on the EM-DM\u2019s (EM-TR\u2019s) output.\\n\\n\u2022 **FSD**: Same architecture as BiLSTM-bert.\\n\\nFor the FSD part, we experimented with word embeddings and representations from Sentence-BERT. We extract features either by considering the nearest neighbor or by considering the centroid, on the basis of the previous [1, 2, ..., 10] posts, as well as on the basis of the complete timeline preceding the current post (11 features, overall). The two versions (nearest neighbor, centroid) were run independently from each other.\\n\\n4\\n\\nen-core-web-lg @ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl\\n\\n\u2022 **SCD-OP & SCD-FP**: We experimented with average post-level word embeddings and representations from Sentence-BERT (results are reported for the latter, as it performed better). For SCD-FP, we stacked two BiLSTM layers (128 units each), each followed by a dropout (rate: 0.25), and a final dense layer for the prediction, with its size being the same as the desired output size (300 for the case of word embeddings, 768 for Sentence-BERT). We train in batches of 64, optimising the cosine similarity via the Adam Optimizer with a learning rate of .0001, and employing an early stopping criterion (5 epochs patience). The final model (i.e., after the SCD part) follows the exact same specifications as BiLSTM-bert, operating on the outputs from the SCD components.\\n\\n\u2022 **BERT(ce) & BERT(f)**: We used BERT-base (uncased) as our base model and added a Dropout layer (rate: .25) operating on top of the [CLS] output, followed by a linear layer for the class prediction. We trained our models for 3 epochs using Adam (learning rate: [1e-5, 3e-5]) and perform five runs with different random seeds (0, 1, 12, 123, 1234). Batch sizes of 8 are used in train/dev/test sets. For the alpha-weighted Focal loss in BERT(f), we used $\\\\gamma = 2$ and $\\\\alpha = \\\\frac{p_t}{1 - p_t}$, where $p_t$ is the probability of class $t$ in our training data. Results reported in the paper (as well as the results for BiLSTM-bert) are averaged across the five runs with the different random seeds.\\n\\nWe trained each model on five folds and selected the best-performing combination of hyperparameters on the basis of macro-F1 on a dev set (33% of training data) for each test fold.\\n\\nB Libraries\\n\\nThe code for the experiments is written in Python 3.8 and relies on the following libraries: keras (2.7.0), numpy (1.19.5), pandas (1.2.3), scikit-learn (1.0.1), sentence_trasformers (1.1.0), spacy (3.0.5), tensorflow (2.5.0), torch (1.8.1), transformers (4.5.1).\\n\\nC Infrastructure\\n\\nAll experiments were conducted on virtual machines (VM) deployed on the cloud computing platform Microsoft Azure. We have used two different VMs in our work:\"}"}
{"id": "acl-2022-long-318", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 the experiments that involved the use of BERT were ran on a Standard NC12_Promo, with 12 cpus, 112 GiB of RAM and 2 GPUs;\\n\u2022 all other experiments were ran on a Standard F16s_v2, with 16 cpus and 32 GiB of RAM.\"}"}
