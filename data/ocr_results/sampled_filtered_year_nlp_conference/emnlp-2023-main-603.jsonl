{"id": "emnlp-2023-main-603", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Explicitation\\n\\nTranslation\\n\\nSource\\n\\nthan language.} speakers may need additional information, as in the red colored text, to enhance their understanding. Flags represent national identity and its associated cultural milieu rather than language.}\\n\\nto convey the necessary background knowledge of the respective source and target cultures (Nida 1964, 1965, 1969, 2003).\\n\\nThe day before Christmas\\n\\nThe day before Christmas\\n\\nThe day before Christmas\\n\\nLa veille de No\u00ebl\\n\\nLa veille de No\u00ebl\\n\\nLa veille de No\u00ebl\\n\\nDespite its utility, research on explicitation is limited because of the dearth of explicitation data that hampers the study of automatic generation. Existing research including Hoek et al. (2015) and McKee (2018) proposes techniques for automatically generating explicitations, motivated by work in human linguistic processing.\\n\\nBridging Background Knowledge Gaps in Translation with Automatic Explicitation\\n\\nHyoJung Han\\n\\nComputer Science, UMIACS, iSchool, LCS, University of Maryland\\n\\njbg@umiacs.umd.edu\\n\\nAbstract\\n\\nWe release our dataset at https://github.com/h-j-han/automatic_explicitation, which contains 15,610 sentence pairs with explicitations and 19,410 dataset splits.\\n\\nFigure 1: Examples of cultural explicitation in French-English. Translators compensate for background knowledge gaps with explicitation in the target language. Here, while ununderlined parts do not need to be introduced in the French-speaking world, English speakers may need additional information, as in the red colored text, to enhance their understanding. Flags represent national identity and its associated cultural milieu rather than language.}\\n\\nCurrent Figure 1\\n\\n Literal\\n\\n Literal\\n\\nLiteral\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n \u2713\\n\\n"}
{"id": "emnlp-2023-main-603", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lapshinova-Koltunski and Hardmeier (2017) is confined to the explicitation of connectives or relational coreferences in discourse translation and lacks systematic strategies to automatically detect or generate the explicitation.\\n\\nWe take a new focus on explicitation and explore whether making necessary implicit knowledge explicit can help downstream translation. Thus, we generate explicitations for culturally relevant content, mimicking human translators. To capture when translators expliciate, we build a dataset (which we call WIKIEXPL) that collects the entities that are described differently across languages. This dataset allows us to identify entities that should be explained in translation and to generate those explicitations. Finally, we test whether our explicitations were useful through an automatic evaluation of the usefulness of explicitations with a multilingual question answering (QA) system, based on the assumption that good explicitations of culturally\u2014for example\u2014Polish entities will increase the accuracy of a QA system.\\n\\nAlthough explicitation is very rare (0.3%) in the training corpus, the collected examples are adequate for developing an explicitation system that helps on a task that needs explicitation: question answering. Moreover, explicitation need not be onerous or expensive: a short phrase is enough to explain obscure entities.\\n\\n2 What is Explicitation?\\n\\nThe term explicitation was first defined by Vinay and Darbelnet (1958) as \u201ca procedure that consists in introducing in the target language details that remain implicit in the source language, but become clear through the relevant context or situation\u201d. Explicitation has been refined over the next decades: Nida (1964) use \u2018amplification\u2019 to refer to \u2018additions\u2019, which are derivable from the socio-cultural context to enhance readability or to avoid misunderstanding due to ambiguity. Blum-Kulka (1986) conduct the first systematic study of explicitation focusing on structural, stylistic, or rhetorical differences between the two languages, formulating an \u201cexplicitation hypothesis\u201d which broadly states that a translation tends to be more explicit than a corresponding non-translation. S\u00e9guinot (1988), however, find that the definition of explicitation is limited and suggests that the term should be reserved for additions in a translated text that cannot be explained by those linguistic differences. Klaudy (1993, 1996, 1998) elaborate on the idea and develop Blum-Kulka\u2019s work, proposing four types of explicitation: obligatory, optional, pragmatic, and translation-inherent. Subsequent studies (Baker, 1996; \u00d8ver\u00e5s, 1998; Dimitrova, 2005) further refine these definitions and categories. We focus on pragmatic explicitation in Klaudy\u2019s sense, the explicitation of implicit background information of the source side speaker where the main purpose is to clarify information that might not be available to the target audience. Other types of explicitation focus on synthetic or stylistic changes\u2014for example, obligatory explicitation is mainly driven by difference of syntactic and semantic structure (e.g., different functions of prepositions and inflections) while optional is by fluency and naturalness (cf. translationese) (Klaudy, 1993, 1996). The main motivation of pragmatic explicitation, on the other hand, is to produce a well-suited translation aimed at a target audience to enable a communicative act (Snell-Hornby, 2006) by bridging the general knowledge gap.\\n\\nWe study automatic explicitation as one of the various efforts to accommodate not only linguistic but also cultural diversity, and further benefit the users of machine translation (MT) and cross-cultural NLP systems on a larger scale (Hershcovich et al., 2022; Dev et al., 2023).\\n\\n3 Building the WIKIEXPL Dataset of Explicitations\\n\\nThis section describes how we collect and examine naturally-occurring explicitations in bitexts commonly used as MT training data. The resulting WIKIEXPL corpus lets us reason about when explicitation is necessary and lets our automatic explicitation method learn how to generate it.\\n\\n3.1 How to Find Explicitation\\n\\nFinding explicitation in the parallel text is a daunting task, so we first need to triage candidates of possible explicitation. To find explicitation examples, we follow the explicitation hypothesis (Blum-Kulka, 1986) and the traits of pragmatic explicitation (Klaudy, 1993, 1998) mentioned in Section 2 and assume the following properties of explicitation in our search:\\n\\n2 In the Wikipedia multilingual data, we do not know whether a text and its aligned equivalents in other languages were generated by translation, from scratch in each language, or through some editing process that mixes the two. We set the direction of translation and find the explicitation under\"}"}
{"id": "emnlp-2023-main-603", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Explicitations are part of unaligned token sequences: an unaligned segment in the target sentence could be an explicitation, as the explicitation hypothesis broadly states that a translation tends to be more explicit than a corresponding non-translation (Blum-Kulka, 1986; Pym, 2005).\\n\\nExplicitations are close to named entities: the unaligned segment could be an explicitation if there is a named entity near the segment while its content is related to the entity and helpful for bridging a background knowledge gap between source and target language users. The gap is more likely to be significant if the entity is more specific to the background or culture of the source audience.\\n\\nExplicitations are more likely for culturally distant entities: a major shift of some property values conditioned on one language to another could indicate the boundness of an entity to a specific language community. For example, the Wiki page for \u201cDominique de Villepin\u201d is closer to pages of French-speaking than English-speaking countries in the relative relational distance.\\n\\nBased on these assumptions, we develop a process to detect explicitation in bitext and decide whether the given entity needs explicitation or not. As a main source of explicitation examples, we choose a not-too-clean parallel corpus from Wikipedia that naturally includes unaligned segments and divergence, as explicitation is by definition a non-literal translation making an implicit detail in the source explicit in the target, and thus introducing unaligned content in one of the languages. If the parallel corpus is too \u201cclean\u201d or too parallel, it is more likely to contain literal translations rather than explicitation examples.\\n\\nOverall, building the WikiEXPL dataset takes three main steps. First, we process the bitext and detect potential explicitation pairs of unaligned segments and entities (Sec 3.2). Secondly, we decide if the entity in the pair needs explicitation, resulting in the selection of candidates among the pairs (Sec 3.3). Lastly, we present extracted candidates the assumptions we made based on the explicitation hypothesis. What we focus on is how some entities are discussed differently in the language of their original culture vs. another language and using that information to design explicitation strategies. More details in Limitations.\\n\\n3.2 Detecting the Explicitation in Bitext\\n\\nWe seek to find instances of explicitation candidates. We first find unaligned segments via word alignment, then we pair segment $u$ with the closest entity $e$ identified by named entity recognizer (NER). Next, we determine whether the segment $u$ is likely an explicitation of entity $e$ by checking if a pair of the segment $u$ and the entity $e$ are nearly positioned within the sentence and related. Formulation of detection is in Appendix, Algorithm 1.\\n\\n3.3 Deciding If Explicitation is Needed\\n\\nThe ability to identify if the entity needs explicitation is the key to both detecting the explicitation example and automating the explicitation process. Since our focus is on culturally-specific explicitation, we need to algorithmically define what makes an entity specific to a socio-linguistic context. Given a relational knowledge base ($KB$) graph, this can be implemented as the number of hops from an entity to source and target language-speaking countries. For instance, \u201cDominique de Villepin\u201d is one hop away from France in Wikipedia, but multiple hops away from English-speaking countries.\\n\\nA complementary method is to compare the number of incoming links to the Wikipedia page in a given language, and the length of the Wikipedia page in a given language, as these indicate the popularity of pages in a given language.\\n\\nWe implement each of these properties and measure whether the property values $prop$ of the given entity $e$ conditioned on each language $l$ of bitext pairs, and check if the shifts from the source language $l_{src}$ to target language $l_{tgt}$ are above the threshold $\\\\tau$:\\n\\n$$prop(e|l_{src}) - prop(e|l_{tgt}) > \\\\tau$$\\n\\nAn entity $e$ that passes all of these checks is considered to be strongly associated with the source language community. For example, if the property shift of the closeness (negative number of hops in $KB$) and normalized length in Wikipedia page is:\\n\\n3. https://linkcount.toolforge.org/api\\n\\n4. https://{lang}.wikipedia.org/w/api.php\\n\\n5. We standardize (zero mean and unit variance) the value of both properties on each language within the entities in extracted WikiMatrix sentence pairs to account for the different offsets within a language.\"}"}
{"id": "emnlp-2023-main-603", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"meaningful enough, then the entity is considered as culturally bounded entity. We additionally exclude entities that are well known globally from our annotation effort even if they are bound to a source community (e.g., the Eiffel Tower in Paris), as entities that are well-known globally are less likely to require explicitation. We use the number of languages in which a Wikipedia page is available for the entity to measure how well-known it is. Formulation of decision is in Appendix, Algorithm 2.\\n\\n3.4 Annotation Framework for WIKIE\\n\\nTo design explicitation models, we need ground truth examples and thus ask humans to manually verify the candidates automatically extracted from bitext as described above. We present the candidates to the human annotators and let them label 1) whether the candidates are explicitation and 2) the span of explicitation. Annotators categorize unaligned words into \u201cAdditional Information\u201d, \u201cParaphrase (Equivalent and no additional info)\u201d, or \u201cTranslation Error/Noise\u201d with a focus on the \u201cAdditional Information\u201d class that potentially contains explicitation. Annotators then determine if explicitation is present by assessing if the additive description explicitly explains implicit general knowledge of the source language to the target language speakers. If confirmed, they mark the explicitation span in both source and target sentences and provide an optional note to justify their decision. Examples for each of categories and more details of the annotation framework are in Appendix B and Figure 8a.\\n\\nWithin the candidates, there are multiple nations within a single linguistic milieu, particularly in the case of French and Spanish. The annotators mark candidates that come from the same country as they do, since the precision and consensus of explicitation might be sub-optimal if, for instance, we assign French entities to Canadian annotators. All annotators are translators who are fluent in both English and the source language.\\n\\nEach example is annotated by three annotators and we assign the label based on the majority vote. We consider the candidates as final explicitation if two or more annotators agree.\\n\\n3.5 Experiment Settings\\n\\nFor the noisy parallel data, we use Wikimatrix (Schwenk et al., 2021) and extract the fr/pl/es-en pairs around the threshold of 1.051 to 1.050 for Source Language French Polish Spanish WikiMatrix 29826 21392 28900 Candidates 791 245 307 Top 1 country Annotated 460 244 220 Average \\\\( \\\\kappa \\\\) 0.66 0.72 0.74 At Least one vote 236 111 73 Explicitation 116 67 44\\n\\nTable 1: WIKIE Construction Statistics. (pragmatic) Explicitations are rare in the noisy parallel corpus. France, Poland, and Spain are the country that is most frequently associated with the candidate entities for each source language. Candidates are shown to three annotators, and labeled as true explicitation by majority votes from the annotators. Cohen (1960)\u2019s \\\\( \\\\kappa \\\\) coefficient shows high agreement among annotators.\\n\\nFrench and Spanish, 1.052 to 1.050 for Polish. 6 We use WikiNEuRal (Tedeschi et al., 2021) for NER and mGENRE (De Cao et al., 2022) to get the Wikidata id for named entities. We ensemble the results of alignment tools, SimAlign (Jalili Sabet et al., 2020) and awesome-align (Dou and Newbig, 2021) to find un-aligned segments in bitext. For proximity, we decide that a segment is near an entity if it is within three words distance. We define the distance as a difference in the index of the tokenized words. For the relatedness between a segment and an entity, we check if a segment is in the content of an entity fetched from Wikipedia. For the decision algorithm, we set the threshold as 1 for the property of the closeness (negative number of hops from entity to given language speaking country in KB), and implement it in practice by checking the existence of a direct relational link to the source country. We use this one property for extracting candidates, and after the data collection, we add two more properties of the noramlized number of incoming links and length of the Wikipedia page to optimize our decision function based on the collected data to have better accuracy. For entities that are well known globally, we exclude entities with Wikipedia pages in more than 250 languages.\\n\\n6 https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix\\n7 The hyperparameters are set on a development set drawn from a preliminary annotation stage, which uses the same framework in the main annotation but with non-expert annotators (volunteer graduate and undergraduate students). The method is applied to each language and it results in the same hyperparameter values.\"}"}
{"id": "emnlp-2023-main-603", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Examples of explicitation are categorized into five types. Integrated are examples with two or more types of explicitation. The boldface indicates added text by explicitation. The spans of added text are marked by annotators. More examples and full-sentence version available in Table 8 and Figure 8b.\\n\\n3.6 Quantitative Analysis on WIKIEXPL\\nExplicitation is quite rare (Table 1) which agrees with pragmatic explicitation statistics in Becher (2010). About 1\u20133% of sentences are explicitation candidates. France, Poland, and Spain are the country that is most frequently associated with the candidate entities for each source language. The overall ratio of the final explicitation example from the initial corpus is 0.2\u20130.4%.\\n\\nThe agreement among annotators is reliable ($\\\\kappa \\\\approx 0.7$) across the languages but suggests there is some subjectivity. About one-third to half of the candidates are marked as explicitation by at least one annotator.\\n\\n3.7 Qualitative Analysis on WIKIEXPL\\nAmong the collected data from all three languages, we analyze examples and categorize them into five types (Table 2): Hypernym, Occupation/Title, Acronym Expansion, Full names, and Nationality. A final type, Integrated, is for the examples with two or more types of explicitation. All the patterns we discovered are consistent with explicitation literature (Klaudy, 1996; Baumgarten et al., 2008; Gumul et al., 2017). The most common type in our collection is adding nationality information to the entity, especially for locations.\\n\\nRealization of explicitation is diverse. The additional information could be accompanied by adjectives, prepositions, integrated into an appositive with commas, or in a parenthetical expression. Detailed descriptions on each type are in Appendix C. Additional examples of each type are available in Table 8, and the full-sentence version with the comments from the annotators are in Figure 8b.\\n\\n4 Automating Explicitation\\nPreviously, the only source of explicitation has been human translators. This section builds on the data from Section 3 to explore generating explicitations automatically.\\n\\n4.1 Deciding if Explicitation is Needed\\nAbualadas (2015) contends translators judge the assumed target reader to decide if an explicitation is needed. Simulating such a decision process is challenging as it requires both instantiating a hypothetical reader and predicting what they know. In contrast, Section 3.3 simplifies this by providing explicitations for entities that are tightly bound to the source socio-linguistic context. We further optimize our decision function to have better accuracy by diversifying the types of properties and tuning the thresholds based on WIKIEXPL (Section 3.5).\\n\\n4.2 Generating the Explanation\\nWe explore several forms in generating the explicitation: 1) SHORT: inserting one or two words after or before the entity, 2) MID: several words or a phrase integrated into the original translation in the form of appositives or parenthetical clauses 3) LONG: 1\u20133 sentences apart from the original translation text as a form of a footnote (examples in Table 3). Although SHORT and MID are in the examples of explicitation in Table 2 and LONG is not, such a long explanation is also considered explicitation and its usual surface manifestation would be a footnote (Gumul et al., 2017). We explore the validity of these three generation types of explicitation and seek to find the most effective one.\\n\\nOur generation is grounded in Wikidata and Wikipedia\u2014rather than free-form text generation\u2014to prevent hallucinations and to control length or the type of explanation. For SHORT explicitations, we fetch a word from instance of or country of from Wikidata (cf. Hypernym, Title, and Nationality in Table 2). For MID, we fetch a description of the entity from Wikidata (mostly the Integrated in Type Source Target\\nHypernym (h) la Sambre the Sambre river\\nOccupation/Title (o) Javier Gurruchaga showman Javier ... and Nationality\"}"}
{"id": "emnlp-2023-main-603", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Automatic Explicitation as Bridging Background Knowledge Gap in Translation and its Evaluation with Multilingual QA\\n\\nDetecting Explicitation\\n\\nWe ask the same human annotators who identified explicitations in the source QA setup to rate the English translation. The annotators rate both aspects of explicitation\u2014whether the explicitation matches the surrounding context and whether the explicitation is appropriate and well-generated or necessary. We evaluate Polish to English translation, and we see how our automatic explicitation is as useful as natural explicitation.\\n\\nExplicitation Example Collection\\n\\nWe are focusing on explicitation that introduces a grammatical error.\\n\\nTable 2. For language type, we fetch three sentences: intrinsic, extrinsic, and community.\\n\\nTable 3: Three types of generation by the length. The form of explicitation for Sambre:\\n\\n| Type | Length | Form | Example of \u201cSambre,\u201d |\\n|------|--------|------|----------------------|\\n| SHORT | 1\u20132 words | Appositive | Sambre river, |\\n| MEDIUM | 3 words\u2013a phrase + Parenthetical | Sambre, river in France and Belgium, |\\n| LONG | 5\u20136 words | Sentence to paragraph | The Sambre river joins the Meuse in the Wallonian capital Namur (France). |\\n\\n5.1 Intrinsic Evaluation\\n\\nWe assume that the explicitation may not be helpful if the information was useful and well-generated or necessary. We suggest two evaluations in English will improve English understanding.\\n\\nIntrinsic Evaluation\\n\\nWe assume multilingual QA system simulate the knowledge gap. We assume that unaligned content in the target language might have minimal performance changes after feeding the system get to the right answer. If that new information helps a system get to the right answer, it suggests that the information was useful and well-generated.\\n\\nExtrinsic Evaluation using Question Answering System\\n\\nWe utilize \u201canswerable\u201d questions to local community of [4]. We assume that the explicitation may not be helpful if the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\n5 Evaluating Explicitation\\n\\nExplicitation is challenging to evaluate as each one has its own limitations but are complementary. Among various and broad coverage of Explicitation, we focus on Explicitation that explicitly introduces information into the target language or language using country in KB graph.\\n\\nWe first extract candidates from the noisy parallel data [3]. We assume that unaligned content in the target language might have minimal performance changes after feeding the system get to the right answer. If that new information helps a system get to the right answer, it suggests that the information was useful and well-generated.\\n\\nTo see the difference in the effectiveness between source QA and target QA, we assume multilingual QA system simulate the knowledge gap. We assume that unaligned content in the target language might have minimal performance changes after feeding the system get to the right answer. If that new information helps a system get to the right answer, it suggests that the information was useful and well-generated.\\n\\nFigure 2: How we check if our explicitations work: detected and targeted. We assume that the explicitation may not be helpful if the information was useful and well-generated or necessary.\\n\\nAssuming that unaligned content in the target language might have minimal performance changes after feeding the system get to the right answer, we assume that the information was useful and well-generated or necessary.\\n\\nUsefulness is how much it improves the timeliness property of QA system in [4]. We also see how human process the additional information as Explicitation by human annotators. We assume that the explicitation may not be helpful if the information was useful and well-generated or necessary.\\n\\nExplicitation and automatically generate Explicitation? We assume that the explicitation may not be helpful if the information was useful and well-generated or necessary.\\n\\nWe assume that the explicitation may not be helpful if the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume that the information was useful and well-generated or necessary.\\n\\nWe assume that the information was useful and well-generated or necessary. We assume"}
{"id": "emnlp-2023-main-603", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Statistics of XQB dataset for the evaluation.\\n\\nFor the case of XQB-es, the 598 named entities are detected in both sides of Spanish and English pairs of 144 questions. Among 598 entities, our decision algorithm decides 116 entities need explicitation. The extraction rate of explicitation in the domain of QA dataset is higher than in the general domain.\\n\\n5.3 Experiment Settings\\n\\nDataset. For the parallel QA dataset, we use the Cross-lingual Quizbowl test set (Han et al., 2022, XQB). XQB-es has 148 parallel Spanish to English question pairs and XQB-pl has 512 for Polish to English. One question usually consists of 3\u20135 sentences (Table 4). We use question pairs that have named entities recognized in both source and target languages text.\\n\\nModel. Our multilingual QA system is LLAMA (Touvron et al., 2023) (7B for XQB-es and 13B for XQB-pl). This model is comparable or better without finetuning compared to RNN models trained on Quizbowl (Rodriguez et al., 2019) datasets (Appendix A Table 6). The input prompt is one full question, one partial question, answers, and simulated scores $\\\\in [0, 1]$ as a guiding example, and append a real question at the end. An example of an input prompt is in Appendix, Figure 5. We set the output of the LLAMA model to have one guess and its confidence score, and use a threshold buzzer to decide buzz. The confidence threshold for the buzzer in EW is set to 0.4 for XQB-pl and 0.8 for XQB-es, which is fit to the original text without explicitation. We accept any of the synonyms from Wikidata in either the source or target language as a correct answer. We set 30\u201350 character splits for the step size, usually having 30\u201332 splits for one question.\\n\\nMetric. We adopt the same metrics, Expected Wins ($EW$) and $EW$ with an oracle buzzer ($EWO$) as Rodriguez et al. (2019) and Han et al. (2022). Both metrics map the position of where a system answers a question to a number between 0 and 1 (higher is better). The difference is when the system provides an answer: $EWO$ assumes a QA system can answer as soon as it predicts correctly as the top-ranked hypothesis while $EW$ is a more conservative measure that requires not just producing an answer but also deciding whether to offer its guess as the answer (i.e., confidence estimation).\\n\\nIn both cases, the number represents the probability that the system will answer before a \u201ctypical\u201d human from the Boyd-Graber et al. (2012) dataset, weighting early answers higher than later answers. Full Input Accuracy is a measurement with a whole text input of a question unlike $EW$ or $EWO$.\\n\\n6 Intrinsic Evaluation Results\\n\\nAnnotators evaluate explicitations in English questions of XQB-pl on decision, generation, and integration (setup in Section 5.1, example in Figure 6). To turn the Likert scale into a single number, we interpret high as 1, mid as 0.5, and low as 0. For the quality of generation and integration, we show the result for each generation type. Annotators score the explicitation decision 0.71 (Table 5). Negative examples (boldface as additional explanation added by explicitation) include too obvious ones (e.g., \u201cWarsaw, the capital of Poland\u201d) or those evident given the context of the sentence (\u201cauthors include the novelist Sienkiewicz\u201d). The quality of generation is assessed highest on LONG type where the annotator evaluates about 9. Further details of metrics are available in Rodriguez et al. (2019).\"}"}
{"id": "emnlp-2023-main-603", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Extrinsic evaluation of automatic explicitation by comparison of the effect of explicitation. The original is the performance without explicitation. A higher increase rate in the English QA task indicates the effectiveness of our automatic explicitation methods. The generation type for all plots is M\\\\textsubscript{ID}.\\n\\n95% of the generated explanations are appropriate and useful. The quality of generation decreases with shorter explanations. The added explanation should be easy for the target reader, but sometimes the explanation itself needs an explanation: for \\\"Sejny, city and urban gmina of Poland\\\", the annotators point out that \\\"town\\\" is more familiar than \\\"gmina\\\" to an English audience.\\n\\nExtrinsic Evaluation Results\\n\\nMain Results. Explicitation is more effective in English QA tasks than Polish on all metrics in XQB\\\\textsubscript{-pl} (Figure 3a), which indicates that our decision algorithm effectively selects entities that need explicitation and that the added information is provides useful information (complementing the intrinsic evaluation in Section 6). XQB\\\\textsubscript{-es} shows similar trends on EW and EWO while full input accuracy shows almost the same results after the explicitation in Figure 3b.\\n\\nWe attribute the different trends on full input accuracy for XQB\\\\textsubscript{-pl} and XQB\\\\textsubscript{-es} to different levels of difficulty of full-text questions: the questions that includes culturally bounded entity in XQB\\\\textsubscript{-es} are easier than in XQB\\\\textsubscript{-pl}. If a question is already easy, then additional information would not be very helpful and thus shows a small or no increase rate like in XQB\\\\textsubscript{-es}. This is corroborated by the huge accuracy gap of 0.2 between XQB\\\\textsubscript{-es} and XQB\\\\textsubscript{-pl} in English QA. On the other hand, both in XQB\\\\textsubscript{-pl} and XQB\\\\textsubscript{-es} show great improvements of increase rate.\\n\\nFigure 4: Increase rate of EW on English QA task by different types of explicitation generation, and the rate of answer inclusion. M\\\\textsubscript{ID} turns out to be most effective generation type for both XQB\\\\textsubscript{-pl} and XQB\\\\textsubscript{-es}. Answer inclusion rate increases as the length of additional explanation increases, while does not affect the increased rate of performance.\\n\\nAn additional comparison between explicitation and non-explicitation strengthens the validity of our decision algorithm in Appendix E, Figure 7.\"}"}
{"id": "emnlp-2023-main-603", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We compare the generation type described in Section 4.2 and analyze the effect of answer inclusion in the explicitation (Figure 4). **MID** is the most effective type of explicitation, and longer explicitations are not necessarily more effective but are more likely to include the answer, which differs from what we observe in intrinsic evaluation where the longer explanation tends to have a better quality of explanation. The output example of each generation type within a XQB-pl question pair is available in Figure 6. (Further analysis in Appendix D, Table 7)\\n\\n### Related Work\\n\\n**Explicitation in Contemporary Works.** Many of contemporary works with the term \u201cexplicitation\u201d focus on discourse MT (Hoek et al., 2015; Webber et al., 2015) and mainly developed by Lapshinova-Koltunski et al. (2019, 2020, 2021). However, despite the broad coverage of the term, the explicitation in these studies are limited to insertion of connectives or annotations of coreference in the target side. Although the high-level concept of explicitation is common and our detection of explicitation starts similarly to Lapshinova-Koltunski and Hardmeier (2017) by finding alignment discrepancies, our focus is on different aspects of explicitation where the main purpose is to fill the gap of background knowledge between source and target sides. Kr\u00fcger (2020) attempts to identify instances of explicitation in machine translated documents, while it deals with more general definition of explicitation rather than culturally-specific explicitation. Explicitations can also be viewed as a form of divergence in meaning between source and target text. However prior work on detecting these based on cross-lingual semantic representations (Vyas et al., 2018; Briakou and Carpuat, 2020) target a much broader category of divergences than those that are culturally relevant.\\n\\nOur work also relates to contemporaneous work on culturally aware MT. Yao et al. (2023) introduce a data curation pipeline to construct a culturally specific parallel corpus, and explore LLM prompting strategies to incorporate culturally specific knowledge into MT. Lou and Niehues (2023) introduce a semi-automatic technique to extract explanations based on Wikikpedia and alignment tools, similar to ours. Our work complements these studies by grounding the definition of explicitation in the translation studies literature, and by evaluating explicitations with both an intrinsic human evaluation and an extrinsic evaluation of their usefulness in multilingual QA.\\n\\n### Elaboration and Clarification\\n\\nThe explicitation of implicit background knowledge resembles text simplification and question rewriting, in the sense that these techniques make the text more accessible to targets, thus enhancing communications. Srikanth and Li (2021) and Wu et al. (2023) present elaborative text simplification where it adds the contents to elaborate the difficult concepts. Rao and Daum\u00e9 III (2018, 2019) develop methods for generation and reranking clarification questions that ask for information that is missing from a given context. Elgohary et al. (2019) introduces the task and the dataset of question-in-context rewriting that rewrite the context-dependent question into a standalone question by making the context explicit. Ishiwatari et al. (2019) performs a task of describing unfamiliar words or phrases by taking important clues from both \u201clocal\u201d and \u201cglobal\u201d context, where we have in common in the methods of generating description from Wikidata.\\n\\n### Conclusion and Future Work\\n\\nWe introduce techniques for automatic explicitation to bridge the gap of background knowledge between the source speaker and the target audience. We present an explicitation dataset, WIKIEXPL, extracted from WikiMatrix and annotated by human translators. We verify the effectiveness of our automatic explication with a direct evaluation by human translators and with extrinsic evaluation by using a multilingual QA and comparing its influence on different language tasks. Our automatic explicitation system is effective based on both intrinsic and extrinsic evaluation.\\n\\nFuture works include more closely simulating an explicitation decision and generation process of a human translator as it requires both instantiating a hypothetical reader and predicting what they might know and how to describe it while trying not to overdo it. Rather than machine QA systems, having human participants complete the QA task with and without explicitations will let us measure their usefulness more directly. Another extension would be adapting a speech-to-speech simultaneous interpretation format where the lengthy explicitation will be penalized, and thus taking the context into account to have less redundant output.\"}"}
{"id": "emnlp-2023-main-603", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nThe number of explicitations samples we collected in WIKI EXPL is small, particularly when compared to the wealth of massively multilingual benchmarks that annotate relatively common language phenomena. Still, we argue that it provides a sufficient basis for a first study of cross-lingual explicitations, a relatively infrequent and understudied phenomenon, as a valuable resource for research on translation. Furthermore, we contend that the methodology we introduced can be used to expand it further.\\n\\nWe simplify the concept of culture by choosing one majority country to which the entities of candidate explicitation belong among extracted candidates from Wikidata (Section 3.5 and 3.6). For example, we choose France among many French-speaking countries as the number of entities from France is the largest among the extracted candidates. Then, we collect an explicitation example with French annotators as the accuracy and agreement of explicitation could be sub-optimal if we ask, for example, French entity to Canadian annotators. As a consequence, there is a possible mismatch between languages and cultures that would degrade automated explicitation as the hyperparameters like the threshold of properties in Section 3.2 may not be optimal for different cultures.\\n\\nThe experiments are limited to one direction, into English. Our methods of automating explicitation and its evaluation focus on English speakers. We use simplified methods of integrating explicitation into a translation that mainly works for the English language, which could be further improved by improving fluency.\\n\\nIn finding the explicitation in Wikipedia, we set the direction of translation and collect the example base on the trait of explicitation that includes the unaligned tokens as discussed in Section 3.1. However, there is no information about the exact translation direction, the methods of translation, or even if it is a non-translation and just aligned equivalents that are generated from scratch independently in each language. For example, in the given bitext of French and English, we do not know if this is translated from French to English or English to French. Our focus is on how the same entity is presented differently in its original culture and in different language-speaking cultures, and this can be conducted without knowing the exact generation process of bitext. Better quality of explicitation could be collected via extracting the bitext that has a clear translation process.\\n\\nOur decision and generation algorithm are restricted to named entity based on the assumption in Section 3.1. However, the background gap is not always related to named entities as the year \\\"1800\\\" in the second example in Figure 1. This is annotated as explicitation since French readers are well acquainted with the period of the French revolution, while non-French readers may not be. Deciding whether a non-named entity, such as a certain period of time, needs explicitation and generating an explanation for it are more challenging problems than those related to named entities, and therefore, they require more advanced algorithms.\\n\\nOur generation methods in Section 4.2 are not specifically tailored for the target audience. Instead, they are designed to retrieve information using the Wikidata and Wikipedia API in a simplified manner. In addition to the analysis of Section 6, the annotators point out some examples have too much information integrated into one place, for example, \\\"August\u00f3w, Sejny, Poland\\\", and suggest the replacement rather than the mere addition like \\\"August\u00f3w, Poland\\\" by removing unnecessary details. This failure case clearly displays the limitation of our generation methods, where the generated explanation is both incorrect and needlessly adding another difficult term in the explicitation for the target audience. Another example could be explaining a French-based entity, \\\"J'accuse...!\\\". The explanation from Wikipedia (\\\"the open letter published by Emile Zola in response to the Dreyfus affair.\\\") barely helps target reader as it introduces another unknown information like \\\"Emile Zola\\\" or \\\"Dreyfus affair\\\". When generating explicitation, a human translator would consider the importance of the time period and decide to do recursive explicitation which recursively explains the not well-known words in the explicitation itself. (e.g., \\\"Dreyfus affair, a political scandal in France, 1906\\\") These require further exploration of generation methods that need to be conditioned on target readers.\\n\\nOur proposed method of automating explicitation is grounded in structured data. It enables precise control over deciding and generating explicitation by benefiting from consistency and enrichment of the data while avoiding hallucinations as Large Language Models (LLM). However, this approach may suffer from rigidity and limited creativity, thus having less flexibility on dealing with diverse natural language input, which could lower the quality.\"}"}
{"id": "emnlp-2023-main-603", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of explicitation. Exploring integration of structured data and language models could be next feasible step as in Yao et al. (2023).\\n\\nEthics Statement\\n\\nThe workers who annotated and evaluated the explicitation candidates extracted from WikiMatrix and our automatically generated explicitation were paid at a rate of USD 0.25 per explicitation example. The resulting estimated hourly wage is above the minimum wage per hour in the United States.\\n\\nAcknowledgment\\n\\nWe thank the anonymous reviewers, Pranav Goel, Sathvik Nair, Ishani Mondal, Eleftheria Briakou and the members of the CLIP lab at UMD for their insightful and constructive feedback. This work was funded in part by NSF Grants IIS-1750695, IIS-2147292, and IIS-1822494.\\n\\nReferences\\n\\nOthman Ahmad Ali Abualadas. 2015. A linguistically-oriented approach to literary translation: A comparative pragmatic study of three Arabic renditions of the English novel \u201cWuthering Heights.\u201d\\n\\nAus Adil Abdulwahab. 2012. Necessities of pragmatic explicitation in translating English short stories into Arabic. Adab AL Rafidayn, 42(61):1\u201334.\\n\\nM. Baker. 1996. Corpus-based Translation Studies: The Challenges that Lie Ahead, Benjamins Translation Library, pages 175\u2013188. John Benjamins Publishing Company, Netherlands.\\n\\nNicole Baumgarten, Bernd Meyer, and Demet \u00d6z\u00e7etin. 2008. Explicitness in translation and interpreting: A critical review and some empirical evidence (of an elusive concept). Across Languages and Cultures, 9(2):177\u2013203.\\n\\nViktor Becher. 2010. Towards a more rigorous treatment of the explicitation hypothesis in translation studies. Trans-kom, 3(1):1\u201325.\\n\\nShoshana Blum-Kulka. 1986. Shifts of cohesion and coherence in translation. The Translation Studies Reader. London/New York: Routledge, pages 209\u2013305.\\n\\nJordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daum\u00e9 III. 2012. Besting the quiz master: Crowdsourcing incremental classification games. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1290\u20131301, Jeju Island, Korea. Association for Computational Linguistics.\\n\\nEleftheria Briakou and Marine Carpuat. 2020. Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1563\u20131580, Online. Association for Computational Linguistics.\\n\\nEwa S Callahan and Susan C Herring. 2011. Cultural bias in Wikipedia content on famous persons. Journal of the American society for information science and technology, 62(10):1899\u20131915.\\n\\nJ. Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37.\\n\\nNicola De Cao, Ledell Wu, Kashyap Popat, Mikel Artetxe, Naman Goyal, Mikhail Plekhanov, Luke Zettlemoyer, Nicola Cancedda, Sebastian Riedel, and Fabio Petroni. 2022. Multilingual autoregressive entity linking. Transactions of the Association for Computational Linguistics, 10:274\u2013290.\\n\\nSunipa Dev, Vinodkumar Prabhakaran, David Adelani, Dirk Hovy, and Luciana Benotti, editors. 2023. Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP). Association for Computational Linguistics, Dubrovnik, Croatia.\\n\\nB.E. Dimitrova. 2005. Expertise and Explicitation in the Translation Process. Benjamins translation library: EST subseries. John Benjamins Publishing Company.\\n\\nZi-Yi Dou and Graham Neubig. 2021. Word alignment by fine-tuning embeddings on parallel corpora. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2112\u20132128, Online. Association for Computational Linguistics.\\n\\nAhmed Elgohary, Denis Peskov, and Jordan Boyd-Graber. 2019. Can you unpack that? learning to rewrite questions-in-context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5918\u20135924, Hong Kong, China. Association for Computational Linguistics.\\n\\nShi Feng and Jordan Boyd-Graber. 2019. What AI can do for me: Evaluating machine learning interpretations in cooperative play. In International Conference on Intelligent User Interfaces.\\n\\nEwa Gumul et al. 2017. Explicitation in simultaneous interpreting. A study into explicitating behaviour of trainee interpreters. Wydawnictwo Uniwersytetu \u015al\u0105skiego.\\n\\nScott A Hale. 2014. Multilinguals and Wikipedia editing. In Proceedings of the 2014 ACM conference on Web Science, pages 99\u2013108.\"}"}
{"id": "emnlp-2023-main-603", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-603", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Detecting explicitation span in bitext\\n\\nInput: Sentence pair $(X_{src},X_{tgt})$\\n\\nOutput: Candidate $C$ with unaligned segment $u$ and its related entity $e$\\n\\n1. $U = \\\\text{unaligned} \\\\_ \\\\text{segments}_{tgt}(X_{src},X_{tgt})$\\n2. $R = \\\\{(e,u)|\\\\text{is}\\\\_\\\\text{near}\\\\_\\\\text{and}\\\\_\\\\text{related}(e,u), e \\\\in E, u \\\\in U\\\\}$\\n3. $C = \\\\{(e,u)|\\\\text{decide}\\\\_\\\\text{explicitation}(e),\\\\#\\\\text{Alg2}(e,u) \\\\in R\\\\}$\\n\\nReturn: $C$\"}"}
{"id": "emnlp-2023-main-603", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Dataset | Guesser Buzzer Task | Expected Wins | Expected Wins with Full Input |\\n|---------|---------------------|---------------|-------------------------------|\\n| XQB-pl  | Rnn Rnn English QA  | 0.35          | 0.70                          |\\n|         |                     |               |                               |\\n| LLaMA   | Threshold English QA | 0.26         | 0.64                          |\\n|         |                     |               |                               |\\n| LLaMA   | Threshold Polish QA  | 0.17          | 0.56                          |\\n|         |                     |               |                               |\\n| XQB-es  | Rnn Rnn English QA  | 0.30          | 0.65                          |\\n|         |                     |               |                               |\\n| LLaMA   | Threshold English QA | 0.10         | 0.72                          |\\n|         |                     |               |                               |\\n| LLaMA   | Threshold Spanish QA | 0.09         | 0.75                          |\\n\\nTable 6: Baseline performance of the original question without explicitation. LLaMA is comparable or better without finetuning compared to RNN models trained on Quizbowl datasets.\\n\\nGive your top guess with the confidence score to the following English questions:\\n\\nQuestion: \u201cThis country has a winding, unpaved road that crosses the Los Yungas region and is dubbed as \u201cthe most dangerous road in the world\u201d. A mountain that overshadows the city of Potosi in this country provides a large part of the silver ore that made Spain rich during the colonial era. This country\u2019s capital city is the highest in the world and shares Lake Titicaca with its northwestern neighbour, Peru. For 10 points, name this landlocked country in South America which has two capital cities: Sucre and Paz.\u201d\\n\\nTop guess and its confidence score: (\u201cBolivia\u201d, 0.6)\\n\\nQuestion: \u201cThis person appears as one of the main characters in the detective novel, \u201cTeor\u00eda del Manglar\u201d, written by Luis Carlos Musso, winner of the Miguel Riofrio National Literature Competition. Yoshinori Yamamoto, revealed that he had managed to collect more than 4,500 recordings. He is one of Ecuador\u2019s most popular singer-songwriters and his most famous song is \u201cNuestro Jurament...\u201d\\n\\nTop guess and its confidence score: (\u201cJulio Jaramillo\u201d, 0.1)\\n\\nQuestion: \u201cA famous portrait of this man, created by Jacques-Louis David, shows him pointing to the sky while he prepares to drink hemlock. The account of this man\u2019s execution was written by one of his students in \u201cApology\u201d. For 10 points, name this Athenian philosopher who taught thinkers like Plato and is famous for saying, I know only one thing: that I know nothing\u201d.\\n\\nTop guess and its confidence score: \\n\\nFigure 5: Example prompt input to LLaMA for multilingual QA task. This example is from XQB-es and English QA task.\\n\\nAlgorithm 2: Deciding the necessity of explicitation on given entity\\n\\nInput: Entity e\\n\\nOutput: True if Entity e needs explicitation else False\\n\\nParam: Language Pair (lsrc, l tgt), \\\\( P = \\\\{ (prop, \\\\tau_k) \\\\mid \\\\text{Property and its threshold} \\\\} \\\\)\\n\\n1 for \\\\((prop_k, \\\\tau_k) \\\\in P \\\\) do\\n\\n2 if not \\\\((prop_k(e | lsrc) - prop_k(e | tgt)) > \\\\tau_k \\\\) then\\n\\n3 Return False\\n\\n4 end\\n\\n5 if is general(e) then\\n\\n6 Return False\\n\\nReturn True\\n\\nExplicitation Explicitation with Answer without Answer Type Metric Orig Expl Orig Expl S\\nSHORT EW 0.44 0.29 0.08 0.18\\nEWO 0.82 0.84 0.51 0.54\\nFIA 0.43 1.00 0.46 0.34\\nM\\nLONG EW 0.29 0.26 0.04 0.15\\nEWO 0.66 0.71 0.48 0.50\\nFIA 0.65 0.85 0.43 0.39\\nL\\nO NG EW 0.27 0.20 0.05 0.06\\nEWO 0.63 0.67 0.49 0.47\\nFIA 0.64 0.68 0.41 0.30\\n\\nTable 7: Performance differences by explicitation in the separate case of answer inclusion in XQB-es. A Baselines Comparison Table 6 shows the baseline wins metric of original questions without explicitation. We also compare the English RNN models used in QANTA (Rodriguez et al., 2019) and SimQA (Han et al., 2022). RNN has higher EW as it uses the guesser and the buzzer specifically trained for Quizbowl. However, EWO and Full Input Accuracy are not affected by the buzzer, so LLaMA is comparable even though...\"}"}
{"id": "emnlp-2023-main-603", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This mountain range, which is one of the largest in Europe, stretches across the territory of eight countries. This area is the watershed between the catchment areas of the Baltic Sea and the Black Sea and there are many rivers flowing out of it, including the Vistula. To get a point, name the mountain range of which the Tatras are a part, and the highest peak is Gerlach.\\n\\nPolish Question\\n\\nTen \u0142a\u0144cuch g\u00f3rski, b\u0119d\u0105cy jednym z najwi\u0119kszych w Europie, ci\u0105gnie si\u0119 przez terytorium o\u015bmiu kraj\u00f3w. Aby otrzyma\u0107 punkt, nazwij \u0142a\u0144cuch g\u00f3rski, w kt\u00f3rego sk\u0142ad wchodz\u0105 mi\u0119dzy innymi Tatry, a najwy\u017cszym szczytem jest Gerlach.\"}"}
{"id": "emnlp-2023-main-603", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to select \\\"Translation Noise/Error\\\" (e.g. participa\u2192 introduce). We focus on the class \\\"Additional Information\\\" as it could possibly contain real explicitation which continues to the next question. The second question is to decide \\\"Is this explicitation\\\". We ask annotators \\\"Does this additive description explicitly explain the implicit general knowledge of the source language (SL) speaker for the target language (TL) speaking audience?\\\". We indicate to them that this additional information is expected to be more useful to the target readers but not necessary to most speakers of the source language, and such implicit knowledge is less likely to be known by TL speaker compared to someone who is fluent in SL or familiar with SL speaking culture. This kind of general knowledge becomes explicit by the translator to enhance the reader's understanding of the translated text. For example, let's see la Sambre\u2192 the Sambre river. This would be an example of explicitation, as it gives more context to the target audience who is not familiar with the name \\\"Sambre\\\" by adding the word \\\"river\\\" which does not exist in the source, while source language speakers may not need such explanation because it could be obvious to them. However, Jeremy\u2192 her policeman husband Jeremy is not explicitation but a simple addition of specific facts because the named entity is not famous figures or the added facts are not commonly well-known knowledge in the community of source language speakers. If the annotator selects yes to the second question, then we instruct them to mark the span of explicitation in both the source and target sentence and leave a note about the reason for their decision if they have any as in Figure 8b. \\n\\nC Types of Explicitation\u2014Full Examples and Analysis \\n\\nTable 8 shows additional examples of explicitation are categorized into five types from Table 2. One representative case of explicitation is introducing the hypernym of the entity (h) or adding titles/occupations to the human name (o). While the name representation alone might be clear for source language speakers due to its familiarity, the target audience may lack awareness or familiarity with the name. Acronym Expansion (a) is also common types of explicitation (Baumgarten et al., 2008; Gumul et al., 2017). However, not all acronym expansion is marked as explicitation by annotators if the acronym is not commonly used by native speakers. Full name representation is considered as one type of explicitation (Klaudy, 1996). A famous person's first or last name is often omitted for convenience. As our annotator comments, we do not add the first name, William when mentioning Shakespeare, and likewise, there's no need to add the first name, Miguel de to Cervantes (f\u20131) in Spain. Full name representations (f) are marked as explicitation as their name is famous within their country but presented in full name in English as it might not be the case outside. The most common type is adding nationality information to the entity, especially for the location. Usually, the name of the nation is added to the name of a not well-known city, providing the context information of its country. All these types can be integrated into one example (i). Typically, the explicitation adds the nationality and its title or the hypernym for the target audience who might not know who or what the entity is. The form of explicitation is diverse. The additional information could be accompanied by prepositions or integrated into the form of appositive with commas or in a parenthetical expression.\\n\\nD Analysis on Answer Inclusion Cases \\n\\nTable 7 shows the performance changes by explicitation in the separate case of answer inclusion. The original performance before the explicitation is already higher in the case of answer including explicitation compare to without answer. This indicates that the answer inclusion tends to happen in easy questions, where the original question text is already evident and the additional information is highly probable to include the answer because it is so obvious.\\n\\nE Comparison between Explicitation and Non-explicitation \\n\\nIn Figure 7, we specifically examine the validity of the decision algorithm by comparing the influence of additional information in all named entities (left) and in non-explicitation (right) to those in the explicitation (center). Here, the non-explicitation indicates the entities with additional information that is not considered to be needed to do explicitation. In the explicitation results (center), the increase rate of the English QA task is higher than that of the Polish one, while additional information in all entities (left) and non-explicitation (right) is\"}"}
{"id": "emnlp-2023-main-603", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Additional examples of explicitation from Table 2. We categorize the collected explicitation example into five types. Integrated is for the examples that include two or more types of explicitation. The boldface indicates added text by explicitation. The spans of added text are marked by annotators. Full-sentence version available in Figure 8b.\\n\\nmore effective in Polish QA task in both full input accuracy and EW metrics. This demonstrates the effectiveness of our algorithm in determining the need for explicitation.\\n\\nAdditional Related Work\\nStudies on pragmatic explicitation. There are several succeeding research that studies on pragmatic explicitation (Saldanha, 2008; Becher, 2010; Adil Abdulwahab, 2012). Adil Abdulwahab (2012) emphasizes the need for pragmatic explicitation in the translation of English short stories into Arabic. Becher (2010) did a rigorous search of every type of explicitation in a corpus of English popular scientific magazine articles and their German translations and find that pragmatic explicitation is rare, which corresponds to our empirical results in Section 3.6.\\n\\nCross-cultural NLP and Wikipedia. Despite the impressive gains in NLP fields, Hershcovich et al. (2022) identifies the intractable challenges in cross-cultural NLP by pointing out that the production and the consumption of the contents are largely varied not just by language but also by culture. An epitomic example would be the multilinguality in Wikipedia and its differences in content across languages. Callahan and Herring (2011) specify a cultural bias in the content, especially if it is related to famous people. Massa and Scrinzi (2012) emphasize differences in perspectives among diverse Wikipedia communities in distinct languages. Hale (2014) and Hecht and Gergle (2010) find a \\\"surprisingly\\\" small degree of content overlap between different languages in Wikipedia. Still, various efforts have been made to reflect these cultural varieties in the data and the models (Dev et al., 2023). As one of the various efforts to accommodate cultural diversity better to serve users of cross-cultural NLP systems, we explore the possibility of automatic explicitation to be beneficial on a larger scale which could help bridge the cultural gap between the language user communities.\"}"}
{"id": "emnlp-2023-main-603", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The work begins on a line of country works in front of the fortress, with its endpoints on the Sambre, to cut the fortress of land access.\\n\\nIn this case it's a clear explicitation and it's useful because in the source text there is an acronym that everybody in Spain would easily identify, as a gift from his soldiers.\\n\\nVery clear example. 'Jan III' is enough for a Polish person to know it's about Sobieski and he was a king.\\n\\nIn addition to the presentation of the rich collections of European and Far Eastern art, the central part of the palace was devoted to the memory of {king Jan III} and the great national past.\\n\\nOn June 28, 1914, together with Franciszek W\u00f3jcik, he addressed the peasants with financial support for the Polish People's Left.\\n\\nIn the 16th century and after the battle of San Quint\u00edn that ended on August 10, 1557, feast of San Lorenzo, {Felipe II} decided to build {San Lorenzo del Escorial} in honor of the saint.\\n\\nThey give them a book, remembering the death of two greats of European literature, {Cervantes} and Shakespeare and the Spanish-American literary personality, Inca Garcilaso.\\n\\nIn this promotion was also admitted, alongside Patrick Levaye, Richard Descoings (former director of the Institute of Political Studies in Paris), Fran\u00e7ois Asselineau (second of promotion and president of the People's Republican Union), Patrick Galouzeau de Villepin (brother of Dominique de Villepin), Jean-Fran\u00e7ois Cirelli (President From Blackrock France), Jean-Claude Mallet (former secretary general of national defense), Fran\u00e7ois Asselineau (second of promotion and president of the People's Republican Union) and Inca Garcilaso.\\n\\nThe only television network that could record inside the square at night from June 3 to 4 was {tve}.\\n\\nThe target audience with the contextual information about Dominique de Villepin, even though he is a well-known figure in France and requires no further explanation for the French audience.\\n\\nThe only network which was able to record shots during the night of 4 June was {tve}.\\n\\nOn June 28, 1914, he and Franciszek W\u00f3jcik with a appeal to the peasants for financial support {PSL - left}.\\n\\nShe spent her youth mainly in Warsaw, where she lived with her family in {Belvedere} and in Sulej\u00f3wek at the cottage of Milusin, which had been given as a gift from his soldiers.\\n\\nBesides European and Oriental art, the central part of the palace displayed a commemoration of {king Jan III} and the great national past.\\n\\nIn this promotion was also admitted, at the sides of Patrick Levaye, Richard Descoings (director of the Paris Institute of Political Studies), Patrick Galouzeau de Villepin (brother of the former Prime Minister), Fran\u00e7ois Asselineau (second of promotion and president of the People's Republican Union), Jean-Fran\u00e7ois Cirelli (President From Blackrock France), Jean-Claude Mallet (former secretary general of national defense), Fran\u00e7ois Asselineau (second of promotion and president of the People's Republican Union), Patrick Galouzeau de Villepin (brother of Dominique de Villepin) and Inca Garcilaso.\\n\\nIt's just adding the name of a really famous person, so there's no need as this person is known by his surname. The same way that they didn't add the first name (William) when mentioned Shakespeare, there's no need to add the first name (Miguel de) to Cervantes.\"}"}
