{"id": "acl-2023-long-641", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MULTI INSTRUCT: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning\\n\\nZhiyang Xu\u2217, Ying Shen\u2217, Lifu Huang\\n\\nComputer Science Department\\nVirginia Tech\\n{zhiyangx, yings, lifuh}@vt.edu\\n\\nAbstract\\nInstruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multi-modal tasks. In this work, we introduce MULTI INSTRUCT, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA (Wang et al., 2022a) as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale NATURAL INSTRUCTIONS dataset (Mishra et al., 2022). Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric \u2013 Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task.\\n\\n1 Introduction\\nWith the advances in large-scale pre-trained language models (PLMs), recent studies have explored various efficient learning paradigms (Brown et al., 2020; Liu et al., 2021; Wei et al., 2021; Xie et al., 2021) to generalize PLMs to new tasks without task-specific tuning. Among these, instruction tuning (Wei et al., 2021) has achieved significant success in zero-shot learning on natural language processing tasks. By fine-tuning a PLM on tasks described through instructions, instruction tuning allows the model to learn to understand and follow the instructions to perform predictions on unseen tasks. Recent advancement in multimodal pre-training (Wang et al., 2022a; Alayrac et al., 2022; Bao et al., 2022; Wang et al., 2022c) has shown the potential of jointly interpreting text and images in a shared semantic space, which further leads us to ask: can the instruction tuning be leveraged to improve the generalizability of Vision-Language pre-trained models on multi-modal and vision tasks?\\n\\nIn this work, we propose MULTI INSTRUCT, the first benchmark dataset for multimodal instruction tuning with 62 diverse tasks from 10 broad categories, including Visual Question Answering (Goyal et al., 2017; Suhr et al., 2017), Commonsense Reasoning (Zellers et al., 2019; Xie et al., 2019), Visual Relationship Understanding (Krishna et al., 2017) and so on. We equipped each task with 5 instructions that are written by two experts in natural language processing. As shown in Figure 1, we formulate all the tasks into a unified sequence-to-sequence format in which the input text, images, instructions, and bounding boxes are represented in the same token space.\\n\\nWe use OFA (Wang et al., 2022a)2, a unified model that is pre-trained on a diverse set of multi-modal and unimodal tasks in a single Transformer-based sequence-to-sequence framework, as the base pre-trained multimodal language model, and fine-tune it on MULTI INSTRUCT. To utilize NATURAL INSTRUCTIONS (Mishra et al., 2022), a large-scale text-only instruction tuning dataset, we further explore two transfer learning strategies, in...\"}"}
{"id": "acl-2023-long-641", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Example Instances from MUltIINSTRUCT for Four Tasks. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks with instruction tuning and the potential of further improving it by leveraging large-scale text-only instruction datasets.\\n\\nAs suggested by previous studies (Webson and Pavlick, 2022; Liu et al., 2022b), PLMs are highly sensitive toward the wording and length of instructions. Thus, we propose a new metric \u2013 Sensitivity, which measures how sensitive the model is toward the variety of instructions for the same task. Experimental results demonstrate that (1) instruction tuning significantly reduces the sensitivity of OFA to the varying wording of instructions. The more tuning tasks and instructions for each task are introduced, the lower sensitivity tends to be achieved, and (2) transferring from a larger text-only instruction dataset can also significantly reduce the sensitivity of OFA.\\n\\n2 Related Work\\nMultimodal Pretraining\\nMultimodal pretraining (Tan and Bansal, 2019; Cho et al., 2021; Singh et al., 2022; Alayrac et al., 2022; Wang et al., 2022a; Li et al., 2022b,a) has significantly advanced the vision-language tasks. Several recent studies (Cho et al., 2021; Wang et al., 2022a,c; Lu et al., 2022) also started to build a unified pre-training framework to handle a diverse set of cross-modal and unimodal tasks. Among them, VLT5 (Cho et al., 2021) tackles vision-and-language tasks with a unified text-generation objective conditioned on multimodal inputs, while OFA (Wang et al., 2022a) further extends it to image generation tasks by using a unified vocabulary for all text and visual tokens. BEIT-3 (Wang et al., 2022c) utilizes a novel shared Multiway Transformer network with a shared self-attention module to align different modalities and provide deep fusion. Building on the success of multimodal pretraining, our work focuses on improving the generalization and zero-shot performance on various unseen multimodal tasks through instruction tuning.\\n\\nEfficient Language Model Tuning\\nTo improve the generalizability and adaptivity of large-scale pre-trained language models, various efficient language model tuning strategies have been proposed recently. Prompt tuning (Liu et al., 2021; Li and Liang, 2021; Han et al., 2022; Wang et al., 2022b; Sanh et al., 2022) aims to learn a task-specific prompt by reformulating the downstream tasks to the format that the model was initially trained on and has shown competitive performance across various natural language processing applications. As a special form of prompt tuning, in-context learning (Xie et al., 2021; Min et al., 2021) takes one or a few examples as the prompt to demonstrate the task. Instruction tuning (Wei et al., 2021) is another simple yet effective strategy to improve the generalizability of large language models. NATURALINSTRUCTIONS (Mishra et al., 2022) is a meta-dataset containing diverse tasks with human-authored definitions, things to avoid, and demonstrations. It has shown effectiveness in improving the generalizability of language models even when the size is relatively small (e.g., BART_base) (Mishra et al., 2022).\"}"}
{"id": "acl-2023-long-641", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2022; Wang et al., 2022d). InstructDial (Gupta et al., 2022) applies instruction tuning to the dialogue domain and shows significant zero-shot performance on unseen dialogue tasks. While these studies have been successful in text-only domains, it has not yet been extensively explored for vision or multimodal tasks.\\n\\n### 3.1 Multimodal Task and Data Collection\\n\\nThe **MULTINSTRUCT** dataset is designed to cover a wide range of multimodal tasks that require reasoning among regions, images, and text. These tasks are meant to teach machine learning models to perform various tasks such as object recognition, visual relationship understanding, text-image grounding, and so on by following instructions so that they can perform zero-shot prediction on unseen tasks. To build **MULTINSTRUCT**, we first collect 34 tasks from the existing studies in visual and multimodal learning, covering Visual Question Answering (Goyal et al., 2017; Krishna et al., 2017; Zhu et al., 2016; Hudson and Manning, 2019; Singh et al., 2019; Marino et al., 2019), Commonsense Reasoning (Suhr et al., 2017; Liu et al., 2022a; Zellers et al., 2019; Xie et al., 2019), Region Understanding (Krishna et al., 2017), Image Understanding (Kafle and Kanan, 2017; Chiu et al., 2020), Grounded Generation (Krishna et al., 2017; Yu et al., 2016; Lin et al., 2014), Image-Text Matching (Lin et al., 2014; Goyal et al., 2017), Grounded Matching (Krishna et al., 2017; Veit et al., 2016; Yu et al., 2016), Visual Relationship (Krishna et al., 2017; Pham et al., 2021), Temporal Ordering tasks that are created from WikiHow, and Miscellaneous (Yao et al., 2022; Kiela et al., 2020; Das et al., 2017; Lin et al., 2014; Veit et al., 2016; Alam et al., 2022). Each of the 34 tasks can be found with one or multiple open-source datasets, which are incorporated into **MULTINSTRUCT**. Details of each task and their corresponding datasets are shown in Tables 7 to 9 in Appendix.\\n\\nFor each of these tasks, we further examine the possibility of deriving new tasks based on the input and output of the original task to augment the task repository. For example, Visual Grounding requires the model to generate a caption for a given region in the image. We derive two additional tasks from it: Grounded Caption Selection, which is a simpler task that requires the model to select the corresponding caption from multiple candidates for the given region, and Visual Grounding Selection, which requires the model to select the corresponding region from the provided candidate regions based on a given caption. Compared with Visual Grounding, these two new tasks require different skills based on distinct input and output information. In this way, we further derived 28 new tasks from the 34 existing tasks. We divide all 62 tasks into 10 broad categories as shown in Figure 2.\\n\\nFor the existing tasks, we use their available open-source datasets to create instances (i.e., input and output pairs) while for each new task, we create its instances by extracting the necessary information from instances of existing tasks or reformulating them. Each new task is created with 5,000 to 5M instances. We split the 62 tasks into training and evaluation based on the following criteria: (1) we take the tasks that are similar to the pre-training tasks of OFA (Wang et al., 2022a) for training; and (2) we select the challenging multimodal tasks that do not overlap with the training tasks for evaluation. Table 5 and Table 6 in Appendix A show the detailed statistics for the training and evaluation tasks in **MULTINSTRUCT** and Tables 7 to 9 show their corresponding datasets.\\n\\n### 3.2 Task Instruction Creation\\n\\nWe first provide a definition for \u201cinstruction\u201d used in **MULTINSTRUCT**. An instruction is defined with a template that describes how the task should be performed and contains an arbitrary number of placeholders, including `<TEXT>`, `<REGION>`, and `<OPTION>`, for the input information from the original task. For example, in the instruction of the Grounded Captioning task, \u201cGenerate a caption for `<REGION>`\u201d, `<REGION>` is the placeholder for region-specific information. Note that the placeholder `<OPTION>` is only used in classification tasks and for some tasks, the input may also include an image that is not included in the instruction and will be fed as a separate input to the model. Figure 1 provides several instruction examples for the tasks included in **MULTINSTRUCT**.\\n\\nTo produce high-quality instructions that accurately convey the intended tasks, we employ an iterative annotation process involving two expert annotators who have a thorough understanding of the task and the dataset.\\n\\nStep 1: each annotator first writes 2-3 instructions for each task by giving them the specific goals of the task. The instructions are then reviewed and refined by the second annotator. This process is iterative and involves multiple rounds of annotation and discussion to ensure that the instructions are clear and unambiguous. Once the instructions are finalized, they are used to create instances for training and evaluation.\\n\\n\"}"}
{"id": "acl-2023-long-641", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Task Groups Included in MULTINSTRUCT. The yellow boxes represent tasks used for evaluation, while the white boxes indicate tasks used for training.\\n\\nThis task, the format of input data, and 10 example instances randomly sampled from the dataset. The information about the dataset is obtained from the dataset's README file or the publication that introduced the dataset. For newly derived tasks, we provide annotators with task descriptions along with 10 constructed example instances.\\n\\nStep 2: to guarantee the quality of the instructions and that they effectively convey the intended tasks, we have each annotator review the instructions created by their peers, checking if they can clearly understand and identify the intended task by just reading the instruction. If any issues are identified, the reviewing annotator provides suggestions and works with the original annotator to revise the instructions.\\n\\nStep 3: to ensure the consistency and avoid conflicts or repetition among instructions from different annotators, we have both annotators review the sets of instructions together, identifying any discrepancies or inconsistencies. If any are found, the annotators collaborate to resolve them and create a final set of instructions that accurately and clearly describe the task. In this way, each task will be created with 5 high-quality instructions.\\n\\nStep 4: we repeat steps 1-3 to create 5 instructions for each of the training and evaluation tasks. Finally, both annotators review each task and its instructions and filter out the task that is not representative or overlaps with other tasks.\\n\\n3.3 Multimodal Instruction Formatting\\nTo unify the processing of various input/output data types, we follow the method from OFA (Wang et al., 2022a), which involves representing images, text, and bounding box coordinates as tokens in a unified vocabulary. Specifically, we apply byte-pair encoding (BPE) (Sennrich et al., 2016) to encode the text input. For the target image, we apply VQ-GAN (Esser et al., 2021) to generate discrete image tokens through image quantization. To represent regions or bounding boxes of an image, we discretize the four corner coordinates into location tokens such as \\\"<bin_242> <bin_180> <bin_736> <bin_475>\\\" where each location token \\\"<bin_NUM>\\\" represents a quantized coordinate obtained by dividing the image into 1,000 bins. This approach allows us to convert different types of input into a unified vocabulary.\\n\\nAll tasks in MULTINSTRUCT can then be formulated as natural language sequence-to-sequence generation problems, where the input includes: (1) an image (if there is no input image, a black picture is used as the input); and (2) an instruction where the placeholders such as <TEXT>, <REGION> or <OPTION> are filled with specific information of each input instance. Notably, for the <OPTION> of the instructions for classification tasks, we intro-\"}"}
{"id": "acl-2023-long-641", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"reduce two special tokens for this field: \\\"[Options]\\\" to mark the beginning of the option field and \\\"||||\\\" to delimit the given options. We concatenate all the options with \\\"||||\\\" in the option field and the model will directly generate one option from them. Figure 1 provides several examples of the formulated input and illustrates how the original data input is combined with the instruction in the MULTIINSTRUCT.\\n\\n4 Problem Setup and Models\\n\\n4.1 Problem Setup\\n\\nWe follow the same instruction tuning setting as the previous study (Wei et al., 2021) and mainly evaluate the zero-shot learning capabilities of the fine-tuned large language models. Specifically, given a pre-trained multimodal language model $M$, we aim to finetune it on a collection of instruction tasks $T$. Each task $t \\\\in T$ is associated with a number of training instances $D_t = \\\\{(I_t, x_t, y_t) \\\\in I_t \\\\times X_t \\\\times Y_t\\\\}^{N_j=1}$, where $x_t$ denotes the input text, image, region, and options if provided, $y_t$ denotes the output of each instance, and $I_t$ represents the set of five task instructions written by experts. The input information from $x_t$ will be used to fill in the placeholders in the instruction.\\n\\nWe use OFA (Wang et al., 2022a) as the pre-trained multimodal model due to its unified architecture and flexible input-output modalities. We finetune it on our MULTIINSTRUCT dataset to demonstrate the effectiveness of instruction tuning. Specifically, we use the transformer-based encoder of OFA to encode the instruction along with all necessary information and an optional image, and predict the output with the transformer-based decoder. Given that the training dataset contains many tasks, we mix all the training instances from these tasks and randomly shuffle them. For each instance, we also randomly sample an instruction template for each batch-based training. Note that, though some of the training tasks in MULTIINSTRUCT are similar to the pre-training tasks of OFA, we ensure that the evaluation tasks in MULTIINSTRUCT do not overlap with either the pre-training tasks in OFA nor the training tasks in MULTIINSTRUCT.\\n\\nTable 10 in Appendix lists the multimodal tasks and dataset used in OFA pre-training.\\n\\n4.2 Transfer Learning from NATURALINSTRUCTIONS\\n\\nWe notice that the scale of NATURALINSTRUCTIONS (Mishra et al., 2022) is significantly larger than MULTIINSTRUCT, indicating the potential of transferring the instruction learning capability from the larger set of natural language tasks to multimodal tasks. We take 832 English tasks in NATURALINSTRUCTIONS and explore several simple transfer-learning strategies:\\n\\n**Mixed Instruction Tuning (OFA MixedInstruct)**\\n\\nWe combine the instances of NATURALINSTRUCTIONS and MULTIINSTRUCT and randomly shuffle them before finetuning OFA with instructions. Note that, each task in NATURALINSTRUCTIONS is just associated with one instruction while for each instance from MULTIINSTRUCT, we always randomly sample one instruction from the five instructions for each instance of training.\\n\\n**Sequential Instruction Tuning (OFA SeqInstruct)**\\n\\nInspired by the Pre-Finetuning approach discussed in Aghajanyan et al. (2021), we propose a two-stage sequential instruction tuning strategy where we first fine-tune OFA on the NATURALINSTRUCTIONS dataset to encourage the model to follow instructions to perform language-only tasks, and then further fine-tune it on MULTIINSTRUCT to adapt the instruction learning capability to multimodal tasks. To maximize the effectiveness of the NATURALINSTRUCTIONS dataset, we use all instances in English-language tasks to tune the model in the first training stage.\\n\\n5 Experimental Setup\\n\\nEvaluation Metrics\\n\\nWe report the accuracy for classification tasks and ROUGE-L (Lin, 2004) for all generation tasks. For the region classification task, we compute the Intersection over Union (IoU) between the generated region and all regions in the options, select the option with the highest IoU as the prediction, and compute accuracy based on this prediction. If the predicted region has no intersection with any of the regions in the options, we treat this prediction as incorrect. For classification tasks where the answer is not a single-word binary classification, we also report ROUGE-L scores following Mishra et al. (2022), which treats all tasks as text generation problems. For each task, we conduct five experiments by evaluating the model using one of the five instructions in each experiment.\"}"}
{"id": "acl-2023-long-641", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"port the mean and maximum performance and the standard deviation of the performance across all five experiments. We also compute the aggregated performance for each model based on the mean of the model\u2019s performance on all multimodal and NLP unseen tasks. We use Rouge-L as the evaluation metric for most tasks and accuracy for tasks that only have accuracy as a metric.\\n\\nIn addition, as instruction tuning mainly relies on the instructions to guide the model to perform prediction on various unseen multimodal tasks, we further propose to evaluate how sensitive the model is to the variety of human-written instructions in the same task, which has not been discussed in previous instruction tuning studies but is necessary to understand the effectiveness of instruction tuning. We thus further design a new metric as follows:\\n\\nSensitivity refers to the model\u2019s capability of consistently producing the same results, regardless of slight variations in the wording of instructions, as long as the intended task remains the same. Specifically, for each task $t \\\\in T$, given its associated instances with task instructions: $D_t = \\\\{ (i_t, x_t, y_t) \\\\in I_t \\\\times X_t \\\\times Y_t \\\\}_{j=1}^N$, we formally define sensitivity as:\\n\\n$$\\nE_{t \\\\in T} \\\\left[ \\\\sigma_{i \\\\in I_t} \\\\left[ E_{(x, y) \\\\in D_t} [L(f_{\\\\theta}(i, x), y)] \\\\right] \\\\right] \\\\\\\\\\nE_{i \\\\in I_t} \\\\left[ E_{(x, y) \\\\in D_t} [L(f_{\\\\theta}(i, x), y)] \\\\right]\\n$$\\n\\nwhere $L$ denotes the evaluation metric such as accuracy or ROUGE-L, $f_{\\\\theta}(\\\\cdot)$ represents the multimodal instruction-tuned model. The standard deviation and mean of the model\u2019s performance across all instructions are denoted by $\\\\sigma_{i \\\\in I_t} \\\\cdot$ and $\\\\mu_{i \\\\in I_t} \\\\cdot$, respectively.\\n\\nEvaluation datasets\\nWe evaluate the models on nine unseen multimodal tasks: Text VQA (Singh et al., 2019), Grounded VQA (Zhu et al., 2016), Commonsense VQA (Zellers et al., 2019), Visual Entailment (Xie et al., 2019), Visual Spatial Reasoning (Liu et al., 2022a), Natural Language for Visual Reasoning (NLVR) (Suhr et al., 2017), Visual Text Extraction (Kiela et al., 2020), Visual Dialogue (Das et al., 2017), and Disaster Type Classification (Alam et al., 2022). These tasks belong to three task groups: Commonsense Reasoning, VQA, and Miscellaneous as shown in Figure 2. Tasks in the Commonsense Reasoning group have no overlap with any training task groups. Tasks in Miscellaneous do not share similarities with other tasks in the group. Although Text VQA and Grounded VQA belong to the VQA task group, they require additional skills such as extracting text from images or generating regions, making them fundamentally different from other tasks in VQA. In addition to multimodal tasks, we also evaluate the model on 20 NLP tasks collected from the test split of NATURAL INSTRUCTIONS.\\n\\nApproaches for Comparison\\nWe denote the OFA finetuned on MULTI Instruct as OFA MultiInstruct, and compare it with the original pre-trained OFA 5, OFA TaskName which is fine-tuned on MULTI Instruct but uses the task name instead of instruction to guide the model to make predictions, and several approaches that leverage the large-scale NATURAL INSTRUCTIONS dataset, including OFA NaturalInstruct which only fine-tunes OFA on NATURAL INSTRUCTIONS with instruction tuning, OFA MixedInstruct and OFA SeqInstruct that are specified in Section 4.2.\\n\\nMore details regarding the evaluation datasets, baseline approaches and training details can be found in Appendix B.\\n\\n6 Results and Discussion\\n6.1 Effectiveness of Instruction Tuning on MULTI Instruct\\nWe evaluate the zero-shot performance of various approaches on all the unseen evaluation tasks, as shown in Table 1 and 2. Our results indicate that OFA MultiInstruct significantly improves the model\u2019s zero-shot performance over the original pre-trained OFA model across all unseen tasks and metrics, demonstrating the effectiveness of multimodal instruction tuning on MULTI Instruct.\\n\\nAs seen in Table 2, OFA achieves extremely low (nearly zero) zero-shot performance on the Grounded VQA task, which requires the model to generate region-specific tokens in order to answer the question. By examining the generated results, we find that OFA, without instruction tuning, failed to follow the instruction and produce results that contain region tokens. However, by fine-tuning OFA on MULTI Instruct, the model is able to better interpret and follow the instructions to properly generate the expected output. Additionally, OFA MultiInstruct outperforms OFA TaskName on all unseen tasks, particularly on the Grounded VQA task, where OFA TaskName achieves nearly zero performance.\"}"}
{"id": "acl-2023-long-641", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Model | Commonsense VQA | Visual Entailment | Visual Spatial Reasoning | NLVR\\n--- | --- | --- | --- | ---\\nOFA | 17.93 \u00b1 4.30 | 14.97 \u00b1 0.73 | 0.40 \u00b1 0.29 | 49.99 \u00b1 10.99 | 41.86 \u00b1 5.29 | 54.99 \u00b1 22.21 | 35.29 \u00b1 3.35\\n\\nTable 1: Zero-shot Performance on Multimodal Commonsense Reasoning. The best performance is in bold.\"}"}
{"id": "acl-2023-long-641", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Effect of Different Number of Instructions. Performance of OFA MultiInstruct finetuned on different numbers of instructions.\\n\\n| Instructions | Aggregated Performance | Sensitivity |\\n|--------------|------------------------|-------------|\\n| 1            | 42.81                  | 24.62       |\\n| 5            | 47.82                  | 10.45       |\\n\\nAs we increase the number of task clusters, we observe an improvement in both the mean and maximum aggregated performance and a decrease in sensitivity, as shown in Figure 3. Note that low sensitivity indicates that the model can produce consistent results despite variations in the wording of instructions. These results suggest that increasing the number of task clusters improves the model's performance on unseen tasks and leads to more consistent outputs. The results also support the effectiveness of our proposed MULTIINSTRUCT dataset.\\n\\n6.4 Effect of Diverse Instructions on Instruction Tuning\\n\\nWe hypothesize that using a diverse set of instructions for each task during multimodal instruction tuning can improve the model's zero-shot performance on unseen tasks and reduce its sensitivity to variation in the instructions. To test this hypothesis, we train an OFA model on MULTIINSTRUCT with a single fixed instruction template per task and compare its performance with OFA finetuned on 5 different instructions. As shown in Table 3, OFA finetuned on 5 instructions achieves much higher aggregated performance on all evaluation tasks and shows lower sensitivity. These results demonstrate the effectiveness of increasing the diversity of instructions and suggest that future work could explore crowd-sourcing or automatic generation strategies to create even more diverse instructions for instruction tuning.\\n\\n6.5 Effect of Fine-tuning Strategies on Model Sensitivity\\n\\nIn Section 6.3 and 6.4, we have shown that the more tasks and instructions used for instruction tuning, the lower sensitivity the model will achieve toward the variations in instructions for each task. We further investigate the impact of fine-tuning and transfer learning strategies on model sensitivity. Figure 4 shows the averaged sensitivity of each model across all multimodal unseen tasks. The original OFA exhibits significantly higher sensitivity to variations in instructions compared to models fine-tuned on instruction datasets, indicating that multimodal instruction tuning significantly improves the model's capability on interpreting instructions, even with varying wordings. In addition, by transferring the large-scale NATURALINSTRUCTIONS dataset to MULTIINSTRUCT, sensitivity is also reduced by a large margin, highlighting the benefit of fine-tuning the model on a larger instruction dataset, regardless of different formats and modalities.\\n\\n7 Zero-Shot Performance on NLP Tasks\\n\\nSo far, our focus has been on evaluating the zero-shot performance of multimodal tasks. In this section, we investigate the effect of multimodal instruction tuning on the performance of text-only tasks. To do this, we evaluate all our approaches on 20 natural language processing (NLP) tasks from the default test split in NATURALINSTRUCTIONS.\\n\\nThe detailed task list can be found in Appendix B.2. As shown in Table 4, OFA MultiInstruct outperforms OFA, despite the instruction tuning dataset and the unseen dataset are in different modalities. This suggests that multimodal instruction tuning can help improve the zero-shot performance on NLP tasks. In addition, we observe that OFA NaturalInstruct achieves the best performance on NLP tasks and OFA MixedInstruct is more effective in preserving the zero-shot capability gained from NATURALINSTRUCTIONS on NLP tasks compared to other approaches.\\n\\nhttps://github.com/allenai/natural-instructions\"}"}
{"id": "acl-2023-long-641", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Zero-shot Performance on NLP tasks. The performance is reported in Rouge-L and the best performance is in bold. Based on the results in Tables 1, 2 and 4, we conclude that OFA MixedInstruct is able to achieve overall best aggregated performance on all multimodal and NLP tasks and shows much lower sensitivity towards variations in the wording of instructions, making it the most promising approach.\\n\\n8 Conclusion\\nWe present a new large-scale multi-modal instruction tuning benchmark dataset \u2013 MULTIINSTRUCT, which covers a wide variety of vision and multimodal tasks while each task is associated with multiple expert-written instructions. By finetuning OFA (Wang et al., 2022a), a recently state-of-the-art multimodal pre-trained language model, on MULTIINSTRUCT with instruction tuning, its zero-shot performance on various unseen multimodal tasks is significantly improved. We also explore several transfer learning techniques to leverage the much larger text-only NATURALINSTRUCTIONS dataset and demonstrate its benefit. Moreover, we design a new evaluation metric Sensitivity to assess the model's sensitivity towards the variations in the wording of instructions. Results show that the model becomes less sensitive to these variations after being fine-tuned on a variety of tasks and instructions.\\n\\nLimitations\\nLimitations of Data Collection\\nOur proposed dataset only targets English language tasks. Future work should explore multimodal instruction tuning in a more diverse language setting and augment our MULTIINSTRUCT with multi-multilingual tasks. In addition, our current dataset mainly focuses on vision-language tasks. Datasets from more diverse modalities should be considered such as audio (Panayotov et al., 2015; Gemmeke et al., 2017; You et al., 2022) and video (Soomro et al., 2012; Ionescu et al., 2014). While we have built a novel multimodal instruction dataset containing 62 tasks, the number of tasks and associated instructions remains limited. To address this, future research could consider utilizing crowd-sourcing or automatic generation and augmentation techniques to increase the variety of instructions available.\\n\\nLimitations of Experiments and Evaluation\\nOur work is the first to explore instruction tuning on multimodal tasks and shows improved performance compared to baseline methods. However, there is still room for improvement, specifically in utilizing text-only instruction datasets. Future research could explore alternative architectures and stronger vision-language pre-trained models, or develop additional training loss functions to better utilize these unimodal instruction datasets. Additionally, we only used OFA as the baseline model as it was the largest open-source multimodal pre-trained model available when we conducted this research. As more and stronger multimodal pre-trained models being publicly available, it would be interesting to conduct a thorough comparison between models with different sizes. Finally, we take the first step to define sensitivity as a metric to evaluate the robustness of the models on understanding and following human-written instructions, which can be a potential standard metric for all the following instruction-tuning studies. However, it's only based on the variation of model performance across different instructions for the same task. In the future, we will consider more broad factors, e.g., the model's capability to understand different instructions for different tasks (Inter-task sensitivity), to further improve the sensitivity metric for instruction tuning.\\n\\nAcknowledgments\\nThis research is based upon work supported by the U.S. DARPA KMASS Program # HR001121S0034. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.\"}"}
{"id": "acl-2023-long-641", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive multi-task representations with pre-finetuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5799\u20135811, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nFiroj Alam, Tanvirul Alam, Md Hasan, Abul Hasnat, Muhammad Imran, Ferda Ofli, et al. 2022. Medic: a multi-task learning dataset for disaster image classification. Neural Computing and Applications, pages 1\u201324.\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198.\\n\\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. 2022. Beit: Bert pre-training of image transformers. In ICLR 2022.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nTai-Yin Chiu, Yinan Zhao, and Danna Gurari. 2020. Assessing image quality issues for real-world problems. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3646\u20133656.\\n\\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying vision-and-language tasks via text generation. In International Conference on Machine Learning, pages 1931\u20131942. PMLR.\\n\\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 326\u2013335.\\n\\nPatrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883.\\n\\nJort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2017, New Orleans, LA, USA, March 5-9, 2017, pages 776\u2013780. IEEE.\\n\\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913.\\n\\nPrakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey P. Bigham. 2022. Improving zero and few-shot generalization in dialogue through instruction tuning.\\n\\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2022. Ptr: Prompt tuning with rules for text classification. AI Open.\\n\\nDrew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6700\u20136709.\\n\\nCatalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. 2014. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE Trans. Pattern Anal. Mach. Intell., 36(7):1325\u20131339.\\n\\nKushal Kafle and Christopher Kanan. 2017. An analysis of visual question answering algorithms. In Proceedings of the IEEE international conference on computer vision, pages 1965\u20131973.\\n\\nDouwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in Neural Information Processing Systems, 33:2611\u20132624.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373.\\n\\nHao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, and Jifeng Dai. 2022a. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. CoRR, abs/2211.09808.\\n\\nLinjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. 2022b. Lavender: Unifying video-language understanding as masked language modeling. arXiv preprint arXiv:2206.07160.\\n\\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1144\u20131154.\"}"}
{"id": "acl-2023-long-641", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer.\\n\\nFangyu Liu, Guy Emerson, and Nigel Collier. 2022a. Visual spatial reasoning. arXiv preprint arXiv:2205.00363.\\n\\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022b. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. CoRR, abs/2205.05638.\\n\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.\\n\\nJiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. 2022. Unified-ino: A unified model for vision, language, and multi-modal tasks.\\n\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204.\\n\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943.\\n\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487, Dublin, Ireland. Association for Computational Linguistics.\\n\\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015, pages 5206\u20135210. IEEE.\\n\\nKhoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, and Abhinav Shrivastava. 2021. Learning to predict visual attributes in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13018\u201313028.\\n\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. Multi-task prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725.\\n\\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15638\u201315650.\\n\\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326.\\n\\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. UCF101: A dataset of 101 human actions classes from videos in the wild. CoRR, abs/1212.0402.\\n\\nAlane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 217\u2013223.\\n\\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490.\\n\\nAndreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. 2016. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140.\\n\\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022a. Unifying architectures.\"}"}
{"id": "acl-2023-long-641", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5 shows the distribution of input and output modalities for both training and evaluation tasks in MULTINSTRUCT, and Table 6 shows the detailed statistics for all the training and evaluation tasks separately. Tables 7 to 9 provide a comprehensive list of the 62 tasks included in MULTINSTRUCT, along with one example of instruction for each task.\\n\\n| Input Modality | Output Modality | # of Training | # of Testing |\\n|----------------|-----------------|---------------|--------------|\\n| Region         | Image           | \u2713             | \u2713            |\\n| Text           |                 | \u2713             |              |\\n| Image          | Text            | \u2713             | \u2713            |\\n| Region         |                 | \u2713             |              |\\n| Text           |                 | \u2713             |              |\\n\\nTable 5: Distribution of input and output modalities for all the tasks in MULTINSTRUCT.\\n\\n| Average # of Tokens per Instruction | 14.67 |\\n|-------------------------------------|-------|\\n| Averaged # of Character per Instruction | 85.78 |\\n| Average Levenshtein Distance of Instructions | 63.63 |\\n| # of Instructions per Task | 5     |\\n| # of Classification Tasks | 21    |\\n| # of Generation Tasks | 19    |\\n| # of Existing Tasks | 19    |\\n| # of Created Datasets | 21    |\\n\\nTable 6: Detailed statistics in MULTINSTRUCT.\\n\\nB More Details for Experimental Setup\\n\\nB.1 Multimodal Evaluation Datasets\\n\\nText VQA (Singh et al., 2019) requires models to read and reason about the text in an image to answer questions based on them.\"}"}
{"id": "acl-2023-long-641", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Grounded VQA (Zhu et al., 2016) requires models to answer the questions about an image, with the answers being specific visual regions within the image.\\n\\nCommonsense VQA (Zellers et al., 2019) requires the model to answer a multiple-choice question that requires commonsense reasoning about an image. Both the question and answers are presented in a combination of natural language and references to specific image regions within the image.\\n\\nVisual Entailment (Xie et al., 2019) requires the model to determine whether the image semantically entails the text.\\n\\nNatural Language for Visual Reasoning (NLVR) (Suhr et al., 2017) requires the model to answer a question that requires visual and set-theoretic reasoning on a synthetic image.\\n\\nVisual Text Extraction is a new task derived from Hateful Memes (Kiela et al., 2020) dataset. This task requires the model to extract the text that appears in the image.\\n\\nVisual Dialogue (Das et al., 2017) requires the model to answer a question given an image and a dialogue history.\\n\\nDisaster Type Classification (Alam et al., 2022) requires the model to determine the disaster type based on the image.\\n\\nB.2 NLP Evaluation Tasks\\n\\nBelow are the task names of the 20 NLP tasks that we used to test the zero-shot performance of all the methods. The 20 NLP tasks are from the default test split of the NATURAL INSTRUCTIONS dataset.\\n\\nDuring testing, we leverage the 'Definition' of the task as an instruction and prepend it with each input.\\n\\n- task1624_disfl_qa_question_yesno_classification\\n- task133_winowhy_reason_plausibility_detection\\n- task569_recipe_nlg_text_generation\\n- task1631_openpi_answer_generation\\n- task957_e2e_nlg_text_generation_generate\\n- task1386_anli_r2_entailment\\n- task393_plausible_result_generation\\n- task670_ambigqa_question_generation\\n- task890_gcwd_classification\\n- task1534_daily_dialog_question_classification\\n- task1388_cb_entailment\\n- task190_snli_classification\\n- task1533_daily_dialog_formal_classification\\n- task1598_nyc_long_text_generation\\n- task199_mnli_classification\\n- task1439_doqa_cooking_isanswerable\\n- task1409_dart_text_generation\\n- task1529_scitail1.1_classification\\n- task648_answer_generation\\n- task050_multirc_answerability\\n\\nB.3 Approaches for Comparison\\n\\nOFA (Wang et al., 2022a) denotes the original pre-trained OFA model without any fine-tuning. Here, we use OFA-large which contains 472M parameters and was trained on 8 tasks shown in Table 10. As reported in Wang et al. (2022a), OFA has demonstrated certain zero-shot capability on unseen multimodal tasks.\\n\\nOFA TaskName is finetuned on MULTIINSTRUCT but it does not use the instructions we created for the tasks. Instead, we prepend the task name to each input and use a semicolon to separate the task name and the input. For a fair comparison, we still keep the two special tokens \\\"[Options]\\\\ \\\" and \\\"|||\\\" for the option field.\\n\\nOFA MultiInstruct only fine-tunes OFA on our newly introduced MULTIINSTRUCT dataset with instruction tuning.\\n\\nOFA NaturalInstruct only fine-tunes OFA on the large-scale NATURAL INSTRUCTIONS dataset (Mishra et al., 2022; Wang et al., 2022d) with instruction tuning. To ensure a fair comparison, we evaluate this baseline on instruction templates that removed all specific tokens, including \\\"[Options]\\\\ \\\" and \\\"|||\\\", since the model being tested has not been exposed to these specific tokens during instruction-tuning. We want to ensure that the evaluation is not biased in favor of models that have seen these tokens during training.\\n\\nOFA MixedInstruct fine-tunes OFA on the mix of the large-scale NATURAL INSTRUCTIONS (Mishra et al., 2022; Wang et al., 2022d) and MULTIINSTRUCT dataset with instruction tuning.\\n\\nOFA SeqInstruct sequentially fine-tunes OFA on the large-scale NATURAL INSTRUCTIONS (Mishra et al., 2022; Wang et al., 2022d) with instruction tuning.\"}"}
{"id": "acl-2023-long-641", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.4 Training Details\\nWe set the maximum length of input tokens to 1024 and the maximum target length to 512. For image preprocessing, we strictly follow the process in the OFA. Please refer to the original paper for more details. We train the models on 8 Nvidia A100 GPUs with a batch size 8 per GPU, a learning rate of 1e-05, and float16 enabled for 3 epochs for all the setups and datasets. We run all the experiments once.\\n\\nC Attention Analysis\\nIn Section 6.1, we have demonstrated that fine-tuning OFA with NATURAL INSTRUCTIONS alone results in a decline in its zero-shot performance. In this section, we examine one possible reason for this decline by examining if fine-tuning the model on a text-only instruction dataset causes it to give less attention to image inputs.\\n\\nTo understand this, we conduct an analysis of the self-attention layers within the OFA encoder. The OFA encoder comprises 12 self-attention layers, each with 16 attention heads. We denote the input to self-attention layer $l$ as $h(l) = [x_1(l),...,x_p(l),...x_L(l)]$, where $L$ is the length of sequence. The input $h(0) = [x_1(0),...,x_I(0),x_I(0)+1,...x_I(0)+T]$ to the first self-attention layer is actually the concatenation of image embeddings and text embeddings, where $I$, $T$ is the length of image and text embeddings respectively. For ease of understanding and simplicity, we have altered the naming conventions and refer to $x_{l,p},p = [1,...,I]$ as image states and $x_{l,p},p = [I+1,...,I+T]$ as text states.\\n\\nFor each self-attention layer, we first compute the attention given to the image states in relation to text states for each attention head. Specifically, for each text state as the query, we sum its attention scores on image states (i.e. the attention scores where the text state is the query and image states are the keys). We then compute the text-to-image attention across all text states. Finally, we average the text-to-image across all attention heads. This results in a text-to-image attention score for each self-attention layer.\\n\\nFigure 5 illustrates the results of text-to-image attention scores on three unseen multimodal tasks: Text VQA, Visual Entailment, and Visual Text Extraction. The results on all three unseen tasks show that, in all self-attention layers of the OFA encoder, OFA_NaturalInstruct has significantly lower text-to-image attention scores compared to other models. This decrease is particularly pronounced in the first two self-attention layers. This suggests that fine-tuning the model on a text-only instruction dataset leads to a reduction in the attention paid to image inputs, which may explain the decline in zero-shot performance.\"}"}
{"id": "acl-2023-long-641", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: Text-to-Image Attention of OFA Encoder.\"}"}
{"id": "acl-2023-long-641", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category                  | Task Name                        | Dataset                                                                 | Description                                                                 | Exist |\\n|---------------------------|----------------------------------|-------------------------------------------------------------------------|----------------------------------------------------------------------------|-------|\\n| VQA                       | Open-Domain VQA                  | VQAv2 (Goyal et al., 2017), Visual Genome (Krishna et al., 2017)        | Answer the question based on the content of the given image.               | \u2713     |\\n|                           | Visual7w (Zhu et al., 2016)      |                                                                         | Answer a visual question by selecting an answer from given options.        | \u2713     |\\n|                           | Compositional VQA                | GQA (Hudson and Manning, 2019)                                         | Answer a compositional question based on the content of the given image.  | \u2713     |\\n|                           | Outside Knowledge VQA            | OK-VQA (Marino et al., 2019)                                           | Based on your knowledge, ?                                               | \u2713     |\\n| Grounded Generation       | Grounded Captioning              | Visual Genome (Krishna et al., 2017)                                   | Given the region in the image, generate a caption for that region.         | \u2713     |\\n|                           | Visual Grounding                 | Visual Genome (Krishna et al., 2017)                                   | Given a caption for some region in the image, identify the region and generate its bounding box. | \u2713     |\\n|                           | Grounded Object Identification   | MSCOCO (Lin et al., 2014)                                              | Identify the type of an object in the region.                             | \u2713     |\\n|                           | Object Grounding                 | MSCOCO (Lin et al., 2014)                                              | What are the regions containing the object?                               | \u2713     |\\n|                           | Referring Expression Grounding   | RefCOCO (Yu et al., 2016)                                              | Locate a region in an image based on the referring expression.            | \u2713     |\\n|                           | Referring Expression Generation  | RefCOCO (Yu et al., 2016)                                              | Generate the referring expression for an object in the region.            | \u2713     |\\n|                           | Text Localization                | COCO-Text (Veit et al., 2016)                                          | Select a region from options that contain the text in the image.          | \u2713     |\\n|                           | Most-Overlapping Region Selection| Visual Genome (Krishna et al., 2017)                                   | Given the region, decide which region in the options overlaps most with given region. | \u2713     |\\n|                           | Non-Overlapping Region Selection | Visual Genome (Krishna et al., 2017)                                   | Which option does not share common area with the given region?            | \u00d7     |\\n|                           | Least-Overlapping Region Selection| Visual Genome (Krishna et al., 2017)                                   | \u201cWhich option has the least shared area with the given region?\u201d           | \u00d7     |\\n|                           | Overlapping Region Selection     | Visual Genome (Krishna et al., 2017)                                   | Which region from options that has common area with the given region?    | \u00d7     |\\n|                           | Region Overlapping Detection     | Visual Genome (Krishna et al., 2017)                                   | Does the given region share common area with another region?             | \u00d7     |\\n|                           | Region Area                      | Visual Genome (Krishna et al., 2017)                                   | Compute the area of the given region.                                    | \u00d7     |\\n|                           | Grounded Matching                | Region-Caption Matching Visual Genome (Krishna et al., 2017)            | Decide if the caption matches the given region in the image.              | \u00d7     |\\n|                           | Grounded Caption Selection       | Visual Genome (Krishna et al., 2017)                                   | Given a region in the image, select a caption from given options for that region. | \u00d7     |\\n|                           | Visual Grounding Selection       | Visual Genome (Krishna et al., 2017)                                   | Given a caption for some region in the image, select the region from the options. | \u00d7     |\\n|                           | Referring Expression Selection   | RefCOCO (Yu et al., 2016)                                              | Select a region from options based on the referring expression.          | \u00d7     |\\n|                           | Object-Region Matching           | MSCOCO (Lin et al., 2014)                                              | Does region contain the object?                                          | \u00d7     |\\n|                           | Object-Region Selection          | MSCOCO (Lin et al., 2014)                                              | Select the region containing the given object.                           | \u00d7     |\\n|                           | Object Matching                  | MSCOCO (Lin et al., 2014)                                              | Do objects in region and region have the same type?                      | \u00d7     |\\n|                           | Missing Object Selection         | MSCOCO (Lin et al., 2014)                                              | Select an object from options that does not appear in any of the given regions. | \u00d7     |\\n|                           | Region-Text Matching             | COCO-Text (Veit et al., 2016)                                          | Does region contain the text?                                            | \u00d7     |\"}"}
{"id": "acl-2023-long-641", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category          | Task Name                  | Dataset          | Description                                                                 | Exist |\\n|-------------------|----------------------------|------------------|----------------------------------------------------------------------------|-------|\\n| Image Understanding | Color Recognition          | TDIUC            | Answer the question based on the color of an object.                      | \u2713     |\\n|                   | Object Detection            | TDIUC            | This task asks you to identify if an object appears in the image.          | \u2713     |\\n|                   | Object Recognition          | TDIUC            | In this task you are asked a question about the type of an object in the image. | \u2713     |\\n|                   | Scene Recognition           | TDIUC            | Look at the environment in the image and answer the question accordingly.  | \u2713     |\\n|                   | Counting                    | TDIUC            | Question: Please answer the question by counting the object mentioned in the question. | \u2713     |\\n|                   | Sentiment Understanding      | TDIUC            | Question: Please answer the question by interpreting the sentiment in the image. | \u2713     |\\n|                   | Position Reasoning          | TDIUC            | In this task, you need to analyze the position of objects in an image and answer the following question. | \u2713     |\\n|                   | Utility Affordance           | TDIUC            | Please take a look at the picture and answer the following question by thinking about what each object in the picture can be used for. | \u2713     |\\n|                   | Sport Understanding          | TDIUC            | There are some sports taking place in the image.                           | \u2717     |\\n| Image Quality     | IQA                         | IQA              | Select a reason from the options to explain why the image quality is bad.   | \u2713     |\\n| Visual Relationship | Object Relationship       | Visual Genome    | What is the relationship between the subject in region and object in region? | \u2713     |\\n|                   | Visual Object Identification | Visual Genome    | Given the subject in region, what is the object that has a relationship with that subject? | \u2717     |\\n|                   | Visual Subject Identification | Visual Genome    | Given the object in region, what is the subject that has a relationship with that object? | \u2717     |\\n|                   | Visual Object Localization  | Visual Genome    | Given the subject in region, where is the object in the image that has relationship with the subject? | \u2717     |\\n|                   | Visual Subject Localization | Visual Genome    | Given the object in region, where is the subject in the image that has relationship with the object? | \u2717     |\\n| Grounded Image Attribute | Identification | VAW              | Decide which option is the attribute of the object in the region.           | \u2713     |\\n| Image-Text Matching | Matching                  | MSCOCO           | Decide if the text matches the image.                                      | \u2717     |\\n| Question-Image Matching |                      | VQAv2            | Decide if the image contains an answer to the question.                     | \u2717     |\\n| Image-Text Selection |                           | MSCOCO           | Select the text that best matches the image.                               | \u2717     |\\n| Miscellaneous     | Multimodal Factual Checking | MOCHEG           | Decide if the claim can be supported by the given image and the context.    | \u2713     |\\n| Text Legibility   |                             | COCO-Text        | Decide if the text in the given region is legible.                         | \u2713     |\\n| Text Type Classification |                         | COCO-Text        | Read the text in the given region and determine the type of text from options. | \u2713     |\\n| Image Captioning  |                             | MSCOCO           | Generate a sentence to describe the content of the image.                  | \u2713     |\\n| Temporal Ordering | Wikihow Next Step Generation | WikiHow         | For task, given the history steps and the current step with its corresponding image, what is the next step for this task? | \u2717     |\\n|                   | Wikihow Next Step Selection | WikiHow         | For task, select the immediate next step to the step specified by the image. | \u2717     |\\n|                   | Wikihow Text-Image Temporal Ordering | WikiHow | For the task, given the current step, decide if the content of the image is the next or previous step. | \u2717     |\\n|                   | Wikihow Image-Text Temporal Ordering | WikiHow | For the task, given the current step specified by the image, decide if the step is the next or previous step. | \u2717     |\\n\\nTable 8: (Continued) Detailed Group of Training Tasks Included in MULTINSTRUCT. The complete list of 53 multi-modal tasks, along with examples of the instructions for each task. The existing tasks are indicated with \u2713, while the newly derived tasks are indicated using \u00d7.\"}"}
{"id": "acl-2023-long-641", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category                        | Task Name                          | Dataset                                | Description                                                                 | Exist |\\n|--------------------------------|------------------------------------|----------------------------------------|-----------------------------------------------------------------------------|-------|\\n| **VQA**                        |                                    |                                        |                                                                             |       |\\n|                                | Text VQA                           | Text VQA (Singh et al., 2019)          | There is some text on the image. Answer based on the text in the image.    | \u2713     |\\n|                                |                                    |                                        |                                                                             |       |\\n|                                | Grounded VQA                       | Visual7W (Zhu et al., 2016)            | Which region is the answer to <QUESTION>?                                    | \u2713     |\\n|                                |                                    |                                        | <OPTION>                                                                   |       |\\n|                                |                                    |                                        |                                                                             |       |\\n|                                | Commonsense Reasoning              | Natural Language for Visual Reasoning  | NLVR (Suhr et al., 2017) Decide if the sentence <TEXT> correctly describes the geometric relationships of objects in a synthesized image. | \u2713     |\\n|                                |                                    |                                        |                                                                             |       |\\n|                                | Visual Spatial Reasoning           | VSR (Liu et al., 2022a)                | Decide if the proposed spatial relationship between two objects in an image is \u201cTrue\u201d or \u201cFalse\u201d | \u2713     |\\n|                                |                                    |                                        |                                                                             |       |\\n|                                | Visual entailment                  | SNLI-VE (Xie et al., 2019)             | Can you conclude <TEXT> from the content of image? Select your answer from the options. | \u2713     |\\n|                                |                                    |                                        |                                                                             |       |\\n|                                | Commonsense Visual Question Answering | VCR (Zellers et al., 2019)             | Look at the image and the regions in the question, <QUESTION>?               | \u2713     |\\n|                                |                                    |                                        | <OPTION>                                                                   |       |\\n|                                |                                    |                                        |                                                                             |       |\\n|                                | Miscellaneous                      | Visual Text Extraction                | Hateful Memes (Kiela et al., 2020) What is the text written on the image?   | \u00d7     |\\n|                                |                                    |                                        |                                                                             |       |\\n|                                | Visual Dialogue                    | Visual Dialogue (Das et al., 2017)     | Given the image and the dialog history below: <HISTORY> <QUESTION>?           | \u2713     |\\n|                                |                                    |                                        |                                                                             |       |\\n|                                | Disaster Type                      | MEDIC (Alam et al., 2022)              | What disaster happens in the image? <OPTION>                                | \u2713     |\\n\\nTable 9: Detailed Group of Evaluation Tasks Included in MULTINSTRUCT. The complete list of 9 multi-modal tasks, along with examples of the instructions for each task. The existing tasks are indicated with \u2713, while the newly derived tasks are indicated using \u00d7.\"}"}
{"id": "acl-2023-long-641", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset Name                        | Task Name          |\\n|------------------------------------|--------------------|\\n| Conceptual Caption 12M (CC12M)     | Image Captioning   |\\n| Conceptual Captions (CC3M)         | Image Captioning   |\\n| MSCOCO image captions (COCO)      | Image Captioning   |\\n| Visual Genome Captions (VG Captions)| Image Captioning   |\\n| VQAv2                              | Visual Question Answering |\\n| VG-QA (COCO)                       | Visual Question Answering |\\n| GQA (VG)                           | Visual Question Answering |\\n| RefCOCO                            | Visual Grounding    |\\n| RefCOCO+                           | Visual Grounding    |\\n| RefCOCOg                           | Visual Grounding    |\\n| VG captions                        | Visual Grounded Captioning |\\n| OpenImages                         | Object Detection    |\\n| Object365                          | Object Detection    |\\n| VG                                  | Object Detection    |\\n| COCO                                | Object Detection    |\\n| OpenImages                         | Image Infilling     |\\n| YFCC100M                           | Image Infilling     |\\n| ImageNet-21K                       | Image Infilling     |\\n\\nTable 10: Multimodal Pre-training Tasks in OFA.\"}"}
{"id": "acl-2023-long-641", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A\\n\u25a1 A1. Did you describe the limitations of your work? section 9\\n\u25a1 A2. Did you discuss any potential risks of your work? Not applicable. Left blank.\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims? abstract, section 1\\n\u25a1 A4. Have you used AI writing assistants when working on this paper? Left blank.\\n\\nB\\n\u25a1 B. Did you use or create scientific artifacts? Left blank.\\n\u25a1 B1. Did you cite the creators of artifacts you used? No response.\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts? No response.\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? No response.\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/ anonymize it? No response.\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. No response.\\n\\nC\\n\u25a1 C. Did you run computational experiments? section 5, 6, 7, appendix B\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? appendix B.4\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-641", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\"}"}
