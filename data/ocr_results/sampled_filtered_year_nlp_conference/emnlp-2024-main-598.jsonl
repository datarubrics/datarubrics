{"id": "emnlp-2024-main-598", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nLarge Language Models (LLMs) frequently memorize long sequences verbatim, often with serious legal and privacy implications. Much prior work has studied such verbatim memorization using observational data. To complement such work, we develop a framework to study verbatim memorization in a controlled setting by continuing pre-training from Pythia checkpoints with injected sequences. We find that (1) non-trivial amounts of repetition are necessary for verbatim memorization to happen; (2) later (and presumably better) checkpoints are more likely to verbatim memorize sequences, even for out-of-distribution sequences; (3) the generation of memorized sequences is triggered by distributed model states that encode high-level features and makes important use of general language modeling capabilities. Guided by these insights, we develop stress tests to evaluate unlearning methods and find they often fail to remove the verbatim memorized information, while also degrading the LM. Overall, these findings challenge the hypothesis that verbatim memorization stems from specific model weights or mechanisms. Rather, verbatim memorization is intertwined with the LM\u2019s general capabilities and thus will be very difficult to isolate and suppress without degrading model quality.\\n\\n1 Introduction\\nVerbatim memorization refers to LLMs outputting long sequences of texts that are exact matches of training examples (Carlini et al., 2021, 2023). Unlike recalling factual knowledge or fixed expressions, verbatim memorization can have serious copyright and privacy implications (Karamolegkou et al., 2023; Chen et al., 2024c; Lee et al., 2023; Carlini et al., 2021; Shokri et al., 2017) and potentially waste model capacity (Nasr et al., 2023). Recent work has identified data frequency and model size as factors contributing to verbatim memorization in LLMs (Carlini et al., 2023; Prashanth et al., 2024; Karamolegkou et al., 2023). However, it is still not well understood why and how LLMs verbatim memorize certain texts in training data.\\n\\nOne hypothesis is that there are specialized model weights or mechanisms dedicated to recalling the verbatim memorized texts (Nasr et al., 2023; Chang et al., 2024b; Stoehr et al., 2024). Under this view, preventing verbatim memorization should be straightforward. For example, localizing and intervening on these dedicated components (e.g., a few neurons; Chang et al. 2024b; Maini et al. 2023 or a particular attention head; Stoehr et al. 2024) should remove verbatim memorized texts while preserving model quality. However, recent work indicates that removing verbatim memorized information is challenging. Preventing verbatim memorization during decoding does not stop variants of the memorized texts from being generated (Ippolito et al., 2023), and memorized texts can be retrieved in contexts different from the ones seen in training (Karamolegkou et al., 2023; Ippolito et al., 2023; Kassem et al., 2024). In addition, unlearning via fine-tuning and pruning degrades model quality.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"These findings suggest an alternative view: rather than having specialized weights or mechanisms dedicated to verbatim memorization, models might be reconstructing memorized sequences using features learned from general language modeling. This would explain why we are unable to localize memorized texts and why mitigating memorization can fundamentally alter model behaviors.\\n\\nIn this paper, we seek to answer these questions. We develop a framework for studying memorization in controlled settings: given an LM checkpoint $M$, we continue pre-training $M$ on the original training data but with specific novel sequences inserted at controlled frequencies. This framework complements existing observational methods, and allows us to decouple factors that potentially affect memorization, including model size, data frequency, and model quality. We use this framework in experiments with the Pythia family of models (Biderman et al., 2023b). Our core findings are as follows: (1) Sequences need to be repeated a non-trivial number of times to be memorized. The perception that a model verbatim memorizes a sequence that occurs once in pre-training is likely an illusion. (2) Later (and presumably better) check-points are more likely to verbatim memorize sequences, and even out-of-domain sequences are memorized at non-trivial rates by the best models. (3) Only some tokens in verbatim memorized sequences causally depend on a set of distributed triggering states that encode high-level semantic features, with the rest produced by regular LM decoding. Based on these findings, we develop stress tests to evaluate unlearning methods and find they often fail to remove verbatim memorized information while also degrading model quality.\\n\\nOverall, these results challenge the view that verbatim memorization stems from specific model weights or mechanisms. Instead, they suggest that verbatim memorization is the result of many interacting factors related to data and language modeling. Thus, removing verbatim memorized information without degrading model quality will be very difficult, especially for our best models.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3 A Framework for Studying Verbatim Memorization\\n\\nWe first introduce a framework to study the effects of language modeling quality on verbatim memorization in a tightly controlled setting. This framework adapts the data injection methods of Jagielski et al. (2023) and Carlini et al. (2019), and aims to create minimally different models with and without specific sequences injected into their training data.\\n\\n**Sequence injection**\\n\\nWe begin with a model checkpoint $M_i$. Let $O_i$ be the state of the optimizer at checkpoint $i$, and let $D_i$ be the final datapoint from the dataset $D$ that $M_i$ was trained on. Using the state $(M_i, O_i, D_i)$, we create two models. The control model $M(\u2205)$ continues training $M_i$ for $s$ steps using the data $D_i[\u2236i+s]$, with $O_i$ as the optimizer. For the treatment model $M(X)$, we minimally alter $D_i[\u2236i+s]$ to include a set of sequences $X$ that does not otherwise occur anywhere in $D$. Each sequence in this set is repeated uniformly every $m$ steps from a random offset, replacing the sequence at that point in $D_i$, until training step $i+s$.\\n\\nThe framework allows us to independently control three factors: the language model quality of $M$, the sequences $X$ to be memorized, and the frequency of the target sequence in the training data. Moreover, it creates approximate counterfactuals that allow us to observe what the model would be like if the model had not seen a particular sequence.\\n\\n**Optimizer state**\\n\\nTo simulate pre-training, we want an optimizer state that reflects the pre-training process prior to step $i$. Resetting the optimizer would lead to the first few batches having an undue large impact on the model loss. To achieve this, we first continue training the model $M_{i\u2212t}$ from the pre-training checkpoint at $i\u2212t$ over examples correspond to the next $t$ steps, using a freshly initialized optimizer. We then use the optimizer state of $M_{i\u2212t}$ as the optimizer state $O_i$.\\n\\n**Measuring verbatim memorization**\\n\\nWe adopt the $\\\\text{kl}$-extractable definition (Carlini et al., 2023) and define the verbatim memorization length of an injected sequence $x$ as the number of tokens in the longest memorized substring. We prompt the model with all substrings in $x$ of $k=8, 16, 32, 64$ tokens, where the first 8 tokens of the continuation in $x$ is not a substring in the prompt. For each prompt, we greedy decode the next 64 tokens as the prediction. Among all the predictions, we compute the longest prefix match between the prediction and the actual continuation in $x$ as the verbatim memorization length $l$.\\n\\n**4 Experiments**\\n\\nWe now report on a sequence of experiments aimed at helping to characterize the nature of verbatim memorization via the following four analyses.\\n\\n4.1 General Experimental Setup\\n\\n**Models**\\n\\nWe use checkpoints from the Pythia 160m, 2.8b, and 6.9b models (Biderman et al., 2023b) trained on the Pile (Gao et al., 2020) deduped data.\\n\\n**Injection sequences**\\n\\nWe curate a set of 100 sequences, each with 256 tokens, sampled from internet content published after the Pile cutoff date. We verify that the overlap between each sequence and the Pile is less than 50 characters (see Appendix B.2). Additionally, we create a set of 100 shuffled sequences by randomly shuffling tokens in each original sequence. The shuffled set preserves the overall vocabulary distribution but with little or no predictable structure.\\n\\n**Realistic injection frequencies**\\n\\nTo determine realistic frequencies, we study the frequency range that triggers memorization using 5K sequence samples. A sequence is considered memorized if it has a verbatim memorization length of 32 given a prefix of at most 32 tokens. We then hand-select a frequency where the 160m model produces a mix of the memorized and non-memorized sequences, which is about every 10K to 100K examples. We detail the sampling and counting procedure in Appendix A. Additionally, we observe that at the 6.9B scale, 94% of memorized sequences occur at least 1 in 5M examples, which raises the question of whether a model could memorize a sequence it has seen only once. We address this question in \u00a74.2, finding that purported instances are likely illusory.\\n\\n**Optimizer state**\\n\\nIn \u00a74.2, we use a freshly initialized AdamW optimizer (Loshchilov and Hutter, 2019). In \u00a74.3 and \u00a74.4, we initialize the optimizer by pre-training on 1M examples. Additional setup details are in Appendix B.\\n\\n4.2 The Illusion of Single-shot Verbatim Memorization\\n\\nDo LLMs verbatim memorize a sequence in pre-training after only seeing the sequence once?\\n\\nData and code available at [https://github.com/explanare/verbatim-memorization](https://github.com/explanare/verbatim-memorization)\"}"}
{"id": "emnlp-2024-main-598", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Four types of sequences that create the illusion of single-shot verbatim memorization.\\n\\nFigure 2: Single-shot verbatim memorization length of the 2.8b and 6.9b models after 200 training steps.\\n\\nWe first manually annotate the 6% low-frequency sequences verbatim memorized by the 6.9b model in \u00a74.1 and identify four patterns that create the illusion of single-shot verbatim memorization in Table 1, with examples shown in Appendix A.2. These seemingly low-frequency sequences are either under-counted due to limitations of string-based matching or simply not verbatim memorized, i.e., a checkpoint produced before the sequence occurs can already generate the sequence verbatim. Prashanth et al. (2024) has also identified similar sequences as \u201cReconstruction\u201d and \u201cRecollection\u201d. These patterns suggest not all tokens in the verbatim memorized sequences are actually memorized; some might be completed by the LM. One may argue that a memorized sequence that only occurs once in the training data is inherently hard to discover. To complement counting, we directly measure a model's ability to verbatim memorize a sequence after one occurrence.\\n\\nSetup\\nWe train the 2.8b and 6.9b 80K checkpoints for 200 steps, where a sequence to memorize is injected into the first batch. We measure the verbatim memorization length every 10 steps.\\n\\nResults\\nResults are shown in Figure 2, averaged over 16 injection sequences and their shuffled versions. The verbatim memorization length decreases significantly as the batch size increases. Moreover, the verbatim memorization length peaks around 25\u2013100 steps after seeing the injected sequence, likely due to momentum terms in the optimizer (Chang et al., 2024a). Even at the peak, the 6.9b model only verbatim memorizes 12 \u00b1 3.7 tokens from the original sequences at batch size 128. Shuffled sequences are memorized 5 \u00b1 1 tokens regardless of batch size or model size. With a batch size of 1024 in pre-training, it is extremely unlikely that models with a size smaller than 6.9B can just verbatim memorize an arbitrary sequence in a single-shot.\\n\\n4.3 Better LMs Memorize Longer Sequences\\nAre better LMs more likely to memorize sequences? Intuitively, better LMs are those that achieve lower perplexity on novel sequences drawn from their training distribution. From this perspective, we might expect them to be better at memorizing such sequences as well, since they simply require fewer bytes on average to encode such sequences (Deletang et al., 2024).\\n\\nSetup\\nTo decouple model quality from model size, we experiment with three checkpoints at 1K, 10K, and 80K steps from the 160m and 2.8b models. We use two injection frequencies for both models: every 50K and 10K examples. For each model run, we pack a set of 4\u201310 sequences, so that the total number of injected sequences is less than 0.04% of the training data. We measure the verbatim memorization after 40 and 20 occurrences for the two models respectively, as with 20 occurrences the 2.8b model can already memorize longer sequences than the 160m one.\\n\\nResults\\nFigure 3 (solid blue lines) shows the results for 1 in 50K frequency; results for 10K frequency are in Appendix C.1. The findings are clear: later checkpoints memorize more, and the larger model is able to memorize more, even when seeing...\"}"}
{"id": "emnlp-2024-main-598", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Overall, checkpoints corresponding to higher quality models are more likely to memorize the injected sequences.\\n\\n4.4 Sequences without Structure Are Harder to Memorize\\n\\nThe previous section shows that better models are more capable of memorizing sequences in their training distributions. What about sequences from a different distribution? One hypothesis is that out-of-domain sequences are more likely to be memorized, as they contain rare sequences of tokens that can be used as identifiers to recall the memorized content (Tirumala et al., 2022). The other hypothesis is that in-domain sequences are more likely to be memorized, because they have lower perplexity before training, as in the single-shot case in \u00a74.2.\\n\\n**Setup**\\n\\nTo investigate which hypothesis holds, we use the set of shuffled sequences, which naturally have a higher perplexity than the original sequences when measured using the model $M$, i.e., before training on the sequences. We follow the training and evaluation protocol from \u00a74.3.\\n\\n**Results**\\n\\nResults are shown in Figure 3 (dashed orange lines). On average, the original sequences drawn from the training distribution are more likely to be verbatim memorized than shuffled sequences, except for the $2.8b-10K$ checkpoint. Even though the shuffled sequences are not memorized as well, we do still see the trend in which later checkpoints memorize longer sequences. In terms of perplexity changes, the perplexity of memorized shuffled sequences does decrease faster during training than the perplexity of original sequences.\\n\\nThese findings suggest that the verbatim memorization observed in pre-trained LLMs is more complex than recalling a sequence based on some unique identifiers, as otherwise we would see the models memorize more shuffled sequences. Multiple factors might contribute to the process: general mechanisms to efficiently store free-form texts, as well as structures that favor in-domain sequences. The former might be learned from modeling random sequences such as URLs, UUID, or even digits of $\\\\pi$, while the later might emerge for modeling structures in natural languages. We investigate these mechanisms in the following sections.\\n\\n4.5 Memorization is Triggered by Abstract Model States Distributed Across Tokens\\n\\nA core question for verbatim memorization is how models encode memorized sequences. We consider two aspects of the question: (1) Which tokens encode the information of the verbatim memorized sequences? (2) Do models encode token-level information (low-level representations) or more abstract states of the model (high-level representations)?\\n\\nTo answer these questions, we seek to identify the causal connections between the sequence that triggers memorization and the tokens in the verbatim memorized sequence. In more detail, consider a treatment model $M(X)$ that takes as input a trigger prefix $p = x_1, \\\\ldots, x_n$ and outputs a verbatim memorized sequence $s = x_n+1, \\\\ldots, x_n+k$. From this it follows that the trigger prefix $p$ creates an internal state $S$ in $M(X)$ that is sufficient for generating $s$.\\n\\nIf verbatim memorized information is localized to the trigger $p$, then every token in $s$ should have a causal connection to the internal state $S$. To test whether every token in $s$ in fact depends on $S$, we use interchange interventions (also known as activation patching; Geiger et al. 2021; Vig et al. 2020), which is calculated as follows. First, let $\\\\text{GetVal}(M(x), l)$ be the representation $v$ that model $M$ computes at location $l$ when it processes input $x$. Second, let $M_l \\\\leftarrow w(x)$ be the model that is just like $M(x)$ except that the representations computed at location $l$ have been replaced by the values $w$. An interchange intervention is one in which the value used in this intervention is one created when the model processes a different input $x'$. This results in a nesting of $\\\\text{GetVal}$ inside the intervention: $M_l \\\\leftarrow \\\\text{GetVal}(M(x'), l)(x)$. In other words, the interchange intervention replaces the values computed at $l$ with those obtained from processing a different example.\\n\\nOur interchange intervention focuses on the decoding process given a trigger prefix. Suppose $M(X)$ has run a forward pass on $x = x_1, \\\\ldots, x_{n+1}, \\\\ldots, x_{n+t-1}$, i.e., the trigger $p$ and the subsequent $t-1$ tokens, and so is going to predict token $x_{n+t}$. Our interchange intervention replaces the residual stream representations in layer $k$ of a token $x_j$ (for $j \\\\leq n$) in $p$ with residual stream representations extracted at the same layer and token position where the model input is a random sequence $r$ sampled from the Pile: $M(X)_{j,k} \\\\leftarrow v(x)$ where $v = \\\\text{GetVal}(M(X)(r), (j,k))$.\\n\\nIf the next token prediction is causally dependent on the trigger representation, we expect the predicted token to change after this intervention, since...\"}"}
{"id": "emnlp-2024-main-598", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you.\\n\\nFigure 4: Causal dependencies between the trigger and verbatim memorized tokens. (a) An example of a memorized token that depends on the trigger (the yellow box). A darker color indicates that the output has a stronger causal dependency on the residual stream at the location. (b) An example of a memorized token that does not depend on the same trigger. (c) The percentage of memorized tokens that causally depend on the trigger decreases by step. (d) For memorized tokens that depend on the trigger, there is on average one causal dependency even at the middle layers.\\n\\nThe chance of a random sampled token having a similar representation as the intervened token is extremely low. Alternatively, if the representation has no causal effect, we expect the output to be the same. Interventions on model representations allow us to measure which tokens have causal effects (our question 1 above), and at which layers the token information was used by the model to decode the next memorized token (our question 2).\\n\\nMetrics\\n\\nLet $l$ be an intervention location, i.e., a residual stream at the token position $i$ and layer $\\\\ell$, and $L$ be the total number of layers. Let $p_l$ represent the percentage of interventions that lead the model to output the verbatim memorized token when intervened on at $l$. We estimate the causal effect of the trigger on the verbatim memorized token $x_{n+1}$ as a causal dependency score:\\n\\n$$d_t = 1 - \\\\max_{l \\\\in \\\\{(i,\\\\ell)|1 \\\\leq i \\\\leq n, 1 \\\\leq \\\\ell \\\\leq L\\\\}} \\\\{p_l\\\\}$$\\n\\nThe score is between 0 and 1, where 1 means strong causal dependency on the trigger and 0 means no causal dependency. By definition, $d_1 = 1$, since the last layer residual stream at the last token always has causal effects on the first predicted token.\\n\\nWe define the number of dependencies a memorized token $x_{n+1}$ has in a given layer $\\\\ell$ as $N_{t,\\\\ell}$:\\n\\n$$N_{t,\\\\ell} = \\\\sum_{l \\\\in \\\\{(i,\\\\ell)|1 \\\\leq i \\\\leq n\\\\}} [p_l > T]$$\\n\\nwhere $T$ is a threshold that we set to 0.1 to filter dependencies with weak causal effects.\\n\\nSetup\\n\\nWe analyze the models in \u00a74.3 trained from the 160m-80K checkpoint with an injection frequency of 1 in 10M examples. We sample 50 injected sequences (original and shuffled) and compute $p_l$ over 100 random sequences from the Pile.\\n\\nResults\\n\\nFigure 4 summarizes our results: (1) Not all tokens in the verbatim memorized sequence are causally dependent on the trigger representations, e.g., Figure 4b measured by $1 - p_l$ and Figure 4c measured by $d_t$. Instead, these tokens often exhibit dependencies that resemble syntactic structures, e.g., the direct object depends on the preceding verb and the closing parenthesis depend on the opening parenthesis. For sequences with no clear structure, memorized tokens depend on more trigger tokens that are relatively rare, a pattern observed in previous work (Tirumala et al., 2022; Stoehr et al., 2024).\\n\\n(2) Most memorized tokens depend on higher-level representations produced by middle layers. In Figure 4d, at layer 4, there still exists on average one dependency. We observe similar patterns in the 6.9b model (see Appendix C.2).\\n\\nOverall, these results show that information about the verbatim memorized sequence is (1) distributed across tokens and (2) encoded in abstract states as opposed to token-level features. There is simply no representation of the trigger $p$ that causally encodes the entire memorized sequence.\\n\\nMoreover, the fact that not all verbatim memorized tokens are causally dependent on the trigger suggests models might only memorize information about a subset of tokens, filling in the gaps with general language modeling. The verbatim memorized sequence might be reconstructed token-by-token, where each token is predicted using different mechanisms depending on the structures involved. This might explain why in-domain sequences are more likely to be memorized. In fact, the two mechanisms we observed \u2013 attending to syntactic structures and rare tokens \u2013 are identified in Transformers that have not seen or memorized a particular...\"}"}
{"id": "emnlp-2024-main-598", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sequence (Tian et al., 2023; Chen et al., 2024a). Lastly, models encode abstract states as opposed to\\ntoken-level information, which might explain why\\nmemorized sequences can be triggered in contexts\\nthat are different from those seen in training. We\\nfurther test this hypothesis in \u00a74.6.\\n\\n4.6 Verbatim Memorization Leverages\\nGeneral Language Modeling Capabilities\\n\\nThe results of \u00a74.3 and \u00a74.4 provide behavioral\\nevidence that memorization depends on general\\nlanguage capabilities. In this section, we extend\\nthe intervention-based methods of \u00a74.5 in an effort\\nto characterize this relationship in terms of the un-\\nderlying computation they share. The core analytic\\ntechnique is an interchange intervention that seeks\\nto get a control model $M(\\\\emptyset)$ to produce memorized\\nstrings by intervening with internal states from a\\nminimally different treatment model $M(X)$.\\n\\nOur core finding is that, while such interventions\\ndo not lead $M(\\\\emptyset)$ to produce entire memorized\\nsequences, we can get it to produce the first few\\ntokens of such sequences. Moreover, among the\\ninterventions at a layer that do produce memorized\\ntokens, more than 50% can still produce the same\\nmemorized tokens using model components at the\\nCorresponding layer from $M(\\\\emptyset)$, which are weights\\nlearned only from general language modeling.\\n\\nCross-model interchange interventions\\nWe pro-\\npose a novel intervention that replaces representa-\\ntions in $M(\\\\emptyset)$ with corresponding ones in $M(X)$:\\n\\n$$M(\\\\emptyset)_l \\\\leftarrow v(p)$$\\n\\nwhere $v = \\\\text{GetVal}(M(X), p, l)$ (4)\\n\\nSuppose $p$ is a trigger for memorized sequence $s$. If\\nthis intervention leads $M(\\\\emptyset)_l \\\\leftarrow v(p)$ to generate parts\\nof $s$, then we have evidence that the memorization\\nbehavior was guided in part by the representation\\nat $l$ and in part by the general structure of\\n$M(\\\\emptyset)$.\\n\\nIt may seem surprising to transfer representa-\\ntions between two models. However, the models\\nbegin from the same checkpoint and are trained\\non almost identical sequences. This weakly sug-\\ngests that their representations will be compatible.\\nIn addition, prior work has shown that even rep-\\nresentations from different families of models are\\ninterchangeable with some affine transformations\\n(Csisz\u00e1rik et al., 2021; Ghandeharioun et al., 2024).\\nWe also experimentally verify the coherence of\\nthese interventions in our results section below.\\n\\nAs in \u00a74.5, we explore intervention sites across\\nall layers, since we do not know a priori where\\nthe relevant information might be stored. For\\neach layer, we target both attention and MLP com-\\nponents, which have been identified as related\\nto memorization behaviors in Transformer-based\\nLMs (Geva et al., 2021; Dai et al., 2022; Geva\\net al., 2023; Stoehr et al., 2024; Allen-Zhu and\\nLi, 2024). We aim to understand to what extent\\nthese components reuse computations learned from\\ngeneral language modeling.\\n\\nMetrics\\nFor an intervention at location $l$, let $p_{l,n}$ be\\nthe percentage of examples where the first $n$ tokens\\npredicted by $M(\\\\emptyset)$ match the verbatim memorized\\ntokens generated by $M(X)$. We consider small\\nvalues of $n \\\\in \\\\{1, 2, 4\\\\}$; as we will see, by\\n$n = 4$, success rates have gone to effectively\\n0.\\n\\nFor each layer, we want to measure whether ver-\\nbatim memorization reuses computations learned\\nfrom general modeling, i.e, computations defined\\nby components in $M(\\\\emptyset)$. We compute $p_{l,n}$ at three\\nsets of intervention locations across all trigger to-\\nkens. We use MLP as an example in Figure 5.\\n\\n- $l\\\\text{none},i$: Attention output at layer $i$ + Residuals\\n  at layer $i-1$\\n- $l\\\\text{in},i$: $l\\\\text{none},i + \\\\text{MLP input at layer }i$\\n- $l\\\\text{out},i$: $l\\\\text{none},i + \\\\text{MLP output at layer }i$\\n  (i.e., Residuals at layer $i$)\\n\\nIn $l\\\\text{none},i$, the residual from the treatment model\\n$M(X)$ is not propagated into the MLP layer of\\nthe control model $M(\\\\emptyset)$. The MLP output is still\\ncomputed using the MLP input from\\n$M(\\\\emptyset)$. The lo-\\ncations for attention can be defined symmetrically.\\n\\nLet $R_{i,n}$ be the percentage of interventions that\\nlead to $M(\\\\emptyset)$ producing a memorized short prefix\\nof $n$ tokens using only MLP input from the treat-\\nment model $M(X)$, but not the MLP layer weights\\nfrom the treatment model $M(X)$:\\n\\n$$R_{i,n} = p_{l\\\\text{in},i,n} - p_{l\\\\text{none},i,n}$$\\n\\n$$p_{l\\\\text{out},i,n} - p_{l\\\\text{none},i,n}$$\\n\\n(5)\\n\\n$R_{i,n}$ is only meaningful when the denominator is\\n1.0\"}"}
{"id": "emnlp-2024-main-598", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Results of cross-model interventions. Dotted lines: There are interventions that can control $M(\\\\emptyset)$ to produce the next 1\u20132 memorized tokens, but not any longer. Solid lines: Among interventions that produce the next memorized token, more than 50% can still produce the same token using components of $M(\\\\emptyset)$. Sufficiently large, i.e., the layer has causal effects on the next $n$ verbatim memorized tokens. A higher $R_{i,n}$ value suggests that the MLP (or attention) component in $M(\\\\emptyset)$ plays a similar causal role on the next $n$ memorized tokens as the corresponding component in $M(X)$. In other words, a sign of leveraging general language modeling capabilities.\\n\\nSetup\\nWe use the 160m models in \u00a74.3 trained from the step 80K checkpoint, with treatment model data injected at a frequency of every 10K examples. We analyze 2,000 tokens predicted as part of 120 verbatim memorized sequences (including shuffled sequences), which covers about 1000 distinct tokens. About 25% of these verbatim memorized tokens can be correctly predicted by the control model as well. We exclude these from further analysis. However, these tokens suggest that a quarter of the verbatim memorized tokens result from general language modeling. For the remaining tokens, we compute $p_{l_{out}}$ and $R_{i,n}$.\\n\\nResults\\nResults are shown in Figure 6. We first look at the dotted lines: (1) When intervening on the last layer residuals, $p_{l_{out},11}=85\\\\%$, which validates our intervention setup \u2013 representations from the two models are indeed interchangeable for the majority of the inputs. (2) As $n$ increases, $p_{l_{out},i}$ drops to almost zero, which means interventions on individual model components have little to no causal effect on producing the memorized prefixes. This aligns with our findings in \u00a74.5: Memorized information is distributed across tokens.\\n\\nFor the solid lines, which are $R_{i,1}$ (a setting where a significant percentage of interventions can produce memorized tokens), we find that (1) the majority of attention and MLP layers have $R_{i,1}$ values above 50%, suggesting the $M(X)$ model is performing similar computations as $M(\\\\emptyset)$, which are computations learned from general language modeling. In fact, verbatim memorization can still happen with frozen attention heads, i.e., only using attention patterns learned from general language modeling (Appendix C.3). (2) There are a few layers where $R_{i,1}$ is around 30%, i.e., MLP components in layer 1 and 3 and attention in layer 1 and 4, suggesting these components are largely different between $M(\\\\emptyset)$ and $M(X)$ and likely store memorized information. Indeed, previous work that uses gradient-based approaches also indicates that lower layers play an important role in verbatim memorization (Stoehr et al., 2024). However, an $R_{i,1}$ around 30% means it is still challenging to fully isolate the memorized information, even just for predicting a single token.\\n\\nAnalysis\\nThe ability to leverage computations learned from general language modeling provides an explanation of why higher quality models verbatim memorize more sequences. This also suggests that verbatim memorization is fundamentally intertwined with language modeling capabilities, as the control and treatment models largely share both attention and MLP structures across multiple layers.\\n\\n5 Stress Testing on Unlearning Verbatim Memorized Texts\\nGiven the nature of the verbatim memorization discussed in \u00a74.5 and \u00a74.6, we propose a suite of automated stress tests to evaluate whether unlearning methods truly remove verbatim memorized information without systematically altering the LM.\\n\\n5.1 A Stress Testing Dataset\\nOur stress tests are built on two observations: (i) Memorized information is distributed across tokens, hence evaluation should include prompts that cover different spans of a memorized sequence (\u201cPosition Perturbations\u201d). (ii) Verbatim memorization is triggered by abstract model states, hence evaluations should cover semantically similar variations of the prompt trained on (\u201cSemantic Perturbations\u201d).\\n\\nConsider a trigger prompt of $n$ tokens $x_1...x_n$ with memorized continuation $x_{n+1}...x_{n+k}$ in the original training set (which is also the evaluation set in the unlearning setup). For \u201cPosition Perturbations\u201d, we generate two sets of perturbed prompts:\\n\\n\\\\[\\n\\\\{x_1...x_n+i | i \\\\in [0,t]\\\\} \\\\cup \\\\{x_n-i...x_n | i \\\\in [t,n)\\\\}\\n\\\\]\"}"}
{"id": "emnlp-2024-main-598", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The exact match length of model outputs with the original and stress testing prompts. On average, stress testing prompts can extract 10\u201315 more tokens.\\n\\nFor \\\"Semantic Perturbations\\\", we replace each word or a consecutive sequence of digits (or characters) in the prompt with a similar word. Example stress tests are in Appendix C.4.\\n\\n5.2 Evaluation\\n\\nWe evaluate the gradient ascent, sparse fine-tuning, and pruning methods of Stoehr et al. (2024) and Chang et al. (2024b). These methods have been shown to prevent models from generating verbatim memorized texts on the fine-tuned prompts, at the cost of increasing perplexity on other texts (Chang et al., 2024b; Stoehr et al., 2024).\\n\\nSetup\\n\\nWe follow the setup in Stoehr et al. (2024) (see Appendix B.6). Given a 50-token trigger prompt and a 50-token continuation memorized by the GPT-Neo 125M model, the goal is to unlearn the continuation while retaining model quality on other prompts. For each sequence, we generate \u22481K perturbed prompts with $t=20$ for Position Perturbations and use ChatGPT to generate around 10 similar word substitutions per word for Semantic Perturbations. For both original and stress test prompts, we report the longest continuation that matches the memorized sequence. For stress test prompts, the length is max-pooled over all prompts.\\n\\nResults\\n\\nTable 2 shows the results, with full length distributions shown in Appendix C.4. On average, the perturbed prompts increase the exact match length by 10\u201315 tokens. For gradient ascent and sparse fine-tuning, the stress tests increase the fully extractable sequences (i.e., exact match of 50 tokens) from 22% to 56%. The neuron pruning method is more robust to the stress tests. However, it often leads to degeneration on the perturbed prefixes, e.g., outputting repetitive texts. Overall, while these unlearning methods largely prevent models from generating the verbatim memorized sequence given a particular prefix, they do not completely remove the verbatim memorized information \u2013 the model can still generate the memorized texts when prompted with variants of the prefix.\\n\\n6 Discussion and Conclusion\\n\\nVerbatim memorization is a pressing issue for LM research, as it has ramifications for privacy, copyright, and other legal issues. Thus, one might hope that we will find ways to identify and control memorization. The present paper suggests that such control may be extremely difficult to achieve because verbatim memorization is thoroughly intertwined with general language modeling quality. For example, given current training procedures, LMs will memorize more strings as their quality improves. Strings that resemble those from the LM's training data are more likely to be memorized ($\\\\S4.3$), but even OOD strings (which may include private identifiers, usual metadata patterns, etc.) are memorized at non-trivial rates by our best models ($\\\\S4.4$).\\n\\nIn light of these findings, one might simply accept that LMs will memorize strings and try to mitigate memorization by blocking specific triggering strings. Unfortunately, this method is bound to have very low recall. As we showed in $\\\\S4.5$, the notion of a trigger is extremely complex. Overall, the trigger is actually a set of distributed model internal states that encode generalizable high-level features that numerous inputs can lead to. In $\\\\S4.6$, we deepened this result by showing that even a control model that has never seen a specific memorized input $x$ can be made to produce parts of $x$ via an intervention from a model that has memorized $x$.\\n\\nIn $\\\\S5$, we show the practical implications of these distributed, abstract triggering states on unlearning methods, which lead to failures in removing verbatim memorized information or degrading general model quality. These results all point to the idea that generating memorized strings is in part simply language model decoding as usual.\\n\\nMore broadly, these findings suggest that \\\"verbatim memorization\\\" is something of a misnomer, as the phenomenon involves memorization of more abstract model states as opposed to only memorization of token-level information. Thus, to be successful, future attempts to control memorization will likely require new techniques for characterizing and controlling these abstract model states. Such techniques are likely to greatly improve our understanding of LLMs in general.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nOur work contributes to understanding verbatim memorization behaviors in LLMs, an important problem that has practical implications and applies to almost all LLMs trained on large-scale web corpora. However, constrained by the availability of fully open sourced LLMs (i.e., LLMs with training dataset, checkpoints, and training hyperparameters fully available), we only conducted experiments on the Pythia family of models, focusing on model sizes up to $2.8b$. As more fully open source models come out, such as OLMo, we would like to see if our findings on Pythia models generalize to other model families.\\n\\nOne important finding of our paper is that verbatim memorization actually involves memorization of abstract model states as opposed to just token-level information. This raises the concern of whether focusing on verbatim memorization reveals the full scale of what models actually memorize. LLMs could memorize long sequences of abstract states as well, which might remain undetected if we only focus on verbatim memorization. For example, models memorize syntactic templates (Shaib et al., 2024). We discuss these findings in \u00a76.\\n\\nFor verbatim memorization treatments, our discussion is focused on post-training treatments, including unlearning (\u00a75) and string-based matching (\u00a76). If we consider the broader LLM development cycle, there are alternative approaches to the verbatim memorization problem, such as deduplication of training data (Lee et al., 2022), interventions during training, e.g., modifying the loss function (Hans et al., 2024), or even building an ecosystem that properly attributes the value of training data to its creators. We hope the findings from our work will help motivate this community to explore more solutions in these spaces.\\n\\nAcknowledgments\\n\\nThe authors gratefully acknowledge the contributors of fully open LLMs and the developers of indexing tools for LLMs training corpora. This work would not be possible with fully open LLMs like the Pythia suite and dataset indexing tools like the Data portraits and infini-gram.\\n\\nWe would also like to thank Atticus Geiger, Jiaao Chen, Robin Jia, John Hewitt, Ken Liu, Zhengxuan Wu, and Aryaman Arora for insightful discussion and feedback. This work is supported in part by a grant from ONR and the Laboratory Directed Research and Development program at Sandia National Laboratories. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia LLC, a wholly owned subsidiary of Honeywell International Inc. for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525.\\n\\nReferences\\n\\nZeyuan Allen-Zhu and Yuanzhi Li. 2024. Physics of language models: Part 3.3, knowledge capacity scaling laws.\\n\\nDevansh Arpit, Stanis\u0142aw Jastrz\u0119bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. 2017. A closer look at memorization in deep networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 233\u2013242. PMLR.\\n\\nSander Beckers and Joseph Y. Halpern. 2019. Abstracting causal models.\\n\\nStella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Gregory Anthony, Shivanshu Purohit, and Edward Raff. 2023a. Emergent and predictable memorization in large language models. In Thirty-seventh Conference on Neural Information Processing Systems.\\n\\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023b. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR.\\n\\nGavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar. 2021. When is memorization of irrelevant training data necessary for high-accuracy learning? In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2021, page 123\u2013132, New York, NY, USA. Association for Computing Machinery.\\n\\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-598", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, and Tom Goldstein. 2024. Be like a goldfish, don\u2019t memorize! mitigating memorization in generative llms.\\n\\nAdi Haviv, Ido Cohen, Jacob Gidron, Roei Schuster, Yoav Goldberg, and Mor Geva. 2023. Understanding transformer memorization recall through idioms. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 248\u2013264, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nDaphne Ippolito, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher Choquette Choo, and Nicholas Carlini. 2023. Preventing generation of verbatim memorization in language models gives a false sense of privacy. In Proceedings of the 16th International Natural Language Generation Conference, pages 28\u201353, Prague, Czechia. Association for Computational Linguistics.\\n\\nMatthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Guha Thakurta, Nicolas Papernot, and Chiyuan Zhang. 2023. Measuring forgetting of memorized training examples. In The Eleventh International Conference on Learning Representations.\\n\\nAntonia Karamolegkou, Jiaang Li, Li Zhou, and Anders S\u00f8gaard. 2023. Copyright violations and large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7403\u20137412, Singapore. Association for Computational Linguistics.\\n\\nAly M. Kassem, Omar Mahmoud, Niloofar Mireshghalilah, Hyunwoo Kim, Yulia Tsvetkov, Yejin Choi, Sherif Saad, and Santu Rana. 2024. Alpaca against Vicuna: Using LLMs to uncover memorization of LLMs.\\n\\nJooyoung Lee, Thai Le, Jinghui Chen, and Dongwon Lee. 2023. Do language models plagiarize? In Proceedings of the ACM Web Conference 2023. ACM.\\n\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424\u20138445, Dublin, Ireland. Association for Computational Linguistics.\\n\\nPietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, and Tiago Pimentel. 2024. Causal estimation of memorisation profiles.\\n\\nJiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, and Hannaneh Hajishirzi. 2024. Infini-gram: Scaling unbounded n-gram language models to a trillion tokens. arXiv preprint arXiv:2401.17377.\\n\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.\\n\\nAengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. 2024. Eight methods to evaluate robust unlearning in llms.\\n\\nPratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C Lipton, and J Zico Kolter. 2024. Tofu: A task of fictitious unlearning for llms. arXiv preprint arXiv:2401.06121.\\n\\nPratyush Maini, Michael C. Mozer, Hanie Sedghi, Zachary C. Lipton, J. Zico Kolter, and Chiyuan Zhang. 2023. Can neural network memorization be localized? In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org.\\n\\nMarc Marone and Benjamin Van Durme. 2023. Data portraits: Recording foundation model training data. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\\n\\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. In Advances in Neural Information Processing Systems, volume 35, pages 17359\u201317372. Curran Associates, Inc.\\n\\nMilad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram\u00e8r, and Katherine Lee. 2023. Scalable extraction of training data from (production) language models.\\n\\nJudea Pearl. 2001. Direct and indirect effects. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI'01, pages 411\u2013420, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.\\n\\nJudea Pearl. 2009. Causality. Cambridge University Press.\\n\\nUSVSN Sai Prashanth, Alvin Deng, Kyle O'Brien, Jyothir SV , Mohammad Aflah Khan, Jaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy Ke, Katherine Lee, and Naomi Saphra. 2024. Recite, reconstruct, recollect: Memorization in lms as a multifaceted phenomenon.\\n\\nChantal Shaib, Yanai Elazar, Junyi Jessy Li, and Byron C. Wallace. 2024. Detection and measurement of syntactic templates in generated text.\\n\\nR. Shokri, M. Stronati, C. Song, and V. Shmatikov. 2017. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3\u201318, Los Alamitos, CA, USA. IEEE Computer Society.\\n\\nNiklas Stoehr, Mitchell Gordon, Chiyuan Zhang, and Owen Lewis. 2024. Localizing paragraph memorization in language models.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-598", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Sequence Frequency vs. Verbatim Memorization\\n\\nA.1 Choice of Sequence Injection Frequencies\\n\\nTo estimate a realistic frequency for sequence injection, we need to know roughly what percentage of sequences in the Pile are memorized at each frequency range. The deduped version of Pile contains about 98M sequences, each of length 2048 tokens. Ideally, one would build an index of the entire corpus to count substrings, as is done in Carlini et al. (2023); Liu et al. (2024). However, the storage and computation required to build an index is costly. We employ a sampling-based approach instead.\\n\\nSampling\\n\\nWe first describe how to sample a relatively small set of sequences to estimate memorization rates at each frequency range. We start with random sampling 1M sequences of length 128 from the Pile and compute verbatim memorization length using the `pythia-6.9b-deduped` model. For a sequence to be considered memorized, the sequence must have a verbatim memorization length of at least 32, (i.e., there must exist a substring of length \\\\( \\\\leq 32 \\\\) tokens, such that when prompted with this substring, the model outputs a continuation that matches the next 32 tokens or more). Among the 1M sequences, there are about 9K memorized sequences and 991K non-memorized sequences. Next, we randomly sample 2.5K memorized and 2.5K non-memorized sequences, which means that we downsample the non-memorized sequences 110 times relative to memorized ones. For each sequence, we further sample a substring of 16 tokens. For memorized sequences, the 16 tokens are sampled from the memorized substring, i.e., the model outputs instead of the prompts. We refer to these 5K 16-token sequences as probes.\\n\\nCounting\\n\\nWe uniformly sample about 10M sequences from the Pile deduped dataset to estimate the frequency of each probe. The 10M sequences are sampled at every 10K training step starting at step 0, with a total of 10 \\\\( \\\\times \\\\) 1000 \\\\( \\\\times \\\\) 1024 sequences of length 2048 tokens. We count the number of occurrences of each probe in the 10M sequences. These probes indeed cover a wide range of frequencies from 0 to \\\\( 5 \\\\times 10^{-3} \\\\). The full distribution is shown in Figure 7.\\n\\nEvaluating models\\n\\nFor each model, we measure the verbatim memorization length on the 5K set of probes. The distribution of memorization length is shown in Figure 7. Aligned with findings from Carlini et al. (2023) and Prashanth et al. (2024), we observe that, as model size increases, the median frequency of memorized sequences decreases from \\\\( 4 \\\\times 10^{-5} \\\\), \\\\( 1 \\\\times 10^{-5} \\\\), to \\\\( 9 \\\\times 10^{-6} \\\\). As we mainly experiment with the 160m model, we choose two frequencies where there is a mix of memorized and non-memorized sequences: \\\\( 2 \\\\times 10^{-5} \\\\) (which is at about the bottom 25th percentile, where sequences are more likely to be non-memorized) and \\\\( 1 \\\\times 10^{-4} \\\\) (which is around the top 25th percentile, where sequences are very likely memorized).\"}"}
{"id": "emnlp-2024-main-598", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PROMPT: imGroupExternalMembershipManager getExternal\\n\\nOUTPUT: MembershipManager() {\\n return externalMembershipManager;\\n }\\n\\n public void setExternalMembershipManager\\n\\nPROMPT: madesimple.statoil\\n\\ncmsmadesimple.lips\\n\\ncmsmadesimple.next\\n\\ncmsmadesimple.nextdirect\\n\\ncmsmadesimple.ubs\\n\\ncmsmadesimple.war\\n\\nVariation\\n\\nPROMPT: the testimony.\\n\\n Rule 702 which\\ngoverns the admissibility of expert testimony provides:\\n\\n If scientific, technical, or other specialized knowledge will assist the trier of fact\\nto understand the\\n\\nPROMPT: 13: Thou shalt tread upon the lion and adder: the young\\n\\nOUTPUT: lion and the dragon shalt thou trample under feet. 14: Because he hath\\nset his love upon me, therefore will I deliver him: I will set\\n\\nInduction\\n\\nPROMPT: 01 3600\\n265 3586 3587 3602 3601\\n266 35\\nOUTPUT: 87 3588 3603 3602\\n267 3588 3589 3604 3603\\n268 3589 3590 3605 3604\\n269\\n3590 3591 3606 3605\\n\\nPROMPT: ang12.bdf batang12b.bdf \\\\\\n\\t\\tbatang14.bdf batang14b.bdf batang16.\\nOUTPUT: bdf batang16b.bdf \\\\\\n\\t\\tbatang18.bdf batang18b.bdf batang20.bdf batang\\n\\nComposition\\n\\nPROMPT: normal; AST: aspartate aminotransferase (i.e. SGOT: serum glutamic oxaloacetic transaminase); ALT: alanine aminotransferase (i.e. SGPT: serum glutamic pyruvic transaminase G),\\ntenofovir alafenamide/emtricitabine/bic\\n\\nOUTPUT: tegravir (TAF/FTC/BIC), and tenofovir alafenamide/emtricitabine/bic\\ntegravir (TAF/FTC/BIC), and tenofovir alafenamide/emtricitabine/rilpivirine (TAF\\n\\nFigure 8: Examples of the single-shot verbatim memorization illusion. Each example is a sequence that occurs once\\nor twice in the\\n\\npythia-6.9b-deduped\\n\\ntraining data and can be generated by the model verbatim. However, these\\nsequences are likely not learned from a single instance or simply not verbatim memorized \u2013 even with a model\\ncheckpoint produced before the training step where the memorized sequence occurs, the model can already output\\nthe \\\"memorized\\\" sequence or a close variant.\\n\\nA.2 Examples of the Single-shot Verbatim Memorization Illusion\\n\\nFigure 8 shows examples of sequences that only occur in the Pile once or twice according to the infini-gram\\n\\ntool 5\\n\\nand would be considered as verbatim memorized based on the most commonly used extractability\\ndefinition (Carlini et al., 2021, 2023; Prashanth et al., 2024), i.e., a memorized sequence of 32 tokens can\\nbe extracted with a prefix length of 32 tokens. In reality, these sequences are either under-counted due to\\nlimitations of string-based matching or simply not verbatim memorized \u2013 a checkpoint produced before\\nthe step where the sequence occurs can already generate a close variant of the sequence.\\n\\nThese findings suggest that a model generates a sequence that only occurs once in the training data\\ndoes not necessarily mean that the model verbatim memorized a sequence after one exposure. As shown\\nin \u00a74.5 and \u00a74.6, these sequences may well be \\\"reconstructed\\\" by the general language model.\\n\\n5https://huggingface.co/spaces/liujch1998/infini-gram\"}"}
{"id": "emnlp-2024-main-598", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B Details of Experiment Setup\\n\\nB.1 Pre-training Data\\nWe use the Pile deduped version released here,\\\\(^6\\\\) which contains training data in the exact order they were seen by the Pythia deduped models. For our training runs, we use the data from step 80K to 82K, which contain 2M training examples that has not been seen by any of the checkpoints that we experimented with (except for individual examples with duplicates).\\n\\nB.2 Injection Sequences\\n\\nData sources\\nWe sampled 100 documents from the Internet that are published after Dec 31th 2020, i.e., the Pile corpus cutoff date. These documents are from five sources that have clear publication timestamps: Wikipedia,\\\\(^7\\\\) BBC News,\\\\(^8\\\\) GitHub,\\\\(^9\\\\) open-access academic papers on ArXiv\\\\(^10\\\\) and Nature,\\\\(^11\\\\) and quotes from novels.\\\\(^12\\\\) All these sources are covered in the original training corpus. For Wikipedia, we sample articles from 2023 categories curated by Wikipedia, for example, the new product category.\\\\(^13\\\\) For BBC news, we use the preprocessed corpus on Huggingface.\\\\(^14\\\\) For GitHub, we use code samples from three new programming languages released after 2020: Mojo,\\\\(^15\\\\) Gleam,\\\\(^16\\\\) and Carbon.\\\\(^17\\\\)\\n\\nVerify a sequence is not in the Pile\\nIn our study, an important criterion for injected sequences is that they do not have significant overlap with the pre-training corpus. This is partially ensured by the document publication date. However, we conduct additional verification. We use two recently open sourced tools that create a searchable index of the Pile. We primarily rely on Data portraits,\\\\(^18\\\\) which directly checks for overlap between a query text and the Pile corpus using Bloom filters computed from 50-character hashes (Marone and Van Durme, 2023). Bloom filters guarantee no false negatives, however, there will be false positives, i.e., 50-char texts that are not in the Pile but are marked as overlaps. We further confirm these false positives using infini-gram. With both tools, we verify that none of the documents have an overlap with the Pile of more than 50 characters.\\n\\nB.3 Model Checkpoints\\nFor the sequence injection and the causal dependency experiments, we use the 1K, 10K, 40K, 80K, and the final checkpoints from pythia-160m-deduped,\\\\(^19\\\\) pythia-2.8b-deduped\\\\(^20\\\\), and pythia-6.9b-deduped models.\\\\(^21\\\\) For the unlearning stress test evaluation, we follow the setup in Stoehr et al. (2024) and use gpt-neo-125m,\\\\(^22\\\\) which is also pre-trained on the Pile.\\n\\nB.4 Setup for the Single-shot Verbatim Memorization Experiment in \u00a74.2\\nWe randomly sample 16 sequences from the 100 injection sequences curated in Appendix B.2. For each injection sequence, we use the first 224 tokens instead of the full 256 tokens, i.e., a window size of 224, so that we can fit a batch of 32 sequences on a single GPU. In general, with a fixed batch size, a smaller window size makes verbatim memorization more likely to happen, since there are fewer tokens in the batch. Given the actual window size in pre-training is 2048, the verbatim memorization length after a...\"}"}
{"id": "emnlp-2024-main-598", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We observe that these two initial states do not affect which checkpoints verbatim memorize more sequences. Thus, when comparing models trained from two different checkpoints, we use the same optimizer state for both. For learning rate, we use the learning rate at the 80K checkpoint for each model family, namely $79 \\\\times 10^4$ for 160m models and $7.46 \\\\times 10^5$ for 2.8b models. We observe the learning rate affects all checkpoints equally, with larger learning rates leading to more memorization. We keep the learning rate constant throughout the training, as the amount of data we trained on only corresponds to 1\u20132K steps in the original training process.\\n\\n### B.6 Unlearning Method Hyperparameters\\n\\nFor gradient ascent and sparse fine-tuning, we use the implementation from Stoehr et al. (2024). We follow the hyperparameters here, namely, we run optimization for 10 steps using a learning rate of $1 \\\\times 10^{-5}$ and a weight decay of 1.0. For sparse fine-tuning, we only fine-tune 0.1% of weights with the highest gradient.\\n\\nFor neuron pruning, we use the implementation from Chang et al. (2024b). We prune 0.1% of the neurons. The L1 penalty is set to 1000. We find that higher L1 penalty leads to degeneration. We run optimization for 1000 steps using a learning rate of $1 \\\\times 10^{-2}$. This set of hyperparameters leads to a $\\\\Delta$ self-accuracy of $-0.248$ and $\\\\Delta$ neg-accuracy of $-0.094$ on the 90 sequences to unlearn.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All models are trained on NVIDIA A100 GPUs. For models in \u00a74.2, the training is distributed across multiple GPUs, with a local batch size of 32. For models in \u00a74.3 and \u00a74.4, the training is on a single GPU. The training of $2.8b$ models over 1M examples takes about 16 hours, while the training of $160m$ models takes about 3 hours.\\n\\nC Additional Experiment Results\\n\\nC.1 Additional Results on Checkpoint vs. Verbatim Memorization Length\\n\\nFigure 9: Checkpoint vs. verbatim memorization length of original and shuffled sequences, with a sequence frequency of every 10K examples.\\n\\nIn Figure 9, we show the results of verbatim memorization length when continue pre-training from different checkpoints of the $160m$ model and the $2.8b$ model with a sequence injection frequency of 1 in 10K examples. This is a frequency that both models are expected to memorize most of the injection sequences. We still see the consistent trend that we observed in \u00a74.3 on in-domain sequences: later checkpoints memorize longer sequences. The gap between shuffled sequences and original sequences is narrowed, especially on the $160m$ models, possibly because the model is seeing the injection sequences more frequently. For the $2.8b$ models, which see the injection sequences fewer times, shuffled sequences are still harder to memorize than the original ones for all checkpoints except the 10K step.\\n\\nC.2 Additional Results on Causal Dependencies\\n\\nBehavioral evidence of verbatim memorization is triggered by abstract model states\\n\\nIn Figure 10, we show that when prompted with prefixes sharing similar high-level features, e.g., synonyms or proper nouns belong to the same category, the $6.9b$ model can produce the memorized continuation. Semantically similar prefixes do not always trigger verbatim memorization, nor does verbatim memorization strictly require prefixes semantically similar to the one in training, however, semantically relevant substitutions do have a higher probability to trigger the verbatim memorized continuation than random substitutions. Overall, the trigger is a set of distributed abstract states and does not require a particular token to be presented in the prefix, i.e., the verbatim memorization is not triggered by a single n-gram match. This finding motivates the stress tests we developed in \u00a75.\\n\\nResults of the $6.9b$ model\\n\\nIn Figure 11, we show the causal dependency results of the pre-trained pythia-6.9b-deduped model on 50 memorized sequences sampled from the 5K sequences in \u00a74.1. The results are consistent with what we observed from the $160m$ models trained using our sequence injection framework \u2013 namely, not all verbatim memorized tokens depend on the trigger sequences. Moreover, for memorized tokens that depend on the trigger, the dependencies are also around middle layers, suggesting high-level features are involved in the verbatim memorization.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Original Trigger Prefix\\n\\nMr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\\n\\nTrigger Prefixes with Similar High-level Features\\n\\nMrs and Mr Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\\n\\nThe Dursley family, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\\n\\nMr and Mrs Weasley, residing at number four Privet Drive, were proud to say they were perfectly normal, thank you very much.\\n\\nMr and Mrs Slytherin, of number twenty-one, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\\n\\nMr and Mrs Dursley, of #4, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\\n\\nMr and Mrs Dursley, of number ten, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\\n\\nMr and Mrs Dursley, of Privet Drive, were proud to say that they were perfectly normal, thank you very much.\\n\\nMr and Mrs Dursley, of number four, Oak Street, were proud to say that they were perfectly normal, thank you very much.\\n\\nMr and Mrs Dursley, residing at number four Privet Drive, were delighted to assert they were perfectly normal, thank you very much.\\n\\nThe Dursley family, of number four, Privet Drive, were pleased to declare that they were perfectly normal, thank you very much.\\n\\nNon-Trigger Prefixes with Similar or Different High-level Features\\n\\nMr and Mrs Kingsley, of number four, Privet Drive, were proud to say that they were the proud parents of a bouncing baby boy.\\n\\nMr and Mrs Weasley, of number four, Privet Drive, were proud to say that they were expecting their first child.\\n\\nMr and Mrs Dursley, of number four, Privet Drive, were glad to say that they were only too delighted to have the young man staying with them.\\n\\nFigure 10: Examples of trigger prefixes that lead to similar abstract states, i.e., similar high-level semantic features.\\n\\nThe gray texts are the prompts. The green texts are the memorized continuations. The red texts are the non-memorized continuations.\\n\\nFigure 11: Causal dependencies between memorized tokens and tokens in the trigger for the 6.9b model.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: Trainable components vs. verbatim memorization length of original and shuffled sequences. Models are trained from the 160m model checkpoint at step 80K with two different data injection frequencies (every 50K and 10K examples).\\n\\nC.3 Verbatim Memorization Can Still Happen with Frozen Attention Heads\\n\\nExperiments in \u00a74.6 show that both attention and MLP components are involved in verbatim memorization. We now investigate which components are strictly necessary for verbatim memorization, taking the capacity of these components into account.\\n\\nSetup\\n\\nWe conduct an ablation study by training three sets of models using the 160m model checkpoints: (1) only MLP layer weights are trainable (2) only attention head weights are trainable (3) all weights are trainable. For Pythia models, which use the GPTNeoX architecture, the MLP layers contain 35% of model weights while attention heads contain 17% of model weights. We experiment with two sequence frequency of 1 in 50K and 1 in 10K.\\n\\nResults\\n\\nResults are shown in Figure 12. With a frequency of every 50K examples, neither MLP-only nor attention-only models can verbatim memorize a sequence. However, at the frequency of every 10K examples, the model with frozen attention heads surprisingly can verbatim memorize sequences of 40 tokens on average, which is about 80% of the tokens memorized by a model with all weights trainable. These results suggest that MLP layers are strictly necessary for verbatim memorization, while attention mechanisms learned from general language modeling can largely be reused assuming the sequence to memorize occurs frequently enough in training.\\n\\nC.4 Additional Results on Stress Testing Unlearning Methods\\n\\nIn Figure 13, we show examples of the original prompt and the perturbed stress testing prompts, along with the model output before and after unlearning. In Figure 14, we show the verbatim memorization length distribution shift when evaluate with the original prompts and the stress testing prompts.\\n\\nD License\\n\\nFor artifacts used in this work, the Pythia models are licensed under Apache-2.0 License. The Pile dataset is licensed under MIT License. Our use of the model and the dataset are permitted under the license.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unlearning with Gradient Ascent\\n\\nOriginal Test\\n\\nPROMPT: 0f86e5b48e01b996cadc001622fb5e363b421\\n\\nOUTPUT: dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\\n\\nUNLEARNED: 67\\n\\nPosition Perturbations\\n\\nPROMPT: 5b48e01b996cadc001622fb5e363b421\\n\\nOUTPUT: dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\\n\\nUNLEARNED: dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\\n\\nSemantic Perturbations\\n\\nPROMPT: e105b48e01b996cadc001622fb105e363b421\\n\\nOUTPUT: dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\\n\\nUNLEARNED: dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347\\n\\nUnlearning with Gradient Ascent\\n\\nOriginal Test\\n\\nPROMPT: NOT TO BE PUBLISHED IN OFFICIAL REPORTS\\n\\nCalifornia Rules of Court, rule 8.1115(a), prohibits courts and parties from citing or relying on opinions not certified for publication or ordered published, except as specified by rule 8.1115(b). This opinion has not been certified for publication or ordered published for purposes of rule 8.1115.\\n\\nUNLEARNED: or ordered published, except as specified by rule 8.1115(b). This opinion has not been certified for publication or ordered published for purposes of rule 8.1115.\\n\\nPosition Perturbations\\n\\nPROMPT: NOT TO BE PUBLISHED IN OFFICIAL REPORTS\\n\\nCalifornia Rules of Court, rule 8.1115(a), prohibits courts and parties from citing or relying on opinions not certified for publication or ordered published, except as specified by rule 8.1115(b). This opinion has not been certified for publication or ordered published for purposes of rule 8.1115.\\n\\nUNLEARNED: 15(b). This opinion has not been certified for publication or ordered published for purposes of rule 8.1115.\\n\\nSemantic Perturbations\\n\\nPROMPT: stay PUBLISHED IN OFFICIAL REPORTS\\n\\nCalifornia Rules of Court, rule 8.1115(a), prohibits courts and parties from citing or relying on opinions not certified for publication or ordered published, except as specified by rule 8.1115(b). This opinion has not been certified for publication or ordered published for purposes of rule 8.1115.\\n\\nUNLEARNED: 15(b). This opinion has not been certified for publication or ordered published for purposes of rule 8.1115.\\n\\nFigure 13: Examples of stress tests and failure cases of unlearning. Each example consists of the original prompts and the perturbed prompts, along with the model outputs before and after unlearning. Overlaps between the memorized output and the unlearned output are bolded. These examples show that while unlearning largely prevents the model from outputting the memorized sequence given the original prompt, it does not fully remove the verbatim memorized information.\"}"}
{"id": "emnlp-2024-main-598", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unlearning with Neuron Pruning\\n\\nOriginal Test\\n\\nPROMPT: From fairest creatures we desire increase,\\nThat thereby beauty's rose might never die.\\nBut as the riper should by time decease,\\nHis tender heir might bear his memory:\\nBut thou, contracted to thine own bright eyes,\\nFeed'st thy light's flame with self-substantial fuel,\\nMaking a famine where abundance lies,\\nThyself thy foe\\n\\nThe world is a book, and those who do not travel read only a page.\\nThe world is a book, and those who do not travel read only\\n\\nPosition Perturbations\\n\\nPROMPT: From fairest creatures we desire increase,\\nThat thereby beauty's rose might never die.\\nBut as the riper should by time decease,\\nHis tender heir might bear his memory:\\nBut thou, contracted to thine own bright eyes,\\nFeed'st thy light's flame with self-substantial fuel,\\nMaking a famine where abundance lies,\\nThyself thy foe\\n\\nSemantic Perturbations\\n\\nPROMPT: From fairest creatures we desire increase,\\nThat thereby beauty's rose might never die.\\nBut as the riper should by time decease,\\nHis tender heir might bear his memory:\\nBut thou, contracted to thine own bright eyes,\\nFeed'st thy light's flame with self-substantial fuel;\\n\\nMaking a famine where abundance lies,\\nThyself thy foe\\n\\nThe poem is a parody of the famous \\\"Ode\"}"}
