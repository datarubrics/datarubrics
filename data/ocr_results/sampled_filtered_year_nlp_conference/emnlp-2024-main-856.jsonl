{"id": "emnlp-2024-main-856", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AppBench: Planning of Multiple APIs from Various APPs for Complex User Instruction\\n\\nHongru Wang\u03b1\u2020, Rui Wang\u03b1\u2020, Boyang Xue\u03b1, Heming Xia\u03b3, Jingtao Cao\u03b1, Zeming Liu\u03c3, Jeff Z. Pan\u03b4\u2021, Kam-Fai Wong\u03b1\u2021\\n\\nThe Chinese University of Hong Kong\\nThe Hong Kong Polytechnic University\\nBeihang University,\\nThe University of Edinburgh\\nhrwang, kfwong@se.cuhk.edu.hk\\n\\nAbstract\\nLarge Language Models (LLMs) can interact with the real world by connecting with versatile external APIs, resulting in better problem-solving and task automation capabilities. Previous research primarily focuses on APIs with limited arguments from a single source or overlooks the complex dependency relationship between different APIs. However, it is essential to utilize multiple APIs collaboratively from various sources (e.g., different Apps in the iPhone), especially for complex user instructions. In this paper, we introduce AppBench, the first benchmark to evaluate LLMs' ability to plan and execute multiple APIs from various sources in order to complete the user's task. Specifically, we consider two significant challenges in multiple APIs:\\n\\n1) graph structures: some APIs can be executed independently while others need to be executed one by one, resulting in graph-like execution order;\\n2) permission constraints: which source is authorized to execute the API call.\\n\\nWe have experimental results on 9 distinct LLMs; e.g., GPT-4 achieves only a 2.0% success rate at the most complex instruction, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning and finetuning.\\n\\nOur code and data are publicly available at https://github.com/ruleGreen/AppBench.\\n\\n1 Introduction\\nEmpowering Large Language Models (LLMs) (Zhao et al., 2023) with versatile tools such as retrievers (Wang et al., 2023a, 2024b), models (Shen et al., 2023), and even physical robots (Liang et al., 2023), holds significant promise in overcoming inherent limitations, such as hallucination (Ji et al., 2023) and outdated information (Nakano et al., 2021; Liu et al., 2023a), and unveils the immense potential for LLMs to tackle increasingly complex and interactive real-world tasks (Li et al., 2023; Lu et al., 2023). Over the past several months, lots of new benchmarks and datasets have been proposed to evaluate the performance of different LLMs to adeptly select and execute various tools (Li et al., 2023; Shen et al., 2023; Huang et al., 2024a), marking a pivotal milestone in their evolution. Out of plentiful tools in practice, APIs have become one of the fundamental and promising tools in today's digital world, due to greater flexibility and customizability with well-defined format and ease of execution (Qin et al., 2023).\\n\\nPrevious works have attempted to evaluate LLMs on their ability to call the correct API in multiple turn dialogues, such as API-Bank (Li et al., 2023) and ToolBench (Qin et al., 2024), or single turn instructions, like APIBench (Patil et al., 2023). However, most existing benchmarks focus either on a single API call in a single turn or on APIs with limited arguments. For instance, API-Bank mainly evaluate one API call per turn in multi-turn dialogues, while APIBench and ToolBench considers APIs only with one or two arguments (e.g., only...\"}"}
{"id": "emnlp-2024-main-856", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"one output with one or two inputs). Furthermore, the small number of arguments makes it difficult to fully explore the complex dependency relationships between multiple APIs. For instance, the input arguments for a current API may depend on the return arguments of several previous APIs. These limitations highlight a gap in addressing complex user instructions when it is necessary to utilize multiple APIs in practice, underscoring the need for more comprehensive and practical evaluation benchmarks.\\n\\nTo bridge the gap, we introduce a new evaluation benchmark: AppBench, representing the first effort to assess the aptitude of LLMs to function as the meta planner for multiple APIs from various sources for complex user instruction. Specifically, we simulate a situation in which the user instruction can be fulfilled through collaboratively API calls from various APPs in the mobile device. Figure 1 shows one typical example. Given the complex user instruction, the meta LLM, such as Apple's Siri and Google Assistant, need to plan an executable path according to user instruction and corresponding API descriptions. To fulfill this requirement, it is necessary not only to indicate which APP will distribute and execute each API but also to specify the execution order of the APIs, including all necessary inputs and returned arguments. We consider this setting aligns well with the complexity and practical limitations in the real world, and presents a great opportunity for advanced AI assistants like Apple's Siri to showcase their intelligence and capability in orchestrating collaborative API executions across multiple Apps.\\n\\nIn this way, two significant challenges are identified: graph structure and permission isolation. Firstly, the inter-dependency between multiple APIs creates a more complex execution structure. Some APIs can be executed independently, while others are dependent and must be executed sequentially, resulting in a graph-like structure. Secondly, these APIs may originate from different sources, and the LLM might not have permission to call them directly. This necessitates identifying the authorized source for each API. For instance, APIs from one company may only be executed by an LLM within the same company. In doing so, we aim to chart a path towards realizing the vision of an intelligent assistant capable of seamlessly navigating and interfacing with the myriad APPs and APIs pervasive in contemporary digital ecosystems.\\n\\nTo conclude, our contribution can be summarized in three folds:\\n\\n\u2022 To the best of our knowledge, we are the first to identify graph structure and permission isolation issues of multiple API calls when addressing complex user instructions.\\n\\n\u2022 We propose AppBench, serving as an important complementary evaluation benchmark to assess the planning capabilities of different LLMs as meta planner for these APIs. Additionally, we introduce an automatic data collection pipeline, which can be used to gather data efficiently and effectively.\\n\\n\u2022 Our experimental results on 9 distinct LLMs demonstrate almost all models, including the latest GPT-4, fall short in this setting, particularly when dealing with complex graph planning structures. Further analysis shows that simple in-context learning and fine-tuning do not significantly improve performance.\\n\\nRelated Work\\nTool Benchmarks. The complexity of real-world tasks necessitates the integration of diverse tools and services, consisting of three types of tools (Qin et al., 2023): 1) physical interaction-based tools (Liang et al., 2023); 2) GUI-based tools (Wang et al., 2024d); and 3) program-based tools (Wang et al., 2023a; Li et al., 2023). On the one hand, some work focuses on models, retrievers, or calculators to address the intrinsic limitations of LLMs, such as ToolQA (Zhuang et al., 2023) and ToolBench (Qin et al., 2024). On the other hand, another line of work targets APIs since they are particularly crucial for bridging smooth interaction between humans and the digital realm (Li et al., 2023; Qin et al., 2024; Huang et al., 2024a). Most previous works formulate this as an API selection task given all related information about each API and current input, which overlooks the nuanced dependencies and permission constraints between different APIs, such as APIBench (Patil et al., 2023) and API-Bank (Li et al., 2023). Nevertheless, the successful execution of APIs in the real world necessitates meeting requirements fulfilled (either the value is provided by the user or previous APIs) and obtaining permission from trusted agents beyond just knowing API names and a few arguments. More details can refer to latest survey (Qu et al., 2024) and tutorial (Wang et al., 2024a).\"}"}
{"id": "emnlp-2024-main-856", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Existing frameworks for language agents have made notable strides in facilitating interaction with external tools (Shen et al., 2023; Li et al., 2023; Huang et al., 2024a) and environment (Puig et al., 2018a; Wang et al., 2022). They usually follow the single-agent paradigm to access different tools or services sequentially (Lu et al., 2023; Li et al., 2023), or multi-agent framework by assigning different agents different roles to call different cognitive tools (Wang et al., 2023b). For example, Lu et al. (2023) propose Chameleon which utilizes one agent to plan the execution order of different services by outputs a sequence of names of tools, which assume that the agents to call these tools are already known, and lots of works follow (Xu et al., 2023; Huang et al., 2024a). Furthermore, various benchmarks are proposed to evaluate the abilities of LLMs serving as agents in different situations (Li et al., 2023; Liu et al., 2023b; Ma et al., 2024). For instance, Yao et al. (2023) proposes WebShop to evaluate whether LLMs are capable of interacting with the Web. Similarly, (Puig et al., 2018b) simulates household activities through programs, and many works use this as a testbed for embodied agents (Hao et al., 2024). Latest work focus on using APIs or functions to control the whole planning processing of agents (Wang et al., 2024c).\\n\\n3. AppBench\\n\\nIn this section, we start with a formal task definition and then provide a detailed explanation of how we efficiently and effectively built our AppBench by leveraging existing datasets.\\n\\n3.1 Task Definition\\n\\nGiven the user instruction $u$ and a virtual mobile environment with an APP family, $E = \\\\{APP_1, APP_2, ..., APP_n\\\\}$ where each APP contains several APIs $\\\\{p_{i1}, .., p_{ij}\\\\}$ where $i$ stands for $i$th APP and $j$ means $j$th API inside this APP, the meta-agent need to decide an executable path to call different APIs from various APPs to fulfill the instruction in the format of the list which each item in the list is $\\\\{APP_i: r_1, r_2, .., r_m = p_{ij}(k_{i1} = v_1, ..., k_{in} = v_n)\\\\}$. The $APP_i$ and $p_{ij}$ denote the name of the APP and corresponding API of this APP, and the $r_i$ and $k_i$ mean the $i$th returned and input arguments respectively. The $v_i$ can be the actual value provided by the user or a returned argument by previous APIs.\\n\\nFigure 2: A high-level processing to collect the AppBench, taking advantages of existing task-oriented dialogue datasets.\\n\\n3.2 Data Categorization\\n\\nBased on the number of APPs and APIs utilized in each user instruction, the data can be categorized into four distinct types. Each category represents a typical use case in practical scenarios, creating a comprehensive benchmark for evaluating real-world applications when combined.\\n\\n- Single APP Single API (SS): The instructions of the users only need to utilize one API from one APP.\\n- Single APP Multiple API (SM): The instructions of the users need to utilize multiple API from one APP. It is important to note that these APIs can be called either sequentially or concurrently, depending on whether there is a dependency between their arguments.\\n- Multiple APPs Single API (MS): The instructions of the users need to utilize multiple APIs and each of them belongs to one different APP. Also, there may exist dependency between APIs across different APPs.\\n- Multiple APPs Multiple API (MM): The instructions of the users need to utilize multiple APIs and multiple APPs. The difference with MS is there may exist multiple APIs come from the\"}"}
{"id": "emnlp-2024-main-856", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison with existing evaluation benchmarks at the turn-level for a fair comparison. DP stands for Dependency.\\n\\nTable 1 shows the detailed comparison between AppBench with other popular benchmarks. Most existing benchmark focus on part of these typical situations or overlook the complex dependency relationships between multiple APIs. In addition, our formulation highlights the potential for investigating graph structure and permission management, considering the inherent complexity of APIs and Apps, particularly in terms of handling DP in multiple input and output arguments.\\n\\n3.3 Data Collection\\n\\nTo maximize the authenticity of user instructions and minimize human efforts, we prioritize using existing task-oriented dialogue datasets (Rastogi et al., 2020; Budzianowski et al., 2018). These datasets are typically collected through human-to-human interactions in real-world scenarios and contain a wide range of APIs across numerous domains and services. Specifically, we selected the SGD (Rastogi et al., 2020) dataset as the seed dataset because it encompasses most of the domains and APIs. We then utilized LLMs and Python scripts to generate the desired inputs and outputs, respectively. Figure 2 illustrates the detailed procedures.\\n\\nInstruction Acquisition.\\n\\nFirstly, we extract the utterances of the user and system in the task-oriented dialogue and feed it into the LLM to summarize the user's requirements in one instruction. For example, the user may want to know the city and date of EMNLP 2024, and book a hotel according to the city and date. In the previous task-oriented dialogue, this is achieved by multi-turn interactions. In contrast, we summarize the whole dialogue into one complex user instruction to mimic more natural and complex cases in practice.\\n\\nPlanning Path.\\n\\nBesides the instruction part, we write a Python script to automatically parse the API calls at different system turns in the multi-turn dialogue to form the planning path as the output. Specifically, we regard different domains (a.k.a., services) in task-oriented dialogue as different APPs such as restaurants and hotels, and extract the name of the domain and API first to locate which APP should invoke to call the API, and then we follow the execution order of different APIs to build the dependency between various arguments. For example, if the returned arguments from the previous API are required in the current API, we use #name to indicate it such as #date and #city in the Figure 2. In this way, we can get an executable and unique path to execute APIs from different APPs.\\n\\nQuality Assessment\\n\\nTo ensure the quality of data, we utilize a Python script to validate whether or not all actual values are provided from the user side, and none of values are provided from the system side. Furthermore, we adapt GPT-4 to score each instruction in terms of fluency and diversity from 1 to 10, and then remove cases whose score is lower than 6. Approximately 20% of the samples were removed, and the average score of the remaining samples is around 8.05. We finally manually check each instruction-path pair, and remove some cases.\"}"}
{"id": "emnlp-2024-main-856", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SS Instruction: Find a house with a rating of 4.6 or higher for a trip to Delhi for two people, inquire about laundry service availability.\\n\\nOutput: \\n```\\nHouse: address, phone_number, total_price, has_laundry_service, ...\\n```\\n```\\n= searchhouse(number_of_adults='2', rating='4.60', where_to='Delhi')\\n```\\n\\nSM Instruction: Please book a Hatchback car with insurance to be picked up from Warsaw Chopin Airport on March 7th at 1:30 pm, and returned on March 13th in Warsaw.\\n\\nOutput: \\n```\\nRents: pickup_location, price_per_day, ...\\n```\\n```\\n= getcarsavailable(car_type='Hatchback', city='Warsaw', end_date='2019-03-13', pickup_time='13:30', start_date='2019-03-07')\\n```\\n```\\nRents: car_type, car_name, ...\\n```\\n```\\n= reservecar(add_insurance='True', car_type=car_type, end_date=end_date, pickup_location=pickup_location, pickup_time=pickup_time, start_date=start_date)\\n```\\n\\nMS Instruction: Search for a locomotive departing from Portland, OR on the 2nd of this month to Vancouver, BC, and then search for a residence in Vancouver for two people with a rating of 4.2 or higher.\\n\\nOutput: \\n```\\nTrain: from, total, class, ...\\n```\\n```\\n= findtrains(date_of_journey=2019-03-02, from=Portland, to=Vancouver)\\n```\\n```\\nHouse: address, phone_number, total_price, has_laundry_service, ...\\n```\\n```\\n= searchhouse(number_of_adults='2', rating='4.2', where_to=Vancouver)\\n```\\n\\nMM Instruction: Please make a reservation for 3 people at one Korean restaurant in San Francisco at 1:30 pm on March 12th, and also book a Luxury taxi for 3 to 4 Embarcadero Center.\\n\\nOutput: \\n```\\nRestaurant: restaurant_name, has_vegetarian_options, phone_number, rating, address, price_range, category, ...\\n```\\n```\\n= findrestaurants(category='Korean', has_seating_outdoors='True', location='San Francisco')\\n```\\n```\\nRestaurant: date, time, location, ...\\n```\\n```\\n= reserverestaurant(date='2019-03-12', location=location, number_of_seat='3', restaurant_name=restaurant_name, time='13:30')\\n```\\n```\\nRents: destination, ride_type, ride_fare, wait_time, number_of_seats, ...\\n```\\n```\\n= getride(destination='4 Embarcadero Center', number_of_seats='3', ride_type='Luxury')\\n```\\n\\n---\\n\\nFigure 3: An example of different types of samples in AppBench. We color APP, API, and returned arguments and input arguments. We also present the structure of the example using grey nodes and colorful nodes to indicate user instruction and APIs from different APPs, respectively. We bold the argument which is returned by the previous API call (a.k.a., dependency relationship). Para. and Seq. represents the parallel and sequential size of the corresponding data sample. We emphasize we only choose the simplest examples in each type for better understanding, there are data samples with much more complex logic structures in the original dataset.\\n\\nPara.=1 Seq.=1\\nPara.=1 Seq.=2\\nPara.=2 Seq.=(1,1)\\nPara.=2 Seq.=(2,1)\"}"}
{"id": "emnlp-2024-main-856", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The main results of different LLMs on AppBench. Bold highlights the best score among all models, and underline underscores the best score under the same model scale.\\n\\nInstruction, we carefully design two F1 scores for APP and API, and one overall success rate considering the complexity of the task. We also provide the results of EM metrics in the Appendix.\\n\\nF1 of App.\\nWe first get the precision $P_{app}$ as the number of correctly predicted APPs divided by the total number of APPs predicted by the model:\\n\\n$$P_{app} = \\\\frac{\\\\text{app}_{hit}}{\\\\text{num}_{app}_{pred}}$$\\n\\nand recall $R_{app}$ as the number of correctly predicted APPs divided by the total number of APPs that are in the ground truth as follows.\\n\\n$$R_{app} = \\\\frac{\\\\text{app}_{hit}}{\\\\text{num}_{app}_{ground}\\\\text{truth}}$$\\n\\nThe F1 of App score is $2PR / (P+R)$, as usual.\\n\\nF1 of API.\\nSimilarly, the metrics of API predictions can be evaluated using $F_{1\\\\text{api}}$. Note that we only consider the name of the API here to determine LLM whether or not to choose the right API, and the performance of arguments of APIs is evaluated in the next metric.\\n\\nSuccess Rate (Succ):\\nThis metric evaluates whether the LLMs can fully execute the user's instruction by correctly identifying all required APPs, APIs, and arguments. It is defined as the proportion of instances where all elements\u2014APP, API, and arguments\u2014are in perfect alignment with the ground truth, considering the complex dependency relationship between different APIs across APPs, resulting in a direct measure of model capability in full instruction fulfillment. Since there may exist different output orders, we calculate this at the structure level since the execution structure is unique.\\n\\n4.3 Main Results\\nTable 3 shows the results of different LLMs for different types of user instructions on AppBench, respectively. Several conclusions can be drawn from the results.\\n\\nOverall, GPT-4o achieves the best overall performance, while LLaMA3-70B sometimes outperforms GPT-3.5, mostly in scenarios only involving single APP. In general, other models significantly lag behind GPT-4o in all types of instructions, and only QWen1.5-72B or LLaMA3-70B achieves better or competitive performance compared with GPT-4o. Despite significant advancements in LLMs, the existing models still fall short in addressing the complexities of planning cases such as multiple APPs and multiple APIs. One fact is that all LLMs only get less than 3% Succ in MM situations.\\n\\nAs the size of the model increases, the performance can get further improved regardless of the type of instructions and the improvement becomes less significant with multiple APPs. As evidenced by LLaMA3 and QWen1.5 series models, we can find that large models mostly lead to better performance. However, when the instruction requires coordination between multiple APPs, most models show a significant drop in performance and some models even get 0 at Succ, such as QWen1.5-7B and 14B. Moreover, the $F_{1\\\\text{app}}$ can get around 10% improvement in a single APP while only less than 5% in LLaMA3 series models.\\n\\nThe complexity of planning highly impacts the performance of these models. From the varying scores of different LLMs across different scenarios, the conclusions are consistent with EM results at Table 11.\"}"}
{"id": "emnlp-2024-main-856", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: The relationship between GPT-4o's performance with parallel and sequential scaling. Both parallel and sequential scaling cause challenges for model performance. A trend in performance emerges: the observed order of performance is approximately: MM < MS < SM < SS. This trend exists in most LLMs such as GPT-4o, QWen1.5-14B, LLaMA3-8B, and LLaMA3-70B. The slight difference between SM and MS can be attributed to different percentages of specific data examples such as the number of APPs and APIs. This kind of trend also aligns well with our intuition that the MM scenario is the most complex, followed by MS and SM, and SS is the simplest.\\n\\n5 Analysis\\nIn this section, we conduct a comprehensive analysis, aiming to answer three research questions.\\n\\nRQ1: How do the parallel and sequential dependencies influence the model performance? (Sec 5.1)\\n\\nRQ2: Is it necessary to identify APP first to reduce the context window? (Sec 5.2) and\\n\\nRQ3: What is the major bottleneck of current LLMs (Sec 5.3), and can fine-tuning or in-context learning alleviate it? (Sec 5.4, 5.5).\\n\\n5.1 The Effects of Dependency Structures\\nWe classify the dependency structures among APIs as twofold: parallel execution and sequential execution. For each data sample, we measure the parallel execution scale by the number of connected components of APIs and use the average size of these API-connected components as the sequential execution scale. The data sample with a sequential scale of 1 means no sequential dependencies among APIs. All of the APIs can be finished in a parallel way. Then, we classify the data samples of AppBench based on the above criteria and discard the categories with less than 10 samples.\\n\\nFigure 5: The performance gap between hierarchical and flat prompting on GPT-3.5 and GPT-4o.\\n\\nWe illustrate the Exact Match (EM) of Arguments of GPT-4o in Figure 4 since arguments are directly related to the dependency relationship. First of all, when the parallel scale is fixed, an increased sequential scale becomes more challenging for GPT-4o, and vice versa. Secondly, GPT-4o appears to struggle more with sequential-complex data than parallel-complex samples. The gap between different para. size (i.e., when seq. size is fixed) is much smaller than the gap between different seq. size (i.e., when para. size is fixed).\\n\\n5.2 The Effects of Different Prompting\\nIn the main experiments, we initially required LLMs to select candidate APPs based on user input and the APP's descriptions, and then generate API calls, resulting in hierarchical prompting. Recently, many studies have expanded the context of LLMs to 200K or more (Huang et al., 2024b). Many of these works proposed LLMs with a context window that is sufficient to accommodate all the descriptions of APPs and APIs at once (flat prompting). Therefore, this section explores how the model would perform if we directly provided all apps and APIs to the model. We test GPT-3.5 and GPT-4o and compare the results in Figure 5. We can observe that flat prompting has impacted the performance of the GPT-3.5, with obvious declines in metrics such as $F_1$ app scores across data types. We attribute this to the introduction of a large amount of irrelevant information, which affects the model's understanding and extraction of useful APPs and APIs. Surprisingly, the GPT-4o model achieved better performance using flat prompting.\"}"}
{"id": "emnlp-2024-main-856", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: Error analysis of GPT-4o on AppBench. \\\\( I \\\\) and \\\\( D \\\\) stand for independent and dependent variables or values, respectively, between multiple APIs. \\\\( T/S \\\\) refers to time-related or space-related values, such as start date and location.\\n\\nPrompting. We believe this is due to the GPT-4o's more powerful long-context understanding capabilities, which allow it to accurately identify the required APP and API. Moreover, the absence of the error propagation effect that occurs during the first APP selection step of hierarchical prompting, has led to a clear improvement in performance. However, flat prompting requires a strong contextual capability that few models possess, and it necessitates the input of a large number of irrelevant tokens, which incurs additional computational power consumption.\\n\\n5.3 Error Analysis\\n\\nWe further conduct error analysis at the argument level since it is directly related to different relationships between multiple APIs, to identify potential bottlenecks of the current best model: GPT-4o. Specifically, there are two main categories of errors to consider:\\n\\n1) **key error.** It occurs when the model predicts fewer keys than expected to successfully execute the API call, and it can be further divided into two types:\\n   - Independent: The missing or incorrect keys are from the independent variables or arguments\\n   - Dependent: The missing or incorrect keys are from the dependent variables or arguments\\n\\n2) **value error.** It occurs when the model predicts values that do not match the ground truth values, given the name of the key. Value errors can also be divided into \\\\( I \\\\) and \\\\( D \\\\) types.\\n\\nTable 4 presents the percentage of error cases over the number of total arguments in each category while \\\\( T/S \\\\) is the percentage over all error arguments in each category. It is found that as complexity increases, errors also increase. The lower \\\\( D \\\\)-key error and \\\\( D \\\\)-value error in MS can be attributed to a smaller percentage of dependency cases in this category. Out of all types of errors, the \\\\( D \\\\)-value error appears to be the biggest bottleneck or challenge for the LLM. Further analysis reveals that the value errors are particularly prevalent for time and space-related keys. For example, the language models may struggle to accurately recognize or reason about date/time expressions used in the user's input, such as \\\"next Monday\\\".\\n\\n5.4 The Effects of Fine-tuning\\n\\nWe additionally collected around 1,000 samples for each category from the training dataset of SGD, resulting in 4,000 samples total. We then used this mixed dataset to fine-tune the LLaMA3-8B model. Figure 6 shows the final results. Further fine-tuning on in-domain data did bring some improvement in the F1 score of the APP and API, but cannot boost the performance for the Succ. Upon closer inspection, we found that the major reason for the lower performance on Succ was due to issues with recognizing or matching the keys and values in the input arguments. The model sometimes failed to recognize all the necessary input keys and values, or mistakenly used keys from other APIs. This appears to be strongly related to the complexity of the task. Factors like the dependency relationships between multiple APIs, as well as the lengthy API descriptions, made it challenging for the LMs to fully capture the necessary patterns and logic.\\n\\n5.5 The Effects of In-context Learning\\n\\nTable 5 shows the in-context performance of GPT-4o when using different shots at the demonstrations. Specifically, we randomly sample (instruction, outputs) from the same training set created during fine-tuning according to the used APP in the current instruction. For example, if the used APP in current instruction is Hotel, we sample the first 3 appeared samples with the same APP in the training set to form 3-shot demonstrations, aiming to save the space of additional API descriptions and make the agent familiar the utilization of current API.\\n\\nFrom the table, we find that in-context learning...\"}"}
{"id": "emnlp-2024-main-856", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: In-context learning results of GPT-4o on AppBench.\\n\\nshows some improvement in simpler cases, such as SS (\u2248 10 points increase on Succ). However, the performance does not improve further as situations become more complex and even decreases in scenarios like SM or MS, highlighting the challenges of complex planning. The worst performance in SM may be strongly related to our sampling strategy, as we only consider the APP level rather than different APIs within the same APP. More effective in-context learning for complex planning is desired and warrants further exploration and attention.\\n\\n6 Conclusion\\nIn this paper, we introduce a new benchmark, AppBench, addressing the challenge of complex user instructions that require the involvement of multiple APIs. These scenarios demand advanced planning capabilities from LLMs to effectively handle graph structures and ensure permission isolation in practical applications. We left the self-evolving or more effective fine-tuning framework in our future work.\\n\\n7 Acknowledgement\\nThanks for the insightful comments and feedback from the reviewers. This work was supported by the National Key R&D Program of China (No. 2023YFF0725600) and the National Natural Science Foundation of China (No. 62406015). This research work also is partially supported by CUHK direct grant (No. 4055209) and CUHK Knowledge Transfer Project Fund No. (KPF23GWP20).\\n\\n8 Limitations\\nWe acknowledge the following limitations in terms of the evaluations and benchmarks.\\n\\nEvaluations. We do not consider the existing agent framework since we mainly focus on the base capabilities of various LLMs on this new benchmark. We anticipate that introducing additional reflection or a carefully designed agent framework may further boost the performance of original LLMs.\\n\\nBenchmarks. We mainly take advantage of existing task-oriented datasets to build our benchmark, which brings two limitations: 1) We mainly focus on text-based natural language interactions while the API also works at different modalities. We leave this in future work, and 2) We do not consider APPs with overlap or similar functions, but they exist in practice such as different platforms to buy tickets. We argue that these apps can often be distinguished through minor modifications to the app names and APIs. The specific choice of which app a user selects ultimately comes down to individual user preferences, which is outside the scope of this paper.\\n\\n9 Ethical Considerations\\nIn conducting our research, we have thoroughly reviewed and ensured compliance with ethical standards. Our study utilizes existing datasets, which have been publicly available and previously vetted for ethical use. These datasets have been carefully selected to avoid any form of offensive or biased content. Therefore, we consider that our research does not present any ethical issues. The data used is ethically sourced, the analysis is unbiased, and all procedures align with established ethical guidelines.\\n\\nReferences\\nAI@Meta. 2024. Llama 3 model card.\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang\"}"}
{"id": "emnlp-2024-main-856", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\\n\\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Raman, and Milica Ga\u0161i\u0107. 2018. Multiwoz\u2014a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv preprint arXiv:1810.00278.\\n\\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2024. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings.\\n\\nShijue Huang, Wanjun Zhong, Jianqiao Lu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, and Qun Liu. 2024a. Planning, creation, usage: Benchmarking llms for comprehensive tool utilization in real-world complex scenarios.\\n\\nYunpeng Huang, Jingwei Xu, Junyu Lai, Zixu Jiang, Taolue Chen, Zenan Li, Yuan Yao, Xiaoxing Ma, Lijuan Yang, Hao Chen, Shupeng Li, and Penghao Zhao. 2024b. Advancing transformer architecture in long-context large language models: A comprehensive survey.\\n\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1\u201338.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Menesch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b.\\n\\nMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-bank: A comprehensive benchmark for tool-augmented LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3102\u20133116, Singapore. Association for Computational Linguistics.\\n\\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023. Code as policies: Language model programs for embodied control.\\n\\nXiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023a. Webglm: Towards an efficient web-enhanced question answering system with human preferences. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 4549\u20134560.\\n\\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023b. Agent-bench: Evaluating llms as agents.\\n\\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models.\\n\\nChang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. 2024. Agentboard: An analytical evaluation board of multi-turn llm agents.\\n\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.\\n\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis.\\n\\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018a. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nXavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018b. Virtualhome: Simulating household activities via programs.\\n\\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023. Tool learning with foundation models.\\n\\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. In The Twelfth International Conference on Learning Representations.\\n\\nChangle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\"}"}
{"id": "emnlp-2024-main-856", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-856", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Prompt Details\\n\\nYour task is to generate a complex instruction in one sentence which exactly reflects what the user wants to do during the dialogue with the dialogue system as follows.\\n\\nPlease give all specific values of user requirements in user-aware arguments \\\\{user_aware_arguments\\\\}.\\n\\nYou should not know any values of other arguments specified by the system side.\\n\\nTable 6: The prompt used to prompt LLM to generate the summarized instruction\\n\\nPlease evaluate the given instruction based on the following criteria:\\n\\nFluency:\\n\\n\u2013 Evaluate the prompt's clarity, coherence, and ease of understanding.\\n\u2013 Consider factors such as the organization, language flow, and presentation of the prompt.\\n\\nDiversity:\\n\\n\u2013 Evaluate the range of topics, perspectives, and related APP and APIs covered by the prompt.\\n\\nPlease only output the overall score considering both fluency and diversity. The overall score should be a value between 1 and 10, with 10 representing the best.\\n\\nTable 7: Prompts to evaluate the quality of generated instructions.\\n\\nA.2 Data Statistics\\n\\nDefinition of Parallel and Sequential\\n\\nIn this section, we delve into the execution logical structure inherent in each data sample to have a better understanding of task complexity. We conceptualize the APIs used within a single data instance as nodes within a directed graph and the dependency among them as the directed edges. Consequently, we analyze the interrelations among all APIs within the data sample and construct a corresponding graph for each of them. It is important to notice that not all APIs within a sample are interdependent, and some may operate independently. As a result, the APIs within the same data sample generally form several distinct components. We treat each component as a unit to perform topological sorting. The execution process of each unit can be parallel, while the procedure within the component is sequential.\\n\\nAs shown in Figure 3, the illustrated MM sample needs to leverage 2 APIs from APP-1 and 3 APIs from APP-2 to fulfill the user instruction. Though APIs of APP-1 are interdependent, they do not need results from APIs of APP-2. Hence the formed graph of this sample has 2 components, with a size of 2 and 1. These two components can be fulfilled simultaneously, but the results from APIs of APP-1 or APP-2 need to be executed one by one.\\n\\nAbove all, to quantify the complexity inherent in these interactions, we compute both the average and maximum sizes of these components as sequential scale (Max. and Avg. Seq. in Table 2), which reflect the complexity of sequential dependency among the APIs. Additionally, we measure both the average and the maximum number of components within each sample (Max. and Avg. # Para. as the parallel scale, providing insight into the level of parallelism among the APIs or components.\\n\\nAs shown in Table 2, the instances of SS and SM are relatively simple. Since the samples of MS have the most complex sequential scales. The samples of MM are the most complex since their parallel and sequential scales are relatively larger than the others.\\n\\nB Experimental Details\\n\\n15333\"}"}
{"id": "emnlp-2024-main-856", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: List of All Apps and their corresponding APIs in the MetaBench.\\n\\nYour task is to determine the required App list according to the description of each App and user requirements.\\n\\nHere is the information about all accessible Apps:\\n\\nMake your response short and concise. Your ONLY need to return needed app names and your output MUST follow this format: \\\\[app1, app2, ...\\]\\n\\nUser Instruction: {user_instruction}\\n\\nTable 9: Prompts to select APP first.\\n\\n15334\"}"}
{"id": "emnlp-2024-main-856", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Your task is to generate App name and corresponding API calls to complete the user requirements according to given descriptions of all Apps and APIs. Here is the information about all accessible Apps and corresponding APIs. \\n\\nYour output should follow the format as follows:\\n\\napp1: \\\\[returned\\\\_argument1, returned\\\\_argument2, ... = app1\\\\_api1(#argument1=value1, #argument2=value2, ...)\\\\]\\n\\napp1: \\\\[returned\\\\_argument1, returned\\\\_argument2, ... = app1\\\\_api2(#argument1=value1, #argument2=value2, ...)\\\\]\\n\\napp2: \\\\[returned\\\\_argument1, returned\\\\_argument2, ... = app2\\\\_api1(#argument1=value1, #argument2=value2, ...)\\\\]\\n\\nHere are explanations:\\n\\n1. API Naming Convention\\n   - The API call format is \\\\[returned\\\\_argument1, returned\\\\_argument2, ... = app1\\\\_api1(#argument1=value1, #argument2=value2, ...)\\\\].\\n   - app1 signifies the name of app1, and app1\\\\_api1 signifies the name of api1 in the app1.\\n\\n2. Arguments\\n   - argument1 is the first input arguments for the corresponding api, and so on.\\n   - returned\\\\_argument1 is the first output arguments from the corresponding api, and so on.\\n   - Input arguments include both required and optional arguments as described in the corresponding API description of App.\\n   - The order and names of input and returned arguments must exactly match the given description.\\n\\n3. Values of Input Arguments\\n   - If specified by the user, replace the placeholder with the actual value.\\n   - If not specified by the user, omit the optional arguments from the API call.\\n   - If an argument value is dependent on another API's output, use the name of the returned argument as the value.\\n   - There are no default values for any arguments. All required arguments must be provided by the user or through dependencies on other APIs' outputs.\\n   - You should be careful about the date value, you need to infer it based on current date \\\"2019-03-01\\\".\\n\\n4. Order of Execution:\\n   - Execute APIs in a sequence that respects their dependencies. For example, if api2 requires an output from api1, ensure api1 is executed before api2.\\n   - Handle cases where multiple APIs' outputs are required for a single API's input by waiting for all dependent APIs to execute before calling the dependent API.\\n\\nExample:\\n\\nIf api2 in app1 depends on the output of api1 in app1 and an optional argument is not provided by the user:\\n\\napp1: \\\\[output1 = app1\\\\_api1(#argument1=value1)\\\\]\\n\\napp1: \\\\[output2 = app1\\\\_api2(#argument2=output1)\\\\]\\n\\nIf api3 in app2 requires outputs from both api1 in app1 and api2 in app1:\\n\\napp1: \\\\[output1 = app1\\\\_api1(#argument1=value1)\\\\]\\n\\napp1: \\\\[output2 = app1\\\\_api2(#argument2=output1)\\\\]\\n\\napp2: \\\\[output3 = app2\\\\_api3(#argument3=output1, #argument4=output2)\\\\]\\n\\nUser Instruction: {user\\\\_instruction}\\n\\nTable 10: Prompts to generate the final planning path to fulfill the user instruction.\\n\\n15335\"}"}
{"id": "emnlp-2024-main-856", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Models       | SS | SM | MS | MM | APP | API | APP | API | APP |\\n|--------------|----|----|----|----|-----|-----|-----|-----|-----|\\n| Mistral-7B   | 27.27 | 14.14 | 19.50 | 4.50 | 1.50 | 1.00 | 2.00 | 0.00   |\\n| Vicuna-13B   | 31.82 | 21.21 | 7.00 | 3.00 | 1.50 | 0.50 | 0.00 | 0.00   |\\n| LLaMA3-8B    | 47.98 | 47.47 | 19.00 | 17.50 | 12.50 | 9.50 | 4.50 | 5.50   |\\n| LLaMA3-70B   | 60.94 | 58.33 | 51.00 | 49.00 | 12.00 | 6.50 | 16.00 | 8.50   |\\n| QWen1.5-7B   | 28.28 | 12.63 | 11.50 | 4.00 | 2.50 | 0.50 | 4.00 | 1.50   |\\n| QWen1.5-14B  | 56.57 | 41.92 | 10.50 | 10.00 | 5.60 | 4.00 | 1.50 | 1.50   |\\n| QWen1.5-72B  | 71.88 | 43.29 | 38.50 | 9.50 | 2.47 | 1.85 | 4.50 | 3.50   |\\n| GPT-3.5      | 44.44 | 52.02 | 30.50 | 31.00 | 31.00 | 19.00 | 18.50 | 19.50  |\\n| GPT-4       | 79.59 | 78.06 | 55.50 | 51.50 | 35.50 | 26.50 | 29.50 | 24.00   |\\n\\nTable 11: The Exact Match (EM) results of different LLMs on MetaBench. Bold highlights the best score among all models, and underline underscores the best score under the same model scale.\"}"}
