{"id": "emnlp-2022-main-531", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models\\n\\nHongyu Zhu, Yan Chen, Jing Yan, Jing Liu, Yu Hong, Ying Chen, Hua Wu, Haifeng Wang\\n\\nSchool of Computer Science and Technology, Soochow University, China\\nBaidu Inc., Beijing, China\\n{hines.zhu, tianxianer}@gmail.com\\n{chenyan22, yanjing09, liujing46, chenying04, wu_hua, wanghaifeng}@baidu.com\\n\\nAbstract\\nIn this paper, we focus on the robustness evaluation of Chinese Question Matching (QM) models. Most of the previous work on analyzing robustness issues focus on just one or a few types of artificial adversarial examples. Instead, we argue that a comprehensive evaluation should be conducted on natural texts, which takes into account the fine-grained linguistic capabilities of QM models. For this purpose, we create a Chinese dataset namely DuQM which contains natural questions with linguistic perturbations to evaluate the robustness of QM models. DuQM contains 3 categories and 13 subcategories with 32 linguistic perturbations. The extensive experiments demonstrate that DuQM has a better ability to distinguish different models. Importantly, the detailed breakdown of evaluation by the linguistic phenomenon in DuQM helps us easily diagnose the strength and weakness of different models. Additionally, our experiment results show that the effect of artificial adversarial examples does not work on natural texts. Our baseline codes and a leaderboard are now publicly available.\\n\\n1 Introduction\\nThe task of Question Matching (QM) aims to identify the question pairs that have the same meaning, and it has been widely used in many applications, e.g., community question answering and intelligent customer services, etc. Though neural QM models have shown compelling performance on various datasets, including Quora Question Pairs (QQP) (Iyer et al., 2017), LCQMC (Liu et al., 2018), BQ (Chen et al., 2018) and AFQMC, neural models are often not robust to adversarial examples, which means that the neural models predict unexpected outputs given just a small perturbation on the inputs. As the example 1 in Tab. 1 shows, a model might not distinguish the minor difference (\\\"\u9762 noodles\\\") between the two sentences, and thus predicts the two questions semantically equivalent. Recently, it attracts a lot of attentions from the research community to deal with the robustness issues of neural models on various NLP tasks, such as question matching, natural language inference and machine reading comprehension. Early works examine the robustness of neural models by creating certain types of artificial adversarial examples (Jia and Liang, 2017; Alzantot et al., 2018; Ren et al., 2019; Jin et al., 2020), and involving human-and-model-in-the-loop to create dynamic adversarial examples (Nie et al., 2020; Wallace et al., 2019). Further studies discover that a few types of superficial cues (i.e. shortcuts) in the training data, are learned by the models and hence affect the model robustness (Gururangan et al., 2018; McCoy et al., 2019; Lai et al., 2021). Besides, several studies try to improve the robustness of the neural models by adversarial data augmentation (Min et al., 2020) and data filtering (Bras et al., 2020). All these efforts motivate us to better find and fix the robustness issues.\\n\\nHowever, there are several limitations in previous studies. Firstly, the analysis and evaluation in previous work focus on just one or a few types of adversarial examples or shortcuts, but we need normative evaluation (Linzen, 2020; Ettinger, 2020; Phang et al.). In the normative evaluation (Linzen, 2020), the objective is not to fool the system by exploiting its particular weaknesses, but to comprehensively evaluate the basic linguistic capabilities of the models with a variety of systemically controlled datasets. Checklist (Ribeiro et al., 2020), QuAIL (Rogers et al., 2020) and Textflint (Wang et al., 2021) are great attempts of normative evaluation.\"}"}
{"id": "emnlp-2022-main-531", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, it is not clear that if the artificial adversarial training is effective on natural texts from real-world applications (Morris et al., 2020). Some other works manually perturb the examples to construct natural examples, but the manual perturbation is time consuming and costly (Gardner et al., 2020). Moreover, to the best of our knowledge, there are few Chinese datasets for QM robustness evaluation.\\n\\nTowards this end, we create an open-domain Chinese dataset namely DuQM containing natural questions with linguistic perturbation for evaluating the robustness of QM models. (1) By linguistic, we mean that the dataset provides a detailed breakdown of evaluation by linguistic phenomenon. As shown in Tab. 1, there are 3 categories and 13 subcategories with 32 linguistic perturbations in DuQM, which enables us to evaluate the model performance by each category instead of just a single metric. (2) By natural, we mean all the questions in DuQM are natural, that are issued by the users in Baidu search. This design can help us to properly evaluate model\u2019s robustness on natural texts rather than artificial texts, which may not preserve semantics and the distribution of which is far from real-world applications.\\n\\nThe contributions of this paper can be summarized as follows:\\n\u2022 We construct a Chinese dataset namely DuQM that contains linguistically perturbed natural questions from Baidu search. It is a systemically controlled dataset to test various linguistic capabilities of the models (see Sec. 2). Additionally, except for few categories, most of the categories\u2019 construction methods can be easily extended to other languages (see Sec. 3).\\n\u2022 Our experimental results reveal three characteristics of DuQM: (1) DuQM is challenging, and has better discrimination power to distinguish the models that perform comparably on other datasets (see Sec. 4.2). (2) The detailed breakdown of evaluation by linguistic phenomena in DuQM helps diagnose the advantages and disadvantages of different models (see Sec. 4.3). (3) The whole DuQM dataset composed of natural examples. Our experimental result shows that the artificial adversarial training fails in natural texts of DuQM. DuQM can help us properly evaluate the models\u2019 robustness (see Sec. 4.4).\\n\\nThe remaining of this paper is organized as follows. Sec. 2 describes the 3 categories and 13 subcategories with 32 linguistic perturbations in DuQM. Sec. 3 gives the construction process of DuQM. In Sec. 4, we conduct experiments to demonstrate 3 characteristics of DuQM. We conclude our work in Sec. 5.\\n\\n2 Linguistic Perturbations in DuQM\\nThe design of DuQM is aimed at normative evaluation, that contains a detailed breakdown of evaluation by linguistic phenomenon. Hence, we create DuQM by introducing a set of linguistic features that we believe are important for model diagnosis in terms of linguistic capabilities. Basically, 3 categories of linguistic features are used to build DuQM, i.e., lexical features (see Sec. 2.1), syntactic features (see Sec. 2.2), and pragmatic features (see Sec. 2.3). We list 3 categories, 13 subcategories with 32 operations of perturbation in Tab. 1. The detailed descriptions of all categories are given in this section.\\n\\n2.1 Lexical Features\\nLexical features are associated with vocabulary items, i.e. words. As a word is the smallest independent but meaningful unit of speech, an operation on a single word may change the meaning of the entire sentence. It is a basic but crucial capability for models to understand word and perceive word-level perturbations. To provide a fine-grained evaluation for model\u2019s capability of lexical understanding, we further consider 6 subcategories: Part of Speech. Parts of speech (POS), or word classes, describe the part a word plays in a sentence. DuQM considers 6 POS in Chinese grammar, including noun, verb, adjective, adverb, numeral and quantifier, which are content words that carry most meaning of a sentence. In this subcategory, we aim to test the models\u2019 understanding of words with different POSs by replacing them with related but not identical words. As the example 1 in Tab. 1 shows, inserting only one noun \u201c\u9762\u6761\u201d makes the sentence meaning different. Furthermore, in this subcategory we provides a set of examples focusing on phrase-level perturbations to check model\u2019s capability on understanding word groups that act collectively as a single part of speech (see Exp. 11).\\n\\nNamed Entity. Different from common nouns that refer to generic things, a named entity (NE) is a proper noun which refers to a specific real-world object. The close relation to world knowledge makes NE ideal for observing models\u2019 understandings. However, it is not clear that if the artificial adversarial training is effective on natural texts from real-world applications (Morris et al., 2020). Some other works manually perturb the examples to construct natural examples, but the manual perturbation is time consuming and costly (Gardner et al., 2020). Moreover, to the best of our knowledge, there are few Chinese datasets for QM robustness evaluation.\\n\\nTowards this end, we create an open-domain Chinese dataset namely DuQM containing natural questions with linguistic perturbation for evaluating the robustness of QM models. (1) By linguistic, we mean that the dataset provides a detailed breakdown of evaluation by linguistic phenomenon. As shown in Tab. 1, there are 3 categories and 13 subcategories with 32 linguistic perturbations in DuQM, which enables us to evaluate the model performance by each category instead of just a single metric. (2) By natural, we mean all the questions in DuQM are natural, that are issued by the users in Baidu search. This design can help us to properly evaluate model\u2019s robustness on natural texts rather than artificial texts, which may not preserve semantics and the distribution of which is far from real-world applications.\\n\\nThe contributions of this paper can be summarized as follows:\\n\u2022 We construct a Chinese dataset namely DuQM that contains linguistically perturbed natural questions from Baidu search. It is a systemically controlled dataset to test various linguistic capabilities of the models (see Sec. 2). Additionally, except for few categories, most of the categories\u2019 construction methods can be easily extended to other languages (see Sec. 3).\\n\u2022 Our experimental results reveal three characteristics of DuQM: (1) DuQM is challenging, and has better discrimination power to distinguish the models that perform comparably on other datasets (see Sec. 4.2). (2) The detailed breakdown of evaluation by linguistic phenomena in DuQM helps diagnose the advantages and disadvantages of different models (see Sec. 4.3). (3) The whole DuQM dataset composed of natural examples. Our experimental result shows that the artificial adversarial training fails in natural texts of DuQM. DuQM can help us properly evaluate the models\u2019 robustness (see Sec. 4.4).\\n\\nThe remaining of this paper is organized as follows. Sec. 2 describes the 3 categories and 13 subcategories with 32 linguistic perturbations in DuQM. Sec. 3 gives the construction process of DuQM. In Sec. 4, we conduct experiments to demonstrate 3 characteristics of DuQM. We conclude our work in Sec. 5.\\n\\n2 Linguistic Perturbations in DuQM\\nThe design of DuQM is aimed at normative evaluation, that contains a detailed breakdown of evaluation by linguistic phenomenon. Hence, we create DuQM by introducing a set of linguistic features that we believe are important for model diagnosis in terms of linguistic capabilities. Basically, 3 categories of linguistic features are used to build DuQM, i.e., lexical features (see Sec. 2.1), syntactic features (see Sec. 2.2), and pragmatic features (see Sec. 2.3). We list 3 categories, 13 subcategories with 32 operations of perturbation in Tab. 1. The detailed descriptions of all categories are given in this section.\\n\\n2.1 Lexical Features\\nLexical features are associated with vocabulary items, i.e. words. As a word is the smallest independent but meaningful unit of speech, an operation on a single word may change the meaning of the entire sentence. It is a basic but crucial capability for models to understand word and perceive word-level perturbations. To provide a fine-grained evaluation for model\u2019s capability of lexical understanding, we further consider 6 subcategories: Part of Speech. Parts of speech (POS), or word classes, describe the part a word plays in a sentence. DuQM considers 6 POS in Chinese grammar, including noun, verb, adjective, adverb, numeral and quantifier, which are content words that carry most meaning of a sentence. In this subcategory, we aim to test the models\u2019 understanding of words with different POSs by replacing them with related but not identical words. As the example 1 in Tab. 1 shows, inserting only one noun \u201c\u9762\u6761\u201d makes the sentence meaning different. Furthermore, in this subcategory we provides a set of examples focusing on phrase-level perturbations to check model\u2019s capability on understanding word groups that act collectively as a single part of speech (see Exp. 11).\\n\\nNamed Entity. Different from common nouns that refer to generic things, a named entity (NE) is a proper noun which refers to a specific real-world object. The close relation to world knowledge makes NE ideal for observing models\u2019 understandings. However, it is not clear that if the artificial adversarial training is effective on natural texts from real-world applications (Morris et al., 2020). Some other works manually perturb the examples to construct natural examples, but the manual perturbation is time consuming and costly (Gardner et al., 2020). Moreover, to the best of our knowledge, there are few Chinese datasets for QM robustness evaluation.\\n\\nTowards this end, we create an open-domain Chinese dataset namely DuQM containing natural questions with linguistic perturbation for evaluating the robustness of QM models. (1) By linguistic, we mean that the dataset provides a detailed breakdown of evaluation by linguistic phenomenon. As shown in Tab. 1, there are 3 categories and 13 subcategories with 32 linguistic perturbations in DuQM, which enables us to evaluate the model performance by each category instead of just a single metric. (2) By natural, we mean all the questions in DuQM are natural, that are issued by the users in Baidu search. This design can help us to properly evaluate model\u2019s robustness on natural texts rather than artificial texts, which may not preserve semantics and the distribution of which is far from real-world applications.\\n\\nThe contributions of this paper can be summarized as follows:\\n\u2022 We construct a Chinese dataset namely DuQM that contains linguistically perturbed natural questions from Baidu search. It is a systemically controlled dataset to test various linguistic capabilities of the models (see Sec. 2). Additionally, except for few categories, most of the categories\u2019 construction methods can be easily extended to other languages (see Sec. 3).\\n\u2022 Our experimental results reveal three characteristics of DuQM: (1) DuQM is challenging, and has better discrimination power to distinguish the models that perform comparably on other datasets (see Sec. 4.2). (2) The detailed breakdown of evaluation by linguistic phenomena in DuQM helps diagnose the advantages and disadvantages of different models (see Sec. 4.3). (3) The whole DuQM dataset composed of natural examples. Our experimental result shows that the artificial adversarial training fails in natural texts of DuQM. DuQM can help us properly evaluate the models\u2019 robustness (see Sec. 4.4).\\n\\nThe remaining of this paper is organized as follows. Sec. 2 describes the 3 categories and 13 subcategories with 32 linguistic perturbations in DuQM. Sec. 3 gives the construction process of DuQM. In Sec. 4, we conduct experiments to demonstrate 3 characteristics of DuQM. We conclude our work in Sec. 5.\\n\\n2 Linguistic Perturbations in DuQM\\nThe design of DuQM is aimed at normative evaluation, that contains a detailed breakdown of evaluation by linguistic phenomenon. Hence, we create DuQM by introducing a set of linguistic features that we believe are important for model diagnosis in terms of linguistic capabilities. Basically, 3 categories of linguistic features are used to build DuQM, i.e., lexical features (see Sec. 2.1), syntactic features (see Sec. 2.2), and pragmatic features (see Sec. 2.3). We list 3 categories, 13 subcategories with 32 operations of perturbation in Tab. 1. The detailed descriptions of all categories are given in this section.\\n\\n2.1 Lexical Features\\nLexical features are associated with vocabulary items, i.e. words. As a word is the smallest independent but meaningful unit of speech, an operation on a single word may change the meaning of the entire sentence. It is a basic but crucial capability for models to understand word and perceive word-level perturbations. To provide a fine-grained evaluation for model\u2019s capability of lexical understanding, we further consider 6 subcategories: Part of Speech. Parts of speech (POS), or word classes, describe the part a word plays in a sentence. DuQM considers 6 POS in Chinese grammar, including noun, verb, adjective, adverb, numeral and quantifier, which are content words that carry most meaning of a sentence. In this subcategory, we aim to test the models\u2019 understanding of words with different POSs by replacing them with related but not identical words. As the example 1 in Tab. 1 shows, inserting only one noun \u201c\u9762\u6761\u201d makes the sentence meaning different. Furthermore, in this subcategory we provides a set of examples focusing on phrase-level perturbations to check model\u2019s capability on understanding word groups that act collectively as a single part of speech (see Exp. 11).\\n\\nNamed Entity. Different from common nouns that refer to generic things, a named entity (NE) is a proper noun which refers to a specific real-world object. The close relation to world knowledge makes NE ideal for observing models\u2019 understandings. However, it is not clear that if the artificial adversarial training is effective on natural texts from real-world applications (Morris et al., 2020). Some other works manually perturb the examples to construct natural examples, but the manual perturbation is time consuming and costly (Gardner et al., 2020). Moreover, to the best of our knowledge, there are few Chinese datasets for QM robustness evaluation.\"}"}
{"id": "emnlp-2022-main-531", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Categories of DuQM (described in Sec. 2) and performance of 6 models on each subcategory (discussed in Sec. 4).\\n\\n| Category                   | Total          | Pragmatic Feature | Lexical Feature | Discourse Particle(Complex) | Asymmetry | Symmetry | PerturbationOperation |\\n|----------------------------|----------------|-------------------|-----------------|------------------------------|-----------|----------|-----------------------|\\n|                            |                |                   |                 |                              |           |          |                       |\\n|                            |                |                   |                 |                              |           |          | Synonym               |\\n|                            |                |                   |                 |                              |           |          | Antonym               |\\n|                            |                |                   |                 |                              |           |          | Negation              |\\n|                            |                |                   |                 |                              |           |          | NegativeAsymmetry     |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\n|                            |                |                   |                 |                              |           |          | Replace + Negate      |\\"}
{"id": "emnlp-2022-main-531", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"standing of the meaning of names and background knowledge about entities. Thus, we include Named Entity as an independent subcategory to test the model's behavior of named entity recognition, and focus on 4 types of NE most commonly seen, i.e., location, organization, person and product. Example 12 is a search query and its perturbation on NE. The two named entities, \\\"\u5c71\u897fShanxi\\\" and \\\"\u9655\u897fShaanxi\\\", are similar at character level but denote two different locations. We expect that the models can capture the subtle difference.\\n\\nSynonym. A synonym is a word or phrase that means exactly or nearly the same as another word or phrase in a given language. This subcategory aims to test whether models can identify two semantically equivalent questions whose surface forms only differ in a pair of synonyms. As in example 16, the two sentences differ only in two words, both of which refer to Kiwifruit, and has the same meaning.\\n\\nAntonym. In contrast to synonyms, antonyms are words within an inherently incompatible binary relationship. This subcategory examines model\u2019s capability on distinguishing words with opposed meanings. We mainly focus on adjective\u2019s opposite, e.g., \\\"\u9ad8high\\\" and \\\"\u4f4elow\\\" (see example 20).\\n\\nNegation. Negation is another way to express contradiction. To negate a verb or an adjective in Chinese, we normally put a negative before it, e.g., \\\"\u4e0dnot\\\" before \\\"\u54edcry\\\" (example 21), \\\"\u4e0d\u662fnot\\\" before \\\"\u7ea2\u7684red\\\" (example 22). The negative before the verb or the adjective negates the statement. It is an effective way to analyze model\u2019s basic skill of figuring out the contradictory meanings even there is only a minor change. Moreover, we include some equivalent paraphrases with negation in this subcategory. In example 23, \\\"\u65e0\u6cd5\u5e73\u9759can\u2019t calm down\\\" is the negative paraphrase of \\\"\u6fc0\u52a8excited\\\", so that the paraphrase sentence is equivalent to the positive sentence. We believe that a robust QM system should be able to recognize this kind of paraphrase question pairs.\\n\\nTemporal Word. Temporal reasoning is the relatively higher-level linguistic capability that allows the model to reason about time. Unlike English, verbs in Chinese do not have morphological inflections. Tenses and aspects are expressed either by temporal noun phrases like \\\"\u660e\u5929tomorrow\\\" (examples 24) or by aspect particles like \\\"\u4e86le\\\" which indicates the completion of an action (example 25). This subcategory focuses on the temporal distinctions and helps us evaluate the models\u2019 temporal reasoning capability.\\n\\n2.2 Syntactic Features\\nWhile single word sense is important to question meaning, how words composed together into a whole also affects sentence understanding. We believe that the relations among words in a sentence is important for models to capture, so we focus on several types of syntactic features in this category.\\n\\nWe pre-define 4 linguistic phenomena that we believe is meaningful to locate model\u2019s strength and weakness, and introduce them in this subsection.\\n\\nSymmetry. Sometimes paraphrases can be generated by only swapping two conjuncts around in a structure of coordination. As shown in example 26, \\\"\u9c7cfish\\\" and \\\"\u9e21\u86cbegg\\\" are joined together by the conjunction \\\"\u548cand\\\", which have the symmetric relation to each other. Even if we swap them around, the sentence meaning will not change. We name this subcategory Symmetry.\\n\\nAsymmetry. Some words (such as \\\"\u548cand\\\") denote symmetric relations, while others (for example, preposition \\\"\u5230to\\\") denote asymmetric. Example 27 shows a sentence pair in which the word before the preposition \\\"\u5230to\\\" is an adverbial and the word after it is the object. Swapping around the adverbial and the object of the prepositional phrase will definitely leads to a nonequivalent meaning. If a model performs well only on subcategory Symmetry or Asymmetry, it may rely on shortcuts instead of the understanding of the syntactic information.\\n\\nNegative Asymmetry. To further explore the syntactic capability of QM model, DuQM includes a set of test examples which consider both syntactic asymmetry and antonym, and we name this category Negative Asymmetry. In example 28, the asymmetric relation between \\\"\u7537\u4ebamen\\\" and \\\"\u5973\u4ebawomen\\\" and the opposite meaning of \\\"\u9ad8taller\\\" and \\\"\u77eeshorter\\\" resolve to an equivalent meaning. With this subcategory, we can better explore model\u2019s capability of inferring more complex syntactic structure.\\n\\nVoice. Another crucial syntactic capability of models is to differentiate active and passive voices. In Chinese, the most common way to express the passive voice is using Bei-constructions which feature an agentive case marker \\\"\u88abbei\\\". The subject of a Bei-construction is the patient of an action, and the object of the preposition \\\"\u88abbei\\\" is the agent. Compared to Fig.2(a) (in Appendix A), the additional...\"}"}
{"id": "emnlp-2022-main-531", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.3 Pragmatic Features\\n\\nLexical items ordered by syntactic rules are not all that make a sentence mean what it means. Context, or the communicative situation that influence language use, has a part to play. We include some pragmatic features in DuQM so as to observe whether models are able to understand the contextual meaning of sentences.\\n\\nMisspelling.\\n\\nMisspellings are quite often seen by search engines and question-answering systems, which are mostly unintentional. Models should have the capability to capture the true intention of the questions with spelling errors to ensure the robustness. In example 30, despite the misspelled word \\\"\u6587\u8eabtatoo\\\", the two questions mean the same, In some real world situations, models should understand misspellings appropriately. For example, when users search a query but type in misspelling, a robust model will still give the correct result.\\n\\nDiscourse Particle.\\n\\nDiscourse particles are words and small expressions that contribute little to the information the sentence conveys, but play some pragmatic functions such as showing politeness, drawing attention, smoothing utterance, etc. As shown in example 32, the word \\\"\u6c42help\\\" is used to draw attention and brings no additional information to the sentence. Whether using these little words does not change the sentence meaning. It is necessary to a model to identify the semantic equivalency when such words are used.\\n\\n3 Construction\\n\\nWe design DuQM as a diverse and natural corpus. The construction process of DuQM is divided into 4 steps and illustrated in Fig. 1. Firstly, we pre-process the source questions to obtain their linguistic knowledge, which will be used to perturb the source texts. Then we pair the source and perturbed question as an example. The examples' naturalness is reviewed manually. At last, the examples are annotated manually and DuQM is finally constructed. We will introduce the construction details in this section.\\n\\n3.1 Linguistic Preprocessing\\n\\nWe collect a large number of source questions from the search query log of Baidu search, and filter out question sentences with a question identification model (the accuracy is higher than 95%). All the source questions are natural that users have entered into Baidu search and then we perform several linguistic preprocessings on them: named entity recognition, POS tagging, dependency parsing, and word importance analysis. The linguistic knowledge of the source questions we obtained in this step will be used for perturbation.\\n\\n3.2 Perturbation.\\n\\nWe conduct different perturbation operations for different subcategories. In general, we perturb the sentences in three ways:\\n\\n\u2022 replace: replace a word with another word, e.g., for category Synonym, we replace one word with its synonym;\\n\u2022 insert: insert an additional word, e.g., for category Temporal Word, we insert temporal word to the source question;\\n\u2022 swap: swap two words. This operation is only used in Syntactic Feature.\\n\\nThe perturbation for each linguistic category is listed in column Perturbation Operation of Tab. 1, and the perturbation details are as follows:\"}"}
{"id": "emnlp-2022-main-531", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lexical Features. For each source question, we select the word with specific POS tag or entity type and high word importance score as target word, and perturb the source questions with some other words we collect from following 4 sources:\\n\\n\u2022 Elasticsearch: to collect words which have high character overlap with target words; Elasticsearch uses similarity ranking (relevancy) algorithm (BM25 in default) to build a search engine. Hence, we can easily obtain high character overlap with target words with it.\\n\\n\u2022 Faiss: to collect words which are semantically similar to target words; Specifically, we use RocketQA to train a question dense retrieval model and employ it by faiss for similarity search.\\n\\n\u2022 Bigcilin: to collect synonym of target words.\\n\\n\u2022 Baidu Hanyu: to collect antonym and synonym of target words.\\n\\n\u2022 XLM-RoBERTa (Conneau et al., 2020): to insert additional words to source sentences.\\n\\n\u2022 Vocabulary lists: to insert some specific words, such as negation word and temporal word.\\n\\nSyntactic Features. For Symmetry and Asymmetry, we retrieve the source questions from the search log and select the questions whose edit distance to source question is equal to 4 as candidate questions. Then we compare the dependency structures of the source question and candidate questions. Only the question pairs which contain symmetric or asymmetric relations are retained. To generate examples for Negative Asymmetry, from Asymmetry we select the example pairs, one side of which can be negated, and negate one side of the pairs. The asymmetric syntactic structure of two sentences and one-sided negation resolves to a positive meaning. For Voice, we add \\\"\u88ab\\\" word to source questions to conduct a change of voice.\\n\\nPragmatic Features. Misspelling. With a Chinese heteronym list, we obtain a set of common typos and substitute the correct-spelling words with typos. Additionally, the perturbation should satisfy two constraints: 1) the typos should be commonly used Chinese characters; 2) only one character in the source sentence is replaced with its typo.\\n\\nDiscourse Particle. We construct this category in 2 ways: 1) we replace or add some question words, auxiliary words or punctuation marks to generate Simple Discourse Particle examples (Discourse Particle (Simple) in Tab. 1); 2) for Complex Discourse Particle examples (Discourse Particle (Complex) in Tab. 1), we select some question pairs from a Frequently-Asked-Questions (FAQ) log, especially pairs with big differences in sentence length. Then the pairs are annotated manually and we retained the positive examples.\\n\\nWith above approaches, we perturb the source questions and obtain a large set of question pairs. Then the generated question pairs are manually reviewed in terms of naturalness and quality.\\n\\n3.3 Naturalness Review\\nTo ensure the generated sentences are natural, we examine their appearances in the search log and only retain the sentences which have been entered into Baidu search. Then the source question and generated question are paired together as an example.\\n\\n3.4 Manual Annotation\\nTo ensure the quality, linguistic experts from our internal data team evaluate the examples in terms of fluent, grammatically correct, and correctly categorized. The low-quality examples are discarded and the examples with inappropriate categories are re-classified. Notably, examples of different sub-categories are not overlapped, as we re-classified data categories and guarantee each example has one category.\\n\\nThen the question pairs are annotated by the annotators. Semantically equivalent question pairs are positive examples, and inequivalent pairs are negative. Each example is annotated by three annotators, and the examples will be tagged with the majority label given by the annotators. To further ensure the annotation quality, 10% of the annotated examples are selected randomly and reviewed by another senior linguistic expert and if the review accuracy is lower than 95%, the annotators need to re-annotate all the examples until...\"}"}
{"id": "emnlp-2022-main-531", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Data statistics of DuQM.\\n\\n| Category     | Length | #       | Y     | N     | All    |\\n|--------------|--------|---------|-------|-------|--------|\\n| **Lexical**  | 8.58   | 8.89    | 1,315 | 6,784 | 8,099  |\\n| **Syntactic**| 9.86   | 9.89    | 678   | 532   | 1,210  |\\n| **Pragmatic**| 8.73   | 9.03    | 812   | 0     | 812    |\\n\\nTable 3: Accuracy(%) on LCQMC test and DuQM.\\n\\n| Model  | LCQMC test | DuQM | \u25b3   |\\n|--------|------------|------|-----|\\n| BERT  | 87.1\u00b10.1   | 66.6\u00b10.6 | -20.5 |\\n| ERNIE | 87.3\u00b10.1   | 69.8\u00b10.3 | -17.5 |\\n| RoBERTa | 87.2\u00b10.4 | 69.5\u00b10.1 | -17.7 |\\n| MacBERT | 87.4\u00b10.3 | 70.3\u00b10.6 | -17.1 |\\n| RoBERTa | 87.7\u00b10.1 | 73.8\u00b10.3 | -13.9 |\\n| MacBERT | 87.6\u00b10.1 | 73.8\u00b10.5 | -13.8 |\\n\\nb indicates base, and l indicates large.\\n\\nOut-of-domain (ood) test set of LCQMC, so that models' low performance on DuQM could not be attributed to being ood.\\n\\n4.2 Char. 1: DuQM is Challenging and with Better Discrimination Ability\\n\\nTab. 3 shows the performances of models on held-out set LCQMC test and our DuQM, which presents the primary characteristic of DuQM: it is challenging and can better discriminate models' abilities.\\n\\nAs shown in Tab. 3, all models achieve accuracy higher than 87% on LCQMC test, but show a significant performance drop on DuQM. Column \u25b3 in Tab. 3 shows the differences between models' performances on LCQMC test and DuQM, which presents that the performance on DuQM is lower than on LCQMC test by at most 20.5%. This indicates that DuQM is more challenging, and we claim that a challenging dataset could better distinguish the models' performance. As shown in Tab. 3, all the models have similar performances...\"}"}
{"id": "emnlp-2022-main-531", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: The micro-averaged and macro-averaged accuracy are on each category of DuQM.\\n\\n| Model    | Lexical | Syntactic | Pragmatic | DuQM | POS | NE | Synonym | Antonym | Negation | Temporal | Lexical |\\n|----------|---------|-----------|-----------|------|-----|----|---------|---------|----------|----------|---------|\\n| BERT     |         |           |           |      |     |    |         |         |          |          |         |\\n| micro    | 62.1\u00b11.1| 92.3\u00b10.5  | 69.5\u00b10.4  | 50.6\u00b13.4 | 64.4\u00b15.9 | 35.1\u00b13.3 |         |         |          | 67.2\u00b10.7 |         |\\n| macro    | 51.9\u00b11.5| 91.2\u00b10.7  | 76.4\u00b10.6  | 50.6\u00b13.4 | 57.6\u00b14.4 | 35.5\u00b13.3 |         |         |          | 61.4\u00b11.2 |         |\\n| ERNIE    |         |           |           |      |     |    |         |         |          |          |         |\\n| micro    | 64.6\u00b10.5| 92.8\u00b10.4  | 73.2\u00b10.9  | 69.6\u00b12.9 | 77.8\u00b11.1 | 48.0\u00b11.9 |         |         |          | 71.0\u00b10.3 |         |\\n| macro    | 52.4\u00b10.7| 92.3\u00b10.6  | 79.5\u00b10.7  | 69.6\u00b12.9 | 69.1\u00b11.2 | 48.5\u00b11.9 |         |         |          | 65.5\u00b10.5 |         |\\n| RoBERTa  |         |           |           |      |     |    |         |         |          |          |         |\\n| micro    | 64.2\u00b10.1| 90.6\u00b11.8  | 74.2\u00b11.4  | 65.0\u00b11.5 | 76.3\u00b11.7 | 43.7\u00b10.2 |         |         |          | 70.1\u00b10.1 |         |\\n| macro    | 53.3\u00b10.2| 89.4\u00b12.5  | 80.3\u00b11.1  | 65.0\u00b11.5 | 68.8\u00b11.3 | 44.0\u00b10.2 |         |         |          | 65.0\u00b10.1 |         |\\n| MacBERT  |         |           |           |      |     |    |         |         |          |          |         |\\n| micro    | 64.8\u00b11.1| 92.0\u00b10.7  | 73.3\u00b11.1  | 73.1\u00b14.3 | 80.7\u00b10.5 | 47.6\u00b11.3 |         |         |          | 71.2\u00b10.7 |         |\\n| macro    | 54.2\u00b10.9| 90.7\u00b10.6  | 79.7\u00b10.5  | 73.1\u00b14.6 | 70.7\u00b10.1 | 47.7\u00b10.2 |         |         |          | 66.3\u00b10.2 |         |\\n\\nTable 5: Statistics of the adversarial examples.\\n\\n| Method     | Train | Test |\\n|------------|-------|------|\\n| PWWS nat   | 159,503 | 400 |\\n| FOOLER nat | 64,086  | 200 |\\n| CHECKLIST nat | 400 |      |\\n\\nOn LCQMC test (around 87%), but different performance on DuQM: the accuracy of base models differs from 66.6% to 70.3%, and the large models show higher performance (73.8%). In conclusion, DuQM shows a better discrimination ability to evaluate models.\\n\\n4.3 Char. 2: Diagnose Model in Diverse Ways\\n\\nDuQM is a corpus which has 3 linguistic categories and 13 subcategories and enables a detailed breakdown of evaluation on different linguistic phenomena. In Tab. 1, we give the performance of 6 models on all fine-grained categories of DuQM, and Tab. 4 reports the micro-averaged and macro-averaged accuracy. By comparing these results, we introduce the second characteristic of DuQM: it can diagnose the strengths and weaknesses of the models in diverse ways. Several interesting observations are noticed: (from Tab. 1 and 4):\\n\\n1) In most categories, large models outperform base models. As the large models have more parameters and larger pre-training corpus, it is reasonable that they have better capabilities than relatively smaller models.\\n\\n2) In Named Entity, all models show good performance (higher than 90%). Another interesting finding is that although ERNIE b is a relatively small model, it performs slightly better than RoBERTa l on this subcategory, which might attribute to the entity masking strategy during pre-training.\\n\\n3) MacBERT l is significantly better than other models on Synonym. We suppose that it benefits from its pre-training strategy that using similar words instead of random words for masking. Moreover, RoBERTa l and MacBERT l have remarkable better performance on Antonym.\\n\\n4) Low performance in Temporal word show that all models lack the ability of temporal reasoning.\\n\\n5) All models have surprisingly poor performance on Asymmetry while good performance on Symmetry. We suppose that lack of learning word orders would result in a wrong prediction when the words orders are altered.\\n\\n6) BERT b and ERNIE b perform better on Misspelling, and RoBERTa b and MacBERT b are relatively better on Complex Discourse Particles.\\n\\nIn general, DuQM diagnoses models from a linguistic perspective and can help us identify the strengths and weaknesses of the models.\\n\\n4.4 Char. 3: Natural Examples\\n\\nDuQM is composed of adversarial testing examples generated by linguistically perturbed natural questions. We consider that natural examples can better evaluate models' robustness than artificial examples. To demonstrate it, we conduct an experiment to compare the performance of two adversarial training (AT) methods PWWS (Ren et al., 2019) and TextFooler (Jin et al., 2020) on artificial and natural test examples:\\n\\nAn adversarial example is an input to a machine learning model that is purposely designed to cause a model to make a mistake in its predictions despite resembling a valid input to a human. As they are designed to evaluate different linguistic capabilities of the modes, all examples in DuQM are adversarial examples.\"}"}
{"id": "emnlp-2022-main-531", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Training set | LCQMC | Attack test set |\\n|--------------|-------|----------------|\\n| CHECKLISTnat | DuQM  |\\n| PWWS PWWS nat FOOLER FOOLER nat | Micro Macro |\\n| LCQMC 87.7  | 58.1  | 81.5  |\\n| 57.1        | 87.8  |\\n| 76.9        | 73.8  |\\n| 69.8        |       |\\n| LCQMC+PWWS  | 87.7  | 0.0  |\\n| 97.6        | 39.5  |\\n| 81.8        | 0.3   |\\n| 73.1        | 16.0  |\\n| 87.6        | -0.2  |\\n| 76.0        | -0.9  |\\n| 75.2        |       |\\n| LCQMC+FOOLER | 87.5  | -0.2 |\\n| 78.5        | 20.4  |\\n| 83.8        | 2.3   |\\n| 80.8        | 23.7  |\\n| 82.0        | -5.8  |\\n| 79.2        | 2.3   |\\n| 71.4        | -2.4  |\\n| 68.8        | -1.0  |\\n\\nTable 6: Adversarial training results of RoBERTa. \u2018FOOLER\u2019 refers to \u2018TEXTFOOLER\u2019. We use green and red subscripts to represent a higher and lower accuracy respectively.\\n\\n- Artificial test examples, which are generated artificially and may not preserve semantics and introduce grammatical errors. We employ two methods PWWS and TextFooler on LCQMC test to generate artificial adversarial examples. These two methods generate adversarial examples by replacing words with synonyms until models are fooled.\\n\\n- Natural test examples are texts within linguistic and semantics constraints. Our annotators from the internal data team reviewed and annotated all the generated texts with methods PWWS, TextFooler and the translated texts of Checklist dataset (Ribeiro et al., 2020), and we finally get three natural test sets, PWWS nat, TextFooler nat and Checklist nat.\\n\\nBesides, we employ PWWS and TextFooler on LCQMC train to generate artificial adversarial training examples, which are combined with original LCQMC train as training data (Row LCQMC+PWWS and LCQMC+FOOLER in Tab. 6). The detailed data statistics are shown in Tab. 5. AT details are in Appendix C.2.\\n\\nEvaluation with artificial and natural adversarial examples. We fine-tune RoBERTa l on LCQMC and the artificial adversarial examples generated by PWWS and TextFooler, and evaluate on the adversarial test sets. The results are shown in Tab. 6. Row LCQMC shows that only training with LCQMC train shows a low performance on PWWS and TextFooler (we provide a detailed analysis in Appendix C.3), and the performance on PWWS and TextFooler are significantly higher on PWWS nat and TextFooler nat respectively. However, if we incorporate LCQMC train with the examples generated by PWWS and TextFooler, the model\u2019s performance on PWWS and TextFooler increase greatly (both methods achieve an great improvement of more than 16%) , but the effects on natural examples PWWS nat and TextFooler nat are not significant (-5.8% ~2.3%). On the other 2 natural test sets, Checklist nat and DuQM, the effects of 2 adversarial methods are also not obvious (-2.4% ~2.3%).\\n\\nIn conclusion, the common artificial AT methods are not so effective on the natural datasets. As a corpus consisting linguistically perturbed natural questions, DuQM is beneficial to a robustness evaluation to help us mitigate models\u2019 undesirable performance in real-world applications.\\n\\n5 Conclusion\\n\\nIn this work, we create a Chinese dataset namely DuQM which contains linguistically perturbed natural questions for evaluating the robustness of QM models. DuQM is designed to be fine-grained and natural. Specifically, DuQM has 3 categories and 13 subcategories with 32 linguistic perturbations. We conduct extensive experiments with DuQM and the results demonstrate that DuQM has 3 characteristics: 1) DuQM is challenging and has better discrimination ability; 2) The fine-grained design of DuQM helps to diagnose the strengths and weaknesses of models, and enables us to evaluate the models in diverse ways; 3) Artificial adversarial training fails in the natural texts of DuQM.\\n\\nAcknowledgements\\n\\nThis research is supported by the National Key Research and Development Project of China (No.2018AAA0101900).\"}"}
{"id": "emnlp-2022-main-531", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ethical Considerations\\n\\nThis work presents DuQM, a diverse and natural dataset for the research community to evaluate the robustness of QM models. Data in DuQM are collected from Baidu search (we are legally authorized by this company), the details are presented in Sec. 3. Since DuQM do not have any user information, there is no privacy concerns. In addition, to ensure that the DuQM is free potential biased and toxic content, we desensitize all the instances in it. Regarding to the issue of labor compensation, all annotators are employees from our internal data team and are fairly compensated.\\n\\nLimitations\\n\\nOur dataset DuQM provides a new resource for evaluating the robustness of QM models. However, the categories can be further expanded to consider more behavioral capabilities of QM models, such as symmetry \\\\(((a, b) = (b, a))\\\\) and transitivity \\\\(((a, c) \\\\text{ if } a = b \\\\text{ and } b = c)\\\\).\\n\\nReferences\\n\\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. 2018. Generating natural language adversarial examples. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2890\u20132896, Brussels, Belgium. Association for Computational Linguistics.\\n\\nRonan Le Bras, Swabha Swayamdipta, Chandra Bhatagavatula, Rowan Zellers, Matthew E. Peters, Ashish Sabharwal, and Yejin Choi. 2020. Adversarial filters of dataset biases. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1078\u20131088. PMLR.\\n\\nJing Chen, Qingcai Chen, Xin Liu, Haijun Yang, Daohe Lu, and Buzhou Tang. 2018. The BQ corpus: A large-scale domain-specific Chinese corpus for sentence semantic equivalence identification. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4946\u20134951, Brussels, Belgium. Association for Computational Linguistics.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020. Revisiting pre-trained models for Chinese natural language processing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 657\u2013668, Online. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nAllyson Ettinger. 2020. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34\u201348.\\n\\nMatt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating models\u2019 local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307\u20131323, Online. Association for Computational Linguistics.\\n\\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107\u2013112, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nShankar Iyer, Nikhil Dandekar, and Korn\u00e9l Csernai. 2017. First quora dataset release: Question pairs. data.quora.com.\\n\\nRobin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021\u20132031, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is BERT really robust? A strong baseline for natural language attack on text classification and entailment. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The\"}"}
{"id": "emnlp-2022-main-531", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-531", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u72d7\u54ac\u4e86\u732b\u600e\u4e48\u529e\\nSBV VOB ADV to if dog What do cat bites\\n(a) Active voice question.\\n\\n\u4e86 \u600e\u4e48 \u529e\\nSBV ADV \u72d7\u732b to if What do cat is bitten dog by\\n\u54ac\u88ab POB ADV SBV\\n(b) Passive voice paraphrase question.\\n\\n(1) Passive voice non-paraphrase question.\\n\\nFigure 2: The dependency relations of active voice and passive voice questions.\\n\\nA Dependency Relations of Bei-Construction\\nFig. 2 illustrates the dependency relations of active voice and passive voice questions in Chinese.\\n\\nInput: 10,167\\nStart\\nNo Fluent / Correct grammar? Not fluent / Incorrect grammar: 20\\nCorrectly classified? Yes\\nCan be classified into the pre-defined category? No\\nCannot be classified: 26\\nClassified into correct category: 972\\nYes\\nCorrectly annotated? Yes\\nRe-annotate: 185\\nNo\\nOutput: 10,121\\nYes\\nEnd\\n\\nFigure 3: Overall annotation process.\\n\\nB Annotation Process\\nFig. 3 illustrates the overall annotation process.\\n\\nC Supplementary Experiments\\nC.1 Additional Experimental Setting\\nC.1.1 Training Details\\nIn the fine-tuning stage, we insert a [SEP] between the question pairs. The pooled output is passed to a classifier. We use different learning rates and epochs for different pre-trained models. Specifically, for large models, the learning rate is 5e-6 and the number of epochs is 3. For base models, the learning rate is 2e-5, and we set the number of epochs as 2. The batch size is set as 64 and the maximal length of question pair is 64. We use early stopping to select the best checkpoint. We choose the model with the best performance on LCQMC dev to report test results and each model is fine-tuned 3 times on LCQMC train.\\n\\nC.1.2 Datasets Details\\nTab. 8 gives a detailed description of LCQMC Corpus.\\n\\nC.2 Adversarial Training Details\\nTab. 5 gives a detailed statistics of adversarial examples generated with TextFooler, PAWS and Checklist. To generate training samples, we select a set of LCQMC training questions and apply the methods PWWS and TextFooler on them. The labels are same as original samples. To generate test samples and ensure a robust evaluation, we utilize 4 datasets, PWWS nat, TextFooler nat, Checklist nat17 and DuQM, which are natural adversarial examples. We conduct an experiment about adversarial training by feeding the models both the original data and the adversarial examples, and observe whether the original models become more robust. We use pre-trained model RoBERTa l (described in Tab. 7) for fine-tuning and the details are described in Sec. 4.1.\\n\\nC.3 Results of Attacks\\nWe give the main results of attacks to BERT b and RoBERTa l in Tab. 9. The results show that the un-natural attacks (on artificial adversarial samples, i.e. PWWS and TextFooler in Tab. 9) have higher success rate than DuQM. However, if we select the natural examples from the artificial adversarial samples (PWWS nat and TextFooler nat in Tab. 9), the attack success rate of PWWS and TextFooler is significantly decreasing by at least 18.5% on BERT b 17\\n\\nBefore annotating, we translate original Checklist dataset into Chinese using Baidu translate\"}"}
{"id": "emnlp-2022-main-531", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Models       | L  | H   | A   | # of Parameters | Masking | LM Task | Corpus                  |\\n|--------------|----|-----|-----|-----------------|---------|---------|-------------------------|\\n| BERT         | 12 | 768 | 12  | 110M            | T       | MLM     | Wikipedia               |\\n| ERFIE        | 12 | 768 | 12  | 110M            | T/E/Ph  | MLM     | Wikipedia+Baike+Tieba, etc. |\\n| RoBERTa      |    |     |     |                 |         | MLM     | -                        |\\n| MacBERT      | 24 | 1024| 16  | 340M            | Mac     | SOP     | -                        |\\n\\nTable 7: The hyper-parameters of public pre-trained language models we use (L: number of layers, H: the hidden size, A: the number of self-attention heads, T: Token, E: Entity, Ph: Phrase, WWM: Whole Word Masking, NM: N-gram Masking, MLM: Masked LM, Mac: MLM as correction).\\n\\n| Corpus | Train | Dev | Test | Fine-grained |\\n|--------|-------|-----|------|--------------|\\n| LCQMC  | 238,766 | 8,802 | 12,500 | No            |\\n\\nTable 8: Data statistics of LCQMC.\\n\\n| Data       | BERT | RoBERTa | PWWS | PWWS | TEXTFOOLER | TEXTFOOLER | DuQM |\\n|------------|------|---------|------|------|------------|------------|------|\\n| nat        | 23.0 | -18.5   | -32.0| -30.7| 14.6       | -30.7      | 12.2 |\\n\\nTable 9: Attack success rate (%) on different test data.\\n\\nand 30.7% on RoBERTa l respectively. DuQM, in which all the samples are natural and grammarly correct, gets the best performance when black-box attacking (compare to PWWS nat and TextFooler nat in Tab. 9). In summary, the artificial adversarial examples training is not effective on natural texts, such as DuQM. It is reasonable that we should pay more attention to the naturalness when generating the adversarial examples.\"}"}
