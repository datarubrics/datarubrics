{"id": "acl-2024-long-135", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nRecent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased performance in the presence of noisy contexts. In this paper, we introduce Reasoning with Attributions, a novel approach that prompts LMs to supply attributions for each assertion during their reasoning. We validate our approach through experiments on three multi-hop datasets, employing both proprietary and open-source models, and demonstrate its efficacy and resilience. Furthermore, we explore methods to augment reasoning capabilities via fine-tuning and offer an attribution-annotated dataset and a specialized training strategy. Our fine-tuned model achieves competitive performance on multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as ChatGPT and Claude-instant.\\n\\n1 Introduction\\nThe field of long-context modeling has garnered significant attention due to its importance in applications that demand extensive comprehension and generation capabilities (Lewis et al., 2020; Liu et al., 2023b). Techniques for long-context modeling (Chen et al., 2023a; Peng et al., 2023; Chen et al., 2023b) have been proposed with encouraging results on established benchmarks (An et al., 2023; Bai et al., 2023).\\n\\nNevertheless, we have identified a gap in the performance of these models when it comes to multi-hop reasoning tasks, where a model must navigate and synthesize information from disparate sources to answer complex questions. Evidence from key benchmarks such as LongBench (Bai et al., 2023), as well as our experimental results in Section 4.3, indicate that these long-context LMs underperform compared to leading multi-hop reasoning systems (Zhang et al., 2023). The reasons for this shortfall in multi-hop reasoning effectiveness are not yet fully understood.\\n\\nWe contend that the limitations in multi-hop reasoning observed in long-context LMs stem from two main issues: The inability to discern pertinent information within noisy contexts (Liu et al., 2023a) and the struggle to incorporate knowledge within the context effectively, particularly for smaller-scale models (Zheng et al., 2023a). To address these challenges, we introduce Reasoning with Attributions, a methodology that compels LMs to substantiate their reasoning by linking assertions to relevant context segments, such as citations (Gao et al., 2023) or direct quotations (Menick et al., 2022). This approach not only guides LMs to perform targeted information retrieval to identify the position of relevant contexts, thereby reducing noise, but also ensures their responses are well-grounded in the source material. Our preliminary and comprehensive experimental findings, detailed in Sections 2.1 and 4.3, confirm the efficacy and resilience of this method across various multi-hop reasoning benchmarks.\\n\\nDespite these advancements, smaller long-context LMs exhibit continued difficulties in reasoning. We explore the potential for these models to improve through learning to reason and attribute simultaneously. Utilizing ChatGPT (Brown et al., 2020) to annotate the multi-hop reasoning dataset MuSiQue (Trivedi et al., 2022), we create a specialized dataset MuSiQue-Attribute for fine-tuning models in this dual capacity. We propose a potent learning strategy that leverages multi-task learning and data augmentation to fully exploit these annotations. Our experiments with five long-context LMs across three multi-hop reasoning datasets and two general instruction-following datasets reveal...\"}"}
{"id": "acl-2024-long-135", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that our fine-tuned Vicuna-7B model (Zheng et al., 2023b) surpasses similar-scale baselines by a substantial margin, i.e., more than 20 points on average, and even outperforms ChatGPT and Claude-instant on MuSiQue, albeit with a slight trade-off in other capabilities. This study illuminates a promising avenue to enhance the multi-hop reasoning capabilities of long-context LMs through a focus on attributions.\\n\\nOur contributions can be summarized as follows:\\n\\n\u2022 We introduce **Reasoning with Attributions**, an innovative reasoning paradigm that enhances both the performance and robustness of long-context LMs in multi-hop reasoning tasks.\\n\\n\u2022 We provide **MuSiQue-Attribute**, an attribution-annotated multi-hop reasoning dataset to support further research in this domain.\\n\\n\u2022 We develop a bespoke learning strategy that incorporates novel auxiliary tasks for multi-task learning and employs tailored data augmentation techniques.\\n\\n\u2022 Through rigorous testing on three multi-hop reasoning datasets and two general instruction-following benchmarks with both proprietary and open-source models, we demonstrate that our fine-tuned Vicuna-7B model achieves comparable multi-hop reasoning performance to ChatGPT with minimal impact on other capabilities. Additionally, our model shows resilience against varying degrees of contextual noise, underscoring the effectiveness of our methods.\\n\\n2 Reasoning with Attributions\\n\\n2.1 Pilot Study\\n\\nThe challenge of large language models becoming mired in irrelevant contexts, known as the \u201cLost in the Middle\u201d phenomenon, has been documented across various NLP tasks, such as multi-document QA (Liu et al., 2023a) and mathematical reasoning (Shi et al., 2023). This issue is also apparent in multi-hop reasoning, which we illustrate later in Figure 2. Prior research has noted this problem but has not decoded the underlying mechanisms. For example, while Liu et al. (2023a) found that introducing the query before the context can aid in better information retrieval from the context, they did not achieve an improvement in QA performance using this query-aware approach. As we suggest in Section 1, the reasons might extend beyond mere retrieval challenges to include complications in effectively applying the retrieved knowledge.\\n\\nTo tackle the issues outlined earlier, we introduce **Reasoning with Attributions**, a strategy that mandates language models to link the claims made during reasoning to specific sections of the provided context. This implicit requirement effectively decomposes a complex multi-hop question into two more manageable tasks: Pinpointing pertinent information within the context and constructing well-founded claims based on that information.\\n\\nWe adapt the concept of Chain-of-Thought (CoT) (Wei et al., 2022) reasoning to create two distinct variants aligned with our attribution-based approach: **Chain-of-Citation** (CoC) and **Chain-of-Quote** (CoQ). In CoC, models are prompted to reference citations corresponding to each step of the reasoning chain. CoQ goes further by requiring models to include direct quotations from the cited material for each reasoning step. An illustrative example highlighting the nuances between these methods is provided in Table 1.\\n\\nThe results of our preliminary study (Please refer to Section 4 for the setup), detailed in Table 1.\"}"}
{"id": "acl-2024-long-135", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Exact-Match (EM) and F1 scores of ChatGPT and Claude-instant with 5-shot prompting on multi-hop reasoning datasets, e.g., MuSiQue, 2WikiMultiHopQA (2Wiki for short) and HotpotQA. The best results are in bold.\\n\\n| Model          | MuSiQue  | 2Wiki  | HotpotQA |\\n|----------------|----------|--------|----------|\\n|                | EM F1    | EM F1  | EM F1    |\\n| ChatGPT (gpt-3.5-turbo-1106) + AO | 15.8     | 26.9   | 46.2     | 57.2 | 51.0 | 65.4 |\\n|                | CoT      | 36.2   | 50.1     | 55.2     | 70.1 |\\n|                | CoC      | 37.0   | 51.0     | 55.4     | 71.1 |\\n|                | CoQ      | 36.4   | 51.3     | 54.0     | 68.7 |\\n| Claude-instant (claude-instant-1.2) + AO | 26.2     | 39.4   | 47.0     | 57.5 | 54.4 | 68.4 |\\n|                | CoT      | 26.0   | 37.9     | 40.8     | 52.3 |\\n|                | CoC      | 32.2   | 46.2     | 53.4     | 67.0 |\\n|                | CoQ      | 30.2   | 45.9     | 49.8     | 62.1 |\\n\\nThe findings suggest that both CoC and CoQ generally yield improvements over CoT, indicating that attribution-based reasoning enhances the precision and coherence of the models\u2019 reasoning processes. CoQ appears to slightly underperform CoC, likely due to the increased complexity of producing exact quotations.\\n\\nIt is noteworthy that even in instances where CoT reduces the Answer Only (AO) performance, CoC is able to not only mitigate this decline but also surpass the AO baseline. This demonstrates the potential of CoC as a robust reasoning method. The success of our approach with various open-sourced models is further elaborated upon in Section 4.3.\\n\\n2.2 Dataset Curation\\n\\nOur analysis, evidenced by the data in Tables 2 and 5, confirms that while reasoning with attributions holds promise, smaller open-source long-context language models significantly underperform compared to their proprietary counterparts in multi-hop reasoning tasks. To address this, we investigate whether training these models to perform attributions can boost their reasoning capabilities.\\n\\nA hurdle in this process is the lack of attribution annotations within existing multi-hop reasoning benchmarks. To bridge this gap, we have generated new annotations by prompting ChatGPT with 5-shot CoQ. This has been done to create CoT with attributions for 5,000 instances randomly selected from the answerable training set of the MuSiQue dataset (Trivedi et al., 2022). Although CoC generally outperforms CoQ, we chose CoQ for annotation because it provides more detailed information. This richness is beneficial not only for evaluating the quality of the annotations but also proves advantageous for the fine-tuning processes discussed in Section 3.\\n\\nAfter generating the annotations, we implemented a filtering process to exclude annotations with any of the following errors (Please refer to Appendix E for implementation details):\\n\\n- Incorrect Answer: The model's predicted answer does not align with the reference answer, which typically indicates an erroneous CoT.\\n- Non-Existent Attributions: Fabricated citations or quotes that do not correspond to the actual context are indicative of model hallucination.\\n- Incorrect Citations: Citations do not match the manually identified supporting facts, suggesting flawed attributions.\\n- Repeated Citations: Redundant citations contravene the multi-hop requirement of sourcing from multiple documents.\\n- Extreme Quotes: Quotes that are either too terse (under five words) or excessively lengthy (spanning an entire document) lack utility.\\n\\nTable 3 presents the substantial incidence rates of different error types.\\n\\n| Error Type                        | Portion    |\\n|----------------------------------|------------|\\n| Incorrect Answer                 | 58.44%     |\\n| Non-Existent Attributions        | 12.56%     |\\n| Incorrect Citations              | 9.80%      |\\n| Repeated Citations               | 6.35%      |\\n| Extreme Quotes                   | 10.55%     |\\n\\nTable 3: Incidence rates of different error types.\\n\\nTable 4: Statistics of MuSiQue-Attribute.\\n\\n| Entry                              | Value  |\\n|------------------------------------|--------|\\n| #Max Words per Sample             | 3385   |\\n| #Mean Words per Sample            | 1809.10|\\n| #Averaged Words per CoT Step      | 11.64  |\\n| #Averaged Words per Quote         | 16.60  |\\n| #Total Samples                    | 1358   |\\n| 2-Hop Samples [%]                 | 82.18% |\\n| 3-Hop Samples [%]                 | 14.06% |\\n| 4-Hop Samples [%]                 | 3.76%  |\\n\\n2-Hop Samples [\\\\%] 82.18\\n3-Hop Samples [\\\\%] 14.06\\n4-Hop Samples [\\\\%] 3.76\\n\\nAfter generating the annotations, we implemented a filtering process to exclude annotations with any of the following errors (Please refer to Appendix E for implementation details):\\n\\n- Incorrect Answer: The model's predicted answer does not align with the reference answer, which typically indicates an erroneous CoT.\\n- Non-Existent Attributions: Fabricated citations or quotes that do not correspond to the actual context are indicative of model hallucination.\\n- Incorrect Citations: Citations do not match the manually identified supporting facts, suggesting flawed attributions.\\n- Repeated Citations: Redundant citations contravene the multi-hop requirement of sourcing from multiple documents.\\n- Extreme Quotes: Quotes that are either too terse (under five words) or excessively lengthy (spanning an entire document) lack utility.\\n\\nTable 3 presents the substantial incidence rates of different error types.\\n\\n| Error Type                        | Portion    |\\n|----------------------------------|------------|\\n| Incorrect Answer                 | 58.44%     |\\n| Non-Existent Attributions        | 12.56%     |\\n| Incorrect Citations              | 9.80%      |\\n| Repeated Citations               | 6.35%      |\\n| Extreme Quotes                   | 10.55%     |\\n\\nTable 3: Incidence rates of different error types.\\n\\nTable 4: Statistics of MuSiQue-Attribute.\\n\\n| Entry                              | Value  |\\n|------------------------------------|--------|\\n| #Max Words per Sample             | 3385   |\\n| #Mean Words per Sample            | 1809.10|\\n| #Averaged Words per CoT Step      | 11.64  |\\n| #Averaged Words per Quote         | 16.60  |\\n| #Total Samples                    | 1358   |\\n| 2-Hop Samples [%]                 | 82.18% |\\n| 3-Hop Samples [%]                 | 14.06% |\\n| 4-Hop Samples [%]                 | 3.76%  |\\n\\nAfter generating the annotations, we implemented a filtering process to exclude annotations with any of the following errors (Please refer to Appendix E for implementation details):\\n\\n- Incorrect Answer: The model's predicted answer does not align with the reference answer, which typically indicates an erroneous CoT.\\n- Non-Existent Attributions: Fabricated citations or quotes that do not correspond to the actual context are indicative of model hallucination.\\n- Incorrect Citations: Citations do not match the manually identified supporting facts, suggesting flawed attributions.\\n- Repeated Citations: Redundant citations contravene the multi-hop requirement of sourcing from multiple documents.\\n- Extreme Quotes: Quotes that are either too terse (under five words) or excessively lengthy (spanning an entire document) lack utility.\\n\\nTable 3 presents the substantial incidence rates of different error types.\\n\\n| Error Type                        | Portion    |\\n|----------------------------------|------------|\\n| Incorrect Answer                 | 58.44%     |\\n| Non-Existent Attributions        | 12.56%     |\\n| Incorrect Citations              | 9.80%      |\\n| Repeated Citations               | 6.35%      |\\n| Extreme Quotes                   | 10.55%     |\\n\\nTable 3: Incidence rates of different error types.\\n\\nTable 4: Statistics of MuSiQue-Attribute.\"}"}
{"id": "acl-2024-long-135", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of each error type, which could negatively impact fine-tuning effectiveness. After filtering, we obtain a training dataset of 1,358 samples, referred to as MuSiQue-Attribute. The statistics of the MuSiQue-Attribute training set are outlined in Table 4. It is important to note that the hop distribution in MuSiQue-Attribute is skewed. This skewness arises both because generated CoT for questions with more hops is more prone to errors and because such questions represent a smaller fraction of the original MuSiQue training set. Additionally, we conducted a human evaluation of the MuSiQue-Attribute quality in Appendix A.\\n\\n3 Learning to Attribute in Reasoning\\n\\nOne intuitive approach to enhancing the multi-hop reasoning capabilities of LMs is to fine-tune them on our curated MuSiQue-Attribute, thereby teaching them to integrate attribution into their reasoning processes, specifically to generate CoC. Despite the simplicity of this method, our subsequent analysis in Section 4.4 demonstrates that this direct approach fails to produce robust results.\\n\\nMulti-Task Learning.\\n\\nBeyond simply fine-tuning LMs on the MuSiQue-Attribute to learn to attribute in reasoning (denoted as LA), we propose three auxiliary tasks that serve as simplified analogs of LA. These tasks are designed to train LMs in conjunction with LA to enhance their proficiency in attribution-based reasoning:\\n\\n- **Answer Prediction** (AP for short): This task focuses on direct answer prediction without the need for an explicit reasoning process. AP is intended to help LMs internalize the reasoning needed for straightforward questions where CoT is not required.\\n- **CoT Generation** (CG for short): In the CG task, models are trained to generate a CoT before providing an answer. This is aimed at developing LMs' abilities to reason explicitly and methodically across multiple pieces of information for complex questions.\\n- **Quotes Identification** (QI for short): This task trains models to pinpoint critical quotes for reasoning. QI is designed to fine-tune the ability of LMs to filter out irrelevant details and zero in on the pertinent segments of text, thereby sharpening the accuracy of reasoning.\\n\\nFigure 1 illustrates the distinctions between our primary LA task and the three auxiliary tasks.\\n\\nData Augmentation.\\n\\nA recognized limitation of direct fine-tuning on our MuSiQue-Attribute is the potential for models to develop biases, such as favoring certain locations of relevant documents (Liu et al., 2023a), sensitive to a fixed number of documents, or accommodating only a narrow range of noise levels. To counteract these biases, we have devised the following data augmentation strategies:\\n\\n- **Distractor Sampling**: By randomly selecting a varying number of irrelevant documents, we modify the positioning of relevant documents and the total document count within the context. This approach also mimics the fluctuating noise levels encountered in real-world scenarios, training language models to cope with noisy contexts effectively.\\n- **Document Shuffling**: Reordering the documents helps to remove any superficial positional cues that could lead to reasoning bias. For example, this ensures that models do not learn to associate the sequence of relevant documents with a fixed reasoning chain sequence.\\n\\nThese data augmentation strategies are applied in sequence for each training instance.\\n\\n4 Experiments\\n\\n4.1 Datasets\\n\\nOur method's effectiveness in multi-hop reasoning is assessed on the following datasets: HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (2Wiki for short) (Ho et al., 2020), and MuSiQue (Trivedi et al., 2022). For each question, we provide a context composed of shuffled relevant and irrelevant documents. These irrelevant documents are the official retrieved distractor documents. We adopt the development and test sets from Trivedi et al. (2023) for evaluation, which contains 100 and 500 examples respectively. The results we present are the mean values from three separate trials, each with a distinct random seed.\"}"}
{"id": "acl-2024-long-135", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Exact-Match (EM) results on three multi-hop reasoning datasets. The best small-scale long-context LM results are in bold and the best baseline results are underlined.\\n\\nTo understand the broader impact of enhancing multi-hop reasoning on LMs' overall capabilities, we also conduct evaluations on general instruction-following benchmarks, namely MT-Bench (Zheng et al., 2023b) and AlpacaEval (Li et al., 2023c).\\n\\n4.2 Models\\n\\nThe following long-context baselines are chosen in our experiments.\\n\\n- **ChatGPT**: We choose gpt-3.5-turbo-1106, which supports a window size of 16K tokens.\\n- **Claude-instant**: We choose claude-instant-1.2, which has a window size of 100K tokens.\\n- **LongChat**: We use longchat-7b-16k, a 7B fine-tuned LLaMA model (Touvron et al., 2023a). It has a window size of 16K tokens.\\n- **LongLoRA**: We use LongAlpaca-7B-16k, which has a window size of 16K.\\n- **Vicuna**: We use vicuna-7b-v1.5-16k, a 7B fine-tuned LLaMA-2 model (Touvron et al., 2023b). It supports a window size of 16K tokens.\\n\\nWe prompt all models with 5-shot to evaluate their multi-hop reasoning performance. These 5 demonstrations are randomly sampled from the 20 annotated training examples provided by Trivedi et al. (2023). If the input length exceeds the window size, we drop the last demonstration until the input length fits. The prompts we used are presented in Appendix D.\\n\\nOur model **AttrLoRA** is fine-tuned on vicuna-7b-v1.5-16k with LoRA (Hu et al., 2022), following hyper-parameters used in FastChat (Zheng et al., 2023b). For its training data, we perform augmentations to double the training data for all tasks in Section 3 except for the QI task. Note that we subsample same-sized instruction-tuning data from the Alpaca dataset (Taori et al., 2023) and mix it with the reasoning data. These instruction-tuning data serve the purpose of minimizing the risk of hampering other abilities Vicuna already possesses before fine-tuning.\\n\\n4.3 Main Results\\n\\nEffectiveness and Robustness of Reasoning with Attributions. The results in Table 5 underscore the efficacy of our CoC prompting across three multi-hop reasoning datasets, benchmarked against five baselines. In 77% of the evaluated cases (disregarding instances of near-zero model performance) CoC outperforms CoT. Notably, Claude-instant exhibits strong results with AO, and its performance diminishes when CoT is used. However, CoC not only mitigates this decline but also attains results on par with AO, demonstrating the robustness of attribution-based reasoning. For a detailed analysis...\"}"}
{"id": "acl-2024-long-135", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Exact-Match (EM) results of different models under various noise levels in three multi-hop reasoning datasets. Note that all models except our AttrLoRA use 5-shot prompting. A higher noise ratio indicates more distractors, i.e., irrelevant documents, are presented in the context of both the test instance and the demonstrations.\\n\\n| Model       | MT-Bench (Score) | AlpacaEval (Win Rate) |\\n|-------------|------------------|-----------------------|\\n| ChatGPT     | 8.245            | 9.178%                |\\n| Claude-instant | 8.131           | 15.664%               |\\n| Vicuna      | 6.068            | 5.415%                |\\n| + Alpaca Data | 4.850           | 3.287%                |\\n| AttrLoRA    | 4.978            | 3.106%                |\\n\\nTable 6: Results on general instruction-following benchmarks. \"+ Alpaca Data\" is a Vicuna-7B model continued fine-tuning on Alpaca data.\\n\\nPerformance of AttrLoRA Against Proprietary Models. Table 5 presents a zero-shot performance comparison between our AttrLoRA and five-shot outcomes from various baselines. AttrLoRA surpasses baselines of comparable scale by an average margin of over 20 points. It exceeds the performance of two notable proprietary models on MuSiQue and delivers closely competitive results on the other two benchmarks.\\n\\nIn particular, AttrLoRA with AO achieves superior results on 2Wiki and surpasses CoT on HotpotQA. This can be attributed to the relative simplicity of these datasets, where explicit reasoning does not significantly enhance performance. For instance, CoT's advantage is noticeably smaller on these datasets compared to MuSiQue for both the ChatGPT and Claude-instant. Additionally, according to Jiang and Bansal (2019), over half of the \u201cbridge-type\u201d questions in HotpotQA contain shortcuts, which can locate the answer by keyword matching, circumventing the need for the intended two-hop reasoning. Similarly, 2Wiki's predictable nature, due to its question construction from a limited set of rules, simplifies the task for LMs. Another contributing factor is that AttrLoRA is trained on MuSiQue-Attribute, which does not encompass the full range of question types found in 2Wiki and HotpotQA, such as \u201ccomparison-type\u201d questions.\\n\\nResilience of AttrLoRA to Noisy Contexts. A key aspect of AttrLoRA is its robustness to contextual noise. To investigate this, Figure 2 illustrates AttrLoRA's performance against varying degrees of synthesized noise. This synthesized noise is implemented by adding varied numbers of random irrelevant documents to the context. The data indicates that while the performance of baseline models markedly declines with increased noise, e.g., Vicuna drops by over 30 points on MuSiQue, AttrLoRA shows greater resilience, with a reduction of only about 10 points.\\n\\nImpact of Attribution Learning on General Abilities. Our investigation extends beyond multi-hop reasoning to examine how attribution learning affects AttrLoRA's general instruction-following capabilities post-fine-tuning, as compared to the Vicuna baseline. The results in Table 6 from two...\"}"}
{"id": "acl-2024-long-135", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Ablation study on multi-task learning.\\n\\n| Model          | MuSiQue | 2Wiki | HotpotQA |\\n|---------------|---------|-------|----------|\\n| Vicuna (5-Shot) | 0.00    | 28.4  | 33.1     |\\n| + AP          | 32.3    | 47.8  | 52.1     |\\n| + CG          | 37.3    | 45.3  | 50.7     |\\n| + LA          | 37.3    | 46.9  | 52.1     |\\n\\nTable 8: Ablation study on data augmentation.\\n\\n| Model          | MuSiQue | 2Wiki | HotpotQA |\\n|---------------|---------|-------|----------|\\n| Vicuna (5-Shot) | 0.00    | 28.4  | 33.1     |\\n| + Augmentation | 27.4    | 44.9  | 50.9     |\\n| + CG          | 28.1    | 37.7  | 49.4     |\\n| + Augmentation | 30.5    | 37.3  | 50.4     |\\n| + LA          | 29.1    | 38.8  | 49.0     |\\n| + Augmentation | 30.1    | 37.1  | 49.9     |\\n\\n4.4 Analysis\\n\\nAttribution Quality of AttrLoRA.\\n\\nDrawing from the insights of Gao et al. (2023), we scrutinize the citation precision and recall for AttrLoRA, as presented in Figure 3. The model demonstrates high precision, indicating its proficiency in correctly attributing statements to pertinent documents. Nonetheless, the moderate recall highlights that AttrLoRA does not consistently identify all relevant documents, a potential consequence of the disconnected reasoning patterns observed in MuSiQue-Attribute (detailed in Appendix A). Further exploration of the correlation between attribution quality and reasoning performance is in Appendix B.\\n\\nThe Effectiveness of Multi-Task Learning.\\n\\nOur ablation study in Table 7 assesses our multi-task learning approach. Results indicate a marked enhancement in Vicuna's reasoning capabilities upon fine-tuning with our dataset (\u201c+ AP\u201d). However, explicitly training Vicuna to generate CoT (\u201c+ CG\u201d) yields mixed outcomes: It benefits performance on MuSiQue but adversely affects results on 2Wiki and HotpotQA. This discrepancy can be attributed to the relative ease of the latter datasets, where simpler questions and shortcuts reduce the effectiveness of complex reasoning strategies, as discussed in Section 4.3. Importantly, integrating the LA task (\u201c+ LA\u201d) mitigates the performance drops associated with CoT and notably boosts MuSiQue scores. This implies that attributions are instrumental in enabling the model to reason over complicated questions without compromising its ability to handle simpler queries. Finally, the addition of the QI task (\u201c+ QI\u201d) appears to further refine the model\u2019s multi-hop reasoning proficiency, underscoring the value of our multi-task learning framework.\\n\\nThe Effectiveness of Data Augmentation.\\n\\nWe explore the impact of our data augmentation strategy, intentionally omitting the QI task, as it alone is insufficient for training models to conduct multi-hop reasoning. The data in Table 8 demonstrates that including augmented data generally enhances model performance across various datasets. However, augmenting CG and LA data does not yield improvements on 2Wiki. In this case, the model readily learns from a limited amount of annotated data due to the simplicity of the automatically generated questions within 2Wiki. Conversely, on MuSiQue and HotpotQA, which feature more complex and varied human-crafted questions, the model benefits from exposure to a larger dataset to accommodate the diversity of question formulations.\\n\\nThe Effectiveness of Scaling Fine-Tuning Data.\\n\\nIn Figure 4, we investigate how the expansion of fine-tuning data influences model performance. It is evident that incorporating additional data steadily enhances performance on MuSiQue and 2Wiki, while optimal results are attained with just 60% of our data for HotpotQA. This fact suggests that more...\"}"}
{"id": "acl-2024-long-135", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: Who is the mascot of the university related to Randy Conrads?\\n\\nAnswer: Benny Beaver\\n\\nQuestion: What is the record label of the co-writer and recording artist of Permission to Fly?\\n\\nAnswer: Hollywood Records\"}"}
{"id": "acl-2024-long-135", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We focus on multi-hop reasoning rather than hallucination reduction. Moreover, we delve into optimizing training methodologies to maximize the efficacy of scarce attribution annotations.\\n\\n6 Conclusion\\n\\nThis study demonstrates that long-context LMs face challenges with multi-hop reasoning within noisy contexts. We introduce a reasoning paradigm that incorporates attributions, which significantly improves the reasoning capabilities of long-context LMs. Alongside, we contribute a new dataset annotated with attributions and study training strategies tailored for multi-hop reasoning. Our comprehensive experiments across five models and five benchmarks validate the superiority of our approach in enhancing multi-hop reasoning performance.\\n\\nLimitations\\n\\nThe proposed reasoning with attributions method currently leverages citations and quotations, leaving room for future exploration of other attribution forms, such as URLs. This work concentrates on refining training strategies, yet the potential of custom model architectures for this task remains untapped. Additionally, our approach relies on contexts pre-supplied by dataset creators. Emerging research suggests that language models integrated with search engines can achieve enhanced outcomes. A promising avenue for further research lies in developing models that not only manage noisy contexts more effectively, as demonstrated in our work, but also actively engage with search tools to improve information retrieval.\\n\\nEthics Statement\\n\\nOur dataset builds upon MuSiQue, adhering to its original copyright provisions; We distribute our supplementary annotations under the CC BY 4.0 license. While our model-generated annotations could potentially include misinformation or harmful content, our manual quality review in Appendix A did not encounter such instances. Throughout the human annotation phase, we did not gather any demographic data or information that could reveal the identity of the annotators. All annotators provided informed consent for the use of their annotations exclusively for research purposes.\\n\\nAcknowledgements\\n\\nThis work was supported by National Key R&D Program of China (Project No. 2022ZD0161200, 2022ZD0161201). This work is also supported by Hong Kong Research Grant Council - Early Career Scheme (Grant No. 24200223) and Hong Kong Innovation and Technology Commission Project No. ITS/228/22FP. This work was also partially funded by the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK.\\n\\nReferences\\n\\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. CoRR, abs/2307.11088.\\n\\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. CoRR, abs/2204.05862.\\n\\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. CoRR, abs/2308.14508.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\"}"}
{"id": "acl-2024-long-135", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. CoRR, abs/1604.06174.\\n\\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Longora: Efficient fine-tuning of long-context large language models.\\n\\nAntonia Creswell and Murray Shanahan. 2022. Faithful reasoning using large language models. CoRR, abs/2208.14271.\\n\\nAntonia Creswell, Murray Shanahan, and Irina Higgins. 2023. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\n\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6465\u20136488, Singapore. Association for Computational Linguistics.\\n\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6609\u20136625, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\n\\nYichen Jiang and Mohit Bansal. 2019. Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2726\u20132736, Florence, Italy. Association for Computational Linguistics.\\n\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\n\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktitus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\n\\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a. How long can open-source llms truly promise on context length?\\n\\nDongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Ziyang Chen, Baotian Hu, Aiguo Wu, and Min Zhang. 2023b. A survey of large language models attribution.\\n\\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023c. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.\\n\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023a. Lost in the middle: How language models use long contexts.\\n\\nTianyang Liu, Canwen Xu, and Julian J. McAuley. 2023b. Repobench: Benchmarking repository-level code auto-completion systems. CoRR, abs/2306.03091.\\n\\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, H. Francis Song, Martin J. Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. 2022. Teaching language models to support answers with verified quotes. CoRR, abs/2203.11147.\\n\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332.\\n\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071.\\n\\nSamyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. 2021. Zero-infinity: breaking the GPU memory wall for extreme scale deep learning. In International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2021, St. Louis, Missouri, USA, November 14-19, 2021, page 59. ACM.\\n\\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Sch\u00e4rli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 31210\u201331227. PMLR.\"}"}
{"id": "acl-2024-long-135", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stanford alpaca: An instruction-following llama model.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di\u00e1na Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.\\n\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2020. Is multihop QA in DiRe condition? measuring and reducing disconnected reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8846\u20138863, Online. Association for Computational Linguistics.\\n\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multi-hop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539\u2013554.\\n\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10014\u201310037, Toronto, Canada. Association for Computational Linguistics.\\n\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.\\n\\nSiye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. 2024. How easily do irrelevant inputs skew the responses of large language models?\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics.\\n\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023. Making retrieval-augmented language models robust to irrelevant context. CoRR, abs/2310.01558.\\n\\nWenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, and Dong Yu. 2023. Chain-of-note: Enhancing robustness in retrieval-augmented language models. CoRR, abs/2311.09210.\\n\\nJiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong Liu, and Shen Huang. 2023. Beam retrieval: General end-to-end retrieval for multi-hop question answering.\\n\\nCe Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. 2023a. Can we edit factual knowledge by in-context learning? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 4862\u20134876. Association for Computational Linguistics.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023b. Judging llm-as-a-judge with mt-bench and chatbot arena. CoRR, abs/2306.05685.\\n\\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering.\"}"}
{"id": "acl-2024-long-135", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Correlation coefficients between citation precision (denoted as $P$) or recall (denoted as $R$) and the multi-hop reasoning performance ($EM$).\\n\\n| Model      | EM vs. P (Spearman) | EM vs. P (Kendall) | EM vs. R (Spearman) | EM vs. R (Kendall) |\\n|------------|---------------------|--------------------|---------------------|--------------------|\\n| MuSiQue    | 0.887\u2217              | 0.951\u2217             | 0.826\u2217              |                    |\\n| 2Wiki      | 0.917\u2217              | 0.896\u2217             | 0.766\u2217              |                    |\\n\\nA Human Assessment of MuSiQue-Attribute\\n\\nWe recruit a student possessing expertise in NLP and holding a Master's degree in Computer Science to annotate 100 random samples from our MuSiQue-Attribute. The tool we used for annotations is shown in Figure 5. The annotation results reveal that 19% of the CoT annotations exhibited reasoning issues despite correct final answers, a problem referred to as reasoning unfaithfulness (Creswell and Shanahan, 2022). These erroneous CoTs fall into three categories: 5.26% with Disordered Steps where reasoning steps are improperly arranged, 78.95% with Missing Steps indicating omitted essential reasoning steps, also known as disconnected reasoning (Trivedi et al., 2020), and 10.53% with Incorrect Steps containing invalid reasoning steps. Our assessment also identified that 9% of questions in the dataset allowed for shortcuts (Jiang and Bansal, 2019), enabling models to find answers by keyword matching without reasoning.\\n\\nThe study further showed that within incorrect CoT annotations, 47.37% pertained to 2-hop questions, 36.84% to 3-hop, and 15.79% to 4-hop questions. Considering the distribution of hops in our dataset, the unfaithfulness issue affected 11.84% of 2-hop CoTs, 38.89% of 3-hop CoTs, and 50% of 4-hop CoTs, suggesting a decrease in reliability of LMs with the increase in reasoning steps required.\\n\\nB Attribution Quality and Reasoning Performance\\n\\nFigure 6 presents a visualization of the relationship between citation precision or recall and multi-hop reasoning performance. Our analysis identifies a notable positive correlation between citation precision and multi-hop reasoning capabilities. Table 10 provides statistical support for this observation, with correlation coefficients indicating a significant positive correlation. These results suggest the potential of using citation precision as a reference-free proxy for assessing multi-hop reasoning performance, which could be facilitated by Natural Language Inference (NLI) methods (Gao et al., 2023).\\n\\nC Robustness to Noisy Context\\n\\nWe assessed the resilience of CoT and CoC against varying degrees of contextual noise by evaluating the performance variability of different models across multi-hop reasoning benchmarks. Table 11 indicates that in 84% of the cases, CoC exhibits a smaller performance range compared to CoT. Notably, even in instances where CoC demonstrates a greater range, the disparity with CoT remains marginal. These findings suggest that CoC is generally more robust to noisy contexts than CoT.\\n\\nD Prompting Details\\n\\nIn this section, we highlight some details when prompting long-context LMs with various strategies, e.g., AO, CoT, CoC and CoQ.\\n\\n- Each prompting strategy has its own instructions.\\n- For CoT, CoC and CoQ, we add \u201cThink step-by-step.\u201d to the end of the question.\\n- For few-shot prompting, we put demonstrations to different turns as the dialogue history.\\n- Each demonstration is formatted according to Table 1.\\n\\nThe instruction for AO is (unique descriptions are underlined):\"}"}
{"id": "acl-2024-long-135", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: Screenshot of our human annotation tool.\\n\\nCitation Precision  Citation Recall\\n0 0.2 0.4 0.6 0.8 ...\\n\\nAttribution Quality\\nMuSiQue (EM) 0.4 0.6 0.8 1.0 1.2\\n2Wiki (EM) 0.4 0.6 0.8 1.0 1.2\\nHotpotQA (EM) 0.4 0.6 0.8 1.0 1.2\\n\\nFigure 6: Attribution quality (citation precision and citation recall) vs. the multi-hop reasoning performance (EM).\\n\\nData are collected from different trials of LongLoRA, LongChat, Vicuna and AttrLoRA.\"}"}
{"id": "acl-2024-long-135", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E Filtering Implementation\\nThe implementation of each filtering strategy is detailed as follows:\\n\\n- **Incorrect Answer**: Leveraging the evaluation script from QuAC, we normalize both model predictions and official reference answers. We filter out data instances where the model prediction does not exactly match the answer.\\n\\n- **Non-Existent Attributions**: Utilizing regular expressions, we extract all citations and verify if the predicted citations are present in the context. Each quote is checked for its exact presence in the cited document, and instances with fabricated citations or quotes are removed.\\n\\n- **Incorrect Citations**: We assess the correctness of predicted citations using officially annotated supporting documents, ensuring the cited document is among the supporting documents. Any example with at least one incorrect citation is deleted.\\n\\n- **Repeated Citations**: Regular expressions are used to extract all citations, and we check for duplicates. Examples with any duplicate citations are discarded.\\n\\n- **Extreme Quotes**: We extract all quotes using regular expressions and tokenize them with NLTK. Examples are kept only if all quotes contain more than five words but do not span entire cited documents.\\n\\nF Training and Inference Cost\\nFor fine-tuning using LoRA (Hu et al., 2022), we employed ZeRO-3 (Rajbhandari et al., 2021) optimization and gradient checkpointing (Chen et al., 2016) techniques. The model was trained on 8 NVIDIA A100 80GB GPUs for no more than 14 hours. Approximately 4M parameters were tuned, constituting 6.22% of the total parameters. For inference, each test set was processed in about 30 minutes using a single NVIDIA A100 80GB GPU.\"}"}
