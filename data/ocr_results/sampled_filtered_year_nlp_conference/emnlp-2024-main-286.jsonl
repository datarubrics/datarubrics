{"id": "emnlp-2024-main-286", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cross-Domain Audio Deepfake Detection: Dataset and Analysis\\n\\nYuang Li, Min Zhang, Mengxin Ren, Xiaosong Qiao, Miaomiao Ma, Daimeng Wei, Hao Yang\\n\\nHuawei Translation Services Center, China\\n\\n{liyuang3, zhangmin186, renmengxin2, qiaoxiaosong, mamiaomiao, weidaimeng, yanghao30}@huawei.com\\n\\nAbstract\\n\\nAudio deepfake detection (ADD) is essential for preventing the misuse of synthetic voices that may infringe on personal rights and privacy. Recent zero-shot text-to-speech (TTS) models pose higher risks as they can clone voices with a single utterance. However, the existing ADD datasets are outdated, leading to suboptimal generalization of detection models. In this paper, we construct a new cross-domain ADD dataset comprising over 300 hours of speech data that is generated by five advanced zero-shot TTS models. To simulate real-world scenarios, we employ diverse attack methods and audio prompts from different datasets. Experiments show that, through novel attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve equal error rates of 4.1% and 6.5% respectively. Additionally, we demonstrate our models' outstanding few-shot ADD ability by fine-tuning with just one minute of target-domain data. Nonetheless, neural codec compressors greatly affect the detection accuracy, necessitating further research. Our dataset is publicly available.\\n\\nIntroduction\\n\\nAudio deepfakes, created by text-to-speech (TTS) and voice conversion (VC) models, pose severe risks to social stability by spreading misinformation, violating privacy, and undermining trust. For advanced TTS models, the subjective score of the synthetic speech can surpass that of the authentic speech (Ju et al., 2024) and humans are often unable to recognize deepfake audio (M\u00fcller et al., 2022; Cooke et al., 2024). Consequently, it is imperative to develop robust audio deepfake detection (ADD) models capable of identifying imperceptible anomalies.\\n\\nSeveral datasets built upon various TTS and VC models have been released to benchmark the ADD task (Yi et al., 2022; Yamagishi et al., 2021; Frank and Sch\u00f6nherr, 2021; Wang et al., 2020; Yi et al., 2023). However, these datasets mainly include the traditional TTS models rather than the emerging zero-shot TTS models. Moreover, there is a lack of transparency regarding the specific types of models used within these datasets, hindering comprehensive analysis of cross-model performance. Additionally, the range of attacks these datasets consider is confined to conventional methods, excluding attacks associated with deep neural networks (DNNs), such as noise reduction and neural codec models. Based on the aforementioned datasets, a multitude of detection models have been proposed. These models incorporate diverse features, such as the traditional linear frequency cepstral coefficient (Yan et al., 2022) and features derived from self-supervised learning (Zeng et al., 2023; Mart\u00edn-Do\u00f1as and \u00c1lvarez, 2022), emotion recognition (Conti et al., 2022), and speaker identification models (Pan et al., 2022). These studies mainly concentrate on a single benchmark dataset. To demonstrate generalization capabilities, several studies have implemented cross-dataset evaluation (M\u00fcller et al., 2022; Ba et al., 2023). Furthermore, to enhance the models' generalizability, researchers have explored the combination of data from various sources (Kawa et al., 2022) and the integration of multiple features (Yang et al., 2024).\\n\\nIn this paper, we present a novel cross-domain ADD (CD-ADD) dataset, which encompasses more than 300 hours of speech data generated by five cutting-edge, zero-shot TTS models. We test nine different attacks, including those involving DNN-based codecs and noise reduction models. For cross-domain evaluation, rather than adopting the naive cross-dataset scenario, we formulate a unique task for zero-shot TTS models by analyzing pairwise cross-model performance and utilizing audio prompts from different domains. Experiments reveal:\"}"}
{"id": "emnlp-2024-main-286", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Zero-shot TTS architectures. a) Decoder-only.\\nb) Encoder-decoder.\\n\\n\u2022 The cross-domain task is challenging.\\n\u2022 Training with attacks improves adaptability.\\n\u2022 The ADD model is superior in the few-shot scenario.\\n\\n2 Methods\\n\\n2.1 Dataset Construction\\n\\nAs shown in Figure 1, we can categorize the zero-shot TTS models into two types:\\n\\n\u2022 Decoder-only (VALL-E (Wang et al., 2023)): It accepts phoneme representations and the speech prompt's discrete codes as input, and generates output speech codes autoregressively. These codes are transformed into personalized speech signals.\\n\\n\u2022 Encoder-decoder (YourTTS (Casanova et al., 2022), WhisperSpeech (Kharitonov et al., 2023), Seamless Expressive (Barrault et al., 2023), and OpenVoice (Qin et al., 2023)): An encoder extracts semantic information, while a decoder incorporates speaker embeddings from the speech prompt. Together with the vocoder, the autoregressive (AR) or non-autoregressive (NAR) decoder generates personalized speeches. When the encoder is trained to remove speaker-specific information from the input speech, it transforms into a VC model.\\n\\nFor zero-shot TTS, AR decoding may introduce instability, leading to errors such as missing words. Additionally, poor-quality speech prompts, characterized by high noise levels, can result in unintelligible output. To address these issues, we enforce quality control during dataset construction.\\n\\nAlgorithm 1\\n\\nDataset construction\\n\\nRequire: prompts, text, retry, threshold\\n\\n1: \\\\( i \\\\leftarrow 0 \\\\)\\n2: success \\\\leftarrow False\\n3: while \\\\( i < \\\\text{retry} \\\\) do\\n4: \\\\( p \\\\leftarrow \\\\text{random} \\\\_\\\\text{select} \\\\left( \\\\text{prompts} \\\\right) \\\\)\\n5: audio \\\\leftarrow \\\\text{TTS} \\\\left( \\\\text{text}, p \\\\right) \\\\)\\n6: \\\\( \\\\hat{\\\\text{text}} \\\\leftarrow \\\\text{ASR} \\\\left( \\\\text{audio} \\\\right) \\\\)\\n7: if \\\\( \\\\text{CER} \\\\left( \\\\text{text}, \\\\hat{\\\\text{text}} \\\\right) < \\\\text{threshold} \\\\) then\\n8: success \\\\leftarrow True\\n9: break\\n10: end if\\n11: \\\\( i \\\\leftarrow i + 1 \\\\)\\n12: end while\\n13: return audio, success\\n\\n(Algorithm 1). Specifically, we utilize an automatic speech recognition (ASR) model to predict the transcription of the generated speech. If the character error rate (CER) exceeds the threshold, we regenerate the speech using alternative prompts. Utterances are discarded if the CER remains above the threshold after a predefined number of retries.\\n\\nPrompts from different domains are used to evaluate the generalizability of ADD models. Our dataset introduces two tasks:\\n\\n\u2022 In-model ADD considers all models during both training and testing.\\n\\n\u2022 Cross-model ADD excludes data from one TTS model during training and uses data from this TTS model only during testing.\\n\\nADD models should generalize to in-the-wild synthetic data, which requires a well-designed cross-model evaluation that can represent the real-world scenario. To select the appropriate TTS model for testing, we conduct a pairwise cross-model evaluation, where the Wav2Vec2-base model is trained exclusively on the data produced by a single TTS model and subsequently evaluated on the datasets generated by alternative TTS models. We identify the TTS model that poses the greatest challenge, as evidenced by the high equal error rate (EER), and use it as the test set.\\n\\n2.2 Attacks\\n\\nFigure 2 presents the nine attacks we test. For traditional attacks, we add white Gaussian noise. Note that we use CER rather than word error rate to mitigate the influence of the ASR model's limited ability to recognize out-of-vocabulary words.\\n\\nWe use the term 'attack' to describe perturbations to the speech signals.\"}"}
{"id": "emnlp-2024-main-286", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Categories of tested attacks.\\n\\n(Noise-white) and environmental noise (Noise-env) (Maciejewski et al., 2020) with a signal-to-noise ratio ranging from 15dB to 20dB, use artificial reverberation (Reverb) with a duration of 0.2 to 0.4 seconds, and apply a low-pass filter (LPF) within the 4kHz to 8kHz range. Furthermore, we employ lossy compression methods such as MP3 and a DNN-based Encodec model (D\u00e9fossez et al., 2022) operating at bit rates of 6kbps (Codec-6) and 12kbps (Codec-12). In terms of noise reduction, we utilize the conventional noise gate approach to eliminate stationary noise and the time-domain SepFormer model (Subakan et al., 2021).\\n\\n2.3 ADD Methods\\n\\nWe fine-tune pre-trained speech encoders for the ADD task, namely, Wav2Vec2 (Baevski et al., 2020) and the Whisper encoder (Radford et al., 2022). We merge multi-layer features by using learnable weights, and employ a classifier head with two projection layers and one global pooling layer to obtain the final logits. To adapt the model to attacks, we consider all attacks with the same probability on-the-fly during training. We also consider a few-shot scenario, where we extend the cross-model evaluation by fine-tuning the ADD model with just one minute of target-domain speech data. This experiment simulates a situation where only the limited synthetic speech from a TTS model is available, such as the speech from a demo website or a single video.\\n\\n3 Experimental Setups\\n\\nThe training set for the CD-ADD dataset was generated using the train-clean-100 subset of LibriTTS (Zen et al., 2019), and the dev-clean and test-clean subsets of LibriTTS, along with the test set of TEDLium3 (Hernandez et al., 2018), were utilized for the evaluation datasets. The transcriptions were used as the input text, and the real speech signals were used as the real samples and the speech prompts. For dataset construction, we used the five zero-shot TTS models mentioned in Section 2.1.\\n\\nWe adopted a CER threshold of 10% and a maximum retry limit of five. For cross-model evaluation, the speech from Seamless Expressive served as the test set. Appendix A provides comprehensive details on the TTS model checkpoints and the models used for attacks, and Appendix B presents the specific statistics of the CD-ADD dataset that is comprised of over 300 hours of training data and 50 hours of test data.\\n\\nFor the ADD task, we combined our CD-ADD dataset with the ASVSpoof2019 (Wang et al., 2020) training set and fine-tuned the base model, which includes Wav2Vec2 (Baevski et al., 2020) and the Whisper encoder (Radford et al., 2022), for four epochs with a learning rate of $3 \\\\times 10^{-5}$ and a batch size of 128. For attack-augmented training, we increased the number of epochs to eight, as the model converges more slowly due to attacks. The probability of each attack was 10% and only one attack type was used for each utterance. For the evaluation metric, we adopted the widely used equal error rate (EER).\\n\\n4 Experimental Results\\n\\n4.1 Pairwise Cross-Model Evaluation\\n\\nAs illustrated in Figure 3, the pairwise evaluation indicates that the ADD system exhibits optimal performance when both the training and testing sets are derived from the same TTS model. This trend holds true irrespective of the speech prompts' domain (whether they originate from the in-domain LibriTTS dataset or the cross-domain TEDLium dataset), with the EERs consistently remaining below 1%. However, in the cross-model evaluation, the EERs vary significantly among different TTS model combinations. For example, the Wav2Vec2-base model was trained using data generated from a single TTS model and subsequently evaluated on data originating from other TTS models.\"}"}
{"id": "emnlp-2024-main-286", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance of Wav2Vec2-base measured by EER (%).\\n\\n| Attack       | In-model CD-ADD | Cross-model CD-ADD |\\n|--------------|-----------------|--------------------|\\n| Baseline     | 0.1 / 0.1       | 7.9 / 21.4         |\\n| Noise-white  | 9.4 / 9.1       | 34.7 / 45.0        |\\n| Noise-env    | 9.0 / 4.7       | 29.2 / 31.1        |\\n| Reverb       | 13.0 / 17.1     | 29.6 / 33.1        |\\n| LPF          | 1.3 / 1.2       | 14.3 / 23.4        |\\n| MP3          | 0.3 / 0.2       | 13.2 / 22.1        |\\n| Codec-12     | 2.9 / 1.4       | 21.4 / 31.0        |\\n| Codec-6      | 7.4 / 5.2       | 30.5 / 35.2        |\\n\\nTable 2: Performance of Wav2Vec2-base under various attacks measured by EER (%) on Libri and TED test sets respectively. \u201c+Aug.\u201d indicates all attacks are included during training.\\n\\nBase model fine-tuned with YourTTS-synthesized data can generalize to VALL-E-synthesized data, achieving EERs of 0.14% and 0.61% for the Libri and TED subsets of the CD-ADD test sets, respectively. However, it struggles to generalize to the Seamless Expressive model, resulting in much higher EERs of 29.71% and 44.00%. This indicates that randomly choosing a test set whose speech data is generated by a TTS model could result in overestimated generalizability of the ADD model, due to shared artifacts between TTS models and potential overfitting. Therefore, we selected Seamless Expressive as the test set as it has notably high EERs. It is worth noting that the model trained on the prevalent ASVSpoof dataset fails to generalize to the zero-shot TTS models. However, combining ASVspoof with the CD-ADD dataset can slightly improve the performance (Table 1), so these two datasets are combined by default in subsequent experiments.\\n\\n4.2 Comparisons Between Attacks\\n\\nAs shown in Table 2, without augmentation, all attacks negatively impact the model, with more noticeable effects in cross-model configurations. With attack-augmented training, the Wav2Vec2-base model demonstrates resilience against most attacks. In the in-model setup, the EERs of the attacked models are only slightly higher than the baseline. In the cross-model setup, a significant decrease in EERs is observed for the augmented model compared to the non-augmented model. Notably, certain attacks improve the ADD model\u2019s generalizability, as indicated by the reduced EERs in the TED subset. For example, compared with the EER of 10.1% for the baseline, the LPF reduces the EER to 8.9%, the MP3 compression reduces the EER to 8.3%, and the SepFormer reduces the EER to 5.5%. All these attacks remove spectral information and force the ADD model to rely more on features from the low-frequency band, thus mitigating overfitting. However, certain attacks, such as reverberation and the Encodec, lead to relatively high EERs. The encoder-decoder architecture and the vector quantization of the Encodec, especially at lower bit rates, have the potential to obliterate essential features for detecting synthetic speeches.\\n\\n4.3 Results of Few-Shot Fine-Tuning\\n\\nFigure 4 compares the cross-model ADD performance of three base models: Wav2Vec2-base, Wav2Vec2-large, and Whisper-medium. The Wav2Vec2-large and the Whisper-medium models have similar performance, notably superior to the Wav2Vec2-base model (Figure 4 (a, b)). With the most challenging Encodec attack, the Whisper model performs significantly better than the Wav2Vec2 models (Figure 4 (c, d)). We can also observe...\"}"}
{"id": "emnlp-2024-main-286", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"serve that with only one minute of in-domain data from Seamless Expressive, the EER can be reduced significantly. This suggests that our models are capable of fast adaptation to in-the-wild TTS systems with just a few samples from a demo website or a video, which is crucial for real-world deployment. However, we find that in-domain fine-tuning is less effective when the audio is compressed with the Encodec, as the reduction in EER is less significant.\\n\\n5 Conclusion\\nIn conclusion, our study presents a CD-ADD dataset, addressing the urgent need for up-to-date resources to combat the evolving risks of zero-shot TTS technologies. Our dataset, comprising over 300 hours of data from advanced TTS models, enhances model generalization and reflects real-world conditions. This paper highlights the risks of attacks and the potential of few-shot learning in ADD, facilitating future research.\\n\\n6 Limitation\\nThe current CD-ADD dataset is limited to five zero-shot TTS models. Future expansions are planned to include a broader range of zero-shot TTS models, as well as conventional TTS and VC models, to improve the dataset diversity. Additionally, the attack-augmented training is constrained to a single attack per sample, with separate analysis conducted for each attack. Subsequent research will focus on investigating the effects of combined attacks. Furthermore, the performance in ADD tasks with audio compressed by neural codecs is suboptimal, requiring the development of optimization strategies and the exploration of more neural codec models.\\n\\nReferences\\nZhongjie Ba, Qing Wen, Peng Cheng, Yuwei Wang, Feng Lin, Li Lu, and Zhenguang Liu. 2023. Transferring audio deepfake detection capability across languages. In Proc. ACM Web, pages 2033\u20132044.\\nA. Baevski, Y. Zhou, A. Mohamed, and M. Auli. 2020. Wav2Vec 2.0: A framework for self-supervised learning of speech representations. Proc. NeurIPS.\\nLo\u00efc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al. 2023. Seamless: Multilingual expressive and streaming speech translation. arXiv preprint arXiv:2312.05187.\\nEdresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren G\u00f6lge, and Moacir A Ponti. 2022. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone. In Proc. ICML, pages 2709\u20132720. PMLR.\\nEmanuele Conti, Davide Salvi, Clara Borrelli, Brian Hosler, Paolo Bestagini, Fabio Antonacci, Augusto Sarti, Matthew C Stamm, and Stefano Tubaro. 2022. Deepfake speech detection through emotion recognition: a semantic approach. In Proc. ICASSP, pages 8962\u20138966. IEEE.\\nDi Cooke, Abigail Edwards, Sophia Barkoff, and Kathryn Kelly. 2024. As good as a coin toss human detection of ai-generated images, videos, audio, and audiovisual stimuli. arXiv preprint arXiv:2403.16760.\\nAlexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438.\\nJoel Frank and Lea Sch\u00f6nherr. 2021. Wavefake: A dataset to facilitate audio deepfake detection. In Proc. NeurIPS.\\nFran\u00e7ois Hernandez, Vincent Nguyen, Sahar Ghannay, Natalia Tomashenko, and Yannick Esteve. 2018. Tedlium 3: Twice as much data and corpus repartition for experiments on speaker adaptation. In Proc. SPECOM, pages 198\u2013208. Springer.\\nZeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, et al. 2024. Natural-speech 3: Zero-shot speech synthesis with factorized codec and diffusion models. arXiv preprint arXiv:2403.03100.\\nPiotr Kawa, Marcin Plata, and Piotr Syga. 2022. Attack agnostic dataset: Towards generalization and stabilization of audio deepfake detection.\\nEugene Kharitonov, Damien Vincent, Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Sertan Girgin, Olivier Pietquin, Matt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. 2023. Speak, read and prompt: High-fidelity text-to-speech with minimal supervision. Transactions of the Association for Computational Linguistics, 11:1703\u20131718.\\nMatthew Maciejewski, Gordon Wichern, Emmett McQuinn, and Jonathan Le Roux. 2020. Whamr!: Noisy and reverberant single-channel speech separation. In Proc. ICASSP, pages 696\u2013700. IEEE.\\nJuan M Mart\u00edn-Do\u00f1as and Aitor \u00c1lvarez. 2022. The vicomtech audio deepfake detection system based on wav2vec2 for the 2022 add challenge. In Proc. ICASSP, pages 9241\u20139245. IEEE.\\nNicolas M\u00fcller, Pavel Czempin, Franziska Diekmann, Adam Froghyar, and Konstantin B\u00f6ttinger. 2022. Does audio deepfake detection generalize? INTERSPEECH 2022.\"}"}
{"id": "emnlp-2024-main-286", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jiahui Pan, Shuai Nie, Hui Zhang, Shulin He, Kanghao Zhang, Shan Liang, Xueliang Zhang, and Jianhua Tao. 2022. Speaker recognition-assisted robust audio deepfake detection. In Proc. Interspeech, pages 4202\u20134206.\\n\\nZengyi Qin, Wenliang Zhao, Xumin Yu, and Xin Sun. 2023. Openvoice: Versatile instant voice cloning. arXiv preprint arXiv:2312.01479.\\n\\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022. Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356.\\n\\nCem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, and Jianyuan Zhong. 2021. Attention is all you need in speech separation. In Proc. ICASSP, pages 21\u201325. IEEE.\\n\\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. 2023. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111.\\n\\nXin Wang, Junichi Yamagishi, Massimiliano Todisco, H\u00e9ctor Delgado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vestman, Tomi Kinnunen, Kong Aik Lee, et al. 2020. Asvspoof 2019: A large-scale public database of synthesized, converted and replayed speech. Computer Speech & Language, 64:101114.\\n\\nJunichi Yamagishi, Xin Wang, Massimiliano Todisco, Md Sahidullah, Jose Patino, Andreas Nautsch, Xuechen Liu, Kong Aik Lee, Tomi Kinnunen, Nicholas Evans, et al. 2021. Asvspoof 2021: accelerating progress in spoofed and deepfake speech detection. In ASVspoof 2021 Workshop\u2014Automatic Speaker Verification and Spoofing Countermeasures Challenge.\\n\\nRui Yan, Cheng Wen, Shuran Zhou, Tingwei Guo, Wei Zou, and Xiangang Li. 2022. Audio deepfake detection system with neural stitching for add 2022. In ICASSP 2022\u20142022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 9226\u20139230. IEEE.\\n\\nYujie Yang, Haochen Qin, Hang Zhou, Chengcheng Wang, Tianyu Guo, Kai Han, and Yunhe Wang. 2024. A robust audio deepfake detection system via multi-view feature. In Proc. ICASSP, pages 13131\u201313135. IEEE.\\n\\nJiangyan Yi, Ruibo Fu, Jianhua Tao, Shuai Nie, Haoxin Ma, Chenglong Wang, Tao Wang, Zhengkun Tian, Ye Bai, Cunhang Fan, et al. 2022. Add 2022: the first audio deep synthesis detection challenge. In Proc. ICASSP, pages 9216\u20139220. IEEE.\\n\\nJiangyan Yi, Jianhua Tao, Ruibo Fu, Xinrui Yan, Chenglong Wang, Tao Wang, Chu Yuan Zhang, Xiaohui Zhang, Yan Zhao, Yong Ren, et al. 2023. Add 2023: the second audio deepfake detection challenge. arXiv preprint arXiv:2305.13774.\\n\\nHeiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. 2019. Libritts: A corpus derived from librispeech for text-to-speech. Proc. Interspeech.\\n\\nXiao-Min Zeng, Jiang-Tao Zhang, Kang Li, Zhuo-Li Liu, Wei-Lin Xie, and Yan Song. 2023. Deepfake algorithm recognition system with augmented data for add 2023 challenge. In Proc. IJCAI Workshop on Deepfake Audio Detection and Analysis.\\n\\nA Appendix: Open Source Tools\\n\\nZero-shot TTS models:\\n\u2022 VALL-E: https://github.com/Plachtaa/VALL-E-X\\n\u2022 YourTTS: https://github.com/coqui-ai/TTS\\n\u2022 Seamless Expressive: https://github.com/facebookresearch/seamless_communication\\n\u2022 WhisperSpeech: https://github.com/collabora/WhisperSpeech?tab=readme-ov-file\\n\u2022 OpenVoice: https://github.com/myshell-ai/OpenVoice\\n\\nBase models:\\n\u2022 Wav2Vec2-base: https://huggingface.co/facebook/wav2vec2-base\\n\u2022 Wav2Vec2-large: https://huggingface.co/facebook/wav2vec2-large\\n\u2022 Whisper-medium: https://huggingface.co/openai/whisper-medium\\n\\nASR model:\\n\u2022 HuBERT-large-CTC: https://huggingface.co/facebook/hubert-large-ls960-ft\\n\\nAttacks:\\n\u2022 Noise-gate: https://github.com/timsainb/noisereduce\\n\u2022 SepFormer: https://huggingface.co/speechbrain/sepformer-whamr\\n\u2022 Codec-6/12: https://github.com/facebookresearch/encodec\"}"}
{"id": "emnlp-2024-main-286", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Num. | Total | Avg. |\\n|---------------|------|-------|------|\\n| Real          | 18339| 49.6  | 9.7  |\\n| V ALL-E       | 15869| 41.0  | 9.3  |\\n| Seamless Expressive | 17829| 42.6  | 8.6  |\\n| YourTTS       | 18202| 49.3  | 9.8  |\\n| WhisperSpeech | 18300| 54.8  | 10.8 |\\n| OpenVoice     | 18024| 40.9  | 8.2  |\\n\\nTable 3: The numbers of utterances (Num.), the total duration (Total), and the average duration of each utterance (Avg.) of the CD-ADD dataset.\\n\\n| Model          | WER  | Spk. |\\n|---------------|------|------|\\n| Real          | 2.4  | 1.00 |\\n| V ALL-E       | 10.1 | 0.56 |\\n| Seamless Expressive | 5.3  | 0.52 |\\n| YourTTS       | 5.4  | 0.53 |\\n| WhisperSpeech | 3.2  | 0.56 |\\n| OpenVoice     | 2.6  | 0.36 |\\n\\nTable 4: Zero-shot TTS performance measured by WER (%) and speaker similarity (Spk.).\\n\\nB Appendix: CD-ADD Dataset\\n\\nTable 3 presents the statistics of the CD-ADD dataset. The average utterance length exceeds eight seconds, which is longer than that of traditional ASR datasets. The number of utterances for TTS models is less than that of real utterances because some synthetic utterances fail to meet the CER requirements. Among them, V ALL-E has the fewest utterances due to the decoder-only model's relative instability. Table 4 compares five zero-shot TTS models in terms of the word-error-rate (WER) and speaker similarity. Speaker similarity is based on the LibriTTS test-clean subset, where ECAPA-TDNN is used to extract speaker embeddings. V ALL-E and WhisperSpeech have the highest speaker similarity scores, while OpenVoice ranks lowest. Conversely, V ALL-E achieves the highest WER, and OpenVoice has the lowest.\"}"}
