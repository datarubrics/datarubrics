{"id": "lrec-2024-main-118", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Unsupervised Framework for Adaptive Context-aware Simplified-Traditional Chinese Character Conversion\\n\\nWei Li\u2020, Shutan Huang\u2020, Yanqiu Shao\\nSchool of Information Science, Beijing Language and Culture University\\nXueyuan Road 15th, Haidian District, Beijing, China\\nliweitj47@blcu.edu.cn, shutan2022@163.com, shaoyanqiu@blcu.edu.cn\\n\\nAbstract\\nTraditional Chinese character is an important carrier of Chinese culture, and is still actively used in many areas. Automatic conversion between traditional and simplified Chinese characters can help modern people understand traditional culture and facilitate communication among different regions. Previous conversion methods rely on rule-based mapping or shallow feature-based machine learning models, which struggle to convert simplified characters with different origins and constructing training data is costly. In this study, we propose an unsupervised adaptive context-aware conversion model that learns to convert between simplified and traditional Chinese characters under a denoising auto-encoder framework requiring no labeled data. Our model includes a Latent Generative Adversarial Encoder that transforms vectors to a latent space with generative adversarial network, which adds noise as an inevitable side effect, Based on which a Context-aware Semantic Reconstruction Decoder restores the original input while considering a broader range of context with a pretrained language model. Additionally, we propose to apply early exit mechanism during inference to reduce the computation complexity and improve the generalization ability. To test the effectiveness of our model, we construct a high quality test dataset with simplified-traditional Chinese character text pairs. Experiment results and extensive analysis demonstrate that our model outperforms strong unsupervised baselines and yields better conversion result for one-to-many cases.\\n\\nKeywords: Simplified-traditional Chinese Character Conversion, Unsupervised Learning Framework, Multi-to-one Mapping Problem\\n\\n1. Introduction\\nTraditional Chinese characters have long been used in the history of China, and are still being actively applied in regions like Hong Kong and Taiwan. As a result of their widespread use, traditional Chinese characters play a crucial role in preserving historical knowledge and culture of China. Despite its importance, due to the adoption of simplified Chinese characters, many individuals in the mainland China struggle with reading and writing traditional Chinese characters. This difference in the writing system not only hinders the communication among different regions, but also brings difficulty for the modern Chinese in understanding their ancient culture. Therefore, it is of great significance to automatically convert between traditional Chinese characters and simplified Chinese characters.\\n\\nTraditional converting methods rely on mapping tables (Li et al., 2010). Such methods work for many of the characters. However, the simplification process of Chinese characters not only simplified the writing pattern, but also reduced the vocabulary size by merging different characters to a single one, creating ambiguity in the character system, which makes mapping-based methods ineffective when dealing with one-to-many conversion problems. To eliminate the ambiguity, Chen et al. (2011) propose to apply log-linear model with human engineered features. Although considers the context around the target characters, it is heavily reliant on the quality of human-crafted features and can only model shallow semantics within a limited context. Furthermore, compared with the traditional mapping based methods, such supervised methods require large amount of labeled training data to cover as many as Chinese characters, which is costly to build and may render the model vulnerable to out-of-vocabulary problems.\\n\\nTo make the task more difficult, the documents cover a long period of time, ranging from ancient to modern times, resulting in a diverse range of expression patterns. Therefore, it is essential for the model to be adaptive to the targeted documents from different times. Furthermore, due to the variance of expression forms, it is expensive and impractical to manually align text pairs of traditional and simplified Chinese characters across different ages, making self-supervised methods an attractive alternative. However, we observe that luckily many characters remain the same after simplification and the character number (sentence length) within the source text is also the same. These features make this task more plausible to apply unsupervised methods.\\n\\nInspired by the success of pretrained language models (PLM), we propose to introduce PLM to\"}"}
{"id": "lrec-2024-main-118", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"model the semantic context of targeted characters for character disambiguation, which is powerful for long range dependency. To liberate the model from the need for parallel labeled data and enhance its adaptability to diverse target texts, we propose an unsupervised learning model under a denoising auto-encoder framework. Our model takes monolingual text in characters (either simplified or traditional) as input, which are encoded and mapped into the representation space of the other character set (e.g., simplified as input, mapped into traditional space) with Latent Generative Adversarial Encoder. The model is then asked with restoring the original text from the mapped hidden states, which finishes the loop of auto-encoding. Due to the unsupervised nature of the encoding process, noise is inevitably introduced, which makes the entire training loop equivalent to denoising from polluted representations. Based on the observation that different characters require varying levels of contextual information (e.g., many characters remain the same after simplification), we propose to incorporate early exit mechanism to optimize the computation complexity considering the needs of each character, which also enhances the generalization of the model.\\n\\nTo test the effectiveness of our proposed method, we collect high-quality text pairs of traditional and simplified Chinese characters from Chinese Text Project as the test set. The experimental results show that our model outperforms all existing publicly available software for simplified-traditional Chinese character conversion. Extensive analysis demonstrates that our proposed early exit mechanism can not only reduce the theoretic computation complexity, but also improve the prediction accuracy because of its better generalization ability.\\n\\nWe conclude our contributions as follows,\\n\\n\u2022 We propose an unsupervised adaptive context-aware model that learns to convert between simplified and traditional Chinese characters under a denoising auto-encoder framework, requiring no parallel data.\\n\u2022 We propose to model the context information with pretrained language model and apply early exit mechanism when predicting the target character, which reduces the theoretical computation complexity, as well as improving the generalization ability of the model.\\n\u2022 We construct a test set with high quality text pairs. Extensive experiments testify that our model surpasses the existing publicly available simplified-traditional Chinese character conversion systems.\\n\\n2. Approach\\nIn this section, we introduce our Unsupervised Adaptive Context-aware Conversion Model, whose overall framework is shown in Figure 1. To liberate the model from the need for parallel data, we propose to train our model under a denoising auto-encoder framework, which comprises three components: the Latent Generative Adversarial Encoder, the Context-aware Semantic Reconstruction Decoder, and the Token Prediction Module with Early Exit Mechanism. The encoder looks up the original embedding of the characters and maps the embedded input into the latent semantic space, which should be as similar as possible to the counterpart character space achieved by a generative adversarial network (GAN). The decoder then restores the original semantic representation while considering contextual information. The token prediction module predicts the original input based on the contextual representation, which applies early exit mechanism to consider needs of different contextual levels. The parameters of the output layer within token prediction module are shared with the embedding layer of the encoder module, making the parameters more efficient. The denoising auto-encoder training process is computed as follows,\\n\\n1. Take the text of only traditional (denoted as $I_t$) Chinese characters as input and gets their embeddings (denoted as $e_t$) by looking up in the traditional Chinese character embedding table $E_t$.\\n2. Map the vector sequence $e_t$ into the counterpart latent semantic space (simplified Chinese character space) with a linear transformation and yields $e_s$, which can be seen as the process of adding noise on the embedding level. A discriminator is introduced to make the transformed vectors indistinguishable from the real simplified Chinese character space under GAN.\\n3. Model the contextual information by feeding $e_s$ into BERT pretrained on the simplified Chinese character text and gets the representation $h_s$. Note that the BERT is in accordance with the latent semantic space (e.g., simplified Chinese).\\n4. Restore the original traditional text sequence $I_t$ by predicting the original text input based on $h_s$.\\n\\nThe detailed computation process is illustrated in Algorithm 1.\"}"}
{"id": "lrec-2024-main-118", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1: Overall Architecture of the proposed model taking simplified to traditional conversion as an example. On the left is the auto-encoder training procedure, on the right is the data flow of training and inference. The shadowed area indicates the latent semantic space of simplified Chinese. The part below the dashed line is the encoder. A discriminator is employed to ensure that the transformed embedding is mapped towards the simplified embedding space by the transformation matrix in GAN. The contextual information is modeled using a PLM, and the output representation is transformed back to the traditional Chinese character space.\\n\\n2.1. Latent Generative Adversarial Encoder\\n\\nWe first separately learn the character embeddings of simplified and traditional Chinese character texts with unsupervised method word2vec (Mikolov et al., 2013) with monolingual data, yielding $E_s$ and $E_t$ for simplified and traditional Chinese.\\n\\nInspired by the approach of unsupervised machine translation (Lample et al., 2018), we propose to map the vectors in the traditional Chinese character space into the simplified Chinese character space (taking simplified to traditional conversion as an example) with a linear transformation $e'_s = W_t e_t$.\\n\\nTo learn more accurate transformation matrix, we apply the Generative Adversarial Network (GAN) (Goodfellow et al., 2014) framework following Lample et al. (2018), where a Discriminator is applied to judge whether the vector is from the target semantic space ($E_s$), the prediction score of which is calculated as,\\n\\n$$s_D = \\\\sigma(W_D \\\\tilde{e}) \\\\tag{1}$$\\n\\nwhere $W_D$ is the parameters of the discriminator, $\\\\tilde{e}$ indicates the vector to be judged, $\\\\sigma$ is the sigmoid activation function that maps the output to the range within $0 \\\\sim 1$.\\n\\n$W_t$ plays the role of generator, whose objective is to confuse the discriminator from recognizing the transformed vectors $e'_s$.\\n\\nFollowing the canonical GAN training paradigm, the discriminator and the generator are trained iteratively, which stops at the point where the discriminator can hardly distinguish the difference between the generated vector and the real ones.\\n\\n2.2. Context-aware Semantic Reconstruction Decoder\\n\\nAlthough there is still a discrepancy between the transformed vectors $e'_s$ and the real vectors from the simplified Chinese character space, the transformed vectors can be interpreted as distorted vectors with noise. By taking the surrounding context into consideration, our model can not only reduce the influence caused by the transformation process, but also make more accurate predictions when encountering one-to-many mappings. Therefore, we propose to use the pretrained language model trained on the target text (simplified Chinese character text) to model a more extensive range of context.\\n\\n$$h_s = PLM(e'_s) \\\\tag{2}$$\\n\\nConcretely, we use roberta-classical-chinese as the PLM in Equation 2 to obtain the semantic representation with context information. After obtaining the contextualized representation $h_s$ with the pretrained language model, we propose a symmetrical transformation process to map the vectors back to the original input semantic space one by one. Note that this transformation is applied\\n\\n3 https://huggingface.co/KoichiYasuoka/roberta-classical-chinese-base-char\"}"}
{"id": "lrec-2024-main-118", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2: Inference Procedure with Early Exit Mechanism\\n\\nRequire:\\n- Input Text \\\\( S \\\\) in simplified Chinese character\\n- Word Embedding Lookup Table for Simplified Chinese characters \\\\( E_s \\\\)\\n- Pretrained Language Model \\\\( PLM \\\\), Transformation Matrix \\\\( W \\\\) and \\\\( W' \\\\), Exit threshold\\n\\n1. for each sample text \\\\( s \\\\) in \\\\( S \\\\) do\\n2. \\\\( e_s \\\\leftarrow E_s(s) \\\\) // lookup the embeddings of \\\\( s \\\\) in \\\\( E_s \\\\)\\n3. \\\\( h_0 \\\\leftarrow e_s \\\\)\\n4. for \\\\( j \\\\) in 1 to total layer number do\\n5. \\\\( h_j \\\\leftarrow PLM_j(h_{j-1}) \\\\) // get the \\\\( j \\\\)-th layer hidden states with \\\\( PLM_j \\\\)\\n6. \\\\( \\\\tilde{h}_j \\\\leftarrow W_j h_j \\\\) // transform to the traditional Chinese space\\n7. \\\\( p_j = \\\\text{softmax}(W_{\\\\text{out}} \\\\tilde{h}_j) \\\\)\\n8. if \\\\( \\\\max(p_j) > \\\\text{threshold} \\\\) // exit when exceeding threshold\\n9. then\\n10. return \\\\( p_j \\\\)\\nAfter getting the prediction probability \\\\( p \\\\), we can calculate the cross entropy between \\\\( p \\\\) and input \\\\( I_t \\\\) as the loss function,\\n\\\\[\\n\\\\text{loss} = -\\\\sum_i I_{it} \\\\log p_i\\n\\\\]\\nwhere \\\\( I_{it} \\\\) indicates the \\\\( i \\\\)-th input token, which also serves as the restoration target in our auto-encoder.\\n\\n2.3. Inference with Early Exit Mechanism\\n\\nDuring inference, only the decoder part of the model is needed. The traditional to simplified decoder is taken from the model trained on only simplified Chinese documents, while the simplified to traditional decoder is taken from the model trained on only traditional Chinese documents. However, the aforementioned model suffers from the over-computation problem during inference due to the fact that many characters remain unchanged after conversion. This means that shallow context information is sufficient for such characters, and forwarding all the hidden layers of the pretrained language models is unnecessary. To address this issue, we propose to introduce early exit mechanism to our model. Inspired by DeeBERT (Xin et al., 2020), we add an additional transformation module to each candidate layer that transforms the hidden states of that layer to the space of the last hidden layer, which is then used for prediction, calculated as follows:\\n\\n\\\\[\\n\\\\tilde{h}_j = W_j \\\\text{exit} h_j\\n\\\\]\"}"}
{"id": "lrec-2024-main-118", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where \\\\( j \\\\) indicates the \\\\( j \\\\)-th layer, \\\\( \\\\mathbf{W} \\\\) indicates the transformation matrix for the \\\\( j \\\\)-th layer.\\n\\nDuring training, all the candidate layers will predict the original character token based on their transformed hidden states \\\\( \\\\tilde{h}_j \\\\), \\\\( \\\\tilde{p}_j = \\\\text{softmax} (\\\\mathbf{W}_{\\\\text{out}} \\\\tilde{h}_j) \\\\) (7).\\n\\nNote that the parameters for output \\\\( \\\\mathbf{W}_{\\\\text{out}} \\\\) is shared across different layers including the final layer in Equation 4. To optimize the transformation parameters of each layer, the new loss function becomes the sum of the cross entropy for each layer, \\\\( \\\\text{loss}_{\\\\text{exit}} = -\\\\sum_i \\\\sum_j I_t \\\\log \\\\tilde{p}_j \\\\) (8).\\n\\nDuring Inference, each candidate layer in the decoder module predicts the traditional Chinese character conversion step by step, and the prediction process stops when the prediction score (confidence) of a layer achieves a certain threshold, which is a pre-defined empirical hyper-parameter. This early exit mechanism can not only reduce the theoretical computation complexity, but also help alleviate the over-fitting problem. The detailed computation process is shown in Algorithm 2.\\n\\nAdditionally, to cope with the out-of-vocabulary (OOV) problem, we propose a technique called \\\"OOV Skipping\\\", which involves using the original input as the conversion result for characters that appear less than a predefined threshold (e.g., 5 times). This is based on the observation that the Chinese character simplification process mostly focuses on the frequently used characters, which means it is reasonable to keep infrequent characters the same before and after conversion.\\n\\n3. Experiment\\n\\nIn this section, we present the experimental details and extensive analysis of the results.\\n\\n3.1. Data\\n\\nWe collect the data from an online open-access digital library CTEXT. We utilize the \\\"zhon\\\" package for sentence segmentation. The simplified to traditional Chinese conversion model is taken from the Inference module of the model trained on traditional Chinese documents of 1,141,001 sentences. The traditional to simplified Chinese conversion model is taken from the Inference module of the model trained on simplified Chinese documents of 1,141,001 sentences. To highlight the one-to-many problem, we choose 20,000 high-quality sentence pairs with one-to-many characters as the test set that are proof-read by humans.\\n\\n4. Additional Notes\\n\\nhttps://ctext.org\\nhttps://github.com/tsroten/zhon\\nhttps://radimrehurek.com/gensim/models/word2vec.html\\nhttps://github.com/BYVoid/OpenCC\\nhttps://github.com/gumblex/zhconv\\nhttps://pypi.org/project/pylangtools/\\njianfan.hwxnet.com\\naies.cn\\nwww.kjson.com/office/zhcn_zhtw\\nhttp://chat.openai.com/\"}"}
{"id": "lrec-2024-main-118", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Character-wise Accuracy Comparison of Unsupervised Baselines and Proposed Method for Chinese Character Conversion. We apply character-wise accuracy because this is a strictly one-to-one conversion task, which is different from sequence-to-sequence translation. The conversion directions are indicated by \u201cS-to-T\u201d for converting from simplified to traditional Chinese characters and \u201cT-to-S\u201d for the reversed direction. Our proposed method achieves the highest accuracy compared to the baselines for both directions.\\n\\n3.4. Result\\nIn this subsection, we present the character-wise accuracy of our proposed model compared with the baseline models in Table 1. It shows that our model achieves the highest accuracy for both directions of Chinese character conversion. Remarkably, even the proposed base model outperforms all the baselines, providing compelling evidence for the effectiveness of our proposed method. Moreover, even with initially aligned seed tokens, Artetxe et al. (2018b) reaches a bottleneck because of the lack of considering context information. Furthermore, we notice that although large language models like ChatGPT can perform simplified-traditional Chinese conversion, it does not yield satisfactory results under zero-shot setting. Additionally, we note that converting from simplified to traditional Chinese characters is more challenging than the opposite direction due to the fact that one simplified Chinese character can have multiple traditional Chinese character counterparts, whereas one traditional Chinese character usually maps to only one simplified Chinese character. As a result, converting from simplified Chinese characters requires disambiguation, making it more complex.\\n\\n3.5. Ablation Study\\nIn this part, we show the results of each designed module in our model. The basic module named Latent Generative Adversarial Encoder (mentioned in section 2.1) learns a linear mapping from the simplified Chinese characters space to the traditional Chinese characters space with GAN. From the result we can see that because this kind of static model does not consider the target context, it does not yield very satisfactory accuracy (93.91), which is just slightly better than the performance of OpenCC, which also applies a static mapping strategy. By further applying our proposed context-aware semantic reconstruction decoder (mentioned in section 2.2), the accuracy improves significantly reaching 96.79, which is comparable to the strong baselines (e.g., MSWord 96.89, pylangtools 96.77, etc.). This improvement validates the effectiveness of our proposed denoising auto-encoder training method, which restores the original input by considering semantic context to alleviate the noise introduced by linear embedding space transformation. Additionally, we observe that sharing parameters between the encoder and the decoder by unifying the restored embedding space and the real input embedding space further improves the accuracy to 97.13, surpassing most of the baselines except for the website aies.cn.\\n\\nFurthermore, we can observe that applying early exit not only reduces the theoretical computation complexity, but also improves the conversion accuracy to 98.35, which is higher than all the baselines. We argue that this is because dynamically selecting the exiting point can improve the generalization ability, which is especially important for those infrequent characters. Finally, the post processing that directly predicts the infrequent characters to themselves gives a small improvement. We assume that this is because although this operation can fix some errors made by the model, the frequency of occurrence of such characters in the corpus is relatively small.\\n\\n3.6. Analysis\\n3.6.1. Multiple Mapping\\nIn this part, we analyse the multiple mapping phenomenon in simplified-to-traditional Chinese character conversion task. We separately show the accuracy for different mapping patterns in Figure 2 with bar plot, while their corresponding sentence appearance ratio is shown with line plot. The ratio is calculated as the number of sentences where the\"}"}
{"id": "lrec-2024-main-118", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: This table displays the results of an ablation study where modules were added one by one to evaluate their impact on the performance of the proposed model. The pattern appears divided by the total number of sentences, therefore they do not sum to 1, as different patterns can appear within the same sentence.\\n\\nFrom the results we can see that generally the more complex the mapping pattern is, the lower the accuracy gets, which is in accordance with the intuition. However, the gap among 1 vs. 2, 1 vs. 3 and 1 vs. 4 is quite small, which are less than 0.02 percent. The most complicated mapping pattern (m vs. n) would influence the accuracy the most. Luckily, generally the more complex the pattern is, the lower ratio it takes in the whole dataset. The m vs. n pattern only takes a small part of the data, which means it would not severely influence the overall performance.\\n\\n3.6.2. Backbone PLM\\n\\nIn this part, we show the results obtained by testing our model with different pretrained language models as backbone in Figure 3. Specifically, we evaluated guwen-bert, roberta-classical-chinese and bert-ancient-chinese, where both base model and large model of guwen-bert and roberta-classical-chinese are applied, noted respectively as \u201cbase\u201d and \u201clarge\u201d. From the results we can see that roberta based model is generally better than bert based model. More importantly, large models generally gives better result over base models, which is expected. However, the gap between base model and large model is not very significant, indicating that base models could also be used when faster inference speed is a more important factor to consider.\\n\\n3.6.3. Sentence Length\\n\\nIn this part, we investigate the effect of maximum sentence length. We evaluate the performance of the proposed model with increasing maximum sentence length and compare it with the baseline encoder-only model. The results are shown in Figure 4. The results indicate that as the maximum sentence length increases, the accuracy of the proposed model first improves and then stabilizes after a certain length. This suggests that longer context can provide more information for predicting the target character, which is in accordance with our hypothesis. However, the accuracy of the baseline encoder-only model remains the same across different maximum sentence lengths, as it does not consider context. Overall, these results demonstrate the importance of context in the simplified-to-traditional Chinese character conversion, and highlight the effectiveness of the proposed model in utilizing context to improve performance.\\n\\n3.7. Case Study\\n\\nIn this subsection, we provide some concrete examples for simplified Chinese character to traditional Chinese character conversion and compare the results of our proposed method with two strong baselines. The correct predictions are in black, while the wrong predictions are in red.\\n\\nIn the first set of examples, our model gives the correct predictions in the second two cases, while gives wrong predictions in the other two cases. We assume that this is because the usage of last three traditional Chinese character regarding to the character \\\"\u91cc\\\" (inner) is very similar, which all follow the appearance of \\\"\u8868\\\"(surface), forming the word \\\"\u8868\u91cc\\\" still in usage nowadays. In fact, the second and third case \\\"\u88e1\\\" and \\\"\u88cf\\\" in traditional Chinese character systems are variant Chinese characters, expressing the same meaning with different forms, while the fourth case \\\"\u91cc\\\" is actually misused because such occurrence usually means the residence place or distance unit rather than the normally referred meaning of inner. For the first case, our model correctly predicts \\\"\u91cc\\\", which means the distance unit. However, the strong baseline models (zhconv and aies.com) give the wrong characters (\\\"\u88e1\\\" and \\\"\u88cf\\\"). This shows the superiority of our model on contextual semantic modeling.\\n\\nIn the second set of examples, the simplified Chinese character \\\"\u6597\\\" has two distinct meanings, weighting bucket \\\"\u6597\\\" (extended in meaning the Plough star) pronounced dou3 and fighting \\\"\u9b25\\\" pronounced dou4. Our model gives the correct predictions in both of the cases, while the strong baselines incorrectly predict the character for fighting \\\"\u9b25\\\" in the first case.\\n\\nThe third set of examples involves the simplified Chinese character \\\"\u9965\\\", which has two closely re-\"}"}
{"id": "lrec-2024-main-118", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Accuracy and Appearance Ratio of Mapping Patterns in Simplified-to-Traditional Chinese Character Conversion.\\n\\nFigure 3: Accuracy of different backbone PLMs for simplified to traditional Chinese conversion. Both large and base models are included, except for \u201cBert-ancient-Chinese\u201d.\\n\\nFigure 4: Accuracy of Proposed Model and Encoder-Only Base-line for Different Maximum Sentence Lengths in Simplified-to-Traditional Chinese Character Conversion.\\n\\nFigure 5: Examples of Simplified-to-Traditional Chinese Character Conversion with Incorrect Predictions Highlighted in Red. The comparison targets are in bold. The translations are provided in the Appendix.\\n\\n4. Related Work\\n\\nAlthough simplified-traditional Chinese character conversion is a practical and important task, there has not been much research on it. Early works on simplified-traditional Chinese character conversion apply table lookup methods (Li et al., 2010). This kind of methods consider little context information, and thus yielding weak performance. Hao and Zhu (2011) also address the importance of the one-to-many cases in simplified to traditional Chinese character conversion. However, their proposed Fused Conversion Algorithm from Multi-Data resources only considers n-gram statistical model, which is too shallow to model enough contextual semantics.\\n\\nSimilar to their work, Chen et al. (2011) and Shi et al. (2011) propose to use log-linear model that takes features such as language models and lexical semantic consistency weighs. Xu et al. (2017) still apply statistical conversion model, but provides a proof reading web interface.\\n\\n5. Conclusion\\n\\nIn this work, we propose an unsupervised adaptive context-aware model for the simplified-traditional Chinese character conversion task, which not only liberates the model from the needs of parallel data, but also endows it with better adaptation ability. To alleviate the one-to-many problem, we propose to introduce PLM for contextual semantic modeling in a reconstruction decoder, which restores the original input in a denoising auto-encoder framework. Based on the observation that different characters may require different levels of semantic modeling, we propose to apply early exit mechanism for inference, which can not only reduce theoretical computation complexity, but also improve the generalization ability. Experiments on the constructed simplified-traditional Chinese character conversion test set show the superiority of our proposed model.\"}"}
{"id": "lrec-2024-main-118", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"against strong baselines. Extensive analysis testifies that modeling contextual information with pre-trained language models can indeed help distinguish different meanings in the one-to-many scenarios.\\n\\nAcknowledgements\\n\\nThis research project is supported by the National Natural Science Foundation of China (No. 62306045), Science Foundation of Beijing Language and Culture University (supported by \\\"the Fundamental Research Funds for the Central Universities\\\") (No. 21YBB19).\\n\\n6. Bibliographical References\\n\\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018a. Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pages 5012\u20135019.\\n\\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018b. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 789\u2013798.\\n\\nYidong Chen, Xiaodong Shi, and Changle Zhou. 2011. A simplified-traditional Chinese character conversion model based on log-linear models. In 2011 International Conference on Asian Language Processing, pages 3\u20136.\\n\\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial networks.\\n\\nTianyong Hao and Chunshen Zhu. 2011. Simplified-traditional Chinese character conversion based on multi-data resources: Towards a fused conversion algorithm. Volume 3, pages 50\u201356.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\\n\\nGuillaume Lample, Alexis Conneau, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2018. Word translation without parallel data. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.\\n\\nMin-Hsiang Li, Shih-Hung Wu, Ping-che Yang, and Tsun Ku. 2010. (Chinese characters conversion system based on lookup table and language model) [in Chinese]. In Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing (ROCLING 2010), pages 113\u2013127, Nantou, Taiwan. The Association for Computational Linguistics and Chinese Language Processing (ACLCLP).\\n\\nTom\u00e1s Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In 1st International Conference on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings.\\n\\nPeter H Sch\u00f6nemann. 1966. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):1\u201310.\\n\\nXiaodong Shi, Yidong Chen, and Xiuping Huang. 2011. Key problems in conversion from simplified to traditional Chinese characters. In International Conference on Asian Language Processing.\\n\\nPengyu Wang and Zhichen Ren. 2022. The uncertainty-based retrieval framework for ancient Chinese CWS and POS. In Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages, pages 164\u2013168.\\n\\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. DeeBERT: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2246\u20132251, Online. Association for Computational Linguistics.\\n\\nJiarui Xu, Xuezhe Ma, Chen-Tse Tsai, and Eduard Hovy. 2017. STCP: Simplified-traditional Chinese conversion and proofreading. In Proceedings of the IJCNLP 2017, System Demonstrations, pages 61\u201364, Tapei, Taiwan. Association for Computational Linguistics.\"}"}
