{"id": "lrec-2024-main-1518", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What do Transformers Know about Government?\\n\\nJue Hou, \u2020 \u22c6\\nAnisia Katinskaia, \u2020 \u22c6\\nLari Kotilainen, \u2662\\nSathianpong Trangcasanchai, \u2020\\nAnh-Duc Vu, \u2020 \u22c6\\nRoman Yangarber \u22c6\\n\\nUniversity of Helsinki, Finland\\n\u2020 Department of Computer Science\\n\u2662 Department of Finnish Language and Culture\\n\u22c6 Department of Digital Humanities\\nfirst.last@helsinki.fi\\n\\nAbstract\\n\\nThis paper investigates what insights about linguistic features and what knowledge about the structure of natural language can be obtained from the encodings in transformer language models. In particular, we explore how BERT encodes the government relation between constituents in a sentence. We use several probing classifiers, and data from two morphologically rich languages. Our experiments show that information about government is encoded across all transformer layers, but predominantly in the early layers of the model. We find that, for both languages, a small number of attention heads encode enough information about the government relations to enable us to train a classifier capable of discovering new, previously unknown types of government, never seen in the training data.\\n\\nCurrently, data is lacking for the research community working on grammatical constructions, and government in particular. We release the Government Bank\u2014a dataset defining the government relations for thousands of lemmas in the languages in our experiments.\\n\\nKeywords: transformer language models, BERT, government relations, probing\\n\\n1. Introduction\\n\\nMany contemporary linguistic theories, such as Construction Grammar (Fillmore and Kay, 1996; Goldberg, 2006), view form-function pairs, called constructions, as fundamental elements of language. In this paper, our interest in constructions is driven by both theoretical and practical considerations. From the theoretical perspective, describing linguistic phenomena in terms of constructions provides a powerful and convenient means of capturing the complex interactions at the interface between lexical semantics and syntax.\\n\\nIn practical terms, constructions can provide a way of tracking language learners' progress. Constructions can be seen as describing the linguistic knowledge of a native speaker, and thus as constituting the knowledge needed to master a language\u2014mastery of a language is mastery of its constructions. If we had a description of the constructions of a language, we could follow a learner's progress in terms of the proportion of constructions s/he has mastered so far; plan learning paths in terms of constructions of increasing complexity, etc.\\n\\nThe present study focuses on one of the most fundamental types of constructions: patterns of government.\\n\\n1\\nA verb has several syntactic dependents (arguments) in a sentence; they are analyzed as complements vs. adjuncts. One may say \u201cI listened to many songs on a trip through Europe.\u201d Here the verb listen has two prepositional phrase (PP) modifiers: A. \u201cto many songs\u201d, and B. \u201con a trip\u201d. Phrase A is a complement\u2014it is semantically required by the verb; phrase B is an adjunct\u2014it is optional, in that many actions can take place while \u201con a trip,\u201d nothing binds this PP to this verb in particular. However, in \u201cI went on a trip,\u201d the same PP is a complement of went. Government is about which complements a verb governs: about its valency\u2014the patterns of complements that a verb expects in a sentence. Our linguistic competency informs us that one listens to a sound / person / hunch / etc., while one goes on a trip / rampage / quest / etc.\\n\\nThe high performance of pre-trained transformer Language Models (LM) on many NLP tasks has stimulated an interest in investigating the inner representations of these models to find out how linguistic knowledge is encoded inside them, and whether it agrees with linguistic theories in general, and theories of grammar in particular. Corresponding to our theoretical and practical objectives, this paper explores two Research Questions, respectively\u2014\\n\\nRQ1: do transformer LMs encode knowledge about government, and where this knowledge is represented; and\\nRQ2: can we extract this knowledge about government from the LM, to build government resources (or enhance existing resources), e.g., for language learning. To our knowledge, how LMs encode government has not been explored systematically to date.\\n\\nTo explore RQ1, we probe the BERT model using...\"}"}
{"id": "lrec-2024-main-1518", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We find that information about government relations is encoded across all layers and heads of BERT; however, there is enough information in the initial (5 or 7) layers to detect the presence of the government relations with almost the same accuracy as by using all layers. Probing classifiers indicate that just a few attention heads encode government relations, and can be used for government prediction without a significant loss in accuracy.\\n\\nTo explore RQ2, we perform experiments on (a) detecting governing verbs and (b) detecting government patterns, which were held out from training data. Results show that probing classifiers perform well on both tasks, and therefore can be used to discover new, previously unseen patterns.\\n\\nDescriptions of constructions are important for both general and computational linguistics research, evidenced, e.g., by the \\\"constructicon\\\" efforts (Janda et al., 2018; Hunston, 2019; Lyngfelt et al., 2018), although the resources resulting from this work are for human consumption, not directly usable, e.g., in CALL systems.\\n\\n2 Collecting banks of constructions for any language consumes a great deal of labor; we propose a method for searching for patterns in collections of text automatically, by leveraging the information already learned by the LM and encoded in its attention heads.\\n\\nThe paper is organized as follows: Section 2 briefly reviews prior work on probing neural language models, and Section 3 provides some background on syntactic government. Section 4 introduces our government data and its structure. Section 5 introduces our probing classifier and the experimental setup. Section 6 discusses the results of our experiments. Section 7 concludes with a discussion of future work.\\n\\n2. Related Work\\n\\nLanguage modeling is a fundamental task in natural language processing (NLP), on which pretrained language models (PLMs) currently achieve the best performance. One approach to making inferences about the model \\\"internals\\\" is through probing, also known as BERTology (Rogers et al., 2021). Approaches to probing PLMs usually include a specific probing task\u2014e.g., investigating predicate-argument agreement, or how gender is encoded in contextual representations,\u2014data prepared for this task\u2014e.g., minimal pairs of examples that differ only by the studied linguistic category,\u2014and some mechanism that allows us to interact with or query the model's components. Such a mechanism can be a probing classifier (or probe). The behavior of a simple probe, trained on representations from the PLM, on a probing task can inform us whether the representations include the linguistic information in question (Adi et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019; Dalvi et al., 2019; Maudslay et al., 2020; Weissweiler et al., 2022; Conia and Navigli, 2022; Arps et al., 2022). Some researchers criticize probing classifiers and question their effectiveness, in particular, whether the probed LM in fact uses the information that is discovered by the probe (Hewitt and Liang, 2019; Tamkin et al., 2020; Ravichander et al., 2021; Voita and Titov, 2020). Later research suggests this criticism can be addressed by designing appropriate control tasks and datasets (Belinkov, 2022).\\n\\nThere are two research directions in probing for dependency relations: token representation or the weights of attention heads. Christopher et al. (2020) reconstruct dependency structures based on token representation. Wu et al. (2020) propose parameter-free probing based on masking tokens and measuring the impact of the masked tokens. They found that a Masked LM (MLM) such as BERT can learn the \\\"natural\\\" dependency structure of language. Although the dependencies learned by the MLM may differ from human annotation or linguistic theory, they consider it a good \\\"lower bound\\\" for unsupervised syntactic parsing.\\n\\nChristopher et al. (2020) and Clark et al. (2019) probe attention head weights. They find that no single attention head corresponds well to dependency syntax overall, however, the combination of several attention heads can correspond substantially better than a baseline, where dependency is defined simply by a fixed offset. Furthermore, they find certain attention heads are specialized in certain dependency relations. Kovaleva et al. (2019) analyze different attention patterns and suggest that some attention heads could potentially be linguistically interpretable. Our work is inspired by these studies, and we likewise study how information on government is distributed among attention heads.\\n\\nWeissweiler et al. (2023) analyze methodologies for probing applied to constructions, as well as probing specifically for certain constructions. They stress that probing constructions is challenging for several reasons, in particular, the non-compositional meaning of constructions and the training objectives of PLMs.\\n\\n3. Background on Government\\n\\nGovernment is a relation between a token and its syntactic dependents. In the context of this paper, the term government refers to relations between a governor (verb, noun, adjective) and a \\\"governee,\\\" which can be a noun (phrase), infinitive verb (with its own dependents), an adpositional phrase, etc. Here are some examples from Finnish:\"}"}
{"id": "lrec-2024-main-1518", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Examples of government relations: verb lemmas with their complements. The direct object is displayed as a \u201cspecial\u201d argument, for transitive verbs; other arguments follow. Third and fourth verbs show examples of a post-position \u201cpuolesta\u201d (for) and a preposition \u201c\u043a\u201d (to).\\n\\n(1)\\nEhdotan recommend. V.Pres.1P.Sg\\nteille you. Pl.Allat\\npitk\u00a8 a\u00a8 a long. Sg.Partit\\nlomaa. break. Sg.Partit\\n\\\"I recommend to you a long break.\\\"\\n\\nIn example 1, the governor verb \u201cehdottaa\u201d (recommend) requires a direct object\u2014a noun phrase \u201cpitk\u00a8a loma\u201d (long break), in the partitive case. The same governor requires another complement\u2014pronoun \u201cte\u201d (you) to be in the allative case.\\n\\n(2)\\nKuva picture. Sg.Nominat\\nhelpottaa ease. Pres.3P.Sg\\nymm\u00a8 art\u00a8 am\u00a8 a\u00a8 an understand.3-Infinit\\nasiaa. thing. Sg.Partit\\n\\\"A picture makes it easier to understand.\\\"\\n\\nIn example 2, the governor \u201chelpottaa\u201d (ease) requires its argument\u2014verb \u201cymm \u00a8art \u00a8a \u00a8a\u201d (to understand) to be in the illative case of 3-rd infinitive.\\n\\n(3)\\nH\u00a8 an S/he. Sg.Nominat\\nprotestoi protest. Past.3P.Sg\\np\u00a8 a\u00a8 at\u00a8 ost\u00a8 a decision. Sg.Partit\\nvastaan. against. Postpos\\n'S/he protested the decision.'\\n\\nIn example 3, the governor \u201cprotestoida\u201d (protest) requires the specific postposition \u201cvastaan\u201d, which in turn governs a noun phrase in partitive case.\\n\\nIn the context of this paper, we focus only on verbs as governors, and use the terms \u201ccomplement\u201d, \u201cargument\u201d, or \u201cgovernee\u201d interchangeably.\\n\\n4. Government Data\\nAs of publish date, we collaborated with expert linguists, and manually collected 1184 Finnish government rules for 765 verb lemmas, and 2635 Russian governments for 1976 verb lemmas, together with several examples for each rule. For each Finnish verb, we include information about its transitivity (whether it is transitive or intransitive) and its government rules, as illustrated in Table 1.\\n\\nEach object or governee indicates the part of speech governed by the verb; note, that the governee can have its own dependents. Case denotes the case governed by the verb, preposition, or postposition. Each verb governor can govern more than one argument. We release this extensive Government Bank, which encodes thousands of government patterns for verbs, nouns, and adjectives\u2014for Finnish and Russian. The Bank is not \u201ccomplete\u201d, but rather encodes the most frequent government patterns in each language. The patterns are stored in a structured, human-readable form, easily transformed into JSON, tsv, or other formats.\\n\\n4.1. Data for Probing Experiments\\nWe build a dataset to probe the LM for government relations, using the patterns in the Government Bank. We start with sentences in the parsed Universal Dependency corpus (v2.12). We processed 37K sentences from all Finnish datasets in UD, and 116K Russian sentences from the Taiga (Shavrina and Shapovalova, 2017) and SynTagRus corpora (Droganova et al., 2018). We included 27.7K additional Finnish sentences and 102K Russian sentences from the WMT News Crawl monolingual training data (Bojar et al., 2017).\\n\\nThe data passes through morphological analysis, and parsers\u2014TurkuNLP for Finnish (Kanerva et al., 2018), and DeepPavlov for Russian (Burtsev et al., 2018). Then, we filter the sentences and identify valid government instances according to the Government Bank, via rule-based pattern matching. This results in a dataset with 18K Finnish instances by 582 unique verb lemmas. For Russian, we collected 143K instances and 2805 unique verbs.\\n\\nUsing the syntactic parser, we label each dependent of a verb that is also a governee in the Government Bank as a positive instance. Each sentence can yield one or more positive instances. As a negative instance, we label any noun phrase or prepositional phrase that is a dependent of the governor but not matching any pattern for this governor's complements in our set of rules. This setup helps us avoid the situation where the classifier learns to identify dependency between two words, which is an easier task than identifying the government relation between the governor verb and its\\n\\n3 https://github.com/RevitaAI/GovProbing\"}"}
{"id": "lrec-2024-main-1518", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The number of Finnish and Russian government instances for training and testing when \u201cfar\u201d means the distance between governor and governor is 3 tokens away.\\n\\n|          | Finnish | Russian |\\n|----------|---------|---------|\\n| Dist > 3 | POS     | NEG     |\\n|          | 402 381 | 943 925 |\\n|          | 123 144 | 193 211 |\\n\\nTable 3: The number of Finnish and Russian government instances for training and testing when \u201cfar\u201d means the distance between governor and governor is 2 tokens away.\\n\\n|          | Finnish | Russian |\\n|----------|---------|---------|\\n| Dist > 2 | POS     | NEG     |\\n|          | 407 413 | 999 991 |\\n|          | 118 112 | 137 145 |\\n\\n4.2. Data Balancing\\n\\nThe government instances in our datasets are also highly skewed in two respects: in terms of linguistic features (in particular, the case of the governor), and in terms of distance between the governor and governor. We balance the instances in these respects as well.\\n\\nLinguistic Features:\\nInstances with verbs governing arguments with a certain linguistic feature may outnumber other kinds of instances\u2014the distribution of features of the arguments is not balanced. For example, the direct object is the most frequent type of argument in Finnish government patterns, which results in a large number of such instances dominating the training data.\\n\\nDistance:\\nWe also observe that in most government instances, the governor is adjacent or very near the governor. To prevent the classifier from learning to classify adjacency rather than government, we divide the instances into two types: \u201cfar\u201d vs. \u201cnear\u201d. The governor is \u201cfar\u201d if its distance to the governor is over \\\\( \\\\text{Dist} > 3 \\\\) complete words (not sub-word tokens); otherwise, we consider it to be \u201cnear\u201d. In this paper, we consider two distance thresholds for \u201cfar\u201d and \u201cnear\u201d: \\\\( \\\\text{Dist} > 3 \\\\) and \\\\( \\\\text{Dist} > 2 \\\\). The \u201cnear\u201d instances are much more frequent than \u201cfar\u201d.\\n\\nWe balance the data by down-sampling the \u201cnear\u201d instances so that the amount of \u201cnear\u201d and \u201cfar\u201d instances is about the same.\\n\\nGovernment Patterns:\\nTo verify whether the classifier is capable of identifying patterns with verbs or arguments that it has never seen before (RQ2), we withhold instances with certain verb lemmas from the training data, and keep such instances only in the test data. We also withhold certain argument types, so that the probing classifier is not trained using all possible types of complements in the Government Bank. This allows us to evaluate the classifier\u2019s ability to discover entirely new, unseen government patterns.\\n\\nTable 2 and Table 3 summarize the number of instances used in these experiments, as well as their distance distribution, for Finnish and Russian.\\n\\n5. Government Probing Classifier\\nWe build the probing classifier based on the BERT model, specifically on the weights of its attention heads, from each transformer layer. Figure 1 illustrates the input to the probing classifier. From each attention head, we extract the weights for the governor...\"}"}
{"id": "lrec-2024-main-1518", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Overall performance of the classifiers\\n\\n| Model   | Acc  | P    | R    | F   |\\n|---------|------|------|------|-----|\\n| Finnish |      |      |      |     |\\n| LogReg  | 79.20| 85.04| 75.35| 79.90|\\n| MLP-1   | 78.85| 82.56| 77.91| 80.17|\\n| MLP-2   | 77.13| 81.32| 75.70| 78.41|\\n| RF      | 80.61| 82.82| 81.57| 82.19|\\n| Russian |      |      |      |     |\\n| LogReg  | 80.63| 83.10| 78.46| 80.72|\\n| MLP-1   | 84.87| 86.20| 84.19| 85.18|\\n| MLP-2   | 83.86| 84.88| 83.66| 84.27|\\n| RF      | 83.77| 81.75| 88.32| 84.90|\\n\\nWe experiment by training four types of probing classifiers: logistic regression, multi-layer perceptron (MLP) with one fully-connected layer, MLP with two fully-connected layers, and Random Forests. More details on the hyper-parameters of each classifier can be found in Appendix B. We implement all four classifiers using the Scikit-Learn Toolkit and mostly follow their default hyper-parameter settings.\\n\\nFor logistic regression, we set the maximum number of training iterations to 10000. For the MLP 1-layer classifier, we use 144 neurons. We use the same number of neurons for the first layer in the MLP 2-layer classifier, and 72 neurons for its second layer. For the Random Forest classifier, we use 300 trees.\\n\\n6. Results and Discussion\\n\\nWe train four sets of classifier for each language and for each distance group, corresponding to four classifier types in Section 5. For each set of classifiers, we train 12 classifiers using the attention head weights only from the first $N$ transformer layers, with $N = 1, 2, ..., 12$. The first classifier of each set is trained with attention head weights from only the first transformer layer, whereas the last one is trained with all attention head weights. Therefore, we expect the last model, which is trained with all weights, should be the best performing one in each set of classifiers. Since training involves some randomness, we repeat the process 5 times and average the results for more objective evaluation.\\n\\n6.1. Detecting Government Relations\\n\\nWe first explore the overall performance of the probing classifiers. Table 4 shows the performance of the probing classifiers trained with all attention head weights, average over five repetitions. All classifiers reach generally high scores, which are\u2014within each language\u2014quite close to each other. The random forest classifier gives the highest score in Finnish, while the MLP-1 classifier performs the best in Russian for most metrics (except recall when Dist > 2). All classifiers also show a very close performance within each distance group. Except for recall in Russian, the difference among all metrics is not statistically significant, with p-values all above 0.05. The p-value for recall in Russian is 0.04.\\n\\nSince the test and training data contain a balanced number of positive and negative instances, we can conclude that the probing classifiers can distinguish well between government vs. non-government relations, with accuracy and $F_1$ over 80% for both Finnish and Russian.\\n\\nNear vs. Far Arguments:\\n\\nTo confirm the generality of the probing classifiers, we explore the best\"}"}
{"id": "lrec-2024-main-1518", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Performance on detecting Near vs. Far government between governor and complement.\\n\\nPerforming models trained with all attention head weights from our experiments, and evaluate their performance on near vs. far governors separately. These tests check that the probes learn to predict government, rather than focusing only on the adjacency of governors to their arguments. The results are in Table 5. For Finnish, the majority of the models have higher F1 scores on detecting the government of the near than the far governors. However, the difference between far and near in Dist > 3 is not statistically significant. (The p-values for all metrics are above 0.05, except for recall.)\\n\\nFor Russian, the models score higher on detecting far governors than near ones, while for the Finnish models the scores are reversed. There is a significant difference, with all p-values below 0.05 for both Dist > 2 and Dist > 3. This confirms that the probes are not selective for the adjacency of the governors and their arguments.\\n\\nPositive vs. Negative instances: To visualize the difference between the attention weights of the positive vs. negative instances, we use the t-SNE dimensionality reduction technique (van der Maaten and Hinton, 2008), implemented in Scikit learn, with default parameters. Figure 2 shows how t-SNE projects the high-dimensional input vectors of the probing classifiers onto the 2-D plane. We plot all instances from the test data, for both languages. We can observe some strikingly well-defined clusters of the positive vs. the negative instances, particularly for Russian. This separability may help explain the very high performance of all probes on government prediction in Table 4.\\n\\n6.2. Selection of Transformer Layers\\n\\nWe next investigate how syntactic information about government is distributed across different transformer layers. We select the best-performing model trained with all attention weights, for Dist > 3, and visualize it together with all the other models in its classifier set. This lets us evaluate the contribution of the first N layers to the probe's ability to predict government.\\n\\nFigure 3 shows the performance in terms of the F1 measure, for all four classifier types. Almost all curves increase monotonically as the number of selected layers increases, which is expected, as more information can be inferred with more transformer layers. Crucially, all curves also indicate that the performance of classifiers increases rapidly for the early layers\u2013layers 1 to 4 for Finnish, to layer 5\u20136 for Russian\u2014and then the performance plateaus, with much smaller subsequent increases. This suggests that the syntactic information important for government predictions is encoded in the lowest levels of BERT. For Russian, this information is slightly more spread out across the first 5\u20136 layers. This mostly aligns with the observation in other probing contexts (Hewitt and Manning, 2019; Lin et al., 2019; Liu et al., 2019; Goldberg, 2019).\\n\\nWe check similar plots for models with Dist > 2 in Figure 5, which can be found in Appendix C, and which shows a very similar pattern as in Figure 3.\\n\\n6.3. Ablation of Attention Heads\\n\\nWe perform detailed ablation studies to see how attention heads at different layers contribute to government prediction. We use the logistic regression classifiers, trained as explained above, and explore how the weights learned in their attention heads...\"}"}
{"id": "lrec-2024-main-1518", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: t-SNE visualization of positive vs. negative instances. (left: Finnish, right: Russian)\\n\\nFigure 3: Probing government prediction with attention weights from the first N layers of BERT (X-axis) when \\\\( \\\\text{Dist} > 3 \\\\). Y-axis\u2014\\\\( F_1 \\\\) measure. (left: Finnish, right: Russian)\\n\\nWe first rank all attention heads according to their logistic regression coefficients, and then ablate attention heads from two opposite perspectives: (A) training the classifier by including only the top-\\\\( N \\\\) heads, and (B) training the classifier excluding the top-\\\\( N \\\\) heads, with \\\\( N \\\\) ranging from 1 to 144 (\\\\( N = 12 \\\\times 12 \\\\)). For comparison, we also train classifiers using a random subset of \\\\( N \\\\) heads, as a baseline.\\n\\nIn this experiment, we assess the relative importance and contribution of the attention heads, by \u201cprobing\u201d our probing classifiers. We train the random forest classifier with \\\\( N = 1, 2, \\\\ldots, 144 \\\\), for both languages. We explore models with \\\\( \\\\text{Dist} > 3 \\\\) and plot their \\\\( F_1 \\\\) scores in Figure 4.\\n\\nModels with \\\\( \\\\text{Dist} > 2 \\\\) show very similar behavior\u2014we visualize their performance in Figure 6, in Appendix C.\\n\\nWe observe that for Finnish, the model achieves an \\\\( F_1 \\\\) score of 80% either with the 3 top heads (represented by the blue curve), or by using all other heads together except the 3 top heads (orange curve). This suggests that the top 3 heads contain most of the needed information and are reliable indicators of government. Training without the top \\\\( N \\\\) heads yields superior performance compared to the baseline, where the heads are chosen at random (represented by the green curve).\\n\\nSimilarly for Russian, 17\u201320 top heads are required to achieve the same performance (\\\\( F_1 = 85\\\\% \\\\)) as when excluding these top heads. This indicates that Russian BERT requires more heads to sufficiently represent government relations. Training probing classifiers without the top \\\\( N \\\\) heads eventually shows a slightly worse performance for Finnish, which indicates that the Finnish BERT has less distributed government information across its heads. It also suggests that the top \\\\( N \\\\) heads are reliable, but not the only indicators of government relations between words. Government can still be inferred well from all of the remaining attention heads as well.\\n\\n6.4. Error Analysis\\n\\nWe manually examined instances that were misclassified by the probes. Some classification errors are caused by errors in the data (\\\"noise\\\")\u2014parsers...\"}"}
{"id": "lrec-2024-main-1518", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: F1 score for Random forest classifier with selected attention heads when Dist > 3 (left: Finnish, right: Russian).\\n\\nFor both languages occasionally assign incorrect dependency relations. For example, in Finnish: \u201cYritykset investoivat eli laajentavat toimintaa, rakentavat uutta ja hankkivat tekniikka.\u201d (Companies invest or expand operations, build new ones and acquire technology.) The parser incorrectly labels \u201ctoimintaa\u201d (operations) as the object of the governor \u201cinvestoida\u201d (invest). In fact, it is governed by another verb, \u201claajentaa\u201d (expand), which is conjoined with \u201cinvestoivat\u201d. All probes correctly predicted \u201cno government,\u201d which decreases recall in our evaluation.\\n\\nSimilar problems occur in Russian data: \u201c\u041f\u0440\u0438\u0447\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435\u0432\u0441\u0435\u0433\u043e \u043d\u0430 \u0441\u0432\u0435\u0442\u0435 \u043e\u043d\u0438 \u0431\u043e\u044f\u043b\u0438\u0441\u044c, \u0447\u0442\u043e \u043a\u0442\u043e-\u043d\u0438\u0431\u0443\u0434\u044c \u043e \u043d\u0435\u043c \u0443\u0437\u043d\u0430\u0435\u0442.\u201d (Incidentally, more than anything, they feared that someone would find out about it.) The parser labels \u201c\u0432\u0441\u0435\u0433\u043e\u201d (everything) as an object of the governor \u201c\u0431\u043e\u044f\u0442\u044c\u0441\u044f\u201d (fear). In reality, this verb\u2019s object is the following relative clause, and \u201c\u0431\u043e\u043b\u044c\u0448\u0435 \u0432\u0441\u0435\u0433\u043e\u201d (more than anything) is an adverbial phrase modifier.\\n\\nThere are some inconsistencies between the representation of dependency relations\u2014on one hand by, the third-party neural parsers, and on the other hand, by other components that we use to detect positive vs. negative government instances. This leads to some amount of misclassified instances. For example, in Finnish, subjects of passive verbs in the nominative case are parsed as objects. While the probing classifiers correctly reject government relations in such instances, this results in \u201cfalse negatives\u201d in evaluation. These examples represent \u201cnoise\u201d in the data. They will be fixed in future work; this gives us hope that after that the results may further slightly improve.\\n\\nSome proportion of the instances are true misclassifications. An analysis of these errors reveals that, occasionally, the probes fail to detect government relations with a long-distance governee that precedes its governor, or incorrectly identify adjacent adverbial modifiers as arguments. In Finnish, true misclassifications are rare, mostly caused by missed arguments and misclassified adjacent words.\\n\\n6.5. Discovering New Government Patterns\\n\\nResearch question RQ2 is: Are the probing classifiers capable of discovering new, previously unseen government patterns? To evaluate the generalization power of the probing classifiers, and check whether they can be used to enhance the Government Bank, we assess the performance of the probes on governors and government patterns that were held out from the training data (\u201cunseen\u201d).\\n\\nUnseen government patterns: For Finnish, we ran each probing classifier 8 times, while withholding two patterns from the labeled data, using these patterns only in the test data. In each run, we held out different patterns, e.g., all arguments in ablative case, or all third-infinitive arguments (e.g., \u201cH\u00e4n auttoi leipomaan leip\u00e4\u00e4.\u201d, \u201cS/he helped to bake bread.\u201d). We test analogously for Russian.\\n\\nTable 6 presents the accuracy of discovering unseen government. The overall good performance implies that we can use the information encoded in the LM to discover new patterns, and extend the Government Bank. Higher accuracy for the Random Forest classifier suggests that the probes are capable of finding more unseen patterns compared to other classifiers.\\n\\nUnseen governing verbs: We perform a similar assessment while withholding verbs from the training data. For each language, we run the probing classifiers 5 times. For each run, we withhold a random set of 66 verbs for Finnish (146 verbs for Russian). This lets us check the ability of the probes to discover new, unseen governor verbs. Table 7 shows the performance, averaged across\"}"}
{"id": "lrec-2024-main-1518", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model | Finnish Acc | Finnish F | Russian Acc | Russian F |\\n|-------|-------------|-----------|-------------|-----------|\\n| LogReg | 79.12       | 78.15     | 80.63       | 80.72     |\\n| MLP-1 | 78.67       | 78.52     | 84.87       | 85.18     |\\n| MLP-2 | 76.82       | 76.49     | 83.86       | 84.27     |\\n| RF    | 79.93       | 80.12     | 83.77       | 84.90     |\\n\\nTable 6: Accuracy of classifiers on discovering unseen government patterns for Finnish and Russian. Reported metrics are micro-averaged.\\n\\n| Model | Finnish Acc | Finnish F | Russian Acc | Russian F |\\n|-------|-------------|-----------|-------------|-----------|\\n| LogReg | 69.79       | 77.94     | 80.63       | 80.72     |\\n| MLP-1 | 73.81       | 78.52     | 84.87       | 85.18     |\\n| MLP-2 | 72.99       | 76.49     | 83.86       | 84.27     |\\n| RF    | 77.42       | 81.29     | 83.77       | 84.90     |\\n\\nTable 7: Performance of classifiers on unseen governors. Reported metrics are micro-averaged.\\n\\nall runs. The results indicate that probing classifiers can detect new verbs governing their complements, not previously seen during training.\\n\\nThese evaluations conclusively demonstrate the practical applicability of the government probing results\u2014the information encoded in BERT\u2019s attention weights can be used for building new valuable resources\u2014Government Banks.\\n\\n7. Conclusion\\n\\nThe contribution of this paper is twofold:\\n\\n1. We release a Government Bank (for Finnish and Russian) to the research community for work on the representation of grammatical government and constructions in neural language models.\\n2. We present an exploration of the representation of government constructions in transformer LMs. To the best of our knowledge, these are the first such resources to be made publicly available\u2014not only in human-readable, but also in machine-readable form; and the first exploration of how well LMs can predict complex constructions, such as government.\\n\\nWe probe the syntactic information encoded in BERT\u2019s attention heads, to reveal what it knows about government relations. Our objective is to extract knowledge about government relations from inside the LM\u2019s attention mechanism.\\n\\nWe evaluate the performance of the classifiers on Finnish and Russian data, from several perspectives. To study RQ1, we assess the overall performance of the classifiers, and explore their probing selectivity with respect to the distance between the governor and its governees. We show that the performance of the probing classifiers is very high. Notably, the classifiers perform as well or better when the governee is far from its governor.\\n\\nWe probed for the distribution of information about government across BERT\u2019s attention heads across different layers. We show that the probing classifiers are able to infer sufficient information from the first few layers of BERT.\\n\\nWe further explore the contribution of each attention head, using the coefficients learned by the logistic regression classifier. The idea is that the learned coefficients serve as a good indication of the contribution of the corresponding heads. Overall, the result shows that the most \u201cimportant\u201d attention heads (with the highest coefficients) are reliable indicators of government relations, but they are not the only ones. The probing classifiers are capable of detecting government using the remaining heads, which have lower coefficients. Comparing Finnish and Russian, we show that the information is slightly less spread out across the Finnish BERT heads, as compared to Russian BERT heads. We also performed an error analysis to understand the limitations of our probing classifiers.\\n\\nFor RQ2, we evaluate the ability of the probe to identify novel government relations, never seen during the training phase. We held out: (a) specific government patterns, and (b) governing verbs. The experiments show that the probes are able to discover novel government relations and novel governors, unseen in training.\\n\\nIn future work, we plan to use larger datasets, and identify large sets of government relations. This will extend the Government Bank with new patterns. We plan to work with additional languages, in particular, we are extending this approach to German and Italian government. Crucially, we will extend this work government of other parts of speech (nouns, adjectives, etc.), and to more complex types of constructions than government.\\n\\nLimitations\\n\\nThe current work has a number of limitations to consider. (A) For now, our probing of government relations is limited to two languages. Extending to additional languages is challenging when probing this type of construction because it requires a Government Bank for each additional language, and collection of language-specific data. We are currently conducting experiments with an Italian and a German Government Bank; if the paper is accepted, we expect to be able to include those.\"}"}
{"id": "lrec-2024-main-1518", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"So far, we have performed probing using only one type of transformer models\u2014BERT-base. In future work, we plan to extend experiments to other models as well.\\n\\nSo far, we have performed only one type of probing\u2014using correlation probing with probing classifiers. This type of probing has received some criticism, of which we are aware. In future work, we experiment with the model representations from hidden layers and other types of probing methodologies.\\n\\nEthics Statement\\nWe do not see any ethical issues with the current work. We use publicly available resources for all conducted experiments, and release language resources, which were created in collaboration with linguists, who were aware of how the data will be used.\\n\\nReferences\\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2017. Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks. In International Conference on Learning Representations.\\n\\nDavid Arps, Younes Samih, Laura Kallmeyer, and Hassan Sajjad. 2022. Probing for constituency structure in neural language models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6738\u20136757, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nYonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207\u2013219.\\n\\nOnd\u02c7rej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (WMT17). In Proceedings of the Second Conference on Machine Translation, pages 169\u2013214, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nMikhail Burtsev, Alexander Seliverstov, Rafael Airapetyan, Mikhail Arkhipov, Dilyara Baymurzina, Nickolay Bushkov, Olga Gureenkova, Taras Khakhulin, Yuri Kuratov, Denis Kuznetsov, Alexey Litinsky, Varvara Logacheva, Alexey Lymar, Valentin Malykh, Maxim Petrov, Vadim Polulyakh, Leonid Pugachev, Alexey Sorokin, Maria Vikhreva, and Marat Zaynutdinov. 2018. DeepPavlov: Open-source library for dialogue systems. In Proceedings of ACL 2018, System Demonstrations, pages 122\u2013127, Melbourne, Australia. Association for Computational Linguistics.\\n\\nChristopher, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. 2020. Emergent linguistic structure in artificial neural networks trained by self-supervision. Proceedings of the National Academy of Sciences, 117(48):30046\u201330054.\\n\\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? an analysis of BERT\u2019s attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276\u2013286, Florence, Italy. Association for Computational Linguistics.\\n\\nSimone Conia and Roberto Navigli. 2022. Probing for predicate argument structures in pretrained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4622\u20134632, Dublin, Ireland. Association for Computational Linguistics.\\n\\nAlexis Conneau, German Kruszewski, Guillaume Lample, Lo\u00a8\u0131c Barrault, and Marco Baroni. 2018. What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2126\u20132136, Melbourne, Australia. Association for Computational Linguistics.\\n\\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James Glass. 2019. What is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI\u201919/IAAI\u201919/EAAI\u201919. AAAI Press.\\n\\nKira Droganova, Olga Lyashevskaya, and Daniel Zeman. 2018. Data conversion and consistency of monolingual corpora: Russian UD treebanks. In Proceedings of TL T: the 17th international workshop on treebanks and linguistic theories, volume 155, pages 53\u201366.\"}"}
{"id": "lrec-2024-main-1518", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Charles J. Fillmore and Paul Kay. 1996. *Construction Grammar*. Manuscript, University of California at Berkeley Department of linguistics.\\n\\nAdele Goldberg. 2006. *Constructions at work: The nature of generalization in language*. Oxford University Press.\\n\\nYoav Goldberg. 2019. *Assessing bert's syntactic abilities*. arXiv preprint arXiv:1901.05287.\\n\\nJohn Hewitt and Percy Liang. 2019. *Designing and interpreting probes with control tasks*. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2733\u20132743, Hong Kong, China. Association for Computational Linguistics.\\n\\nJohn Hewitt and Christopher D. Manning. 2019. *A structural probe for finding syntax in word representations*. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129\u20134138, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nSusan Hunston. 2019. *Patterns, constructions, and applied linguistics*. International Journal of Corpus Linguistics, 24(3):324\u2013353.\\n\\nLaura A Janda, Olga Lyashevskaya, Tore Nesset, Ekaterina Rakhilina, and Francis M Tyers. 2018. *A constructicon for Russian: Filling in the gaps*. In *Constructicography*, pages 165\u2013182. John Benjamins.\\n\\nJenna Kanerva, Filip Ginter, Niko Miekka, Akseli Leino, and Tapio Salakoski. 2018. *Turku neural parser pipeline: An end-to-end system for the conll 2018 shared task*. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics.\\n\\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. *Revealing the dark secrets of BERT*. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4365\u20134374, Hong Kong, China. Association for Computational Linguistics.\\n\\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019. *Open sesame: Getting inside BERT's linguistic knowledge*. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 241\u2013253, Florence, Italy. Association for Computational Linguistics.\\n\\nNelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019. *Linguistic knowledge and transferability of contextual representations*. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1073\u20131094, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nBenjamin Lyngfelt, Lars Borin, Kyoko Ohara, and Tiago Timponi Torrent, editors. 2018. *Constructicography: Constructicon development across languages*. John Benjamins.\\n\\nRowan Hall Maudslay, Josef Valvoda, Tiago Pimentel, Adina Williams, and Ryan Cotterell. 2020. *A tale of a probe and a parser*. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7389\u20137395, Online. Association for Computational Linguistics.\\n\\nAbhilasha Ravichander, Yonatan Belinkov, and Eduard Hovy. 2021. *Probing the probing paradigm: Does probing accuracy entail task relevance?* In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3363\u20133377, Online. Association for Computational Linguistics.\\n\\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. *A Primer in BERTology: What We Know About How BERT Works*. Transactions of the Association for Computational Linguistics, 8:842\u2013866.\\n\\nTatiana Shavrina and Olga Shapovalova. 2017. *To the methodology of corpus construction for machine learning: \u201cTaiga\u201d syntax tree corpus and parser*. Corpus Linguistics, page 78.\\n\\nAlex Tamkin, Trisha Singh, Davide Giovanardi, and Noah Goodman. 2020. *Investigating transferability in pretrained language models*. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1393\u20131401, Online. Association for Computational Linguistics.\\n\\nLaurens van der Maaten and Geoffrey Hinton. 2008. *Visualizing data using t-sne*. Journal of Machine Learning Research, 9(86):2579\u20132605.\\n\\nElena Voita and Ivan Titov. 2020. *Information-theoretic probing with minimum description\"}"}
{"id": "lrec-2024-main-1518", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices\\n\\nA. Overall instance statistics\\n\\nTable 8 and Table 9 show the distribution of different types of governees\u2014syntactic dependents of the head verb, which are governed by the verb\u2014before balancing and sub-sampling. We should mention that we exclude certain common governees from our experiments, despite the fact that they are present in our government bank. The following direct objects governees are excluded: for Finnish\u2014nouns in objective case and in partitive case, and for Russian noun in the accusative case. This is done because we are unable to find negative instances with these cases for data balancing, unlike for all other governee types. A large proportion of \u201cnegative\u201d instances for these cases have been manually verified as positive instances. This suggests that our government bank is not yet complete, which is as expected. Therefore, we exclude these types of governees from our experiments.\\n\\n| PoS         | Case          | Total |\\n|-------------|---------------|-------|\\n| Noun        | Inessive      | 2509  |\\n|             | Genitive      | 2424  |\\n|             | Illative      | 2337  |\\n|             | Elative       | 2190  |\\n|             | Adessive      | 1630  |\\n|             | Essive        | 1529  |\\n|             | Allative      | 1096  |\\n|             | Ablative      | 522   |\\n|             | Translative   | 659   |\\n| Verb        | Ma-Infinitive | 1112  |\\n|             | A-Infinitive  | 1555  |\\n| Postposition| Partitive     | 2     |\\n|             | Genitive      | 11    |\\n| Adjective   | Ablative      | 93    |\\n|             | Adessive      | 65    |\\n|             | Allative      | 42    |\\n|             | Elative       | 77    |\\n|             | Essive        | 230   |\\n|             | Genitive      | 312   |\\n|             | Illative      | 35    |\\n|             | Inessive      | 54    |\\n|             | Translative   | 235   |\\n|             | total         | 18719 |\\n\\nTable 8: Detailed overview of Finnish instances, before sampling\\n\\nB. Hyper-parameters of classifiers\\n\\nWe implement the classifiers, including both MLP 1-layer and 2-layer classifiers, based on the Scikit Learn library, and mostly follow their default hyper-parameter settings. For the logistic regression classifier, we set the maximum number of training iterations to 10000. For the MLP 1-layer classifier, we set 144 neurons. We use the same number of neurons for the first fully-connected layer in the MLP 2-layer classifier, while we set its second fully-connected layer to 72 neurons. For the Random Forest classifier, we use 300 trees.\\n\\nC. Selection of Transformer Layers and Attention Heads for Dist > 2\\n\\nIf we lower the threshold for separating \u201cnear\u201d syntactic dependents from \u201cfar\u201d dependents to \\\\( \\\\text{Dist} > 2 \\\\), we observe the results in figures 5 and 6 for ablation studies in Section 6.3. These figures are included here for comparison with Figures 3 and 4, respectively.\\n\\n7 https://scikit-learn.org/\"}"}
{"id": "lrec-2024-main-1518", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: Probing government prediction with attention weights from the first N layers of BERT (X-axis) when Dist > 2. Y-axis\u2014F\u2081 measure. (left: Finnish, right: Russian)\\n\\nFigure 6: F\u2081 score for Random forest classifier with selected attention heads when Dist > 2 (left: Finnish, right: Russian.\"}"}
{"id": "lrec-2024-main-1518", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Part of Speech | Case | Total |\\n|---------------|------|-------|\\n| Verb          |      | 41090 |\\n| Noun          | Dative | 11406 |\\n|               | Genitive | 28926 |\\n|               | Instrumental | 8671 |\\n| Preposition   | \u0432    | 6674  |\\n|               | \u0437\u0430   | 1520  |\\n|               | \u043d\u0430   | 8599  |\\n|               | \u043e    | 42    |\\n|               | \u043f\u043e\u0434  | 17    |\\n|               | \u043f\u0440\u043e  | 236   |\\n|               | \u043a    |       |\\n|               | Dative | 4278 |\\n|               | \u043f\u043e   | 933   |\\n|               | \u0431\u0435\u0437  |       |\\n|               | \u0434\u043b\u044f  | 210   |\\n|               | \u0434\u043e   | 480   |\\n|               | \u0438\u0437   | 2111  |\\n|               | \u0438\u0437-\u0437\u0430| 25    |\\n|               | \u0438\u0437-\u043f\u043e\u0434| 16   |\\n|               | \u043e\u0442   | 2598  |\\n|               | \u043f\u0440\u043e\u0442\u0438\u0432| 56  |\\n|               | \u0440\u0430\u0434\u0438 | 6     |\\n|               | \u0441    | 1150  |\\n|               | \u0443    | 800   |\\n|               | \u0437\u0430   |       |\\n|               | Locative | 3925 |\\n|               | \u043d\u0430   | 2425  |\\n|               | \u043e    | 2816  |\\n|               |      | 143836|\\n\\nTable 9: Detailed overview of Russian instances, before sampling\"}"}
