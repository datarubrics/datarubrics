{"id": "acl-2022-long-135", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sense Embeddings are also Biased\u2013 Evaluating Social Biases in Static and Contextualised Sense Embeddings\\n\\nYi Zhou1, Masahiro Kaneko2, Danushka Bollegala1, 3\\n\\nUniversity of Liverpool1, Tokyo Institute of Technology2, Amazon3\\n\\n{y.zhou71,danushka}@liverpool.ac.uk\\nmasahiro.kaneko@nlp.c.titech.ac.jp\\n\\nAbstract\\n\\nSense embedding learning methods learn different embeddings for the different senses of an ambiguous word. One sense of an ambiguous word might be socially biased while its other senses remain unbiased. In comparison to the numerous prior work evaluating the social biases in pretrained word embeddings, the biases in sense embeddings have been relatively understudied. We create a benchmark dataset for evaluating the social biases in sense embeddings and propose novel sense-specific bias evaluation measures. We conduct an extensive evaluation of multiple static and contextualised sense embeddings for various types of social biases using the proposed measures. Our experimental results show that even in cases where no biases are found at word-level, there still exist worrying levels of social biases at sense-level, which are often ignored by the word-level bias evaluation measures.\\n\\n1 Introduction\\n\\nSense embedding learning methods use different vectors to represent the different senses of an ambiguous word (Reisinger and Mooney, 2010; Nellakantan et al., 2014; Loureiro and Jorge, 2019). Although numerous prior works have studied social biases in static and contextualised word embeddings, social biases in sense embeddings remain underexplored (Kaneko and Bollegala, 2019, 2021a,a; Ravfogel et al., 2020; Dev et al., 2020; Schick et al., 2021; Wang et al., 2020).\\n\\nWe follow Shah et al. (2020) and define social biases to be predictive biases with respect to protected attributes made by NLP systems. Even if a word embedding is unbiased, some of its senses could still be associated with unfair social biases.\\n\\n\u2217 Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon.\\n\\nThe dataset and evaluation scripts are available at github.com/LivNLP/bias-sense.\\n\\n1924\"}"}
{"id": "acl-2022-long-135", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ARES (Scarlini et al., 2020), as well as contextualised sense embeddings obtained from SenseBERT (Levine et al., 2020). To the best of our knowledge, we are the first to conduct a systematic evaluation of social biases in sense embeddings. Specifically, we make two main contributions in this paper:\\n\\n- First, to evaluate social biases in static sense embeddings, we extend previously proposed benchmarks for evaluating social biases in static (sense-insensitive) word embeddings by manually assigning sense ids to the words considering their social bias types expressed in those datasets (\u00a73).\\n\\n- Second, to evaluate social biases in sense-sensitive contextualised embeddings, we create the Sense-Sensitive Social Bias (SSSB) dataset, a novel template-based dataset containing sentences annotated for multiple senses of an ambiguous word considering its stereotypical social biases (\u00a75). An example from the SSSB dataset is shown in Figure 1.\\n\\nOur experiments show that, similar to word embeddings, both static as well as contextualised sense embeddings also encode worrying levels of social biases. Using SSSB, we show that the proposed bias evaluation measures for sense embeddings capture different types of social biases encoded in existing SoTA sense embeddings. More importantly, we see that even when social biases cannot be observed at word-level, such biases are still prominent at sense-level, raising concerns on existing evaluations that consider only word-level social biases.\\n\\n2 Related Work\\n\\nOur focus in this paper is the evaluation of social biases in English and not the debiasing methods. We defer the analysis for languages other than English and developing debiasing methods for sense embeddings to future work. Hence, we limit the discussion here only to bias evaluation methods.\\n\\nBiases in Static Embeddings:\\n\\nThe Word Embedding Association Test (WEAT; Caliskan et al., 2017) evaluates the association between two sets of target concepts (e.g. male vs. female) and two sets of attributes (e.g. Pleasant (love, cheer, etc.) vs. Unpleasant (ugly, evil, etc.)). Here, the association is measured using the cosine similarity between the word embeddings. Ethayarajh et al. (2019) showed that WEAT systematically overestimates the social biases and proposed relational inner-product association (RIPA), a subspace projection method, to overcome this problem.\\n\\nWord Association Test (WAT; Du et al., 2019) calculates a gender information vector for each word in an association graph (Deyne et al., 2019) by propagating information related to masculine and feminine words. Additionally, word analogies are used to evaluate gender bias in static embeddings (Bolukbasi et al., 2016; Manzini et al., 2019; Zhao et al., 2018). Loureiro and Jorge (2019) showed specific examples of gender bias in static sense embeddings. However, these datasets do not consider word senses, hence are unfit for evaluating social biases in sense embeddings.\\n\\nBiases in Contextualised Embeddings:\\n\\nMay et al. (2019) extended WEAT to sentence encoders by creating artificial sentences using templates and used cosine similarity between the sentence embeddings as the association metric. Kurita et al. (2019) proposed the log-odds of the target and prior probabilities of the sentences computed by masking respectively only the target vs. both target and attribute words. Template-based approaches for generating example sentences for evaluating social biases do not require human annotators to write examples, which is often slow, costly and require careful curation efforts. However, the number of sentence patterns that can be covered via templates is often small and less diverse compared to manually written example sentences.\\n\\nTo address this drawback, Nadeem et al. (StereoSet; 2021) created human annotated contexts of social bias types, while Nangia et al. (2020) proposed Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). Following these prior work, we define a stereotype as a commonly-held association between a group and some attribute. These benchmarks use sentence pairs of the form \\\"She is a nurse/doctor\\\". StereoSet calculates log-odds by masking the modified tokens (nurse, doctor) in a sentence pair, whereas CrowS-Pairs calculates log-odds by masking their unmodified tokens (She, is, a).\\n\\nKaneko and Bollegala (2021b) proposed All Unmasked Likelihood (AUL) and AUL with Attention weights (AULA), which calculate log-likelihood by predicting all tokens in a test case, given the contextualised embedding of the unmasked input.\"}"}
{"id": "acl-2022-long-135", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluation Metrics for Social Biases in Static Sense Embeddings\\n\\nWe extend the WEAT and WAT datasets that have been frequently used in prior work for evaluating social biases in static word embeddings such that they can be used to evaluate sense embeddings. These datasets compare the association between a target word $w$ and some (e.g. pleasant or unpleasant) attribute $a$, using the cosine similarity, $\\\\cos(w, a)$, computed using the static word embeddings $w$ and $a$ of respectively $w$ and $a$. Given two same-sized sets of target words $X$ and $Y$ and two sets of attribute words $A$ and $B$, the bias score, $s(X, Y, A, B)$, for each target is calculated as follows:\\n\\n$$s(X, Y, A, B) = \\\\frac{1}{|X|} \\\\sum_{x \\\\in X} w(x, A, B) - \\\\frac{1}{|Y|} \\\\sum_{y \\\\in Y} w(y, A, B)$$\\n\\n(1)\\n\\n$$w(t, A, B) = \\\\text{mean}_{a \\\\in A} \\\\cos(t, a) - \\\\text{mean}_{b \\\\in B} \\\\cos(t, b)$$\\n\\n(2)\\n\\nHere, $\\\\cos(a, b)$ is the cosine similarity between the embeddings $a$ and $b$. The one-sided $p$-value for the permutation test for $X$ and $Y$ is calculated as the probability of $s(X_i, Y_i, A, B) > s(X, Y, A, B)$.\\n\\nThe effect size is calculated as the normalised measure given by (3):\\n\\n$$\\\\frac{1}{|t \\\\in X \\\\cup Y|} \\\\sum_{t \\\\in X \\\\cup Y} w(t, A, B) - \\\\frac{1}{|X|} \\\\sum_{x \\\\in X} w(x, A, B) - \\\\frac{1}{|Y|} \\\\sum_{y \\\\in Y} w(y, A, B)$$\\n\\n(3)\\n\\nWe repurpose these datasets for evaluating the social biases in sense embeddings as follows. For each target word in WEAT, we compare each sense $s_i$ of the target word $w$ against each sense $a_j$ of a word selected from the association graph using their corresponding sense embeddings, $s_i, a_j$, and use the maximum similarity over all pairwise combinations (i.e. $\\\\max_{i,j} \\\\cos(s_i, a_j)$) as the word association measure. Measuring similarity between two words as the maximum similarity over all candidate senses of each word is based on the assumption that two words in a word-pair would mutually disambiguate each other in an association-based evaluation (Pilehvar and Camacho-Collados, 2019), and has been used as a heuristic for disambiguating word senses (Reisinger and Mooney, 2010).\\n\\nWAT considers only gender bias and calculates the gender information vector for each word in a word association graph created with Small World project (Deyne et al., 2019) by propagating information related to masculine and feminine words $(w_i^m, w_i^f) \\\\in L$ using a random walk approach (Zhou et al., 2003). It is non-trivial to pre-specify the sense of a word in a large word association graph considering the paths followed by a random walk. The gender information is encoded as a vector $(b_m, b_f)$ in 2 dimensions, where $b_m$ and $b_f$ denote the masculine and feminine orientations of a word, respectively. The bias score of a word is defined as $\\\\log(b_m/b_f)$. The gender bias of word embeddings are evaluated using the Pearson correlation coefficient between the bias score of each word and the score given by (4), computed as the average over the differences of cosine similarities between masculine and feminine words.\\n\\n$$X_i = 1 - \\\\cos(w_i^m, w) + \\\\cos(w_i^f, w)$$\\n\\n(4)\\n\\nTo evaluate gender bias in sense embeddings, we follow the method that is used in WEAT, and take $\\\\max_{i,j} \\\\cos(s_i, a_j)$ as the word association measure.\\n\\n4 Sense-Sensitive Social Bias Dataset\\n\\nContextualised embeddings such as the ones generated by masked language models (MLMs) return different vectors for the same word in different contexts. However, the datasets discussed in \u00a7 3 do not provide contextual information for words and cannot be used to evaluate contextualised embeddings. Moreover, the context in which an ambiguous word occurs determines its word sense. Contextualised sense embedding methods such as SenseBERT (fine-tuned using WordNet super senses), have shown to capture word sense information in their contextualised embeddings (Zhou and Bollegala, 2021).\\n\\nCrowS-Pairs and StereoSet datasets were proposed for evaluating contextualised word embeddings. Specifically, an MLM is considered to be...\"}"}
{"id": "acl-2022-long-135", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Category Ambiguous words considered\\nnoun vs. verb engineer, carpenter, guide, mentor, judge, nurse\\nrace vs. colour black\\nnationality vs. language Japanese, Chinese, English, Arabic, German, French, Spanish, Portuguese, Norwegian, Swedish, Polish, Romanian, Russian, Egyptian, Finnish, Vietnamese\\n\\nTable 2: Bias categories covered in the SSSB dataset\\n\\nunfairly biased if it assigns higher pseudo log-likelihood scores for stereotypical sentences, $S_{st}$, than anti-stereotypical ones, $S_{at}$. However, both of those datasets do not consider multiple senses of words and cannot be used to evaluate social biases in contextualised sense embeddings.\\n\\nTo address this problem, we create the Sense-Sensitive Social Bias (SSSB) dataset, containing template-generated sentences covering multiple senses of ambiguous words for three types of social biases: gender, race and nationality. Templates are used in the same sense as in prior work such as Ku-rita et al. (2019). For example, we manually create templates such as [gender word] is a [pleasant/unpleasant attribute] engineer. We then fill the gender word by male and female gender pronouns (he/she), pleasant attributes (e.g. careful, skilful, efficient, etc.) and unpleasant attributes (e.g. clumsy, un-skillful, inefficient, etc.) to generate many example sentences demonstrating social biases.\\n\\nTo the best of our knowledge, SSSB is the first-ever dataset created for the purpose of evaluating social biases in sense embeddings. Table 1 shows the summary statistics of the SSSB dataset. Table 2 shows the bias categories covered in the SSSB dataset. Next, we describe the social biases covered in this dataset.\\n\\n4.1 Nationality vs. Language Bias\\n\\nThese examples cover social biases related to a nationality (racial) or a language (non-racial). Each test case covers two distinct senses and the following example shows how they represent biases.\\n\\nJapanese people are nice is an anti-stereotype for Japanese as a nationality because it is associated with a pleasant attribute (i.e. nice) in this example sentence. On the other hand, Japanese people are stupid is a stereotype for Japanese as a nationality because it is associated with an unpleasant attribute (i.e. stupid). These can be considered as examples of racial biases.\\n\\nLikewise, for the language sense of Japanese we create examples as follows.\\n\\nJapanese language is difficult to understand is a stereotype for Japanese as a language because it is associated with an unpleasant attribute (i.e. difficult). On the other hand, Japanese language is easy to understand is an anti-stereotype for Japanese as a language because it is associated with a pleasant attribute (i.e. easy).\\n\\nIn SSSB, we indicate the sense-type, WordNet sense-id and the type of social bias in each example as follows:\\n\\nJapanese people are beautiful.\\n\\n[nationality, japanese%1:18:00::, anti]\\n\\nHere, sense-type is nationality, sense-id as specified in the WordNet is japanese%1:18:00:: and the bias is anti (we use the labels anti and stereo to denote respectively anti-stereotypical and stereotypical biases).\\n\\nWe use the likelihood scores returned by an MLM to nationality vs. language sentence pairs as described further in \u00a75 to evaluate social biases in MLMs. Essentially, if the likelihood score returned by an MLM for the example that uses an unpleasant attribute is higher than the one that uses a pleasant attribute for a member in the disadvantaged group, then we consider the MLM to be socially biased. Moreover, if a member in the disadvantaged group is associated with a positive attribute in a stereotypical manner, we consider this as a anti-stereotype case. For example, we classify Asians are smart as anti-stereotype rather than \\\"positive\\\" stereotypes following prior work on word-level or sentence-level bias evaluation datasets (e.g., Crows-Pairs and StereoSet) to focus on more adverse types of biases that are more direct and result in discriminatory decisions against the disadvantaged groups.\\n\\nNote that one could drop the modifiers such as people and language and simplify these examples such as Japanese are nice and Japanese is diffic...\"}"}
{"id": "acl-2022-long-135", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cult to generate additional test cases. However, the sense-sensitive embedding methods might find it difficult to automatically disambiguate the correct senses without the modifiers such as language or people. Therefore, we always include these modifiers when creating examples for nationality vs. language bias in the SSSB dataset.\\n\\n4.2 Race vs. Colour Bias\\n\\nThe word black can be used to represent the race (black people) or the colour. We create examples that distinguish these two senses of black as in the following example.\\n\\nBlack people are friendly represents an anti-stereotype towards black because it is associated with a pleasant attribute (i.e. friendly) of a disadvantaged group whereas, Black people are arrogant represents a stereotype because it is associated with an unpleasant attribute (i.e. arrogant).\\n\\nOn the other hand, for the colour black, The black dress is elegant represents an anti-stereotype because it is associated with a pleasant attribute (i.e. elegant) whereas The black dress is ugly represents a stereotype because it is associated with an unpleasant attribute (i.e. ugly). If the likelihood score returned by an MLM for a sentence containing the racial sense with an unpleasant attribute is higher than one that uses a pleasant attribute, the MLM is considered to be socially biased.\\n\\n4.3 Gender Bias in Noun vs. Verb Senses\\n\\nTo create sense-related bias examples for gender, we create examples based on occupations. In particular, we consider the six occupations: engineer, nurse, judge, mentor, (tour) guide, and carpenter. These words can be used in a noun sense (e.g. engineer is a person who uses scientific knowledge to solve practical problems, nurse is a person who looks after patients, etc.) as well as in a verb sense expressing the action performed by a person holding the occupation (e.g. design something as an engineer, nurse a baby, etc.). Note that the ambiguity here is in the occupation (noun) vs. action (verb) senses and not in the gender, whereas the bias is associated with the gender of the person holding the occupation.\\n\\nTo illustrate this point further, consider the following examples.\\n\\nShe is a talented engineer is considered as an anti-stereotypical example for the noun sense of engineer because females (here considered as the disadvantaged group) are not usually associated with pleasant attributes (i.e. talented) with respect to this occupation (i.e. engineer). He is a talented engineer is considered as a stereotypical example for the noun sense of engineer because males (here considered as the advantaged group) are usually associated with pleasant attributes with regard to this occupation.\\n\\nAs described in \u00a7 5, if an MLM assigns a higher likelihood to the stereotypical example (second sentence) than the anti-stereotypical example (first sentence), then that MLM is considered to be gender biased. Here again, if an MLM assigns a higher likelihood to the stereotypical example (first sentence) than the anti-stereotypical example (second sentence), then it is considered to be gender biased. Note that the evaluation direction with respect to male vs. female pronouns used in these examples is opposite to that in the previous paragraph because we are using an unpleasant attribute in the second set of examples.\\n\\nVerb senses are also used in the sentences that contain gender pronouns in SSSB. For example, for the verb sense of engineer, we create examples as follows: She used novel material to engineer the bridge. Here, the word engineer is used in the verb sense in a sentence where the subject is a female. The male version of this example is as follows: He used novel material to engineer the bridge. In this example, a perfectly unbiased MLM should not systematically prefer one sentence over the other between the two sentences both expressing the verb sense of the word engineer.\"}"}
{"id": "acl-2022-long-135", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AUL is known to be robust against the frequency biases of words and provides more reliable estimates compared to the other metrics for evaluating social biases in MLMs. Following the standard evaluation protocol, we provide AUL the complete sentence $S = w_1, \\\\ldots, w_{|S|}$, which contains a length $|S|$ sequence of tokens $w_i$, to an MLM with pre-trained parameters $\\\\theta$. We first compute PLL($S$), the Pseudo Log-Likelihood (PLL) for predicting all tokens in $S$ excluding begin and end of sentence tokens, given by (5):\\n\\n$$PLL(S) := \\\\frac{1}{|S|} \\\\sum_{i=1}^{|S|} \\\\log P(w_i|S;\\\\theta)$$\\n\\nHere, $P(w_i|S;\\\\theta)$ is the probability assigned by the MLM to token $w_i$ conditioned on $S$. The fraction of sentence-pairs in SSSB, where higher PLL scores are assigned to the stereotypical sentence than the anti-stereotypical one is considered as the AUL bias score of the MLM associated with the contextualised embedding, and is given by (6):\\n\\n$$AUL = \\\\left( \\\\frac{100}{N} \\\\sum_{(S_{st}, S_{at})} I(PLL(S_{st}) > PLL(S_{at})) - 50 \\\\right)$$\\n\\nHere, $N$ is the total number of sentence-pairs in SSSB and $I$ is the indicator function, which returns 1 if its argument is True and 0 otherwise.\\n\\nAUL score given by (6) falls within the range $[-50, 50]$ and an unbiased embedding would return bias scores close to 0, whereas bias scores less than or greater than 0 indicate bias directions towards respectively the anti-stereotypical or stereotypical examples.\\n\\n6 Experiments\\n\\n6.1 Bias in Static Embeddings\\n\\nTo evaluate biases in static sense embeddings, we select two current SoTA sense embeddings: LMMS (Loureiro and Jorge, 2019) and ARES (Scarlini et al., 2020). In addition to WEAT and WAT datasets described in \u00a7 3, we also use SSSB to evaluate static sense embeddings using the manually assigned sense ids for the target and attribute words, ignoring their co-occurring contexts. LMMS and ARES sense embeddings associate each sense of a lexeme with a sense key and a vector, which we use to compute cosine similarities as described in \u00a73. To compare the biases in a static sense embedding against a corresponding sense-insensitive static word embedding version, we compute a static word embedding $w$ for an ambiguous word $w$ by taking the average ($\\\\text{avg}$) over the sense embeddings $s_i$ for all of $w$'s word senses as given in (7), where $M(w)$ is the total number of senses of $w$:\\n\\n$$w = \\\\frac{1}{M(w)} \\\\sum_{s_i \\\\in S(w)} s_i$$\\n\\nThis would simulate the situation where the resultant embeddings are word-specific but not sense-specific, while still being comparable to the original sense embeddings in the same vector space.\\n\\nAs an alternative to (7), which weights all different senses of $w$ equally, we can weight different senses by their frequency. However, such sense frequency statistics are not always available except for sense labelled corpora such as SemCor (Miller et al., 1993). Therefore, we use the unweighted average given by (7).\\n\\nFrom Table 3 we see that in WEAT in all categories considered, sense embeddings always report a higher bias compared to their corresponding sense-insensitive word embeddings. This shows that even if there are no biases at the word-level, we can still observe social biases at the sense-level in WEAT. However, in the W AT dataset, which covers only gender-related biases, we see word embeddings to have higher biases than sense embeddings. This indicates that in WAT gender bias is more likely to be observed in static word embeddings than in static sense embeddings.\\n\\nIn SSSB, word embeddings always report the same bias scores for the different senses of an ambiguous word because static word embeddings are neither sense nor context sensitive. As aforementioned, the word \u201cblack\u201d is bias-neutral with respect to the colour sense, while it often has a social bias for the racial sense. Consequently, for black we see a higher bias score for its racial than colour sense in both LMMS and ARES sense embeddings.\\n\\nThree bias types (European vs. African American, Male vs. Female, and Old vs. Young) had to be excluded because these biases are represented using personal names that are not covered by LMMS and ARES sense embeddings.\"}"}
{"id": "acl-2022-long-135", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Sentence                                                                                       | WEAT | SSSB                  |\\n|-----------------------------------------------------------------------------------------------|------|-----------------------|\\n| Flowers vs Insects                                                                             | 1.63 | 4.64                  |\\n| Instruments vs Weapons                                                                         | 1.42 | 4.64                  |\\n| Math vs Art                                                                                    | 1.52 | 7.78                  |\\n| Science vs Art                                                                                 | 1.38 | 7.78                  |\\n| Physical vs. Mental condition                                                                  | 0.42 | 7.78                  |\\n| WAT                                                                                           | 0.53 | 7.78                  |\\n\\nTable 3: Bias in LMMS and ARES Static Sense Embeddings. In each row, between sense-insensitive word embeddings and sense embeddings, the larger deviation from 0 is shown in bold. All results on WEAT are statistically significant ($p < 0.05$) according to (3).\\n\\nIn the bias scores reported for nationality vs. language senses, we find that nationality obtains higher biases at word-level, while language at the sense-level in both LMMS and ARES. Unlike black, where the two senses (colour vs. race) are distinct, the two senses nationality and language are much closer because in many cases (e.g. Japanese, Chinese, Spanish, French etc.) languages and nationalities are used interchangeably to refer to the same set of entities. Interestingly, the language sense is assigned a slightly higher bias score than the nationality sense in both LMMS and ARES sense embeddings. Moreover, we see that the difference between the bias scores for the two senses in colour vs. race (for black) as well as nationality vs. language is more in LMMS compared to that in ARES sense embeddings.\\n\\nBetween noun vs. verb senses of occupations, we see a higher gender bias for the noun sense than the verb sense in both LMMS and ARES sense embeddings. This agrees with the intuition that gender biases exist with respect to occupations and not so much regarding what actions/tasks are carried out by the persons holding those occupations. Compared to the word embeddings, there is a higher bias for the sense embeddings in the noun sense for both LMMS and ARES. This trend is reversed for the verb sense where we see higher bias scores for the word embeddings than the corresponding sense embeddings in both LMMS and ARES. Considering that gender is associated with the noun than verb sense of occupations in English, this shows that there are hidden gender biases that are not visible at the word-level but become more apparent at the sense-level. This is an important factor to consider when evaluating gender biases in word embeddings, which has been largely ignored thus far in prior work.\\n\\nTo study the relationship between the dimensionality of the embedding space and the social biases it encodes, we compare 1024, 2048 and 2048 dimensional LMMS static sense embeddings and their corresponding word embeddings (computed using (7)) on the WEAT dataset in Figure 2. We see that all types of social biases increase with the dimensionality for both word and sense embeddings. This is in agreement with Silva et al. (2021) who also reported that increasing model capacity in contextualised word embeddings does not necessarily remove their unfair social biases. Moreover, in higher dimensionalities sense embeddings show a higher degree of social biases than the corresponding (sense-insensitive) word embeddings.\\n\\n6.2 Bias in Contextualised Embeddings\\n\\nTo evaluate biases in contextualised sense embeddings, we use SenseBERT (Levine et al., 2020), which is a fine-tuned version of BERT (Devlin et al., 2019) to predict supersenses in the WordNet. For both BERT and SenseBERT, we use base and large pretrained models of dimensionalities 768 and 1024. Using AUL, we compare 1024, 2048 and 2048 dimensional LMMS static sense embeddings and their corresponding word embeddings (computed using (7)) on the WEAT dataset in Figure 2. We see that all types of social biases increase with the dimensionality for both word and sense embeddings. This is in agreement with Silva et al. (2021) who also reported that increasing model capacity in contextualised word embeddings does not necessarily remove their unfair social biases. Moreover, in higher dimensionalities sense embeddings show a higher degree of social biases than the corresponding (sense-insensitive) word embeddings.\"}"}
{"id": "acl-2022-long-135", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Bias in BERT and SenseBERT contextualised word/sense embeddings. In each row, between the AUL bias scores for the word vs. sense embeddings, the larger deviation from 0 is shown in bold.\\n\\nPare biases in BERT and SenseBERT using SSSB, CrowS-Pairs and StereoSet 10 datasets. Note that unlike SSSB, CrowS-Pairs and StereoSet do not annotate for word senses, hence cannot be used to evaluate sense-specific biases.\\n\\nTable 4 compares the social biases in contextualised word/sense embeddings. For both base and large versions, we see that in CrowS-Pairs, BERT to be more biased than SenseBERT, whereas the opposite is true in StereoSet. Among the nine bias types included in CrowS-Pairs, gender bias related test instances are the second most frequent following racial bias. On the other hand, gender bias related examples are relatively less frequent in StereoSet (cf. gender is the third most frequent bias type with 40 instances among the four bias types in StereoSet following race with 149 instances and profession with 120 instances out of the total 321 intrasentence instances). This difference in the composition of bias types explains why the bias score of BERT is higher in CrowS-Pairs, while the same is higher for SenseBERT in StereoSet.\\n\\nIn SSSB, in 8 out of the 12 cases SenseBERT demonstrates equal or higher absolute bias scores than BERT. This result shows that even in situations where no biases are observed at the word-level, there can still be significant degrees of biases at the sense-level. In some cases (e.g. verb sense in base models and colour, language and verb senses for the large models), we see that the direction of the bias is opposite between BERT and SenseBERT. Moreover, comparing with the corresponding bias scores reported by the static word/sense embeddings in Table 3, we see higher bias scores reported by the contextualised word/sense embeddings in Table 4. Therefore, we recommend future work studying social biases to consider not only word embedding models but also sense embedding models.\\n\\n7 Gender Biases in SSSB\\n\\nIn this section, we further study the gender-related biases in static and contextualised word and sense embeddings using the noun vs. verb sense instances (described in \u00a74.3) in the SSSB dataset. To evaluate the gender bias in contextualised word/sense embeddings we use AUL on test sentences in SSSB noun vs. verb category. To evaluate the gender bias in static embeddings, we follow Bolukbasi et al. (2016) and use the cosine similarity between (a) the static word/sense embedding of the occupation corresponding to its noun or verb sense and (b) the gender directional vector $g$, given by (8):\\n\\n$$g = \\\\frac{1}{|C|} \\\\sum_{(m,f) \\\\in C} (m-f)$$\\n\\nHere, $(m, f)$ are male-female word pairs used by Kaneko and Bollegala (2019) such as (he, she) and $m$ and $f$ respectively denote their word embeddings. Corresponding sense-insensitive word embeddings are computed for the 2048 dimensional LMMS embeddings using (7).\\n\\nFigure 3 shows the gender biases in LMMS embeddings. Because static word embeddings are not sense-sensitive, they report the same bias scores for both noun and verb senses for each occupation. For all noun senses, we see positive (male) biases, except for nurse, which is strongly female-biased. Moreover, compared to the noun senses, the verb senses of LMMS are relatively less gender biased. This agrees with the intuition that occupations and not actions associated with those occupations are related to gender, hence can encode social biases. Overall, we see stronger biases in sense embeddings than in the word embeddings.\\n\\nFigure 4 shows the gender biases in BERT/SenseBERT embeddings. Here again, we see that for all noun senses there are high stereotypical biases in both BERT and SenseBERT embeddings, except for nurse where BERT is slightly anti-stereotypically biased whereas SenseBERT shows a similar in magnitude but a stereotypical bias. Recall that nurse is stereotypically associated with the female gender, whereas other occupations are...\"}"}
{"id": "acl-2022-long-135", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Pseudo log-likelihood scores computed using Eq. (5) for stereo and anti-stereo sentences (shown together due to space limitations) using BERT-base and SenseBERT-base models. Here, diff = stereo - anti.\\n\\nFigure 3: Gender biases found in the 2048-dimensional LMMS static sense embeddings and corresponding word embeddings computed using (7). Positive and negative cosine similarity scores with the gender directional vector (computed using (8)) represent biases towards respectively the male and female genders.\\n\\nDespite being not fine-tuned on word senses, BERT shows different bias scores for noun/verb senses, showing its ability to capture sense-related information via contexts. The verb sense embeddings of SenseBERT of guide, mentor and judge are anti-stereotypical, while the corresponding BERT embeddings are stereotypical. This shows that contextualised word and sense embeddings can differ in both magnitude as well as direction of the bias. Considering that SenseBERT is a fine-tuned version of BERT for a specific downstream NLP task (i.e. super-sense tagging), one must not blindly assume that an unbiased MLM to remain as such when fine-tuned on downstream tasks.\\n\\nHow social biases in word/sense embeddings change when used in downstream tasks is an important research problem in its own right, which is beyond the scope of this paper.\\n\\nA qualitative analysis is given in Table 5 where the top-two sentences selected from SSSB express the noun sense of nurse, whereas the bottom-two sentences express its verb sense. From Table 5, we see that SenseBERT has a higher preference (indicated by the high pseudo log-likelihood scores) for stereotypical examples than BERT over anti-stereotypical ones (indicated by the higher diff values).\"}"}
{"id": "acl-2022-long-135", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with different senses of words in this dataset. We specifically considered three types of social biases in SSSB: (a) racial biases associated with a nationality as opposed to a language (e.g. Chinese people are cunning, Chinese language is difficult, etc.), (b) racial biases associated with the word black as opposed to its sense as a colour (e.g. Black people are arrogant, Black dress is beautiful, etc.) and (c) gender-related biases associated with occupations used as nouns as opposed to verbs (e.g. She was a careless nurse, He was not able to nurse the crying baby, etc.). As seen from the above-mentioned examples, by design, SSSB contains many offensive, stereotypical examples. It is intended to facilitate evaluation of social biases in sense embeddings and is publicly released for this purpose only. We argue that SSSB should not be used to train sense embeddings. The motivation behind creating SSSB is to measure social biases so that we can make more progress towards debiasing them in the future. However, training on this data would defeat this purpose.\\n\\nIt is impossible to cover all types of social biases related to word senses in any single dataset. For example, the stereotypical association of a disadvantaged group with a positive attribute (e.g. All Chinese students are good at studying) can also raise unfairly high expectations for the members in that group and cause pressure to hold up to those stereotypes. Such positive biases are not well covered by any of the existing bias evaluation datasets, including the one we annotate in this work.\\n\\nGiven that our dataset is generated from a handful of manually written templates, it is far from complete. Moreover, the templates reflect the cultural and social norms of the annotators from a US-centric viewpoint. Therefore, SSSB should not be considered as an ultimate test for biases in sense embeddings. Simply because a sense embedding does not show any social biases on SSSB according to the evaluation metrics we use in this paper does not mean that it would be appropriate to deploy it in downstream NLP applications that require sense embeddings. In particular, task-specific fine-tuning of even bias-free embeddings can result in novel unfair biases from creeping in.\\n\\nLast but not least we state that the study conducted in this paper has been limited to the English language and represents social norms held by the annotators. Moreover, our gender-bias evaluation is limited to binary (male vs. female) genders and racial-bias evaluation is limited to Black as a race. Extending the categories will be important and necessary future research directions.\\n\\nReferences\\n\\nTolga Bolukbasi, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.\\n\\nAylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356:183\u2013186.\\n\\nSunipa Dev, Tao Li, Jeff Phillips, and Vivek Srikumar. 2020. On Measuring and Mitigating Biased Inferences of Word Embeddings. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 7659\u20137666.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186.\\n\\nSimon De Deyne, Danielle J. Navarro, Amy Perfors, Marc Brysbaert, and Gert Storms. 2019. The \u201csmall world of words\u201d English word association norms for over 12,000 cue words. Behavior Research Methods, 51(3):987\u20131006.\\n\\nYupei Du, Yuanbin Wu, and Man Lan. 2019. Exploring human gender stereotypes with word association test. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6132\u20136142, Hong Kong, China. Association for Computational Linguistics.\\n\\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst. 2019. Understanding undesirable word embedding associations. In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 1696\u20131705, Florence, Italy. Association for Computational Linguistics.\\n\\nChristiane Fellbaum and George Miller. 1998. WordNet: An electronic lexical database. MIT press.\\n\\nMasahiro Kaneko and Danushka Bollegala. 2019. Gender-preserving debiasing for pre-trained word embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1641\u20131650, Florence, Italy.\"}"}
{"id": "acl-2022-long-135", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Masahiro Kaneko and Danushka Bollegala. 2021a. De-biasing pre-trained contextualised embeddings. In Proceedings of 16th conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 1256\u20131266, Online.\\n\\nMasahiro Kaneko and Danushka Bollegala. 2021b. Unmasking the mask\u2013evaluating social biases in masked language models. arXiv preprint arXiv:2104.07496.\\n\\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. 2019. Measuring bias in contextualized word representations. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 166\u2013172, Florence, Italy. Association for Computational Linguistics.\\n\\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, and Yoav Shoham. 2020. SenseBERT: Driving some sense into BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4656\u20134667, Online. Association for Computational Linguistics.\\n\\nDaniel Loureiro and Alipio Jorge. 2019. Language modelling makes sense: Propagating representations through wordnet for full-coverage word sense disambiguation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5682\u20135691, Florence, Italy.\\n\\nThomas Manzini, Lim Yao Chong, Alan W Black, and Yulia Tsvetkov. 2019. Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 615\u2013621, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nChandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger. 2019. On measuring social biases in sentence encoders. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 622\u2013628, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nGeorge A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993.\\n\\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356\u20135371, Online. Association for Computational Linguistics.\\n\\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953\u20131967, Online. Association for Computational Linguistics.\\n\\nArvind Neelakantan, Jeevan Shankar, Alexandre Pasos, and Andrew McCallum. 2014. Efficient non-parametric estimation of multiple embeddings per word in vector space. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059\u20131069.\\n\\nMohammad Taher Pilehvar and Jose Camacho-Collados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267\u20131273, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nShauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. 2020. Null it out: Guarding protected attributes by iterative nullspace projection. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7237\u20137256. Association for Computational Linguistics.\\n\\nJoseph Reisinger and Raymond Mooney. 2010. Multi-prototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 109\u2013117.\\n\\nBianca Scarlini, Tommaso Pasini, and Roberto Navigli. 2020. With more contexts comes better performance: Contextualized sense embeddings for all-round word sense disambiguation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3528\u20133539, Online.\\n\\nTimo Schick, Sahana Udupa, and Hinrich Sch\u00fctze. 2021. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Computing Research Repository, arXiv:2103.00453.\\n\\nDeven Santosh Shah, H. Andrew Schwartz, and Dirk Hovy. 2020. Predictive biases in natural language processing models: A conceptual framework and overview. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5248\u20135264, Online. Association for Computational Linguistics.\\n\\nAndrew Silva, Pradyumna Tambwekar, and Matthew Gombolay. 2021. Towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers. In Proceedings of the 2021\"}"}
{"id": "acl-2022-long-135", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Rajani, Bryan McCann, Vicente Ordonez, and Caiming Xiong. 2020. Double-hard debias: Tailoring word embeddings for gender bias mitigation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.\\n\\nJieyu Zhao, Yichao Zhou, Zeyu Li, Wei Wang, and Kaiwei Chang. 2018. Learning Gender-Neutral Word Embeddings. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4847\u20134853, Brussels, Belgium.\\n\\nDengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch\u00f6lkopf. 2003. Learning with local and global consistency. In Advances in neural information processing systems, volume 16. MIT Press.\\n\\nYi Zhou and Danushka Bollegala. 2021. Learning sense-specific static embeddings using contextualised word embeddings as a proxy. In Proceedings of the 35-th Pacific Asia Conference on Language, Information and Computation (PACLIC), pages 11\u201320, Shanghai, China. Association for Computational Linguistics.\"}"}
