{"id": "lrec-2022-1-475", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HiNER: A Large Hindi Named Entity Recognition Dataset\\n\\nRudra Murthy, Pallab Bhattacharjee, Rahul Sharnagat, Jyotsana Khatri, Diptesh Kanojia, Pushpak Bhattacharyya\\n\\n1 CFILT Lab, IIT Bombay, India. 2 IBM IRL, Bangalore, India. 3 Walmart Labs, USA. 4 Surrey Institute for People-centred AI, University of Surrey, United Kingdom.\\n\\nbhattacharjee.pallab9@gmail.com, {jyotsanak, pb}@iitb.ac.in, rmurthyv@in.ibm.com, rdsharnagat@gmail.com, d.kanojia@surrey.ac.uk\\n\\nAbstract\\n\\nNamed Entity Recognition (NER) is a foundational NLP task that aims to provide class labels like Person, Location, Organisation, Time, and Number to words in free text. Named Entities can also be multi-word expressions where the additional I-O-B annotation information helps label them during the NER annotation process. While English and European languages have considerable annotated data for the NER task, Indian languages lack on that front\u2014both in terms of quantity and following annotation standards. This paper releases a significantly sized standard-abiding Hindi NER dataset containing 109,146 sentences and 2,220,856 tokens, annotated with 11 tags. We discuss the dataset statistics in all their essential detail and provide an in-depth analysis of the NER tag-set used with our data. The statistics of tag-set in our dataset show a healthy per-tag distribution, especially for prominent classes like Person, Location and Organisation. Since the proof of resource-effectiveness is in building models with the resource and testing the model on benchmark data and against the leader-board entries in shared tasks, we do the same with the aforesaid data. We use different language models to perform the sequence labelling task for NER and show the efficacy of our data by performing a comparative evaluation with models trained on another dataset available for the Hindi NER task. Our dataset helps achieve a weighted F1 score of 88.78 with all the tags and 92.22 when we collapse the tag-set, as discussed in the paper. To the best of our knowledge, no available dataset meets the standards of volume (amount) and variability (diversity), as far as Hindi NER is concerned. We fill this gap through this work, which we hope will significantly help NLP for Hindi. We release this dataset with our code and models for further research.\\n\\nKeywords: named entity recognition, dataset, Hindi, human-annotated, low-resource language\\n\\n1. Introduction\\n\\nNamed Entity Recognition (NER) is an essential lower-level task (Ma and Hovy, 2016) in Natural Language Processing (NLP), used to extract and categorize naming entities into a predefined set of classes such as person, location, organization, numeral and temporal entities. A well-performing NER system can help the downstream tasks of Machine Translation (Babych and Hartley, 2003), Information Extraction (Neudecker, 2016), and Questions Answering (Moldovan and Surdeanu, 2002). With the recent surge in the NER research (Sohrab and Miwa, 2018; Plank, 2019; Correa et al., 2020; Grancharova and Dalianis, 2021), the NLP community has also created large annotated datasets for the NER task (Ali et al., 2020; Ding et al., 2021) including code-mixed datasets (Singh et al., 2018). Research in NER has seen remarkable progress since the early approaches and evaluation metrics proposed by Sang (2002; Sang and De Meulder (2003). The task of NER belongs to the class of NLP problems, which can be modelled as a \u2018sequence labelling\u2019 problem akin to the tasks of Part-of-Speech (PoS) tagging and chunking. With the advent of deep learning-based approaches, sequence labelling tasks have invited much attention with successful methods like BiLSTM-CRF (Huang et al., 2015) and Transformers architecture-based fine-tuning (Vaswani et al., 2017; Wolf et al., 2019). However, these methods require significant data to produce a well-performing NER system for any language.\\n\\nTable 1: Comparison of HiNER data statistics with existing Hindi NER datasets\\n\\n|            | Hindi NER Wiki | Hindi NER Fire | Hindi NER ANN | Hindi NER IJCNLP |\\n|------------|---------------|---------------|---------------|-----------------|\\n| Sentences  | 109,146       | 7000          | 9622          | 21833           |\\n| Tokens     | 2,220,856     | 41256         | 116,103       | 541,682         |\\n| Person     | 37,605        | 22,959        | 2,112         | 4,235           |\\n| Location   | 198,282       | 20,131        | 2,268         | 4,307           |\\n| Organization | 26,509       | 14,204        | 170           | 1,272           |\\n\\nNLP for Indian languages has shown progress with the availability of large language models (Kumar et al., 2020; Kakwani et al., 2020; Khanuja et al., 2021) which can help perform various NLP tasks. However, there has been little progress in terms of producing NER datasets for Indian languages, especially for Hindi, which approximately 342 million people speak across the world. NER systems trained on our dataset are expected to perform better than the existing systems trained on lesser data. Existing datasets are either much smaller or have been automatically annotated (silver standard), rendering them incapable of performing the NER task with high accuracy. Moreover, during the creation of a Hindi NER system, one faces various linguistic challenges like:\\n\\nWikipedia: List of Language by Speakers\"}"}
{"id": "lrec-2022-1-475", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unlike English or other languages which use the Latin script, Hindi does not have capitalization as a feature which should have been helpful for performing the NER task.\\n\\nAmbiguity:\\nProper nouns in Hindi can be ambiguous as the same word can belong to a different PoS category. For example, a common Indian female name like 'Pushpa' can be both a proper noun and a common noun meaning 'flower'.\\n\\nSpelling Variations:\\nThe spelling of some words in Hindi can differ depending on the local region in India. For example, the concept or sense of 'Plant' can be denoted by both the words- 'vanaspati' and 'banaspati'.\\n\\nFree Word Order:\\nLanguages like Hindi, which follow a free word order, make the NER task more challenging as computational approaches cannot be complemented with a pattern of PoS tags, or strict word order.\\n\\nDue to the challenges discussed above, it is imperative to train Hindi NER models with a sizeable human-annotated dataset so that deep learning-based approaches can generalize and perform well.\\n\\nThis paper describes our longstanding efforts toward creating a sizeable human-annotated dataset for Hindi NER, which we call \\\"HiNER\\\". We collect this dataset with the help of one annotator and perform experiments to evaluate the efficacy of various deep learning-based approaches. We also include the current public datasets in these experiments and compare the performance of these approaches across datasets. Our work also describes the NER tool developed in-house to help our annotators. This tool also provides a NER service on the back-end, which helps tag the NER data initially, and allows our annotators to post-edit the NER tags with ease. We describe the creation of the back-end NER engine in detail. We also discuss our dataset regarding the various sources and domains and provide an in-depth analysis of the NER tag-set we use for our dataset. The contributions of this work are summarized below:\\n\\n\u2022 We collect a large manually annotated NER dataset for Hindi (HiNER) and release it publicly.\\n\u2022 We evaluate the performance of various deep learning-based NER approaches on our dataset and compare the performance with other publicly available datasets.\\n\u2022 We also release our data, code and models.\\n\\n2. Related Work\\nFor the task of Named Entity Recognition, much existing literature attempts to solve the problem in different languages and domains. However, in this section, we discuss existing literature for Hindi and other Indian languages. We also describe research that highlights different approaches for the NER task. The IJCNLP 2008 NER dataset comprises NER data in five languages, namely Hindi, Bengali, Oriya, Telugu, and Urdu (IJCNLP, 2008). This data has been used extensively in previous research for the Hindi NER task (Ekbal et al., 2008; Gupta and Bhattacharyya, 2010; Bhagavatula et al., 2012; Gali et al., 2008; Saha et al., 2008b; Saha et al., 2008a). The FIRE 2014 dataset (Lalitha Devi et al., 2014) consists of NER data in four languages, namely Hindi, Tamil, Malayalam, and English (Choudhury et al., 2014). Similarly, the WikiANN data (Pan et al., 2017) consists of NER data in 282 languages, including Hindi; however, it is tagged automatically and a known 'silver-standard' dataset for the NER task. Moreover, it consists of only 10000 sentences in total. Rahimi et al. (2019) utilise transfer learning for multilingual NER and discuss their results for 41 languages in zero-shot, few-shot and high-resource scenarios. Singh et al. (2018) use Long Short Term Memory (LSTM), Decision Trees, and Conditional Random Fields (CRF) to perform the NER task on code-mixed Hindi-English social media text. Past research has also tried to utilise voting algorithm-based hybrid approaches, which take CRF, Maximum Entropy (MaxEnt) and rules into account (Srivastava et al., 2011). The authors use the IJCNLP-08 dataset for Hindi, and their approach achieved 82.95 as the F-score. Gupta and Bhattacharyya (2010) also identify a local context within the global information for the task of Hindi NER and report a performance gain of about 10% resulting in a 72% F1 score.\\n\\nRecent work on Indian language NER utilises various deep learning-based approaches for the task. Singh et al. (2021) utilise a Bidirectional LSTM (BiLSTM) architecture with the help of contextualized ELMo word representations (Peters et al., 2018). Similarly, for the Hindi NER task, Athavale et al. (2016) explore the use of BiLSTM and utilise multiple datasets to report around 77.48% F1 score for all tags. Among multilingual approaches, past research has attempted to utilise morphological and phonological sub-word representations to help the NER task for four languages, including Hindi (Chaudhary et al., 2018). C S and Lalitha Devi (2020) also propose various typological features and propose a machine learning-based approach for the NER task in many language families. (Murthy et al., 2018a; Murthy et al., 2018b) demonstrate on FIRE 2014 data that training with combined labelled data of multiple languages can help in Indian language NER. With the help of non-speaker annotations, Tsygankova et al. (2020) show that even without the help of native speakers of the language, manual annotation for an NER task helps perform better than the available cross-lingual methods, which use modern contextualised representations. Focusing on the challenge of code-switching in NER data, Aguilar et al. (2020) propose a new benchmark for code-\"}"}
{"id": "lrec-2022-1-475", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: HiNER dataset statistics in terms of the number of sentences (#sentences), number of words (#words), and the splits created for the Hindi NER task.\\n\\n- **Split Size**\\n  - **Training**: 76025 sentences, 1382979 words (70%)\\n  - **Development**: 10861 sentences, 200259 words (10%)\\n  - **Testing**: 21722 sentences, 553961 words (20%)\\n  - **Total**: 108,608 sentences, 2,137,199 words (100%)\\n\\nTable 3: Number of Entity mentions (Phrases) in Train, Dev, Test splits for the HiNER dataset.\\n\\n- **PERSON**: 26310 (Train), 3771 (Dev), 7524 (Test), Total 37605\\n- **LOCATION**: 137995 (Train), 20100 (Dev), 40187 (Test), Total 198282\\n- **NUMEX**: 17194 (Train), 2555 (Dev), 4662 (Test), Total 24411\\n- **ORGANIZATION**: 18508 (Train), 2645 (Dev), 5356 (Test), Total 26509\\n- **MISC**: 4070 (Train), 553 (Dev), 1080 (Test), Total 5703\\n- **LANGUAGE**: 4187 (Train), 571 (Dev), 1190 (Test), Total 5948\\n- **GAME**: 1214 (Train), 180 (Dev), 369 (Test), Total 1763\\n- **TIMEX**: 13047 (Train), 1762 (Dev), 3653 (Test), Total 18462\\n- **RELIGION**: 823 (Train), 133 (Dev), 234 (Test), Total 1190\\n- **LITERATURE**: 597 (Train), 74 (Dev), 181 (Test), Total 852\\n- **FESTIVAL**: 203 (Train), 30 (Dev), 40 (Test), Total 273\\n- **Total**: 224148 (Train), 32374 (Dev), 64476 (Test), Total 320998\\n\\nFigure 1: HiNER Tagset Details\\n\\n3.2. NER Tool\\n\\nTo ease the annotation task, we create an online tool based on PaCMan (Kanojia et al., 2014). We modify the architecture of PaCMan to allow the upload of untagged NER data. Further, we make changes in the tool front-end to show the full tag-set on the source and target sides of the screen as shown in Figure 2. The untagged data is also shown on the left side of the screen in a text box for clarity to the annotator; however, the annotator must tag the sentence on the right side. Borrowing a feature from the PaCMan interface, we modify the customized right-click-based context menu for different NER tags. The annotator must go through the sentence manually, highlight the named entity and then right-click to provide it with the correct label. This simplified annotation process allows our annotators to label the data with ease. The tool stores the data on a MySQL-based back-end and allows for downloading data files from the interface. Each time an annotator...\"}"}
{"id": "lrec-2022-1-475", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Sentences flagged by the annotator with the entity highlighted and the reasoning for the final decision.\\n\\nprogresses onto the following sentence, the previously tagged sentence is saved automatically. The tool also saves the annotation state in the database, thus allowing an annotator to arrive at the next untagged instance in the database when they log on later. We further simplify the annotation by providing them with a baseline NER engine that allows them to tag the sentence initially and simply \\\"post-edit\\\" the annotations and save the correctly labelled sentence. We describe this baseline NER engine in the following subsection.\\n\\n3.3. NER Engine\\n\\nWe developed a NER engine to provide Named Entity suggestions to our annotators. Each sentence from our dataset is presented on the tool interface as shown in the screenshot (Figure 2), and a button (\\\"Tag Sentence\\\") which allows the NER engine to perform NE tagging of the sentence on the back-end. The tagged sentence is shown to the annotator on the annotation screen's right side, which can be edited later. Our annotators reported that they could easily modify the tool's engine.\"}"}
{"id": "lrec-2022-1-475", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"errors. This NER engine was developed using FIRE 2013 Hindi NER corpus (RK and Lalitha Devi, 2013). Due to the limited size of the training corpus, it was hard to create a tagger that could learn a generic sequence of tags. To support the model, we employed word2vec (Mikolov et al., 2013) to learn the semantic embeddings for single and multi-word tokens based on a large Hindi Wikipedia dump. These learned embeddings were then used to train a simple perceptron-based neural network model to infer named entities. A separate service was created in conjunction with the front-end UI of our NER tool to handle the annotation requests. Our annotators reported that this engine was prone to errors, especially when tagging multi-word named entities, but it could handle commonly used named entities.\\n\\n3.4. Annotation Ambiguity\\nAs only one annotator annotated the data, ensuring that the dataset's quality is not compromised is essential. We encouraged the annotator to raise reports for entities he was not confident in tagging. The authors then take a majority voting on such instances to assign an appropriate entity or not an entity label. We now provide a few examples of such instances raised by the annotator in Table 4.\\n\\n4. Dataset Evaluation\\nIn this section, we discuss the evaluation of our dataset based on different approaches to NER. With the help of our annotator, we collected the NER-labelled dataset as described above. We perform the task of Hindi NER with the help of various contextual language models and in different settings. With our dataset, we create a data split of 70% for training, 10% for development, and 20% for testing, with statistics, as shown in Table 2. We ensured a balanced percentage of tags in each of the splits with stratification, as can be seen from Table 3.\\n\\n4.1. Experimental Setup\\nWith the advent of contextualized word representations, various language models have been proposed which can be utilized to perform NLP tasks (Devlin et al., 2019; Conneau et al., 2020; Kakwani et al., 2020; Khanuja et al., 2021). We use these four models to evaluate the performance of the NER task on our dataset. Additionally, we use the FIRE 2014 dataset to compare the efficacy of both datasets and present the results in the next section. We also utilize the models trained on our data and test on the FIRE 2014 test split to evaluate the model performance in a cross-dataset scenario.\\n\\nWe use the variation mBERT base \u2212 cased of multilingual BERT (mBERT), which supports 104 languages, and has 12 layers with 768 hidden layers, along with a total of 110M parameters. We use XLM-R base and XLM-R large (Conneau et al., 2020) which are pre-trained multilingual language models to fine-tune for NER task. However, IndicBERT (Kakwani et al., 2020), and MuRIL (Khanuja et al., 2021) are more suited to the task as it supports Indian languages in particular and is trained on shared vocabulary from Indic languages. IndicBERT is trained on 12 major Indian languages, including Hindi, is trained on around 9 billion tokens, and has a restriction on the maximum sequence length (128). Similarly, MuRIL is a model pre-trained on 17 Indian languages and their transliterated counterparts. We perform hyper-parameter tuning of each model and select the hyper-parameters giving the best F-Score on the development set. The model is trained using the best hyper-parameter for 5 runs. We report the mean F1-Score of various pre-trained LMs on our HiNER dataset. This table reports a mean F1-score and its standard deviation over 5 runs.\"}"}
{"id": "lrec-2022-1-475", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Test Set F1-Score of various pre-trained LMs on our HiNER dataset (Collapsed). This table reports a mean F1-score and its standard deviation over 5 runs.\\n\\n| Model          | HiNER (collapsed) 2014 Zero-Shot | FIRE 2014 Zero-Shot |\\n|---------------|-------------------------------|-------------------|\\n| Indic-BERT    | 91.37 \u00b1 0.67                 | 62.79 \u00b1 0.68      |\\n| mBERT         | 91.10 \u00b1 0.34                 | 62.14 \u00b1 0.59      |\\n| MuRIL         | 92.09 \u00b1 0.27                 | 62.58 \u00b1 2.44      |\\n| XLM-R base    | 92.06 \u00b1 0.27                 | 65.63 \u00b1 0.76      |\\n| XLM-R large   | 92.20 \u00b1 0.22                 | 66.75 \u00b1 0.30      |\\n\\nTable 7: Test Set Micro F1-Score of various pre-trained LMs on both datasets where HiNER (collapsed) is our dataset with only the Person, Location, and Organization tags. This table reports a mean F1-score and its standard deviation over 5 runs.\\n\\n| Model          | HiNER (collapsed) 2014 Zero-Shot | FIRE 2014 Zero-Shot |\\n|---------------|-------------------------------|-------------------|\\n| Indic-BERT    | 91.37 \u00b1 0.67                 | 62.79 \u00b1 0.68      |\\n| mBERT         | 91.10 \u00b1 0.34                 | 62.14 \u00b1 0.59      |\\n| MuRIL         | 92.09 \u00b1 0.27                 | 62.58 \u00b1 2.44      |\\n| XLM-R base    | 92.06 \u00b1 0.27                 | 65.63 \u00b1 0.76      |\\n| XLM-R large   | 92.20 \u00b1 0.22                 | 66.75 \u00b1 0.30      |\"}"}
{"id": "lrec-2022-1-475", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"by the XLM-R large Model on HiNER data. Table 10 provides more detailed insights into the performance of the system by reporting strict, exact evaluation metrics (Chinchor and Sundheim, 1993). We use the nervaluate package to calculate the above statistics for each entity type. Specifically, we pick the predictions from one of the runs using XLM-R large as this model consistently gave better results compared to the other pre-trained language models. We use two different evaluation schemas mentioned in the Table 8. Exact encourages models to identify the named entity phrase correctly while ignoring the type mismatch. We observe that for some entity type like Location, NUMEX, Organization Missed errors are more than the Spurious errors. On the other hand, for entity types like Person, Misc, Language, Game, TIMEX Spurious errors are more. We additionally report F1-Score according to the evaluation schema for each entity type. The most challenging entity categories are Literature, Festival, MISC, Language, Religion, and Game entities. We observe that the model is able to identify Misc, Language, Religion, Literature as named entities but unable to assign the correct entity type. This can be seen in the F-Score different between Strict and Exact evaluation schema.\\n\\n| Evaluation Schema Explanation | Strict | Exact |\\n|-------------------------------|--------|-------|\\n| The exact boundary surface string match and entity type match | 6475 | 6565 |\\n| The exact boundary match over the surface string, regardless of the type | 622 | 532 |\\n\\nTable 8: Short Description of the Evaluation Schema used\\n\\nFor each type of evaluation schema (i.e., strict and exact) we report the following categories of errors listed in Table 9.\\n\\n| Error type Explanation | Strict | Exact |\\n|------------------------|--------|-------|\\n| Correct | 37960 | 38113 |\\n| Incorrect | 1028 | 875 |\\n| Missed | 1199 | 1199 |\\n| Spurious | 688 | 688 |\\n\\nTable 9: Short Description of the Categories of Errors\\n\\nError Category Strict Exact\\nPerson Correct 6475 6565\\nIncorrect 622 532\\nMissed 427 427\\nSpurious 537 537\\nF1 0.8543 0.8662\\n\\nLocation Correct 37960 38113\\nIncorrect 1028 875\\nMissed 1199 1199\\nSpurious 688 688\\nF1 0.9506 0.9545\\n\\nNUMEX Correct 3047 3097\\nIncorrect 567 517\\nMissed 1048 1048\\nSpurious 526 526\\nF1 0.6923 0.7037\\n\\nOrganization Correct 4195 4263\\nIncorrect 535 467\\nMissed 626 626\\nSpurious 544 544\\nF1 0.7893 0.8021\\n\\nMisc Correct 804 882\\nIncorrect 137 59\\nMissed 139 139\\nSpurious 265 265\\nF1 0.7034 0.7717\\n\\nLanguage Correct 1115 1133\\nIncorrect 60 42\\nMissed 15 15\\nSpurious 42 42\\nF1 0.9265 0.9414\\n\\nGame Correct 276 279\\nIncorrect 57 54\\nMissed 36 36\\nSpurious 145 145\\nF1 0.6517 0.6588\\n\\nTIMEX Correct 3018 3055\\nIncorrect 328 291\\nMissed 307 307\\nSpurious 363 363\\nF1 0.8199 0.8299\\n\\nReligion Correct 175 183\\nIncorrect 26 18\\nMissed 33 33\\nSpurious 32 32\\nF1 0.7495 0.7837\"}"}
{"id": "lrec-2022-1-475", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10, continued\\n\\n|      | Literature | Festival |\\n|------|------------|----------|\\n|       |            |          |\\n|       |            |          |\\n|       |            |          |\\n\\n|      |       |       |\\n|------|------|------|\\n|      |      |      |\\n\\nTable 10: Detailed Strict and Exact Results on HiNER data from XLM-R large\\n\\n6. Conclusion and Future Work\\n\\nWe describe our efforts to create a sizeable human-annotated dataset, HiNER, for the task of Named Entity Recognition in the Hindi language. We discuss the motivation for this research, the challenges specific to Hindi NER, and provide coverage of the past research performed for the NER task in Hindi. We discuss the dataset creation in detail and provide an in-depth analysis of the tag-set used to label our NER data. We also describe the NER annotation tool created to help our annotators along with the NER engine it utilises to label the data initially on the tool interface. We split our data and performed experiments to evaluate different language models to perform the NER task by fine-tuning them. We also perform similar experiments on another dataset for a comparative evaluation. We discuss our results in detail and show how large human-annotated NER data is essential for the task of Hindi NER. We release this dataset and the models we train; for the NLP community to utilise them for the downstream NLP tasks. We choose the CC-BY-SA 4.0 Licensing terms to release this data. In future, we plan to keep extending this dataset with the help of our ongoing annotation process.\\n\\nBibliographical References\\n\\nAguilar, G., Kar, S., and Solorio, T. (2020). LinCE: A centralized benchmark for linguistic code-switching evaluation. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 1803\u20131813, Marseille, France, May. European Language Resources Association.\\n\\nAli, W., Lu, J., and Xu, Z. (2020). SiNER: A large dataset for Sindhi named entity recognition. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 2953\u20132961, Marseille, France, May. European Language Resources Association.\\n\\nAthavale, V., Bharadwaj, S., Pamecha, M., Prabhu, A., and Shrivastava, M. (2016). Towards deep learning in Hindi NER: An approach to tackle the labelled data sparsity. In Proceedings of the 13th International Conference on Natural Language Processing, pages 154\u2013160, Varanasi, India, December. NLP Association of India.\\n\\nBabych, B. and Hartley, A. (2003). Improving machine translation quality with automatic named entity recognition. In Proceedings of the 7th International EAMT workshop on MT and other language technology tools, Improving MT through other language technology tools, Resource and tools for building MT at EACL 2003.\\n\\nBhagavatula, M., Santosh, G., and Varma, V. (2012). Language independent named entity identification using wikipedia. In Proceedings of the First Workshop on Multilingual Modeling, pages 11\u201317.\\n\\nC S, M. and Lalitha Devi, S. (2020). A deeper study on features for named entity recognition. In Proceedings of the WILDRE5\u20135th Workshop on Indian Language Data: Resources and Evaluation, pages 66\u201372, Marseille, France, May. European Language Resources Association (ELRA).\\n\\nChaudhary, A., Zhou, C., Levin, L., Neubig, G., Mortensen, D. R., and Carbonell, J. G. (2018). Adapting word embeddings to new languages with morphological and phonological subword representations. arXiv preprint arXiv:1808.09500.\\n\\nChinchor, N. and Sundheim, B. (1993). MUC-5 evaluation metrics. In Fifth Message Understanding Conference (MUC-5): Proceedings of a Conference Held in Baltimore, Maryland, August 25-27, 1993.\\n\\nChoudhury, M., Chittaranjan, G., Gupta, P., and Das, A. (2014). Overview of fire 2014 track on transliterated search. Proceedings of FIRE, pages 68\u201389.\\n\\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, E., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online, July. Association for Computational Linguistics.\\n\\nCopara, J., Knafou, J., Naderi, N., Moro, C., Ruch, P., and Teodoro, D. (2020). Contextualized French language models for biomedical named entity recognition. In Actes de la 6e conf\u00e9rence conjointe Journ\u00e9es d\u2019\u00c9tudes sur la Parole (JEP, 33e \u00e9dition), Traitement Automatique des Langues Naturelles (TALN, 27e \u00e9dition), Rencontre des \u00c9tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R\u00c9CITAL, 22e \u00e9dition). Atelier D\u2019\u00c9fici\u2122 Fouille de Textes, pages 36\u201348, Nancy, France, 6. ATALA et AFCP.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics.\"}"}
{"id": "lrec-2022-1-475", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ding, N., Xu, G., Chen, Y., Wang, X., Han, X., Xie, P., Zheng, H.-T., and Liu, Z. (2021). Few-nerd: A few-shot named entity recognition dataset. arXiv preprint arXiv:2105.07464.\\n\\nEkbal, A., Haque, R., Das, A., Poka, V., and Bandyopadhyay, S. (2008). Language independent named entity recognition in Indian languages. In Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages.\\n\\nGali, K., Surana, H., Vaidya, A., Shishtla, P. M., and Sharma, D. M. (2008). Aggregating machine learning and rule based heuristics for named entity recognition. In Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages.\\n\\nGoldhahn, D., Eckart, T., and Quasthoff, U. (2012). Building large monolingual dictionaries at the Leipzig corpora collection: From 100 to 200 languages. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 759\u2013765, Istanbul, Turkey, May. European Language Resources Association (ELRA).\\n\\nGrancharova, M. and Dalianis, H. (2021). Applying and sharing pre-trained BERT-models for named entity recognition and classification in Swedish electronic patient records. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 231\u2013239, Reykjavik, Iceland (Online), May 31\u20132 June. Link\u00f6ping University Electronic Press, Sweden.\\n\\nGupta, S. and Bhattacharyya, P. (2010). Think globally, apply locally: using distributional characteristics for Hindi named entity identification. In Proceedings of the 2010 Named Entities Workshop, pages 116\u2013125.\\n\\nHuang, Z., Xu, W., and Yu, K. (2015). Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991.\\n\\nIJCNLP. (2008). IJCNLP NER Dataset. In Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages.\\n\\nJha, G. N. (2010). The TDIL Program and the Indian Language Corpora Initiative (ILCI). In Proceedings of the Seventh conference on International Language Resources and Evaluation, LREC 2010.\\n\\nKakwani, D., Kunchukuttan, A., Golla, S., Gokul, N., Bhattacharyya, A., Khapra, M. M., and Kumar, P. (2020). inlpsuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 4948\u20134961.\\n\\nKhanuja, S., Bansal, D., Mehtani, S., Khosla, S., Dey, A., Gopalan, B., Margam, D. K., Aggarwal, P., Nagpipogu, R. T., Dave, S., et al. (2021). Muril: Multilingual representations for Indian languages. arXiv preprint arXiv:2103.10730.\\n\\nKumar, S., Kumar, S., Kanojia, D., and Bhattacharyya, P. (2020). \\\"A Passage to India\\\": Pre-trained word embeddings for Indian languages. In Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL), pages 352\u2013357, Marseille, France, May. European Language Resources Association.\\n\\nLalitha Devi, S., RK Rao, P., C.S, M., and Sundar Ram, R. V. (2014). Indian Language NER Annotated FIRE 2014 Corpus (FIRE 2014 NER Corpus). In Named Entity Recognition Indian Languages FIRE 2014 Evaluation Track.\\n\\nMa, X. and Hovy, E. H. (2016). End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. CoRR, abs/1603.01354.\\n\\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119.\\n\\nMoldovan, D. and Surdeanu, M. (2002). On the role of information retrieval and information extraction in question answering systems. In International Summer School on Information Extraction, pages 129\u2013147. Springer.\\n\\nMurthy, R., Khapra, M. M., and Bhattacharyya, P. (2018a). Improving NER Tagging Performance in Low-Resource Languages via Multilingual Learning. ACM Trans. Asian Low-Resour. Lang. Inf. Process., 18(2), dec.\\n\\nMurthy, R., Kunchukuttan, A., and Bhattacharyya, P. (2018b). Judicious selection of training data in assisting language for multilingual neural NER. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 401\u2013406, Melbourne, Australia, July. Association for Computational Linguistics.\\n\\nNakayama, H. (2018). seqeval: A python framework for sequence labeling evaluation. Software available from https://github.com/chakki-works/seqeval.\\n\\nNeudecker, C. (2016). An open corpus for named entity recognition in historic newspapers. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 3706\u20133711, Barcelona, Spain, May. European Language Resources Association.\"}"}
{"id": "lrec-2022-1-475", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
