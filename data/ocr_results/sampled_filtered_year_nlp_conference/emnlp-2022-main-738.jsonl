{"id": "emnlp-2022-main-738", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nRelation extraction has focused on extracting semantic relationships between entities from the unstructured written textual data. However, with the vast and rapidly increasing amounts of spoken data, relation extraction from speech is an important but under-explored problem. In this paper, we propose a new information extraction task, speech relation extraction (SpeechRE). To facilitate further research, we construct the first synthetic training datasets, as well as the first human-spoken test set with native English speakers. We establish strong baseline performance for SpeechRE via two approaches. The pipeline approach connects a pretrained ASR module with a text-based relation extraction module. The end-to-end approach employs a cross-modal encoder-decoder architecture. Our comprehensive experiments reveal the relative strengths and weaknesses of these approaches, and shed light on important future directions in SpeechRE research. We share the source code and datasets on https://github.com/wutong8023/SpeechRE.\\n\\n1 Introduction\\nRelation extraction (RE) (Han et al., 2020) is an important information extraction task, which aims at extracting structured semantic relations between entities from unstructured data, typically text. Besides text, there is also a plethora of speech data that is being continually produced. These include news reports, interviews, meetings and dialogues, to name a few. Extracting relations from speech is an important but under-explored problem.\\n\\nIn this work, we take the first step towards addressing relation extraction (RE) from speech, introducing a new information extraction task, Speech Relation Extraction (SpeechRE). The input for this task is raw audio and the output is one or more triplets, each of which representing a relation between a pair of two entities appearing in the speech, e.g., [entity1, relation, entity2]. SpeechRE and text-based RE (TextRE) both involve content understanding. The former is more challenging than the latter, mainly due to the characteristics of speech. (i) Speech carries much richer information beyond linguistic content (unlike text), for instance, emotion, speaker style and background noise; and it is non-trivial to disentangle the content element (Mohamed et al., 2022), which is needed for SpeechRE. (ii) Speech is continuous without sequence/word boundaries, implying the difficulty of determining the exact audio spans for target words (entity and relation). (iii) Audio signals are orders of magnitude longer than the corresponding transcripts, which makes speech encoding for long-span extraction more challenging due to more demanding hardware requirements, especially with Transformer (Vaswani et al., 2017).\\n\\nIn the absence of SpeechRE training data, we construct three benchmark datasets for this task by converting two commonly used TextRE datasets.\"}"}
{"id": "emnlp-2022-main-738", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"datasets (i.e., CoNLL04 (Roth and Yih, 2004a) and ReTACRED (Stoica et al., 2021a) to speech with a SOTA text-to-speech (TTS) system. We then pair the synthetic speech with the corresponding target relation triplets as instances. To better evaluate model performance on real speeches, we also compile a human-read test set.\\n\\nWe approach SpeechRE with a pipeline method, SpeechRE\\\\textsubscript{pipe}, and an end-to-end (e2e) method, SpeechRE\\\\textsubscript{e2e}. In SpeechRE\\\\textsubscript{pipe}, we train our pipeline model with an automatic speech recognition (ASR) module that converts speech to text, followed by a RE module that extracts triplets. In SpeechRE\\\\textsubscript{e2e}, we build a single speech-to-text model that extracts triplets directly from speech. We use a SOTA pretrained speech encoder, WAV2VEC2 (Baevski et al., 2020) and BART (Lewis et al., 2020) decoder. Inspired by Li et al. (2021), we attach a length adaptor on top of the encoder to bridge the length mismatch between speech representation and text representation. The end-to-end approaches in speech processing tasks often face more severe data scarcity issues than the pipeline approaches (Sperber and Paulik, 2020), as the latter can essentially leverage massive ASR data and labelled data for the downstream text-based tasks. To tackle this challenge, we further propose two data augmentation techniques: upsampling via generating speech with different voices, and pseudo-labelling (He et al., 2021) by leveraging abundant ASR data and a SOTA TextRE system.\\n\\nOur contributions can be summarized as follows.\\n\\n\u2022 We present a new task, Speech Relation Extraction (SpeechRE). To support the development of this task, we create and release a synthetic SpeechRE dataset, including training/dev/test sets, as well as a human-read test set.\\n\\n\u2022 We establish strong baseline performance via a pipeline approach and an e2e approach. Our extensive experiments identify a performance gap between TextRE and SpeechRE, and the gap between the pipeline approach and the e2e approach, motivating further research.\\n\\n\u2022 Our analysis shows that the performance gap of the end-to-end approach mainly comes from the data scarcity problem and the difficulty of spoken name recognition. We propose two data augmentation methods to the problem.\\n\\n\u2022 Based on our findings, we suggest three main directions for future exploration to advance speech relation extraction.\\n\\n2 Related Work\\n\\n2.1 Relation Extraction\\n\\nAs an essential component of information extraction, named entity recognition (NER) and relation extraction (RE) have attracted much attention in the research community. Relation extraction is usually studied as a natural language processing task of textual data (Nasar et al., 2021; Wu et al., 2021; Zheng et al., 2021c; Chen et al., 2022b). With the widespread of multimedia data on social media, some researchers have begun to explore relation extraction from data in other modalities such as images (Zheng et al., 2021a; Chen et al., 2022a; Zheng et al., 2021b). Although some work has focused on spoken language, such as dialogue relation extraction (Yu et al., 2020; Zhou et al., 2021), these studies are all based on transcripts, i.e. high-quality transcribed text from speech, which is still within the confines of text-based relation extraction. Moreover, given the transcribed text, the side information of voice, e.g., emotion, speaker identity is ignored from the spoken language.\\n\\n2.2 Spoken Language Understanding\\n\\nSpoken Language Understanding (SLU) aims to extract the meaning from speech utterances. It has wide applications from voice search to meeting summarization and has received great attention from industry and academia (Tur and De Mori, 2011). A typical SLU system involves mainly two tasks, i.e., intent detection and slot filling (Tur and De Mori, 2011). Traditionally, SLU systems have a pipeline structure, in which an ASR module is first used to convert speech to text and then a NLU system is deployed to determine semantics from text.\\n\\nA major drawback of this approach is that each module is trained and optimized independently (Serdyuk et al., 2018). (i) The ASR model is optimized to minimized Word Error Rate (WER), often equally weighting every word, whereas not every word has the same impact on SLU. (ii) The NLU model is trained on clean text without ASR errors, i.e. transcripts. During evaluation, however, it receives erroneous ASR outputs and these errors are propagated to NLU, impairing its performance. End-to-end (e2e) learning has\"}"}
{"id": "emnlp-2022-main-738", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"thus attracted interests from the community, for its potential to addressing SLU in a more principled way (Serdyuk et al., 2018). Since the first e2e approach proposed by Serdyuk et al. (2018), the field has made significant advances (Qin et al.) and many techniques, such as pretraining (Castellucci et al., 2019), have been proposed.\\n\\nSimilar to other speech processing tasks (e.g. speech translation (Sperber and Paulik, 2020)), SLU also faces the data scarcity issue, as it can be very expensive to annotate such a dataset, whereas the pipeline method can benefit from existing and emerging massive ASR data and NLU datasets.\\n\\nSpeech relation extraction (SpeechRE) is a new SLU task and thus inherits the merits and demerits of the pipeline and e2e approaches. We leverage advances that have been developed in related disciplines in this work and evaluate their relative strengths and weaknesses in \u00a74.\\n\\n3 Speech Relation Extraction\\n\\nWe define SpeechRE as a joint entity and relation extraction task that takes a speech utterance as the input and generates a set of relational triplets in the form of $[\\\\text{entity}_1, \\\\text{relation}, \\\\text{entity}_2]$ as the output.\\n\\nIn this section, we first describe the data construction method (\u00a73.1). Next, we present our two approaches to the task (\u00a73.2). Last, we describe our two data augmentation techniques (\u00a73.3) to improve end-to-end SpeechRE performance.\\n\\n3.1 Dataset Construction\\n\\nSynthetic Data. As there is no readily available SpeechRE data, we generate SpeechRE data from existing TextRE corpora. Given a TextRE dataset consisting of pairs of $<$source (i.e. transcript), triplet$>$ we convert the transcript to human-like speech with a TTS model. A typical TTS system comprises a Text-to-Spectrogram module, which takes discrete text as input and produces mel-spectrograms, and a vocoder, which converts the mel-spectrograms into waveforms.\\n\\nWe choose Tacotron2-DCA as the TTS system and Mulitband-Melgan as the vocoder.\\n\\nOnce the synthesis process is complete, our data would contain triples of $<$synthetic speech, transcripts, triplets$>$. Training/dev/test sets are compiled following this process, while obeying the original TextRE split.\\n\\nReal Data. To evaluate the performance of our models on realistic speech, we randomly choose 200 instances from the ReTACRED10 test set and engage a native English speaker to read the corresponding transcripts. This real SpeechRE test set can be used as a benchmark for future research. Please refer to \u00a74.8 for demonstration of synthetic and real data.\\n\\n3.2 SpeechRE Approaches\\n\\nWe describe our pipeline (SpeechRE pipe) and end-to-end (SpeechRE e2e) approaches in this section. As depicted in Figure 2, SpeechRE pipe consists of an ASR module for turning speech into text and a TextRE module for extracting triplets from the text, whereas SpeechRE e2e has a simple architecture with a speech encoder, a length adaptor and a text decoder, which outputs triplets directly.\\n\\nThe Pipeline Approach. We use W2V2-large as our ASR module. It is a speech encoder, pretrained in a self-supervised manner. Its architecture starts with a feature encoder composed of several 1D convolutional neural networks that process raw waveforms and emits latent speech representations. Following that, a quantization module is attached to extract discrete latent vectors. Next, a context encoder made of 24 Transformer (Vaswani et al., 2017) layers is used to learn contextualized representations from masked outputs of the feature encoder. The whole model is optimized to discriminate a true masked vector from the ones produced by the model. After pre-training, only the feature and context encoders are retained and used for downstream tasks. Compared with other ASR models, W2V2 obtains superior performance by fine-tuning it with a small amount of labelled speech. Additionally, it works on raw audio signals directly, avoiding the risk of information loss using hand-crafted features (Latif et al., 2020). The W2V2 model we use is already fine-tuned on ASR data and we do not further fine-tune it.\\n\\nWe utilize REBEL (Cabot and Navigli, 2021) as our TextRE module. It uses a pretrained language model BART (Lewis et al., 2020) as the backbone and treats TextRE as a text generation task. Concretely, the input is text (the output from ASR in our case), and the output is linearized triplets, in the form of \\\"$<\\\\text{triplet}> \\\\text{entity}_1 <\\\\text{subj}> \\\\text{entity}_2 <\\\\text{obj}> \\\\text{relation}$\\\". The generative model may not restrict the output entities exactly the same with a mention in input text, which is an advantage for SpeechRE with ASR outputs, such a task extracting from the text containing noisy entity mentions.\\n\\nAn alternative to REBEL is to employ a classification-based model as our TextRE module. We experiment with Spert (Eberts and Ulges, 2020)\"}"}
{"id": "emnlp-2022-main-738", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and TPlinker (Wang et al., 2020) in this work. Given that the ground-truth entity may not appear in the transcribed text computed by ASR, we modify the ground-truth entity in the training set by referring to the fuzzy-matched longest substring mentioned in transcribed text, such a fuzzy-matched longest substring is measured by Levinstein distance.\\n\\nThe end-to-end Approach\\n\\nWe formulate SpeechRE as a speech-to-text task that requires a speech encoder and a text decoder. We employ the aforementioned W2V2 as our encoder, for its capabilities of encoding general-purpose knowledge. We take the decoder component of BART-large (Lewis et al., 2020) as the text decoder. Na\u00efvely jointing them may lead to optimization issues, as they are pretrained on different modalities which differ significantly in length. To address this issue, inspired by Li et al. (2021), we introduce a length adaptor made of \\\\( n \\\\) number of 1-d convolutional layers, each of which is parametrized with kernel \\\\( p \\\\), stride \\\\( s \\\\) and padding \\\\( p \\\\). This adaptor has a sequence reduction effect of \\\\( \\\\sim s^n \\\\).\\n\\nWe follow the partial training strategy used by G\u00e1llego et al. (2021). We train the length adaptor together with part of the encoder and decoder (including encoder self-attention, encoder-decoder cross-attention and layer normalization), while freezing the rest of the parameters. The trained parameters account for \\\\( \\\\sim 20\\\\% \\\\) of the entire model. This training strategy has shown to be efficient, while retaining performance in speech translation tasks (Zhao et al., 2022).\\n\\n### 3.3 Data Augmentation for Speech Relation Extraction\\n\\nTo address the data scarcity issue facing SpeechRE, we propose two data augmentation techniques: upsampling and pseudo-labelling. For upsampling, given a SpeechRE corpus, we use a multi-speaker TTS system (Kim et al., 2021) and generate synthetic speeches with 4 different voices. This yields 4 more synthetic SpeechRE datasets.\\n\\nPseudo-labelling has been widely used in NLP (He et al., 2021) due to its effectiveness in improving task performance. Specifically, given a SpeechRE dataset \\\\( D \\\\), we fine-tune the pretrained REBEL model on \\\\(<\\\\text{transcript, triplet}>\\\\) training instances to adapt it to the current domain. Next, we run the fine-tuned REBEL over the large-scale English dataset of CommonVoice (V9) where audios are recorded by volunteers of different demographic characteristics. Together with the original speech, this gives a total of 922k instances containing \\\\( <\\\\text{real_speech, transcript, pseudo_triplet}>\\\\). Then, we filter out noisy data if a pseudo_triplet meets any of the following criteria: i) relation is \u201cno_relation\u201d; 2) no subject/object entity is generated; and 3) subject and object entities are both pronouns. We thus obtain 380k clean instances. Depending on the type of relations available in \\\\( D \\\\), further filtering may be applied to remove spurious relation triplets.\\n\\n4https://commonvoice.mozilla.org/en/datasets\"}"}
{"id": "emnlp-2022-main-738", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Experiment\\n\\n4.1 Dataset\\nWe conduct experiments on our proposed SpeechRE datasets, i.e., Speech-CoNLL04 and Speech-ReTACRED, aligning with their original dataset CoNLL04 (Roth and Yih, 2004b) and ReTACRED (Stoica et al., 2021b). Furthermore, we pick 10 relations with the largest number of instances from the ReTACRED dataset, and remove the instances with none of relation or containing the other 30 relations. We named the sub-dataset of ReTACRED as ReTACRED10, and utilize it as the test-bed for sufficient supervised learning. We detail data statistics in Table 1.\\n\\nMoreover, we use ReTACRED10 to fine-tune REBEL for pseudo-labelling. The fine-tuned model generates pseudo labels of 137 relations. We remove pseudo instances whose labels fall out of Re-TACRED10. We then have 363k instances remained and each instance has one triple. Furthermore, we sample from pseudo set to \\\\(1.8 \\\\times\\\\) for each relation in ReTACRED10. At this point, the total number of data sampled is \\\\(2.5 \\\\times\\\\) to ReTACRED10, as a transcript in ReTACRED10 often has multiple relations.\\n\\n4.2 Baselines\\nWe select three joint entity and relation extraction methods as baselines: TP-Linker (Wang et al., 2020) formulates joint extraction as a token pair linking problem and introduces a handshaking tagging scheme that aligns the boundary tokens of entity pairs under each relation type. Spert (Eberts and Ulges, 2020) formulates the task as a two-stage classification task, with classifying each candidate continuous span for entity detection and then classifying the inter-context for relation classification. REBEL (Cabot and Navigli, 2021) treats joint entity and relation extraction as a text generation task. We also attempt these three methods as the pluggable TextRE modules in SpeechRE pipe.\\n\\n4.3 Evaluation metrics\\nEvaluating SpeechRE is difficulty because of the strict matching of entities. The error of a letter or the difference in case lead to failure in entity matching, which lowers the results of triplets. For this reason, we evaluate the baseline models and SpeechRE models based on the metrics (i.e., Recall, Precision and micro-F1) commonly used in TextRE, with modifications. Specific to entities, we ignore the span of entities due to the lack of span information in audio, and TextRE applies the same method. Additionally, we do not consider entity when evaluating relations, which is equivalent to the task of relation classification of sentences. The reason is that spoken entity recognition is a very difficult tasks as most entities have low frequency in a dataset; when predicted entities were taken into account, the results would cover the true performance of relation generation. When evaluating triplets, we make sure that the head entity and tail entity and the relation between them are all correct.\\n\\n4.4 Implementation Details\\nWe use a pretrained W2V2 model to convert speech to text, without fine-tuning it. Since the ASR outputs the model produces are all lower-cased without punctuation, we perform post-processing on the outputs for punctuation restoring and casing with another pretrained model.\\n\\nFor TextRE model, we mostly follow the instructions in Cabot and Navigli (2021) and start from the REBEL that using Bart as the pretrained model. The original REBEL labels the entities in the input text using punctuation marks to indicate entities' position in the input. Since SpeechRE pipe does not use labels to bias the model with entity information from plain audio or text, we remove the entity labels when preprocessing ReTACRED (source sentences) and ReTACRED10.\\n\\nTo train our SpeechRE model, we use the pretrained W2V2 large and the pretrained Bart. We keep the W2V2 feature extractor frozen. We set kernel size, stride and padding to 3, 2, 1 for all 3 CNN layers for the length adaptor. We apply data augmentation (Potapczyk et al., 2019) on the audio data on the fly by applying the effects of \\\"tempo\\\" and \\\"pitch\\\" to change the speech speed, and \\\"echo\\\" to simulate echoing in large rooms. We train our SpeechRE models for 23k updates and set early stopping of 20 updates. We use Adam (Kingma and Ba, 2015) optimizer with parameters (0.99, 0.98), while setting clip norm to 20. We use the learning rate to 1e-4, monitored by a tri-stage scheduler.\\n\\n5 https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self\\n6 https://huggingface.co/flexudy/t5-small-wav2vec2-grammar-fixer\\n7 https://github.com/Babelscape/rebel\\n8 https://huggingface.co/facebook/bart-large\\n9 https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_vox_960h_pl.pt\\n10 https://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz\"}"}
{"id": "emnlp-2022-main-738", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets # Relations # Instances # Triplets # Avg. tokens # Avg. audio length\\n(train || dev || test) (train || dev || test) (in transcripts) (in seconds)\\n\\nCoNLL04 5 922 || 231 || 288 1,283 || 343 || 422 29.1 11.3\\n\\nReTACRED 40 33,477 || 9,350 || 5,805 58,465 || 19,584 || 13,418 36.3 12.9\\n\\nReTACRED10 10 11,116 || 3,892 || 2,513 15,665 || 5,970 || 4,204 34.7 12.6\\n\\nTable 1: Dataset statistics.\\n\\nMethod Entity Relation Triplet\\n\\nTextRE TP-Linker\\n\\nCONLL04 78.63 83.49 58.56\\nReTACRED 50.46 51.83 20.39\\nReTACRED10 65.51 65.17 37.01\\n\\nSpert\\n\\nCONLL04 76.38 81.83 63.45\\nReTACRED 60.26 63.48 21.46\\nReTACRED10 64.88 64.72 34.61\\n\\nREBEL\\n\\nCONLL04 85.36 89.86 71.46\\nReTACRED 60.09 65.15 25.15\\nReTACRED10 64.91 69.80 39.68\\n\\nSpeechRE\\n\\npipe\\n\\nTP-Linker\\n\\nCONLL04 32.41 77.54 8.70\\nReTACRED 28.60 51.43 6.77\\nReTACRED10 38.19 61.85 13.79\\n\\nSpert\\n\\nCONLL04 28.95 75.44 10.47\\nReTACRED 33.20 58.36 7.10\\nReTACRED10 55.23 57.42 27.43\\n\\nREBEL\\n\\nCONLL04 35.78 82.86 12.53\\nReTACRED 30.21 53.20 6.93\\nReTACRED10 51.08 67.46 28.06\\n\\nSpeechRE\\n\\ne2e\\n\\nCONLL04 24.89 59.57 12.50\\nReTACRED 27.70 52.10 6.59\\nReTACRED10 29.87 51.32 14.79\\n\\nTable 2: Main results.\\n\\nUpper rows: TextRE models for which inputs are transcripts.\\nMiddle rows: SpeechRE pipe where inputs are ASR outputs.\\nBottom row: SpeechRE e2e where inputs are speech.\\n\\nAll experiments are conducted with fairseq. All our models are evaluated on the best performing checkpoint on the validation set. All experiments are conducted in a V100 GPU. Full training details can be found in Appendix A.1.\\n\\n4.5 Results of TextRE and SpeechRE\\n\\nWe first compare and contrast among the text relation extraction method and two speech extraction methods, to understand the performance gap. We train various models, including three TextRE models, the pipeline version of them and the e2e model, over CoNLL04, ReTACRED and ReTACRED10. Results are summarized in Table 2.\\n\\nDespite of the good performance of REBEL with transcripts being inputs (up to 71.46 on CoNLL04), its performance drops hugely when the input becomes ASR outputs, which are erroneous. Notably, the performance gap on entity prediction is huge. These highlight the challenge with the pipeline approach. SpeechRE e2e does not outperform SpeechRE pipe across the three datasets. All SpeechRE methods have achieved low accuracy of entity recognition. Particularly, the gap between TextRE and SpeechRE on entity detection is far larger than the gap on relation classification. It suggests that speech entity recognition may be the core bottleneck of the performance degradation of triplet extraction.\\n\\n4.6 SpeechRE in Low-resource Scenarios\\n\\nTo evaluate and compare the performance of our SpeechRE models in resource-constrained conditions, we simulate different training conditions by sampling 20%, 40%, 60%, 80% (and 100%) from the ReTACRED10 training set. We use REBEL as the TextRE model, since both REBEL and our SpeechRE models are generative models. For evaluation, we randomly sample 20 instances for each relation (10 relations) from the ReTACRED10 test set, totally 200 instances (the same subset corresponding to our human-read test set, described in \u00a73.1). We present F1 scores on entity prediction in the left plot of Figure 3, and F1 scores on relation prediction in the right plot, in both of which the lines left to the red dashed vertical line refer to the setting discussed in this subsection. To measure the extent of error propagation in SpeechRE pipe, we also evaluate its performance when the TextRE modules are trained on noisy ASR output as their input (instead of ground-truth transcripts). We summarize our observations from different perspectives below.\\n\\nTraining with transcripts v.s. ASR outputs. To investigate the impact of the quality of the text input to REBEL, we compare the performance of REBEL and SpeechRE pipe models, referring to TextRE (Transcript, TTS) and SpeechRE e2e (Single speaker, TTS) in Figure 3, whose inputs are transcripts and ASR outputs, respectively. Overall, the SpeechRE pipe model, compared to REBEL, produces comparable, yet slightly lower results on relation prediction, whereas performing significantly worse on entity prediction.\"}"}
{"id": "emnlp-2022-main-738", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On the one hand, this indicates the reliability of transcribed texts on relation words. On the other hand, the significant ASR errors on entity words are propagated to the downstream extraction module, greatly degrading its performance.\\n\\nComparison of SpeechRE pipeline and end-to-end. Comparing the two approaches, SpeechRE \\\\textsuperscript{e2e} performs worse than SpeechRE \\\\textsuperscript{pipe}, referring to SpeechRE \\\\textsuperscript{pipe} (ASR, TTS) and SpeechRE \\\\textsuperscript{e2e} (Single speaker, TTS) in Figure 3. However, with the increase in training data, its performance starts to catch up with SpeechRE \\\\textsuperscript{pipe}. This is expected, because not only does SpeechRE \\\\textsuperscript{pipe} have a bigger model size, its two components also excel in their own tasks by leveraging abundant ASR and TextRE data. In comparison, SpeechRE \\\\textsuperscript{e2e} has a smaller model size and is trained with a much smaller training set. Despite the large gap, the rising trend is promising, indicating the potential of SpeechRE \\\\textsuperscript{e2e} reaching parity with, and even surpassing SpeechRE \\\\textsuperscript{pipe}.\\n\\nTraining SpeechRE \\\\textsuperscript{e2e} with multi-speaker vs. single-speaker. We examine the impacts of single-speaker and multiple-speaker data. In most cases, when a model is trained with multi-speaker data, it has better performance on relational classification than the one trained on single-speaker data. Their performance on entity recognition is roughly the same.\\n\\nEvaluation on synthetic and human-read data. When comparing the performance of our models on the synthetic test set and the human-read test set, it is surprising to observe that most of the time, both SpeechRE \\\\textsuperscript{pipe} and SpeechRE \\\\textsuperscript{e2e} models have higher accuracy on relation prediction on the human-read data than on the synthetic one. This demonstrates the effectiveness of the use of synthetic speech. Please see Appendix A.2 for full results.\\n\\n4.7 SpeechRE with Data Augmentation. Based on the trend observed previously, we expect the SpeechRE \\\\textsuperscript{e2e} model to improve with more training data. We leverage the two data augmentation methods introduced in \u00a73.3, namely, up-sampling and pseudo-labelling. For each method, we build larger training corpora by adding augmented SpeechRE data to ReTACRED10, with sizes 100\\\\%, 200\\\\%, 250\\\\%, 300\\\\% and 350\\\\% that of ReTACRED10. This gives us 10 new training sets. The evaluation protocol is identical to the one in \u00a73.3.\\n\\nResults of these data augmentation can be found in Figure 3, to the right of the vertical dashed line in each subfigure. We outline our findings below.\\n\\nTraining with transcripts vs. ASR Outputs. With more training data, REBEL trained on ground-truth transcripts plus augmented pairs, \\\\langle transcript, pseudo_triplet \\\\rangle, has roughly the same accuracy on relation prediction in all conditions. We can observe a slightly decreasing trend on entity prediction. The performance of SpeechRE \\\\textsuperscript{pipe} has a moderately rising trend before leveling out.\\n\\nPipeline vs. end-to-end SpeechRE. Both data augmentation techniques bring significant improvements to SpeechRE \\\\textsuperscript{e2e} with pseudo-labelling being superior. Pseudo-labelling reaches the same performance both on entity and relation predictions as TextRE on synthetic speech at 350\\\\%. The results are surprising, especially with entity generation, considering the difficulty of the task in the speech domain in general. In contrast, augmented data do not help much with SpeechRE \\\\textsuperscript{pipe} due to the error propagation issue discussed above. Please see Appendix A.3 and A.4 for full results.\\n\\n4.8 Case Study. We perform a qualitative error analysis of SpeechRE through a case study. Table 3 shows typical errors in this task.\\n\\nError accumulation in the pipeline method. The two rows \u201cTTS ASR\u201d and \u201cHuman ASR\u201d illustrate that it is challenging for the SOTA ASR model to spell entity names correctly, especially the names of people and institutions. Being a deep learning model, it may tend to generate high-frequency words (Razeghi et al., 2022). This presents both a great challenge and opportunity for entity-sensitive tasks such as relation extraction, since low-frequency entities often contain more information and are more likely to be useful knowledge.\\n\\nHallucination in the end-to-end method. As shown in the two rows \u201cSpeechRE \\\\textsuperscript{e2e}\u201d, the \\\\textsuperscript{e2e} model may generate entities and relation types that are not present in speech, creating hallucinations. TextRE, in contrast, can restrict generated words via controllable text generation techniques. This is less surprising: being a cross-modal task, it is difficult for a SpeechRE model to effectively restrict the generated content, especially when the size of training data is limited. We also detail the impact of data augmentation on the accuracy of entity prediction by entity type. Appendix A.4 contains further results.\"}"}
{"id": "emnlp-2022-main-738", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: F1 scores of entity (left) and relation (right) predictions on 200 synthetic and human-read instances in various training resource conditions.\\n\\nMODEL (Train, Test): MODEL (i.e. SpeechRE and TextRE) is trained on Train and tested on Test.\\n\\nText\\nWhen bin Laden fled the US invasion in 2001, he took refuge with Haqqani in a safe house between the Afghan city of Khost and Miran Shah, according to Pakistani author Ahmed Rashid.\\n\\nTTS ASR\\nWhen bin-laden fled the U-S invasion in 2001, he took refuge with Hakone in a safe house between the Afghan City of Cost and Muran Shaw, according to Pakistani author Akmed Rashid.\\n\\nHuman ASR\\nWhen bin Laden fled the U-S invasion in 2001, he took refuge with Hakwani in a safe house between the Afghan City of Cost and Mirishah, according to Pakistani author Ahmed Rashid.\\n\\nTTS Audio/Synthetic Audio\\nSpeechRE <triplet>Bernama<subj>U.S.<obj>organization country of branch\\n\\nHuman Audio\\nSpeechRE <triplet>Mohamed ElBaradei<subj>Sultan<obj>person title\\n\\nTable 3: A qualitative error analysis for both the pipeline and end-to-end approaches. Models are trained with 100% ReTACRED10 data.\\n\\n5 Discussion\\nBased on our analysis, we discuss the following question:\\n\\nFor a new SpeechRE task, should we choose a pipeline or an end-to-end approach?\\n\\nWhile raw performance is largely attributed to data resources, to answer this question, other factors need to be taken into account in addition to the availability of data resources. These include compute power and latency.\\n\\nPipeline method is suitable in the low-resource scenarios. As shown in Figure 3, prior to 100%, SpeechRE pipe requires less training data to train than SpeechRE e2e, while exhibiting reasonably good performance. Therefore, the general ASR method based on pre-training provides a reasonable performance lower bound for low-resource speech extraction. The major concern is errors contained in entities, as shown in \u00a74.8. As a future direction, we conjecture that this issue could be potentially alleviated by the mixed extraction method from both transcript and speech. Yet, the pipeline approach may be limited by fundamental issues (e.g. error propagation and high latency) that cannot be solved easily.\\n\\nThe end-to-end method is preferred when labelled training data size is sufficient or external data is accessible. According to Figure 3, with the increasing volumes in training set, the performance of SpeechRE e2e on extracting correct entities and relations steadily improves. Extracting meaning from speech directly avoids the risk of information loss and error propagation, unlike in the pipeline setting. Because of this, the e2e approach can potentially solve the extraction task in a principled manner. The data scarcity issue that it faces can be eased through data augmentation, for its effectiveness on both machine-generated speech and realistic speech. Exploring more sophisticated augmentation and filtering techniques is thus a fruitful future direction. Further, it is of importance to improve data efficiency and enhance entity prediction performance. Particularly, enforcing constraints on entities in the decoding process and the inclusion of memory banks are promising directions.\\n\\n6 Conclusion\\nWe propose a new spoken language understanding task, Speech Relation Extraction (SpeechRE), and present two synthetic datasets and a human-read test set. We approach SpeechRE with two methods, the pipeline and e2e approaches. Through extensive experiments, quantitative and qualitative analyses,\"}"}
{"id": "emnlp-2022-main-738", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we identify data scarcity and spoken entity recognition as two main challenges for this task. We then present two augmentation techniques that are effective in addressing these challenges. Lastly, being the first working on the task, we outline key directions for future research.\\n\\n7 Limitations\\n\\nThis paper discusses the utterance-level speech relation extraction task where the average length of audio inputs is less than 15s. Constrained by computing resources, processing long audio signals is challenging, a known issue in the speech domain. For this reason, while speech relations can be useful in other scenarios such as summarization of dialogues, news and meetings, we were not in the position to carry out our study in these scenes. Another limitation is that we did not fully utilize the information contained in speech signals (e.g. speaker style and emotion), which could be beneficial to the task. Addressing these two limitations is part of our plan for future research.\\n\\nReferences\\n\\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33:12449\u201312460.\\n\\nPere-Llu\u00eds Huguet Cabot and Roberto Navigli. 2021. Rebel: Relation extraction by end-to-end language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2370\u20132381.\\n\\nGiuseppe Castellucci, Valentina Bellomaria, Andrea Favalli, and Raniero Romagnoli. 2019. Multi-lingual intent detection and slot filling in a joint bert-based model. arXiv preprint arXiv:1907.02884.\\n\\nXiang Chen, Ningyu Zhang, Lei Li, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022a. Good visual guidance makes a better extractor: Hierarchical visual prefix for multimodal entity and relation extraction. CoRR, abs/2205.03521.\\n\\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2022b. Knowprompt: Knowledge-aware prompt-tuning with synergistic optimization for relation extraction. In Proceedings of WWW, pages 2778\u20132788.\\n\\nMarkus Eberts and Adrian Ulges. 2020. Span-based joint entity and relation extraction with transformer pre-training. In Proceedings of ECAI, pages 2006\u20132013.\\n\\nGerard I G\u00e1llego, Ioannis Tsiamas, Carlos Escolano, Jos\u00e9 AR Fonollosa, and Marta R Costa-juss\u00e0. 2021. End-to-end speech translation with pre-trained models and adapters: Upc at iwslt 2021. In Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021), pages 110\u2013119.\\n\\nXu Han, Tianyu Gao, Yankai Lin, Hao Peng, Yaoliang Yang, Chaojun Xiao, Zhiyuan Liu, Peng Li, Jie Zhou, and Maosong Sun. 2020. More data, more relations, more context and more openness: A review and outlook for relation extraction. In Proceedings of AACL, pages 745\u2013758.\\n\\nXuanli He, Islam Nassar, Jamie Ryan Kiros, Gholamreza Haffari, and Mohammad Norouzi. 2021. Generate, annotate, and learn: Generative models advance self-training and knowledge distillation.\\n\\nJaehyeon Kim, Jungil Kong, and Juhee Son. 2021. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech. In International Conference on Machine Learning, pages 5530\u20135540. PMLR.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\\n\\nSiddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak, Junaid Qadir, and Bj\u00f6rn W Schuller. 2020. Deep representation learning in speech processing: Challenges, recent advances, and future trends. arXiv preprint arXiv:2001.00378.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880.\\n\\nXian Li, Changhan Wang, Yun Tang, Chau Tran, Yuqing Tang, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli. 2021. Multilingual speech translation from efficient finetuning of pretrained models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 827\u2013838.\\n\\nAbdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob D Havtorn, Joakim Edin, Christian Igel, Katrin Kirchhoff, Shang-Wen Li, Karen Livescu, Lars Maal\u00f8e, et al. 2022. Self-supervised speech representation learning: A review. arXiv preprint arXiv:2205.10643.\\n\\nZara Nasar, Syed Waqar Jaffry, and Muhammad Kamran Malik. 2021. Named entity recognition and relation extraction: State-of-the-art. ACM Computing Surveys (CSUR), 54(1):1\u201339.\"}"}
{"id": "emnlp-2022-main-738", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-738", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Training Details\\n\\nA.1.1 Implementation Details\\n\\nFor all the experiments, we train our REBEL for 30 epochs with Adam optimizer (0.9, 0.999) of a linear scheduler with a warmup rate of 0.1, a learning rate of 5e-5, a weight decay of 0.01, and a gradient clip value of 10. For other settings, our REBEL is consistent with the original ones.\\n\\nA.1.2 More details about pseudo-labelling\\n\\nThe Common Voice Corpus 9.0 dataset consists of 2,224 validated hours in English and 81,085 voices. Each entry in the dataset consists of a unique MP3 and corresponding text file. Many of the recorded hours in the dataset also include demographic metadata like age, sex, and accent that can help train speech recognition engines. Here we use the script from speechbrain to help us process text files in the dataset, but we made two changes to the processing script. Firstly, this processing script will make all text uppercase, which we do not do, but retain the original case of the text. Secondly, we add full stops to all sentences, whereas the original text has no full stops. We have made these two changes to make the processed text more realistic and to harmonise it with other datasets (e.g. CoNLL04, ReTACRED, etc.). We fine-tune REBEL (using rebel-large as the pretrained model) on the RetACRED10 dataset and conduct pseudo-labelling on the processed text to extract sentences and corresponding audio that contains target relations. A total of 922k instances were extracted from the Common Voice Corpus 9.0 dataset, of which 380k clean instances were retained after filtering.\\n\\nA.2 Low-Resource Analysis\\n\\nWe report the exact values of low resource analysis in Table 4, which corresponds to the left half of each sub-figure of Figure 3.\\n\\nA.3 Data Augmentation Analysis\\n\\nWe report the exact values of low resource analysis in Table 4, which corresponds to the right half of each sub-figure of Figure 3.\\n\\nA.4 Entity Analysis\\n\\nTo further understand why the pseudo labelling can perform better than the multi-speaker up-sampling, we conduct the following analysis experiments. Firstly, we selected high-frequency entities with frequency greater than three from the test set of RetACRED10. Moreover, we count the frequency of these entities in the training set constructed by two augmentation manners, i.e., pseudo labelling and multi-speaker up-sampling (as shown in Table 6). Then, we counted the classification accuracy of these entities in the test set (in Table 7). Comparing the two tables by location, we can observe that the method of pseudo labelling can effectively improve the recognition accuracy of the model for unseen entities.\"}"}
{"id": "emnlp-2022-main-738", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: The frequency in the training set of some entities which is demonstrated because their frequency in the test set is greater than 3.\\n\\nTable 7: The prediction accuracy of some entities which is demonstrated because their frequency in the test set is greater than 3.\"}"}
