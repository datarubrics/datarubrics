{"id": "lrec-2024-main-1021", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MUCH: A Multimodal Corpus Construction for Conversational Humor Recognition Based on Chinese Sitcom\\n\\nHongyu Guo 1, Wenbo Shang 2, Xueyao Zhang 1, Shubo Zhang 1, Xu Han 3, Binyang Li 1\\n\\n1 University of International Relations, Beijing, China; 2 Hong Kong Baptist University, Hong Kong, China; 3 Capital Normal University, Beijing, China\\n\\nAbstract\\n\\nConversational humor is the key to capturing dialogue semantics and dialogue comprehension, which is usually generated in multiple modalities, such as linguistic rhetoric (textual modality), exaggerated facial expressions or movements (visual modality), and quirky intonation (acoustic modality). However, existing multimodal corpora for conversation humor are coarse-grained, and the modality is insufficient to support the conversational humor recognition task. This paper designed an annotation scheme for multimodal humor datasets, and constructed a corpus based on a Chinese sitcom for conversational humor recognition, named MUCH. The MUCH corpus consists of 34,804 utterances in total, and 7,079 of them are humorous. We employed both unimodal and multimodal methods to test our MUCH corpus. Experimental results showed that the multimodal approach could achieve 75.94% in terms of F1-score and surpassed the performance of most unimodal methods, which demonstrated that the MUCH corpus was effective for multimodal humor recognition tasks.\\n\\nKeywords: Conversational Humor, Annotation Scheme, Chinese Sitcom\\n\\n1. Introduction\\n\\nHumor, as the nature of experiences to induce laughter and provide amusement (Warren et al., 2018), plays an important role in machine translation (Hutchins, 1995), reading comprehension (Mckee, 2012), and sentiment analysis (Taboada, 2016), etc. There are two main forms of humorous expression (Attardo et al., 2013): one-liners and conversational humor. One-liners are concise, non-narrative sentences in a joke, while conversational humor are linguistic narrative sentences that express humor in dialogues. Different from one-liners, conversational humor is generated based on the context of the dialogue and expressed more flexibly in conversations. Therefore, conversational humor recognition is significant for capturing the humorous semantics and dialogue comprehension.\\n\\nIn the real world, a conversation between individuals is usually adopted in a face-to-face way. Therefore, in addition to text, other modalities will be used to generate humor, such as funny facial expressions and quirky intonations. In Figure 1, actors frequently employ humorous language and quirky tones, together with exaggerated expressions and actions to evoke humor. Similar situations widely occur in the real world. Therefore, more information in different modalities should be taken into consideration to recognize conversational humor.\\n\\nHowever, most of the existing research mainly used textual modality to recognize conversational humor (Taylor, 2004; Yang et al., 2015), which was difficult to identify conversational humor from unimodality. In conversations, to enhance humorous expressions, people may combine the acoustic rhythm (quirky intonation, etc.) or the visual features (comical expressions or movements, etc.) interacting with the textual contents. Some research has attempted to employ the visual modality as a supplement to the textual modality (Purandare and Litman, 2006). More recent studies have built several datasets that accounted for other modal information rather than textual information alone. For instance, Bertero and Fung (2016a) built a humor dataset that employed canned laughter as an indicator to denote the humor scenes; Boccignone et al. (2017) proposed a multimodal dataset to detect humor from images. However, these datasets...\"}"}
{"id": "lrec-2024-main-1021", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"did not cover all modalities. As a result, they are coarse-grained in capturing the differentiation of multimodalities and perform suboptimal in conversational humor recognition.\\n\\nTo this end, this paper aims to construct a multi-modality corpus for conversational humor recognition, named MUCH, which was constructed based on a Chinese sitcom, iPartment. In order to better represent conversational humor, MUCH covers three modalities, including text, vision, and acoustics.\\n\\nThe main contributions of this paper are listed below:\\n\\n\u2022 An annotation scheme for conversational humor recognition datasets is designed, involving three modalities (text, vision, and acoustics). Among them, the visual modality includes exaggerated facial expressions and movements, the acoustic modality involves abnormal intonation and homophones that can indicate humor. According to the scheme, a multi-modal conversational humor corpus (MUCH) was constructed by manually annotating based on the Chinese sitcom iPartment.\\n\\n\u2022 The MUCH corpus consists of 34,804 utterances in total, and 7,079 of them are humorous. Among the humorous utterances, 5,163 utterances evoke humor by unimodal expressions (i.e., textual (T), visual (V), and acoustic (A) modalities), and 1,916 required two or more modalities (i.e., T+V, T+A, V+A, T+V+A) to generate humor.\\n\\n\u2022 To assess the MUCH corpus, we conducted several experiments to compare some classical methods, including both unimodal and multimodal. The experimental results showed that the multimodal method outperformed unimodal methods. As to unimodal methods, the textual method RoBERTa performed best, achieving the accuracy of 69.17% and the F1-score of 67.73%. As to the multimodal method, CLIP performed best, reaching the accuracy of 82.96% and the F1-score of 75.94%. This proved the suitability of the corpus for multimodal conversational humor recognition.\\n\\n2. Related Work\\n\\nHumor, an essential element in interpersonal communication, serves as a vital medium for expressing emotions in humans (Meyer, 2010), can be classified into two categories according to whether it is narrative, i.e., one-liners and conversational humor (Attardo et al., 2013).\\n\\nNon-narrative one-liners are characterized by simple syntax and creative semantic structures. Mihalcea and Strapparava (2005) conducted humor analysis on one-liners, and Yang et al. (2015) utilized hand-crafted and non-neural models to recognize the underlying semantic structures of humor in one-liners.\\n\\nSimilar to one-liners recognition, research on conversational humor firstly focused on the textual modality for recognition. Zhang and Liu (2014) collected humor dataset from Twitter, and Chen and Lee (2017) collected and annotated TED speech transcripts.\\n\\nIn recent work, multimodal datasets were constructed to better understand semantics and employed to recognize conversational humor. Chandrasekaran et al. (2016) analyzed humorous expressions in abstract scenes, and Boccignone et al. (2017) proposed a multimodal dataset for detecting humor in images.\\n\\nMore recently, to better capture the humorous expressions in real conversations, some research on multimodal conversational humor datasets has been built based on sitcoms, including Friends (Poria et al., 2018), The Big Bang Theory (Patro et al., 2021), Seinfeld (Bertero and Fung, 2016b) etc.\\n\\nHowever, most of the above datasets used acoustic modality, i.e., canned laughter, as an indicator for annotation, and other modalities were not fully exploited. Therefore, in order to better recognize conversational humor, this paper constructed a multimodal humor corpus annotation scheme and manually annotated a fine-grained conversational humor corpus (MUCH) based on a Chinese sitcom.\\n\\n3. Corpus Construction\\n\\nAccording to the analysis of the multimodal conversational humor recognition task, the corpus for the task should satisfy the following requests (Hasan et al., 2019): 1) More than one speaker should be involved to form a conversation; 2) Different topics or scenes should be involved that can showcase various humor styles.\\n\\nFor this purpose, we choose the classic Chinese sitcom iPartment as the basic dataset, which has 7 main characters, and each episode revolving around different plot topics being densely packed with laughter.\\n\\nBased on the above sitcom dataset, we firstly divide each episode of iPartment into several dialogues, while each dialogue contains a series of sequential utterances. For each individual utterance, it will be annotated as humorous or non-humorous. More specifically, each humorous utterance is further annotated to indicate the presence of peculiar intonation, comical actions, or facial expressions.\\n\\n1 The full MUCH corpus is publicly available for use at MUCH_Corpus.\"}"}
{"id": "lrec-2024-main-1021", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additionally, each utterance is associated with other attributes, such as Speaker and Sentence. The detailed description and analysis of the MUCH will be shown in the following subsections.\\n\\n3.1. Annotation Scheme\\n\\nWithout the loss of generality, we divide each episode of the sitcom into several dialogues based on different scenes and plots. For each dialogue, there are \\\\( n \\\\) utterances, \\\\( i \\\\) (\\\\( i = 1, 2, ..., n \\\\)).\\n\\nFigure 2 demonstrates our proposed annotation scheme for the conversational humor dataset, which has the following attributes:\\n\\n- **Speaker**: Speaker plays a particular role in the generation of humor, and different speakers have different styles of humor.\\n- **Text**: Text represents the textual content of the utterance that indicates the specific manifestation of humor in the textual modality. This attribute is labeled as 1 when textual humor occurs and labeled as 0 otherwise.\\n- **Abnormal Intonation**: Abnormal intonation can enhance the speaker's emotional expression. Moreover, due to language differences between Chinese and English, Chinese individuals often employ techniques such as rhyming and regional accents when expressing humor. In this case, humor cannot be recognized through the textual and visual modalities but can be through the acoustic modality. This attribute is labeled as 1 when an abnormal intonation occurs and labeled as 0 otherwise.\\n- **Antic**: Antic refers to the characters exhibit comical expressions or gestures during the conversation. When comical expressions or gestures occur, Antic is labeled as 1; otherwise, it is labeled as 0. It allows for humor recognition in the visual modality through the annotation of comical actions and expressions.\\n\\nIn our annotation scheme, three modalities are all taken into consideration. Each utterance is annotated to indicate each modality. For example, if the textual modality is judged humorous, its corresponding label is annotated as 1, otherwise, the label is annotated as 0. Similarly, when humor is recognized in acoustic modality and visual modality, their label is annotated as 1. Moreover, if the label of the acoustic or visual modality is 1, we also recorded the time_stamp and the video clip of the utterance.\\n\\n3.2. Annotation Process\\n\\nUnlike other NLP annotation tasks, the recognition of conversational humor could vary from one person to another. Therefore, we found 12 annotators to annotate the MUCH dataset, all of whom have experience in data annotation. The annotators were divided into four groups for annotation. The annotation processing has the following steps:\\n\\n1. Divide each episode into several dialogues based on different plots and scenes. \\n2. Record all the utterances in each dialogue and record the speaker and the content of each utterance. \\n3. Judge whether each utterance embodies humor in textual, visual, and acoustic modalities following the proposed scheme. At the same time, when humor is expressed visually or acoustically, the time_stamp of the corresponding utterance is also provided.\\n\\nToward a specific instance, the processing of annotation can be classified into three cases:\\n\\n1. If 3 annotators achieved the agreement, e.g., they all regarded the instance as humorous or non-humorous, the instance was labeled as 1 or 0, and the annotation was completed; \\n2. If 2 annotators labeled the instance as humorous, the instance was labeled as 1 according to the majority rule; \\n3. If only 1 annotator considered the instance to be humorous, considering the contingency of humor, it required the other 9 annotators to annotate the instance and determine the final label by applying the majority rule.\\n\\nDuring our annotation process, we also discovered that sitcoms often use canned laughter to induce laughter in the audience. For the utterance with canned laughter, which our annotators did not think was humorous in any modality, we also marked it and retained the time_stamp and other information for further fine-grained research of the corpus in the future.\\n\\nFigure 3 provides an example of our annotation. Based on Wei Zhang's performance, the annotators firstly determine whether the overall utterance is humorous (labeled as 1) or non-humorous (labeled as 0). Then, the annotators judge whether it is humorous in each modality. In the textual modality, taking utterance 1 as an example, \u201cI don't know how to use those men's tricks,\u201d conflicts with speaker's masculine gender and plays a taunting role, so it is annotated as humorous in the textual modality (labeled as 1). At the same time, Wei Zhang uses...\"}"}
{"id": "lrec-2024-main-1021", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: An annotation example.\\n\\n| # Dialogue | 1,626 |\\n|-----------|------|\\n| # Utterance | 34,804 |\\n| # Speaker | 423 |\\n| # Humorous utterance | 7,079 |\\n| Total duration in hour | 62 |\\n| Avg. duration of dialogue (minutes) | 2.76 |\\n| Avg. duration of utterance (seconds) | 2.89 |\\n| # Humor in unimodal |  |  |  |\\n| T  | 3,661 |\\n| V  | 647 |\\n| A  | 855 |\\n| # Humor in multimodal |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |"}
{"id": "lrec-2024-main-1021", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The MUCH corpus is used as a benchmark to evaluate the performance of multimodal conversational humor recognition. The corpus consists of 34,804 utterances, out of which 7,079 are humorous. Several experiments were conducted using both unimodal and multimodal methods. The results showed that the multimodal approach outperformed most unimodal methods.\\n\\n### Table 2: Comparison between MUCH and other multimodal humor datasets\\n\\n| Method   | Language | Modality       | Annotation Process                                                                 |\\n|----------|----------|----------------|-------------------------------------------------------------------------------------|\\n| UR-Funny (Hasan et al., 2019) | English | T, V, A | Provide videos and their transcripts from the TED portal. |\\n| MuStARD (Castro et al., 2019)   | English | T, V, A | Provide the utterance and the corresponding original fragment, while also proving contextual information. |\\n| TBBT (Kayatani et al., 2021)    | English | T, V | Only the overall label. |\\n| MHD (Patro et al., 2021)        | English | T, V, A | Annotation based on canned laughter. |\\n| M2H2 (Chauhan et al., 2021)     | Hindi  | T, V, A | Provide the utterance and the corresponding original fragment; Only the overall label. |\\n| MUCH (Ours)                     | Chinese | T, V, A | Provide an overall label and labels for each of the three modalities for each utterance. |\\n\\n### Table 3: Performance of conversational humor recognition via unimodal and multimodal approaches on the MUCH corpus\\n\\n| Modality | Method | Acc. (%) | P (%) | R (%) | F1 (%) |\\n|----------|--------|----------|-------|-------|--------|\\n| Unimodal | Text   | BERT     | 65.18 | 60.72 | 61.44  |\\n|          |        | RoBERTa  | 69.17 |       |        |\\n| Vision   | ViT    |          | 65.72 | 65.17 | 60.25  |\\n|          | OMNIVORE |        | 60.13 | 60.37 | 59.12  |\\n| Acoustic | openSMILLE |      | 56.41 | 55.93 | 61.01  |\\n| Multimodal | CLIP   |          | 82.96 | 74.86 | 77.05  |\\n\\n### 4.2. Evaluation Metrics\\n\\nThe MUCH corpus was divided into training, development, and testing sets with a ratio of 65%, 15%, and 20%, respectively. Following Liang et al. (2022), accuracy (Acc.), precision (P), recall (R), and F1-score (F1) were used as evaluation metrics.\\n\\n### 4.3. Experimental Result\\n\\nThe performance of recognizing conversational humor using both unimodal and multimodal methods is shown in Table 3.\\n\\nFor the unimodality, the BERT and RoBERTa achieved the accuracy of 65.18% and 69.17%, and the F1-score of 61.08% and 67.73%, respectively. The visual modality employed the ViT and OMNIVORE, achieved the accuracy of 65.72% and 60.13% and the F1-score of 62.61% and 59.47%, respectively. In the acoustic modality, the openSMILLE was employed and attained the accuracy of 56.41% and the F1-score of 58.36%.\\n\\nFor the multimodality, the CLIP performed well, achieved the accuracy of 82.96% and the F1-score of 75.94%.\\n\\n### 5. Conclusion\\n\\nIn this paper, a new multimodal conversational humor annotation scheme was proposed and manually annotated the MUCH corpus. The MUCH corpus was constructed based on a Chinese sitcom and includes three modalities: text, vision, and acoustics. It consists of 34,804 utterances in total, and 7,079 of them are humorous. Several experiments were conducted using both unimodal and multimodal methods. The results showed that the multimodal approach surpassed the performance of most unimodal methods.\"}"}
{"id": "lrec-2024-main-1021", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Ethical Consideration\\n\\nWe constructed the MUCH corpus based on the Chinese sitcom, *iPartment*, which has been licensed for academic research, and the annotation for the MUCH corpus was done by human experts, who are regular employees of our research group. The MUCH corpus is freely available and will be used only for the purpose of academic research. There are no other issues to declare.\\n\\n8. References\\n\\nSalvatore Attardo, Lucy Pickering, Fofo Lomotey, and Shigehito Menjo. 2013. Multimodality in conversational humor. *Review of Cognitive Linguistics*. Published under the auspices of the Spanish Cognitive Linguistics Association, 11(2):402\u2013416.\\n\\nDario Bertero and Pascale Fung. 2016a. Deep learning of audio and language features for humor prediction. In *Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916)*, pages 496\u2013501.\\n\\nDario Bertero and Pascale Fung. 2016b. Predicting humor response in dialogues from tv sitcoms. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5780\u20135784. IEEE.\\n\\nGiuseppe Boccignone, Donatello Conte, Vittorio Cuculo, and Raffaella Lanzarotti. 2017. Amhuse: a multimodal dataset for humour sensing. In *Proceedings of the 19th ACM international conference on multimodal interaction*, pages 438\u2013445.\\n\\nSantiago Castro, Devamanyu Hazarika, Ver\u00f3nica P\u00e9rez-Rosas, Roger Zimmermann, and Soujanya Poria. 2019. Towards multimodal sarcasm detection (an obviously perfect paper).\\n\\nArjun Chandrasekaran, Ashwin K Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2016. We are humorbeings: Understanding and predicting visual humor. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 4603\u20134612.\\n\\nDushyant Singh Chauhan, Gopendra Vikram Singh, Navonil Majumder, Amir Zadeh, Asif Ekbal, Pushpak Bhattacharyya, Louis-philippe Morency, and Soujanya Poria. 2021. M2h2: A multimodal multi-party hindi dataset for humor recognition in conversations. In *Proceedings of the 2021 International Conference on Multimodal Interaction*, pages 773\u2013777.\\n\\nLei Chen and Chong Min Lee. 2017. Convolutional neural network for humor recognition. *arXiv preprint arXiv:1702.02584*.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.\\n\\nFlorian Eyben, Martin W\u00f6llmer, and Bj\u00f6rn Schuller. 2010. Opensmile: the munich versatile and fast open-source audio feature extractor. In *Proceedings of the 18th ACM international conference on Multimedia*, pages 1459\u20131462.\\n\\nBobak Farzin, Piotr Czapla, and Jeremy Howard. 2019. Applying a pre-trained language model to spanish twitter humor prediction. *arXiv preprint arXiv:1907.03187*.\\n\\nRohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der Maaten, Armand Joulin, and Ishan Misra. 2022. Omnivore: A single model for many visual modalities. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 16102\u201316112.\\n\\nMd Kamrul Hasan, Wasifur Rahman, Amir Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis Philippe Morency, Mohammed, and Hoque. 2019. Ur-funny: A multimodal language dataset for understanding humor.\\n\\nW. John Hutchins. 1995. Machine translation: A brief history. In E.F.K. KOERNER and R.E. ASHER, editors, *Concise History of the Language Sciences*, pages 431\u2013445. Pergamon, Amsterdam.\\n\\nYuta Kayatani, Zekun Yang, Mayu Otani, Noa Garcia, Chenhui Chu, Yuta Nakashima, and Haruo Takeamura. 2021. The laughing machine: Predicting humor in video. In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pages 2073\u20132082.\\n\\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In *Proceedings of naacL -HLT*, volume 1, page 2.\\n\\nBin Liang, Chenwei Lou, Xiang Li, Min Yang, Lin Gui, Yulan He, Wenjie Pei, and Ruifeng Xu. 2022. Multi-modal sarcasm detection via cross-modal graph convolutional network. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, volume 1, pages 1767\u20131777. Association for Computational Linguistics.\"}"}
{"id": "lrec-2024-main-1021", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- \\nddar Joshi, Danqi Chen, Omer Levy, Mike Lewis, \\nLuke Zettlemoyer, and Veselin Stoyanov. 2019. \\nRoberta: A robustly optimized bert pretraining ap-\\nproach. arXiv preprint arXiv:1907.11692.\\n\\nRod A Martin. 2007. The psychology of humor: an \\nintegrative approach. Academic Pr Inc.\\n\\nSteve Mckee. 2012. Reading comprehension, what \\nwe know: A review of research 1995 to 2011. \\nLanguage Testing in Asia, 2(1):45.\\n\\nJohn C Meyer. 2010. Humor as a double-edged \\nsword: Four functions of humor in communication. \\nCommunication Theory, 10(3):310\u2013331.\\n\\nRada Mihalcea and Carlo Strapparava. 2005. Mak-\\ning computers laugh: Investigations in automatic \\nhumor recognition. In Proceedings of Human Lan-\\nguage Technology Conference and Conference on \\nEmpirical Methods in Natural Language Process-\\ning, pages 531\u2013538.\\n\\nBadri N Patro, Mayank Lunayach, Deepankar Srivas-\\ntava, Hunar Singh, Vinay P Namboodiri, et al. 2021. \\nMultimodal humor dataset: Predicting laughter \\ntracks for sitcoms. In Proceedings of the IEEE/CVF \\nWinter Conference on Applications of Computer Vi-\\nsion, pages 576\u2013585.\\n\\nSoujanya Poria, Devamanyu Hazarika, Navonil Ma-\\njumder, Gautam Naik, Erik Cambria, and Rada \\nMihalcea. 2018. Meld: A multimodal multi-party \\ndataset for emotion recognition in conversations. \\narXiv preprint arXiv:1810.02508.\\n\\nAmruta Purandare and Diane Litman. 2006. Hu-\\nmor: Prosody analysis and automatic recognition \\nfor f*r*i*e*n*d*s*. In Empirical Methods in Natural \\nLanguage Processing.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya \\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish \\nSastry, Amanda Askell, Pamela Mishkin, Jack \\nClark, et al. 2021. Learning transferable visual \\nmodels from natural language supervision. In \\nInternational conference on machine learning, pages \\n8748\u20138763. PMLR.\\n\\nMaite Taboada. 2016. Sentiment analysis: An \\noverview from linguistics. Annual Review of Lin-\\nguistics, 2(1):325\u2013347.\\n\\nJulia M Taylor. 2004. Computationally recogniz-\\ning wordplay in jokes. Proceedings of Cogsci, \\n53(1):1315\u20131320.\\n\\nCaleb Warren, Adam Barsky, and A Peter McGraw. \\n2018. Humor, comedy, and consumer behavior. \\nJournal of Consumer Research, 45(3):529\u2013552.\\n\\nDiyi Yang, Alon Lavie, Chris Dyer, and Eduard Hovy. \\n2015. Humor recognition and humor anchor ex-\\ntraction. In Proceedings of the 2015 conference on \\nempirical methods in natural language processing, \\npages 2367\u20132376.\\n\\nZekun Yang, Yuta Nakashima, and Haruo Takemura. \\n2023. Multi-modal humor segment prediction in \\nvideo. Multimedia Systems, pages 1\u201310.\\n\\nRenxian Zhang and Naishi Liu. 2014. Recognizing \\nhumor on twitter. In Proceedings of the 23rd ACM \\ninternational conference on conference on informa-\\ntion and knowledge management, pages 889\u2013898.\"}"}
