{"id": "acl-2024-long-442", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I would like you to act as a hallucination annotator in an answer. I will provide a reference document and a question about {name} and you will judge whether the answer point contains hallucinations.\\n\\nThe specific requirements are as follows:\\n1. If the point is supported by and consistent with the reference document, please write <Hallucination> None. And write the specific reference segment: <Reference> XXX. If there are multiple reference segments, please use \\\"<SEP>\\\" to separate them. Reference segments should be copied directly from the original text without modification.\\n2. If the point contradicts the reference document, please write: <Hallucination> Contradictory. And write the specific reference segment: <Reference> XXX. Also, write how to modify the answer: <Correction> \\\"XXX\\\" to \\\"YYYY\\\". If you need to delete XXX, write: <Correction> \\\"XXX\\\" to \\\"\\\"\\n3. If the point cannot be verified and there is no evidence in reference to support it, please write: <Hallucination> Unverifiable. And write the specific reference segment: <Reference> XXX. Also, write how to modify the answer: <Correction> \\\"XXX\\\" to \\\"YYYY\\\". If you need to delete XXX, write: <Correction> \\\"XXX\\\" to \\\"\\\"\\n4. If the point does not contain any factual information to be judged, please write: <No Fact>\\n\\nReference: {reference document}\\n\u95ee\u9898: {question}\\n\u56de\u7b54\u8981\u70b9: {answer sentence}\\n\u8bf7\u6807\u6ce8:\\nFigure A7: Prompts for Fine-grained Hallucination Annotation.\"}"}
{"id": "acl-2024-long-442", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A8: Prompts for Fine-grained Hallucination Annotation.\"}"}
{"id": "acl-2024-long-442", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Imagine you are a detective who specializes in identifying hallucinations. I will provide you with reference documents and questions about \\\"name\\\" and you will need to evaluate each point of information in the responses for the presence of hallucinations. Please follow the steps below:\\n\\n- If the information point does not contain a fact that can be judged, mark: <No Fact> and end the annotation.\\n- If the information point contains a fact, list the corresponding reference: <Reference> XXX. If there is more than one, separate them with \\\"<SEP>\\\". Please ensure that the reference information is copied directly from the original text and does not need to be altered.\\n- If the information point is consistent with the reference, please mark: <Hallucination> None.\\n- If the information point contradicts the reference, please mark it as <Hallucination> Contradictory and include a correction: <Correction> \\\"XXX\\\" to \\\"YYYY\\\". When something needs to be eliminated, write: <Correction> \\\"XXX\\\" to \\\"\\\"\\n- If the information point cannot find relevant evidence, or cannot be verified, please mark: <Hallucination> Unverifiable, and include a correction: <Correction> \\\"XXX\\\" to \\\"YYYY\\\". When you need to eliminate something, please write: <Correction> \\\"XXX\\\" to \\\"\\\".\\n\\nQuestion: {question}\\nReference: {reference document}\\nPlease annotate the information point: {answer sentence}\\n\\nFigure A9: Prompts for Fine-grained Hallucination Annotation.\"}"}
{"id": "acl-2024-long-442", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are now a hallucination detection system. I will provide you with a reference document and a question on the topic \\\"name\\\". Your task is to analyze the responses to the question and determine whether or not there is a hallucination for each point. The steps of the assessment are as follows:\\n\\n- If it does not contain factual information that needs to be judged, write: <No Fact> and stop the assessment.\\n- If facts are included, identify the relevant reference clip. Write: <Reference> XXX. Separate multiple references with \\\"<SEP>\\\". Please copy the reference fragment directly from the original without modification.\\n- If the points are identical to the reference, write: <Hallucination> None.\\n- If the main points are contradictory to the reference document, write: <Hallucination> Contradictory. Include a suggestion for revision: <Correction> \\\"XXX\\\" to \\\"YYY\\\". If a section needs to be deleted, write: <Correction> \\\"XXX\\\" to \\\"\\\".\\n- If no evidence can be found to support a point, or if it cannot be verified, write: <Hallucination> Unverifiable, with a suggested change: <Correction> \\\"XXX\\\" to \\\"YYYY\\\". If a section needs to be deleted, write: <Correction> \\\"XXX\\\" to \\\"\\\".\\n\\nQuestion: {question}\\n\\nReference: {reference document}\\n\\nPlease analyze the point:{answer sentence}\"}"}
{"id": "acl-2024-long-442", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The fear of crime refers to the fear of being a victim of crime as opposed to the actual probability of being a victim of crime. The fear of crime, along with fear of the streets and the fear of youth, is said to have been in Western culture for \u201ctime immemorial\u201d. While fear of crime can be differentiated into public feelings, thoughts and behaviors about the personal risk of criminal victimization, distinctions can also be made between the tendency to see situations as fearful, the actual experience while in those situations, and broader expressions about the cultural and social significance of crime and symbols of crime in people\u2019s neighborhoods and in their daily, symbolic lives...\\n\\n# Interpersonal communication and the mass media\\n\\nHearing about events and knowing others who have been victimised are thought to raise perceptions of the risk of victimisation. This has been described as a \u2018crime multiplier\u2019, or processes operating in the residential environment that would \u2018spread\u2019 the impacts of criminal events... A subject\u2019s criminal risk perception is exaggerated by peer-communication on crime and only moderated by the own experience. Public perceptions of the risk of crime are no doubt also strongly shaped by mass media coverage.\\n\\n**Question**\\n\\nWhat are the three different components of risk perception that may interact in relation to the likelihood of falling victim to crime?\\n\\n**Selected Ans Sent**\\n\\nThe three different components of risk perception that may interact in relation to the likelihood of falling victim to crime are:\\n\\n1. **Perceived Risk**: This refers to an individual\u2019s subjective assessment of the likelihood of being a victim of crime. It is influenced by factors such as personal experience, media coverage, and social norms.\\n\\n2. **Actual Risk**: This refers to the objective probability of being a victim of crime, based on statistical data and crime patterns.\\n\\n3. **Perceived Benefits and Costs**: This refers to an individual\u2019s assessment of the benefits and costs of taking precautions to reduce their risk of being a victim of crime. It is influenced by factors such as the perceived effectiveness of different precautions, the perceived costs of taking precautions, and the perceived benefits of not taking precautions.\"}"}
{"id": "acl-2024-long-442", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reference 0: Neo-Freudianism is a psychoanalytic approach derived from the influence of Sigmund Freud but extending his theories towards typically social or cultural aspects of psychoanalysis over the biological. Karen Horney theorized that to deal with basic anxiety, the individual has three options:\\n\\n- Moving toward others: Accept the situation and become dependent on others. This strategy may entail an exaggerated desire for approval or affection.\\n- Moving against others: Resist the situation and become aggressive. This strategy may involve an exaggerated need for power, exploitation of others, recognition, or achievement.\\n- Moving away from others: Withdraw from others and become isolated. This strategy may involve an exaggerated need for self-sufficiency, privacy, or independence.\\n\\nKaren Horney theorized that individuals have three options to deal with basic anxiety:\\n\\n1. **Moving towards people**: This option involves seeking out social connections and relationships in order to feel safe and secure. Individuals who choose this option may become overly dependent on others and struggle with feelings of inadequacy.\\n2. **Moving against people**: This option involves competing with others and striving for power and control in order to feel safe and secure. Individuals who choose this option may become overly aggressive and struggle with feelings of insecurity.\\n3. **Moving away from people**: This option involves withdrawing from social connections and relationships in order to feel safe and secure. Individuals who choose this option may become overly detached and struggle with feelings of loneliness and isolation.\\n\\n**Question**\\n\\nHow are hallucinations related to abnormal brain activity?\\n\\n**Selected Answer**\\n\\nHallucinations are a symptom of abnormal brain activity. Hallucinations can be caused by a variety of factors, including mental illness, substance abuse, and neurological disorders. When the brain is not functioning properly, it can produce hallucinations as a result of abnormal activity in the visual, auditory, or other sensory systems. For example, in schizophrenia, the brain's dopamine system is overactive, leading to hallucinations and delusions. Similarly, in Parkinson's disease, abnormal activity in the basal ganglia can cause visual hallucinations. Other conditions, such as epilepsy and migraines, can also cause hallucinations as a result of abnormal brain activity. Overall, hallucinations are a symptom of abnormal brain activity and can be caused by a variety of underlying conditions.\"}"}
{"id": "acl-2024-long-442", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table A4: Topic-specific automatic evaluation results for generative hallucination annotators ANAH, where 'R', 'BERT', and 'Pre4' refer to 'RougeL', 'BERTScore', and '4-gram Precision', respectively.\\n\\n| Setting | BERT | TQ |\\n|---------|------|----|\\n| G-7B    | 87.29| 87.27|\\n| G-20B   | 87.96| 88.93|\\n\\nTable A5: Evaluation results for generative annotators, noted by 'G'. 'T' represents the unseen-topic test set, while 'Q' represents the unseen-question test set.\\n\\n| Setting | BERT | TQ |\\n|---------|------|----|\\n| S.T.    | 87.29| 87.27|\\n| M.T.    | 85.94| 87.55|\\n| above + D. | 86.05 | 86.71 |\\n| M.T. + P.A. | 86.95 | 87.60 |\\n\\nTable A6: Ablation Study for Generative Annotator based on InternLM-7B in different settings. Here, 'S.T.' means single-task training, which only includes hallucination annotation task in training, while 'M.T.' adopts multi-task training, which further encompasses several generative tasks. \"+ D\" indicates that testing the annotations with prompt disturbance i.e., the instructions used in testing are unseen in training. \"P.A.\" indicates prompt augmentation is adopted in training.\\n\\nFigure A11: Hallucination Type Confusion Matrices for Generative Annotators. (a) InternLM2-7B-based annotator (b) InternLM2-20B-based annotator\\n\\nFigure A12: Hallucination Type Confusion Matrices for Discriminative Annotators based on InternLM2-7B. (a) without reference (b) with reference\\n\\nFigure A13: Hallucination Type Confusion Matrices for discriminative annotators based on InternLM2-20B. (a) without reference (b) with reference\"}"}
{"id": "acl-2024-long-442", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Victor Hugo gained wider fame in 1831 with his historical novel Notre-Dame de Paris (The Hunchback of Notre-Dame), an evocation of life in medieval Paris during the reign of Louis XI...\\n\\nVictor Hugo gained wider fame in 1831 when he published his novel Les Mis\u00e9rables. This novel was published in 1831 and was well received by critics and the public...\"}"}
{"id": "acl-2024-long-442", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ANAH: Analytical Annotation of Hallucinations in Large Language Models\\n\\nZiwei Ji1,2\u2217 Yuzhe Gu1* Wenwei Zhang1\u2020 Chengqi Lyu1 Dahua Lin1,3 Kai Chen1\u2020\\n\\n1 Shanghai AI Laboratory\\n2 Hong Kong University of Science and Technology\\n3 The Chinese University of Hong Kong\\n\\nzjiad@connect.ust.hk {guyuzhe,zhangwenwei,lvchengqi,chenkai}@pjlab.org.cn\\n\\nAbstract\\n\\nReducing the 'hallucination' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community. Thus, we present ANAH, a bilingual dataset that offers analytical annotation of hallucinations in LLMs within Generative Question Answering. Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of \u223c12k sentence-level annotations for \u223c4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions.\\n\\n1 Introduction\\n\\nLarge Language Models (LLMs) have achieved significant performance improvements across a diverse array of Natural Language Processing tasks (Petroni et al., 2021; Kamalloo et al., 2023; Sun et al., 2023; Chen et al., 2023, 2024). However, Large Language Models still face a worrisome problem that significantly hinders their real-world applications, hallucination, in which they produce plausible-sounding but unfaithful or nonsensical information (Ji et al., 2022; Bang et al., 2023) when answering the user questions, especially those require intensive knowledge. Given the fluency and convincing nature of the responses produced by LLMs, the detection of their hallucinations becomes increasingly difficult (Adlakha et al., 2023; Ren et al., 2023; Pezeshkpour, 2023). Such a challenge impedes the deep analysis and reduction of LLM hallucination and leads to extensive dissemination of misleading information as the user base widens and real-world applications proliferate (Mallen et al., 2023).\\n\\nThere have been extensive efforts on effectively...\"}"}
{"id": "acl-2024-long-442", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"detecting and evaluating hallucination (Durmus et al., 2020; M\u00fcndler et al., 2023; Du et al., 2023a). However, most benchmarks were proposed before the advent of LLM and targeted specific English tasks (Dziri et al., 2021; Rohrbach et al., 2018), which are not challenging for current models. Recent benchmarks (Li et al., 2023a, 2024) for LLMs only categorize whether the entire response contains hallucinations without explanation and reference. This coarse-grained nature makes it difficult to trace the exact trigger of hallucinations and obstructs further mitigation of them.\\n\\nTherefore, we establish a novel large-scale Chinese-English benchmark, named ANAH, that assesses the LLMs\u2019 ability to annotate the LLM hallucinations sentence-by-sentence, in the scenario of knowledge-based generative question answering. Rather than solely result-oriented, for each answer to a question, our approach prompts the model to annotate hallucination for each sentence, including retrieving reference fragment for the sentence, judging the hallucination type (No/Contradictory/Unverifiable Hallucinations, and No Fact), and correcting the sentence based on the reference fragment if hallucination exists (Fig. 1).\\n\\nTo facilitate the scale-up of datasets, we ensure the comprehensiveness and diversity of ANAH across various topics, questions, and answers. As shown in Fig. 2, first, we curate topics in both English and Chinese, encompassing a broad domain range including things, places, people, and historical events (Fig. 3). Second, we craft around three related questions for each topic to ensure originality and avoid contamination. Third, for each question, we construct a high-quality and a low-quality response with and without reference in generation, respectively, enabling a comparative analysis of hallucination distributions across different response scenarios. The final and pivotal stage is fine-grained hallucination annotation, as exemplified in Fig. 1. Eventually, we form \u223c12k hallucination annotations of \u223c4.3k answers to \u223c2.2k questions spanning a broad domain range, which is challenging for hallucination detection.\\n\\nThanks to the completeness and fine-granularity of ANAH, the statistical results of the hallucination annotations quantitatively confirm that hallucinations progressively accumulate in the LLM responses. Furthermore, ANAH can be used to train and evaluate hallucination annotators. We first discovered that only GPT-4 could do this task well. Thus, we further investigate training generative and discriminative hallucination annotators using ANAH and observe the advantages of generative annotators over discriminative annotators in handling the imbalance issue of hallucination types. Remarkably, our generative annotators achieve an accuracy of 81.01%, surpassing open-source models and rivaling GPT-4 (86.97%) in performance with a smaller size and lower source cost. We also observe that the hallucination annotators consistently exhibit better generalization regarding the number of questions than the breadth of topics, thereby guiding us toward prioritizing data scaling to cover a broader array of topics in future research.\\n\\n2.1 Topic Selection and Reference Retrieval\\n\\nThe initial stage involves the selection of topics and corresponding references from knowledge-intensive datasets. To ensure diversified and wide-ranging information, our topic choices are categorized into celebrities, events, locations, and things. We also encompass various domains, including but not limited to Politics and Military, Art, Science and Technology, Religion, etc. (Fig. 3). Topics are meticulously chosen based on the frequency of their occurrence via Google Ngram Viewer 3\\n\\n3 https://books.google.com/ngrams/\"}"}
{"id": "acl-2024-long-442", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The topic distribution by chart of (a) categories (inner) and domains (outer), and (b) word cloud.\\n\\nsince topics that more frequently occur and are of public interest should be more important for real-world applications of LLMs. We also collect topics from publicly available summaries like historical timelines and the ranking of influential persons.\\n\\nAfter selecting the topics, their corresponding reference documents are retrieved from pre-training databases (He et al., 2022), including Wikipedia, Baidu Baike, and Encyclopedia Britannica. We select the datasets that have been widely used in the pre-training stage of LLMs (Touvron et al., 2023) so that we can make sure that the model saw the truth, which is important for further analysis and mitigation of hallucinations.\\n\\nDuring the reference retrieval process, the discrepancies in nomenclature across different sources and the potential of a single name having multiple meanings present challenges. To address these challenges, we adopt a strategy that progresses from hard to soft matching. First, we perform exact matching (i.e., hard matching) of the entries. Then, we sort the candidate entries according to the sentence semantic similarity and further judge them with InternLM (Team, 2023) to select the correct ones. Finally, manual filtering is performed to iron out the problem of renaming. Overall, this phase establishes a robust foundation for the ensuing steps of benchmark construction.\\n\\n2.2 Question Generation and Selection\\n\\nThe second stage involves the generation and selection of several questions based on the provided reference documents about a particular topic. To increase the possibility that the data is unseen and untainted, we create new questions rather than repurposing existing datasets. The questions are framed in a manner so that they can be fully answered exclusively grounded on the provided reference documents, avoiding being overly subjective or open-ended. To ensure diversity and comprehensiveness across questions, they are designed to cover different types, such as 'what', 'when', 'where', 'why', etc, and perspectives such as descriptions, explanations, reasons, etc., encapsulating all facets of the information. The questions also traverse diverse levels of knowledge, ranging from basic, generic knowledge to more intricate, specialized knowledge or domain-specific expertise. The generation prompt is shown in Fig. A2.\\n\\nTo assure the uniqueness of each question and avoid duplication, we leverage CoSENT for Chinese and MiniLM for English, respectively, to calculate similarities among questions and sift out overly similar ones. We then employ GPT-3.5 (OpenAI, 2023) to assess their answerability, i.e., whether the given questions can be answered based solely on the provided reference documents. This ensures that the questions are fact-based, objective, and possess a definitive answer, thus increasing the reliability and consistency of the evaluation process. The prompt details are in Fig. A3.\\n\\nFinally, we utilize GPT-4 (OpenAI, 2023) to select the top three questions from the above candidate questions, considering the following characteristics:\\n\\n1. High authenticity: The questions should be free from any intentionally misleading, ambiguous, or false information.\\n2. High answerability: The questions exhibiting excessive subjectivity, controversy, or predictability.\"}"}
{"id": "acl-2024-long-442", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Omar Khayyam was a polymath, known for his contributions to mathematics, astronomy, philosophy, and Persian poetry. As a mathematician, he is most notable for his work on the classification and solution of cubic equations, where he provided geometric solutions by the intersection of conic.\\n\\n## Mathematics\\nKhayyam was famous during his life as a mathematician. His surviving mathematical works include:\\n- A commentary on the difficulties concerning the postulates of Euclid's Elements,\\n- On the division of a quadrant of a circle,\\n- And On proofs for problems concerning Algebra.\\n\\n### Real number concept\\nKhayyam discusses the relationship between the concept of ratio and the concept of number and explicitly raises various theoretical difficulties. In particular, he contributes to the theoretical study of the concept of irrational number.\\n\\nWhat were Omar Khayyam's notable contributions to mathematics?\\n\\nOmar Khayyam was a mathematician and poet who is best known for his contributions to algebra and geometry. He wrote a book called \u201cTreatise on the Circumference of a Circle\u201d which was a major work in the field of geometry. He is also known for his work on the theory of numbers and for his contributions to the development of the decimal system. I hope my reply is helpful.\"}"}
{"id": "acl-2024-long-442", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Number of topics, annotated answers, annotated sentences, and tokens (with and without reference documents) for each language of ANAH.\\n\\neach answer sentence that accurately encapsulates related information. We empirically choose BM25 (Robertson et al., 2009) for both language, and further apply two CoSENT models for Chinese, and MiniLM for English, to rank reference fragments. The ensemble of multiple embedding models significantly improves retrieval accuracy, which serves as a foundation for accurate hallucination-type classification and hallucination correction and reduces the cost of human annotators to correct the reference fragment. Furthermore, to optimize resource utilization of GPT-4 without compromising the annotation accuracy, we empirically determine the context length of reference fragments to be 540 tokens for Chinese and 400 tokens for English. For the remaining unverifiable sentences due to the failure of retrieval, we extend the window length by sixfold for secondary annotation and finally fix the remaining cases after secondary annotation by human annotation.\\n\\nBased on the document window for each answer sentence, GPT-4 is prompted to identify reference fragments and assess whether hallucinations exist. If the sentence contains factual information and aligns with the reference, its type is 'No Hallucination'. Annotators should also pinpoint the specific reference fragments from the original documents. If the sentence contradicts the reference, its type is 'Contradictory Hallucination'. The specific reference fragments and a suggestion on correcting the response are required. If the sentence lacks supporting evidence and cannot be verified, its type is 'Unverifiable Hallucination' and a revision suggestion is required. If the sentence does not contain any factual information for evaluation, it falls under the category of 'No Fact' without further annotation. See detailed GPT-4 prompts in Fig. A6. After preliminary annotation, human annotation is conducted following a similar workflow.\\n\\n| Hallucination Type | Ref Corr. |\\n|-------------------|-----------|\\n| None              | 90.19     |\\n| Cont.             | 83.70     |\\n| Unver.            | 75.69     |\\n| N.F.              | 28.67     |\\n| Ref.              | 85.37     |\\n| Corr.             | 78.98     |\\n\\nTable 3: Consistency between GPT-4 and human Annotations, where 'Cont.', 'Unver.', 'N.F.', 'Ref.', and 'Corr.' are abbreviations of Contradictory, Unverifiable, No Fact, Reference, and Correction, respectively.\\n\\n2.5 Dataset Statistics\\n\\nEventually, our dataset covers both English and Chinese and comprises over 700 topics, \u223c4.3k annotated answers, \u223c12k annotated sentences, and \u223c7M tokens with reference documents (Tab. 2). The topics also cover celebrities, events, locations, and things, from an array of domains, such as military/politics, health/medicine, and sports, as depicted in Fig. 3. The statistics underscore the comprehensiveness and extensive scale of our dataset.\\n\\nWe also verify the quality of GPT-4 generated annotations by analyzing their consistency with human annotations (the higher, the better). As shown in Tab. 3, the average consistency is 86.97% for hallucination type, 85.37% for reference, and 78.98% for correction. GPT-4 tends to erroneously annotate sentences as 'No Fact' when sentences contain referential ambiguity or summary discussion, while the type of 'No Fact' only accounts for \u223c2% of annotated sentences. We provide inconsistent examples in \u00a7B.\\n\\nTab. 4 presents the proportions of hallucination type for answers generated by GPT-3.5 with reference and InternLM without reference. The hallucination proportions for answers generated with reference are much higher than those without. Such an observation which is consistent with recent research interests in retrieval augmented generation (RAG) (Lewis et al., 2020).\\n\\nAccumulation Effect\\n\\nThanks to the fine granularity of ANAH, we can quantitatively analyze the accumulation or snowball effect of hallucinations (Zhang et al., 2023). The probability of hallucinations occurring in the current sentence when the previous sentences contain hallucinations, $P(H_t | H_{[0:t-1]})$, is defined as\\n\\n$$P(H_t | H_{[0:t-1]}) = \\\\frac{P(H_t, H_{[0:t-1]})}{P(H_{[0:t-1]})}$$\\n\\n(1)\\n\\n$H_t$ is a Boolean indicator that returns true if the current sentence is hallucinated. The hallucination probability is 58.51% for English and 52.54% for Chinese, while the hallucination probability when\\n\\n$\\\\ldots$\"}"}
{"id": "acl-2024-long-442", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Proportion of each annotation type for answers generated with and without reference in English and Chinese.\\n\\nThe previous sentences don't contain, $P(H_t | H_{[0:t-1]})$, is 14.61% for English and 17.2% for Chinese. $P(H_t | H_{[0:t-1]})$ is significantly higher than $P(H_t | \\\\sim H_{[0:t-1]})$ indicates that the probability of hallucinations increases when the previous sentences contain hallucinations compared to when there are not, which quantitatively confirms the accumulation effect of hallucinations.\\n\\n3 Hallucination Annotator\\n\\nTaking advantage of the rich fine-grained annotations in ANAH, we explore training and evaluating both generative and discriminative annotators. The generative annotator generates textual annotations including reference fragments, hallucination type, and correction; while the discriminative annotator only focuses on discriminating hallucination type.\\n\\n3.1 Generative Annotator\\n\\nWe adopt the same pipeline and prompts as the preliminary annotation of GPT-4 for the generative annotator. We first comprehensively analyze the current open-source and close-source LLMs' ability to generate fine-grained hallucination annotation using ANAH. Specifically, consistency with humans is assessed through the examination of an array of multilingual LLMs including Llama2 (Touvron et al., 2023), InternLM2, Qwen (Bai et al., 2023), Baichuan2 (Baichuan, 2023) in different sizes, GPT-3.5, and GPT-4.\\n\\nIn addition, we explore training hallucination annotators using InternLM on our dataset. The fine-grained annotation involves constructing multiple sentence annotations from each answer. When constructing the training data, each sentence from an answer forms a sample.\\n\\nData Augmentation\\n\\nWe perform a multi-task setting where besides fine-grained hallucination annotation, we incorporate other tasks including question generation, question selection, answer generation from intermediate products of ANAH, and dialogue generation from ShareGPT (None, 2023) and Dolly (Conover et al., 2023). In addition, we apply prompt augmentation by the design of multiple prompts with varying instruction descriptions, relative locations of reference and question, etc. Please refer to \u00a7 A.4 for details.\\n\\n3.2 Discriminative Annotator\\n\\nRecent works (Wu et al., 2023; Lightman et al., 2023; Uesato et al., 2022) explore process-supervised reword models to provide fine-grained signals in RLHF, which are also useful in hallucination mitigation process such as RLHF (Wu et al., 2023). Thus, we also explore training a sentence-level process-supervised discriminative annotator using InternLM, based on ANAH, which has the potential to be applied for fine-grained RLHF.\\n\\nFollowing the sentence-level information including references and hallucination type of ANAH, the model is trained to categorize each sentence into one of four types: No/Contradictory/Unverifiable Hallucination, and No Fact. To enable process supervision and reuse the learned knowledge in LLMs, we replace the last layer of the pre-trained LLM with a four-category linear layer and load the remaining parameters of pre-trained LLMs for further training the annotators. This approach ensures that the scoring results are compatible with reward models in various aspects, including relevance and completeness (Wu et al., 2023). Additionally, the inference time of the discriminative annotator is significantly shorter than that of its generative counterparts.\\n\\n4 Experiments\\n\\n4.1 Implementation\\n\\nData Split\\n\\nANAH is divided into training and testing sets. To investigate the direction of annotator generalization and dataset scaling, we further divide the testing set equally into unseen-topic and unseen-question groups. In the unseen-topic test set, the topics and corresponding references, questions, and answers remain unexposed during training. In the unseen-question test set, the topics have been exposed during training, while the questions remain unexposed.\\n\\nFurther details regarding the experimental implementation can be found in \u00a7 C.1 for generative annotator and \u00a7 C.2 for discriminative annotator.\\n\\n4.2 Evaluation Protocols\\n\\nFor the hallucination type predicted by generative and discriminative annotators, we utilize F1 and\"}"}
{"id": "acl-2024-long-442", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Automatic evaluation results for generative hallucination annotators based on different models, where 'R', 'BERT', and 'Pre4' refer to 'RougeL', 'BERTScore', and '4-gram Precision', respectively.\\n\\n| Model       | F1    | ACC   | RougeL | Pre4   |\\n|-------------|-------|-------|--------|--------|\\n| GPT-3.5     | 48.01 | 47.94 | 29.4   | 78.78  |\\n| GPT-4       | 87.11 | 86.97 | 86.32  | 96.21  |\\n| Qwen-7B     | 8.46  | 4.67  | 24.28  | 77.28  |\\n| Baichuan2-7B| 9.63  | 5.50  | 4.21   | 10.65  |\\n| LLama2-7B   | 13.76 | 8.31  | 4.37   | 19.93  |\\n| InternLM2-7B| 12.44 | 12.34 | 9.54   | 64.19  |\\n| Qwen-14B    | 14.94 | 8.82  | 10.53  | 55.2   |\\n| Baichuan2-13B | 42.17 | 38.04 | 23.39  | 75.27  |\\n| LLama2-13B  | 8.55  | 4.80  | 5.15   | 20.16  |\\n| InternLM2-20B | 61.49 | 63.17 | 46.36  | 84.68  |\\n| Qwen-72B    | 58.27 | 55.69 | 35.96  | 79.21  |\\n| Llama2-70B  | 18.42 | 12.53 | 7.13   | 20.95  |\\n| ANAH-7B     | 78.69 | 79.92 | 58.51  | 87.27  |\\n| ANAH-20B    | 80.49 | 81.01 | 58.82  | 88.44  |\\n\\nTable 6: Evaluation results for generative and discriminative annotators, noted by 'G' and 'D', respectively. 'T' represents the unseen-topic test set, while 'Q' represents the unseen-question test set.\\n\\nDue to the space limit, we put BERTScore in Tab A5.\\n\\nFigure 4: Hallucination Type Confusion Matrices for InternLM2-20B-based generative annotator (a) and discriminative annotator (b).\\n\\nRelatively lower than that of the generative annotator. Thus, we analyze the confusion matrices of hallucination type for both annotators. Fig. 4 shows the discriminative annotator is more prone to misjudge into the largest category (No Hallucination), with the 2nd to 4th row of the 1st column totaling 255, exceeding 147 for generative annotator, given the data imbalance issue depicted in Tab. 4. This suggests the current discriminative annotators are more affected by the imbalance issue of hallucination types and require further modification for improvements, which we leave for future research.\\n\\nRefer to \u00a7 D for all confusion matrices.\\n\\nGeneralization Analysis\\nTab. 6 also indicates both generative and discriminative annotators perform better on the unseen-question test set than the unseen-topic test set in the hallucination-type classification task. This suggests leveraging prior knowledge learned from the same topic in training aids in handling exposed references in testing. This implies extending the breadth of topics has higher priority than extending questions of the same topic when scaling the data sizes of hallucination annotation in the future. In addition, we assess the generalization of ANAH annotator to other LLMs (e.g. Qwen-7B, Baichuan2-7B) in Appendix D.1.\\n\\n4.4 Ablation Study\\nData Augmentation\\nAs shown in the first two rows of Tab. 7, results are superior in the mix-task setting (introduced in \u00a7 3.1) compared to the single-task setting. Due to the space limit, we put BERTScore in Tab A6.\"}"}
{"id": "acl-2024-long-442", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Ablation Study for Generative Annotator based on InternLM-7B in different settings. Here, 'S.T.' means single-task training, which only includes hallucination annotation task in training, while 'M.T.' adopts multi-task training, which further encompasses several generative tasks. \"+ D\" indicates that testing the annotations with prompt disturbance i.e., the instructions used in testing are unseen in training. \\\"P.A.\\\" indicates prompt augmentation is adopted in training.\\n\\n| Model  | F1 w/ Ref | ACC w/ Ref | F1 w/o Ref | ACC w/o Ref |\\n|--------|-----------|------------|------------|-------------|\\n| G-7B   | 75.93     | 77.24      | 77.89      | 78.12       |\\n| G-20B  | 79.82     | 81.18      | 80.21      | 81.81       |\\n| D-7B   | 66.20     | 68.53      | 69.15      | 70.86       |\\n| D-20B  | 69.74     | 73.98      | 72.10      | 75.95       |\\n\\nTable 8: Evaluation results for generative and discriminative annotators. Here, \\\"w/ Ref\\\" means providing reference documents when annotating, while \\\"w/o Ref\\\" means without reference documents.\\n\\nIn addition, to evaluate the robustness of generative annotators, we introduce disturbance by altering the test instruction descriptions, ensuring they differ from the training instructions. We compare the results obtained without and with prompt augmentation without and with disturbance in the last four rows of Tab. 7. The model trained with prompt argumentation declines due to perturbations, less than that with augmentation (0.39% vs. 6.37% in accuracy). It reveals models trained on diverse prompt formats increase robustness compared to their single prompt format-trained counterparts.\\n\\nWe further examine the effectiveness of reference documents to the performance of the generative and discriminative annotators when judging the hallucination type. We test the annotators by compelling the model to rely solely on its parametric internal knowledge without any references. Tab. 8 reveals that only relying on its parametric knowledge decreases the prediction F1 and accuracy, indicating the importance of reference in annotating hallucinations.\\n\\n5 Related Work\\n\\nHallucination Benchmarks can be broadly divided into two categories. One type of benchmark mainly constructs challenging queries in one/multiple tasks and then evaluates the hallucination level in the responses (Lin et al., 2022; Dziri et al., 2022a,b, 2021; Rohrbach et al., 2018; Li et al., 2024). There are also domain-specific benchmarks curated recently, such as sports (Elaraby et al., 2023) and medical (Umapathi et al., 2023) domains. Besides these English benchmarks, a Chinese benchmark, HalluQA (Cheng et al., 2023), designs 450 adversarial questions spanning multiple domains. While these benchmarks lean toward arising hallucinations, ANAH aims to provide an analytical framework for hallucination annotation.\\n\\nAnother type of benchmarks can be used to train a hallucination detector/annotator and evaluate the hallucination level via the detector/annotator (Liu et al., 2021; Dziri et al., 2022a; Gupta et al., 2022; Laban et al., 2022; Durmus et al., 2020; Wang et al., 2020; Li et al., 2023a; Varshney et al., 2023; Yang et al., 2023; Muhlgay et al., 2023). All these works classify the whole response of LLMs as either hallucinatory or not. Such a coarse-grained nature makes it difficult to conduct more detailed statistical analysis. On the contrary, ANAH annotates hallucination for each sentence to different hallucination types with correction based on the retrieved reference documents. Furthermore, ANAH collects natural responses from LLMs instead of artificially guiding LLMs to produce hallucinatory responses (Li et al., 2023a; Muhlgay et al., 2023).\\n\\nHallucination Mitigation\\n\\nIn the training stage, various techniques such as multi-task learning (Weng et al., 2020; Garg et al., 2019), model editing (Daheim et al., 2023; Ji et al., 2023a), and fine-grained RLHF (Wu et al., 2023) are proposed to mitigate hallucination. For inference time mitigation, different decoding strategies (Rebuffel et al., 2022; Chuang et al., 2023; Shi et al., 2023; Li et al., 2023b) are attempted. There are also multi-agent methods (Du et al., 2023b) and variants of the Chain-of-Thought approach involving verification or reflection (Dhuliawala et al., 2023; Lei et al., 2023; Ji et al., 2023b; Wang et al., 2023) proposed for LLMs. The hallucination annotators trained on ANAH have the potential to be integrated into the training and inference pipeline by offering fine-grained hallucination information for further mitigation.\"}"}
{"id": "acl-2024-long-442", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conclusion and Future Work\\n\\nHallucinations in generative tasks present substantial obstacles to the reliability and creditability of LLMs but lack a comprehensive and fine-grained detecting strategy. Thus, we present a bilingual dataset, ANAH for fine-grained hallucination annotation in GQA covering diverse topics, offering the opportunity to quantitatively analyze hallucination phenomena such as accumulation effect, and facilitating the development of state-of-the-art fine-grained hallucination annotators. Our generative hallucination annotators surpass all open-source LLMs and GPT-3.5 and obtain performance on par with GPT-4. Our generalization experiments indicate that improving the breadth of topics in the dataset is more important than extending questions under existing topics in the dataset.\\n\\nThis paper paves the way for further scaling up the dataset of ANAH to conduct a systematic evaluation and analysis of LLM hallucinations, with the trained hallucination annotators. The hallucination annotators also have the potential to be used in the hallucination mitigation pipeline in both the training and inference stages.\\n\\nLimitations\\n\\nThis benchmark primarily incorporates the widely recognized and representative knowledge-intensive task, GQA. However, it does not encompass other tasks such as summarization and dialogue. During the dataset construction, we use GPT-3.5 with a reference document to construct a high-quality answer and an early version of InternLM-7B without reference to generate low-quality answers, respectively. Different models are used in that stage, we will further complete and analyze the other settings including GPT-3.5 without reference and InternLM-7B with reference.\\n\\nIn addition, our focus predominantly lies on the answer generation stage, without considering other stages such as the model's ability to recognize adversarial questions (Kumar et al., 2023; Zhu et al., 2023), red teaming (Ganguli et al., 2022), acknowledge unknown knowledge (Yin et al., 2023; Rajpurkar et al., 2018; Amayuelas et al., 2023), and retrieve accurate external knowledge once they realize their parametrical knowledge is not enough.\\n\\nEthical Considerations\\n\\nWe used publicly available reference documents for our benchmarks, effectively circumventing any possible harm toward individuals or groups. The generated data by LLMs were carefully selected and processed by humans to secure privacy and confidentiality. No personal identification information was involved, and all data were made anonymous before any analysis was conducted.\\n\\nReferences\\n\\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. 2023. Evaluating correctness and faithfulness of instruction-following models for question answering. CoRR, abs/2307.16877.\\n\\nAlfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Yang Wang. 2023. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. CoRR, abs/2305.13712.\\n\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\\n\\nBaichuan. 2023. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305.\\n\\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.\\n\\nZehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, et al. 2023. T-eval: Evaluating the tool utilization capability step by step. arXiv preprint arXiv:2312.14033.\\n\\nZehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881.\\n\\nQinyuan Cheng, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, et al. 2023. Evaluating hallucinations in chinese large language models. arXiv preprint arXiv:2310.03368.\"}"}
{"id": "acl-2024-long-442", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883.\\n\\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the world's first truly open instruction-tuned llm.\\n\\nNico Daheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and Edoardo M Ponti. 2023. Elastic weight removal for faithful and abstractive dialogue generation. arXiv preprint arXiv:2303.17574.\\n\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jackson Weston. 2023. Chain-of-verification reduces hallucination in large language models. arXiv preprint arXiv:2309.11495.\\n\\nLi Du, Yequan Wang, Xingrun Xing, Yiqun Ya, Xiafang Li, Xin Jiang, and Xuezhi Fang. 2023a. Quantifying and attributing the hallucination of large language models via association analysis. CoRR, abs/2309.05217.\\n\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023b. Improving factuality and reasoning in language models through multiagent debate.\\n\\nEsin Durmus, He He, and Mona Diab. 2020. Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055\u20135070.\\n\\nNouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M Ponti, and Siva Reddy. 2022a. Faithdial: A faithful benchmark for information-seeking dialogue. Transactions of the Association for Computational Linguistics, 10:1473\u20131490.\\n\\nNouha Dziri, Sivan Milton, Mo Yu, Osmar R Zaiane, and Siva Reddy. 2022b. On the origin of hallucinations in conversational models: Is it the datasets or the models? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5271\u20135285.\\n\\nNouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2021. Evaluating groundedness in dialogue systems: The begin benchmark. Findings of ACL.\\n\\nMohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, and Shizhu Liu. 2023. Halo: Estimation and reduction of hallucinations in open-source weak large language models. arXiv preprint arXiv:2308.11764.\\n\\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. CoRR, abs/2209.07858.\\n\\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. 2019. Jointly learning to align and translate with transformer models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4453\u20134462.\\n\\nPrakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. Dialfact: A benchmark for fact-checking in dialogue. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3785\u20133801.\\n\\nConghui He, Wei Li, Zhenjianng Jin, Bin Wang, Chao Xu, and Lin. 2022. Opendatalab: Empowering general artificial intelligence with open datasets. https://opendatalab.com.\\n\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2022. Survey of hallucination in natural language generation. ACM Computing Surveys.\\n\\nZiwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan Wilie, Min Zeng, and Pascale Fung. 2023a. RHO: Reducing hallucination in open-domain dialogues with knowledge grounding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4504\u20134522, Toronto, Canada. Association for Computational Linguistics.\\n\\nZiwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023b. Towards mitigating hallucination in large language models via self-reflection. EMNLP Findings.\\n\\nEhsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, and Davood Rafiei. 2023. Evaluating open-domain question answering in the era of large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 5591\u20135606. Association for Computational Linguistics.\\n\\nAounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju. 2023. Certifying...\"}"}
{"id": "acl-2024-long-442", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-442", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Dataset Construction\\n\\nA.1 Topic Selection and Reference Retrieval\\n\\nWe use InternLM to assess whether a query and its candidate entries are synonymous via the prompt in Figure A1.\\n\\nIf the sentence similarity between two questions exceeds the threshold, we consider them overly similar. The threshold is 300 for Chinese (via CoSENT) and 0.9 for English (via MiniLM), which are selected by case study.\\n\\nA.2 Question Generation and Selection\\n\\nFirst, we generate multiple questions based on the reference documents via prompts in Figure A2. We use GPT-3.5 to filter the open-ended subjective questions and make sure of their answerability via the prompts in Figure A3.\\n\\nWe use GPT-4 to select the final questions based on authenticity, answerability, difficulty, and variety via prompts in Figure A4.\\n\\nA.3 Answering under Different Models and Scenarios\\n\\nWe generate answers with the document via prompts in Figure A5.\\n\\nA.4 Fine-grained Hallucination Annotation\\n\\nWe utilize GPT-4 to generate fine-grained hallucination annotation via prompts in Figure A6 to A10.\"}"}
{"id": "acl-2024-long-442", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I will provide two entries along with introductions. Please determine if the two entries are synonymous, i.e., if the two entries refer to the same event, object, person, or location, etc.\\n\\nEntry 1: {name1}\\nIntroduction 1: {doc1}\\n\\nEntry 2: {name2}\\nIntroduction 2: {doc2}\\n\\nAre the two entries synonymous?\\n\\nFigure A1: Prompts for Reference Retrieval.\\n\\nB Case Study\\nTable A1, A2, and A3 show the examples where the GPT-4 generated annotation is inconsistent with human annotation.\\n\\nC Implementation Details\\nC.1 Generative Annotator\\nThe maximum sequence length is set to 16k. This setting is also held constant in baselines. We load the pre-trained InternLM2-7B model and train it with the following settings and hyper-parameters: the epoch is 1, the batch size is 2, the learning rate is 4e-5, and the AdamW optimizer is with a linear scheduler. We generate responses using sampling implemented via the LMDeploy library.\\n\\nOur model is trained on 8 NVIDIA A800 GPUs. It takes approximately 1 hour to train.\\n\\nC.2 Discriminative Annotator\\nWe use InternLM2-7B and 20B as the base model for training. We train the discriminative annotator on our benchmark with the following settings and hyper-parameters: the epoch is 2, the batch size is 8, the learning rate is 1e-5, the AdamW optimizer is with a linear scheduler, and the maximum sequence length is 16k. Our model is trained on 8 NVIDIA A800 GPUs.\\n\\nD Results and Analysis\\nTopic-specific automatic evaluation results for generative hallucination annotators are shown in Tab. A4. The trained ANAH-7B performs best on location topics while struggling with event topics. Figure A11 shows the confusion matrices of hallucination type for annotators in different sizes. Figure A12 and A13 show the confusion matrices for discriminative annotators under different scenarios in different sizes.\\n\\nD.1 Generalization on other LLMs\\nTo assess generalizability, we sample 100 sentence-level annotations for answers generated by other models (Qwen-7B, Baichuan2-7B). We manually check the quality of ANAH-7B annotator as in Table A7. The accuracy for other models is similar to that of GPT3.5 and InternLM. It proves that our annotator is still relatively stable on other models. We find that for the same query, generated answers from different models are around the topic and they are not far apart. Thus, in the context of factual QA, the divergence is not substantial and the answers are relatively in domain. Please find some examples in Tab. A8.\\n\\nE Human Annotation\\nThe annotation platform is developed internally by the laboratory. Human annotators, comprising well-educated undergraduates. Their salary is 300 yuan per day which is adequate given the participants\u2019 demographic. An ethics review board approved the data collection protocol.\\n\\nHuman annotation involves two stages: (1) screening topics and references; and (2) fine-grained hallucination annotation. We provide comprehensive instructions for each task, including task descriptions, precautions, estimated time, three examples, and three negative cases, to facilitate understanding.\\n\\nWe also employ a double annotation process during human annotation: (1) Annotators fix the GPT4 pre-annotations. (2) Experienced annotators (selected by platform) review the annotations and give feedback. Multiple rounds of (1) and (2) are performed until the platform deems the annotation...\"}"}
{"id": "acl-2024-long-442", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I would like you to act as a question generator. I will provide references and you will generate 10 questions about \\\"{topic}\\\" based on the reference. The specific requirements are as follows:\\n\\n1. the questions can be fully answered based only on the reference document, i.e. the answers to the questions are fully contained in the reference document. The questions should be objective and not too subjective or open-ended.\\n\\n2. the 10 questions should be of as many different types as possible, e.g. what, when, where, why. Questions can be asked from different perspectives, e.g. descriptions, explanations, reasons, etc. Ensure that the questions are of different types and cover all aspects of the information.\\n\\n3. 10 questions can cover different levels of knowledge, from general, basic knowledge to more specialized, complex subject knowledge or domain knowledge.\\n\\n4. have only one question per item.\\n\\nReference: {reference document}\\n\\nPlease list the 10 questions directly based on the above reference without any explanation.\"}"}
{"id": "acl-2024-long-442", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I would like you to act as a question judge. Given several questions, determine if each question meets all of the following conditions: objective, about facts, has a definitive answer, and not open-ended.\\n\\n{questions}\\n\\nPlease answer \\\"yes\\\" or \\\"no\\\" in label order, separated by line breaks and without any explanation.\\n\\nI would like you to act as a question answerability judge. I will provide a question and reference document, and you will judge whether the question is fully answerable based only on the reference document, i.e., whether the answer is included in the reference.\\n\\nReference document: {reference document}\\n\\nQuestion: {question}\\n\\nIs it possible to answer the question at all, based only on the reference document? Please answer \\\"yes\\\" or \\\"no\\\" directly without any explanation.\\n\\nGood questions have the following characteristics: 1. high degree of truthfulness: the question contains no intentionally misleading, ambiguous or false information. 2. high answerability: remove questions that are too subjective, controversial, or predictive. 3. have a certain level of difficulty for the model. 4. increase the overall diversity (in terms of type, complexity, depth of knowledge, etc.), and remove questions that are similar to other questions. Combine the above evaluation metrics and select the 3 best problems among these. Please respond directly to the question numbers, separated by commas, without any explanation.\\n\\nFigure A3: Prompts for Question Answerability Judge.\\n\\nFigure A4: Prompts for Question Selection.\"}"}
{"id": "acl-2024-long-442", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I would like you to act as a hallucination annotator in an answer. I will provide a reference document and a question about \\\"{name}\\\" and you will judge whether the answer point contains hallucinations. The specific requirements are as follows:\\n\\n1. If the point is supported by and consistent with the reference document, please write `<Hallucination> None. And write the specific reference segment: <Reference> XXX. If there are multiple reference segments, please use `<SEP>` to separate them. Reference segments should be copied directly from the original text without modification.\\n\\n2. If the point contradicts the reference document, please write: `<Hallucination> Contradictory. And write the specific reference segment: <Reference> XXX. Also, write how to modify the answer: `<Correction> \\\"XXX\\\" to \\\"YYY\\\". If you need to delete XXX, write: `<Correction> \\\"XXX\\\" to \\\".\\\"\\n\\n3. If the point cannot be verified and there is no evidence in reference to support it, please write: `<Hallucination> Unverifiable. And write the specific reference segment: <Reference> XXX. Also, write how to modify the answer: `<Correction> \\\"XXX\\\" to \\\"YYY\\\". If you need to delete XXX, write: `<Correction> \\\"XXX\\\" to \\\"\\\".\\n\\n4. If the point does not contain any factual information to be judged, please write: `<No Fact>.\\n\\nQuestion: {question}\\n\\nReference: {reference document}\\n\\nPoint: {answer sentence}\\n\\nPlease annotate:\"}"}
