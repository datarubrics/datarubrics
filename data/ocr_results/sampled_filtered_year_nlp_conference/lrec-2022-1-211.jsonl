{"id": "lrec-2022-1-211", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we introduce BEA (Hungarian Speech Databases) \u2013 a benchmark dataset comprising speech data of 140 speakers captured during the MALACH (Mihajlik et al., 2007) project. Unfortunately, only spontaneous (read) Hungarian databases are publicly available, however, none are directly comparable to the WSJ database (Garafolo et al. 1993) or the BUSZI dataset (Roach & Tak\u00e1cs, 1991) mentioned in the introduction. Through this study, we have made great efforts to capture spontaneous speech data, which is still not easily accessible.\\n\\nAs for non-Hungarian datasets, some of the most relevant model checkpoints are provided in the supplement. These datasets are mostly spontaneous speech, and the results of the past approaches augmented by cross language transfer learning are reported. Therefore, the reproduction of model results poses significant difficulties.\\n\\nThe development of Hungarian speech databases is hindered by the lack of a central database. Consequently, it is not possible to create a benchmark dataset. However, we must find a solution, as the reproduction of results may pose similar difficulties for Hungarian research. This is especially true for areas such as language ASR, spontaneous speech recognition, and evaluation.\\n\\nThus, we present the BEA database \u2013 a benchmark dataset for spontaneous Hungarian ASR. Spontaneous Hungarian databases are typically small, composition or accessibility. In the past, Hungarian language conversational speech research has focused on various applications. However, it is possible to develop Hungarian ASR toolkits and public toolkits.\\n\\nAfter defining the speech recognition system, automatic speech recognition research, spontaneous speech, and evaluation of Hungarian language were published. Namely, in the early stage of this agglutinating language, the WSJ dataset was launched and was used to create a benchmark for spontaneous Hungarian ASR. Another benchmark for spontaneous Hungarian ASR was published by another group. Unfortunately, non-Hungarian databases are not comparable to the Hungarian ones.\\n\\nBEA database \u2013 a benchmark for spontaneous Hungarian ASR. Another benchmark for spontaneous Hungarian ASR was published by another group. Hungarian research groups have developed various approaches to Hungarian speech databases. However, the most recent research groups have used their own databases without considering other ones. As for non-Hungarian datasets, some of the most relevant model checkpoints are provided in the supplement. These datasets are mostly spontaneous speech, and the results of the past approaches augmented by cross language transfer learning are reported. Therefore, the reproduction of model results poses significant difficulties.\\n\\nWe look forward to the future development of Hungarian speech databases. The best results of the past approaches augmented by cross language transfer learning are reported. Therefore, the reproduction of model results poses significant difficulties.\\n\\nSpontaneous Hungarian ASR Results\\n\\nHungarian speech databases are typically small, composition or accessibility. In the past, Hungarian language conversational speech research has focused on various applications. However, it is possible to develop Hungarian ASR toolkits and public toolkits.\\n\\nAfter defining the speech recognition system, automatic speech recognition research, spontaneous speech, and evaluation of Hungarian language were published. Namely, in the early stage of this agglutinating language, the WSJ dataset was launched and was used to create a benchmark for spontaneous Hungarian ASR. Another benchmark for spontaneous Hungarian ASR was published by another group. Unfortunately, non-Hungarian databases are not comparable to the Hungarian ones.\\n\\nSpontaneous Hungarian ASR Results\\n\\nHungarian speech databases are typically small, composition or accessibility. In the past, Hungarian language conversational speech research has focused on various applications. However, it is possible to develop Hungarian ASR toolkits and public toolkits.\\n\\nAfter defining the speech recognition system, automatic speech recognition research, spontaneous speech, and evaluation of Hungarian language were published. Namely, in the early stage of this agglutinating language, the WSJ dataset was launched and was used to create a benchmark for spontaneous Hungarian ASR. Another benchmark for spontaneous Hungarian ASR was published by another group. Unfortunately, non-Hungarian databases are not comparable to the Hungarian ones.\"}"}
{"id": "lrec-2022-1-211", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"difficult to draw valid conclusions about the effectiveness of the techniques applied throughout the studies.\\n\\n3. The BEA Base Data\\n\\n3.1 About the BEA Database\\n\\nThe original BEA (\\\"BEsz\u00e9lt nyelvi Adatb\u00e1zis\\\" in Hungarian, meaning spoken language database) aimed at collecting speech data from 500 speakers, representative in age, sex, dialect, and educational background, primarily for linguistic research purposes. The recording sessions with 1-hour target length followed a protocol where the following modules were subsequently applied to each participant:\\n\\n- **Interview**: introduction of the speaker (job, family, hobby, etc.) \u2013 spontaneous monologue.\\n- **Repeat**: sentence repetitions \u2013 the interviewer reads, the subject repeats fixed, phonetically rich sentences.\\n- **Opinion**: opinion on a given topic \u2013 spontaneous monologue.\\n- **Summhist** and **Summplant**: oral content summarization of two short stories (about history and plants) told previously to the subject.\\n- **Discourse**: free three-party conversation on a new topic.\\n- **Readsent**: reading the previously repeated fixed sentences.\\n- **Readtext**: reading a fixed coherent text unrelated to any other modules.\\n\\nFor more detailed description, see (G\u00f3sy, 2013; Neuberger et al., 2014). So far, speech data of 470 speakers have been recorded in a studio environment. Besides the target speaker's voice (SPK), the speech of the experiment leader (EXP) and a third discourse partner (DP) is recorded as well.\\n\\nThe acoustic conditions and digital processing (1 channel, sampling frequency of 44.1 kHz, 16-bit linear quantization) remained constant during the whole process of data acquisition. The annotation/transcription procedure is evolved from typing into MS Word to using Transcriber and then Praat, and currently preliminary ASR transcripts are being corrected manually using crowd sourcing tools. The transcription of the BEA is still in progress. For this reason and because of the heterogeneity of annotation formats and styles of the BEA database, it is not yet suitable for machine learning purposes.\\n\\n3.2 Construction of BEA-Base\\n\\nTo make use of the BEA database in ASR, we selected a core part of the dataset \u2013 which was originally annotated using the Transcriber software tool. Due to some inconsistencies of the annotations, we kept only the 'core' of the transcription, the words in plain text, and all other notations (e.g., hesitations, noises) were deleted in the current v0.1 release. Based on manual segmentation, all segments containing unintelligible or parallel speech were excluded from further processing. Acoustic data was resampled to 16kHz, 16 bit.\\n\\nSpeech data of 114 speakers was assigned to the training set, 10 for development, 16 for evaluation sets. The ratio of Male/Female speakers in the train set is close to 40% whereas the dev and eval sets are perfectly balanced with a ratio of 50%. The age distributions are displayed in Figure 1\u20133.\\n\\n| Segment Type | Train | Dev | Eval |\\n|--------------|-------|-----|------|\\n| Duration     | 71.2  | 0.65| 4.02 |\\n| Number of Segments | 76 | 568 | 4893 |\\n| Number of Characters | 3104 | 165 | 28467 |\\n| Number of Words | 555 | 4110 | 27939 |\\n| OOV Words (%) | 2.0  | 7.3 | 1.4 |\\n| 3-gram PPL | 43.7 | 283 | 317.4 |\\n\\nTable 1: Statistics of BEA-Base.\\n\\nTable 2: Composition of the train set in [%].\"}"}
{"id": "lrec-2022-1-211", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"information criteria and TDNN.\\n\\nThe recordings were launched in 2011.\\n\\nFor the reproducibility of the results, we extend the chain approach with hyperparameters.\\n\\nSince the recordings did not change in each recording, different lifetimes have to be set.\\n\\nAs Table 1 shows, the best results are achieved with the proposed model.\\n\\nTable 1: Comparison of different models in transfer learning.\\n\\n| Model       | WER [%] | CER [%] |\\n|-------------|---------|---------|\\n| Base        | 27.01   | 8.95    |\\n| ESPNet      | 27.08   | 9.26    |\\n| WPDM        | 27.12   | 9.36    |\\n| ESPNet-HMM  | 27.08   | 9.26    |\\n| ESPNet-SWFST| 27.01   | 8.95    |\\n| ESPNet-TDNN | 27.01   | 8.95    |\\n| ESPNet-DNN  | 27.01   | 8.95    |\\n\\nThe recordings were launched in 2011, and the recordings were launched in 2011.\"}"}
{"id": "lrec-2022-1-211", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As (Zeghidour et al. 2018) showed, applying sequential 1D convolutions \u2013 with normalizations and residual connections \u2013 can be effective in ASR not only in terms of accuracy but in computational efficacy. (Kriman et al., 2020) introduced time-channel separated convolutions reducing the number of model parameters significantly.\\n\\n### 5.1.1 Training from Scratch\\n\\nIn the experiments we followed the recipe of (Kriman et al., 2020) applying the same QuartzNet (encoder) structure consisting of BxR consecutive convolutional blocks, a shallow decoder with CTC loss (Graves et al., 2006) and character output labels. Similarly, speed perturbation and SpecAugment (Park et al., 2019) was used for data augmentation and long, 1200 epoch training. By default, we used a batch size of 32 per GPU in a 2\u20134 A6000 graphic accelerator setup. All other hyperparameters of (Kriman et al., 2020) were kept, only the learning rate was optimized for each specific structure. Beyond greedy decoding, we applied a beam-search decoder for LM rescoring, with a beam size of 80. The same word 3-gram language model was used as in the previous experiments (see Section 4). The WER and CER results can be seen in Table 5 when solely BEA-Base was used as a language resource for the estimation of QuartzNet model parameters, i.e., no pretraining was applied. The best numerical results achieved so far are marked in bold.\\n\\nIn opposite to (Kriman et al., 2020), where the 5x3 structure was found optimal in the similar size WSJ task, it performed poorly in our experiments as compared to the 15x1 structure, having practically the same number of parameters. The best results were obtained with the 15x3 structure \u2013 we could not yet explore why the 15x5 version gave higher error rates; performing more extensive hyperparameter optimization might have achieved better performance.\\n\\nAccording to the expectations, adding a language model (LM) helps in the case of the less accurate end-to-end models and especially in the case of the repeated sentences where the perplexities are low. However, in case of the best 5\\n\\n| Structure          | num of param. | LM dev-repet | dev-repet | eval-repet | eval-repet |\\n|--------------------|---------------|--------------|-----------|------------|------------|\\n| 5x3 / 6.4M         | \u2013 23.82 / 5.38 | 36.16 / 13.24 | 27.85 / 6.56 | 38.30 / 13.64 |\\n| 3-gram             | 8.91 / 2.93   | 31.63 / 14.11 | 10.05 / 3.71 | 32.91 / 14.77 |\\n| 15x1 / 6.5M        | \u2013 13.72 / 2.99 | 28.33 / 9.36  | 17.00 / 3.87 | 29.59 / 9.70 |\\n| 3-gram             | 6.76 / 2.01   | 26.66 / 10.45 | 6.90 / 2.43  | 27.97 / 11.29 |\\n| 15x2 / 9.6M        | \u2013 10.05 / 2.42 | 26.02 / 8.58  | 12.49 / 3.12 | 27.22 / 9.06 |\\n| 3-gram             | 6.57 / 1.95   | 25.52 / 10.22 | 6.71 / 2.42  | 27.09 / 10.93 |\\n| 15x3 / 12.7M       | \u2013 9.73 / 2.20  | 25.20 / 8.33  | 11.56 / 2.91 | 26.70 / 8.84 |\\n| 3-gram             | 6.50 / 1.86   | 25.50 / 10.0  | 6.86 / 2.36  | 26.83 / 10.76 |\\n| 15x5 / 18.9M       | \u2013 12.70 / 2.89 | 26.43 / 8.42  | 13.79 / 3.32 | 27.63 / 8.96 |\\n| 3-gram             | 7.30 / 2.20   | 25.58 / 10.25 | 6.90 / 2.38  | 26.98 / 10.71 |\"}"}
{"id": "lrec-2022-1-211", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1974\\n\\nrepresent the corpus used for fine tuning, and so the model generality is reduced. Eventually, both languages provide better results as compared to initialization with random weights (see Table 5), confirming the effectivity of cross-language transfer learning.\\n\\n5.2 Sequence-to-sequence Models with Attention\\n\\nA theoretical limitation of CTC (and HMM) based ASR is the conditional independence assumption in the calculation of output symbol probabilities. To overcome this, (Chan et al., 2016) introduced an attention layer between the encoder and decoder modules of the neural network. (Watanabe et al., 2017) improved end-to-end models further by applying a joint CTC-Attention model based multi-task learning. In the next phase of the experiments, we used the SpeechBrain toolkit (Ravanelli et al., 2021) that supports sequence-to-sequence ASR models\u2014including recurrent and transformer structures\u2014and joint CTC-attention training.\\n\\n5.2.1 Training from Scratch\\n\\nFirst, the parameter estimation relied entirely on the BEA-Base (the weights were initialized randomly) and we applied the CRDNN (Convolutional, Recurrent and Deep Neural Nets) encoder and GRU (Chung et al., 2014) decoder modules connected by embedding and attention layers. We trained a BPE (Sennrich et al., 2015) tokenizer on the BEA-Base transcription with a vocabulary size of 600. The end-to-end neural model was trained for 60 epochs using the joint CTC-attention loss. All other hyperparameters were derived from SpeechBrain\u2019s Common Voice recipe. No external language model was applied. During the evaluation, we used a beam size of 80\u2014similarly to the previous experiments. The pretraining-free results can be seen in Table 7. By comparing Table 5 and Table 7, we can observe a slight improvement regarding spontaneous data (marked in bold) and on the repetitive evaluation set. The error rates, however, are significantly higher than the ones obtained in 6th\\n\\nhttps://huggingface.co/models\\n\\nthe previous transfer learning experiments (see Table 6). Therefore, we continued our investigations in this direction.\\n\\n5.2.2 Self-Supervised Pretraining based Transfer Learning\\n\\nSince the introduction of transformer (Vaswani et al., 2017), it is used widely for sequence learning, including end-to-end ASR. One of the most popular ASR applications of transformers is the wav2vec 2.0 encoder approach (Baevski et al., 2020) mainly due to its self-supervised contrastive learning ability inspired by the BERT model (Devlin et al., 2018). Therefore, in the subsequent experiments we replaced the encoder module to the wav2vec 2-large transformer structure with 320 million parameters, by using SpeechBrain\u2019s corresponding CV recipe. This time, we did not train from scratch\u2014the amount of data in BEA-Base would have been clearly insufficient for this. Instead, we applied several pretrained wav2vec 2.0 (large) models available through HuggingFace (see Table 9) for encoder weight initialization. Some of the pretrained models had been already fine-tuned in a target language, but in our perspective, this fine-tuning step is just a second (retraining) phase of pretraining by using supervised data. Unlike in the previous experiments in Section 5.1.2, the first (or only) phase of pretraining used here does not require any transcription text. Note that because the applied self-supervised pretraining relies entirely on the acoustic signal, there is no theoretical difficulty in doing it on multilingual corpora as has been done in (Conneau et al., 2021; Babu et al., 2021). We again trained the neural models for 60 epochs on the train-114 set with a total batch size of 16 and left all other hyperparameters unchanged. As Table 8 shows, self-supervised pretraining based acoustic transfer learning reduces the error rates dramatically, especially on the most challenging spontaneous sets. In this setup, Italian (Wang et al., 2021) and English (Baevski et al., 2020) based results are on the same level even if the second one was pretrained with a\\n\\nTable 6: QuartzNet based transfer learning WER[%] / CER[%] results on BEA-Base.\\n\\n| Language | Data size | LM dev-repet | dev-repeat | eval-repet | eval-repeat |\\n|----------|-----------|--------------|------------|------------|------------|\\n| English  | 3k        | 9.80         | 24.68      | 6.61       | 25.21      |\\n| German   | 3k + 700  | 11.48        | 24.42      | 13.12      | 25.67      |\\n\\nTable 7: CRDNN+GRU+CTC+Attention+BPE_600 based WER[%] / CER[%] results on BEA-Base.\"}"}
{"id": "lrec-2022-1-211", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As an effect of the \\\"deep learning revolution\\\", we made a truly remarkable progress in automatic speech recognition (ASR) \u2013 a technology that allows us to create conversational AI applications. The experiments show that the accurate transcription of spontaneous speech is much more challenging than that of read speech. Without removing the background noise, the manual segmentations are particularly suitable for evaluating ASR for conversational AI applications.\\n\\nOur work was supported by the following grants: Hungarian National Research, Development and Innovation Office (OTKA K 135038); Hungarian Academy of Sciences (NKFIH 138822); NVIDIA Academic Hardware Grant 2021.\\n\\nIn this paper we introduced the BEA Base, a multilingual audio database with almost six thousand hours of speech, which is aimed to provide new benchmarks for ASR research. It includes Finnish, Hungarian, German, and Turkish corpora, but the database can be extended with any other language. The BEA Base is built purposely for more challenging spoken language tasks and it is an even more valuable language resource for linguist.\\n\\nBesides audio files, we also provide Praat TextGrid annotation which will provide an even more valuable resource for linguist. Allowling linguistic research can be more objective and meaningful.\\n\\n### Pretraining\\n\\nPretraining is a must (if affordable) and based approaches, we can achieve competitive results. Looking at the different Huggingface model IDs and details, see Table 8:\\n\\n| Model ID | Language | WER [%] / CER [%] |\\n|----------|----------|--------------------|\\n| jonasgrosman/wav2vec2 | English | 5.89 / 1.20 |\\n| facebook/wav2vec2 | Italian | 6.25 / 1.35 |\\n| m3hrdadfi/wav2vec2 | Turkish | 7.45 / 2.39 |\\n| facebook/wav2vec2 | Hungarian | 6.16 / 1.90 |\\n\\nBy keeping the amount of pretraining data, we can achieve a still performs better results on the most relevant languages. Therefore, to German and Hungarian languages, we can achieve competitive results. The results demonstrate that even more challenging language, in the case of read speech, it is easier to achieve accurate transcription than in the case of spontaneous speech.\\n\\nThe size of unsupervised data is essential. The previous research showed that more challenging than even more unsupervised data. The size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, which was obtained by retraining of this \\\"Multi\\\" model on \\\"Mega\\\" data, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold.\\n\\nInterestingly, the size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nOn BEA Base, the self supervised retraining of this \\\"Multi\\\" model gives identical results to the multilingual approach, pretrained on 440k audio \u2013 6/7 training. In the case of supervised retraining, the \\\"Multi\\\" model gives 15.61% WER and 5.11% CER on BEA Base. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe.\\n\\nThe size of multilingual pretrained model for Hungarian ASR is 0.53k + 700 hours of speech, in average, 53k + 33 hours. The multi approach, pretrained on 440k audio \u2013 6/7 training \u2013 is marked in bold. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to observe. The results are worse than those for German. In general, the superiority of multilingual pretrained models is easy to"}
{"id": "lrec-2022-1-211", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bibliographical References\\n\\nAbu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., ... & Auli, M. (2021). XLSR: Self-supervised Cross-lingual Speech Representation Learning at Scale. arXiv preprint arXiv:2111.09296.\\n\\nBaevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. arXiv preprint arXiv:2006.11477.\\n\\nBeke, A. and Szasz\u00e1k, G. (2016). Automatic summarization of highly spontaneous speech. International Conference on Speech and Computer.\\n\\nChen, S. F. and Goodman, J. (1999). An empirical study of smoothing techniques for language modeling, Computer Speech & Language, vol. 13, no. 4, pp. 359\u2013393.\\n\\nChan, W., Jaitly, N., Le, Q., & Vinyals, O. (2016). Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) pp. 4960-4964.\\n\\nChung, J., G\u00fcl\u00e7ehre, \u00c7., Cho, K., & Bengio, Y. (2014). Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. ArXiv, abs/1412.3555.\\n\\nConneau, A., Baevski, A., Collobert, R., Mohamed, A., & Auli, M. (2021). Unsupervised Cross-lingual Representation Learning for Speech Recognition. ArXiv, abs/2006.13979.\\n\\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\\n\\nElenius, K., & Tak\u00e1cs, G. (1991). Phoneme recognition with an artificial neural network. In Second European Conference on Speech Communication and Technology.\\n\\nGordos, G. (1991). New feature extraction methods and the concept of time-warped distance in speech processing,\" IEEE Global Telecommunications Conference GLOBECOM '91: pp. 725\u2013729 vol.2.\\n\\nGraves, A.; Fern\u00e1ndez, S.; Gomez, F., Schmidhuber, J. (2006). Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. ICML 2006, pp. 369\u2013376.\\n\\nGraves, A. and Jaitly, N. (2014). Towards End-To-End Speech Recognition with Recurrent Neural Networks.\" ICML.\\n\\nHannun, A., Case, C., Casper, J. (2014). Catanzaro, B., Diamos, G., Elsen, E., ... & Ng, A. Y.: Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567.\\n\\nHuang, J., Kuchaiev, O., O'Neill, P., Lavrukhin, V., Li, J., Flores, A., ... & Ginsburg, B. (2020). Cross-language transfer learning, continuous learning, and domain adaptation for end-to-end automatic speech recognition. arXiv preprint arXiv:2005.04290.\\n\\nJelinek, F.; Bahl, L.; Mercer, R. (1975). Design of a linguistic statistical decoder for the recognition of continuous speech. IEEE Transactions on Information Theory. 21 (3), pp. 250.\\n\\nKriman S. et al. (2020). Quartznet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions, In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, , pp. 6124-6128.\\n\\nKuchaiev, O., Li, J., Nguyen, H., Hrinchuk, O., Leary, R., Ginsburg, B., ... & Cohen, J. M. (2019). Nemo: a toolkit for building ai applications using neural modules. arXiv preprint arXiv:1909.09577.\\n\\nMihajlik, P., Fegy\u00f3, T., N\u00e9meth, B., T\u00fcske, Z., & Tr\u00f3n, V. (2007). Towards automatic transcription of large spoken archives in agglutinating languages\u2014Hungarian asr for the Malach project. In International Conference on Text, Speech and Dialogue (pp. 342-349). Springer, Berlin, Heidelberg.\\n\\nMihajlik, P, T\u00fcske, Z, Tarj\u00e1n, B, N\u00e9meth, B, Fegy\u00f3, T (2010). Improved recognition of spontaneous Hungarian speech\u2014Morphological and acoustic modeling techniques for a less resourced task, IEEE Transactions on Audio, Speech, and Language Processing 18 (6), 1588-1600.\\n\\nPark, D. S. et al. (2019). SpecAugment: A simple data augmentation method for automatic speech recognition, in Proc. Interspeech 2019.\\n\\nPovey, Daniel & Ghoshal, Arnab & Boulianne, Gilles & Burget, Luk\u00e1\u0161 & Glembek, Ondrej & Goel, Nagendra & Hannemann, Mirko & Motl\u00ed\u010dek, Petr & Qian, Yanmin & Schwarz, Petr & Silovsk\u00fd, Jan & Stemmer, Georg & Vesel, Karel (2011). The Kaldi speech recognition toolkit. IEEE 2011 Workshop on Automatic Speech Recognition and Understanding.\\n\\nPovey, D., Cheng, G., Wang, Y., Li, K., Xu, H., Yarmohammadi, M., Khudanpur, S. (2018). Semi-Orthogonal Low-Rank Matrix Factorization for Deep Neural Networks. Proc. Interspeech, 3743-3747.\\n\\nRavanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., ... & Bengio, Y. (2021). SpeechBrain: A General-Purpose Speech Toolkit. arXiv preprint arXiv:2106.04624.\\n\\nSennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.\\n\\nSzarvas, M., Fegy\u00f3, T., Mihajlik, P., & Tatai, P. (2000). Automatic recognition of Hungarian: Theory and practice. International Journal of Speech Technology, 3(3), 237-251.\\n\\nSzasz\u00e1k, G., T\u00fcndik, \u00c1. M. and Vicsi, K. (2011). Automatic speech to text transformation of spontaneous job interviews on the HuComTech database, 2011 2nd International Conference on Cognitive Infocommunications (CogInfoCom), pp. 1-4.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).\\n\\nWang, Y., Chen, T., Xu, H., Ding, S., Lv, H., Shao, Y., ... & Khudanpur, S. (2019, December). Espresso: A fast end-to-end neural speech recognition toolkit. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) (pp. 136-143). IEEE.\"}"}
{"id": "lrec-2022-1-211", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wang, C., Rivi\u00e8re, M., Lee, A., Wu, A., Talnikar, C., Haziza, D., ... & Dupoux, E. (2021). Voxpopuli: A large-scale multi-lingual speech corpus for representation learning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390.\\n\\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi T. (2017). Hybrid CTC/Attention Architecture for End-to-End Speech Recognition, in IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240-1253.\\n\\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., ... & Ochiai, T. (2018). Espnet: End-to-end speech processing toolkit. arXiv preprint arXiv:1804.00015.\\n\\nZeghidour, N., Xu, Q., Liptchinsky, V., Usunier, N., Synnaeve, G., & Collobert, R. (2018). Fully convolutional speech recognition. arXiv preprint arXiv:1812.06864.\\n\\nZorila, C., Boeddeker, C., Doddipatla, R., & Haeb-Umbach, R. (2019). An investigation into the effectiveness of enhancement in ASR training and test for CHiME-5 dinner party transcription. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) (pp. 47-53).\\n\\n9. Language Resource References\\n\\nArdila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., ... & Weber, G. (2019). Common voice: A massively multilingual speech corpus. arXiv preprint arXiv:1912.06670.\\n\\nGarofolo, John S., et al. (1993). CSR-I (WSJ0) Complete. LDC93S6A. Web Download. Philadelphia: Linguistic Data Consortium.\\n\\nG\u00f3sy, M. (2013). BEA\u2013A multifunctional Hungarian spoken language database. Phonetician, 105, 50-61.\\n\\nKontra, M. (1997). The Budapest Sociolinguistic Interview: Version 3 (No. 2). Hungarian Academy of Sciences.\\n\\nNeuberger, T., Gyarmathy, D., Gr\u00e1czi, T. E., Horv\u00e1th, V., G\u00f3sy, M., & Beke, A. (2014). Development of a large spontaneous speech database of agglutinative Hungarian language. In International Conference on Text, Speech, and Dialogue (pp. 424-431). Springer, Cham.\\n\\nOard, D. W., Demner-Fushman, D., Haji\u010d, J., Ramabhadran, B., Gustman, S., Byrne, W. J., ... & Picheny, M. (2002). Cross-language access to recorded speech in the MALACH project. In International Conference on Text, Speech and Dialogue (pp. 57-64). Springer, Berlin, Heidelberg.\\n\\nPanayotov V., Chen G., Povey D., and Khudanpur S. (2015). Librispeech: an ASR corpus based on public-domain audio books, in ICASSP, pp. 5206\u20135210.\\n\\nPoll\u00e1k, P., Cernocky, J., Boudy, J., Choukri, K., Heuvel, H., Vicsi, K., ... & Trnka, M. (2000). SpeechDat (E)\u2013Eastern European telephone speech databases. Siemund, R., H\u00f6ge, H., Kunzmann, S., & Marasek, K. (2000): SPEECON\u2013Speech Data for Consumer Devices. In LREC.\\n\\nP\u00e1pay, K., Szeghalmy, S., & Szekr\u00e9nyes, I. (2011). Hucomtech multimodal corpus annotation. Argumentum, 7, 330-347.\\n\\nRoach, P., Arnfield, S., Barry, W., Baltova, J., Boldea, M., Fourcin, A., ... & Vicsi, K. (1996, October). BABEL: An Eastern European multi-language database. In Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP'96 (Vol. 3, pp. 1892-1893). IEEE.\"}"}
