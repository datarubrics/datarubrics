{"id": "acl-2022-long-517", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nHumanities scholars commonly provide evidence for claims that they make about a work of literature (e.g., a novel) in the form of quotations from the work. We collect a large-scale dataset (RELiC) of 78K literary quotations and surrounding critical analysis and use it to formulate the novel task of literary evidence retrieval, in which models are given an excerpt of literary analysis surrounding a masked quotation and asked to retrieve the quoted passage from the set of all passages in the work. Solving this retrieval task requires a deep understanding of complex literary and linguistic phenomena, which proves challenging to methods that overwhelmingly rely on lexical and semantic similarity matching. We implement a RoBERTa-based dense passage retriever for this task that outperforms existing pretrained information retrieval baselines; however, experiments and analysis by human domain experts indicate that there is substantial room for improvement over our dense retriever.\\n\\n1 Introduction\\n\\nWhen analyzing a literary work (e.g., a novel or short story), scholars make claims about the text and provide supporting evidence in the form of quotations from the work (Thompson, 2002; Finnegan, 2011; Graff et al., 2014). For example, Monaghan (1980) claims that Elizabeth, the main character in Jane Austen's *Pride and Prejudice*, doesn't just refuse an offer to join the standoffish bachelor Darcy and the wealthy Bingleys on their morning walk, \\\"but does so in such a way as to group Darcy with the snobbish Bingley sisters,\\\" and then directly quotes Elizabeth's tongue-in-cheek rejection: \\\"No, no; stay where you are. You are charmingly grouped, and appear to uncommon advantage. The picturesque would be spoilt by admitting a fourth.\\\"\\n\\nLiterary scholars construct arguments like these by making complex connective inferences between their interpretations, framed as claims, and quotations (e.g., recognizing that Elizabeth says \\\"charmingly grouped\\\" and \\\"picturesque\\\" ironically in order to group Darcy with the snobbish Bingley sisters). This process requires a deep understanding of both literary phenomena, such as irony and metaphor, and linguistic phenomena (coreference, paraphrasing, and stylistics). In this paper, we computationally study the relationship between literary claims and quotations by collecting a large-scale dataset for Retrieving Evidence for Literary Claims (RELiC), which contains 78K scholarly excerpts of literary analysis that each directly quote a passage from one of 79 widely-read English texts. The complexity of the claims and quotations in RELiC makes it a challenging testbed for modern neural retrievers: given just the text of the claim and analysis that surrounds a masked quotation, can a model retrieve the quoted passage from the set of all possible passages in the literary work? This literary evidence retrieval task (see Figure 1) differs considerably from retrieval problems commonly studied in NLP, such as those used for fact checking (Thorne et al., 2018), open-domain QA (Chen et al., 2017; Chen and Yih, 2020), and text generation (Krishna et al., 2021), in the relative lack of lexical or even semantic similarity between claims and queries. Instead of latching onto surface-level cues, our task requires models to understand complex devices in literary writing and apply general theories of interpretation. RELiC is also challenging because of the large number of retrieval candidates: for *War and Peace*, the longest literary work in the dataset, models must choose from one of $\\\\sim 32K$ candidate passages.\\n\\nHow well do state-of-the-art retrievers perform on RELiC? Inspired by recent research on dense passage retrieval (Guu et al., 2020; Karpukhin et al., 2020), we build a neural model (dense-RELiC) by embedding both scholarly claims and candidate literary quotations with pretrained RoBERTa networks (Liu et al., 2019), which are then fine-tuned...\"}"}
{"id": "acl-2022-long-517", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Elizabeth comes to Pemberley full of fear of being treated as an interloper, a trespasser; even before any plans of visiting the ancient house are made, the mention of visiting Derbyshire makes Elizabeth feel like a thief: She seems to be afraid of encountering, if not the horrors of a Gothic castle, at least the resentment of a stern aristocrat\u2026 It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\\n\\nStep 1: compute context embedding $c$ by passing the text of the literary claims and analysis that surround a missing quotation to a RoBERTa network.\\n\\n\\\"But surely,\\\" said she, \\\"I may enter his county with impunity, and rob it of a few petrified spars without his perceiving me.\\\"\\n\\nDarcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude\u2026\\n\\nStep 2: compute candidate quotation embeddings $q_i$ by passing each sentence in the book through a separate RoBERTa model.\\n\\n$q_{4387}$\\n\\n$q_{7514}$\\n\\nStep 3: apply a contrastive objective to push the context vector $q$ close (+) to the correct quotation vector ($q_{4387}$) and far (-) from all other candidates.\\n\\nFigure 1: An example of our literary evidence retrieval task and the model we built to solve it. The model must retrieve a missing quotation from *Pride and Prejudice* given the literary claims and analysis that surround the quotation. The retrieval candidate set for this example consists of all 7,514 sentences from *Pride and Prejudice*.\\n\\nOur dense-RELiC model is trained with a contrastive loss to push a learned representation of the surrounding context close to a representation of the ground-truth missing quotation (here, the 4,387th sentence from the novel). Using a contrastive objective that encourages the representation for the ground-truth quotation to lie nearby to that of the claim. Both sparse retrieval methods such as BM25 and pretrained dense retrievers such as DPR and REALM perform poorly on RELiC, which underscores the difference between our dataset and existing information retrieval benchmarks (Thakur et al., 2021) on which these baselines are much more competitive. Our dense-RELiC model fares better than these baselines but still lags far behind human performance, and an analysis of its errors suggests that it struggles to understand complex literary phenomena.\\n\\nFinally, we qualitatively explore whether our dense-RELiC model can be used to support evidence-gathering efforts by researchers in the humanities. Inspired by prompt-based querying (Jiang et al., 2020), we issue our own out-of-distribution queries to the model by formulating simple descriptions of events or devices of interest (e.g., symbols of Gatsby's lavish lifestyle) and discover that it often returns relevant quotations. To facilitate future research in this direction, we publicly release our dataset and models.\\n\\n1 Collecting a Dataset for Literary Evidence Retrieval\\n\\nWe collect a dataset for the task of retrieving evidence for literary claims, or RELiC, the first large-scale retrieval dataset that focuses on the challenging literary domain. Each example in RELiC consists of two parts: (1) the context surrounding the quoted material, which consists of literary claims and analysis, and (2) a quotation from a widely-read English work of literature. This section describes our data collection and preprocessing, as well as a fine-grained analysis of 200 examples from RELiC to shed light on the types of quotations it contains. See Table 1 for corpus statistics.\\n\\n2.1 Collecting and Preprocessing RELiC\\n\\nSelecting works of literature: We collect 79 primary source works written or translated into English from Project Gutenberg and Project Gutenberg Australia. These public domain sources were selected because of their popularity and status as members of the Western literary canon, which also yield more scholarship (Porter, 2018). All primary sources were published in America or Europe between 1811 and 1949. 77 of the 79 are fictional novels or novellas, one is a collection of short stories (*The Garden Party and Other Stories* by Katherine Mansfield), and one is a collection of essays (*The Souls of Black Folk* by W. E. B. Du Bois).\\n\\nCollecting quotations from literary analysis: We queried all documents in the HathiTrust Digital Library, a collaborative repository of volumes from academic and research libraries, for exact matches of all sentences of ten or more tokens from each of the 79 works. The overwhelming majority...\"}"}
{"id": "acl-2022-long-517", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: RELiC statistics. Primary sources are from Project Gutenberg and Project Gutenberg Australia. Secondary sources are from the HathiTrust.\\n\\nFiltering and preprocessing: The scholarly articles we collected from our HathiTrust queries were filtered to exclude duplicates and non-English sources. We then preprocessed the resulting text to remove pervasive artifacts such as in-line citations, headers, footers, page numbers, and word breaks using a pattern-matching approach (details in Appendix A). Finally, we applied sentence tokenization using spaCy's dependency parser-based sentence segmenter to standardize the size of the windows in our dataset. Each window in RELiC contains the identified quotation and four sentences of claims and analysis on each side of the quotation (see Table 2 for examples). To avoid asking models to retrieve a quote they have already seen during training, we create training, validation, and test splits such that primary sources in each fold are mutually exclusive. Statistics of our dataset sources are provided in Appendix A.3.\\n\\n2.2 Comparison to other retrieval datasets\\n\\nTable 1 contains detailed statistics of RELiC. To the best of our knowledge, RELiC is the first retrieval dataset in the literary domain, and the only one that requires understanding complex phenomena like irony and metaphor. We provide a detailed comparison of RELiC to other retrieval datasets in the recently-proposed BEIR retrieval benchmark (Thakur et al., 2021) in Appendix Table A6. RELiC has a much longer query length (157.7 tokens on average) than all BEIR datasets except ArguAna (Wachsmuth et al., 2018). Furthermore, our results in Section 3.3 show that while these longer queries confuse pretrained retriever models (which heavily rely on token overlap), a model trained on RELiC is able to leverage the longer queries for better retrieval.\\n\\n2.3 Analyzing different types of quotation\\n\\nWhat are the different ways in which literary scholars use direct quotation in RELiC? We perform a manual analysis of 200 held-out examples to gain a better understanding of quotation usage, categorizing each quotation into the following three types:\\n\\nClaim-supporting evidence: In 151 of the 200 annotated examples, literary scholars used direct quotation to provide evidence for a more general claim about the primary source work. In the first row of Table 2, Hartstein (1985) claims that \u201cthis whale... brings into focus such fundamental questions as the knowability of space:\" and then quotes the following metaphorical description from Moby Dick as evidence: \u201cAnd as for this whale spout, you might almost stand in it, and yet be undecided as to what it is precisely.\u201d When quoted material is used as claim-supporting evidence, the context before and after usually refers directly to the quoted material; for example, the paradoxes of reality and uncertainties of this world are exemplified by the vague nature of the whale spout.\\n\\nParaphrase-supporting evidence: In 31 of the examples, we observe that scholars used the primary source work to support their own paraphrasing of the plot in order to contextualize later analysis. In the second row of Table 2, Blackstone (1972) uses the quoted material to enhance a summary of a specific scene in which Jacob\u2019s mind is wandering during a chapel service. Jacob\u2019s day-dreaming is later used in an analysis of Cambridge as a location in Virginia Woolf\u2019s works, but no literary argument is made in the immediate context. When quoted material is being employed as paraphrase-supporting evidence, it serves to support and contextualize the scholar\u2019s analysis, not as direct evidence for a claim.\\n\\nOther types of quotation: There are a variety of other ways in which scholars use direct quotation in RELiC, such as asides, direct quotes for emphasis, and as a way to introduce a new idea or topic. In 18 of the 151 claim-supporting evidence examples, scholars introduce quoted material by explicitly referring to a specific \u201csentence,\u201d \u201cpassage,\u201d \u201cscene,\u201d or similar delineation. In 20 of the 31 paraphrase-supporting evidence examples, scholars use quoted material to support a more general claim about the primary source work, such as \u201cthe existentialist themes of the novel are well-represented in the description of this character.\u201d\\n\\nThe variety of ways in which literary scholars use direct quotation in RELiC is a testament to the richness and complexity of the dataset, and highlights the challenges and opportunities for research in the field.\"}"}
{"id": "acl-2022-long-517", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"If this whale inspires the most lyrical passages in the novel, it also brings into focus such fundamental questions as the knowability of space:\\n\\nAnd as for this whale spout, you might almost stand in it, and yet be undecided as to what it is precisely.\\n\\nBut Ishmael stands before the paradoxes of reality with historical and scientific intellect, wisdom, and comic elasticity that accommodates\u2014however tenuously\u2014the uncertainties of this world (Hartstein, 1985).\\n\\nBut then, suddenly, Jacob's thought switches back to the lantern under the tree, with the old toad and the beetles and the moths crossing from side to side in the light, senselessly.\\n\\nNow there was a scraping and murmuring. He caught Timmy Durrant's eye; looked very sternly at him; and then, very solemnly, winked.\\n\\nFrom a boat on the Cam there is another sort of beauty to be seen. There are buttercups gilding the meadows, and cows munching, and the legs of children deep in the grass. Jacob looks at all these things and becomes absorbed (Blackstone, 1972).\\n\\nThe relationship between Alexandra and the earth is an intensely personal one:\\n\\nFor the first time, perhaps, since that land emerged from the waters of geologic ages, a human face was set toward it with love and yearning...\\n\\nThe religious connotations of the more lyrical descriptions of the land prepare us for the emergence of Alexandra as its goddess (Helmick, 1968).\\n\\nO Pioneers! is the story of a Swedish immigrant, Alexandra Bergson, who some to Nebraska with her parents when she is young. Her father dies, and she has to take over the farm and look after her younger brothers. Her courage, vision, and energy bring life and civilization to the wilderness. As Alexandra faces the future after her father's death, Willa Cather writes:\\n\\nFor the first time, perhaps, since that land emerged from the waters of geologic ages, a human face was set toward it with love and yearning.\\n\\nThe history of every country begins in the heart of a man or a woman. Alexandra succeeds in taming the wild land, and after a heaping measure of material success and personal tragedy, she faces the future calmly. (Woodress, 1975).\\n\\n| Claim-supporting evidence uses quotations to support more general literary claims, while paraphrase-supporting evidence uses quotations to corroborate summaries of the plot. The bottom two rows show the same quotation (from Willa Cather's *O Pioneers!*) being used as evidence in different ways, highlighting the dataset's complexity. |\\n| --- |\\n\\n**Table 2: Examples of the two major types of evidence identified in our manual analysis of RELiC.**\\n\\n| Claim-supporting evidence | Paraphrase-supporting evidence |\\n| --- | --- |\\n| \\\"If this whale inspires the most lyrical passages in the novel, it also brings into focus such fundamental questions as the knowability of space: And as for this whale spout, you might almost stand in it, and yet be undecided as to what it is precisely.\\\" (Hartstein, 1985) | \\\"But then, suddenly, Jacob's thought switches back to the lantern under the tree, with the old toad and the beetles and the moths crossing from side to side in the light, senselessly.\\\" (Blackstone, 1972) |\\n\\n**Miscellaneous:**\\n\\n18 of the 200 samples were not literary analysis, though some were still related to literature (for example, analysis of the film adaptation of *The Age of Innocence*). Others were excerpts from the primary sources that suffered from severe OCR artifacts and were not detected or extracted by the methods in Appendix A.2.\\n\\n3 Literary Evidence Retrieval\\n\\nHaving established that the examples in RELiC contain complex interplay between literary quotation and scholarly analysis, we now shift to measuring how well neural models can understand these interactions. In this section, we first formalize our evidence retrieval task, which provides the scholarly context without the quotation as input to a model, along with a set of candidate passages that come from the same book, and asks the model to retrieve the ground-truth missing quotation from the candidates. Then, we describe standard information retrieval baselines as well as a RoBERTa-based ranking model that we implement to solve our task.\\n\\n### 3.1 Task formulation\\n\\nFormally, we represent a single window in RELiC from book $b$ as $(... , l_{-2}, l_{-1}, q_n, r_1, r_2, ...)$ where $q_n$ is the quoted $n$-sentence long passage, and $l_i$ and $r_j$ correspond to individual sentences before and after the quotation in the scholarly article, respectively. The window size on each side is bounded by hyperparameters $l_{max}$ and $r_{max}$, each of which can be up to 4 sentences. Given the $l_{-l_{max}}: -1$ and $r_{1:+r_{max}}$ sentences surrounding the missing quotation, we ask models to identify the quoted passage $q_n$ from the candidate set $C_{b,n}$, which consists of all $n$-sentence long passages in book $b$ (see Figure 1). This is a particularly challenging retrieval task because the candidates are part of the same overall narrative and thus mention the same overall set of entities (e.g., characters, locations) and other plot elements, which is a disadvantage for methods based on string overlap.\\n\\n**Evaluation:**\\n\\nModels built for our task must produce a ranked list of candidates $C_{b,n}$ for each example. We evaluate these rankings using both recall@$k$ for $k = 1, 3, 5, 10, 50, 100$ and mean rank of $q$ in the ranked list. Both types of metrics focus on the position of the ground-truth quotation.\"}"}
{"id": "acl-2022-long-517", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Overall comparison of different systems and context sizes (L/R indicates the number of sentences on the left and right side of the missing quote) on the test set of RELiC using recall@k metrics, normalized to a maximum score of 100. Our trained dense-RELiC retriever significantly outperforms BM25 and all pretrained dense retrieval models. The average number of candidates per example is 4888. We report the accuracy of different systems on a proxy task that we administered to human domain experts, which shows that there is huge room for improvement.\\n\\n- recall@1 alone is overly strict when the quotation length \\\\( l > 1 \\\\), which is why we show recall at multiple values of \\\\( k \\\\). An additional motivation is that there may be multiple different candidates that fit a single context equally well. We also report accuracy on a proxy task with only three candidates, which allows us to compare with human performance as described in Section 4.\\n\\n### 3.2 Models\\n\\n**Baselines:** Our baselines include both standard term matching methods as well as pretrained dense retrievers. BM25 (Robertson et al., 1995) is a bag-of-words method that is very effective for information retrieval. We form queries by concatenating the left and right context and use the implementation from the rank_bm25 library to build a BM25 model for each unique candidate set \\\\( C_{b,n} \\\\), tuning the free parameters as per Kamphuis et al. (2020).\\n\\nColBERT does not provide a ranking for candidates outside the top 1000, so we cannot report mean rank. We do not report BM25's accuracy on the proxy task because its top-ranked quotes were used as candidates in the proxy task in addition to the ground-truth quotation.\\n\\n**Dense Retrieval Baselines:** Meanwhile, our dense retrieval baselines are pretrained neural encoders that map queries and candidates to vectors. We compute vector similarity scores (e.g., cosine similarity) between every query/candidate pair, which are used to rank candidates for every query and perform retrieval. We consider the following four pretrained dense retriever baselines in our work, which we deploy in a zero-shot manner (i.e., not fine-tuned on RELiC):\\n\\n- **DPR** (Dense Passage Retrieval) is a dense retrieval model from Karpukhin et al. (2020) trained to retrieve relevant context paragraphs in open-domain question answering. We use the DPR context encoder pretrained on Natural Questions (Kwiatkowski et al., 2019) with dot product as a similarity function.\\n- **SIM** is a semantic similarity model from Wieting et al. (2019) that is effective on semantic textual similarity benchmarks (Agirre et al., 2016). SIM is trained on ParaNMT (Wieting and Gimpel, 2018), a dataset containing 11 million sentence pairs.\\n\\n**Human domain experts:**\\n\\n| System                          | L/R | Recall@1 | Recall@3 | Recall@5 | Recall@10 | Recall@50 | Recall@100 | Proxy task acc@100 |\\n|--------------------------------|-----|----------|----------|----------|-----------|-----------|------------|-------------------|\\n| BM25                           | 1/1 | 1.2      | 4.2      | 5.9      | 12.5      | 17.0      | 1386.8     | \u2013                 |\\n| BM25                           | 4/4 | 1.3      | 2.9      | 4.1      | 6.7       | 14.5      | 19.7       | \u2013                 |\\n| SIM (Wieting et al., 2019)     | 1/1 | 1.3      | 2.8      | 3.8      | 5.6       | 13.4      | 18.8       | 1350.0            |\\n| SIM (Wieting et al., 2019)     | 4/4 | 0.9      | 2.1      | 3.0      | 4.7       | 12.2      | 17.3       | 1358.2            |\\n| DPR (Karpukhin et al., 2020)   | 1/1 | 1.3      | 3.0      | 4.3      | 6.6       | 15.4      | 22.2       | 1205.3            |\\n| DPR (Karpukhin et al., 2020)   | 4/4 | 1.0      | 2.2      | 3.2      | 5.2       | 13.9      | 20.7       | 1208.1            |\\n| c-REALM (Krishna et al., 2021) | 1/1 | 1.6      | 3.5      | 4.8      | 7.1       | 15.9      | 21.7       | 1332.0            |\\n| c-REALM (Krishna et al., 2021) | 4/4 | 0.9      | 2.1      | 3.3      | 5.0       | 12.9      | 18.8       | 1333.9            |\\n| ColBERT (Khattab and Zaharia, 2020) | 1/1 | 2.9      | 6.0      | 7.8      | 11.0      | 21.4      | 27.9       | N/A               |\\n| ColBERT (Khattab and Zaharia, 2020) | 4/4 | 1.9      | 3.9      | 5.3      | 8.0       | 18.2      | \u2013          | 18.9              |\\n| dense-RELiC (trained on RELiC training set) | 0/1 | 3.4      | 7.1      | 9.3      | 12.6      | 24.1      | 31.3       | 1094.4            |\\n| dense-RELiC (trained on RELiC training set) | 0/4 | 5.2      | 10.7     | 13.6     | 18.5      | 32.4      | 40.2       | 887.8             |\\n| dense-RELiC (trained on RELiC training set) | 1/0 | 5.2      | 10.5     | 13.6     | 18.7      | 34.7      | 43.2       | 788.5             |\\n| dense-RELiC (trained on RELiC training set) | 4/0 | 6.8      | 14.4     | 19.3     | 25.7      | 43.9      | 52.8       | 538.3             |\\n| dense-RELiC (trained on RELiC training set) | 1/1 | 7.8      | 15.1     | 19.3     | 25.7      | 43.3      | 52.0       | 558.0             |\\n| dense-RELiC (trained on RELiC training set) | 4/4 | 9.4      | 18.3     | 24.0     | 32.4      | 51.3      | 60.8       | 377.3             |\\n\\n**Human domain experts:**\\n\\n- 4/4: 93.5\"}"}
{"id": "acl-2022-long-517", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"16.8M paraphrases; we follow the original implementation, and use cosine similarity as the similarity function.\\n\\n\u2022 c-REALM (contrastive Retrieval Augmented Language Model) is a dense retrieval model from Krishna et al. (2021) trained to retrieve relevant contexts in open-domain long-form question answering, and shown to be a better retriever than REALM (Guu et al., 2020) on the ELI5 KILT benchmark (Fan et al., 2019; Petroni et al., 2021).\\n\\n\u2022 ColBERT is a ranking model from Khattab and Zaharia (2020) that estimates the relevance between a query and a document using contextualized late interaction. It is trained on MS MARCO ranking data (Nguyen et al., 2016).\\n\\nTraining retrievers on RELiC (dense-RELiC):\\nBoth BM25 and the pretrained dense retriever baselines perform similarly poorly on RELiC (Table 3). These methods are unable to capture more complex interactions within RELiC that do not exhibit extensive string overlap between quotation and context. As such, we also implement a strong neural retrieval model that is actually trained on RELiC, using a similar setup to DPR and REALM. We first form a context string $c$ by concatenating a window of sentences on either side of the quotation $q$ (replaced by a MASK token), $c = (l_{-\\\\ell_{\\\\text{max}}}, ..., l_{-1}, [\\\\text{MASK}], r_1, ..., r_{r_{\\\\text{max}}})$\\n\\nWe train two encoder neural networks to project the literary context and quote to fixed 768-d vectors. Specifically, we project $c$ and $q$ using separate encoder networks initialized with a pretrained RoBERTa-base model (Liu et al., 2019). We use the $<$s$>$ token of RoBERTa to obtain 768-d vectors for the context and quotation, which we denote as $c_i$ and $q_i$. To train this model, we use a contrastive objective (Chen et al., 2020) that pushes the context vector $c_i$ close to its quotation vector $q_i$, but away from all other quotation vectors $q_j$ in the same minibatch (\u201cin-batch negative sampling\u201d):\\n\\n$$\\\\text{loss} = -\\\\sum_{(c_i, q_i) \\\\in B} \\\\log \\\\frac{\\\\exp (c_i \\\\cdot q_i)}{\\\\sum_{q_j \\\\in B} \\\\exp (c_i \\\\cdot q_j)}$$\\n\\nwhere $B$ is a minibatch. Note that the size of the minibatch $|B|$ is an important hyperparameter since it determines the number of negative samples.\\n\\nAll elements of the minibatch are context/quotation pairs sampled from the same book. During inference, we rank all quotation candidate vectors by their dot product with the context vector.\\n\\n### 3.3 Results\\nWe report results from the baselines and our dense-RELiC model in Table 3 with varying context sizes where $L/R$ refers to $L$ preceding context sentences and $R$ subsequent context sentences. While all models substantially outperform random candidate selection, all pretrained neural dense retrievers perform similarly to BM25, with ColBERT being the best pretrained neural retriever (2.9 recall@1). This result indicates that matching based on string overlap or semantic similarity is not enough to solve RELiC, and even powerful neural retrievers struggle on this benchmark. Training on RELiC is crucial: our best-performing dense-RELiC model performs 7x better than BM25 (9.4 vs 1.3 recall@1).\\n\\nContext size and location matters for model performance: Table 3 shows that dense-RELiC effectively utilizes longer context \u2014 feeding only one sentence on each side of the quotation (1/1) is not as effective as a longer context (4/4) of four sentences on each side (7.8 vs 9.4 recall@1). However, the longer contexts hurt performance for pretrained dense retrievers in the zero-shot setting (1.6 vs 0.9 recall@1 for c-REALM), perhaps because context further away from the quotation is less likely to be helpful. Finally, we observe that dense-RELiC performance is strictly better (5.2 vs 6.8 recall@1) when the model is given only preceding context (4/0 or 1/0) compared to when the model is given only subsequent context (0/4 or 0/1).\\n\\nDense vs. sparse retrievers:\\nAs expected, BM25 retrieves the correct quotation when there is significant string overlap between the quotation and context, as in the following example from *The Great Gatsby*, in which the terms sky, bloom, Mrs. McKee, voice, call, and back appear in both places:\\n\\nWe set $|B| = 100$, and train all models for 10 epochs on a single RTX8000 GPU with an initial learning rate of $1e^{-5}$ using the Adam optimizer (Kingma and Ba, 2015), early stopping on validation loss. Models typically took 4 hours to complete 10 epochs. Our implementation uses the Hugging-Face transformers library (Wolf et al., 2020). The total number of model parameters is $249M$. \"}"}
{"id": "acl-2022-long-517", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yet his analogy also implicitly unites the two women. Myrtle's expansion and revolution in the smoky air are also outgrowths of her surreal attributes, stemming from her residency in the Valley of Ashes.\\n\\nThe late afternoon sky bloomed in the window for a moment like the blue honey of the Mediterranean\u2014then the shrill voice of Mrs. McKee called me back into the room.\\n\\nThe objective talk of Monte Carlo and Marseille has made Nick daydream. In Chapter I Daisy and the rooms had bloomed for him, with him, and now the sky blooms. The fact that Mrs. McKee's voice \\\"calls him back\\\" clearly reveals the subjective daydreamy nature of this statement.\\n\\nHowever, this behavior is undesirable for most examples in RELiC, since string overlap is generally not predictive of the relationship between quotations and claims. The top row of Table 5 contains one such example, where dense-RELiC correctly chooses the missing quotation while BM25 is misled by string overlap.\\n\\n4 Human performance and analysis\\n\\nHow well do humans actually perform on RELiC? To compare the performance of our dense retriever to that of humans, we hired six domain experts with at least undergraduate-level degrees in English literature from the Upwork freelancing platform. Because providing thousands of candidates to a human evaluator is infeasible, we instead measure human performance on a simplified proxy task: we provide our evaluators with four sentences on either side of a missing quotation from Pride and Prejudice and ask them to select one of only three candidates to fill in the blank. We obtain human judgments both to measure a human upper bound on this proxy task as well as to evaluate whether humans struggle with examples that fool our model.\\n\\nHuman upper bound:\\n\\nFirst, to measure a human upper bound on this proxy task, we chose 200 test set examples from Pride and Prejudice and formed a candidate pool for each by including BM25's top two ranked answers along with the ground-truth quotation for the single sentence case. As the task is trivial to solve with random candidates, we decided to use a model to select harder negatives, and we chose BM25 to see if humans would be distracted by high string overlap in the negatives. Each of the 200 examples was separately annotated by three experts, and they were paid $100 for annotating 100 examples. The last column of Table 3 compares all of our baselines along with dense-RELiC against human domain experts on this proxy task. Humans substantially outperform all models on the task, with at least two of the three domain experts selecting the correct quote 93.5% of the time; meanwhile, the highest score for dense-RELiC is 67.5%, which indicates huge room for improvement. Interestingly, all of the zero-shot dense retrievers except ColBERT underperform random selection on this task; we theorize that this is because all of these retrievers are misled by the high string overlap of the negative BM25-selected examples. Table 4 confirms substantial agreement among our annotators.\\n\\n|                  | Random | Humans |\\n|------------------|--------|--------|\\n| Agree            | 0.00   | 0.68   |\\n| Disagree         | 11.1%  | 11.1%  |\\n| No agree         | 22.2%  | 22.2%  |\\n\\nTable 4: Inter-annotator agreement of our three human annotators compared to a random annotation. In our 3-way classification task, all three annotators chose the same option 68.5% of the time, while they each chose a different option in just 0.5% of instances. Our annotators also show substantial agreement in terms of Fleiss Kappa (Fleiss, 1971).\\n\\nHuman error analysis of dense-RELiC:\\n\\nTo evaluate the shortcomings of our dense-RELiC retriever, we also administered a version of the proxy task where the candidate pool included the ground-truth quotation along with dense-RELiC's two top-ranked candidates, where for all examples the model ranked the ground-truth outside of the top 1000 candidates. Three domain experts attempted 100 of these examples and achieved an accuracy of 94%, demonstrating that humans can easily disambiguate cases on which our model fails, though we note our model's poorer performance when retrieving a single sentence (as in the proxy task) versus multiple sentences (A5). The bottom two rows of Table 5 contain instances in which all human annotators agreed on the correct candidate but dense-RELiC failed to rank it in the top 1000. In one, all human annotators immediately recognized the opening line of Pride and Prejudice, one...\"}"}
{"id": "acl-2022-long-517", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"She is caught up for a moment or two in a fantasy of possession:\\n\\n\\\"The thought that she would not have been allowed to invite the Gardiners is a lucky recollection\u2014it saved her from something like regret. (Paris, 1978)\\\"\\n\\nHuman: \\\"And of this place,\\\" thought she, \\\"I might have been mistress! With these rooms I might now have been familiarly acquainted!\\\"\\n\\ndense-RELiC: \\\"I should not have been allowed to invite them.\\\" This was a lucky recollection\u2014it saved her from something very like regret.\\n\\nIt is delicious from the opening sentence:\\n\\n\\\"Mr. Bingley, with his four or five thousand a year, had settled at Netherfield Park. (Masefield, 1967)\\\"\\n\\nHuman: \\\"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\\n\\ndense-RELiC: \\\"My dear Mr. Bennet,\\\" said his lady to him one day, \\\"have you heard that Netherfield Park is let at last?\\\"\\n\\nSometimes we hear Mrs Bennet's idea of marriage as a market in a single word:\\n\\n\\\"Her stupidity about other people shows in all her dealings with her family... (McEwan, 1986)\\\"\\n\\nHuman: \\\"I do not blame Jane,\\\" she continued, \\\"Jane would have got Mr. Bingley if she could.\\\"\\n\\ndense-RELiC: \\\"You must and shall be married by a special licence.\\\"\\n\\nTable 5: Examples that show failure cases of BM25 (top row) and our dense-RELiC retriever (bottom two rows) from our proxy task on Pride and Prejudice. BM25 is easily misled by string overlap, while dense-RELiC lacks world knowledge (e.g., knowing the famous first sentence) and complex linguistic understanding (e.g., the relationship between marriage as a market and got) that humans can easily rely on to disambiguate the correct quotation.\\n\\nLimitations: While these results show dense-RELiC's potential to assist research in the humanities, the model suffers from the limited expressivity of its candidate quotation embeddings $q_i$, and addressing this problem is an important direction for future work. The quotation embeddings do not incorporate any broader context from the narrative, which prevents resolving coreferences to pronominal character mentions and understanding other important discourse phenomena. For example, Table A5 shows that dense-RELiC's top two 1-sentence candidates for the above Pride and Prejudice example are not appropriate evidence for the literary claim; the increased relevancy of the 2-sentence candidates (Table 6, third row) over the 1-sentence candidates suggests that dense-RELiC may benefit from more contextualized quotation embeddings. Furthermore, dense-RELiC struggles with retrieving concepts unique to a text, such as the \\\"hypnopaedic phrases\\\" strewn throughout Brave New World (Table 6, bottom).\\n\\n5 Related Work\\n\\nDatasets for literary analysis: Our work relates to previous efforts to apply NLP to literary datasets...\"}"}
{"id": "acl-2022-long-517", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Given a novel and a short out-of-distribution prompt, this table shows the top 3 quotations from the novel that dense-RELiC returns as evidence. The relevance of many of the returned quotations, even without string overlap between the prompt and candidates, indicates the model is learning some non-trivial relationships that could have potential impact for building tools that support humanities research. However, it is not perfect, as shown in the final example where none of the retrieved quotations is actually an instance of a hypnopaedic phrase.\\n\\nsuch as LitBank (Bamman et al., 2019; Sims et al., 2019), an annotated dataset of 100 works of fiction with annotations of entities, events, coreferences, and quotations. Papay and Pad\u00f3 (2020) introduced RiQuA, an annotated dataset of quotations in English literary text for studying dialogue structure, while Chaturvedi et al. (2016) and Iyyer et al. (2016) characterize character relationships in novels. Our work also relates to quotability identification (MacLaughlin and Smith, 2021), which focuses on ranking passages in a literary work by how often they are quoted in a larger collection. Unlike RELiC, however, these datasets do not contain literary analysis about the works.\\n\\nRetrieving cited material:\\nCitation retrieval closely relates to RELiC and has a long history of research, mostly on scientific papers: O'Connor (1982) formulated the task of document retrieval using \u201cciting statements\u201d, which Liu et al. (2014) revisit to create a reference retrieval tool that recommends references given context. Bertin et al. (2016) examine the rhetorical structure of citation contexts. Perhaps closest to RELiC is the work of Grav (2019), which concentrates on the quotation of secondary sources in other secondary sources, unlike our focus on quotation from primary sources. Finally, as described in more detail in Section 2.2 and Appendix A6, RELiC differs significantly from existing NLP and IR retrieval datasets in domain, linguistic complexity, and query length.\\n\\n6 Conclusion\\nIn this work, we introduce the task of literary evidence retrieval and an accompanying dataset, RELiC. We find that direct quotation of primary sources in literary analysis is most commonly used as evidence for literary claims or arguments. We train a dense retriever model for our task; while it significantly outperforms baselines, human performance indicates a large room for improvement. Important future directions include (1) building better models of primary sources that integrate narrative and discourse structure into the candidate representations instead of computing them out-of-context, and (2) integrating RELiC models into real tools that can benefit humanities researchers.\"}"}
{"id": "acl-2022-long-517", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nFirst and foremost, we would like to thank the HathiTrust Research Center staff (especially Ryan Dubnicek) for their extensive feedback throughout our project. We are also grateful to Naveen Jafer Nizar for his help in cleaning the dataset, Vishal Kalakonnavar for his help with the project webpage, Marzena Karpinska for her guidance on computing inter-annotator agreement, and the UMass NLP community for their insights and discussions during this project. KT and MI are supported by awards IIS-1955567 and IIS-2046248 from the National Science Foundation (NSF). KK is supported by the Google PhD Fellowship awarded in 2021.\\n\\nEthical Considerations\\n\\nWe acknowledge that the group of authors from whom we selected primary sources lacks diversity because we selected from among digitized, public domain sources in the Western literary canon, which is heavily biased towards white, male writers. We made this choice because there are relatively few primary sources in the public domain that are written by minority authors and also have substantial amounts of literary analysis written about them. We hope that our data collection approach will be followed by those with access to copyrighted texts in an effort to collect a more diverse dataset.\\n\\nExperiments involving humans were reviewed by the UMass Amherst IRB with a status of Exempt.\\n\\nReferences\\n\\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2016. SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016).\\n\\nDavid Bamman, Sejal Popat, and Sheng Shen. 2019. An annotated dataset of literary entities. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2138\u20132144.\\n\\nMarc Bertin, Iana Atanassova, Cassidy R Sugimoto, and Vincent Lariviere. 2016. The linguistic patterns and rhetorical structure of citation context: an approach using n-grams. Scientometrics, 109(3):1417\u20131434.\\n\\nBernard Blackstone. 1972. Virginia Woolf: A Commentary. London.\\n\\nAlexander Bondarenko, Maik Fr\u00f6be, Meriem Bebloucif, Lukas Gienapp, Yamen Ajjour, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, Martin Potthast, and Matthias Hagen. 2020. Overview of Touch\u00e9 2020: Argument Retrieval. In Working Notes Papers of the CLEF 2020 Evaluation Labs, volume 2696 of CEUR Workshop Proceedings.\\n\\nVera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. 2016. A full-text learning to rank dataset for medical information retrieval. In Proceedings of the 38th European Conference on Information Retrieval (ECIR 2016), pages 716\u2013722.\\n\\nSnigdha Chaturvedi, Shashank Srivastava, Hal Daume III, and Chris Dyer. 2016. Modeling evolving relationships between characters in literary novels. In Proceedings of the AAAI Conference on Artificial Intelligence.\\n\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\u20131879, Vancouver, Canada. Association for Computational Linguistics.\\n\\nDanqi Chen and Wen-tau Yih. 2020. Open-domain question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 34\u201337, Online. Association for Computational Linguistics.\\n\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In Proceedings of the International Conference of Machine Learning.\\n\\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. 2020. SPECTER: Document-level representation learning using citation-informed transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270\u20132282, Online. Association for Computational Linguistics.\\n\\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu\u00dfian, Massimiliano Ciaramita, and Markus Leippold. 2020. Climate-fever: A dataset for verification of real-world climate claims.\\n\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558\u20133567, Florence, Italy. Association for Computational Linguistics.\\n\\nRuth Finnegan. 2011. Why do we quote?: the culture and history of quotation. Open Book Publishers.\\n\\nJoseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.\"}"}
{"id": "acl-2022-long-517", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gerald Graff, Cathy Birkenstein, and Cyndee Maxwell. 2014. They say, I say: The moves that matter in academic writing. Gildan Audio.\\n\\nPeter F. Grav. 2019. Harnessing Sources in the Humanities: A Corpus-based Investigation of Citation Practices in English Literary Studies. Discourse and Writing/R\u00e9dactologie, 29:24\u201350.\\n\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Papisupat, and Ming-Wei Chang. 2020. REALM: Retrieval-augmented language model pre-training. In Proceedings of the International Conference of Machine Learning.\\n\\nArnold M. Hartstein. 1985. Myth and History in Moby Dick. American Transcendental Quarterly, 57:31\u201343.\\n\\nFaegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. 2017. Dbpedia-entity v2: A test collection for entity search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '17, pages 1265\u20131268. ACM.\\n\\nEvelyn Thomas Helmick. 1968. Myth in the Works of Willa Cather. Midcontinent American Studies Journal, 9(2):63\u201369.\\n\\nMark M. Hennelly, Jr. 1983. The Eyes Have It. Jane Austen: New Perspectives, 3.\\n\\nDoris Hoogeveen, Karin M Verspoor, and Timothy Baldwin. 2015. CQADupStack: A benchmark data set for community question-answering research. In Proceedings of the 20th Australasian Document Computing Symposium, pages 1\u20138.\\n\\nMohit Iyyer, Anupam Guha, Snigdha Chaturvedi, Jordaan Boyd-Graber, and Hal Daum\u00e9 III. 2016. Feuding families and former friends: Unsupervised learning for dynamic fictional relationships. In Conference of the North American Chapter of the Association for Computational Linguistics.\\n\\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How Can We Know What Language Models Know? Transactions of the Association for Computational Linguistics, 8:423\u2013438.\\n\\nChris Kamphuis, Arjen P de Vries, Leonid Boytsov, and Jimmy Lin. 2020. Which BM25 do you mean? a large-scale reproducibility study of scoring variants. In European Conference on Information Retrieval, pages 28\u201334. Springer.\\n\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of Empirical Methods in Natural Language Processing.\\n\\nOmar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT, page 39\u201348. Association for Computing Machinery, New York, NY, USA.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\\n\\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In North American Association for Computational Linguistics.\\n\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics.\\n\\nColin Legum. 1972. Congo Disaster. Peguin Books Ltd.\\n\\nShengbo Liu, Chaomei Chen, Kun Ding, Bo Wang, Kan Xu, and Yuan Lin. 2014. Literature retrieval based on citation context. Scientometrics, 101(2):1293\u20131307.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mardar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nAnsel MacLaughlin and David A Smith. 2021. Content-based models of quotation. In Proceedings of the European Chapter of the Association for Computational Linguistics, pages 2296\u20132314.\\n\\nDeborah L. Madsen. 2000. Feminist Theory and Literary Practice. London.\\n\\nHena Maes-Jelinek. 1970. Criticism of Society in the English Novel Between the Wars. Paris.\\n\\nMacedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018. Www'18 open challenge: Financial opinion mining and question answering. In Companion Proceedings of the The Web Conference 2018, WWW '18, page 1941\u20131942, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee.\\n\\nMuriel Agnes Bussell Masefield. 1967. Women Novelists from Fanny Burney to George Eliot. Books for Libraries Press, New York.\\n\\nNeil McEwan. 1986. Style in English prose. York handbooks. Longman, Harlow, Essex.\"}"}
{"id": "acl-2022-long-517", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Monaghan. 1980. *Jane Austen, Structure and Social Vision*. Barnes & Noble Books, New York.\\n\\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. *MS MARCO: A human generated machine reading comprehension dataset*. In *Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016*, volume 1773 of *CEUR Workshop Proceedings*. CEUR-WS.org.\\n\\nJohn O'Connor. 1982. *Citing statements: Computer recognition and use to improve retrieval*. *Information Processing & Management*, 18(3):125\u2013131.\\n\\nSean Papay and Sebastian Pad\u00f3. 2020. *RiQuA: A corpus of rich quotation annotation for English literary text*. In *Proceedings of the 12th Language Resources and Evaluation Conference*, pages 835\u2013841, Marseille, France. European Language Resources Association.\\n\\nBernard J. Paris. 1978. *Character and Conflict in Jane Austen's Novels: A Psychological Approach*. Wayne State University Press, Detroit.\\n\\nKenneth Parker. 1985. *The Revelation of Caliban: 'The Black Presence' in the Classroom*. In David Dabydeen, editor, *The Black Presence in English Literature*. Manchester University Press.\\n\\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. *KILT: a benchmark for knowledge intensive language tasks*. In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 2523\u20132544, Online. Association for Computational Linguistics.\\n\\nJ.D. Porter. 2018. * Literary Lab Pamphlet 17: Popular-Prestige*. Pamphlet.\\n\\nJustus Randolph. 2010. *Free-Marginal Multirater Kappa (multirater kfree): An Alternative to Fleiss Fixed-Marginal Multirater Kappa*. *Advances in Data Analysis and Classification*, 4.\\n\\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. *Okapi at trec-3*. *Nist Special Publication Sp*, 109:109.\\n\\nMatthew Sims, Jong Ho Park, and David Bamman. 2019. *Literary event detection*. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 3623\u20133634.\\n\\nIan Soboroff, Shudong Huang, and Donna Harman. 2018. *Trec 2018 news track overview*. In *TREC*.\\n\\nAxel Suarez, Dyaa Albakour, David Corney, Miguel Martinez, and Jose Esquivel. 2018. *A data collection for evaluating the retrieval of related tweets to news articles*. In *40th European Conference on Information Retrieval Research (ECIR 2018), Grenoble, France, March, 2018.*, pages 780\u2013786.\\n\\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. *Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models*. *arXiv preprint arXiv:2104.08663*.\\n\\nJennifer Wolfe Thompson. 2002. *The death of the scholarly monograph in the humanities? citation patterns in literary scholarship*. *Libri*, 52.\\n\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. *FEVER: a large-scale dataset for fact extraction and VERification*. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 809\u2013819, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nGeorge Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. 2015. *An overview of the bioasq large-scale biomedical semantic indexing and question answering competition*. *BMC bioinformatics*, 16(1):138.\\n\\nEllen Voorhees. 2005. *Overview of the TREC 2004 robust retrieval track*. Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021. *Trec-covid: constructing a pandemic information retrieval test collection*. In *ACM SIGIR Forum*, volume 54, pages 1\u201312. ACM New York, NY, USA.\\n\\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein. 2018. *Retrieval of the best counterargument without prior topic knowledge*. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 241\u2013251. Association for Computational Linguistics.\\n\\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. *Fact or fiction: Verifying scientific claims*. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 7534\u20137550, Online. Association for Computational Linguistics.\\n\\nJohn Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig. 2019. *Beyond BLEU: Training neural machine translation with semantic similarity*. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 7534\u20137550, Online. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-517", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"John Wieting and Kevin Gimpel. 2018. ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451\u2013462, Melbourne, Australia. Association for Computational Linguistics.\\n\\nBrian Wilkie. 1992. Jane Austen: Amore and Amoralism. Journal of English and German Philology, 91(1):529\u2013555.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nJames Woodress. 1975. Willa Cather: The World and the Parish. Architectural Association Quarterly, 7:51\u201359.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-517", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Filtering secondary sources: The HathiTrust is not exclusively a repository of literary analysis, and we observe that many matching quotes come from different editions of a primary source, writing manuals, and even advertisements. Because we are seeking only scholarly work that directly analyzes the quoted sentences, we performed a combination of manual and automatic filtering to remove such extraneous matches. For each primary source, we first aggregate all secondary sources matches by their unique HathiTrust-assigned identifier. From manual inspection of the secondary source titles, most sources that quote a particular literary work only once or twice are not likely to be literary scholarship, while sources with hundreds of matches are almost always a different edition of the primary source itself. For each primary source, we create upper and lower thresholds for number of matches, discarding sources that fall outside of these bounds. Additionally, we discard secondary sources whose titles contain the words \u201cdictionary\u201d, \u201canthology\u201d, \u201cencyclopedia,\u201d and others that indicate that a secondary source is not literary scholarship.\\n\\nPreprocessing: After the above filtering, we identified and removed all non-English secondary sources using langid, a Python tool for language identification. Next, because the secondary source texts in the HathiTrust are digitized via OCR, various artifacts appear throughout the pages we download. Some of these, such as citations that include the page number of primary source quotes, allow models trained on our task to \u201ccheat\u201d to identify the proper quote (see Table A1), necessitating their removal. Using a pattern-matching approach, we eliminate the most pervasive: in-line citations, headers, footers, and word breaks. Finally, we apply sentence tokenization in order to standardize the length of preceding and subsequent context windows for the final dataset. Specifically, we feed the preprocessed text through spaCy\u2019s dependency parser-based sentence segmenter on the cleaned text. The default segmenter in spaCy is modified to use ellipses, colons, and semicolons as custom sentence boundaries, based on the observation that literary scholars often only quote part of what would typically be defined as a sentence (Table A2).\\n\\nRaw text from HathiTrust:\\n\\nThe prejudice in these same eyes, however, keeps them \u201cless clear-sighted\u201d to Bingley\u2019s feelings for Jane and totally closed to the real worth-lesness of Wickham and worth of Darcy. When Jane\u2019s letter reporting Lydia\u2019s disappearance with Wickham confirms Darcy\u2019s earlier indictment of him, though, Elizabeth\u2019s \u201ceyes were opened to his real character\u201d.\\n\\nTable A1: An analysis of Jane Austen\u2019s Pride and Prejudice from Hennelly (1983) that contains artifacts (bold) such as citations and page numbers that we remove during preprocessing.\\n\\nQuoted span in context of literary analysis:\\n\\nEdna tries to discuss this issue of possession versus self-possession with Madame Ratignolle but to no avail; \u2019the two women did not appear to understand each other or to be talking the same language. \\n\\nQuote in original context from The Awakening:\\n\\nEdna had once told Madame Ratignolle that she would never sacrifice herself for her children, or for any one. Then had followed a rather heated argument; the two women did not appear to understand each other or to be talking the same language. Edna tried to appease her friend, to explain.\\n\\nTable A2: An analysis of Kate Chopin\u2019s The Awakening from Madsen (2000) that quotes part of a sentence (following a semi-colon) from the primary source. We detect such partial matches during preprocessing.\\n\\nIdentifying quoted sentences: As previously mentioned, HathiTrust does not provide the exact indices corresponding to the primary source quote. As such, we identify which secondary source sentences (from the output of the sentence tokenizer) include quotes from primary source works using RapidFuzz, a fuzzy string match library, with the QRatio metric and a score threshold of 80.0. Fuzzy match is essential for detecting quotes with OCR mistakes or with author modifications; in Appendix Table A3, for instance, the author adds clarification [the natives] and omits \u201che would say\u201d when citing two sentences from Joseph Conrad\u2019s Heart of Darkness. Once a fuzzy match is identified in a secondary source document, we replace it with its corresponding primary source sentence.\\n\\n19https://github.com/maxbachmann/RapidFuzz\"}"}
{"id": "acl-2022-long-517", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kurtz's credo, like his royal employer's, was a simple one.\\n\\n1. \u201cYou show them [the natives] you have in you something that is really profitable, and then there will be no limits to the recognition of your ability.\\n\\n2. Of course you must take care of the motives\u2014right motives\u2014always.\u201d\\n\\nKurtz dies screaming: \u201cThe Horror! The Horror!\u201d\\n\\nLeopold, so far as one knows, died more peacefully.\\n\\nWindow in RELiC with standardized quote:\\n\\nKurtz's credo, like his royal employer's, was a simple one.\\n\\n\u201cYou show them you have in you something that is really profitable, and then there will be no limits to the recognition of your ability,\u2019 he would say.\\n\\n\u2018Of course you must take care of the motives\u2014right motives\u2014always.\u2019\\n\\nKurtz dies screaming: \u201cThe Horror! The Horror!\u201d\\n\\nLeopold, so far as one knows, died more peacefully.\\n\\nTable A3: This example demonstrates the necessity of fuzzy match and block quote identification. Consecutive sentences are quoted and one is slightly modified from its original form in the primary source.\\n\\nIdentifying block quotes:\\n\\nWhile we query HathiTrust at a sentence level, many of the returned results are actually block quotes in which multiple contiguous sentences from the primary source are quoted. Correct identification of these block quotes is integral to the quality of our dataset and formulated task: if the preceding or subsequent context contains part of the quoted span, our evidence retrieval task becomes trivial because part of the answer exists in the input. In our approach, if the fuzzy match yields consecutive matches in secondary source documents for sentences that also appear consecutively in the primary source, we concatenate them together and consider them a single block quote.\\n\\nHandling ellipses:\\n\\nOne prevalent technique for direct quotation in literary analysis is the use of ellipses to condense primary source material. As our fuzzy match method still falls short in detecting block quotes that contain ellipses, we implement an additional method for insuring that block quotes are properly delineated. Once the fuzzy match approach fails to identify any more consecutively quoted sentences in a secondary source, we continue to search for matches adjacent to the block quote using the Longest Common Substring (LCS) metric. If a block-quote-adjacent sentence in the secondary source shares an LCS of 15 or more characters with the block-quote-adjacent sentence in the primary source, this is considered a match and concatenated with the block quote (see Appendix A.1 for an example).\\n\\nA.1 LCS example\\n\\nFor example, in Parker (1985), Kenneth Parker cites a passage from Joseph Conrad's Heart of Darkness: \u201cThe narrator, Marlow, informs us, approvingly:...\\n\\nI met a white man, in such an unexpected elegance of get-up that in the first moment I took him for a sort of vision. I saw a high starched collar, white cuffs, a light alpaca jacket, snowy trousers, a clean necktie, and varnished boots.\u201d Fuzzy match alone is insufficient for detecting the first sentence in this block quote that contains an ellipse in place of primary source text. With our LCS approach, we are able to replace the first sentence of block quote above with \u201cWhen near the buildings I met a white man, in such an unexpected elegance of get-up that in the first moment I took him for a sort of vision.\u201d\\n\\nA.2 Noise when standardizing quotes:\\n\\nIn a small number of cases, our quote standardization process removes important context. For example, the analysis of Maes-Jelinek (1970) quotes a sentence from D.H. Lawrence's The Rainbow as \u201cAs to Will, his intimate life was so violently active, that it set another man free in him. After standardization, the example in our dataset becomes \u201cHis intimate life was so violently active, that it set another man free in him.\u201d dropping the critical \u201cAs to Will\u201d necessary for the integration of the quote in the surrounding analysis.\\n\\nModel-predicted quotes are sometimes as valid as the gold quote: Human raters also identify cases in which multiple quotes appear to be appropriate evidence for a literary claim, which illustrates the model's potential in helping humanities scholars find evidence. In Table A4, both model and experts failed to identify the correct quote that both depicts Elizabeth's \u201cdiscomfiture\u201d and has a \u201cGreek ring to it:\u201d \u201cTill this moment I never knew myself.\u201d However, the experts all selected the model's second ranked choice which mentions Elizabeth's \u201canger\u201d at \u201cherself.\u201d This quote also shows Elizabeth's displeasure while referring to the Greek idea of self.\"}"}
{"id": "acl-2022-long-517", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For example, Elizabeth's anger with herself, after reading Darcy's letter, is couched largely in the vocabulary of rectifiable intellectual error\u2014blind, partial, prejudiced, absurd, and the like\u2014rather than in the relentless, coercive vocabulary of moral contrition. Her discomfiture, though profound, has a Greek ring to it: Till this moment I never knew myself.\\n\\nHeuristically, the distinction between moral and other spheres of value throws light also on other Austen novels that we can only glance at here (Wilkie, 1992).\\n\\nTable A4: The model ranked the correct quote outside of the top ten percent of 5,278 candidates, but all 3 domain experts selected the model's second ranked candidate over the ground-truth quote.\\n\\nA.3 More dataset statistics\\n\\nEach primary source has relevant windows from an average of 112 unique secondary sources, and an average of 16.35% of the sentences in each primary source are quoted in secondary sources. On average, each primary source has 995 corresponding windows in our dataset, and each secondary source produced an average of 9 windows. Figure 2 shows the distribution of quote lengths in RELiC, suggesting that successful models will have to learn to understand both single-sentence and block quotes in context.\\n\\nFigure 2: Distribution of RELiC quote lengths.\\n\\nB Best Model Detailed Results\\n\\nCandidate length does not significantly affect model performance:\\n\\nWe observe in Table A9 that the length of the ground-truth quote and the candidates does not significantly impact model performance\u2014 for a fixed $k$, model performance is within 10% for any candidate length. Model performance is slightly worse for longer candidates of length 4 or 5, and for the shortest single sentence contexts (possibly due to under-specification).\"}"}
{"id": "acl-2022-long-517", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"From Pride and Prejudice, given \\\"Elizabeth displays frustration towards her mother:\\\"\\n\\n1. Elizabeth was again deep in thought, and after a time exclaimed, \\\"To treat in such a manner the godson, the friend, the favourite of his father!\\\"\\n\\n2. Far be it from me,\\\" he presently continued, in a voice that marked his displeasure, \\\"to resent the behaviour of your daughter.\\n\\n3. Her mother's ungraciousness, made the sense of what they owed him more painful to Elizabeth's mind;\\n\\nTable A5: When querying the model using out-of-distribution prompts, number of sentences of the desired candidates can be specified. This table shows the top 3 quotations from the Pride and Prejudice that dense-RELiC returns as evidence for single-sentence candidates. The suitability of the 2-sentence candidates (shown in Table 6) over the single-sentence candidates suggests that contextualizing the quotation embeddings will improve model performance.\\n\\nTable A6: A comparison between datasets in the BEIR benchmark and our RELiC dataset. Ours is the first retrieval dataset in the literary domain, formulating a new task of literary evidence retrieval.\"}"}
{"id": "acl-2022-long-517", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Year | Title            | Author (Translator) | Type     | Language |\\n|------|------------------|---------------------|----------|----------|\\n| 1811 | Sense and Sensibility | Jane Austen         | novel    | English  |\\n| 1814 | Mansfield Park   | Jane Austen         | novel    | English  |\\n| 1818 | Frankenstein     | Mary Shelley        | novel    | English  |\\n| 1837 | The Pickwick Papers | Charles Dickens    | novel    | English  |\\n| 1839 | Nicholas Nickleby | Charles Dickens    | novel    | English  |\\n| 1839 | Oliver Twist     | Charles Dickens    | novel    | English  |\\n| 1843 | A Christmas Carol | Charles Dickens    | novella  | English  |\\n| 1844 | Martin Chuzzlewit | Charles Dickens    | novel    | English  |\\n| 1847 | Jane Eyre        | Charlotte Bront\u00eb    | novel    | English  |\\n| 1847 | Wuthering Heights | Emily Bront\u00eb       | novel    | English  |\\n| 1850 | David Copperfield | Charles Dickens    | novel    | English  |\\n| 1850 | The Scarlet Letter | Nathaniel Hawthorn | novel    | English  |\\n| 1851 | Moby Dick        | Herman Melville    | novel    | English  |\\n| 1852 | Uncle Tom's Cabin | Harriet Beecher Stowe | novel | English  |\\n| 1856 | Madame Bovary    | Gustave Flaubert (Eleanor Marx-Avelin) | novel | French  |\\n| 1857 | Little Dorrit    | Charles Dickens    | novel    | English  |\\n| 1859 | Adam Bede        | George Eliot       | novel    | English  |\\n| 1861 | Great Expectations | Charles Dickens | novel | English  |\\n| 1865 | Alice's Adventures in Wonderland | Lewis Carroll | novel | English  |\\n| 1866 | Crime and Punishment | Fyodor Dostoevsky (Constance Garnett) | novel | Russian |\\n| 1867 | War and Peace | Leo Tolstoy (Garnett) | novel | Russian |\\n| 1871 | Middlemarch | George Eliot | novel | English  |\\n| 1878 | Daisy Miller | Henry James | novella | English |\\n| 1880 | Brothers Karamazov | Fyodor Dostoevsky (Garnett) | novel | Russian |\\n| 1884 | Adventures of Huckleberry Finn | Mark Twain | novel | English  |\\n| 1890 | The Picture of Dorian Gray | Oscar Wilde | novel | English  |\\n| 1893 | Maggie: A Girl of the Streets | Stephen Crane | novella | English |\\n| 1895 | The Red Badge of Courage | Stephen Crane | novel | English  |\\n| 1892 | Iola Leroy | Frances Harper | novel | English  |\\n| 1897 | What Maisie Knew | Henry James | novel | English  |\\n| 1898 | The Turn of the Screw | Henry James | novella | English |\\n| 1899 | The Awakening | Kate Chopin | novel | English  |\\n| 1900 | Sister Carrie | Theodore Dreiser | novel | English  |\\n| 1902 | The Sport of the Gods | Paul Laurence Dunbar | novel | English  |\\n| 1903 | The Ambassadors | Henry James | novel | English  |\\n| 1903 | The Call of the Wild | Jack London | novel | English  |\\n| 1903 | The Souls of Black Folk | W. E. B. Du Bois | collection | English |\\n| 1905 | House of Mirth | Edith Wharton | novel | English  |\\n| 1913 | O Pioneers! | Willa Cather | novel | English  |\\n| 1916 | A Portrait of the Artist as a Young Man | James Joyce | novel | English  |\\n| 1915 | The Rainbow | D. H. Lawrence | novel | English  |\\n| 1918 | My Antonia | Willa Cather | novel | English  |\\n| 1920 | The Age of Innocence | Edith Wharton | novel | English  |\\n| 1920 | This Side of Paradise | F. Scott Fitzgerald | novel | English  |\\n| 1922 | Jacob's Room | Virginia Woolf | novel | English  |\\n| 1922 | Swann's Way | Marcel Proust (C. K. Scott Moncrieff) | novel | French  |\\n| 1925 | An American Tragedy | Theodore Dreiser | novel | English  |\\n| 1925 | Mrs Dalloway | Virginia Woolf | novel | English  |\\n| 1927 | To the Lighthouse | Virginia Woolf | novel | English  |\\n| 1928 | Lady Chatterly's Lover | D. H. Lawrence | novel | English  |\\n| 1932 | Brave New World | Aldous Huxley | novel | English  |\\n| 1936 | Gone with the Wind | Margaret Mitchell | novel | English  |\\n| 1931 | The Waves | Virginia Woolf | novel | English  |\\n| 1945 | Animal Farm | George Orwell | novel | English  |\\n| 1949 | 1984 | George Orwell | novel | English  |\"}"}
{"id": "acl-2022-long-517", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Year | Title          | Author (Translator) | Type          | Language |\\n|------|----------------|---------------------|---------------|----------|\\n| 1815 | Emma           | Jane Austen         | novel         | English  |\\n| 1817 | Northanger Abbey | Jane Austen       | novel         | English  |\\n| 1830 | The Red and the Black | Stendhal (Horace B. Samuel) | novel | French |\\n| 1841 | Barnaby Rudge  | Charles Dickens     | novel         | English  |\\n| 1847 | Agnes Grey     | Anne Bront\u00eb         | novel         | English  |\\n| 1848 | The Tenant of Wildfell Hall | Anne Bront\u00eb | novel | English |\\n| 1854 | Hard Times     | Charles Dickens     | novel         | English  |\\n| 1859 | A Tale of Two Cities | Charles Dickens | novel | English |\\n| 1869 | Little Women   | Louisa May Alcott   | novel         | English  |\\n| 1877 | Anna Karenina  | Leo Tolstoy (Garnett) | novel | Russian |\\n| 1883 | Treasure Island| Robert Louis Stevenson | novel | English |\\n| 1898 | The War of the Worlds | H. G. Wells | novel | English |\\n| 1911 | Ethan Frome    | Edith Wharton       | novel         | English  |\\n| 1915 | The Song of the Lark | Willa Cather | novel | English |\\n| 1920 | Main Street    | Sinclair Lewis      | novel         | English  |\\n| 1922 | Babbitt        | Sinclair Lewis      | novel         | English  |\\n| 1922 | The Garden Party and Other Stories | Katherine Mansfield | collection (fiction) | English |\\n| 1925 | Arrowsmith     | Sinclair Lewis      | novel         | English  |\\n\\n| Year | Title          | Author (Translator) | Type          | Language |\\n|------|----------------|---------------------|---------------|----------|\\n| 1813 | Pride and Prejudice | Jane Austen       | novel         | English  |\\n| 1817 | Persuasion     | Jane Austen         | novel         | English  |\\n| 1899 | Heart of Darkness | Joseph Conrad | novella | English |\\n| 1925 | The Great Gatsby | F. Scott Fitzgerald | novel | English |\\n| 1934 | Tender Is the Night | F. Scott Fitzgerald | novel | English |\\n\\n| # of sents | # instances | recall@k | mean rank | avg. # candidates |\\n|------------|-------------|----------|------------|-------------------|\\n| 1          | 3279        | 8.8      | 16.2       | 21.0              |\\n| 2          | 2028        | 11.0     | 21.5       | 27.4              |\\n| 3          | 1189        | 9.3      | 20.1       | 26.8              |\\n| 4          | 796         | 9.0      | 17.8       | 24.0              |\\n| 5          | 493         | 6.9      | 15.8       | 22.3              |\\n\\nTable A8: Primary sources from which validation and test set windows were derived.\\n\\nTable A9: A breakdown of performance by quote length in sentences of the performance of our best model, the dense retriever with 4 context sentences on each side. All numbers are on the test set of RELiC.\"}"}
