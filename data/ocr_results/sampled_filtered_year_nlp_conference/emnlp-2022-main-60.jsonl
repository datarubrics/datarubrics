{"id": "emnlp-2022-main-60", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction\\n\\nXiaozhi Wang, Yulin Chen, Ning Ding, Hao Peng, Zimu Wang, Yankai Lin, Xu Han, Lei Hou, Juanzi Li, Zhiyuan Liu, Peng Li, Jie Zhou\\n\\nAbstract\\n\\nThe diverse relationships among real-world events, including coreference, temporal, causal, and subevent relations, are fundamental to understanding natural languages. However, two drawbacks of existing datasets limit event relation extraction (ERE) tasks: (1) Small scale. Due to the annotation complexity, the data scale of existing datasets is limited, which cannot well train and evaluate data-hungry models. (2) Absence of unified annotation. Different types of event relations naturally interact with each other, but existing datasets only cover limited relation types at once, which prevents models from taking full advantage of relation interactions. To address these issues, we construct a unified large-scale human-annotated ERE dataset MAVEN-ERE with improved annotation schemes. It contains 103,193 event coreference chains, 1,216 temporal relations, 57,992 causal relations, and 15,841 subevent relations, which is larger than existing datasets of all the ERE tasks by at least an order of magnitude. Experiments show that ERE on MAVEN-ERE is quite challenging, and considering relation interactions with joint learning can improve performances. The dataset and source codes can be obtained from https://github.com/THU-KEG/MAVEN-ERE.\\n\\n1 Introduction\\n\\nCommunicating events is a central function of human languages, and understanding the complex relationships between events is essential to understanding events (Levelt, 1993; Miller and Johnson-Laird, 2013; Pinker, 2013). Thus event relation extraction (ERE) tasks, including extracting event coreference, temporal, causal and subevent relations (Liu et al., 2020b), are fundamental challenges for natural language processing (NLP) and also support various applications (Chaturvedi et al., 2017; Rashkin et al., 2018; Khashabi et al., 2018; Sap et al., 2019; Zhang et al., 2020).\\n\\nDue to the widely acknowledged importance, many efforts have been devoted to developing advanced ERE methods (Liu et al., 2014; Hashimoto et al., 2014; Ning et al., 2017). Recently, data-driven neural models have become the mainstream of ERE methods (Dligach et al., 2017; Aldawsari and Finlayson, 2019; Liu et al., 2020a; Lu and Ng, 2021a). However, these data-driven methods are severely limited by two drawbacks of existing event relation datasets: (1) Small data scale. Due to the high inherent annotation complexity, the data scale of existing human-annotated datasets is limited. From the statistics shown in Table 1, we can see existing popular datasets contain only hundreds of documents and limited numbers of relations, which cannot adequately cover the diverse event semantics.\"}"}
{"id": "emnlp-2022-main-60", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Battle of Sultanabad occurred on Feb. 13, 1812. \u2026 The Persians won the battle by moving faster than the Russians and attacking them near their camp. \u2026 In the end, however, the Persians lost the invasion due to the Russian maneuvering around the Aras River.\\n\\nIn this paper, we construct MAVEN-ERE, the first unified large-scale event relation dataset, based on the previous MAVEN (Wang et al., 2020b) dataset, which is a massive general-domain event detection dataset covering 4,480 English Wikipedia documents and 168 fine-grained event types. As the example in Figure 1, MAVEN-ERE makes up the absence of unified annotation by annotating 4 kinds of event relations in the same documents. MAVEN-ERE has 103,193 event coreference chains, 1,216,217 temporal relations, 57,992 causal relations, and 15,841 subevent relations. To our knowledge, MAVEN-ERE achieves the first million-scale human-annotated ERE dataset. As shown in Table 1, in every ERE task, MAVEN-ERE is larger than existing datasets by at least an order of magnitude, which shall alleviate the limitation of data scale and facilitate developing ERE methods.\\n\\nAs shown in Figure 1, event relations are dense and complex. Hence constructing MAVEN-ERE requires thorough and laborious crowd-sourcing annotation. To ensure affordable time and resource costs, we further develop a new annotation methodology based on O\u2019Gorman et al. (2016), which is the only existing annotation scheme supporting all the relation types. Specifically, we decompose the overall annotation task into multiple sequential stages, which reduces competence requirements for annotators. The overhead of later stages can also be reduced with the results of previous stages. First, we annotate coreference relations so that the later-stage annotations only need to consider one of all the coreferent event mentions. For temporal relation annotation, we develop a new timeline annotation scheme, which avoids laboriously identifying temporal relations for every event pair like previous works (Chambers et al., 2014; Ning et al., 2018b). This new scheme brings much denser annotation results. For every 100 words, MAVEN-ERE has more than 6 times the number of temporal relations as the previous most widely-used dataset MATRES (Ning et al., 2018b). For causal and subevent relation annotation, we set annotation constraints with temporal relations and the relation transitivity to reduce annotation scopes.\\n\\nWe develop strong baselines for MAVEN-ERE based on a widely-used sophisticated pre-trained language model (Liu et al., 2019). Experiments show that: (1) ERE tasks are quite challenging and achieved performances are far from promising; (2) Our large-scale data sufficiently trains the models and brings performance benefits; (3) Considering the relation interactions with straightforwardly joint training improves the performances, which...\"}"}
{"id": "emnlp-2022-main-60", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We also provide some empirical analyses to inspire future works.\\n\\n2 Dataset Construction\\n\\nBased on the event triggers in MA VEN (Wang et al., 2020b), we annotate data for four ERE tasks: extracting event coreference, temporal, causal, and subevent relations. For each task, we introduce its definition, the annotation process, and basic statistics of MA VEN-E RE compared with its typical existing datasets. For the overall statistic comparisons, please refer to appendix A.\\n\\n2.1 Coreference Relation\\n\\nTask Description\\nEvent coreference resolution requires identifying the event mentions referring to the same event. Event mentions are the key texts expressing the occurrences of events. For example, in Figure 1, the \u201cBattle of Sulatnabad\u201d and the later \u201cbattle\u201d are two event mentions referring to the same real-world event, so they have a coreference relation. Like entity coreference resolution, event coreference resolution is important to various applications and is widely acknowledged as more challenging (Choubey and Huang, 2018).\\n\\nAnnotation\\nWe follow the annotation guidelines of O\u2019Gorman et al. (2016) and invite 29 annotators to annotate event coreference relations. The annotators are all trained and pass a qualification test before annotation. Given the documents and highlighted event mentions, the annotators are required to group the coreferent mentions together. The outputs are event coreference chains, each linking a set of different event mentions. Each document is annotated by 3 independent annotators, and the final results are obtained by majority voting. To improve the data quality on top of the original MA VEN and avoid annotation vagueness, we allow the annotators to report if the provided mentions do not express events, and we will delete the mentions reported by all the annotators. The B-Cubed F-1 (Bagga and Baldwin, 1998) between each pair of annotation results is 91% on average, which shows that the annotation consistency is satisfactory.\\n\\nStatistics\\nAfter annotation, we get 103,193 event coreference chains in total. In Table 2, we compare the size of MA VEN-E RE with existing widely-used datasets, including ACE 2005 (Walker et al., 2006), ECB+ (Cybulska and Vossen, 2014), and TAC KBP. Following the setup of previous works (Lu and Ng, 2021a,b), the TAC KBP here includes LDC2015E29, LDC2015E68 and TAC KBP 2015 (Ellis et al., 2015), 2016 (Ellis et al., 2016) and 2017 (Getman et al., 2017). We can see that MA VEN-E RE has much more annotated event coreference chains, which shall benefit event coreference resolution methods.\\n\\n| Dataset    | Doc. # | Mention # | Chain # |\\n|------------|--------|-----------|---------|\\n| ACE 2005   | 599    | 5,349     | 4,090   |\\n| ECB+       | 982    | 14,884    | 9,875   |\\n| TAC KBP    | 1,075  | 29,471    | 19,257  |\\n| MA VEN-E RE| 4,480  | 112,276   | 103,193 |\\n\\nTable 2: Statistics about event coreference relations of MA VEN-E RE and existing widely-used datasets.\\n\\n2.2 Temporal Relation\\n\\nTask Description\\nTemporal relation extraction aims at extracting the temporal relations between events and temporal expressions (TIMEXs). TIMEXs are the definitive references to time within texts. Considering them in temporal relation extraction helps to anchor the relative temporal orders to concrete timestamps. Hence we need to annotate TIMEXs before annotating temporal relations.\\n\\nFollowing the ISO-TimeML standard (Pustejovsky et al., 2010), we annotate four types of TIMEX: DATE, TIME, DURATION, and PREPOSTEXP, but we ignore the QUANTIFIER and SET, since they are harder for crowd-sourcing workers and less helpful for linking events to real-world timestamps. For temporal relations, we follow O\u2019Gorman et al. (2016) and comprehensively set 6 types of temporal relations: BEFORE, CONTAINS, OVERLAP, BEGINS-ON, ENDS-ON, SIMULTANEOUS. Except for SIMULTANEOUS and BEGINS-ON, the relation types are unidirectional, i.e., the head event must start before the tail event in a relation instance.\\n\\nAnnotation\\nIn TIMEX annotation, we invite 112 trained and qualified annotators. Each document is annotated by 3 annotators, and the final results are obtained through majority voting. The average inter-annotator agreement is 78.4% (Fleiss\u2019 kappa).\\n\\nPrevious works (Styler IV et al., 2014; Chambers et al., 2014; Ning et al., 2018b) show that annotating temporal relations is very challenging since densely annotating relations for every event pair is extremely time-consuming, and the expressions of temporal relations are often vague. Hence we design a sophisticated annotation scheme inspired...\"}"}
{"id": "emnlp-2022-main-60", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"by the multi-axis scheme of Ning et al. (2018b) and the time-anchoring scheme of Reimers et al. (2016). As illustrated in Figure 1 (c), instead of identifying relations for every single event pair, we ask the annotators to sort the beginnings and endings of events and TIMEXs on a timeline. Thus the annotators only need to consider how to arrange the bounding points of temporally close events and TIMEXs, and the relations between the events and TIMEXs on the timeline can be automatically inferred from their relative positions. However, due to the narrative vagueness, the temporal relations between some events cannot be clearly determined from contexts, such as the \u201cmaneuvering\u201d and \u201cattacking\u201d in Figure 1. As discussed by Ning et al. (2018b), this often happens when expressing opinions, intentions, and hypotheses. In these cases, we allow the annotators to create sub-timelines, and we treat events on different timelines as no temporal relations. An event may be placed on multiple timelines like the \u201clost\u201d in Figure 1.\\n\\nWith this annotation scheme, we can get high-quality temporal relations for all the pairs at an affordable cost with no need to reduce the annotation scope like previous works (Chambers et al., 2014; Ning et al., 2018b) which only annotate events within adjacent sentences. To control data quality and resource costs, each document will be annotated by a well-trained annotator at first. Then an expert will check and revise the annotation results. We invite 49 annotators and 17 experts in temporal relation annotation. To measure data quality, we randomly sample 100 documents and annotate them twice in the above pipeline. The average agreement is 67.8% (Cohen\u2019s kappa).\\n\\nStatistics\\n\\nWe obtain 25,843 TIMEXs, including 20,654 DATE, 4,378 DURATION, 793 TIME, and 18 PREPOSTEXP. Based on the events and TIMEXs, we annotate 1,216,217 temporal relations in total, including 1,042,709 BEFORE, 152,702 CONTAINS, 9,937 SIMULTANEOUS, 9,850 OVERLAP, 639 BEGINS-ON, and 380 ENDS-ON. We can see the data unbalance among types is serious. To ensure that the created dataset well reflects the real-world data distribution, we do not intervene the label distribution and keep the unbalanced distribution in MAVEN-E. This poses a challenge for future temporal relation extraction models.\\n\\nIn Table 3, we compare the size of MAVEN-E with existing widely used datasets, including TimeBank 1.2 (Pustejovsky et al., 2003b), TempEval-3\u2217, RED (O'Gorman et al., 2016), TB-Dense (Chambers et al., 2014), MATRES (Ning et al., 2018b), and TCR (Ning et al., 2018a). MAVEN-E is orders of magnitude larger than existing datasets and is the first million-scale temporal relation extraction dataset to our knowledge. Our timeline annotation scheme also brings denser annotation results. For every 100 words, MAVEN-E has 95.3 temporal relations, while MATRES has 14.3. We believe a leap in data size could significantly facilitate temporal relation extraction research and promote broad temporal reasoning applications.\\n\\n2.3 Causal Relation Task Description\\n\\nUnderstanding causality is a long-standing goal of artificial intelligence. Causal relation extraction, which aims at extracting the causal relations between events, is an important task to evaluate it. To enable crowd-sourcing annotation, we do not adopt the complicated causation definitions (Dunietz et al., 2017) but instead annotate two types of straightforward and clear causal relation types: CAUSE and PRECONDITION following previous discussions (Ikuta et al., 2014; O'Gorman et al., 2016).\\n\\nCAUSE is defined as \u201cthe tail event is inevitable given the head event\u201d, and PRECONDITION is defined as \u201cthe tail event would not have happened if the head event had not happened\u201d (Ikuta et al., 2014). Note that we allow to annotate causal relations for negative events, which are the events that did not actually happen. In this way, we also cover the negative causation discussed in previous literatures (Mirza et al., 2014).\\n\\nAnnotation\\n\\nConsidering the temporal nature of causality, we limit the annotation scope to event pairs with BEFORE and OVERLAP relations labeled in temporal annotation. To further reduce annotation overhead, we ask the annotators to consider the transitivity of causal relations and make minimal\"}"}
{"id": "emnlp-2022-main-60", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Statistics about causal relations (C-Links) of MAVEN-E and existing widely-used datasets.\\n\\nAnnotations. That is if \\\"A CAUSE/PRECONDITION B\\\" and \\\"B CAUSE/PRECONDITION C\\\" have been annotated, the causal relation between A and C can be discarded. Furthermore, we annotate causal relations and subevent relations in the same stage so that we can involve subevent relations in the transitivity rules. This means that you can discard the causal relations between A and C if you have (1) \\\"A CAUSES/PRECONDITIONS B and C SUBEVENT B\\\" or (2) \\\"A SUBEVENT B and B PRECONDITION C\\\". The discarded relations are then automatically completed after human annotation. We invite trained and qualified annotators, and each document is annotated by 3 independent annotators. The final results are obtained through majority voting. The average inter-annotator agreement for causal relations is 69.5% (Cohen's kappa).\\n\\nStatistics\\n\\nWe obtain 57,992 causal relations, including 10,617 CAUSE and 47,375 PRECONDITION.\\n\\nTable 4 shows the size of MAVEN-E and existing widely-used datasets, including BECauSE 2.0 (Dunietz et al., 2017), CaTeRS (Mostafazadeh et al., 2016), RED (O'Gorman et al., 2016), Causal-TB (Mirza et al., 2014), and EventStoryLine (Caselli and Vossen, 2017). MAVEN-E is still much larger than all the existing datasets.\\n\\n2.4 Subevent Relation\\n\\nTask Description\\n\\nSubevent relation extraction requires identifying whether event A is a subevent of event B. \\\"A SUBEVENT B\\\" means that A is a component part of B and spatiotemporally contained by B (Hovy et al., 2013; Glava\u0161 et al., 2014; O'Gorman et al., 2016). Subevent relations organize the unconnected events into hierarchical structures, which support the event understanding applications (Aldawsari and Finlayson, 2019).\\n\\nAnnotation\\n\\nWe limit the annotation scope to event pairs with CONTAINS relations considering the inherent temporal containment property in...\"}"}
{"id": "emnlp-2022-main-60", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Distribution of documents with different rates of transitivity inferrable temporal and causal relations.\\n\\nthe same or adjacent sentences and ignore long-distance temporal relations, which are also informative (Reimers et al., 2016; Naik et al., 2019). This also limits the causal relation datasets based on them like Causal-TB. As shown in Table 6, with the help of our timeline annotation scheme, MAVEN-E has much more long-distance temporal and causal relations compared to existing datasets, which can better support real-world applications and poses new challenges for ERE models.\\n\\nFor coreference relations, MAVEN-E has shorter average distances and much higher short-distance rates. This is because MAVEN (Wang et al., 2020b) covers much more generic events and annotates much denser event mentions. For comparison, MAVEN-E has 8.8 event mentions per 100 words, while this number is 1.8 and 4.2 for ACE 2005 and TAC KBP, respectively. For subevent relations, the distributions of HiEve and MAVEN-E are similar, and we think HiEve has a longer average distance because of its longer average document length (333 vs. 284 words).\\n\\n3.2 Relation Transitivity\\n\\nTemporal and causal relations follow a certain transitivity rule (Allen, 1983; Gerevini and Schubert, 1995), e.g., if there exists \u201cA BEFORE B\u201d and \u201cB BEFORE C\u201d, \u201cA BEFORE C\u201d also holds. Previous ERE methods often use these natural transitivity rules as constraints in post-processing (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Ning et al., 2018a) and training (Wang et al., 2020a). Here we estimate the importance of considering transitivity in handling MAVEN-E by counting how many relations can be inferred from other relations with transitivity rules. The detailed transitivity rules that we consider are shown in appendix B. Overall, 88.8% temporal relations and 23.9% causal relations are inferrable with transitivity rules. We further plot the distribution of documents containing different rates of transitivity inferrable relations in Figure 2. We can see that more than 60% temporal relations can be inferred with transitivity rules for most of the documents. The transitivity inferrable causal relations, although significantly less, also take up a substantial proportion. These results suggest that considering the relation transitivity is helpful for handling MAVEN-E, and we encourage future works to explore it.\\n\\n4 Experiments and Analyses\\n\\nTo demonstrate the challenges of MAVEN-E and analyze the potential future directions for ERE, we conduct a series of experiments.\\n\\n4.1 Experiment Setup\\n\\nModel Considering that pre-trained language models (PLMs) have dominated broad NLP tasks, we adopt a widely-used PLM RoBERTa BASE (Liu et al., 2019) as the backbone and build classification models on top of it, which provides simple but strong baselines for the 4 ERE tasks. To extract the event relations in a document, we encode the whole document with RoBERTa BASE and set an additional classification head taking the contextualized representations at the positions of different event pairs\u2019 corresponding event triggers as inputs. Then we fine-tune the model to classify relation labels. Besides training the 4 tasks independently, we also set a straightforward jointly training model combining the losses of the 4 tasks, which is to demonstrate the benefits of our unified annotation. The implementation details are shown in appendix C.\\n\\nBenchmarks To assess the challenges of MAVEN-E, we also include existing most widely-used datasets of the 4 ERE tasks into evaluations, including ACE 2005, TAC KBP, TB-Dense, MATRES, TCR, Causal-TB, EventStoryLine, and HiEve. Following previous works (Ning et al., 2018a), TCR is used only as an additional test set for models developed on MATRES. Due to the small data scale of Causal-TB and EventStoryLine, previous works (Gao et al., 2019; Cao et al., 2021) typically adopt 5-fold cross-validation on them and only do causality identification, which ignores the directions of causal relations. In our...\"}"}
{"id": "emnlp-2022-main-60", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We report averages and standard deviations over TRES and TCR are significantly higher than that on previous datasets, this comes from TB-Dense's small data scale. (2) Directly training on the test results (F-1, MUC for ACE 2005, BLY, BLANC, CEAF) indicates the benefits of our larger data scale. (3) Compared with existing datasets, M\\ndenotes higher values among the two results on M\\n3\\n\\neworks on M\\n3\\n\\nevaluation on the two datasets, we also do cross-\\n4.4 Analysis on Data Scale\\n\\neurons. (4) Straightforwardly joint training on the\\ntasks can bring certain improvements, especially on\\n4\\n\\neady for NLP models and needs more research\\ndiverse and complex event relations is a huge chal-\\nding temporal, causal, and subevent relations on M\\n3\\n\\ne RNN networks, which cannot well train the model. (3)\\n3\\n\\neral statistics in appendix D.\\n\\nFor temporal relations, the performances on MA-\\n3\\n\\nection. For the other\\ntasks, we adopt the standard\\n3\\n\\neases: for event coreference resolu-\\n3\\n\\ne, BLY, BLANC, CEAF\\n\\nMetrics\\n\\nFollowing previous works (Choubey and\\n3\\n\\nevalution in accordance with M\\n3\\n-ERE and existing datasets.\\n\\nFor extracting coreference, causal and subevent\\n3\\n\\ndemonstrates that understanding the\\nd complex ERE tasks.\\n\\nThe challenge of temporal understanding. The perfor-\\n4\\n\\nglobal temporal relations within documents, and\\n\\nAnnotation scheme, M\\n4\\n\\ncompares the performances on M\\n4\\n\\n4.2 Experimental Result\\n\\nError bars indicate standard deviations over\\n5\\n\\ntype and micro-averaged precision, recall and F-1 metrics.\\n\\n3\\n\\nTo assess the benefits brought by larger data scale and evaluate whether MAVEN-ERE provides\\n3\\n\\ne enough training data, we conduct an ablation study\\n3\\n\\nTable 7: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 8: Performances (%) of RoBERTa\\n\\nTable 3: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 4: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 5: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 6: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 1: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 2: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 9: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 10: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 11: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 12: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 13: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 14: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 15: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 16: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 17: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 18: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 19: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 20: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 21: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 22: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 23: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 24: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 25: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 26: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 27: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 28: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 29: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 30: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 31: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 32: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 33: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 34: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 35: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 36: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 37: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 38: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 39: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 40: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 41: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 42: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 43: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 44: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 45: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 46: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 47: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 48: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 49: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 50: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 51: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 52: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 53: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 54: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 55: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 56: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 57: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 58: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 59: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 60: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 61: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 62: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 63: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 64: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 65: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 66: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 67: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 68: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 69: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 70: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 71: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 72: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 73: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 74: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 75: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 76: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 77: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 78: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 79: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 80: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 81: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 82: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 83: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 84: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 85: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 86: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 87: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 88: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 89: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 90: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 91: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 92: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 93: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 94: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 95: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 96: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 97: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 98: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 99: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 100: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 101: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 102: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 103: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 104: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 105: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 106: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 107: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 108: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 109: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 110: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 111: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 112: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 113: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 114: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 115: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 116: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 117: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 118: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 119: Event coreference resolution performances (%) of RoBERTa\\n\\nTable 120: Event coreference resolution performances (%) of RoBERTa\"}"}
{"id": "emnlp-2022-main-60", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3 shows how RoBERTa BASE's test performance changes along with different proportions of data used in training. We can see that increasing training data scale brings substantially higher and stabler performances, which shows the benefits of MAVEN-ERE's large scale. The performance improvements are quite marginal at the scale of MAVEN-ERE. It indicates that MAVEN-ERE is generally sufficient to train ERE models.\\n\\n4.4 Analysis on Distance between Events\\nLike \u00a7 3.1, we analyze how the distances between related events influence model performances. We sample a jointly-trained model and see how it performs on data with different distances in Table 9. Since the evaluation of event coreference resolution is based on clusters, which cannot be divided by distances, we only study the other 3 tasks here.\\n\\nFor causal and subevent relations, performances on data with longer distances are lower, which intuitively suggests that modeling long-range dependency is still important to ERE, although the PLMs are effective. However, for temporal relations, data with longer distances are easier. We think this is because event pairs with longer narrative distances are typically also with longer temporal distances, which makes their relations easier to classify.\\n\\n4.5 Error Analysis\\nWe further analyze the errors in the predictions of a jointly trained model to provide insights for further improvements. Considering the event coreference resolution task has reached a high performance and its different cluster-based evaluation, we only analyze the other 3 tasks. The results are shown in Table 10. We can see that identification mistakes (false positive and false negative) make up the majority of all the mistakes. It indicates that the most important challenge for ERE is still identifying whether there is a relation or not. Furthermore, like \u00a7 3.2, we analyze how many mistakes can be fixed by applying transitivity rules to other predictions.\\n\\n| Type    | FP (%) | FN (%) | Transitivity Fixable (%) |\\n|---------|--------|--------|--------------------------|\\n| Temporal| 38.78  | 53.75  | 0                        |\\n| Causal  | 37.73  | 59.88  | 0                        |\\n| Subevent| 48.64  | 51.36  | \u2212                        |\\n\\nTable 10: Rates (%) of different kinds of mistakes in RoBERTa BASE predictions. FP denotes false positive. FN denotes false negative. These transitivity fixable mistakes only account for small proportions, which suggests that sophisticated models have imperfectly but substantially learned the transitivity rules from massive data.\\n\\n5 Related Work\\nSince the fundamental role of understanding event relations in NLP, various ERE datasets have been constructed. Event coreference relations are often covered in event extraction datasets like MUC (Grisman and Sundheim, 1996), ACE (Walker et al., 2006) and TAC KBP (Ellis et al., 2015, 2016; Getman et al., 2017). Besides, some datasets focus on unrestricted coreference resolution and ignore event semantic types, like OntoNotes (Pradhan et al., 2007) and ECB datasets (Bejan and Harabagiu, 2008; Lee et al., 2012; Cybulska and Vossen, 2014). Following the TimeML specification (Pustejovsky et al., 2003a, 2010), established temporal relation datasets like TimeBank (Pustejovsky et al., 2003b) and TempEval (Verhagen et al., 2009, 2010; UzZaman et al., 2013) have been constructed. However, these works exhibit low annotation agreements and efficiency issues. Ning et al. (2018b) develop a multi-axis annotation scheme based on the dense scheme of Chambers et al. (2014) to alleviate them, and Reimers et al. (2016) propose to anchor the event starting and ending points to specific time. Our timeline annotation scheme is inspired by them. Based on the temporal understanding, causal relation datasets (Do et al., 2011; Mirza et al., 2014; Mostafazadeh et al., 2016; Dunietz et al., 2017; Caselli and Vossen, 2017; Tan et al., 2022) are developed. To organize events into hierarchies, subevent relation datasets (Hovy et al., 2013; Glava\u0161 et al., 2014) are collected.\\n\\nHowever, the scale of these datasets is limited, and different types of relations are rarely integrated into one dataset. Some datasets (Hovy et al., 2013; Mirza et al., 2014; Glava\u0161 et al., 2014; Caselli and Vossen, 2017; Minard et al., 2016; Ning et al., 2018a) annotate two or three kinds of relations.\"}"}
{"id": "emnlp-2022-main-60", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"O\u2019Gorman et al. (2016) and Hong et al. (2016) provide unified annotation schemes for within-document and cross-document event relations, respectively, but their constructed datasets are also small. We construct MAVEN-ERE referring to the guidelines of O\u2019Gorman et al. (2016).\\n\\n6 Conclusion and Future Work\\n\\nWe present MAVEN-ERE, a unified large-scale dataset for event coreference, temporal, causal, and subevent relations, which significantly alleviates the small scale and absence of unified annotation issues of previous datasets. Experiments show that real-world event relation extraction is quite challenging and may be improved by jointly considering multiple relation types and better modeling long-range dependency. In the future, we will extend the dataset to more scenarios like covering more event-related information and languages.\\n\\nLimitations\\n\\nThe most important limitation of MAVEN-ERE is that it only covers English documents, which is inherited from the original MAVEN (Wang et al., 2020b) dataset. This limits the linguistic features covered by MAVEN-ERE and the scope of applications built on it. We encourage future works to explore (1) develop models for the low-resource languages by applying multilingual transfer learning techniques to MAVEN-ERE; (2) annotate native datasets for the low-resource languages with the annotation schemes of MAVEN-ERE. Another limitation is that MAVEN-ERE only covers the within-document event relations. Future works may extend MAVEN-ERE to cross-document event relations with the help of existing explorations (Cybulska and Vossen, 2014; Hong et al., 2016).\\n\\nEthical Considerations\\n\\nThis paper presents a new dataset, and we discuss some related ethical considerations here. (1) Intellectual property. The original MAVEN dataset is shared under the CC BY-SA 4.0 license and the Wikipedia corpus is shared under the CC BY-SA 3.0 license. They are both free for research use, and we develop MAVEN-ERE with the consent of the authors of MAVEN. (2) Worker Treatments. We hire the annotators from multiple professional data annotation companies and fairly pay them with agreed salaries and workloads. All employment is under contract and in compliance with local regulations. (3) Controlling Potential Risks. Since the texts in MAVEN-ERE do not involve private information and annotating event relations does not require many judgments about social issues, we believe MAVEN-ERE does not create additional risks. To ensure it, we manually checked some randomly sampled data and did not note risky issues.\\n\\nAcknowledgements\\n\\nThis work is supported by the New Generation Artificial Intelligence of China (2020AAA0106501), the Institute for Guo Qiang, Tsinghua University (2019GQB0003), the Pattern Recognition Center, WeChat AI, Tencent Inc, and the Tsinghua University - Siemens Ltd., China Joint Research Center for Industrial Intelligence and Internet of Things. We thank all the annotators for their efforts and the anonymous reviewers for their valuable comments.\\n\\nReferences\\n\\nMohammed Aldawsari and Mark Finlayson. 2019. Detecting subevents using discourse and narrative features. In Proceedings of ACL, pages 4780\u20134790.\\n\\nJames F. Allen. 1983. Maintaining knowledge about temporal intervals. Commun. ACM, 26(11):832\u2013843.\\n\\nAmit Bagga and Breck Baldwin. 1998. Entity-Based Cross-Document Coreferencing Using the Vector Space Model. In Proceedings of ACL-COLING, pages 79\u201385.\\n\\nCosmin Bejan and Sanda Harabagiu. 2008. A linguistic resource for discovering event structures and resolving event coreference. In Proceedings of LREC.\\n\\nPengfei Cao, Xinyu Zuo, Yubo Chen, Kang Liu, Jun Zhao, Yuguang Chen, and Weihua Peng. 2021. Knowledge-enriched event causality identification via latent structure induction networks. In Proceedings of ACL-IJCNLP, pages 4862\u20134872.\\n\\nTommaso Caselli and Piek Vossen. 2017. The event StoryLine corpus: A new benchmark for causal and temporal relation extraction. In Proceedings of the Events and Stories in the News Workshop, pages 77\u201386.\\n\\nNathanael Chambers. 2013. Event Schema Induction with a Probabilistic Entity-Driven Model. In Proceedings of EMNLP, pages 1797\u20131807.\\n\\nNathanael Chambers, Taylor Cassidy, Bill McDowell, and Steven Bethard. 2014. Dense event ordering.\"}"}
{"id": "emnlp-2022-main-60", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with a multi-pass architecture.\\nNathanael Chambers and Daniel Jurafsky. 2008. Jointly combining implicit constraints improves temporal ordering. In Proceedings of EMNLP, pages 698\u2013706.\\nSnigdha Chaturvedi, Haoruo Peng, and Dan Roth. 2017. Story comprehension for predicting what happens next. In Proceedings of EMNLP, pages 1603\u20131614.\\nPrafulla Kumar Choubey and Ruihong Huang. 2017. Event coreference resolution by iteratively unfolding inter-dependencies among events. In Proceedings of EMNLP, pages 2124\u20132133.\\nPrafulla Kumar Choubey and Ruihong Huang. 2018. Improving event coreference resolution by modeling correlations between event coreference chains and document topic structures. In Proceedings of ACL, pages 485\u2013495.\\nAgata Cybulska and Piek Vossen. 2014. Using a sledgehammer to crack a nut? lexical diversity and event coreference resolution. In Proceedings of LREC, pages 4545\u20134552.\\nPascal Denis and Philippe Muller. 2011. Predicting globally-coherent temporal structures from texts via endpoint inference and graph decomposition. In Proceedings of IJCAI, page 1788\u20131793.\\nDmitriy Dligach, Timothy Miller, Chen Lin, Steven Bethard, and Guergana Savova. 2017. Neural temporal relation extraction. In Proceedings of EACL, pages 746\u2013751.\\nQuang Do, Yee Seng Chan, and Dan Roth. 2011. Minimally supervised event causality identification. In Proceedings of EMNLP, pages 294\u2013303.\\nJesse Dunietz, Lori Levin, and Jaime Carbonell. 2017. The BECauSE corpus 2.0: Annotating causality and overlapping relations. In Proceedings of the 11th Linguistic Annotation Workshop, pages 95\u2013104.\\nJoe Ellis, Jeremy Getman, Dana Fore, Neil Kuster, Zhiyi Song, Ann Bies, and Stephanie M Strassel. 2015. Overview of linguistic resources for the TAC KBP 2015 evaluations: Methodologies and results. In TAC.\\nJoe Ellis, Jeremy Getman, Dana Fore, Neil Kuster, Zhiyi Song, Ann Bies, and Stephanie M Strassel. 2016. Overview of Linguistic Resources for the TAC KBP 2016 Evaluations: Methodologies and Results. In TAC.\\nLei Gao, Prafulla Kumar Choubey, and Ruihong Huang. 2019. Modeling document-level causal structures for event causal relation identification. In Proceedings of NAACL-HLT, pages 1808\u20131817.\\nAlfonso Gerevini and Lenhart Schubert. 1995. Efficient algorithms for qualitative reasoning about time. Artificial Intelligence, 74(2):207\u2013248.\\nJeremy Getman, Joe Ellis, Zhiyi Song, Jennifer Tracey, and Stephanie Strassel. 2017. Overview of linguistic resources for the tac kbp 2017 evaluations: Methodologies and results. In TAC.\\nGoran Glava\u0161, Jan \u0160najder, Marie-Francine Moens, and Parisa Kordjamshidi. 2014. HiEve: A corpus for extracting event hierarchies from news stories. In Proceedings of LREC, pages 3678\u20133683.\\nRalph Grishman and Beth Sundheim. 1996. Message understanding conference- 6: A brief history. In Proceedings of COLING.\\nChikara Hashimoto, Kentaro Torisawa, Julien Kloetzer, Motoki Sano, Istv\u00e1n Varga, Jong-Hoon Oh, and Yutaka Kidawara. 2014. Toward future scenario generation: Extracting event causality exploiting semantic relation, context, and association features. In Proceedings of ACL, pages 987\u2013997.\\nYu Hong, Tongtao Zhang, Tim O'Gorman, Sharone Horowit-Hendler, Heng Ji, and Martha Palmer. 2016. Building a cross-document event-event relation corpus. In Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with ACL 2016 (LAW-X 2016), pages 1\u20136.\\nEduard Hovy, Teruko Mitamura, Felisa Verdejo, Jun Araki, and Andrew Philpot. 2013. Events are not simple: Identity, non-identity, and quasi-identity. In Proceedings of Workshop on Events: Definition, Detection, Coreference, and Representation, pages 21\u201328.\\nRei Ikuta, Will Styler, Mariah Hamang, Tim O'Gorman, and Martha Palmer. 2014. Challenges of adding causation to richer event descriptions. In Proceedings of the Second Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 12\u201320.\\nMandar Joshi, Omer Levy, Luke Zettlemoyer, and Daniel Weld. 2019. BERT for coreference resolution: Baselines and analysis. In Proceedings of EMNLP-IJCNLP, pages 5803\u20135808.\\nDaniel Khashabi, Tushar Khot, Ashish Sabharwal, and Dan Roth. 2018. Question answering as global reasoning over semantic abstractions. In Proceedings of AAAI.\\nDiederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In Proceedings of ICLR.\\nHeeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of EMNLP-CoNLL, pages 489\u2013500.\\nWillem JM Levelt. 1993. Speaking: From intention to articulation. MIT press.\\nJian Liu, Yubo Chen, and Jun Zhao. 2020a. Knowledge enhanced event causality identification with mention masking generalizations. In Proceedings of IJCAI, pages 3608\u20133614.\"}"}
{"id": "emnlp-2022-main-60", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kang Liu, Yubo Chen, Jian Liu, Xinyu Zuo, and Jun Zhao. 2020b. Extracting events and their relations from texts: A survey on recent research progress and challenges. *AI Open*, 1:22\u201339.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Manimir Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. *CoRR*, abs/1907.11692.\\n\\nZhengzhong Liu, Jun Araki, Eduard Hovy, and Teruko Mitamura. 2014. Supervised within-document event coreference using information propagation. In Proceedings of LREC, pages 4539\u20134544.\\n\\nJing Lu and Vincent Ng. 2021a. Constrained multi-task learning for event coreference resolution. In Proceedings of NAACL-HLT, pages 4504\u20134514.\\n\\nJing Lu and Vincent Ng. 2021b. Conundrums in event coreference resolution: Making sense of the state of the art. In Proceedings of EMNLP, pages 1368\u20131380.\\n\\nYaojie Lu, Hongyu Lin, Jialong Tang, Xianpei Han, and Le Sun. 2022. End-to-end neural event coreference resolution. *Artificial Intelligence*, 303:103632.\\n\\nXiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of EMNLP-HLT, pages 25\u201332.\\n\\nGeorge A Miller and Philip N Johnson-Laird. 2013. *Language and perception*. In *Language and Perception*. Harvard University Press.\\n\\nAnne-Lyse Minard, Manuela Speranza, Ruben Urizar, Bego\u00f1a Altuna, Marieke van Erp, Anneleen Schoen, and Chantal van Son. 2016. MEANTIME, the NewsReader multilingual event and time corpus. In Proceedings of LREC, pages 4417\u20134422.\\n\\nParamita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating causality in the TempEval-3 corpus. In Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL), pages 10\u201319.\\n\\nNasrin Mostafazadeh, Alyson Grealish, Nathanael Chambers, James Allen, and Lucy Vanderwende. 2016. CaTeRS: Causal and temporal relation scheme for semantic annotation of event structures. In Proceedings of the Fourth Workshop on Events, pages 51\u201361.\\n\\nAakanksha Naik, Luke Breitfeller, and Carolyn Rose. 2019. TDDiscourse: A dataset for discourse-level temporal ordering of events. In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 239\u2013249.\\n\\nQiang Ning, Zhili Feng, and Dan Roth. 2017. A structured learning approach to temporal relation extraction. In Proceedings of EMNLP, pages 1027\u20131037.\\n\\nQiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018a. Joint reasoning for temporal and causal relations. In Proceedings of ACL, pages 2278\u20132288.\\n\\nQiang Ning, Sanjay Subramanian, and Dan Roth. 2019. An improved neural baseline for temporal relation extraction. In Proceedings of EMNLP-IJCNLP, pages 6203\u20136209.\\n\\nQiang Ning, Hao Wu, and Dan Roth. 2018b. A multi-axis annotation scheme for event temporal relations. In Proceedings of ACL, pages 1318\u20131328.\\n\\nTim O\u2019Gorman, Kristin Wright-Bettner, and Martha Palmer. 2016. Richer event description: Integrating event coreference with temporal, causal and bridging annotation. In Proceedings of CNS, pages 47\u201356.\\n\\nSteven Pinker. 2013. *Learnability and Cognition, new edition: The Acquisition of Argument Structure*. MIT press.\\n\\nSameer S. Pradhan, Lance Ramshaw, Ralph Weischedel, Jessica MacBride, and Linnea Micciulla. 2007. Unrestricted coreference: Identifying entities and events in ontonotes. In Proceedings of ICSC, pages 446\u2013453.\\n\\nJames Pustejovsky, Jos\u00e9 M. Castano, Robert Ingria, Roser Saur\u00ed, Robert J. Gaizauskas, Andrea Setzer, Graham Katz, and Dragomir R. Radev. 2003a. Timeml: Robust specification of event and temporal expressions in text. *New directions in question answering*, 3:28\u201334.\\n\\nJames Pustejovsky, Patrick Hanks, Roser Saur\u00ed, Andrew See, Rob Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, and Marcia Lazo. 2003b. The TimeBank corpus. *Proceedings of Corpus Linguistics*.\\n\\nJames Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent Romary. 2010. ISO-TimeML: An international standard for semantic annotation. In Proceedings of LREC.\\n\\nHannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith, and Yejin Choi. 2018. Event2Mind: Commonsense inference on events, intents, and reactions. In Proceedings of ACL, pages 463\u2013473.\\n\\nM. Recasens and E. Hovy. 2011. Blanc: Implementing the rand index for coreference evaluation. *Nat. Lang. Eng.*, 17(4):485\u2013510.\\n\\nNils Reimers, Nazanin Dehghani, and Iryna Gurevych. 2016. Temporal anchoring of events for the TimeBank corpus. In Proceedings of ACL, pages 2195\u20132204.\\n\\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chandria Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, and Yejin Choi. 2019. ATOMIC: an atlas of machine commonsense for if-then reasoning. In Proceedings of AAAI, pages 3027\u20133035.\"}"}
{"id": "emnlp-2022-main-60", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-60", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA Dataset Statistics Comparison\\n\\nWe show the detailed statistics of MAVEN-ERE and other existing widely-used datasets in Table 12.\\n\\nFor the TAC KBP data, we follow the setup of Lu and Ng (2021b), which includes TAC KBP 2015 (Ellis et al., 2015), 2016 (Ellis et al., 2016), 2017 (Getman et al., 2017) as well as LDC2015E29 and LDC2015E68. For the multilingual datasets, we only report their statistics of the English parts in accordance with MAVEN-ERE.\\n\\nB Transitivity Rules\\n\\nTable 11 shows the transitivity rules of temporal and causal relations that we consider in \u00a7 3.2. \\\"Relation_1 + Relation_2 = Relation_3\\\" means if there exists \\\"A Relation_1 B\\\" and \\\"B Relation_2 C\\\", \\\"A Relation_3 C\\\" also holds.\\n\\nTemporal Transitivity Rules\\n\\nBEFORE + BEFORE = BEFORE\\nBEFORE + CONTAINS = BEFORE\\nBEFORE + SIMULTANEOUS = BEFORE\\nBEFORE + OVERLAP = BEFORE\\nBEFORE + BEGINS-ON = BEFORE\\nBEFORE + ENDS-ON = BEFORE\\nCONTAINS + CONTAINS = CONTAINS\\nCONTAINS + SIMULTANEOUS = CONTAINS\\nSIMULTANEOUS + SIMULTANEOUS = SIMULTANEOUS\\nSIMULTANEOUS + BEFORE = BEFORE\\nSIMULTANEOUS + CONTAINS = CONTAINS\\nSIMULTANEOUS + OVERLAP = OVERLAP\\nSIMULTANEOUS + BEGINS-ON = BEGINS-ON\\nSIMULTANEOUS + ENDS-ON = ENDS-ON\\nOVERLAP + BEFORE = BEFORE\\nOVERLAP + SIMULTANEOUS = OVERLAP\\nBEGINS-ON + BEGINS-ON = BEGINS-ON\\nBEGINS-ON + SIMULTANEOUS = BEGINS-ON\\nSIMULTANEOUS + BEFORE = BEFORE\\nSIMULTANEOUS + CONTAINS = CONTAINS\\nSIMULTANEOUS + OVERLAP = OVERLAP\\nSIMULTANEOUS + BEGINS-ON = BEGINS-ON\\nSIMULTANEOUS + ENDS-ON = ENDS-ON\\n\\nCausal Transitivity Rules\\n\\nCAUSE + CAUSE = CAUSE\\nCAUSE + PRECONDITION = PRECONDITION\\nPRECONDITION + PRECONDITION = PRECONDITION\\n\\nTable 11: Relation transitivity rules considered.\\n\\nC Implementation Details\\n\\nWe implement the RoBERTa BASE model using the Huggingface's Transformers library (Wolf et al., 2020). RoBERTa BASE contains 110 M parameters, and we add a two-layer perceptron with 150 hidden dimensions and 0.2 dropout rate as the classification head. We use the standard cross-entropy loss for event temporal, causal, and subevent relation extraction tasks. For event coreference resolution, we follow the design of Joshi et al. (2019). We use the Adam (Kingma and Ba, 2014) optimizer to train the models and set 200 warmup steps. For independently trained models, we set the learning rates as $1 \\\\times 10^{-4}$ and $1 \\\\times 10^{-5}$ for the classification head and the RoBERTa BASE encoder. For jointly trained models, the learning rates are $3 \\\\times 10^{-4}$ and $2 \\\\times 10^{-5}$ for the classification head and the encoder, respectively. We set the factors as 0.4, 2.0, 4.0, and 4.0 for the losses of coreference, temporal, causal, and subevent relations. These hyper-parameters are manually tuned with 10 runs and selected with F-1 scores. We use a GeForce RTX 3090 GPU to run the experiments. Average runtimes for an experiment are about 2.2, 2.3, 1.1, 0.5, and 3.4 hours for coreference ERE, temporal ERE, causal ERE, subevent ERE, and joint training.\\n\\nIn evaluation, we implement the standard precision, recall, and F-1 scores with the scikit-learn toolkit. For event coreference resolution, we implement the evaluation metrics referring to https://github.com/kentonl/e2e-coref.\"}"}
{"id": "emnlp-2022-main-60", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In event temporal relation extraction, we follow the splits of Ning et al. (2019) and Tan et al. (2021). The detailed statistics are shown in Table 14.\\n\\nIn event causal relation extraction, we follow previous works (Gao et al., 2019; Cao et al., 2021) to do 5-fold cross-validation on Causal-TB and EventStoryLine. The statistics for MAVEN-E are shown in Table 15.\\n\\nIn subevent relation extraction, we split HiEve following previous works (Zhou et al., 2020; Wang et al., 2021). The statistics are shown in Table 16.\\n\\nDiscussions on Genre Diversity\\nMAVEN-E inherits all the documents of MAVEN (Wang et al., 2020b), which are all Wikipedia articles. One may wonder if MAVEN-E are diverse enough in genre and topic and if the ERE skills learned from the large-scale MAVEN-E can transfer to other ERE tasks (datasets). First, the original MAVEN work shows that the 4,480 documents cover 90 topics, such as Military conflict, Concert tour, etc. Hence we believe MAVEN-E also exhibits a good coverage for general-domain topics. Second, we conduct cross-dataset transfer experiments following Wang et al. (2020b). By further fine-tuning the RoBERTa BASE models previously trained on MAVEN-E, the (MUC) F1 scores increase 0.7%, 0.5%, 0.8%, 0.5%, 1.1%, 16.0% on ACE 2005, TAC KBP, TB-Dense, MATRES, TCR, Causal-TB, EventStoryLine, and HiEve, respectively. This shows that the general ERE skills learned from MAVEN-E are transferable and can help ERE on datasets in other genres, especially for these small-scale datasets. We encourage future works to explore the influence of genre gaps deeply.\"}"}
{"id": "emnlp-2022-main-60", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset                  | #Doc. | #Sentence | #Word | #Event Type | #Mention | #Chain | #TIMEX Type | #TIMEX | T-Link Type | #T-Link | C-Link Type | #C-Link | Subevent Rel. |\\n|-------------------------|-------|-----------|-------|-------------|----------|-------|-------------|--------|-------------|---------|-------------|--------|---------------|\\n| ACE 2005 (Walker et al., 2006) | 599   | 15        |       |             |          |       |             |        |             |         |             |        |               |\\n| TAC KBP                 | 1     | 075       | 33    |             | 294      | 694   |             |        |             |         |             |        |               |\\n| OntoNotes\u2020 (Pradhan et al., 2007) | 2    | 384       | 84    |             | 789      | 673   |             | 210   |             | 47     |             | 834    |               |\\n| ECB+ (Cybulska and Vossen, 2014) | 982   | 15       | 812   | 362         | 546      | 14    | 14          | 884   |             | 9      |             | 875    |               |\\n| TimeBank 1.2 (Pustejovsky et al., 2003b) | 183   | 2        | 611   | 63          | 987      | 7     | 7           | 935   |             | 4      |             | 414    |               |\\n| TempEval-1 (Verhagen et al., 2009) | 183   | 2        | 611   | 63          | 987      | 7     | 7           | 935   |             | 4      |             | 414    |               |\\n| TempEval-2 (Verhagen et al., 2010) | 173   | 2        | 383   | 58          | 214      | 7     | 6           | 158   |             | 4      |             | 127    |               |\\n| TempEval-3\u2217 (UzZaman et al., 2013) | 2     | 472       | 25    |             | 824      | 672   |             | 684   |             | 82     |             | 061    |               |\\n| TCR (Ning et al., 2018a) | 25    | 694       | 17    |             |          |       |             |        |             |         |             |        |               |\\n| TB-Dense (Chambers, 2013) | 36    | 598       | 12    |             |          |       |             |        |             |         |             |        |               |\\n| MATRES (Ning et al., 2018b) | 275   | 2        | 172   | 108         | 999      | 7     | 11          | 861   |             | 4      |             | 955    |               |\\n| EventCausality (Do et al., 2011) | 25    | 694       | 17    |             |          |       |             |        |             |         |             |        |               |\\n| BECauSE 2.0 (Dunietz et al., 2017) | 121   | 4        | 038   | 124         | 581      | 4     | 1           | 803   |             | 109    |             | 3      | 217           |\\n| CaTeRS\u2021 (Mostafazadeh et al., 2016) | 320   | 1        |       |             |          |       |             |        |             |         |             |        |               |\\n| EventStoryLine (Caselli and Vossen, 2017) | 258   | 4        | 316   | 94          | 594      | 7     | 4           | 732   |             |         |             | 9      | 8             |\\n| Causal-TB (Mirza et al., 2014) | 183   | 2        | 654   | 63          | 811      | 7     | 6           | 811   |             |         |             | 13     | 5             |\\n| Intelligence Community (Hovy et al., 2013) | 100   | 1        | 985   | 51          | 093      | 2     | 3           | 919   |             |         |             | 919    | 1             |\\n| HiEve (Glava\u0161 et al., 2014) | 100   | 1        | 354   | 33          | 273      | 6     | 3           | 185   |             |         |             | 185    | 2             |\\n| RED (O'Gorman et al., 2016) | 95    | 2        | 719   | 54          |          |       |             | 731   |             | 2      |             | 731    | 8             |\\n| MAVEN-E\u2020RE\u2021 (Glava\u0161 et al., 2014) | 100   | 1        | 354   | 33          | 273      | 6     | 3           | 185   |             |         |             | 185    | 2             |\\n| MAVEN-E\u2020RE\u2021 (Glava\u0161 et al., 2014) | 100   | 1        | 354   | 33          | 273      | 6     | 3           | 185   |             |         |             | 185    | 2             |\\n| MAVEN-E\u2020RE\u2021 (Glava\u0161 et al., 2014) | 100   | 1        | 354   | 33          | 273      | 6     | 3           | 185   |             |         |             | 185    | 2             |\\n| MAVEN-E\u2020RE\u2021 (Glava\u0161 et al., 2014) | 100   | 1        | 354   | 33          | 273      | 6     | 3           | 185   |             |         |             | 185    | 2             |\\n\\nTable 12: Detailed statistics of MAVEN-E\u2020RE\u2021 and existing widely-used datasets of all the ERE tasks. T-Link denotes temporal relations. C-Link denotes causal relations.\\n\\n\u2020: OntoNotes does not specify whether a mention is an entity or an event, so the #Mention and #Chain count both entities and events.\\n\\n\u2217: The majority of TempEval-3 is automatically annotated silver data.\\n\\n\u2021: The original CaTeRS data is unavailable, so the statistics are taken from the original paper, and some statistics are missed.\"}"}
{"id": "emnlp-2022-main-60", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 13: Data split statistics for datasets used in event coreference resolution experiments.\\n\\n| Dataset   | #Doc. | #Mention | #T-Link |\\n|-----------|-------|----------|---------|\\n| ACE 2005  | 529   | 4        | 420     |\\n|           |       |          | 437     |\\n| TAC KBP   | 826   | 23       | 175     |\\n|           | 991   |          | 303     |\\n| MAVEN-E   | 913   | 73       | 939     |\\n|           | 984   |          | 710     |\\n|           | 278   |          | 780     |\\n\\n## Table 14: Data split statistics for datasets used in temporal relation extraction experiments.\\n\\n| Dataset   | #Doc. | #Mention | #T-Link |\\n|-----------|-------|----------|---------|\\n| TB-Dense  | 22    | 1        | 212     |\\n|           |       |          | 553     |\\n| MATRES    | 182   | 6        | 684     |\\n|           | 332   |          | 73      |\\n| TCR       | 25    | 1        | 134     |\\n| MAVEN-E   | 913   | 73       | 939     |\\n|           | 984   |          | 710     |\\n|           | 278   |          | 780     |\\n\\n## Table 15: MAVEN-E split statistics for causal relation extraction experiments.\\n\\n| Dataset   | #Doc. | #Mention | #T-Link |\\n|-----------|-------|----------|---------|\\n| HiEve     | 60    | 1        | 944     |\\n|           |       |          | 367     |\\n| MAVEN-E   | 913   | 73       | 939     |\\n|           | 984   |          | 710     |\\n|           | 826   |          | 780     |\\n\\n## Table 16: Data split statistics for datasets used in subevent relation extraction experiments.\\n\\n| Dataset   | #Doc. | #Mention | #T-Link |\\n|-----------|-------|----------|---------|\\n| MAVEN-E   | 60    | 1        | 944     |\\n|           |       |          | 367     |\\n| MAVEN-E   | 913   | 73       | 939     |\\n|           | 984   |          | 710     |\"}"}
