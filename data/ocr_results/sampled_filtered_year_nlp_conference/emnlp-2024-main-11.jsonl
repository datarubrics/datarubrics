{"id": "emnlp-2024-main-11", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Eyes Don't Lie: Subjective Hate Annotation and Detection with Gaze\\n\\n\u00d6zge Ala\u00e7am1,2, Sanne Hoeken1, and Sina Zarrie\u00df1\\n\\n1Computational Linguistics, Department of Linguistics, Bielefeld University, Germany\\n2Center for Information and Language Processing, LMU Munich, Germany\\n{oezge.alacam, sanne.hoeken, sina.zarriess}@uni-bielefeld.de\\n\\nAbstract\\nHate speech is a complex and subjective phenomenon. In this paper, we present a dataset (GAZE4HATE) that provides gaze data collected in a hate speech annotation experiment. We study whether the gaze of an annotator provides predictors of their subjective hatefulness rating, and how gaze features can improve Hate Speech Detection (HSD). We conduct experiments on statistical modeling of subjective hate ratings and gaze and analyze to what extent rationales derived from hate speech models correspond to human gaze and explanations in our data. Finally, we introduce MEANION, a first gaze-integrated HSD model. Our experiments show that particular gaze features like dwell time or fixation counts systematically correlate with annotators\u2019 subjective hate rating, and improve predictions of text-only hate speech models.\\n\\n1 Introduction\\nHate speech is a real threat that harms individuals, groups, and societies in a profound way. Even though research in NLP has developed many different datasets and models for HSD (Poletto et al., 2021), the accurate modeling of hate speech is far from being solved (Ocampo et al., 2023; R\u00f6ttger et al., 2021). One of the key challenges in this area is that the definition and annotation of hate speech are highly complex and subjective, depending on the topic and domain of hate as well as on the individual annotators\u2019 backgrounds and biases (Waseem and Hovy, 2016; Abercrombie et al., 2023; ElSherief et al., 2018; Kov\u00e1cs et al., 2021). This combines with the fact that state-of-the-art HSD models are typically designed as black-box neural models that are well-known to pick up superficial, dataset-dependent patterns rather than learning a generalizable model of the underlying task. Therefore, it is still an open question of how to handle subjective variation in human annotations and detection of hate speech.\\n\\nThis paper contributes a new dataset (GAZE4HATE) that provides gaze and annotations from hate speech annotators, illustrated in Figure 1. We recorded the eye movements of annotators while they read statements, which were carefully controlled and constructed. This was followed by the annotation of hatefulness. Annotators\u2019 gaze provides us with an extremely rich signal of the subjective cognitive processes involved in human hate speech evaluation while reading. In this paper, we explore whether subjective hatefulness rating can be predicted by the gaze of an annotator, and whether gaze features can be used to evaluate and improve HSD models.\\n\\nGenerally, the NLP community has recently started to leverage eye-tracking data as a means of analyzing the internal mechanisms in transformer language models as elaborated on in Section 2.1. To the best of our knowledge, however, there is no available dataset of human reading of hate speech. Other work along these lines has adopted so-called rationale annotations, where annotators mark text spans that they consider indicative of their labeling decisions (e.g. DeYoung et al. (2020); Mathew et al. (2021)). These rationales can be used to measure the plausibility and explainability of model decisions, by testing whether model-internal weights...\"}"}
{"id": "emnlp-2024-main-11", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and gradients correlate with or even predict these human rationales (Atanasova et al., 2020). Yet, to date, it is unclear how rational annotations compare to gaze signals recorded during plain reading for the task of hate speech classification. Our **GAZE4HATE** data closes this gap, as our annotators did not only rate texts for hatefulness but also annotated token-level rationales for their ratings. Figure 1 shows an example that illustrates human gaze and rationales aligned with a model\u2019s rationale.\\n\\nOur analyses and experiments center around the following research questions:\\n\\n**RQ1** Do gaze features provide robust predictors for subjective hate speech annotations?\\n\\n**RQ2** How do gaze features correlate with human and model rationales?\\n\\n**RQ3** Are gaze features useful for enriching LMs for HSD?\\n\\nWe address the first question by conducting statistical modeling on our collected eye-tracking and annotation data (Section 4). To answer the second question, we evaluate a range of existing HSD models on our data, comparing models\u2019 and humans\u2019 rationales to human gaze (Section 5). Section 6 presents the **MEANION** model, which integrates text-based HSD with gaze features. In sum, our experiments show that particular gaze features like dwell time or fixation counts systematically differ with respect to annotators\u2019 subjective hate ratings. Models\u2019 rationales, however, correlate more with explicit, annotated rationales than with annotator gaze. Finally, in some settings, adding gaze features improves predictions of text-only hate speech models more than human rationales do.\\n\\n**2. Related Work**\\n\\n**2.1 Eyetracking Data in NLP**\\n\\nIn work on testing the cognitive plausibility of attention-based transformer language models, human gaze is a very relevant indicator of readers\u2019 cognitive processes and a valuable source of evaluation data (Das et al., 2016; Malmaud et al., 2020; Sood et al., 2020; Hollenstein and Beinborn, 2021; Eberle et al., 2022; de Langis and Kang, 2023). Unfortunately, the collection of eyetracking data is costly and existing task-specific datasets are small and scarce (de Langis and Kang, 2023). Our work contributes to enriching the landscape of available NLP-tailored eyetracking datasets.\\n\\nPrevious studies on using gaze to extend NLP models usually focus on a few high-level gaze features (Barrett et al., 2016; Long et al., 2019; Eberle et al., 2022), with some exceptions (Mishra et al., 2017; Hollenstein et al., 2019; Alacam et al., 2022). As one of the most commonly used group of gaze features in NLP, fixations measure the pause of the eye movement on an area of the visual field, and are strongly associated with visual intake (Rayner, 1998; Kowler, 2011; Skaramagkas et al., 2021). However, reading hateful text also involves intense emotions (e.g. feeling empathy, being the target of the hate speech). Little NLP work has been done on emotion-related eye movements such as pupil dilation, which is associated with emotional and cognitive arousal (Bradley et al., 2008). Our work considers a range of gaze features and compares their predictive power for subjective hate ratings. Furthermore, gaze features are commonly preprocessed in non-trivial ways, e.g. by aggregating all token-level features or arranging them in a token-based discretized sequence as in the above-mentioned studies. We adopt such a simple token-based preprocessing for our **MEANION** model, and leave exploration of more advanced architectures such as time series-based gaze transformers (Alacam et al., 2022) for future work.\\n\\n**2.2 Explainability**\\n\\nTo assess whether models attend to relevant parts of an input, various explanation and rationale extraction methods have been developed, e.g., model simplification methods (Ribeiro et al., 2016), gradient-based techniques (Simonyan et al., 2014; Sundararajan et al., 2017), perturbation-based methods (Zeiler and Fergus, 2013) and Shapley-based methods (Shapley, 1953). The work of Atanasova et al. (2020) evaluates different methods for text classification models, concluding that \u201cthe gradient-based explanations perform best across tasks and model architectures\u201d. Yet, the \u2018best\u2019 method highly depends on the dataset/task, model, and diagnostic property used for evaluation. In this study, we evaluate a selection of explanation methods for hate speech classification, which has not been attempted before. We do so not only on human annotations of salient tokens (as e.g. Atanasova et al. (2020) did) but also on human gaze measurements.\\n\\n**2.3 Hate Speech and Subjectivity**\\n\\nSince the advent of research on hate speech detection (HSD), the reliable annotation of hate in texts...\"}"}
{"id": "emnlp-2024-main-11", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"has been recognized as a notorious issue (Waseem, 2016; Schmidt and Wiegand, 2017). Still, HSD is often modeled with text classifiers, trained and fine-tuned on ground-truth annotations and benchmarks (Davidson et al., 2017; Basile et al., 2019; Zampieri et al., 2019). Recent approaches and shared tasks, though, shifted the focus to specific domains of hate such as sexism (Kirk et al., 2023) as well as explainable HSD (Mathew et al., 2021; Pavlopoulos et al., 2022; ElSherief et al., 2021). R\u00f6ttger et al. (2021) present the HateCheck benchmark, which is composed of linguistically controlled functional tests designed to systematically assess language understanding in hate speech models. Davani et al. (2022) take some first steps in dealing with disagreements between annotators in HSD and compare the prediction of majority vote vs. individual labels. Similarly, Wojatzki et al. (2018) compare hate speech annotations of female and male annotators on hateful statements about women.\\n\\nFurthermore, there is an emerging research that explores the contribution of injecting annotators' demographics and preferences along with the annotated text (Kanclerz et al., 2022; Fleisig et al., 2023). The results of these studies indicate that demographic information is a successful predictor for annotators' ratings on the sentence-level hate speech. Furthermore, Hoeken et al. (2024) shows that annotator's demographics are also useful for predicting subjective annotations at the lexical level i.e. predicting hateful words in context.\\n\\nOur collection of annotator gaze provides a new direction for tackling the issues of explainability and subjectivity in an integrated fashion.\\n\\n3 GAZE\\n4 HATE\\n\\nDataset\\n\\nWe collected a hate speech annotated dataset that provides information from three different sources: hatefulness ratings of text w.r.t. gender, eye movements during plain readings of the statements, and explicit rationales marked by annotators. In this section, we explain the design of the dataset.\\n\\n3.1 Data and Sentence Selection\\n\\nTo obtain a dataset for systematic analysis of hate speech understanding in models, and of subjective differences between annotators and their gaze, we opted for a carefully controlled set of constructed items, similar to R\u00f6ttger et al. (2021). As is common in eyetracking studies in linguistics, we design our items as minimal pairs: we first collect a set of \\\"seed\\\" hateful statements. Within these statements, we manipulate specific tokens that change the hatefulness of the statement and turn it into a neutral or even positive statement. Furthermore, we consider (i) items that express hate explicitly, through direct lexical cues, and (ii) items where the expression of hate is implicit and results from the social meaning of the sentence as a whole. These conditions roughly correspond to the explicit vs. implicit derogation category in R\u00f6ttger et al. (2021)'s HateCheck taxonomy.\\n\\nAs an example, consider the hateful statement \\\"Women can do nothing and are too stupid\\\" in Table 1. When \\\"women\\\" is replaced with \\\"minions\\\", the statement is neutral towards women. When changing \\\"nothing\\\" and \\\"stupid\\\" the meaning of the statement even turns positive. This example belongs to the \\\"explicit\\\" condition in our design as it contains hateful lexical cues (e.g. \\\"stupid\\\"). The statement \\\"Women belong in the kitchen\\\" illustrates the \\\"implicit\\\" condition, as none of its words is hateful on its own. Analog to the \\\"explicit\\\" condition, minimal pairs can be constructed, i.e., when changing \\\"women\\\" to \\\"pots\\\", the statement is not hateful anymore.\\n\\nWe conducted our in-lab experiments in Germany and based the construction of our annotation targets on the German FEMHATE (Wojatzki et al., 2018) dataset that contains 400 statements about women. We have selected 20 sentences from FEMHATE with respect to two criteria; (i) being rated as hateful and (ii) allowing for minimal pair manipulation (for comparable hateful-neutral-positive triples). With minimal pair manipulations, we have created 20 positive statements and 20 neutral (w.r.t gender context) sentences. This subset of 60 sentences constitutes our main manipulation instances. To avoid participants becoming aware of these manipulations, we have introduced control sentences (10 mean and 10 no-hate sentences from the FEMHATE dataset, and 10 new sentences that express hate against men, see Table 1). In total, we have collected eye movements on 90 items, divided into 40 a priori hateful and 50 a priori non-hateful sentences. The selection from the FEMHATE dataset and controlled sentence creations were conducted by one of the authors and the stimuli set was finalized after the validation by two other NLP researchers. For the validation of the a priori-level annotations, we used the definition inspired by Davidson et al. (2017) and the EU\"}"}
{"id": "emnlp-2024-main-11", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: GAZE 4HATE provides annotated statements about women, divided into 3 conditions (explicit, implicit, control). Explicit and implicit examples figure in minimal pairs where words (in pink and lime) in (a priori) hateful sentences are manipulated to obtain neutral and positive examples. Control examples are not manipulated.\\n\\n| Cond. | Example | Translation | A priori label | n | P |\\n|-------|---------|-------------|----------------|---|---|\\n| Explicit | Frauen k\u00f6nnen nichts und sind zu dumm. | Women can do nothing and are too stupid. | hate | 10 | 40 |\\n|        | Minions k\u00f6nnen nichts und sind zu dumm. | Minions can do nothing and are too stupid. | neutral | 10 | 40 |\\n|        | Frauen k\u00f6nnen alles und sind sehr kreativ. | Women can do everything and are very creative. | positive | 10 | 40 |\\n| Implicit | Frauen geh\u00f6ren in die K\u00fcche. | Women belong in the kitchen. | hate | 10 | 40 |\\n|        | T\u00f6pfe geh\u00f6ren in die K\u00fcche. | Pots belong in the kitchen. | neutral | 10 | 40 |\\n|        | Frauen geh\u00f6ren in die Regierung. | Women belong in the government. | positive | 10 | 40 |\\n| Control | Frauen k\u00f6nnen ohne Kinder gl\u00fccklich sein. | Women can be happy without children. | neutral (no hate) | 10 | 40 |\\n|        | Spezielle Frauenquoten sind unn\u00f6tig. | Special quotas for women are unnecessary. | hate (unclear/mean) | 10 | 40 |\\n|        | Alle M\u00e4nner sind Machos. | All men are machos. | hate (men) | 10 | 40 |\\n\\nCode of Conduct\\n\\n1 on hate speech formulated as \\\"any rude, hurtful, derogatory language that upsets or embarrasses people or groups of people and the extreme form of hate speech incites violence and hatred\\\".\\n\\n3.2 Experimental Procedure for Subjective Hate Speech Annotation\\n\\nOur study follows a within-subject design, i.e. all subjects read and rate all items. Each trial consists of two phases. In the first phase, we record annotator\u2019s eye movements while they read the statements. In the second phase, we collect their explicit annotations. We ask participants to rate the statement\u2019s hatefulness, to rate their confidence and to mark the words in the statement that contribute to their rating decision. The order of sentences was randomized for each participant.\\n\\nParticipants.\\n\\n43 university students (native speakers of German) participated in the experiment (32 female, 10 male, 1 non-binary, Mean age = 23.5, SD = 5.3). They were paid or given a course credit to participate. The experiment took approximately 40 minutes for each participant.\\n\\nEyetracking Procedure.\\n\\nThe stimuli were displayed on an SR Eyelink 1000 Plus eye tracker integrated into a 27\\\" monitor with a resolution of 2560 \u00d7 1440. We utilized a total of 94 sentences (including 4 familiarization trials). Each trial began with a drift correction located to the left of the sentence onset location. Then followed the reading phase, in which the participants read the sentence at their own pace. We set a time limit of 20 seconds for the reading task, but the participants were instructed to read as quickly as possible.\\n\\nAnnotation Procedure.\\n\\nThe instruction given to the participants is detailed in Appendix A.1. For collecting subjective annotation, we intentionally did not provide a strict hate speech definition to be able to get annotators\u2019 interpretation of the statements closest to their personal stance. First, participants rated the hatefulness of the statement in 1-to-7 Likert Scale (1:very positive, 2:positive, 3:somehow positive, 4:neutral, 5:mean, 6:hateful, 7:extremely hateful). Next, they rated their confidence regarding their rating on a 5-Likert scale (1:not certain, 2:somewhat certain, 3:moderate, 4:certain, 5:very certain). Finally, they annotated the rationale for the decision, by clicking words in the statements that contributed most to their rating. Figure 1 (top) illustrates the rationale annotation.\\n\\n3.3 Overview\\n\\nGAZE 4HATE provides gaze, hatefulness ratings and rationales for 90 items and 43 participants each summing up to 3870 unique instances of subjective hate ratings. Our dataset is comparable in size to existing eye-tracking datasets like, e.g. (de Langis and Kang, 2023). Figure 2 shows the average subjective hate ratings given by participants for a priori categories. Some sentences were rated differently than their a priori labels (especially a priori positive ones as neutral). The subjective ratings for sentences in other a priori categories also exhibit variations except for the very hateful statements.\\n\\n2The data and code are publicly available to the research community under a CC-BY-NC 4.0 license at https://gitlab.ub.uni-bielefeld.de/clause/gaze4hate\"}"}
{"id": "emnlp-2024-main-11", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Subjective hate ratings in GAZE 4HATE w.r.t. annotators' gender for the a priori labels (Appendix B.3). These mismatches between the a priori labels and our human ratings once again underline the fact that subjectivity is one of the major challenges in hate speech annotation. Yet, for this study, variation in the annotator's ratings is a feature rather than a bug as it allows us to study subjective hate speech annotations with the help of gaze features, which are highly participant-specific. For the following analysis, we group sentence-based subjective hate ratings provided by users into their hate speech labels (<=3: positive, 4:neutral, >=5: hate).\\n\\nTrain-Test Splits. Sentences from each a priori category were split into three groups (train, validation and test) with a 70:10:20 ratio using 5-fold cross-validation. Each split has instances from each participant, but not from the same sentence.\\n\\nPreprocessing Gaze Features. Eye movements often show participant-specific patterns and comparing raw gaze features can be misleading. We normalized gaze features with min/max scaling for each participant separately. The description of each feature and pre-processing steps are given in the Appendix A.3.\\n\\n4 Analysis of Annotators' Gaze\\nWe start with testing whether the gaze parameters show significant differences among the subjective hate categories. We use Anova tests using the OLS library in R on the continuous gaze features. On the categorical gaze features, we utilized Chi-square tests. Multiclass comparison is conducted among hate, neutral and positively rated statements. The binary classification (similar to many existing hate speech classifiers) involves hate and non-hate categories. The non-hate category consists of both neutral and positive statements. For each gaze feature, we checked whether there is a significant main effect of subjective hate categories on the gaze features. Table 2 presents F-scores and significance levels of the above-mentioned statistical tests. The first two columns in the table correspond to measurements on all tokens in the dataset, the last two columns on the right present the results conducted only on the words selected as rationales.\\n\\nSix out of 13 features consistently show significant differences with high F-score values between the subjective hate ratings for multiclass (hate, neutral, and positive) and for binary comparisons (hate and no hate): FIXATION-COUNT, DWELL-TIME, MAX-FIX-PUPIL-SIZE, MIN-FIX-PUPIL-SIZE, AVERAGE-FIX-PUPIL-SIZE and FIRST-RUN-FIXATION-COUNT. Some features result in low F-score values despite showing significant differences in terms of subjective hate rating. In the following, we remove features that yield low F-scores or non-significant results.\\n\\nAll features that are significant in the multiclass condition are also significant in the binary one, but not the other way around. This indicates that merging neutral and positive categories has a negative impact on the statistical difference. FIXATION-COUNT, DWELL-TIME and FIRST-RUN-FIXATION-COUNT are showing higher F-scores in the binary comparison. Tukey's tests for pairwise comparisons indicate that the differences in the fixation and dwell time originate from the difference between the hate vs. neutral and hate vs. positive conditions, while there is no difference between neutral and positive conditions. On the other hand, differences in the pupil size related parameters originate from difference in neutral conditions to hate and positive conditions without showing a significant difference between the latter two. This also confirms the theory of pupil size being more sensitive to the magnitude of the emotion rather than its polarity (Bradley et al., 2008).\\n\\n5 HSD Models and rationales\\nIn this Section, we evaluate several hate speech detection (HSD) models on our GAZE 4HATE dataset to answer RQ2, which are described in Section 5.1. We not only evaluate classification performance (Section 5.2), but also measure the plausibility and explainability of model decisions by looking into...\"}"}
{"id": "emnlp-2024-main-11", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: F and Chi-square scores (for continuous and categorical features respectively) of multiclass and binary comparison of subjective hate ratings on (i) all tokens and (ii) rationale tokens\\n\\n| Feature                      | Multiclass Binary Multiclass Binary |\\n|------------------------------|-------------------------------------|\\n| FIXATION-COUNT               | 28.01** 49.98**                     |\\n| DWELL-TIME                   | 25.20** 44.25**                     |\\n| MAX-FIX-PUPIL-SIZE           | 31.39** 29.38**                     |\\n| MIN-FIX-PUPIL-SIZE           | 42.32** 34.82**                     |\\n| AVERAGE-FIX-PUPIL-SIZE       | 37.85** 32.84**                     |\\n| RUN-COUNT                    | 0.61ns. 0.08ns.                     |\\n| REG.-IN-COUNT                | 1.04ns. 2.07ns.                     |\\n| REG.-OUT-COUNT               | 0.32ns. 0.56ns.                     |\\n| FIRST-FIX.-DURATION          | 3.28* 0.19ns.                       |\\n| FIRST-RUN-FIXATION           | 41.49** 54.19**                     |\\n| REG.-OUT                     | 1.04ns. 2.07ns.                     |\\n| REG.-IN                      | 1.61ns. 2.37ns.                     |\\n| SKIP                         | 0.32** 0.56ns.                      |\\n\\nTable 3: Overview of the off-the-shelf models for HSD in German tested in this study.\\n\\n| Model            | Pretrained Model | Fine-tuning Dataset(s)       |\\n|------------------|------------------|-----------------------------|\\n| deepset          | G-BERT (Chan et al., 2020) | GermEval 2018 (Wiegand et al., 2019) |\\n| ortiz            | G-BERT HASOC      | 2019 (Mandl et al., 2019)   |\\n| aluru            | M-BERT            | Aluru et al. (2020)         |\\n| rott             | G-BERT            | Assenmacher et al. (2021), Demus et al. (2022), Glasenbach (2022) |\\n| ml6              | G-DistilBERT      | GermEval 2018, GermEval 2021 (Risch et al., 2021), Ross et al. (2017), Bretschneider and Peters (2017), HASOC 2019 |\\n\\nBased on the performance results of the off-the-shelf models on our dataset (Section 5.2), we took the best-performing model for further finetuning. rott-hc\\n\\nWe finetuned the rott model (see Table 3) on the German HateCheck corpus (R\u00f6ttger et al., 2021), which comprises 3645 crafted sentences, of which 2550 hateful and 509 sentences (hateful and non-hateful) are targeting women. Finetuning details can be found in Appendix C.2.\\n\\n5.2 Classification results\\n\\nWe evaluate all models regarding the subjective hate ratings of all individual participants. Both human and model output labels are converted to a binary classification scheme (details in Table 8 in Appendix C.3). It must be emphasized that our task is not to detect a majority-class annotation label. Instead, we aim to detect whether a sentence is perceived as hate by an individual.\\n\\nThe F1-scores results are presented in Table 4. rott shows the best performance on detecting HATE sentences (F1 on HATE of 0.59), probably due to the fact that this model is the only one that has deliberately been trained to detect sexist hate speech. Fine-tuning this model further on the HateCheck dataset, resulted in a significant performance increase (the rott-hc model shows a macro avg. F1 of 0.68).\\n\\n5.3 Model rationales\\n\\nModel rationales for the best performing model (i.e. rott-hc) were generated using Captum (Kokhlikyan et al., 2020), an open source library built on PyTorch. Based on Atanasova et al. (2020), we selected three methods that showed the best results for Transformer-based models on a sentiment classification task: (1) InputXGradient (\\\\(\\\\ell^2\\\\) aggregated), (2) Saliency (\\\\(\\\\ell^2\\\\) aggregated) and (3) Shapley value (sampling).\\n\\nFor each sentence, we extract model rationales for both classes, i.e. a rationale for classifying a sentence as HATE and a rationale for classifying that same sentence as NO HATE. The extracted rationales are then converted from sub-word level (the output level that is inherent to BERT-based models) to word level (aligning with the human rationales), by averaging over multiple sub-word values that constitute a single word.\\n\\n9For the details of the algorithms, please visit Captum library: https://captum.ai/docs/algorithms\"}"}
{"id": "emnlp-2024-main-11", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Mean Correlation | -0.100 | 0.012 | 0.125 | 0.237 | 0.350 |\\n|------------------|--------|-------|-------|-------|-------|\\n\\n**Hum. Rationale**\\n\\n| DWELL-TIME | FIXATION-COUNT | FIRST-FIX-COUNT | MAX-FIX-PUPIL | AVG-FIX-PUPIL | MIN-FIX-PUPIL |\\n|------------|----------------|-----------------|--------------|--------------|--------------|\\n\\n**Figure 3:** Mean correlation (Pearson's r) between model rationales, human rationales and gaze features.\\n\\nFor each sentence and annotator, we compare the subjective hate rating ($h$), human rationale or a gaze feature ($f$) with a model rationale ($r$) with respect to class $c$, where $c = r$. We aggregate correlation values, each calculated as Pearson's r correlation metric between $f$ and $r$, over all sentences and annotators by taking the mean.\\n\\nFigure 3 reports mean correlation values of the human rationales and gaze features with the model rationales extracted with different methods (details in Table 9 in Appendix C.4). The six gaze features that showed a significant effect on subjective hate ratings (Table 2) are selected for this analysis. For all human rationales and gaze features, InputXGradient and Saliency rationales show substantially higher correlation than Shapley Value rationales. Additionally, InputXGradient rationales, although less substantial, consistently show higher agreement than Saliency rationales. The variation in agreement among the different gaze features and human rationale show the same pattern for all three rationale methods. Human rationales correlate the highest with model rationales. Among the gaze features, three features, i.e. DWELL-TIME, FIXATION-COUNT, and FIRST-FIXATION-COUNT, show a higher correlation (> 0.2) with InputXGradient rationales, while the other three features, AVERAGE-FIX-PUPIL-SIZE, MAX-FIX-PUPIL-SIZE, and MIN-FIX-PUPIL-SIZE show small to no correlation (between -0.1 and 0.1).\\n\\n### 6. MEANIONS: A Gaze-integrated Baseline Model\\n\\nIn this section, we explore whether gaze features improve pretrained and finetuned models on classifying hate speech (RQ 3). We introduce the first member of our new family of gaze-integrated HSD models (MEANIONS).\\n\\n#### 6.1 Multimodal Representation\\n\\nOur MEANIONS model uses multimodal embeddings that combine three types of embeddings: CLS-token from (L)LMs, token-level gaze features, and rationales as bag-of-words (bow) vector (Figure 4). We trained MLP classifiers using the scikit-learn library on multimodal sentence representations (see Appendix D.3 for the training details). As changes in eye movement patterns are rather local (e.g. fixation duration increases if the token is unexpected), gaze features for some tokens might be more informative than others for the classification, and averaging over tokens might lose a significant amount of signal. Therefore, we kept the values of each feature for each token in the representation. We first add text features. We use German BERT-base (Chan et al., 2020) and (the fine-tuned) roberta-base model, which is the best model from the previous experiments. We also investigate two larger decoder-only LLMs. We selected quantized (legacy) models from the German EM family, namely em-LLaMA2 and em-Mistral. The sentence embeddings are extracted via the LLaMA.cpp tool.\\n\\nWe give the sentence as input to an (L)LM and extract the CLS token embeddings (dim=768 or 4096). Depending on the testing configuration, we add either gaze features (G) or rationales (R) or both, to the sentence embeddings (E). For each gaze feature, we create a feature vector $f_i$ that contains a series of token values for that feature as shown in Figure 1 padded to the maximum token length of the sentences in GAZE (t=14). The rationales selected in each instance added as bag-of-words vector calculated using the COUNTVectorizer module from sklearn (N=248, number of unique words in the dataset). We have also experimented with token-level rationale representation, see Appendix D.1.\\n\\n---\\n\\n10. [https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\\n\\n11. [https://huggingface.co/jphme/em_german_7b_v01](https://huggingface.co/jphme/em_german_7b_v01)\\n\\n12. em_german_7b_v01.Q5_0.gguf\\n\\n13. TheBloke/em_german_leo_mistral.Q5_0.gguf\\n\\n14. [https://github.com/ggerganov/](https://github.com/ggerganov/)\"}"}
{"id": "emnlp-2024-main-11", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Macro and F1 scores for each category of the MLP Classifier. E: word embeddings, G: individual gaze feature, R: Rationale, GPlus: all 6 gaze features (underline: highest score in vertical orientation, bold: highest score among the respective f1-metric (macro, hate or nohate) (horizontal)).\\n\\n|                | bert-base | bert-ft (rott-hc) | em-LLaMA2 | em-Mistral |\\n|----------------|-----------|--------------------|------------|------------|\\n|                | condition | macro_f1 | hate_f1 | nohate_f1 | macro_f1 | hate_f1 | nohate_f1 | macro_f1 | hate_f1 | nohate_f1 | macro_f1 | hate_f1 | nohate_f1 | macro_f1 | hate_f1 | nohate_f1 |\\n|                |           |          |        |          | 0.56      | 0.54     | 0.57     | 0.63      |          | 0.60     | 0.58     | 0.56     | 0.65     |          | 0.56     |          | 0.73     |\\n| E              |           |          |        |          | 0.59      | 0.57     | 0.61     | 0.69      |          | 0.68     | 0.60     | 0.56     | 0.63     |          | 0.68     | 0.62     | 0.75     |\\n| EG             |           |          |        |          | 0.65      | 0.63     | 0.68     | 0.66      |          | 0.61     | 0.56     | 0.64     |          | 0.61     | 0.56     | 0.65     |\\n| ER             |           |          |        |          | 0.63      | 0.61     | 0.65     | 0.68      |          | 0.65     | 0.60     | 0.57     | 0.62     |          | 0.61     | 0.58     | 0.65     |\\n| EGR            |           |          |        |          | 0.57      | 0.53     | 0.61     | 0.67      |          | 0.64     | 0.57     | 0.56     | 0.59     |          | 0.65     | 0.58     |          |\\n| EGPlus         |           |          |        |          | 0.63      | 0.62     | 0.64     | 0.62      |          | 0.54     | 0.58     | 0.61     |          | 0.71     |          |          |          |\\n| EGPlusR        |           |          |        |          | 0.63      | 0.62     | 0.64     | 0.62      |          | 0.54     | 0.58     | 0.61     |          | 0.61     | 0.58     | 0.64     |\\n\\n6.2 Results\\n\\nTable 5 summarizes the performance of various feature combinations on predicting subjective hate (binary classification as hate versus no-hate). We report macro-F1 and F1-scores for both hate and no-hate classes. The first row corresponds to the performance of the model trained on only CLS embeddings (E). CLS&Gaze (EG) row provides the highest score obtained with the inclusion of a gaze feature one at a time. The third row belongs to the CLS&Rationale (ER) model (no gaze feature). The next variation includes rationales added to the EG Model (EGR). Finally, the last two variations include all gaze features (Plus). The contribution of each individual feature is presented in Appendix 7.\\n\\nFor the subjective HSD, the finetuned MEANION models predominantly outperform other MEANION models. The injection of gaze features increases performance: 0.03 F1-score improvement using the BERT-base, 0.06 using therott-hc, 0.02 with em-LLaMA2, and 0.03 using em-Mistral. The rationales contribute more to the BERT-base MEANION (0.09), slightly improve the performance of the MEANIONs with the finetuned (0.03) and em-LLaMA2 models (0.02), and it drops the performance of the em-Mistral (\u22120.04). Except for the BERT-base model, they even hurt the performance up to \u22120.07 when combined with gaze features. It should also be highlighted that integrating gaze and rationale features to BERT-base MEANION brings the performance closer to the text-only rott-hc MEANION.\\n\\nThe results highlight that gaze features provide substantial complementary information for subjective HSD and produce similar effects to fine-tuning on hate speech data.\\n\\nFor E-only models, MEANIONs with only the em-LLaMA2 and em-Mistral embeddings (without fine-tuning) indicate higher performance compared to the BERT-base MEANION. The contribution of gaze and rationales to em-LLaMA2 embeddings seems to be at the similar level. Furthermore, em-Mistral plus gaze embeddings are the best among the em-Mistral variations, and these results are significantly better than em-LLaMA2 performances and BERT-base models. The results demonstrate that EG models outperform all other variations. These also further confirm our conclusion that gaze features provide complementary information for subjective HSD, which is not represented in smaller or large LLMs.\\n\\nIn conclusion, MEANION with the finetuned BERT, especially the gaze-integrated one, outperforms all other variations. E-only em-LLaMA2 and BERT-base models perform on a similar level. Among these variations, E-only em-Mistral achieves higher macro-F1, yet the finetuned (rott-hc) ones show better F1-score for the hate class.\\n\\nThe contribution of eye movements on (L)LM only models is consistently observed and statistically proven with our further pairwise model comparisons using the McNemar's test (see Appendix Figure 11).\\n\\n7 Discussion\\n\\nBased on the above described experiments we revisit our research questions.\\n\\nRQ 1: Do gaze features provide robust predictors for subjective hate speech annotations? Yes.\\n\\nAccording to the analysis of annotators\u2019 gaze patterns, 6 out of 13 gaze features differ with respect to the subjective hate categories.\\n\\nRQ 2: How do gaze features correlate with human and model rationales? InputXGradient method seems to be more aligned with the fixation-based gaze and human rationales, which makes it more suitable explanation method for subjective hate ratings. But the pupil size related parameters are not correlated with model rationales, this might mean that the signal carried by pupil size might be one of the missing components in the HSD models.\\n\\nMore systematic analysis on the individual token level among the systematically manipulated conditions is needed.\"}"}
{"id": "emnlp-2024-main-11", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RQ 3: Are gaze features useful for enriching LMs for HSD?\\n\\nYes. For a MEANION model all six features as well as the human rationale improve performance (compared to using embeddings alone).\\n\\nA further question arises from this conclusion: Do features that correlate badly with model rationales (i.e. carrying complementary information) improve the performance of a model enriched with these features? Figure 5 plots the relationship between subjective hate rating effects, correlation with InputXGradient rationales, and error reduction in MEANION models. It shows that the features badly correlating with the model rationales do not necessarily improve the MEANION models (they do for base (B) but not for the rott-hc model (F)).\\n\\nFigure 5: Effect of subjective hate rating, the correlation with model rationales and the error reduction for both the base and rott-hc MEANIONs, for six gaze features and human rationale.\\n\\n| Human rationale     | Effect of subjective hate rating | Correlation InputXGradient rationales | Error reduction MEANION (B) model | Error reduction MEANION (F) model |\\n|---------------------|---------------------------------|--------------------------------------|----------------------------------|----------------------------------|\\n| DWELL-TIME          | -0.33                           | 0.00                                 | 0.33                             | 0.67                             |\\n| FIX-COUNT           | 0.67                            | 1.00                                 | -0.33                            | 0.00                             |\\n| FIRST-FIX-COUNT     | 0.33                            | 0.00                                 | 0.33                             | 0.67                             |\\n| MIN-FIX-PUPIL       | 0.67                            | 1.00                                 | -0.33                            | 0.00                             |\\n| MAX-FIX-PUPIL       | 1.00                            | 0.00                                 | 0.33                             | 0.67                             |\\n| AVG-FIX-PUPIL       | 0.33                            | 0.00                                 | 0.33                             | 0.67                             |\\n\\n8 Conclusion\\n\\nWe introduce a rich dataset of human readings of hate speech. Our GAZE4HATE dataset is enriched with gaze features and subjective hatefulness ratings collected from 43 participants on 90 sentences (3870 unique subjective annotation instances). We compare subjective human hate ratings, human gaze and human rationales with hate speech models rationales. By doing so, we also experiment with various model explanation methods and compare their performance in aligning with human behaviour. The human attention values (represented with a set of gaze features and rationales) are a highly valuable source not only for evaluating the models, but also for training them with cognitively guided attention mechanisms (Ding et al., 2022; Long et al., 2019; Hollenstein et al., 2019). In addition, we also introduce the first gaze-integrated hate speech model (MEANION), which successfully shows the contribution of gaze features on subjective hate speech classification.\\n\\nAcknowledgements\\n\\nThe authors acknowledge financial support by the project \u201cSAIL: SustAInable Life-cycle of Intelligent Socio-Technical Systems\u201d (Grant ID NW21-059A), which is funded by the program \u201cNetzwerk 2021\u201d of the Ministry of Culture and Science of the State of Northrhine Westphalia, Germany. Additionally, we would like to thank Elisabeth Tiemann and Maria Garcia-Abadillo Velasco for their valuable contribution to the annotation and data collection phases.\\n\\nLimitations\\n\\nTo evaluate the individual effect of the human gaze and rationale, we implement a basic solution without complex training schemes or multimodal fusion techniques. Our results encourage pursuing more sophisticated implementation for modeling the human gaze for classifying subjective hate speech. Because of space constraints, we could not elaborate on the differences between linguistic manipulations, which can help explain the relations between human gaze, human rationales, and model rationales.\\n\\nThere are linguistic or even non-linguistic factors like (word length, word frequency, expectations etc.) in our experimental set-up that influence cognitive processes. We attempt to minimize these risks with the careful selection of minimal pairs, the random ordering of the sentences, dealing with null values etc. It should be noted that the decoder-only models are trained on different objectives than BERT-based models. There is a significant amount of ongoing research on how sentence or token embeddings should be extracted or how they could be interpreted. In our paper, we do not aim to address these issues.\\n\\nDue to the controlled data collection procedure to explore the statistical robustness of different types of gaze features for subjective hate speech detection, the experimental setup may not fully reflect real-world scenarios of hate speech detection. We know that the participant pool lacks diversity, primarily consisting of university students. This might raise concerns about ecological validity. Despite...\"}"}
{"id": "emnlp-2024-main-11", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"this limited diversity, our results indicate subjective variation, especially concerning specific statements, as could be seen in Figure 8 and Figure 9 in Appendix B.3. Even in the same apriori category, we observe variation in terms of averaged hatefulness score. Besides, the deviation for each sentence also varies. To address this limitations, future work will address extending the diversity in the participant pool (different backgrounds, cultures, languages, ages etc) and the target groups addressed in the dataset.\\n\\nEthics Statement\\nAll recordings have been made after the signed consent of the annotators. Participants' identities are anonymized using pseudo-participant ID. The shared data do not contain any cues to reveal their identities. The dataset contains hateful statements about women and men, which do not reflect the opinion of any of the authors.\\n\\nHate speech is widespread in social media and causes a lot of harm to individuals, groups, and societies. Therefore, we consider social media as a possible application area, where models fine-tuned with gaze information can be used for individualized content moderation. Yet, our research does not imply that individual gaze information needs to be shared with/evaluated by social media companies. Eye-tracking technology, already part of many virtual headsets (HTC VIVE, Apple Vision, etc.), seems to be entering our daily lives through our phones and laptops (e.g., Rathnayake et al. (2023); Brousseau et al. (2020)). From an application point-of-view, incorporating users' gaze into phone applications via offline applications or through federated learning (by deploying a trained model) that can be integrated into social media or messaging APIs might take the privacy concerns into account.\"}"}
{"id": "emnlp-2024-main-11", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-11", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-11", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"on twitter. In Proceedings of the first workshop on NLP and computational social science, pages 138\u2013142.\\nZeerak Waseem and Dirk Hovy. 2016. Hateful symbols or hateful people? predictive features for hate speech detection on twitter. In Proceedings of the NAACL student research workshop, pages 88\u201393.\\nMichael Wiegand, Melanie Siegel, and Josef Ruppenhofer. 2019. Overview of the germeval 2018 shared task on the identification of offensive language. Proceedings of GermEval 2018, 14th Conference on Natural Language Processing (KONVENS 2018), Vienna, Austria \u2013 September 21, 2018, pages 1 \u2013 10.\\nMichael Wojatzki, Tobias Horsmann, Darina Gold, and Torsten Zesch. 2018. Do women perceive hate differently: Examining the relationship between hate speech, gender, and agreement judgments. In Proceedings of the 14th Conference on Natural Language Processing (KONVENS 2018).\\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. SemEval-2019 task 6: Identifying and categorizing offensive language in social media (OffensEval). In Proceedings of the 13th International Workshop on Semantic Evaluation, pages 75\u201386, Minneapolis, Minnesota, USA. Association for Computational Linguistics.\\nMatthew D. Zeiler and Rob Fergus. 2013. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901.\\n\\nA Appendix\\n\\nA.1 Instructions for the Annotators\\n\\nThe experimental instructions were given in written format in German. After the instructions, the participants completed 4 familiarization trials. Before starting with the main experiments, we make sure that they do not have any further questions regarding the task. The following text corresponds to the translated instructions:\\n\\nDuring this experimental session, you will be presented with 90 sentences. While some sentences have highly positive sentiments, some of them are hateful. There are also sentences that are neither positive nor hateful. For the current study, we define hate speech as expressions that carry a very negative stance (in terms of their intent). Please always keep this definition in mind and annotate the sentences carefully. One trial consists of (i) reading a sentence, (ii) evaluating its hatefulness, (iii) evaluating your confidence in this decision, and finally, (iv) highlighting the parts of the sentence that contribute to its hateful meaning (if any).\\n\\nStep-1: Read the sentence freely and press a key when you are done reading.\\n\\nStep-2: You will be asked to evaluate the sentence on a 1 to 7 Likert scale. Please think thoroughly.\\n\\nStep-3: You will be asked to evaluate your certainty/confidence while giving this score.\\n\\nStep-4: In this final step, each word in the sentence is shown in a bounding box. Please click on the words that contribute to your decision. You can have multiple selections. The boxes will be highlighted when you click them or hover them with your mouse during a press. To unselect a box or a series of boxes, you can click on them again. Feel free to try the annotation tool out during the familiarization period.\\n\\nA.2 Data Availability\\n\\nIn addressing the reproducibility of our study as well as the availability of software and datasets, we provide the following link to our GitHub repository under a CC-BY-NC 4.0 license: https://gitlab.ub.uni-bielefeld.de/clause/gaze4hate.\\n\\nA.3 Appendix: SR Eyelink definitions of gaze features\\n\\nThe description of row features which are directly taken from SR-Eyelink Dataviewer Export (User Manual : Data Viewer 4.3.210 https://www.sr-research.com/support/):\\n\\n\u2022 FIXATION: Percentage of all fixations in a trial falling in the current interest area.\\n\u2022 DWELL-TIME_%: Percentage of trial time spent on the current interest area\\n\u2022 MAX-FIX-PUPIL-SIZE: Maximum pupil size among all fixations falling within the interest area\\n\u2022 MIN-FIX-PUPIL-SIZE: Minimum pupil size among all fixations falling within the interest area\\n\u2022 AVERAGE-FIX-PUPIL-SIZE: Pupil size of the current sample averaged across the two eyes.\\n\u2022 RUN_COUNT: Number of times the Interest Area was entered and left (runs).\\n\u2022 REGRESSION_IN (categorical): Whether the current interest area received at least one regression from the later part of the sentence\\n\u2022 REGRESSION_IN_COUNT: Number of times the current interest area was entered from interest areas with higher IA_IDs.\\n\u2022 REGRESSION_OUT (categorical): Whether regression(s) was made from the current interest area to the earlier part of the sentence\\n\u2022 REGRESSION_OUT_COUNT: Number of times the current interest area was exited to a lower IA_ID before an interest area with a higher IA_ID was fixated in the trial.\"}"}
{"id": "emnlp-2024-main-11", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Number of tokens per subjective hate categories\\n\\n- SKIP (categorical): An interest area is considered skipped (i.e., SKIP = 1) if no fixation occurred in first-pass reading.\\n\\nIn addition to the participant-specific gaze normalization, the data needs to be preprocessed concerning missing values, which are not uncommon in gaze data. For example, if a participant skips a word during reading or a blink is detected, the respective data point is null. If all token values for a gaze feature are null, the trial is removed from the dataset, otherwise, null values are replaced with either zero (if it is skipped) or the average (if a blink is detected).\\n\\nB Gaze4Haze Annotation Results\\n\\nB.1 A Closer look at the manipulated tokens and rationales\\n\\nA Chi-square test has been conducted to see the difference on rationale selections among subjective hate categories. It revealed a significant main effect ($\\\\chi^2(1) = 110.49, p<.001$).\\n\\nFigure 6 shows the distribution of rationales, manipulated words and other tokens in the entire dataset. Since manipulated tokens occur only in the minimal pair conditions (see 3), their frequency is overall lower compared to rationales and other tokens. The ratio of rationales to all tokens is similar among the subjective hate categories (hate: 32.9%, neutral: 29.1%, positive: 33.49%). On the other hand, the ratio of the tokens that are both manipulated and selected is higher in hate category (13.0%) compared to neutral (8.13%) and positive categories (8.33%). A detailed look on the interaction between these two token types are beyond the scope of this paper, here we will provide a glimpse of a bigger analysis.\\n\\nManipulated words (parts of minimal word pairs) are the markers that change the hatefulness of the statement. As an example, for the following sentences, \\\"Women belong in the kitchen\\\" and \\\"Pots belong in the kitchen\\\", \\\"women\\\" and \\\"pots\\\" are the minimal pairs, which are manipulated. For the former case, this manipulated token is selected as rationale, in the latter, not.\\n\\nSince (i) the annotators consistently selected more words in their rationales than only the word we manipulated, and (ii) they select rationales for the positive statements too, the selection of a word for a rationale is not always an indication of hate, but also of general importance for the annotation decision.\\n\\nWe conducted further Anova tests to check whether the gaze features differ on words being manipulated and/or selected for the rationale from the minimal pair conditions. Table 6 shows statistical significance levels of the Anova tests in multiclass and binary comparisons. The gaze measurements on the rationales differ among the subjective hate categories. But when it comes to tokens which are manipulated but not selected (e.g. pots as in the example above), while fixation-based parameters still show significance difference, only pupil size related parameters do not differ, this might tell that pupil size parameters might be more sensitive at the token level while fixation-based parameters are more in line with the overall sentence stance.\\n\\nRegarding the restricted subset of both manipulated and selected tokens, we also observe cases where gaze measurements show no sensitivity in terms of the hate category (e.g. DWELL-TIME, RUN-COUNT, FIRST-RUN-FIXATION) which differs highly significantly when we look at the all dataset. This means that regardless of their hatefulness, they exhibiting similar gaze patterns. Our manipulations successfully provide fine-grained control conditions, yet their evaluations are beyond the scope of this paper.\\n\\n| Multi (Binary) | Multi (Binary) | Multi (Binary) |\\n|---------------|---------------|---------------|\\n| FIXATION-COUNT | ns. (0.05) | ns. (ns.) | 0.01 (0.01) |\\n| DWELL-TIME | 0.01 (0.01) | ns. (ns.) | 0.01 (0.01) |\\n| MAX-FIX-PUPIL-SIZE | 0.05 (0.01) | 0.05 (0.05) | ns. (ns.) |\\n| MIN-FIX-PUPIL-SIZE | 0.01 (0.01) | 0.01 (0.01) | ns. (0.05) |\\n| AVERAGE-FIX-PUPIL-SIZE | 0.01 (0.01) | 0.01 (0.01) | ns. (ns.) |\\n| RUN-COUNT | 0.01 (0.01) | ns. (ns.) | 0.01 (0.01) |\\n| FIRST-RUN-FIXATION-COUNT | 0.01 (0.01) | ns. (ns.) | 0.05 (0.01) |\\n\\nTable 6: Significance levels of feature-wise comparison of subjective hate ratings on manipulated conditions w.r.t. whether the token is (i) manipulated or not (M) and (ii) selected as rationale or not (R).\"}"}
{"id": "emnlp-2024-main-11", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.2 Confidence Ratings\\n\\nThe average confidence score for the a priori categories is above 3.5 out of 5 indicating that the sentences were rated with sufficient confidence rather than random assignment.\\n\\nFigure 7: Average confidence scores on the a priori labels (as hate, neutral, positive)\\n\\nB.3 Subjective Hatefulness Ratings per sentence and condition\\n\\nThe following Figure 8 illustrates the variation for individual sentences with the a priori labels (x-axis) and their average subjective hate ratings (y-axis), with the confidence intervals representing the variation in the ratings.\\n\\nWe initially created minimal triples (hate, neutral, positive apriori). These are aligned in Figure 8. For example, the first bars in each subfigure are variations of the first hateful sentence (1:hate, 52:neutral, 73:positive), and the second bars in each subfigure belong to the same minimal pair conditions (2, 53, 74) and go like that. The error bars indicate a general agreement between the annotators.\\n\\nThese graphs show that there are several statements which were rated differently and the range of the confidence intervals vary substantially.\\n\\nIn \u201cthe high hate against women\u201d (c1) condition (Figure 8a), they are all evaluated as hateful, while the sentences [3: \u201cFrauen haben im statistischen Mittel ein kleineres Allgemeinwissen.\u201d] and [15: \u201cEine klassische Rollenverteilung in der Ehe ist zu bevorzugen.\u201d] are at the lower band.\\n\\nIn the neutral (in terms of offensiveness towards gender) statements (Figure 8b), they are overall around the middle band. The sentences [59: \u201cMinions k\u00f6nnen nichts und sind zu dumm.\u201d] and [61: \u201cDie #keinurlaub-Bewegung ist hysterisch.\u201d] are evaluated as hateful on average, while [64: \u201cWissenschaftler sind \u00fcberwiegend angetrieben durch Neugierde.\u201d] and [68: \u201cGleichstellung der Geschlechter findet zunehmend h\u00e4ufiger statt.\u201d] as positive.\\n\\nIn the positive towards women condition (Figure 8c), the evaluation shows mixed, but generally neutral or positive ratings with the exceptions [78: \u201cFrauen sind M\u00e4nnern im Erinnerungsverm\u00f6gen \u00fcberlegen.\u201d] [83: \u201cFrauen sollten nicht zu Hause bleiben und sich um ihre Karriere k\u00fcmmern.\u201d]\\n\\nOn the other hand, other conditions which are included as control conditions also display interesting tendencies. (Figure 9a) is directly taken from the subset of FEMHATE dataset, namely \u201cmedium hate against women\u201d. Our participants mostly consider these statements in either mean or neutral...\"}"}
{"id": "emnlp-2024-main-11", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Subjective hate ratings per experimental condition and stimulus conditions except the sentence [34: Frauenquote muss \u00fcberall sein.]\\n\\nThe statements in the \u201cHate against men\u201d condition (Figure 9b) are evaluated as hate on average except the sentence [49: M\u00e4nner sind bei Stellenvergaben privilegiert.]\\n\\nThe statements in the \u201cNo Hate\u201d condition (Figure 9c) are generally evaluated as positive statements.\\n\\nC.1 Details of Huggingface Models\\n\\nDeepset\\n\\nDeepset Model is finetuned on GermEval18 (coarse and fine) (Wiegand et al., 2019), collected from Twitter data. GermEval18(Coarse) requires a system to classify a tweet into one of two classes: OFFENSE if the tweet contains some form of offensive language, and OTHER if it does not. For this dataset, similar to our study, the target groups are not explicitly mentioned in the hate speech definition. The author uses the following definition: \u201cIn the case of PROFANITY, profane words are used. However, the tweet does not want to insult anyone. In the case of INSULT, unlike PROFANITY, the tweet clearly wants to offend someone. In the case of ABUSE, the tweet does not just insult a person but represents the stronger form of abusive language ascribing a social identity to a person that is judged negatively by a (perceived) majority of society.\u201d All these categories were treated in one category in GermEval18 (Coarse) dataset. This model that makes binary classification on broader terms of hate speech aligns with our content as well, yet the inclusion/ratio of gender-related hate in the training data is not known.\\n\\nOrtiz\\n\\nThe model Ortiz is a fine-tuned version of bert-base-german-cased using the HASOC dataset (Mandl et al., 2019) to detect hate speech, specifically in the German language. It has binary class as hate versus no hate, which aligns with our binary classification. Hate speech is defined as \u201cDescribing negative attributes or deficiencies to groups of individuals because they are members of a group (e.g. all poor people are stupid). Hateful comment toward groups because of race, political opinion, sexual orientation, gender, social status, health condition or similar.\u201d Although gender is not directly mentioned as target group in the hate speech definition, the definition itself looks inclusive. The inclusion/ratio of gender-related hate in the training data is also not known.\\n\\nALURU\\n\\nHate-Speech-CNERG (Aluru et al., 2020), another well-known hate speech model, is fine-tuned on the multilingual BERT model. They use two labels, hate speech and normal, and discard other labels like (offensive, profanity, abusive, insult, etc.). For German, the model is trained on (Ross et al., 2017; Bretschneider and Peters, 2017) datasets. Both German datasets carry hate speech against foreigners. As definition, Ross et al. (2017) dataset uses the Twitter rule as \u201cYou may not promote violence against or directly attack or threaten other people on the basis of race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, disability, or disease. We also do not allow accounts whose primary purpose is inciting harm towards others on the basis of these categories.\u201d The Bretschneider and Peters (2017) dataset contains sentences against the government represented by political parties and politicians, the press and media, other identifiable targets, and unknown targets. Yet, gender-related hate speech is...\"}"}
{"id": "emnlp-2024-main-11", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Feature                        | BERT-base finetuned | em-LLaMA2 | em-Mistral |\\n|-------------------------------|---------------------|-----------|-----------|\\n| AVERAGE FIX_PUPIL_SIZE        | 0.578               | 0.579     | 0.614     |\\n| DWELL_TIME_%                  | 0.548               | 0.539     | 0.616     |\\n| FIRST_FIXATION_DURATION       | 0.544               | 0.551     | 0.631     |\\n| FIRST_RUN_FIXATION_%          | 0.540               | 0.507     | 0.631     |\\n| FIXATION_%                    | 0.542               | 0.515     | 0.616     |\\n| MAX_FIX_PUPIL_SIZE            | 0.536               | 0.554     | 0.613     |\\n| MIN_FIX_PUPIL_SIZE            | 0.567               | 0.530     | 0.605     |\\n| REGRESSION_IN_COUNT           | 0.540               | 0.532     | 0.595     |\\n| REGRESSION_OUT                 | 0.519               | 0.514     | 0.605     |\\n| Pupilsize_variation           | 0.531               | 0.534     | 0.629     |\\n| Forward_reg_count             | 0.588               | 0.570     | 0.629     |\\n\\nThis table shows the individual contribution of each gaze feature for BERT-base finetuned, em-LLaMA2, and em-Mistral models. The metrics include AVERAGE FIX_PUPIL_SIZE, DWELL_TIME_%, FIRST_FIXATION_DURATION, FIRST_RUN_FIXATION_%, FIXATION_%, MAX_FIX_PUPIL_SIZE, MIN_FIX_PUPIL_SIZE, REGRESSION_IN_COUNT, REGRESSION_OUT, and Pupilsize_variation.\"}"}
{"id": "emnlp-2024-main-11", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, we continue with this model for further fine-tuning on the HateCheck Dataset and use the HateCheck further fine-tuned version with multimodal integration. The base models are integrated into our model in a plug-and-play fashion, which makes the extension to include other models straightforward.\\n\\nC.2 Finetuning Details of rott-hc\\n\\nWe finetuned the rott model (see Table 3) on the German HateCheck corpus (R\u00f6ttger et al., 2021). For finetuning, we used 80% for training and 20% as development set (for evaluation over different epochs). We finetuned the model for 3 epochs with a batch size of 8, running just on a Macbook Pro\u2019s CPU. Other details: implementation with pytorch and transformers libraries, AdamW optimizer for training with learning rate of 5e-5 (and all other default hyperfeatures), applying linear scheduler with 0 warmup steps.\\n\\nC.3 Label Alignment\\n\\nTable 8 gives an overview of the label aligning of the different model classes and the binary classification schedule that we used for evaluating the different models.\\n\\n| Binary Human Deepset Ortiz Aluru Rott ML6 Rott-Hc | Hate Speech |\\n|-----------------------------------------------|-------------|\\n| HATE                                          | hateful     |\\n| OFFENSE                                      | 1           |\\n| OTHER                                         | HATE        |\\n| Political                                     | HSRacist    |\\n| HSexist                                       | HS          |\\n| Toxic                                         | hateful     |\\n| NO HATE                                      | neutral     |\\n| Positive                                      | positive    |\\n| OTHER                                         | 0           |\\n| NON_HATE                                     | No HS       |\\n| Non toxic                                     | non-hateful |\\n| Non hateful                                   |              |\\n\\nC.4 Model Rationales\\n\\nTable 9 reports mean correlation values of the human rationales and six gaze features with the model rationales extracted with the three different methods.\\n\\n| Table 9: Mean correlation (Pearson's r) between model and human rationales and features. (No correlation values are included for constant feature arrays) |\\n|-----------------------------------------------|-------------|-------------|-------------|\\n| input_x_gradient                             | saliency    | shapley_value |\\n| FIXATION-COUNT                               | 0.249       | 0.221       | 0.035       |\\n| DWELL-TIME                                   | 0.257       | 0.228       | 0.038       |\\n| AVERAGE-FIX-PUPIL-SIZE                       | -0.009      | -0.004      | -0.002      |\\n| MAX-FIX-PUPIL-SIZE                           | 0.079       | 0.075       | 0.005       |\\n| MIN-FIX-PUPIL-SIZE                           | -0.089      | -0.078      | -0.010      |\\n| FIRST_RUN_FIXATION-COUNT                     | 0.220       | 0.200       | 0.031       |\\n| Human rationale                              | 0.335       | 0.298       | 0.077       |\\n\\nD.1 Position-based and BOW Rationale Representation\\n\\nFigure 12 illustrates the effect of different rationale representations combined with various LM and gaze embeddings on the HSD classification. As seen from the graph, for the BERT-based models, adding rationales as bag-of-words representation results in higher performance, while for LLMs, we observe the opposite trend, this might indicate that semantic information regarding those words selected as rationales were already represented by the CLS embedding, highlighting the position of the rationales in combination with gaze information bring forth more complementary information.\\n\\nD.2 Implicit versus Explicit Hate Speech\\n\\nInsights into performance values of the different models with respect to implicitness (Table 10) show...\"}"}
{"id": "emnlp-2024-main-11", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that for the instances rated as hateful, the models perform better on the sentences where hatefulness is based on lexical cues (F1-score of 0.68 for rott-hc) rather than on implicit knowledge (F1-score of 0.65 for rott-hc). For the instances rated as non-hateful, it seems to be the other way around (F1-score of 0.76 for implicit, 0.63 for explicit cues).\\n\\nWe further plotted the accuracy scores in Figure 13 (i) to understand the models' capabilities to detect explicit and implicit hate speech and (ii) to explore the effect of gaze and rationales on this distinction. Among the base models (BERT, em-LLaMA2 and em-Mistral), the performance difference between hate (red lines) and no-hate (blue lines) classes with BERT and Mistral-based models are pretty clear. Overall patterns indicates the implicit no hate is the easier to classify, while implicit hate is the most challenging case as expected.\\n\\nD.3 Training Parameters of MLP Classifier\\nFor each LLM model and feature configuration, we conducted grid search using sklearn. Later, each configuration is trained with its best hyperparameters (Table 11.\\n\\n**Parameter space** = {\\n\\n- hidden_layer_sizes: [(64, 32), (128, 64), (128, 64, 32), (256, 100), (256, 100, 32)]\\n- activation: ['tanh', 'relu']\\n- solver: ['sgd', 'adam']\\n- alpha: [0.0001, 0.0005, 0.001, 0.005, 0.01] #\\n- learning_rate: ['constant', 'adaptive']\\n\\nTable 11: Best hyper-parameters after grid search for each configuration\\n\\n| BERT-base and finetuned-BERT features lr hidden layer sizes | bow | pos |\\n|------------------------------------------------------------|-----|-----|\\n| lr = 0.001 (256, 100)                                      | B   | B   |\\n| lr = 0.0001 (128, 64, 32)                                  | BG  | BG  |\\n| lr = 0.001 (128, 64, 32)                                   | BR  | BR  |\\n| lr = 0.0001 (128, 64)                                      | BGR | BGR |\\n\\n| em-LLaMA2 and em-Mistral features lr hidden layer sizes | bow | pos |\\n|-------------------------------------------------------|-----|-----|\\n| lr = 0.001 (256, 100)                                  | B   | B   |\\n| lr = 0.001 (256, 100)                                  | BG  | BG  |\\n| lr = 0.0001 (64, 32)                                   | BR  | BR  |\\n| lr = 0.0001 (64, 32)                                   | BGR | BGR |\\n\\nTable 10: Model performance w.r.t. linguistic types.\"}"}
