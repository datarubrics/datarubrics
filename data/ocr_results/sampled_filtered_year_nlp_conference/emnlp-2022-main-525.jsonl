{"id": "emnlp-2022-main-525", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nThe widespread diffusion of medical and political claims in the wake of COVID-19 has led to a voluminous rise in misinformation and fake news. The current vogue is to employ manual fact-checkers to efficiently classify and verify such data to combat this avalanche of claim-ridden misinformation. However, the rate of information dissemination is such that it vastly outpaces the fact-checkers' strength. Therefore, to aid manual fact-checkers in eliminating the superfluous content, it becomes imperative to automatically identify and extract the snippets of claim-worthy (mis)information present in a post. In this work, we introduce the novel task of Claim Span Identification (CSI).\\n\\nWe propose CURT, a large-scale Twitter corpus with token-level claim spans on more than 7.5k tweets. Furthermore, along with the standard token classification baselines, we benchmark our dataset with DABERTa, an adapter-based variation of RoBERTa. The experimental results attest that DABERTa outperforms the baseline systems across several evaluation metrics, improving by about 1.5 points. We also report detailed error analysis to validate the model's performance along with the ablation studies. Lastly, we release our comprehensive span annotation guidelines for public use.\\n\\n1 Introduction\\nThe swift acceleration of Online Social Media (OSM) platforms has led to tremendous democratized content creation and information exchange. Consequently, these platforms serve as ideal breeding grounds for malicious rumormongers and talebearers, abetting a colossal upsurge of misinformation. Such misinformation manifests in many ways, including bogus claims, fabricated information, and rumors. The massive COVID-19 'Infodemic' (Naeem and Bhatti, 2020) is one such malignant byproduct that led to the rampant spread of political and social calumny (Ferrara, 2020; Margolin, 2020; Ziems et al., 2020), accompanied by counterfeit pharmaceutical claims (O'Connor and Murphy, 2020). Therefore, finding such claim-ridden posts on OSM platforms, investigating their plausibility, and differentiating the credible claims from the apocryphal ones has risen to be a pertinent research problem in Argument Mining (AM).\\n\\n'Claim', as coined by Toulmin (2003), is 'an assertion that deserves our attention'. It is the key component of any argument (Daxenberger et al., 2017). Consider the second tweet, 'We don't have evidence... ', as given in Figure 1. For the task of claim identification at the coarse level, the entire tweet will be marked as a claim. However, on closer inspection, we find that the text fragments of 'our wine keeps you from getting #COVID19' and 'Better alternative to #DisinfectantInjection' represent the finer argumentative units of claim and form the set of evidence, based on which this tweet is considered a claim. Segregating such argumentative units of misinformed claims from their benign counterparts fosters many benefits. To begin with, it partitions the otherwise independent claims in a single post, enabling us to retrieve a larger number of claims. Secondly, it acts as a precursor to the downstream tasks of claim checking.\"}"}
{"id": "emnlp-2022-main-525", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"worthiness and claim verification. Thirdly, it will also bring in the angle of explainability in coarse-grained claim identification. Finally, it will serve the manual fact-checkers and hoax-debunkers to conveniently strain out the unnecessary shreds of text from further processing. We further elaborate on the necessity of claim span identification and exemplify it in Section 2.\\n\\nThough the recent literature reflects extensive work done in claim detection (Daxenberger et al., 2017; Chakrabarty et al., 2019; Gupta et al., 2021), limited forays have been made in claim span identification i.e., recognizing the argumentative components of a claim (W\u00fchrl and Klinger, 2021). In the recent past, commendable work has been done on span-level argument unit recognition pertaining to other computational counterparts under the umbrella of AM, such as hate speech (Mathew et al., 2021), toxic language (Pavlopoulos et al., 2021) etc. Such study, however, has eluded the realm of claims, owning to the lack of quality annotated datasets. This heralds a specialized corpus creation on claim span identification.\\n\\nTo this end, we propose CURT (Claim Unit Recognition in Tweets), a large-scale, claim span annotated Twitter corpus. We also present several baseline models for solving claim span identification as a token classification task and evaluate them on CURT. Furthermore, we introduce claim descriptions, which are generic prompts aimed to assist the model in focusing on the most significant regions of the input text using explicit instructions on what to designate as a \u2018claim\u2019. They are elucidated later in detail. Finally, we benchmark our dataset with DABERTa (Description Aware RoBERTa), a plug-and-play adapter-based variant of RoBERTa (Liu et al., 2019), endeavored to infuse the Pre-trained Language Model (PLM) with the description information. Empirical results attest that DABERTa outperforms the conventional baselines and generic PLMs for our task consistently across various metrics.\\n\\nContributions.\\n\\nThrough this work, we make the following tangible contributions:\\n\\n1. Formulation of a novel problem statement: We propose the novel task of Claim Span Identification that aims to identify argument units of claims in the given text.\\n2. Claim span identification dataset and extensive annotation guidelines: We posit a large-scale Twitter dataset, the first of its kind, with 7.5k claim span annotated tweets, to placate the absence of the annotated dataset for claim span identification. Additionally, we develop comprehensive annotation guidelines for the same.\\n3. Claim span identification system: We propose a robust claim span identification framework based on Compositional De-Attention (CoDA) and Interactive Gating Mechanism (IGM).\\n4. Extensive evaluation and analysis: We evaluate our model against different baselines to confirm sizable improvements over them. We also report thorough qualitative and quantitative analysis along with the ablation studies.\\n\\nReproducibility.\\n\\nWe release our dataset and code for DABERTa at https://github.com/LCS2-IIITD/DABERTA-EMNLP-2022. We present detailed dataset annotation guidelines in the Appendix (A.1).\\n\\n2 Why Claim Span Identification?\\n\\nAs stated in Section 1, we hypothesize that claim span identification would aid fact-checkers to quickly segregate claim-ridden content from the rest of the post. Moreover, we suppose that it will be a propitious precursor for claim verification and fact-checking, facilitating better retrieval of relevant evidences. We back our hypothesis with a small experiment of evidence-based document retrieval. We collect 50 random samples from CURT, along with their corresponding ground-truth claim spans. Further, for both the tweets and the claim spans, we extract top-k relevant articles from a knowledge-base leveraging the traditional retrieval system, BM25 (Robertson et al., 1995). We use the recently released publicly available CORD19 corpus (Wang et al., 2020) to retrieve factual documents. Finally, we present retrieved documents to three evaluators and ask them to mark whether or not the retrieved shreds of evidence are relevant to the given input tweet/span from our dataset. All three annotators label each text-evidence pair independently. Eventually, to obtain the final relevancy score, majority voting is employed. We obtain a high inter-annotator score (Fleiss Kappa) of 0.63 and 0.67 for tweets and spans, respectively.\\n\\nWe compare the performance of tweet-based and span-based retrievals in terms of precision (P) and recall (R) and continue to experiment with different values of k. We further analyze the performance of tweet-based and span-based retrievals in terms of precision (P) and recall (R) and continue to experiment with different values of k. We further analyze the performance of tweet-based and span-based retrievals in terms of precision (P) and recall (R) and continue to experiment with different values of k.\"}"}
{"id": "emnlp-2022-main-525", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: nDCG@k and P@k scores for tweet and spans using BM25 retrieval system and CORD19 dataset.\\n\\n|        | Tweets | Spans |\\n|--------|--------|-------|\\n| nDCG@5 | 0.3922 | 0.4407 |\\n| P@5    | 0.2745 | 0.3390 |\\n| nDCG@3 | 0.2733 | 0.3038 |\\n| P@3    | 0.2280 | 0.2521 |\\n\\nFor comparison, we consider two different top-k settings (k = 3 and 5). We begin by examining the retrieval performance using P@k, which measures the fraction of relevant documents extracted in the top-k set. Span-based document retrieval consistently improves precision scores when compared to tweets. For nDCG@5, we discover that span-based retrieval outperforms tweet-based retrieval by more than 3%.\\n\\nWhen we limit the retrieval depth to 3, we see a similar pattern. This, in turn, demonstrates that entire posts contain much extraneous information, frequently impeding the performance of evidence retrieval systems that are a prerequisite for both automated and manual fact-checking. In summary, we reinforce that our hypothesis positively stands true, as span-based document retrieval results in a better score for precision as well as nDCG. This attests to the task's feasibility and importance in the realm of claims.\\n\\n3 Related Work\\nClaims on Social Media.\\n\\nThe prevailing research on claims could be cleft into three categories \u2013 claim detection (Levy et al., 2014; Chakrabarty et al., 2019; Gupta et al., 2021), claim checkworthiness (Jaradat et al., 2018; Wright and Au- genstein, 2020), and claim verification (Zhi et al., 2017; Hanselowski et al., 2018; Soleimani et al., 2020). Bender et al. (2011) pioneered the efforts in claim detection by introducing the AAWD corpus. Subsequent studies largely relied on using linguistically motivated features such as sentiment, syntax, context-free grammars, and parse-trees (Rosenthal and McKeown, 2012; Levy et al., 2014; Lippi and Torroni, 2015). Recent works in claim detection have engendered the use of large language models (LMs). Chakrabarty et al. (2019) re-enforced the power of fine-tuning, as their ULMFiT LM, fine-tuned on a large Reddit corpus of about 5M opinionated claims, showed notable improvements in claim detection benchmark. Gupta et al. (2021) proposed a generalized claim detection model for detecting claims independent of its source. They handled structured and unstructured data in conjunction by training a blend of linguistic encoders (POS and dependency trees) and a contextual encoder (BERT) to exploit the input text's semantics and syntax. As LMs account for significant computational overheads, Sundriyal et al. (2021) addressed this quandary and proposed a lighter framework that attempted to fabricate discernible feature spaces. The CheckThat! Lab\u2019s CLEF-2020 shared task (Barr\u00f3n-Cedeno et al., 2020) has garnered the attention of several researchers. Williams et al. (2020) won the task by fine-tuning the RoBERTa (Liu et al., 2019) accentuated by mean pooling and dropout. Nikolov et al. (2020) ranked second with their out-of-the-box RoBERTa vectors supplemented with Twitter meta-data.\\n\\nSpan Identification.\\n\\nZaidan et al. (2007) introduced the concept of rationales, which highlighted text segments that supported their label's judgment. Trautmann et al. (2020) released AURC-8 dataset with token-level span annotations for the argumentative components of stance along with their corresponding label. Mathew et al. (2021) proposed a quality corpus for explainable hate identification with token-level annotations. The SemEval community has initiated fine-grained span identification concerning other domains of argument mining such as toxic comments (Pavlopoulos et al., 2021) and propaganda techniques (Da San Martino et al., 2020). These shared tasks amassed many solutions constituting transformers (Chhablani et al., 2021), convolutional neural networks (Coope et al., 2020), data augmentation techniques (Rusert, 2021; Pluci\u00b4nski and Klimczak, 2021), and ensemble frameworks (Zhu et al., 2021a; Nguyen et al., 2021). W\u00fchrl and Klinger (2021) resembled the closest study to ours, wherein they compiled a corpus of around 1.2k biomedical tweets with claim phrases.\\n\\nIn summary, existing literature on claims concentrates entirely on sentence-level claim identification and does not investigate on eliciting fine-grained claim spans. In this work, we endeavor to move from coarse-grained claim detection to fine-grained claim span identification. We consolidate a large manually annotated Twitter dataset for claim span identification task and benchmark it with various baselines and a dedicated description-based model.\\n\\n4 Dataset\\n\\nOver the past few years, several claim detection datasets have been released (Rosenthal and McKeown, 2012; Chakrabarty et al., 2019). However,\"}"}
{"id": "emnlp-2022-main-525", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dataset Train Test Validation\\nTotal no. of claims 6044 755 756\\nAvg. length of tweets 27.40 26.93 27.29\\nAvg. length of spans 10.90 10.97 10.71\\nNo. of span per tweet 1.25 1.20 1.27\\nNo. of single span tweets 4817 629 593\\nNo. of multiple span tweets 1201 121 161\\n\\nTable 2: Dataset statistics. All the lengths are in tokens.\\n\\nnone of these corpora come with claim-based rationales that quantify a post as a claim. To bridge this gap, we propose CURT (Claim Unit Recognition in Tweets), a large scale Twitter corpus with token-level claim span annotations.\\n\\nData Selection. We annotate claim detection Twitter dataset released by Gupta et al. (2021) for our task. However, the guidelines they presented have certain reservations, wherein they do not explicitly account for benedictions, proverbs, warnings, advice, predictions, and indirect questions. As a result, tweets such as \u2018Dear God, Please put an end to the Coronavirus. Amen\u2019 and \u2018@FLOTUS Melania, do you approve of ingesting bleach and shining a bright light in the rectal area as a quick cure for #COVID19? #BeBest\u2019 have been mislabeled claims. This prompted us to extend the existing guidelines and introduce a more exclusive and nuanced set of definitions based on claim span identification. We present details of the extended annotation guidelines and guideline development procedure in Appendix (A.1). In total, we annotated 7555 tweets from the Twitter corpus by Gupta et al. (2021) which met our guidelines.\\n\\nDataset Statistics and Analysis. We segment CURT into three partitions \u2013 training set, validation set, and test set, in the split of 80:10:10. Dataset related statistics are given in Table 2. One important point to note here is that while a claim tweet is typically 27 tokens long, a claim span is only around 10 tokens long. This implies that the claim-ridden tweets have a lot of extraneous information. Arguments can also perhaps comprise several claims that may or may not be related to each other. Around 19% of the claim tweets in our dataset contain multiple claim spans. As a result, in total, we obtain 9458 claim spans from 7555 tweets. We observe that the majority of the tweets contain single claims. Out of 7555 tweets, 6039 include a single claim, demonstrating that the majority of tweets contemplate single assertions at a time.\\n\\n5 Proposed Methodology\\n\\nIn this section, we outline DABERTa and its intricacies. The main aim is to seamlessly coalesce critical domain-specific information into Pre-trained Language Models (PLM). To this end, we introduce Description Infuser Network (DescNet), a plug-and-play adapter module that conditions the LM representations with respect to the handcrafted descriptions. The underlying principle behind this theoretical formalization is to link a claim span to a claim description to guide the model on what to focus on explicitly. As shown in Figure 2, DescNet houses two sub-components, namely, Compositional De-Attention block (CoDA) and Interactive Gating Mechanism (IGM). The particulars of each component are delineated in the following sections.\\n\\nClaim Descriptions. Before delving into CoDA and IGM, we first examine Claim Descriptions, which are the cornerstone of the proposed model. Claim Descriptions are handcrafted templates that guide the model where to concentrate its focus. The inclusion of claim description encourages the model to focus on the most essential phrases in the input tweet, which may be thought of as guided attention that leads to increased performance. We judiciously curated our claim descriptions in accordance with the annotation guidelines for claims and non-claims offered by Gupta et al. (2021). In Table 3 we list some of the claim descriptions along with the claims that they most align with. It is noteworthy that a claim can align with more than one claim descriptions as well.\\n\\nOverview of PLMs for Token Classification. To begin with the details of the proposed framework, DABERTa, we present the working of PLMs for the token classification task. PLMs such as BERT (Devlin et al., 2019), DistilBERT (Sanh et al., 2019), and RoBERTa (Liu et al., 2019) are widely used for various downstream NLP tasks owning to their strong contextual language representation capabilities and fine-tuning ease. As the input to these PLMs, each $i$th input text is first tokenized into a sequence of sub-word embeddings $X_i \\\\in \\\\mathbb{R}^{N \\\\times d}$, where $N$ is the maximum sequence length and $d$ is the feature dimension. Then a positional embedding vector $PE_{pos} \\\\in \\\\mathbb{R}^{N \\\\times d}$ is added to the token embeddings in a pointwise fashion to retain the positional information (Vaswani et al., 2017). The vector $Z_i \\\\in \\\\mathbb{R}^{N \\\\times d}$, hence obtained, is fed to a stack of transformer encoder blocks. Each\"}"}
{"id": "emnlp-2022-main-525", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Description Infuser Network (DescNet). DescNet is designed to facilitate deep semantic interaction among the input text and claim descriptions, and help underline the key fragments of claims. It consists of precisely engineered components of CoDA and IGM, each devised to augment the process of claim span identification.\\n\\nTo put formally, consider \\\\( D = \\\\{d_1, d_2, \\\\ldots, d_m\\\\} \\\\) as the set of \\\\( m \\\\) claim descriptions and \\\\( T = \\\\{t_1, t_2, \\\\ldots, t_n\\\\} \\\\) as the corpus of \\\\( n \\\\) input texts. The description representations are extracted from pre-trained RoBERTa (Liu et al., 2019) and passed through a transformer encoder layer. To begin with, each \\\\( i \\\\)th PLM generated vector \\\\( Z_i \\\\in \\\\mathbb{R}^{N \\\\times d} \\\\) of input text \\\\( t_i \\\\) interacts with each \\\\( j \\\\)th description vector \\\\( D_j \\\\in \\\\mathbb{R}^{M \\\\times d} \\\\) via the CoDA block. Here the vector \\\\( Z_i \\\\) forms the query, which is processed against the vector \\\\( D_j \\\\) acting as the key and value (Equation 1).\\n\\n\\\\[\\nZ_{ij} = \\\\text{CoDA}(Z_i, D_j)\\n\\\\]\\n\\nAll such compositionally manipulated vectors \\\\( Z_{ij} \\\\), after interacting with each \\\\( j \\\\)th description vectors are concatenated and passed through a dropout layer before going through a non-linear transformation for dimensionality reduction (Equation 2). The resultant vector \\\\( Z'_i \\\\) along with the vector \\\\( Z_i \\\\) is...\"}"}
{"id": "emnlp-2022-main-525", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"passed to the IGM module to extract the semantically appropriate features pertinent for fine-grained claim span identification (Equation 3).\\n\\n\\\\[ Z'_i = \\\\text{Concat}(Z_{C1},...,Z_{C_im}) \\\\] (2)\\n\\n\\\\[ \\\\hat{Z}_i = \\\\text{IGM}(Z'_i, W, Z_i) \\\\] (3)\\n\\nThe vector \\\\( \\\\hat{Z}_i \\\\) is then passed to a CRF layer.\\n\\nCompositional De-Attention Block (CoDA).\\n\\nThe traditional narrative on attention mechanism (Bahdanau et al., 2015; Parikh et al., 2016; Seo et al., 2016; Vaswani et al., 2017) is heavily biased on the use of Softmax operator where the attention weights are always bounded between \\\\([0, 1]\\\\). Such a convex weighted addition scheme allows the vectors to only contribute in an additive manner. To counter this bottleneck, Tay et al. (2019) devised a quasi-attention technique that enables learning of additive as well as subtractive attention weights, allowing the input vectors to add to \\\\((+1)\\\\), not contribute to \\\\((0)\\\\), and even subtract from \\\\((-1)\\\\) the output vector. They decomposed the original Softmax-based self-attention as pointwise multiplication between two matrices as shown in Equation 4, where \\\\(G(Q,K)\\\\) is the negative pointwise \\\\(L_1\\\\) distance between query \\\\(Q\\\\) and key \\\\(K\\\\).\\n\\n\\\\[\\nA_{\\\\text{quasi}} = \\\\left( \\\\tanh \\\\left( QK^T \\\\sqrt{d_k} \\\\right) \\\\circ \\\\sigma \\\\left( G(Q,K) \\\\sqrt{d_k} \\\\right) \\\\right) V\\n\\\\] (4)\\n\\nWe adopt this quasi-attention strategy to promote more meaningful interaction between the input text and claim descriptions and generate more precise claim-relevant representations.\\n\\nInteractive Gating Mechanism (IGM). To further distinguish salient tokens inclusive in claim spans, we posit Interactive Gating Mechanism. To begin with, the vectors \\\\( Z_i \\\\) and \\\\( Z'_i \\\\) are max pooled to obtain \\\\( Z_{ip}, Z'_{ip} \\\\in \\\\mathbb{R}^{d} \\\\). These vectors are passed through a series of gates, the first of them being the conflict gate \\\\( C \\\\), aimed at capturing the semantically conflicting features in \\\\( Z_i \\\\) and \\\\( Z'_i \\\\) (Equation 6).\\n\\n\\\\[\\n\\\\mu_c = \\\\sigma(Z_{ip}W_c1 + Z'_{ip}W_c2 + b_c1)\\n\\\\] (5)\\n\\n\\\\[\\nC = \\\\tanh(Z_{ip} \\\\circ \\\\mu_c W_c3 + Z'_{ip} \\\\circ (1 - \\\\mu_c) W_c4 + b_c2)\\n\\\\] (6)\\n\\nThe refine gate \\\\( R \\\\), on the other hand, endeavors to capture the semantically similar features between \\\\( Z_{ip} \\\\) and \\\\( Z'_{ip} \\\\) (Equation 8).\\n\\n\\\\[\\n\\\\mu_r = \\\\sigma(Z_{ip}W_r1 + Z'_{ip}W_r2 + b_r1)\\n\\\\] (7)\\n\\n\\\\[\\nR = \\\\tanh(Z_{ip} \\\\circ \\\\mu_r W_r3 + Z'_{ip} \\\\circ \\\\mu_r W_r4 + b_r2)\\n\\\\] (8)\\n\\nTo congregate the conflicting and similar semantic representations spawned by the gates \\\\( C \\\\) and \\\\( R \\\\), we employ an adaptive gating scheme to retain maximum differential information from each gate. It is given by Equation 10.\\n\\n\\\\[\\nA = R + (1 - \\\\mu_r) \\\\circ C\\n\\\\] (9)\\n\\n\\\\[\\n\\\\hat{Z}_i = \\\\tanh(AW_a + b_a) \\\\circ Z_i\\n\\\\] (10)\\n\\nFinally, this vector \\\\( \\\\hat{Z}_i \\\\) is passed to a CRF layer for token classification.\\n\\n6 Experiments and Results\\n\\nBaseline Models.\\n\\nWe employ the following baseline systems.\\n\\n- CNN+CRF: A Convolutional Neural Network (CNN) trained with GloVe (Pennington et al., 2014) and a CRF head on top.\\n- BiLSTM+CRF (Huang et al., 2015): A sequence labeling model comprising Bidirectional Long Short-Term Memory (BiLSTM) and CRF layer.\\n- BERT (Devlin et al., 2019): A bidirectional transformer-inspired auto-encoder language model fine-tune for our span identification task.\\n- DistilBERT (Sanh et al., 2019): A smaller, faster, and lighter version of BERT fine-tune on our dataset for the task at hand.\\n- SpanBERT (Joshi et al., 2020): An enhanced version of the BERT trained on span prediction objective.\\n- RoBERTa (Liu et al., 2019): A robustly optimized BERT approach, RoBERTa, is a variant of BERT with improved training methodology. We fine-tune it on our dataset.\\n- NLRG (Chhablani et al., 2021): A system proposed at SemEval-2021 Task 5 on toxic span detection (Pavlopoulos et al., 2021). It is a combination of SpanBERT and RoBERTa where the former model is used for predicting the span start and end, while the latter is used for token classification.\\n- HITSZ-HLT (Zhu et al., 2021b): The system topped the SemEval-2021 task on toxic span detection. They approached the task as a combination of sequence labeling and span extraction and proposed an ensemble of three BERT-based models.\\n\\nEvaluation Metrics.\\n\\nIn concordance with Pavlopoulos et al. (2021), we evaluate the performance of all the systems, based on token-level precision (P), recall (R), and F1 scores. To further put a lens over how the models fare for different token types, we calculate the micro-level precision, recall, and F1 score for each of the 'B', 'I', and 'O' tokens.\\n\\nLastly, to quantify the number of tokens included in the spans, we also report the Dice Similarity Coefficient (DSC) (Dice, 1945).\"}"}
{"id": "emnlp-2022-main-525", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Experimental results of DABERTa, its variants (last two rows), and baselines. DSC, P, and R denote Dice Similarity Coefficient, Precision, and Recall respectively.\\n\\nPerformance Comparison. We summarize our collated results in Table 4. Evidently, DABERTa outperforms all the baseline systems against majority of the evaluation metrics.\\n\\nWe analyze all the systems based on the following questions. How accurately do the models predict? To gauge how well each model performs for the token classification task, we monitor precision, recall, and F1 scores. As it can be inferred from Table 4, the traditional word embedding-based deep learning models of CNN and BiLSTM give the poorest token classification performance. An appreciable improvement of about 10-14% across all three metrics is observed when we move from the classical deep learning architectures to the transformer-based models of DistilBERT, BERT, SpanBERT, and RoBERTa. This underlines the importance of using contextual word embeddings and transformer-based architectures for the task at hand. The addition of the CRF layer further amplifies the performance of these models. SpanBERT also fares better than BERT as it is trained using span prediction objective. We also notice that employing the CRF layer results in a somewhat better balance of precision and recall when compared to using a basic linear layer. The ensemble-based models of NLRG and HITSZ-HLT also give admissible results for our task. Our proposed model, DABERTa, surpasses all the models in terms of the precision, recall, and F1 scores. An improvement of about 1.5% is observed between RoBERTa and DABERTa in terms of these metrics. This justifies the inclusion of claim descriptions that amalgamate domain-specific semantic information into RoBERTa architecture via the deftly crafted adapter module. In summary, we see that all the models show a good trade-off between precision and recall.\\n\\nAre the models aggressive or defensive? Observing the precision, recall, and F1 scores for each of the 'B', 'I', and 'O' tags, as shown in Table 4, we get an idea of how aggressive or defensive the models are at predicting claim spans. CNN and BiLSTM show considerable resistance in predicting the claim spans, as evidenced by high precision, recall, and F1 scores for the token 'O' and less for the tokens 'B' and 'I'. The BERT-based models show a sizable improvement of about 22% and 15% for predicting tokens 'B' and 'I', respectively, over the traditional deep learning models. The addition of CRF layers further bolsters the predictive power for the token 'B'. DABERTa offers an improvement of about 4-5% over its traditional counterpart for predicting the token 'B'. Upon close inspection, we observe that the ranges of precision, recall, and F1 scores for predicting the tokens 'I' and 'O' vary by not more than 3%. However, the predictive power for the token 'B' varies vastly by about 25%. Hence, we hypothesize that the inclusion of descriptions makes our model cognizant of the syntactic and semantic constructs of claims.\\n\\nHow the models behave for multiple spans? Figure 3 illustrates how well the models identify...\"}"}
{"id": "emnlp-2022-main-525", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It is observed that the models of CNN and BiLSTM find it challenging to identify multiple spans. The transformer-based models with a linear head tend to predict more claim spans in the tweet than required. This issue is mitigated when the linear head is replaced with a CRF layer. Still, these models can identify roughly 80% of the time the occurrence of multiple spans. On the other hand, our model, DABERTa, correctly predicts multiple spans almost more than 85% of the time. Moreover, it does not predict more claim spans than required. Thus, the addition of domain-specific claim descriptions appropriately guides DABERTa in identifying the correct occurrence of spans.\\n\\nAblation Study.\\n\\nTable 4 also reports the ablation studies. Replacing CoDA with a na\u00efve Dot-Product Attention (DPA), we observe a drop in the performance across almost all the metrics. Amongst all, the performance drop in predicting the token \u2018B\u2019 is the most prominent (\u223c1.5% across precision, recall, and F1). Thus, we conjecture that the quasi-attention mechanism is better able to spot the starting of a claim fragment than DPA. When IGM is removed, the performance for predicting token \u2018B\u2019 slightly improves. However, it leads to a decrease in the predictive power for \u2018O\u2019 token (\u223c2.5% in F1). Therefore, the combination of CoDA and IGM obtains the most balanced performance.\\n\\nHyper-parameter Tuning.\\n\\nWe utilize the base version of RoBERTa (Liu et al., 2019) to propose DABERTa. The model is trained end-to-end using the Adam optimizer (Kingma and Ba, 2014), learning rate of $4 \\\\times 10^{-5}$, and batch size of 32 for 20 epochs with early stopping if the dice score does not improve after 5 epochs. We used the Nvidia Tesla v100 32 GB GPU. The hyper-parameter tuning is done with respect to the validation dataset.\\n\\nFigure 4: Performance of DABERTa when the adapter module is inserted at different layers of RoBERTa. Figure 4 reflects the effect of integrating the adapter DescNet at different layer of RoBERTa. It is observed that the performance consistently increases as the integration is done at a higher level of RoBERTa layers. This is admissible as studies on probing the PLM layers suggest that different layers encode distinct linguistic properties (Tenney et al., 2019). Furthermore, evidence by Peters et al. (2018) suggests that the lower layers of a language model encode the syntactic information, whereas the higher layers capture the complex semantics. As we strive to employ deep semantic interaction between the PLM representations and the claim descriptions, our results are consistent with their findings.\\n\\nError Analysis.\\n\\nIn this section, we manually analyze the errors the models are prone to make. Table 5 highlights randomly sampled tweets from our dataset, CURT, along with their gold spans and predictions from DABERTa. In addition, we also consider the predictions from the best-performing baseline, RoBERTa, for a fair comparison. We analyze the errors committed by both the systems and divide them into three different categories: (i) tweets with a single-claim span, (ii) tweets with claim-like premises, and (iii) tweets with claims that can be inferred from the underlying undertone of the tweet but no explicit span can be marked to highlight the claim-specific connotation, e.g., figurative sentences, satire, indirect questions etc. (Note: For simplicity, we refer to such claims as implicit claims.) In the most straightforward situation where the tweets only contain a single claim DABERTa makes more precise predictions than the baseline system as shown in the first example of Table 5. We observe that both the models identify the claim-span correctly; however, RoBERTa identifies some unnecessary spans, which trespasses our objective of equipping the fact-checkers with only relevant information. The second type of error related to spans is the presence of claim-like premises. Claims and premises are closely related components of argument mining, and differentiating them is strenuous, even for humans. Example 2 in Table 5, exhibits a post containing claim-premise pair. There are two conclusive claims in the tweet \u2013 \u2018#coronavirus was used by the #CCP as a bio weapon\u2019 and \u2018CCP is kicking out black people from hotels even if they don't have covid\u2019. Even though \u2018not only to kill people but to encourage racism\u2019 (sub)sentences used to support the concluding claim...\"}"}
{"id": "emnlp-2022-main-525", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gold Truly sobering analysis: US more vulnerable than many countries to #coronavirus owing to combination of high numbers of uninsured, many w/o paid sick leave, and a leadership that has downplayed the challenge while not preparing the country for it.\\n\\nWhether made on purpose or not #coronavirus was used by the #CCP as a bio weapon, not only to kill people but to encourage racism among their citizens against foreigners. Especially black people, CCP is kicking out black people from hotels even if they don't have covid.\\n\\nRT @HealtheNews: Can honey, ginger, garlic or turmeric or any other home remedies cure #Covid19? No, here's why.\"}"}
{"id": "emnlp-2022-main-525", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgement\\n\\nThe authors would like to thank the Ramanujan Fellowship, SERB and CAI, IIIT-Delhi for the valuable support.\\n\\nReferences\\n\\nEhud Aharoni, Anatoly Polnarov, Tamar Lavee, Daniel Hershcovich, Ran Levy, Ruty Rinott, Dan Gutfreund, and Noam Slonim. 2014. A benchmark dataset for automatic detection of claims and evidence in the context of controversial topics. In Proceedings of the First Workshop on Argumentation Mining, pages 64\u201368, Baltimore, Maryland. Association for Computational Linguistics.\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\\n\\nAlberto Barr\u00f3n-Cedeno, Tamer Elsayed, Preslav Nakov, Giovanni Da San Martino, Maram Hasanain, Reem Suwaileh, and Fatima Haouari. 2020. Checkthat! at clef 2020: Enabling the automatic identification and verification of claims in social media. In European Conference on Information Retrieval, pages 499\u2013507. Springer, Nature Publishing Group.\\n\\nEmily M. Bender, Jonathan T. Morgan, Meghan Oxley, Mark Zachry, Brian Hutchinson, Alex Marin, Bin Zhang, and Mari Ostendorf. 2011. Annotating social acts: Authority claims and alignment moves in Wikipedia talk pages. In Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 48\u201357, Portland, Oregon. Association for Computational Linguistics.\\n\\nTuhin Chakrabarty, Christopher Hidey, and Kathy McKeeown. 2019. IMHO fine-tuning improves claim detection. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 558\u2013563, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nGunjan Chhablani, Abheesht Sharma, Harshit Pandey, Yash Bhartia, and Shan Suthaharan. 2021. NLRG at SemEval-2021 task 5: Toxic spans detection leveraging BERT-based token classification and span prediction techniques. In Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 233\u2013242, Online. Association for Computational Linguistics.\\n\\nSamuel Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli\u0107, and Matthew Henderson. 2020. Span-ConveRT: Few-shot span extraction for dialog with pretrained conversational representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 107\u2013121, Online. Association for Computational Linguistics.\\n\\nGiovanni Da San Martino, Alberto Barr\u00f3n-Cede\u00f1o, Henning Wachsmuth, Rostislav Petrov, and Preslav Nakov. 2020. SemEval-2020 task 11: Detection of propaganda techniques in news articles. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 1377\u20131414, Barcelona (online). International Committee for Computational Linguistics.\\n\\nJohannes Daxenberger, Steffen Eger, Ivan Habernal, Christian Stab, and Iryna Gurevych. 2017. What is the essence of a claim? cross-domain claim identification. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2055\u20132066, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nLee R Dice. 1945. Measures of the amount of ecologic association between species. Ecology, 26(3):297\u2013302.\\n\\nEmilio Ferrara. 2020. What types of covid-19 conspiracies are populated by twitter bots? arXiv preprint arXiv:2004.09531.\\n\\nShreya Gupta, Parantak Singh, Megha Sundriyal, Md. Shad Akhtar, and Tanmoy Chakraborty. 2021. LESA: Linguistic encapsulation and semantic amalgamation based generalised claim detection from online content. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3178\u20133188, Online. Association for Computational Linguistics.\\n\\nAndreas Hanselowski, Hao Zhang, Zile Li, Daniil Sorokin, Benjamin Schiller, Claudia Schulz, and Iryna Gurevych. 2018. UKP-athene: Multi-sentence textual entailment for claim verification. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 103\u2013108, Brussels, Belgium. Association for Computational Linguistics.\\n\\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991.\\n\\nIsraa Jaradat, Pepa Gencheva, Alberto Barr\u00f3n-Cede\u00f1o, Llu\u00eds M\u00e0rquez, and Preslav Nakov. 2018. ClaimRank: Detecting check-worthy claims in Arabic and English. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 7709\u20137710, Online. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-525", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-525", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jonathan Rusert. 2021. NLP_UIOW A at Semeval-2021 task 5: Transferring toxic sets to tag toxic spans. In Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 881\u2013887, Online. Association for Computational Linguistics.\\n\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\\n\\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603.\\n\\nAmir Soleimani, Christof Monz, and Marcel Worring. 2020. Bert for evidence retrieval and claim verification. Advances in Information Retrieval, 12036:359.\\n\\nChristian Stab and Iryna Gurevych. 2017. Parsing argumentation structures in persuasive essays. Computational Linguistics, 43(3):619\u2013659.\\n\\nMegha Sundriyal, Parantak Singh, Md Shad Akhtar, Shubhashis Sengupta, and Tanmoy Chakraborty. 2021. Desyr: Definition and syntactic representation based claim detection on the web. arXiv preprint arXiv:2108.08759.\\n\\nYi Tay, Anh Tuan Luu, Aston Zhang, Shuohang Wang, and Siu Cheung Hui. 2019. Compositional de-attention networks. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.\\n\\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593\u20134601, Florence, Italy. Association for Computational Linguistics.\\n\\nStephen E Toulmin. 2003. The uses of argument. Cambridge university press.\\n\\nDietrich Trautmann, Johannes Daxenberger, Christian Stab, Hinrich Sch\u00fctze, and Iryna Gurevych. 2020. Fine-grained argument unit recognition and classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9048\u20139056.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\\n\\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Doug Burdick, Darrin Eide, Kathryn Funk, Yannis Katsis, Rodney Michael Kinney, Yunyao Li, Ziyang Liu, William Merrill, Paul Mooney, Dewey A. Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan Wang, Nancy Xin Ru Wang, Christopher Wilhelm, Boya Xie, Douglas M. Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020. CORD-19: The COVID-19 open research dataset. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Online. Association for Computational Linguistics.\\n\\nEvan Williams, Paul Rodrigues, and Valerie Novak. 2020. Accenture at checkthat! 2020: If you say so: Post-hoc fact-checking of claims using transformer-based models. arXiv: 2009.02431.\\n\\nDustin Wright and Isabelle Augenstein. 2020. Claim check-worthiness detection as positive unlabelled learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 476\u2013488, Online. Association for Computational Linguistics.\\n\\nAmelie W\u00fchrl and Roman Klinger. 2021. Claim detection in biomedical Twitter posts. In Proceedings of the 20th Workshop on Biomedical Language Processing, pages 131\u2013142, Online. Association for Computational Linguistics.\\n\\nOmar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using \u201cannotator rationales\u201d to improve machine learning for text categorization. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 260\u2013267, Rochester, New York. Association for Computational Linguistics.\\n\\nShi Zhi, Yicheng Sun, Jiayi Liu, Chao Zhang, and Jiawei Han. 2017. Claimverif: A real-time claim verification system using the web and fact databases. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM '17, page 2555\u20132558, New York, NY, USA. Association for Computing Machinery.\\n\\nQinglin Zhu, Zijie Lin, Yice Zhang, Jingyi Sun, Xiaoyang Li, Qihui Lin, Yixue Dang, and Ruifeng Xu. 2021a. HITSZ-HLT at SemEval-2021 task 5: Ensemble sequence labeling and span boundary detection for toxic span detection. In Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 521\u2013526, Online. Association for Computational Linguistics.\\n\\nQinglin Zhu, Zijie Lin, Yice Zhang, Jingyi Sun, Xiaoyang Li, Qihui Lin, Yixue Dang, and Ruifeng Xu. 2021b. HITSZ-HLT at SemEval-2021 task 5: Ensemble sequence labeling and span boundary detection for toxic span detection. In Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 521\u2013526, Online. Association for Computational Linguistics.\\n\\nCaleb Ziems, Bing He, Sandeep Soni, and Srijan Kumar. 2020. Racism is a virus: Anti-asian hate and counter-hate in social media during the covid-19 crisis. arXiv preprint arXiv:2005.12423.\"}"}
{"id": "emnlp-2022-main-525", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Annotations\\n\\nA.1.1 Guideline Development\\n\\nWhile different frameworks and models of argumentation range in intricacy and claim conceptualization, the claim element is colloquially perceived as a principal component of an argument. Following Stab and Gurevych (2017), we define the claim as 'the argumentative component in which the speaker or writer conveys the central, contentious conclusion of their argument'. Aharoni et al. (2014) proposed a framework in which an argument is often divided into two parts: claim and premise. The premise, which is another crucial component of an argument, encompasses all shreds of evidence obliged to either corroborate or refute the claim. We confine our corpus to claim components only. However, claims and premises are usually indistinguishable and frequently blend together. As a result, distinguishing them can be challenging, especially when authors use claim-like statements as a premise.\\n\\nDue to the highly subjective nature of claims, it is imperative to devise structured annotation guidelines to annotate a new dataset for the claim span identification task. Therefore, after rigorous analysis and discussion, we established an initial set of annotation guidelines. To acclimate better with the dataset, we progressed through iterations of improvements. In every iteration, 100 random tweets were annotated by three annotators following the initial set of annotation guidelines. The annotators resolved the ambiguous cases mutually. In successive iterations, we further addressed the unsettled tweets that necessitated clarifications in the annotation guidelines. We reconsidered all prior annotations for every change in the guideline to ensure that the annotations emulated the most advanced version of the annotation guidelines. The final sprint of pilot annotation included annotating another set of randomly chosen 100 tweets with the final guidelines. Following Trautmann et al. (2020), we calculated the inter-annotator agreement using the $\\\\alpha$ agreement measure (Krippendorff et al., 2016). We computed the mean pairwise value per post, where each token can be classified into two classes, claim, and non-claim. We obtained a more than satisfactory agreement score of 0.87.\\n\\nThey are linguistic experts and their age ranges between 20-35 years.\\n\\nFinally, the entire Twitter dataset was annotated by the same annotators that carried out the prefatory pilot annotations.\\n\\nA.1.2 General Instructions\\n\\n\u2022 A claim is a statement that says you strongly believe that something is true. The action of showing, using or stating something strongly.\\n\\n\u2022 We use tweets that are annotated with a binary label using LESA guidelines (Gupta et al., 2021), which indicates whether a tweet is a claim or not.\\n\\n\u2022 The claim span is that part of a sentence that contains the semantic representation of the claim. Example: @realDonaldTrump A lot of people are saying cocaine cures COVID-19. Claim span: cocaine cures COVID-19.\\n\\n\u2022 Since our primary goal is to tackle misinformation in OSM, we majorly focus on claims that have some social impact.\\n\\nA.1.3 Guidelines and Examples\\n\\n\u2022 In the case of facts, we annotate the fact/span that may not be known by everyone, for example, scientific facts or legal (law) facts, and doesn\u2019t involve any commonsense. However, we do not include universal facts in the claim span. Example 1: \u201cWater is colorless\u201d is a universally known fact and hence should not be marked as claim span. Example 2: \u201cVirus always mutate\u201d is a scientific fact that may not be known by everyone. Hence we will annotate this fact as a claim.\\n\\n\u2022 An assertion about future eventualities/predictions will not be included in the claim span. Prediction is an extrapolation based on an assertion and is associated with a confidence level that can never be greater than or equal to 100%. Thus, we will not consider predictions as a part of the claim span. Example: @realDonaldTrump Uh no actually The virus will never go away Scientists will develop a vaccine for it that should be ready by next June which will allow nearly everyone to be immune to #CoronaVirus This really isn\u2019t hard to understand even for a very stable genius.\\n\\n\u2022 A proverb is a simple, concrete, traditional saying that expresses a perceived truth based on common sense or experience which contains wisdom,\"}"}
{"id": "emnlp-2022-main-525", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"truth, morals, and traditional views in a metaphor-\\nical, fixed, and memorizable form. The proverbs\\nare not facts. The elements of proverbs should\\nbe annotated as claim span.\\n\\nExample: \\\"Prevention is better than cure\\\" is not\\na claim.\\n\\n\u2022 If a claim contains statistics or dates, they should\\nbe included in the span. But not all numbers are\\nimportant.\\n\\nExample 1: \\\"@FernandoSVZLA @AP So far\\n50 people outside China have it with no deaths.\\nIf China was hiding information and it was more\\nlethal, we would see that fairly quickly\\\". Here\\nthe claim span is [ 50 people outside China have\\nit with no deaths]\\n\\nExample 2: \\\"57 round trip to LA thanks coron-\\nvirus\\\". The number is not important here.\\n\\n\u2022 In case there are multiple conclusive independent\\nclaims in one tweet, we annotate each one of\\nthem separately.\\n\\nExample: \\\"5 million left Wuhan before the lock-\\ndown. If they were really interested in knowing,\\nthey'd be testing at least 1 in 100 cases of all\\nviral pneumonia. They're limiting who's being\\ntested so they aren't accused of lying. Oh, and\\nthey might be asked to actually do something.\\\".\\nClaim span would consist of: [1] \\\"5 million left\\nWuhan before the lockdown\\\" and [2] \\\"They're\\nlimiting who's being tested so they aren't accused\\nof lying\\\"\\n\\n\u2022 Tweets that negate a possibly false claim are also\\nconsidered to be claims.\\n\\nExample: \\\"disinfectants are not a cure for coron-\\nvirus\\\".\\n\\n\u2022 Tweets 'reporting' something to be true or an\\ninstance to have happened or will happen are\\nclaims.\\n\\n\u2022 In cases of claims made in the form of a con-\\nditional sentence, the premise/context would be\\nincluded in the span.\\n\\nExamples: if you've been in the McDonald's play\\nplace you're immune to the coronavirus.\\n\\n\u2022 For claims containing humor/sarcasm, only the\\nhumorous phrase will be considered as a claim\\nspan if it has some social impact. For satire, the\\ncomplete sentence will be considered.\\n\\nExample: @TheRickWilson Drinking bleach\\nand/or injecting Disinfectant will cure COVID19.\\nAnd cancer, heart disease, OCD, schizophrenia\\nand AIDS. And life. #Covid19 #COVID Claim:\\nDrinking bleach and/or injecting Disinfectant\\nwill cure COVID19. And cancer, heart disease,\\nOCD, schizophrenia and AIDS. And life.\\n\\n\u2022 Personal experience will only be part of the\\nclaim phrase if they are opinions with societal\\nimpacts/implications.\\n\\nExample: Story about how #HydroxyChloro-\\nquine likely help people recover from #Coron-\\nvirus. IMO, it was never touted as the cure but\\nas option for treatment doctors should consider\\nand it appears to work in some cases....39 in one\\nplace. https://t.co/2hhi6aSVrY\\n\\nClaim: [it was never touted as the cure but as op-\\ntion for treatment doctors should consider and it\\nappears to work in some cases....39 in one place.]\\n\\n\u2022 A claim can be a sub-part of a question, only if it\\nis not a direct question.\\n\\nExample: @FLOTUS Melania, do you approve\\nof ingesting bleach and shining a bright light in\\nthe rectal area as a quick cure for #COVID19 ?\\n#BeBest\\\"\\n\\nClaim: [ingesting bleach and shining a bright\\nlight in the rectal area as a quick cure for\\n#COVID19]\"}"}
{"id": "emnlp-2022-main-525", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Data Preprocessing\\n\\nWe employ NLTK\\\\(^8\\\\) to tokenize the tweets. Each token in the tweet is BIO (Begin-Inside-Outside) encoded to generate the labels (Ramshaw and Marcus, 1999). Tag 'B' indicates that the token is at the start of a span, tag 'I' indicates that the token is within the span, while tag 'O' denotes that the token is outside the span. As RoBERTa tokenizes each word into subwords (Liu et al., 2019), each subword is given the BIO tag as per their parent word. We eliminate tokens made of non-ASCII and special characters, as well as remove the URLs provided in the tweets. Finally, we split hashtag terms by underscore delimiter and over non-consecutive uppercase character. For instance, #WuhanLab splits into 'Wuhan' and 'Lab'.\\n\\n\\\\(^8\\\\)https://www.nltk.org/\"}"}
