{"id": "emnlp-2023-main-1006", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A NY TOD: A Programmable Task-Oriented Dialog System\\nJeffrey Zhao, Yuan Cao, Raghav Gupta, Harrison Lee, Abhinav Rastogi, Mingqiu Wang, Hagen Soltau, Izhak Shafran, Yonghui Wu\\nGoogle Research\\n{jeffreyzhao, yuancao}@google.com\\n\\nAbstract\\nWe propose A NY TOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to unseen schemas and programs without prior training, A NY TOD adopts a neuro-symbolic approach. A neural LM keeps track of events that occur during a conversation, and a symbolic program implementing dialog policy is executed to recommend actions A NY TOD should take. This approach drastically reduces data annotation and model training requirements, addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains. We demonstrate state-of-the-art results on STAR (Mehri and Eskenazi, 2021), ABCD (Chen et al., 2021) and SGD (Rastogi et al., 2020) benchmarks. We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot transfer onto MultiWOZ (Budzianowski et al., 2018a). In addition, we release STAR V 2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot task transfer for end-to-end TOD models.\\n\\n1 Introduction\\nAn enduring challenge in building and maintaining task-oriented dialog (TOD) systems is efficiently adapting to a new task or domain. For instance, if we were to add the ability to book flight tickets to an existing system that can only handle booking train tickets, this requires manual data collection and labeling for new conversations about flight booking, as well as retraining of natural language understanding (NLU) and policy models. These data efficiency and scaling problems compound for multi-task TOD systems, as each task may have its own bespoke ontology and policy.\\n\\nTo tackle this problem, we propose A NY TOD, an end-to-end TOD system that can be programmed to adapt to unseen tasks or domains without prior training, significantly reducing the data collection and training requirements for enabling new TOD systems. We demonstrate state-of-the-art results on STAR (Mehri and Eskenazi, 2021), ABCD (Chen et al., 2021) and SGD (Rastogi et al., 2020) benchmarks. We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot transfer onto MultiWOZ (Budzianowski et al., 2018a). In addition, we release STAR V 2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot task transfer for end-to-end TOD models.\\n\\n16189\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To the best of our knowledge, ANYTOD is the first end-to-end TOD system capable of zero-shot transfer. To this end, we view TOD as a program that a language model (LM) must execute throughout a conversation, and can rely on to provide guidance. ANYTOD can be controlled by any predefined task policy if implemented as a program, allowing arbitrary business logic to be executed for a specific task. To demonstrate the efficacy of this paradigm, we experiment with the STAR (Mehri and Eskenazi, 2021), ABCD (Chen et al., 2021), SGD (Rastogi et al., 2020) and MultiWOZ (Eric et al., 2019) benchmarks. We show that ANYTOD achieves state-of-the-art results in both full-shot and zero-shot transfer settings.\\n\\nOverview of ANYTOD\\nTo adhere to a given program, ANYTOD adopts a neuro-symbolic approach (Figure 1). A neural LM is trained for Dialog State Tracking (DST) and Action State Tracking (AST), abstracting both states and actions into a sequence of symbols. To support zero-shot task adaptation, we follow the schema-guided paradigm advocated by Rastogi et al. (2020), which provides a schema to the LM as contextual information, describing all parameters and actions that should be tracked in natural language. By training on a large corpus of diverse schemas, the LM generalizes to arbitrary and unseen schemas (Lee et al., 2021; Zhao et al., 2022). A schema should also provide a symbolic program that declares the task logic, which is executed to recommend possible next actions the agent can take, conditioned on the current dialog states. These recommendations are then reincorporated into the LM, which selects a single Next Action Prediction (NAP), and generates a response. Note that the symbolic program forces ANYTOD to consider a dialog policy explicitly, driving zero-shot transfer onto unseen policies and allowing arbitrarily complex business logic to be employed. However, the program's recommendations are only considered as guidelines, and it is up to the LM to make a final decision on the NAP.\\n\\nSTAR V2\\nWe also introduce STAR V2, an improved version of the STAR dataset (Mosig et al., 2020). The original STAR dataset is very valuable for benchmarking zero-shot dialog policy and NAP across a diverse set of tasks or domains, through following a provided policy graph that outlines the intended flow of a conversation. However, the original dataset made following these policy graphs difficult, due to its lack of training data for DST and AST. Moreover, we found that the schema entity descriptions provided by the original dataset were not intuitive enough to truly support zero-shot DST and AST. To resolve these limitations, the STAR V2 dataset provides new belief state and action state annotations to the STAR dataset, as well as more intuitive natural language descriptions for many schema elements. In Section 4.2, we show that these changes facilitate stronger zero-shot DST and AST. However, the ground truth NAP on each system turn is left untouched, allowing direct comparison to results trained on the original STAR dataset. We hope that STAR V2 can serve as a new benchmark for TOD systems and drive further research for zero-shot TOD.\\n\\n2 Related Work\\nTOD and adaptation onto unseen tasks\\nFueled by the difficulty of adapting existing TOD systems to new tasks/domains, TOD systems capable of zero-shot task adaptation onto unseen tasks or domains have recently seen increasing interest. Much of this work has been on DST, with the primary approach being characterizing parameters through names (Wu et al., 2019) or descriptions (Lin et al., 2021; Lee et al., 2021; Zhao et al., 2022). Another approach has been through in-context fine-tuning (Shah et al., 2019; Gupta et al., 2022), in which a labeled exemplar conversation is given. Mi et al. (2021) demonstrated a more comprehensive approach, including task instructions, constraints, and prompts. In general, these results follow the schema-guided paradigm advocated by Rastogi et al. (2020); Mosig et al. (2020).\\n\\nBy contrast, there are fewer results on task adaptation onto unseen dialog policies (AST and NAP). To the best of our knowledge, the only result is SAM (Mehri and Eskenazi, 2021), which aligns an LM to an unseen dialog policy by following an explicit policy graph. While similar to the policy graph execution we demonstrate in ANYTOD, there are two differences. First, SAM lacks supervised training on DST and AST, and relies on ground truth NAP only, forcing user state and action tracking to be inextricably linked with the NAP, and hurting its ability to generalize to arbitrary policy graphs. Second, SAM is a classification model limited to NAP, and unlike ANYTOD, cannot support DST or natural language generation (NLG). Indeed, we show that ANYTOD is empirically more...\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"powerful than SAM in Section 4.2.\\nTo our knowledge, no method has yet to combine zero-shot task adaptation for DST, AST, and NAP into an end-to-end TOD system. All existing end-to-end TOD systems (Hosseini-Asl et al., 2020; He et al., 2021; Yang et al., 2020; Peng et al., 2020) are trained and evaluated on the popular MultiWOZ dataset (Eric et al., 2019). As a result, these systems are only aware of the policy for MultiWOZ, and are not robust to arbitrary/unseen policies. In contrast, AnyTOD can generalize to arbitrary policies, and we demonstrate strong performance on MultiWOZ without prior training (Section 4.4).\\n\\n### TOD as Programming\\n\\nHistorically, most TOD approaches use an explicit plan-based dialog policy module (Rich and Sidner, 1998; Ferguson and Allen, 1998; Bohus and Rudnicky, 2009). However, the NLU models powering these TOD systems are tightly coupled to a specific plan, and must be retrained for even slight changes to the plan. In contrast, AnyTOD enables zero-shot dialog policy by training NLU models to be robust to arbitrary programs as policies. Further, AnyTOD uses the program as contextual information to NLU, and refines its NAP with respect to the conversation, belief state, and action history instead of simply accepting the plan's dictated next action(s).\\n\\nRecent work has also focused on discovering structure within conversations i.e. a latent schema, policy graph, or program (Shi et al., 2019; Yu et al., 2022; Xu et al., 2020). Notably, SMCalFlow (Machines et al., 2020) constructs \\\"dataflow graphs\\\" from a conversation, parsing semantic intents into executable programs. Cheng et al. (2020); Shin et al. (2021) further explore this setup. However, these aim to manipulate an external API/database instead of controlling the agent's behavior.\\n\\nBeyond the scope of TOD, there has been some work in general neuro-symbolic programming with LMs, in which an LM is influenced by the results of a symbolic system. Nye et al. (2021) demonstrated a symbolic reasoning module that accepts or rejects the logical consistency of generations from a neural LM. Lu et al. (2020) explored using predicated logic constraints to control lexical aspects from the generation of an LM. However, AnyTOD is the first application of such an approach to a practical TOD setting.\\n\\n### 3 Methodology\\n\\n#### 3.1 The AnyTOD System\\n\\nAn overview of the AnyTOD system is presented in Fig. 1. We decompose AnyTOD into three steps, and describe each step in detail below:\\n\\n1. **Schema and program construction**: A chatbot designer constructs a AnyTOD schema describing the ontology of a specific task, and a policy graph that declares the task logic. This is the only thing AnyTOD requires from the designer. For instance, suppose the designer is creating a flight booking chatbot. They should define parameters to be tracked (e.g. \\\"flight id\\\", \\\"name of the airline\\\"), and enumerate possible actions the user and agent can take (\\\"user saying they would like to search for flights\\\", \\\"agent should query flight booking api\\\"). Following the schema-guided paradigm (Rastogi et al., 2020), each element in this schema is characterized by a short natural language description, allowing the LM to understand its meaning and facilitate zero-shot transfer. The schema should also include a program, which can be considered as a function from predicted belief states and actions, and dictate possible NAPs following explicit symbolic rules. Examples can be seen in Section A.2. At a high level, this program should describe agent actions in response to user behavior (e.g. \\\"if user wants to search for flights, query the flight search api\\\").\\n\\n2. **Schema-guided DST and AST**: A LM performs DST and AST capable of transfer onto unseen tasks with reference to the schema, and without task-specific training.\\n\\n3. **Program execution and NAP**: Given predicted DST and AST, we execute the schema program, which recommends possible NAP to the LM. The LM then predicts the final system action(s) conditioned on these recommendations, conversation history, and belief states.\\n\\n#### Schema Construction\\n\\nA chatbot designer constructs a AnyTOD schema describing the ontology of a specific task, and a policy graph that declares the task logic. This is the only thing AnyTOD requires from the designer. For instance, suppose the designer is creating a flight booking chatbot. They should define parameters to be tracked (e.g. \\\"flight id\\\", \\\"name of the airline\\\"), and enumerate possible actions the user and agent can take (\\\"user saying they would like to search for flights\\\", \\\"agent should query flight booking api\\\"). Following the schema-guided paradigm (Rastogi et al., 2020), each element in this schema is characterized by a short natural language description, allowing the LM to understand its meaning and facilitate zero-shot transfer. The schema should also include a program, which can be considered as a function from predicted belief states and actions, and dictate possible NAPs following explicit symbolic rules. Examples can be seen in Section A.2. At a high level, this program should describe agent actions in response to user behavior (e.g. \\\"if user wants to search for flights, query the flight search api\\\").\\n\\n#### Schema-guided DST and AST Adaptation\\n\\nAdaptation to novel tasks without training data critically hinges on an LM performing zero-shot DST and AST. For this purpose, we adopt and extend the D3ST approach (Zhao et al., 2022). We provide a description of D3ST here. Let $p_0, \\\\ldots, p_n$ be parameters...\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"defined in the schema, and let \\\\( \\\\text{desc}(p_i) \\\\) denote a parameter's natural language description. Then, construct a parameter context string\\n\\n\\\\[\\n\\\\text{[params]} p_0 = \\\\text{desc}(p_0) ... p_n = \\\\text{desc}(p_n)\\n\\\\]\\n\\nNote that the strings \\\\( p_0, ... p_n \\\\) are used as indices. Similar context strings are generated for actions for AST. These context strings are concatenated with the entire conversation history, forming the input to the LM. This input is contextualized by the schema information, allowing the LM to refer to the schema, and enabling zero-shot transfer. The target string contains the conversation belief state and history of actions at each turn of the conversation, both in a parseable format. Let \\\\( p_i_0, ... p_i_m \\\\) be the active parameters in the conversation, with corresponding values \\\\( v_i_0, ... v_i_m \\\\). The belief state is represented as\\n\\n\\\\[\\n\\\\text{[state]} p_i_0 = v_i_0; ...; p_i_m = v_i_m\\n\\\\]\\n\\nNote that inactive slots do not appear in the belief state string. Note that D3ST's original formulation is limited to DST, but, in principle, D3ST supports tracking arbitrary events that occur during a conversation, as long as their descriptions are provided. For ANYTOD, this approach can be extended to perform schema-guided AST, in which we can provide an action context string as contextual input, providing a list of user and system actions. We also build a target string consisting of a history of actions that were active at each turn of the conversation. Let \\\\( u_j \\\\) and \\\\( s_k \\\\) be the format of D3ST indices for user and system actions. Then, an action history string may look like\\n\\n\\\\[\\n\\\\text{[history]} u_0 u_9; s_2; u_1; s_3; ...\\n\\\\]\\n\\nThis denotes that, on the first turn, the user was performing user actions \\\\( u_0 \\\\) and \\\\( u_9 \\\\). On the second turn, the system was performing system action \\\\( s_2 \\\\), and so on. Note that the active actions for each turn are separated by a ; character.\\n\\nProgram Execution\\n\\nThe LM's predicted DST and AST are then parsed and passed to the schema program. This program should execute the dialog policy, and control ANYTOD by recommending possible NAPs. Section A.2 shows some example programs for STARV2. In the example shown in Figure 1, the current conversation state (\\\"user would like to search for flights to Dubai with Emirates\\\") satisfies multiple dependency rules (\\\"since the user would like to search for flights, query the flight search api\\\" and \\\"since the user has not provided their flight departure location, ask the user for it\\\"). These system actions are then passed back to the LM as a string of system action indices.\\n\\n\\\\[\\n\\\\text{[recommend]} s_0 s_2\\n\\\\]\\n\\nFinally, given the policy graph's recommended actions as extra conditional information, the LM makes predictions about NAP with respect to the conversation, previously predicted belief states and action history. A response is also generated following the action prediction.\\n\\n\\\\[\\n\\\\text{[selected]} s_2 \\\\ [\\\\text{response}] \\\\text{hello!}\\n\\\\]\\n\\nNote that the selected action need not be one of the actions recommended by the program, as actual conversations may not rigorously follow the predefined business logic. Indeed, violations like this are common within the STAR dataset. This step allows ANYTOD to \\\"softly\\\" execute the policy graph, balancing between the model's belief before and after receiving recommendations.\\n\\nZero-shot adaptation onto unseen tasks\\n\\nANYTOD's zero-shot transfer ability is enabled by a combination of two design considerations. The first is the LM's description-driven DST and AST. Since this schema information is provided as context, if this LM is trained on a corpus of diverse schemas, it learns to make predictions by \\\"reading\\\" and understanding the schema descriptions. This leads to robustness on ANYTOD's state and event tracking for unseen schemas, as shown in D3ST (Zhao et al., 2022). Moreover, ANYTOD facilitates zero-shot policy transfer by executing the provided policy graphs as explicit programs, and by similarly training an LM with a large number of diverse policy graphs when considering recommended NAPs.\\n\\n3.2 The STARV2 Dataset\\n\\nTo train ANYTOD, we construct STARV2, an updated version of STAR with new ground truth belief state and action annotations, supporting supervised training on DST and AST. These annotations were generated from few-shot training with D3ST (Zhao et al., 2022). We first train D3ST on the SGD dataset, then continue finetuning on a few hand-labeled conversations from STAR. While not the 24 conversations were labeled from each task in STAR.\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The focus of this paper is the labeling of the STARV2 dataset, which demonstrates the use of few-shot D3ST in labeling unlabeled conversations on new tasks/domains. Furthermore, the STARV2 dataset adds richer natural language descriptions for actions in the STAR schemas. Prior work on STAR (Mosig et al., 2020; Mehri and Eskenazi, 2021) leverages template utterances as schema descriptions, which we qualitatively found to not fully describe the complexity of an action. For example, in the STAR dataset, the action user_weather_inform_city has a template utterance of just [CITY]. STARV2 provides user is informing city as a more natural action description. We show in Section 4.2 that these actions improve zero-shot AST.\\n\\n4 Experiments\\n\\n4.1 Setup\\n\\nDatasets\\n\\nWe benchmark ANYTOD on zero-shot task adaptation settings on these datasets: STAR and STARv2: As described in Section 3.2, we upgrade the original STAR (Mehri and Eskenazi, 2021) dataset to STARv2. The dataset has 24 tasks across 13 domains, many tasks requiring the model to adhere to a novel policy, providing an important zero-shot AST and NAP benchmark. ABCD (Chen et al., 2021): The design of the ABCD dataset follows a realistic setup, in which an agent's actions must be balanced between the customer's expressed desires and the constraints set by task policies. It is thus a natural fit for the AnyTOD framework for both training and evaluation. SGD (Rastogi et al., 2020): SGD is another schema-guided dataset in which schema elements are provided with natural language descriptions to facilitate task transfer. It contains 45 domains and was generated via simulation. Thus, the agent actions and responses follow pre-defined task logic. MultiWOZ (Budzianowski et al., 2018b): MultiWOZ is the standard dataset for benchmarking TOD models. It contains 7 domains and was generated through Wizard-of-Oz (Kelley, 1984) data collection, leading to natural conversations.\\n\\nTraining\\n\\nOur implementation is based upon the open-source T5X codebase (Roberts et al., 2022) initialized with the public T5 1.1 checkpoints as the LM backend. We augmented the LM to execute a schema program and reincorporate the results before making the final NAP, as described in Section 3.1. We experimented on two T5 sizes: base (250M parameters, trained on 16 TPUv3 chips (Jouppi et al., 2017)) and XXL (11B parameters, trained on 64 TPUv3 chips). We otherwise adopt the default T5X finetuning hyper-parameter settings throughout our experiments.\\n\\n4.2 Results on STAR\\n\\nTable 1 shows ANYTOD results on the STARV2 dataset on the full-shot and zero-shot task transfer settings, with both \u201chappy\u201d and \u201cunhappy\u201d conversations. In full-shot, models train on 80% of conversations across all tasks, and evaluate on the remaining 20%. The zero-shot domain setting is a leave-one-out cross-validation across the STARV2 dataset's 13 domains, evaluating quality on an unseen schema in a completely novel domain. The following metrics are used in our report: joint goal accuracy (JGA) to measure DST, user action F1 (UaF1) to measure AST, system action F1 (SaF1) to measure NAP, and response BLEU.\\n\\nEach STAR task schema defines the intended dialog policy by providing a policy graph, where nodes describe conversation actions, and edges connect subsequent actions. An ANYTOD program (Figure A.2) is implemented to recommend next actions with respect to this policy graph.\\n\\nTwo baselines are used for comparison: BERT+S (Mosig et al., 2020) and SAM (Mehri and Eskenazi, 2021), both of which add a policy graph following module for zero-shot transfer to unseen schema. Note that, though these models were trained on the original STAR data, their SaF1 results are directly comparable to ANYTOD trained on STARV2 on NAP (SaF1), as these ground truth labels were left untouched. However, ANYTOD has additional training supervision on AST and DST due to STARV2's new annotations. For a fair comparison with SAM, we also report results on SAM-User, a modified version of SAM trained on STARV2 that also includes supervised training on user annotations.\\n\\n5 Note that both BERT+S and SAM are based on BERT-base (110M parameters), comparable to T5 base (220M parameters).\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BASE / XXL are given in Table 1. For conciseness, we shorten A\\\\textsubscript{NY}TOD to AT. As an ablation, we also report results with AT-NOREC, which removes the policy graph guidance from A\\\\textsubscript{NY}TOD by recommending no system actions. In the full-shot setting, both A\\\\textsubscript{NY}TOD and -NOREC, along with reported baselines, achieve very high SaF1, due to direct supervised training on NAP removing the need for program guidance. However, we see a huge gap between A\\\\textsubscript{NY}TOD and -NOREC in the zero-shot setting; the guidance from the program becomes necessary \u2014 we see 60.6 vs. 55.8 SaF1 at BASE, and 68.0 vs. 62.3 SaF1 at XXL. Moreover, AT-XXL has zero-shot performance comparable to that of full-shot, with 75.4 SaF1 at XXL.\\n\\n### Effect of Natural Language Descriptions\\n\\nAs mentioned in Section 3.2, STAR\\\\textsuperscript{V2} provides new natural language descriptions that better characterize the actions within STAR. Our main result AT-BASE/XXL takes advantage of these new descriptions, but to see the impact of these descriptions, we train AT-TMPL on the original template utterances from STAR. On BASE we see little difference between descriptions and templates, but a sizeable improvement in using descriptions appears on XXL, with a larger LM that is better at NLU. This evidences that more intuitive natural language descriptions help A\\\\textsubscript{NY}TOD understand task semantics better and perform zero-shot transfer.\\n\\n### A\\\\textsubscript{NY}TOD vs. baselines\\n\\nTo compare against available results on STAR\\\\textsuperscript{V2}, we compare AT-TMPL BASE against SAM-User. Both results use template responses provided by STAR, and additionally trained with the new DST and AST annotations in STAR\\\\textsuperscript{V2}. However, we see far stronger performance with A\\\\textsubscript{NY}TOD than with SAM or SAM-User, due to the flexibility provided by the program execution ability demonstrated by A\\\\textsubscript{NY}TOD, and enabled by supervised training on DST and AST. SAM is not suited to use these contextual signals, likely due to no attention between schema elements and conversation and a rigid classification architecture unsuitable for multiple losses.\\n\\n### Multitask Training with SGD\\n\\nTo demonstrate further robustness for A\\\\textsubscript{NY}TOD, we also report A\\\\textsubscript{NY}TOD-SGD, which jointly trains with SGD as a multitask training dataset. SGD includes a large number of tasks, each defined by a schema with highly diverse parameters and actions. The -SGD results in Table 1 show that at BASE, SGD multitask training improves both DST (61.9 \u2192 66.1 SaF1), and by extension NAP (60.6 \u2192 61.3 SaF1). A similar but smaller improvement is seen on XXL, suggesting that the larger LM may not need more diverse training owing to its better language understanding.\\n\\n### Complex Program Logic\\n\\nSTAR\\\\textsuperscript{V2} is also a good testbed for complex zero-shot task adaptation, as it includes some tasks which are more complex than simple policy-graph following, specifically the bank, trivia, and trip domains. For instance, the trivia task requires the agent to ask the user a trivia question and extract their answer. Different system actions must be taken by the agent depending on the user's response.\\n\\n---\\n\\n**Table 1:** Results on STAR\\\\textsuperscript{V2}. For compactness we show just UaF1 and SaF1 here \u2014 see Section A.3 for a complete table. For clarity, we bold SaF1 results for A\\\\textsubscript{NY}TOD BASE/XXL, our key result.\\n\\n| Model     | Bank  | Trip | Trivia |\\n|-----------|-------|------|--------|\\n| A\\\\textsubscript{NY}TOD BASE | 54.3  | 52.4 | 73.8   |\\n| A\\\\textsubscript{NY}TOD XXL   | 74.8  | 79.2 | 68.0   |\\n| A\\\\textsubscript{NY}TOD-SGD XXL | 53.1  | 51.5 | 81.1   |\\n| A\\\\textsubscript{NY}TOD-PROG XXL | 61.0  | 60.8 | 73.7   |\\n| A\\\\textsubscript{NY}TOD-PROG+SGD XXL | 65.0  | 62.9 | 86.3   |\\n\\n---\\n\\nNote that this SAM zero-shot domain SaF1 differs from the original 55.7 from Mehri and Eskenazi (2021). See Section A.4 for more details.\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: A NY-TOD JGA, SaF1 on SGD test set.\\n\\nWe report results with these programs in Table 1 under the -PROG name. There is a clear win on zero-shot domain SaF1 when averaged over all domains, with a very high 70.7 SaF1 on -PROG+SGD XXL, narrowing the gap with the full-shot 75.4 SaF1. When examining the complex tasks individually (Table 1c), the win on NAP is even more apparent. The only exception is AT XXL on trivia, which has little difference with or without the program. In general however, the guidance provided by this specialized program is necessary for higher-level logic in the dialog policy, since the policy graph does not specify enough information to approach the task in zero-shot.\\n\\n4.3 Results on ABCD and SGD\\n\\nWe conduct similar experiments for AST on ABCD (Chen et al., 2021) and DST and NAP on SGD (Rastogi et al., 2020) datasets. ABCD contains 10 flows, each describing the business logic for handling a customer request, which are relatively similar to each other. AST on ABCD is measured by joint action accuracy (JAA). We report full-shot results by training and evaluating on all flows, and zero-shot flow transfer results where the model is trained on one randomly sampled flow and evaluated on all other nine flows. The SGD test set consists of 21 services, 15 of these not seen during training. The dataset is generated via simulation with a generalized policy graph (shared across all services) encoding dialog act transitions. The per-service policy graphs are then constructed by inserting intents and slots and, as a result, end up similar.\\n\\nTables 2 and 3 show A NY-TOD results on SGD and ABCD respectively. For both datasets on both full-shot and zero-shot setups we generally see an improvement on action prediction using policy guidance, achieving state-of-the-art results for ABCD. However, the gain is not as large as STAR V 2, as the task policies are not as diverse. Even without explicit policy guidance, features from different tasks in ABCD/SGD can transfer to each other. Notably, policy guidance helps more on the one-flow setup for ABCD and unseen services for SGD, further establishing the efficacy of policy guidance on unseen setups, even if related.\\n\\n4.4 Zero-shot Transfer to MultiWOZ\\n\\nTo demonstrate A NY-TOD's generalizability and robustness in zero-shot task adaptation, we demonstrate cross-dataset transfer results on the end-to-end MultiWOZ 2.2 (Zang et al., 2020) benchmark, a popular dataset for TOD research. We train A NY-TOD-XXL on the SGD dataset, and evaluate it on MultiWOZ in zero-shot with a small policy program (Section A.7). Responses from A NY-TOD were constructed using the template utterances from Kale and Rastogi (2020). We compare against SOLOIST (Peng et al., 2020) and Mars (Sun et al., 2022), two end-to-end TOD models directly trained on MultiWOZ with supervision. Results are shown in Table 5, with metrics reported by the MultiWOZ eval script (Nekvinda and Dusek, 2021). Although no training examples from MultiWOZ was used at all, A NY-TOD demonstrates strong JGA, Inform, and Success comparable to results that do train on MultiWOZ. Note that since we applied templates for response generation, we do not consider BLEU to be important, as the responses are very different from ground truth labels.\\n\\n5 Analysis\\n\\n5.1 Impact of Policy Guidance\\n\\nTo see the value of the schema program's recommended NAP, we reevaluate already finetuned A NY-TOD models on the STAR V 2 zero-shot task transfer setting, but with changes to the program recommendations during eval. First, to see how dependent A NY-TOD is on policy graph guidance, we modify the graph to output no recommendations (denoted as 0 REC), forcing the model to do NAP only using the conversation, belief state, and action history. Secondly, we modify the graph to output deliberately bad recommendations (denoted as 1 REC), testing the resilience of the model to poor policy guidance.\\n\\n7 The results reported in Table 5 for ANYTOD-XXL were improved from an earlier version of this paper. See Section A.1 for more details.\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Table 3:** JAA on ABCD Action State Tracking (AST) for full-shot (All Flows) and zero-shot transfer (One Flow). The zero-shot JAA is the mean across three experiments.\\n\\n| Model          | SaF1 AT BASE | SaF1 AT-0 REC BASE | SaF1 AT-BADREC BASE | SaF1 AT XXL | SaF1 AT-0 REC XXL | SaF1 AT-BADREC XXL |\\n|----------------|--------------|--------------------|---------------------|------------|-------------------|--------------------|\\n|                | 60.6         | 31.3               | 25.8                | 68.0       | 39.3              | 35.0               |\\n\\n**Table 4:** STAR V2 zero-shot domains SaF1 with BADREC and 0 REC.\\n\\n| Model          | SaF1 AT BASE | SaF1 AT-0 REC BASE | SaF1 AT-BADREC BASE | SaF1 AT XXL | SaF1 AT-0 REC XXL | SaF1 AT-BADREC XXL |\\n|----------------|--------------|--------------------|---------------------|------------|-------------------|--------------------|\\n|                | 60.6         | 31.3               | 25.8                | 68.0       | 39.3              | 35.0               |\\n\\n**Figure 2:** A NY TOD error analysis on STAR V2 zero-shot domain.\\n\\n**Table 5:** Results on MultiWOZ end-to-end benchmark.\\n\\n| Model          | JGA Inform Success BLEU |\\n|----------------|-------------------------|\\n| SOLOIST        | 35.9 81.7 67.1 13.6     |\\n| Mars           | 35.5 88.9 78.0 19.6     |\\n| A NY TOD-XXL   | 30.8 76.9 47.6 3.4      |\\n\\nA NY TOD-XXL is trained on SGD, and evaluated in zero-shot transfer onto MultiWOZ. Note we applied templates for response generation, yielding low BLEU in comparison with other models.\\n\\n**Table 6:** JAA on ABCD Action State Tracking (AST) with policy corruption. On \u201cone flow\u201d, we average three runs with a randomly selected flow for training.\\n\\n| Model          | Policy Graph Error |\\n|----------------|--------------------|\\n|                | System Action Error |\\n|                | State Tracking Error |\\n| RoBERTa        | 65.8               |\\n| AST-T5-Small   | 87.9               |\\n| AT-NOREC BASE  | 90.5 47.4          |\\n| AT BASE        | 90.5 48.9          |\\n| AT-NOREC XXL   | 91.6 64.3          |\\n| AT XXL         | 91.9 67            |\\n\\nThe major drops in SaF1 for both setups shown in Table 4 confirm that the model, while able to predict actions without it, does consider the policy guidance heavily. Notably, 75% and 83% of correct predictions for 0 REC and BADREC are actions common to all tasks e.g., hello or query.\\n\\nWe conduct a similar \u201cpolicy corruption\u201d experiment on ABCD (Table 6), in which policy graphs for evaluation tasks have a 0%, 40%, and 80% chance of being replaced by graphs from incorrect flows during evaluation. We see a consistent quality drop with increasing probability of corruption for both BASE and XXL.\\n\\n**5.2 Error Analysis**\\n\\nWe also analyze A NY TOD errors on STAR V2. We classify all incorrect NAPs into three possible error categories: (1) System action error: the program recommends the correct system action, but this was not chosen by the LM, (2) Policy graph error: the predicted belief state and action history are correct, but the program\u2019s execution of the policy graph does not recommend the expected system action, and (3) State tracking error: the predicted belief states and action history are incorrect, which leads to incorrect recommendations from the policy graph. Results are shown in Figure 2. In general, we see that the benefit to scaling the LM from BASE to XXL comes from improvements to state and action tracking, which aligns with better DST and AST results on XXL as in Table 1.\\n\\n**6 Conclusion**\\n\\nWe proposed A NY TOD, an end-to-end TOD system that can be programmed to adapt to unseen tasks without domain-specific training. A NY TOD adopts a neuro-symbolic approach, in which a LM performs robust DST and AST with respect to a provided schema, and abstracts both into a sequence of symbols. These symbol sequences are then parsed and passed to an explicitly symbolic program implementing a task\u2019s dialog policy, which is executed to make recommendations for the next agent action(s). Agent designers are free to implement arbitrarily complex business logic this program A NY-TOD to determine its policy on unseen tasks or domains. To demonstrate the value of this approach, we show state-of-the-art results on zero-shot transfer TOD benchmarks, such as STAR, ABCD, SGD and MultiWOZ. For further training and benchmarking zero-shot end-to-end TOD systems, we also release the STAR V2 dataset, an improved version of STAR.\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nANYTOD is a task-oriented dialogue system designed for efficient building of conversational agents with little training data. A large 11B parameter language model (T5) is trained to make generalized structured predictions of dialogue states. A symbolic policy program takes these dialogue states as arguments, and then recommends possible actions ANYTOD should take in response to user behavior. By training on the STAR V2 dataset, ANYTOD robustly generalizes to arbitrary and unseen domains for any chatbot policies.\\n\\nConversational agents built with ANYTOD are explicitly designed to follow policies predefined by the ANYTOD schema and policy program. As such, ANYTOD is guaranteed to follow predictable and safe behavior when interacting with human users, but is not capable of taking actions outside of the discrete set of actions defined by the schema. As such, we do not intend to use ANYTOD in open-domain, free-form conversation generation scenarios.\\n\\nWhile we note that generating free-form natural language responses is possible due to supervised training on ground truth system responses, there is no guarantee that these generated responses are robust on unseen schema. We instead advocate that responses should be built with deterministic templates predefined by agent designers.\\n\\nEthics Statement\\n\\nModels, codebases, and datasets used in this paper follow their respective licenses and terms of use. Moreover, the task-oriented dialogue datasets used in this paper do not contain any personally-identifiable information or offensive content. The code for ANYTOD and the STAR V2 dataset will be released upon this paper\u2019s publication.\\n\\nOne particular risk with language models is the possible generation of factually incorrect or biased content (Lin et al., 2022; Bender et al., 2021). However, we note that this risk does not apply to ANYTOD, as (1) the language model is trained to make structured predictions that must be parseable by the policy program, and (2) we rely on response templates rather than using free form natural language generation.\\n\\nReferences\\n\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? . In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Association for Computing Machinery.\\n\\nDan Bohus and Alexander I Rudnicky. 2009. The RavenClaw dialog management framework: Architecture and systems. Comput. Speech Lang., 23(3):332\u2013361.\\n\\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Ultes Stefan, Ramadan Osman, and Milica Ga\u0161i\u0107. 2018a. Multiwoz - a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nPawel Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasic. 2018b. Multiwoz - A large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. CoRR, abs/1810.00278.\\n\\nDerek Chen, Howard Chen, Yi Yang, Alexander Lin, and Zhou Yu. 2021. Action-based conversations dataset: A corpus for building more in-depth task-oriented dialogue systems. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3002\u20133017, Online. Association for Computational Linguistics.\\n\\nJianpeng Cheng, Devang Agrawal, Hector Martinez Alonso, Shruti Bhargava, Joris Driesen, Federico Flego, Shaona Ghosh, Dain Kaplan, Dimitri Kartsaklis, Lin Li, Dhivya Piraviperumal, Jason D Williams, Hong Yu, Diarmuid O Seaghdha, and Anders Johannsen. 2020. Conversational semantic parsing for dialog state tracking.\\n\\nMihail Eric, Rahul Goel, Shachi Paul, Adarsh Kumar, Abhishek Sethi, Peter Ku, Anuj Kumar Goyal, Sanchit Agarwal, Shuyang Gao, and Dilek Hakkani-Tur. 2019. MultiWOZ 2.1: A consolidated Multi-Domain dialogue dataset with state corrections and state tracking baselines.\\n\\nGeorge Ferguson and James F Allen. 1998. TRIPS: An integrated intelligent problem-solving assistant. https://www.aaai.org/Papers/AAAI/1998/AAAI98-080.pdf. Accessed: 2022-12-14.\\n\\nRaghav Gupta, Harrison Lee, Jeffrey Zhao, Yuan Cao, Abhinav Rastogi, and Yonghui Wu. 2022. Show, don\u2019t tell: Demonstrations outperform descriptions for schema-guided task-oriented dialogue. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4541\u20134549, Seattle, United States. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, Jian Sun, and Yongbin Li. 2021. GALAXY: A generative pre-trained model for Task-Oriented dialog with Semi-Supervised learning and explicit policy injection.\\n\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue.\\n\\nNorman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski, Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar, Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Water, Walter Wang, Eric Wilcox, and Doe Hyun Yoon. 2017. In-datacenter performance analysis of a tensor processing unit. SIGARCH Comput. Archit. News, 45(2):1\u201312.\\n\\nMihir Kale and Abhinav Rastogi. 2020. Few-shot natural language generation by rewriting templates. CoRR, abs/2004.15006.\\n\\nJ F Kelley. 1984. An iterative design methodology for user-friendly natural language office information applications. ACM Trans. Inf. Syst. Secur., 2(1):26\u201341.\\n\\nChia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021. Dialogue state tracking with a language model using Schema-Driven prompting.\\n\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214\u20133252, Dublin, Ireland. Association for Computational Linguistics.\\n\\nZhaojiang Lin, Bing Liu, Seungwhan Moon, Paul Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu, Andrea Madotto, Eunjoon Cho, and Rajen Subba. 2021. Leveraging slot descriptions for Zero-Shot Cross-Domain dialogue StateTracking. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5640\u20135648, Online. Association for Computational Linguistics.\\n\\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Neuro-Logic decoding: (un)supervised neural text generation with predicate logic constraints.\\n\\nSemantic Machines, Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford, Kate Crim, Jordan DeLoach, Leah Dorner, Jackson Eisner, Hao Fang, Alan Guo, David Hall, Kristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krishnamurthy, Theo Lanman, Percy Liang, Christopher H Lin, Ilya Lintsbakh, Andy McGovern, Aleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam Thomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and Alexander Zotov. 2020. Task-Oriented dialogue as dataflow synthesis.\\n\\nShikib Mehri and Maxine Eskenazi. 2021. Schema-guided paradigm for zero-shot dialog. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 499\u2013508, Singapore and Online. Association for Computational Linguistics.\\n\\nFei Mi, Yitong Li, Yasheng Wang, Xin Jiang, and Qun Liu. 2021. CINS: Comprehensive instruction for few-shot learning in task-oriented dialog systems.\\n\\nJohannes E M Mosig, Shikib Mehri, and Thomas Kober. 2020. STAR: A Schema-Guided dialog dataset for transfer learning.\\n\\nTom\u00e1s Nekvinda and Ondrej Dusek. 2021. Shades of bleu, flavours of success: The case of multiwoz. CoRR, abs/2106.05555.\\n\\nMaxwell Nye, Michael Henry Tessler, Joshua B Tenenbaum, and Brenden M Lake. 2021. Improving coherence and consistency in neural sequence models with Dual-System, Neuro-Symbolic reasoning.\\n\\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-deh, Lars Liden, and Jianfeng Gao. 2020. SOLOIST: Building task bots at scale with transfer learning and machine teaching.\\n\\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8689\u20138696.\\n\\nCharles Rich and Candace L Sidner. 1998. COLLAGEN: A collaboration manager for software interface agents. User Model. User-adapt Interact., 8(3):315\u2013350.\\n\\nAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohtuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha 10.\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The current version of this paper is v3 (Oct 2023).\\n\\nVersions of this paper so far:\\n1. v1 (Dec 2022): The initial version of the A NY-TOD paper.\\n2. v2 (Feb 2023): An updated version that included results on SGD and zero-shot transfer onto MultiWOZ. Sent for review to EMNLP 2023 through ACL ARR.\\n3. v3 (Oct 2023): Camera ready version for EMNLP 2023. Improved results on zero-shot transfer for MultiWOZ.\\n\\nExamples of A NY-TOD program implementations for STAR V2 can be found in Figures A.2 and A.3.\\n\\nFor compactness, Table 1 showed just UaF1 and SaF1. We also report user action accuracy (UaAcc) and system action accuracy (SaAcc) in Table A.1.\\n\\nDuring the development of A NY-TOD, we found that the zero-shot domain results reported on SAM in Mehri and Eskenazi (2021) were incorrect. An annotation issue within the STAR dataset set marked some conversations as having an invalid domain; due to how SAM was implemented, these conversations would always be included in the training dataset, even if they were in the evaluation domain. For instance, dialog ID 102 is marked as a null domain in the original STAR dataset.\\n\\nRetraining SAM with this issue fixed caused a drop in SaF1, from 55.7 to 51.2. We fix these annotation errors in the STAR V2 dataset.\\n\\nDetails in calculating metrics on STAR V2 are as follows. For DST, JGA is calculated with an exact match on belief state parameters and values. For AST, we only consider the quality of the most recent turn within the action history prediction. This is always a user turn, which may have multiple user actions active. This may be considered a multilabel.\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | JGA | UaAcc | UaF1 | SaAcc | SaF1 | BLEU |\\n|-------------|-----|-------|------|-------|------|------|\\n| BERT+S      |     | -     | -    | -     | 73.8 | 74.9 |\\n| SAM         |     | -     | -    | -     | 70.4 | 71.5 |\\n| SAM-User    |     | -     | -    | -     | 70.4 | 71.7 |\\n| AT-NOGUIDE BASE | 81.5 | 74.4 | 83.8 | 73.7 | 73.3 | 72.8 |\\n| AT-TMPL BASE | 82.9 | 75.6 | 84.6 | 71.0 | 70.6 | 72.7 |\\n| AT BASE     | 82.4 | 75.2 | 84.1 | 71.6 | 70.7 | 72.0 |\\n| AT-NOGUIDE XXL | 85.6 | 78.3 | 86.4 | 75.7 | 75.4 | 76.4 |\\n| AT-TMPL XXL | 85.1 | 72.6 | 82.5 | 70.7 | 71.3 | 75.8 |\\n| AT XXL      | 85.7 | 75.9 | 84.7 | 73.8 | 73.3 | 73.5 |\\n\\n(a) Full-shot results on STAR\\n\\n| Model       | JGA | UaAcc | UaF1 | SaAcc | SaF1 | BLEU |\\n|-------------|-----|-------|------|-------|------|------|\\n| BERT+S      |     | -     | -    | -     | 29.7 | 32.3 |\\n| SAM         |     | -     | -    | -     | 49.8 | 51.2 |\\n| SAM-User    |     | -     | -    | -     | 53.9 | 44.4 |\\n| AT-NOGUIDE BASE | 57.8 | 55.4 | 71.0 | 56.1 | 55.8 | 32.4 |\\n| AT-TMPL BASE | 62.2 | 56.0 | 74.0 | 62.5 | 61.9 | 56.0 |\\n| AT BASE     | 61.9 | 56.6 | 72.1 | 61.6 | 60.6 | 34.3 |\\n| AT-SGD BASE | 66.1 | 59.5 | 74.3 | 63.5 | 61.3 | 34.4 |\\n| AT-PROG + REPLY BASE | 62.7 | 55.8 | 73.9 | 63.1 | 62.9 | 56.3 |\\n| AT-PROG BASE | 61.9 | 56.6 | 72.1 | 61.9 | 61.0 | 34.4 |\\n| AT-PROG + SGD BASE | 66.1 | 59.5 | 74.3 | 64.2 | 61.9 | 34.6 |\\n| AT-NOGUIDE XXL | 72.7 | 65.9 | 80.0 | 62.3 | 62.3 | 41.8 |\\n| AT-TMPL XXL | 66.8 | 58.9 | 72.9 | 60.9 | 60.8 | 52.9 |\\n| AT XXL      | 74.8 | 64.6 | 79.2 | 68.0 | 68.0 | 44.3 |\\n| AT-SGD XXL | 75.8 | 67.8 | 80.9 | 69.3 | 68.5 | 43.9 |\\n| AT-PROG + REPLY XXL | 73.7 | 61.6 | 76.6 | 65.7 | 66.3 | 63.6 |\\n| AT-PROG XXL | 74.4 | 64.7 | 79.3 | 68.5 | 68.4 | 44.9 |\\n| AT-PROG + SGD XXL | 75.7 | 68.5 | 81.4 | 70.8 | 70.7 | 44.2 |\\n\\n(b) Zero-shot domain results on STAR\\n\\nTable A.1: Complete results on STAR\\n\\nClassification problem. Then, we calculate UaAcc through exact set match on the predicted user actions at the current turn, as well as weighted multilabel F1 on the predicted user actions. Both SaAcc and SaF1 are calculated as described in Mosig et al. (2020).\\n\\nA.6 Implementation of Sam-User\\n\\nTo implement supervised training of AST on SAM, we modify the methodology described in Mehri and Eskenazi (2021), which embeds both conversation and schema elements to produce an attention vector $p_i$. Here, $p_i$ gives the attention weight between the conversation and the $i$-th user action of the policy graph. This is then interpreted to be a proxy for probability, and converted to a probability for NAP on all system actions $a$ according to the policy graph edges:\\n\\n$$ g(i, a) = \\\\begin{cases} p_i, & \\\\text{if } \\\\text{action}(\\\\text{next}(u_i)) = a \\\\text{, otherwise} \\\\end{cases} $$\\n\\n$$ \\\\mathbb{P}(a) = \\\\sum_{i \\\\leq |S|} g(i, a) $$\\n\\nHere, $\\\\text{action}(\\\\text{next}(u_i))$ gives the next system action of the user action $u_i$ according to the policy graph. Note that $p_i$ is an attention weight that is interpreted to be the probability of user action $u_i$ being active at the current turn; however, no supervised training was done with ground truth user action labels. Then, to implement supervised training on these user actions, we train $p_i$ to be actual probabilities, and apply a sigmoid on $p_i$ to form a user action prediction head. Note that this is a\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"multilabel binary prediction. We then calculate a binary cross-entropy loss on this head.\\n\\nA.7 MultiWOZ Zero-Shot Policy\\n\\nFigure A.1 contains the A\\\\textsubscript{NY}TOD policy program used when evaluating over MultiWOZ. This policy program was handcrafted, and provides a simplified conversation flow from our own personal understanding of the MultiWOZ dialogue policies.\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def multiwoz_policy(active_domain, belief_state, act_hist):\\n    rec = []\\n    last_useracts = act_hist[-1]\\n\\n    # We define a new action within the MultiWOZ schema that tracks whether\\n    # the user wants to book a provided entity.\\n    # Since this is zero-shot we don't train on this action at all, just provide\\n    # a natural language description \\\"user is saying they wants to book this hotel\\\"\\n    user_wants_to_book = any(act == 'user-wants-to-book' for act, _ in last_useracts)\\n\\n    if user_wants_to_book:\\n        rec.append(('book', None))\\n        # Inform the name of what we're booking for the user\\n        if active_domain in ['restaurant', 'hotel', 'attraction']:\\n            rec.append(('inform', f'{active_domain}-name'))\\n        elif active_domain == 'train':\\n            rec.append(('inform', f'train-trainid'))\\n        # Ask the user if they need anything else\\n        rec.append(('reqmore', None))\\n    else:\\n        # We're still trying to find an entity for the user\\n        # Recommend / select entities\\n        if active_domain in ['restaurant', 'hotel', 'attraction']:\\n            rec.append(('inform', f'{active_domain}-name'))\\n        elif active_domain == 'train':\\n            rec.append(('inform', f'train-trainid'))\\n        rec.append(('recommend', None))\\n        rec.append(('select', None))\\n        rec.append(('booking-inform', None))\\n\\n    for act, slot in last_useracts:\\n        if act == 'inform':\\n            # We often repeat back info the user has given us in next turn\\n            rec.append(('inform', slot))\\n        elif act == 'request':\\n            # If the user is requesting a slot, provide the value\\n            rec.append(('inform', slot))\\n        elif act == 'thank':\\n            # If the user is thanking us, say you're welcome / bye / anything else?\\n            rec.append(('welcome', None))\\n            rec.append(('bye', None))\\n            rec.append(('reqmore', None))\\n\\n    return set(rec)\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def anytod_star_policy_program(\\n    belief_state: dict[str, str],\\n    act_hist: list[list[str]],\\n    api: Json,\\n    graph: Json,\\n    convo_hist: list[str],\\n    primary_item: Json)\\n\\n    # a list of next action predictions to recommend to the lm\\n    next_act_recs = []\\n\\n    # get the \\\"bye\\\" actions for both user and system\\n    user_bye_act = _user_bye_act(graph)\\n    sys_bye_act = _sys_bye_act(graph)\\n\\n    slot_actions = graph['slot_actions']\\n    inform_user_acts = set()\\n    for _, user_acts in slot_actions.items():\\n        inform_user_acts.add(user_acts[0])\\n\\n    if act_hist:\\n        # iterate through last turn's active user actions, result of AST\\n        for last_useract in act_hist[-1]:\\n            # some transitions are common to all star graphs, but not explicit\\n            # if user is performing something out-of-scope, return out_of_scope\\n            if last_useract == USER_CUSTOM_LABEL:\\n                next_act_recs.append(OUT_OF_SCOPE_LABEL)\\n            # if user is saying bye, agent can say bye\\n            if last_useract == user_bye_act:\\n                next_act_recs.append(sys_bye_act)\\n\\n        # if the user is performing an action that isn't informing a param,\\n        # look it up in the policy graph\\n        if last_useract not in inform_user_acts and last_useract in graph['graph']:\\n            next_act_recs.append(graph['graph'][last_useract])\\n\\n        # if the agent can do the anything_else action, it can also say bye\\n        if 'anything_else' in next_act_recs:\\n            next_act_recs.append(bye_act)\\n\\n    # if all required params are provided, we can query api\\n    query_label = 'query' if 'query' in graph['replies'] else 'query_check'\\n    if all(p.name in belief_state for p in api.params if p.required):\\n        next_act_recs.append(query_label)\\n\\n    # param name -> api param json\\n    api_params_by_name = {}\\n    for param in api['input']:\\n        if param['Name'] != 'RequestType':\\n            api_params_by_name[param['Name']] = param\\n\\n    # if a param is not known, we can request it from the user\\n    for slot in graph['slot_actions']:\\n        p = api_params_by_name[slot]\\n        if p.name not in belief_state:\\n            ask_sysact = slot_actions[p.name][0]\\n            next_act_recs.append(ask_sysact)\\n\\n    return next_act_recs\\n\\nFigure A.2: The A NYTOD program implementation for a given STAR policy graph.\"}"}
{"id": "emnlp-2023-main-1006", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def anytod_star_trivia_policy(belief_state: dict[str, str], act_hist: list[list[str]], api: Json, graph: Json, convo_hist: list[str], primary_item: Json):\\n    if act_hist and len(convo_hist) >= 2:\\n        for last_useract in act_hist[-1]:\\n            if last_useract == 'user_trivia_answer':\\n                answer = primary_item.get('Answer', None)\\n                if answer:\\n                    last_user_utt = convo_hist[-2]\\n                    if answer.lower() in last_user_utt.lower():\\n                        return ['trivia_inform_answer_correct_ask_next']\\n                    else:\\n                        return ['trivia_inform_answer_incorrect_ask_next']\\n    return normal_policy(belief_state, act_hist, api, graph, convo_hist, primary_item)\\n\\ndef anytod_star_bank_policy(belief_state: dict[str, str], act_hist: list[list[str]], api: Json, graph: Json, convo_hist: list[str], primary_item: Json):\\n    # next_act_recs should be populated already by graph following same as normal_policy() ...\\n\\n    first_auth_slots = ['FullName', 'AccountNumber', 'PIN']\\n    second_auth_slots = ['FullName', 'DateOfBirth', 'SecurityAnswer1', 'SecurityAnswer2']\\n    if (all(slot in bs for slot in first_auth_slots) or\\n        all(slot in bs for slot in second_auth_slots)):\\n        next_action_recs.append('query')\\n\\n    seen_useracts = set()\\n    for turn, turn_acts in enumerate(act_hist):\\n        if turn % 2 == 0:\\n            seen_useracts.update(turn_acts)\\n    forgot_acts = ['user_bank_forgot_account_number', 'user_bank_forgot_pin']\\n    if any(fa in seen_useracts for fa in forgot_acts):\\n        is_second_auth = True\\n    else:\\n        is_second_auth = False\\n        slots = second_auth_slots if is_second_auth else first_auth_slots\\n    if graph['task'] == 'bank_fraud_report':\\n        slots.append('FraudReport')\\n    for slot in slots:\\n        if slot not in belief_state:\\n            next_act_recs.append(graph['slot_actions'][slot][0])\\n    return next_act_recs\\n\\nFigure A.3: The A NYTOD program implementation specialized for bank and trivia domains.\"}"}
