{"id": "acl-2024-long-479", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"models sometimes fail to output a label in the label list. In these cases, we assign random labels to these samples.\\n\\n### Additional Results\\n\\n#### E.1 WCM-v2\\nIn Table 13, we report the performance in different languages on WCM-v2.\\n\\n#### E.2 Flores-200\\nConsidering that encoder-only models cannot perform the MT task, we use generative models for experiments. Since the models have limited abilities to generate texts in low-resource languages, we only conduct experiments to translate the low-resource languages into Chinese.\\n\\nThe results are shown in Table 14. As they are not finetuned for the translation task, the four models generally perform poorly on this task merely through in-context learning. However, our model achieves non-zero scores, demonstrating its preliminary ability to understand these languages, thanks to the training on MC2. We leave the supervised training setting of MT for future work.\\n\\n#### E.3 XQuAD-MT\\nIn Table 15 we report the performance on the full set and EXACT subset of XQuAD-MT.\"}"}
{"id": "acl-2024-long-479", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: The supported languages of the models in our experiments and their parameters.\\n\\n| Model                | Open-source | Corpus         | Tibetan | Uyghur | Kazakh | (Arabic) | Mongolian | (Traditional) |\\n|----------------------|-------------|----------------|---------|--------|--------|----------|-----------|---------------|\\n| mBERT-base           |             |                | \u2717       | \u2717      | \u2717      | \u2717        | \u2717         |               |\\n| XLM-RoBERTa-large    |             |                | \u2717       | \u2713      | \u2717      | \u2717        | \u2717         |               |\\n| CINO-large-v2        |             |                | \u2713       | \u2713      | \u2713      | \u2713        | \u2713         |               |\\n| BLOOM-7.1B           |             |                | \u2717       | \u2717      | \u2717      | \u2717        | \u2717         |               |\\n| mT5-xxl              |             |                | \u2717       | \u2717      | \u2717      | \u2717        | \u2717         |               |\\n| ByT5-xxl             |             |                | \u2717       | \u2717      | \u2717      | \u2717        | \u2717         |               |\\n| MC2 XLMR-large (Ours)|             |                | \u2713       | \u2713      | \u2713      | \u2713        | \u2713         |               |\\n| MC2 Llama-13B (Ours) |             |                | \u2713       | \u2713      | \u2713      | \u2713        | \u2713         |               |\\n\\nTable 12: Prompt templates used in our experiments. The text in italics are the English translations of the Chinese instructions.\\n\\n| Model                | Open-source | Classification (Accuracy / Weighted F1) | Tibetan Uyghur BLOOM-7.1B | mT5-xxl | ByT5-xxl | MC2 Llama-13B (Ours) |\\n|----------------------|-------------|----------------------------------------|---------------------------|---------|----------|---------------------|\\n|                      |             | 11.8 / 9.5 5.7 / 8.9 8.4 / 7.4 9.9 / 13.0 9.2 / 8.9 | 0.4 / 1.9 0.9 / 3.4 | 0.0 / 0.3 0.0 / 0.4 | 2.5 / 4.4 6.5 / 8.2 |\\n\\nTable 13: Performance of different models on WCM-v2 under in-context learning. The best scores are made bold.\\n\\nWe use accuracy and weighted F1 as metrics.\"}"}
{"id": "acl-2024-long-479", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                   | Uyghur  | Kazakh  |\\n|-------------------------|---------|---------|\\n|                         | EM/F1   | EM/F1   |\\n| mBERT-base              | 0.2/3.8 | 0.8/6.2 |\\n| XLM-RoBERTa-large       | 2.4/9.5 | 10.9/20.9 |\\n| CINO-large-v2           | 1.8/9.8 | 8.6/20.7 |\\n| MC2                     | 2.0/9.6 | 9.3/19.5 |\\n| XLMR-large (Ours)       | 2.3/16.4| 6.5/27.0 |\\n\\nTable 15: Performance of different models on XQuAD-MT under zero-shot transfer. The best scores are made bold, with the second underlined. The scores in each cell are EM and F1.\"}"}
{"id": "acl-2024-long-479", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nCurrent large language models demonstrate deficiencies in understanding low-resource languages, particularly the minority languages in China. This limitation stems from the scarcity of available pre-training data. To address this accessibility challenge, we present MC\\\\textsuperscript{2}, a Multilingual Corpus of Minority Languages in China, which is the largest open-source corpus of its kind so far. MC\\\\textsuperscript{2} includes four underrepresented languages: Tibetan, Uyghur, Kazakh, and Mongolian. Notably, we focus on the less common writing systems of Kazakh and Mongolian, i.e., Kazakh Arabic script and traditional Mongolian script, which have been long neglected in previous corpus construction efforts. Recognizing the prevalence of language contamination within existing corpora, we adopt a quality-centric solution for collecting MC\\\\textsuperscript{2}, prioritizing accuracy while enhancing diversity. Furthermore, we underscore the importance of attending to the multiplicity of writing systems, which is closely related to the cultural awareness of the resulting models. The MC\\\\textsuperscript{2} corpus and related models are made public to the community.\\n\\n1 Introduction\\nRecently, the rapid development of large language models (LLMs) has been fueled by the availability of high-quality pre-training data. However, only a handful of high-resource languages, such as English and Chinese, have benefitted from the advantages of this progress in LLMs. Despite having a substantial user base, many languages remain excluded from the benefits of these advancements due to a lack of suitable corpora. In this paper, we focus on such underrepresented languages, particularly addressing the minority languages in China, including Tibetan, Uyghur, Kazakh, and Mongolian.\\n\\n* Equal contribution.\\n\u2020 Corresponding author.\\n\\n1 https://github.com/luciusssss/mc2_corpus\\n\\nAs illustrated in Figure 1, the four languages, although spoken by tens of millions, face a critical deficiency in linguistic resources, posing obstacles to both academic research and the development of practical AI applications. Notably, there is no open-source corpus available for Kazakh in Arabic script and Mongolian in traditional Mongolian script \u2013 two language variants in China adopting underrepresented writing systems. In the case of Tibetan and Uyghur, although several multilingual datasets such as OSCAR (Abadji et al., 2022) and CulturaX (Nguyen et al., 2023) cover these languages, their quality falls short. Our preliminary study reveals a critical issue of language misidentification. Up to 34% of data in the Uyghur split of CulturaX is actually Kazakh or Arabic texts. This raises concerns for future research in low-resource NLP, as conducting experiments on these significantly contaminated monolingual corpora may yield misleading conclusions.\"}"}
{"id": "acl-2024-long-479", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: ISO codes, languages families, and writing systems of the languages in MC\\n\\n| Language     | ISO 639-1 | ISO 639-3 | Language Family | Writing System       |\\n|--------------|-----------|-----------|-----------------|----------------------|\\n| Tibetan      | bo        | bod       | Sino-Tibetian   | Tibetan script       |\\n| Uyghur       | ug        | uig       | Turkic          | Uyghur Arabic script |\\n| Kazakh       | kk        | kaz       | Turkic          | Kazakh Arabic script |\\n| Mongolian    | mn        | mvf       | Mongolic        | Traditional Mongolian script |\\n\\nTable 2: Dataset sizes of different multilingual datasets in the four minority languages. MC^2 (crawl) denotes the subset of our newly-collected web crawls. MC^2 (full) is the complete set of MC^2, which additionally contains texts collected from existing resources.\\n\\n\u2020 We observe many cases of language misidentification in the Uyghur split of OSCAR and CulturaX. We report the data sizes after manual re-identification. The crossed-out numbers in the brackets indicate the original data sizes.\\n\\nTo facilitate more transparent and reproducible NLP research on the minority languages in China, we present MC^2, a Multilingual Corpus of Minority Languages in China, which is the largest open-source corpus for these languages so far. During data collection, we adhere to a quality-centric principle. We carefully design strategies for the selection of web pages to crawl, ensuring the language purity of the crawled texts. Afterwards, we conduct a thorough data cleaning process, eliminating extraneous elements such as web page headers and retaining only meaningful content. Our newly collected MC^2 dataset surpasses existing datasets by containing more than twice the amount of Tibetan and Uyghur data, while maintaining superior data quality.\\n\\nMC^2 particularly emphasize underrepresented writing systems, recognizing their significance both technically and culturally. In languages with multiple writing systems, such as Kazakh and Mongolian, previous research has predominantly focused on the more common scripts (Abadji et al., 2022; Nguyen et al., 2023). We illustrate that transliterating texts from high-resource scripts into lower-resource ones may not be effective for training, especially when the script conversion is flawed. Moreover, the texts collected in different scripts inherently embody cultural nuances of various language communities. Through probing, we demonstrate that writing systems play a crucial role in developing culturally-aware NLP systems.\\n\\nIn addition, we continually train language models with MC^2. They achieve comparable performance to their counterparts trained with closed-source data. This further validates the value of our data in facilitating transparent NLP research.\\n\\nTo summarize, we make the following contributions: (1) We present MC^2, the largest open-source corpus to date for four underrepresented languages in China. (2) We highlight quality issues, especially language misidentification, prevalent in previous corpora, which may threaten trustworthy research on low-resource languages. (3) We reveal the cultural complexities arising from the multiplicity of writing systems within languages.\\n\\n2 Related Works\\nMultilingual Corpus\\nTo facilitate the development of multilingual LLMs, a series of large-scale multilingual corpora has been released, including CC100 (Conneau et al., 2020), mC4 (Raffel et al., 2020), ROOTS (Lauren\u00e7on et al., 2022), OSCAR (Abadji et al., 2022), CulturaX (Nguyen et al., 2023), and Madlad-400 (Kudugunta et al., 2024). These corpora cover both high-resource languages and a handful of low-resource ones. Recently, there are also efforts in constructing open-source corpora for low-resource languages in certain regions (Cahyawijaya et al., 2021; Teodorescu et al., 2022; Doddapaneni et al., 2023). However, the minority languages in China are still underrepresented in existing datasets, which calls for more attention to constructing open-source corpus for these languages.\"}"}
{"id": "acl-2024-long-479", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In terms of the minority languages in China, previous works try to improve the accessibility of these languages, by collecting a handful of annotated datasets for specific NLP tasks, each covering one or some languages investigated in our study. They mainly focus on three types of tasks: text classification (Qun et al., 2017; Yang et al., 2022; Shi et al., 2022b), question answering (Sun et al., 2021), and machine translation (Costa-juss\u00e0 et al., 2022; Zhang et al., 2024). The models trained specifically for the minority languages in China include CINO (Yang et al., 2022), MiLMo (Shi et al., 2022b), and CMPT (Li et al., 2022). However, none of these works releases their pre-training corpus. In contrast, we release a large-scale high-quality corpus for four minority languages in China, which, we hope, will help relieve the data scarcity problem and improve transparency in research on low-resource languages.\\n\\nCulturally-Aware NLP\\n\\nCultural considerations are increasingly recognized as crucial in NLP research and applications (Hershcovich et al., 2022). An important aspect is the connection between cultural factors and linguistic forms within languages. Previous works on intra-language variations have primarily focused on dialects (Zampieri et al., 2020; Ziems et al., 2022; Liu et al., 2023) and sociolinguistic differences (McCormack et al., 2011; Zhang et al., 2021). However, the dimension of writing systems within languages has received little attention. We demonstrate that texts in underrepresented writing systems may encode unique cultural nuances that differ significantly from those found in communities using more common scripts.\\n\\n3 Data Collection and Analysis\\n\\nHere we describe the procedure of creating MC\u00b2, the largest open-source corpus so far for four minority languages in China. Before collecting MC\u00b2, we first conduct a preliminary audit in the low-resource language splits of previous multilingual corpora and find severe quality issues such as language contamination. Therefore, we propose a quality-centric solution for data collection of low-resource languages, which aims to ensure accuracy while improving the comprehensiveness and coverage of the data.\\n\\nThe collection procedure of MC\u00b2 mainly consists of three steps: (1) gathering web crawls, (2) incorporating existing datasets, and (3) deduplicating and filtering. Throughout the collection process, we adhere to the protocol of quality-centric collection. We hope to establish a reliable groundwork for subsequent language model training or linguistic research. Notably, our framework for data collection is generally language-agnostic and can be easily applied to collecting web corpora for other low-resource languages.\\n\\n3.1 Quality Issues in Previous Corpora\\n\\nWhen auditing previous multilingual web corpora for low-resource languages (Abadji et al., 2022; Nguyen et al., 2023), we find two critical quality issues: language misidentification and insufficient data cleaning. These defects pose a significant threat to effective model training and might undermine the credibility of research findings.\\n\\nLanguage Misidentification\\n\\nAn important step in collecting web corpora for low-resource languages is to identify the language of a web page. However, current language identification tools such as fastText (Joulin et al., 2016) are prone to error (Kreutzer et al., 2022). This issue is more severe in languages with similar writing systems. Specifically, we audit the Uyghur splits in CultureX and OSCAR. Since the documents in these two datasets are paired with their respective URLs, we can efficiently label their languages according to the website of their URLs. We manually check the 654 websites comprising the Uyghur corpus in CultureX. Our examination reveals that 16% of the data is actually in Kazakh or Arabic, languages utilizing scripts akin to that of Uyghur. Similarly, 33% of the Uyghur corpus in OSCAR are Kazakh or Arabic. See the example of misidentification in Appendix A.1.\\n\\nThis issue presents a significant concern for future research in low-resource NLP, as experiments conducted on heavily contaminated corpora can produce misleading results. For instance, this contamination can adversely impact research on language transfer. Researchers may aim to evaluate whether a model trained on a monolingual corpus in Language A can transfer its capabilities to Language B. However, if the corpus already contains a considerable amount of Language B, the target language, the conclusions from such experiments are unlikely to be reliable.\\n\\nInsufficient Data Cleaning\\n\\nThe web crawls in previous corpora often contain unwanted texts such as spam and noise. This can negatively affect the performance of downstream NLP tasks. We address this issue by implementing a series of data cleaning steps, including removing spam and filtering out irrelevant texts. By doing so, we ensure that the final corpus is clean and ready for use by researchers.\\n\\nOverall, the creation of MC\u00b2 represents a significant step forward in the field of NLP for minority languages in China. Our work not only provides a valuable resource for future research but also highlights the importance of cultural awareness in NLP. By focusing on underrepresented languages and ensuring high-quality data, we hope to promote more inclusive and accurate NLP research.\"}"}
{"id": "acl-2024-long-479", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We sample 100 pages from the Tibetan corpus of CulturaX and our manual annotation shows that 42% contain headers or footers. These undesired texts affect the coherence of the document texts, which might hinder models from learning the linguistic patterns of low-resource languages.\\n\\n3.2 Step 1: Web Crawling\\n\\nOur corpus is mainly made up of web crawls. We combine both human labor and AI assistance to prevent the flaws in the previous corpora.\\n\\nLanguage Identification\\n\\nDifferent from previous efforts for curating massively-multilingual web crawls (Nguyen et al., 2023), the number of websites in each language is limited for the four minority languages in our study. We manually maintain a list of high-quality websites for each language of our study, to avoid language contamination resulting from mislabeling by identification tools. These websites are collected from a variety of sources, including URLs in existing corpora, search engines, web portals, website recommendations on social media, and hyperlinks on these websites. For each website in the list, we start from the homepage and crawl all the pages through a breadth-first search.\\n\\nText Extraction\\n\\nThe web crawls in previous corpora often contain unwanted texts such as sidebars, headers, and footers. As the number of websites in our corpus is manageable and the web pages in the same website usually share the same structure, we automatically design text extraction rules for each website. These rules can precisely extract the title and main content of a web page, discarding other distracting texts. Specifically, for each website, we ask Github Copilot to analyze the HTML structure of a sampled web page and write a Python code to extract its title and main content. In this way, we can extract the wanted texts in the raw web crawls with high accuracy and efficiency.\\n\\nIn total, we collected 2.0G Tibetan, 1.1G Uyghur, 1.3G Kazakh, and 1.7G Mongolian texts in our new web crawls.\\n\\n3.3 Step 2: Incorporation of Existing Datasets\\n\\nThanks to the existing efforts in the community, we incorporate open-source resources into our corpus, including CulturaX (Nguyen et al., 2023), Wikipedia, and NLGIW 2023 Shared Task. We remove the data with language contamination when merging these datasets.\\n\\nCulturaX is a cleaned collection of mC4 and OS-CAR. Its Tibetan and Uyghur splits can be used to supplement our corpus. However, there are errors in language identification in CulturaX, especially the confusion between Uyghur texts and Kazakh texts in Arabic scripts. We thus conduct language re-identification on the Uyghur split of CulturaX, using the website-centric strategy explained in Section 3.1. From CulturaX, we obtained 718M Tibetan, 338M Uyghur, and 65M Kazakh texts.\\n\\nWikipedia is a high-quality resource for low-resource languages. We collected 127M Tibetan and 41M Uyghur texts from Wikipedia. NLGIW 2023 Shared Task provides an open-source corpus of Tibetan news, where we add 342M texts.\\n\\n3.4 Step 3: Deduplication and Filtering\\n\\nPrevious works claim that properly filtered and deduplicated web data is crucial to effective pre-training (Lee et al., 2022; Penedo et al., 2023; Ranathunga et al., 2024). Therefore, we take a series of measures for deduplication and filtering to ensure the high quality of our corpus.\\n\\nDeduplication\\n\\nWe find two sources of duplication in our corpus. One is that the websites in minority languages often repost from each other. The other is the duplication between the web crawls in Step 1 and the existing corpus merged in Step 2. We thus use a combination of deduplication methods, including URL-based, exact, and fuzzy deduplication. See the implementation details in Appendix A.2.\\n\\nFiltering\\n\\nWe design the filtering rules from several aspects, including repetition, document lengths, and unexpected characters. See details in Appendix A.2. We additionally make efforts to remove privacy information from the corpus. We use heuristics to identify emails, telephone numbers, and Resident Identity Card numbers. We replace them with special tokens, i.e., [email], [phone], and [idcard].\\n\\nAfter deduplication and filtering, we obtain 2.2G Tibetan, 736M Uyghur, 937M Kazakh, and 970M Mongolian texts, which compose the current version of MC2.\"}"}
{"id": "acl-2024-long-479", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.5 Statistical Analysis of MC\\n\\nWe provide an in-depth analysis from the perspective of domain diversity and document length.\\n\\nDomain Diversity\\n\\nFor the pretraining corpus, both quality and diversity are crucial. When curating the website list for crawling, we comprehensively collect high-quality websites from diverse domains to enhance the dataset's diversity while ensuring quality. Specifically, our data sources fall into five categories: News, Culture, Government, Wikipedia, and Others. Figure 2 shows the proportion of different categories of websites in MC.\\n\\nOur corpus covers formal official documents, and miscellaneous news articles about politics, important events, daily life, and entertainment, as well as cultural content related to literature, art, and religion. It is noteworthy that news articles constitute the majority of our datasets. News articles are usually grammatically correct, which could potentially facilitate learning language modeling under a low-resource setting.\\n\\nDocument Length\\n\\nConstrained by the absence of high-quality long-form texts, prior efforts like CINO (Yang et al., 2022) have to train their models with a collection of individual sentences, which might restrict their abilities on longer texts. In contrast, our work introduces a document-level corpus. We report the distribution of document length for each language in Figure 3. The average document length of MC far exceeds the maximum input length, i.e., 512, of the previous BERT-size model for minority languages in China. We observe that CultureX also includes long documents. However, as mentioned in Section 3.2, its data cleaning might not be adequate, introducing undesired texts like sidebars, headers, and footers. These issues could disrupt the semantic coherence of the entire document, making it challenging for models to learn correct dependencies within long texts. We expect that our high-quality, long-form corpus will assist future efforts in training models with better capacities for understanding and generating longer texts in minority languages.\\n\\n4 Discussion on Underrepresented Writing Systems\\n\\nMany languages adopt distinct writing systems across various regions. For instance, in China, minority languages such as Kazakh and Mongolian employ scripts that differ from the Cyrillic scripts used in Kazakhstan and Mongolia, as illustrated in Table 3. Unfortunately, existing datasets predominantly concentrate on the more prevalent writing systems, neglecting the less common ones. In re-\"}"}
{"id": "acl-2024-long-479", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: Comparison between training XLM-RoBERTA-large with authentic and transliterated traditional Mongolian data. (a) The loss on the training data. (b) The perplexity on the hold-out evaluation data from the authentic traditional Mongolian corpus.}\\n\\n4.1 Flawed Training with Imperfect Transliteration\\n\\nA common approach to addressing different writing systems is transliteration (Nakov and Ng, 2009; Chakravarthi et al., 2019; Muller et al., 2021; Sun et al., 2022). To obtain a model for low-resource scripts, it is intuitive to transliterate the corpus in the high-resource scripts into low-resource ones for training. However, there are no one-to-one conversion rules between scripts for languages such as Mongolian (Bao et al., 2014). The transliteration between traditional and Cyrillic Mongolian is context-dependent and current open-source tools are far from perfect. We investigate how the noisy transliteration results affect the training of models.\\n\\nExperimental Setup\\n\\nWe prepare two versions of training data in traditional Mongolian. One is 900M transliterated traditional Mongolian data, which is converted from the Cyrillic Mongolian data in CulturaX, using the only open-source transliteration tool available so far. The transliterated results often contain errors, as shown by the examples in Appendix C. The other version of data is an equivalent volume of authentic traditional Mongolian data in MC$^2$. We then train XLM-RoBERTa-large (Conneau et al., 2019) with these two different versions of data respectively and compare the convergence speed and performance of the models.\\n\\nResults\\n\\nThe training processes are illustrated in Figure 4. The training loss of transliterated data is significantly higher than that of authentic data. This discrepancy arises due to the substantial noise in transliterated data, making it challenging for the model to capture underlying linguistic patterns. We further measure the perplexity of both models on a hold-out validation set of traditional Mongolian text. The perplexity of the model trained on authentic data steadily decreases with the training steps, maintaining a similar magnitude to the training loss. Conversely, the model trained on transliterated data exhibits a sharp increase in perplexity when evaluated on authentic data, indicating that it fails to learn the language modeling of true traditional Mongolian.\\n\\nLessons Learned\\n\\nUsing noisy data transliterated from high-resource writing systems will greatly hinder the learning of low-resource writing systems. This highlights the necessity of collecting an authentic corpus for underrepresented scripts in the absence of mature transliteration tools.\\n\\n4.2 Cultural Differences Behind Writing Systems\\n\\nFor some languages such as Kazakh, we can achieve perfect transliteration between different writing systems using pre-defined rules. Nevertheless, there exist disparities in the cultural backgrounds between the language variants using different writing systems.\"}"}
{"id": "acl-2024-long-479", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With the technique of probing (Jiang et al., 2020; Zhang et al., 2021), we investigate whether the training data collected from different writing systems will lead to distinct cultural knowledge in the resulting models.\\n\\n### Probing Questions\\n\\nWe take the Kazakh language as our research target. The Kazakh community in China uses the Arabic script while the Cyrillic script is adopted in Kazakhstan. We design probing questions that reflect the cultural differences between the two Kazakh communities, such as those in Table 4. Each question offers two options: Option A aligns with the cultural context of the Kazakh community in China, while Option B corresponds to the cultural context of Kazakhstan. These questions are manually formulated by comparing Wikipedia articles that describe the two communities. We compile 25 pairs of questions covering topics such as geography, politics, and daily life. Originally written in English, these questions are translated into Cyrillic Kazakh using Google Translate, and then transliterated into Arabic Kazakh.\\n\\n### Experimental Setup\\n\\nWe train two distinct Kazakh language models based on XLM-RoBERTa-large, each tailored to one of the writing systems. One is trained with 900M authentic Cyrillic Kazakh texts from CulturaX. And the other is trained with an equivalent volume of Arabic Kazakh texts from our MC corpus. We subject the two models trained on different scripts to the cultural probing questions. We query the Arabic Kazakh model with questions written in the Arabic script. Similarly, for the Cyrillic Kazakh model, we use questions written in the Cyrillic script.\\n\\n### Results\\n\\nThe Arabic Kazakh model selects Option A for 84% of the probing questions, indicating a strong alignment with the cultural characteristics of Kazakh communities in China. Conversely, the Cyrillic Kazakh model chooses Option B for 56% of the questions, reflecting the cultural characteristics of Kazakh communities in Kazakhstan. The cultural distinction is probably inherent in the corpus collected from websites using different writing systems. Subsequently, it is captured by the models trained on data in different scripts.\\n\\nIn Table 4, we present three examples of probing questions. For Query 1, the two models yield divergent responses concerning the holiday celebrated on May 1st. This discrepancy arises from the fact that the official holidays on this date differ between China and Kazakhstan. Similarly, Query 2 and Query 3 reflect the cultural differences in terms of economy and geography. See their explanations in Appendix B.\\n\\n### Lessons Learned\\n\\nThe corpora in different writing systems contain unique cultural context specific to their respective communities. Hence, it is sub-optimal to directly transliterate high-resource scripts into low-resource ones for training, which may undermine the cultural nuances underlying the low-resource writing system. The inclusiveness of writing systems is crucial to the construction of culturally-aware models. We encourage future research on properly leveraging the data in the high-resource writing systems while preserving the cultural uniqueness in the underrepresented scripts.\\n\\n5 Continual Pretraining with MC\\n\\nTo demonstrate the practical value of our corpus, we train models with MC and compare their performance with competitive counterparts. Instead of training from scratch, we conduct continual pretraining on existing models with MC, which is an effective technique to adapt models to new languages (Ebrahimi and Kann, 2021; Muller et al., 2021; Yong et al., 2023). We continually pretrain XLM-RoBERTa-large (Conneau et al., 2019) with MC, obtaining MC2XLMR-large. Thanks to the high quality of MC, it performs comparably to CINO, a counterpart trained with closed-source data. We also attempt to adapt larger models such as Llama (Touvron et al., 2023) to the four languages, obtaining MC2Llama-13B. The newly trained model shows the superior ability of in-context learning in these languages.\"}"}
{"id": "acl-2024-long-479", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Performance of different models under the zero-shot transfer setting. The best scores are made bold, with the second underlined. For classification, we use accuracy and weighted F1 as metrics. For QA, we use EM and F1.\\n\\n| Model                  | Open-source | Corpus |\\n|------------------------|-------------|--------|\\n| mBERT-base             | No          | bo     |\\n| XLM-RoBERTa-large      | No          | ug     |\\n| CINO-large-v2          | No          | kk     |\\n| MC^2 XLMR-large (Ours) | Yes         | mn     |\\n| mBERT-base             | No          | All    |\\n| XLM-RoBERTa-large      | No          | bo     |\\n| CINO-large-v2          | No          | ug     |\\n| MC^2 XLMR-large (Ours) | Yes         | kk     |\\n| CINO-large-v2          | No          | mn     |\\n\\n5.1 Model Training\\n\\nTwo models with different architectures and sizes are used for continual pretraining. One is XLM-RoBERTa-large (Conneau et al., 2019), a 560M encoder-only multilingual model. The other is Llama2-ZH-13B (Huang et al., 2023), a bilingual model supporting both English and Chinese. The vocabularies of XLM-RoBERTa-large and Llama2-ZH-13B hardly contain tokens for the four languages in our study. Thus, we add 3K new tokens for each language, obtained by BPE (Sennrich et al., 2016).\\n\\nTo avoid the catastrophic forgetting of learned languages, we add 0.25B tokens of Chinese data from Wanjuan (He et al., 2023) and 0.25B tokens of English data from C4 (Raffel et al., 2020) for training. See more training details in Appendix D.1.\\n\\n5.2 Evaluation Setup\\n\\nDatasets\\n\\nFor the four minority languages, we can only find limited datasets for evaluation. WCM-v2 (Yang et al., 2022) is a 10-category text classification dataset. TibetanQA (Sun et al., 2021) is a Tibetan machine reading comprehension (MRC) dataset. Flores-200 (Costa-juss\u00e0 et al., 2022) contains machine translation (MT) tasks for Tibetan and Uyghur. We additionally construct an MRC dataset, XQuAD-MT, by translating XQuAD (Artetxe et al., 2020) into Uyghur and Tibetan with Google Translate. All the evaluated datasets only consist of testing sets. See data statistics in Appendix D.2.\\n\\nSettings\\n\\nFor smaller encoder-only models, we adopt zero-shot transfer, i.e., finetuning a model with instances in a high-resource language and directly testing it on the low-resource language instances (Artetxe et al., 2020). For WCM-v2, we use its Chinese instances for training. For TibetanQA and XQuAD-MT, we use CMRC2018 (Cui et al., 2019), a Chinese MRC dataset for training. For larger generative models, we adopt in-context learning, providing exemplars in the prompt (Brown et al., 2020). See the prompts in Appendix D.3.\\n\\nCompared Models\\n\\nWe compare our models with a wide range of multilingual models, including CINO (Yang et al., 2022), mBERT (Devlin et al., 2019), XLM-RoBERTa (Conneau et al., 2019), BLOOM (BigScience et al., 2022), mT5 (Xue et al., 2021) and ByT5 (Xue et al., 2022). These models (potentially) process one or more languages in our study. XLM-RoBERTa is a multilingual pretrained language model (PLM), whose vocabulary contains 3 Tibetan tokens, 5 Mongolian tokens, and more than 14K tokens in the Arabic script. However, it has only been pretrained in Uyghur. CINO is the largest PLM so far focusing on the minority languages in China, but its pretraining corpora are closed source. BLOOM, mT5, and ByT5 are multilingual LLMs, which can represent tens of languages. Although they are not trained in the four minority languages, their tokenizers can encode unseen languages instead of treating them as unknown tokens. See more details of the supported languages and numbers of parameters in Appendix D.3.\\n\\n5.3 Results\\n\\nWe show the experiment results of zero-shot transfer in Table 5, with XLM-RoBERTa-large as the baseline. We find both MC^2 XLMR-large and CINO can outperform vanilla XLM-RoBERTa-large on the classification and MRC tasks. It indicates that pretraining on minority language corpora can effectively enhance the model's ability to represent these languages. When comparing the two models for Chinese minority languages, we find our MC^2 XLMR-large can exhibit comparable performance to CINO, which is trained on a closed-source corpus three times larger than MC^2. This proves the high quality of MC^2, which can contribute to more transparent and reproducible\"}"}
{"id": "acl-2024-long-479", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6 illustrates the results of LLMs under the in-context learning setting. The performance of evaluated multilingual LLMs on WCM-v2 resembles random guessing, with an accuracy of around 10% in the ten-class classification tasks. These models struggle with the Tibetan MRC task. In contrast, the MC\\\\textsuperscript{2} Llama-13B model, pretrained on the MC\\\\textsuperscript{2} corpus, outperforms others by more than +26% accuracy and F1 on WCM-v2. It also achieves 31.5% F1 on TibetanQA. These results show that MC\\\\textsuperscript{2} Llama-13B has a superior ability to handle tasks in the four minority languages of China. Besides smaller models, MC\\\\textsuperscript{2} can also be leveraged for the pretraining of LLMs, thereby effectively enhancing the models' ability to represent these languages.\\n\\nThe results on Flores-200 and XQuAD-MT are reported in Appendix E. The models trained on MC\\\\textsuperscript{2} also demonstrate superior performance on these tasks.\\n\\n6 Conclusion\\n\\nWe present MC\\\\textsuperscript{2}, an open-source corpus for four minority languages in China. It is also the first corpus focusing on two underrepresented writing systems, i.e., the Kazakh Arabic script and the traditional Mongolian script. To address the severe quality issues in previous low-resource datasets, we adopt a quality-centric strategy to collect MC\\\\textsuperscript{2}.\\n\\nWe also emphasize the cultural significance of including low-resource writing systems through empirical studies. Our data and models are openly accessible to the community to facilitate research and applications on low-resource languages.\\n\\nLimitations\\n\\nData Sources\\n\\nMC\\\\textsuperscript{2} is mainly composed of web crawls. We have done our best to enrich the domains covered by the MC corpus. However, it is not feasible to collect data from all websites worldwide. In this work, due to copyright constraints, we are also unable to collect and publicly release a corpus sourced from books in these minority languages. We note that the texts from books can also be used to pretrain language models.\\n\\nNews articles are one of our primary data sources, which are crawled from public websites. The data could potentially contain the biases of these media. Therefore, the data should be used with caution to avoid potential biases and misrepresentations.\\n\\nProbing Questions\\n\\nIt is challenging to collect the kind of probing question in our study, which requires mining the differences between the two communities and formulating them as multi-choice questions. We made great efforts and collected 25 verified probing questions covering a diverse range of topics. As a preliminary study, the size of probing questions is small. We plan to design efficient methods for cultural question sourcing and expand the data scale.\\n\\nAcknowledgments\\n\\nThis work is supported in part by NSFC (62161160339) and Beijing Science and Technology Program (Z231100007423011). We thank the anonymous reviewers for their valuable suggestions. We thank native speakers of minority languages who contribute to this work. For any correspondence, please contact Yansong Feng.\\n\\nReferences\\n\\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Beno\u00eet Sagot. 2022. Towards a cleaner document-oriented multilingual crawled corpus. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 4344\u20134355, Marseille, France. European Language Resources Association.\\n\\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637, Online. Association for Computational Linguistics.\\n\\nFeilong Bao, Guanglai Gao, Xueliang Yan, and Hongxi Wei. 2014. Research on conversion approach between traditional mongolian and cyrillic mongolian. Computer Engineering and Computation, 23:206\u2013211.\"}"}
{"id": "acl-2024-long-479", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BigScience, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, and Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\\n\\nA. Z. Broder. 1997. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of Sequences 1997 (Cat. No. 97TB100171), pages 21\u201329.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nSamuel Cahyawijaya, Genta Indra Winata, Bryan Wilie, Karissa Vincentio, Xiaohong Li, Adhiguna Kun-coro, Sebastian Ruder, Zhi Yuan Lim, Syafri Bahar, Masayu Khodra, Ayu Purwarianti, and Pascale Fung. 2021. IndoNLG: Benchmark and resources for evaluating Indonesian natural language generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8875\u20138898, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nBharathi Raja Chakravarthi, Mihael Arcan, and John P. McCrae. 2019. Comparison of different orthographies for machine translation of under-resourced dravidian languages. In 2nd Conference on Language, Data and Knowledge (LDK 2019). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\\n\\nMarta R. Costa-Juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\\n\\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. 2019. A span-extraction dataset for Chinese machine reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5883\u20135889, Hong Kong, China. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nSumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, and Pratyush Kumar. 2023. Towards leaving no Indic language behind: Building monolingual corpora, benchmark and models for Indic languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12402\u201312426, Toronto, Canada. Association for Computational Linguistics.\\n\\nA. Seza Do\u011fru\u00f6z, Sunayana Sitaram, Barbara E. Bullock, and Almeida Jacqueline Toribio. 2021. A survey of code-switching: Linguistic and social perspectives for language technologies. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1654\u20131666, Online. Association for Computational Linguistics.\\n\\nAbteen Ebrahimi and Katharina Kann. 2021. How to adapt your pretrained multilingual model to 1600 languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4555\u20134567, Online. Association for Computational Linguistics.\\n\\nConghui He, Zhenjiang Jin, Chao Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, JiaQi Wang, and Dahua Lin. 2023. Wanjuan: A comprehensive multimodal dataset for advancing English and Chinese large models. arXiv preprint arXiv:2308.10755.\\n\\nDaniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Pi-queras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders S\u00f8gaard. 2022. Challenges and strategies in cross-cultural NLP. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997\u20137013, Dublin, Ireland. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-479", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-479", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Surangika Ranathunga, Nisansa De Silva, Velayuthan Menan, Aloka Fernando, and Charitha Rathnayake. 2024. Quality does matter: A detailed look at the quality and utility of web-mined parallel corpora. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 860\u2013880, St. Julian's, Malta. Association for Computational Linguistics.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.\\n\\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2022a. Language models are multilingual chain-of-thought reasoners. Preprint, arXiv:2210.03057.\\n\\nHanru Shi, Sisi Liu, Xinhe Yu, Wugedele Bao, Yuan Sun, and Xiaobing Zhao. 2022b. Milmo: Minority multilingual pre-trained language model. arXiv preprint arXiv:2212.01779.\\n\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.\\n\\nSimeng Sun, Angela Fan, James Cross, Vishrav Chaudhary, Chau Tran, Philipp Koehn, and Francisco Guzm\u00e1n. 2022. Alternative input signals ease transfer in multilingual machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5291\u20135305, Dublin, Ireland. Association for Computational Linguistics.\\n\\nYuan Sun, Sisi Liu, Chaofan Chen, Zhengcuo Dan, and Xiaobing Zhao. 2021. Teaching machines to read and comprehend tibetan text. Journal of Computer and Communications, 9(09):143\u2013152.\\n\\nDaniela Teodorescu, Josie Matalski, Delaney Lothian, Denilson Barbosa, and Carrie Demmans Epp. 2022. Cree corpus: A collection of n\u00eahiyaw\u00eawin resources. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6354\u20136364, Dublin, Ireland. Association for Computational Linguistics.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n\\nGenta Winata, Alham Fikri Aji, Zheng Xin Yong, and Thamar Solorio. 2023. The decades progress on code-switching research in NLP: A systematic survey on trends and challenges. In Findings of the Association for Computational Linguistics: ACL 2023, pages 2936\u20132978, Toronto, Canada. Association for Computational Linguistics.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moir, Pierre Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291\u2013306.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.\\n\\nZiqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and Zhigang Chen. 2022. CINO: A Chinese minority pre-trained language model. In Proceedings of the 29th International Conference on Computational Linguistics, pages 3937\u20133949, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\\n\\nZheng Xin Yong, Hailey Schoelkopf, Niklas Muenighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Winata, Stella Biderman, Edward Raff, Dragomir Radev, and Vasilina Nikoulina. 2023. BLOOM+1: Adding language support to BLOOM for zero-shot prompting. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11682\u201311703, Toronto, Canada. Association for Computational Linguistics.\\n\\nMarcos Zampieri, Preslav Nakov, and Yves Scherrer. 2020. Natural language processing for similar languages, varieties, and dialects: A survey. Natural Language Engineering, 26(6):595\u2013612.\\n\\nChen Zhang, Xiao Liu, Jiuheng Lin, and Yansong Feng. 2024. Teaching large language models an unseen language on the fly. arXiv preprint arXiv:2402.19167.\"}"}
{"id": "acl-2024-long-479", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-479", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Corpus Details\\n\\nA.1 Quality Issues in Previous Corpora\\n\\nWe provide several cases of quality issues in previous corpora.\\n\\nLanguage Misidentification\\n\\nIn the Uyghur split of CulturaX, the website kazakh.people.com.cn ranks third in terms of document count, comprising 9% of the total documents in the dataset. However, this website is a Kazakh-language news website. These documents are mislabeled as Uyghur because the website uses the Kazakh Arabic script, which is similar to the Uyghur Arabic script.\\n\\nInsufficient Data Cleaning\\n\\nIn Figure 5, we show an example of insufficient data cleaning from the Tibetan corpus in CulturaX. The highlighted parts are irrelevant texts on a web page, such as headers and footers.\\n\\nA.2 Deduplication and Filtering\\n\\nWe describe in detail the de-uplication and filtering process of MC\\\\textsuperscript{2}.\\n\\nDeduplication\\n\\nWe first discard the web pages from different sources with the same URLs. We then adopt both exact and fuzzy deduplication. For exact deduplication, we use the SHA-256 hash function to calculate the hash value of each web page's content and keep only one page for the pages with the same hash value. For fuzzy deduplication, we remove similar web pages by applying MinHash (Broder, 1997): for each page, we compute the minhash values and measure its approximate similarity with other pages, removing pairs whose minhash values are the same in at least one bucket. We use the same parameters as Lee et al. (2022): \\\\( n = 5 \\\\) (5-grams), \\\\( b = 450 \\\\), and \\\\( r = 20 \\\\).\\n\\nFiltering\\n\\nWe use the following rules:\\n\\n- Repetition: We remove the documents that have a high ratio of paragraph repetition or n-gram repetition.\\n- Document Length: We remove the documents whose lengths are below the threshold.\\n- Unexpected Character: We remove the document whose ratios of the characters in the target language are below the threshold.\\n\\nNote that the hyperparameters in the heuristic for filtering English corpora are not necessarily applicable to low-resource languages due to the difference in encodings and linguistic characteristics. One needs to decide the hyperparameters for each language respectively.\\n\\nA.3 Code-Switching in MC\\\\textsuperscript{2}\\n\\nCode-switching is an important and interesting phenomenon in multilingual NLP. Here we share some analyses and findings of the code-switching phenomenon present in our corpus. We mainly focus on the code-switching between Chinese and the four minority languages in China.\\n\\nFirst, we calculate the proportion of documents containing Chinese characters for the four languages in MC\\\\textsuperscript{2}. The results are shown in Table 7. 3.3% of the documents in MC\\\\textsuperscript{2} contain Chinese characters, indicating the commonness of the code-switching phenomenon in our corpus. Uyghur exhibits the highest rate of code-switching. We find that the Uyghur documents in our corpus are more likely to attach Chinese translations after entity mentions.\\n\\nSecond, we analyze the types of code-switching present in our corpus. Inspired by previous works (Do\u02d8gru\u00f6z et al., 2021; Winata et al., 2023),\"}"}
{"id": "acl-2024-long-479", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we classify documents containing Chinese characters into three types:\\n\\n- Intra-sentential code-switching: switching that occurs in a sentence where a word was switched to another language.\\n- Inter-sentential code-switching: switching that occurs in two different sentences.\\n- Attached translations of entities: In many cases, authors of articles often append Chinese translations after entity mentions. Strictly speaking, this doesn't fall under code-switching, but this situation is quite common.\\n\\nWe sample 50 documents containing Chinese characters and manually classify them into three types. The proportion of each type is shown in Table 8. The most common phenomenon is attaching Chinese translations to the entities mentioned in the documents. The common entities under these circumstances include scientific and cultural terms. Regarding intra-sentential code-switching, the switched phrases are often entities of Chinese origin, such as Han Chinese names and literary works written in Chinese. These Chinese terms currently lack official translations in these languages, so they are directly borrowed in their original form.\\n\\nMany instances of inter-sentential code-switching are sentences in different languages, where the sentences before and after each other are translations of one another. Based on this property, we can mine parallel sentences from the corpus, which we leave as future work.\\n\\nB Cultural Probing\\n\\nWe provide a detailed explanation of the probing cases in Table 4. For Query 1, May 1st is a public holiday in both China and Kazakhstan. However, the people in the two countries celebrate different festivals. In China, people celebrate International Workers' Day on May 1st, to commemorate the struggles of laborers and the working class for the eight-hour workday. However, according to the Kazakhstan law signed on October 18, 1995, May 1st was renamed People's Unity Day, and the Soviet-era Labor Day was formally canceled.\\n\\nFor Query 2, in mainland China, the official currency is Renminbi. Meanwhile, in Kazakhstan, the official currency is Kazakhstani Tenge.\\n\\nFor Query 3, Sayram Lake is an endorheic freshwater lake in the northern Tianshan Mountains, near to Ili Kazakh Autonomous Prefecture in China. Its name, Sayram, is believed to originally derive from Kazakh, meaning blessing. Sayram Lake is famous in China as the last drop of the Atlantic tears. Lake Balkhash is also famous in Kazakhstan since it is the largest lake in Kazakhstan. Around 20% of Kazakhstani people are living in the drainage basin of Lake Balkhash, including the residents of Almaty, the largest city in Kazakhstan.\\n\\nC Mongolian Transliteration\\n\\nWe use an open-source tool to transliterate Cyrillic Mongolian into traditional Mongolian. It is trained on 80K sentence pairs crawled from a closed-source transliteration system. When transliterating a Cyrillic Mongolian corpus, we find that the transliteration quality deteriorates with the increase in the input length. So we split each document into sentences and do the transliteration at a sentence level to improve the quality. However, we find that the transliteration results are imperfect, especially in the case of long sentences. In Figure 6, the transliteration tool produces a bad result for a long input sentence, repeating similar words at the end of the output.\\n\\nD Experiment Details\\n\\nD.1 Model Training\\n\\nHyperparameters\\n\\nWe list in Table 9 the hyperparameters for the continual pretraining of MC$_2$XLMR-large and MC$_2$Llama-13B.\\n\\nTraining Frameworks\\n\\nFor MC$_2$XLMR-large, we use Transformers (Wolf et al., 2020) for training. For MC$_2$Llama-13B, we use Megatron (Shoeybi et al., 2019) for training.\\n\\nComputing Infrastructure\\n\\nWe continually pretrain MC$_2$XLMR-large on one A100 GPU, and an epoch takes 17 hours. We continually pretrain MC$_2$Llama-13B on eight A100 GPUs, and an epoch takes 20 hours.\\n\\n7https://github.com/tugstugi/mongolian-nlp/tree/master/bichig2cyrillic\"}"}
{"id": "acl-2024-long-479", "page_num": 16, "content": "{\"primary_language\":\"mn\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u0414\u0430\u0432\u0430\u0430 \u0433\u0430\u0440\u0430\u0433\u0442 \u0428\u0432\u0435\u0434\u0438\u0439\u043d \u0430\u043a\u0430\u0434\u0435\u043c\u0438 \u0434\u0430\u0445\u044c \u041d\u043e\u0431\u0435\u043b\u0438\u0439\u043d \u0443\u0440\u0430\u043d \u0437\u043e\u0445\u0438\u043e\u043b\u044b\u043d \u0445\u043e\u0440\u043e\u043e\u043d\u044b \u0431\u0430\u0439\u043d\u0433\u044b\u043d \u043d\u0430\u0440\u0438\u0439\u043d \u0431\u0438\u0447\u0433\u0438\u0439\u043d \u0434\u0430\u0440\u0433\u0430 \u0421\u0430\u0440\u0430 \u0414\u0430\u043d\u0438\u0443\u0441 \u0428\u0432\u0435\u0434 \u0434\u044d\u0445 \u0421\u0432\u0435\u0440\u0438\u0436\u0435\u0441 \u0440\u0430\u0434\u0438\u043e\u0433\u0438\u0439\u043d \u043d\u044d\u0432\u0442\u0440\u04af\u04af\u043b\u0433\u0438\u0439\u043d \u04af\u0435\u044d\u0440 2016 \u043e\u043d\u044b \u0443\u0440\u0430\u043d \u0437\u043e\u0445\u0438\u043e\u043b\u044b\u043d \u041d\u043e\u0431\u0435\u043b\u0438\u0439\u043d \u0448\u0430\u0433\u043d\u0430\u043b\u044b\u0433 \u04af\u0440\u0442\u0441\u044d\u043d \u0411\u043e\u0431 \u0414\u0438\u043b\u0430\u043d\u0442\u0430\u0439 \u0448\u0443\u0443\u0434 \u0445\u043e\u043b\u0431\u043e\u0433\u0434\u043e\u0436 \u0447\u0430\u0434\u0430\u0445\u0433\u04af\u0439 \u0431\u0430\u0439\u0441\u0430\u043d \u0442\u0443\u043b \u0442\u04af\u04af\u043d\u0442\u044d\u0439 \u0445\u043e\u043b\u0431\u043e\u0433\u0434\u043e\u0445\u044b\u0433 \u0445\u0438\u0447\u044d\u044d\u0445\u044d\u044d \u0431\u043e\u043b\u044c\u0441\u043e\u043d \u043d\u044d\u044d\u043b\u0442\u0442\u044d\u0439 \u0437\u0430\u0440\u043b\u0430\u0436\u044d\u044d.\"}"}
