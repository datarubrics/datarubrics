{"id": "acl-2024-long-805", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MultiLegalPile: A 689GB Multilingual Legal Corpus\\nJoel Niklaus1,2,4 Veton Matoshi2\\nMatthias St\u00fcrmer1,2 Ilias Chalkidis3 Daniel E. Ho4\\n\\n1 University of Bern\\n2 Bern University of Applied Sciences\\n3 University of Copenhagen\\n4 Stanford University\\n\\nAbstract\\nLarge, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, few datasets are available for specialized critical domains such as law and the available ones are often small and only in English. To fill this gap, we curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. MultiLegalPile includes diverse legal data sources and allows for pretraining NLP models under fair use, with most of the dataset licensed very permissively. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, trained models, and all code under the most open licenses possible.\\n\\n1 Introduction\\nRecent years have seen LLMs achieving remarkable progress, as demonstrated by their performance on various benchmarks such as SuperGLUE (Wang et al., 2019), MMLU (Hendrycks et al., 2021), and several human Exams (OpenAI, 2023), including U.S. bar exams for admission to practice law (Katz et al., 2023). These models are typically trained on increasingly large corpora, such as the Pile (Gao et al., 2020a), C4 (Raffel et al., 2020), and mC4 (Xue et al., 2021). However, public corpora available for training these models are predominantly in English, and often constitute web text with unclear licensing. This even led to lawsuits against LLM producers1, highlighting this critical issue. Furthermore, there is a scarcity of large-scale, domain-specific pretraining corpora, which constitutes a significant gap in the current body of resources available for the training of LLMs. We find that only one in every thousand document in mC4 contains legal text. Similarly, LLMs are predominantly English, especially considering domain-specific models, e.g., ones specialized in biomedical, legal, or financial texts.\\n\\nLegal texts, often produced by public instruments (e.g., state governments, international organizations), are typically available under public licenses, offering a rich resource for domain-specific pretraining. Given this context, we curate a massive, openly available, corpus of multilingual law text spanning across numerous jurisdictions (legal systems), predominantly under permissive licenses. Further on, we continue pretraining XLM-R models (Conneau and Lample, 2019) on our corpus and evaluated these models on the recently introduced LEXTREME (Niklaus et al., 2023) and LexGLUE (Chalkidis et al., 2022) benchmarks. Given the often extensive nature of legal text, we also pretrained a Longformer model (Beltagy et al., 2020).\"}"}
{"id": "acl-2024-long-805", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2020) for comparison with hierarchical models\\n(Chalkidis et al., 2019; Niklaus et al., 2021, 2022).\\nOur multilingual models set a new state-of-the-art (SotA) on LEXTREME overall. Our legal Longformer outperforms all other models in four LEXTREME datasets and reaches the highest dataset aggregate score. Our monolingual models outperform their base model XLM-R in 21 out of 24 languages, even reaching language specific SotA in five. On LexGLUE our English models reach SotA in five out of seven tasks with the large model achieving the highest aggregate score.\\n\\nIn the spirit of open science, we provide the dataset under a CC BY-NC-SA 4.0 license, with some subsets licensed more permissively. Dataset creation scripts, models, and pretraining code are public under Apache 2.0 licenses. This open-source approach encourages further research and advancements in the field of legal text analysis and understanding using LLMs.\\n\\nContributions\\nThe contributions of this paper are three-fold: First, we curate and release a large multilingual legal text corpus, dubbed MULTE LEGAL FILE, covering 24 languages and 17 legal systems (jurisdictions). Second, we release two multilingual and 24 monolingual legal PLMs, termed LEGAL XLM S, initiated from XLM-R (Conneau and Lample, 2019) and further pretrained on the MULTE LEGAL FILE. We also pretrain a Longformer (Beltagy et al., 2020) based on our multilingual base-size model on context lengths of up to 4096 tokens. Third, we benchmark the newly released models on LEXTREME and LexGLUE, reaching SotA for base- and large-size models and increasing performance drastically in Greek legal code. Our Longformer model achieves SotA in four tasks and the highest dataset aggregate score. Our monolingual models set language-specific SotA in five languages.\\n\\n2 Related Work\\nIn this section, we briefly discuss prior general and domain-specific pretraining corpora. See Appendix B for a more detailed discussion of related works.\\n\\n2.1 General Pretraining Corpora\\nThe One Billion Word Language Model Benchmark (LM1B) (Chelba et al., 2014), Wikipedia, and derived datasets like WikiText (Merity et al., 2016) and BookCorpus (Zhu et al., 2015) have been crucial in developing language models such as GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019). Large-scale datasets like the Colossal Clean Crawled Corpus (C4) (Raffel et al., 2020), OpenWebText (Gokaslan and Cohen, 2019), The Pile (Gao et al., 2020b), and Glot500 (ImaniGooghari et al., 2023) have further advanced the field, contributing to the training of models like T5, MegatronBERT (Shoeybi et al., 2020), GPT-3 (Brown et al., 2020), and Glot500-m. Although general pretraining datasets are large and widely available, we find that mC4 only contains around 0.1% legal text (see Section 3.1), exemplifying the need for datasets specifically tailored to the legal domain.\\n\\n2.2 Domain Specific Corpora\\n\\n| Model | Domain | Languages | Size in # Words |\\n|-------|--------|-----------|-----------------|\\n| SciBERT | scientific English | 2.38B (3.17B tokens) |\\n| Galactica | scientific English | 12B (15GB) |\\n| LexFiles | legal English | 18.8B |\\n| LegalXLMs (ours) | legal | 24 EU langs | 87B (689GB) |\\n\\nTable 1: Previous domain-specific pretraining corpora. For some, only GB or tokens were available. We converted 8 GB into 1B words and 1 token to 0.75 words. Pretraining on domain-specific text like medicine, law, or science can boost Language Model (LM) performance on related tasks (Beltagy et al., 2019; Gu et al., 2021; Chalkidis et al., 2020; Niklaus and Giofr\u00e9, 2022). In the scientific field, SciBERT was pretrained on a mix of computer science and biomedical papers (Beltagy et al., 2019). Similarly, models like PubMedBERT (Gu et al., 2021) and BioBERT (Lee et al., 2020) were pretrained on biomedical datasets. ClinicalBERT utilized the Medical Information Mart for Intensive Care III (MIMIC-III) dataset, encompassing 2 million clinical notes, demonstrating superior performance on medical NLP tasks (Huang et al., 2019). In the legal realm, LegalBERT was pretrained on 12 GB of English legal texts, achieving high performance on domain-specific tasks (Chalkidis et al., 2020). CaseLaw-BERT utilized the English Harvard Law case corpus from 1965 to 2021 (Zheng et al., 2021). Recently, LexFiles was released, with 11 sub-corpora covering six English-speaking legal systems and 19B tokens (Chalkidis* et al., 2023). It was used to train new legal English PLMs, showing enhanced results in legal tasks. Though efforts to\"}"}
{"id": "acl-2024-long-805", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"pretrain legal LMs exist in languages like Italian, Romanian, and Spanish (Licari and Comand\u00e8, 2022; Masala et al., 2021; Guti\u00e9rrez-Fandi\u00f1o et al., 2021), English remains predominant, emphasizing the need for multilingual legal corpora. Table 1 compares previous domain-specific corpora, all in English and all legal corpora less than 1/4 of MULTILEGALFILE\u2019s size.\\n\\n3 MULTILEGALFILE\\n\\n3.1 Construction\\n\\nWe transformed all datasets into xz compressed JSON Lines (JSONL) format. The combination of XZ compression and JSONL is ideal for streaming large datasets due to reduced file size and efficient decompression and reading.\\n\\nFiltering mC4\\n\\nWe used the vast multilingual web crawl corpus, mC4 (Xue et al., 2021), as our base dataset. To effectively isolate legal content, we used regular expressions to identify documents with legal references, such as \u201cArt. 5\u201d or \u201c\u00a7 8\u201d. We found that detecting legal citations, such as references to laws and rulings, served as a reliable indicator of legal-specific documents in the corpus.\\n\\n| Iteration | German | English | Spanish | French | Italian |\\n|-----------|--------|---------|---------|--------|---------|\\n| 1st       | 100%   | 20%     | 100%    | 65%    | 80%     |\\n| 2nd       | 100%   | 85%     | 100%    | 100%   | 95%     |\\n\\nTable 2: Per language precision in legal mC4 (n=20)\\n\\nTo ensure the accuracy of our filtering, we engaged legal experts to aid in identifying citations to laws and rulings across different jurisdictions and languages. We manually reviewed the precision of the retrieved documents for five languages, namely German, English, Spanish, French, and Italian, as shown in Table 2. The proficiency levels of the evaluators included native German, fluent English and Spanish, intermediate French, and basic Italian.\\n\\nSubsequent to the initial review, we performed a second round of precision evaluation, during which we refined our regex expressions based on our findings from the first iteration. This iterative process not only enhanced the precision of the legal content detection, but also resulted in a reduction of the corpus size from 133GB to 106GB. Although the overall volume of data was reduced, this process significantly improved the quality and specificity of the corpus by focusing on legal content with a higher degree of precision. A major reason for using regexes instead of an machine learning based classifier was speed. Already when utilizing regexes, filtering through such a huge corpus like mC4 (27TB in total, of which 10.4TB are in English) took several days on our hardware. An ML model based on Bag-of-Words, Word vectors or even contextualized embeddings would a) need an annotated dataset and b) likely be much slower. We find that on average, only one in every thousand pages in mC4 contains legal content. We show a precise overview of language-specific percentages of legal text in mC4 in Figure 4.\\n\\nFigure 2: MULTILEGALFILE Text Type Distribution\\n\\nCompiling Native MULTILEGALFILE\\n\\nTo compile the corpus, we scraped several sources containing legal language materials. Our search was conducted in a loose manner, meaning that when we found a suitable source with legal text data, we included it in our corpus. It is important to note that we do not claim completeness, as we were unable to perform quality analysis for all available languages. For a detailed overview of sources used for the Native MULTILEGALFILE corpus, please refer to Table 9. Most sources offered direct data download links. For inconsistently formatted data, we converted them to a unified format like jsonl. The post-processing steps involved performing various tasks depending on the initial data format. For example, in the case of CASS, we extracted the textual data from XML tags.\\n\\nCurating Eurlex Resources\\n\\nTo curate the Eurlex resources, we utilized the eurlex R package (Ov\u00e1dek, 2021) to generate SPARQL queries and\"}"}
{"id": "acl-2024-long-805", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"download the data. Subsequently, we converted the data into a format more amenable to handling large datasets using Python.\\n\\nIntegrating Pile of Law\\n\\nHenderson et al. (2022) released a large corpus of diverse legal text in English mainly originating from the US. We integrated the latest version with additional data (from January 8, 2023) into our corpus.\\n\\n3.2 Description\\n\\nMULTILEGAL PILE consists of four subsets: a) Native Multi Legal Pile (112 GB), b) Eurlex Resources (179 GB), c) Legal MC4 (106 GB) and d) Pile of Law (Henderson et al., 2022) (292 GB). Figure 3 details the distribution of languages. Due to Pile of Law integration, English dominates, comprising over half the words. Figure 2 shows text type distribution. Caselaw comprises over half the corpus, due to the good public access to court rulings especially in common law countries. Even in civil law countries, where legislation is crucial, caselaw often outnumbers legislation, as seen in the Swiss case in Table 9. Publicly available contracts are scarce, contributing less than 10% to the corpus, despite potentially making up most existing legal texts (from the private sector). Note that most contracts in our corpus originate from the US or EU international treaties. Table 9 in Appendix E provides additional information on M ULTILEGAL PILE, including sources and licenses.\\n\\n3.3 Licenses and Usage of M ULTILEGAL PILE\\n\\nThe Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license applied to the M ULTILEGAL PILE corpus depends on the upstream licenses of the data subsets described above. First, our Native Multi Legal Pile consists of data sources with different licenses. They range from restrictive licenses such as CC BY-NC-SA 4.0 up to the most liberal Creative Commons Zero (CC0) license, which, in essence, releases the data into the public domain. Many sources, however, do not explicitly state the license used for the available data. We assume that such data sources allow pretraining usage, since the creators are usually public agencies such as courts and administrations. Such legislation and caselaw is usually not protected by copyright law. Table 9 provides an overview of the license or copyright situation for each of the 29 sources in the Native Multi Legal Pile. Second, the Eurlex Resources is CC BY 4.0 licensed by the European Union, thus posing no legal issues for pretraining. Third, the Legal mC4 corpus was created by filtering multilingual C4 (Xue et al., 2021) for legal content as described above. As mC4 is licensed under ODC-BY, we also release the filtered Legal mC4 corpus under the same license. Finally, the Pile of Law (Henderson et al., 2022) is published under CC BY-NC-SA 4.0 and the dataset is unaltered, thus preserving the license. Usage of the M ULTILEGAL PILE corpus is presumably possible for pretraining of NLP models. In general, we assume that the fair use doctrine allows employing the data for legal NLP models because the results are rather transformative (Henderson et al., 2023). Nevertheless, copyright issues in generative AI remain an unresolved problem for the moment. Several court cases are currently pending, such as Getty Images suing Stability AI for intellectual property infringement (Sag, 2023).\\n\\n4 Pretraining Legal Models\\n\\nAs part of this study, we release 2 new multilingual legal-oriented PLMs, dubbed Legal-XLM-Rs, trained on the newly introduced M ULTILEGAL PILE corpus (Section 3). For the newly released Legal-XLM-Rs we followed a series of best-practices in the LM development literature: (a) We warm-start (initialize) our models from the original XLM-R checkpoints (base or large) of Conneau and Lample (2019). Model recycling is a standard process followed by many (Wei et al., 2021; Ouyang et al., 2022) to benefit from starting from an available \u201cwell-trained\u201d PLM, rather from scratch (random). XLM-R was trained on 2.5TB of cleaned CommonCrawl data in 100 languages. (b) We train a new tokenizer of 128K BPEs on the training subsets of M ULTILEGAL PILE to better cover legal language across all available legal systems and languages. However, we reuse the original XLM-R embeddings for all lexically overlapping tokens (Pfeiffer et al., 2021), i.e., we warm-start word embeddings for tokens that already exist in the original XLM-R vocabulary, and use random ones for the rest. Similarly to Liang et al. (2023) who increased the vocabulary size from around 2.5K tokens per language (250K for 100 languages) to around 10K (1M for 100 languages),\"}"}
{"id": "acl-2024-long-805", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Models: All models process up to 512 tokens, except Legal-XLM-LF-base (4096 tokens). BS is short for batch size. MLP is short for MUltiLevel.\\n\\n- **MiniLM** Wang et al. (2020) 118M 250K 1M steps / BS 256 2.5TB CC100 100\\n- **DistilBERT** Sanh et al. (2020) 135M 120K BS up to 4000 Wikipedia 104\\n- **mDeBERTa-v3** He et al. (2021b,a) 278M 128K 500K steps / BS 8192 2.5TB CC100 100\\n- **XLM-R base** Conneau et al. (2020) 278M 250K 1.5M steps / BS 8192 2.5TB CC100 100\\n- **XLM-R large** Conneau et al. (2020) 560M 250K 1.5M steps / BS 8192 2.5TB CC100 100\\n- **Legal-XLM-R-base** ours 184M 128K 1M steps / BS 512 689GB MLP 24\\n- **Legal-XLM-R-large** ours 435M 128K 500K steps / BS 512 689GB MLP 24\\n- **Legal-XLM-LF-base** ours 208M 128K 50K steps / BS 512 689GB MLP 24\\n- **Legal-mono-R-base** ours 111M 32K 200K steps / BS 512 689GB MLP 1\\n- **Legal-mono-R-large** ours 337M 32K 500K steps / BS 512 689GB MLP 1\\n\\nWe increased to around 5K (128K for 24 languages), thus roughly doubling compared to XLM-R.\\n\\n(c) We continue pretraining our models on the diverse MUltiLevel corpus with batches of 512 samples for an additional 1M/500K steps for the base/large model. We do initial warm-up steps for the first 5% of the total training steps with a linearly increasing learning rate up to $10^{-4}$, and then follow a cosine decay scheduling, following recent trends. For half of the warm-up phase (2.5%), the Transformer encoder is frozen, and only the embeddings, shared between input and output (MLM), are updated. We also use an increased 20/30% masking rate for base/large models respectively, where 100% of token predictions are based on masked tokens, compared to Devlin et al. (2019). Based on the findings of Wettig et al. (2023).\\n\\n(d) For both training the tokenizer and our legal models, we use a sentence sampler with exponential smoothing of the sub-corpora sampling rate following Conneau and Lample (2019) and Raffel et al. (2020), since there is a disparate proportion of tokens across sub-corpora and languages (Figures 1 and 3) and we aim to preserve per-corpus and language capacity, i.e., avoid overfitting to the majority (approx. 50% of the total number of tokens) US-origin English texts.\\n\\n(e) We consider mixed cased models, i.e., both upper- and lowercase letters covered, similar to recently developed large PLMs (Conneau and Lample, 2019; Raffel et al., 2020; Brown et al., 2020). To better account for long contexts often found in legal documents, we continue training the base-size multilingual model on long contexts (4096 tokens) with windowed attention (128 tokens window size) (Beltagy et al., 2020) for 50K steps, dubbing it Legal-XLM-LF-base. We use the standard 15% masking probability and increase the learning rate to $3 \\times 10^{-5}$ before decaying but otherwise use the same settings as for training the small-context models.\\n\\nIn addition to the multilingual models, we also train 24 monolingual models on each of the language-specific subsets of the corpus. Except for choosing a smaller vocab size of 32K tokens, we use the same settings as for the multilingual models. Due to resource constraints, we only train base-size models and stop training at 200K steps. Due to limited data available in some low-resource languages, these models sometimes do multiple passes over the data. Because of plenty of data and to achieve a better comparison on LexGLUE, we continued training the English model for 1M steps and also trained a large-size model for 500K steps. See Table 7 in appendix C for an overview.\\n\\nWe make all our models publicly available alongside all intermediate checkpoints (every 50K/10K training steps for RoBERTa/Longformer models) on the Hugging Face Hub. Link will be released upon acceptance.\"}"}
{"id": "acl-2024-long-805", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the absence of established legal benchmarks for generative tasks and our focus on pretraining encoder-only models, we select two established legal benchmarks involving challenging text classification and named entity recognition tasks involving long documents: LEXTREME and LexGLUE.\\n\\n5.1 Benchmark Description\\n\\nBelow, we briefly describe each dataset and refer the reader to the original works for more details.\\n\\n**LEXTREME** (Niklaus et al., 2023) is a multilingual legal benchmark. It includes five single-label text classification datasets, three multi-label text classification datasets and four Named Entity Recognition (NER) datasets.\\n\\n**Brazilian Court Decisions (BCD)** (Lage-Freitas et al., 2022) is from the State Supreme Court of Alagoas (Brazil) and involves predicting case outcomes and judges' unanimity on decisions.\\n\\n**German Argument Mining (GAM)** (Urchs et al., 2021) contains 200 German court decisions for classifying sentences according to their argumentative function.\\n\\n**Greek Legal Code (GLC)** (Papaloukas et al., 2021) tackles topic classification of Greek legislation documents. Tasks involve predicting topic categories at volume, chapter, and subject levels.\\n\\n**Swiss Judgment Prediction (SJP)** (Niklaus et al., 2021) focuses on predicting the judgment outcome from 85K cases from the Swiss Federal Supreme Court.\\n\\n**Online Terms of Service (OTS)** (Drawezski et al., 2021) contains 100 contracts for detecting unfair clauses with the tasks of classifying sentence unfairness levels and identifying clause topics.\\n\\n**COVID19 Emergency Event (C19)** (Tziafas et al., 2021): consists of legal documents from several European countries related to COVID-19 measures where models identify the type of measure described in a sentence.\\n\\n**MultiEURLEX (MEU)** (Chalkidis et al., 2021a) is a corpus of 65K EU laws annotated with EUROVOC taxonomy labels. Task involves identifying labels for each document.\\n\\n**Greek Legal NER (GLN)** (Angelidis et al., 2018) is a dataset for NER in Greek legal documents.\\n\\n**LegalNERo (LNR)** (Pais et al., 2021) tackles NER in Romanian legal documents.\\n\\n**LeNER BR (LNB)** (Luz de Araujo et al., 2018) addresses NER in Brazilian legal documents.\\n\\n**MAPA (MAP)** (Baisa et al., 2016) is a multilingual corpus based on EUR-Lex for NER annotated at a coarse-grained and fine-grained level.\\n\\n**LexGLUE** (Chalkidis et al., 2022) is a legal benchmark covering two single-label and four multi-label text classification datasets, and a multiple choice question answering dataset.\\n\\n**ECtHR Tasks A & B** (Chalkidis et al., 2019, 2021b) contain approx. 11K cases from the European Court of Human Rights (ECtHR) public database. Based on case facts, Task A predicts violated articles and Task B allegedly violated articles of the European Convention of Human Rights (ECHR).\\n\\n**SCOTUS** (Spaeth et al., 2020) combines information from US Supreme Court (SCOTUS) opinions with the Supreme Court DataBase (SCDB). The task is to classify court opinions into 14 issue areas.\\n\\n**EUR-LEX** (Chalkidis et al., 2021a) contains 65K EU laws from the EUR-Lex portal, annotated with EuroVOC concepts. The task is to predict EuroVOC labels for a given document.\\n\\n**LEDGAR** (Tuggener et al., 2020) contains approx. 850K contract provisions from the US Securities and Exchange Commission (SEC) filings. The task is to classify contract provisions into categories.\\n\\n**UNFAIR-ToS** (Lippi et al., 2019) contains 50 Terms of Service (ToS) from online platforms, annotated with types of unfair contractual terms. The task is to predict unfair types for a given sentence.\\n\\n**CaseHOLD** (Zheng et al., 2021) contains approx. 53K multiple choice questions about holdings of US court cases. The task is to identify the correct holding statement out of five choices.\\n\\n5.2 Experimental Setup\\n\\nTo ensure comparability, we followed the experimental setups described in the original papers (Niklaus et al., 2023; Chalkidis et al., 2022) using hierarchical transformers for datasets where the sequence length of most documents exceeds the maximum sequence length of the model (Aletras et al., 2016; Niklaus et al., 2022). The hyperparameters used for running experiments on each dataset are provided in Table 8 in the appendix. We follow previous work (Niklaus et al., 2023; Chalkidis et al., 2022) and do not tune hyperparameters. All scores are macro-F1 scores, equally weighing each class for fairness in unbalanced datasets.\\n\\nTo obtain Table 6, we follow Chalkidis et al. (2022), running five repetitions with different random seeds (1-5) and report test scores from the best-performing seed on the development data. For values in Tables 4 and 5, we follow the procedure in Niklaus et al. (2023), taking the harmonic mean of the results of 3 random seeds (1-3). We calculate...\"}"}
{"id": "acl-2024-long-805", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 4: Dataset aggregate scores (macro-F1) for multilingual models on LEXTREME with the best scores in bold.\\n\\n| Model           | BCD   | GAM   | GLC   | SJP   | OTS   | C19   | MEU   | GLN   | LNR   | LNB   | MAP   | Agg.  |\\n|-----------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\\n| MiniLM          | 53.0  | 73.3  | 42.1  | 67.7  | 44.1  | 5.0   | 29.7  | 74.0  | 84.5  | 93.6  | 57.8  | 56.8  |\\n| DistilBERT      | 54.5  | 69.5  | 62.8  | 66.8  | 56.1  | 25.9  | 36.4  | 71.0  | 85.3  | 89.6  | 60.8  | 61.7  |\\n| mDeBERTa-v3     | 60.2  | 71.3  | 52.2  | 69.1  | 66.5  | 29.7  | 37.4  | 73.3  | 85.1  | 94.8  | 67.2  | 64.3  |\\n| XLM-R-base      | 63.5  | 72.0  | 57.4  | 69.3  | 67.8  | 26.4  | 33.3  | 74.6  | 85.8  | 94.1  | 62.0  | 64.2  |\\n| XLM-R-large     | 58.7  | 73.1  | 57.4  | 69.0  | 75.0  | 29.0  | 42.2  | 74.1  | 85.0  | 95.3  | 68.0  | 66.1  |\\n| Legal-XLM-R-base| 62.5  | 72.4  | 68.9  | 70.2  | 70.8  | 30.7  | 38.6  | 73.6  | 84.1  | 94.1  | 69.2  | 66.8  |\\n| Legal-XLM-R-large| 63.3 | 73.9  | 59.3  | 70.1  | 74.9  | 34.6  | 39.7  | 73.1  | 83.9  | 94.6  | 67.3  | 66.8  |\\n\\n## Table 5: Language aggregate scores (macro-F1) for multilingual models on LEXTREME with the best scores in bold.\\n\\nFor each language, we list the top-performing monolingual legal and non-legal models under NativeLegalBERT and NativeBERT, and our legal models under Legal-mono-R-base. Missing values signify no suitable models found.\\n\\n| Model              | ECtHR-A | ECtHR-B | SCOTUS | EUR-LEX | LEDGAR | UNFAIR-ToS | CaseHOLD | Agg.  |\\n|--------------------|---------|---------|--------|---------|--------|------------|----------|-------|\\n| TFIDF+SVM          | 48.9    | 63.8    | 64.4   | 47.9    | 81.4   | 75.0       | 22.4     | 49.0  |\\n| BERT               | 63.6    | 73.4    | 58.3   | 57.2    | 81.8   | 81.3       | 70.8     | 68.2  |\\n| DeBERTa            | 60.8    | 71.0    | 62.7   | 57.4    | 83.1   | 80.3       | 72.6     | 68.5  |\\n| RoBERTa-base       | 59.0    | 68.9    | 62.0   | 57.9    | 82.3   | 79.2       | 71.4     | 67.5  |\\n| RoBERTa-large      | 67.6    | 71.6    | 66.3   | 58.1    | 83.6   | 81.6       | 74.4     | 70.9  |\\n| Longformer         | 64.7    | 71.7    | 64.0   | 57.7    | 83.0   | 80.9       | 71.9     | 69.5  |\\n| BigBird            | 62.9    | 70.9    | 62.0   | 56.8    | 82.6   | 81.3       | 70.8     | 68.4  |\\n| Legal-BERT         | 64.0    | 74.7    | 66.5   | 57.4    | 83.0   | 83.0       | 75.3     | 70.8  |\\n| CaseLaw-BERT       | 62.9    | 70.3    | 65.9   | 56.6    | 83.0   | 82.3       | 75.4     | 69.7  |\\n| Legal-en-R-base (ours) | 65.2 | 73.7    | 66.4   | 59.2    | 82.7   | 78.7       | 73.3     | 70.5  |\\n| Legal-en-R-large (ours) | 70.3 | 77.0    | 67.7   | 58.4    | 82.5   | 82.4       | 77.0     | 72.7  |\\n| Legal-XLM-R-base (ours) | 64.8 | 73.9    | 63.9   | 58.2    | 82.8   | 79.6       | 71.7     | 69.7  |\\n| Legal-XLM-R-large (ours) | 68.2 | 74.2    | 67.5   | 58.4    | 82.7   | 79.9       | 75.1     | 71.4  |\\n| Legal-XLM-LF-base (ours) | 67.9 | 76.2    | 61.6   | 59.1    | 82.1   | 78.9       | 72.0     | 70.2  |\\n\\n## Table 6: Results on LexGLUE (macro-F1) with the best scores in bold. Results marked with * are from Chalkidis et al. (2022). Similar to LEXTREME, we calculate the aggregate score as the harmonic mean of dataset results.\\n\\nWe notice that our Legal-XLM-R-base model is on par with XLM-R large even though it only contains 33% of the parameters (184M vs 560M).\"}"}
{"id": "acl-2024-long-805", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All our models outperform XLM-R large on the dataset aggregate score. Our base model sets a new SotA on MAPA (MAP), the large model on CoViD 19 emergency event (C19) and the Longformer on Brazilian court decisions (BCD), German argument mining (GAM), Greek legal code (GLC) and Swiss judgment prediction (SJP). Surprisingly, the legal models slightly underperform in three NER tasks (GLN, LNR, and LNB). Sensitivity to hyperparameter choice could be a reason for this underperformance (we used the same hyperparameters for all models without tuning due to limited compute resources). We see the largest improvements over prior art in BCD (72.4 vs. 63.5) and in GLC (70.2 vs 62.8). Maybe these tasks are particularly hard, and therefore legal in-domain pretraining helps more. For BCD especially, the large amount of Brazilian caselaw in the pretraining corpus may offer an additional explanation.\\n\\nThe monolingual models underperform their base model XLM-R base only in Italian, Polish, and Romanian. In some languages the monolingual model even outperforms XLM-R base clearly (Estonian, Croatian, Hungarian, Latvian, Maltese, Dutch, Slovenian, and Swedish), and in five of them even set the new SotA for the language, sometimes clearly outperforming all other models (the Dutch model even outperforms its closest competitor mDeBERTa-v2 by 11.2 macro F1 and its base model XLM-R by almost 20 macro F1). These languages are all in the lower end of the data availability in the MULTI LEGAL PILE with the richest language (Dutch) containing only 810M words (see Figure 3). Pretraining a monolingual model on in-domain data may therefore be worth it, especially in low-resource languages.\\n\\nEven though our legal Longformer model performs best on the dataset level, it performs much worse on the language level, possibly due to its lower scores in the most multilingual tasks MEU, MAP and C19 (24, 24 and 6 languages, respectively). Our legal base and large models achieve SotA in some languages, and are in aggregate almost as robust across languages as XLM-R.\\n\\nComputing the final LEXTREME scores (harmonic mean of dataset aggregate and language aggregate scores), we find that the Legal-XLM-R-large is the new SotA on LEXTREME with a score of 59.5 vs 59.4 for Legal-XLM-R-base and 59.3 for XLM-R large. The legal Longformer's LEXTREME score (56.5) is not competitive due to its low language aggregate score.\\n\\n5.4 Evaluation on LexGLUE\\n\\nWe evaluate our English and multilingual models on LexGLUE (Chalkidis et al., 2022) and compare with baselines (see Table 6). Our models excel on the ECtHR, SCOTUS, EUR-LEX, and CaseHOLD tasks, setting new SotA. In the other two tasks, our models match general-purpose models such as RoBERTa. A reason for slight underperformance of the legal models in the LEDGAR and especially the Unfair ToS tasks may be the relatively low availability of contracts in the MULTI LEGAL PILE.\\n\\n6 Conclusions and Future Work\\n\\nConclusions\\n\\nDue to a general lack of multilingual pretraining data especially in specialized domains such as law, we curate a large-scale high-quality corpus in 24 languages from 17 jurisdictions. We continue pretraining XLM-R checkpoints on our data, achieving a new SotA for base and large models on the LEXTREME benchmark and vastly outperforming previous methods in Greek legal code. We turn our XLM-R base model into a Longformer and continue pretraining on long documents. It reaches a new SotA in four LEXTREME datasets and reaches the overall highest dataset aggregate score. Monolingual models achieve huge gains over their base model XLM-R in some languages and even set language specific SotA in five languages outperforming other models by as much as 11 macro F1. On LexGLUE our English models reach SotA in five out of seven tasks with the large model achieving the highest aggregate score. To conclude, following best practices in continued pretraining on our comprehensive multilingual legal corpus establishes new state-of-the-art across multiple datasets and languages, significantly enhancing performance in legal text analysis.\\n\\nFuture Work\\n\\nWe focused on the 24 EU languages, but in the future, we would like to expand the corpus in terms of languages and jurisdictions covered. Especially in China there exist many accessible sources suitable to extend the corpus. Additionally, we would like to find out whether our findings on in-domain pretraining hold for multibillion generative models. Finally, a detailed examination of the contents of the MULTI LEGAL PILE could provide valuable insights into its structure and efficacy in enhancing legal language models.\"}"}
{"id": "acl-2024-long-805", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This study focuses on evaluating legal-specific LMs from multiple aspects, expanding the dialogue to aid in creating support technologies for both legal professionals and the general public. This area represents a vital field for research, as emphasized by Tsarapatsanis and Aletras (2021), aiming to enhance legal services and make legal knowledge more accessible. The study also aims to shed light on the multifaceted limitations that need addressing to ensure the responsible and ethical application of legal-oriented technologies.\\n\\nIn pursuit of these goals, we introduce novel resources encompassing a range of legal systems. These resources are designed to construct new models that more accurately reflect legal nuances and evaluate their effectiveness more precisely. All resources created and shared in this work are derived from data that is publicly accessible, often distributed across various online platforms.\\n\\nLimitations\\nWe did not perform deduplication, thus data from legal mC4 might be present in other parts. However, Muennighoff et al. (2023) suggest that data duplication does not degrade performance during pretraining for up to four epochs. Overlap between other subsets is highly unlikely, since they originate from completely different jurisdictions.\\n\\nDue to limited compute, we were not able to pretrain a large generative model and leave this to future work.\\n\\nReferences\\nNikolaos Aletras, Dimitrios Tsarapatsanis, Daniel Preo\u0163iuc-Pietro, and Vasileios Lampos. 2016. Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspective. PeerJ Computer Science, 2:e93. Publisher: PeerJ Inc.\\n\\nI. Angelidis, Ilias Chalkidis, and M. Koubarakis. 2018. Named Entity Recognition, Linking and Generation for Greek Legislation. In JURIX.\\n\\nV\u00edt Baisa, Jan Michelfeit, Marek Medved', and Milo\u0161 Jakub\u00edcek. 2016. European Union language resources in Sketch Engine. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 2799\u20132803, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\\n\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: A pretrained language model for scientific text. In Conference on Empirical Methods in Natural Language Processing.\\n\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. arXiv:2004.05150 [cs]. ArXiv: 2004.05150.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nIlias Chalkidis, Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019. Neural legal judgment prediction in English. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4317\u20134323, Florence, Italy. Association for Computational Linguistics.\\n\\nIlias Chalkidis, Manos Fergadiotis, and Ion Androutsopoulos. 2021a. MultiEURLEX - a multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6974\u20136996, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. LEGAL-BERT: The muppets straight out of law school. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2898\u20132904, Online. Association for Computational Linguistics.\\n\\nIlias Chalkidis, Manos Fergadiotis, Dimitrios Tsarapatsanis, Nikolaos Aletras, Ion Androutsopoulos, and Prodromos Malakasiotis. 2021b. Paragraph-level rationale extraction through regularization: A case study on European court of human rights cases. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 226\u2013241, Online. Association for Computational Linguistics.\\n\\nIlias Chalkidis*, Nicolas Garneau*, Catalina Goanta, Daniel Martin Katz, and Anders S\u00f8gaard. 2023. Lexfiles and legallama: Facilitating english multinational legal language model development.\\n\\nIlias Chalkidis, Abhik Jana, Dirk Hartung, Michael Bommarito, Ion Androutsopoulos, Daniel Katz, and Nikolaos Aletras. 2022. LexGLUE: A Benchmark\"}"}
{"id": "acl-2024-long-805", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-805", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-805", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21(140):1\u201367.\\n\\nMatthew Sag. 2023. Copyright safety for generative AI. Forthcoming in the Houston Law Review.\\n\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv:1910.01108 [cs]. ArXiv: 1910.01108.\"}"}
{"id": "acl-2024-long-805", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Takeuchi, Marc P\u00e0mies, Maria A. Castillo, Marianna Nezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S. Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Sriskhti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. ArXiv:2211.05100 [cs].\\n\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2020. Megatron-lm: Training multi-billion parameter language models using model parallelism.\\n\\nHarold J. Spaeth, Lee Epstein, Andrew D. Martin, Jeffrey A. Segal, Theodore J. Ruger, and Sara C. Benesh. 2020. Supreme Court Database, Version 2020 Release 01.\\n\\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A Large Language Model for Science. ArXiv:2211.09085 [cs, stat].\\n\\nJ\u00f6rg Tiedemann. 2016. Finding alternative translations in a large corpus of movie subtitle. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 3518\u20133522, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. ArXiv:2302.13971 [cs].\\n\\nDimitrios Tsarapatsanis and Nikolaos Aletras. 2021. On the Ethical Limits of Natural Language Processing on Legal Text. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3590\u20133599, Online. Association for Computational Linguistics.\\n\\nDon Tuggener, Pius von D\u00e4niken, Thomas Peetz, and Mark Cieliebak. 2020. LEDGAR: A large-scale multi-label corpus for text classification of legal provisions in contracts. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1235\u20131241, Marseille, France. European Language Resources Association.\\n\\nGeorgios Tziafas, Eugenie de Saint-Phalle, Wietse de Vries, Clara Egger, and Tommaso Caselli. 2021. A multilingual approach to identify and classify exceptional measures against covid-19. In Proceedings of the Natural Legal Language Processing Workshop 2021, pages 46\u201362. Dataset URL: https://tinyurl.com/ycysvtbm.\\n\\nStefanie Urchs, Jelena Mitrovi\u0107, and Michael Granitzer. 2021. Design and Implementation of German Legal Decision Corpora:. In Proceedings of the 13th International Conference on Agents and Artificial Intelligence, pages 515\u2013521, Online Streaming, \u2014 Select a Country \u2014. SCITEPRESS - Science and Technology Publications.\\n\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. page 30.\\n\\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. In Advances in Neural Information Processing Systems, volume 33, pages 5776\u20135788. Curran Associates, Inc.\\n\\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. CoRR, abs/2109.01652.\\n\\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. 2023. Should you mask 15% in masked language modeling? In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2985\u20133000, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. arXiv:2010.11934 [cs]. ArXiv: 2010.11934.\\n\\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against Neural Fake News. Curran Associates Inc., Red Hook, NY, USA.\\n\\nLucia Zheng, Neel Guha, Brandon R Anderson, Peter Henderson, and Daniel E Ho. 2021. When Does Pretraining Help? Assessing Self-Supervised Learning for Law and the CaseHOLD Dataset of 53,000+ Legal Holdings. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law, ICAIL '21, pages 159\u2013168, New York, NY, USA. Association for Computing Machinery.\\n\\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: 15089\"}"}
{"id": "acl-2024-long-805", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards story-like visual explanations by watching movies and reading books.\\n\\nA Use of AI assistants\\nWe used ChatGPT and Grammarly for improving the grammar and style of our writing.\\n\\nB Additional Related Work\\nB.1 General Pretraining Corpora\\nThe use of pretrained Language Models (PLMs) has become increasingly popular in NLP tasks, particularly with the advent of models such as BERT (Devlin et al., 2019) that can be finetuned for specific applications. One key factor in the success of pretraining is the availability of large and diverse text corpora, which can help the model learn the nuances of natural language. In the following, we discuss large-scale general-purpose text corpora used for pretraining.\\n\\nOne of the earliest widely-used datasets is the One Billion Word Language Model Benchmark (LM1B) (Chelba et al., 2014). It was created by extracting one billion words from web pages to evaluate novel language modeling techniques. It has been used, among others, to evaluate GPT-2 (Radford et al., 2019).\\n\\nWikipedia is a commonly used multilingual dataset for pretraining language models, and has been used to pretrain BERT (Devlin et al., 2019), MegatronBERT (Shoeybi et al., 2020), T5 (Raffel et al., 2020), and GPT-3 (Brown et al., 2020), among others.\\n\\nBased on Wikipedia, Merity et al. (2016) created WikiText by selecting articles fitting the Good or Featured article criteria. The dataset contains 103M words and has two versions: WikiText2 and the larger WikiText103. It has been used to pretrain models like MegatronBERT (Shoeybi et al., 2020) and GPT-2 (Radford et al., 2019).\\n\\nThe BookCorpus (Zhu et al., 2015), also known as the Toronto Books Corpus, is an English dataset used for pretraining BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel et al., 2020). It consists of almost 1B words from over 11K books collected from the web.\\n\\nThe Common Crawl corpus is a publicly available multilingual dataset of scraped web pages, regularly updated with new \u201csnapshots\u201d. It has been used to pretrain GPT-3 (Brown et al., 2020) as well as XLM-R (Conneau et al., 2020). One significant drawback of Common Crawl is the presence of uncleaned data, which includes a considerable amount of \u201cgibberish or boiler-plate text like menus, error messages, or duplicate text\u201d (Raffel et al., 2020). As a result, utilizing the Common Crawl dataset necessitates additional post-filtering and cleaning procedures. To address this issue, Raffel et al. (Raffel et al., 2020) performed several cleaning steps on the April 2019 snapshot of Common Crawl, resulting in the creation of the Colossal Clean Crawled Corpus (C4), comprising 750 GB of English-language text. It was used for pretraining models such as T5 (Raffel et al., 2020) and Switch Transformer (Fedus et al., 2022).\\n\\nOpenWebText (Gokaslan and Cohen, 2019) openly replicates OpenAI\u2019s closed English WebText dataset (Radford et al., 2019), used to pretrain GPT-2 (Radford et al., 2019). WebText comprises over 8M documents with a combined text size of 40 GB. To ensure data uniqueness, any documents sourced from Wikipedia were excluded from WebText, as they are commonly utilized in other datasets. OpenWebText, on the other hand, consists of 38 GB of text data from 8M documents and was used for pretraining RoBERTa (Liu et al., 2019) and MegatronBERT (Shoeybi et al., 2020).\\n\\nNews articles are also a common source for pretraining corpora. The RealNews dataset (Zellers et al., 2019) is a large corpus extracted from Common Crawl, containing news articles from December 2016 to March 2019 (training) and April 2019 (evaluation), totaling 120 GB. It was used for pretraining MegatronBERT (Shoeybi et al., 2020). For pretraining RoBERTa, Liu et al. (2019) used an English subset of RealNews, comprising 63M English news articles crawled from September 2016 to February 2019.\\n\\nThe rise of LLMs brought about the creation of ever larger training datasets. The Pile (Gao et al., 2020b) combines 22 distinct, well-curated datasets, such as Wikipedia (English), OpenWebText2 (Gokaslan and Cohen, 2019), OpenSubtitles (Tiedemann, 2016) etc., encompassing 825 GB of data. Besides general-purpose textual datasets, it also contains domain-specific datasets, such as ArXiv (Science), FreeLaw (Legal), PubMed Abstracts (Biomedicine), and GitHub data (to improve code-related task performance (Gao et al., 2020b)).\\n\\nGPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020)\"}"}
{"id": "acl-2024-long-805", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al., 2020) were evaluated on this dataset. Touvron et al. (2023) compiled a substantial dataset from various publicly available sources, including CommonCrawl, C4, Github, Wikipedia, etc., totaling 1.4T tokens. They trained the 13B-parameter LLaMA model using this dataset, surpassing the performance of the 175B-parameter GPT-3 on most benchmark tasks. However, the dataset itself is not publicly available. To address this, a collaborative effort resulted in the creation of the RedPajama-Data-1T dataset, replicating LLaMA's dataset with a similar size of 1.2T tokens.\\n\\nSome of the afore-mentioned datasets, such as Common Crawl, are used to pretrain multilingual versions of BERT, DistilBERT, RoBERTa etc. These models were pretrained on datasets that cover approximately 100 languages, thereby neglecting low-resource languages. ImaniGooghari et al. (2023) addressed this by compiling Glot500, a 700 GB dataset covering 500 diverse languages, with a focus on low-resource ones. The Glot500-m model, pretrained on this dataset, outperformed the XLM-RoBERTa base model on six out of seven tasks.\\n\\nB.2 Domain Specific Corpora\\n\\nWhile pretraining on general-purpose text like Wikipedia and news articles shows promise, evidence suggests that pretraining on domain-specific text can enhance language model performance on related tasks (Beltagy et al., 2019; Gu et al., 2021; Chalkidis et al., 2020; Niklaus and Giofr\u00e9, 2022). Domain-specific text corpora include texts specific to fields like medicine, law, or science. Several studies have examined pretraining on scientific text corpora. Beltagy et al. (2019) pretrained SciBERT, a BERT-based model, on a random subset of 1.14M papers sourced from Semantic Scholar. This collection comprises 18% of computer science papers and 82% of papers from the broader biomedical field. Similarly, PubMed and PubMedCentral are common sources for biomedical datasets. Gu et al. (2021) trained PubMedBERT using PubMed abstracts and PubMedCentral articles; BioBERT (Lee et al., 2020) was pretrained similarly. Johnsson et al. (2016) compiled the Medical Information Mart for Intensive Care III (MIMIC-III) dataset, a large single-center database of critical care patients. \\\"a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital\\\". Huang et al. (2019) used over 2 million de-identified clinical notes from this dataset to pretrain ClinicalBERT. These models outperformed general-purpose models on biomedical NLP tasks.\\n\\nIn the legal domain, similar strategies are observed. Chalkidis et al. (2020) collected 12 GB of diverse English legal texts, including legislation, court cases, and contracts. They pretrained Legal-BERT on this dataset, showing SotA performance, especially in tasks requiring domain knowledge. Another study by Zheng et al. (2021) used the entire English Harvard Law case corpus (1965-2021) comprising 37 GB of text to pretrain CaseLaw-BERT. Recently, Chalkidis* et al. (2023) released LexFiles, an English legal corpus with 11 sub-corpora covering legislation and case law from six English-speaking legal systems (EU, Council of Europe, Canada, US, UK, India). The corpus contains approx. 6M documents or approx. 19B tokens. They trained two new legal English PLMs, showing improved performance in legal probing and classification tasks. Efforts to pretrain legal language models also exist for Italian (Licari and Comand\u00e8, 2022), Romanian (Masala et al., 2021), and Spanish (Guti\u00e9rrez-Fandi\u00f1o et al., 2021). However, English dominates, underscoring the importance of compiling multilingual legal corpora.\\n\\nC Training Details\\n\\nFor finetuning the pretrained models on the evaluation benchmarks we used the following NVIDIA GPUs: 24GB RTX3090, 32GB V100 and 80GB A100. We used v3-8 TPUs for pretraining. All our experiments were run on Linux machines (Debian).\"}"}
{"id": "acl-2024-long-805", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Name      | # Steps | Vocabulary Size |\\n|-----------------|---------|----------------|\\n| Legal-bg-R-base | 200K    | 32K            |\\n| Legal-hr-R-base | 200K    | 32K            |\\n| Legal-cs-R-base | 200K    | 32K            |\\n| Legal-da-R-base | 200K    | 32K            |\\n| Legal-nl-R-base | 200K    | 32K            |\\n| Legal-en-R-base | 200K    | 32K            |\\n| Legal-en-R-large| 500K    | 32K            |\\n| Legal-et-R-base | 200K    | 32K            |\\n| Legal-fi-R-base | 200K    | 32K            |\\n| Legal-fr-R-base | 200K    | 32K            |\\n| Legal-de-R-base | 200K    | 32K            |\\n| Legal-el-R-base | 200K    | 32K            |\\n| Legal-hu-R-base | 200K    | 32K            |\\n| Legal-ga-R-base | 200K    | 32K            |\\n| Legal-it-R-base | 200K    | 32K            |\\n| Legal-lv-R-base | 200K    | 32K            |\\n| Legal-lt-R-base | 200K    | 32K            |\\n| Legal-mt-R-base | 200K    | 32K            |\\n| Legal-pl-R-base | 200K    | 32K            |\\n| Legal-pt-R-base | 200K    | 32K            |\\n| Legal-ro-R-base | 200K    | 32K            |\\n| Legal-sk-R-base | 200K    | 32K            |\\n| Legal-sl-R-base | 200K    | 32K            |\\n| Legal-es-R-base | 200K    | 32K            |\\n| Legal-sv-R-base | 200K    | 32K            |\\n| Legal-XLM-R-base| 1M      | 128K           |\\n| Legal-XLM-R-large| 500K    | 128K           |\\n| Legal-XLM-LF-base| 50K     | 128K           |\\n\\nTable 7: Model Details\"}"}
{"id": "acl-2024-long-805", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Hyperparameters for each dataset and task. However, there were a few exceptions. For the multilingual MEU tasks, given the dataset's size, we trained them for only 1 epoch with 1000 steps as the evaluation strategy when using multilingual models. When using monolingual models, we trained for 50 epochs with epoch-based evaluation strategy, as we utilized only the language-specific subset of the dataset. Regarding LexGlue, we followed the guidelines of Chalkidis et al. (2022) for RoBERTa-based large language models, which required a maximum learning rate of 1e-5, a warm-up ratio of 0.1, and a weight decay rate of 0.06.\\n\\nTable 9: Information about size and number of words and documents for Native Multi Legal Pile are provided according to language and text type. For the remaining subsets of Multi Legal Pile we provide general statistics.\"}"}
{"id": "acl-2024-long-805", "page_num": 18, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
