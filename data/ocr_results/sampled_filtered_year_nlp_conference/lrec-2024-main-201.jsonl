{"id": "lrec-2024-main-201", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BenLLM-Eval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP\\n\\nMohsinul Kabir,\u2217 Mohammed Saidul Islam,\u2020 Md Tahmid Rahman Laskar,\u2217 Mir Tafseer Nayeem, M Saiful Bari, Enamul Hoque\\n\\n1Islamic University of Technology, 2York University, 3University of Alberta, 4Nanyang Technological University, 5Dialpad Inc.\\nmohsinulkabir@iut-dhaka.edu, saidulis@yorku.ca, tahmid20@yorku.ca, mnayeem@ualberta.ca, bari0001@e.ntu.edu.sg, enamulh@yorku.ca\\n\\nAbstract\\n\\nLarge Language Models (LLMs) have emerged as one of the most important breakthroughs in NLP for their impressive skills in language generation and other language-specific tasks. Though LLMs have been evaluated in various tasks, mostly in English, they have not yet undergone thorough evaluation in under-resourced languages such as Bengali (Bangla). To this end, this paper introduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to benchmark their performance in the Bengali language that has modest resources. In this regard, we select various important and diverse Bengali NLP tasks, such as text summarization, question answering, paraphrasing, natural language inference, transliteration, text classification, and sentiment analysis for zero-shot evaluation of popular LLMs, namely, GPT-3.5, LLaMA-2-13b-chat, and Claude-2. Our experimental results demonstrate that while in some Bengali NLP tasks, zero-shot LLMs could achieve performance on par, or even better than current SOTA fine-tuned models; in most tasks, their performance is quite poor (with the performance of open-source LLMs like LLaMA-2-13b-chat being significantly bad) in comparison to the current SOTA results. Therefore, it calls for further efforts to develop a better understanding of LLMs in modest-resourced languages like Bengali.\\n\\nKeywords: Large Language Models, LLM evaluation, Bengali NLP\\n\\n1. Introduction\\n\\nSince the advent of pre-trained language models (Devlin et al., 2019; Liu et al., 2019; Rogers et al., 2020), NLP has witnessed revolutionary advancements over the years. These pre-trained language models have produced SOTA results on a variety of NLP tasks with little task-specific fine-tuning. This leads to the development of various pre-trained language models specialized in the Bengali language, such as BanglaBERT (Bhattacharjee et al., 2022), BanglaT5 (Bhattacharjee et al., 2023), and etc. These models have demonstrated exciting progress in many downstream Bengali NLP tasks (Ekram et al., 2022; Akash et al., 2023). However, one major concern for these pre-trained models is that they require fine-tuning using domain-specific large annotated datasets, which is challenging for Bengali due to its under-representation in the NLP domain (Joshi et al., 2020; Chakraborty et al., 2021; Chowdhury et al., 2021) despite being the sixth most spoken language in the world with over 300 million native speakers (Wikipedia, 2023a).\\n\\nRecent developments in large language models (LLMs) (Brown et al., 2020; Shoeybi et al., 2019; Rae et al., 2022; Zhang et al., 2022) have transformed the landscape in NLP. These LLMs, with parameter sizes exceeding a hundred billion, leverage the in-context learning capability to achieve impressive performance in few-shot and zero-shot learning scenarios without the need for task-specific fine-tuning. This capability makes it possible to reduce the need for the annotation of huge datasets, allowing the model to perform well on tasks that it was not trained on.\\n\\nDespite the impressive capabilities of LLMs, they may still frequently generate untruthful facts that diverge from the original input (Ouyang et al., 2022). To address this issue, the Reinforcement Learning from Human Feedback (RLHF) mechanism has been utilized to help LLMs generate honest and harmless responses. ChatGPT and other recently proposed LLMs like PaLM-2, Claude-2, LLaMA-2-chat (Touvron et al., 2023; Anil et al., 2023; Jahan et al., 2024) are trained via leveraging this RLHF technique to mitigate various limitations of the previous generation LLMs and gained widespread popularity. While these LLMs are trained in multiple languages, English possesses the majority of the training data. Though ChatGPT like LLMs has demonstrated strong zero-shot performance in various NLP tasks in English (Laskar et al., 2023a, b) and some other languages (Lai et al., 2023) and domains (Jahan et al., 2023, 2024; Fu et al., 2024), these LLMs are yet to be investigated in the widely spoken, yet modest-resourced, Bengali language domain.\"}"}
{"id": "lrec-2024-main-201", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this regard, we aim to assess the efficacy of LLMs in prevalent downstream NLP tasks specific to the Bengali language, a domain that has not been thoroughly examined compared to the numerous LLM evaluations conducted for English and other Indo-European languages. Due to the lack of task-specific annotated datasets in Bengali, we conducted a zero-shot evaluation with LLMs to investigate if LLMs could be utilized to address the scarcity of large annotated datasets in Bengali. Thus, the findings from this paper would give insights into the capabilities and limitations of LLMs in Bengali, which may pave the way to implement LLMs in real-world applications like Bengali Chatbots. To this end, we present BenLLM-Eval\u2014a comprehensive benchmark that evaluates the zero-shot performance of various LLMs on diverse NLP tasks in the Bengali language, including text summarization, question answering, paraphrasing, natural language inference, transliteration, text classification, and sentiment analysis. The evaluation incorporates carefully crafted prompts to ensure rigorous assessment of the following three popular LLMs: GPT-3.5, LLaMA-2-13b-chat, and Claude-2, and compare them with SOTA fine-tuned models. To the best of our knowledge, this is the first work that evaluates LLMs in the Bengali language. Our experimental results in this paper are summarized as follows:\\n\\n1. Despite some exceptional cases, the zero-shot performance of LLMs is generally inferior compared to the SOTA fine-tuned models across the majority of the tasks in our evaluation. Given the substantial performance disparities observed, it is reasonable to deduce that LLMs, in their current form, are not suitable for serving as a comprehensive solution for modest-resource tasks in Bengali.\\n\\n2. Considering LLMs remarkable performance in zero-shot scenarios within the English language and its subpar performance in modest-resource languages like Bengali, this paper emphasizes the significance of investigating the limitations of LLMs tailored to diverse modest-resource language groups.\\n\\nMethodology\\n\\nThe objective of our study is to assess the efficacy of LLMs in the context of NLP tasks specific to the Bengali language. We cover 7 diverse and important Bengali NLP tasks: (i) Text Summarization, (ii) Question Answering (QA), (iii) Paraphrasing, (iv) Natural Language Inference (NLI), (v) Transliteration, (vi) Text Classification, and (vii) Sentiment Analysis over 8 benchmark datasets. For this purpose, we evaluate ChatGPT (GPT-3.5), Claude-2, and LLaMA-2-13b-Chat models. Similar to the prior work on LLM evaluation (Laskar et al., 2023a; Jahan et al., 2023), we also focus on designing a zero-shot learning setting to benchmark the performance of the LLMs. We prepare a task instruction \\\\( T \\\\) for a given test sample \\\\( X \\\\) and concatenate the text in the test sample with the task instruction to construct the prompt \\\\( P \\\\). The prompt \\\\( P \\\\) is then passed as input to the LLMs, which generates the response \\\\( R \\\\). A comprehensive description of the tasks, datasets, and prompts devised for evaluating each specific task is presented below and also summarized in Table 1. Next, we briefly discuss each task.\\n\\nText Summarization: Summarization is the process of automatically generating a concise and coherent summary of a longer text document (Nayeem and Chali, 2017a, b; Laskar et al., 2022), preserving the most important information while reducing the length (Nayeem et al., 2018). In this paper, we evaluate the XL-Sum dataset (Hasan et al., 2021) that consists of 1 million manually annotated data samples from 44 languages. We only took the Bengali samples for evaluation.\\n\\nQuestion Answering: For the question-answering task, we evaluate the performance of LLMs on the SQuAD_Bangla dataset (Bhattacharjee et al., 2022). This dataset was constituted using two benchmark English datasets: SQuAD 2.0 (Williams et al., 2018) and TyDi QA (Clark et al., 2020). The objective of this task is to determine whether the answer to a given question \\\\( Q \\\\) can be inferred from the reference context \\\\( C \\\\). We provide the reference context along with the question and ask LLMs whether they can infer the answer to the question from the given reference.\\n\\nParaphrasing: The paraphrasing task aims to generate a paraphrase of the input text. To evaluate this task, we choose the Bengali samples from the IndicParaphrase dataset (Kumar et al., 2022), which is the largest Indic paraphrasing dataset across 11 Indic languages (Wikipedia, 2023b).\\n\\nNatural Language Inference: Natural Language Inference (NLI) aims to predict the entailment/contradiction relations between two input sentences: a premise and a hypothesis. To evaluate LLMs for Bengali NLI, we utilize the BNLI dataset (Bhattacharjee et al., 2022) (curated from the benchmark XNLI dataset (Conneau et al., 2018)) that provides annotated data with three categories: Entailment, Contradiction, and Neutral.\\n\\nTransliteration: Transliteration is the process...\"}"}
{"id": "lrec-2024-main-201", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Datasets Details with our Prompts for Various Tasks.\\n\\nThe Dakshina dataset (Roark et al., 2020) is utilized to evaluate transliteration errors. Errors can be (i) substitutions, (ii) insertions, or (iii) deletions. A substitution error occurs when the predicted transliteration differs from the reference transliteration. An insertion error happens when extra characters/words are present in the predicted transliteration. A deletion error occurs when characters/words are missing from the predicted transliteration. The error rate is calculated as:\\n\\n$$\\\\text{error rate} = \\\\frac{\\\\text{substitutions} + \\\\text{deletions} + \\\\text{insertions}}{\\\\text{total reference tokens}} \\\\times 100$$\\n\\nTwo types of error rates are proposed for this task in the Dakshina dataset (Roark et al., 2020): the Character-Error Rate (CER) treats each token as an individual Unicode character, while the Word-Error Rate (WER) treats each token as a substring separated by whitespace. We report both CER and WER for single-word transliteration, and WER for full-sentence transliteration.\\n\\nText Classification: This task refers to the classification of the category for a given input text. To evaluate the text classification capability of LLMs, we use the Soham Bengali News Classification dataset that is included in the IndicGLUE (Kakwani et al., 2020) benchmark. This dataset contains six news categories: kolkata, state, national, international, sports, and entertainment.\\n\\nSentiment Analysis: We evaluate the Sentiment Analysis capability of the LLMs with two datasets: SentNoB (Islam et al., 2021), and IndicSentiment (Doddapaneni et al., 2022). The SentNoB dataset comprises texts that are informally written in Bengali, collected from public comments on news and videos in social media covering a wide range of domains like politics, agriculture, education, etc. The second dataset, IndicSentiment, was manually curated for the IndicXTREME benchmark and contains product reviews of multiple categories. We only used the Bengali samples from the dataset out of the 12 Indic languages.\\n\\nResults and Discussion: We report the performance of LLMs for different tasks and compare their performance with the current SOTA fine-tuned models (see Table 2). Furthermore, we provide an in-depth error analysis for each of the tasks in the Appendix A.\"}"}
{"id": "lrec-2024-main-201", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Task Pair | 6-gram LSTM Transformer | Noisy Channel | GPT-3.5 | LLaMA-2-13b-chat | Claude-2 |\\n|-----------|------------------------|--------------|---------|------------------|---------|\\n| CER (\u2193)   | 14.2                   | 13.9         | 13.2    | 18.1             | 39.85   |\\n| WER (\u2193)   | 54.0                   | 54.7         | 50.6    | 60.6             | 80.72   |\\n| CER (\u2193)   | 13.9                   | 13.2         | 18.1    | -                | 39.85   |\\n| WER (\u2193)   | 54.7                   | 50.6         | 60.6    | -                | 80.72   |\\n\\nTable 3: Single-word and Full-sentence Transliteration results. Here, the baseline results are adopted from Roark et al. (2020). Best results are boldfaced and lower (\u2193) is better.\\n\\nThe LLMs, Claude-2 performs the best in terms of ROUGE-1 and ROUGE-L, while GPT-3.5 performs the best in terms of ROUGE-2. We also find that GPT-3.5 tends to generate much longer summaries (229 words on average) than the gold reference summaries (148 words on average), while the length of the Claude-2 generated summaries (137 words on average) is more consistent with the gold reference summaries (148 words on average). Moreover, to find out why LLaMA-2-13b-chat achieves very low ROUGE scores, we manually reviewed the responses and found out that it ended up generating all the summaries in English. By translating the summaries to Bengali using Google's Translation API (Google, 2020), the R-1, 2, and L scores are increased to 4.69, 0.61, and 3.61, respectively, for LLaMA-2-13b-chat.\\n\\nQuestion Answering Evaluation: Since LLMs frequently generate responses that may not be an exact match of the gold label but are nonetheless correct, our QA evaluation utilizes human intervention to compute the F1 score. We find that Claude-2 and GPT-3.5 achieve performance almost similar to the current SOTA results in terms of F1. However, the performance of all LLMs is quite poor in terms of Exact Match due to generating descriptive responses, as well as paraphrases.\\n\\nParaphrasing Evaluation: For the paraphrasing task, we observe a very low BLEU score for all LLMs, which is a phenomenon similar to what happened with the EM metric for the QA task. Note that the BLEU score also computes word-level similarity, while such limitations of word-based similarity metrics have also been noticed LLMs are evaluated in English datasets (Laskar et al., 2023a).\\n\\nNatural Language Inference Evaluation: We observe that while GPT-3.5 achieves the best results among all LLMs on the BNLI dataset, it is still much lower in comparison to the current SOTA results in BanglaBERT. Surprisingly, we find that Claude-2 is the worst performer in this task while being outperformed by the much smaller LLaMA-2-13b-chat model. To further investigate in which cases LLMs perform poorly in the NLI task, we demonstrate their predictions using the Confusion Matrix in Figure 1 of Appendix and find that LLaMA-2-13b-chat was quite bad while predicting the Neutral types and Claude-2 performed poorly while predicting the Entailment types. In the case of GPT-3.5, it performed poorer while predicting the Neutral type class in comparison to the other types.\\n\\nTransliteration Evaluation: From Table 3, we find that GPT-3.5 achieves superior performance over other LLMs in single-word transliteration. While it fails to outperform the existing results, it still achieves competitive performance compared to the prior SOTA models. This is an interesting observation given the intricate structure and infrequent usage of lexicons in the test set. In full-sentence transliteration, GPT-3.5 makes fewer word errors and character errors (60.6% & 18.1%) compared to other LLMs ((80.72% & 39.85%) for Llama-2-13b-chat and (68.07% & 23.16%) for Claude-2). However, the results are quite poor compared to other SOTA models.\\n\\nText Classification Evaluation: LLMs performed poorly on the Soham News Article classification dataset, with the best-performing LLM in this dataset, the Claude-2 model achieving only 48.61% accuracy, followed by GPT-3.5 achieving 48.47% accuracy while the LLaMA-2-13b-chat being the worst performer with an accuracy of 29.27%. On the contrary, the XLM-R model, which is the SOTA for this task, obtained an accuracy of 87.60% in this dataset. While we achieved these results by providing the class labels alongside our prompts (see Table 1), if the class labels were not mentioned, the accuracy dropped to 20.76%.\"}"}
{"id": "lrec-2024-main-201", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"18.36%, and 1.06% for Claude-2, GPT-3.5, and LLaMA-2-13b-chat, respectively. This indicates the importance of using descriptive prompts to improve the zero-shot performance of LLMs.\\n\\nSentiment Analysis Evaluation:\\n\\nIn the sentiment classification task, we observe that GPT-3.5 performed exceptionally well on the IndicSentiment dataset (Doddapaneni et al., 2022), attaining a new SOTA accuracy of 90.20%. While we also find that the Claude-2 model achieved impressive performance (88.48% accuracy), the LLaMA-2-13b-chat model performed much poorer. We also find a similar trend for sentiment analysis in the SentNoB (Islam et al., 2021) dataset. In this dataset, to ensure a fair comparison with the current SOTA, we use the precision, recall, and F1 scores as evaluation metrics. While none of the LLMs could outperform the current SOTA, all of them (especially GPT-3.5 and Claude-2) achieved performance comparable to the SOTA results. This is consistent with LLMs' performance for this task in English (Laskar et al., 2023a).\\n\\nA detailed error analysis and sample outputs for all tasks are presented in Appendix A.\\n\\nTask Contamination Analysis\\n\\nTask contamination analysis (Li and Flanigan, 2023) is essential to ensure a fair model evaluation since it helps identify a model's prior exposure to test tasks on its training data. Inspired by the work of Li and Flanigan (2023), we include task contamination analysis in our evaluation to appropriately assess the performance of the LLMs. We utilize two methods: Task Example Extraction (TEE) and Membership Inference (for generative tasks like summarization and paraphrasing) to verify the evidence of task contamination.\\n\\nTEE involves retrieving task examples from instruction-tuned models, although it doesn't require an exact match with existing training data, helps identify potential task contamination in zero and few-shot learning scenarios, as any task examples found indicate possible prior exposure. Additionally, Membership Inference is a method used for generation tasks, where the model's generated content is checked against the original dataset for exact matches. If a match is found, it implies that the content was part of the LLM's training data, indicating direct exposure rather than general learning ability.\\n\\nAt first, we applied TEE to all tasks without explicitly mentioning the dataset name. Our findings reveal that only GPT-3.5 could generate examples related to these tasks (except Natural Language Inference), while Claude-2 and LLaMA-2-13b-chat models failed to extract task examples for any tasks. Therefore, there is a possibility that such tasks were already included in the pre-training data of GPT-3.5. Regarding the BNLI dataset where no models could extract any task examples, we find that the premises, hypotheses, and labels generated by all LLMs for Bengali were significantly inaccurate, providing evidence that contamination did not occur. In terms of extracting task examples in the transliteration task, we find that only GPT-3.5 could extract the task examples for both word-level and sentence-level transliteration, whereas both LLaMA-2-13b-chat and Claude-2 failed to extract any task examples.\\n\\nFinally, we applied the Membership Inference technique in the generative tasks (e.g., summarization and paraphrase generation) to analyze task contamination. In the summarization task, none of the LLMs produced an output that aligned with the test data labels. However, on the paraphrasing task, GPT-3.5 produced around 50 exact match instances, while Claude-2 produced 30 and LLaMA-2-13b-chat produced 15 exact matches of the generated outputs and test labels. So it is possible that the test set of the IndicPara dataset was exposed to the LLMs while training.\\n\\nIn summary, contamination could be an issue with the GPT-3.5 model in Sentiment Analysis, Text Classification, Summarization, and QA tasks, while all the models, i.e., GPT-3.5, LLaMA-2-13b-chat, and Claude-2 were affected by task contamination in the Paraphrasing task. However, in Natural Language Inference, we did not see any evidence of task contamination.\\n\\nConclusions and Future Work\\n\\nIn this paper, we introduce BenLLM-Eval, which provides a comprehensive zero-shot evaluation of LLMs on seven benchmark NLP tasks to understand the capability of LLMs in the modest-resource Bengali language. The results revealed that while in some tasks, zero-shot closed-source LLMs like GPT-3.5 or Claude-2 perform on par (e.g., summarization) or even outperform (e.g., sentiment analysis) current SOTA models, in most tasks the LLMs exhibited considerably lower performance compared to supervised fine-tuned models. We also observed that the open-source LLaMA-2-13b-chat model performed significantly poorer in most tasks. Thus, open-source LLMs should be extensively evaluated on low to modest-resource languages to ensure a proper understanding of their capabilities and limitations. Our findings also reveal that in order to achieve optimal performance across languages, LLMs should also be trained on corpus covering various languages along with English. In the future, we intend to expand our experiments by including additional low to modest-resource languages, tasks, datasets, and settings to obtain broader insights.\"}"}
{"id": "lrec-2024-main-201", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nThe datasets used to train LLMs like GPT-3.5 or Claude-2 are not disclosed; consequently, some datasets used to evaluate the model may or may not have existed during model training. In addition, the API used to evaluate the GPT-3.5 model is based on OpenAI's GPT-3.5 turbo. Although an improved version GPT-4 is also available, GPT-4 is significantly more expensive than GPT-3.5. Thus, we could not use GPT-4 in this work. Other than these limitations, this study will provide a practical direction for future research into the performance of LLMs in modest-resourced languages in the NLP domain.\\n\\nEthics Statement\\n\\nThis study assesses the performance of LLMs in seven benchmark NLP tasks in the Bengali language wherein LLMs are tasked with generating an output based on the information given in the input. Therefore, no prompt or information was presented to LLMs that could result in the generation of any output raising any ethical or privacy issues. Furthermore, since this study is targeted only to the evaluation of the effectiveness and limitations of LLMs in Bengali NLP downstream tasks, conducting the study does not lead to any unwanted biases. There will be no licensing issue as all the datasets that are utilized in this research are publicly available.\\n\\nAcknowledgements\\n\\nThis research is supported by the Generic (Minor/Startup/Other) research funds of York University awarded to Md Tahmid Rahman Laskar. We also thank Anthropic for providing free access to the Claude-2 model, the Canada Foundation for Innovation grant and Compute Canada for its computing resources. Mir Tafseer Nayeem is also supported by the Huawei PhD Fellowship.\\n\\nReferences\\n\\nAbu Ubaida Akash, Mir Tafseer Nayeem, Faisal Tareque Shohan, and Tanvir Islam. 2023. Shironaam: Bengali news headline generation using auxiliary information. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 52\u201367, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Tapaopa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.\\n\\nAbhik Bhattacharjee, Tahmid Hasan, Wasi Ahmad, Kazi Samin Mubasshir, Md Saiful Islam, Anindya Iqbal, M. Sohel Rahman, and Rifat Shahriyar. 2022. BanglaBERT: Language model pretraining and benchmarks for low-resource language understanding evaluation in Bangla. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1318\u20131327, Seattle, United States. Association for Computational Linguistics.\\n\\nAbhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, and Rifat Shahriyar. 2023. BanglaNLG and BanglaT5: Benchmarks and resources for evaluating low-resource natural language generation in Bangla. In Findings of the Association for Computational Linguistics: EACL 2023, pages 726\u2013735, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nSusmoy Chakraborty, Mir Tafseer Nayeem, and Wasi Uddin Ahmad. 2021. Simple or complex? learning to predict readability of bengali texts. Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12621\u201312629.\\n\\nRadia Rayan Chowdhury, Mir Tafseer Nayeem, Tahsin Tasnim Mim, Md. Saifur Rahman Chowdhury, and Taufiqul Jannat. 2021. Unsupervised abstractive summarization of Bengali text documents. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2612\u20132619, Online. Association for Computational Linguistics.\\n\\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages.\"}"}
{"id": "lrec-2024-main-201", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Transactions of the Association for Computational Linguistics, 8:454\u2013470.\\n\\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475\u20132485, Brussels, Belgium. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nSumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, and Pratyush Kumar. 2022. Indicxtreme: A multi-task benchmark for evaluating indic languages. Syed Mohammed Sartaj Ekram, Adham Arik Rahman, Md. Sajid Altaf, Mohammed Saidul Islam, Mehrab Mustafy Rahman, Md Mezbaur Rahman, Md Azam Hossain, and Abu Raihan Mostofa Kamal. 2022. BanglaRQA: A benchmark dataset for under-resourced Bangla language reading comprehension-based question answering with diverse question-answer types. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2518\u20132532, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nXue-Yong Fu, Md Tahmid Rahman Laskar, Elena Khasanova, Cheng Chen, and Shashi Bhushan TN. 2024. Tiny titans: Can smaller large language models punch above their weight in the real world for meeting summarization? arXiv preprint arXiv:2402.00841.\\n\\nGoogle. 2020. Googletrans: Free and unlimited google translate api for python. Accessed: March 12, 2024.\\n\\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XL-sum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693\u20134703, Online. Association for Computational Linguistics.\\n\\nKhondoker Ittehadul Islam, Sudipta Kar, Md Saiful Islam, and Mohammad Ruhul Amin. 2021. SentNoB: A dataset for analysing sentiment on noisy Bangla texts. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3265\u20133271, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nIsrat Jahan, Md Tahmid Rahman Laskar, Chun Peng, and Jimmy Huang. 2023. Evaluation of ChatGPT on biomedical tasks: A zero-shot comparison with fine-tuned generative transformers. In The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 326\u2013336, Toronto, Canada. Association for Computational Linguistics.\\n\\nIsrat Jahan, Md Tahmid Rahman Laskar, Chun Peng, and Jimmy Xiangji Huang. 2024. A comprehensive evaluation of large language models on benchmark biomedical text processing tasks. Computers in Biology and Medicine, page 108189.\\n\\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282\u20136293, Online. Association for Computational Linguistics.\\n\\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and Pratyush Kumar. 2020. IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4948\u20134961, Online. Association for Computational Linguistics.\\n\\nAman Kumar, Himani Shrotriya, Prachi Rani Sahu, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, Amogh Mishra, Mitesh M. Khapra, and Pratyush Kumar. 2022. Indicnlg benchmark: Multilingual datasets for diverse nlg tasks in indic languages. In Conference on Empirical Methods in Natural Language Processing.\\n\\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning.\"}"}
{"id": "lrec-2024-main-201", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. 2023a. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets. In Findings of the Association for Computational Linguistics: ACL 2023, pages 431\u2013469, Toronto, Canada. Association for Computational Linguistics.\\n\\nMd Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen, and Shashi Bhushan TN. 2023b. Building real-world meeting summarization systems using large language models: A practical perspective. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 343\u2013352, Singapore. Association for Computational Linguistics.\\n\\nMd Tahmid Rahman Laskar, Enamul Hoque, and Jimmy Xiangji Huang. 2022. Domain adaptation with pre-trained transformers for query-focused abstractive text summarization. Computational Linguistics, 48(2):279\u2013320.\\n\\nChangmao Li and Jeffrey Flanigan. 2023. Task contamination: Language models may not be few-shot anymore.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pre-training approach. ArXiv, abs/1907.11692.\\n\\nMir Tafseer Nayeem and Yllias Chali. 2017a. Extract with order for coherent multi-document summarization. In Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for Natural Language Processing, pages 51\u201356, Vancouver, Canada. Association for Computational Linguistics.\\n\\nMir Tafseer Nayeem and Yllias Chali. 2017b. Paraphrastic fusion for abstractive multi-sentence compression generation. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM '17, pages 2223\u20132226, New York, NY, USA. Association for Computing Machinery.\\n\\nMir Tafseer Nayeem, Tanvir Ahmed Fuad, and Yllias Chali. 2018. Abstractive unsupervised multi-document summarization using paraphrastic sentence fusion. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1191\u20131204, Santa Fe, New Mexico, USA. Association for Computational Linguistics.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.\\n\\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver?\"}"}
{"id": "lrec-2024-main-201", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A.1. Prompt Construction Details\\n\\nIn the process of adapting datasets into prompts, we carefully analyzed the prompt templates in various LLM evaluation papers on English datasets (Laskar et al., 2023a; Qin et al., 2023) and then explored several prompt variants in Bengali for each task to ascertain the optimal one. For example, at the beginning of the Soham News Article Classification task, our prompts did not feature class labels, which caused a pronounced failure in the accurate classification of news articles by all tested LLMs, markedly degrading their effectiveness. However, this situation improved considerably when we updated the prompts to integrate class labels, leading to a notable improvement in the models' capacity to correctly associate articles with their respective classes. Thus, experimenting with different variations of prompts to come up with the optimal one is a crucial step in our prompt design phase.\\n\\nAppendix A.2. In-depth Error Analysis\\n\\nIn this section, we discuss some interesting cases where GPT-3.5 failed to generate coherent responses given the input prompt. We discuss these cases in the following sections:\\n\\nA.3. Generative Tasks\\n\\nThe error cases for generative tasks like abstractive summarization, question answering, and paraphrasing is discussed in this section.\\n\\nA.4. Abstractive Summarization\\n\\nIn the summarization task, we notice that GPT-3.5's responses are highly inconsistent. While GPT-3.5 occasionally recognizes the context and provides a good summary of the key points, it frequently misses these details in the reference text and overstuffs the response. One example can be demonstrated as follows:\\n\\nPrompt:\\nPlease provide an one-sentence summary of the following Bangla text input. The input will be a long Bangla paragraph, the output should be a short Bangla paragraph summarizing only the vital information of the input text in one sentence. Please make sure that the output contains the most essential statistical data. Note: Please do not provide anything other than the summarized Bangla output.\\n\\nInput:\\n\u09ae\u09be\u09a8\u09b8\u09c1\u09b0 \u0986\u09ac\u09ac\u09cd\u09be\u09b8 \u09aa\u09f0\u09cd\u09a7\u09be\u09a8\u09ae\u09a8\u09cd\u09a4\u09f0\u09cd\u09c0 \u09ac\u09be \u09a4\u09be\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u09aa\u0995\u09cd\u09b7 - \u09c7\u0995\u0989\u0987 \u0986\u09b8\u09c7\u09b2 \u0995\u09cd\u09b7\u09ae\u09a4\u09be\u09af\u09bc \u09af\u09be\u09ac\u09be\u09b0 \u09ae\u09c7\u09a4\u09be \u09bf\u09a8\u09b0\u0999\u09cd\u0995\u09c1 \u09b6 \u09b8\u0982\u0996\u09af\u09cd\u09be\u0997\u09bf\u09b0\u09b7\u09cd\u09a0\u09a4\u09be \u09aa\u09be\u09a8\u09bf\u09a8\u0964 \u0986\u09b0 \u098f\u09b0 \u09ae\u09c7\u09a7\u09af\u09cd\u0987 \u09bf\u0995\u0982\u09c7\u09ae\u0995\u09be\u09b0 \u09b9\u09c7\u09af\u09bc \u0989\u09c7\u09a0\u09c7\u099b \u09b0\u09be\u09ae (Raam) \u09a8\u09be\u09c7\u09ae \u098f\u0995\u09bf\u099f \u0987\u09b8\u09b2\u09be\u09ae\u09aa\u09a8\u09cd\u09a5\u09c0 \u0986\u09b0\u09ac \u09a6\u09b2 (\u0987\u0989\u09a8\u09be\u0987\u09c7\u099f\u09a1 \u0986\u09b0\u09ac \u09bf\u09b2\u09b8\u09cd\u099f \u09bf\u09b9\u09c7\u09b8\u09c7\u09ac\u0993 \u09a6\u09b2-\u09bf\u099f \u09aa\u09bf\u09b0\u09bf\u099a\u09a4)\u0964 \u098f\u09ac\u09be\u09c7\u09b0\u09b0 \u09bf\u09a8\u09ac\u09b0\u09cd\u09be\u099a\u09c7\u09a8 \u09aa\u09be\u0981\u099a\u09bf\u099f \u0986\u09b8\u09c7\u09a8 \u099c\u09af\u09bc \u09c7\u09aa\u09c7\u09af\u09bc\u09c7\u099b \u098f \u09a6\u09b2\u09bf\u099f- \u09af\u09be \u09bf\u09ae\u09b8\u09cd\u099f\u09be\u09b0 \u09c7\u09a8\u09a4\u09be\u09bf\u09a8\u09af\u09bc\u09be\u09b9\u09c1\u09c7\u0995 \u0995\u09cd\u09b7\u09ae\u09a4\u09be\u09af\u09bc \u09b0\u09be\u0996\u09be \u09ac\u09be \u0995\u09cd\u09b7\u09ae\u09a4\u09be \u09c7\u09a5\u09c7\u0995 \u09bf\u09ac\u09a6\u09be\u09af\u09bc \u0995\u09c7\u09b0 \u09bf\u09a6\u09c7\u09a4 \u09ad\u09c2 \u09bf\u09ae\u0995\u09be \u09b0\u09be\u0996\u09be\u09b0 \u09b8\u09c1\u09c7\u09af\u09be\u0997 \u0995\u09c7\u09b0 \u09bf\u09a6\u09c7\u09af\u09bc\u09c7\u099b\u0964 \u09bf\u09ac\u09bf\u09ac\u09bf\u09b8 \u09ac\u09be\u0982\u09b2\u09be\u09af\u09bc \u0986\u09b0\u0993 \u09aa\u09a1\u09bc\u09c1 \u09a8: \u09bf\u09ab\u09bf\u09b2\u09bf\u09b8\u09cd\u09a4\u09bf\u09a8-\u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09bf\u09b2 \u09b8\u0982\u0998\u09be\u09c7\u09a4\u09b0 \u09ae\u09c2\u09c7\u09b2 \u09c7\u09af \u09a6\u09b6\u09bf\u099f \u09aa\u09f0\u09cd\u09b6\u09cd\u09a8 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09b2\u09c7\u0995 \u0987\u09b9\u09c1\u09bf\u09a6 \u09b0\u09be\u09b7\u09cd\u099f\u09f0\u09cd \u09c7\u0998\u09be\u09b7\u09a3\u09be\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u0995\u09f0\u09cd\u09af\u09bc\u09be \u0995\u09c0 \u09b9\u09c7\u09ac? \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u0995\u09be\u09c7\u099b \u0986\u09b0\u09ac\u09b0\u09be \u09c7\u0995\u09a8 \u09aa\u09b0\u09be\u09bf\u099c\u09a4 \u09b9\u09c7\u09af\u09bc\u09bf\u099b\u09b2? \u09e8\u09e6\u09e7\u09ef \u09b8\u09be\u09b2 \u09c7\u09a5\u09c7\u0995 \u09c7\u09a8\u09a4\u09c3\u09a4\u09ac\u09cd \u09bf\u09a6\u09c7\u099a\u09cd\u099b\u09a8 \u09ae\u09be\u09a8\u09b8\u09c1\u09b0 \u0986\u09ac\u09ac\u09cd\u09be\u09b8 \u09b0\u09be\u09ae: \u09bf\u09ac\u09b8\u09cd\u09ae\u09af\u09bc\u0995\u09b0 \u09bf\u0995\u0982\u09c7\u09ae\u0995\u09be\u09b0 \u09b0\u0995\u09cd\u09b7\u09a3\u09b6\u09c0\u09b2 \u09ae\u09c1\u09b8\u09bf\u09b2\u09ae \u09ae\u09be\u09a8\u09b8\u09c1\u09b0 \u0986\u09ac\u09ac\u09cd\u09be\u09c7\u09b8\u09b0 \u09c7\u09a8\u09a4\u09c3\u09c7\u09a4\u09ac\u09cd \u09b0\u09be\u09ae \u09a6\u09b2\u09bf\u099f \u09ae\u09c2\u09b2\u09a4 \u09bf\u09ab\u09bf\u09b2\u09bf\u09b8\u09cd\u09a4\u09c7\u09a8\u09b0 \u0997\u09be\u099c\u09be \u09b6\u09be\u09b8\u09a8 \u0995\u09b0\u09be \u09b9\u09be\u09ae\u09be\u09c7\u09b8\u09b0 \u09a7\u09ae\u09b0\u09cd\u09c0\u09af\u09bc \u09ad\u09be\u09ac\u09a7\u09be\u09b0\u09be\u09af\u09bc \u0997\u09c7\u09a1\u09bc \u0993\u09a0\u09be \u098f\u0995\u09bf\u099f \u0987\u09b8\u09b2\u09be\u09ae\u09aa\u09a8\u09cd\u09a5\u09c0 \u09a6\u09b2\u0964 \u09e7\u09ef\u09ef\u09ec \u09b8\u09be\u09c7\u09b2 \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u09b7\u09cd\u09a0\u09a4 \u098f\u0987 \u09a6\u09b2\u09bf\u099f \u09b6\u09c1\u09b0\u09c1 \u09c7\u09a5\u09c7\u0995 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u09aa\u09be\u09b2\u09b0\u09cd\u09be\u09c7\u09ae\u09a8\u09cd\u099f \u0986\u09b8\u09a8 \u09c7\u09aa\u09c7\u09af\u09bc \u0986\u09b8\u09c7\u099b\u0964 \u09af\u09bf\u09a6\u0993 \u09e8\u09e6\u09e6\u09ef \u09b8\u09be\u09c7\u09b2\u09b0 \u09bf\u09a8\u09ac\u09b0\u09cd\u09be\u099a\u09c7\u09a8 \u0985\u0982\u09b6 \u09c7\u09a8\u09af\u09bc\u09be\u09b0 \u09c7\u0995\u09cd\u09b7\u09c7\u09a4\u09f0\u09cd \u09a4\u09be\u09c7\u09a6\u09b0 \u0993\u09aa\u09b0 \u09bf\u09a8\u09c7\u09b7\u09a7\u09be \u099c\u09cd\u099e\u09be \u09bf\u09a6\u09c7\u09af\u09bc\u09bf\u099b\u09c7\u09b2\u09be \u09c7\u09a6\u09b6\u09bf\u099f\u09b0 \u09bf\u09a8\u09ac\u09b0\u09cd\u09be\u099a\u09a8\u09c0 \u0995\u09a4\u09c3\u09b0\u09cd\u09aa\u0995\u09cd\u09b7\u0964 \u09aa\u09c7\u09b0 \u09b8\u09c1\u09bf\u09aa\u09f0\u09cd\u09ae \u09c7\u0995\u09be\u099f\u09b0\u09cd \u0993\u0987 \u09bf\u09a8\u09c7\u09b7\u09a7\u09be\u099c\u09cd\u099e\u09be \u09ac\u09be\u09bf\u09a4\u09b2 \u0995\u09c7\u09b0 \u09c7\u09a6\u09af\u09bc\u0964 \u09e8\u09e6\u09e8\u09e6 \u09b8\u09be\u09c7\u09b2 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u0986\u09b0\u09ac \u09b0\u09be\u099c\u09c8\u09a8\u09bf\u09a4\u0995 \u09a6\u09b2\u0997\u09c1\u09c7\u09b2\u09be\u09b0 \u09c7\u099c\u09be\u099f- \u099c\u09c7\u09af\u09bc\u09a8\u09cd\u099f \u0986\u09b0\u09ac \u09bf\u09b2\u09c7\u09b8\u09cd\u099f\u09b0 \u0985\u0982\u09b6 \u09bf\u099b\u09c7\u09b2\u09be \u09af\u09be\u09b0\u09be \u09a8\u09bf\u099c\u09b0\u09bf\u09ac\u09b9\u09c0\u09a8 \u09ad\u09be\u09c7\u09ac \u09aa\u09c7\u09a8\u09b0\u09bf\u099f \u09bf\u09b8\u099f \u09c7\u09aa\u09c7\u09af\u09bc\u09c7\u099b \u09aa\u09be\u09b2\u09b0\u09cd\u09be\u09c7\u09ae\u09c7\u09a8\u09cd\u099f\u0964 \u09a4\u09c7\u09ac \u098f\u0995\u09be\u0987 \u09bf\u09a8\u09ac\u09b0\u09cd\u09be\u099a\u09c7\u09a8 \u09b2\u09a1\u09bc\u09be\u09b0 \u099c\u09a8\u09af\u09cd \u099a\u09b2\u09bf\u09a4 \u09ac\u099b\u09c7\u09b0\u09b0 \u09e8\u09ee\u09c7\u09b6 \u099c\u09be\u09a8\u09c1\u09af\u09bc\u09be\u09bf\u09b0 \u0993\u0987 \u099c\u09c7\u09be\u099f \u09c7\u099b\u09c7\u09a1\u09bc \u0986\u09c7\u09b8 \u09b0\u09be\u09ae\u0964 \u0986\u09b0\u09ac \u09b8\u09ae\u09cd\u09aa\u09f0\u09cd\u09a6\u09be\u09c7\u09af\u09bc\u09b0 \u09b8\u09be\u09ae\u09bf\u0997\u09f0\u09cd\u0995 \u09a6\u09b0\u0995\u09b7\u09be\u0995\u09bf\u09b7\u09b0\"}"}
{"id": "lrec-2024-main-201", "page_num": 10, "content": "{\"primary_language\":\"bn\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u0995\u09cd\u09b7\u09ae\u09a4\u09be\u0995\u09c7 \u09b9\u09c1\u09ae\u09bf\u0995\u09b0 \u09ae\u09c1\u09c7\u0996\u09c7\u09b2\u0993 \u098f \u09bf\u09b8\u09a6\u09cd\u09a7\u09be\u09a8\u09cd\u09a4 \u09a6\u09b2\u09bf\u099f\u09b0 '\u09bf\u0995\u0982\u09c7\u09ae\u0995\u09be\u09b0' \u09b9\u09c7\u09af\u09bc \u0993\u09a0\u09be\u09b0 \u09aa\u09a5\u0993 \u09c8\u09a4\u09bf\u09b0 \u0995\u09c7\u09b0 \u09bf\u09a6\u09c7\u09af\u09bc\u09c7\u099b\u0964 \u09bf\u09a8\u09ac\u09b0\u09cd\u09be\u099a\u09c7\u09a8 \u09c7\u09ad\u09be\u099f \u09bf\u09a6\u09c7\u099a\u09cd\u099b\u09a8 \u098f\u0995 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09bf\u09b2-\u0986\u09b0\u09ac \u09a8\u09be\u09b0\u09c0 \u09a6\u09b2\u09bf\u099f\u09b0 \u09c7\u09aa\u099b\u09c7\u09a8 \u09a5\u09be\u0995\u09be \u09ac\u09af\u09cd\u09bf\u0995\u09cd\u09a4 \u09ea\u09ec \u09ac\u099b\u09b0 \u09ac\u09af\u09bc\u09b8\u09c0 \u09ae\u09be\u09a8\u09b8\u09c1\u09b0 \u0986\u09ac\u09ac\u09cd\u09be\u09b8 \u09b9\u09c7\u09b2\u09a8 \u09a6\u09b2\u09bf\u099f\u09b0 \u09ae\u09c2\u09b2 \u09ac\u09af\u09cd\u09bf\u0995\u09cd\u09a4\u0964 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2 \u099c\u09be\u09a4\u09bf\u09a4\u09c7\u0995 \u0987\u09bf\u09a4\u09b9\u09be\u09c7\u09b8\u09b0 \u0997\u09c1\u09b0\u09c1\u09a4\u09ac\u09cd\u09aa\u09c2\u09a3\u09b0\u09cd \u09b8\u09ae\u09af\u09bc\u09bf\u099f\u09c7\u09a4 \u09bf\u09a4\u09bf\u09a8 \u098f\u0996\u09a8 \u09b0\u09be\u099c\u09a8\u09c0\u09bf\u09a4\u09b0 \u09c7\u0995\u09c7\u09a8\u09cd\u09a6\u09f0\u09cd \u09b0\u09c7\u09af\u09bc\u09c7\u099b\u09a8\u0964 \u09c7\u099c\u09b0\u09c1\u099c\u09be\u09c7\u09b2\u09c7\u09ae\u09b0 \u09bf\u09b9\u09ac\u09f0\u09cd\u09c1 \u09bf\u09ac\u09b6\u09ac\u09cd\u09bf\u09ac\u09a6\u09af\u09cd\u09be\u09b2\u09af\u09bc \u09c7\u09a5\u09c7\u0995 \u09aa\u09be\u09b6 \u0995\u09b0\u09be \u09a6\u09a8\u09cd\u09a4 \u09bf\u099a\u09bf\u0995\u09ce\u09b8\u0995 \u09bf\u09a4\u09bf\u09a8\u0964 \u09af\u09bf\u09a6\u0993 \u09aa\u09c7\u09b0 \u09b9\u09be\u0987\u09ab\u09be \u09bf\u09ac\u09b6\u09ac\u09cd\u09bf\u09ac\u09a6\u09af\u09cd\u09be\u09b2\u09af\u09bc \u09c7\u09a5\u09c7\u0995 \u09b0\u09be\u09b7\u09cd\u099f\u09f0\u09cd\u09bf\u09ac\u099c\u09cd\u099e\u09be\u09b0 \u09ae\u09be\u09b8\u09cd\u099f\u09be\u09b8\u09b0\u09cd \u09bf\u09a1\u09bf\u0997\u09f0\u09cd \u09b2\u09be\u09ad \u0995\u09c7\u09b0\u09a8\u0964 \u09e8\u09e6\u09e7\u09ef \u09b8\u09be\u09c7\u09b2 \u09bf\u09a4\u09bf\u09a8 \u0987\u0989\u09a8\u09be\u0987\u09c7\u099f\u09a1 \u0986\u09b0\u09ac \u09bf\u09b2\u09c7\u09b8\u09cd\u099f\u09b0 \u09c7\u09a8\u09a4\u09be \u09ae\u09c7\u09a8\u09be\u09a8\u09c0\u09a4 \u09b9\u09a8 \u098f\u09ac\u0982 \u0993\u0987 \u09c7\u099c\u09be\u09c7\u099f\u09b0 \u0985\u0982\u09b6 \u09bf\u09b9\u09c7\u09b8\u09c7\u09ac \u09aa\u09be\u09b2\u09b0\u09cd\u09be\u09c7\u09ae\u09c7\u09a8\u09cd\u099f\u0993 \u09bf\u09a8\u09ac\u09b0\u09cd\u09be\u09bf\u099a\u09a4 \u09b9\u09a8\u0964 \u0985\u09ac\u09b6\u09af\u09cd \u09c7\u09ac\u09bf\u09a8\u09af\u09bc\u09be\u09bf\u09ae\u09a8 \u09c7\u09a8\u09a4\u09be\u09bf\u09a8\u09af\u09bc\u09be\u09b9\u09c1 \u0993 \u09a4\u09be\u09b0 \u09bf\u09b2\u0995\u09c1 \u09a6 \u09aa\u09be\u09bf\u099f\u09b0\u09cd\u09b0 \u09b8\u09be\u09c7\u09a5 \u09c7\u09af\u09be\u0997\u09be\u09c7\u09af\u09be\u0997 \u09ac\u09be\u09a1\u09bc\u09be\u09c7\u09a8\u09be\u09b0 \u09c7\u099a\u09b7\u09cd\u099f\u09be \u0995\u09b0\u09c7\u09a4 \u09bf\u0997\u09c7\u09af\u09bc \u09bf\u09a4\u09bf\u09a8 \u09a6\u09c7\u09b2\u09b0 \u09ae\u09c7\u09a7\u09af\u09cd \u09bf\u09ac\u09ad\u09bf\u0995\u09cd\u09a4\u0993 \u09c8\u09a4\u09bf\u09b0 \u0995\u09c7\u09b0\u09a8\u0964 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2 \u0986\u09b0\u09ac \u09b0\u09be\u099c\u09a8\u09c0\u09bf\u09a4 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u09aa\u09f0\u09cd\u09bf \u09a8\u09ac\u09ac\u09cd\u0987 \u09b2\u09be\u0996 \u09ae\u09be\u09a8\u09c1\u09c7\u09b7\u09b0 \u09ae\u09c7\u09a7\u09af\u09cd \u0989\u09bf\u09a8\u09b6 \u09b2\u09be\u09c7\u0996\u09b0 \u09ae\u09c7\u09a4\u09be \u0986\u09b0\u09ac \u0986\u09c7\u099b\u09a8 \u09af\u09be\u09b0\u09be \u09e7\u09ef\u09ea\u09ee \u09b8\u09be\u09c7\u09b2 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u09b8\u09c0\u09ae\u09be\u09a8\u09be\u09af\u09bc \u09c7\u09a5\u09c7\u0995 \u09bf\u0997\u09c7\u09af\u09bc\u09bf\u099b\u09c7\u09b2\u09a8\u0964 \u0993\u0987 \u09aa\u09f0\u09cd\u09bf \u09b8\u09be\u09c7\u09a1\u09bc \u09b8\u09be\u09a4 \u09b2\u09be\u0996 \u09bf\u09ab\u09bf\u09b2\u09bf\u09b8\u09cd\u09a4\u09bf\u09a8 \u09a4\u09be\u09c7\u09a6\u09b0 \u09ac\u09be\u09bf\u09a1\u09bc\u0998\u09b0\u09c7\u099b\u09c7\u09a1\u09bc \u09aa\u09be\u09bf\u09b2\u09c7\u09af\u09bc\u09bf\u099b\u09c7\u09b2\u09be \u0985\u09a5\u09ac\u09be \u09a4\u09be\u09c7\u09a6\u09b0 \u09ac\u09bf\u09b9\u09b7\u09cd\u0995\u09be\u09b0 \u0995\u09b0\u09be \u09b9\u09c7\u09af\u09bc\u09bf\u099b\u09c7\u09b2\u09be\u0964 \u09af\u09bf\u09a6\u0993 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u09a8\u09be\u0997\u09bf\u09b0\u0995 \u098f\u09ae\u09a8 \u0985\u09c7\u09a8\u0995 \u0986\u09b0\u09ac \u09bf\u09a8\u09c7\u099c\u09c7\u09a6\u09b0 \u09bf\u09ab\u09bf\u09b2\u09bf\u09b8\u09cd\u09a4\u09bf\u09a8 \u09ac\u09be \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09bf\u09b2 \u09bf\u09ab\u09bf\u09b2\u09bf\u09b8\u09cd\u09a4\u09bf\u09a8 \u09bf\u09b9\u09c7\u09b8\u09c7\u09ac \u09aa\u09bf\u09b0\u09bf\u099a\u09a4 \u09b9\u09c7\u09a4 \u09aa\u099b\u09a8\u09cd\u09a6 \u0995\u09c7\u09b0\u09a8 \u0986\u09b0 \u0985\u09a8\u09af\u09cd\u09b0\u09be \u09bf\u09a8\u09c7\u099c\u09c7\u09a6\u09b0 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09bf\u09b2 \u0986\u09b0\u09ac \u09bf\u09b9\u09c7\u09b8\u09c7\u09ac \u0989\u09c7\u09b2\u09cd\u09b2\u0996 \u0995\u09c7\u09b0 \u09a5\u09be\u09c7\u0995\u0964 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u0986\u09b0\u09ac\u09c7\u09a6\u09b0 \u09c7\u09ac\u09bf\u09b6\u09b0\u09ad\u09be\u0997\u0987 \u09b8\u09c1\u09bf\u09a8\u09cd\u09a8 \u09ae\u09a4\u09be\u09a6\u09c7\u09b6\u09b0\u09cd \u09bf\u09ac\u09b6\u09ac\u09cd\u09be\u09b8\u09c0 \u0986\u09b0 \u09bf\u09a6\u09ac\u09cd\u09a4\u09c0\u09af\u09bc \u09ac\u09a1\u09bc \u0985\u0982\u09b6\u09bf\u099f \u09bf\u0996\u09f0\u09cd\u09b8\u09cd\u099f\u09be\u09a8\u0964 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u09a6\u09b6 \u09b6\u09a4\u09be\u0982\u09b6 \u09ae\u09c1\u09b8\u09bf\u09b2\u09ae \u0986\u09b0\u09ac \u09c7\u09ac\u09a6\u09c1\u0987\u09a8 \u09c7\u0997\u09be\u09a4\u09f0\u09cd \u09c7\u09a5\u09c7\u0995 \u0986\u09b8\u09be\u0964 \u09e7\u09ef\u09ea\u09ef \u09b8\u09be\u09c7\u09b2\u09b0 \u09bf\u09a8\u09ac\u09b0\u09cd\u09be\u099a\u09c7\u09a8 \u09a4\u09be\u09b0\u09be \u0985\u0982\u09b6 \u09bf\u09a8\u09c7\u09af\u09bc\u09bf\u099b\u09c7\u09b2\u09be\u0964 \u0986\u09b0\u09ac \u09b0\u09be\u099c\u09c8\u09a8\u09bf\u09a4\u0995 \u09a6\u09b2\u0997\u09c1\u09c7\u09b2\u09be \u09c7\u09a6\u09b6\u09bf\u099f\u09c7\u09a4 \u0986\u09b0\u09ac-\u09c7\u09a6\u09b0 \u09b8\u09ae\u09be\u09a8 \u0985\u09bf\u09a7\u0995\u09be\u09c7\u09b0\u09b0 \u09aa\u09c7\u0995\u09cd\u09b7 \u09c7\u09b8\u09be\u099a\u09cd\u099a\u09be\u09b0 \u09ad\u09c2 \u09bf\u09ae\u0995\u09be \u09aa\u09be\u09b2\u09a8 \u0995\u09c7\u09b0 \u098f\u09ac\u0982 \u098f\u0995\u0987 \u09b8\u09be\u09c7\u09a5 \u09bf\u09ab\u09bf\u09b2\u09bf\u09b8\u09cd\u09a4\u09c7\u09a8\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u0993 \u09a4\u09be\u09c7\u09a6\u09b0 \u09b8\u09ae\u09a5\u09b0\u09cd\u09a8 \u0986\u09c7\u099b\u0964 \u09c7\u09ac\u09a8\u099c\u09be\u09bf\u09ae\u09a8 \u09c7\u09a8\u09a4\u09be\u09bf\u09a8\u09af\u09bc\u09be\u09b9\u09c1 \u098f\u0996\u09a8 \u09b0\u09be\u09ae \u09c7\u0995\u09be\u09c7\u09a8\u09be \u09c7\u099c\u09be\u09c7\u099f \u09af\u09be\u09c7\u09ac? \u0997\u09a4 \u09ac\u099b\u09b0 \u09bf\u09ae\u09b8\u09cd\u099f\u09be\u09b0 \u09c7\u09a8\u09a4\u09be\u09bf\u09a8\u09af\u09bc\u09be\u09b9\u09c1\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u09aa\u0995\u09cd\u09b7 \u09c7\u09ac\u09bf\u09a8 \u0997\u09be\u09a8\u09cd\u099f\u099c \u0986\u09b0\u09ac \u09a6\u09b2\u0997\u09c1\u09c7\u09b2\u09be\u09b0 \u09b8\u09be\u09c7\u09a5 \u09c7\u099c\u09be\u099f \u0995\u09c7\u09b0 \u09b8\u09b0\u0995\u09be\u09b0 \u0997\u09a0\u09c7\u09a8\u09b0 \u09bf\u09ac\u09b7\u09af\u09bc\u09bf\u099f \u09aa\u09f0\u09cd\u09a4\u09af\u09cd\u09be\u0996\u09af\u09cd\u09be\u09a8 \u0995\u09c7\u09b0\u09bf\u099b\u09c7\u09b2\u09a8\u0964 \u098f\u09b0 \u0986\u09c7\u0997 \u09c7\u09a8\u09a4\u09be\u09bf\u09a8\u09af\u09bc\u09be\u09b9\u09c1 \u09bf\u09a8\u09c7\u099c\u0993 \u09b0\u09be\u09c7\u09ae\u09b0 \u09b8\u09be\u09c7\u09a5 \u09c7\u099c\u09be\u09c7\u099f\u09b0 \u09b8\u09ae\u09cd\u09ad\u09be\u09ac\u09a8\u09be \u0989\u09bf\u09a1\u09bc\u09c7\u09af\u09bc \u09bf\u09a6\u09c7\u09af\u09bc-\u09bf\u099b\u09c7\u09b2\u09a8\u0964 \u09af\u09bf\u09a6\u0993 \u09aa\u09f0\u09cd\u099a\u09be\u09b0\u09a3\u09be\u09af\u09bc \u09a4\u09be\u09b0 \u09b8\u09c1\u09b0 \u09bf\u099b\u09c7\u09b2\u09be \u09a8\u09b0\u09ae\u0964 \u098f\u0996\u09a8 \u09af\u09bf\u09a6 \u09a4\u09be\u09b0\u09be \u098f\u0995\u09bf\u099f \u09c7\u099c\u09be\u099f \u0995\u09b0\u09c7\u09a4 \u09aa\u09be\u09c7\u09b0\u09a8 \u09a4\u09be\u09b9\u09c7\u09b2 \u09bf\u09ae\u09b8\u09cd\u099f\u09be\u09b0 \u09c7\u09a8\u09a4\u09be\u09bf\u09a8\u09af\u09bc\u09be\u09b9\u09c1 \u0993 \u09bf\u09ae\u09b8\u09cd\u099f\u09be\u09b0 \u0986\u09ac\u09ac\u09cd\u09be\u09b8- \u09a6\u09c1\u099c\u09c7\u09a8\u0987 \u09b2\u09be\u09ad\u09ac\u09be\u09a8 \u09b9\u09c7\u09ac\u09a8\u0964 \u09af\u09bf\u09a6\u0993 \u09b8\u0982\u09ac\u09be\u09a6\u09a6\u09be\u09a4\u09be\u09b0\u09be \u09ac\u09b2\u09c7\u099b\u09a8, \u09c7\u09a8\u09a4\u09be\u09bf\u09a8\u09af\u09bc\u09be\u09b9\u09c1\u09b0 \u0985\u09a8\u09af\u09cd \u09b6\u09b0\u09c0\u0995\u09c7\u09a6\u09b0 \u09b8\u09be\u09c7\u09a5 \u09a4\u09be\u09b0\u09be \u09bf\u0995\u09ad\u09be\u09c7\u09ac \u098f\u0995\u09c7\u09af\u09be\u09c7\u0997 \u0995\u09be\u099c \u0995\u09b0\u09c7\u09ac\u09a8 \u09a4\u09be \u098f\u0996\u09c7\u09a8\u09be \u09aa\u09bf\u09b0\u09b7\u09cd\u0995\u09be\u09b0 \u09a8\u09af\u09bc\u0964 \u09c7\u099c\u09b0\u09c1\u099c\u09be\u09c7\u09b2\u09c7\u09ae \u09bf\u09ac\u09bf\u09ac\u09bf\u09b8 \u09b8\u0982\u09ac\u09be\u09a6\u09a6\u09be-\u09a4\u09be \u0985\u09ac\u09b6\u09af\u09cd \u09ac\u09b2\u09c7\u099b\u09a8 \u09c7\u0995\u09be\u09af\u09bc\u09be\u09bf\u09b2\u09b6\u09c7\u09a8 \u09a8\u09be \u09c7\u09a5\u09c7\u0995\u0993 \u09bf\u09ae\u09b8\u09cd\u099f\u09be\u09b0 \u09c7\u09a8\u09a4\u09be\u09bf\u09a8\u09af\u09bc\u09be\u09b9\u09c1\u09c7\u0995 \u09b8\u09ae\u09a5\u09b0\u09cd\u09a8 \u09c7\u09a6\u09af\u09bc\u09be\u09b0 \u099a\u09c1\u09bf\u0995\u09cd\u09a4 \u0995\u09b0\u09c7\u09a4 \u09aa\u09be\u09c7\u09b0 \u09b0\u09be\u09ae\u0964 \u09bf\u0995\u09a8\u09cd\u09a4\u09c1 \u099a\u09c1\u09bf\u0995\u09cd\u09a4 \u09ac\u09be \u09b8\u09ae\u09c7\u099d\u09be\u09a4\u09be \u09af\u09be\u0987 \u09c7\u09b9\u09be\u0995- \u09bf\u09ae\u09b8\u09cd\u099f\u09be\u09b0 \u0986\u09ac\u09ac\u09cd\u09be\u09b8 \u098f\u0996\u09a8 \u09bf\u09a8\u09bf\u09b6\u09cd\u099a\u09a4 \u09c7\u09af \u09a6\u09b0\u0995\u09b7\u09be\u0995\u09bf\u09b7\u09b0 \u09b8\u09c1\u09c7\u09af\u09be\u0997 \u09a4\u09be\u09b0 \u09b9\u09be\u09c7\u09a4\u0964 \u09bf\u09a4\u09bf\u09a8 \u09ac\u09c7\u09b2\u09c7\u099b\u09a8 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u0986\u09b0\u09ac \u099c\u09a8\u09c7\u0997\u09be\u09b7\u09cd\u09a0\u09c0\u09b0 \u099c\u09a8\u09af\u09cd \u098f\u09b8\u09b0\u09be \u09bf\u09b8\u09a6\u09cd\u09a7\u09be\u09a8\u09cd\u09a4\u09bf\u099f\u0987 \u09bf\u09a4\u09bf\u09a8 \u09c7\u09a8\u09c7\u09ac\u09a8\u0964 \u09af\u09bf\u09a6\u0993 \u0985\u09c7\u09a8\u09c7\u0995\u0987 \u098f\u0996\u09c7\u09a8\u09be \u098f \u09bf\u09a8\u09c7\u09af\u09bc \u09bf\u09a8\u09bf\u09b6\u09cd\u099a\u09a4 \u09a8\u09a8\u0964 \u09c7\u099c\u09b0\u09c1\u099c\u09be\u09c7\u09b2\u09ae \u0985\u09bf\u09a7\u0995\u09be\u09b0 \u0995\u09ae\u09b0\u09cd\u09c0 \u0987\u09a4\u09be\u09ab \u0986\u0993\u09af\u09bc\u09be\u09a6 \u09ac\u09b2\u09c7\u099b\u09a8, \u09bf\u09b2\u0995\u09c1 \u09a6 \u0993 \u09b0\u09be\u09ae \u09a6\u09c7\u09b2\u09b0 \u09c7\u099c\u09be\u099f \u09bf\u09ab\u09bf\u09b2\u09bf\u09b8\u09cd\u09a4\u09bf\u09a8\u09c7\u09a6\u09b0 \u099c\u09a8\u09af\u09cd \u09ad\u09be\u09c7\u09b2\u09be \u09b9\u09c7\u09ac \u09a8\u09be\u0964\\n\\nIn the aforementioned example, GPT -3.5 fails to highlight the most crucial information from the input and saturates the output with superfluous texts. GPT-3.5 Response:\\n\\n\u09ae\u09be\u09a8\u09b8\u09c1\u09b0 \u0986\u09ac\u09ac\u09cd\u09be\u09b8 \u09c7\u09a8\u09a4\u09c3\u09c7\u09a4\u09ac\u09cd\u09b0 \u09b0\u09be\u09ae \u09a6\u09b2 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u09aa\u09be\u09b2\u09b0\u09cd\u09be\u09c7\u09ae\u09c7\u09a8\u09cd\u099f \u09aa\u09be\u0981\u099a\u09bf\u099f \u0986\u09b8\u09a8 \u09bf\u099c\u09c7\u09a4\u09c7\u099b \u098f\u09ac\u0982 \u098f\u09c7\u09a6\u09b0 \u09bf\u0995\u0982\u09c7\u09ae\u0995\u09be\u09b0 \u09b9\u09c7\u09af\u09bc \u0989\u09a0\u09be\u09b0 \u09aa\u09a5 \u09c8\u09a4\u09bf\u09b0 \u0995\u09c7\u09b0\u09c7\u099b, \u09bf\u09a4\u09bf\u09a8 \u0987\u0989\u09a8\u09be\u0987\u09c7\u099f\u09a1 \u0986\u09b0\u09ac \u09bf\u09b2\u09c7\u09b8\u09cd\u099f\u09b0 \u09c7\u09a8\u09a4\u09be \u09bf\u09b9\u09c7\u09b8\u09c7\u09ac \u09aa\u09bf\u09b0\u09bf\u099a\u09a4 \u098f\u09ac\u0982 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2 \u0986\u09b0\u09ac \u09b0\u09be\u099c\u09a8\u09c0\u09bf\u09a4 \u09aa\u09f0\u09cd\u09bf \u09a8\u09ac\u09ac\u09cd\u0987 \u09b2\u09be\u0996 \u09ae\u09be\u09a8\u09c1\u09c7\u09b7\u09b0 \u09ae\u09c7\u09a7\u09af\u09cd \u0989\u09bf\u09a8\u09b6 \u09b2\u09be\u09c7\u0996\u09b0 \u09ae\u09c7\u09a4\u09be \u0986\u09b0\u09ac \u0986\u09c7\u099b\u09a8 \u09af\u09be\u09b0\u09be \u09e7\u09ef\u09ea\u09ee \u09b8\u09be\u09c7\u09b2 \u0987\u09b8\u09b0\u09be\u09c7\u09af\u09bc\u09c7\u09b2\u09b0 \u09b8\u09c0\u09ae\u09be\u09a8\u09be\u09af\u09bc \u09c7\u09a5\u09c7\u0995 \u09bf\u0997\u09c7\u09af\u09bc\u09bf\u099b\u09c7\u09b2\u09a8\u0964\\n\\nIn the Question Answering task, GPT -3.5 is given instructions to provide responses based on the given context. However, there are instances where GPT -3.5 deviates from the context and attempts to answer using information from its general knowledge, resulting in a mismatch with the gold label. While this problem occurred only in a limited number of cases, the authors were unable to independently verify the accuracy of the responses in these instances, leading to their classification as incorrect answers.\"}"}
{"id": "lrec-2024-main-201", "page_num": 11, "content": "{\"primary_language\":\"bn\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u09aa\u09c2\u09a0\u09b0\u09cd\u09ac\u09be\u09ce) \u09bf\u09af\u09bf\u09a8 \u09c7\u0997\u09d7\u09a4\u09ae \u09ac\u09c1\u09a6\u09cd\u09a7 \u09ac\u09be \u09b6\u09c1\u09a7\u09c1 \u09ac\u09c1\u09a6\u09cd\u09a7 \u09a8\u09be\u09c7\u09ae\u0987 \u09aa\u09bf\u09b0\u09bf\u099a\u09a4\u0964 \u09bf\u09a4\u09bf\u09a8 \u09aa\u09bf\u09ac\u09a4\u09f0\u09cd\\n\u0993 \u09b8\u09be\u09a7\u09a8\u09be\u09ae\u09af\u09bc \u099c\u09c0\u09ac\u09a8\u09af\u09be\u09aa\u09c7\u09a8\u09b0 \u099c\u09a8\u09af\u09cd \u09a4\u09be\u09b0 \u09b0\u09be\u099c\u09a4\u09ac\u09cd \u09a4\u09af\u09cd\u09be\u0997 \u0995\u09c7\u09b0\u09bf\u099b\u09c7\u09b2\u09a8\u0964\\n\u09e8\u09eb\u09e6 \u0996\u09c3\u09b8\u09cd\u099f\u09aa\u09c2\u09ac\u09b0\u09cd\u09be\u09c7\u0140 \u098f\u0987 \u0985\u099e\u09cd\u099a\u09b2\u09bf\u099f \u0989\u09a4\u09cd\u09a4\u09b0 \u09ad\u09be\u09b0\u09c7\u09a4\u09b0 \u09c7\u09ae\u09d7\u09af\u09b0\u09cd \u09b8\u09ae\u09f0\u09cd\u09be\u09c7\u099c\u09af\u09cd\u09b0\\n(Mauryan) \u0985\u09a7\u09c0\u09c7\u09a8 \u0986\u09c7\u09b8 \u098f\u09ac\u0982 \u09aa\u09b0\u09ac\u09a4\u09b0\u09cd\u09c0\u09c7\u09a4 \u09ea\u09a5\u09b0\u09cd \u09b6\u09a4\u09be\u0140\u09c0\u09c7\u09a4 \u098f\u09bf\u099f\\n\u0997\u09c1\u09aa\u09cd\u09a4 \u09b8\u09ae\u09f0\u09cd\u09be\u09c7\u099c\u09af\u09cd\u09b0 \u0985\u09a7\u09c0\u09c7\u09a8 \u098f\u0995\u09bf\u099f \u09aa\u09c1\u09a4\u09c1\u09b2 \u09b0\u09be\u09c7\u09b7\u09cd\u099f\u09f0\u09cd \u09aa\u09bf\u09b0\u09a3\u09a4 \u09b9\u09af\u09bc\u0964 \u09aa\u099e\u09cd\u099a\u09ae\\n\u09b6\u09a4\u09be\u0140\u09c0\u09b0 \u09c7\u09b6\u09b7 \u09b9\u09c7\u09a4 \u09b6\u09c1\u09b0\u09c1 \u0995\u09c7\u09b0 \u09aa\u09b0\u09ac\u09a4\u09b0\u09cd\u09c0\u09c7\u09a4 \u09c7\u09ac\u09b6 \u09bf\u0995\u099b\u09c1\u099f\u09be \u09b8\u09ae\u09af\u09bc \u09b6\u09be\u09b8\u09a8\\n\u0995\u09c7\u09b0 \u098f\u0995\u09a6\u09b2 \u09b6\u09be\u09b8\u0995 \u09af\u09be\u09b0\u09be \u09b8\u09be\u09a7\u09be\u09b0\u09a3\u09ad\u09be\u09c7\u09ac \u09bf\u09b2\u099a\u09cd\u099b\u09bf\u09ac (Licchavis) \u09a8\u09be-\\n\u09c7\u09ae \u09aa\u09bf\u09b0\u09bf\u099a\u09a4\u0964 \u09bf\u09b2\u099a\u09cd\u099b\u09bf\u09ad \u09b8\u09be\u09ae\u09f0\u09cd\u09be\u09c7\u099c\u09af\u09cd\u09b0 (Licchavi) \u09aa\u09a4\u09a8 \u0998\u09c7\u099f \u0985\u09b7\u09cd\u099f\u09ae\\n\u09b6\u09a4\u09be\u0140\u09c0\u09c7\u09a4 \u098f\u09ac\u0982 \u098f\u09b0\u0987 \u09b8\u09be\u09c7\u09a5 \u09b6\u09c1\u09b0\u09c1 \u09b9\u09af\u09bc \u09c7\u09a8\u0993\u09af\u09bc\u09be\u09bf\u09b0 (Newari) \u09af\u09c1-\\n\u09c7\u0997\u09b0\u0964 \u09ee\u09ed\u09ef \u09b8\u09be\u09c7\u09b2 \u09c7\u09a8\u0993\u09af\u09bc\u09be\u09bf\u09b0\u09c7\u09a6\u09b0 \u09b0\u09be\u099c\u09a4\u09ac\u09cd \u09b6\u09c1\u09b0\u09c1 \u09b9\u09c7\u09b2\u0993 \u09b8\u09ae\u0997\u09f0\u09cd \u09b0\u09be\u09c7\u09b7\u09cd\u099f\u09f0\u09cd\u09b0\\n\u0989\u09aa\u09b0 \u09a4\u09be\u09c7\u09a6\u09b0 \u09bf\u09a8\u09af\u09bc\u09a8\u09cd\u09a4\u09f0\u09cd\u09a3 \u0985\u09c7\u09a8\u0995\u099f\u09be\u0987 \u0985\u09bf\u09a8\u09bf\u09b6\u09cd\u099a\u09a4 \u09bf\u099b\u09b2\u0964 \u098f\u0995\u09be\u09a6\u09b6 \u09b6\u09a4\u09be\u0140\u09c0\u09b0\\n\u09c7\u09b6\u09b7 \u09ad\u09be\u09c7\u0997 \u09c7\u09a8\u09aa\u09be\u09c7\u09b2\u09b0 \u09a6\u09bf\u0995\u09cd\u09b7\u09a3\u09be\u0982\u09b6 \u09a6\u09bf\u0995\u09cd\u09b7\u09a3 \u09ad\u09be\u09b0\u09c7\u09a4\u09b0 \u099a\u09be\u09b2\u09c1\u0995\u09af\u09cd \u09b8\u09be\u09ae\u09f0\u09cd\u09be\u09c7\u099c\u09af\u09cd\u09b0\\n(Chalukaya) \u0985\u09a7\u09c0\u09c7\u09a8 \u0986\u09c7\u09b8\u0964 \u099a\u09be\u09b2\u09c1\u0995\u09af\u09cd\u09a1\u09b0 \u09b0\u09be\u099c\u09a4\u09ac\u09cd\u0995\u09be\u09c7\u09b2 \u09c7\u09a8\u09aa\u09be\u09c7\u09b2\u09b0\\n\u09a7\u09c7\u09ae\u09b0\u09cd \u09ac\u09af\u09cd\u09be\u09aa\u0995 \u09aa\u09bf\u09b0\u09ac\u09a4\u09b0\u09cd\u09a8 \u0986\u09c7\u09b8 \u0995\u09be\u09b0\u09a3 \u09b8\u09ac \u09b0\u09be\u099c\u09be\u0987 \u09bf\u09b9\u09a8\u09cd\u09a6\u09c1 \u09a7\u09c7\u09ae\u09b0\u09cd\u09b0 \u09aa\u09c3\u09cd\u09b7\u09cd\u09a0\u09c7\u09aa\u09be-\\n\u09b7\u0995\u09a4\u09be \u0995\u09b0\u09c7\u09a4\u09a8 \u098f\u09ac\u0982 \u09c7\u09ac\u09d7\u09a6\u09cd\u09a7 \u09a7\u09c7\u09ae\u09b0\u09cd\u09b0 \u09aa\u09f0\u09cd\u09b8\u09be\u09c7\u09b0\u09b0 \u09bf\u09ac\u09aa\u09b0\u09c0\u09c7\u09a4 \u09bf\u09b9\u09a8\u09cd\u09a6\u09c1 \u09a7\u09c7\u09ae\u09b0\u09cd\u09b0\\n\u09aa\u09f0\u09cd\u099a\u09be\u09c7\u09b0 \u0985\u09ac\u09a6\u09be\u09a8 \u09b0\u09be\u09c7\u0996\u09a8\u0964\\n\\nQuestion:\\n\u09c7\u09a8\u09aa\u09be\u09b2 \u09bf\u0995 \u098f\u0995\u09bf\u099f \u09b8\u09be\u09ac\u09b0\u09cd\u09c7\u09ad\u09d7\u09ae \u09c7\u09a6\u09b6?\\n\\nExpected Response:\\nNo Answer.\\n\\nGPT-3.5 Response:\\n\u09b9\u09af\u09cd\u09be\u0981, \u09c7\u09a8\u09aa\u09be\u09b2 \u098f\u0995\u09bf\u099f \u09b8\u09be\u09ac\u09b0\u09cd\u09c7\u09ad\u09d7\u09ae \u09c7\u09a6\u09b6\u0964\\n\\nA.4.2.\\nParaphrasing\\nIn some instances, GPT -3.5 generates paraphrase that completely changes the meaning of the input sentences. For example:\\n\\nInput:\\n\u0986\u0997\u09c1\u09c7\u09a8 \u09c7\u0995\u09be\u09a8\u0993 \u09b9\u09a4\u09be\u09b9\u09c7\u09a4\u09b0 \u0996\u09ac\u09b0 \u09aa\u09be\u0993\u09af\u09bc\u09be \u09af\u09be\u09af\u09bc\u09bf\u09a8\u0964\\n\\n[No injury was reported because of the fire]\\n\\nExpected Responses:\\n[\u2018\u0985\u09bf\u0997\u09cd\u09a8\u0995\u09be\u09c7\u09a3\u09cd\u09a1 \u0989\u09c7\u09b2\u09cd\u09b2\u0996\u09c7\u09af\u09be\u0997\u09af\u09cd \u09c7\u0995\u09be\u09a8 \u0995\u09cd\u09b7\u09af\u09bc\u0995\u09cd\u09b7\u09bf\u09a4 \u09b9\u09af\u09bc\u09bf\u09a8\u0964\u2019, \u2018\u0985\u09bf\u0997\u09cd\u09a8\u0995\u09be\u09c7\u09a3\u09cd\u09a1 \u0989\u09c7\u09b2\u09cd\u09b2\u0996\u09c7\u09af\u09be\u0997\u09af\u09cd \u09c7\u0995\u09be\u09a8 \u0995\u09cd\u09b7\u09af\u09bc\u0995\u09cd\u09b7\u09bf\u09a4 \u09b9\u09af\u09bc\u09bf\u09a8\u0964\u2019, \u2018\u098f\u0987 \u0985\u09bf\u0997\u09cd\u09a8\u0995\u09be\u09c7\u09a3\u09cd\u09a1 \u09b9\u09a4\u09be\u09b9\u09c7\u09a4\u09b0 \u09c7\u0995\u09be\u09a8\u0993 \u0996\u09ac\u09b0 \u09c7\u09ae\u09c7\u09b2\u09bf\u09a8\u0964\u2019, \u2018\u098f\u0987 \u0985\u09bf\u0997\u09cd\u09a8\u0995\u09be\u09c7\u09a3\u09cd\u09a1 \u09b9\u09a4\u09be\u09b9\u09c7\u09a4\u09b0 \u09c7\u0995\u09be\u09c7\u09a8\u09be \u0996\u09ac\u09b0 \u09aa\u09be\u0993\u09af\u09bc\u09be \u09af\u09be\u09af\u09bc\u09bf\u09a8\u0964\u2019, \u2018\u0985\u09bf\u0997\u09cd\u09a8\u0995\u09be\u09c7\u09a3\u09cd\u09a1 \u09b9\u09a4\u09be\u09b9\u09c7\u09a4\u09b0 \u09c7\u0995\u09be\u09c7\u09a8\u09be \u0996\u09ac\u09b0 \u09aa\u09be\u0993\u09af\u09bc\u09be \u09af\u09be\u09af\u09bc\u09bf\u09a8\u0964\u2019, \u2018\u0985\u09bf\u0997\u09cd\u09a8\u0995\u09be\u09c7\u09a3\u09cd\u09a1 \u0995\u09be\u09b0\u0993 \u09b9\u09a4\u09be\u09b9\u09c7\u09a4\u09b0 \u0996\u09ac\u09b0 \u09aa\u09be\u0993\u09af\u09bc\u09be \u09af\u09be\u09af\u09bc\u09bf\u09a8\u0964\u2019]\\n\\nGPT-3.5 Response:\\n\u09c7\u0995\u09be\u09a8 \u0986\u09aa\u09bf\u09a4\u09cd\u09a4\u0995\u09b0 \u0998\u099f\u09a8\u09be \u09a8\u09be \u09b9\u0993\u09af\u09bc\u09be\u09af\u09bc \u0986\u0997\u09c1\u09c7\u09a8\u09b0 \u09b8\u09ae\u09cd\u09aa\u09c7\u0995\u09b0\u09cd \u09c7\u0995\u09be\u09a8 \u0996\u09ac\u09b0 \u09aa\u09be\u0993\u09af\u09bc\u09be \u09af\u09be\u09af\u09bc\u09bf\u09a8\u0964\\n\\n[There was no news about the fire as there was no untoward in-\\ncident.\\] This is a matter of concern and presents additional evidence that GPT -3.5 is not currently suitable for serving as a universal problem solver in the Bangla language.\\n\\nA.5.\\nClassification T asks\\nIn this section, we discuss the error cases in the other three tasks that we consider: Natural Language Inference (NLI), T ext Classification, and Sentiment Analysis.\\n\\nA.5.1.\\nNatural Language Inference\\nThe confusion matrix obtained by evaluating GPT -3.5 for the NLI task is demonstrated in Figure 1. The matrix reveals that GPT -3.5 demonstrates high accuracy in predicting the Contradiction and Entailment labels, but encounters significant chal-\\nlenges in accurately predicting the Neutral labels. Approximately 49% of the misclassifications arise when attempting to predict the Neutral class. A thorough manual examination uncovers that GPT -3.5 often exhibits bias towards expressing a par-\\nticular opinion polarity (Contradiction, Entailment) when dealing with logical relationships in Bangla, and it fails to appropriately recognize and convey neutrality even in cases where it is evident.\\n\\nPrompt:\\nPlease determine the logical relation-\\nship between the given hypothesis and premise.\\nThe input will consist of two sentences written in the Bangla language. The first sentence rep-\\nresents the premise, while the second sentence rep-\\nresents the hypothesis. Y our task is to deter-\\nmine whether the hypothesis is false (contradic-\\ntion), true (entailment), or inconclusive (neutral) given the premise. Please output a number indi-\\ncating the logical relationship between them: 0 for false (contradiction), true (entailment), and 2 for inconclusive (neutral) for neutrality. Note: Please avoid providing any additional information beyond the logical relationship.\\n\\nPremise:\\n\u09bf\u09a0\u0995 \u0986\u09c7\u099b, \u09c7\u09af\u0996\u09be\u09c7\u09a8 \u09a6\u09c1\u0987-\u09bf\u09a4\u09a8\u099f\u09be \u09bf\u09ac\u09ae\u09be\u09a8 \u098f\u0995 \u09b8\u09aa\u09cd\u09a4\u09be\u09c7\u09b9 \u098f\u09c7\u09b8 \u09c7\u09aa\u09d7\u09c7\u099b\u09c7\u099b \u098f\u09ac\u0982 \u0986\u09bf\u09ae \u099c\u09be\u09a8\u09a4\u09be\u09ae \u09a8\u09be \u09a4\u09be\u09b0\u09be \u09c7\u0995\u09be\u09a5\u09be\u09af\u09bc \u09af\u09be\u09c7\u099a\u09cd\u099b\u0964\\n\\nHypothesis:\\n\u09aa\u09f0\u09cd\u09bf\u09a4 \u09b8\u09aa\u09cd\u09a4\u09be\u09c7\u09b9 \u098f\u0995\u09bf\u099f\u09b0\u0993 \u09c7\u09ac\u09bf\u09b6 \u09bf\u09ac\u09ae\u09be\u09a8 \u0986\u09c7\u09b8\u0964\\n\\nExpected Response:\\n1 (Entailment)\\n\\nGPT-3.5 Response:\\n2 (Neutral)\\n\\nA.5.2.\\nTransliteration\\nA deeper look into the cases where LLMs fail yields the following observations:\\n\\n(i) Intricate pronunciation rules in Bengali pose challenges in transliteration. Bengali, be-\\ning a language rich in nuances, possesses dis-\\ntinct rules for pronunciation and spelling. In Ben-\\ngali, the same letter may produce different sounds\\nwhen used in different words. All LLMs encounter\\nsignificant challenges in scenarios where specific\\nrules govern the pronunciation of a particular let-\\nter. For instance, the Latin lexicon 'andre'\\nis pro-\\nnounced as '\u09be\u09c7\u09a6\u09f0\u09cd' in Bengali (similar to\\n'addre' in\\nEnglish). The\\n'n' letter of the lexicon 'andre'\\nis\\nsilent in Bengali and instead represented as '\u25cc\u0981'\\n, which creates a nasal sound, whereas even the re-\\nsponse of GPT -3.5 (which is the best-performing\\nLLM in this task) is '\u09be\u09c7\u09a8\u09cd\u09a6\u09f0\u09cd', which includes an 'n'\\nsound. Such unconventional cases represent the\\nprimary reasons for LLMs to struggle while translit-\\nerating lexicons from Latin to Bengali.\\n\\n(ii) Difficulty in transliterating lexicons that closely resemble English words. For instance,\\nwhen encountering the term 'cador', which bears\\na strong resemblance to the English word 'cadre',\\nGPT -3.5 mistakenly transliterates it as '\u0995\u09af\u09cd\u09be\u09a1\u09b0', a\"}"}
{"id": "lrec-2024-main-201", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"transliteration that shares a similar pronunciation to 'cadre'. However, the correct gold label for this particular example is '\u099a\u09be\u09a6\u09b0'. Such instances of erroneous substitution lead to an increase in CER.\\n\\n(iii) Transliteration of outdated pronunciations.\\nLanguages continuously undergo changes based on their usage by individuals. In the Dakshina dataset, certain gold labels are annotated with outdated pronunciations. While LLMs correctly transliterate various lexicons into more contemporary pronunciations, many such instances in the Dakshina dataset are considered incorrect transliterations due to outdated gold labels. For instance, the term 'ssoshur' is transliterated as '\u09b6\u09ac\u09cd\u09b6\u09c1\u09b0' by GPT-3.5, which represents an accurate contemporary transliteration. Nevertheless, the gold label for this example is given as '\u09b6\u09b6\u09c1\u09b0'. Such cases result in 90% of the substitution errors in GPT-3.5.\\n\\nFull Sentence Transliteration:\\nAs seen from Table 3, the zero-shot GPT-3.5 on full sentence transliteration surpasses all the SOTA models except one (i.e., Noisy Channel). Meanwhile, Claude-2 achieves comparable performance with the prior models. The error patterns encountered in full-sentence transliteration largely align with those observed in single-word transliteration, with the addition of the following error case:\\n\\n(i) Code-mixing creates a stumbling block for GPT-3.5. While transitioning between vocabulary words, LLMs demonstrate a tendency to make mistakes. For instance, the following sentence 'Amake to tomra mute kore rekhechile' contains an occurrence of code-mixing with the English word 'mute'. GPT-3.5 misidentifies it as a Bengali word and transliterates the sentence as '\u0986\u09ae\u09be\u09c7\u0995 \u09c7\u09a4\u09be \u09c7\u09a4\u09be\u09ae\u09b0\u09be \u09ae\u09c1\u09c7\u099f \u0995\u09c7\u09b0 \u09c7\u09b0\u09c7\u0996\u09bf\u099b\u09c7\u09b2', whereas the accurate transliteration of the word 'mute' should be '\u09bf\u09ae\u0989\u099f' instead of '\u09ae\u09c1\u09c7\u099f'. Such cases contribute to substitution errors in full-sentence transliteration.\\n\\nA.5.3. News Article Classification\\nOur experimental results show that GPT-3.5 misclassified the category 'kolkata' the most, failing to generate the correct response in 503 of the test set's 569 examples (88.4%). The subsequent category with the highest frequency of misclassification is 'national'. GPT-3.5 was unable to accurately classify this particular category in 95 out of 175 instances, representing a misclassification rate of 54.20%. On the other hand, GPT-3.5 effectively identified the category labeled as 'entertainment' in 110 out of 130 occurrences, resulting in a success rate of 92.30%.\\n\\nA.5.4. Sentiment Analysis\\nThe examples listed in Table 5 illustrate the error cases where GPT-3.5 misclassifies the sentiment of the given input. The first seven examples are from the IndicSentiment test set and the rest of the examples are from the SentNoB dataset. Notably, the positive class exhibits the most frequent misclassification in both test sets, suggesting that GPT-3.5 still has a considerable way to go toward attaining a complete understanding of the Bangla language. In the instances where the class is neutral, the examples are mostly simple statements with no sentiment word associated with them (table entries 11, and 12), yet GPT-3.5 classified them as either negative or positive. Furthermore, GPT-3.5 demonstrates difficulties in capturing challenging Bangla sentiment words, i.e., '\u0985\u09ae\u09be\u09bf\u09af\u09bc\u0995' (Accommodating in English), '\u09b8\u09c1\u09bf\u09ac\u09a7\u09be' (Advantage in English), '\u0986\u09aa\u09b8\u09b9\u09c0\u09a8' (Uncompromising in English) etc (example 2, 8, 13 in Table 5).\\n\\nA.6. Unexpected, out of range response\\nIn the tasks of text classification and sentiment analysis, the prompts are designed to include the target classes in order to identify any unexpected or non-class responses. Based on our experimental findings, we observe that the outputs generated by GPT-3.5 exhibit outcomes that deviate from the expected range of outputs.\\n\\nNews Article Classification.\\nPrompt: For the Bengali news article given in the input, identify the appropriate section title for the article from the following classes: kolkata, state, sports, national, entertainment, international. Note: Do not output any unnecessary words other than just the section title. The response should be in English language and should be one word. Input:\\n\\n\u09e7\u09e6\u09e6 \u09bf\u09a6\u09c7\u09a8\u09b0 \u0995\u09be\u09c7\u099c\u09b0 \u09aa\u09f0\u09cd\u0995\u09c7\u09b2\u09cd\u09aa\u09b0 \u09b8\u09be\u09ab\u09b2\u09af\u09cd\u09c7\u09a6\u0996\u09c7\u09a4 \u09b0\u09be\u09c7\u099c\u09af\u09cd \u0986\u09b8\u09c7\u099b\u09a8 \u09c7\u0995\u09c7\u09a8\u09cd\u09a6\u09f0\u09cd\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u09a8\u09bf\u09a7\u0964 \u09b8\u09c7\u0999\u09cd\u0997 \u09e7\u09e6\u09bf\u099f \u09b0\u09be\u09c7\u099c\u09af\u09cd\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u09a8\u09bf\u09a7\u0993\u0964 \u09ef \u09c7\u09a5\u09c7\u0995 \u09e7\u09e8 \u099c\u09be\u09a8\u09c1\u09af\u09bc\u09be\u09bf\u09b0\u09b0 \u09ae\u09c7\u09a7\u09af\u09cd \u09aa\u09be\u0981\u099a\u09bf\u099f \u09c7\u099c\u09b2\u09be\u09af\u09bc \u09b0\u09be\u09c7\u099c\u09af\u09cd\u09b0 '\u0989\u09a6\u09cd\u09ad\u09be\u09ac\u09a8\u09c0 \u0995\u09be\u09c7\u099c'\u09b0 \u09b8\u09be\u09ab\u09b2\u09af\u09cd \u09c7\u09a6\u0996\u09c7\u09ac\u09a8 \u09a4\u09be\u0981\u09b0\u09be\u0964 \u0998\u099f\u09a8\u09be\u099a\u09c7\u0995\u09f0\u09cd, \u09aa\u099e\u09cd\u099a\u09be\u09c7\u09af\u09bc\u09a4\u09bf\u09a8\u09ac\u09b0\u09cd\u09be\u099a\u09c7\u09a8\u09b0 \u0986\u09c7\u0997 \u098f\u0987 \u09b8\u09ab\u09b0 \u09b9\u09c7\u09a4 \u099a\u09c7\u09b2\u09c7\u099b\u0964 \u09ab\u09c7\u09b2 \u09aa\u09f0\u09cd\u09b6\u09be\u09b8\u09c7\u09a8\u09b0 \u098f\u0995\u09be\u0982-\u09c7\u09b6\u09b0 \u09ae\u09c7\u09a4, \u098f\u0987 \u09b8\u09ab\u09b0 \u09c7\u09ad\u09be\u09c7\u099f\u09b0 \u0986\u09c7\u0997 \u09b6\u09be\u09b8\u0995\u09a6\u09c7\u09b2\u09b0 \u0995\u09be\u09c7\u099b \u09ac\u09a1\u09bc \u09aa\u09f0\u09cd\u09be\u09bf\u09aa\u09cd\u09a4 \u09b9\u09c7\u09a4 \u09aa\u09be\u09c7\u09b0\u0964 \u09bf\u09ac\u09c7\u09b6\u09b7\u09a4, \u09b0\u09be\u099c\u09af\u09cd\u0997\u09c1\u09bf\u09b2\u09b0 \u09ae\u09c7\u09a7\u09af\u09cd \u0985\u09bf\u09a7\u0995\u09be\u0982\u09b6\u0987 \u09af\u0996\u09a8 \u09bf\u09ac\u09c7\u099c-\u09bf\u09aa \u0985\u09a5\u09ac\u09be \u09bf\u09ac\u09c7\u099c\u09bf\u09aa'\u09b0 \u09b8\u09b9\u09c7\u09af\u09be\u0997\u09c0 \u09a6\u09c7\u09b2\u09b0 \u09b8\u09b0\u0995\u09be\u09b0 \u09aa\u09bf\u09b0\u099a\u09be\u09bf\u09b2\u09a4\u0964 '\u0987\u09a8\u09cd\u099f\u09be\u09b0\u09c7\u09b8\u09cd\u099f\u09c7\u0995\u09cd\u09b8\u09c7\u099a\u099e\u09cd\u099c \u09c7\u09aa\u09f0\u09cd\u09be\u0997\u09f0\u09cd\u09be\u09c7\u09ae'\u09b0 \u0986\u0993\u09a4\u09be\u09af\u09bc \u09aa\u09f0\u09cd\u09bf\u09a4 \u09ac\u099b\u09b0 \u098f\u0995\u09bf\u099f \u09ac\u09be \u09a6\u09c1'\u09bf\u099f \u09b0\u09be\u099c\u09af\u09cd\u09c7\u0995 \u09c7\u09ac\u09c7\u099b \u09c7\u09a8\u0993\u09af\u09bc\u09be \u09b9\u09af\u09bc\u0964 \u099a\u09b2\u09bf\u09a4 \u09ac\u099b\u09c7\u09b0 \u098f \u09b0\u09be\u099c\u09af\u09cd\u09c7\u0995 \u09c7\u09ac\u09c7\u099b \u09bf\u09a8\u09c7\u09af\u09bc\u09c7\u099b \u09c7\u0995\u09a8\u09cd\u09a6\u09f0\u09cd\u0964 \u09c7\u0995\u09a8\u09cd\u09a6\u09f0\u09cd\u09c0\u09af\u09bc \u09b8\u09b0\u0995\u09be\u09c7\u09b0\u09b0 \u09af\u09c1\u0997\u09cd\u09ae\u09b8\u09bf\u099a\u09ac (\u098f\u09a8\ufffd\u09b0\u0987\u09bf\u099c\u098f) \u0985\u09aa\u09b0\u09be\u09bf\u099c\u09a4\u09be \u09b8\u09be\u09c7\u09b0\u09bf\u0999\u09cd\u0997\u09b0 \u09c7\u09a8\u09a4\u09c3\u09c7\u09a4\u09ac\u09cd \u09e7\u09e7 \u099c\u09a8 \u09aa\u09a6\u09b8\u09cd\u09a5 \u0986\u09bf\u09a7\u0995\u09be\u09bf\u09b0\u0995 \u0986\u09b8\u09c7\u09ac\u09a8\u0964 \u09a5\u09be\u0995\u09c7\u09ac\u09a8 \u09e7\u09e6\u09bf\u099f \u09b0\u09be\u099c\u09af\u09cd \u09c7\u09a5\u09c7\u0995 \u09aa\u09be\u0981\u099a\u099c\u09a8 \u0995\u09c7\u09b0 \u09b8\u09b0\u0995\u09be\u09bf\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u09a8\u09bf\u09a7\u0964 \u09b0\u09be\u099c\u09af\u09cd\u0997\u09c1\u09bf\u09b2\u09b0 \u09ae\u09c7\u09a7\u09af\u09cd \u09b0\u09c7\u09af\u09bc\u09c7\u099b \u09a4\u09be\u09bf\u09ae\u09b2\u09a8\u09be\u09a1\u09bc\u09c1 , \u0985\u09a8\u09cd\u09a7\u09f0\u09cd, \u09c7\u09a4\u09b2\u0999\u09cd\u0997\u09a8\u09be, \u0995\u09a8\u09b0\u09cd\u09be\u099f\u0995, \u0993\u09bc\u09bf\u09a1\u09bc\u09b6\u09be, \u09bf\u09b9\u09ae\u09be\u099a\u09b2\u09aa\u09f0\u09cd\u09c7\u09a6\u09b6, \u099b\u09a4\u09cd\u09a4\u09c0\u09b8\u0997\u09a2\u09bc \u098f\u09ac\u0982 \u09ae\u09a7\u09af\u09cd\u09aa\u09f0\u09cd\u09c7\u09a6\u09b6\u0964 \u099c\u09ae\u09cd\u09ae\u09c1 \u0993 \u0995\u09be\u09b6\u09cd\u09ae\u09c0\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf-\u09bf\u09a8\u09bf\u09a7 \u09aa\u09be\u09a0\u09be\u09c7\u09ac \u09bf\u0995 \u09a8\u09be, \u09a4\u09be\u09b0 \u09bf\u09a8\u09b6\u09cd\u099a\u09af\u09bc\u09a4\u09be \u09c7\u09ae\u09c7\u09b2\u09bf\u09a8\u0964 \u09aa\u09be\u0981\u099a\u09bf\u099f \u09a6\u09c7\u09b2 \u09ad\u09be\u0997 \u0995\u09c7\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u09a8\u09bf\u09a7\u09c7\u09a6\u09b0 \u09bf\u09a8\u09c7\u09af\u09bc \u09af\u09be\u0993\u09af\u09bc\u09be \u09b9\u09c7\u09ac \u09aa\u09bf\u09b6\u09cd\u099a\u09ae \u09ac\u09a7\u09b0\u09cd\u09ae\u09be\u09a8, \u09ac\u09be\u0981\u0995\u09c1 \u09a1\u09bc\u09be, \u09aa\u09c1\u09b0\u09c1\u09bf\u09b2\u09af\u09bc\u09be, \u099d\u09be\u09a1\u09bc\u0997\u09f0\u09cd\u09be\u09ae \u098f\u09ac\u0982 \u09ac\u09c0\u09b0\u09ad\u09c2 \u09c7\u09ae\u0964 \u09e7\u09e6\u09e6 \u09bf\u09a6\u09c7\u09a8\u09b0 \u0995\u09be\u09c7\u099c\u09b0 \u09aa\u09f0\u09cd\u0995\u09c7\u09b2\u09cd\u09aa\u09b0 \u0986\u0993\u09a4\u09be\u09af\u09bc \u09b6\u09c1\u09bf\u0995\u09c7\u09af\u09bc \u09af\u09be\u0993\u09af\u09bc\u09be \u099d\u09b0\u09a8\u09be\u09af\u09bc \u09bf\u09ac\u0995\u09b2\u09cd\u09aa \u0989\u09aa\u09be\u09c7\u09af\u09bc \u099c\u09c7\u09b2\u09b0 \u09c7\u099c\u09be\u0997\u09be\u09a8 \u09bf\u09ab\u09bf\u09b0\u09c7\u09af\u09bc \u0986\u09a8\u09be, \u09ab\u09cd\u09b2\u09be\u0987 \u0985\u09af\u09cd\u09be\u09c7\u09b6\u09b0 \u0987\u099f \u09c8\u09a4\u09bf\u09b0, \u09aa\u09c1\u09b0\u09c1\u09bf\u09b2\u09af\u09bc\u09be\u09b0 \u09ae\u09be\u09bf\u099f\u09c7\u09a4 \u09c7\u09ac\u09a6\u09be\u09a8\u09be\u09b0 \u09ab\u09b2\u09a8, \u09ae\u09be-\u09bf\u099f\u09b0 \u099a\u09bf\u09b0\u09a4\u09f0\u09cd \u09bf\u09ad\u09a8\u09cd\u09a8 \u09b9\u09c7\u09b2\u0993 \u0986\u09b0\u09c7\u092c\u09b0 \u09c7\u0996\u099c\u09c1\u09b0 \u099a\u09be\u09b7, \u09b2\u09be\u09bf\u09a4\u09a8 \u0986\u09c7\u09ae\u09bf\u09b0\u0995\u09be \u098f\u09ac\u0982 \u09a6\u09bf\u0995\u09cd\u09b7\u09a3 \u09aa\u09c2\u09ac\u09b0\u09cd \u098f\u09bf\u09b6\u09af\u09bc\u09be\u09b0 \u099c\u09a8\u09bf\u09aa\u09f0\u09cd\u09af\u09bc \u09a1\u09f0\u09cd\u09be\u0997\u09a8 \u09ab\u09f0\u09cd\u09c1 \u09c7\u099f\u09b0 \u099a\u09be\u09b7, \u09ac\u09c3\u0995\u09cd\u09b7\u09aa\u09be\u099f\u09cd\u099f\u09be \u09bf\u09a6\u09c7\u09af\u09bc \u0997\u09be\u099b \u09ac\u09be\u0981\u099a\u09be\u09c7\u09a8\u09be-\u09b8\u09b9 \u09bf\u09ac\u09bf\u09ad\u09a8\u09cd\u09a8 \u0995\u09be\u099c \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u09a8\u09bf\u09a7\u09a6\u09c7\u09b2\u09b0 \u09b8\u09be\u09ae\u09c7\u09a8 \u09a4\u09c1\u09c7\u09b2\"}"}
{"id": "lrec-2024-main-201", "page_num": 13, "content": "{\"primary_language\":\"bn\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u09a7\u09b0\u09be \u09b9\u09c7\u09ac\u0964 \u098f\u0987 \u0985\u09bf\u09ad\u099c\u09cd\u099e\u09a4\u09be \u09ad\u09bf\u09ac\u09b7\u09af\u09cd\u09c7\u09a4 \u0995\u09c0\u09ad\u09be\u09c7\u09ac \u099b\u09bf\u09a1\u09bc\u09c7\u09af\u09bc \u09c7\u09a6\u0993\u09af\u09bc\u09be \u09af\u09be\u09af\u09bc, \u09a4\u09be \u09c7\u09a6\u0996\u09c7\u09ac\u09a8 \u09a4\u09be\u0981\u09b0\u09be\u0964 \u09ef \u099c\u09be\u09a8\u09c1\u09af\u09bc\u09be\u09bf\u09b0 \u09a6\u09c1\u0997\u09b0\u09cd\u09be\u09aa\u09c1\u09c7\u09b0 \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u09a8\u09bf\u09a7\u09a6\u09c7\u09b2\u09b0 \u09b8\u09c7\u0999\u09cd\u0997 \u09c8\u09ac-\u09a0\u0995 \u0995\u09b0\u09c7\u09ac\u09a8 \u09b0\u09be\u09c7\u099c\u09af\u09cd\u09b0 \u09aa\u099e\u09cd\u099a\u09be\u09c7\u09af\u09bc\u09a4\u09ae\u09a8\u09cd\u09a4\u09f0\u09cd\u09c0 \u09b8\u09c1\u09ac\u09f0\u09cd\u09a4 \u09ae\u09c1\u09c7\u0996\u09be\u09aa\u09be\u09a7\u09af\u09cd\u09be\u09af\u09bc, \u09ae\u09c1\u0996\u09af\u09cd\u09b8\u09bf\u099a\u09ac \u09ae\u09b2\u09af\u09bc \u09c7\u09a6-\u09b8\u09b9 \u09a6\u09ab\u09a4\u09c7\u09b0\u09b0 \u0986\u09bf\u09a7\u0995\u09be\u09bf\u09b0\u09c7\u0995\u09b0\u09be\u0964 \u09b8\u09ab\u09c7\u09b0\u09b0 \u09c7\u09b6\u09c7\u09b7\u0993 \u09c8\u09ac\u09a0\u0995 \u09b9\u09c7\u09ac\u0964 \u09b0\u09be\u09c7\u099c\u09af\u09cd\u09b0 \u098f\u09a8\u0986\u09b0\u0987\u09bf\u099c\u098f \u0995\u09bf\u09ae\u09b6\u09a8\u09be\u09b0 \u09bf\u09a6\u09c7\u09ac\u09af\u09cd\u09a8\u09cd\u09a6\u09c1 \u09b8\u09b0\u0995\u09be\u09c7\u09b0\u09b0 \u0995\u09a5\u09be\u09af\u09bc, '\u0986\u09ae\u09be\u09c7\u09a6\u09b0 \u09c7\u09b8\u09cd\u09b2\u09be\u0997\u09be\u09a8 \u0986\u099c\u09c7\u0995\u09b0 \u09ae\u099c\u09c1\u09bf\u09b0, \u0986\u0997\u09be\u09ae\u09c0\u09b0 \u099c\u09c0\u09bf\u09ac\u0995\u09be\u0964 \u09b6\u09c1\u09a7\u09c1 \u09ae\u09be\u09bf\u099f \u0995\u09be\u099f\u09be \u09a8\u09af\u09bc, \u09b8\u09bf\u09a6\u099a\u09cd\u099b\u09be \u09a5\u09be\u0995\u09c7\u09b2 \u098f\u0987 \u09aa\u09f0\u09cd\u0995\u09c7\u09b2\u09cd\u09aa\u09b0 \u0986\u0993\u09a4\u09be\u09af\u09bc \u09c7\u09af \u09bf\u09ad\u09a8\u09cd\u09a8 \u09a7\u09b0\u09c7\u09a8\u09b0 \u0995\u09be\u099c \u0995\u09b0\u09be \u09b8\u09ae\u09cd\u09ad\u09ac, \u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u09a8\u09bf\u09a7\u09c7\u09a6\u09b0 \u09a4\u09be \u09c7\u09a6\u0996\u09be\u09c7\u09a8\u09be \u09b9\u09c7\u09ac\u0964'\\n\\nExpected Response:\\n\\nState\\n\\nGPT-3.5 Response:\\n\\nDevelopment\\n\\nOur study reveals that GPT -3.5, in particular, generated the word Development, which deviates from the prompted range of responses. The generated output is considered out of range as the prompt specifically instructs the model to produce text within the given categories of kolkata, state, sports, national, entertainment, and international. In addition, the results of our experiment indicate that GPT -3.5 produced 12 classes that are outside the target class range, thus demonstrating the model's inability to comply with the specified instructions for this specific task.\\n\\nSentiment Analysis.\\n\\nPrompt:\\nFor the given Input, is the sentiment in the input positive or negative? Note: Please do not output anything other than the sentiment. Exclude any word like, Sentiment in the response.\\n\\nInput :\\n\u09b8\u09cd\u099f\u09af\u09cd\u09be\u09a8\u09cd\u09a1-\u098f \u09b0\u09c7\u09af\u09bc\u09c7\u099b \u09c7\u09b2\u09c7\u09ad\u09b2 \u0987\u09bf\u09a8\u09cd\u09a1\u09c7\u0995\u099f\u09b0 \u09af\u09be \u09bf\u09ac\u09b2\u09cd\u099f-\u0987\u09a8 \u09ac\u09be\u09ac\u09b2 \u09bf\u09ad\u0989 \u09c7\u09b2\u09c7\u09ad\u09b2 \u098f\u09ac\u0982 \u09bf\u09a5\u09cd\u09f0-\u0993\u09c7\u09af\u09bc \u09c7\u09b9\u09a1 \u09af\u09be \u09e9\u09ec\u09e6 \u09bf\u09a1\u09bf\u0997\u09f0\u09cd \u09c7\u0998\u09be\u09b0\u09be\u09c7\u09a8\u09be \u09af\u09be\u09af\u09bc\u0964\\n\\nExpected Response:\\n\\nPositive\\n\\nGPT-3.5 Response:\\n\\nNeutral\\n\\nIn this instance, although we explicitly prompted GPT -3.5 to generate output that is either positive or negative, it produced a response that falls outside of the designated range, i.e., Neutral. This observation implies that the model may possess certain inherent biases towards sentiment classification among the three widely used categories, namely positive, negative, and neutral.\"}"}
{"id": "lrec-2024-main-201", "page_num": 14, "content": "{\"primary_language\":\"bn\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u09a2\u09be\u0995\u09be\u09b0 \u0987\u09b8\u09cd\u099f \u0993\u09c7\u09af\u09bc\u09c7\u09b8\u09cd\u099f \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc \u099a\u09a4\u09ac\u09cd\u09ac\u09c7\u09b0 \u0997\u09af\u09bc\u09be\u09b8 \u0995\u09be\u0981\u09a6\u09be\u09c7\u09a8 \u09b0 \u09aa\u09b0 \u0985\u09a8\u09bf\u09a6\u09b0\u09cd\u09b7\u09cd\u099f\u0995\u09be\u09b2 \u09b9\u09b2\u09c7 \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc \u0995\u09be\u09b0\u09cd\u09af\u0995\u09cd\u09b0\u09ae\u09c7\u09b0 \u099c\u09a8\u09cd\u09af \u09ac\u09a8\u09cd\u09a7 \u0995\u09b0\u09be \u09b9\u09af\u09bc\u09c7\u099b\u09c7\u0964 \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc\u09c7\u09b0 \u098f\u0995\u099c\u09a8 \u0995\u09ae\u09cd\u09af\u09c1\u09a8\u09bf\u099f\u09bf \u09a8\u09bf\u09af\u09bc\u09a8\u09cd\u09a4\u09cd\u09b0\u09a3 \u0995\u09b0\u09c7\u099b\u09c7\u09a8, \u099b\u09be\u09a4\u09cd\u09b0-\u099b\u09be\u09a4\u09cd\u09b0\u09c0\u09a6\u09c7\u09b0 \u098f\u09b8\u098f\u09ae\u098f\u09b8 \u099c\u09bf\u09a8\u09cd\u09a6\u0997\u09c0 \u099c\u09be\u09a8\u09be\u09c7\u09a8\u09be \u09b9\u09c7\u099a\u09cd\u099b\u0964 \u0993\u09ac\u09bf\u09a6\u09c7\u0995 \u09ac\u09be\u09a1\u09bc\u09bf \u098f\u09b2\u09be\u0995\u09be\u09af\u09bc \u0987\u09b8\u09cd\u099f \u0993\u09c7\u09af\u09bc\u09c7\u09b8\u09cd\u099f \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc\u0993 \u09a6\u09c1\u0987\u09a6\u09c7\u09b0 \u099c\u09a8\u09cd\u09af \u09ac\u09a8\u09cd\u09a7 \u0995\u09b0\u09be \u09b9\u09c7\u09af\u09bc\u09c7\u099b\u09c7\u0964 \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc\u09c7\u09b0 \u0993\u09af\u09bc\u09c7\u09ac\u09b8\u09be\u0987\u099f \u098f\u0995 \u09c7\u0998\u09be\u09b7\u09a3\u09be\u09af\u09bc \u09ac\u09b2\u09be \u09b9\u09c7\u09af\u09bc\u09c7\u099b, \u0985\u09a8\u09ac\u09be\u09af\u09bc \u0995\u09be\u09b0\u09a3 \u0986\u0997\u09be\u09ae \u09ed \u0993 \u09ee \u0985\u0997\u09be\u09b8\u09cd\u099f \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc \u09ac\u09a8\u09cd\u09a7 \u09a5\u09be\u0995\u09c7\u09ac\u0964 \u09b8\u0995\u09be\u09b2\u09c7\u09b0 \u0993\u09ac\u09bf\u09a6\u09c7\u0995 \u098f\u0987 \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc\u09c7\u09b0 \u099b\u09be\u09a4\u09cd\u09b0\u09b6\u09b0\u09cd\u09a4\u09c7 \u09aa\u09c1\u09b2\u09bf\u09b6 \u098f\u09ac\u0982 \u099b\u09be\u09a4\u09cd\u09b0\u09b2\u09c0\u0997 \u09b8\u0982\u0998\u09b7\u09b0\u09cd\u09b7 \u09b9\u09af\u09bc\u0964\\n\\n\u0986\u09b0\u0993 \u09aa\u09a1\u09bc\u09c1 \u09a8: \u099c\u09bf\u0997\u09be\u09a4\u09b2\u09be-\u09b8\u09cb\u09af\u09bc\u09a8\u09cd\u09b8 \u09b2\u09af\u09bc\u09be\u09ac '\u09a0\u09be\u09a8\u09cd\u09a1\u09be', \u0985\u09b6\u09be\u09a8\u09cd\u09a4 \u09ac\u09be\u09a1\u09bc\u09bf \u0986\u0987\u09bf\u09b8\u09bf\u099f \u0986\u0987\u09c7\u09a8 \u09ae\u09be\u09ae\u09b2\u09be\u09af\u09bc \u09b6\u09b9\u09c0\u09a6\u09c1\u09b2 \u0986\u09b2\u09ae \u09ed \u09bf\u09a6\u09c7\u09b0 \u0987\u09b0\u09ae\u09be\u09c7\u09a8\u09cd\u09a1 \u099c\u09be\u09a8\u09be\u09c7\u09a8 \u09b8\u09b0\u0995\u09be\u09b0\u09c7\u09b0 \u0985\u0999\u09cd\u0997 \u09b8\u0982\u0997\u09a0\u09a8 \u099b\u09be\u09a4\u09cd\u09b0\u09b2\u09c0\u0997 \u09b9\u09c7\u09ac\u09b6 \u09bf\u0995\u099b\u09c1 \u09b8\u09a6\u09b8\u09cd\u09af \u09b8\u0995\u09be\u09b2 \u09e7\u09e6\u099f\u09be\u09b0 \u0993\u09ac\u09bf\u09a6\u09c7\u0995 \u099b\u09be\u09a4\u09cd\u09b0-\u099b\u09be\u09a4\u09cd\u09b0\u09c0\u09a6\u09c7\u09b0 \u0995\u09af\u09bc\u09be\u09ae\u09cd\u09af\u09be\u09a8 \u09a2\u09c1\u0995\u09c7\u09a4 \u09ac\u09be\u09a7\u09be \u09b2\u09c7\u09b2 \u09b8\u0982\u0998\u09b7\u09b0\u09cd\u09b7 \u09b6\u09c1\u09b0\u09c1 \u09b9\u09af\u09bc\u0964 \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc\u09c7\u09b0 \u099c\u09bf\u09a8\u09be\u0987\u099f\u09bf\u0995 \u0987\u09bf\u099e\u09cd\u099c\u09bf\u09a8\u09bf\u09b0\u09bf\u0982 \u09b8\u09cd\u09ae\u09c3\u09a4\u09bf\u0997\u09a4 \u099b\u09be\u09a4\u09cd\u09b0 \u09b9\u09c3\u09a6\u09af\u09bc \u0987\u09b8\u09b2\u09be\u09ae \u099c\u09bf\u09ac\u09bf\u09b8\u09c7\u0995 \u09ac\u09c7\u09b2\u09c7\u099b\u09a8, \u099b\u09be\u09a4\u09cd\u09b0\u09b2\u09c0\u0997\u09c7\u09b0 \u099b\u09c7\u09b2\u09c7\u09b0\u09be \u09bf\u0995\u099b\u09c1 \u09bf\u09b6\u0995\u09cd\u09b7\u09a3\u09a4\u09b0\u09cd\u0995\u09c7 \u09ae\u09be\u09b0\u09a7\u09b0 \u0995\u09b0\u09c7\u0964 \u09ad\u09be\u0999\u099a\u09c1\u09b0 \u0995\u09b0\u09c7\u0964 \u09aa\u09c7\u09b0 \u09aa\u09c1\u09b2\u09bf\u09b6 \u0995\u09c7\u09af\u09bc\u0995 \u09a6\u09ab\u09be \u0995\u09be\u0981\u09a6\u09be\u09c7\u09a8 \u0997\u09af\u09bc\u09be\u09b8 \u09ac\u09af\u09cd\u09ac\u09b9\u09be\u09b0 \u0995\u09b0\u09c7\u0964 \u09ac\u09be\u09b8 \u099a\u09be\u09aa\u09be\u09af\u09bc \u09a6\u09c1\u099c\u09a8 \u09bf\u09b6\u0995\u09cd\u09b7\u09a3\u09a4\u09b0\u09cd\u0995\u09c7\u09b0 \u09ae\u09c3\u09a4\u09cd\u09af\u09c1 \u09b8\u09aa\u09cd\u09a4\u09be\u09b9\u0995 \u09a7\u09c7\u09b0 \u09b8\u09cd\u0995\u09c1\u09b2\u09c7\u09b0 \u099b\u09be\u09a4\u09cd\u09b0-\u099b\u09be\u09a4\u09cd\u09b0\u09c0\u09b0\u09be \u09a4\u09be\u09c7\u09a4 \u0986\u09c7\u09a8\u09cd\u09a6\u09be\u09b2\u09a8 \u0995\u09b0\u09bf\u099b\u09c7\u09b2\u09be - \u09a4\u09be\u09b0\u09ac\u09be\u09b0 \u09c7\u09a5\u09c7\u0995 \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc\u09c7\u09b0 \u099b\u09be\u09a4\u09cd\u09b0-\u099b\u09be\u09a4\u09cd\u09b0\u09c0\u09a6\u09b0\u0993 \u09a4\u09be\u0997 \u09bf\u09a6\u09c7\u09a4 \u09c7\u09a6\u0996\u09be \u09af\u09be\u09af\u09bc\u0964 \u0986\u099c (\u09c7\u09b8\u09be\u09ae\u09ac\u09be\u09b0) \u09b0\u09be\u09b8\u09cd\u09a4\u09be\u09af\u09bc \u09b8\u09cd\u0995\u09c1\u09b2\u09c7\u09b0 \u099b\u09be\u09a4\u09cd\u09b0-\u099b\u09be\u09a4\u09cd\u09b0\u09c0\u09a6\u09b0 \u09b8\u09be\u09c7\u09a5 \u09a6\u09c7\u0996\u09be \u09a8\u09be \u0997\u09c7\u09b2\u0993, \u0995\u09c7\u09af\u09bc\u0995\u09bf\u099f \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc\u09c7\u09b0 \u09bf\u09b6\u0995\u09cd\u09b7\u09a3\u09a4\u09b0\u09cd\u0995\u09c7\u09b0\u09be \u09bf\u09ac\u09c7\u0995\u09cd\u09b7\u09be\u09ad \u09bf\u09ae\u09bf\u099b\u09b2\u09c7 \u09ac\u09b0 \u0995\u09b0\u09be\u09b0 \u09bf\u099a\u09b7\u09cd\u099f\u09be \u0995\u09b0\u09c7\u0964 \u09a8\u09bf\u09b0\u09be\u09aa\u09a6 \u09b8\u09a1\u09bc\u0995: \u09a8\u09a5\u09b0 \u09b8\u09be\u0989\u09a5 \u0993 \u0987\u09b8\u09cd\u099f-\u0993\u09c7\u09af\u09bc\u09b8\u09cd\u099f \u09ac\u09bf\u09b6\u09cd\u09ac\u09ac\u09bf\u09a6\u09cd\u09af\u09be\u09b2\u09af\u09bc \u09ac\u09a8\u09cd\u09a7 \u09c7\u0998\u09be\u09b7\u09a3\u09be\u0964\\n\\n---\\n\\n\u09aa\u09a4\u09be\u0995\u09be \u09b0\u09c7\u09af\u09bc\u09c7\u099b \u09af\u09a5\u09be\u09b8\u09cd\u09a5\u09be\u09a8\u0987\u0964 \u09a4\u09be\u09b0 \u09b8\u09be\u09ae\u09a8\u09c7 \u099c\u09be\u09aa\u09be\u09a8\u09b0 \u09aa\u09b0\u09cd\u09af\u09be\u09af\u09bc\u09c7\u09b0 \u09a8\u09ae\u09a8\u09cd\u09a4\u09b0\u09c0 \u09b6\u09a8\u09c7\u099c\u09be \u098f\u09ac-\u09b0 \u09b8\u09be\u09a5\u09c7 \u09b9\u09be\u09bf\u09b8-\u09b9\u09be\u09bf\u09b8 \u09ae\u09c1\u0996 \u0995\u09b0\u09ae\u09be\u0995\u09c3\u09a4 \u09a8\u09c7\u09b0\u09a8\u09cd\u09a6\u09b0 \u09c7\u09ae\u09be\u09a6\u09c0\u0964 \u09b8\u09ac \u09bf\u0995\u099b\u09c1\u0987 \u09aa\u09b0\u09cd\u09a5\u09be\u09ae\u09be\u09a8\u09ae\u09c1\u0995\u09cd\u09a4\u09ae\u09c2\u09b2\u0995, \u09ae\u09be\u09aa\u09b8\u0987, \u09af\u09a5\u09be\u09af\u09a5\u0964 \u09ae\u09be\u09b2\u09c7\u09af\u09bc\u09bf\u09b6\u09af\u09bc\u09be\u09b0 \u09b0\u09be\u099c\u09a7\u09be\u09a8\u09c0 \u0995\u09c1\u09af\u09bc\u09be\u09b2\u09be\u09b2\u09be\u09ae\u09aa\u09c1\u09b0\u09c7 \u099a\u09b2\u09c7\u099b \u09a4\u09c1\u09b0\u09cd\u09a8\u09be\u09a6\u09b6 \u0986\u0987\u09b8\u09af\u09bc\u09be\u09a8-\u0987\u09bf\u09a8\u09cd\u09a1\u09af\u09bc\u09be \u09b6\u09c0\u09b7\u09b0\u09cd\u09b8\u09c7\u09ae\u09cd\u09ae\u09b2\u09a8\u0964 \u09a4\u09be\u09b0 \u09b8\u09ae\u09be\u09a8\u09cd\u09a4\u09b0\u09be\u09b2 \u098f\u0995 \u09bf\u09a6\u09ac\u09cd\u09aa\u09be\u09b6\u09cd\u099a\u09be\u09b2\u0995 \u09bf\u09ae\u09bf\u09b2\u09a4 \u09b9\u09c7\u09af\u09bc\u09bf\u099b\u09c7\u09b2\u09a8 \u09ad\u09be\u09b0\u09a4 \u0993 \u099c\u09be\u09aa\u09be\u09a8\u09b0 \u09aa\u09b0\u09cd\u09af\u09be\u09af\u09bc\u09c7\u09b0 \u09a8\u09ae\u09a8\u09cd\u09a4\u09b0\u09c0\u09a6\u09ac\u09cd\u09af\u09bc\u0964 \u098f\u0987 \u09aa\u09a4\u09be\u0995\u09be-\u09bf\u09ac\u09aa\u09af\u09b0\u09cd\u09af\u09cd\u09af\u0964 \u09ad\u09be\u09b0\u09c7\u09a4\u09b0 \u099c\u09be\u09a4\u09c0\u09af\u09bc \u09aa\u09a4\u09be\u0995\u09be\u09b0 \u09b8\u09ac\u09c1\u099c \u0986\u0982\u09b6\u09bf\u099f\u09c7\u0995 \u0989\u09aa\u09c7\u09b0 \u09c7\u09b0\u09c7\u0996 \u09b8\u09cd\u09a5\u09be\u09aa\u09a8 \u0995\u09b0\u09be \u09b9\u09c7\u09af\u09bc\u09bf\u099b\u09b2 \u098f\u0987 \u09aa\u09a4\u09be\u0995\u09be\u0964 \u098f\u09ae\u09a8 \u09b9\u09b2\u2014 \u0989\u09a4\u09cd\u09a4\u09c7\u09b0 \u09b8\u0982\u09b6\u09cd\u09b2\u09b7\u09cd\u099f \u09ad\u09be\u09b0\u09aa\u09c3\u09aa\u09cd\u09a4\u09b0\u09be \u09b8\u0982\u09ac\u09be\u09a6 \u09b8\u0982\u09b8\u09cd\u09a5\u09be\u09c7\u0995 \u099c\u09be\u09a8\u09c7\u0999\u09cd\u0997\u09c7\u099b\u09a8, \u098f\u099f\u09be \u098f\u0995\u09ac\u09be\u09b0\u0987 \u0985\u09a8\u0997\u09cd\u09b0\u09b9 \u099a\u09c1\u099f\u0964 \u098f\u0987 \u09b8\u09be\u09a5\u09c7 \u098f-\u0993 \u099c\u09be\u09a8\u09c7\u0999\u09cd\u0997\u09c7\u099b\u09a8, \u098f\u0995\u09be\u09a8\u0993 \u09ae\u09c7\u09a4\u0987 \u09a8\u09be\u09ae \u09aa\u09c3\u0995\u09cd\u09b7 \u09a8\u09be-\u09aa\u09be\u09af\u09bc \u09ad\u09be\u09b0\u09aa\u09c3\u09aa\u09cd\u09a4\u09c7\u09a6\u09b0\u0964\\n\\n---\\n\\n\u09aa\u09a4\u09be\u0995\u09be \u09b0\u09c7\u09af\u09bc\u09c7\u099b \u09af\u09a5\u09be\u09b8\u09cd\u09a5\u09be\u09a8\u0987\u0964 \u09a4\u09be\u09b0 \u09b8\u09be\u09ae\u09a8\u09c7 \u099c\u09be\u09aa\u09be\u09a8\u09b0 \u09aa\u09b0\u09cd\u09af\u09be\u09af\u09bc\u09c7\u09b0 \u09a8\u09ae\u09a8\u09cd\u09a4\u09b0\u09c0 \u09b6\u09a8\u09c7\u099c\u09be \u098f\u09ac-\u09b0 \u09b8\u09be\u09a5\u09c7 \u09b9\u09be\u09bf\u09b8-\u09b9\u09be\u09bf\u09b8 \u09ae\u09c1\u0996 \u0995\u09b0\u09ae\u09be\u0995\u09c3\u09a4 \u09a8\u09c7\u09b0\u09a8\u09cd\u09a6\u09b0 \u09c7\u09ae\u09be\u09a6\u09c0\u0964 \u09b8\u09ac \u09bf\u0995\u099b\u09c1\u0987 \u09aa\u09b0\u09cd\u09a5\u09be\u09ae\u09be\u09a8\u09ae\u09c1\u0995\u09cd\u09a4\u09ae\u09c2\u09b2\u0995, \u09ae\u09be\u09aa\u09b8\u0987, \u09af\u09a5\u09be\u09af\u09a5\u0964 \u09ae\u09be\u09b2\u09c7\u09af\u09bc\u09bf\u09b6\u09af\u09bc\u09be\u09b0 \u09b0\u09be\u099c\u09a7\u09be\u09a8\u09c0 \u0995\u09c1\u09af\u09bc\u09be\u09b2\u09be\u09b2\u09be\u09ae\u09aa\u09c1\u09b0\u09c7 \u099a\u09b2\u09c7\u099b \u09a4\u09c1\u09b0\u09cd\u09a8\u09be\u09a6\u09b6 \u0986\u0987\u09b8\u09af\u09bc\u09be\u09a8-\u0987\u09bf\u09a8\u09cd\u09a1\u09af\u09bc\u09be \u09b6\u09c0\u09b7\u09b0\u09cd\u09b8\u09c7\u09ae\u09cd\u09ae\u09b2\u09a8\u0964 \u09a4\u09be\u09b0 \u09b8\u09ae\u09be\u09a8\u09cd\u09a4\u09b0\u09be\u09b2 \u098f\u0995 \u09bf\u09a6\u09ac\u09cd\u09aa\u09be\u09b6\u09cd\u099a\u09be\u09b2\u0995 \u09bf\u09ae\u09bf\u09b2\u09a4 \u09b9\u09c7\u09af\u09bc\u09bf\u099b\u09c7\u09b2\u09a8 \u09ad\u09be\u09b0\u09a4 \u0993 \u099c\u09be\u09aa\u09be\u09a8\u09b0 \u09aa\u09b0\u09cd\u09af\u09be\u09af\u09bc\u09c7\u09b0 \u09a8\u09ae\u09a8\u09cd\u09a4\u09b0\u09c0\u09a6\u09ac\u09cd\u09af\u09bc\u0964 \u098f\u0987 \u09aa\u09a4\u09be\u0995\u09be-\u09bf\u09ac\u09aa\u09af\u09b0\u09cd\u09af\u09cd\u09af\u0964 \u09ad\u09be\u09b0\u09c7\u09a4\u09b0 \u099c\u09be\u09a4\u09c0\u09af\u09bc \u09aa\u09a4\u09be\u0995\u09be\u09b0 \u09b8\u09ac\u09c1\u099c \u0986\u0982\u09b6\u09bf\u099f\u09c7\u0995 \u0989\u09aa\u09c7\u09b0 \u09c7\u09b0\u09c7\u0996 \u09b8\u09cd\u09a5\u09be\u09aa\u09a8 \u0995\u09b0\u09be \u09b9\u09c7\u09af\u09bc\u09bf\u099b\u09b2 \u098f\u0987 \u09aa\u09a4\u09be\u0995\u09be\u0964 \u098f\u09ae\u09a8 \u09b9\u09b2\u2014 \u0989\u09a4\u09cd\u09a4\u09c7\u09b0 \u09b8\u0982\u09b6\u09cd\u09b2\u09b7\u09cd\u099f \u09ad\u09be\u09b0\u09aa\u09c3\u09aa\u09cd\u09a4\u09b0\u09be \u09b8\u0982\u09ac\u09be\u09a6 \u09b8\u0982\u09b8\u09cd\u09a5\u09be\u09c7\u0995 \u099c\u09be\u09a8\u09c7\u0999\u09cd\u0997\u09c7\u099b\u09a8, \u098f\u099f\u09be \u098f\u0995\u09ac\u09be\u09b0\u0987 \u0985\u09a8\u0997\u09cd\u09b0\u09b9 \u099a\u09c1\u099f\u0964 \u098f\u0987 \u09b8\u09be\u09a5\u09c7 \u098f-\u0993 \u099c\u09be\u09a8\u09c7\u0999\u09cd\u0997\u09c7\u099b\u09a8, \u098f\u0995\u09be\u09a8\u0993 \u09ae\u09c7\u09a4\u0987 \u09a8\u09be\u09ae \u09aa\u09c3\u0995\u09cd\u09b7 \u09a8\u09be-\u09aa\u09be\u09af\u09bc \u09ad\u09be\u09b0\u09aa\u09c3\u09aa\u09cd\u09a4\u09c7\u09a6\u09b0\u0964\"}"}
{"id": "lrec-2024-main-201", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| No. | Input | Translation | Sentiment |\\n|-----|-------|-------------|-----------|\\n| 1   | \u0986\u09bf\u09ae \u09ac\u09af\u09cd\u09bf\u0995\u09cd\u09a4\u0997\u09a4\u09ad\u09be\u09c7\u09ac\u09c7\u09a6\u09c7\u0996\u09bf\u099b\u09c7\u09af\u09bc, \u0997\u09be\u09c7\u09a8\u09b0 \u0995\u09a5\u09be\u0997\u09c1\u09bf\u09b2 \u0985\u09bf\u09a4\u09b0\u09bf\u099e\u09cd\u099c\u09a4, \u098f\u09ae\u09a8\u09bf\u0995 \u099a\u09b2\u09bf\u099a\u09cd\u099a\u09a4\u09f0\u09cd\u09bf\u099f\u09c7\u09a6\u0996\u09be\u09b0 \u09b8\u09ae\u09af\u09bc\u0993 \u0986\u09ae\u09be\u09b0 \u09ae\u09c7\u09a8 \u09b9\u09c7\u09af\u09bc\u09bf\u099b\u09b2\u09c7\u09af\u09bc, \u0986\u09bf\u09ae\u09c7\u09b8\u0997\u09c1\u09bf\u09b2\u09c7\u0995 \u098f\u09bf\u0997\u09c7\u09af\u09bc \u09bf\u09a8\u09c7\u09af\u09bc \u09af\u09be\u09ac\u0964 | I personally found the lyrics to be exaggerated, even while watching the film I felt like I was going to take them on. | Negative |\\n| 2   | \u09bf\u09ac\u09af\u09bc\u09be\u09c7\u09b0\u09b0 \u099c\u09a8\u09af\u09cd \u09ae\u09b0\u09c7\u09a4\u09b9\u09c7\u09ac \u098f\u09ac\u0982 \u09ad\u09af\u09cd\u09be\u09c7\u09b2\u099f \u09aa\u09be\u09bf\u0995\u09b0\u09cd\u0982\u09c7\u09af\u09bc\u09b0 \u09b8\u09c1\u09bf\u09ac\u09a7\u09be\u0993 \u09b0\u09c7\u09af\u09bc\u09c7\u099b\u0964 | The beer is to die for and there is also valet parking. | Positive |\\n| 3   | \u098f\u0996\u09be\u09c7\u09a8 \u09bf\u09ac\u09bf\u09ad\u09a8\u09cd\u09a8\u09a7\u09b0\u09c7\u09a8\u09b0 \u09c7\u09b8\u09bf\u099f\u0982\u09b8, \u09af\u09a8\u09cd\u09a4\u09f0\u09cd\u09aa\u09be\u09bf\u09a4 \u098f\u09ac\u0982 \u0985\u09aa\u09b6\u09a8 \u09b0\u09c7\u09af\u09bc\u09c7\u099b, \u09af\u09be\u09b0 \u09ae\u09c7\u09a7\u09af\u09cd \u09b0\u09c7\u09af\u09bc\u09c7\u099b \u09bf\u09ac\u09b2\u09cd\u099f \u0987\u09a8 \u09c7\u09b2\u09b8\u09a8\u0964 | There are a variety of settings, tools and options, including built-in lessons. | Neutral |\\n| 4   | \u098f\u09ac\u09b6 \u0995\u09c7\u09af\u09bc\u0995\u09bf\u099f \u099a\u09bf\u09b0\u09a4\u09f0\u09cd\u09a5\u09be\u0995\u09be \u09b8\u09c7\u09a4\u09cd\u09a4\u09ac\u09cd\u0993 \u09ac\u0987\u09bf\u099f\u09b0 \u09b8\u09c7\u0999\u09cd\u0997 \u0986\u09ae\u09be\u09b0 \u09c7\u09af\u09be\u0997\u09be\u09c7\u09af\u09be\u0997 \u09bf\u09ac\u09bf\u099a\u09cd\u099b\u09a8\u09cd\u09a8 \u09b9\u09af\u09bc\u09bf\u09a8\u0964 | Despite several characters, I never lost touch with the book. | Positive |\\n| 5   | \u09ac\u09af\u09cd\u09ac\u09b9\u09c3\u09a4 \u0989\u09aa\u09be\u09a6\u09be\u09a8\u09aa\u09c1\u09b0\u09c1 \u098f\u09ac\u0982 \u09aa\u09c7\u0995\u09c7\u099f \u09ac\u09be\u0987\u09c7\u09b0 \u09c7\u09a5\u09c7\u0995 \u09bf\u09a8\u0993\u09bf\u09aa\u09f0\u09cd\u09a8 \u0986\u09b8\u09cd\u09a4\u09b0\u09a3 \u09a5\u09be\u09c7\u0995 \u09af\u09be \u09c7\u09b2\u09a8\u09cd\u09b8\u09c7\u0995 \u09ac\u09be\u09ae\u09cd\u09aa \u09c7\u09a5\u09c7\u0995 \u09b0\u0995\u09cd\u09b7\u09be \u0995\u09c7\u09b0 \u098f\u09ac\u0982\u09b9\u09c1\u0995 \u0989\u09aa\u09bf\u09b8\u09cd\u09a5\u09a4 \u09a5\u09be\u09c7\u0995\u0964 | The material used is thick and the pocket has a neoprene lining on the outside to protect the lens from bumps and hooks are present. | Neutral |\\n| 6   | \u09b8\u09cd\u099f\u09af\u09cd\u09be\u09a8\u09cd\u09a1-\u098f \u09b0\u09c7\u09af\u09bc\u09c7\u099b\u09c7\u09b2\u09c7\u09ad\u09b2 \u0987\u09bf\u09a8\u09cd\u09a1\u09c7\u0995\u099f\u09b0 \u09af\u09be \u09bf\u09ac\u09b2\u09cd\u099f-\u0987\u09a8 \u09ac\u09be\u09ac\u09b2 \u09bf\u09ad\u0989 \u09c7\u09b2\u09c7\u09ad\u09b2 \u098f\u09ac\u0982 \u09bf\u09a5\u09cd\u09f0-\u0993\u09c7\u09af\u09bc \u09c7\u09b9\u09a1 \u09af\u09be \u09e9\u09ec\u09e6 \u09bf\u09a1\u09bf\u0997\u09f0\u09cd \u09c7\u0998\u09be\u09b0\u09be\u09c7\u09a8\u09be \u09af\u09be\u09af\u09bc\u0964 | The stand has a level indicator with a built-in bubble view level and a three-way head that can be rotated 360 degrees. | Neutral |\\n| 7   | \u0993\u09bf\u09b0\u09c7\u09af\u09bc\u09c7\u09a8\u09cd\u099f\u09b0 \u099f\u09be\u0993-\u09af\u09bc\u09be\u09b0 \u098f\u09af\u09bc\u09be\u09b0 \u0995\u09c1 \u09b2\u09be\u09c7\u09b0\u09b0 \u099f\u09af\u09cd\u09be\u0999\u09cd\u0995 \u0995\u09cd\u09b7\u09ae\u09a4\u09be \u09e7\u09e8 \u09bf\u09b2\u099f\u09be\u09b0\u0964 \u098f\u09bf\u099f \u09bf\u09b0\u09bf\u09ab\u09bf\u09b2\u0982 \u099b\u09be\u09a1\u09bc\u09be\u0987 \u09e8-\u09e9 \u09bf\u09a6\u09a8 \u099a\u09c7\u09b2, \u09af\u09be \u09c7\u099b\u09be\u099f\u0986\u0995\u09be\u09c7\u09b0\u09b0 \u0995\u09c2 \u09b2\u09be\u09c7\u09b0\u09b0 \u099c\u09a8\u09af\u09cd \u09aa\u09f0\u09cd\u09a4\u09af\u09cd\u09be\u09bf\u09b6\u09a4 \u09a8\u09af\u09bc\u0964 | Orient's tower air cooler has a tank capacity of 12 liters. It lasts for 2-3 days without refilling, which is not expected for a small sized cooler. | Positive |\\n| 8   | \u0985\u09ae\u09be\u09bf\u09af\u09bc\u0995 \u09ac\u09af\u09cd\u09ac\u09b9\u09be\u09b0 \u098f\u0987\u09c7\u09b0\u09b8\u09cd\u099f\u09c1 \u09c7\u09b0\u09c7\u09a8\u09cd\u099f\u09b0 \u09b8\u09cd\u099f\u09be\u09ab\u09c7\u09a6\u09b0\u0964 | The staff of this restaurant are accommodating. | Neutral |\\n| 9   | \u09b0\u09c1\u09bf\u099f\u0993\u09af\u09bc\u09be\u09b2\u09be\u09c7\u0995 \u0985\u09bf\u09a4\u09b8\u09a4\u09ac\u09cd\u09b0\u09ac\u09be\u0982\u09b2\u09be\u09c7\u09a6\u09b6 \u09a6\u09c7\u09b2 \u09a4\u09be\u09bf\u09ae\u09c7\u09ae\u09b0 \u099c\u09be\u09af\u09bc\u0997\u09be\u09af\u09bc \u09c7\u09a8\u0993\u09af\u09bc\u09be \u09c7\u09b9\u09be\u0995 \u09a4\u09be\u09b9\u09c7\u09b2 \u0986\u09b0 \u0995\u09af\u09cd\u09be\u099a \u09bf\u09ae\u09b8 \u09b9\u09c7\u09ac\u09a8\u09be\u0964 | Rutiwala should be replaced by T amim in the Bangladesh team as soon as possible. Then the catch will not be missed. | Positive |\\n| 10  | \u098f\u0995 \u09ac\u09c7\u09b2\u09c7\u099b \u0986\u09aa\u09a8\u09be\u09c7\u0995\u098f\u0996\u09be\u09c7\u09a8 \u09c7\u0995\u09ac\u09a8 \u09ac\u09cd\u09b2\u09af\u09cd\u09be\u0995 \u0995\u09bf\u09ab \u09aa\u09be\u0993\u09af\u09bc\u09be \u09af\u09be\u09af\u09bc ? \u09ad\u09be\u09c7\u09b2\u09be \u0995\u09c7\u09b0 \u09c7\u099c\u09c7\u09a8 \u09a4\u09be\u09b0\u09aa\u09b0 \u09bf\u09ad\u09bf\u09a1\u0993 \u09ac\u09be\u09a8\u09be\u09a8\u0964 | Who told you Kebon black coffee is available here? Know better then make videos. | Neutral |\\n| 11  | \u09a8 \u09a5\u09be\u0995 \u0986\u09aa\u09c7\u09a8 \u09af\u09be\u09a8\u0986\u09ae\u09be\u09b0 \u098f\u0995\u09cd\u09b8\u09be\u09ae \u0986\u09c7\u09b8\u0964 | No you go. I have examination. | Negative |\\n| 12  | \u09af\u09be\u09b0\u09be \u09c7\u09ae\u09be\u09ac\u09be\u0987\u09b2 \u09bf\u09a6\u09c7\u09af\u09bc\u09aa\u09f0\u09cd\u09bf\u09a4\u09bf\u09a6\u09a8 \u09ea\u09e6\u09e6 \u09c7\u09a5\u09c7\u0995 \u09eb\u09e6\u09e6 \u099f\u09be\u0995\u09be \u0987\u09a8\u0995\u09be\u09ae \u0995\u09b0\u09c7\u09a4 \u099a\u09be\u09a8 \u09a4\u09be\u09b0\u09be \u0986\u09ae\u09be\u09b0 \u09c7\u099a\u09c7\u09a8\u09b2\u099f\u09be \u09b8\u09be\u09ac\u09b8\u09cd\u0995\u09f0\u09cd\u09be\u0987\u09ac \u0995\u09c7\u09b0\u09a8\u0964 | Those who want to earn 400 to 500 taka per day with mobile subscribe my channel. | Positive |\\n| 13  | \u0986\u09aa\u09b8\u09b9\u09c0\u09a8 \u09bf\u09b0\u09aa\u09bf\u099f\u0982\u098f\u09ad\u09be\u09c7\u09a8\u0987 \u0986\u09aa\u09b8\u09b9\u09c0\u09a8 \u09ad\u09be\u09c7\u09ac \u0996\u09ac\u09b0 \u09aa\u09f0\u09cd\u099a\u09be\u09b0 \u0995\u09c7\u09b0 \u099c\u09be\u09a8\u0964 | Uncompromising reporting Know how to report news without compromise. | Negative |\\n| 14  | \u098f\u099f\u09be\u0987 \u09a4 \u099a\u09be\u0987 \u0964 \u09b8\u09ac\u09be\u0987 \u09bf\u09ae\u09c7\u09b2\u09bf\u09ae\u09c7\u09b6 \u09a5\u09be\u0995\u09be \u0964 \u09bf\u0995\u099b\u09c1 \u09bf\u0995\u099b\u09c1 \u0987\u0989\u09bf\u099f\u0989\u09ac\u09be\u09b0\u09b0\u09be \u09a4 \u098f\u0995\u099c\u09a8 \u0986\u09c7\u09b0\u0995\u099c\u09c7\u09a8\u09b0 \u09bf\u09aa\u099b\u09c7\u09a8 \u09b8\u09be\u09b0\u09be\u09bf\u09a6\u09a8\u0997\u09c1\u09c7\u09a4\u09be\u0997\u09c1\u09bf\u09a4 \u0995\u09c7\u09b0\u0964 | That's what I want. Everyone together . Some YouT ubers gossip about each other all day long. | Neutral |\"}"}
