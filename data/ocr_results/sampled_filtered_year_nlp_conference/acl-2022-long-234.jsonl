{"id": "acl-2022-long-234", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task\\nText: They have been bred to be good at sports and entertainment, but not much else.\\n\\nTargeted Group: Black folks\\n\\nAnswer questions about the text.\\n\\n1) Do you think the text is human- or machine-generated?\\n- Human\\n- AI\\n\\n2) In your opinion, would this text be seen as harmful to anyone if an AI system wrote it?\\n- Not at all\\n- Very much so\\n\\n3) In your opinion, would this text be seen as harmful to anyone if a person wrote it?\\n- Not at all\\n- Very much so\\n\\n4) In your opinion, does the text express positive stereotyping?\\n- Yes\\n- Maybe\\n- No\\n\\n5) In your opinion, was the text intentionally written to be harmful to anyone?\\n- Not at all\\n- Very much so\\n\\n6) In your opinion, does this text contain or allude to sexual content/acts?\\n- Yes\\n- Maybe\\n- No\\n\\n7) In your opinion, does the text refer to the given targeted group?\\n- Please select one option from the list below.\\n\\n8) In your opinion, how does the text refer to the targeted individual/group?\\n- Please select one option from the list below.\\n\\n9) In your opinion, does the text explicitly claim to be factual?\\n- Please select one option from the list below.\"}"}
{"id": "acl-2022-long-234", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Prompt Label | Top-k ALICE | Average Human-Validated Toxicty Scores |\\n|--------------|-------------|---------------------------------------|\\n| Toxic        | 3.36        | 1.63                                  |\\n| Non-Toxic    | 3.64        | 1.52                                  |\\n\\nFigure 7: Average human-validated toxicity scores for training set examples based on prompt label (toxic vs. non-toxic) and decoding method (top-k vs. ALICE).\\n\\nFigure 8: Comparing the proportion of identity group mentions that were desired based on the prompts vs. that were generated, in our large-scale validated training set. We include the actual proportions as data labels.\"}"}
{"id": "acl-2022-long-234", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TOXI GEN: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection\\n\\nWarning: this paper discusses and contains content that can be offensive or upsetting.\\n\\nThomas Hartvigsen\u2660 Saadia Gabriel\u2665 Hamid Palangi\u2663 Maarten Sap\u25b2\u25b3 Dipankar Ray\u2666 Ece Kamar\u2663\\nMassachusetts Institute of Technology\u2665 University of Washington\u2663 Microsoft Research\u25b2 Allen Institute for AI\u25b3 Carnegie Mellon University\u2666 Microsoft\\ntomh@mit.edu, skgabrie@cs.washington.edu, hpalangi@microsoft.com, maartensap@cmu.edu {diray,eckamar}@microsoft.com\\n\\nAbstract\\nToxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create TOXI GEN, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model (Brown et al., 2020). Controlling machine generation in this way allows TOXI GEN to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of TOXI GEN and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that TOXI GEN can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset.\\n\\n1 Introduction\\nToxic language detectors often over-rely on minority identity mentions when flagging a statement as toxic, without considering the deeper semantic meaning of the statement (Dixon et al., 2018; R\u00f6ttger et al., 2021). This can lead to severe under-detection of subtle hate (e.g., \\\"They have been bred to be good at sports and entertainment, but not much else\\\"; Figure 1) and over-detection of benign statements (e.g., \\\"child abuse is wrong, racism is wrong, sexism is wrong\\\"; Figure 1). Importantly, such biases in toxicity detection risk further marginalizing or censoring minority groups (Yasin, 2018; Sap et al., 2019; Dias Oliva et al., 2020; Are, 2020; D\u00edaz and Hecht-Felella, 2021).\\n\\nWe introduce TOXI GEN, a large-scale machine-generated dataset of 274,186 toxic and benign statements. To create this dataset, we leverage the massive pretrained language model GPT-3 (Brown et al., 2020), which is known to produce close-to-human-like text (Clark et al., 2021; Dou et al., 2021) but also easily generates socially biased and toxic content (Sheng et al., 2019; Gehman et al., 2020). While such human-like bias and toxicity poses real threats, we use this undesirable behavior in models like GPT-3 to improve existing toxic language classifiers, providing a path forward for mitigating systemic bias. Created using demonstration-based prompting and pretrained toxicity classifiers, TOXI GEN covers over 135k toxic and 135k benign statements about 13 minority identity groups (e.g., African Americans, women, LGBTQ+ folks, etc.).\\n\\nUsing this machine generated approach has two advantages over scraping posts from the web as done by previous work (e.g., Davidson et al., 2017; Founta et al., 2018; Zampieri et al., 2019). First, it allows us to limit spurious identity-toxicity correlations (Dixon et al., 2018; Zhou et al., 2021) by generating equal numbers of toxic/benign statements for each demographic group, including those that are often overlooked in toxic language corpora (e.g., Native Americans). Second, machine generation and careful prompting enables us to generate implicit toxicity (i.e., without swearwords or slurs), which is by definition hard to detect or find and thus often missing in toxic language corpora (Wiegand et al., 2021). Indeed, 98.2% of TOXI GEN statements are implicit, i.e., devoid of explicit profanity,\"}"}
{"id": "acl-2022-long-234", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Examples of statements that fool Google's Perspective API ( ), HateBERT ( ), Open AI content filter ( ), AI2 Delphi ( ), and Roberta ( ). Five statements are benign, but mention minorities and so classifiers find them hateful. Five are toxic sentences, but the classifiers find them neutral. ALICE attacks these classifiers to generate a large-scale, implicit, and balanced dataset.\\n\\nToxic\\n\\nBenign\\n\\n4\\n\\nToxic Benign\\n\\n2\\n\\nAdversarial Language Imitation with Constrained Exemplars\\n\\nDelphi does not produce toxicity probabilities, so we use Open AI's content filter to game Delphi. A Delphi author has confirmed probabilities will be available soon.\\n\\nWe fine-tune existing classifiers on ToxiGen consistently improves performance (+7\u201319%) on 3 existing human-written implicit toxic datasets: ImplicitHateCorpus (ElSherief et al., 2021), SocialBiasesFrames (Sap et al., 2020), and DynaHate (Vidgen et al., 2021). This indicates that the dataset generated in this work and the approaches for generating data provide major steps towards improving toxicity classifiers, and could potentially be used downstream to address the issues from biased machine generation (Sheng et al., 2019) or neutral toxic degeneration (Gehman et al., 2020).\\n\\nWe release our code and the ToxiGen dataset publicly. We also include two models pretrained on ToxiGen along with our human evaluations.\\n\\n2 Implicit Hate Against Minority Groups\\n\\nDetecting implicit toxicity about minority groups (e.g., stereotyping, microaggressions), remains an elusive goal for NLP systems (Han and Tsvetkov, 2020; Wiegand et al., 2021). One key challenge is that, in contrast to explicit toxicity, implicit toxicity is not marked by the use of profanity or swearwords, is sometimes positive in sentiment, and is generally harder to detect or collect at scale (MacAveney et al., 2019; Breitfeller et al., 2019). Nonetheless, implicitly toxic language about minority or marginalized groups is often psychologically damaging to members of those groups (Sue et al., 2007; 3https://github.com/microsoft/ToxiGen\"}"}
{"id": "acl-2022-long-234", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparing toxic language datasets.\\n\\n| Source               | Size     | % Implicit | % Hate Class |\\n|----------------------|----------|------------|--------------|\\n| Breitfeller et al. (2019) Reddit | 2,934    | 99.4       | 100.0        |\\n| TweetBLM (Kumar and Pranesh, 2021) Twitter | 9,165    | 99.0       | 33.7         |\\n| de Gibert et al. (2018) StormFront | 9,916    | 92.2       | 11.3         |\\n| Waseem (2016) Twitter | 16,914   | 82.4       | 31.7         |\\n| ImplicitHateCorpus (ElSherief et al., 2021) Twitter | 22,584   | 96.8       | 39.6         |\\n| Davidson et al. (2017) Twitter | 24,802   | 30.2       | 5.0          |\\n| Kennedy et al. (2018) Hate Forums | 27,665   | 71.8       | 9.1          |\\n| DynaHate (Vidgen et al., 2021) Human-Machine Adv. | 41,134   | 83.3       | 53.9         |\\n| SocialBiasFrames (Sap et al., 2020) Social Media | 44,671   | 71.5       | 44.8         |\\n| Founta et al. (2018) Twitter | 80,000   | 26.1       | 7.5          |\\n| TOXI GEN (ours) GPT-3 | 274,186  | 98.2       | 50.1         |\\n\\n% Hate Class is the percent labeled as hate (according to prompts for TOXI GEN). TOXI GEN is large, almost entirely implicit, and balanced between toxic and benign statements.\\n\\nThe challenge of detecting subtle toxicity about minority groups is that minority mentions are more often the targets of social biases and toxicity (Hudson, 2017). As such, minority mentions often co-occur with toxicity labels in datasets scraped from online platforms (Dixon et al., 2018). For example, over 93% of mentions of Jewish folk in Sap et al. (2020) are toxic (Wiegand et al., 2021). In turn, models trained on such data can exploit these spurious minority-toxicity correlations instead of considering the deeper semantics of text (Zhou et al., 2021). Importantly, the spurious correlations are also learned by large language models, which are known to produce stereotypical, biased, or toxic content when prompted with minority mentions (Sheng et al., 2019). Given that the main mitigation approach to prevent Large Language Models (LLM) from generating toxic language is to train new classifiers to detect such language, these classifiers also learn the spurious correlations and start blocking most language referencing minority groups. This risks erasure (Xu et al., 2021).\\n\\nWith TOXI GEN, we aim for generating a large scale dataset that represents implicit toxicity while balancing between toxic and benign statements, to address the gaps of previous work. As shown in Table 1, existing datasets contain large amounts of explicit toxicity. While valuable, most previous work has relied on scraping data from online platforms, which leads to dataset imbalances with respect to minority-mentioning posts that are toxic vs. benign. Examples are collected at scale using keyword-based scraping approaches (Waseem, 2016; Davidson et al., 2017; Zampieri et al., 2019), the bootstrapped scraping approaches (Founta et al., 2018), and machine-vs-human adversarial data collection (Dinan et al., 2019; Vidgen et al., 2021), among others. In contrast, using large language models to generate our dataset allows us to control the minority groups mentioned in our statements, as well as their implicitness, at larger scale.\\n\\n3 Creating TOXI GEN\\n\\nTo create TOXI GEN, we use demonstration-based prompting for LLMs, encouraging a text generator to produce both toxic and benign sentences that mention minority groups without using explicit language. We introduce a classifier-in-the-loop decoding method based on constrained beam search, ALICE, which, along with samples generated without ALICE, contributes to generating a challenging subset of TOXI GEN. Using these methods, we generate a massive set of statements (over 274,000) containing equal numbers of toxic and benign sentences for 13 identity groups\u2014see Table 2.\\n\\n3.1 Prompt Engineering\\n\\nTOXI GEN is generated by prompting a language model to produce both benign and toxic sentences that (1) include mentions of minority groups by name and (2) contain mainly implicit language, which does not include profanity or slurs. To achieve this, we perform demonstration-based prompt engineering: Acquiring example sentences,\"}"}
{"id": "acl-2022-long-234", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The only way to civilize the natives...\\n\\nPrompt engineered to steer language model towards hate speech\\n\\nPrompt: - thinking about savage hunger-gatherers, that is basically what a native american is today\\n\\nThe only way to civilize the natives is to give them a bath, a haircut and a pair of shoes.\\n\\nEnd\\n\\nHand-written and machine-generated example prompts from \\\"hate\\\" class\\n\\nFigure 2: Adversarial set up and constrained beam search decoding with ALICE. The examples shown are for HateBERT and PerspectiveAPI and the input text is designed to write implicitly-toxic statements about Native American people. Using ALICE, a pre-trained language model like GPT-3 produces toxicity for which the scores from HateBERT and PerspectiveAPI decrease.\\n\\nToxicity Scores\\n- 66% Toxic\\n- 70% Toxic\\n- 43% Toxic\\n- 40% Toxic\\n\\n3.1.1 Demonstration-based prompting\\n\\nPrompts are text fragments passed into language models that can encourage certain behaviors (Brown et al., 2020). However, designing prompts is notoriously challenging (Liu et al., 2021c). While there are several approaches for prompt- ing pretrained LLMs (Liu et al., 2021b), a recent and promising direction is demonstration-based prompting (Gao et al., 2021; Mishra et al., 2021). Here, example statements are passed to an LLMs, encouraging it to produce a similar, but distinct, statement. For example, to encourage LLMs to produce benign statements about Asian people, we might use the following examples: (1) many impactful people are of asian descent, (2) there are hundreds of asian cultures. To engineer prompts that lead to high-quality, group-mentioning toxic and benign statements at scale, we first gather and curate sets of examples.\\n\\n3.1.2 Collecting demonstrations\\n\\nTo generate both benign and toxic responses from LLMs that mention minority groups, we first collect many examples. Intuitively, given many examples of benign sentences that mention one particular group, a language model can be used to produce more. For benign prompts, we encourage realistic text generation and include diverse voices by collecting benign sentences from blog posts and news articles that mention a group. However, finding large amounts of such data at scale is challenging\u2014this is why implicit datasets are hard to acquire. To build a large enough set of demonstrations, we begin with a small number of examples from the wild, then engage a human-in-the-loop process: collect some demonstrations, pass them to our LLM, comb through many responses, and add the best examples to a growing set. Ensuring that a set of examples consistently produces benign responses that still mention the targeted minority group is challenging and so we iterate this loop many times, sampling random subsets of our examples to serve as prompts and observing the responses. This way, we collect 20-50 demonstration sentences per group, all of which we release.\\n\\nTo encourage implicit toxicity from a LLM, we find examples of human-written sentences with implicit toxicity towards each group from hate forums (de Gibert et al., 2018) and Reddit (Breitfeller et al., 2019). We repeat the human-in-the-loop process to expand our sets of examples. Overall, by repeating this process for both toxic and benign examples for all 13 target groups, we create 26 sets of prompts.\"}"}
{"id": "acl-2022-long-234", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Statistics for TOXigen across all groups. Avg. characters denotes the average number of characters per sentence, including the standard deviation.\\n\\n3.2 ALICE: Attacking Toxicity Classifiers with Adversarial Decoding\\n\\nDemonstration-based prompting alone consistently produces toxic and benign statements about minority groups (see Section 4). There is no guarantee that these statements will be challenging to existing toxicity detectors. Therefore, we also develop ALICE, a variant of constrained beam search (CBS; Anderson et al., 2017; Hokamp and Liu, 2017; Holtzman et al., 2018; Lu et al., 2021) during decoding that generates statements that are adversarial to a given pre-trained toxicity classifier.\\n\\nALICE creates an adversarial game between a pre-trained language model (PLM) and a toxicity classifier (CLF) during constrained beam search decoding. In many CBS settings, constraints are added during beam search decoding to force the model to either include or exclude a specific word or group of words in the output (Anderson et al., 2017; Hokamp and Liu, 2017; Lu et al., 2021). With ALICE, we instead want to enforce soft constraints on the probabilities coming from a given toxicity classifier \\\\( CLF \\\\) during beam search:\\n\\n\\\\[\\n\\\\log p(w_{i+1}|w_0:i) \\\\propto \\\\lambda_L \\\\log p_{LM}(w_{i+1}|w_0:i) + \\\\lambda_C \\\\log p_{CLF}(w_0:i+1)\\n\\\\]\\n\\nHere, \\\\( \\\\lambda_L \\\\) and \\\\( \\\\lambda_C \\\\) denote hyperparameters that determine the respective contribution of the language model and classifier to the decoding scoring function. By using this weighted combination, we can steer generations towards a higher or lower probability of toxicity without sacrificing coherence enforced by the language model. To create examples that challenge existing toxicity classifiers, we use two adversarial setups:\\n\\n- **False negatives**: We use toxic prompts to encourage the language model to generate toxic outputs, then maximize the classifier\u2019s probability of the benign class during beam search.\\n- **False positives**: We use benign prompts to encourage the language model to generate non-toxic outputs, then maximize the probability of the toxic class during beam search.\\n\\nIn the first approach, we are also able to detoxify model outputs when the classifier successfully steers the generations towards non-toxic language. ALICE is illustrated in Figure 2.\\n\\n3.3 Decoding Details\\n\\nWe generate TOXigen data with and without ALICE. Without ALICE, we use top-k decoding (Fan et al., 2018) alone with our toxic and benign prompts. With ALICE, we use the HateBERT fine-tuned OffensEval model from Caselli et al. (2021) as the toxicity classifier (CLF). This model covers a range of direct and veiled offense types. We use GPT-3 for the language model. For decoding, we use \\\\( \\\\lambda_L = \\\\lambda_C = 0 \\\\), a maximum generation length of 30 tokens, a beam size of 10, and a temperature of 0.9. Due to limitations imposed by the OpenAI GPT-3 API on accessing log probabilities for the full model vocabulary, we restricted the vocabulary.\"}"}
{"id": "acl-2022-long-234", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We force beam search decoding to not use tokens from the prompt to prevent direct copying. Certain tokens appearing in the prompt such as punctuation are allowed.\\n\\nWe compute the % of implicit samples using https://github.com/RobertJGabriel/Google-profanity-words, the same as ElSherief et al. (2021), also removing ambiguous terms (e.g., \u201cbloody\u201d).\\n\\nWe find that annotators agreed moderately and are higher than or equal rates to prior work on hate speech annotation (Ross et al., 2020).\\n\\nSpecifically, we take the max of the HARMFUL IFAI and HARMFUL IFHUMAN scores and map it into three classes (scores <3: \u201cnon-toxic\u201d, =3: \u201cambiguous\u201d, >3: \u201ctoxic\u201d).\"}"}
{"id": "acl-2022-long-234", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I started wearing a head scarf at the age of 12 because my mom told me that it was my duty as a woman to cover my hair.\\n\\nIf the discussion is for the sake of the greater black community rather than for you, quit trying to hyper-centrally control it.\\n\\nWe do not need to mention trans people in an article about LGBTQ+ issues.\\n\\nThe mentally disabled should be able to live in their communities and not be forced into institutions.\\n\\nTable 3: Example responses from human evaluation where machine-generated text fools annotators into thinking the writer is human. Average toxicity scores are on a 1-5 scale (1 being benign and 5 being clearly offensive), and are averaged across annotator responses. We report scores for the case where annotators assume the writer/speaker is AI and the writer/speaker is human respectively.\\n\\nFigure 4: Summary statistics for the human annotations on the evaluation set. Each statistic that the annotators are asked to evaluate is shown along the x-axis, while the y-axis gives the percentage of examples per annotated class (non-toxic, toxic, ambiguous).\\n\\n2017; Sap et al., 2020), with a Fleiss' $\\\\kappa = 0.46$ (Fleiss, 1971) and Krippendorff's $\\\\alpha = 0.64$ (Krippendorff, 1980). In 55.17% of cases, all 3 annotators agree, while a majority (\u22652/3) agree for 93.4%.\\n\\nHuman validation results. First, we find that our machine-generated statements are largely indistinguishable from human-written statements. For example\u2014see Table 3\u2014human annotators often predict that our text is generated by a human. In fact, on average 90.5% of machine-generated examples are thought to be human-written by a majority of annotators, as shown in Figure 4. We also note that harmful text confuses readers slightly more than non-harmful text: 92.9% of toxic examples are mislabeled as human-written compared to 90.2% for non-toxic. Most toxic examples are also hate speech (94.56%). While opinions are common in both toxic and non-toxic examples, most fact-claiming text is non-toxic.\\n\\nSecond, we find that demonstration-based prompting reliably generates toxic and benign statements about minority groups (\u00a74.3). Further, for the machine-generated examples, we find that 30.2% are harmful (given a score of >3), while only 4% are ambiguous. This indicates that these data are sufficiently toxic or benign. We also find...\"}"}
{"id": "acl-2022-long-234", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that all identity groups covered by the dataset were represented in the human study (see Figure 3), and observe that the identity group referenced by the prompt is generally the same as the group referenced by the corresponding TOXIGEN text, though there is some deviation. This is likely due to GPT-3 conflating identities or mentioning multiple groups. Interestingly, there is no significant difference in toxicity when we account for whether annotators perceive scores as written by humans or AI (Figure 5). This finding indicates that our machine-generated text is perceived as similarly harmful to human text. We also find that the most common framing tactic is \u201cmoral judgement\u201d, or questioning the morality of an identity group, which has been linked to toxicity by prior work (Hoover et al., 2019).\\n\\n### 4.3 Comparing Generation Methods\\n\\nAs further validation, we investigate whether ALICE-generated statements are more adversarial compared to top-k-generated ones. For 125 randomly-selected prompts (62 toxic and 63 non-toxic), we generate two statements: one with ALICE and one without (top-k). We then collect annotations for the 250 statements using the setup described in \u00a74.1, and get toxicity scores from HateBERT.\\n\\nWe find that for top-k sampled sentences, the prompt label indeed matches the desired label (95.2% of non-toxic examples and 67.7% of toxic examples). For ALICE, 40.3% of toxic examples match the prompt label and 92.1% of non-toxic examples match. We also find that ALICE succeeds in fooling HateBERT (26.4% of ALICE-decoded sentences fool HateBERT vs. 16.8% of top-k sampled sentences). Finally, ALICE is effective for detoxifying generated text: the avg. human-annotated toxicity score for ALICE-decoded sentences with a toxic prompt is 2.97, compared to 3.75 for top-k. This difference is statistically significant with p < 0.001. ALICE therefore leads to harder, more ambiguous examples. We greatly expand on these findings in Appendix E with a larger scale human evaluation (\u223c10,000 samples) comparing sentences generated with and without ALICE.\\n\\n### 5 Improving Toxicity Classifiers\\n\\nTo further showcase the usefulness of TOXIGEN, we investigate how it can enhance classifiers' abilities to detect human-written and machine-generated implicit toxic language. We fine-tune Test Data Finetune Data\\n\\n| Model          | HateBERT | RoBERTa |\\n|----------------|----------|---------|\\n| None           | 0.60     | 0.65    |\\n| ALICE          | 0.66     | 0.70    |\\n| ALICE + top-k | 0.65     | 0.70    |\\n| ALICE + top-k | 0.71     | 0.70    |\\n\\nTable 4: AUC for HateBert and RoBERTa both zero-shot and fine-tuned on 3 versions of our dataset: ALICE only, top-k only, and both combined. Since there are fewer ALICE samples than top-k, we downsample top-k for fair comparison via equal-sized datasets. ALICE + top-k combines these two datasets. Each model is evaluated on three external human-written datasets and the human-validated portion of TOXIGEN. Bolding denotes the best performance. In the zero-shot setting (first column) ALICE creates more challenging evaluation samples by attacking HateBERT and RoBERTa. the widely-used HateBERT (Caselli et al., 2021) and ToxDectRoBERTa (Zhou et al., 2021) models on the training portion of TOXIGEN, using the prompt labels as proxies for a true toxicity label. Then, we compare the performance of the out-of-the-box models to those fine-tuned on TOXIGEN on three publicly available human-written datasets (IMPLICIT HATE CORPUS (ElSherief et al., 2021), the SOCIAL BIASES test set (Sap et al., 2020), and DYNAHATE (Vidgen et al., 2021)) as well as the evaluation portion of our machine-generated dataset (TOXI GEN-HUMAN VAL). To ablate the contribution of each decoding method, we also split TOXIGEN into equal numbers of ALICE-generated and top-k-generated examples.\\n\\nOur results\u2014see Table 4\u2014show that fine-tuning HateBERT and ToxDectRoBERTa on TOXIGEN improves performance across all datasets. The improvement on human-written datasets shows that TOXIGEN can be used to improve existing classifiers, helping them better tackle the challenging human-generated implicit toxicity detection task. Fine-tuned HateBERT performs strongly on TOXIGEN-HUMAN VAL, demonstrating that our data can successfully help guard against machine-generated toxicity.\\n\\n### 6 Conclusions\\n\\nIn this work, we used a large language model to create and release TOXIGEN, a large-scale, balanced, and implicit toxic language dataset. TOXIGEN is...\"}"}
{"id": "acl-2022-long-234", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"far larger than previous datasets, containing over 274k sentences, and is more diverse, including mentions of 13 minority groups at scale. The generated samples are balanced in terms of number of benign and toxic samples for each group. We proposed ALICE, an adversarial decoding scheme to evaluate robustness of toxicity classifiers and generate sentences to attack them, and showed the effectiveness of ALICE on a number of publicly-available toxicity detection systems. In our experiments, we showed that fine-tuning pre-trained hate classifiers on TOXIGEN can improve their performance on three popular human-generated toxicity datasets. We also conducted a human study on a subset of TOXIGEN, verifying that our generation methods successfully create challenging statements that annotators struggle to distinguish from human-written text: 90.5% of machine-generated examples were thought to be human-written.\\n\\n### Societal and Ethical Considerations\\n\\n**Risks in dataset release**\\n\\nWhile the purpose of our work is to curate diverse and effective hate speech detection resources, our methods encourage a large language model to make its generation more toxic. This poses a potential misuse case where bad actors exploit these methods for nefarious purposes like spreading machine-generated hate speech. Still, ignoring this possibility does not make it go away and our work introduces an opportunity for the community to push back against harm towards minority groups. Our ultimate aim is to shift power dynamics to targets of oppression. Therefore, we do not consider identity dimensions that are historically the agents of oppression (e.g., whiteness, heterosexuality, able-bodied-ness). Please also note that there is still a lot that this dataset is not capturing about toxic language. Our annotations might not capture the full complexity of these issues related to human experiences. There is need for multi-disciplinary work to better understand these aspects.\\n\\n### ALICE\\n\\nThe proposed method in this work attacks content filters via an adversarial game between two AI systems and thus passes the existing content filters\u2014as we show for 5 publicly-available systems. It is important to leverage this and similar approaches to improve content filters and prevent large scale attacks against sensitive platforms.\\n\\n### Improving Toxicity Detection\\n\\nEffective classifiers for machine biases are required to combat the scale of online harm. Without such systems, minority groups are likely to be targeted by current (biased) systems. Our work is a significant step towards advancing this crucial classification task. Still, toxicity is inherently subjective (Sap et al., 2021). Therefore, moving beyond binary detection tasks to a focus on more nuanced labeling systems (ElSherief et al., 2021; Leonardelli et al., 2021) will prove crucial in developing responsible systems.\\n\\n### Relationship to Policy\\n\\nThe topic of detecting and mitigating toxicity is relevant to the ongoing work and discussions in the space of policy and legislation for AI technology (Wischmeyer and Rademacher, 2020; Reich et al., 2021). Carefully crafted policy and regulation can play an important role in providing oversight into the development and deployment of content moderation systems and toxicity detection algorithms in practice (Benesch, 2020; Gillespie et al., 2020). Getting this right carries a crucial importance for the society as errors in content moderation can disproportionately affect minority groups (Sap et al., 2019). We see a path forward in which tools and techniques like those presented in this work are paired with human expertise and well-informed policy & regulation in bringing scalable and reliable solutions to practice.\\n\\nWe acknowledge and encourage the critical role the NLP research community is poised to play in this interdisciplinary effort.\\n\\n### Acknowledgements\\n\\nWe thank Azure AI Platform and Misha Bilenko for sponsoring this work and providing compute resources, Microsoft Research for supporting our large scale human study, and Alexandra Olteanu for her feedback on human evaluation. We also thank the crowdworkers for their time and effort.\\n\\n### References\\n\\n- Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2017. Guided open vocabulary image captioning with constrained beam search. In EMNLP.\\n- Carolina Are. 2020. How Instagram\u2019s algorithm is censoring women and vulnerable users but helping online abusers. Feminist media studies, 20(5):741\u2013744.\\n- Elizabeth Behm-Morawitz and Dana E Mastro. 2008. Mean girls? the influence of gender portrayals in\"}"}
{"id": "acl-2022-long-234", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-234", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ona de Gibert, Naiara Perez, Aitor Garc\u00eda-Pablos, and Montse Cuadros. 2018. Hate speech dataset from a white supremacy forum. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 11\u201320.\\n\\nTarleton Gillespie, Patricia Aufderheide, Elinor Carmi, Ysabel Gerrard, Robert Gorwa, Ariadna Matamoros-Fernandez, Sarah T Roberts, Aram Sinnreich, and Sarah Myers West. 2020. Expanding the debate about content moderation: Scholarly research agendas for the coming policy debates. Internet Policy Review, 9(4): Article number: 41\u201329.\\n\\nPeter Glick and Susan T Fiske. 1996. The ambivalent sexism inventory: Differentiating hostile and benevolent sexism. Journal of Personality and Social Psychology, 70(3):491.\\n\\nXiaochuang Han and Yulia Tsvetkov. 2020. Fortifying toxic speech detectors against veiled toxicity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7732\u20137739, Online. Association for Computational Linguistics.\\n\\nChris Hokamp and Qun Liu. 2017. Lexically constrained decoding for sequence generation using grid beam search. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1535\u20131546, Vancouver, Canada. Association for Computational Linguistics.\\n\\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. 2018. Learning to write with cooperative discriminators. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1638\u20131649, Melbourne, Australia. Association for Computational Linguistics.\\n\\nJoseph Hoover, Mohammad Atari, Aida M Davani, Brendan Kennedy, Gwenyth Portillo-Wightman, Leigh Yeh, Drew Kogon, and Morteza Dehghani. 2019. Bound in hatred: The role of group-based morality in acts of hate.\\n\\nDavid L Hudson, Jr. 2017. Hate speech online. https://web.archive.org/web/20211115012316/https://www.freedomforuminstitute.org/first-amendment-center/topics/freedom-of-speech-2/internet-first-amendment/hate-speech-online/. Accessed: 2021-11-14.\\n\\nJonathan W Kanter, Monnica T Williams, Adam M Kuczynski, Katherine E Manbeck, Marlena Debreaux, and Daniel C Rosen. 2017. A preliminary report on the relationship between microaggressions against black people and racism among white college students. Race and Social Problems, 9(4):291\u2013299.\\n\\nBrendan Kennedy, Mohammad Atari, Aida Mostafazadeh Davani, Leigh Yeh, Ali Omrani, Yehsong Kim, Kris Coombs, Shreya Havaldar, Gwenyth Portillo-Wightman, Elaine Gonzalez, et al. 2018. The gab hate corpus: A collection of 27k posts annotated for hate speech.\\n\\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2020. Gedi: Generative discriminator guided sequence generation. arXiv preprint arXiv:2009.06367.\\n\\nKlaus Krippendorff. 1980. Content analysis: An introduction to its methodology.\\n\\nSumit Kumar and Raj Ratn Pranesh. 2021. Tweetblm: A hate speech dataset and analysis of black lives matter-related microblogs on Twitter. arXiv preprint arXiv:2108.12521.\\n\\nElisa Leonardelli, Stefano Menini, Alessio Palmero Aprosio, Marco Guerini, and Sara Tonelli. 2021. Agreeing to disagree: Annotating offensive language datasets with annotators' disagreement. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10528\u201310539, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021a. Dexperts: Decoding-time controlled text generation with experts and antiexperts. In ACL.\\n\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021b. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.\\n\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021c. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.\\n\\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Neurologic decoding: (un)supervised neural text generation with predicate logic constraints. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4288\u20134299, Online. Association for Computational Linguistics.\\n\\nSean MacAvaney, Hao-Ren Yao, Eugene Yang, Katina Russell, Nazli Goharian, and Ophir Frieder. 2019. Hate speech detection: Challenges and solutions. PLoS One, 14(8):e0221152.\\n\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Natural instructions: Benchmarking generalization to new tasks from natural language instructions. arXiv preprint arXiv:2104.08773.\"}"}
{"id": "acl-2022-long-234", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-234", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-234", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Generation Details\\n\\nTo generate sentences for a given minority group, we sample 5 random sentences from the corresponding set of examples, then join them into one string with each example being preceded by a hyphen (\\\"\u2013\\\") and ending with a newline character (\\\\n). By appending an extra hyphen to the end of the prompt, LLMs writes a new sentence matching the style of the presented examples. We stop GPT-3's generation once it produces a new newline character, indicating the end of the sentence. For each generated sentence, we use a new, randomly-selected set of 5 random sentences.\\n\\nA.1 Language Model Selection\\n\\nWhile we use GPT-3 to generate statements in this work, in principle, our methods can be used with any models that generate realistic text, such as GPT-Neo (Black et al., 2021), GPT-J (Wang and Komatsuzaki, 2021), or Turing-NLG (Rasley et al., 2020).\\n\\nB Human Validation Details\\n\\nB.1 Selecting MTurk Workers\\n\\nFor human validation, we select 156 MTurk workers with prior experience annotating toxic language (Sap et al., 2020). 51 of these workers participated in data annotation. We collect worker demographics using an optional survey at the end of the annotation task. We find that 56.9% identify as White, 9.8% as Black, 3.9% as Hispanic, 3.9% as Asian and 5.9% as Other. Also, 45.1% of workers identify as female, 37.3% as male and 2% as non-binary. The majority of workers are between 25 and 45 (58.8%). Politically, 25.5% of workers identify as left-leaning, 23.5% as very left-leaning, 13.7% as moderate, 17.6% as right-leaning and 3.9% as very right-leaning.\\n\\nB.2 Annotation Interface\\n\\nFigure 6 shows a screenshot of the annotation interface given to the Amazon Mechanical Turk workers. Prior to annotation, we provide a strong warning and require signed consent before any text is shown.\\n\\nC How does perplexity change across groups?\\n\\nOur decoding approaches should ideally generate low-perplexity sentences. We measure the perplexity assigned by a pre-trained language model across different minority groups for sentences generated with and without ALICE. This will give us an idea of how good the set of sentences are from the perspective of the pre-trained language model in terms of perplexity. We use GPT-2 model from Huggingface to measure perplexity. As some sentences have extremely high perplexity according to GPT-2, we drop sentences (roughly 10% of the dataset) with perplexity over 500 for this analysis. As shown in Table 5, the ALICE-generated sentences have significantly lower perplexity than top-k across all minority groups. We also find that the average perplexity can range significantly between subgroups, though perplexity varies more for top-k-generated text. Interestingly, text mentioning Black people is deemed most-likely across the board, while the least-likely generations differ by generation method: amongst the ALICE-generated text, sentences mentioning Latino people is the least likely, while for top-k, text mentioning Women is the least likely. In all cases, ALICE generates text with up to 5 times lower perplexity than regular decoding.\\n\\n| Group     | ALICE | top-k |\\n|-----------|-------|-------|\\n| Black     | 16.10 | 86.88 |\\n| Asian     | 17.75 | 108.83|\\n| Native Am.| 25.92 | 103.87|\\n| Muslim    | 17.16 | 84.92 |\\n| Latino    | 36.69 | 96.68 |\\n| Jewish    | 19.37 | 96.71 |\\n| Chinese   | 33.60 | 121.54|\\n| LGBTQ+    | 18.15 | 87.93 |\\n| Mental Dis.| 21.22 | 92.21 |\\n| Physical Dis.| 30.46 | 129.15|\\n| Mexican   | 28.36 | 113.62|\\n| Women     | 21.44 | 131.52|\\n| Middle Eastern | 30.71 | 127.95|\\n| Total     | 23.54 | 105.31|\\n\\nTable 5: Perplexity for different minority groups. Sentences with perplexity over 500 are dropped.\\n\\nD Does generated text actually mention the targeted groups?\\n\\nIn the human validation study (\u00a74), we ask annotators to determine whether or not the text actually includes references to the targeted groups;\"}"}
{"id": "acl-2022-long-234", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Table 6: Proportion of generated sentences that mention targeted identity groups in text generated with and without ALICE.**\\n\\nEach prompt was generated with one group in mind. Here, we compare the proportion of text that mentions each group, split by decoding method. As shown in Table 6, we find that both ALICE and top-\\\\(k\\\\) generate text that mentions corresponding minority group in the prompt almost equally good (slightly better for ALICE), though the exact proportion changes by the group. For instance, in text generated for Latino people, ALICE has a 100% hit rate, while top-\\\\(k\\\\) has only 72%. However, for text mention LGBTQ+ people, top-\\\\(k\\\\) text succeeds to mention them 97% of the time while ALICE has only 91%. These values may depend on the underlying language model: in our case, GPT-3 may have been trained on less Latino-mentioning text and therefore benefit more from controlled decoding.\\n\\n**Analysis of Large-Scale Human Validation**\\n\\nSummary Statistics. In addition to the human-validated evaluation set described in Section 4, we obtain labels for 8,960 randomly sampled training examples using the same annotation framework and pool of MTurk workers. This sample is evenly split between top-\\\\(k\\\\) and ALICE generated texts (50.9% for top-\\\\(k\\\\), 49.1% for ALICE). Please note that the samples are drawn randomly from TOXIC training data and we did not enforce having the same prompt for top-\\\\(k\\\\) and ALICE. The analysis for having the same prompt for top-\\\\(k\\\\) and ALICE has already been done in \u00a74.3. In Figure 7, we show that average toxicity scores are similar for the two decoding methods given a prompt label, though ALICE-generated texts have a slightly higher average toxicity for generations from toxic-labeled prompts and a lower average toxicity for generations from non-toxic prompts than top-\\\\(k\\\\). We observe that 66.86% of ALICE-generated texts with a toxic prompt label are actually toxic (compared to 57.91% of top-\\\\(k\\\\) examples) and 93.21% of ALICE-generated texts with a non-toxic prompt label are actually non-toxic (compared to 90.01% of top-\\\\(k\\\\) examples). We also find that ALICE is more effective at generating adversarial language - 58.97% of toxic ALICE-generated examples fool HateBERT, compared to 26.88% of toxic top-\\\\(k\\\\) generated examples. ALICE-generated non-toxic examples also fool HateBERT more often than top-\\\\(k\\\\), though the difference is smaller (15.51% of ALICE-generated non-toxic examples vs. 11.35% of top-\\\\(k\\\\) generations). Lastly, ALICE-generated examples lead to more consistent target group references. At least one annotator identified a direct or indirect reference to the exact target group for 70.4% of top-\\\\(k\\\\) generated examples compared to 78.3% of ALICE-generated examples. A full breakdown is given in Figure 8.\\n\\n**Subjectivity in Annotation.** As we address broadly in Section 7, subjectivity is an area of concern for annotation of toxicity. Prior work has pointed out the role that annotators' belief systems and sociodemographic backgrounds play in their perception of toxicity (Sap et al., 2019, 2021; Davani et al., 2022). This is highlighted by such training examples as \\\"I'd like to thank you... I don't think I'd have made it if I didn't find MGTOW! You guys are great!\\\" Annotators predicted this example to be non-toxic, likely due to not recognizing MGTOW as a misogynistic group.\"}"}
{"id": "acl-2022-long-234", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: Breaking the TOXIGEN-Vsubset 0.44 1.00 0.80 0.99\\n\\nsubset into sub-\\nsets with and without ALICE. HB denotes HateBERT, RB is ToxDectRoBERTa.\\\"\\n\\n- it is a common misconception that na-\\ntive americans are all savages but they\\nhave a rich culture and history\\n- indigenous people are often left out of\\nconversations around policies that im-\\npact them\\n- some native american communities\\nhave reclaimed the term indian, despite\\na complex history\\n- some native americans were granted\\nland after the british were defeated in\\nthe american revolution by arrows\\n- indigenous people have lived in north\\namerica for more than 12,000 years\\n- there was a lot of resentment in the 19th\\ncentury to native americans and some\\nindians were even killed off\\n-\\\"\\n\\nThe trailing hyphen encourages the language\\nmodel to provide another sentence in the growing\\nlist that resembles the prior examples, ending in\\n\\nAll of our generated prompts (26,000) are released\\nwith the dataset.\\n\\nG Releasing a Pretrained Model and its\\nPropagated Labels\\n\\nWe further finetune and release a RoBERTa clas-\\nsifier on the 8,960 human-annotated sampled in\\n\\nalong with our publicly-available\\ncode, this pretrained model will serve as an entry\\npoint for community engagement with our work.\\nWe run this pretrained model on the full\\n\\nalong with TOXIGEN. These new labels may serve\\nto correct some mislabeling.\\n\\nH Dataset Description\\n\\nWe release TOXIGEN as a dataframe with the fol-\\nlowing fields:\\nprompt contains the prompts we\\nuse for each generation.\\ngeneration is the TOXIGEN-\\ngenerated text.\\ngeneration method denotes\\nwhether or not ALICE was used to generate the cor-\\nresponding generation. If this value is ALICE, then\\nALICE was used, if it is top-k, then ALICE\\nwas not used.\\nprompt_label is the binary value indicating\\nwhether or not the prompt is toxic (1 is toxic, 0\\nis benign), and therefore the generation should be\\ntoxic as well. This label is slightly noisy, though\\nlargely accurate\u2014as deemed by human annotators.\\ngroup indicates for which group the prompt was\\ngenerated. Finally, roberta_prediction is the prob-\\nability predicted by our corresponding RoBERTa\\nmodel for each instance. This field can be used as\\npropagated labels according to this model.\\n\\nI Further comparing toxicity classifiers\\n\\nWe also compare finetuning classifiers on subsets of\\n\\n- Test Data Finetune Data\\n  - None ALICE top-k ALICE + top-k\\n  - HB TOXIGEN - VAL ALICE subset 0.44 1.00 0.80 0.99\"}"}
