{"id": "lrec-2024-main-133", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Are Large Language Models Good at Lexical Semantics?\\nA Case of Taxonomy Learning\\n\\nViktor Moskvoretskii 1,2, Alexander Panchenko 1,3, Irina Nikishina 4\\n1 Skoltech, 2 HSE University, 3 AIRI, 4 Universit\u00e4t Hamburg\\nV.Moskvoretskii@skol.tech, A.Panchenko@skol.tech, irina.nikishina@uni-hamburg.de\\n\\nAbstract\\nRecent studies on LLMs do not pay enough attention to linguistic and lexical semantic tasks, such as taxonomy learning. In this paper, we explore the capacities of Large Language Models featuring LLaMA-2 and Mistral for several Taxonomy-related tasks. We introduce a new methodology and algorithm for data collection via stochastic graph traversal leading to controllable data collection. Collected cases provide the ability to form nearly any type of graph operation. We test the collected dataset for learning taxonomy structure based on English WordNet and compare different input templates for fine-tuning LLMs. Moreover, we apply the fine-tuned models on such datasets on the downstream tasks achieving state-of-the-art results on the TexEval-2 dataset.\\n\\nKeywords: taxonomy construction, WordNet, hypernym prediction, LLMs\\n\\n1. Introduction\\nLarge Language Models (LLMs) are recently considered to be magic pills to every Natural Language Processing (NLP) and real-life task nowadays. People use ChatGPT 1 and other LLM-based systems for recommending books and films, retrieving encyclopedic knowledge, for language learning and teaching, grammar correction, translating, writing letters and sometimes academic papers and many other (Kasnci et al., 2023; Moskvoretskii et al., 2023).\\n\\nAt the same time, LLMs show state-of-the-art performance on the NLP benchmarks (Song et al., 2023) and are considered to be the first approach to try.\\n\\nTherefore, in this paper, we would like to challenge modern LLMs with a lexical semantic task \u2014 taxonomy learning, which requires the model to learn not only words and their meanings but also \u201cIS-A\u201d relations between them. Taxonomy organizes concepts into a tree structure summarizing the worldview of a human expert. Indeed, most often, such lexical-semantic resources are constructed and updated manually by highly skilled lexicographers as fully automatic construction of such resources is prone to errors. Previous approaches show that Transformer-based models do not demonstrate high-quality results (Hanna and Mare\u010dek, 2021; Radford et al., 2019), yet these did not experiment with the latest LLMs based on in-\"}"}
{"id": "lrec-2024-main-133", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we aim to address this gap and try to understand if these bring the solution of the task to a new level. Taxonomies are graph structures, where nodes are words or concepts and IS-A relations between them are denoted as edges. The most popular taxonomy for English is WordNet (Miller, 1998), which consists of synsets\u2014lexical nodes that contain word\u2019s lemmas, definition, and sense number specifying meaning. Apart from IS-A relations, WordNet also possesses synonym and meronym relations.\\n\\nTaxonomies are used for various NLP tasks, such as Named Entity Recognition (Toral and Mu\u00f1oz, 2006), Entity Linking (Corro et al., 2015) and others (Wang et al., 2023; Lenz and Bergmann, 2023). Even though some papers working on taxonomic structures, e.g. Nikishina et al. (2023), Chernomorchenko et al. (2024) and Nikishina et al. (2022b) do consider two-directional relations (hypernyms and hyponyms), however, none of them tackles the ability of LLMs to learn different substructures of taxonomy graph.\\n\\nTo sum up, the contribution of the paper is three-fold:\\n\u2022 we explore the capacities of LLMs to learn taxonomic structures and to predict entities to any level of taxonomy using learned representation;\\n\u2022 we introduce a new dataset creation method that collects different types of taxonomy-related subtasks: hypernym prediction, hyponym prediction, insertion between two existing nodes, and synset mixing, as previous setups considered only hypernym prediction;\\n\u2022 we test the fine-tuned model on the downstream tasks achieving state-of-the-art results on the SemEval 2016 Task-13 (Bordea et al., 2016) on the Environment dataset and provides comparable to SotA results in the Science dataset.\\n\\nWe also make data, code and models publicly available.\\n\\n2. Related Work\\n\\nThe most prominent directions in the field are Taxonomy Induction (Camacho-Collados et al., 2018), Hypernym Discovery (Bordea et al., 2015, 2016; Velardi et al., 2013) and Taxonomy Enrichment (Jurgens and Pilehvar, 2016; Tanev and Rotondi, 2016; Espinosa-Anke et al., 2016). There exist several studies that cover most previous approaches to taxonomy learning (Nikishina et al., 2022a, 2020; Cho et al., 2020; Takeoka et al., 2021). However, those papers do not cover more recent studies using LLMs which are the most relevant previous work for our research.\\n\\nPrevious methods in taxonomy construction primarily involve either sophisticated graph neural networks, such as Graph2Taxo (Shang et al., 2020), or approaches based on Hearst patterns accompanied by intricate refinement steps, like TAXI+ with Poincar\u00e9 embeddings (Aly et al., 2019).\\n\\nTo the best of our knowledge, most existing papers do not consider generative transformers for taxonomy learning, but Encoder-based instead, like CTP (Chen et al., 2021), and others (Davies et al., 2023; Hanna and Mare\u010dek, 2021). Most existing papers describe the application of LLMs for taxonomy construction. For instance, LM-Scorer Jain and Espinosa Anke (2022) interrogate BERT (Devlin et al., 2019) and RoBERTa (Li et al., 2019) among masked LMs, and GPT2 (Radford et al., 2019) among causal LMs. The authors use zero-shot taxonomy learning methods which are based on distilling knowledge from language models via prompting and sentence scoring. However, they achieve results that are lower than SotA approaches for the TexEval-2 task. However, there are no current studies that perform taxonomy learning and construction using more recent open-source models to compare with, such as LLaMA-2 (Touvron et al., 2023) and Mistral (Jiang et al., 2023).\"}"}
{"id": "lrec-2024-main-133", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1. Dataset Creation\\n\\nWhile constructing our dataset, we primarily rely on the English WordNet 3.0 due to its clean and well-organized structure. Our predominant dependence is on the nouns subgraph, as only the most common class in the WordNet but also a difficult class for LMs to learn, according to Lazaridou et al. (2021).\\n\\n| Category                  | #Samples | Test |\\n|---------------------------|----------|------|\\n| Hyponym prediction        | 16789    | 828  |\\n| Synset mixing             | 1461     | 47   |\\n| Hypernym prediction       | 1338     | 364  |\\n| Insertion                 | 648      | 35   |\\n| **Total**                 | **20236**| **1274** |\\n\\nTable 1: The statistics of the dataset samples for Taxonomy Learning based on WordNet.\\n\\nWe start the dataset creation with a Directed Acyclic Graph (DAG) from WordNet which is based on the \u201cIS A\u201d relations. Then we randomly sample edges or subsets from the graph into different subsets, considering all possible tree operations. The detailed algorithm for the dataset construction is presented in Subsection 3.2. We assume that such a diverse dataset with various scenarios is beneficial for two reasons:\\n\\n- diverse dataset will help the model to generalize better and grasp the broader relationships between words from a wider variety of subtasks;\\n- diverse dataset will equip the model with the capability to employ a range of strategies for constructing taxonomies.\\n\\nTherefore, we collect four different subsets in order to consider the most possible tree operations within the graph, giving higher priority to hyponym and hypernym prediction. The tasks comprise the following scenarios (Figure 1):\\n\\n1. **hyponym prediction (1.A):** predicting a list of hyponyms associated with the input synset from taxonomy;\\n2. **hypernym prediction (1.B):** predicting the hypernym based on the input word;\\n3. **synset mixing (1.C):** predicting single hyponym based on two synsets.\\n4. **insertion (1.D):** predicting a word when provided with its hypernym and hyponym.\\n\\nWe guarantee that there is no overlap between our test and training datasets, none of the test nodes is included in any subtask scenario. The statistics for each subset are presented in Table 1.\\n\\n3.2. Dataset Collection Algorithm\\n\\nTo formulate a precise algorithm, we introduce subtask sets derived from the graph, represented as a collection of the following mini-sets:\\n\\n- \\\\( A_i = \\\\{ p, \\\\{ c_j \\\\} \\\\} \\\\in A, \\\\)\\n- \\\\( B_i = \\\\{ p, c \\\\} \\\\in B, \\\\)\\n- \\\\( C_i = \\\\{ p_1, p_2, c \\\\} \\\\in C, \\\\)\\n- \\\\( D_i = \\\\{ g, p, c \\\\} \\\\in D, \\\\)\\n\\nwhere \\\\( p \\\\) is a parent node, \\\\( c \\\\) is a child node, \\\\( g \\\\) is a generic node, and \\\\( \\\\deg \\\\) is the degree of a node.\\n\\n**Algorithm 1**\\n\\n**Input:** Sets \\\\( A, B, C, D \\\\) sampled from Graph\\n\\n**Output:** Train and Test Sets\\n\\n1. \\\\( \\\\text{Train} := \\\\text{Empty Array} \\\\)\\n2. \\\\( \\\\text{Test} := \\\\text{Empty Array} \\\\)\\n3. Collect sets \\\\( A, B, C, D \\\\).\\n4. while \\\\((A \\\\cup B \\\\cup C \\\\cup D) \\\\neq \\\\emptyset\\\\) do\\n   5. \\\\( \\\\text{cur_set} \\\\sim \\\\text{P data} \\\\)\\n   6. \\\\( \\\\text{cur_sample} = \\\\text{cur_set}.\\\\text{pop()} \\\\)\\n   7. if \\\\( \\\\text{cur_sample} \\\\cap \\\\text{Train} = \\\\emptyset \\\\) then\\n      8. \\\\( \\\\text{to_test} \\\\sim \\\\text{P test} \\\\)\\n      9. if \\\\( \\\\text{to_test} == 1 \\\\) then\\n         10. \\\\( \\\\text{Test}.\\\\text{append}(\\\\text{cur_sample}) \\\\)\\n      11. else\\n         12. \\\\( \\\\text{Train}.\\\\text{append}(\\\\text{cur_sample}) \\\\)\\n      13. end if\\n   14. else\\n      15. \\\\( \\\\text{Train}.\\\\text{append}(\\\\text{cur_sample}) \\\\)\\n   16. end if\\n   17. end while\"}"}
{"id": "lrec-2024-main-133", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | GPT2 | Llama2-7B Numbers | Llama2-7B Lemmas | Llama2-7B Definitions | Mistral-7B Definitions |\\n|-------------|------|------------------|-----------------|----------------------|-----------------------|\\n| MRR scores  | 0.006 | 0.267            | 0.329           | 0.498                | 0.085                 |\\n\\nTable 2: Fine-tuned models MRR scores on the test set. Bold represents the best result, underlined are second-ranked.\\n\\nHere, c denotes hyponyms, p - hypernyms, and g - hyperhypernyms.\\n\\nIn order to perform comprehensive set intersections, we introduce the concept of \u201cdeep intersection\u201d, denoted as $\\\\cap$. This operation characterizes the intersection between the elements of elements from two sets, not solely the elements themselves, expressed as:\\n\\n$$S_1 \\\\cap S_2 = S_{ij} (S_1 \\\\cup S_2)$$\\n\\nIn the following phase, our objective is to create random training and testing sets, ensuring around 1000 samples in the test set and a predominant number of hyponyms predictions and hypernyms prediction in the training set, with other sample types evenly distributed. This task is complex due to possible large intersections among different cases and the order of sample collection. To address this, we introduce a distribution on subtasks denoted as $P_{\\\\text{data}}$, allowing us to manually adjust the probability of sampling each subtask.\\n\\nWe also introduced a Bernoulli distribution $P_{\\\\text{test}}$ with a parameter $p$ to control the probability of samples being assigned to the test set. Optimal values for these probabilities were determined as follows:\\n\\nFor $P_{\\\\text{data}}$:\\n- $P(A) = 0.51$\\n- $P(B) = 0.39$\\n- $P(C) = 0.05$\\n- $P(D) = 0.05$\\n\\nFor $P_{\\\\text{test}}$:\\n- $p = 0.05$\\n- $q = 0.95$\\n\\nDuring collection, we use the \u201cpop()\u201d operation, that deletes last element from set and returns it.\\n\\nTo handle the intricacies of the prevalent word categories, we employ a topological sort on the graph. Subsequently, we ensure that no vertex within our sets possesses a level lower than a specified parameter denoted as \u201clevel\u201d. This condition can be expressed as follows:\\n\\n$$\\\\forall i, \\\\forall v \\\\in S_i: \\\\text{TopSort}(v) \\\\geq \\\\text{level}$$\\n\\nWe also establish a \u201ctarget\u201d vertex for each element within the subtasks. This allows us to track the presence of this specific target vertex in the test set, ensuring that we maintain the integrity of our evaluation.\\n\\nThe breakdown of the definitions for these \u201ctarget\u201d vertices based on different subtasks can be described as follows:\\n\\n- $A_t(i) = \\\\{c\\\\}$: In this case, we need to track all the hyponyms. If we have not encountered hyponyms in the training set, then we cannot determine the target. However, it is permissible to encounter the hypernym in the test set because it is present in the prompt.\\n\\n- $B_t(i) = c$: If we have not seen the hyponym, it means we have not encountered this pair. Otherwise, we would have added the hyponym to the tracking. Therefore, if we haven't seen the hyponym, it implies we haven't seen this edge.\\n\\n- $C_t(i) = c$: If we have not seen the hyponym, it implies we haven't observed the target.\\n\\n- $D_t(i) = p, c$: This scenario is equivalent to restricting two edges: $g - p$ and $p - c$, which correspond to the cases $A$ and $B$.\\n\\n3.3. Model Finetuning\\n\\nFor our research, we utilize latest foundation models language models Llama2-7B and Mistral-7B. Smaller models, such as GPT2, demonstrated negligible performance across all subsets and were consequently excluded from the analysis. These models were optimized using a 4-bit quantization technique. We\"}"}
{"id": "lrec-2024-main-133", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: MRR Scores difference between easy and hard subsamples (easy \\\\(\u2212\\\\) hard) for the taxonomy learning subtasks. Green color denotes that scores are higher for the \u201ceasy\u201d subset, Red color shows that better results are for the \u201chard\u201d subset.\\n\\nWe employ the AdamW optimizer with a learning rate of \\\\(3\\\\times10^{-4}\\\\) and a cosine annealing scheduler.\\n\\nOur inputs include an LLaMA-2 system prompt that looks as follows:\\n\\n(1) [INST] \u00abSYS\u00bb You are a helpful assistant. List all the possible words divided with a comma. Your answer should not include anything except the words divided by a comma \u00ab/SYS\u00bb\\n\\nThen we introduce a technical-style input prompt and the expected output format:\\n\\n(2) hypernym: dog.n.1 | hyponyms: [/INST]\\n\\n(3) pug, corgi,\\n\\nWe also explore the impact of altering the style of the prompt with numerical representations, lemmas, and definitions: \u201cdog.n.1\u201d, \u201cdog (dog, domestic dog, Canis familiaris)\u201d, \u201cdog (a member of the genus Canis that has been domesticated by man since prehistoric times)\u201d.  \\n\\n3.4. Results\\n\\nIn our study, we assess the quality of our models using Mean Reciprocal Rank (MRR), which reflects the position of the first correct answer. We do not use other possible metrics for ranking as they might be too strict. To evaluate the models, we generate a list of potential candidates, separated by commas, and match them with target words.\\n\\nAs our preliminary experiments, we also conducted a case study to assess ChatGPT performance on the task. We discovered that it failed to provide correct answers, even when employing the few-shot learning technique. For example, for the word \u201cMaltese\u201d candidates from ChatGPT are \u201cdogbreed\u201d, and \u201canimal\u201d instead of \u201ctoy dog\u201d which is the correct hypernym from WordNet; For the phrase \u201cmachine translation\u201d possible hypernyms are \u201cautomated translation\u201d and \u201clanguage translation system\u201d, while true hypernyms are \u201cartificial intelligence\u201d and \u201ccomputationallinguistics\u201d. We can see that the model correctly identifies the area, but is not able to point out the specific synset from WordNet. This was particularly notable in instances where our fine-tuned model excelled.\\n\\nThe results for our fine-tuned models are presented in Table 2. We observe that the best results for hypernym prediction and insertion between the two nodes are quite high. For example, the score of 0.5 for hypernym prediction means that on average the second predicted candidate is the correct one. However, from the manual error analysis, we observe that this score is compiled as the mean of the correctly predicted first candidates for most cases and all incorrect candidates for others. At the same time, we can also see that the results for synset mixing are twice lower than hypernyms or insertion. Quite low scores are achieved for hyponym prediction. Notably, those tasks appear to be more difficult to solve. We hypothesize that the limitations for hyponym prediction may not stem from the amount of data or the model, but rather from the size of the model, which is generally believed to be closely linked to its performance. Initially, we theorized that these limitations could be mitigated through the use of disambiguation via lemmas or definitions. However, our findings suggest that this may not be effective. Additionally, these limitations might arise from the inherent nature of rela-\"}"}
{"id": "lrec-2024-main-133", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tional and instructional tuning. In cases where there is only a single parent, predictions tend to be more straightforward, and the model is trained to predict a single node. This contrasts with the scenario involving hyponyms, where multiple instances exist. Consequently, the model must predict a sequence, and the loss is calculated across the entire sequence in its precise order.\\n\\nWe can also note that incorporating lemmas yields significantly better results compared to numbers, and the highest scores are achieved when definitions are included. This might happen due to the autoregressive generation. Immersing the model in an appropriate context leads to shifting distribution towards correct answers. In that way, providing definitions makes the shift either stronger or more accurate. We also tested the best setup with the brand new Mistral-7B model which showed higher performance than LLaMA-2 on (Jiang et al., 2023), however, it did not perform better on our dataset.\\n\\n3.5. Ablation Study\\n\\nWe were surprised to find that LLaMA-2 performed poorly in predicting hyponyms. To better understand the results, we investigate hyponym predictions more thoroughly.\\n\\n3.5.1. Subtypes of Hyponyms\\n\\nFirst, we assume that the results demonstrated on all types of hyponym relations might be not very representative. Therefore, we split the hyponym prediction task into the following subtasks regarding the type of the predicting nodes (See Figure 2 for examples and more detail):\\n\\n- **Leaves Divided (2A):** all hyponyms are terminal nodes, and 50% of them appear in the input, other part is predicted as a target.\\n- **Internal Nodes (2B):** Hyponyms are not required to be terminal nodes, but they have to contain at least one internal node.\\n- **Only Leaves (2C):** all target hyponyms are terminal nodes;\\n- **Single Leaves (2D):** hyponyms are terminal, and they are the only hyponyms for the node.\\n\\nWe further evaluate the model performance, and present the results in Table 4. LLaMA-2 excels in predicting terminal nodes (2C) compared to internal ones (2B). From the manual error analysis, we can conclude that terminal nodes are usually non-ambiguous and have only one meaning. While predicting internal nodes (2B) the model predicts more subsequent nodes (with hop \u2265 2) instead of the direct hyponyms. We can also see that (2A) scenario demonstrates lower performance than while predicting all possible hyponyms (2C). This implies that the core issue is not ambiguity, which additional hyponyms can address, but rather the inability to generate appropriate hyponyms. The predictive scope of the model is limited with the present candidates in the input. The scenario featuring a single leaf hyponym (2D), is extremely challenging to predict, even with hyperhypernyms as input. It may be explained by the fact that such instances are more complex and less prevalent in the language.\\n\\n3.5.2. Common Words VS Terminology\\n\\nWhen analyzing the outputs of the models in order to understand low resultson average, we notice that the major problem may be in the difficulty of the dataset itself: some synsets in the WordNet taxonomymight be too specific for the model to predict hyponyms or hypernyms for.\"}"}
{"id": "lrec-2024-main-133", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to check this hypothesis, we manually split our dataset into two categories: common knowledge words (\\\"easy\\\" category) and terms, jargon, or rare words (\\\"hard\\\" category). To do that, we asked three annotators, experts in computational linguistics, to annotate the test set. The assessors were required to mark the whole sample as \\\"hard\\\" if there was at least one word that belonged to terms, jargon, or rare words, otherwise, they were supposed to put an \\\"easy\\\" label. Krippendorf\u2019s alpha score on the annotations reached 0.67, indicating sufficient agreement among annotators to take answers into consideration.\\n\\nWe calculated the performance on both sub-sets and present the results in Table 3. Surprisingly, our models tend to perform better on the \\\"hard\\\" instances, particularly when predicting hyponyms. However, for the best model that uses word definitions, \\\"easy\\\" instances achieve higher scores, particularly for the cases that do not involve hyponyms. This pattern, however, doesn\u2019t consistently hold for other types of prompts, where \\\"hard\\\" instances are sometimes predicted more accurately, even for hyponyms or internal nodes.\\n\\nWe believe that the outcomes of the current study demonstrate that the model more correctly predicts less common words. This may be explained by the fact that the distribution of candidates for the terms is narrower, which makes it more focused on the correct answers. Moreover, the model encounters such rare words quite infrequently and usually in a consistent and specific context.\\n\\n4. Downstream Task: TexEval-2 (SemEval 2016 Task 13)\\n\\nIn order to check model abilities to generalize and to learn different strategies of taxonomy creation, we test the fine-tuned models on the downstream task: SemEval 2016 Task 13. We use the Eurovoc taxonomies (\\\"Science\\\" and \\\"Environment\\\") from SemEval-2016 (Bordea et al., 2016). These datasets are commonly used as a benchmark for testing models\u2019 abilities of taxonomy construction.\\n\\n4.1. Taxonomy Construction Procedure\\n\\nTo create the taxonomy, we use perplexity to discover edges between nodes. First, we calculate perplexity for all vertex pairs using two input templates: hyponym prediction 1A and hypernym prediction 1B. After that, we construct the taxonomy by adding edges between vertices with perplexity below a certain threshold. This was done either via considering all possible word pairs (brute-force) or by recursively building the taxonomy from a starting point (root for hyponyms prediction), like a tree (Depth-first search style).\\n\\n4.2. Results and Discussion\\n\\nOur experiments show that predicting hypernyms performs significantly better than predicting hyponyms, which is coherent with the scores for the respective subtasks during the fine-tuning step. Furthermore, the brute-force method of building the taxonomy outperformed the DFS-style approach. That could happen due to error accumulation during graph traversal. Incorrect decision on the first couple levels significantly limits our possible edge space. The results for the additional experiments are presented in Table 6.\\n\\nTable 5 presents the F1-score results for the Science (Sci) and Environment (Env) datasets. We compare our three best-performing models with the previous approaches and the GPT-2 baseline. We deliver results for LlaMA-2 with numerical input and LlaMA-2 with lemmas.\"}"}
{"id": "lrec-2024-main-133", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Comparison of the results for the downstream TexEval-2 task.\\n\\n| Approach       | Method              | Template Sci Env |\\n|----------------|---------------------|------------------|\\n| LlaMA-2        | brute-force          | 0.419            |\\n|                | hypo                | 0.192            |\\n|                | dfs                 | 0.340            |\\n|                | hyper               | 0.426            |\\n|                | hypo                | 0.188            |\\n| LlaMA-2        | brute-force          | 0.416            |\\n|                | dfs                 | 0.426            |\\n|                | hyper               | 0.185            |\\n|                | hypo                | 0.125            |\\n| LlaMA-2        | brute-force          | 0.411            |\\n|                | dfs                 | 0.186            |\\n|                | hyper               | 0.186            |\\n|                | hypo                | 0.125            |\\n\\nTable 6: Results for the downstream TexEval-2 task comparing different fine-tuned models, methods for graph construction, and templates for model inputs. Hyper approach stands for hypernym prediction and hypo for hyponym prediction.\\n\\nFor LlaMA-2 with lemmas (as we have no additional lemmas unlike in WordNet), we tried two approaches (duplicate lemma in listing; provide no lemma at all):\\n\\n(4) \u201chypernym: cat (cat) | hyponyms:\u201d\\n(5) \u201chypernym: cat () | hyponyms:\u201d\\n\\nOur results show that our method performs better than all other existing models on the Environment dataset and is ranked second on the Science dataset. However, the best-performing approach for \u201cScience\u201d, which is Graph2Taxo (Shang et al., 2020) is reached with a GNN-based cross-domain transfer framework. The best score is achieved during their ablation study. The default setup of the framework does not achieve the best scores (see (Shang et al., 2020) (pure) in Table 5). Moreover, we need to take into account that we did not apply any specific taxonomy-building strategy, which leaves room for further improvement on the taxonomy-creation downstream tasks. At the same time, GPT-2 performed extremely bad on this task, as well as zero-shot methods based on distilling knowledge.\"}"}
{"id": "lrec-2024-main-133", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1506\\n\\nedge from language models via prompting and sentence scoring (Jain and Espinosa Anke, 2022), and the pretrained language models (CTP) like BERT for parenthood prediction and tree reconciliation (Chen et al., 2021).\\n\\n5. Conclusion\\n\\nOverall, our primary task was to investigate whether Large Language Models are capable of solving purely linguistic tasks, such as Taxonomy Learning. We can conclude that the models do acquire basic skills in different types of taxonomic operations: insertion, node mixing, hypernym, and hyponym prediction. However, the results are far from being perfect on the test split, which demonstrates model confusion on the task. Surprisingly, the model struggles more with common words than with terms. At the same time, the results for internal nodes are lower than for terminal ones. The above-mentioned outcomes and the best scores achieved by the model with definitions demonstrate that the ambiguity problem is still relevant even for LLMs when a small context is given. When considering the downstream task, we can conclude that the fine-tuned LLMs do learn taxonomic relations and could be further used for different applications. For example, we demonstrated that our fine-tuned LLaMA-2 achieves state-of-the-art results for the TExEval-2 task of taxonomy construction on the \u201cScience\u201d task. However, the application of such models might still require a more elaborate procedure for taxonomy creation. As for future work, we plan to extend the taxonomy to other languages using Open Multilingual WordNet and do further experiments with input structures and downstream tasks. We believe that the issue could potentially be mitigated either by considering larger models or by modifying the training procedure. To improve the results on the hyponym prediction, we plan to modify the training procedure, involving permuting the sequence of hyponyms and conducting multiple training steps on the same relations, or altering the target to focus on a single hyponym and sampling them in portions.\\n\\nLimitations\\n\\nWe find the following limitations of our work:\\n\\n\u2022 The full list of operations over taxonomy might also include deletion, moving a synset from one position to another one in a tree. But we do not consider them to assume the taxonomy is \u201cperfect\u201d as input taxonomy is built by humans. However, for automatically constructed taxonomies such operations are essential to use, as some parts of the tree/graph may be not optimally constructed.\\n\\n\u2022 We expect that it is possible to further push the quality reported in our work if larger versions of large pre-trained transformers are used, such as LLaMA2-13B and Vicuna-13B, as was the case for multiple other tasks. However, the general trend is clear from our experiments.\\n\\n\u2022 We are also aware of new experiments from a very recent paper (Chen et al., 2023) where authors present a comparative study for taxonomy construction using LLMs (GPT-NEO and GPT-3.5 for few-shot learning), evaluating on two datasets, different from TexEval-2. Because of the time constraints, we were not able to test our model on their downstream dataset.\\n\\n\u2022 We did not test the multilingual setting of our approach, which is possible if the multilingual version of sequence-to-sequence models and datasets are used. However, preliminary experiments demonstrated negative results and a very low quality of the existing multilingual taxonomies (BelNet, ConceptNet, Open Multilingual WordNet) (Navigli and Ponzetto, 2010; Speer et al., 2018) for languages distinct from English. This is an important additional experiment to further validation of the method explored in our work.\\n\\n\u2022 Nowadays, dozens of large pre-trained generative models exist and we report results only on a few of them. It may be that some other base models used could...\"}"}
{"id": "lrec-2024-main-133", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"further push the results. Our goal however was to show an example of how similar models and not perform an exhaustive search of all models.\\n\\n- We tried to be exhaustive, but we might not have covered all existing types of taxonomy-related subtasks, which we leave out of the scope of our research.\\n\\n**Ethics Statement**\\n\\nWe use in our work large neural models, such as LlaMA-2, pre-trained on real texts including user-generated content. While authors of the models made an effort to filter obviously toxic or biased content, the model itself still can contain certain biases, and as a consequence outputs of our methods may render such biases. Methodologically it is however straightforward to apply our techniques on other pre-trained models that were debiased in a required way. Otherwise, we do not see any other ethical concern in our work to the best of our knowledge.\\n\\n**Acknowledgements**\\n\\nThis work was supported by the DFG through the project \u201cACQuA: Answering Comparative Questions with Arguments\u201d (grants BI 1544/7-1 and HA 5851/2-1) as part of the priority program \u201cRATIO: Robust Argumentation Machines\u201d (SPP 1999).\\n\\nThe research by Alexander Panchenko was supported by the RSF grant 20-71-10135.\\n\\n**6. Bibliographical References**\\n\\nRami Aly, Shantanu Acharya, Alexander Ossa, Arne K\u00f6hn, Chris Biemann, and Alexander Panchenko. 2019. Every child should have parents: a taxonomy refinement algorithm based on hyperbolic term embeddings.\\n\\nHe Bai, Tong Wang, Alessandro Sordoni, and Peng Shi. 2022. Better language model with hypernym class prediction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1352\u20131362, Dublin, Ireland. Association for Computational Linguistics.\\n\\nGeorgeta Bordea, Paul Buitelaar, Stefano Faralli, and Roberto Navigli. 2015. SemEval-2015 task 17: Taxonomy extraction evaluation (TExEval). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 902\u2013910, Denver, Colorado. Association for Computational Linguistics.\\n\\nGeorgeta Bordea, Els Lefever, and Paul Buitelaar. 2016. SemEval-2016 task 13: Taxonomy extraction evaluation (TExEval-2). In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1081\u20131091, San Diego, California. Association for Computational Linguistics.\\n\\nJose Camacho-Collados, Claudio Delli Bovi, Luis Espinosa-Anke, Sergio Oramas, Tommaso Pasini, Enrico Santus, Vered Shwartz, Roberto Navigli, and Horacio Saggion. 2018. SemEval-2018 task 9: Hypernym discovery. In Proceedings of The 12th International Workshop on Semantic Evaluation, pages 712\u2013724, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nBoqi Chen, Fandi Yi, and D\u00e1niel Varr\u00f3. 2023. Prompting or fine-tuning? A comparative study of large language models for taxonomy construction. CoRR, abs/2309.01715.\\n\\nCatherine Chen, Kevin Lin, and Dan Klein. 2021. Constructing taxonomies from pre-trained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4687\u20134700, Online. Association for Computational Linguistics.\\n\\nPolina Chernomorchenko, Alexander Panchenko, and Irina Nikishina. 2024. Leveraging Taxonomic Information from\"}"}
{"id": "lrec-2024-main-133", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large Language Models for Hyponymy Prediction. In Analysis of Images, Social Networks and Texts \u2014 11th International Conference, AIST 2023, volume 14486 of LNCS. Springer.\\n\\nYejin Cho, Juan Diego Rodriguez, Yifan Gao, and Katrin Erk. 2020. Leveraging WordNet paths for neural hypernym prediction. In Proceedings of the 28th International Conference on Computational Linguistics, pages 3007\u20133018, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nLuciano Del Corro, Abdalghani Abujabal, Rainer Gemulla, and Gerhard Weikum. 2015. FINET: context-aware fine-grained named entity typing. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 868\u2013878. The Association for Computational Linguistics.\\n\\nAdam Davies, Jize Jiang, and ChengXiang Zhai. 2023. Competence-based analysis of language models. CoRR, abs/2303.00333.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics.\\n\\nLuis Espinosa-Anke, Francesco Ronzano, and Horacio Saggion. 2016. TALN at SemEval-2016 task 14: Semantic taxonomy enrichment via sense-based embeddings. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1332\u20131336, San Diego, California. Association for Computational Linguistics.\\n\\nMichael Hanna and David Mare\u010dek. 2021. Analyzing BERT's knowledge of hyponymy via prompting. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 275\u2013282, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\n\\nDevansh Jain and Luis Espinosa Anke. 2022. Distilling hypernymy relations from language models: On the effectiveness of zero-shot taxonomy induction. In Proceedings of the 11th Joint Conference on Lexical and Computational Semantics, pages 151\u2013156, Seattle, Washington. Association for Computational Linguistics.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bresillard, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b.\\n\\nDavid Jurgens and Mohammad Taher Pilehvar. 2016. SemEval-2016 task 14: Semantic taxonomy enrichment. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 1092\u20131102, San Diego, California. Association for Computational Linguistics.\\n\\nEnkelejda Kasneci, Kathrin Sessler, Stefan K\u00fcchemann, Maria Bannert, Daryna Demen tieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan G\u00fcnnemann, Eyke H\u00fcllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, J\u00fcrgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and\"}"}
{"id": "lrec-2024-main-133", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gjergji Kasneci. 2023. Chatgpt for good? on opportunities and challenges of large language models for education. Learning and Individual Differences, 103:102274.\\n\\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Sebastian Ruder, Dani Yogatama, Kris Cao, Tom\u00e1s Kocisk\u00fd, Susan-nah Young, and Phil Blunsom. 2021. Pitfalls of static language modelling. CoRR, abs/2102.01951.\\n\\nMirko Lenz and Ralph Bergmann. 2023. Case-based adaptation of argument graphs with wordnet and large language models. In Case-Based Reasoning Research and Development, pages 263\u2013278, Cham. Springer Nature Switzerland.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.\\n\\nGeorge A Miller. 1998. WordNet: An electronic lexical database. MIT press.\\n\\nViktor Moskvoretskii, Frolov Anton, and Kuznetsov Denis. 2023. Imad: Image-augmented multi-modal dialogue.\\n\\nRoberto Navigli and Simone Paolo Ponzetto. 2010. BabelNet: Building a very large multilingual semantic network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 216\u2013225, Uppsala, Sweden. Association for Computational Linguistics.\\n\\nIrina Nikishina, Polina Chernomorchenko, Anastasiia Demidova, Alexander Panchenko, and Chris Biemann. 2023. Predicting terms in IS-a relations with pre-trained transformers. In Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings), pages 134\u2013148, Nusa Dua, Bali. Association for Computational Linguistics.\\n\\nIrina Nikishina, Varvara Logacheva, Alexander Panchenko, and Natalia Loukachevitch. 2020. Studying taxonomy enrichment on diachronic WordNet versions. In Proceedings of the 28th International Conference on Computational Linguistics, pages 3095\u20133106, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nIrina Nikishina, Mikhail Tikhomirov, Varvara Logacheva, Yuriy Nazarov, Alexander Panchenko, and Natalia V. Loukachevitch. 2022a. Taxonomy enrichment with text and graph vector representations. Semantic Web, 13(3):441\u2013475.\\n\\nIrina Nikishina, Alsu Vakhitova, Elena Tutubalina, and Alexander Panchenko. 2022b. Cross-modal contextualized hidden state projection method for expanding of taxonomic graphs. In Proceedings of TextGraphs-16: Graph-based Methods for Natural Language Processing, pages 11\u201324, Gyeongju, Republic of Korea. Association for Computational Linguistics.\\n\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\\n\\nChao Shang, Sarthak Dash, Md. Faisal Mambub Chowdhury, Nandana Mihindukulasooriya, and Alfio Gliozzo. 2020. Taxonomy construction of unseen domains via graph-based cross-domain knowledge transfer. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2198\u20132208, Online. Association for Computational Linguistics.\\n\\nLinxin Song, Jieyu Zhang, Lechao Cheng, Pengyuan Zhou, Tianyi Zhou, and Irene Li. 2023. Nlpbench: Evaluating large language models on solving NLP problems. CoRR, abs/2309.15630.\\n\\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2018. Conceptnet 5.5: An open multilingual graph of general knowledge.\"}"}
{"id": "lrec-2024-main-133", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kunihiro Takeoka, Kosuke Akimoto, and Masa-fumi Oyamada. 2021. Low-resource taxon-\\nomy enrichment with pretrained language\\nmodels. In Proceedings of the 2021 Confer-\\nce on Empirical Methods in Natural Lan-\\nguage Processing, pages 2747\u20132758, On-\\nline and Punta Cana, Dominican Republic.\\nAssociation for Computational Linguistics.\\n\\nHristo Tanev and Agata Rotondi. 2016. Deftor at SemEval-2016 task 14: Taxonomy en-\\nrichment using definition vectors. In Proceed-\\nings of the 10th International Workshop\\non Semantic Evaluation (SemEval-2016),\\npages 1342\u20131345, San Diego, California.\\nAssociation for Computational Linguistics.\\n\\nAntonio Toral and Rafael Mu\u00f1oz. 2006. A pro-\\posal to automatically build and maintain\\ngazetteers for named entity recognition by\\nusing Wikipedia. In Proceedings of the Work-\\nshop on NEW TEXT Wikis and blogs and\\nother dynamic text sources.\\n\\nHugo Touvron, Louis Martin, Kevin Stone,\\nPeter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal\\nBhargava, Shruti Bhosale, Dan Bikel, Lukas\\nBlecher, Cristian Canton-Ferrer, Moya Chen,\\nGuillem Cucurull, David Esiobu, Jude Fer-\\nnandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman\\nGoyal, Anthony Hartshorn, Saghar Hosseini,\\nRui Hou, Hakan Inan, Marcin Kardas, Viktor\\nKerkez, Madian Khabsa, Isabel Kloumann,\\nArtem Korenev, Punit Singh Koura, Marie-\\nAnne Lachaux, Thibaut Lavril, Jenya Lee,\\nDiana Liskovich, Yinghai Lu, Yuning Mao,\\nXavier Martinet, Todor Mihaylov, Pushkar\\nMishra, Igor Molybog, Yixin Nie, Andrew\\nPoulton, Jeremy Reizenstein, Rashi Rungta,\\nKalyan Saladi, Alan Schelten, Ruan Silva,\\nEric Michael Smith, Ranjan Subramanian,\\nXiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin\\nXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\\nAngela Fan, Melanie Kambadur, Sharan\\nNarang, Aur\u00e9lien Rodriguez, Robert Stojnic,\\nSergey Edunov, and Thomas Scialom. 2023.\\nLlama 2: Open foundation and fine-tuned\\nchat models. CoRR, abs/2307.09288.\\n\\nPaola Velardi, Stefano Faralli, and Roberto\\nNavigli. 2013. OntoLearn reloaded: A\\ngraph-based algorithm for taxonomy induc-\\ntion. Computational Linguistics, 39(3):665\u2013\\n707.\\n\\nXiang Wang, Yanchao Li, Huiyong Wang, and\\nMenglong Lv. 2023. Mkbqa: Question an-\\nswering over knowledge graph based on se-\\nmantic analysis and priority marking method.\\nApplied Sciences, 13(10).\"}"}
