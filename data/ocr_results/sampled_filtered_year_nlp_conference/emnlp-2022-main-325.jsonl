{"id": "emnlp-2022-main-325", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Analyzing and Evaluating Faithfulness in Dialogue Summarization\\nBin Wang\u2020, Chen Zhang\u2020, Yan Zhang\u2020, Yiming Chen\u2020, Haizhou Li\u266e, \u2020, \u00a7\\nNational University of Singapore, Singapore\\n\u266eThe Chinese University of Hong Kong, Shenzhen, China\\n\u00a7Kriston AI, China\\nbwang28c@gmail.com\\n\\nAbstract\\nDialogue summarization is abstractive in nature, making it suffer from factual errors. The factual correctness of summaries has the highest priority before practical applications. Many efforts have been made to improve faithfulness in text summarization. However, there is a lack of systematic study on dialogue summarization systems. In this work, we first perform the fine-grained human analysis on the faithfulness of dialogue summaries and observe that over 35% of generated summaries are faithfully inconsistent respective the source dialogues. Furthermore, we present a new model-level faithfulness evaluation method. It examines generation models with multi-choice questions created by rule-based transformations. Experimental results show that our evaluation schema is a strong proxy for the factual correctness of summarization models. The human-annotated faithfulness samples and the evaluation toolkit are released to facilitate future research toward faithful dialogue summarization. Code available: https://github.com/BinWang28/FacEval.\\n\\n1 Introduction\\nText summarization aims to condense a document into a short paragraph or a single sentence while conveying the core information (El-Kassas et al., 2021). It can be either extractive or abstractive. Extractive summarization methods identify salient sequence spans from the source document and pasting them together (Dorr et al., 2003; Kobayashi et al., 2015; Zhong et al., 2020). Abstractive summarization methods generate completely new summary in a coherent manner (Paulus et al., 2018; Lewis et al., 2020; Zhang et al., 2020; Liu et al., 2021a; Wang et al., 2022). Previous work discovered that abstractive summarization suffers from unfaithful outputs, limiting its applicability in real-world scenarios (Kryscinski et al., 2020; Falke et al., 2019; Zhu et al., 2021a; Ladhak et al., 2022).\\n\\nDialogue:\\nFreddie: Nanna, are you coming to visit us soon?\\nWinnie: Oh darling, Nanna has broken her leg, you'll have to visit me instead.\\nFreddie: I forgot. Well come soon.\\nWinnie: Good, ask Mummy and Daddy and they will come when they can.\\nFreddie: Yes love you. Leg better soon?\\nWinnie: Yes, quite soon. Tell mummy to ring me. Bye darling xxxxx\\n\\nSummaries:\\nHuman: Winnie has broken her leg and will not visit any time soon. Freddie will ask mummy to call Winnie up.\\n\u2713\\nBART: Nanna has broken her leg, so Freddie will have to visit her instead. Nanna will get better soon.\\n\u2713\\nMV-BART: Nanna has broken her leg and Freddie will have to visit Winnie instead. Mummy and Daddy will come to visit them soon.\\n\u2717\\nCoref-BART: Freddie wants to visit Winnie, but Nanna has broken her leg, so he will have to visit her instead. Mummy and Daddy will come when they can.\\n\u2717\\nCondigSum-BART: Winnie's Nanna has broken her leg and Freddie will have to visit her instead.\\n\u2717\\n\\nTable 1: A real example from SAMSum dataset. Span of factual errors are marked with underline.\"}"}
{"id": "emnlp-2022-main-325", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"marization into 6 types. Then, we collect fine-grained factual annotations for human reference and the output of 4 recent dialogue summarization systems ($\\\\S3$). At least two annotators are involved, and a verification process is incorporated to ensure the annotation quality. As a result, our study on human-annotated data suggests that over 35% of the generated dialogue summaries contain at least one factual error. Similar observations have been made in the news summarization domain where 30%-80% of generated text are factually inconsistent (Cao et al., 2018; Pagnoni et al., 2021). More research attention should be made toward faithful dialogue summarization.\\n\\nThe unavailability of faithful evaluation methods hinders the development of effective dialogue summarization models. In this work, we present a model-level evaluation schema, FacEval, targeting dialogue summarisation models' faithfulness ($\\\\S4$). First, we synthesize a set of positive and negative summaries for each dialogue with back-translation or rule-based transformations. Then, a summarization model is asked to distinguish positive and negative summaries based on conditional generation probabilities. More correct judgements indicate the model is more factually competent.\\n\\nTo compare the model-level performance of evaluation methods, we leverage two ad-hoc training schema to synthesize a series of models with different capability ranks. Then, the evaluation methods are used to predict the ranking of trained models. Seven non-factual and factual evaluation methods have been examined, followed by a detailed discussion of their properties. The effectiveness of FacEval is also proven by showing a strong correlation with the factual correctness of summarization models.\\n\\n2 Related Work\\n2.1 Summarization Methods\\nText summarization is one of the most important tasks in natural language generation (NLG). With the development of pre-trained language models, a lot progress has been made to abstractive text summarization (See et al., 2017; Zhang et al., 2020; Liu et al., 2022), especially in news domain (Hermann et al., 2015; Narayan et al., 2018). With the availability of datasets (Carletta et al., 2005; Gliwa et al., 2019; Zhu et al., 2021b), dialogue summarization research has attracted a lot of attention. For dialogue summarization, fine-tuning pre-trained generation models including T5 (Raffel et al., 2020), PEGASUS (Zhang et al., 2020) and BART (Lewis et al., 2020) are served as a strong baseline, where BART achieves the SOTA performance on ROUGE scores. Some recent works consider the dialogue properties for more advanced summarization models. Chen and Yang (2020) and Liu et al. (2021a) incorporate the conversational structures into the semantic encoding process of dialogue. Conversations involve lots of co-references. Therefore, Liu et al. (2021b) proposes injecting co-reference information into the transformer layers by adapting attention maps or through graph convolutional networks (GCN). We include the outputs of recent dialogue summarization models in our analysis.\\n\\n2.2 Faithfulness Analysis\\nPrevious works spot that the factual consistency problem is one key aspect of improving text summarization (Kryscinski et al., 2020; Cao et al., 2020). The analysis of factual errors in summaries is mainly performed in the news domain. Kryscinski et al. (2019) and Falke et al. (2019) conducted the initial crowdsourcing of binary factual annotations and found that nearly 30% of the generated summaries are factually inconsistent. Recent extensions focus on more fine-grained analysis (Cao and Wang, 2021; Pagnoni et al., 2021) and also discovering factual evidences at entity level (Cao et al., 2022) or span level (Huang et al., 2020; Maynez et al., 2020; Goyal and Durrett, 2021).\\n\\nRecently, CONFIT presented the first study on the faithfulness of dialogue summaries (Tang et al., 2022b). Similar to our work, they also define a taxonomy of factual errors and conduct fine-grained annotations. However, they focus on comparing reference summaries and generated summaries without referring to the whole dialogue. It is sub-optimal because the reference summary cannot fully represent the entire dialogue and also can be incorrect according to our analysis in Section 3. Besides, the missing and redundant information is categorized as factual errors, which we consider less proper. More recent advanced dialogue summarization models are also not included in their analysis.\\n\\n2.3 Faithfulness Evaluation\\nThe default evaluation metric for summarization, ROUGE, is based on n-gram overlaps between a generated summary and the corresponding references, rendering it less sensitive for capturing factual\"}"}
{"id": "emnlp-2022-main-325", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fiona doesn't know what she should give to her dad as a birthday gift. He likes military. Jonathan suggests a paintball match.\\n\\nSubObjE: Jonathan doesn't know what she should give to her dad as a birthday gift. He likes military. Jonathan suggests a paintball match.\\n\\nProE: Fiona doesn't know what he should give to her dad as a birthday gift. He likes military. Jonathan suggests a paintball match.\\n\\nNegE: Fiona doesn't know what she should give to her dad as a birthday gift. He hates military. Jonathan suggests a paintball match.\\n\\nParE: Fiona doesn't know what she should give to her dad as a Christmas gift. He likes military. Jonathan suggests a paintball match.\\n\\nHalE: Fiona doesn't know what she should give to her dad as a birthday gift. He likes military. Jonathan invites Fiona to watch a military movie.\\n\\nTable 2: An illustration of the taxonomy on factual error types.\\n\\nFine-grained Faithfulness Analysis\\n\\nPrevious studies of factuality analysis in summarization mainly focus on the news domain. The typology of factual errors for dialogues can be very different. Therefore, we first define a taxonomy of frequently occurred factual errors for dialogue summaries. A fine-grained analysis is then performed by measuring the factual consistency within dialogue summary pairs.\\n\\n3.1 Taxonomy of Factual Errors\\n\\nWe collect the generated summaries using four SOTA dialogue summarization models on the popular dialogue summarization dataset, SAMSum (Gliwa et al., 2019). The selected models are BART (Lewis et al., 2020), MV-BART (Chen and Yang, 2020), Coref-BART (Liu et al., 2021b) and CondigSum-BART (Liu et al., 2021a). We define five most frequently occurred error types in dialogue summaries as below. An example for each error type is shown in Table 2.\\n\\nSubject Object Error (SubObjE): The subject(s) or object(s) involved for an event is (partially) wrong. It includes substitution, addition and deletion of any related subject(s) or object(s).\\n\\nPronoun Error (ProE): Pronoun references are frequently occurred in dialogue summarization. This error includes wrong references and ambiguous ones that cannot be fully understandable relying on the summary.\\n\\nNegation Error (NegE): Dialogues can contain confirmation utterances. This error means that the generated summary makes wrong conclusions when contradictory or unconfirmed events are presented in the dialogue.\\n\\nParticulars Error (ParE): The summary presents related events, but some details are inaccurate or faulty. It can include incorrect information like date, time and location.\\n\\nHallucination Error (HalE): Generation models have the imaginary ability and can be triggered by certain prompt words in the dialogue. The hallucination error refers to the cases where the summary contains events not presented in the dialogue.\\n\\nOther Error (OthE): It is used to classify factual errors that do not belong to any of the above types. Note that the above-mentioned error types are not exclusive to each other. That is, one summary may contain multiple error types.\\n\\nNote that the above-mentioned error types are not exclusive to each other. That is, one summary may contain multiple error types.\"}"}
{"id": "emnlp-2022-main-325", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The proportion of summaries with different types of factual errors. Note that one summary can contain multiple error types.\\n\\nFigure 2: The proportion of summaries with at least one factual error.\\n\\nTherefore, we perform a two-step verification process to ensure the annotation quality. First, each sample is annotated by two distinct annotators. If there is a disagreement about whether a summary contains factual errors, a third annotator is involved in making the final decision while considering inputs from the previous two annotators. As a result, we have collected 750 fine-grained faithfulness annotations from 30 participants.\\n\\n3.3 Results and Analysis\\n\\nThe detailed annotation results are shown in Figure 1. There are several exciting findings: 1) the human annotations contain non-negligible factual errors at around 17%; 2) 36% to 50% of generated summaries from dialogue summarization models contain at least one factual error; 3) three advanced dialogue summarization models perform worse than their baseline on factual consistency.\\n\\nFirst, the popular SAMSum dataset (Gliwa et al., 2019) associates each dialogue with one human-written reference summary. However, we found that 17% of reference summaries have factual errors. Therefore, we encourage people to be aware of the issue, especially for evaluation. It is because the dialogue annotation process for SAMSum only involved one annotator per sample, and no further verification process was executed. We notice that the source of factual errors for human summaries is also different from machine-generated ones. Some factual errors in human-written summaries are caused by typos, which rarely occur in machine-generated summaries.\\n\\nFor dialogue summarization models, we found that 35%-50% of generated summaries contain factual errors. The most frequent error types are SubObjE and ParE. Because dialogue often involves scattered information exchange with multiple speakers in multiple turns, it is very challenging to accurately locate who and whom in who-did-what-to-whom. That is the leading cause of SubObjE. ParE is the second most frequent error type, indicating that the generated summaries express the same topic but do not accurately capture the details. OthE occurs less frequently. It shows that our taxonomy of factual errors can cover the most frequent error types for dialogue summarization.\\n\\nSurprisingly, we found that MV-BART, Coref-BART and CondigSum-BART perform even worse than the baseline model, with an increase of around 10% overall factual error rate. They are accepted as more advanced summarization models and perform better on ROUGE scores. It indicates that enhancing topical information is not necessarily contributing much to factuality (Chen and Yang, 2020; Liu et al., 2021a). Coref-BART aims to improve BART with co-reference information (Liu et al., 2021b). However, our result shows it does not bring obvious benefits. In conclusion, we encourage the future development of summarization models to pay more attention to the factuality perspective, and a more diverse evaluation schema.\"}"}
{"id": "emnlp-2022-main-325", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Model-level Faithfulness Evaluation\\n\\nSome efforts have been made toward sample-level factual error evaluation. An example is shown in Figure 3. The sample-level evaluation methods are model-agnostic and examine a model solely based on its output sequences. Most existing evaluation methods, including ROUGE score, human evaluation and recent factual evaluation methods, belong to this type. One ultimate goal for factuality evaluation is to discriminate better summarization models. We propose directly probing models' generation probability with a constrained search space. First, FacEval generates a set of positive and negative samples with variant factual errors by rule-based transformations. Then, the generation probabilities of positive and negative summaries are compared for each dialogue. A better summarization model should be more likely to generate positive summaries than negative ones.\\n\\n4.1 Dialogue-summary Pair Generation\\n\\nWe design transformations to synthesize negative samples with factual errors. Given the source and target text, one or more modifications are performed to the target text while referring to the information of the source text. It is because the frequently occurred errors are conceptual confusions from the source. Our designed transformations are listed as follows:\\n\\n- Speaker Swap (SS): We first spot the name of speakers from the source text by colon symbol and then swap the names at the target text.\\n- Entity / Pronoun / Date / Number Swap (ES/PS/DS/NS): An NER system is first applied to both source and target text. The entities from the target text are randomly swapped with entities from the source text if they share the same entity type.\\n- Negation (NG): Negation is performed using a set of hand-crafted rules. Auxiliary verbs are first scanned. Then, positive verbs are negated by adding not or n't. Similarly, negative sentences are inverted by negation removal.\\n\\nFirst, we paraphrase the summary to create more positive samples through back-translation (BT). The Google Cloud API is leveraged for this task. Then, we generate new summaries with factual errors by corrupting positive summaries, which means the summaries are treated as the target text, and the dialogue is the source text. Ideally, the negative summaries should be prone to errors generated in real-world scenarios. Therefore, our designed transformations try to mimic that. In the context of the analysis presented in Section 3, we have the following list of correspondences: 1) SS-SubObjE; 2) PS-ProE; 3) NG-NegE; 4) ES/DS/NS-ParE.\\n\\n4.2 Comparison of Generation Probabilities\\n\\nAn illustration of probability comparison is shown in Figure 4. Given a dialogue $D$, a summary $S = [y_1, ..., y_L]$ and a summarization model $f_s(\u00b7)$, we can compute a generation score ($GS$) for $D$-$S$ pair.\"}"}
{"id": "emnlp-2022-main-325", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from the generation probability:\\n\\n\\\\[ GS(S|D) = 1 \\\\]\\n\\\\[ L^\\\\alpha \\\\log P(y_1, ..., y_L|D) = 1 \\\\]\\n\\\\[ L^\\\\alpha \\\\sum_{t=1}^{L} \\\\log P(y_t|y_1, ..., y_{t-1}, D) \\\\]\\n\\nwhere the generation probability for each token is as follows:\\n\\n\\\\[ P(y_t|y_1, ..., y_{t-1}, D) = P(f_s(y_1, ..., y_{t-1}, D) = y_t) \\\\]\\n\\nWe leverage the above generation score from the decision process of beam search algorithm (Graves, 2012), where the sequence length is taken into consideration. In default, we set the length penalty parameter \\\\( \\\\alpha \\\\) as 1.0.\\n\\nFor dialogue \\\\( D_i \\\\), there is positive summary set \\\\( S = [S_1, ..., S_M] \\\\) and negative summary set \\\\( \\\\hat{S} = [\\\\hat{S}_1, ..., \\\\hat{S}_N] \\\\). We evaluate the number of times the positive samples have higher scores than the negative samples concerning the same dialogue. The factuality score (\\\\( FS(f_s) \\\\)) of model \\\\( f_s(\\\\cdot) \\\\) is then computed as follows:\\n\\n\\\\[ FS(f_s) = \\\\frac{1}{|D|} \\\\sum_{i=1}^{MN} \\\\frac{1}{M} \\\\sum_{m=1}^{N} \\\\frac{1}{1} \\\\left[ GS(S_m|D_i) > GS(\\\\hat{S}_n|D_i) \\\\right] \\\\]\\n\\nwhere \\\\( |D| \\\\) is the number of dialogues.\\n\\n### 4.3 Evaluation Preparation\\n\\nA series of models need to be prepared with different faithfulness capabilities to evaluate the effectiveness of model-level evaluation methods. One option is to collect as many well-trained models as possible and refer to human annotations to rank models based on factuality. However, it is hard to reach a high agreement and may not be trustworthy with limited annotators, as indicated by Falke et al. (2019) and Kryscinski et al. (2020). Therefore, instead, we construct a series of models using the following two ad-hoc methods:\\n\\n- **Limited data training (LDT).**\\n  - One joint agreement is that more training data lead to better model performance. Therefore, we train 20 models using different proportions of the training data from 5% to 100%.\\n\\n- **Mixed data training (MDT).**\\n  - In this setting, we randomly replace the human-labelled training samples with noisy ones. The noisy samples are created by corrupting only the dialogue using transformations introduced in Section 4.1. Here, the source and target are both the dialogue. The trained model is more likely to be confused and generate more factuality errors with noisy data. Here, we obtain 21 models with different replacement ratios from 100% to 0%.\\n\\nLDT will cause a model to be less competent for generation in all aspects. In comparison, MDT will lead the model to generate summaries with more factuality errors while less affecting other properties like fluency. Therefore, we expect a better factuality evaluator to correlate more with MDT models. All correlations are computed on model-level instead of sample-level judgements.\\n\\n### 5 Experiments\\n\\n#### 5.1 Experimental Settings\\n\\nSAMSum dataset (Gliwa et al., 2019) is used for all experiments. It consists of 16,369 dialogue-summary pairs written by expert linguistics. One human-written reference summary is provided for each dialogue. The detailed dataset statistics are listed in Table 3. The samples from the test set are used for all evaluation methods.\\n\\nFor backbone models, we exam with BART Large, BART Base (Lewis et al., 2020), T5 Base and T5 Small (Raffel et al., 2020), which are SOTA summarization models. Each model is trained with both LDT and MDT methods. As a result, we obtained 164 trained models, divided into eight groups. The models in each group are associated with increasing levels of capabilities. The Spearman's rank correlation coefficient (\\\\( \\\\rho \\\\)) between these models and evaluation scores is reported. For sample-based evaluation methods, the scores on all test set samples are averaged as the model-level performance. We ensure...\"}"}
{"id": "emnlp-2022-main-325", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Detailed correlation analysis between model series and negative sample types. For each column, one negative type is involved. \u2018all\u2019 indicates the usage of all negative types.\\n\\n5.2 Results and Analysis\\n\\nTable 4 shows the fine-grained results of FacEval. First, we found that FacEval has a higher correlation with MDT models than LDT models. The LDT models are less competent in all aspects as fewer data are involved with training. The generated summaries are weaker in multiple elements, including factuality, fluency, coherence, and granularity. In contrast, the MDT models mainly deteriorate in factuality with factually corrupted training data. Therefore, it is desired that FacEval shows a higher correlation to MDT models.\\n\\nSecond, when considering each negative sample type, a relatively higher correlation is shown with negation (NG), pronoun swap (PS) and speaker swap (SS). It is because more comparison pairs are created with these methods. Also, for chit-chat dialogues, almost all summaries contain reasoning concerning speakers and personnel in the dialogue. And the confirmation of action is happening in multiple utterances. As a result, these several error types are more commonly witnessed in dialogue summarization, as illustrated in Figure 1. In contrast, the negative pairs generated by entity swap (ES), date swap (DS) and number swap (NS) show a lower correlation. It is because these samples are more related to particular errors which appear in various formats and are more challenging to simulate. Even though solely considering these samples shows a lower correlation, we still include them in the overall comparison process to have a more comprehensive evaluation.\\n\\n5.3 Comparison with Other Metrics\\n\\nWe include a list of popular evaluation methods for summarization to compare our evaluation schema with existing ones. It contains three generic evaluation methods and four dedicated faithfulness evaluation methods.\\n\\n5.3.1 Baseline Metrics\\n\\nThree generic evaluation methods are as follows: ROUGE (Lin, 2004) score is the default evaluation metric for summarization. We experiment with the F-measure of ROUGE-1, ROUGE-2 and ROUGE-L, which are derived from the uni-gram overlap, the bi-grams overlap and the longest common subsequence (LCS) between generated and reference summaries, respectively.\\n\\nBLEU (Papineni et al., 2002) score is the primary evaluation metric for machine translation. It is mainly designed for corpus-level similarity computation derived from n-gram overlaps. In the following experiments, we report the most commonly used BLEU-4 score.\\n\\nBERTScore (Zhang* et al., 2020) leverages the pre-trained contextual embeddings from BERT and computes the similarity between text sequences by matching words in candidate and reference by cosine similarity.\\n\\nFour faithful evaluation methods are as follows: FactCC v1 (Kryscinski et al., 2020) first augment summaries by applying rule-based transformations.\"}"}
{"id": "emnlp-2022-main-325", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|           | ROUGE-1 | ROUGE-2 | ROUGE-L | BLEU | BERTScore | FactCC v1 | FactCC v2 | FEQA | NLI | FacEval (ours) |\\n|-----------|---------|---------|---------|------|-----------|-----------|----------|------|-----|----------------|\\n| LDT       | 81.35   | 86.77   | 75.64   | 91.88| 88.87     | \u2014         | 82.39    | 6.02 | 39.40| 83.70          |\\n| MDT       | 95.79   | 96.84   | 96.24   | 90.08| 97.14     | \u2014         | 96.01    | 30.08| 31.28| 99.40          |\\n| Avg. Model| 94.44   | 96.39   | 96.39   | 92.33| 95.79     | \u2014         | 99.27    | -60.15| 93.08| 89.62          |\\n\\nTable 5: Comparison of a series of automatic evaluation metrics. The result shown is Spearman's rank correlation between model ranks and predicted scores.\\n\\nIt is initially trained in the news summarization domain. FactCC v2 is an adapted FactCC v1 to the dialogue domain by us. The negative summaries are generated using our transformations discussed in Section 4.1. We train a T5 Small model as the classifier and take dialogue and summary as input to predict their consistency.\\n\\nFEQA (Durmus et al., 2020) is a question generation and answering method for faithfulness evaluation. It first extracts question-answer pairs from summaries with pre-trained models. Then, a QA model pulls answers from the document with the same questions. A matching method is deployed to measure the similarity between both answer responses from the summary and document as the factuality score. Note that the model is designed for documents.\\n\\nNLI (Falke et al., 2019) is an entailment-based method which takes the maximal entailment probability between summary and document sentence as the factual consistency score. As no dialogue-based entailment model is available, we compute the entailment probability between reference and generated summaries with a BERT-based entailment model trained on SNLI and MultiNLI datasets.\\n\\n5.3.2 Results and Analysis\\n\\nThe experimental results are shown in Table 5.\\n\\nNon-factual Evaluator: The non-factual evaluation methods measure the similarity between reference and generated summaries. ROUGE and BLEU are derived from n-gram overlaps, which indicate the overall generation quality. It is expected that evaluators have a reasonable correlation with LDT models as training with fewer data will result in quality degradation of the summary in all aspects. For MDT models, they also show a good correlation. We observe that R-2 and R-L are better indicators than R-1 for factuality evaluation. It is because simply replacing isolated tokens can easily change the factual correctness of a summary without much influence on the R-1 score.\\n\\nFactual Evaluator: As FactCC v1 is trained for the news summarization, we found that the released model is incapable of making predictions for dialogues. Similarly, FEQA is not a good indicator of model performance because the question and answer generation models are optimized for documents, which limits its transferability to the dialogue domain. In comparison, FactCC v2 and NLI are better evaluation methods for factuality and can make good predictions on MDT models.\\n\\nFacEval Properties: FacEval is the only model-level evaluation schema. The examined model requires reasonable predictions on single sentences and differentiation between positive and negative pairs. Therefore, FacEval shows a strong correlation with LDT and MDT models. The exceptional performance on MDT models indicates that FacEval can effectively reflect model's capability on factuality.\"}"}
{"id": "emnlp-2022-main-325", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Benchmarking results on 4 different models\\n\\n| Model | # Params | All | NG  | PS  | SS  | ES  | DS  | NS  |\\n|-------|----------|-----|-----|-----|-----|-----|-----|-----|\\n| BART Large | 400M     | 86.87 | 89.03 | 89.08 | 84.19 | 89.46 | 84.22 | 68.37 |\\n| BART Base | 140M     | 85.73 | 87.73 | 89.43 | 79.46 | 89.96 | 82.50 | 68.63 |\\n| T5 Base     | 220M     | 85.32 | 86.81 | 88.27 | 79.92 | 90.71 | 84.90 | 67.75 |\\n| T5 Small    | 60M      | 79.87 | 82.09 | 85.94 | 69.68 | 87.17 | 79.88 | 62.96 |\\n\\n5.3.3 Benchmarking and Future Directions\\n\\nIt is beneficial to provide benchmarking performance and analysis on popular dialogue summarization models. As discussed in Sec. 3, dedicated dialogue summarization models do not outperform their baseline models in terms of faithfulness. Therefore, we evaluate on T5 and BART models instead. The benchmarking results are shown in Tab 6.\\n\\nThere are several interesting findings. First, BART Large has the largest model size as well as the overall best performance. We can also conclude that larger pre-trained models are more faithful based on our evaluation. Second, BART model is generally better than T5 in factuality with model size taken into consideration. This may be because that BART is designed for the generation with various denoising objectives, while T5 is a sequence-to-sequence model for different tasks including but not limited to generation. Third, from fine-grained analysis, we can see that speaker information (from SS) is a major challenge for dialogue summarization. This is because dialogue involves multiple speakers and their roles are tightly involved in the ideal summarization. Therefore, how to improve the model's understanding capability on speaker roles is an interesting direction to explore (Liu et al., 2021b). Meantime, because some faithful errors are coming from lack of commonsense for existing models (Wang et al., 2021). How to effectively combine hidden semantics (Wang and Kuo, 2020) and well-structured knowledge (Ge et al., 2022) are also worth exploration.\\n\\n6 Conclusion\\n\\nWe believe our faithfulness analysis and evaluation method can facilitate the development of dialogue summarization systems. Instead of measuring faithfulness on generated summaries, we directly assess the model's capability by multi-choice questions. We expect FacEval to be effectively extended to other generation scenarios.\\n\\n7 Limitations\\n\\nThe testing samples used in our method are obtained by rule-based transformations of the reference and back-translated summaries. It is still limited to the types of transformations designed. More transformation methods need to be proposed to have a comprehensive evaluation. To obtain more natural summaries, we can gather generated summaries and perform annotation by humans. The model can be evaluated in more aspects and closer to real-world scenarios with more available samples.\\n\\nVerifying the effectiveness of the model-level evaluation schema requires various models and their corresponding rankings. However, such model rankings are currently unavailable because 1) there are not enough varieties of dialogue summarization models as it is still a developing field; 2) the annotations on the faithfulness of dialogue summaries are not adequate. Therefore, in this work, we refer to heuristic methods to manually create a series of models with desired capability levels. When new evaluators are proposed, the best practice is to leverage model-level human rankings for performance benchmarking.\\n\\nAcknowledgement\\n\\nThis research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project No. A18A2b0046) and Science and Engineering Research Council, Agency of Science, Technology and Research (A*STAR), Singapore, through the National Robotics Program under Human-Robot Interaction Phase 1 (Grant No. 192 25 00054). This work is also supported by the Internal Project Fund from Shenzhen Research Institute of Big Data under Grant T00120220002.\"}"}
{"id": "emnlp-2022-main-325", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meng Cao, Yue Dong, and Jackie Cheung. 2022. Halucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3340\u20133354, Dublin, Ireland. Association for Computational Linguistics.\\n\\nMeng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit Cheung. 2020. Factual error correction for abstractive summarization models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6251\u20136258, Online. Association for Computational Linguistics.\\n\\nShuyang Cao and Lu Wang. 2021. CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633\u20136649, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In thirtys-second AAAI conference on artificial intelligence.\\n\\nJean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa Kronenthal, et al. 2005. The ami meeting corpus: A pre-announcement. In International workshop on machine learning for multimodal interaction, pages 28\u201339. Springer.\\n\\nJiaao Chen and Diyi Yang. 2020. Multi-view sequence-to-sequence models with conversational structure for abstractive dialogue summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4106\u20134118, Online. Association for Computational Linguistics.\\n\\nBonnie Dorr, David Zajic, and Richard Schwartz. 2003. Hedge trimmer: A parse-and-trim approach to headline generation. In Proceedings of the HLT-NAACL03 Text Summarization Workshop, pages 1\u20138.\\n\\nEsin Durmus, He He, and Mona Diab. 2020. FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055\u20135070, Online. Association for Computational Linguistics.\\n\\nWafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. 2021. Automatic text summarization: A comprehensive survey. Expert Systems with Applications, 165:113679.\\n\\nAlexander R. Fabbri, Wojciech Krysci\u00b4nski, Bryan Mc Cann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391\u2013409.\\n\\nTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214\u20132220, Florence, Italy. Association for Computational Linguistics.\\n\\nXiachong Feng, Xiaocheng Feng, and Bing Qin. 2021. A survey on dialogue summarization: Recent advances and new frontiers. arXiv preprint arXiv:2107.03175.\\n\\nXiou Ge, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo. 2022. CompoundE: Knowledge graph embedding with translation, rotation and scaling compound operations. arXiv preprint arXiv:2207.05324.\\n\\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong Kong, China. Association for Computational Linguistics.\\n\\nTanya Goyal and Greg Durrett. 2020. Evaluating factuality in generation with dependency-level entailment. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3592\u20133603, Online. Association for Computational Linguistics.\\n\\nTanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1449\u20131462, Online. Association for Computational Linguistics.\\n\\nAlex Graves. 2012. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711.\\n\\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in neural information processing systems, 28.\\n\\nDandan Huang, Leyang Cui, Sen Yang, Guangsheng Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. What have we achieved on text summarization? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 446\u2013469, Online. Association for Computational Linguistics.\\n\\nHayato Kobayashi, Masaki Noguchi, and Taichi Yatsuka. 2015. Summarization based on embedding distributions. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4906.\"}"}
{"id": "emnlp-2022-main-325", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-325", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
