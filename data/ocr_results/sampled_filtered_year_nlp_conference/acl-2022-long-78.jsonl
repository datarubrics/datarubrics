{"id": "acl-2022-long-78", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HiTab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation\\nZhoujun Cheng1\u2217, Haoyu Dong2\u2217\u2020, Zhiruo Wang3\u2217, Ran Jia2, Jiaqi Guo4, Yan Gao2, Shi Han2, Jian-Guang Lou2, Dongmei Zhang2\\n\\n1Shanghai Jiao Tong University, 2Microsoft Research Asia, 3Carnegie Mellon University, 4Xi'an Jiaotong University\\n\\n{blankcheng@sjtu.edu.cn, zhiruow@cs.cmu.edu, jasperguo2013@stu.xjtu.edu.cn}\\n\\nAbstract\\n\\nTables are often created with hierarchies, but existing works on table reasoning mainly focus on flat tables and neglect hierarchical tables. Hierarchical tables challenge table reasoning by complex hierarchical indexing, as well as implicit relationships of calculation and semantics. We present a new dataset, HiTab, to study question answering (QA) and natural language generation (NLG) over hierarchical tables. HiTab is a cross-domain dataset constructed from a wealth of statistical reports and Wikipedia pages, and has unique characteristics: (1) nearly all tables are hierarchical, and (2) questions are not proposed by annotators from scratch, but are revised from real and meaningful sentences authored by analysts. (3) To reveal complex numerical reasoning in analysis, we provide fine-grained annotations of quantity and entity alignment. Experimental results show that HiTab presents a strong challenge for existing baselines and a valuable benchmark for future research. Targeting hierarchical structure, we devise an effective hierarchy-aware logical form for symbolic reasoning over tables. Furthermore, we leverage entity and quantity alignment to explore partially supervised training in QA and conditional generation in NLG, which largely reduces spurious predictions in QA and meaningless descriptions in NLG. The dataset and code are available at https://github.com/microsoft/HiTab.\\n\\n1 Introduction\\n\\nIn recent years, there are a flurry of works on reasoning over semi-structured tables, e.g., answering questions over tables (Yu et al., 2018; Pasupat and Liang, 2015) and generating fluent and faithful text from tables (Lebret et al., 2016; Parikh et al., 2020).\\n\\n\u2217 \u2217 Equal contributions. Work done during Zhoujun and Zhiruo's internship at Microsoft Research Asia.\\n\u2020 \u2020 Corresponding author.\\n\\n1 https://www.nsf.gov/statistics/2019/nsf19319/\\n\\n\u2022 Teaching assistantships were most commonly reported as the primary mechanism of support for master's students (11%).\\n\\nFigure 1: A hierarchical table and accompanied descriptions in a National Science Foundation report.\\n\\nBut they mainly focus on simple flat tables and neglect complex tables, e.g., hierarchical tables. A table is regarded as hierarchical if its header exhibits a multi-level structure (Lim and Ng, 1999; Chen and Cafarella, 2014; Wang et al., 2020). Hierarchical tables are widely used, especially in data products, statistical reports, and research papers in government, finance, and science-related domains. Hierarchical tables challenge QA and NLG due to: (1) Hierarchical indexing. Hierarchical headers, such as D2:G3 and A4:A25 in Figure 1, are informative and intuitive for readers, but make cell selection much more compositional than flat tables, requiring multi-level and bi-dimensional indexing. For example, to select the cell E5 (\u201c66.6\u201d), one needs to specify two top header cells, \u201cMaster\u2019s\u201d and \u201cPercent\u201d, and two left header cells, \u201cAll full-time\u201d and \u201cSelf-support\u201d. (2) Implicit calculation relationships among quantities. In hierarchical tables, it is common to insert aggregated rows and columns without explicit indications, e.g., total (columns B,D,F and rows 4,6,7,20) and proportion (columns C,E,G), which challenge precise numerical reasoning.\\n\\n1094\"}"}
{"id": "acl-2022-long-78", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implicit semantic relationships among entities. There are various cross-row, cross-column, and cross-level entity relationships, but lack explicit indications, e.g., \\\"source\\\" and \\\"mechanism\\\" in A2 describe A6:A19 and A20:A25 respectively, and D2 (\\\"Master\u2019s\\\") and F2 (\\\"Doctoral\\\") can be jointly described by a virtual entity, \\\"Degree\\\". How to identify semantic relationships and link entities correctly is also a challenge.\\n\\nIn this paper, we aim to build a dataset for hierarchical table QA and NLG. But without sufficient data analysts, it's hard to ensure questions and descriptions are meaningful and diverse (Gururangan et al., 2018; Poliak et al., 2018). Fortunately, large amounts of statistical reports are public from a variety of organizations (StatCan; NSF; Census; CDC; BLS; IMF), containing rich hierarchical tables and textual descriptions. Take Statistics Canada (StatCan) for example, it consists of 6,039 reports in 27 domains authored by over 1,000 professionals. Importantly, since both tables and sentences are authored by domain experts, sentences are natural and reflective of real understandings of tables.\\n\\nTo this end, we propose a new dataset, HiTab, for QA and NLG on hierarchical tables. (1) All sentence descriptions of hierarchical tables are carefully extracted and revised by human annotators. (2) It shows that annotations of fine-grained and lexical-level entity linking significantly help table QA (Lei et al., 2020; Shi et al., 2020), motivating us to align entities in text with table cells. In addition to entity, we believe aligning quantities (Ibrahim et al., 2019), especially composite quantities (computed by multiple cells), is also important for table reasoning, so we annotate underlying numerical relationships between quantities in text and table cells, as Table 1 shows. (3) Since real sentences in statistical reports are natural, diverse, and reflective of real understandings of tables, we devise a process to construct QA pairs based on existing sentence descriptions instead of asking annotators to propose questions from scratch.\\n\\nHiTab presents a strong challenge to state-of-the-art baselines. For the QA task, MAPO (Liang et al., 2018) only achieves 29.2% accuracy due to the ineffectiveness of the logical form customized for flat tables. To leverage the hierarchy for table reasoning, we devise a hierarchy-aware logical form for table QA, which shows high effectiveness. We propose partially supervised training given annotations of linked mentions and formulas, which helps models to largely reduce spurious predictions and achieve 45.1% accuracy. For the NLG task, models also have difficulties in understanding deep hierarchies and generate complex analytical texts. We explore controlled generation (Parikh et al., 2020), showing that conditioning on both aligned cells and calculation types helps models to generate meaningful texts.\\n\\n2 Dataset Construction and Analysis\\n\\nWe design an annotation process with six steps. To well-handle the annotation complexity, we recruit 18 students or graduates (13 females and 5 males) in computer science, finance, and English majors from top universities, and provide them with comprehensive online training, documents, and QAs. The annotation totally costs 2,400 working hours. We will discuss the ethical considerations in Section 8.\\n\\n2.1 Hierarchical Table Collection\\n\\nWe select two representative organizations, Statistics Canada (StatCan) and National Science Foundation (NSF), that are rich of statistical reports. Different from Census; CDC; BLS; IMF that only provide PDF reports where table hierarchies are hard to extract precisely (Schreiber et al., 2017), StatCan and NSF also provide reports in HTML, from which cell information such as text and formats can be extracted precisely using HTML tags. First, we crawl English HTML statistical reports published in recent five years from StatCan (1,083 reports in 27 well-categorized domains) and NSF (208 reports from 11 organizations in science foundation domain). We merge StatCan and NSF and get the combination of various domains. In addition, ToTTo contains a small proportion (5.03%) of hierarchical tables, so we include them to cover more domains from Wikipedia. To keep the balance between statistical reports and Wikipedia pages, we include random 1,851 tables (50% of our dataset) from ToTTo. Next, we transform HTML tables to spreadsheet tables using a preprocessing script. Since spreadsheet formula is easy to write, execute, and check, the spreadsheet is naturally a great annotation tool to align quantities and answer questions. To enable correct formula execution, we normalize quantities in data cells by excluding surrounding superscripts, internal commas, etc. Extremely small or large tables are filtered out (Appendix A.1 gives more details).\"}"}
{"id": "acl-2022-long-78", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Sentence Extraction and Revision\\n\\nIn this step, annotators manually go through statistical reports and extract sentence descriptions for each table. Sentences consisting of multiple semantic-independent sub-sentences will be carefully split into multiple ones. Annotators are instructed to eliminate redundancy and ambiguity in sentences through revisions including decontextualization and phrase deletion (Parikh et al., 2020).\\n\\nFortunately, most sentences in statistical reports are clean and fully supported by table data, so few revisions are needed to get high-quality text.\\n\\nOperators and templates (ranges are placeholders)\\n- \\\\text{opposite, percent} =-A5, =B2%\\n- \\\\text{kth-argmax/argmin} =XLOOKUP(SMALL(D1:D3, k), D1:D3, A1:A3).\\n- \\\\text{pair-argmax/argmin} =IF(B1 \\\\text{ \\\\textgreater} B2, A1, A2)\\n- \\\\text{sum, average} =SUM(D2:D4), =AVERAGE(D2:D4)\\n- \\\\text{max, count} =MAX(D2:D4), =COUNT(D2:D4)\\n- \\\\text{diff, div} =D3-D4, =D3/D4\\n\\nTable 2: Example operators and formula templates.\\n\\n2.3 Entity and Quantity Alignment\\n\\nIn this phase, annotators are instructed to align mentions in text with corresponding cells in tables. It has two parts, entity alignment and quantity alignment, as shown in Table 1. For entity alignment, we record the mappings from entity mentions in text to corresponding cells. Single-cell quantity mentions can be linked similar with entity mentions, but composite quantity mentions are calculated from two or more cells through operators like max/sum/div/diff (Table 2). The spreadsheet formula is powerful and easy-to-use for tabular data calculation, so we use the formula to record the calculations process of composite quantities in text, e.g., '10 points higher' (=G23-G24).\\n\\n2.4 Converting Sentences to QA Pairs\\n\\nExisting QA datasets instruct annotators to propose questions from scratch, but it's hard to guarantee the meaningfulness and diversity of proposed questions. In HiTab, we simply revise declarative sentences into QA pairs. For each sentence, annotators need to identify a target key part to question about (according to the underlying logic), then convert it to the QA form. All questions are answered by formulas that reflect the numerical inference process. For example, the 'XLOOKUP' operator is frequently used to retrieve the header cells of superlatives, as shown in Table 1. To keep sentences as natural as they are, we do not encourage unnecessary sentence modification during the conversion. If an annotator finds multiple ways to question regarding a sentence, he/she only needs to choose one way that best reflects the overall meaning.\\n\\n2.5 Regular Inspections and the Final Review\\n\\nWe ask the two most experienced annotators to perform regular inspections and the final review. (1) In the labeling process, they regularly sample annotations (about 10\\\\%) from all annotators to give timely feedback on labeling issues. (2) Finally, they review all annotations and fix labeling errors. Also, to assist the final review, we write a script to automatically identify spelling issues and formula issues. To double-check the labeling quality before the final review, we study the agreement of annotators by collecting and comparing annotations on randomly sampled 50 tables from two annotators. It shows 0.89 and 0.82 for quantity and entity alignment in Fleiss Kappa respectively, which are regarded as \u201calmost perfect agreement\u201d (Landis and Koch, 1977), and 64.5 in BLEU-4 after sentence revision, which also indicates high agreement. We further show annotation artifacts are substantially avoided.\"}"}
{"id": "acl-2022-long-78", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset        | Data source | Fine-grained alignment | QA and NLG tasks | Table Question | Real sentences | Entity Quantity | QA NLG Questions | Words per Sentences or sentence revised per table question |\\n|---------------|-------------|------------------------|------------------|----------------|----------------|----------------|------------------|----------------------------------------------------------|\\n| **WTQ** (Pasupat and Liang, 2015) | 2,108 Wikipedia | Post-created | - | - | Yes | - | 22,033 | 10.0 |\\n| **WikiSQL** (Zhong et al., 2017) | 26,521 Wikipedia | Post-created | - | - | Yes | - | 80,654 | 11.7 |\\n| **Spider** (Yu et al., 2018) | 1,020 College data, WikiSQL | Post-created | - | - | Yes | - | 10,181 | 13.2 |\\n| **HybridQA** (Chen et al., 2020b) | 13,000 Wikipedia | Post-created | - | - | Yes | - | 69,611 | 18.9 |\\n| **TAT-QA** (Zhu et al., 2021) | 2,757 Financial reports (PDF) | Post-created | - | - | Yes | - | 16,552 | 12.5 |\\n| **FinQA** (Chen et al., 2021) | 2,776 Financial reports (PDF) | Post-created | - | - | Yes | - | 8,281 | 16.6 |\\n| **DART** (Nan et al., 2020) | 5,623 WTQ, WikiSQL,... | Post-created | - | - | - | Yes | - | - |\\n| **LogicNLG** (Chen et al., 2020a) | 7,392 Wikipedia | Post-created | - | - | - | Yes | - | - |\\n| **ToTTo** (Parikh et al., 2020) | 83,141 Wikipedia | Pre-existing | 1.4 | - | - | Yes | - | - |\\n| **NumericNLG** (Suadaa et al., 2021) | 1,300 Scientific papers (ACL) | Pre-existing | 3.8 | - | - | Yes | - | - |\\n\\n**Figure 2:** Distribution of domains and operations in StatCan and NSF.\"}"}
{"id": "acl-2022-long-78", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and thus cannot perform arithmetic operations on cells in the same row. However, a row in hierarchical tables is not necessarily a subject or record, thus operations can be applied on cells in the same row. Motivated by this, we define region as our operating object, which is a data region in table indexed by both left and top headers (e.g., B6:C19 is a rectangular region indexed by A6,B2). The logical form execution process is divided into two phases: region selection and region operation.\\n\\nRegion Selection\\nWe design two functions \\\\((\\\\text{filter tree} \\\\ h)\\\\) and \\\\((\\\\text{filter level} \\\\ l)\\\\) to do region selection, where \\\\(h\\\\) is a header, \\\\(l\\\\) is a level. Functions can be applied sequentially: the subsequent function applies on the return region of the previous function. \\\\((\\\\text{filter tree} \\\\ h)\\\\) selects a sub-tree region according to a header cell \\\\(h\\\\): if \\\\(h\\\\) is a leaf header (e.g., A8), the selected region should be the row/column indexed by \\\\(h\\\\) (row 8); if \\\\(h\\\\) is a non-leaf header (e.g., A7), the selected region should be the rows/columns indexed by both \\\\(h\\\\) and its children headers (row 7-16). \\\\((\\\\text{filter level} \\\\ l)\\\\) selects a sub-tree from the input tree according to a level \\\\(l\\\\) and return the sub-region indexed by headers on level \\\\(l\\\\).\\n\\nThese two functions mitigate the aforementioned three challenges: (1) hierarchical indexing is achieved by applying these two functions sequentially; (2) with \\\\((\\\\text{filter level} \\\\ l)\\\\), data with different calculation types (e.g., rows 4-5) will not be co-selected, thus not incorrectly operated together; (3) level-wise semantics can be captured by aggregating header cell semantics (e.g., embeddings) on this level. Some logical form execution examples are shown in Appendix C.2.\\n\\nRegion Operation\\nOperators are applied on the selected region to produce the answer. We define 19 operators, mostly following MAPO (Liang et al., 2018), and further include some operators (e.g., difference rate) for hierarchical tables. Complete logical form functions are shown in Appendix C.1.\\n\\n3.2 Experimental Setup\\n3.2.1 Baselines\\nWe present baselines in two branches. One is logical form-based semantic parsing, and the other is end-to-end table parsing without logical forms. Neural Symbolic Machine (Liang et al., 2017) is a powerful semantic parsing framework consisting of a programmer to generate programs from NL and save intermediate results, and a computer to execute programs. We replace the LSTM encoder with BERT (Devlin et al., 2018), and implement a lisp interpreter for our logical forms as executor. Table is linearized by placing headers in level order, which is shown in detail in Appendix C.4.\\n\\nTaPas (Herzig et al., 2020) is a state-of-the-art end-to-end table parsing model without generating logical forms. Its power to select cells and reason over tables is gained from its pretraining on millions of tables. To fit TaPas input, we convert hierarchical tables into flat ones following WTQ (Pasupat and Liang, 2015). Specifically, we unmerge the cells spanning many rows/columns on left/top headers and duplicate the contents into unmerged cells. The first top header row is specified as column names.\\n\\n3.2.2 Weak Supervision\\nIn weak supervision, the model is trained with QA pairs, without golden logical forms. For NSM, we compare three widely-studied learning paradigms: MML (Dempster et al., 1977) maximizes the marginal likelihood of observed programs. REINFORCE (Williams, 1992) maximizes the reward of on-policy samples. MAPO (Liang et al., 2018) learns from programs both inside and outside buffer, and samples efficiently by systematic exploration. Since these methods require consistent programs for learning or warm start, we randomly search 15,000 programs per sample before training. The pruning rules are shown in Appendix C.3. Finally, 6,12 consistent programs are found per sample. For TaPas, we use the pre-trained version and follow its weak supervised training process on WTQ.\\n\\n3.2.3 Partial Supervision\\nGiven labeled entity links, quantity links, and calculations (from the formula), we further explore to guide training in a partially supervised way. These three annotations indicate selected headers, region, and operators in QA. For NSM, we exploit them to prune spurious programs, i.e., incorrect programs that accidentally produce correct answers, in two ways. (1) When searching consistent programs, besides producing correct answers, programs are required to satisfy at least two constraints. In this way, the average consistent programs reduces from 6,12 to 2,13 per sample. (2) When training, satisfying each condition will add 0.2 to the original score. Entity and quantity alignments in text also occur in the question in most cases. In QA, we apply a simple n-gram matching algorithm to filter out the alignments not in questions.\"}"}
{"id": "acl-2022-long-78", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: QA execution accuracy (EA) on dev/test and spurious program rate of 150 samples on dev. h.a. stands for hierarchy-aware.\\n\\n3.2.4 Evaluation Metrics\\nWe use Execution Accuracy (EA) as our metric following (Pasupat and Liang, 2015), measuring the percentage of samples with correct answers. We also report Spurious Program Rate to study the percentage that incorrect logical forms produce correct answer. Since we do not have golden logical forms, we manually annotate logical forms for 150 random samples in dev set for evaluation.\\n\\n3.2.5 Implementations\\nWe split 3,597 tables into train (70%), dev (15%) and test (15%) with no overlap. We download pre-trained models from huggingface. For NSM, we utilize 'bert-base-uncased', and fine-tune 20K steps on HiTab. Beam size is 5 for both training and inference. To test MAPO original logical form, we convert flatten tables as we do for TaPas. For TaPas, we adopt the PyTorch (Paszke et al., 2019) version in huggingface. We utilize 'tapas-base', and fine-tune 40 epochs on HiTab. All experiments are conducted on a server with four V100 GPUs.\\n\\n3.3 Results\\nTable 4 summarizes our evaluation results.\\n\\nWeak Supervision\\nFirst, MAPO with our hierarchy-aware logical form outperforms that using its original logical form by a large margin 11.5%, indicating the necessity of designing a logical form leveraging hierarchies. Second, MAPO achieves the best EA (40.7%) with the lowest spurious rate (19%). But >50% questions are answered incorrectly, proving QA on HiTab is challenging.\\n\\nPartial Supervision\\nFrom Table 4, we can conclude the effectiveness of partial supervision in two aspects. First, it improves EA. The model learns how to deal with more cases given high-quality programs. Second, it largely lowers %Spurious. The model learns to generate correct programs instead of some tricks. MML, whose performance highly depends on the quality of searched programs, benefits the most (36.7% to 45.1%), indicating partial supervision improves the quality of consistent programs by pruning spurious ones. However, TaPas does not gain much improvements from partial supervision, which we will discuss in the next paragraph.\\n\\nError Analysis\\nFor TaPas, 98.7% of success cases are cell selections, which means TaPas benefits little from partial supervision. This may be caused by: (1) TaPas does not support some common operators on hierarchical table like difference; (2) the coarse-to-fine cell selection strategy first selects columns then cells, but cells in different columns may also aggregate in hierarchical tables. For MAPO under partial supervision, we analyze 100 error cases. Error cases fall into four categories: (1) entity missing (23%): the header to filter is not mentioned in question, where a common case is omitted; model failure, including (2) failing to select correct regions (38%) and (3) failing to generate correct operations (20%); (4) out of coverage (19%): question types unsolvable with the logical form, which is explained in Appendix C.1.\\n\\nSpurious programs occur mostly in two patterns. In cell selection, there may exist multiple data cells with correct answers (e.g., G9,G16 in Figure 1), while only one is golden. In superlatives, the model can produce the target answer by operating on different regions (e.g., in both region B21:B25 and B23:B25, B23 is the largest).\\n\\nLevel-wise Analysis\\nIn Figure 3, we present level-wise accuracy of HiTab QA with MAPO and our hierarchy-aware logical form. Level here stands for sum of left and top header levels. As shown, the QA accuracy degrades when table level increases as table structure becomes more complex, except for level = 2, i.e., tables with no hierarchies. The reason level = 2 performs relatively worse might be that only 1.9% tables without hierarchies are seen in HiTab. We also present an annotated table.\"}"}
{"id": "acl-2022-long-78", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Level-wise QA accuracy and proportion of samples with MAPO and hierarchy-aware logical form.\\n\\n4 Hierarchical Table-to-Text\\n\\n4.1 Problem Statement\\n\\nSome works formulate table-to-text as a summarization problem (Lebret et al., 2016; Wiseman et al., 2017). However, since a full table often contains quite rich information, there lack explicit signals on what to generate, which renders the task unconstrained and the evaluation difficult. On the other hand, some recent works propose controlled generation to enable more specific and logical generation: (1) LogicNLG generates a sentence conditioned on a logical form guiding symbolic operations over given cells, but writing correct logical forms as conditions is challenging for common users who are more experienced to write natural language directly, thus restricting the application to real scenario; (2) ToTTo generates a sentence given a table with a set of highlighted cells. In ToTTo\u2019s formulation, the condition of cell selection is much easier to specify than the logical form, but it neglects symbolic operations which are critical for generating some analytical sentences involving numerical reasoning in HiTab.\\n\\nWe place HiTab as a middle-ground of ToTTo and LogicNLG to make the task more controllable than ToTTo and closer to real application than LogicNLG. In our setting, given a table, the model generates a sentence conditioned on a group of selected cells (similar to ToTTo) and operators (much easier to be specified than logical forms). Although we use two strong conditions to guide symbolic operations over cells, there still leaves a considerable amount of content planning to be done by the model, such as retrieving contextual cells in a hierarchical table given selected cells, identifying how operators are applied on given cells, and composing sentences in a faithful and logical manner.\\n\\nWe now define our task as: given a hierarchical table $T$, highlighted cells $C$, and specified operators $O$, the goal is to generate a faithful description $S$. The dataset $H = \\\\{(T_i, S_i)\\\\}$ is a set of $N$ table-description instances. Description $S_i$ is a sentence about a table $T_i$ and involves a series of operations $O_i = [O_{i1}, O_{i2}, \\\\ldots, O_{in}]$ on certain table cells $C_i = [c_{i1}, c_{i2}, \\\\ldots, c_{im}]$.\\n\\n4.2 Controlled Generation\\n\\n4.2.1 With Highlighted Cells\\n\\nAn entity or quantity in text can be supported by table cells if it is directly stated in cell contents, or can be logically inferred by them. Different from only taking data cells as highlighted cells (Parikh et al., 2020), we also take header cells as highlighted cells, and it is usually the case for superlative ARG-type operations on a specific header level in hierarchical tables, e.g., \u201cTeaching assistantships\u201d is retrieved by ARGMAX in Figure 1. In our dataset, highlighted cells are extracted from annotations of the entity and quantity alignment.\\n\\n4.2.2 With Operators\\n\\nHighlighted cells can tell the target for text generation, but is not sufficient, especially for analytical descriptions involving cell operations in HiTab. So we propose to use operators as extra control. It contributes to text clarity and meaningfulness in two ways. (1) It clarifies the numerical reasoning intent on cells. For example, given the same set of data cells, applying SUM, AVERAGE, or COUNT conveys different meanings thus should yield different texts. (2) Operation results on highlighted cells can be used as additional input sources. Existing seq2seq models are not powerful enough to do arithmetic operations (Thawani et al., 2021), e.g., adding up a group of numbers, and it greatly limits their ability to generate correct numbers in sentences. Explicitly pre-computing the calculation results is a promising alternative way to mitigate this gap in seq2seq models. Operators are extracted from annotations of formulas shown in Table 2.\\n\\n4.2.3 Sub Table Selection and Serialization\\n\\nSub Table Selection\\n\\nUnder controls of selected cells and operators, we devise a heuristic to retrieve all contextual cells as a sub table. (1) We start with highlighted cells extracted from our entity and quantity alignment, then use the extracted...\"}"}
{"id": "acl-2022-long-78", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table hierarchy to group the selected cells into the top header, the left header, and the data region. Based on the extracted table hierarchy, we use the source set of top and left header cells to include their indexed data cells, and we also use the source set of data cells to include corresponding header cells. We also include their parent header cells in table hierarchy to construct a full set of headers. In the end, we take the union of them as the result of sub table selection.\\n\\nSerialization\\nOn each sub table, we do a row-turn traversal on linked cells and concatenate their cell strings using [SEP] tokens. Operator tokens and calculation results are also concatenated with the input sequence. We also experimented with other serialization methods, such as header-data pairing or template-based method, yet none reported superiority over the simple concatenation. Appendix B.1 gives an illustration.\\n\\n4.3 Experiments\\nWe conduct experiments by fine-tuning four state-of-the-art text generation methods on HiTab.\\n\\n- **Pointer Generator** (See et al., 2017) A LSTM-based seq2seq model with copy mechanism. While originally designed for text summarization, it is also used in data-to-text (Gehrmann et al., 2018).\\n- **BERT-to-BERT** (Rothe et al., 2020) A transformer encoder-decoder model (Vaswani et al., 2017) initialized with BERT (Devlin et al., 2018).\\n- **BART** (Lewis et al., 2019) A pre-trained denoising autoencoder with standard Transformer-based architecture and shows effectiveness in NLG.\\n- **T5** (Raffel et al., 2019) A transformer-based pre-trained model. It converts all textual language problems into text-to-text and proves to be effective.\\n\\n4.3.1 Evaluation Metrics\\nWe use two automatic metrics, BLEU and PARENT. BLEU (Papineni et al., 2002) is broadly used to evaluate text generation. PARENT (Dhingra et al., 2019) is proposed specifically for data-to-text evaluation that additionally aligns n-grams from the reference and generated texts to the source table.\\n\\n4.3.2 Experiment Setup\\nSamples are split into train (70%), dev (15%), and test (15%) sets just the same as the QA task. The maximum length of input/output sequence is set to 512/64. Implementation details of all baselines are given in Appendix B.2.\\n\\n| Model                  | BLEU-4      | PARENT   |\\n|------------------------|-------------|----------|\\n| Pointer-Generator      | 5.8         | 9.0      |\\n| BERT-to-BERT           | 11.4        | 11.7     |\\n| BART                   | 17.9        | 28.0     |\\n| T5                     | 19.5        | 35.7     |\\n\\nTable 5: Results of hierarchical table-to-text.\\n\\n4.3.3 Experiment Result and Analysis\\nAs shown in Table 5, first, from an overall point of view, both metrics are not scored high. This well proves the difficulty of HiTab. It could be caused by the hierarchical structure, as well as statements with logical and numerical complexity. Second, by comparing two controlled scenarios (cell highlights & both cell highlights and operators), we see that adding operators to conditions greatly help models to generate descriptions with higher scores, showing the effectiveness of our augmented conditional generation setting. Third, results on two controlled scenarios across baselines are quite consistent. Replacing the traditional LSTM with transformers shows large increasing. Leveraging seq2seq-like pretraining yields a rise of +6.5 BLEU and +11.3 PARENT. Lastly, between pretrained transformers, T5 reports higher scores over BART, probably for T5 is more extensively tuned during pre-training.\\n\\nFurther, to study the generation difficulty concerning table hierarchy, we respectively evaluate samples at different hierarchical depths, i.e., table's maximum depths in top and left header trees. In groups of 2, 3, 4+ depth, BLEU scores 31.7, 26.5, and 21.3; PARENT scores 40.9, 36.5, and 31.6. The reason could be that, as the table header hierarchy grows deeper, the data indexing becomes increasingly compositional, rendering it harder to baseline models to configure entity relationships and compose logical sentences.\\n\\n5 Related Work\\nTable-to-Text\\nExisting datasets are restricted in flat tables or specific subjects (Liang et al., 2009; Chen and Mooney, 2008; Wiseman et al., 2017; Novikova et al., 2016; Banik et al., 2013; Lebret et al., 2016; Moosavi et al., 2021). The most related table-to-text dataset to HiTab is ToTTo (Parikh et al., 2020), in which complex tables are also included. There are two main differences between HiTab and ToTTo: (1) in ToTTo, hierarchical tables only account for a small proportion (5%), and there are no indication and usage of table hierarchies. (2) in addition to cell highlights, Hitab conditions on\"}"}
{"id": "acl-2022-long-78", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table QA mainly focuses on DB tables (Wang et al., 2015; Yu et al., 2018; Zhong et al., 2017) and flat web tables (Pasupat and Liang, 2015; Sun et al., 2016). Recently, there are some datasets on domain-specific table QA (Chen et al., 2021; Zhu et al., 2021) and jointly QA over tables and texts (Chen et al., 2020b; Zhu et al., 2021), but hierarchical tables still have not been studied in depth.\\n\\nCFGNN (Zhang, 2020) and GraSSLM (Zhang et al., 2020) uses graph neural networks to encode tables for QA, but all tables are database tables and relational web tables without hierarchies, respectively. Wang et al. (2021) include some hierarchical tables but only focuses on table search.\\n\\n6 Discussion\\nHiTab also presents cross-domain and complicated-calculation challenges. (1) To explore cross-domain generalizability, we randomly split train/dev/test by domains for three times and present the average results of our best methods in Table 6. We found decreases in all metrics in QA and NLG. (2) Figure 4 shows a case that challenges existing methods: performing complicated calculations requires to jointly consider quantity relationships, header semantics, and hierarchies.\\n\\n7 Conclusion\\nWe present a new dataset, HiTab, that simultaneously supports QA and NLG on hierarchical tables, where tables are collected from statistical reports and Wikipedia in various domains. Importantly, we provide fine-grained annotations on entity and quantity alignment. In experiments, we introduce strong baselines and conduct detailed analysis on QA and NLG tasks on HiTab. Results suggest that HiTab can serve as a challenging and valuable benchmark for future research on complex tables.\\n\\n8 Ethical Considerations\\nThis work presents HiTab, a free and open English dataset for the research community to study table question-answering and table-to-text over hierarchical tables. Our dataset contains well-processed tables, annotations (QA pairs, target text, and bidirectionally mappings between entities and quantities in text and the corresponding cells in table), recognized table hierarchies, and source code. Data in HiTab are collected from two public organizations, StatCan and NSF. Both of them allow sharing and redistribution of their public reports, so there is no privacy issue. We collect tables and accompanied descriptive sentences from StatCan and NSF. We also include hierarchical tables in Wikipedia from ToTTo, which is a public dataset under MIT license, so there is no risk to use it. And in the labelling process, annotators need to check if there exist any names or uniquely identifies individual people or offensive content. They did not find any such sensitive information in our dataset. We recruit 18 students or graduates in computer science, finance, and English majors from top universities (13 females and 5 males). Each student is paid $7.8 per hour (above the average local payment of similar jobs), totally spending 2,400 hours. We finally get 3,597 tables and 10,672 well-annotated sentences. And the data got approval from an ethics review board by an anonymous IT company. The details for our data collection and characteristics are introduced in Section 2.\\n\\nReferences\\nEva Banik, Claire Gardent, and Eric Kow. The kbgen challenge. In the 14th European Workshop on Natural Language Generation (ENLG), pages 94\u201397, 2013.\\n\\nBLS. U.S. bureau of labor statistics. https://www.bls.gov. Accessed July 4, 2021.\\n\\nCDC. Centers for disease control and prevention. https://www.cdc.gov. Accessed July 4, 2021.\\n\\nCensus. Census bureau. https://www.census.gov. Accessed July 4, 2021.\"}"}
{"id": "acl-2022-long-78", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhe Chen and Michael Cafarella. Integrating spreadsheet data via accurate and low-effort extraction. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1126\u20131135, 2014.\\n\\nDavid L Chen and Raymond J Mooney. Learning tosportscast: a test of grounded language acquisition. In Proceedings of the 25th international conference on Machine learning, pages 128\u2013135, 2008.\\n\\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang. Logical natural language generation from open-domain tables. arXiv preprint arXiv:2004.10404, 2020.\\n\\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. arXiv preprint arXiv:2004.07347, 2020.\\n\\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al. Finqa: A dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122, 2021.\\n\\nArthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):1\u201322, 1977.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\nBhuwan Dhingra, Manaal Faruqui, Ankur Parikh, Ming-Wei Chang, Dipanjan Das, and William W Cohen. Handling divergent reference texts when evaluating table-to-text generation. arXiv preprint arXiv:1906.01081, 2019.\\n\\nSebastian Gehrmann, Falcon Z Dai, Henry Elder, and Alexander M Rush. End-to-end content and plan selection for data-to-text generation. arXiv preprint arXiv:1810.04700, 2018.\\n\\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.\\n\\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas Mueller, Francesco Piccinno, and Julian Eisenschlos. Tapas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4320\u20134333, 2020.\\n\\nYusra Ibrahim, Mirek Riedewald, Gerhard Weikum, and Demetrios Zeinalipour-Yazti. Bridging quantities in tables and text. In 2019 IEEE 35th International Conference on Data Engineering (ICDE), pages 1010\u20131021. IEEE, 2019.\\n\\nJ Richard Landis and Gary G Koch. The measurement of observer agreement for categorical data. Biometrics, pages 159\u2013174, 1977.\\n\\nR\u00b4emi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with application to the biography domain. arXiv:1603.07771, 2016.\\n\\nWenqiang Lei, Weixin Wang, Zhixin Ma, Tian Gan, Wei Lu, Min-Yen Kan, and Tat-Seng Chua. Re-examining the role of schema linking in text-to-sql. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6943\u20136954, 2020.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\\n\\nPercy Liang, Michael I Jordan, and Dan Klein. Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 91\u201399, 2009.\\n\\nChen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 23\u201333, 2017.\\n\\nChen Liang, Mohammad Norouzi, Jonathan Berant, Quoc V Le, and Ni Lao. Memory augmented policy optimization for program synthesis and semantic parsing. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2018.\\n\\nSeung-Jin Lim and Yiu-Kai Ng. An automated approach for retrieving hierarchical data from html tables. In Proceedings of the eighth international conference on Information and knowledge management, pages 466\u2013474, 1999.\\n\\nNafise Sadat Moosavi, Andreas R\u00a8uckl\u00b4e, Dan Roth, and Iryna Gurevych. Learning to reason for text generation from scientific tables. arXiv:2104.08296, 2021.\\n\\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871, 2020.\\n\\nJekaterina Novikova, Oliver Lemon, and Verena Rieser. Crowd-sourcing nlg data: Pictures elicit better data. arXiv preprint arXiv:1608.00339, 2016.\"}"}
{"id": "acl-2022-long-78", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318, 2002.\\n\\nAnkur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. Totto: A controlled table-to-text generation dataset. arXiv preprint arXiv:2004.14373, 2020.\\n\\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. arXiv preprint arXiv:1508.00305, 2015.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:8026\u20138037, 2019.\\n\\nAdam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis only baselines in natural language inference. arXiv preprint arXiv:1805.01042, 2018.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683, 2019.\\n\\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 8:264\u2013280, 2020.\\n\\nSebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 1162\u20131167. IEEE, 2017.\\n\\nAbigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368, 2017.\\n\\nTianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daum\u00e9 III, and Lillian Lee. On the potential of lexico-logical alignments for semantic parsing to sql queries. arXiv:2010.11246, 2020.\\n\\nLya Hulliyyatus Suadaa, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura, and Hiroya Takamura. Towards table-to-text generation with numerical reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1451\u20131465, 2021.\\n\\nHuan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su, and Xifeng Yan. Table cell search for question answering. In Proceedings of the 25th International Conference on World Wide Web, pages 771\u2013782, 2016.\\n\\nAvijit Thawani, Jay Pujara, Pedro A Szekely, and Filip Ilievski. Representing numbers in nlp: a survey and a vision. arXiv preprint arXiv:2103.13136, 2021.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\\n\\nYushi Wang, Jonathan Berant, and Percy Liang. Building a semantic parser overnight. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1332\u20131342, 2015.\\n\\nZhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. Structure-aware pretraining for table understanding with tree-based transformers. arXiv:2010.12537, 2020.\\n\\nFei Wang, Kexuan Sun, Muhao Chen, Jay Pujara, and Pedro Szekely. Retrieving complex tables with multi-granular graph representation learning. arXiv preprint arXiv:2105.01736, 2021.\\n\\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256, 1992.\\n\\nSam Wiseman, Stuart M Shieber, and Alexander M Rush. Challenges in data-to-document generation. arXiv preprint arXiv:1707.08052, 2017.\\n\\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint understanding of textual and tabular data. arXiv preprint arXiv:2005.08314, 2020.\\n\\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018.\\n\\nYuchen Zhang, Panupong Pasupat, and Percy Liang. Macro grammars and holistic triggering for efficient semantic parsing. arXiv preprint arXiv:1707.07806, 2017.\"}"}
{"id": "acl-2022-long-78", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xingyao Zhang, Linjun Shou, Jian Pei, Ming Gong, Lijie Wen, and Daxin Jiang. A graph representation of semi-structured data for web question answering. arXiv preprint arXiv:2010.06801, 2020.\\n\\nXuanyu Zhang. Cfgnn: Cross flow graph neural networks for question answering on complex tables. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9596\u20139603, 2020.\\n\\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv:1709.00103, 2017.\\n\\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. arXiv preprint arXiv:2105.07624, 2021.\"}"}
{"id": "acl-2022-long-78", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A More Details on Dataset\\n\\nA.1 Dataset Preprocessing\\n\\nWe filter tables using these constraints: (1) number of rows and columns are more than 2 and less than 64; (2) cell strings have no more than one non-ASCII character and 20 tokens; (3) hierarchies are successfully parsed via the method in 2.6. (4) hierarchies have no more than four levels on one side. Finally, 85% tables meet all constraints.\\n\\nA.2 Annotation Artifacts\\n\\nAnnotation artifacts are common in large scale NLP datasets, which may raise unwanted statistical correlations making the task easier (Gururangan et al., 2018). In HiTab, the annotation artifacts may come from homogeneous patterns of questions. To address this issue, we ask annotators to revise questions from the high-quality descriptions from statistical reports from 28 domains to guarantee the diversity and naturalness, and encourage them to choose the best way to raise question reflecting the overall meaning of the description.\\n\\nTo further check whether and where artifacts may exist in our dataset, we conduct two experiments on QA and count the ratio of answer occurring in the question:\\n\\n- Use table as only input without question, to see if there is a potential pattern between table and answer. We train BERT+MAPO for 10,000 steps and TaPas for 10 epochs. Both methods can't converge under this setting, with 4.0% and 2.6% accuracy on the test set. The poor performance indicates model can't learn the answers by exploring and leveraging artifacts between the table and answer, and thus should learn to jointly inference the question and table.\\n\\n- Shuffle the rows and columns of table randomly. Experiments show similar performance (\u00b11%) between our original tables and shuffled tables. The result shows that the correlation between answer and table cell position is very little, thus model can't choose some specific positions, e.g., cell at the first row and first column, as a shortcut prediction.\\n\\nThe ratio that answer occurs in the question is only 5.3%. Model that only learns to retrieve the question can't achieve high performance.\\n\\nA.3 Domain Distribution\\n\\nThe full 29 domains of sample distribution in HiTab are shown in Figure 5.\\n\\nA.4 Annotation Interface\\n\\nThe annotation interface looks like Figure 6. Since spreadsheet formula is easy to write, execute, and check, the spreadsheet is naturally a great annotation tool. Annotators can use the Excel formula conveniently for cell linking and calculation in entity alignment and answering questions.\\n\\nB Hierarchical Table-to-Text\\n\\nB.1 Illustration on Controlled Generation in Hierarchical Table-to-Text.\\n\\nPlease find the illustration shown in Figure 7.\\n\\nB.2 Baseline Implementation Details\\n\\nWe perform optimized tuning for baselines using the following settings.\\n\\n- **Pointer Generator** (See et al., 2017) A LSTM-based seq2seq model with copy mechanism. The model uses two-layer bi-directional LSTMs for the encoder with 300-dim word embeddings and 300 hidden units. We perform fine-tuning using batch size 2, learning rate 0.05, and beam size 5.\\n\\n- **BERT-to-BERT** (Rothe et al., 2020) A transformer encoder-decoder model (Vaswani et al., 2017) where the encoder and decoder are both initialized with BERT (Devlin et al., 2018) by loading the checkpoint named 'bert-base-uncased' provided by the huggingface/transformers repository. We perform fine-tuning using batch-size 2 and learning rate 3e\u22125.\\n\\n- **BART** (Lewis et al., 2019) BART is a pretrained denoising autoencoder for seq2seq language modeling. We fine-tune BART to perform QA on HiTab, using batch size 2 and learning rate 1e\u22125.\"}"}
{"id": "acl-2022-long-78", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For doctoral students, the proportion of support from research assistantships is 10 points higher than that from teaching assistantships.\\n\\nTarget text: For doctoral students, the proportion of support from research assistantships is 10 points higher than that from teaching assistantships.\\n\\nC. Hierarchical Table QA\\n\\nC.1 Logical Form Function List\\n\\nWe list our logical form functions in Table 7. Union selection is required for comparative and arithmetic operations. It is achieved by allowing variable number of headers in filter tree, where \\\"variable\\\" is one or two in practice. In our implementation, a function by default takes the selected region of last function as input region to prune search space. We use grammars to filter left headers before top headers, and a \\\\( (\\\\text{filter level}) \\\\) is necessary after filtering one direction of tree even when only the leaf level is available. And we deactivate order relation functions (e.g., eq function) and the order argument \\\\( k \\\\).\"}"}
{"id": "acl-2022-long-78", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Function          | Arguments | Returns | Description |\\n|-------------------|-----------|---------|-------------|\\n| (filter tree h)   | h         | h       | Select a region indexed by sub-tree of the given header in the given region. |\\n| (filter level l)  | l         | l       | Select a region indexed by headers on the given level in the given region. |\\n| (argmax k)        | k         | k       | Find the header(s) with the k-th largest/smallest value in the region. [Input region should have one row or one column of data] |\\n| (argmin k)        | k         | k       | Find the header(s) with the k-th smallest value in the region. [Input region should have one row or one column of data] |\\n| (max l)           | l         | l       | Maximum/minimum/sum/average of the given region, grouping by headers of the given level. |\\n| (min l)           | l         | l       | i.e., data values aggregate according to their header strings on the given level. |\\n| (sum l)           | l         | l       | |\\n| (average l)       | l         | l       | |\\n| (count l)         | l         | l       | Count the number of headers on the given level of given region. |\\n| (difference)      |           | a number | Absolute difference, proportion and difference rate of given two elements |\\n| (proportion)      | rev       | a number | and b in region. rev means changing order of operands. e.g., proportion applies | b/a and proportion rev applies a/b. |\\n| (proportion rev)  |           | a number | |\\n| (difference rate) |           | a number | |\\n| (greater than n)  | n         | n       | Find the header(s) with data value(s) that have a certain order relation with the given number. |\\n| (less than n)     | n         | n       | [Input region should have one row or one column of data] |\\n| (less eq than n)  | n         | n       | |\\n| (eq n)            |           | n       | |\\n| (not eq n)        | n         | n       | |\\n| (opposite)        |           | n       | Take opposite value of data in a given region. [Input region should have one data element] |\"}"}
{"id": "acl-2022-long-78", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question Logical Forms\\n\\nCell Selection (filter tree 2012)\\n\\nQ: What is the GDP (filter tree china) of China in 2012? (filter level LEFT 2)\\n\\nSuperlative (filter tree 2012)\\n\\nQ: Which country has (filter level LEFT 2) the highest GDP in 2012? (filter tree gdp)\\n\\n(argmax 1)\\n\\nQ: How much more is (filter tree u.s. china) U.S. GDP higher than (filter level LEFT 2) China in 2013? (filter tree gdp)\\n\\n(difference)\\n\\nTable 8: Examples of our logical form. The table to be questioned is in Fig. 8.\\n\\nLEFT 1 is a symbol for the first level on the left.\\n\\nin argmax/argmin because there are few questions in these types and activating them will largely increase number of spurious programs when searching.\\n\\nThe logical form coverage after deactivation is 78.3% in 300 iterations of random exploration.\\n\\nSome typical question types that can not be covered are: (1) scale conversion, e.g., 0.984 to 98.4%, (2) operating data indexed by different levels of heads, e.g., proportion of total, (3) complex composite operations, e.g., Figure 4.\\n\\nC.2 Examples of Logical Form Execution\\n\\nTake the table in Figure 8 as input table, we demonstrate three types of questions with complete logical forms in Table 8.\\n\\nC.3 Pruning Rules in Searching\\n\\nWe use trigger words and POS tags for some functions in random exploration, which is inspired by (Zhang et al., 2017; Liang et al., 2018). Functions are allowed to be selected only when triggers appear in the question. Triggers are listed in Table 9.\\n\\nC.4 Table Linearization\\n\\nWe linearize the question and table according to Figure 8.\\n\\nThe input is concatenation of question and table. Table is linearized by putting headers in level order. Each level is led by a [LEVEL] token to gather current level embedding. The first [LEVEL] token stands for level zero of left. Each header is linearized as name | type. name is the tokenized header string. type is the entity type parsed by Stanford CoreNLP, which includes \u201cstring\u201d, \u201cnumber\u201d, \u201cdatetime\u201d in our case. Headers with the same name will gather token embeddings by mean pooling.\\n\\nC.5 Illustration on Challenges in Hierarchical Table\\n\\nWe present an annotated example in Figure 9 to show the challenges of hierarchical table introduced in Section 1.\\n\\nTo precisely answer the question in the figure, the model/method first needs to hierarchically index the grey region with \u201cfield in science\u201d and \u201cdoctoral\u201d, which requires understanding of textual and spatial semantics of the hierarchical table since the textual headers are spatially (seen as a tree) related with the region. Second, from the phrase \u201cmost enrolled\u201d, it should further indexes \u201cAll\u201d (column G) rather than \u201cPercent\u201d (column H) and infers argmax operation, which calls for the ability to distinguish between different calculation relationships.\"}"}
{"id": "acl-2022-long-78", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is the GDP of China in 2012?\\n\\nA: 8.229\\n\\nSource: National Science Foundation of U.S.\\n\\nAnswer: Biological and biomedical sciences\"}"}
