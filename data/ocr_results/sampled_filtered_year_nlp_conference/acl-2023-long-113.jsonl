{"id": "acl-2023-long-113", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ELQA: A Corpus of Metalinguistic Questions and Answers about English\\n\\nShabnam Behzad\\nGeorgetown University\\nshabnam@cs.georgetown.edu\\n\\nKeisuke Sakaguchi\\nTohoku University\\nkeisuke.sakaguchi@tohoku.ac.jp\\n\\nNathan Schneider\\nGeorgetown University\\nnathan.schneider@georgetown.edu\\n\\nAmir Zeldes\\nGeorgetown University\\namir.zeldes@georgetown.edu\\n\\nAbstract\\n\\nWe present ELQA, a corpus of questions and answers in and about the English language. Collected from two online forums, the >70k questions (from English learners and others) cover wide-ranging topics including grammar, meaning, fluency, and etymology. The answers include descriptions of general properties of English vocabulary and grammar as well as explanations about specific (correct and incorrect) usage examples. Unlike most NLP datasets, this corpus is metalinguistic\u2014it consists of language about language. As such, it can facilitate investigations of the metalinguistic capabilities of NLU models, as well as educational applications in the language learning domain. To study this, we define a free-form question answering task on our dataset and conduct evaluations on multiple LLMs (Large Language Models) to analyze their capacity to generate metalinguistic answers.\\n\\n1 Introduction\\n\\nLanguage is so powerful that it can be reflected back on itself. Statements like \u201cIn informal usage, a steep learning curve means something that is difficult (and takes much effort) to learn\u201d or \u201cIn some cases, an adjective has both -ic and -ical forms, with no difference in meaning\u201d expressly concern linguistic inventories, structures, and behaviors. In other words, they are metalinguistic\u2014they use language to discuss language (cf. Wilson, 2013). They may concern a particular instance of language use, or properties of a language or speaker in general; either way, they are metalinguistic in making linguistic phenomena the subject matter of a linguistic utterance. For the rest of this paper, the term meta-language is used for natural language text in which natural language is also the subject matter.\\n\\nWhile NLP models have become powerful at predicting text in many settings, it remains to be seen whether such capability extends to metalanguage\u2014where linguistic strings are not being deployed to contribute to the discourse with their normal denotations, but rather, are treated as entities with linguistic properties (e.g., grammar, meaning). One way this can be explored is in a question answering framework, which requires suitable datasets, ideally based on questions that are realistic and paired with high-quality answers.\\n\\nIn this paper, we present a corpus of metalinguistic questions and answers about English. The corpus is collected and carefully processed from two Stack Exchange forum sites: English Language & Usage (ENG) and English Language Learners (ELL). It covers more than 70k questions on numerous topics about English such as grammar, meaning, fluency, and etymology along with answers. Our corpus, ELQA (English Language Questions and Answers), can serve as a tool to facilitate metalinguistic studies. Moreover, since questions in ELQA cover a variety of topics in English, it can be used in the educational and English language learning domains.\\n\\nAs the first case study of ELQA, we investigate the performance of current state-of-the-art NLP technology on free-form question answering in the English language domain. Additionally, we explore the possibility of building NLP models that can directly answer questions from language learners. We process a subset of ELQA and make it appropriate for this task. Then, we report on the results of both automatic and human evaluations using different experimental settings of T51 and GPT-32 models. Although most of these models achieve high ratings for well-formedness, the validity of their answers is significantly lower than that of human-authored answers, indicating that this type of metalinguistic QA task is challenging even for large language models.\\n\\nOur main contributions are: 1) we release the\\n\\n1https://github.com/google-research/t5x\\n2https://openai.com/blog/gpt-3-apps\\n\\n2031\"}"}
{"id": "acl-2023-long-113", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Related Work\\n\\nStack Exchange is a network of numerous CQA sites (originally and most famously, Stack Overflow) built on a common platform. Stack Exchange forums have been featured in a number of previous datasets (Yao et al., 2013; Hoogeveen et al., 2015; Ahmad et al., 2018; Penha et al., 2019; Campos et al., 2020; Kumar and Black, 2020; Rogers et al., 2023), including the English site (our ENG) along with others such as Ask Ubuntu, Android, Gaming and WordPress (dos Santos et al., 2015; Nakov et al., 2017). We focus on ENG and ELL as they concern the English language itself; we show that these datasets cover a wide range of metalinguistic questions.\\n\\nOur use of these forums contrasts with previous work on metalanguage in corpora, which annotated and quantified mentions (Anderson et al., 2004; Wilson, 2010, 2011, 2012, 2017), but did not consider entire questions and answers about language. Taylor (2015) studied metalanguage in online forums, but with a focus on the usage of metalinguistic expressions of mock politeness. More recently, Bogetic (2021) published the first corpus of contemporary Slovene, Croatian and Serbian media metalanguage texts.\\n\\nSo far, metalanguage has not been a focus in the QA domain\u2014ours is the first publicly available English metalinguistic QA dataset. Most QA tasks are set up to have a question and a reference document, where the objective is to find the answer based on the document (Fan et al., 2019; Kwiatkowski et al., 2019). In this paper, we explored a type of \\\"closed-book\\\" question answering task (Roberts et al., 2020; Khashabi et al., 2021). To the best of our knowledge, this task has not been explored to date within the realm of English language questions.\"}"}
{"id": "acl-2023-long-113", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: ELQA statistics on Qs (questions) and As (answers). To calculate average length in tokens, sequences were tokenized using SpaCy (https://spacy.io/). This involves a process that converts natural language text into a structured format that can be processed by natural language processing (NLP) tools. This is particularly useful in scenarios that require significant generalization and adaptation rather than looking up facts.\\n\\n### 3 Constructing the Dataset\\n\\nWe collect our data from two sites on Stack Exchange: English Language & Usage (ENG) and English Language Learners (ELL). Sample screenshots of the site are shown in Figure 1. The Stack Exchange data is publicly released under the CC-BY-SA 3.0 license. We preprocessed the data until 2021-12-06 collected from the Internet Archive to be suitable for NLP studies and release it as ELQA. Additionally, some cleanup (e.g., removing posts marked as \\\"spam\\\" or \\\"offensive\\\") was done. Fields for each entry (question) include the title, body, user bio (if available), score (which is calculated based on up-votes and down-votes by other users), tags (user-assigned, related to the area/topic of the question), favorite count, and a list of answers. Textual content (body and user bio) is provided in two formats: HTML and plain text without HTML tags.\\n\\nWe release two versions of ELQA based on different preprocessing steps. In ELQA-large, we keep questions as long as they don't include any images (<img> HTML tag) and have an answer with a score of at least 2 (meaning at least two people other than the user posting the answer found it helpful). For ELQA-small, we applied further filtering to ensure that the data has the least amount of noise: a) questions should have a score of at least 4 (ensuring questions are clear and coherent), b) question has an answer with a score higher than 3 and c) there are no hyperlinks in at least one of the high-rated answers. The last step reduces noise and facilitates a fair comparison for the closed-book question-answering task (\u00a74) with model-generated answers, as models cannot be expected to have access to the web to suggest valid URLs compared to humans who would search the web for appropriate resources to include in their answers.\\n\\nFor quality assurance, we also did a human annotation on ELQA-small. Two of the authors annotated 250 question and answer pairs for the following: 1) Is the question answerable? and 2) Does the answer fully address the question? We found 99.2% of the questions answerable and 91.8% of the answers acceptable.\\n\\nTable 1 contains overall statistics on both versions. Figure 2 shows the distribution of the 10 most common tags in each of the sites. Since users assign these tags to their questions (0 to multiple), similar or near-duplicate tags are common within the collection. Some form more general and more fine-grained variants, e.g. 'meaning' and 'meaning-in-context'. In addition to available user-assigned tags, we manually inspected a large subset of the data to identify salient types of questions. These are defined below and illustrated in Table 2. We then labeled 100 random questions to get a rough estimate of their frequencies (two annotators annotated these 100 samples and they agreed on 92% of cases in an overlapping subset).\\n\\n- **Fluency** (\u224838% of questions): Usually asking about a particular sentence, comparison of multiple sentences, and/or probing how an expression should be used in general. The user wants to know if X is correct, or to decide between multiple choices, which one is correct. \\\"Correct\\\" could mean grammatical, most natural/idiomatic, stylistically appropriate, conveying the intended meaning, etc. In Qs where options are provided by the user, there are cases in which 1) none of the choices are correct, 2) multiple choices are correct, and 3) only one is correct.\\n- **Form to Meaning (Interpretation)** (\u224819% of questions): Questions such as \\\"What does X mean?\\\" (of an expression in general, or an encountered passage) or \\\"What's the difference in meaning between X and Y?\\\".\\n- **Meaning to Form (Encoding)** (\u224820% of questions): In these questions, the user gives some form of the desired output. For example, \\\"What sentence is the most natural/idiomatic way to say X?\\\" or \\\"Which sentence conveys the intended meaning best?\\\"\\n- **Meaning to Form (Encoding)** (\u224820% of questions): In these questions, the user gives some form of the desired output. For example, \\\"What sentence is the most natural/idiomatic way to say X?\\\" or \\\"Which sentence conveys the intended meaning best?\\\"\\n- **Other Complex Questions**: These are questions that don't fit into the above categories but still require a thoughtful response.\\n\\nWe release two versions of ELQA based on different preprocessing steps. In ELQA-large, we keep questions as long as they don't include any images (<img> HTML tag) and have an answer with a score of at least 2 (meaning at least two people other than the user posting the answer found it helpful). For ELQA-small, we applied further filtering to ensure that the data has the least amount of noise: a) questions should have a score of at least 4 (ensuring questions are clear and coherent), b) question has an answer with a score higher than 3 and c) there are no hyperlinks in at least one of the high-rated answers. The last step reduces noise and facilitates a fair comparison for the closed-book question-answering task (\u00a74) with model-generated answers, as models cannot be expected to have access to the web to suggest valid URLs compared to humans who would search the web for appropriate resources to include in their answers.\\n\\nFor quality assurance, we also did a human annotation on ELQA-small. Two of the authors annotated 250 question and answer pairs for the following: 1) Is the question answerable? and 2) Does the answer fully address the question? We found 99.2% of the questions answerable and 91.8% of the answers acceptable.\\n\\nTable 1 contains overall statistics on both versions. Figure 2 shows the distribution of the 10 most common tags in each of the sites. Since users assign these tags to their questions (0 to multiple), similar or near-duplicate tags are common within the collection. Some form more general and more fine-grained variants, e.g. 'meaning' and 'meaning-in-context'. In addition to available user-assigned tags, we manually inspected a large subset of the data to identify salient types of questions. These are defined below and illustrated in Table 2. We then labeled 100 random questions to get a rough estimate of their frequencies (two annotators annotated these 100 samples and they agreed on 92% of cases in an overlapping subset).\\n\\n- **Fluency** (\u224838% of questions): Usually asking about a particular sentence, comparison of multiple sentences, and/or probing how an expression should be used in general. The user wants to know if X is correct, or to decide between multiple choices, which one is correct. \\\"Correct\\\" could mean grammatical, most natural/idiomatic, stylistically appropriate, conveying the intended meaning, etc. In Qs where options are provided by the user, there are cases in which 1) none of the choices are correct, 2) multiple choices are correct, and 3) only one is correct.\\n- **Form to Meaning (Interpretation)** (\u224819% of questions): Questions such as \\\"What does X mean?\\\" (of an expression in general, or an encountered passage) or \\\"What's the difference in meaning between X and Y?\\\".\\n- **Meaning to Form (Encoding)** (\u224820% of questions): In these questions, the user gives some form of the desired output. For example, \\\"What sentence is the most natural/idiomatic way to say X?\\\" or \\\"Which sentence conveys the intended meaning best?\\\"\\n- **Other Complex Questions**: These are questions that don't fit into the above categories but still require a thoughtful response.\"}"}
{"id": "acl-2023-long-113", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fluency\\n\\n\\\"On my own way vs. \\\"in my own way\\\"?\\nWhich one is correct <strong>in or on</strong> own way?\\n\\nblockquote>\\n<ul>\\n<li>I usually help my closest friends on/in my own way.</li>\\n</ul>\\n</blockquote>\\n\\nForm to Meaning\\n\\nWondering what \\\"get by\\\" means in this context\\n\\n<blockquote>\\nHe tries to <strong>get by</strong> with the least amount of <strong>work possible</strong>.\\n</blockquote>\\n\\nCould you tell me what this sentence means?\\n\\nMeaning to Form\\n\\nGrammatically correct synonym for \\\"level of catastrophicness\\\"\\n\\nI'm trying to say something like this:\\n\\n<blockquote>\\nWe have developed a strategy to numerically rate the <strong>relative level of catastrophicness</strong> of a potential hardware failure.\\n</blockquote>\\n\\nLooking at a thesaurus hasn't really helped me with this one. Can someone help me to convey this without using this ugly, incorrect grammar?\\n\\nGrammatical Analysis\\n\\nShould I modify a gerund using an adjective or an adverb?\\n\\nI know that a gerund is a <strong>noun</strong>, so it should be modified by an <em>adjective</em>. However, it is also a <strong>verb form</strong>. Can I modify it by using an <em>adverb</em>?\\n\\nOther\\n\\nWhat is the etymology of 'physician'\\n\\nI find myself confusing 'physician' and 'physicist' occasionally. While I know what they both mean, I am a little confused as to the use of 'physics' in 'physician'. How did the term 'physician' come to be used the way it is meant today? Lucky coincidence?\\n\\nTable 2:\\n\\nExample posts from ELL and ENG sites for different question types. (Original post URLs and author profile URLs are all available in the Appendix.)\\n\\nexplanation/definition and asks for the term or for form to express it.\\n\\n\u2022 Grammatical Analysis (\u224811% of questions): Questions about parts of speech and other aspects of syntactic analysis. (e.g. \\\"Is this a verb or an adjective?\\\"; \\\"Can an article ever go after the noun it modifies?\\\"). Note that Fluency questions may mention grammatical terminology, but the grammatical categories are not the focus.\\n\\n\u2022 Other (\u224810% of questions): Any other type of question not listed above. This includes questions about pronunciation, etymology, etc.\\n\\nAs can be seen from the examples in Table 2, it is common for questions and answers to contain example usages, often visually distinguished with Markdown formatting (such as blockquotes, bullets, and italics) which we retain in the processed corpus markup. Examples can be incorporated into a post in a variety of ways\u2014e.g., asking for an interpretation of one usage, as in the Form to Meaning example in Table 2, or contrasting multiple usages such as in the following question:\\n\\nDid VS Have done What is difference between the following statements: Did you tell your parents yet? Have you told your parents yet? Haven't you told your parents yet? Are these questions correct? why do we use one over another in some cases? What is the difference in meaning?\\n\\nUsage examples provided in a question may be instances that the author encountered \\\"in the wild\\\" (such as in a novel or film), or in a grammar book or dictionary, or they may have been constructed by the user. Answers sometimes include examples found through a corpus search.\\n\\n4 English Language Question Answering\\n\\nLarge language models can produce output that is fluent and (at times) informationally adequate when presented with factual questions about entities in the world (Roberts et al., 2020). But how do such models perform when asked questions about the language itself? In this section, we investigate the free-form English language question answering task.\\n\\nThis task has the potential to benefit educational applications for language learners. Research on NLP for educational purposes has investigated tasks such as automated grammatical error correction (Dale et al., 2012; Ng et al., 2014; Bryant et al., 2019; Wang et al., 2021, inter alia), question and quiz generation for language learning (Sakaguchi et al., 2013; Chinkina and Meurers, 2017; Marrese-Taylor et al., 2018; Vachev et al., 2021), and automated essay scoring (Burstein, 2003; Farag et al., 2018, inter alia). Nevertheless, an application that has not been taken up by the educational NLP community is free-form question answering about language. Second language learners possess a degree of metalinguistic awareness about the language they are learning, and often turn to teachers or more advanced speakers with explicit questions about vocabulary, grammar, and usage. Community Question Answering (CQA) websites such as Stack Exchange have sites for language learners\u2019 questions and answers. These sites require consid...\"}"}
{"id": "acl-2023-long-113", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Automatic evaluation scores (percentage) for different setups. The highest value in each column is bolded.\\n\\n|          | ROUGE-1 | ROUGE-2 | ROUGE-L | BLEU | BERTScore |\\n|----------|---------|---------|---------|------|-----------|\\n| GPT-3 FS | 31.2    | 8.5     | 20.3    | 10.8 | 85.7      |\\n| GPT-3 FT-1000 | 27.1 | 7.0     | 18.7    | 11.8 | 85.2      |\\n| GPT-3 FT-100 | 25.6 | 6.0     | 17.9    | 9.9  | 85.2      |\\n| T5-xxl   | 28.1    | 8.0     | 19.8    | 4.7  | 80.3      |\\n| T5-l     | 21.2    | 6.6     | 17.7    | 4.1  | 78.3      |\\n\\nTable 4: Automatic evaluation scores (percentage) for different setups broken down by site.\\n\\n4.1 Data\\nWe randomly divided ELQA-small into train/test/dev splits. This resulted in 21,175 Q&A pairs in the train split and 3,107 Q&A pairs in each of the dev and test splits. Answers in these splits have a score of at least 4. If there are multiple high-rated answers to a question, we include all of them for training. Some of these questions can be answered by looking at a dictionary or vocabulary list for descriptions. But many of them are explanations in relation to particular instances of language use and require significant reasoning rather than looking up facts. Thus in this setup, we do not have any external context/reference available at evaluation time, i.e. this is a closed-book QA task.\\n\\nThe input for the task is Title: [Q title] <sep> Body: [Q body]. We use the HTML version of ELQA for this task since metalinguistic mentions are usually distinguished via formatting (e.g., blockquotes, bullets) and the ultimate goal is a system that humans can easily use to get answers to their language-related questions.\\n\\n4.2 Setup\\nWe use T5 (Raffel et al., 2020; Roberts et al., 2022) and GPT-3 (Brown et al., 2020) as our models since they have been shown to be strong baselines in other QA domains. We believe the questions in ELQA offer new challenges for the QA task since they require different types of knowledge/understanding to be able to generate answers. Additionally, these questions contain noise (grammatical errors) and cases of textual metalanguage which is likely harder to comprehend for a model. We fine-tune T5-l and T5-xxl for this task.\\n\\nWe saved multiple checkpoints during fine-tuning and evaluated them with the interpolation of BLEU (Papineni et al., 2002), BERTScore (Zhang et al., 2020) and ROUGE (Lin, 2004) on the dev set to choose the best-performing one (checkpoint at 75k updates, hyperparameters available in Table 8 in the Appendix).\\n\\nWith GPT-3 we used text-davinci-003 and experimented with both fine-tuning (FT) on 100 and 1000 samples and a few-shot (FS) setting in which the model is given a few demonstrations of the questions and answers at inference time as conditioning, but no weights are updated (Radford et al., 2019). In the FS setting, we show the model four Q&A pairs since we wanted the model to see different question types but there were also limits on the input length. To select these 4 pairs, we randomly created 5 different sets of Q&A pairs, evaluated on a subset of dev, and chose the best-performing set for the experiments (dev results available in Appendix, Table 9).\\n\\n4.3 Results\\n4.3.1 Automatic Evaluation\\nResults are shown in Table 3. GPT-3 FS outperforms all other methods in all metrics with a large margin except for BLEU Score. We also observed that using GPT-3 in a few-shot setup worked much better than the fine-tuned version. Looking at some of the model-generated answers, we noticed that the fine-tuned model tends to generate longer answers.\\n\\n---\\n\\n7 This took 5 days with v3-8 TPU (provided by Google).\"}"}
{"id": "acl-2023-long-113", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5:\\n\\n| Source              | C1  | C2  | Avg. on ENG | Avg. on ELL | Total Avg. |\\n|---------------------|-----|-----|-------------|-------------|------------|\\n| Top-rated human     | 4.81| 4.87| 4.83        | 0.34        | 4.44       |\\n| Low-rated human     | 4.79| 4.50| 4.68        | 0.15        | 4.02       |\\n| GPT-3 FS            | 4.89| 4.77| 4.84        | 0.35        | 3.72       |\\n| GPT-3 FT-1000       | 4.50| 4.43| 4.47        | 0.07        | 2.90       |\\n| T5-xxl              | 4.03| 3.68| 3.89        | 0.76        | 2.17       |\\n\\nTable 6:\\n\\n| Source              | First | Last |\\n|---------------------|-------|------|\\n| Top-rated human     | 129   | 9    |\\n| Low-rated human     | 114   | 15   |\\n| GPT-3 FS            | 131   | 5    |\\n| GPT-3 FT-1000       | 97    | 28   |\\n| T5-xxl              | 71    | 66   |\\n\\nAnswers containing redundant text. We observed improvements when we used 1000 samples instead of 100 for fine-tuning and hence, fine-tuning on larger data might result in better performance, however, we only experimented with 100 and 1000 samples in this paper due to having limited resources.\\n\\nBased on Table 3, T5-xxl seems to perform similarly to GPT-3 FT-1000. However, a small manual evaluation showed otherwise (GPT-3 FT-1000 answers were slightly better). Furthermore, we observe that the scores for even the best system are very low, but manual evaluations showed that the GPT-3 FS generates fairly good answers in many cases. Due to these observations and also given the well-known limitations of automatic metrics for evaluating generation tasks (Kasai et al., 2022; Celikyilmaz et al., 2020; Bhakthavatsalam et al., 2021), we believe conducting human evaluation for deeper analysis is necessary for this task.\\n\\nIn Table 4, we show results for each site to see if one is more challenging than the other. Overall, models perform slightly better on ELL based on automatic metrics\u2014but we see in the next section (Table 5) that there isn\u2019t really a meaningful difference between the sites when humans evaluate the answers.\\n\\n4.3.2 Human Evaluation\\n\\nHuman evaluators were presented with the question title and body, and then asked to rate 5 answers: a top-rated human-provided answer, a low-rated human-provided answer, and answers generated by 3 of our best models: GPT3 FS, GPT3 FT-1000, T5-xxl.\\n\\nThey were asked to give ratings (via a slider widget, on a 1\u20135 integer scale\u2014the higher, the better) for two criteria (C1 & C2):\\n\\n1. Does the answer look grammatically/structurally like a good answer (ignoring whether it answers the question)?\\n2. Is the information in this answer a valid response to the question (ignoring formatting/stylistic issues)?\\n\\nThe first criterion aims to get a score for fluency and coherence and the second one for correctness and completeness.\\n\\nWe collected ratings for a set of 75 questions (375 different answers). Each question with its set of answers was evaluated by at least 2 raters, and then the average score was calculated based on their responses.\\n\\nWe also report the average z-score which is calculated over each annotator's raw score distribution for each metric, intended to normalize interannotator variation in how the scale is interpreted for each of the two metrics (details in Appendix B).\\n\\nThe results of this study are shown in Table 5. Overall, answers generated by GPT-3 FS have a small gap with human answers in both C1 and C2. GPT-3 FT-1000 comes next, with less accurate answers containing redundant text and hallucinations. The smallest model, T5-xxl, ranks last.\\n\\nRankings based on human evaluations are available in Table 6. These results are also indicating that model-generated answers are fluent in most cases, but they are not as good as human answers.\\n\\nThe survey interface is illustrated in Figure 5 of Appendix C.\\n\\nEvaluators consisted of 6 English native speakers who are senior NLP researchers and graduate students. The answer source was hidden and the order (5 answers) was randomized.\\n\\nAnnotations took about 90 minutes on average.\\n\\nWe selected half of the human evaluation samples from very recent posts (June 2021 until January 2023) on Stack Exchange and compared the results with older posts. The models' scores were comparable or better on the more recent data, so we didn't see evidence of models having an advantage due to the fact that they may have been trained on some of the data available on the web. For reference, human evaluation scores for recent data were Avg. C1=4.82, Avg. C2=3.83 and for older data, Avg. C1=4.86, Avg. C2=3.61.\"}"}
{"id": "acl-2023-long-113", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Explain the verb tense in \\\"All that glitters is not gold\\\"\\n\\nI am confused by the maxim All that glitters is not gold. May I know why the word is used there? In my known according the English grammar glitters is plural, so after plural we add verb+s. As in above example use auxiliary verb so why not \u2013 why is?\\n\\nTop-rated human\\n\\nGood question, but it's worth pointing out that glitters in this context is a verb, not a noun. The subject of the sentence is all. When all is used as the subject of a sentence, it functions as a pronoun, and it can function as a singular or a plural pronoun.\\n\\nFrom NOAD\\n\\nall (pronoun) [with clause] the only thing (used for emphasis): all I want is to be left alone. (used to refer to surroundings or a situation in general): everything: all was well | it was all very strange.\\n\\nSo:\\n\\n- All that glitters is not gold is roughly equivalent to Everything that glitters is not gold and the verb tense agrees.\\n- All who eat are not gluttons is roughly equivalent to All the people who eat are not gluttons so there is no problem there, either.\\n\\nLow-rated human\\n\\nGrammatically is is used with third person singular and also s form of verb is used. \\\"All\\\" indicates a third person single group.\\n\\nGPT-3 FS\\n\\nThe phrase all that glitters is not gold is an idiom, which means that things that appear to be valuable may not actually be so. It is an old proverb, and the verb tense used in the phrase is an example of the so-called \\\"historic present\\\". The historic present is a literary device used to give a sense of immediacy to a narrative. It is used to describe events that happened in the past, but are being described as if they are happening in the present. In this case, the phrase is used to emphasize the timelessness of the proverb. So, while the verb glitters is indeed plural, the verb is is used in the historic present to give the phrase a sense of immediacy and timelessness.\\n\\nGPT-3 FT-1000\\n\\nThe maxim is in the present tense, so the verb is also in the present tense: Glitters. The past tense of glitter is glittered, and the past participle is glittered. So if the maxim was in the past tense, it would be: All that glittered was not gold.\\n\\nThe present tense maxim is a generalisation about something that is true in the speaker's experience. The past tense maxim would be a historical statement about what was true in the past.\\n\\nT5-xxl\\n\\nThis is an interesting question, because it is not immediately obvious what the grammar rules are here. It is not a fixed phrase, and so subject-verb agreement must be based on the context. In this case, the that is a prepositional phrase acting as a complement to the preposition all. It could be rephrased as: All that glitters is not gold.\\n\\nIt is true that all means \\\"everything that glitters\\\" - but it is equally true that not everything that glitters means \\\"not everything that glitters is gold\\\". It is much the same as Everything that glitters is not gold. The subject all is singular, so the verb must be singular as well - is. If it was twice plural, then are would be used, but it is not. It is interesting to note that Shakespeare used the same construction in Hamlet: Everything that glitters is not gold. The subject that is singular, and so the auxiliary verb is agrees with the singular noun.\\n\\nTable 7:\\n\\nExample post with human and model-generated answers. More examples are available in the Appendix.\\n\\nwhen correctness/completeness is considered (GPT-3 FS is ranked first or as good as a top-rated human answer in only 45% of cases).\\n\\nFor each criterion and Top-rated human, Low-rated human and GPT-3 FS, histograms of the average score of the two annotators are plotted in Figure 3. We can observe that GPT-3 FS and Low-rated human have very similar numbers of high-scoring answers (human evaluation scores), but the number of low-scoring human answers drops off gradually as quality decreases, while the distribution is more spread out for GPT-3 FS. I.e., the model has some moderately bad answers as well as some good ones, whereas Low-rated human answers cluster more on the upper end of the scale.\\n\\nC1 (fluency/coherence).\\n\\nAll models generated fairly fluent and well-structured answers. We even notice that GPT-3 FS scores higher in well-formedness than human answers. We looked at those samples and we believe there are two main reasons for this: 1) Some human answers were very long, containing multiple different quotes from different sources. On average, our evaluators preferred the structure of answers from GPT-3 FS, which took the form of a short paragraph addressing the question. 2) Some human answers have a more casual/conversational nature. On the other hand, GPT-3 FS generated more authoritative-sounding, teacher-like answers with complete sentences, which were preferred by our evaluators in some cases.\\n\\nC2 (correctness/completeness).\\n\\nOn average, models are worse on this metric, though sometimes they did produce acceptable answers (perhaps because variants of the question are commonly discussed on the web).\\n\\nOne challenge in this domain is that questions, particularly from ELL, may not be fully fluent. In the Fluency example from Table 7 we see that there are some grammatical errors in the question that are unrelated to the topic of the question. In addition, the questioner uses incorrect terminology, mentioning verb tense in the post title even though the question is actually about subject-verb agreement with respect to number (as can be inferred from the examples). While the good human response correctly flags the incorrect terminology and answers the underlying question, GPT-3 models give irrelevant answers about tense.\\n\\nAnother correctness failure from GPT-3 FS can be seen in the following pair involving a Meaning-to-Form question: What is the analog equivalent of 'digitally'? In electronics, we can solve a problem digitally or analoguely? That doesn't sound right, but...\"}"}
{"id": "acl-2023-long-113", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Histograms of the ratings from our human evaluation of answers to 75 questions: the human-authored answer preferred by site users, the human-authored answer least preferred by site users, and our best model (GPT-3 FS). Each value is an average of two annotators\u2019 ratings. (Histograms for GPT-3 FT-1000 and T5-xxl are available in Figure 4 of Appendix C.)\\n\\nSurely there must be a word I can use?\\n\\nUpdate: some definitions:\\n\\ndigitally: Relating to or being a service that provides information expressed in discrete numerical form.\\n\\nanalogously: Relating to or being a service that provides information expressed in continuous, non-discrete form. (This is my made up definition of a word I made up.)\\n\\nanalogously: Similar or alike in such a way as to permit the drawing of an analogy. As you can see, the word analogously doesn\u2019t mean the opposite of digitally, it means something else.\\n\\nAnswer: The analog equivalent of \u2018digitally\u2019 is \u2018analogously\u2019. Analogously means \u201cin a similar way\u201d or \u201cby comparison\u201d. It is used to describe a situation where two things are similar in some way, but not necessarily opposite. For example, you could say that a digital signal is analogous to an analog signal, because they are both signals, but they are different in terms of how they are represented.\\n\\nWe see that the model begins by ignoring the user\u2019s explanation that analogously does not have the desired meaning, and suggests it anyway. The rest of the model\u2019s answer gives a (valid) general definition of analogously and clumsily attempts to apply it to the two kinds of signals, which is not responsive to the question. It may be the case that models particularly struggle with questions for which there is no straightforward solution (in this case, no adverb derived from analogue has the user\u2019s intended meaning).\\n\\n4.4 Discussion\\n\\n4.4.1 Metrics\\n\\nOur human evaluations found that the high-rated human answers are the best, and GPT-3 FS is roughly on par with the low-rated human answer, while other models are worse.\\n\\nAs noted in \u00a74.3.1, we did not find the automatic metrics very informative. We believe this is due to the long and free-form nature of this task: concepts can be explained in different ways and answers can include different examples which make automatic evaluations very challenging. For example, for the question Why is this sentence: \u201cAdditional nine features were added. . . \u201d incorrect? a human answer is:\\n\\nIt has to do with the order of the adjectives. For example, consider this sentence: <blockquote>Happy nine men walk into a bar. </blockquote> Both nine and happy are adjectives, but we are really intending nine to describe the happy men, not happy describing the nine men. I don\u2019t ....\\n\\nModel-generated answers mostly mirror the examples in the question. But answers given by humans mention new examples\u2014which are not in the question (and therefore unlikely to come up in other answers). Thus a good answer may mention superficially off-topic terms metalinguistically, which presents evaluation challenges unlike those of summarization or MT, for example (Reiter, 2018; Celikyilmaz et al., 2020). For QA tasks in metalinguistic domains such as ours, caution is warranted when applying automatic metrics that are unaware of the use/mention distinction. Human evaluation is likely to be more illuminating. We release our full human evaluation data to enable detailed comparisons in future research.\\n\\n4.4.2 Qualitative Analysis\\n\\nWe examined behavioral patterns for the best model, GPT-3 FS, organized by question type, with observations as follows.\\n\\nInterpretation and Encoding: These are the categories where the model performs the best, especially if the question has a straightforward answer (for example, there aren\u2019t several possible answers based on context). Topics in these categories often center on definitions of vocabulary and idioms. These questions may involve less reasoning to answer, and models may be leveraging content from the online dictionaries and thesauri in their web training data.\"}"}
{"id": "acl-2023-long-113", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When asked about general concepts and rules of grammar, the model-generated answers are usually acceptable:\\n\\n- **ed** and **ing** adjectives\\n\\nWhat is the difference? <blockquote> I am interesting in mathematics. </blockquote> versus <blockquote> I am interested in mathematics. </blockquote> Murphy intermediate grammar Unit 97 tells me that I cannot use the former. Extended and general explanations will be appreciated.\\n\\n**Answer:**\\n\\nThe difference between *-ed* and *-ing* adjectives is that the *-ed* adjectives describe a feeling or emotion that is felt by the subject, while the *-ing* adjectives describe a quality of the subject itself. In the example sentence, *interested* is the correct word to use because it describes the feeling of interest that the subject has in mathematics. *Interesting* would be incorrect because it describes the quality of the subject itself, not the feeling of the subject. For example, if you were to say \\\"This book is interesting,\\\" you would be describing the book itself, not the feeling you have towards the book. On the other hand, if you were to say \\\"I am interested in this book,\\\" you would be describing the feeling you have towards the book.\\n\\nHowever, when questions become more specific to examples, sometimes the model struggles:\\n\\n*What does \\\"in\\\" refer to?*\\n\\n<blockquote> A symbiotic relationship is an interaction between two or more species in which one species lives in or on another species. </blockquote>\\n\\n**Answer:**\\n\\nIt refers to the other species. The symbiotic species lives in the other species.\\n\\nThe example in the question uses the word *in* twice. Rather than addressing the first token, which the questioner has bolded, the answer addresses the second one (denoting the physical relationship between one species and another).\\n\\n**Fluency:**\\n\\nThe model correctly answers many of the fluency questions. Others require reasoning and understanding of context, and the model struggles to generate a correct answer. For example:\\n\\n*Returning for the nth time*\\n\\nSuppose I went to the supermarket three times. Is my third trip considered my \\\"second time returning there\\\" or my \\\"third time returning there?\\\"\\n\\nThanks. (I know this question sounds silly, but English is my second language...)\\n\\n**Answer:**\\n\\nYour third trip to the supermarket would be considered your third time returning there. The phrase *returning for the nth time* is used to refer to the nth time you have returned to a place, not the n-1th time.\\n\\nWe also observe that when the correct usage of a case is not mentioned in the question itself, the model sometimes suggests incorrect corrections and wrong explanations:\\n\\n*Why is this usage of \\\"to\\\" incorrect?*\\n\\nI am in communication with a friend who is learning English. I noticed her using \\\"to\\\" in the wrong context, but I'm unable to explain which rule she is breaking. <blockquote> I can't access to Gmail. </blockquote>\\n\\n**Answer:**\\n\\nIn this sentence, the word *to* is incorrect because it is being used as a preposition. The correct word to use in this context is *for*. <blockquote> I can't access for Gmail. </blockquote>\\n\\nThe word *to* is used as a preposition when it is followed by a verb, such as *go*, *come*, *run*, etc. For example: <blockquote> I am going to the store. </blockquote> However, when *to* is followed by a noun, it is usually incorrect. In this case, the correct word to use is *for*. <blockquote> I need a gift for my friend.</blockquote>\\n\\n4.4.3 Outlook\\n\\nOverall, we were surprised by the quality of many of the answers from GPT-3 FS: many would likely have received high user ratings if submitted as answers on the site. At the same time, the model is not to the point where we would want to trust its answers without human review. We believe that answer confidence estimation\u2014so that users can be shown only the best model-generated answers\u2014is an important direction for using learner QA models in practice (Jiang et al., 2021).\\n\\n5 Conclusion\\n\\nWe presented ELQA, a dataset containing metalinguistic questions and answers about the English language. We provided analysis and a taxonomy of the data, along with experiments on free-form answer generation and investigated the extent to which language models can articulate their generalizations about language. Since many of the questions in ELQA were asked by language learners, it forms a potentially useful and so far untapped resource for educational NLP purposes and metalinguistic question answering. We release the dataset to enable further studies of this task.\\n\\nEthics Statement\\n\\nWe have released a processed version of an already public online forum dataset, in a manner consistent with the terms of the license, which require attribution of all posts (\u00a73). The models we have presented are intended only as baselines for future research, not for deployment. Models should be carefully stress-tested for undesirable heuristics/biases before deployment. Systems for the generation task, in particular, would risk misleading language learners with plausible but incorrect answers, so it is important to not deploy a generation.\"}"}
{"id": "acl-2023-long-113", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"system until it is approximately as reliable as existing non-automated alternatives, and to present the output with caveats. Potential biases reflecting the demographics of authors represented in the training data (in terms of native language, level of English proficiency, etc.) also need to be considered if models are deployed for different target populations.\\n\\nLimitations\\nOne limitation of our dataset, ELQA, is that the corpus only contains questions in English and about English. However, Stack Exchange has sites with questions about other languages and our main data extraction scripts are general enough that they can be used to create corpora for other sites on Stack Exchange. Of course, language-specific processing steps, quality assurance and analysis must be applied before releasing such data.\\n\\nMost importantly, the models we have presented here are intended only as baselines for future research, not for deployment. Potential biases reflecting the demographics of authors represented in the training data (in terms of native language, level of English proficiency, etc.) also need to be considered if models are deployed for different target populations. Moreover, many of these types of questions are found on the web, and a lot of the same topics are brought up by many users, so a model's ability to generate correct answers cannot necessarily be attributed to abstract reasoning.\\n\\nAcknowledgements\\nWe thank the anonymous reviewers for their insightful comments. We thank Daniel Khashabi for helpful discussions and feedback. This research was supported in part by NSF award IIS-2144881.\\n\\nReferences\\nArshad Ahmad, Chong Feng, Shi Ge, and Abdallah Yousif. 2018. A survey on mining stack overflow: question and answering (Q&A) community. Data Technol. Appl., 52:190\u2013247.\\n\\nMichael L. Anderson, Andrew Fister, Bryant Lee, Luwito Tardia, and Danny Wang. 2004. On the types and frequency of meta-language in conversation: A preliminary report. In 14th Annual Meeting of the Society for Text and Discourse.\\n\\nSumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. 2021. Think you have solved direct-answer question answering? Try ARC-DA, the direct-answer AI2 reasoning challenge. arXiv preprint arXiv:2102.03315.\\n\\nKsenija Bogetic. 2021. MetaLangCORP: Presenting the first corpus of media metalanguage in Slovene, Croatian and Serbian, and its cross-discipline applicability. Fluminensia, 33:123\u2013142.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nChristopher Bryant, Mariano Felice, \u00d8istein E. Andersen, and Ted Briscoe. 2019. The BEA-2019 shared task on grammatical error correction. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52\u201375, Florence, Italy. Association for Computational Linguistics.\\n\\nJill Burstein. 2003. The E-rater\u00ae scoring engine: Automated essay scoring with natural language processing. In Automated essay scoring: A cross-disciplinary perspective, pages 113\u2013121. Lawrence Erlbaum Associates Publishers.\\n\\nJon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deiriu, Mark Cieliebak, and Eneko Agirre. 2020. DoQA - accessing domain-specific FAQs via conversational QA. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7302\u20137314, Online. Association for Computational Linguistics.\\n\\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. ArXiv, abs/2006.14799.\\n\\nMaria Chinkina and Detmar Meurers. 2017. Question generation for language learning: From ensuring texts are read to supporting learning. In Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 334\u2013344, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nRobert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A report on the preposition and determiner error correction shared task. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54\u201362, Montr\u00e9al, Canada. Association for Computational Linguistics.\"}"}
{"id": "acl-2023-long-113", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-113", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418\u20135426, Online. Association for Computational Linguistics.\\n\\nAnna Rogers, Matt Gardner, and Isabelle Augenstein. 2023. QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. ACM Comput. Surv., 55(10).\\n\\nKeisuke Sakaguchi, Yuki Arase, and Mamoru Komachi. 2013. Discriminative approach to fill-in-the-blank quiz generation for language learners. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 238\u2013242, Sofia, Bulgaria. Association for Computational Linguistics.\\n\\nCharlotte Taylor. 2015. Beyond sarcasm: The metalanguage and structures of mock politeness. Journal of Pragmatics, 87:127\u2013141.\\n\\nKristiyan Vachev, Momchil Hardalov, Georgi Karadzhov, Georgi Georgiev, Ivan Koychev, and Preslav Nakov. 2021. Generating answer candidates for quizzes and answer-aware question generators. In Proceedings of the Student Research Workshop Associated with RANLP 2021, pages 203\u2013209, Online. INCOMA Ltd.\\n\\nYu Wang, Yuelin Wang, Kai Dang, Jie Liu, and Zhuo Liu. 2021. A comprehensive survey of grammatical error correction. ACM Trans. Intell. Syst. Technol., 12(5).\\n\\nShomir Wilson. 2010. Distinguishing use and mention in natural language. In Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 29\u201333, Los Angeles, CA. Association for Computational Linguistics.\\n\\nShomir Wilson. 2011. In search of the use-mention distinction and its impact on language processing tasks. IJCLA, 2(1-2):139\u2013154.\\n\\nShomir Wilson. 2012. The creation of a corpus of English metalanguage. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 638\u2013646, Jeju Island, Korea. Association for Computational Linguistics.\\n\\nShomir Wilson. 2013. Toward automatic processing of English metalanguage. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 760\u2013766, Nagoya, Japan. Asian Federation of Natural Language Processing.\\n\\nShomir Wilson. 2017. A bridge from the use-mention distinction to natural language processing. In Paul Saka and Michael Johnson, editors, The Semantics and Pragmatics of Quotation, pages 79\u201396. Springer International Publishing, Cham.\\n\\nYuan Yao, Hanghang Tong, Tao Xie, Leman Akoglu, Feng Xu, and Jian Lu. 2013. Want a good answer? Ask a good question first! arXiv preprint arXiv:1311.6876.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In International Conference on Learning Representations.\"}"}
{"id": "acl-2023-long-113", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Data Credits\\n\\nThe Stack Exchange license requires that any Internet use of the content should include a hyperlink directly to the original question and the profile of the authors. Below are URLs for all the examples used in this paper. The post URL incorporates the post title.\\n\\n- [https://ell.stackexchange.com/questions/12/dates-and-times-on-in-at](https://ell.stackexchange.com/questions/12/dates-and-times-on-in-at) (Q by bytebuster, A by waiwai933)\\n- [https://ell.stackexchange.com/questions/146633/on-my-own-way-vs-in-my-own-way](https://ell.stackexchange.com/questions/146633/on-my-own-way-vs-in-my-own-way) (Q by bavyan-yaldo)\\n- [https://ell.stackexchange.com/questions/19684/wondering-what-get-by-means-in-this-context](https://ell.stackexchange.com/questions/19684/wondering-what-get-by-means-in-this-context) (Q by nima)\\n- [https://english.stackexchange.com/questions/74896/grammatically-correct-synonym-for-level-of-catastrophicness?](https://english.stackexchange.com/questions/74896/grammatically-correct-synonym-for-level-of-catastrophicness?) (Q by solvingPuzzles)\\n- [https://english.stackexchange.com/questions/134352/should-i-modify-a-gerund-using-an-adjective-or-an-adverb](https://english.stackexchange.com/questions/134352/should-i-modify-a-gerund-using-an-adjective-or-an-adverb) (Q by worawit-tepsan)\\n- [https://english.stackexchange.com/questions/222567/what-is-the-etymology-of-physician](https://english.stackexchange.com/questions/222567/what-is-the-etymology-of-physician) (Q by casvaart)\\n- [https://ell.stackexchange.com/questions/185516/idid-vs-have-done](https://ell.stackexchange.com/questions/185516/idid-vs-have-done) (Q by learner)\\n- [https://english.stackexchange.com/questions/162824/what-is-the-analog-equivalent-of-digitally](https://english.stackexchange.com/questions/162824/what-is-the-analog-equivalent-of-digitally) (Q by rocketmagnet, first A by AllisonAshley, second A by Hot Licks)\\n- [https://ell.stackexchange.com/questions/13749/explain-the-verb-tense-in-all-that-glitters-is-not-gold](https://ell.stackexchange.com/questions/13749/explain-the-verb-tense-in-all-that-glitters-is-not-gold) (Q by Chinmay235, first A by J.R., second A by sajad)\\n- [https://english.stackexchange.com/questions/162824/what-is-the-analog-equivalent-of-digitally](https://english.stackexchange.com/questions/162824/what-is-the-analog-equivalent-of-digitally)\\n- [https://ell.stackexchange.com/questions/87725/what-does-in-refer-to](https://ell.stackexchange.com/questions/87725/what-does-in-refer-to) (Q by Anfi)\\n- [https://english.stackexchange.com/questions/102996/returning-for-the-nth-time](https://english.stackexchange.com/questions/102996/returning-for-the-nth-time) (Q by AlicornTwilight-isATroll)\\n- [https://english.stackexchange.com/questions/55331/why-is-this-usage-of-to-incorrect](https://english.stackexchange.com/questions/55331/why-is-this-usage-of-to-incorrect) (Q by Ademos)\\n- [https://ell.stackexchange.com/questions/87725/what-does-in-refer-to](https://ell.stackexchange.com/questions/87725/what-does-in-refer-to)\\n- [https://ell.stackexchange.com/questions/322637/he-is-more-than-a-friend-is](https://ell.stackexchange.com/questions/322637/he-is-more-than-a-friend-is) (Q by Loviii, first A by MarcInManhattan, second A by Kirt)\\n- [https://english.stackexchange.com/questions/258060/verb-for-doing-something-unknowingly](https://english.stackexchange.com/questions/258060/verb-for-doing-something-unknowingly) (Q by Daniel Bramhall, first A by chasly - supports Monica, second A by talrnu)\\n- [https://ell.stackexchange.com/questions/322580/know-someone-in-detail](https://ell.stackexchange.com/questions/322580/know-someone-in-detail) (Q by Simo Ita)\\n- [https://english.stackexchange.com/questions/49384/ed-and-ing-adjectives](https://english.stackexchange.com/questions/49384/ed-and-ing-adjectives) (Q by itun)\\n- [https://ell.stackexchange.com/questions/87725/what-does-in-refer-to](https://ell.stackexchange.com/questions/87725/what-does-in-refer-to)\\n- [https://ell.stackexchange.com/questions/322637/he-is-more-than-a-friend-is](https://ell.stackexchange.com/questions/322637/he-is-more-than-a-friend-is)\\n- [https://english.stackexchange.com/questions/258060/verb-for-doing-something-unknowingly](https://english.stackexchange.com/questions/258060/verb-for-doing-something-unknowingly)\\n- [https://ell.stackexchange.com/questions/322580/know-someone-in-detail](https://ell.stackexchange.com/questions/322580/know-someone-in-detail)\\n- [https://english.stackexchange.com/questions/258060/verb-for-doing-something-unknowingly](https://english.stackexchange.com/questions/258060/verb-for-doing-something-unknowingly)\\n- [https://ell.stackexchange.com/questions/322580/know-someone-in-detail](https://ell.stackexchange.com/questions/322580/know-someone-in-detail)\\n\\nB On our use of z-scores\\n\\nIn our human evaluation, raters were presented with a question and five candidate answers and asked to rate each on a scale from 1 to 5 for each of our two criteria (C1 and C2).\\n\\nOur main goal is to compare the quality of the answers across 5 conditions (3 systems, 2 posts from the site). Raters may have different interpretations of the absolute scales\u2014for example, some raters could be more generous than others overall in terms of the numerical rating, even if they agree on the ranking of systems.\\n\\nThere are several possible ways to factor out this bias. One way is to compute standard scores, a.k.a. z-scores, for each annotator's distribution of responses on each criterion. Consider C1: from the ratings of an annotator $a$ we have the empirical distribution $p_{C1 a}(y_{C1 i}, a | x_i)$ where $i$ indexes the items (answers, of which multiple ones may belong to the same question), and likewise for C2. For each of these distributions we fit a normal distribution by computing mean and standard deviation. For an absolute rating $y_{C1 i, a}$, its z-score $z_{C1 i, a}$ is its number of standard deviations above the mean rating for that annotator on that metric (a negative z-score indicates it is below the mean).\\n\\nAveraging the z-scores for a particular condition, we can see whether annotators tended to rate outputs in that condition with higher or lower scores than the other outputs they saw in the sample. Note that the z-score computation ignores the grouping of answers from different conditions into questions, so it is not directly measuring annotators' rankings of candidate answers to a particular question.\\n\\nC Further Details\\n\\n| Parameter Value |\\n|-----------------|\\n| Batch Size 8     |\\n| Max. Gradient Updates 75k |\\n| Max. Input Length 512 |\\n| Max. Output Length 512 |\\n\\nTable 8: T5 hyperparameters used for the Answer Generation task\"}"}
{"id": "acl-2023-long-113", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: GPT-3 few-shot scores on a subset of dev when using different sets of in-context examples\\n\\n| Set  | ROUGE-1 | ROUGE-2 | ROUGE-L | BLEU  | BERTScore |\\n|------|---------|---------|---------|-------|-----------|\\n| Set-1 | 0.303   | 0.084   | 0.201   | 0.092 | 0.859     |\\n| Set-2 | 0.296   | 0.079   | 0.192   | 0.092 | 0.854     |\\n| Set-3 | 0.286   | 0.071   | 0.193   | 0.052 | 0.856     |\\n| Set-4 | 0.273   | 0.067   | 0.188   | 0.039 | 0.857     |\\n| Set-5 | 0.289   | 0.074   | 0.193   | 0.051 | 0.856     |\\n\\nTable 10: Two example posts with human and model-generated answers.\\n\\n| Title Question | Source Answer |\\n|---------------|---------------|\\n| \\\"He is more than a friend is\\\" | Top-rated human: This is a bit tricky, because \\\"than\\\" can be a conjunction or a preposition, and not everyone agrees on when it is each. I'll present one interpretation that I believe is pretty standard. In 1a and 1b, \\\"than\\\" is best understood as a conjunction. It compares two actions (\\\"he is taller\\\" and \\\"his friend is tall\\\"). That is why we can insert \\\"is\\\" at the end of 1b. In 2a and 2b, \\\"than\\\" is best understood as a preposition. The prepositional phrase \\\"than a friend\\\" modifies \\\"more\\\". It does not compare two actions (\\\"he is more\\\" and \\\"a friend is [adjective]\\\"). That is why we cannot insert \\\"is\\\" at the end of 2b. To summarize: In your sentences 1a and 1b, \\\"than\\\" is a conjunction. In 2a and 2b, it is a preposition. Thus, the situations are not analogous. Low-rated human: (1a) He is taller than his friend. \u2014 correct, but incomplete, and natural (1b) He is taller than his friend is. \u2014 correct, but incomplete, and natural (1c) He is taller than his friend is tall. - correct and complete, but awkward (2a) He is more than a friend. \u2014 correct, but incomplete, and natural (2b) He is more than a friend is. - correct, but incomplete, and awkward (2c) He is more than a friend to me. - correct, but incomplete, and natural (2d) He is more than a friend is to me. - correct, but incomplete, and awkward (2e) He is more to me than a friend is to me - correct, complete, and awkward The complete statement of meaning is grammatically correct but may be more, or longer, than people naturally say. Typically words that can be implied are eliminated for more natural speech. Some eliminations of implied words will lead to constructions that are grammatically correct and natural, while other eliminations of implied words can lead to constructions that are grammatically correct but which sound awkward. The awkwardness of an expression has more to do with common usage than it does with rules of grammar. GPT-3 FS: No, (2b) is not correct. The reason is that (2a) is a comparison between two things, while (2b) is not. In (2a), the comparison is between he and a friend, while in (2b) the comparison is between he< and his friend is. This is not a valid comparison, so (2b) is not correct. GPT-3 FT-1000: The difference between (1a) and (1b) is that (1a) is a statement about the friend, whereas (1b) is a statement about the friendship. Consider the following two sentences: <blockquote> John is taller than his friend Bob. </blockquote> vs. <blockquote> John is taller than Bob is. </blockquote> The first sentence is a statement about John and Bob. The second sentence is a statement about John and the friendship between John and Bob. In the first sentence, the friend is the subject of the sentence. In the second sentence, the friend is the object of the sentence. In your examples, (1a) and (2a) are statements about the friend, whereas (1b) and (2b) are statements about the friendship. T5-xxl: Answering your question, I can say that, no, (2b) is not correct. (2a) \\\"He is taller than his friend\\\" is idiomatic and natural, and the simple present tense is used here to describe an action or state. (2b) \\\"He is more than a friend\\\" is not idiomatic and does not mean the same thing. | Top-rated human: They are drifting into a materialistic society. They are sliding into a materialistic society. Low-rated human: Evolve fits your needs and in particular your example pretty closely. Evolution is a natural process, implying the lack of intention you're looking for. It also fits your example as it's an act of change and development. Similarly, you might consider mutate, another natural occurrence with greater emphasis on randomness and less emphasis on a direction of development. More generally, you could say my answer is: use a context-specific verb which emphasizes nature (i.e. the lack of human intention) as the driving force behind the action. GPT-3 FS: The verb drift could be used in this context. It suggests a lack of conscious direction or control, and implies that the person is being carried along by external forces. <blockquote> He drifted into a materialistic society. </blockquote> GPT-3 FT-1000: I think bemused would work well in this context. It means to confuse or bewilder, but can also be used to describe someone who is behaving in a confused way, or who is reacting to something confusing. So, \\\"Jane was bemused by the materialistic society she found herself in.\\\" or \\\"Jane was bemused by her own behaviour as she bought yet another pair of designer shoes. T5-xxl: How about slumbering into a materialistic society? |\"}"}
{"id": "acl-2023-long-113", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: Histograms of the average score of each two annotators from our human evaluation for answers generated by GPT-3 FT-1000 and T5-xxl. The other answers evaluated are represented in Figure 3.\\n\\nFigure 5: Screenshot from the survey we designed for human evaluation on the Qualtrics platform. Five answers were shown for each question as described in \u00a74.\"}"}
{"id": "acl-2023-long-113", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nB Did you use or create scientific artifacts?\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC Did you run computational experiments?\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-113", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\"}"}
