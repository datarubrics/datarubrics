{"id": "emnlp-2024-main-1002", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nGender bias in machine translation (MT) is recognized as an issue that can harm people and society. And yet, advancements in the field rarely involve people, the final MT users, or inform how they might be impacted by biased technologies. Current evaluations are often restricted to automatic methods, which offer an opaque estimate of what the downstream impact of gender disparities might be. We conduct an extensive human-centered study to examine if and to what extent bias in MT brings harms with tangible costs, such as quality of service gaps across women and men. To this aim, we collect behavioral data from \u223c90 participants, who post-edited MT outputs to ensure correct gender translation. Across multiple datasets, languages, and types of users, our study shows that feminine post-editing demands significantly more technical and temporal effort, also corresponding to higher financial costs. Existing bias measurements, however, fail to reflect the found disparities. Our findings advocate for human-centered approaches that can inform the societal impact of bias.\\n\\n1 Introduction\\n\\nNatural language processing (NLP) has evolved from an academic specialty to countless commercial applications that can both benefit and negatively affect people\u2019s lives. With the widespread use of these technologies, researching the ethical and social impact of NLP has become increasingly crucial (Hovy and Spruit, 2016; Sheng et al., 2021), with gender fairness being a major concern (Sun et al., 2019; Stanczak and Augenstein, 2021).\\n\\nIn machine translation (MT) gender bias has received significant attention, also in the public domain (Olson, 2018). Numerous studies have shown that MT perpetuates harmful stereotypes (Stanovsky et al., 2019; Triboulet and Bouillon, 2023) and is skewed towards masculine forms that under-represent women (Vanmassenhove et al., 2018; Alhafni et al., 2022b).\\n\\nAs emphasized by Savoldi et al. (2021) \u2013 if we regard MT as a resource in its own right \u2013 such representational disparities might directly imply allocative harms, i.e. differential access to material benefits that make a social group or individual worse-off (Barocas et al., 2017; Chien and Danks, 2024). For instance, a woman using an MT system to translate her biography (i.e. the first sentence in English in Figure 1) into Italian would need more effort (i.e. represented by insertions \u2013 in green, and substitutions \u2013 in red and green \u2013 in Figure 1) to revise incorrect masculine references, thus experiencing a disparity in the quality of the service.\\n\\nDespite acknowledging the potential harm to individuals, research on gender bias in MT primarily focuses on in-lab automatic evaluations. Such assessments, however, are only assumed to reflect a real-world downstream effect, without verifying if and to what extent biased models might concretely impact users interacting with a system.\\n\\nTo address this gap, we examine the effect of gender bias in MT with a human-centered perspective.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Does gender bias in MT imply tangible service disparities across men and women? And if so, can we meaningfully quantify them via more human-centered measures? To take stock of the current research landscape, we review the involvement of human subjects in prior literature on gender and MT. Motivated by the outcome, we conduct extensive experiments across multiple datasets, languages, and users. In a controlled setup, 88 participants post-edited MT outputs to ensure either feminine or masculine gender translation. In the process, we track behavioral data \u2013 i.e. time to edit and number of edits \u2013 to compare effort across genders. Based on this, we estimate the associated cost for post-editing into each gender if the work were assigned to a third-party translator. Our main findings are:\\n\\n1. Most of current assessments of gender bias in MT either overlook human involvement, or treat individuals as models' evaluators rather than potentially affected users (\u00a72).\\n\\n2. We find substantial gender disparities in the time and technical effort required to post-edit MT, with feminine translation taking on average twice longer and four times the editing operations (\u00a74).\\n\\n3. The cost of the found disparities is also economic, and can unfairly fall onto various stakeholders in the translation process (\u00a75).\\n\\n4. The automatic evaluation of gender bias does not accurately reflect the found human-centered effort disparities (\u00a75).\\n\\nTo sum up, our study marks a step towards understanding the implications of gender bias in MT. While harms have so far been implied, or inferred from automatic scores as a proxy for downstream impact, here we empirically show that gender bias can bring unfair service disparities. What's more, we quantify bias with measures that are more meaningful for potentially impacted individuals: work-load and economic costs.\\n\\nBehavioral data and post-edits are made available at https://huggingface.co/datasets/FBTK-MT/gender-bias-PE.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In fact, only 24 works rely on humans to measure bias, though in a different capacity, which we distinguish into three conceptual categories. In 18 papers, we find that people \u2013 often expert linguists (e.g. Vanmassenhove et al. (2021a); Soler Uguet et al. (2023)) \u2013 are involved in MANUAL EVALUATION. This serves to either ensure correlation with bias metrics (e.g. Kocmi et al. (2020)) or to gain qualitative insights that defy automatic approaches (Popovi\u0107, 2021). While indeed valuable, such analyses are a support for structured, often annotation-based model-centric evaluations \u2013 i.e. that inform and quantify models' behaviour. Differently, the 5 papers in the SURVEY category focus on the feedback and experiences of potentially impacted groups of users (e.g. Piergentili et al. (2023b); Daems and Hackenbuchner (2022)). For instance, to grasp user preference in how models should handle the translation of novel, non-binary pronouns from English \u2013 e.g. ze, xe (Lauscher et al., 2023), or to understand the potential trade-off between overall quality and inclusivity goals (Amrhein et al., 2023). Finally, the study by Gronnmann et al. (2023) recounts a PARTICIPATORY Action Research, where a community-led approach with different stakeholders informs the state and potential direction for gender fair MT.\\n\\nOverall, despite this recent trend towards surveys or participatory methods, humans are rarely involved to estimate gender bias in MT. Moreover, if involved, people mostly serve in the capacity of evaluators, supporting model-centric assessments rather than being considered as potentially impacted users. Our finding stands in contrast with a qualitative survey by Dev et al. (2021), which found MT as an application with a high risk for downstream harms in the context of gender bias. Further motivated by such evidence, we carry out a quantitative, empirical study \u2013 to the best of our knowledge, the very first of its kind \u2013 focusing on human-centered assessments. In particular, we examine whether gender bias in machine translation leads to disparities in the quality of service offered to women and men, by considering different datasets, languages, and users (\u00a73.1).\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Data statistics. For each dataset and language, we provide the average number of words for source (src-W) and output sentences (out-W), as well as the average number of target gendered words (tgt-GW) in the reference translations.\\n\\nOur data samples are organized as follows. (i) MTGEN - A, a subset of MT-Geneval sentences where gender in the source is ambiguous. (ii) MTGEN - UN, which contains feminine/masculine versions of gender-unambiguous English sentences, thus offering favorable conditions for correct translation based on available gender cues in the source. Finally, (iii) a subset of MUST - SHE featuring ambiguous first-person references in the English source sentence. This sample is included for the sake of phenomena variability \u2013 given that MUST - SHE entails gendered translation for many parts-of-speech \u2013 whereas both Wikipedia-derived samples mainly represent gendered translations for occupational nouns.\\n\\nAs a key feature of these datasets, for each English source sentence, two contrastive feminine/masculine pairs of reference translations are provided. These are designed to isolate gender as a factor from overall quality evaluation.\\n\\nAs described in \u00a74, we conduct multidataset (\u00a74.1) experiments for en-it, whereas the multilingual (\u00a74.2) study with en-es/de is based on MTGEN - A. For each dataset (statistics in Table 1), we retrieve a random sample of 250 sentences, while maximizing the number of common sentences across language pairs.\\n\\nUser types\\nThe study aims to reflect an average user, who fixes an MT output by themselves. While including lay users with different levels of language expertise or MT familiarity would represent a comprehensive case study, such a setup adds a notable level of complexity and potential noise to deal with (e.g. gendered expressions to be fixed might be overlooked). To guarantee higher control of our variables, we thus rely on professional translators as a proxy. Still, to also mimic MT interactions with less experienced users, for en-it we carry out multiuser experiments (\u00a74.3) involving high-school students, native speakers of Italian with a B2 level of English (further details in the upcoming \u00a73.2).\\n\\nTo avoid fitting our results to the potentially subjective post-editing activity of one person, we allocate 16 post-editors for MUST - SHE and 16 for MTGEN - UN. Since it consists of shorter sentences (see Table 1), we task 14 subjects for each of the four MTGEN - A conditions \u2013 for a total of 88 participants overall.\\n\\nModel\\nReliable behavioural assessments require a sizable data sample and number of participants, which we prioritize during budget allocation. For this reason, we do not consider MT models as a variable and only use Google Translate (GT). Besides being state-of-the-art, GT is chosen as it represents one of the most widely used consumer-facing commercial MT systems (Pitman, 2021).\\n\\n3.2 Study design\\nTask instructions and platform\\nGiven a source sentence and its MT output, participants were instructed to carry out a light PE \u2013 i.e. targeting only essential fixes to adjust the overall quality of the translation (O'Brien, 2022) \u2013 with a focus on ensuring either feminine or masculine translation for human referents, based on provided gender information. We choose a light PE given the high quality of the MT output, and crucially to limit the number of preferential edits that might introduce noise. The task was performed with Matecat, a mature, computer-assisted translation (CAT) tool supporting PE that is freely available online.\\n\\nWithin-group design\\nFor each data sample of 250 <English source, GT output> pairs, we design a within-subjects study with counterbalancing (Charness et al., 2012), which ensures variation of the order of conditions in the study. Namely, each participant performs both feminine (F) and masculine (M) post-edits, in equal amounts (blocks of 125 pairs).\\n\\nFor each condition, we prepared dedicated guidelines, which are available at https://github.com/balintsavoldi/post-edit_guidelines. E.g., COMET scores are between 82.3-85.3 across all languages and data. See Appendix F.1 for full results.\\n\\nhttps://www.matecat.com/ For more details on the Matecat setup see Appendix B.2.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Multidataset (top), multilanguage (center) and multiuser (bottom) results. Results are shown for all users \u2013 both (P)rofessional and (S)tudents \u2013 languages, and datasets. We provide time to edit (TE, i.e. hour:minutes), HTER, and the number of post-edited sentences (out of 250 per each gender).\\n\\n| Task         | TE (\u2193) | HTER (\u2193) | # Edited (\u2193) | FEM | MAS | \u2206abs | \u2206rel | FEM | MAS | \u2206abs | \u2206rel |\\n|--------------|--------|----------|-------------|-----|-----|------|------|-----|-----|------|------|\\n| Pen-it       | 2:08   | 2:09     | 204         | 81.1| 14.88| 5.76 | 9.12 | 158.3| 242  | 93   | 149  | 160  |\\n| Pen-de       | 2:12   | 0:30     | 188         | 334.0| 15.62| 5.47 | 11.04| 515.0| 228  | 40   | 188  | 470  |\\n| Pen-es       | 2:13   | 1:13     | 149         | 81.1| 14.88| 5.76 | 9.12 | 158.3| 242  | 93   | 149  | 160  |\\n| Pen-it       | 2:33   | 1:06     | 168         | 76.1| 16.51| 5.47 | 13.08| 201.8| 243  | 70   | 173  | 247  |\\n| Pen-it       | 2:38   | 0:57     | 173         | 177.6| 16.51| 5.47 | 13.08| 201.8| 243  | 70   | 173  | 247  |\\n| Pen-es       | 2:15   | 1:10     | 160         | 81.1| 14.88| 5.76 | 9.12 | 158.3| 242  | 93   | 149  | 160  |\\n| Pen-de       | 2:12   | 0:30     | 188         | 334.0| 15.62| 5.47 | 11.04| 515.0| 228  | 40   | 188  | 470  |\\n\\nExperiments for en-it include data from both i) professional translators based on voluntary participation, and ii) paid professionals. We attested no significant difference between these conditions (for further details see Appendix C.2). For en-de/es, we exclusively relied on paid professionals. Experiments were allocated 50m (i.e. \u223c10m instructions and \u223c40m PE), which vastly ensured the sufficient time to complete the task. The experiment with students was carried out as part of their school activities: we allocated double the time and included a warm-up phase to get acquainted with the PE task. No participant was informed of the scope of our study beforehand, and all recorded data are anonymous. For further information on the recruited participants and compensation see Appendix C.\\n\\nData collection and effort measures\\nAt the end of the process, for each sample of 250 source sentences we collect 500 post-edits (250 F and 250 M). We then measure the corresponding \u201cfeminine\u201d/\u201cmasculine\u201d effort for the temporal and technical dimension (Krings, 2001). Respectively, i) time to edit (TE) is recorded within Matecat for each output sentence, whereas ii) the amount of edits is computed with HTER (Snover et al., 2006). We frame the difference between feminine and masculine efforts (\u2206) as our human-centered measure for gender-related quality of service disparities. We also compute statistical significance tests between F and M effort values. We use paired bootstrap resampling (Koehn, 2004) for HTER, and Wilcoxon (Rey and Neuh\u00e4user, 2011) for both HTER and TE, with p-value < 0.05. Tests were calculated for all results presented in the paper, and are all statistically significant.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: HTER distribution across post-edited sentences.\\n\\nFigure 4: Seconds per source word distribution across post-edited sentences.\\n\\nHenceforth, we focus on the particularly biased MTGEN-A sample for multilanguage and multiuser comparisons.\\n\\n4.2 Multilanguage Results\\n\\nMoving onto multilanguage assessments with MTGEN-A, we attest that human-centered disparities are present also for en-de and en-es. Although cumulative results in Table 2 (center) show some variation for TE \u2013 especially for the masculine set \u2013 sentence-level distributions for both types of effort are highly comparable. In figure 3b, median HTERs are the same for en-de/it in the feminine set (14.3), and slightly lower for en-es (12.5). For masculine PE, the median HTER values are systematically 0, although the number of not edited sentences is visibly higher for en-de.\\n\\nMedian temporal efforts based on the number of source words per second are also very close, i.e. always 0 for M; whereas in the feminine PE we find 1.6 (en-it) 1.2 (en-es) 1.1 (en-de) \u2013 see Figure 4b. Overall, differences in efforts based on gender generalize across the considered language pairs.\\n\\n4.3 Multiuser Results\\n\\nAs a last step, we confront the PE activity of professional translators (P) with less experienced high-school students (S). Cumulative results in Table 2 (bottom) show that in the student condition gender gaps widen significantly. More specifically, percentage differences for MTGEN-A en-it go from +177.6% (TE) and +201.8% (HTER) \u2013 assessed with professionals \u2013 up to respectively +329.8% and +636.3% for students. Quite surprisingly, and also confirmed by the distributions in Figures 3c and 4c, students are overall quicker, and edit less across both F and M.\\n\\nWe explain these results by the lower familiarity with both the English language and the PE task itself. In fact, based on observations during the experiments, also confirmed by manual revision of\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the collected post-edits, students did not engage\\nwith the improvement of the overall accuracy of\\nthe translation. Rather, they almost exclusively\\nfocused on adjusting gendered words. Thus, to a\\ncertain extent, students' results allow us to isolate\\neven more neatly the sole effect of gender bias in\\nMT with our human-centered measurements, an\\nissue that might be further amplified should lay\\nusers be involved in similar experiments.\\n\\nDiscussion\\nWe found strong evidence for the human-centered\\nimpact of bias in MT, with a quality of service\\ndisparity that can disproportionately affect women.\\nSuch allocative harm is evident in the extra time\\nand energy required for feminine gender transla-\\ntion. Note that our results are likely conservative,\\ninvolving experienced users with high language\\nproficiency. Indeed, in less controlled conditions,\\nor among individuals with lower proficiency in\\neither target or source language, such a negative\\nimpact would likely be even greater. Misgendered\\nreferences may go unnoticed, propagating errors\\nin texts and communications, or necessitating the\\nuse of external resources such as dictionaries to\\nbe fixed. Due to experimental constraints (\u00a73.1),\\nsuch a scenario remains open to future analyses. To\\nbetter frame the implications of our findings, we\\nconclude with two critical reflections. First, indi-\\nviduals might rely on third-party language services\\nto translate their text, thus raising the question:\\nCan gender bias imply a differential in economic cost?\\nSecond, while informative assessments that cen-\\nter users are crucial to guide the field forward,\\nare current automatic evaluations able to capture such\\nhuman-centered disparities?\\n\\nSomeone has to pay for the cost of gender bias.\\nWe explore the economic costs of F and M post-\\nediting considering two stakeholders:\\n1. a final user, who buys the PE text from\\n2. a third-party translator.\\nAs a case study, we analyze the three\\nen-it datasets edited by professionals (\u00a74.1) \u2013 us-\\nusing averaged HTER and source words shown in\\nTable 3. Note that pricing in the language indus-\\ntry is complex (Lambert and Walker, 2022)\\nand can be based on various parameters (Scansani and\\nMhedhbi, 2020; Cid, 2020). Here, we consider two\\ncommon payment scenarios \u2013 i.e. HTER-Rate\\nand Word-Rate. For both payments, we use a baseline\\nword-rate of $0.09 per source word, reflecting best\\nmarket prices for en-it (Inbox-Translation, 2023).\\n\\nHTER Rate: With this method, prices are ad-\\njusted based on the actual technical effort\\nrequired to post-edit, with lower edit rates leading to lower\\ncosts, and vice versa. Following existing price\\nschemes (Localization, 2022), HTER below 10\\nis paid at 35% of the word rate (i.e. $0.0315 per\\nword), whereas HTER between 10-20 is paid at\\n40% (i.e. $0.036 per word). Hence, and as shown\\nin Table 3 (HTER), feminine PE would cost more.\\n\\nWhile translators are compensated for the addi-\\ntional effort, such a financial burden will inevitably\\nfall on the final user purchasing the F translation.\\n\\nWord Rate: This pricing is based on source text\\nlength, where the cost per word is decided\\na priori. For PE tasks, the word-rates vary depending on the\\ncontent or the language (Sarti et al., 2022).\\nFor en-it data from a general domain such as ours, a\\n35% word rate could be paid. Given that \u2013 to the\\nbest of our knowledge \u2013 this type of pricing does\\nnot consider gendered content, the same word-rate\\nwould be indiscriminately applied to both femi-\\nnine and masculine PE. Thus, as shown in Table 3\\n(Word), a final user buying their translation would\\npay the same price, regardless of gender. However,\\nthis would place the financial cost on the translator,\\nwhose additional effort required for feminine PE\\nwould be underestimated and under-compensated.\\n\\nTo sum up, this analysis shows that gender bias\\nhas an economic cost which can unfairly fall onto\\neither one of the two PE stakeholders. Besides\\nfinancial implications, unfair compensation could\\nalso invite less edits than necessary, thus compro-\\nmising the quality of feminine PE. Analysing such\\npotential quality-oriented implications is a crucial\\naspect for future research.\\n\\n22 Post-editing examples available in Appendix D.\\n23 See Figure 8 in Appendix E.\\n24 e.g. creative texts or certain languages are notably poorly\\nhandled by MT, thus corresponding to higher word-rates.\\n25 Computed using scipy 1.13.1: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Scatter plots with overlaid regression lines of the differences between F and M scores for all datasets, languages and users. Each point represents a sentence-level difference. The correlation between the different metrics is measured with the Pearson $r$ coefficient, and all results are statistically significant ($p$-value $< 0.05$).\\n\\nAutomatic bias measurements do not reliably correlate with human-centered measures. Methods to quantify bias are key to much research that seeks to monitor the creation of equitable technologies (Dev et al., 2022). In this context, growing evidence underscored how intrinsic metrics\u2014focusing on models' representations\u2014might not be a reliable bias indicator in downstream, real-world tasks, as assessed with extrinsic ones\u2014focusing on models' output (Jin et al., 2021; Goldfarb-Tarrant et al., 2021; Cao et al., 2022; Orgad and Belinkov, 2022). Arguably, however, even extrinsic measures are model-centric ($\\\\S$2), and only assumed to reflect more reliably the downstream harms to individuals. We verify this assumption by comparing our human-centered measures of differential effort with the automatic evaluations associated with MT-GenEval and MuST-SHE ($\\\\S$3.1).\\n\\nAs in the original papers, we use the set of contrastive F/M target references to compute gender-related performance differences with BLEU (Papineni et al., 2002), i.e. $\\\\text{BLEU}_F - \\\\text{BLEU}_M$. Scatter plots of the automatic (i.e., BLEU score) and human-centric metrics (i.e., HTER and TE) differences, in absolute values, are reported in Figure 5. We provide aggregate results for all languages, datasets and users.\\n\\nLooking at our results, we notice a Pearson-$r$ of $-0.19$ for $\\\\Delta \\\\text{abs HTER}$ and $\\\\Delta \\\\text{abs BLEU}$ (Figure 5a), and $-0.18$ for $\\\\Delta \\\\text{abs secs}_\\\\text{per_word}$ and $\\\\Delta \\\\text{abs BLEU}$ (Figure 5b). The negative correlation is expected since, while for BLEU the higher the better, the opposite is true for both HTER and TE.\\n\\nStill, the results clearly indicate that both temporal and technical efforts are in weak correlation (Schober et al., 2018) with automatic scores. On the one hand, it is known that time measures may not always have a linear relationship with textual differences measured by automatic metrics (Tatsumi, 2009; Macken et al., 2020), e.g. even small edits can require a high cognitive load and more time. On the other hand, given that both BLEU and HTER capture surface modifications and quantity of edits, their weak correlations are particularly noteworthy.\\n\\nA moderate correlation ($\\\\text{Person-}r 0.54$) is found only between the human-centered measures HTER and TE. As observed in Figure 5c, the higher the number of edits, the more time required.\\n\\nOverall, our results suggest that existing model-centric measures of gender bias in MT might not reliably reflect the downstream harms to users. While the contrastive evaluation approaches explored here have been used to reveal gender gaps (Bentivogli et al., 2020; Currey et al., 2022), they do not correlate with or accurately reflect the magnitude of disparities found through more concrete, human-centered measures.\\n\\nTo ensure that advancements in the field prioritize impacted individuals, future research should explore both the metrics and the data used to compute them (Orgad and Belinkov, 2022). This includes investigating how automatic 29 As a matter of fact, additional results reported in Appendix F.3 show that COMET \u2013 despite its attested higher degree of correlation with human assessments for overall MT quality \u2013 exhibit a very weak correlation with human-centered measures of bias. 30 See also the contrastive, automatic bias scores reported in Table 9 in Appendix F.2.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"metrics relate to human-centered measures and how they can be translated into more transparent, user-relevant evaluations (Liao and Xiao, 2023).\\n\\n6 Conclusion\\n\\nFrom cars' safety measures more effective for men, (Ulfarsson and Mannering, 2004), to virtual reality headsets that are too big to wear (Robertson, 2016), evidence of social and technological advances being less functional for women, or even harmful, abounds (Criado-Perez, 2019). While it is increasingly acknowledged that also language technologies can contribute to broader patterns of gender bias, still little is known about their tangible impact on people. Our study represents a novel effort to empirically examine the implications of gender bias in MT with a human-centered perspective. Previous research has often inferred the downstream impact of bias based on automatic, model-centric scores. In contrast, we provide concrete empirical evidence showing that gender bias in MT leads to tangible service disparities, which can disproportionally affect women. Also, we quantify these disparities using measures that are more meaningful to impacted individuals, such as workload and economic costs. Our study advocates for an understanding of bias and its impact that centers on the actual users of this technology to guide the field. To this aim, we make our collected data and metadata publicly available for future studies on the topic.\\n\\n7 Limitations\\n\\nExperimental construct. To foreground the impact of gender bias, our study employs datasets that include at least one gender translation phenomenon per sentence. While these data more closely simulate our scenarios of interest like the translation of biographies or CVs \u2013 where human gender references are common \u2013 in other contexts such phenomena may be more sparse. Despite potential variations in bias magnitude across different types of text, however, our findings would not change: gender bias would simply be more difficult to detect. Also, while women would likely be the main target of bias-related issues, the found costs and disparities could actually fall on anyone attempting to use feminine expressions, e.g. current attempts to avoid \u201cmasculine default\u201d expressions for generic referents, and rather rely on generically intended feminine forms (Merkel et al., 2017). Overall, since we rely on two widely recognized MT gender bias benchmarks, the density of gender phenomena in our study is actually the same density that is automatically evaluated with current bias metrics.\\n\\nMT system. We prioritize the type of languages, participants and datasets as variables of interest over including MT system comparisons. This choice is also motivated by the fact that gender bias is a widespread issue in generic MT models (Savoldi et al., 2023), and attested with limited variation in commercial MT applications (Rescigno et al., 2020a; Troles and Schmid, 2021). Despite being a commercial system that can limit reproducibility, we pick Google Translate as it represents one of the most used MT engines by the public. Also, we exclude experiments based on instruction-tuned models such as ChatGPT given that the language industry as well as end-users mostly rely on standard MT for core translation tasks (Fishkin, 2023). Also, while \u201cgender-specific translation prompts\u201d could help in the future (S\u00e1nchez et al., 2024), they are currently less realistic as they require users to craft them and \u2013 before that \u2013 to be aware of the presence, and thus the need to control for gender bias in MT.\\n\\nLanguages. Our study focuses on the translation of English sentences into grammatical gender languages that distinguish between masculine and feminine forms to express the gender of human referents (Gygax et al., 2019). As such, we should be cautious in generalizing our findings to languages that mark gender differently, or not at all. Also, we focus on three language pairs (en-it/es/de) that are well-supported by current MT. Hence, it remains open to future investigation if the human-centered impact of gender bias could vary for languages with overall lower MT quality.\\n\\nACL query. The review of prior work on gender (bias) in MT considers only literature from the ACL Anthology. While searching other sources could have enriched our analysis, the Anthology represents the main historical reference point in the field and serves as a good and sufficiently comprehensive litmus test for examining the main trends in NLP.\\n\\nFinally, we discuss the limitations of our binary gender setup in the upcoming section.\\n\\n8 Ethical Statement\\n\\nOur study is limited to binary, feminine and masculine, linguistic expressions of gender. Indeed, 31 This was also confirmed by our study participants.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"this choice, as well as the use of gender as a variable, warrants some ethical reflections. First of all, we stress that \u2013 by working on binary linguistic forms \u2013 we do not imply a binary vision on the extra-linguistic reality of gender and gender identities (D'ignazio and Klein, 2023). The motivation behind our binary design was to ensure comparable conditions between gendered post-edits. While non-binary forms and neutral expressions are increasingly emerging in the target languages of our study (Bonnin and Coronel, 2021; Mirabella et al., 2024; Daems, 2023; Piergentili et al., 2024), the attitude towards these forms, as well as their level of usage can widely vary among speakers (Bonnin and Coronel, 2021; Piergentili et al., 2023b). Given that non-binary and neutral expressions are not standardized like masculine and feminine terms, incorporating them would necessitate controlling for participants' prior familiarity with these forms. This additional variable could introduce cognitive effort complicating the measurement of post-editing effort. By focusing solely on binary gender expressions, we aim to isolate the effort and costs that are exclusively due to the system's bias without confounding it with the potential cognitive load associated with producing non-binary language (Lardelli and Gromann, 2023; Paolucci et al., 2023). While by all means of utmost importance for future research, we were not able for the time being to also account for this cognitive dimension, which would have required additional tools and costs.\\n\\nAcknowledgements\\nBeatrice Savoldi is supported by the PNRR project FAIR - Future AI Research (PE00000013), under the NRRP MUR program funded by the NextGenerationEU. The work presented in this paper is also funded by the Horizon Europe research and innovation programme, under grant agreement No 101135798, project Meetween (My Personal AI Mediator for Virtual MEETings BetWEEN People), and the ERC Consolidator Grant No 101086819. This research was made possible by the participation of several bodies and individuals that took part in our human-centered study. We thank the Directorate-General for Translation (DGT) of the European Commission and the DGT translators that kindly agreed to participate in the activity for en-it. We also thank the independent professional translators that worked with us across all language pairs, as well as the high-school students that participated in our laboratories, thus contributing to the multiuser experiments. Finally, we thank Jasmijn Bastings for the insightful discussion on and contribution to the gender bias papers' review.\\n\\nReferences\\nBashar Alhafni, Nizar Habash, and Houda Bouamor. 2022a. The Arabic parallel gender corpus 2.0: Extensions and analyses. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1870\u20131884, Marseille, France. European Language Resources Association.\\n\\nBashar Alhafni, Nizar Habash, and Houda Bouamor. 2022b. User-centric gender rewriting. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 618\u2013631, Seattle, United States. Association for Computational Linguistics.\\n\\nBashar Alhafni, Ossama Obeid, and Nizar Habash. 2023. The user-aware Arabic gender rewriter. In Proceedings of the First Workshop on Gender-Inclusive Translation Technologies, pages 3\u201311, Tampere, Finland. European Association for Machine Translation.\\n\\nSultan Alrowili and Vijay Shanker. 2022. Generative approach for gender-rewriting task with ArabicT5. In Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP), pages 491\u2013495, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\\n\\nChantal Amrhein, Florian Schottmann, Rico Sennrich, and Samuel L\u00e4ubli. 2023. Exploiting biased models to de-bias text: A gender-fair rewriting model. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4486\u20134506, Toronto, Canada. Association for Computational Linguistics.\\n\\nGiuseppe Attanasio, Flor Miriam Plaza del Arco, Debora Nozza, and Anne Lauscher. 2023. A tale of pronouns: Interpretability informs gender bias mitigation for fairer instruction-tuned machine translation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3996\u20134014, Singapore. Association for Computational Linguistics.\\n\\nSolon Barocas, Kate Crawford, Aaron Shapiro, and Hanna Wallach. 2017. The Problem With Bias: Allocative Versus Representational Harms in Machine Learning. In SIGCIS Conference, Philadelphia, Pennsylvania.\\n\\nChristine Basta, Marta R. Costa-juss\u00e0, and Jos\u00e9 A. R. Fonollosa. 2020. Towards mitigating gender bias in a decoder-based neural machine translation model by adding contextual information. In Proceedings of the The Fourth Widening Natural Language Processing.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-1002", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-1002", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rand Fishkin. 2023. We analyzed millions of chatgpt user sessions: Visits are down 29\\\\% since May. Programming assistance is 30\\\\% of use. Accessed: 2024-06-14.\\n\\nDennis Fucci, Marco Gaido, Sara Papi, Mauro Cettolo, Matteo Negri, and Luisa Bentivogli. 2023. Integrating language models into direct speech translation: An inference-time solution to control gender inflection. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11505\u201311517, Singapore. Association for Computational Linguistics.\\n\\nMarco Gaido, Beatrice Savoldi, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2020. Breeding gender-aware direct speech translation systems. In Proceedings of the 28th International Conference on Computational Linguistics, pages 3951\u20133964, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nMarco Gaido, Beatrice Savoldi, Luisa Bentivogli, Matteo Negri, and Marco Turchi. 2021. How to split: the effect of word segmentation on gender bias in speech translation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3576\u20133589, Online. Association for Computational Linguistics.\\n\\nHarritxu Gete and Thierry Etchegoyhen. 2023. An evaluation of source factors in concatenation-based context-aware neural machine translation. In Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing, pages 399\u2013407, Varna, Bulgaria. INCOMA Ltd., Shoumen, Bulgaria.\\n\\nHarritxu Gete, Thierry Etchegoyhen, David Ponce, Gorka Labaka, Nora Aranberri, Ander Corral, Xabier Saralegi, Igor Ellakuria, and Maite Martin. 2022. TANDO: A corpus for document-level machine translation. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3026\u20133037, Marseille, France. European Language Resources Association.\\n\\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ricardo Mu\u00f1oz S\u00e1nchez, Mugdha Pandya, and Adam Lopez. 2021. Intrinsic bias metrics do not correlate with application bias. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1926\u20131940, Online. Association for Computational Linguistics.\\n\\nHila Gonen and Kellie Webster. 2020. Automatically identifying gender issues in machine translation using perturbations. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1991\u20131995, Online. Association for Computational Linguistics.\\n\\nAna Valeria Gonz\u00e1lez, Maria Barrett, Rasmus Hvin\u00e6gelby, Kellie Webster, and Anders S\u00f8gaard. 2020. Type B reflexivization as an unambiguous testbed for multilingual multi-task gender bias. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2637\u20132648, Online. Association for Computational Linguistics.\\n\\nNavita Goyal, Eleftheria Briakou, Amanda Liu, Connor Baumler, Claire Bonial, Jeffrey Micher, Clare Voss, Marine Carpuat, and Hal Daum\u00e9 III. 2023. What else do i need to know? the effect of background information on users' reliance on qa systems. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3313\u20133330.\\n\\nDagmar Gromann, Manuel Lardelli, Katta Spiel, Sabrina Burtscher, Lukas Daniel Klausner, Arthur Mettinger, Igor Miladinovic, Sigrid Schefer-Wenzl, Daniela Duh, and Katharina B\u00fchn. 2023. Participatory research as a path to community-informed, gender-fair machine translation. In Proceedings of the First Workshop on Gender-Inclusive Translation Technologies, pages 49\u201359, Tampere, Finland. European Association for Machine Translation.\\n\\nAna Guerberof-Arenas and Joss Moorkens. 2023. Ethics and machine translation: The end user perspective. In Towards Responsible Machine Translation: Ethical and Legal Considerations in Machine Translation, pages 113\u2013133. Springer.\\n\\nPascal Mark Gygax, Daniel Elmiger, Sandrine Zufferey, Alan Garnham, Sabine Sczesny, Lisa von Stockhausen, Friederike Braun, and Jane Oakhill. 2019. A language index of grammatical gender dimensions to study the impact of grammatical gender on the way we perceive women and men. Frontiers in Psychology, 10.\\n\\nNizar Habash, Houda Bouamor, and Christine Chung. 2019. Automatic gender identification and reinflection in Arabic. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 155\u2013165, Florence, Italy. Association for Computational Linguistics.\\n\\nDirk Hovy, Federico Bianchi, and Tommaso Fornaciari. 2020. \u201cyou sound just like your father\u201d commercial machine translation systems include stylistic bases. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1686\u20131690, Online. Association for Computational Linguistics.\\n\\nDirk Hovy and Shannon L. Spruit. 2016. The social impact of natural language processing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 591\u2013598, Berlin, Germany. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-1002", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-1002", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"language models. In Proceedings of the 25th Annual Conference of the European Association for Machine Translation (Volume 1), pages 300\u2013314, Sheffield, UK. European Association for Machine Translation (EAMT).\\n\\nJeff Pitman. 2021. Google translate: One billion installs, one billion stories. https://blog.google/products/translate/new-features-make-translate-more-accessible-for-its-1-billion-users/. Engineering Manager, Google Translate.\\n\\nMartin Popel. 2018. CUNI transformer neural MT system for WMT18. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 482\u2013487, Belgium, Brussels. Association for Computational Linguistics.\\n\\nMaja Popovi\u0107. 2021. Agree to disagree: Analysis of inter-annotator disagreements in human evaluation of machine translation output. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 234\u2013243, Online. Association for Computational Linguistics.\\n\\nMaja Popovic and Ekaterina Lapshinova-Koltunski. 2024. Gender and bias in Amazon review translations: by humans, MT systems and ChatGPT. In Proceedings of the 2nd International Workshop on Gender-Inclusive Translation Technologies, pages 22\u201330, Sheffield, United Kingdom. European Association for Machine Translation (EAMT).\\n\\nMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013191, Brussels, Belgium. Association for Computational Linguistics.\\n\\nPaul C Price, RS Jhangiani, IA Chiang, Dana C Leighton, and Carrie Cuttler. 2017. Research methods in psychology (3rd american edition). Washington: PressBooksPublications.\\n\\nElla Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia Specia, and Shuly Wintner. 2017. Personalized machine translation: Preserving original author traits. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1074\u20131084, Valencia, Spain. Association for Computational Linguistics.\\n\\nKrithika Ramesh, Gauri Gupta, and Sanjay Singh. 2021. Evaluating gender bias in Hindi-English machine translation. In Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 16\u201323, Online. Association for Computational Linguistics.\\n\\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online. Association for Computational Linguistics.\\n\\nAdithya Renduchintala, Denise Diaz, Kenneth Heafield, Xian Li, and Mona Diab. 2021. Gender bias amplification during speed-quality optimization in neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 99\u2013109, Online. Association for Computational Linguistics.\\n\\nAdithya Renduchintala and Adina Williams. 2022. Investigating failures of automatic translation in the case of unambiguous gender. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3454\u20133469, Dublin, Ireland. Association for Computational Linguistics.\\n\\nArgentina Anna Rescigno, Vanmassenhove Eva, Johanna Monti, Andy Way, et al. 2020a. A case study of natural gender phenomena in translation\u2014a comparison of google translate, bing microsoft translator and deepl for english to italian, french and spanish. In CEUR Workshop Proceedings, pages 359\u2013364. AILC-Associazione Italiana di Linguistica Computazionale.\\n\\nArgentina Anna Rescigno, Johanna Monti, Andy Way, and Eva Vanmassenhove. 2020b. A case study of natural gender phenomena in translation: A comparison of Google Translate, bing Microsoft translator and DeepL for English to Italian, French and Spanish. In Workshop on the Impact of Machine Translation (iMpacT 2020), pages 62\u201390, Virtual. Association for Machine Translation in the Americas.\\n\\nDenise Rey and Markus Neuh\u00e4user. 2011. Wilcoxon-Signed-Rank Test, pages 1658\u20131659. Springer Berlin Heidelberg, Berlin, Heidelberg.\\n\\nAdi Robertson. 2016. Building for virtual reality? don\u2019t forget about women. The Verge.\\n\\nSamantha Robertson, Wesley Hanwen Deng, Timnit Gebru, Margaret Mitchell, Daniel J Liebling, Michal Lavav, Katherine Heller, Mark D\u00edaz, Samy Bengio, and Niloufar Salehi. 2021. Three directions for the design of human-centered machine translation. Google Research.\\n\\nSandra Sandoval, Jieyu Zhao, Marine Carpuat, and Hal Daum\u00e9 III. 2023. A rose by any other name would not smell as sweet: Social bias in names mistranslation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3933\u20133945, Singapore. Association for Computational Linguistics.\\n\\nGabriele Sarti, Arianna Bisazza, Ana Guerberof-Arenas, and Antonio Toral. 2022. DivEMT: Neural machine translation post-editing effort across typologically diverse languages. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7795\u20137816, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gabriele Sarti, Nils Feldhus, Ludwig Sickert, and Os- \\nkar van der Wal. 2023a. Inseq: An interpretability \\ntoolkit for sequence generation models. In \\nProceedings of the 61st Annual Meeting of the Association \\nfor Computational Linguistics (Volume 3: System \\nDemonstrations), pages 421\u2013435, Toronto, Canada. \\nAssociation for Computational Linguistics.\\n\\nGabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, \\nAnna Currey, Georgiana Dinu, and Maria Nadejde. \\n2023b. RAMP: Retrieval and attribute-marking en-\\nhanced prompting for attribute-controlled translation. \\nIn Proceedings of the 61st Annual Meeting of the \\nAssociation for Computational Linguistics (Volume 2: Short Papers), \\npages 1476\u20131490, Toronto, Canada. Association for Computational Linguistics.\\n\\nDanielle Saunders and Bill Byrne. 2020. Reducing gen-\\nder bias in neural machine translation as a domain \\nadaptation problem. In Proceedings of the 58th An-\\nnual Meeting of the Association for Computational \\nLinguistics, pages 7724\u20137736, Online. Association \\nfor Computational Linguistics.\\n\\nDanielle Saunders and Katrina Olsen. 2023. Gender, \\nnames and other mysteries: Towards the ambiguous \\nfor gender-inclusive translation. In Proceedings of \\nthe First Workshop on Gender-Inclusive Translation \\nTechnologies, pages 85\u201393, Tampere, Finland. Euro-\\nporean Association for Machine Translation.\\n\\nDanielle Saunders, Rosie Sallis, and Bill Byrne. 2020. \\nNeural machine translation doesn't translate gender \\ncoreference right unless you make it. In Proceedings \\nof the Second Workshop on Gender Bias in Natural \\nLanguage Processing, pages 35\u201343, Barcelona, Spain \\n(Online). Association for Computational Linguistics.\\n\\nDanielle Saunders, Rosie Sallis, and Bill Byrne. 2022. \\nFirst the worst: Finding better gender translations dur-\\ning beam search. In Findings of the Association for \\nComputational Linguistics: ACL 2022, pages 3814\u2013 \\n3823, Dublin, Ireland. Association for Computational \\nLinguistics.\\n\\nBeatrice Savoldi, Marco Gaido, Luisa Bentivogli, Mat-\\nteo Negri, and Marco Turchi. 2021. Gender bias in \\nmachine translation. Transactions of the Association \\nfor Computational Linguistics, 9:845\u2013874.\\n\\nBeatrice Savoldi, Marco Gaido, Luisa Bentivogli, Mat-\\nteo Negri, and Marco Turchi. 2022a. On the dy-\\nnamics of gender learning in speech translation. In \\nProceedings of the 4th Workshop on Gender Bias \\nin Natural Language Processing (GeBNLP), pages \\n94\u2013111, Seattle, Washington. Association for Com-\\nputational Linguistics.\\n\\nBeatrice Savoldi, Marco Gaido, Matteo Negri, and Luisa \\nBentivogli. 2023. Test suites task: Evaluation of \\ngender fairness in MT with MuST-SHE and INES. \\nIn Proceedings of the Eighth Conference on Machine \\nTranslation, pages 252\u2013262, Singapore. Association \\nfor Computational Linguistics.\\n\\nBeatrice Savoldi, Andrea Piergentili, Dennis Fucci, Mat-\\nteo Negri, and Luisa Bentivogli. 2024. A prompt \\nresponse to the demand for automatic gender-neutral \\ntranslation. In Proceedings of the 18th Conference of \\nthe European Chapter of the Association for Compu-\\ntational Linguistics (Volume 2: Short Papers), pages \\n256\u2013267, St. Julian's, Malta. Association for Com-\\nputational Linguistics.\\n\\nRandy Scansani and Lamis Mhedhbi. 2020. How do \\nlsps compute mt discounts? presenting a company's \\npipeline and its use. In Proceedings of the 22nd \\nannual conference of the European Association for \\nMachine Translation, pages 393\u2013401.\\n\\nPatrick Schober, Christa Boer, and Lothar A. Schwarte. \\n2018. Correlation coefficients: Appropriate \\nuse and interpretation. Anesthesia & Analgesia, \\n126:1763\u20131768.\\n\\nShanya Sharma, Manan Dey, and Koustuv Sinha. 2022. \\nHow sensitive are translation systems to extra con-\\ntexts? mitigating gender bias in neural machine trans-\\nlation models through relevant contexts. In \\nFindings of the Association for Computational Linguistics: \\nEMNLP 2022, pages 1968\u20131984, Abu Dhabi, United \\nArab Emirates. Association for Computational Lin-\\nguistics.\\n\\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and \\nNanyun Peng. 2021. Societal biases in language \\ngeneration: Progress and challenges. In Proceedings \\nof the 59th Annual Meeting of the Association for \\nComputational Linguistics and the 11th International \\nJoint Conference on Natural Language Processing \\n(Volume 1: Long Papers), pages 4275\u20134293, Online. \\nAssociation for Computational Linguistics.\\n\\nPushpdeep Singh. 2023. Gender inflected or bias in-\\nflicted: On using grammatical gender cues for bias \\nevaluation in machine translation. In Proceedings \\nof the 13th International Joint Conference on Nat-\\nural Language Processing and the 3rd Conference \\nof the Asia-Pacific Chapter of the Association for \\nComputational Linguistics: Student Research Work-\\nshop, pages 17\u201323, Nusa Dua, Bali. Association for \\nComputational Linguistics.\\n\\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\\nnea Micciulla, and John Makhoul. 2006. A study \\nof translation edit rate with targeted human annota-\\ntion. In Proceedings of the 7th Conference of the \\nAssociation for Machine Translation in the Americas: \\nTechnical Papers, pages 223\u2013231.\\n\\nCelia Soler Uguet, Fred Bane, Mahmoud Aymo, \\nJo\u00e3o Pedro Fernandes Torres, Anna Zaretskaya, and \\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, and Os-\\nkar van der Wal.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"T\u00e0nia Blanch Mir\u00f3. 2023. Enhancing gender representation in neural machine translation: A comparative analysis of annotating strategies for English-Spanish and English-Polish language pairs. In Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track, pages 171\u2013172, Macau SAR, China. Asia-Pacific Association for Machine Translation.\\n\\nAgnes S\u00f3lmundsd\u00f3ttir, Dagbj\u00f6rt Gu\u00f0mundsd\u00f3ttir, Lilja Bj\u00f6rk Stef\u00e1nsd\u00f3ttir, and Anton Ingason. 2022. Mean machine translations: On gender bias in Icelandic machine translations. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3113\u20133121, Marseille, France. European Language Resources Association.\\n\\nArt\u00fbrs Stafanovi\u010ds, Toms Bergmanis, and M\u00afarcis Pinnis. 2020. Mitigating gender bias in machine translation with target gender annotations. In Proceedings of the Fifth Conference on Machine Translation, pages 629\u2013638, Online. Association for Computational Linguistics.\\n\\nKarolina Stanczak and Isabelle Augenstein. 2021. A survey on gender bias in Natural Language Processing. arXiv preprint arXiv:2112.14168.\\n\\nGabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679\u20131684, Florence, Italy. Association for Computational Linguistics.\\n\\nDario Stojanovski, Benno Krojer, Denis Peskov, and Alexander Fraser. 2020. ContraCAT: Contrastive coreference analytical templates for machine translation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4732\u20134749, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang. 2019. Mitigating gender bias in natural language processing: Literature review. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1630\u20131640, Florence, Italy. Association for Computational Linguistics.\\n\\nEduardo S\u00e1nchez, Pierre Andrews, Pontus Stenetorp, Mikel Artetxe, and Marta R. Costa-juss\u00e0. 2024. Gender-specific machine translation with large language models.\\n\\nMidori Tatsumi. 2009. Correlation between automatic evaluation metric scores, post-editing speed, and some other factors. In Proceedings of Machine Translation Summit XII: Posters.\\n\\nBertille Triboulet and Pierrette Bouillon. 2023. Evaluating the impact of stereotypes and language combinations on gender bias occurrence in NMT generic systems. In Proceedings of the Third Workshop on Language Technology for Equality, Diversity and Inclusion, pages 62\u201370, Varna, Bulgaria. INCOMA Ltd., Shoumen, Bulgaria.\\n\\nJonas-Dario Troles and Ute Schmid. 2021. Extending challenge sets to uncover gender bias in machine translation: Impact of stereotypical verbs and adjectives. In Proceedings of the Sixth Conference on Machine Translation, pages 531\u2013541, Online. Association for Computational Linguistics.\\n\\nGudmundur F Ulfarsson and Fred L Mannering. 2004. Differences in male and female injury severities in sport-utility vehicle, minivan, pickup and passenger car accidents. Accident Analysis & Prevention, 36(2):135\u2013147.\\n\\nJannis Vamvas and Rico Sennrich. 2021. Contrastive conditioning for assessing disambiguation in MT: A case study of distilled bias. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10246\u201310265, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nMarlies van der Wees, Arianna Bisazza, and Christof Monz. 2016. Measuring the effect of conversational aspects on machine translation quality. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2571\u20132581, Osaka, Japan. The COLING 2016 Organizing Committee.\\n\\nEva Vanmassenhove, Chris Emmery, and Dimitar Shterionov. 2021a. NeuTral Rewriter: A rule-based and neural approach to automatic rewriting into gender neutral alternatives. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8940\u20138948, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nEva Vanmassenhove, Christian Hardmeier, and Andy Way. 2018. Getting gender right in neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3003\u20133008, Brussels, Belgium. Association for Computational Linguistics.\\n\\nEva Vanmassenhove and Johanna Monti. 2021. gENder-IT: An annotated English-Italian parallel challenge set for cross-linguistic natural gender phenomena. In Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 1\u20137, Online. Association for Computational Linguistics.\\n\\nEva Vanmassenhove, Dimitar Shterionov, and Matthew Gwilliam. 2021b. Machine translationese: Effects of algorithmic bias on linguistic complexity in machine translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2203\u20132213, Online. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Leonor Veloso, Luisa Coheur, and Rui Ribeiro. 2023. A rewriting approach for gender inclusivity in Portuguese. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8747\u20138759, Singapore. Association for Computational Linguistics.\\n\\nSebastian Vincent. 2021. Towards personalised and document-level machine translation of dialogue. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 137\u2013147, Online. Association for Computational Linguistics.\\n\\nSebastian Vincent, Robert Flynn, and Carolina Scarton. 2023. MTCue: Learning zero-shot control of extra-textual attributes by leveraging unstructured context in neural machine translation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 8210\u20138226, Toronto, Canada. Association for Computational Linguistics.\\n\\nSebastian T. Vincent, Lo\u00efc Barrault, and Carolina Scarton. 2022. Controlling extra-textual attributes about dialogue participants: A case study of English-to-Polish neural machine translation. In Proceedings of the 23rd Annual Conference of the European Association for Machine Translation, pages 121\u2013130, Ghent, Belgium. European Association for Machine Translation.\\n\\nEric Peter Wairagala, Jonathan Mukiibi, Jeremy Francis Tusubira, Claire Babirye, Joyce Nakatumba-Nabende, Andrew Katumba, and Ivan Ssenkungu. 2022. Gender bias evaluation in Luganda-English machine translation. In Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track), pages 274\u2013286, Orlando, USA. Association for Machine Translation in the Americas.\\n\\nAngelina Wang, Xuechunzi Bai, Solon Barocas, and Su Lin Blodgett. 2024. Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways.\\n\\nJun Wang, Benjamin Rubinstein, and Trevor Cohn. 2022. Measuring and mitigating name biases in neural machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2576\u20132590, Dublin, Ireland. Association for Computational Linguistics.\\n\\nLongyue Wang, Siyou Liu, Mingzhou Xu, Linfeng Song, Shuming Shi, and Zhaopeng Tu. 2023. A survey on zero pronoun translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3325\u20133339, Toronto, Canada. Association for Computational Linguistics.\\n\\nRachel Wicks and Matt Post. 2023. Identifying context-dependent translations for evaluation set production. In Proceedings of the Eighth Conference on Machine Translation, pages 452\u2013467, Singapore. Association for Computational Linguistics.\\n\\nGuillaume Wisniewski, Lichao Zhu, Nicolas Bailler, and Fran\u00e7ois Yvon. 2021a. Screening gender transfer in neural machine translation. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 311\u2013321, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nGuillaume Wisniewski, Lichao Zhu, Nicolas Ballier, and Fran\u00e7ois Yvon. 2021b. Biais de genre dans un syst\u00e8me de traduction automatique neuronale : une \u00e9tude pr\u00e9liminaire (gender bias in neural translation: a preliminary study). In Actes de la 28e Conf\u00e9rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf\u00e9rence principale, pages 11\u201325, Lille, France. ATALA.\\n\\nGuillaume Wisniewski, Lichao Zhu, Nicolas Ballier, and Fran\u00e7ois Yvon. 2022a. Analyzing gender translation errors to identify information flows between the encoder and decoder of a NMT system. In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 153\u2013163, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\\n\\nGuillaume Wisniewski, Lichao Zhu, Nicolas Ballier, and Fran\u00e7ois Yvon. 2022b. Biais de genre dans un syst\u00e8me de traduction automatique neuronale : une \u00e9tude des m\u00e9canismes de transfert cross-langue [gender bias in a neural machine translation system: a study of crosslingual transfer mechanisms]. In Traitement Automatique des Langues, Volume 63, Num\u00e9ro 1 : Varia [Varia], pages 37\u201361, France. ATALA (Association pour le Traitement Automatique des Langues).\\n\\nLichao Zhu, Guillaume Wisniewski, Nicolas Ballier, and Fran\u00e7ois Yvon. 2022. Flux d'informations dans les syst\u00e8mes encodeur-d\u00e9codeur. application \u00e0 l'explication des biais de genre dans les syst\u00e8mes de traduction automatique. (information flow in encoder-decoder systems applied to the explanation of gender bias in machine translation systems). In Actes de la 29e Conf\u00e9rence sur le Traitement Automatique des Langues Naturelles. Atelier TAL et Humanit\u00e9s Num\u00e9riques (TAL-HN), pages 10\u201318, Avignon, France. ATALA.\\n\\nA Details on ACL Anthology Search\\n\\nOur ACL search is based on the combination of keywords displayed in Table 4. Note that we also include terms such as \u201crewriters\u201d, which several works apply to the output of MT models as a bias mitigation strategy to offer double feminine and masculine outputs. To avoid retrieving unrelated works that only marginally mentioned MT or gender in the main body, the searches parsed only the title and abstract of the queried papers.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Number of search results for each specific keyword combination on the ACL anthology. In total, we find 347 results comprising 251 unique articles, of which 146 were discarded as out of scope.\\n\\nManual selection\\nWe retrieved a total of 251 unique articles. Of those, we discarded all unrelated papers that refer to e.g. inductive bias, bias length, or \\\"translation\\\", but not in relation to the MT task. We thus arrive at a total of 105 papers.\\n\\nThe whole selection was carried out manually, and we annotated both papers that matched the query focusing on human assessment as well as those that did not, so not to overlook any paper involving humans. We defined the papers to be considered in-scope as follows:\\n\\n\u2022 MT application: we only keep those works that primarily focused on MT, whereas those that relied on MT as an intermediate tool (e.g. to automatically translate a set of data) are discarded.\\n\\n\u2022 Modality: while limited in number, we keep also MT beyond the text-to-text modality.\\n\\n\u2022 Gender (bias): we include in our selection all works that focus on gender translation in the context of human entities. This includes works that do not explicitly engage with the notion of social bias \u2013 especially prior to 2018. Papers more broadly addressing gender fairness and inclusivity are also included.\\n\\nThe full list of extracted papers that made our final selection is provided below. The first in-scope papers date back to 2016, whereas the latest two are from 2024. As of April, in fact, only few papers had been included in the Anthology. These 2024 papers are thus not shown in the figure to avoid incomplete views on approaches for the present year.\\n\\nMT gender bias papers, no human assessment\\nvan der Wees et al. (2016); Rabinovich et al. (2017); Bawden (2017); Popel (2018); Michel and Neubig (2018); Vanmassenhove et al. (2018); Moryossef et al. (2019); Escud\u00e9 Font and Costa-juss\u00e0 (2019); Cho et al. (2019); Habash et al. (2019); Stafanovi\u0107 et al. (2020); Basta et al. (2020); Costa-juss\u00e0 and de Jorge (2020); Saunders et al. (2020); Gonen and Webster (2020); Stojanovski et al. (2020); Rescigno et al. (2020b); Bentivogli et al. (2020); Saunders and Byrne (2020); Hovy et al. (2020); Gonz\u00e1lez et al. (2020); Costa-juss\u00e0 et al. (2020); Troles and Schmid (2021); Savoldi et al. (2021); Wisniewski et al. (2021b); Ciora et al. (2021); Escolano et al. (2021); Ramesh et al. (2021); Levy et al. (2021); Gaido et al. (2021); Vanmassenhove et al. (2021b); Vincent (2021); Renduchintala et al. (2021); Castilho et al. (2021); Wisniewski et al. (2021a); Vanmassenhove and Monti (2021); Wisniewski et al. (2022b); Costa-juss\u00e0 et al. (2022); Castilho (2022); Gete et al. (2022); S\u00f3l mundsd\u00f3tir et al. (2022); Savoldi et al. (2022a); M\u010deura (2022); Corral and Saralegi (2022); Mohammadshahi et al. (2022); Saunders et al. (2022); Karpinski et al. (2022); Zhu et al. (2022); Sharma et al. (2022); Wisniewski et al. (2022a); Vincent et al. (2022); Wang et al. (2022); Renduchintala and Williams (2022); Alrowili and Shanker (2022); Alhafni et al. (2022a); Gete and Etchegoyhen (2023); Dinh and Niehues (2023); Singh (2023); Iluz et al. (2023); Alhafni et al. (2023); Sandoval et al. (2023); Wicks and Post (2023); Piergentili et al. (2023a); Saunders and Olsen (2023); Kostikova et al. (2023); Cabrera and Niehues (2023); Fucci et al. (2023); Lu et al. (2023); Castilho et al. (2023); Paulo et al. (2023); Le et al. (2023); Sarti et al. (2023b); Vincent et al. (2023); Costa-juss\u00e0 et al. (2023a); Attanasio et al. (2023); Lee et al. (2023); Wang et al. (2023); Veloso et al. (2023); Sarti et al. (2023a).\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B Experimental details\\n\\nB.1 Data Details\\n\\nHere we provide additional information concerning the selection of the data used in our experiments (\u00a7B.1.1). Also, some minor corrections were made on the MTGEN-A reference translation (\u00a7B.1.2).\\n\\nB.1.1 Data selection\\n\\n**MTGenEval-A selection**\\nThe 250 sentences used in our en-it experiments represent a randomly selected sample of the \u201cambiguous\u201d section of the original MTGenEval dataset (Currey et al., 2022). For the multilanguage experiments, we also maximize the overlap between en-it/es/de subsets. Overall, we retrieve 76 sentences which are common across all languages, whereas the remaining are randomly extracted within each monolingual portion of the original dataset.\\n\\n**MTGenEval-UN selection**\\nThe MTGEN-UN sample used in our experiments was randomly extracted from the \u201cunambiguous\u201d section of the original MTGenEval corpus. Note that, by being a subset with unambiguous gender in the English source, for this sample we extract 250 pairs of sentences, for a total of 500. To exemplify, each pair corresponds to i) a feminine <source-target> segment (e.g. en: \u201cSarandon has appeared in two episodes of The Simpsons, once as herself and...\u201d, it: \u201cSarandon \u00e8 apparsa in due episodi dei Simpson, una volta interpretando se stessa...\u201d), and ii) a masculine <source-target> segment (e.g. en: \u201cSarandon has appeared in two episodes of The Simpsons, once as himself and...\u201d, it: \u201cSarandon \u00e8 apparso in due episodi dei Simpson, una volta interpretando se stesso...\u201d). We automatically translate with GT the total 500 English sentences and create the corresponding feminine and masculine samples of 250 sentences each to be post-edited.\\n\\n**MuST-SHE selection**\\nFor MUST-SHE, which by design contains an higher variety of gender phenomena for several parts of speech we relied on preliminary filters to ensure a less noisy experimental environment. Namely, we excluded sentences that in the original dataset are annotated as \u201cFREE-REF\u201d, and for which the human reference translation is known to be quite creative and less literal. Also, prior work based on this dataset has shown that \u2013 due to its higher variability \u2013 a good amount of gendered words available in the reference translation might not be actually generated in the MT output for a range of reasons, i.e. errors, synonyms etc (Savoldi et al., 2022b). Thus, first we translated the whole corpus with Google Translate. Then, we only retained those sentences where the MT output contained at least one gendered word annotated in the corresponding reference translations. To do so, we relied on the coverage evaluation script made available with the corpus. Overall, these filters ensured i) the presence of gender phenomena to revise during the PE task, ii) less creative reference translation that eased more reliable assessments with automatic metrics. The final 250 sentences were randomly extracted from this pre-filtered MuST-SHE subset.\\n\\nB.1.2 MTGenEval-A reference translations\\n\\nFor MTGEN-A, we find that for some English sentences not all ambiguous human entities are translated with masculine or feminine gender in the corresponding reference of the M/F contrastive pair. We thus manually revised all reference translations for the 3 en-it/es/de datasets. This is necessary to align the results of our PE activity \u2013 where all entities whose gender is ambiguous in English are post-edited either as masculine or feminine \u2013 with the automatic bias evaluation method presented in Section 5, which is based on the reference translations. To exemplify, see the following en-es segment:\\n\\n**src-en:** The doctor and some of the patients had signed off to purchase it\\n\\n**tgt-es-F:** La doctora y algunos de los pacientes se hab\u00edan apuntado para comprarlo.\\n\\n**tgt-es-M:** El doctor y algunos de los pacientes se hab\u00edan apuntado para comprarlo.\\n\\nWhile \u201cdoctor\u201d is respectively translated as masculine or feminine in the corresponding references,\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the equally ambiguous \u201csome of the patients\u201d is not, and rather remains masculine in both references.\\nTo fix these instances, for each of the 250 source sentences included in the en-it, en-es and en-de datasets, we manually revised both reference translations.\\n\\nThis was carried out by a linguist with expertise in all language pairs. Overall, 40 segments were modified for en-it, 15 for en-es, and 28 for en-de.\\n\\nB.2 Matecat tool and settings\\nTo work in Matecat, we created two separate projects for each dataset: one for the feminine setting and one for the masculine setting. For each project, we followed the same procedure. We uploaded the input English text and created a corresponding dedicated Translation Memory (TMX).\\n\\nThe TMX contains the translations produced by GT, which are shown to the translators as suggestions to post-edit. Crucially, we ensured our settings as follows:\\n\\ni) each translator had access to the dedicated TMX in a \u201clookup-only\u201d mode, meaning that they could not update it with their post-edits \u2013 which would have otherwise become visible to the other translators and make the experiment ineffective; also,\\n\\nii) the general Matecat TMX was disabled, so as to avoid that translators had access to additional suggestions other than the GT outputs.;\\n\\niii) to ensure that the Matecat tool would maintain the original sentence division of the dataset, we activated the paragraph setting, which does not re-segment the input text. Finally, each M/F project was split into sub-projects of around 15 sentences each to be assigned to participants (14 splits for MTGEN-A, 16 for MTGEN-UN, and 16 for MUST-SHE). Each participant received two links to work on both an M and an F sub-project, for a total of around 30 sentences to post-edit.\\n\\nB.3 Automatic Metrics\\nThe automatic metrics used to evaluate translation quality are BLEU, (Papineni et al., 2002), based on n-gram matching, TER (Olive, 2005), based on edit rates, and the neural-based COMET (Rei et al., 2020). BLEU and TER are computed with the well-established tool for evaluating machine translation outputs, sacrebleu v2.4.0 (Post, 2018).\\n\\nCOMET is computed using the official GitHub repository\\n\\nhttps://github.com/Unbabel/COMET\\n\\n| Year of Experience | en-it | en-es | en-de |\\n|-------------------|-------|-------|-------|\\n| 1-4               | 0%    | 0%    | 0%    |\\n| 5-9               | 25%   | 25%   | 25%   |\\n| 10-14             | 50%   | 50%   | 50%   |\\n| 15-19             | 75%   | 75%   | 75%   |\\n| 20-29             | 100%  | 100%  | 100%  |\\n| 30-39             |       |       |       |\\n\\nFigure 6: Professional translators\u2019 years of experience as translators, and as MT post-editors. Results are shown for each language pair.\\n\\nC Study participants\\nWe relied on two types of participants in our experiments: professional translators and high school students. As for translators, the experiment included professionals who participated on a voluntary basis as well as paid professionals. To ensure comparability, we replicated the same settings and used the same guidelines across all conditions. For students, we added a warm-up phase to introduce them to MT, the PE task, and the Matecat tool.\\n\\nAll the experiments were agreed upon with all participants. The privacy protection of the involved participants is guaranteed by the complete anonymity of the whole collected data, which make it impossible to identify the involved subjects.\\n\\nC.1 Recruitment and Task organization\\nProfessional translators (volunteers) for en-it, a first round of experiments was carried out with professional translators from the European...\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Commission, Directorate-General for Translation, Italian-language Department. These participated on a voluntary basis as part of an educational lab held by the authors of this paper. As such, no compensation was involved.\\n\\nTo carry out experiments on MTGEN-A, MTGEN-UN, and MUST-SHE, we needed data from 14 + 16 + 16 participants, respectively, for a total of 46 participants. However, eventually 22 blocks of sentences (corresponding to the activity of 11 participants) were not carried out or completed. This was due to several reasons: some expected participants were absent, others experienced internet connection problems that hindered them to properly carry out the PE activity, and one participant decided not to take part in the experiment. Thus, in order to complete our data collection, we resorted to paid professional translators.\\n\\nProfessional translators (paid)\\nThe remaining en-it data and all en-es and en-de data were post-edited by paid professionals, who were recruited via a translation agency. The only eligibility criterion we required was that the en-* pair assigned to them represented one of their main language direction in their professional work, and that they were native speakers of the target language (i.e. the same working condition of volunteers). The experiments were carried out via online meetings, in groups of around 8 translators. To avoid introducing any confounding effect that could influence their PE work, all post-editors were requested to remain in the meeting for its entire duration of 50 minutes, and compensation was time-based. The total cost (translation agency recruitment and translator's work) amounted to \u20ac50 per post-editor, taxes excluded.\\n\\nThe similarities of the work carried out by the two types of professional translators, verified as discussed in Appendix C.2, allowed us to merge all en-it data coming from professionals and carry out aggregated dataset-level analyses.\\n\\nStudents (volunteers)\\nThe activity of the students was carried out during a laboratory as a part of their school activity. These students were from a school offering a foreign language specialization, thus ensuring that they had a good (B2 level) proficiency in English. They were all part of the same class, attending the penultimate year of high school. All the activities were allowed under the consensus of their school supervisor and under the supervision of their regular teachers. For this task setting, we also included a warm-up phase to introduce the students to MT, the PE task, and the Matecat tool before starting the experiments.\\n\\nC.1.1 Participant Statistics\\nFor each pair of languages, in Figure 6 we provide the years of experience of the involved professionals, both as translators (i.e. translating from scratch) as well as MT post-editors. In line with overall statistics in field, 38 women make up the majority of involved translators (77%). We did not enforce balanced gender distributions in the recruitment process and did not deem the gender of the translators as a significant variable. Indeed, feminine and masculine lexical terms are equally standard, grammatical forms used to refer to human referents, which are part of the current language. This is also confirmed by prior work (Popovic and Lapshinova-Koltunski, 2024), which did not find translator's gender to be an indicative factor in gender translation. Participants were only instructed to use them in translation according to the provided gender information for each sentence. No personal information was collected for students.\\n\\nC.2 PE effort across voluntary and paid professionals\\nGiven that the PE activity for en-it is carried out by both paid and non-paid professionals (see Appendix C.1), we want to ensure that the two conditions are comparable. For this reason, we collected a control subset of sentences \u2013 edited by both paid professionals and voluntary professionals \u2013 to compare the PE results across these two potentially different types of subjects. To do so, we have paid translators redo 125 sentences for MTGEN-A, which is the dataset upon which most of our experiments are based. Hence, we collect an additional set of 300 post-edited sentences (i.e. the same 125 source sentences correspond to 125 F post-edits and 125 M post-edits).\\n\\nResults are reported in Table 5. As we can see, the type of professional does not appear as a significant confounding variable. In absolute numbers, the two sets are highly comparable, with only a 6-minute difference in TE, and less than 1 HTER score ($\\\\Delta_{\\\\text{abs}}$).\\n\\n38https://www.linkedin.com/pulse/lets-talk-gender-equality-translation-industry-josephine-matser/\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Comparative post-editing results for 125 sentences en-it on MTGEN-A, carried out by the group of voluntary professional translators and the second setting of paid professional translators. We provide time to edit (TE, i.e. hour:minutes) and HTER.\\n\\n|                | VOLUNTARY PROFESSIONALS | PAID PROFESSIONALS |\\n|----------------|-------------------------|-------------------|\\n| **TE**         | 1:13 0:27               | 1:07 0:26         |\\n| **HTER**       | 0:46                    | 0:40              |\\n| **MASC**       | 170.40 14.31 2.39       | 150.95 17.71 4.93 |\\n| **FEM**        | 12.78                   | 11.92             |\\n| **\u2206abs**       |                         |                   |\\n| **\u2206rel**       |                         |                   |\\n\\nFigure 7: Post-editing example for a source English sentence, which is common across all language pairs. Given the source English sentence, we show the GT automatic translation, and its associated feminine and masculine post-edits. For en-it, we show post-editing by both professionals (P) and students (S). In bold, we show gender-related words in the source, output, and post-edited sentences. For the post-edits, we show deletions and insertions.\\n\\nGiven the results of this analysis, we could safely merge the data coming from both types of translators to compose the final en-it datasets. For MTGEN-A, the 125 common sentences that we decided to keep for the main experiments are those post-edited by the professional translators, so as to allow for higher comparability with the fully \u201cpaid\u201d en-es/de data samples.\\n\\n**Student and Professional PE**\\n\\nStill in Figure 7, we show a typical behavioural difference that we attest between types of users for en-it. Namely, between professional translators (P) and less experienced students (S). As discussed in \u00a74.3, we find that students post-edited less (i.e. lower number of edits and in less time) compared to professionals. As a matter of fact, students did not engage with the improvement of the overall quality of the sentence, most likely due to their lower English proficiency, and rather mainly looked at the Italian target to fix gendered translation. In fact, in the provided example (the en-it blocks at the top), the GT output provided a poor translation for \u201coverseeing\u201d \u2013 rendered as \u201csupervisionando\u201d, which...\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is suboptimal in terms of fluency, overall also impacting the adequacy and readability of the sentence. Indeed, for both feminine and masculine PE, professionals carried out a light post-editing that also ensured an alternative translation for that portion of the sentence, whereas it was overlooked by students. Overall, since the adjustments made by students were basically only gender-related, the attested gender disparities measured with HTER and TE become even more visible.\\n\\nTo calculate HTER-based payments, we rely on the discount rates reported in Figure 8. The matrix is publicly available and based on Localization (2022). Note that discount rates can vary across companies. We compare the matrix with the HTER discounts used by other major language service providers. Such rates however cannot be divulged as they are internal to the company and reserved. Overall, we find that the used scheme is highly aligned with those from other private companies and \u2013 if anything \u2013 it is more conservative, with a limited number of HTER ranges.\\n\\nIn Table 6 we report overall translation quality results obtained by Google Translate for all datasets and languages. We used the original target reference translation to compute the results. Details on automatic metrics computation are available in Appendix B.3.\\n\\nWe report contrastive, reference-based gender bias results computed with different metrics in Table 9. For details on the metrics computation, please refer to Appendix B.3.\\n\\n| Language | Dataset | BLEU | EM | FEM | MAS | TER | COMET | \u2206abs | \u2206rel |\\n|----------|---------|-----|----|-----|-----|-----|-------|------|------|\\n| en-it    | MUST - SHE | 40.64 | 47.54 | 84.56 | | | | | |\\n| en-it    | MTGEN - UN | 43.92 | 42.92 | 82.31 | | | | | |\\n| en-it    | MTGEN - A | 35.77 | 50.44 | 84.75 | | | | | |\\n| en-es    | MTGEN - A | 49.72 | 34.2 | 85.29 | | | | | |\\n| en-de    | MTGEN - A | 36.04 | 49.35 | 84.28 | | | | | |\\n\\nAs expected, and in line with our post-editing results discussed in \u00a74, the unambiguous dataset MTGEN - UN obtains the smallest difference in scores. Overall, by looking at the differences in score computed against the feminine and masculine references also automatic evaluation methods confirm that GT exhibits gender bias, leading to a higher generation of masculine forms. However, we immediately see that the magnitude of such differences is notably small compared to our human-centered results reported in the main experiments of the paper (see \u00a74). This is particularly true for COMET, which is less sensitive to surface differences, such morphological gender-related differences. Overall, however, none of these metrics appear particularly sensitive at capturing gender differences, which are at best framed as +26.79 percentage difference as measured with TER (see MTGEN - A for en-es). To further investigate this point, in the upcoming Appendix F.3 we verify the correlation between automatic scores and our human-centered measures.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"F.3 Correlation with automatic metrics\\n\\nF.3.1 Aggregated results with COMET and TER scores\\n\\nAs already discussed in Section 5, performance differences in automatic metrics show a weak correlation with differences in human-centric metrics. This trend is reconfirmed by both COMET and TER scores, as shown in Figure 9. Here, we still present aggregate results computed for all datasets, languages, and types of users.\\n\\nFor the differences in COMET, we observe a relatively sparse distribution in Figure 9.a, with a Pearson-\\\\(r\\\\) coefficient of \\\\(-0.12\\\\), meaning a very weak negative correlation, against HTER. Similarly, the Pearson-\\\\(r\\\\) coefficient against temporal effort (seconds per word) is \\\\(-0.17\\\\), which is slightly higher but still represents a very weak correlation.\\n\\nEven in the case of COMET, the correlation is negative because lower scores are better, while the opposite is true for HTER and sec_per_word. Moreover, when compared to Figure 5, we observe a very similar behavior of BLEU (\u00a75, Figure 5) with the one shown by COMET in Figure 9.a and 9.b, resembling similar distributions. Looking at TER differences, the samples of the distributions are slightly more squeezed towards the regression line. This means that the correlation is slightly higher but, however, still remaining very weak, both considering HTER (\\\\(r = 0.14\\\\)), and secs_per_word (\\\\(r = 0.18\\\\)). In this case, the correlations are positive since the higher TER scores the better, similar to human-centric metrics.\\n\\nF.3.2 BLEU Results per dataset\\n\\nWe report language, users, and dataset-wise results of the correlations between the automatic metric BLEU and the human-centric metrics HTER and secs_per_word. Similar trends are also shown for COMET and TER, as discussed in Appendix F.3.1. Pearson correlation coefficients for each combination are shown in Table 10. Language-wise correlations on MTGEN-A are shown in Figure 7 while dataset-wise correlations on MTGENEVAL_UN and MUST-SHE for en-it are shown in Figure 8.\\n\\nIn Section 5, we elaborated on the weak correlations between automatic metrics such as BLEU scores and temporal and technical effort metrics such as HTER and seconds per word (SPW). When looking at the correlation results for each dataset, we observe similar trends: only HTER and SPW are moderately correlated while automatic and temp...\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: Pearson R Coefficients of correlations between $\\\\Delta^{\\\\text{abs}}$ BLEU, $\\\\Delta^{\\\\text{abs}}$ HTER and $\\\\Delta^{\\\\text{abs}}$ SPW (secs_per_word), for the different datasets and languages analyzed in the paper. Non-statistically significant results are indicated with $\\\\times$.\\n\\nPorcelain/technical effort metrics exhibit no or weak correlation, with also some non-statically significant results. Therefore, the conclusions drawn when looking at aggregated statistics are similar to those obtained individually for each dataset.\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure (7): Scatter plots with overlaid regression lines for all languages on MTGEN -A.\\n\\n(a) $\\\\Delta$abs HTER and BLEU\\n(b) $\\\\Delta$abs secs_per_word and BLEU\\n(c) $\\\\Delta$abs HTER and secs_per_word\"}"}
{"id": "emnlp-2024-main-1002", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure (8): Scatter plots with overlaid regression lines on MTGEN -A en-it (S), MTGEN -UN and MUST -SHE for en-it.\"}"}
