{"id": "lrec-2022-1-29", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generating Questions from Wikidata Triples\\n\\nKelvin Han, Thiago Castro Ferreira, Claire Gardent\\nCNRS/LORIA, Universit\u00e9 de Lorraine, Nancy, France,\\naiXplain, inc., Federal University of Minas Gerais, Belo Horizonte, Brazil\\nhuiyuan.han@loria.fr, thiagocf05@ufmg.br, claire.gardent@loria.fr\\n\\nAbstract\\n\\nQuestion generation from knowledge bases (or knowledge base question generation, KBQG) is the task of generating questions from structured database information, typically in the form of triples representing facts. To handle rare entities and generalize to unseen properties, previous work on KBQG resorted to extensive, often ad-hoc pre- and post-processing of the input triple. We revisit KBQG \u2013 using pre-training, a new (triple, question) dataset and taking question type into account \u2013 and show that our approach outperforms previous work both in a standard and in a zero-shot setting. We also show that the extended KBQG dataset (also helpful for knowledge base question answering) we provide allows not only for better coverage in terms of knowledge base (KB) properties but also for increased output variability in that it permits the generation of multiple questions from the same KB triple. Our code and dataset can be found at:\\nhttps://gitlab.inria.fr/hankelvin/wikidataqg\\n\\nKeywords: question generation, knowledge bases, KBQG, Wikidata\\n\\n1. Introduction\\n\\nWith the rise of large scale knowledge bases (KBs) such as Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), DBpedia (Auer et al., 2007) and Cyc (Lenat and Guha, 1993), large amounts of factual data has become available which can be used to answer factual questions. In that context, teaching machines to generate a question from a KB item (question generation from KB, KBQG) has become an important issue with multiple potential applications. By translating a KB fact (e.g., (HENRY POINCAR\u00c9, BIRTHPLACE, FRANCE)) into a natural language (NL) question (e.g., Where was Henri Poincar\u00e9 born?), KBQG facilitates access to KBs by non experts. It could help improve the ability of dialog models to ask factual questions and support the development of tutoring systems that ask the user a series of questions about some KB entity. Finally, it is useful for creating or augmenting the sets of (KB content, NL question) pairs necessary to train Question Answering (QA) systems on KBs (KBQA). However the scale of these knowledge bases, the high number of rare entities they contain and the lack of NL aliases for KB relations still leave this task a challenging problem.\\n\\nThe state of the art in KBQG have mainly focused on how to address these rare entity and unknown relation issues. Typically, the KB input is enriched with lexicalization information extracted from the KB (semantic type of the entities, domain and range of the relations) or/and using distant supervision from comparable KB/NL data (Elsahar et al., 2018; Liu et al., 2019; Serban et al., 2016). Delexicalization has also been commonly used, where KB entities are replaced with placeholders both in the input and in the output text (see table 13). The model is trained on the delexicalized data and at inference time, post-processing replaces placeholders with the corresponding values (Elsahar et al., 2018; Liu et al., 2019; Serban et al., 2016).\\n\\nYet even if these approaches have yielded good results, they require extensive, often ad-hoc, pre- and post-processing techniques to be effective, increasing the complexity of the model. Moreover, these additional methods might not be generic enough to scale up to new databases with other schema and broader richness. Delexicalization for instance, which requires matching KB entities (e.g., Barack Obama) in the input with their corresponding NL mentions in the output text (e.g., the former President of the United States) may be quite complex and may also result in incorrect or incomplete delexicalizations when applied to a new KB. Similarly, distant supervision is only possible given some comparable data and might only provide partial information. In fact, (Liu et al., 2019) notes that (Elsahar et al., 2018)'s distant supervision approach only provides textual information for 44% of the predicates present in the SimpleQuestion dataset they use for training. Finally, the presence and coverage of type, domain and range information that are relevant for the generation of NL questions varies depending on the database and might not be sufficient to support the verbalization of unknown entities or relations (i.e., entities and relations which have not been seen at training time).\\n\\nIn recent years, pre-training has been shown to be effective for providing neural models with additional information about the structure of natural language and improving generative tasks (Dong et al., 2019; Song et al., 2019; Lawrence et al., 2019). In this paper, we leverage pre-training to provide a model for KBQG which requires neither delexicalizing the training and test data nor enriching the KB input with additional information. We use BART, a Transformer-based encoder-decoder pre-trained using a denoising objective on large quantities of text, and we propose an approach to the KBQG task which differs from pre-\"}"}
{"id": "lrec-2022-1-29", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Input/Output Examples: 1 and 2 show how the same input triple may map to multiple questions with different question types.\\n\\nOur approach outperforms previous approaches in a zero-shot setting for KB properties and entity types (i.e., for KB facts whose property/entity type does not occur in the training data); that additional data increases coverage (more KB properties can be accounted for); and that controlling generation using question type helps improve diversity (one KB triple can be used to produce multiple questions).\\n\\nRelated Work\\n\\nEarly work (Olney et al., 2012; Seyler et al., 2015; Song and Zhao, 2016; Seyler et al., 2017) on KBQG used hand-crafted templates which requires significant human effort, generalizes poorly and is difficult to scale up. Recently, neural models have been proposed which are trained on corpora of (KB triple, NL question) pairs and do not require manual intervention. (Reddy et al., 2017) used an RNN sequence-to-sequence model to convert a set of keywords about a Freebase subgraph into a question. Within the Semantic Web community, (Kumar et al., 2019) introduced a neural question generator over knowledge graph where the complexity of the output can be controlled. (Serban et al., 2016) first trained a recurrent encoder-decoder network with attention on SimpleQuestions dataset (Bordes et al., 2015). To handle unseen entities, they used a placeholder for the subject entity in the question and train on the delexicalized data. (Elsahar et al., 2018) focused on generalisation in a zero-shot setting. To handle unseen entities and properties, they enriched the KB input with a lexicalisation of the input KB property obtained through distant supervision and with the Freebase type of the input subject and object entities. They also delexicalized the data by replacing matching terms in this additional information and the output questions with placeholders, replacing these by their value after inference. The RDF triples are initialized with learned TransE (Bordes et al., 2013) embeddings. Two separate encoders are used for the RDF and the textual context and the decoder attends to both. (Liu et al., 2019) expanded the contextual information used by (Elsahar et al., 2018) with information about the domain and the range of the input property. To improve question specificity, they propose an answer-aware loss by optimizing the cross-entropy between the generated question and the answer type words. Finally, (Bi et al., 2020) developed an encoder-decoder question generator, which also enriches the input facts with additional information, and constrains the decoder with word types to preserve the adequacy of the generated question.\"}"}
{"id": "lrec-2022-1-29", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"279\\n\\n(Vrande\u02c7ci\u00b4c and Kr \u00a8otzsch, 2014), currently one of the largest and most prominent collection of open data on the web. Moreover, we also enrich the standardized datasets with additional typing and lexicalization information, not present in the original versions to allow comparison with previous approaches.\\n\\nIn this section, we first describe the creation and content of these three datasets and how they were mapped to Wikidata. We then explain how these datasets were enriched with additional typing and lexicalization information to help guide generation.\\n\\n4.1. (RDF, Question) Datasets\\n\\nSQ.\\n\\nSimpleQuestions (SQ, (Bordes et al., 2015)) is a benchmark of KBQG models. It comprises 108K pairs between a triple from Freebase (Bollacker et al., 2008), a KB which is no longer maintained, and an NL question collected by crowdsourcing.\\n\\nWe transformed the original SQ dataset (SQ-FB) into its Wikidata version (SQ hereafter) by mapping its entity-pairs into Wikidata using the P646 property (Free-Base ID). We noticed that a small set of Freebase entities map to more than one entity in Wikidata; to resolve these subject entity ambiguities we used token overlap with the target question.\\n\\nNext, we identified the set of all Wikidata properties between each entity pair. The entity pairs (in Wikidata format), are then grouped by their Freebase property (FB cluster). First, we consider all the one-to-one relations; if all entity-pairs in a Freebase cluster with Freebase property $p_{fb}$ share the same WKD property $p_{wkd}$, we map $p_{fb}$ to $p_{wkd}$. If on the other hand not all pairs of the FB cluster are related by the same WKD property, we check whether there exists a Wikidata property shared by at least 75% of the FB cluster. If this is the case, we assign this Wikidata property to all pairs of the FB cluster. Otherwise, we manually inspect the set of Wikidata properties associated with the FB cluster to decide whether to keep (assign one property to the group or from a set of properties for members of the group), or discard the found Wikidata properties. This was done for the forward direction first, and for the remaining unassigned Freebase entity pairs, we repeated the process in the backward direction. Finally, for the remaining entity-pairs without any Wikidata relations between them, we use the Freebase-to-Wikidata property mappings already found.\\n\\nWe note that in the original corpus, the question focus is always the object of the triple. When converting from FreeBase to Wikidata, it sometimes shifted from being the object of the RDF triple to being the subject. We ensured that the relevant information in such samples are appropriately reversed. The final Wikidata RDF triple is represented by the English-language labels for the entities and property of the triple.\\n\\nZQ.\\n\\nZeroshotRE is a KBQA corpus consisting of several questions generated based on 1,192 crowdsourced question templates (e.g., \\\"Where did x graduate from?\\\", \\\"In which university did x study?\\\" and \\\"What is x's alma mater?\\\") for 120 Wikidata properties. We used the version of it that is included in the KILT benchmark (Petroni et al., 2020), publicly available in the HuggingFace datasets library.\\n\\nAlthough the dataset is already in the Wikidata format, its test set comprises of only RDF triples, whose question focus is missing. For example, the question 'Which award did Hrant Melkumyan get?' is paired with the incomplete RDF triple (H $\\\\text{RANTMELKUMYAN}$, AWARD RECEIVED, $\\\\text{0x0}$). To circumvent this, we used SPARQL queries to retrieve the answer set for these questions in Wikidata. For those with more than one possible answer, we instantiated new samples in the corpus.\\n\\nWQ.\\n\\nThe WebNLG dataset (Gardent et al., 2017b) is another popular data-to-text benchmark, with natural language assertions to 3,790 unique RDF triples as well as their combinations. We collected a data-to-question version of this corpus, comprising 11,664 NL questions to 3,625 (95.6%) of the single RDF triples of version 2.1 of the original corpus.\\n\\nThe questions were collected using the AMT crowdworking platform. Participants were asked to produce a question for an RDF triple given a question type and a question focus to ensure wide coverage of such questions which is our aim for WKDQG. In terms of question focus, they were given both subject and object parts of the RDF triples to ask for (e.g., ($\\\\text{ALBBENNIEJONES}$, ACTIVE YEARS END, 1950) \u2192 Which singer closed out her career in 1950? and ($\\\\text{ALBBENNIEJONES}$, ACTIVE YEARSEND, 1950) \u2192 When did Albennie Jones' career come to a close? respectively).\\n\\nAs the WebNLG triples come from the DBpedia knowledge base, we mapped their entities and properties into Wikidata using a process similar to that for the SQ dataset. Details are given in the appendices.\\n\\nA small set of 217 of these incomplete RDFs (534 samples) remained without answers (from the time ZeroshotRE was created to present, the subject in the RDF no longer holds the property); they are marked \\\"NoAnswer\\\" in the dataset and excluded from our experiments.\\n\\nThe question focuses were classified using a coarse-grained set of KB types (Location, Organization, Person, and Event/Date, Measure, Number as well as an Other category) and mapped to the corresponding question types from What, When, Where, Which, Who, How many.\"}"}
{"id": "lrec-2022-1-29", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Dataset | Question Size | prop. ent. Size | Vocab. Size |\\n|---------|---------------|----------------|-------------|\\n| SQ      | 53,624        | 1.0/1.0/2.0   | 196         |\\n| WQ      | 10,272        | 2.26/1.0/4.0  | 184         |\\n| ZQ      | 282,543       | 1.22/1.0/4.0  | 120         |\\n| TOTAL   | 346,439       | 1.19/1.0/4.0  | 342         |\\n\\nTable 1: Datasets (S/O/S&O denotes the proportion of entities occupying the subject, object or subject as well as object positions of the triples in the data. Vocab. Size is the number of distinct subword-tokens in the NL question part of the data, Question Size is the number of word tokens in each NL question.)\\n\\nDue to differences in the data models of DBpedia and Wikidata, 412 out of the 3,625 WebNLG original single triples could not be mapped into Wikidata as their properties have no or multiple counterparts in WikiData. Another set of 90 single triples map into a smaller set of 45 Wikidata triples.\\n\\nAs a result, only 10,272 RDF-Q pairs remained in WQ after the mapping.\\n\\n4.2. Adding typing and lexicalization information\\n\\nFor all datasets, we enrich each RDF-question pair with question type and the semantic type of its question focus. This information was obtained from the Wikidata public SPARQL endpoint in the second half of 2021.\\n\\nFor SQ, we also include the additional information released by (Elsahar et al., 2018).\\n\\n**Question type.** SQ, WQ and ZQ contain What, When, Where, Which and Who questions, whereas WQ also contains quantity-seeking questions (e.g. 'How many pages is the novel A Long Long Way?'). We detect these question types with regular expressions/string match.\\n\\nBesides these question types, SQ and ZQ contain 'inform-me' questions (e.g. 'The date of birth of Glyn Pardoe is?' and 'Name a modern jazz singer.') and polar questions ('Is highly refined pirates a post-rock album?') as well; in our work, we label these questions as being of the Other type.\\n\\n**Question focus type.** For SQ, we use the Freebase entity type information contained in the version of the original dataset released by (Elsahar et al., 2018) (see Section 4.2) to allow for a comparison with Elsahar et al's model. For WQ and ZQ, we retrieved the set of Wikidata supertypes for every entity in the dataset from Wikidata using the 'instance of' (P31) and 'subclass of' (P279) properties. If an entity has multiple possible supertypes, we select the one that is the most common across the training split of the dataset for our experiments. Table 1 shows some statistics about each dataset and Table 2 about the question type distribution.\\n\\n**SQ additional information.** When comparing our approach with (Elsahar et al., 2018) on the SQ dataset, we use the additional lexical information they released. This comprises verbalizations of the RDF property, obtained by distant supervision, as well as the Freebase semantic type of the RDF entities. For SQ instances without such additional information, we used a special token to represent the missing information.\\n\\n**5. Approach**\\n\\nInstead of enriching the input with NL information as was done in previous work, we leverage advances in pre-training and use the WKGQ dataset to adapt the BART pre-trained model to KBQG. BART (Lewis et al., 2020) is a Transformer-based encoder-decoder using sub-word tokenization (byte-pair encoding) and was trained using a generative denoising objective on a combination of news, Wikipedia and books.\\n\\nIn adapting BART for question generation from RDF input, we explore two main options: one where only the RDF is used as input (BART rdf) and another (for the SQ dataset only) where the RDF input is enriched with the additional NL information provided as a support for the lexicalization of the RDF content by (Elsahar et al., 2018) (BART rdf + nl).\"}"}
{"id": "lrec-2022-1-29", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We also explore variants regarding the question type: (BARTrd f), (BARTrd f,qt) and (BARTrd f,mtl) which we describe below; Table 13 in the appendices provide examples of the inputs provided to each of these models.\\n\\nBARTrd f. This is a BART model without modification which takes as input an RDF triple and a token indicating the question focus position. The RDF is represented linearly in the \\\\((s, p, o)\\\\) order with a special token separator ('|') between each of them.\\n\\nBARTrd f,qt. The input to BARTrd f,qt is the concatenation of the input RDF triple, a token representing the question focus position, the semantic type of the question focus (e.g., musical artist, location) and a special control token for the target question type. We separated each of these four fields with markers in the input. The addition of the question type information is equivalent to an oracle setting when evaluating on the SQ test set. Our interest in it is motivated by its usefulness for generating varied questions (see Section 7.3).\\n\\nBARTrd f,mtl. BARTrd f,qt requires that the type of the question that can be generated from an RDF triple be known (since the question type is part of its input). We also explore a setting where this requirement is lifted by using multi-task learning where QG is the main task and predicting the question type is an auxiliary task. The input to both tasks is the concatenation of the triple with the question focus position and the question focus type. The model is trained by minimizing the weighted sum of the loss from the main KBQG task and this auxiliary task. We found that a 0.3 weight for the auxiliary task and 0.7 for the QG task to perform best.\\n\\n6. Experiments\\n\\nTraining Details\\n\\nFor all our experiments, we used the bart-base model from the HuggingFace transformers library. The bart-base model has six layers each in its Transformer encoder and decoder blocks with a hidden size of 768. We used the bart-base tokenizer with a vocabulary size of 50,282 tokens (having added special tokens for the RDF separator, question and question focus types). We fine-tune all of the models by minimizing the standard cross-entropy loss of the outputs against the targets. We use the ADAMW optimizer with a learning rate of 0.0002, and a learning rate scheduler with a linear warm-up of 10% of the training steps. All of the bart-base models were trained for 10 epochs, and tuned on the BLEU-4 score of the development set.\\n\\nElsahar's Model. We compare our approach with the BART model with (Elsahar et al., 2018), an RNN based encoder-decoder model with attention as well as delexicalization. The model is trained with the delexicalized output, at inference time the decoder output is relexicalized. In the RDF-only setting, only information about the RDF (in the form of TransE pretrained embeddings) is provided to the model. Word-based tokenization was used, and word tokens were represented with pretrained 100-D GLOVE embeddings (Pennington et al., 2014). For the RDF triple, pretrained Wikidata embeddings released by (Han et al., 2018) were used to represent the elements of the triples. In the RDF+NL setting, the additional lexicalization information about the property and the entities (see Section 4.1) is added to the RDF input using separate encoders and GLOVE embeddings for the lexicalization part of the input. We used the publicly released code by (Elsahar et al., 2018), making only changes to load the pretrained Wikidata KB embeddings and ensuring that their and our decoders are not constrained by a max length in order to allow comparability.\\n\\nAutomatic Evaluation\\n\\nTo evaluate the models' outputs, we used the BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Denkowski and Lavie, 2014) automatic metrics. These n-gram based measures are widely used as indicators of the surface similarity (BLEU-4 and ROUGE-L) and paraphrase (METEOR) between a model's generated output and a reference. We also report the BERTScore (Zhang et al., 2020) metric, which using a BERT model, provides an indicator of the embedding-based semantic similarity between output and reference. To ensure direct comparability on the outputs from the BART model (subword-based tokenization) and Elsahar's (word-based), we applied the MOSES detokenizer on all the models' outputs and references, as well as a set of regular expression rules on Elsahar's model outputs to detokenize contractions and possessives (e.g. don't) not handled by the MOSES detokenizer, before scoring with the automatic metrics.\\n\\nHuman Evaluation\\n\\nWe also conducted a human evaluation to assess to what extent the question type control token and the variability present in the WQ dataset (one triple can be mapped to multiple questions with different question type) can help generate questions of different types from the same triple. In this evaluation, we compare our best model trained on SQ only with the same model trained on all three datasets (SQ, WQ, ZQ). Our hypothesis is that WQ variability will help the second model learn to generate different types of questions for the same triple. We collect\"}"}
{"id": "lrec-2022-1-29", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from the output of both models 50 randomly selected input triples covering different properties. We show the annotators the input triple, the reference question and the output of the two models, and we ask them which of the two outputs verbalizes the input property best (A: Adequacy); differs most from the reference question (D: Difference); is most natural (N: Naturalness) and verbalizes the entities best (E: Entity). A third option is possible for cases where both output score similarly. We measure the percentage of time one model was chosen over the other, taking the majority agreement between three evaluators.\\n\\n7. Results and Discussion\\n\\nWe first compare our approach with Elsahar\u2019s on SQ considering four settings: with and without NL information, on seen data (RDF properties present in the test data have been seen at training time), and in a property zero-shot setting (RDF properties present in the test data have not been seen at training time). We also examine the seen and zero-shot settings for entity types.\\n\\n7.1. With and without additional NL information on Seen data\\n\\nTable 3 shows the results for the standard setting, where RDF properties present in the test data have been seen at training time.\\n\\nPre-training helps bridge the gap between RDF properties and their lexicalization. Our approach outperforms Elsahar\u2019s in both settings (with and without additional lexicalization information in the input). Moreover, BARTrdf, which uses no additional information, yield comparable results to Elsahar\u2019s model which uses additional NL information. This indicates that pre-training and word pieces suffice to bridge the gap between the name of the RDF properties and the way they are lexicalized in text. It also shows that despite the high ratio of named entities in RDF triples (the subject and object, which make up two thirds of an RDF triple, usually are named entities), delexicalization (used by Elsahar\u2019s) can successfully be replaced by these two methods: both pre-training and word pieces help the model generate names that might not have been seen at training time.\\n\\nQuestion type helps improve performance. Whether it is implicitly learned through multi-task learning (BARTrdf,mtl, BARTrdf+nl,mtl) or explicitly input to the model (BARTrdf,qt, BARTrdf+nl,qt), informing the model with the target question type improves performance.\\n\\n7.2. Zero-Shot Learning\\n\\nWe used the same cross validation approach as (Elsahar et al., 2018) to approximate a zero-shot property setting. Specifically, we split the SQ dataset into 10 folds, with mutually exclusive sets of RDF properties (no fold contains RDF properties found in another fold) and draw two of these folds in turn for use as the test set in each cross-validation run. At the start of each run, we reload the original parameter weights for the pre-trained BART model. Following Elsahar, we repeat the same zero-shot setting on entity types (which is stricter than a zero-shot entity set-up), taking care to account for the fact that a SimpleQuestions triple in Freebase may have a different order when mapped to Wikidata. Table 4 includes the mean and standard deviation of the automatic metrics from cycling through these data splits for a zero-shot property setting. Table 5 shows the same but for the zero-shot entity type settings.\\n\\nPre-training outperforms a delexicalization model whose input is enriched with lexicalization information. Regardless of a zero-shot property or zero-shot entity type setting, our approach outperforms Elsahar\u2019s whether or not the input is enriched with lexicalization information. Notably, the BART model without NL information (BARTrdf) performs on par with Elsahar\u2019s model with NL information (Elsahar nl). This illustrates the capacity of pretrained decoders based on subword units to handle unseen units: while the RDF properties of the test data have not been seen at training time, their subword units probably have been and can be used by the decoder to generate the corresponding NL expressions. There is however, a 10-BLEU point gap between the zero-shot property and zero-shot entity type settings for the top performing model (BARTrdf+nl,qt), indicating the importance of lexicalization data for KB properties.\\n\\n7.3. Additional data\\n\\nThe main contribution of the extended dataset WQDQ (in particular WQ) is that it helps generate multiple questions from the same KB triple. In SQ, each RDF is only paired with a question of a single type as well as a single question focus (either the subject or the object). Accordingly, a model trained (and evalu-\"}"}
{"id": "lrec-2022-1-29", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Results on the SQ dataset under a zero-shot setting for RDF properties.\\n\\n| Model          | Sub-type | Obj-type | Elsahar | BART | BART | BART | BART |\\n|----------------|----------|----------|---------|------|------|------|------|\\n| RDF-only       |          |          |         |      |      |      |      |\\n| Elsahar        |          |          | 29.96   | 32.90| 33.21| 37.30|      |\\n| BART rd f      |          |          | 23.94   | 30.40| 31.07| 35.05|      |\\n| BART rd f,mtl  |          |          | 24.43   | 30.96| 31.92| 35.05|      |\\n| BART rd f,qt   |          |          | 24.43   | 31.24| 33.21| 37.30|      |\\n| RDF+NL         |          |          |         |      |      |      |      |\\n| Elsahar nl     |          |          | 20.54   | 23.42| 26.63| 28.74|      |\\n| BART rd f      |          |          | 27.32   | 27.46| 27.46| 28.74|      |\\n| BART rd f,mtl  |          |          | 27.32   | 27.46| 27.46| 28.74|      |\\n| BART rd f,qt   |          |          | 27.32   | 27.46| 27.46| 28.74|      |\\n\\nTable 5: Results on the SQ dataset under a zero-shot setting for RDF entities (subject/object for Elsahar, question focus and the other entity in the triple for the BART models).\\n\\nTable 6: Results on the SQ dataset under a SEEN setting. BART rd f,qt,wkdqg: model fine-tuned on the WKDQG data.\\n\\nTable 7: Human evaluation on 50 outputs (D:Difference from the reference, A:Semantically Adequate, N:Naturalness, E:Entity Lexicalizations).\\n\\nFor each criteria, the first two lines of the columns indicate which model is preferred. E.g., BART rd f,qt,wkdqg's output is judged more different from the reference 76% of the time than BART rd f,qt's.\"}"}
{"id": "lrec-2022-1-29", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tables 6 (automatic evaluation) and 7 (human evaluation) show the results of this experiment, and Table 10 in the appendices contains examples of the generated outputs here. While the automatic metrics show significantly lower scores for the Test A setting since the question type of the generated questions are different from the reference, the human evaluation indicates that the BART model generates questions with a comparable level of adequacy (A), naturalness (N) and entities (E) than in a Test O setting but a greater difference with the reference. The agreement among the three annotators was 0.521 (Fleiss' kappa). This demonstrates that by controlling for the question type and using training data with greater variability, KBQG models can be used to generate varied questions of high quality which differ from the reference.\\n\\nTable 8: Ablation Study: each line indicates the (non cumulative) removal of the corresponding component from BART on the SQ dataset\\n\\n| Component | BLEU-4 | ROUGE-1 | ROUGE-2 | ROUGE-L |\\n|-----------|--------|---------|---------|---------|\\n| None      | 41.95  | 73.51   | 71.21   | 36.78   |\\n| Question type | 37.73  | 69.72   | 65.41   | 34.51   |\\n| Question focus | 41.43  | 73.17   | 70.81   | 36.54   |\\n\\n7.4. Downstream QA Evaluation\\n\\nWe evaluated the utility of our generated varied questions on the performance of two downstream QA systems \u2013 KEQA (Huang et al., 2019) and BuboQA (Mohammed et al., 2018), leveraging the approach and code of (Han et al., 2020). While we generate questions using Wikidata triples to enrich the SQ dataset, all of the QA experiments described here use Freebase data. The results of these experiments are summarised in Table 9 here and details are provided in Table 11.\\n\\nUsing the same Test A set as in Section-7.3 while leaving the train and development sets unchanged, we show that simply changing the distribution of the question types at inference time results in a 3.5% drop in top-1 accuracy (see SQ w1 and SQ w2 in Table 11).\\n\\nWe were able to reverse this drop in performance on Test A by using an enriched training and development set. We do this using the same question type prediction model above, and using the obtained set of plausible question type tokens as controls for the BART model, we generated the set of paraphrased questions of (different question types) for each SQ sample. We also show that this enrichment approach enables robust QA performance \u2013 in the face of a shift in the\\n\\n| Model  | Acc @ 1 |\\n|--------|---------|\\n| BuboQA | 85.12   |\\n| KEQA   | 86.85   |\\n\\nTable 9: Results of QA systems with and without questions generated with our approach. The column headers denote the train-dev-test set compositions. O denotes original, A alternative and E enriched sets of questions.\\n\\nconsistent with the findings of (Liu et al., 2019), we find that enriching the training data with generated questions leads to a minor decline (\u2248 0.5 percentage points) in top-1 accuracy.\\n\\n7.5. Ablation\\n\\nWe also performed an ablation study using the SQ dataset (cf. table 8) and find that removing the question focus from the input has limited negative impact (-0.52 BLEU-4) but that removing the question type control token leads to a strong decrease in performance across all automatic metrics (-4.22 BLEU-4). This indicates that the benefits brought about by pretraining, through knowledge about the question focus embedded in the model, is limited relative to question type information, at least for the goal of generating questions faithful to the type in the reference.\\n\\n8. Conclusion\\n\\nWe revisited the task of KBQG and introduced a novel approach of generating questions from KB triples using a fine-tuned large language model, without the ad-hoc processing required of earlier work. Experimental evidence on WKDQG reveals that pre-training and question type control contributes to improved performance both in a standard and in a zero-shot setting. We investigate the capacity of the BART model and extended dataset to generate questions whose type is distinct from that of the gold truth, and find that it leads to better or similar quality questions of varied types in our human evaluation. Additionally, we quantified the decline in current QA systems' performance at inference time when the question type distribution is shifted from that seen at training, and show that by enriching the training data with the set of possible questions generated by our approach, these systems' performances are restored. We hope that WKDQG \u2013 which extends an existing benchmark (SQ) by six-fold, introduces a wider coverage of properties and question type per triple, and is updated to an actively maintained KB \u2013 contributes towards advances in QG and QA in general.\"}"}
{"id": "lrec-2022-1-29", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThis research was supported by ANR Project QUANTUM (Project-ANR-19-CE23-0025). Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).\\n\\nBibliographical References\\n\\nAuer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., and Ives, Z. (2007). Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722\u2013735. Springer.\\n\\nBi, S., Cheng, X., Li, Y.-F., Wang, Y., and Qi, G. (2020). Knowledge-enriched, type-constrained and grammar-guided question generation over knowledge bases. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2776\u20132786, Barcelona, Spain (Online), December. International Committee on Computational Linguistics.\\n\\nBollacker, K. D., Evans, C., Paritosh, P., Sturge, T., and Taylor, J. (2008). Freebase: a collaboratively created graph database for structuring human knowledge. In Jason Tsong-Li Wang, editor, Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD 2008, Vancouver, BC, Canada, June 10-12, 2008, pages 1247\u20131250. ACM.\\n\\nBordes, A., Usunier, N., Garcia-Duran, A., Weston, J., and Yakhnenko, O. (2013). Translating embeddings for modeling multi-relational data. In C.J. Burges, et al., editors, Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.\\n\\nBordes, A., Usunier, N., Chopra, S., and Weston, J. (2015). Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075.\\n\\nDenkowski, M. and Lavie, A. (2014). Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376\u2013380, Baltimore, Maryland, USA, June. Association for Computational Linguistics.\\n\\nDong, R., Smith, D., Dudy, S., and Bedrick, S. (2019). Noisy neural language modeling for typing prediction in BCI communication. In Proceedings of the Eighth Workshop on Speech and Language Processing for Assistive Technologies, pages 44\u201351, Minneapolis, Minnesota, June. Association for Computational Linguistics.\\n\\nElsahar, H., Gravier, C., and Laforest, F. (2018). Zero-shot question generation from knowledge graphs for unseen predicates and entity types. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 218\u2013228, New Orleans, Louisiana, June. Association for Computational Linguistics.\\n\\nGardent, C., Shimorina, A., Narayan, S., and Perez-Beltrachini, L. (2017a). Creating training corpora for NLG micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 179\u2013188, Vancouver, Canada, July. Association for Computational Linguistics.\\n\\nGardent, C., Shimorina, A., Narayan, S., and Perez-Beltrachini, L. (2017b). The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124\u2013133, Santiago de Compostela, Spain, September. Association for Computational Linguistics.\\n\\nHan, X., Cao, S., Lv, X., Lin, Y., Liu, Z., Sun, M., and Li, J. (2018). OpenKE: An open toolkit for knowledge embedding. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 139\u2013144, Brussels, Belgium, November. Association for Computational Linguistics.\\n\\nHan, N., Topic, G., Noji, H., Takamura, H., and Miyao, Y. (2020). An empirical analysis of existing systems and datasets toward general simple question answering. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5321\u20135334, Barcelona, Spain (Online), December. International Committee on Computational Linguistics.\\n\\nHuang, X., Zhang, J., Li, D., and Li, P. (2019). Knowledge graph embedding based question answering. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM \u201919, page 105\u2013113, New York, NY, USA. Association for Computing Machinery.\\n\\nKumar, V., Hua, Y., Ramakrishnan, G., Qi, G., Gao, L., and Li, Y.-F. (2019). Difficulty-controllable multi-hop question generation from knowledge graphs. In International Semantic Web Conference, pages 382\u2013398. Springer.\\n\\nLassila, O., Swick, R. R., et al. (1998). Resource description framework (rdf) model and syntax specification.\\n\\nLawrence, C., Kotnis, B., and Niepert, M. (2019). Attending to future tokens for bidirectional sequence generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1\u201310, Hong Kong, China, November. Association for Computational Linguistics.\\n\\nLenat, D. and Guha, R. (1993). Building large knowledge-based systems: Representation and inference in the cyc project. Artificial Intelligence, 61(1):4152.\\n\\nLevy, O., Seo, M., Choi, E., and Zettlemoyer, L. (2017). Zero-shot relation extraction via reading\"}"}
{"id": "lrec-2022-1-29", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333\u2013342, Vancouver, Canada, August. Association for Computational Linguistics.\\n\\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2020). BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online, July. Association for Computational Linguistics.\\n\\nLin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain, July. Association for Computational Linguistics.\\n\\nLiu, C., Liu, K., He, S., Nie, Z., and Zhao, J. (2019). Generating questions for knowledge bases via incorporating diversified contexts and answer-aware loss. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2431\u20132441, Hong Kong, China, November. Association for Computational Linguistics.\\n\\nMohammed, S., Shi, P., and Lin, J. (2018). Strong baselines for simple question answering over knowledge graphs with and without neural networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 291\u2013296, New Orleans, Louisiana, June. Association for Computational Linguistics.\\n\\nOlney, A. M., Graesser, A. C., and Person, N. K. (2012). Question generation from concept maps. Dialogue & Discourse, 3(2):75\u201399.\\n\\nPapineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.\\n\\nPennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543.\\n\\nPetroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De Cao, N., Thorne, J., Jernite, Y., Karpukhin, V., Maillard, J., et al. (2020). Kilt: a benchmark for knowledge intensive language tasks. arXiv preprint arXiv:2009.02252.\\n\\nReddy, S., Raghu, D., Khapra, M. M., and Joshi, S. (2017). Generating natural language question-answer pairs from a knowledge graph using a RNN based question generation model. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 376\u2013385, Valencia, Spain, April. Association for Computational Linguistics.\\n\\nSerban, I. V., Garc\u00eda-Dur\u00e1n, A., Gulcehre, C., Ahn, S., Chandar, S., Courville, A., and Bengio, Y. (2016). Generating factoid questions with recurrent neural networks: The 30M factoid question-answer corpus. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 588\u2013598, Berlin, Germany, August. Association for Computational Linguistics.\\n\\nSeyler, D., Yahya, M., and Berberich, K. (2015). Generating quiz questions from knowledge graphs. In Proceedings of the 24th International Conference on World Wide Web, pages 113\u2013114.\\n\\nSeyler, D., Yahya, M., and Berberich, K. (2017). Knowledge questions from knowledge graphs. In Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval, pages 11\u201318.\\n\\nSong, L. and Zhao, L. (2016). Question generation from a knowledge base with web exploration. arXiv preprint arXiv:1610.03807.\\n\\nSong, Y., Jiang, D., Zhao, W., Xu, Q., Wong, R. C.-W., and Yang, Q. (2019). Chameleon: A language model adaptation toolkit for automatic speech recognition of conversational speech. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pages 37\u201342, Hong Kong, China, November. Association for Computational Linguistics.\\n\\nVrande\u010di\u0107, D. and Kr\u00f6tzsch, M. (2014). Wikidata: a free collaborative knowledgebase. Communications of the ACM, 57(10):78\u201385.\\n\\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and Artzi, Y. (2020). Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.\"}"}
{"id": "lrec-2022-1-29", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Appendices\\n\\nA.1. Mapping the WebNLG DBPedia triples to Wikidata\\n\\nThe approach for the migration is similar to that for SQ. We leveraged the \\\"schema:about\\\" property to map DBpedia entities into Wikidata. Specifically, we started by constructing SPARQL queries to return the set of all properties between each entity-pair (where both subject and object are entities as well as where the object is a literal in the forward and backward directions).\\n\\nFor the set of entity-pair/entity-literal pair with at least one Wikidata property found between them, we group them by their original DBpedia property (we refer to one of these as a DBP property cluster). Given WQ's smaller size, we directly identified the most common Wikidata property found in each DBP property cluster. We used a similar manual inspection approach as SQ above (namely, manually inspect: (a) one entity-pair from the cluster, (b) its DBpedia triple; and (c) the Wikidata property label.) and only mapped the DBpedia property to this most common Wikidata property if (a), (b) and (c) above are semantically aligned. This process was also repeated in the backwards direction.\\n\\nNext we used the mapping from these assignments (i.e. complete WQ triples found in Wikidata) for all other instances the set of their entities and properties appear in WQ.\\n\\nFor the remaining unmapped WQ DBpedia properties, we wrote a SPARQL query with a contain function for the DBpedia property (with camel case removed and lowercased) on the English label and alternative labels for all the properties in Wikidata (approx 8,000) to identify candidate mappings. We manually inspected these candidates and assigned using the same criteria (steps a,b,c above) if it is semantically aligned with the DBpedia property. We took care to check the direction of the Wikidata property and reversed entity-pair/entity-literal-pairs that have this DBpedia property. A further set of properties in WQ remained unmapped after this and a manual search of the Wikidata site was done to identify a suitable mapping.\\n\\nFor the remaining WQ triples not fully mapped (but with their DBpedia property mapped), we used \\\"schema:about\\\" to map their entities into Wikidata to complete the triple's mapping, with a fallback to search for matches of the entity (replacing the underscores with spaces and maintaining case) against the English labels and alternative labels for all entities in Wikidata. In the latter case, where there are multiple candidates, we select the first matched Wikidata entity (Q-code sorted by lexicographic order). Finally, for remaining unmapped entities, we conducted manual searches (with the help of the wikipedia package to identify candidates) on the Wikipedia and Wikidata sites to attempt to map these entities. Unlike SQ above - where we only mapped the triple into Wikidata if both entities in the SQ triple are found in Wikidata, we accepted DBpedia entities that could not be found in Wikidata; for these, we removed the camel case of the DBpedia entity and used them as the entity's mapping. Although the entity may not currently be present in Wikidata, its presence in DBpedia suggests that it is attested in Wikipedia and could be added to Wikidata.\\n\\nA.2. Example outputs with varied question type control\\n\\nSimpleQ\\n\\n1. Reference what category of celestial object is 7624 gluck\\n   (O) Input (7624 GLUCK, INSTANCE OF, ASTEROID), WHAT, ANSOBJ, category\\n   Generated what type of celestial object is 7624 gluck\\n   (A) Input (7624 GLUCK, INSTANCE OF, ASTEROID), WHICH, ANSOBJ, category\\n   Generated which type of celestial object is 7624 gluck\\n\\n2. Reference what is maurizio calvesi's profession\\n   (O) Input (MAURIZIO CALVESI, OCCUPATION, CINEMATOGRAPHER), WHAT, ANSOBJ, profession\\n   Generated what is maurizio calvesi's profession\\n   (A) Input (MAURIZIO CALVESI, OCCUPATION, CINEMATOGRAPHER), OTHER, ANSOBJ, profession\\n   Generated is maurizio calvesi a cinematographer or a technician\\n\\n3. Reference who was born in compton\\n   (O) Input (CLARENCE DUREN, PLACE OF BIRTH, COMPTON), WHO, ANSSUBJ, american football player\\n   Generated who was born in compton, queens\\n   (A) Input (CLARENCE DUREN, PLACE OF BIRTH, COMPTON), WHAT, ANSSUBJ, american football player\\n   Generated what former football player was born in compton, illinois\\n\\nTable 10: Examples of the outputs of BART on Test O compared with those of BART on Test A for the same sample (except for a different question type provided to the model). In bold face are the question type markers provided to the model.\"}"}
{"id": "lrec-2022-1-29", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3. Downstream evaluation on QA systems\\n\\nThe results of our experiments using generated questions from our approach to enrich the SQ dataset on current QA systems can be found in Table 11 below.\\n\\nA.4. Combining the three datasets\\n\\nTable 12 contains the results of our experiments where we fine-tune on the combination of the three datasets (SQ, WQ and ZQ) and evaluate the model on each's test set.\\n\\nA.5. Examples: models' inputs and generation outputs\\n\\nTable 13 provides an overview of the format of the inputs to each of the models (Elsahar and ours), under various settings. The original SQ input and reference is provided, as well as examples of each model's generated output.\"}"}
{"id": "lrec-2022-1-29", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Results of QA system performance on SQ with and without generated varied questions in the train, dev and/or test sets. In the table above, O denotes use of original SQ questions, E denotes enrichment (of O) with generated varied questions in train and dev, A denotes a question of an alternative (to O\u2019s) question type for each sample in test. W denotes the set of SQ samples successfully mapped to Wikidata. In brackets are the sizes of the dataset splits.\\n\\n|         | SQ | WQ | ZQ | ALL |\\n|---------|----|----|----|-----|\\n| BLEU-4  |    |    |    |     |\\n| BART    | 41.95 | 40.55 | 49.72 | 48.24 |\\n| BART + qt | 41.31 | 37.60 | 49.71 | 48.05 |\\n| BART + qtwkdqg | 73.51 | 68.92 | 75.72 | 75.17 |\\n| BART + qt, wkdqg | 72.63 | 68.06 | 75.57 | 74.89 |\\n| ROUGE-L |    |    |    |     |\\n| BART    | 71.21 | 63.86 | 71.26 | 71.03 |\\n| BART + qt, wkdqg | 70.28 | 62.80 | 71.01 | 70.65 |\\n| METEOR  |    |    |    |     |\\n| BART    | 36.78 | 34.95 | 40.78 | 39.99 |\\n| BART + qt, wkdqg | 36.62 | 33.40 | 40.59 | 39.76 |\\n\\nTable 12: Results (automatic metrics) for RDF-only models. BART + qt, wkdqg: model fine-tuned on the W KD QG data, and evaluated on each of the SQ, WQ and ZQ test sets. ALL shows the results proportionally averaged on the three test sets.\"}"}
{"id": "lrec-2022-1-29", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SimpleQ Sample Input:\\n\\n- \\\\((M/03 Y2 SVR, ASTRONOMY/CELESTIAL OBJECT/CATEGORY, M/0JVQ))\\n\\n- reference what category of celestial object is 7624 gluck\\n- what OBJTYPE of SUBTYPE is PLACEHOLDERSUB\\n- what is 7624 gluck (relexicalised from system decoder output: what is PLACEHOLDERSUB)\\n\\n- \\\\(BART\\\\) rd f (7624 GLUCK, INSTANCE OF, ASTEROID), ANSOBJ what category of celestial object is 7624 gluck\\n- what type of celestial object is 7624 gluck\\n\\n- \\\\(BART\\\\) rd f,mtl (7624 GLUCK, INSTANCE OF, ASTEROID), ANSOBJ, category ditto what type of celestial object is 7624 gluck\\n\\n- \\\\(BART\\\\) rd f,qt (7624 GLUCK, INSTANCE OF, ASTEROID), WHAT, ANSOBJ, category ditto what type of celestial body is 7624 gluck\\n\\n- \\\\(BART\\\\) rd f + nl (7624 GLUCK, INSTANCE OF, ASTEROID), ANSOBJ, asteroid designated as 7624 Gluck ditto what kind of celestial body is 7624 gluck\\n\\n- \\\\(BART\\\\) rd f + nl,mtl (7624 GLUCK, INSTANCE OF, ASTEROID), ANSOBJ, category, asteroid designated as 7624 Gluck ditto what type of celestial body is 7624 gluck\\n\\n- \\\\(BART\\\\) rd f + nl,qt (7624 GLUCK, INSTANCE OF, ASTEROID), WHAT, ANSOBJ, category, asteroid designated as 7624 Gluck ditto what kind of celestial object is 7624 gluck\\n\\n- \\\\(BART\\\\) rd f,qt, zero-shot property (7624 GLUCK, INSTANCE OF, ASTEROID), WHAT, ANSOBJ, category ditto what is the category of 7624 gluck\\n\\n- \\\\(BART\\\\) rd f,qt, zero-shot property (7624 GLUCK, INSTANCE OF, ASTEROID), WHAT, ANSOBJ, category, asteroid designated as 7624 Gluck ditto what is the category of 7624 gluck\\n\\nTable 13: Examples of system outputs (Generated), used in the automatic evaluation. Other columns compare the formats for the input and target between the models (Elsahar and ours).\"}"}
