{"id": "emnlp-2022-main-45", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Crossmodal-3600: A Massively Multilingual\\nMultimodal Evaluation Dataset\\nAshish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, Radu Soricut\\nGoogle Research\\n{asht,jponttuset,chillxichen,rsoricut}@google.com\\n\\nAbstract\\nResearch in massively multilingual image captioning has been severely hampered by a lack of high-quality evaluation datasets. In this paper we present the Crossmodal-3600 dataset (XM3600 in short), a geographically-diverse set of 3600 images annotated with human-generated reference captions in 36 languages. The images were selected from across the world, covering regions where the 36 languages are spoken, and annotated with captions that achieve consistency in terms of style across all languages, while avoiding annotation artifacts due to direct translation. We apply this benchmark to model selection for massively multilingual image captioning models, and show strong correlation results with human evaluations when using XM3600 as golden references for automatic metrics.\\n\\n1 Introduction\\nImage captioning is the task of automatically generating a fluent natural language description for a given image. This task is important for enabling accessibility for visually impaired users, and is a core task in multimodal research encompassing both vision and language modeling. However, datasets for this task are primarily available in English (Young et al., 2014; Chen et al., 2015; Krishna et al., 2017; Sharma et al., 2018; Pont-Tuset et al., 2020). Beyond English, there are a few datasets such as Multi30K with captions in German (Elliott et al., 2016), French (Elliott et al., 2017) and Czech (Barerrar et al., 2018), but they are limited to only a few languages that cover a small fraction of the world's population, while featuring images that severely under-represent the richness and diversity of cultures from across the globe. These aspects have hindered research on image captioning for a wide variety of languages, and directly hamper deploying accessibility solutions for a large potential audience around the world.\\n\\nCreating large training and evaluation datasets in multiple languages is a resource-intensive endeavor. Recent works (Thapliyal and Soricut, 2020) have shown that it is feasible to build multilingual image captioning models trained on machine-translated data (with English captions as the starting point). This work also shows that the effectiveness of some of the most reliable automatic metrics for image captioning, such as CIDEr\\n\\n1 measures the similarity of a generated sentence against a set of ground truth sentences written by humans.\u201d quoted from (Vedantam et al., 2015)\\n\\nAs such, the current situation is that trustworthy model evaluation can only be based on extensive and expensive human evaluations. However, such evaluations cannot usually be replicated across different research efforts, and therefore do not offer a fast and robust mechanism for model hill-climbing and comparison of multiple lines of research.\\n\\nThe proposed XM3600 image captioning evaluation dataset provides a robust benchmark for multilingual image captioning, and can be reliably used to compare research contributions in this emerging field. Our contributions are as follows: (i) for human caption annotations, we have devised a protocol that allows annotators for a specific target language to produce image captions in a style that is consistent across languages; this protocol results in image-caption annotations that are free of direct translation artefacts, an issue that has plagued Machine Translation research for many years and is now well understood (Freitag et al., 2020); (ii) for image selection, we have devised an algorithmic approach to sample a set of 3600 geographically-diverse images from the Open Images Dataset (Kuznetsova et al., 2020), aimed at creating a representative set of images from across the world; (iii) for the resulting XM3600 benchmark dataset, we have selected 3600 representative images from a large collection of images available online, curated to ensure that the images are diverse and representative of different cultures and regions.\\n\\nThe XM3600 dataset includes human-generated reference captions in 36 languages, covering a wide range of geographically-diverse regions. The dataset is designed to provide a comprehensive benchmark for evaluating image captioning models in a multilingual setting. We believe that this dataset will be valuable for researchers working on multilingual image captioning, as it provides a high-quality evaluation benchmark that can be used to compare different approaches and evaluate the performance of multilingual image captioning models.\\n\\nIn summary, we introduce the XM3600 dataset as a benchmark for massively multilingual image captioning. This dataset is designed to provide a comprehensive evaluation benchmark for multilingual image captioning models, and we believe that it will be a valuable resource for researchers in this field.\"}"}
{"id": "emnlp-2022-main-45", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Sample captions in three different languages (out of 36 \u2013 see full list of captions in Appendix A), showcasing the creation of annotations that are consistent in style across languages, while being free of direct-translation artefacts (e.g. the Spanish \u201cnumber 42\u201d or the Thai \u201cconvertibles\u201d would not be possible when directly translating from the English versions).\\n\\nWe empirically measure its ability to rank image captioning model variations, and show that it provides high levels of agreement with human judgements, therefore validating its usefulness as a benchmark and alleviating the need for human judgement in the future.\\n\\nFig. 1 shows a few sample captions for an image in XM3600 that exemplify point (i) above, and Fig. 2 shows the variety of cultural aspects captured by the image sampling approach from point (ii). We provide detailed explanations and results for each of the points above in the rest of the paper. We have released XM3600 under a CC-BY4.0 license at https://google.github.io/crossmodal-3600/.\\n\\n2 The XM3600 Dataset\\n\\nIn this section, we describe the heuristics used for language and image selection, the design of the caption annotation process, caption statistics including quality, and annotator details.\\n\\n2.1 Language Selection\\n\\nIn this section, we describe the heuristic used for selecting the languages. As a first step, we take a quantitative stance and choose 30 languages ($L_{30}$) roughly based on their percent of web content. As a second step, we consider an additional five languages ($L_5$) to cover low-resource languages with many native speakers, or major native languages from continents that would not be covered otherwise. The protocol for caption annotation (Sec. 2.3) has been applied to the resulting union of languages plus English, for a total of 36 languages.\\n\\n2.2 Image Selection\\n\\nIn this section, we consider the heuristics used for selecting a geographically diverse set of images. For each of the 36 languages, we select 100 images that, as far as it is possible for us to identify, are taken in an area where the given language is spoken. The images are selected among those in the Open Images Dataset (Kuznetsova et al., 2020) that have GPS coordinates stored in their EXIF metadata. Since there are many regions where more than one language is spoken, and given that some areas are not well covered by Open Images, we design an algorithm that maximizes the percentage of selected images taken in an area in which the assigned language is spoken. This is a greedy algorithm that starts the selection of images by the languages for which we have the smallest pool (e.g. Persian) and processes them in increasing order of their candidate image pool size. Whenever there are not enough images in the area where a language is spoken, we have several back-off levels: (i) selecting from a country where the language is spoken; (ii) a continent where the language is spoken, and, as last resort, (iii) from anywhere in the world.\\n\\nThis strategy succeeds in providing our target number of 100 images from an appropriate region for most of the 36 languages except for Persian (where 14 continent-level images are used).\"}"}
{"id": "emnlp-2022-main-45", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: A sample of images in the XM3600 dataset, together with the language for which they have been selected. Overall, the images span regions over 36 different languages and 6 different continents.\\n\\nHindi (where all 100 images are at the global level because the in-region images are assigned to Bengali and Telugu). We keep the region each image is selected from as part of our data annotation, so that future evaluations can choose to either evaluate on images relevant to particular regions of interest or on the entire dataset.\\n\\n2.3 Caption Annotation\\n\\nIn this section we detail the design of the caption annotation process. For a massively multilingual benchmark such as XM3600, consistency in the style of the description language is critical, since language can serve multiple communication goals. For a more in-depth discussion on these issues as they relate to image captions, we refer the reader to (Alikhani et al., 2020). We borrow from their terminology, as it identifies coherence relations between image and captions such as VISIBLE, META, SUBJECTIVE, and STORY. The goal for our caption annotation is to generate VISIBLE image captions, i.e., use the target language to formulate a sentence that is intended to recognizably characterize what is visually depicted in the image.\\n\\nOne possible approach to generating such captions is to generate them as such in English, and have them translated (automatically, semi-automatically, or manually) into all the other languages. However, this approach results in an English-language bias, as well as other problems that have been already identified in the literature. For instance, translations are often less fluent compared to natural target sentences, due to word order and lexical choices influenced by the source language. The impact of this phenomenon on metrics and modeling has recently received increased attention in the evaluation literature (Toral et al., 2018; Zhang and Toral, 2019; Freitag et al., 2020), and references created in this style are thought to cause overlap-based metrics to favor model outputs that use such unnatural language.\\n\\nWe have designed our caption annotation process to achieve two main goals: (i) produce caption annotations in a VISIBLE relation with respect to the image content, and, strongly, create consistency in the description style across languages; (ii) be free of translation artefacts. To achieve this, we use bi-lingual annotators with a requirement to be reading-proficient in English and fluent/native in the target language. As a preliminary step, we train an image-captioning model on English-annotated data, which results in captions in the VISIBLE style of COCO-CAP (Chen et al., 2015).\\n\\nThe annotation process proceeds as follows. Each annotation session is done over batches of $N = 15$ images, using the images selected as described in Sec. 2.2. The first screen shows the $N$ images with their captions in English as generated by the captioning model, and asks the annotators if the captions are EXCELLENT, GOOD, MEDIUM, or POOR.\"}"}
{"id": "emnlp-2022-main-45", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BAD, or there is NOT-ENOUGH INFO. We refer to this rating scale as the 5-level quality scale in the subsequent text. We provide the annotators with clear guidelines about what constitutes an EXCELLENT caption, and how to evaluate degradations from that quality. This step forces the annotators to carefully assess caption quality and it primes them into internalizing the style of the captions without the need for complicated and lengthy annotation instructions.\\n\\nThe second round shows the same images again, but one image at a time without the English captions, and the annotators are asked to produce descriptive captions in the target language for each image. In the absence of the English captions, the annotators rely on the internalized caption style, and generate their annotations mostly based on the image content \u2013 with no support from the text modality, other than potentially from memory. Note, however, that we have designed the system to support N annotations simultaneously, and we have empirically selected the value of N as to be large enough to \u201coverwrite\u201d the memory of the annotators with respect to the exact textual formulation of the English captions. As a result, we observe that the produced annotations are free of translation artefacts: See the example in Fig. 1 for Spanish mentioning \u201cnumber 42\u201d, and for Thai mentioning \u201cconvertibles\u201d.\\n\\nWe also provide the annotators with an annotation protocol to use when creating the captions, which provides useful guidance in achieving consistent annotations across all the targeted languages. We provide the annotation guidelines in Appendices B and C. For each language, we annotate all 3600 images with captions using replication 2 (two different annotators working independently), except Bengali (bn) with replication 1 and Maori (mi) with roughly 1 for 2/3 and 2 for 1/3 of the images, see Table 1.\\n\\n### 2.4 Caption Statistics\\n\\nIn this section, we take a look at the basic statistics of the captions in the dataset. Table 1 provides detailed caption statistics, including the number of captions per image and the average number of words and characters per caption. There are a total of 261,375 captions across 36 languages, each image having in the vast majority of cases at least 2\\n\\n| Lan. Num. Replication Num. | Cap. 1 | 2  | 3+  | Words | Chars |\\n|---------------------------|-------|----|-----|-------|-------|\\n| ar                         | 7367  | 0  | 0   | 3434  | 166   |\\n| bn                         | 3600  | 3600| 0   | 0     | 11.3  |\\n| cs                         | 7207  | 15 | 3573| 12    | 6.5   |\\n| da                         | 7264  | 0  | 3542| 58    | 8.7   |\\n| de                         | 8643  | 0  | 2240| 1360  | 11.2  |\\n| el                         | 7204  | 0  | 3596| 4     | 7.7   |\\n| en                         | 7200  | 0  | 3600| 0     | 9.4   |\\n| es                         | 8614  | 0  | 2201| 1399  | 9.8   |\\n| fa                         | 7245  | 0  | 3555| 45    | 12.7  |\\n| fi                         | 7127  | 90 | 3500| 10    | 7.5   |\\n| fil                        | 7109  | 91 | 3509| 0     | 12.2  |\\n| fr                         | 8562  | 0  | 2253| 1347  | 12.3  |\\n| he                         | 7200  | 0  | 3600| 0     | 11.9  |\\n| hi                         | 8503  | 0  | 2297| 1303  | 13.4  |\\n| hr                         | 7280  | 0  | 3553| 47    | 9.0   |\\n| hu                         | 7216  | 0  | 3586| 14    | 8.5   |\\n| id                         | 7126  | 74 | 3526| 0     | 14.3  |\\n| it                         | 8471  | 0  | 2329| 1271  | 12.1  |\\n| ja                         | 7185  | 15 | 3585| 0     | 1.0   |\\n| ko                         | 7650  | 15 | 3315| 270   | 7.0   |\\n| mi                         | 4732  | 2483| 1102| 15    | 11.7  |\\n| nl                         | 8059  | 0  | 2771| 829   | 8.0   |\\n| no                         | 7213  | 0  | 3591| 9     | 9.6   |\\n| pl                         | 7141  | 59 | 3541| 0     | 8.3   |\\n| pt                         | 7243  | 0  | 3562| 38    | 10.8  |\\n| quz                        | 7200  | 0  | 3600| 0     | 5.0   |\\n| ro                         | 7123  | 77 | 3523| 0     | 15.6  |\\n| ru                         | 7200  | 0  | 3600| 0     | 9.9   |\\n| sv                         | 7273  | 1  | 3536| 63    | 8.1   |\\n| sw                         | 7046  | 154| 3446| 0     | 10.7  |\\n| te                         | 7200  | 0  | 3600| 0     | 7.1   |\\n| th                         | 7200  | 0  | 3600| 0     | 1.2   |\\n| tr                         | 7233  | 15 | 3538| 47    | 9.4   |\\n| uk                         | 7215  | 0  | 3585| 15    | 10.0  |\\n| vi                         | 7350  | 0  | 3450| 150   | 18.0  |\\n| zh                         | 7174  | 60 | 3508| 32    | 1.0   |\\n\\nTable 1: Caption statistics: A total of 261,375 captions across 36 languages. We provide the replication stats per language, as well as average number of words (where applicable) and characters.\\n\\nFor languages with natural space tokenization, the number of words per caption can be as low as 5 or 6 for some agglutinative languages like Cusco Quechua (quz) and Czech (cs), and as high as 18 for an analytic language like Vietnamese (vi). The number of characters per caption also varies drastically \u2013 from mid-20s for Korean (ko) to mid-90s for Indonesian (id) \u2013 depending on the alphabet and the script of the language.\\n\\n### 2.5 Caption Quality\\n\\nIn this section, we describe the process for ensuring the creation of high quality annotations, and present...\"}"}
{"id": "emnlp-2022-main-45", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Caption quality statistics for the 36 languages. We use the median of three ratings as the aggregated rating for an image-caption pair.\\n\\nIn order to ensure quality, the annotation process is initially started with pilot runs on 150 images. The caption ratings are spot checked by the authors to verify that the raters have a good understanding of the rating scale. Further, the generated captions go through a verification round where they are rated by the human annotators on the 5-level quality scale described in Sec. 2.3. If the annotations are below the desired quality, we clarify the guidelines and add more examples to provide feedback to the human annotators and then conduct another pilot. This process is repeated until very few low-quality captions are being produced.\\n\\nWe started the process with a set of six languages and 4-5 pilots were needed per language. For subsequent languages, only 1-2 pilots were needed because of these clarifications and language, we run the main annotation and finally a verification round where we select one caption for 600 randomly selected images and have the annotator pool (per language) rate them on the 5-level quality scale mentioned in Sec. 2.3. The quality scores are presented in Table 2.\\n\\n2.6 Annotator Details\\nWe use an in-house annotation platform with professional (paid) annotators and quality assurance. Annotators are chosen to be native in the target language whenever possible, and fluent otherwise (for low-resource languages, they are usually linguists that have advanced-level knowledge of that language). All annotators are required to be proficient in English since the instructions and guidelines are given in English.\\n\\n3 Model Comparison using XM3600\\nIn this section, we detail our experiments for comparing several models using human evaluations, and also using XM3600 annotations as gold references for automated metrics.\\n\\nFor model comparison, we train several multilingual image captioning models with different sizes over different datasets, and compare them on XM3600. As our main result, we show a high level of correlation between model rankings based on human-evaluation scores and the scores obtained using CIDEr (Vedantam et al., 2015) with XM3600 annotations as gold references.\\n\\n3.1 Datasets\\nWe build two multilingual datasets for training, CC3M-35L and COCO-35L, by translating Conceptual Captions 3M (Sharma et al., 2018) and COCO Captions (Chen et al., 2015) to the other 34 languages using Google\u2019s machine translation API. The remaining language, Cusco Quechua (quz), is not supported by the API. We use the standard train and validation splits for CC3M. For COCO, we use the Karpathy split (Karpathy and Fei-Fei, 2014).\"}"}
{"id": "emnlp-2022-main-45", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section we detail the model architecture we used for the experiments. Our Transformer-based (Vaswani et al., 2017) model architecture for image captioning is shown in Figure 3. On the vision side, each input image is modeled by a Vision Transformer (ViT) (Dosovitskiy et al., 2020; Zhai et al., 2021). The visual features produced by ViT for every patch of the image are pooled into a single dense feature vector. On the text side, a Language Identifier (LangId) string is used to specify the language. The LangId string is tokenized and embedded into dense token embeddings, which are merged with the dense visual embeddings as the input to a multi-layer Transformer Image and Text Encoder, followed by a multi-layer Transformer Image and Text Decoder to generate the predicted captions.\\n\\nWe take advantage of existing pretrained models to initialize different parts of our model: ViT (Zhai et al., 2021) (green in Fig. 3) and mT5 (Xue et al., 2021) (orange in Fig. 3). We consider different model sizes: mT5-base, mT-large, ViT-B/16, and ViT-g/14, where 16 and 14 are the corresponding patch sizes. We choose three combinations resulting in three different model architectures: mT5-base + ViT-B/16, mT5-base + ViT-g/14 and mT5-large + ViT-g/14.\\n\\nWe train these three models on COCO-35L. In addition, we consider a fourth model based on mT5-base + ViT-B/16 and trained on CC3M-35L. The models are trained on a 4x4x4 TPU-v4 architecture using an Adafactor (Shazeer and Stern, 2018) optimizer with a constant learning rate period between {1k, 10k} steps, followed by a reversed square-root decay with the number of steps. The batch size is 2048 in all the experiments. The initial learning rate is between {1e-4, 3e-4}. We use the same vocabulary (size 250k) as mT5 (Xue et al., 2021). The model trained with CC3M-35L is subsequently finetuned on COCO-35L with constant learning rate 3e-5 for 1 epoch.\\n\\nTable 3 describes the best hyperparameters we found for different training setups and used in our quantitative experiments. In terms of model sizes, mT5-base has about 680 million parameters, mT5-large about 1230 million, the ViT-B/16 86 million, and ViT-g/14 1011 million parameters. Together, all the experiments took around 5000 TPU hours to train.\"}"}
{"id": "emnlp-2022-main-45", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3 Human Evaluation\\n\\nIn this section, we detail the process used for human evaluations comparing the performance of two models.\\n\\nOur main goal in creating XM3600 is to automate the evaluation of massively multilingual image captioning models, by eliminating expensive and time-consuming human evaluations. Our results indicate that they can be substituted by using the XM3600 annotations as gold references for automated metrics such as CIDEr (Vedantam et al., 2015). To quantify the correlation between the two methods, we train four different models (Tab. 3) and conduct side-by-side human evaluations using the outputs of these models in several languages. We observe strong correlations (Sec. 3.4) between the human evaluations and the CIDEr scores using the XM3600 references.\\n\\nSpecifically, we use a randomly selected subset of 600 images from XM3600 for human evaluations, which we call XM600. Image captions generated by a given pairing of models ($m_1$ vs $m_2$, where $m_1$ is considered as the base condition and $m_2$ as the test condition) are compared and rated side-by-side, using a similar pool of annotators as described in Sec. 2.6. Each side-by-side pair (shown in a random per-example left-vs-right order) is rated using a 7-point scale: MUCH BETTER, BETTER, SLIGHTLY BETTER, SIMILAR, SLIGHTLY WORSE, WORSE, MUCH WORSE, with a replication factor of 3 (three annotators rate each pair). We denote by $WINS$ the percentage of images where the majority of raters (i.e. 2 out of 3) mark $m_2$'s captions as better, and by $LOSSES$ the percentage of images where the majority of raters mark $m_2$'s captions as worse. We then define the overall side-by-side gain of $m_2$ over $m_1$ as $\\\\Delta S = WINS - LOSSES$.\\n\\nConducting the full set of six side-by-side evaluations for each pair of models over the 35 languages would require 210 human evaluation sessions. This is prohibitively expensive and time-consuming. Thus, we conduct the full set of six side-by-side evaluations of the pairs of models, on a core set of four languages called $L_{CORE}$. We call this set of 24 evaluation sessions $O_{CORE}$. Furthermore, we also conduct a sparser set of side-by-side evaluations over languages where the CIDEr differences on XM3600 and on COCO-DEV indicate disagreement or ambiguity (e.g., opposite sign of the CIDEr differences, and/or small CIDEr differences); this gives us a set of 28 languages called $L_{EXT}$. We call the resulting set of 41 evaluation sessions $O_{EXT}$. The set of all evaluations is called $O_{ALL} = O_{CORE} + O_{EXT}$, which are conducted over the languages $L_{ALL} = L_{CORE} + L_{EXT}$.\\n\\nThe choice of which model is called $m_1$ and which model is called $m_2$ is arbitrary in the side-by-side evaluations, since we randomly flip left vs right before presenting the captions to the raters. Hence a single side-by-side evaluation gives two points for the correlation calculations: one with the $m_1$ and $m_2$ assigned as per the actual evaluation conducted, and one more with the $m_1$ and $m_2$ assignment flipped and the $\\\\Delta S$ sign flipped correspondingly.\\n\\n| Language | $\\\\Delta S_{XM600}$ | $\\\\Delta S_{XM3600}$ | $\\\\Delta S_{COCO-DEV}$ |\\n|----------|---------------------|----------------------|------------------------|\\n| en       | -38.9               | -0.277               | -0.287                 |\\n| es       | -34.8               | -0.246               | -0.240                 |\\n| hi       | -3.9                | -0.016               | 0.007                  |\\n| hi       | -14.0               | -0.047               | -0.039                 |\\n| hi       | -10.3               | -0.031               | -0.046                 |\\n| es       | -1.3                | 0.002                | -0.012                 |\\n| es       | -8.4                | -0.044               | -0.037                 |\\n| hi       | -29.6               | -0.201               | -0.205                 |\\n| hi       | -28.8               | -0.203               | -0.193                 |\\n| hi       | -36.5               | -0.246               | -0.231                 |\\n| zh       | -3.0                | -0.001               | -0.001                 |\\n| zh       | -29.3               | -0.095               | -0.084                 |\\n| zh       | -2.0                | -0.012               | -0.013                 |\\n| zh       | -36.5               | -0.108               | -0.099                 |\\n| zh       | -5.2                | -0.013               | -0.015                 |\\n| zh       | -16.0               | -0.016               | -0.023                 |\\n| zh       | -11.8               | 0.002                | -0.011                 |\\n\\nTable 5: Model comparisons over $L_{CORE}$ languages ($m_2$ vs $m_1$). $L$ denotes the target language; $\\\\Delta S_{XM600}$ is CIDEr($m_2$)-CIDEr($m_1$) on the XM600 dataset, $\\\\Delta S_{XM3600}$ on the XM3600 dataset, and $\\\\Delta S_{COCO-DEV}$ on the COCO validation split with machine-translated references. Table 7 in the appendix shows model comparisons over the $L_{EXT}$ languages.\"}"}
{"id": "emnlp-2022-main-45", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6:\\nCorrelations between side-by-side human evaluations ($\\\\Delta S \\\\times S$) and CIDEr difference on XM600, XM3600 and the translated COCO validation set. Here N represents the number of points used to compute the correlation coefficient. As noted in Sec. 3.3, each evaluation gives us two points for the correlation calculation.\\n\\n3.4 Results\\n\\nWe present results that show that it is feasible to use the XM3600 annotations as gold references with automated metrics such as CIDEr to compare models in lieu of human evaluations, and that this option is superior to using silver references created via automated translation.\\n\\nTable 5 presents the results for the $O_{CORE}$ set of evaluations on XM600 on the $L_{CORE}$ languages, while Table 7 in the appendix shows the results on the $L_{EXT}$ languages. The reference for the relative strength of each pairing is given by $\\\\Delta S \\\\times S$, with positive numbers indicating the superiority of $m_2$, and negative numbers indicating a superiority of $m_1$. As can be seen from the table, the model comparisons span a range of model differences, from low $\\\\Delta S \\\\times S$ to high $\\\\Delta S \\\\times S$.\\n\\n$\\\\Delta$CIDEr XM600 and $\\\\Delta$CIDEr XM3600 capture similar information, except these numbers are based on CIDEr scores using as references XM600 and XM3600, respectively, while $\\\\Delta$CIDEr COCO-DEV is based on machine-translated references from the validation split of COCO.\\n\\nWe use the results from Table 5 (and Table 7) to compute the correlation between human judge-ments of the relative quality of the captioning models and the ability of the CIDEr metric \u2013 or, rather, of the underlying references used by the metric \u2013 to perform an equivalent task. Table 6 presents the correlation results using three correlation metrics: Pearson, Spearman, and Kendall. The first section shows the correlations over all the side-by-side evaluations (i.e. $O_{CORE}$ and $O_{EXT}$); These cover the $L_{CORE}$ and the $L_{EXT}$ languages. The second section shows the correlations for the $O_{EXT}$ covering the $L_{EXT}$ languages. The third section shows the correlations for the $O_{CORE}$ evaluations covering the $L_{CORE}$ languages.\\n\\nWe observe that $\\\\Delta$CIDEr XM3600 is highly correlated with human judgement according to all the correlation metrics (Bonett and Wright, 2000), over all the evaluations $O_{ALL}$, over the $O_{CORE}$ evaluations, and also the $O_{EXT}$ evaluations. Furthermore, for the $O_{EXT}$ evaluations, where most of the instances have opposite signs for $\\\\Delta$CIDEr COCO-DEV and $\\\\Delta$CIDEr XM3600, we find that the former is strongly anti-correlated with the human evaluation results while the latter is highly correlated with the human evaluation results. Overall, these results indicate that: (i) we can reliably substitute $\\\\Delta$CIDEr XM3600 for human evaluations on XM600 when comparing models similar to the ones we used; (ii) the gold XM3600 references are preferable over the silver references obtained from translating COCO captions, in terms of approximating the judgements of the human evaluators.\\n\\nBased on the results from Table 6, we recommend the use of the XM3600 references as a means to achieve high-quality automatic comparisons between multilingual image captioning models. We have provided the CIDEr scores for XM3600 in 35 languages for all the models, in Tables 8-11 in the Appendix. These can be used as baselines in future work.\\n\\n15 However, it is unclear whether machine translated references for one particular language in XM3600 translated to all others, are worse than using the human generated references. In particular, we studied the correlations of CIDEr computed using XM3600 -en-MT (i.e. the XM3600 English references, machine translated to all the other languages), with the human evaluations. We found that even though the translations have artifacts and disfluencies, CIDEr differences calculated using them show comparable correlations with human judgement observations. We also studied such correlations for machine translated references from German, Greek, Hebrew, Hungarian and Swahili. We found that the correlations are similar and sometimes even a bit higher than using the human generated references. We believe this happens because the rater guidelines weigh informativeness over fluency and the CIDEr metric is also not as sensitive to fluency. Further work is needed to understand the use of translated references as compared to human generated references. We believe that using the human generated references along with the set of machine translated references from all the other languages may provide even stronger correlations and show greater diversity in the coverage of the image constituents.\"}"}
{"id": "emnlp-2022-main-45", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Conclusions\\n\\nWe introduce the XM3600 dataset as a benchmark for evaluating the performance of multilingual image captioning models. The images in the dataset are geographically diverse, covering all inhabited continents and a large fraction of the world population. We believe this benchmark has the potential to positively impact both the research and the applications of this technology, and enable (among other things) better accessibility for visually-impaired users across the world, including speakers of low-resource languages.\\n\\nThe main appeal of this benchmark is that it alleviates the need for extensive human evaluation, which is difficult to achieve across multiple languages and hinders direct comparison between different research ideas and results. We show significant improvements in correlation with human judgements when using the XM3600 dataset as references for automatic metrics, and therefore hope that the adoption of this dataset as a standard benchmark will facilitate faster progress and better comparisons among competing ideas.\\n\\nOur empirical observations are primarily on the full set of side-by-side comparisons over English and three other languages (Spanish, Hindi, Chinese). Due to the similarity in the data collection and the quality control process, we expect similar results to hold for all the other languages as well; we validated this expectation with additional empirical observations covering an additional 28 languages.\\n\\n5 Limitations\\n\\nDue to the high volume of work required and the cost associated with it, we have only targeted 36 languages for our annotation effort; while this number is significantly higher than what is available with previous annotations, it still falls short of including many other languages spoken and written around the world. Additionally, since the L30 languages were selected based on their internet presence, one unintended consequence is that the dataset over-represents European languages. While this is somewhat mitigated by including the L5 low resource languages, building and sharing this dataset can have the unintended effect of perpetuating the issue where computational linguistics and AI work is often unintentionally Eurocentric.\\n\\nDue to the cost and logistical constraints, we have sampled only 100 images for each of the targeted languages, which limits the amount of natural and cultural phenomena that these images capture. While the resulting 3600 images have significantly more variety compared to previous datasets, it may still fall short of including important aspects of natural and cultural life from around the globe. Further, there is the possibility of bias in the dataset due to the uneven access to photographic equipment and internet connectivity (For example, several of the images in Fig. 4 seem to be shared by people with non-native names in the context of the locales. Thus, these images may have been taken by tourists rather than natives. Further exploration into this aspect of the dataset is important as well). Another limitation is around the absence of translation artifacts in the annotations. We primarily rely on the caption generation process outlined in Sec. 2.3 and on rater quality controls for avoiding translation artifacts. Further, we have performed spot checks on captions in several languages and have not found indications of translation artifacts. Additionally, we have also compared the translations of annotations from another language such as English with generated annotations and verified that the translations show peculiar artifacts and disfluencies which are not seen in the generated annotations.\\n\\nWe would also like to emphasize that, while this dataset aims to ameliorate the need for human evaluations for multilingual image captioning, automated evaluation may be less sensitive to small changes, e.g. when comparing highly tuned methods submitted to competitions. This was one of our motivations for comparing models that range from very different (CC+Bg vs Bg/Lg/BB) to moderately different (BB vs Bg/Lg) and quite similar (Bg vs Lg), and the results from Table 5 show that our approach works well over this range of model differences over \\\\( L_{\\\\text{CORE}} \\\\). We also stress-tested our approach by focusing the \\\\( \\\\text{O}_{\\\\text{EXT}} \\\\) evaluations on cases where \\\\( \\\\Delta \\\\text{CIDEr}_{\\\\text{XM3600}} \\\\) or \\\\( \\\\Delta \\\\text{CIDEr}_{\\\\text{COCO-DEV}} \\\\) were quite small or of opposite signs, and the results from Table 7 in the appendix show that \\\\( \\\\Delta \\\\text{CIDEr}_{\\\\text{XM3600}} \\\\) correlated well with human evaluations even for this harder set of evaluations. However, we caution the reader that there will be cases where human judgement will still be needed. Further, automated evaluations may be biased to methods that explicitly optimize the evaluated metric, e.g. via approaches such as Self-Critical Sequence Training (Rennie et al., 2017).\\n\\nWe also note that the model outputs and human\"}"}
{"id": "emnlp-2022-main-45", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"judgements data used for calculating the correlations would be useful for constructing new automated metrics and validating existing automated metrics for model comparisons. Releasing this data would also allow independent calculation of CIDEr and $\\\\Delta S \\\\times S$ shown in Table 5 and Table 7. However, due to the timelines involved and approvals required, we are not able to release this data with the paper. This may hamper the reproducibility of these computations.\\n\\nThe approach to data collection and annotation of COCO-CAP (Chen et al., 2015) and CC3M (Sharma et al., 2018) upholds rigorous privacy and ethics standards, such as the avoidance of offensive content and exposure of personal identification data. This significantly mitigates but does not completely eliminate the risks that the captioning models we train would produce such information. Similarly, the XM3600 dataset mitigates such risks by adopting a defense-in-depth approach: 1) The annotations have been produced in-house and have been quality controlled, while the images used have been vetted to be appropriate for the intended use. 2) Further, the machine translations of the annotations have been scanned with an automated tool to detect personally identifiable information. 3) The machine translations of the annotations have been spot-checked by the authors.\\n\\nOverall, in spite of the above limitations, we believe that this dataset is a significant step toward ameliorating language and geographic bias, and that it should be used for advancing image captioning research over a wider variety of images and languages.\\n\\n6 Acknowledgements\\n\\nWe would like to thank the anonymous reviewers for providing feedback which led to several improvements such as: 1) A discussion about correlations of human judgement with the machine translations of the XM3600 references; 2) A discussion about the possibility of releasing model outputs and human evaluation data which may help with reproducibility and also help evaluation of existing and new automated metrics over L$^{ALL}$.\\n\\nReferences\\n\\nMalihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut, and Matthew Stone. 2020. Cross-modal coherence modeling for caption generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6525\u20136535, Online. Association for Computational Linguistics.\\n\\nLo\u00efc Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. 2018. Findings of the third shared task on multimodal machine translation. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 304\u2013323.\\n\\nDouglas Bonett and Thomas Wright. 2000. Sample size requirements for estimating pearson, kendall and spearman correlations. Psychometrika, 65:23\u201328.\\n\\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2015. Microsoft COCO captions: Data collection and evaluation server. arXiv:1504.00325.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An image is worth 16x16 words: Transformers for image recognition at scale.\\n\\nDesmond Elliott, Stella Frank, Lo\u00efc Barrault, Fethi Bougares, and Lucia Specia. 2017. Findings of the second shared task on multimodal machine translation and multilingual image description. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers, pages 215\u2013233, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nDesmond Elliott, Stella Frank, Khalil Sima'an, and Lucia Specia. 2016. Multi30k: Multilingual english-german image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 70\u201374. Association for Computational Linguistics.\\n\\nMarkus Freitag, David Grangier, and Isaac Caswell. 2020. Bleu might be guilty but references are not innocent. arXiv, abs/2004.06063.\\n\\nAndrej Karpathy and Li Fei-Fei. 2014. Deep visual-semantic alignments for generating image descriptions.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. 123(1):32\u201373.\\n\\nAlina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. 2020. The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale. IJCV.\"}"}
{"id": "emnlp-2022-main-45", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Additional Caption Examples\\n\\nFigure 4 displays the captions in the 36 languages covered in XM3600 for the same image as in Figure 1.\\n\\nB Instructions for Rating Captions\\n\\nThe following instructions are provided to annotators for rating captions:\\n\\nThis task involves rating captions. To guide your ratings, imagine that you are describing the image to a visually impaired friend, then consider: how well does the caption describe the image to this friend?\\n\\nUse the following scale for judging the quality of the captions (for borderline cases, use the lower rating):\\n\\n\u2022 **BAD**: The caption has one or more of the following issues: a). Caption misses the main topic of the image. b). Caption has major grammatical errors (such as being incomplete, words in wrong order, etc). Please ignore capitalization of words and punctuation. c). Caption violates the 'No Hallucination' rule by mentioning objects, activities, or relationships that are definitely not in the image. Note: Apply the 'No-Hallucination' rule only when you are certain that an object/activity/relationship is definitely not implied by the image (see the examples below).\\n\\n\u2022 **MEDIUM**: The caption may capture some objects and activities but misses crucial information (related to activity, important objects/persons in the scene, important modifiers, etc.)\\n\\n\u2022 **GOOD**: The caption explains most of the main objects, activities, and their relationships in the image.\\n\\n\u2022 **EXCELLENT**: The caption covers well the whole image, including all the main objects, activities, and their relationships.\\n\\n\u2022 **NOT ENOUGH INFORMATION**: Not enough information to evaluate the caption quality. Please try to use one of the four categories above as much as possible. Assume that any missing information is favorable to the caption rather than against it.\\n\\nC Instructions for Generating Captions\\n\\nThe following instructions are provided to annotators for generating captions:\\n\\nTo guide your caption generation, imagine that you are describing the image to a visually impaired friend. The caption should explain the whole image, including all the main objects, activities, and their relationships. The objects should be named as specifically as practical: For example when describing a young boy in a picture, \u201cyoung boy\u201d is preferred over \u201cyoung child\u201d, which in turn is preferred over \u201cperson\u201d.\\n\\nNote: the goal is to generate captions that would be labeled as \u201cExcellent\u201d under the Rating guidelines above, but raters should not copy captions from the first phase. We want the raters to generate the captions on their own.\"}"}
{"id": "emnlp-2022-main-45", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A vintage Porsche car in a showroom with many other vintage cars.\\n\\nImage: Example captions in the 36 languages covered in XM3600.\\n\\nSource: Porsche Museum, Stuttgart by Brian Solis.\"}"}
{"id": "emnlp-2022-main-45", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We outline here a procedure that you should try and follow when writing your image caption. Note that not all these steps may be applicable for all images, but they should give you a pretty good idea of how to organize your caption. We will make use of the first image in the table below (the one with the young girl smiling). Note: It is acceptable to make assumptions that are reasonable as long as they don\u2019t contradict the information in the image (e.g., in the second image below, we use \u201cfamilies\u201d in captions 1 and 3 because there seems to be a mix of children and adults though it is not perfectly clear. So it is a reasonable assumption to make and nothing in the image contradicts it. However it is also ok to use \u201cpeople.\u201d)\\n\\n1. Identify the most salient objects(s)/person(s) in the image; use the most informative level to refer to something (i.e., \u201cgirl\u201d rather than \u201cchild\u201d or \u201cperson\u201d); in the example image: \u201cgirl\u201d\\n\\n2. Identify the most salient relation between the main objects; example \u201cgirl standing in front of the whiteboard\u201d\\n\\n3. Identify the main activity depicted; in the example image: \u201csmiling\u201d as an activity (note that this can also be an attribute of the girl), or \u201cstanding\u201d as an activity\\n\\n4. Identify the most salient attributes of the main object(s)/person(s)/activity(es); in the example image: \u201csmiling\u201d and \u201cyoung\u201d as attributes for the girl\\n\\n5. Identify the background/context/environment in which the scene is placed; in the example image: \u201cclassroom\u201d\\n\\n6. Put everything together from steps 1-5 above; for the example image: \u201ca smiling girl standing in a classroom\u201d, or \u201ca young girl smiling in a classroom\u201d.\\n\\n---\\n\\n### Table 7: Model comparison over the languages (m2 vs m1).\\n\\n| Language | \u2206CIDEr XM600 | \u2206CIDEr XM3600 | \u2206CIDEr COCO-DEV |\\n|----------|--------------|---------------|-----------------|\\n| Lg BB ar | -1.2         | -0.003        | 0.055           |\\n| Lg BB bn | -3.5         | -0.025        | -0.026          |\\n| Lg BB cs | -5.2         | -0.031        | -0.016          |\\n| Lg BB da | -2.7         | -0.012        | 0.029           |\\n| Lg BB el | -5.1         | -0.005        | 0.063           |\\n| Lg BB fa | -11.1        | -0.011        | -0.003          |\\n| Lg BB fi | -0.3         | -0.007        | -0.006          |\\n| Lg BB fil | -3.2 | -0.024 | -0.004 | 0.020 |\\n| Lg BB fr | -2.0         | -0.030        | -0.015          |\\n| Lg BB he | 1.7          | 0.005         | 0.001           |\\n| Lg BB hr | -4.5         | -0.007        | 0.002           |\\n| Lg BB hu | -4.7         | -0.005        | -0.009          |\\n| Lg BB id | -4.7         | -0.031        | -0.018          |\\n| Lg BB it | -6.2         | -0.011        | -0.015          |\\n| Lg BB ja | -10.8        | -0.013        | -0.021          |\\n| Lg BB ko | -2.0         | -0.025        | -0.018          |\\n| Lg BB nl | -6.2         | 0.002         | -0.013          |\\n| Lg BB no | -10.3        | -0.043        | -0.033          |\\n| Lg BB pl | -3.5         | -0.022        | -0.023          |\\n| Lg BB pt | -10.8        | -0.027        | -0.026          |\\n| Lg BB ro | -5.6         | -0.027        | -0.017          |\\n| Lg BB sv | -8.1         | -0.028        | -0.035          |\\n| Lg BB sv | -9.3         | -0.024        | -0.032          |\\n| Lg BB sw | -2.2         | -0.016        | 0.007           |\\n| Lg BB te | -1.3         | 0.008         | -0.002          |\\n| Lg BB th | -6.7         | -0.031        | -0.016          |\\n| Lg BB tr | -5.1         | -0.026        | -0.016          |\\n| Lg BB vi | -4.5         | 0.000         | 0.058           |\\n| Lg BB vi | 3.2          | 0.015         | 0.082           |\"}"}
{"id": "emnlp-2022-main-45", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | CIDEr XM3600 | CIDEr COCO-DEV |\\n|----------|--------------|----------------|\\n| ar       | 0.227        | 0.649          |\\n| bn       | 0.200        | 0.682          |\\n| cs       | 0.313        | 0.575          |\\n| da       | 0.329        | 0.877          |\\n| de       | 0.224        | 0.735          |\\n| el       | 0.199        | 0.830          |\\n| en       | 0.584        | 0.980          |\\n| es       | 0.425        | 0.962          |\\n| fa       | 0.311        | 0.898          |\\n| fi       | 0.177        | 0.487          |\\n| fil      | 0.353        | 1.007          |\\n| fr       | 0.410        | 0.957          |\\n| he       | 0.230        | 0.650          |\\n| hi       | 0.197        | 0.759          |\\n| hr       | 0.224        | 0.607          |\\n| hu       | 0.175        | 0.551          |\\n| id       | 0.307        | 1.088          |\\n| it       | 0.321        | 0.902          |\\n| ja       | 0.254        | 0.963          |\\n| ko       | 0.288        | 0.862          |\\n| mi       | 0.405        | 1.175          |\\n| nl       | 0.441        | 0.796          |\\n| no       | 0.385        | 0.856          |\\n| pl       | 0.236        | 0.578          |\\n| pt       | 0.380        | 0.964          |\\n| ro       | 0.188        | 0.832          |\\n| ru       | 0.194        | 0.675          |\\n| sv       | 0.370        | 0.848          |\\n| sw       | 0.319        | 0.796          |\\n| te       | 0.196        | 0.520          |\\n| th       | 0.418        | 0.929          |\\n| tr       | 0.232        | 0.668          |\\n| uk       | 0.189        | 0.653          |\\n| vi       | 0.336        | 1.150          |\\n| zh       | 0.202        | 0.748          |\\n\\nTable 8: CIDEr on XM3600 and COCO-DEV for the best performing model BB+CC on all 35 languages. (COCO-DEV computed using machine-translated references).\\n\\nTable 9: CIDEr on XM3600 and COCO-DEV for the model Bg on all 35 languages. (COCO-DEV computed using machine-translated references).\"}"}
{"id": "emnlp-2022-main-45", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | CIDEr XM3600 | CIDEr COCO-DEV |\\n|----------|--------------|----------------|\\n| ar       | 0.106        | 0.513          |\\n| bn       | 0.133        | 0.555          |\\n| cs       | 0.139        | 0.485          |\\n| da       | 0.192        | 0.765          |\\n| de       | 0.130        | 0.649          |\\n| el       | 0.101        | 0.680          |\\n| en       | 0.343        | 0.875          |\\n| es       | 0.220        | 0.859          |\\n| fa       | 0.155        | 0.766          |\\n| fi       | 0.089        | 0.419          |\\n| fil      | 0.185        | 0.858          |\\n| fr       | 0.217        | 0.825          |\\n| he       | 0.098        | 0.548          |\\n| hi       | 0.111        | 0.624          |\\n| hr       | 0.085        | 0.493          |\\n| hu       | 0.096        | 0.451          |\\n| id       | 0.167        | 0.943          |\\n| it       | 0.168        | 0.770          |\\n| ja       | 0.141        | 0.850          |\\n| ko       | 0.152        | 0.716          |\\n| mi       | 0.243        | 1.049          |\\n| nl       | 0.232        | 0.704          |\\n| no       | 0.230        | 0.736          |\\n| pl       | 0.108        | 0.495          |\\n| pt       | 0.202        | 0.843          |\\n| ro       | 0.100        | 0.709          |\\n| ru       | 0.089        | 0.581          |\\n| sv       | 0.225        | 0.748          |\\n| sw       | 0.151        | 0.640          |\\n| te       | 0.099        | 0.426          |\\n| th       | 0.226        | 0.802          |\\n| tr       | 0.122        | 0.584          |\\n| uk       | 0.081        | 0.560          |\\n| vi       | 0.182        | 0.940          |\\n| zh       | 0.099        | 0.656          |\\n\\nTable 10: CIDEr on XM3600 and COCO-DEV for the model Lg on all 35 languages. (COCO-DEV computed using machine-translated references).\\n\\nTable 11: CIDEr on XM3600 and COCO-DEV for the model BB on all 35 languages. (COCO-DEV computed using machine-translated references).\"}"}
