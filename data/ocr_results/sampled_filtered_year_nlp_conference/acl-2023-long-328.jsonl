{"id": "acl-2023-long-328", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning \\\"O\\\" Helps for Learning More: Handling the Unlabeled Entity Problem for Class-incremental NER\\n\\nRuotian Ma1\u2217, Xuanting Chen1\u2217, Lin Zhang1, Xin Zhou1, Junzhe Wang1, Tao Gui2\u2020, Qi Zhang1\u2020, Xiang Gao3, Yunwen Chen3\\n\\n1 School of Computer Science, Fudan University, Shanghai, China\\n2 Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China\\n3 DataGrand Information Technology (Shanghai) Co., Ltd.\\n{rtma19,xuantingchen21,tgui,qz}@fudan.edu.cn\\n\\nAbstract\\nAs the categories of named entities rapidly increase, the deployed NER models are required to keep updating toward recognizing more entity types, creating a demand for class-incremental learning for NER. Considering the privacy concerns and storage constraints, the standard paradigm for class-incremental NER updates the models with training data only annotated with the new classes, yet the entities from other entity classes are unlabeled, regarded as \\\"Non-entity\\\" (or \\\"O\\\"). In this work, we conduct an empirical study on the \\\"Unlabeled Entity Problem\\\" and find that it leads to severe confusion between \\\"O\\\" and entities, decreasing class discrimination of old classes and declining the model's ability to learn new classes. To solve the Unlabeled Entity Problem, we propose a novel representation learning method to learn discriminative representations for the entity classes and \\\"O\\\". Specifically, we propose an entity-aware contrastive learning method that adaptively detects entity clusters in \\\"O\\\". Furthermore, we propose two effective distance-based relabeling strategies for better learning the old classes. We introduce a more realistic and challenging benchmark for class-incremental NER, and the proposed method achieves up to 10.62% improvement over the baseline methods.\\n\\n1 Introduction\\nExisting Named Entity Recognition systems are typically trained on a large-scale dataset with predefined entity classes, then deployed for entity recognition on the test data without further adaptation or refinement (Li et al., 2020; Wang et al., 2022; Liu et al., 2021; Ma et al., 2022a). In practice, the newly-arriving test data may include new entity classes, and the user's required entity class set might keep expanding. Therefore, it is in demand that the NER model can be incrementally updated for recognizing new entity classes. However, one challenge is that the training data of old entity classes may not be available due to privacy concerns or memory limitations (Li and Hoiem, 2017; Zhang et al., 2020). Also, it is expensive and time-consuming to re-annotate all the old entity classes whenever we update the model (Delange et al., 2021; Bang et al., 2021). To solve the problem, Monaikul et al. (2021) proposes to incrementally update the model with new datasets only covering the new entity classes, adopted by following studies as standard class-incremental NER paradigm. However, as NER is a sequence labeling task, annotating only the new classes means entities from other entity classes are regarded as \\\"Non-entity\\\" (or \\\"O\\\") in the dataset. For example, in step 2 in Fig.1, the training data for model updating is only annotated with \\\"LOC\\\" and \\\"DATE\\\", while the entities from \\\"PER\\\" and \\\"FILM\\\" are unlabeled and regarded as \\\"O\\\" during training. We refer to this problem as the \\\"Unlabeled Entity Problem\\\" in class-incremental NER, which includes two types of unlabeled entities: (1) old entity classes (e.g., \\\"PER\\\" in step 2) that the model learned in previous...\"}"}
{"id": "acl-2023-long-328", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"steps are unlabeled in the current step, causing the model catastrophically forgetting these old classes. (Lopez-Paz and Ranzato, 2017; Castro et al., 2018)\\n\\n(2) potential entity classes that are not annotated till the current step, yet might be required in a future step. For example, the \u201cFILM\u201d class is not annotated till step 2, yet is required in step K.\\n\\nIn this work, we conduct an empirical study to demonstrate the significance of the \u201cUnlabeled Entity Problem\u201d on class-incremental NER. We observe that:\\n\\n1. The majority of prediction errors come from the confusion between entities and \u201cO\u201d.\\n2. Mislabeled as \u201cO\u201d leads to the reduction of class discrimination of old entities during incremental learning.\\n3. The model\u2019s ability to learn new classes also declines as the potential classes are unlabeled during incremental training. These problems attribute to the serious performance drop of incremental learning with the steps increasing.\\n\\nTo tackle the Unlabeled Entity Problem, we propose a novel representation learning method for learning discriminative representations for the unlabeled entity classes and \u201cO\u201d. Specifically, we propose an entity-aware contrastive learning approach, which adaptively detects entity clusters from \u201cO\u201d and learns discriminative representations for these entity clusters. To further maintain the class discrimination of old classes, we propose two distance-based relabeling strategies. By relabeling the entities from old classes with high accuracy, this practice not only keeps the performance of old classes, but also benefits the model\u2019s ability to separate new classes from \u201cO\u201d.\\n\\nWe also argue that the experimental setting of previous works Monaikul et al. (2021) is less realistic. Specifically, they introduce only one or two entity classes in each incremental step, and the number of total steps is limited. In real-world applications, it is more common that a set of new categories is introduced in each step (e.g., a set of product types), and the incremental learning steps can keep increasing. In this work, we provide a more realistic and challenging benchmark based on the Few-NERD dataset (Ding et al., 2021), following the settings of previous studies (Rebuffi et al., 2017; Li and Hoiem, 2017). We conduct intensive experiments on the proposed methods and other comparable baselines, verifying the effectiveness of the proposed method.\\n\\nOur code is publicly available at https://github.com/rtmaww/O_CILNER.\\n\\nTo summarize the contribution of this work:\\n\\n\u2022 We conduct an empirical study to demonstrate the significance of the \u201cUnlabeled Entity Problem\u201d in class-incremental NER.\\n\u2022 Based on our observations, we propose a novel representation learning approach for better learning the unlabeled entities and \u201cO\u201d, and verify the effectiveness of our method with extensive experiments.\\n\u2022 We provide a more realistic and challenging benchmark for class-incremental NER.\\n\\nClass-incremental NER\\nIn this work, we focus on class-incremental learning on NER. Formally, there are $N$ incremental steps, corresponding to a series of tasks $\\\\{T_1, T_2, \\\\ldots, T_N\\\\}$. Here, $T_t = (D_{tr}^t, D_{dev}^t, D_{test}^t, C_{t,new}, C_{t,old})$ is the task at the $t$th step. $C_{t,new}$ is the label set of the current task, containing only the new classes introduced in the current step (e.g., \\\\{\"LOC\", \"DATE\"\\\\} in Fig.1, step 2). $C_{t,old} = \\\\bigcup_{i=1}^{t-1} C_{i,new} \\\\cup \\\\{\"O\"\\\\}$ is the label set of old classes, containing all classes in previous tasks and the class \u201cO\u201d (e.g., \\\\{\"PER\", \"O\"\\\\} in Fig.1, step 2).\\n\\n$D_{tr}^t = \\\\{X_j^t, Y_j^t\\\\}_{j=1}^n$ is the training set of task $t$, where each sentence $X_j^t = \\\\{x_j^t, 1, \\\\ldots, x_j^t,l\\\\}$ and $Y_j^t = \\\\{y_j^t, 1, \\\\ldots, y_j^t,l\\\\}$, $y_j^t,k \\\\in C_{t,new}$ is annotated with only the new classes. In each step $t$, the model $A_{t-1}$ from the last step needs to be updated with only the data $D_{tr}^t$ from the current step, and is expected to perform well on the test set covering all learnt entity types $C_{all}^t = C_{t,new} \\\\cup C_{t,old}$.\"}"}
{"id": "acl-2023-long-328", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance of the new classes on dev set (only containing the new classes) keep decreasing during incremental learning. Here, Full Data is the model trained with datasets labeled with both old and new classes.\\n\\n| Steps | Full Data | iCaRL | Continual NER |\\n|-------|-----------|-------|---------------|\\n| 0     | 72.7      | 71.3  | 72.4          |\\n| 1     | 69.2      | 56.9  | 63.5          |\\n| 2     | 68.3      | 52.6  | 56.9          |\\n| 3     | 67.0      | 48.8  | 52.5          |\\n| 4     | 67.3      | 53.4  | 56.8          |\\n| 5     | 69.1      | 48.1  | 51.8          |\\n| 6     | 68.8      | 39.6  | 42.2          |\"}"}
{"id": "acl-2023-long-328", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning an entity-oriented feature space with Supcon Loss\\nCalculating entity threshold for selecting anchors and positives\\nEntity-aware Contrastive Learning for \\\"O\\\"\\nRe-labeling the unlabeled old entities.\\nPrototype-based Method\\nCalculate prototype & threshold\\nRe-labeling\\nNearest Neighbor-based Method\\nCalculate threshold\\nRe-labeling\\n\\nBill Gates, an American investor, stepped down as CEO of Microsoft.\\n\\n4 Handling the Unlabeled Entity Problem\\nIn order to learn discriminative representations for unlabeled entity classes and the true \\\"O\\\" (connected to Observations 1, 2, 3), we propose entity-aware contrastive learning, which adaptively detects entity clusters in \\\"O\\\" during contrastive learning. To further maintain the class discrimination of old classes (connected to Observation 2), we propose two distance-based relabeling strategies to relabel the unlabeled entities from old classes in \\\"O\\\". Additionally, we propose the use of the Nearest Class Mean classifier based on learnt representations in order to avoid the prediction bias of linear classifier.\\n\\nRehearsal-based task formulation\\nTo better learn representations for entities and \\\"O\\\", in this work, we follow the memory replay (rehearsal) setting adopted by most of the previous works (Rebuffi et al., 2017; Mai et al., 2021; Verwimp et al., 2021). Formally, we retain a set of exemplars \\\\( M_c = \\\\{ x_i c, y_i c, X_i c \\\\} \\\\) for each class \\\\( c \\\\), where \\\\( x_i c \\\\) refers to one token labeled as class \\\\( c \\\\) and \\\\( X \\\\) is the context of \\\\( x \\\\) labeled as \\\"O\\\". In all our experiments, we set \\\\( K = 5 \\\\) because the class number can be large.\\n\\n5.1 Entity-aware Contrastive Learning\\nIn this section, we introduce the entity-aware contrastive learning, which dynamically learns entity clusters in \\\"O\\\". To this aim, we first learn an entity-oriented feature space, where the representations of entities are distinctive from \\\"O\\\". This entity-oriented feature space is learnt through contrastive learning on the labeled entity classes in the first \\\\( M \\\\) epochs of each step. Based on the entity-oriented feature space, we further conduct contrastive learning on \\\"O\\\", with the anchors and positive samples dynamically selected based on an entity threshold.\\n\\nLearning an Entity-oriented Feature Space.\\nFirstly, we are to learn an entity-oriented feature space, where the distance between representations reflects entity semantic similarity, i.e., representations from the same entity class have higher similarity while keeping the distance from other classes. This feature space is realized by learning a non-linear mapping \\\\( F(\\\\cdot) \\\\) on the output representations \\\\( h \\\\) of PLM. We adopt cosine similarity as the similarity metric and train with the Supervised Contrastive Loss (Khosla et al., 2020):\\n\\n\\\\[\\nL_{SCL} = \\\\frac{1}{|I|} \\\\sum_{i \\\\in I} \\\\frac{\\\\log e^{s(z_i, z_p)}}{\\\\tau} \\\\sum_{a \\\\in A(i)} e^{s(z_i, z_a)}/\\\\tau\\n\\\\] (1)\\n\\nwhere \\\\( z = F(h) \\\\) denotes the representation after the mapping and \\\\( s(\\\\cdot) \\\\) is the cosine similarity.\\n\\nHere, we apply contrastive learning only on the entity classes, thus we define:\\n\\n\\\\[\\nI = \\\\{ i | i \\\\in \\\\text{Index}(D_{tr}), y_i \\\\neq \\\"O\\\" \\\\}\\n\\\\]\\n\\n\\\\[\\nA(i) = \\\\{ j | j \\\\in \\\\text{Index}(D_{tr}), j \\\\neq i \\\\}\\n\\\\]\\n\\n\\\\[\\nP(i) = \\\\{ p | p \\\\in A(i), y_p = y_i \\\\}\\n\\\\] (2)\"}"}
{"id": "acl-2023-long-328", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the anchor set $I$ only includes entity tokens.\\n\\nWe train with LSCL in the first $K$ epochs, improving the representations of entities and obtaining an entity-oriented feature space.\\n\\nCalculating an entity threshold for anchors and positive samples selection.\\n\\nBased on the entity-oriented feature space, we are to dynamically select possible entity clusters in \\\"O\\\" and further optimize their representations via contrastive learning. This selection is realized by a dynamically adjusted entity threshold.\\n\\nSpecifically, we first define the class similarity $S_c$ as the average of exemplar similarities inside each class:\\n\\n$$S_c = \\\\frac{1}{|M_c|} \\\\sum_{x_i, x_j \\\\in M_c, x_i \\\\neq x_j} s(h(x_i)), h(x_j))$$ (3)\\n\\nThen, we sort the class similarity of all classes and choose the median as the entity threshold $T_{ent}$ (here we simply choose the median for a modest threshold):\\n\\n$$T_{ent} = \\\\text{Sorted}\\\\left(\\\\{S_1, ..., S_{|C_{all}|}\\\\}\\\\right)[i], i = \\\\frac{|C_{all}|}{2}$$ (4)\\n\\nDuring contrastive learning for \\\"O\\\", we re-calculate $T_{ent}$ before each epoch to dynamically adjust the threshold based on convergence degree.\\n\\nContrastive Learning for \\\"O\\\" with the entity threshold\\n\\nBased on entity threshold $T_{ent}$, we then apply the entity-aware contrastive learning for \\\"O\\\" with auto-selected anchors and positive samples. Specifically, we re-define Eq.2 as:\\n\\n$$I_O = \\\\{i | \\\\exists j \\\\neq i, y_j = y_i, y_i = \\\"O\\\", s(z_i, z_j) > T_{ent}\\\\}$$\\n\\n$$P_O(i) = \\\\{p | p \\\\neq i, y_p = \\\"O\\\", s(z_i, z_p) > T_{ent}\\\\}$$\\n\\n$$A_O(i) = P_O(i) \\\\cup \\\\{n | y_n \\\\in C_{t,new}\\\\}$$ (5)\\n\\nThen, we define the entity-aware contrastive loss of \\\"O\\\" by adopting Eq.1 with the definition in Eq.5:\\n\\n$$L_{SCL,O} = L_{SCL}(I_O, P_O, A_O)$$ (6)\\n\\nIn the last $N-K$ epochs, we jointly optimize the representations of entities and \\\"O\\\" by:\\n\\n$$L = L_{SCL,O} + L_{SCL}(7)$$\\n\\n4.2 Relabeling Old Entity Classes\\n\\nIn order to further retain the class discrimination of old classes, we propose two distance-based relabeling strategies to recognize and relabel the unlabeled old-class entities in \\\"O\\\". These two strategies are designed to make use of the previous model $A_{t-1}$ and the exemplar set $M$.\\n\\nRelabeling with Prototypes.\\n\\nThis strategy relabels \\\"O\\\" samples based on their distance to the class prototypes. Specifically, we first calculate the prototype of each class based on the representations of exemplars from the old model $A_{t-1}$.\\n\\n$$p_c = \\\\frac{1}{|M_c|} \\\\sum_{x \\\\in M_c} h_{t-1}(x)$$ (8)\\n\\nThen, we define a relabeling threshold, denoted as the prototype relabeling threshold, by calculating the lowest similarity of all exemplars with their prototypes:\\n\\n$$T_{th_{proto}} = \\\\beta \\\\cdot \\\\min_{x,y \\\\in M_c, c \\\\in C_{t,old}} \\\\{s(h_{t-1}(x), p_y)\\\\}$$ (9)\\n\\nwhere $\\\\beta$ is a hyper-parameter to control the relabeling degree. Next, for each \\\"O\\\" sample $x_i$ in $D_{tr}$, we relabel it only if its highest similarity to prototypes is larger than $T_{th_{proto}}$:\\n\\n$$S = \\\\{s(h_{t-1}(x_i), p_c) | c \\\\in C_{t,old}\\\\}$$\\n\\n$$y_i = \\\\arg \\\\max_c S, \\\\text{ if } \\\\max S > T_{th_{proto}}$$ (10)\\n\\nRelabeling with Nearest Neighbors.\\n\\nIn this approach, we relabel \\\"O\\\" samples based on their distance to the exemplars of each class. Similarly, we define the NN relabeling threshold $T_{th_{NN}}$ as:\\n\\n$$T_{th_{NN}} = \\\\beta \\\\cdot \\\\min_{x_i,x_j \\\\in M_c, c \\\\in C_{t,old}} \\\\{s(h_{t-1}(x_i), h_{t-1}(x_j))\\\\}$$ (11)\\n\\nFor each \\\"O\\\" sample $x_i$, we then relabel it with $T_{th_{NN}}$ by:\\n\\n$$S = \\\\{s(h_{t-1}(x_i), h_{t-1}(x_c)) | x_c \\\\in M_c, c \\\\in C_{t,old}\\\\}$$\\n\\n$$y_i = \\\\arg \\\\max_c S, \\\\text{ if } \\\\max S > T_{th_{NN}}$$ (12)\\n\\nSince the class discrimination of old entity classes keep declining during incremental learning, the older task needs a lower threshold for relabeling sufficient samples. Therefore, we set $\\\\beta = 0.98 - 0.05 \\\\times (t - i)$ for each old task $i$, where $t$ is the current step.\\n\\n4.3 Classifying with NCM Classifier\\n\\nTo make full use of the learnt representations, we adopt the Nearest Class Mean (NCM) classifier used in (Rebuffi et al., 2017) for classification, which is also widely applied in few-shot learning (Snell et al., 2017). For each sample $x$, the class prediction is calculated by:\\n\\n$$y^* = \\\\arg \\\\max_c \\\\in C_{all} \\\\{s(h_t(x), p_c)\\\\}$$ (13)\\n\\nwhere $p_c$ is the prototype of class $c$ calculated with the exemplars as the same in Eq.8.\"}"}
{"id": "acl-2023-long-328", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Main results of the proposed method and baselines on Few-NERD dataset. For each model, we repeat incremental learning experiments on three different task orders and report the averages of the micro-f1 scores. Detailed results on each task order can be found in Appendix A.4.\\n\\nIn this work, we provide a more realistic and challenging benchmark for class-incremental NER based on the Few-NERD dataset (Ding et al., 2021), which contains 66 fine-grained entity types. Following the experimental settings of previous works (Rebuffi et al., 2017; Wu et al., 2019; PourKeshavarzi et al., 2021; Madaan et al., 2021), we randomly split the 66 classes in Few-NERD into 11 tasks, corresponding to 11 steps, each of which contains 6 entity classes and an \\\"O\\\" class. The training set and development set of each task $T_t$ contains sentences only labeled with classes of the current task. The test set contains sentences labeled with all learnt classes in task $\\\\{0, \\\\ldots, t\\\\}$. The statistics and class information of each task order can be found in Appendix A.6.\\n\\n5.1 Experimental Settings\\n\\nThe main experiments in this work are conducted on the Few-NERD datasets. Specifically, for each model, we repeat incremental experiments on three different task orders and report the averages of the micro-f1 score. To further illustrate the proposed method on different datasets, we also conduct experiments on the OntoNotes 5.0 dataset (by splitting 18 classes into 6 tasks) in the same way.\\n\\nWe compare our method with 7 comparable baselines. Full Data denotes Bert-tagger (Devlin et al., 2019) trained with datasets annotated with both old and new classes, which can be regarded as an upper bound. LwF (Li and Hoiem, 2017) is a regularization-based incremental learning method. iCaRL (Rebuffi et al., 2017) is a typical rehearsal-based representation learning method. SCR (Mai et al., 2021) is also an effective rehearsal-based contrastive learning method with an NCM classifier. Con. NER or Continual NER (Monaikul et al., 2021) is the previous SOTA method on class-incremental NER. Con. NER* is Continual NER trained with exemplars and tested with NCM classifier. For our method, Ours (NN) and Ours (Proto) denote our method using NN-based and prototype-based strategies, respectively.\\n\\nThe implementation details of baselines and our method, the dataset details, and the detailed macro-f1 and micro-f1 results of different task orders can be found in Appendix A.1, A.4, A.5 and A.6.\\n\\n5.2 Main Results\\n\\nTable 2 show the results of the proposed method and baselines on the Few-NERD dataset. From the results, we can observe that: (1) The results of Full Data, which leverages all class annotations for training, is relatively consistent. (2) Although Continual NER has shown good performance on CoNLL03 or OntoNotes 5.0 datasets, its performance is limited on this more challenging benchmark, when encountering multiple classes.\"}"}
{"id": "acl-2023-long-328", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Main results of the proposed method and baselines on OntoNotes 5.0 dataset. We report the averages of the micro-f1 scores of three task orders.\\n\\n- (3) The proposed method shows up to 10.62% improvement over baselines, and consistently exceeded the baselines by about 10% even in the later steps, verifying the advantages of the learnt representations.\\n- (4) The prototype-based relabeling strategy is more stable than the NN-based strategy especially in the later steps. A possible reason is that using the mean vector of exemplars for relabeling is more reliable than using each of the exemplars.\\n\\nWe also conduct experiments on the OntoNotes dataset to further illustrate our method. As shown in Table 3, the results of all methods improve on the less challenging setting, yet the proposed method still significantly outperforms all the baselines.\\n\\n5.3 Ablation Studies\\n\\nTo further illustrate the effect of each component on our method, we carry out ablation studies on Few-NERD task order 1 and show the micro-f1 and macro-f1 results in Figure 6. Here, Normal SCL means applying the normal SupCon Loss on both entity classes and \u201cO\u201d without the entity-aware contrastive learning. Similarly, Normal SCL w/o \u201cO\u201d means applying the normal SupCon Loss only on entity classes. Normal SCL w/o relabeling means applying the normal SupCon Loss without relabeling (not using any of our methods). (Both Normal SCL and Normal SCL w/o \u201cO\u201d adopt prototype-based relabeling) w/o relabel denotes using the entity-aware contrastive learning without relabeling.\\n\\nFrom the result, we can see that:\\n- (1) Both the relabeling strategy and entity-aware contrastive learning contributes to high performance.\\n- (2) The performance of normal SCL without the entity-aware contrastive learning and the relabeling strategy is even worse than iCaRL, indicating that inappropriately learning \u201cO\u201d representations can harm performance.\\n- (3) Comparing the micro-f1 and macro-f1 results, we find that the relabeling strategy contributes less to the micro-f1 results. As the micro-f1 results are dominated by head classes with a larger amount of data, we deduce that entity-aware contrastive learning is more useful for head classes (which also appears more in \u201cO\u201d). Also, as the relabeling strategy is based on the distance between representations, the results indicate its effectiveness for both head classes and long-tailed classes.\\n\\n5.4 Effect of Threshold Selection\\n\\nFig. 5 shows the results of different hyperparameter choices for threshold calculation. The upper figure refers to the relabeling threshold \\\\( T_{\\\\text{proto}} \\\\), which we set \\\\( \\\\beta_i = 0.98 - 0.05 \\\\times (t-i) \\\\) for each task \\\\( t \\\\) in step \\\\( i \\\\). In this experiment, we tried different threshold \\\\( T_{\\\\text{proto}} \\\\), which we set \\\\( \\\\beta_i = 0.98 - 0.05 \\\\times (t-i) \\\\) for each task \\\\( t \\\\) in step \\\\( i \\\\).\"}"}
{"id": "acl-2023-long-328", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"## 5.5 Mitigating the Unlabeled Entity Problem\\n\\nTo demonstrate the effectiveness of the proposed method on mitigating the Unlabeled Entity Problem, we conduct the same experiments as in Section 3. Comparing Fig.7 to Fig.2, we can see that the proposed method largely reduce the confusion between \u201cO\u201d and entities, contributing to much fewer error predictions. Comparing Fig.8 to Fig.3 (b), we find that the proposed method learns discriminative representations for the old classes despite the impact of incremental learning.\\n\\n### 6 Related Works\\n\\n#### 6.1 Class-incremental Learning\\n\\nThere are two main research lines of class-incremental learning: (1) Rehearsal-based methods are the most popular and effective methods, which keeps a set of exemplars from the old classes. Typical researches include regularization-based methods that reduces the impact of new classes on old classes (Chaudhry et al., 2019; Riemer et al., 2019); methods that aim to alleviate the biased prediction problem in incremental learning (Zhao et al., 2020; Hou et al., 2019); methods that replay with generative exemplars (Kamra et al., 2017; Ostapenko et al., 2019; Ramapuram et al., 2020). (2) Regularization-based methods aim to regularize the model learning without maintaining any memory. Typical methods include knowledge distillation-based methods (Zhang et al., 2020; Hou et al., 2019) and gradient-based methods that regularize the model parameters (Kirkpatrick et al., 2017; Schwarz et al., 2018; Aljundi et al., 2018). These methods, when directly applied to incremental-NER, do not consider the Unlabeled Entity Problem, thus show limited performance. Nonetheless, these methods are essential references for us to improve class-incremental NER.\\n\\n#### 6.2 Class-incremental Learning for NER\\n\\nPrevious works have explored the class-incremental problems in NER (Monaikul et al., 2021; Wang et al., 2022; Xia et al., 2022). These methods generally care about maintaining old knowledge. Monaikul et al. (2021) propose a knowledge distillation-based method for learning old classes in \u201cO\u201d. Wang et al. (2022) and Xia et al. (2022) propose method to generate synthetic samples for old classes. Among these studies, we are the first to comprehensively investigate the Unlabeled Entity Problem and propose solutions that benefits both the old classes and new classes. We also provide a more realistic benchmark.\\n\\n#### 6.3 Learning \u201cO\u201d for NER\\n\\nMany previous works have also explored \u201clearning \u2018O\u2019\u201d in NER (Tong et al., 2021; Li et al., 2021, 2022; Monaikul et al., 2021; Wang et al., 2022; Ma et al., 2022b). There are three typical lines of work: (1) Tong et al. (2021) solves the \u201cO\u201d problem for few-shot NER. It proposes a multi-step learning strategy that focuses on the specific characteristics of \u201cO\u201d in different tasks. (2) Li et al. (2021) and (2022) propose methods that leverage the existing knowledge to learn \u201cO\u201d in NER. These methods generally consider the relationship between \u201cO\u201d and other entities. (3) Monaikul et al. (2021) and Wang et al. (2022) propose methods that directly model the \u201cO\u201d problem in NER. These methods generally consider the relationship between \u201cO\u201d and other entities. Among these studies, we are the first to comprehensively investigate the Unlabeled Entity Problem and propose solutions that benefits both the old classes and new classes. We also provide a more realistic benchmark.\"}"}
{"id": "acl-2023-long-328", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"undefined-class detection approach to explicitly classify potential entity clusters in \\\"O\\\", which is similar to our core idea. Different from (Tong et al., 2021), we integrate the clustering and detection of potential entity clusters implicitly into representation learning, through a novel design for anchor and positive selection in contrastive learning. To our best knowledge, we are the first to explore the \\\"O\\\" problem in NER with representation learning.\\n\\n(2) There also exist other works that study the unlabeled entity problem (Li et al., 2021, 2022) in NER. These works focus more on avoiding false-negative samples during training and are not specifically designed for distinguishing potential entity classes.\\n\\n(3) The 'O' problem is also considered by previous works in class-incremental NER (Monaikul et al., 2021; Wang et al., 2022), yet they mainly focus on distilling old knowledge from \\\"O\\\". Our work provides new insight on the \\\"O\\\" problem (or unlabeled entity problem) by comprehensively considers the old classes and new classes, with detailed experimental results.\\n\\n7 Conclusion\\n\\nIn this work, we first conduct an empirical study to demonstrate the significance of the Unlabeled Entity Problem in class-incremental NER. Based on our observations, we propose a novel and effective representation learning method for learning discriminative representations for \\\"O\\\" and unlabeled entities. To better evaluate class-incremental NER, we introduce a more realistic and challenging benchmark. Intensive experiments demonstrate the effectiveness and show the superiority of the proposed method over the baselines.\\n\\n8 Limitations\\n\\nThe limitations of this work are: (1) In this work, we expect to consider more realistic and more applicable settings for class-incremental NER. Therefore, we consider the Unlabeled Entity Problem and provide a more realistic benchmark based on 66 fine-grained entity types. However, there remain some more serious situations unsolved in this work. First, the entity classes in each step might not be disjoint. For example, a new entity type \\\"Director\\\" might be included in an old entity type \\\"Person\\\". This problem is referred to as the coarse-to-fine problem existing in emerging types of NER. Second, the amount of data or labeled data introduced in each step can also be limited, referring to the few-shot class-incremental problem. Therefore, the proposed method can be further improved to solve these problems. Third, the current version of the proposed method cannot handle the nested NER or contiguous NER problems. In the current version, we simply followed typical works in NER and adopted the sequence labeling scheme to model the NER task, which is not suitable for more complicated NER tasks. Nonetheless, as the proposed representation learning and re-labeling methods are agnostic to the formation of representations, we believe our method can also be adapted to a span-level version, which might be future works. (2) The proposed method is a rehearsal-based method that requires keeping exemplar sets for each class. Although the number of exemplars for each class is really small, we believe there can be more data-efficient solutions that totally avoid the need of memorizing data and also achieve good results. (3) The proposed method includes several hyper-parameters such as the entity threshold $T_{entity}$, relabeling threshold $T_{hNN}$ and $T_{hproto}$. Although we have shown that the choice of thresholds is relatively robust (Sec.5.4), it still requires efforts to explore the most suitable thresholds when applied to other datasets or situations. There can be further work to improve this problem by formulating an automatic threshold searching strategy.\\n\\nAcknowledgements\\n\\nThe authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by the National Natural Science Foundation of China (No.62076069,62206057,61976056), Shanghai Rising-Star Program (23QA1400200), and Natural Science Foundation of Shanghai (23ZR1403500).\\n\\nReferences\\n\\nRahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. 2018. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV), pages 139\u2013154.\\n\\nJihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. 2021. Rainbow memory: Continual learning with a memory of diverse samples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8218\u20138227.\"}"}
{"id": "acl-2023-long-328", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Francisco M Castro, Manuel J Mar\u00edn-Jim\u00e9nez, Nicol\u00e1s Guil, Cordelia Schmid, and Karteek Alahari. 2018. End-to-end incremental learning. In Proceedings of the European conference on computer vision (ECCV), pages 233\u2013248.\\n\\nArslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. 2019. Efficient lifelong learning with a-GEM. In International Conference on Learning Representations.\\n\\nMatthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and Tinne Tuytelaars. 2021. A continual learning survey: Defying forgetting in classification tasks.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nNing Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. 2021. Few-NERD: A few-shot named entity recognition dataset. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3198\u20133213, Online. Association for Computational Linguistics.\\n\\nSaihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. 2019. Learning a unified classifier incrementally via rebalancing. Computer Vision and Pattern Recognition.\\n\\nNitin Kamra, Umang Gupta, and Yan Liu. 2017. Deep generative dual memory network for continual learning. arXiv preprint arXiv:1710.10368.\\n\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020. Supervised contrastive learning. In Advances in Neural Information Processing Systems, volume 33, pages 18661\u201318673. Curran Associates, Inc.\\n\\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526.\\n\\nJing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2020. A survey on deep learning for named entity recognition. IEEE Transactions on Knowledge and Data Engineering, 34(1):50\u201370.\\n\\nYangming Li, Lemao Liu, and Shuming Shi. 2021. Empirical analysis of unlabeled entity problem in named entity recognition. In International Conference on Learning Representations.\\n\\nYangming Li, Lemao Liu, and Shuming Shi. 2022. Rethinking negative sampling for handling missing entity annotations. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7188\u20137197, Dublin, Ireland. Association for Computational Linguistics.\\n\\nZhizhong Li and Derek Hoiem. 2017. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):2935\u20132947.\\n\\nZihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung. 2021. Crossner: Evaluating cross-domain named entity recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13452\u201313460.\\n\\nDavid Lopez-Paz and Marc'Aurelio Ranzato. 2017. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30.\\n\\nRuotian Ma, Yiding Tan, Xin Zhou, Xuanting Chen, Di Liang, Sirui Wang, Wei Wu, and Tao Gui. 2022a. Searching for optimal subword tokenization in cross-domain ner. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 4289\u20134295. International Joint Conferences on Artificial Intelligence Organization. Main Track.\\n\\nRuotian Ma, Xin Zhou, Tao Gui, Yiding Tan, Linyang Li, Qi Zhang, and Xuanjing Huang. 2022b. Template-free prompt tuning for few-shot NER. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5721\u20135732, Seattle, United States. Association for Computational Linguistics.\\n\\nDivyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang. 2021. Representational continuity for unsupervised continual learning. In International Conference on Learning Representations.\\n\\nZheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner. 2021. Supervised contrastive replay: Revisiting the nearest class mean classifier in online class-incremental continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3589\u20133599.\\n\\nMarc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost van de Weijer. 2020. Class-incremental learning: survey and performance evaluation on image classification. arXiv preprint arXiv:2010.15277.\"}"}
{"id": "acl-2023-long-328", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-328", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nA.1 Implementation Details\\nWe implemented the proposed method and all baselines based on the bert-base-cased pretrained model using the implementation of huggingface transformers. For our method, we implement the SupCon loss based on the implementation in the SupContrast library. For LwF and iCaRL, we follow the implementations of (Masana et al., 2020). For SCR, we follow the implementation of the online-continual-learning library. There is no public source of Continual NER, so we implement based on the paper (Monaikul et al., 2021) and report the results of our implementation. At each step, we trained the model for 16 epochs and selected the best model on the dev set. For all methods, we use a learning rate of 5e-5, batch size of 16 and the max sequence length of 128. For our method, we start entity-aware contrastive learning for \u201cO\u201d with $L_{SCL,O}$ at the 10-th epoch and train it for 6 epochs at each step. We conducted all experiments on NVIDIA GeForce RTX 3090.\\n\\nConstruction of the exemplar set\\nFor all rehearsal-based method, we keep 5 exemplars for each class, each of which consist of one entity word and its context. The exemplar words of each class are selected by picking the most high-frequency words of each class in the dataset. For each exemplar word, we randomly pick one sentence that contains this word as its context. We use the same exemplar set for all methods.\\n\\nA.2 Performance on Old and New Classes\\nIn figure 9, we show the performance change of different methods on old classes and new classes. As seen, the proposed method can maintain the performance of old classes in a higher degree, which mainly attributes to the relabeling strategy. Meanwhile, the entity-aware contrastive learning method also helps to keep the discrimination of old classes in \u201cO\u201d. Also, the proposed method is more effective on learning the new classes than baseline methods, with a highest improvement of 6.01% in the last step. These results indicate the effectiveness of entity-aware contrastive learning, which helps learn fine-grained and entity-aware representations for \u201cO\u201d, preventing the potential classes from confusing with \u201cO\u201d and other similar classes.\\n\\n| Steps | Precision | Recall | Micro-f1 |\\n|-------|-----------|--------|----------|\\n| Step 1 | 56.61     | 99.04  | 72.04    |\\n| Step 4 | 62.24     | 84.29  | 71.61    |\\n| Step 7 | 74.92     | 70.82  | 72.81    |\\n\\nPrototype-based relabeling ($\\\\beta = 0.9$)\\n\\n| Steps | Precision | Recall | Micro-f1 |\\n|-------|-----------|--------|----------|\\n| Step 1 | 52.52     | 99.16  | 68.67    |\\n| Step 4 | 61.61     | 73.72  | 67.12    |\\n| Step 7 | 79.40     | 67.63  | 73.05    |\\n\\nNN-based relabeling\\n\\n| Steps | Precision | Recall | Micro-f1 |\\n|-------|-----------|--------|----------|\\n| Step 1 | 60.08     | 98.78  | 74.71    |\\n| Step 4 | 64.81     | 81.32  | 72.14    |\\n| Step 7 | 74.56     | 76.55  | 75.54    |\\n\\nTable 4: Relabeling statistics of different strategies.\\n\\nA.3 Relabeling Statistics\\nWe examine the token-level micro-f1 scores of different relabeling strategies based on the gold labeled data of each step on Few-NERD task order 1. The results are shown in Table 4. We find that: (1) The proposed relabeling strategies can achieve acceptable relabeling accuracy, which greatly helps for retaining the knowledge of old classes and improving representation learning for potential classes. (2) Using a fixed $\\\\beta$ leads to higher recall and lower precision in earlier steps, as well as lower recall in later steps. This might because the convergence degree of old classes decrease in later step, thus a fixed threshold will relabel limited...\"}"}
{"id": "acl-2023-long-328", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(3) Compared to prototype-based method, the NN-based method has slightly lower recall and higher precision in earlier steps, which might correspond to its slightly higher performance in earlier steps on Few-NERD task order 1 (Table 5).\\n\\n### A.4 Detailed Results on Few-NERD\\n\\nThe detailed results on the Few-NERD datasets are shown in Table 5 (task order 1), Table 6 (task order 2), Table 7 (task order 3). In each table, the numbers in black denote the micro-f1 scores and the numbers in green denote the macro-f1 scores. The proposed method surpass all baseline methods in all task orders.\\n\\n### A.5 Detailed Results on OntoNotes 5.0\\n\\nWe also conduct experiments on OntoNotes 5.0 by randomly splitting the 18 entity classes into 6 tasks, each of which contains 3 entity classes and a \u201cO\u201d class. The detailed results on the OntoNotes datasets are shown in Table 8 (task order 1), Table 9 (task order 2), Table 10 (task order 3). In each table, the numbers in black denote the micro-f1 scores and the numbers in green denote the macro-f1 scores. The proposed method surpass all baseline methods in all task orders.\\n\\n### A.6 Dataset Details\\n\\nThe dataset details of Few-NERD are shown in Table 11 (task order 1), Table 12 (task order 2), Table 13 (task order 3). The dataset details of OntoNotes 5.0 are shown in Table 14 (task order 1), Table 15 (task order 2), Table 16 (task order 3).\\n\\n9[https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf](https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf)\"}"}
{"id": "acl-2023-long-328", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Methods  | Step 0 | Step 1 | Step 2 | Step 3 | Step 4 | Step 5 | Step 6 | Step 7 | Step 8 | Step 9 | Step 10 |\\n|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\\n| Full Data| 73.30  | 68.73  | 68.02  | 66.21  | 66.60  | 68.79  | 68.40  | 67.29  | 67.44  | 66.61  | 66.35  |\\n| LwF      | 71.51  | 66.21  | 67.14  | 65.79  | 65.20  | 61.92  | 61.83  | 60.24  | 60.83  | 59.93  | 60.32  |\\n| SCR      | 73.21  | 50.74  | 51.44  | 40.41  | 41.73  | 49.07  | 45.52  | 42.88  | 40.50  | 35.80  | 30.47  |\\n| iCaRL    | 72.69  | 48.50  | 48.72  | 43.50  | 43.97  | 50.32  | 48.43  | 46.60  | 45.88  | 45.50  | 43.30  |\\n| Con.NER  | 73.42  | 47.02  | 43.09  | 35.86  | 36.47  | 44.79  | 37.49  | 37.08  | 36.43  | 35.24  | 27.04  |\\n| Con.NER* | 73.56  | 55.84  | 46.27  | 38.71  | 37.34  | 44.20  | 41.61  | 39.34  | 38.23  | 37.44  | 34.19  |\\n| Ours (NN)| 74.04  | 59.22  | 59.08  | 52.18  | 53.24  | 60.51  | 57.81  | 55.41  | 53.61  | 49.44  | 46.93  |\\n| Ours (Proto)| 74.04 | 59.12  | 59.07  | 52.94  | 52.69  | 59.93  | 56.99  | 55.14  | 54.39  | 53.00  | 50.72  |\\n\\nTable 5: Detailed results of Few-NERD task order 1. The numbers in black are the micro-f1 scores and the numbers in green are the macro-f1 scores.\\n\\n| Methods  | Step 0 | Step 1 | Step 2 | Step 3 | Step 4 | Step 5 | Step 6 | Step 7 | Step 8 | Step 9 | Step 10 |\\n|----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\\n| Full Data| 82.26  | 78.90  | 77.47  | 73.02  | 71.57  | 69.10  | 68.08  | 67.67  | 67.32  | 66.48  | 66.29  |\\n| LwF      | 82.33  | 67.39  | 55.59  | 49.60  | 37.25  | 30.90  | 28.86  | 32.30  | 29.35  | 27.86  | 23.57  |\\n| SCR      | 81.95  | 67.68  | 50.66  | 50.98  | 42.82  | 34.78  | 34.11  | 37.90  | 31.68  | 29.40  | 23.82  |\\n| iCaRL    | 81.91  | 66.01  | 56.73  | 51.19  | 45.16  | 40.64  | 41.05  | 41.52  | 41.13  | 41.58  | 41.50  |\\n| Con.NER  | 82.44  | 66.55  | 42.94  | 38.07  | 29.65  | 24.31  | 22.07  | 25.91  | 24.59  | 23.62  | 23.35  |\\n| Con.NER* | 82.38  | 68.89  | 56.38  | 48.42  | 37.71  | 33.76  | 33.81  | 29.43  | 29.84  | 28.89  | 26.75  |\\n| Ours (NN)| 82.32  | 73.27  | 67.96  | 62.07  | 57.49  | 47.27  | 48.95  | 52.33  | 49.75  | 49.44  | 49.75  |\\n| Ours (Proto)| 82.32 | 72.97  | 68.38  | 61.64  | 56.75  | 51.30  | 53.33  | 53.44  | 52.75  | 52.20  | 52.10  |\\n\\nTable 6: Detailed results of Few-NERD task order 2. The numbers in black are the micro-f1 scores and the numbers in green are the macro-f1 scores.\"}"}
{"id": "acl-2023-long-328", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Methods   | Step 0 | Step 1 | Step 2 | Step 3 | Step 4 | Step 5 | Step 6 | Step 7 | Step 8 | Step 9 | Step 10 |\\n|-----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|---------|\\n| **Full Data** | 70.79  | 70.24  | 69.68  | 68.94  | 68.58  | 67.87  | 66.16  | 65.81  | 64.74  | 67.40  | 66.35   |\\n| **LwF**   | 70.87  | 55.91  | 45.79  | 38.79  | 36.68  | 29.58  | 24.61  | 24.93  | 18.39  | 30.32  | 25.90   |\\n| **SCR**   | 70.25  | 53.74  | 44.09  | 45.02  | 43.72  | 38.98  | 33.62  | 31.70  | 28.60  | 38.33  | 34.32   |\\n| **iCaRL** | 70.07  | 52.78  | 48.98  | 45.48  | 45.81  | 40.57  | 38.42  | 37.60  | 34.60  | 42.92  | 42.00   |\\n| **Con.NER** | 70.98 | 52.28  | 41.93  | 33.84  | 31.51  | 22.54  | 19.05  | 20.70  | 15.10  | 25.85  | 25.13   |\\n| **Con.NER** | 70.95 | 54.95  | 46.81  | 39.57  | 33.03  | 31.36  | 26.34  | 27.68  | 25.22  | 28.71  | 23.20   |\\n| **Ours (NN)** | 70.84 | 63.77  | 59.47  | 56.68  | 55.93  | 50.58  | 46.54  | 41.80  | 38.09  | 49.31  | 46.10   |\\n| **Ours (Proto)** | 70.84 | 62.85  | 59.12  | 56.66  | 57.23  | 52.18  | 48.38  | 47.91  | 46.02  | 52.99  | 50.68   |\\n\\n**Table 7:** Detailed results of Few-NERD task order 3. The numbers in black are the micro-f1 scores and the numbers in green are the macro-f1 scores.\\n\\n| Methods   | Step 0 | Step 1 | Step 2 | Step 3 | Step 4 | Step 5 | Step 6 | Step 7 | Step 8 | Step 9 | Step 10 |\\n|-----------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|---------|\\n| **Full Data** | 93.71  | 91.07  | 90.97  | 90.38  | 88.93  | 87.47  | 84.82  | 82.45  | 78.11  | 78.36  | 77.02   |\\n| **LwF**   | 93.28  | 86.63  | 73.58  | 73.60  | 71.70  | 64.24  | 84.74  | 77.11  | 63.58  | 61.68  | 53.73   |\\n| **SCR**   | 93.36  | 89.28  | 86.10  | 82.81  | 81.98  | 78.47  | 81.71  | 75.59  | 71.50  | 69.06  | 67.28   |\\n| **iCaRL** | 93.62  | 87.78  | 78.91  | 79.60  | 76.52  | 75.33  | 84.21  | 78.09  | 65.35  | 68.64  | 65.08   |\\n| **Con.NER** | 93.16 | 83.25  | 70.90  | 71.59  | 60.26  | 63.15  | 83.62  | 71.99  | 59.90  | 59.01  | 50.13   |\\n| **Con.NER** | 93.24 | 83.53  | 73.81  | 72.25  | 64.03  | 62.42  | 83.46  | 72.51  | 60.29  | 59.04  | 51.00   |\\n| **Ours (NN)** | 93.69 | 89.23  | 88.47  | 87.55  | 86.45  | 83.15  | 83.39  | 78.95  | 73.49  | 71.49  | 71.28   |\\n| **Ours (Proto)** | 93.69 | 89.53  | 88.50  | 87.50  | 86.20  | 84.02  | 83.39  | 79.84  | 74.33  | 72.92  | 70.78   |\\n\\n**Table 8:** Detailed results of OntoNotes task order 1. The numbers in black are the micro-f1 scores and the numbers in green are the macro-f1 scores.\"}"}
{"id": "acl-2023-long-328", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Methods  | Step 0 | Step 1 | Step 2 | Step 3 | Step 4 | Step 5 |\\n|----------|--------|--------|--------|--------|--------|--------|\\n| Full Data | 94.92  | 92.07  | 90.24  | 89.94  | 88.92  | 87.33  |\\n|          | 92.85  | 78.90  | 78.54  | 77.84  | 77.97  | 77.27  |\\n| LwF      | 95.29  | 80.10  | 77.75  | 78.88  | 58.99  | 56.81  |\\n|          | 93.19  | 59.83  | 60.18  | 61.11  | 47.86  | 46.95  |\\n| SCR      | 94.63  | 81.37  | 83.97  | 84.39  | 83.40  | 79.76  |\\n|          | 91.33  | 61.60  | 63.79  | 63.47  | 62.55  | 61.78  |\\n| iCaRL    | 95.34  | 84.37  | 81.09  | 81.86  | 82.60  | 78.91  |\\n|          | 92.80  | 69.30  | 65.60  | 65.79  | 67.94  | 66.23  |\\n| Con.NER  | 94.81  | 74.22  | 72.15  | 72.68  | 73.37  | 66.37  |\\n|          | 92.13  | 55.44  | 52.84  | 55.07  | 53.51  | 51.20  |\\n| Con.NER* | 94.99  | 74.66  | 72.80  | 74.11  | 74.28  | 66.09  |\\n|          | 92.45  | 55.63  | 53.23  | 56.07  | 55.70  | 52.45  |\\n| Ours (NN)| 94.65  | 85.87  | 86.17  | 87.80  | 86.97  | 83.40  |\\n|          | 92.72  | 69.93  | 69.86  | 72.34  | 72.68  | 70.21  |\\n| Ours (Proto)| 94.65 | 85.33 | 86.79 | 87.13 | 86.71 | 82.75 |\\n|          | 92.72 | 69.65 | 71.23 | 71.89 | 71.72 | 68.89 |\\n\\nTable 9: Detailed results of OntoNotes task order 2. The numbers in black are the micro-f1 scores and the numbers in green are the macro-f1 scores.\\n\\n| Methods  | Step 0 | Step 1 | Step 2 | Step 3 | Step 4 | Step 5 |\\n|----------|--------|--------|--------|--------|--------|--------|\\n| Full Data | 88.64  | 89.64  | 87.91  | 87.93  | 87.05  | 87.39  |\\n|          | 86.15  | 79.89  | 76.82  | 78.15  | 76.80  | 76.24  |\\n| LwF      | 87.88  | 81.65  | 72.46  | 45.39  | 43.01  | 48.05  |\\n|          | 85.36  | 65.78  | 55.52  | 44.41  | 47.94  | 46.83  |\\n| SCR      | 88.13  | 83.76  | 71.53  | 68.83  | 64.05  | 69.93  |\\n|          | 85.00  | 68.72  | 59.03  | 61.20  | 60.30  | 58.20  |\\n| iCaRL    | 92.49  | 84.89  | 79.65  | 80.03  | 78.76  | 77.14  |\\n|          | 87.56  | 72.97  | 65.95  | 67.17  | 67.09  | 65.86  |\\n| Con.NER  | 88.08  | 82.24  | 66.63  | 64.16  | 59.78  | 51.94  |\\n|          | 85.22  | 66.33  | 57.53  | 51.72  | 48.32  | 42.58  |\\n| Con.NER* | 88.03  | 82.48  | 70.09  | 66.28  | 61.64  | 55.93  |\\n|          | 84.96  | 66.32  | 57.84  | 55.93  | 50.94  | 46.77  |\\n| Ours (NN)| 88.82  | 88.05  | 85.15  | 85.15  | 83.50  | 82.38  |\\n|          | 86.33  | 75.86  | 71.62  | 73.08  | 73.04  | 69.71  |\\n| Ours (Proto)| 88.82 | 87.35 | 85.31 | 84.65 | 83.99 | 82.78 |\\n|          | 86.33 | 73.51 | 70.08 | 71.21 | 73.25 | 69.89 |\\n\\nTable 10: Detailed results of OntoNotes task order 3. The numbers in black are the micro-f1 scores and the numbers in green are the macro-f1 scores.\"}"}
{"id": "acl-2023-long-328", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task | Entity Class | # Train | # Dev | # Test |\\n|------|--------------|---------|------|-------|\\n| 1    | 'building-library', 'organization-showorganization', 'other-award', 'building-other', 'organization-religion', 'organization-sportsteam' | 18435  | 2656 | 5296  |\\n| 2    | 'person-politician', 'art-painting', 'event-disaster', 'organization-other', 'product-weapon', 'building-hotel' | 18966  | 2788 | 10267 |\\n| 3    | 'event-sportsevent', 'other-chemicalthing', 'art-writtenart', 'product-game', 'location-mountain', 'other-livingthing' | 11973  | 1652 | 13055 |\\n| 4    | 'location-island', 'person-scholar', 'building-restaurant', 'other-astronomything', 'building-airport', 'product-other' | 9448   | 1326 | 15178 |\\n| 5    | 'location-road/railway/highway/transit', 'other-educationaldegree', 'building-sportsfacility', 'event-election', 'person-actor', 'art-film' | 10295  | 1477 | 17254 |\\n| 6    | 'location-other', 'product-ship', 'organization-politicalparty', 'person-soldier', 'location-GPE', 'other-god' | 47648  | 6941 | 24429 |\\n| 7    | 'event-attack/battle/war/militaryconflict', 'organization-sportsleague', 'building-theater', 'organization-education', 'product-train', 'other-medical' | 13237  | 1852 | 25631 |\\n| 8    | 'event-protest', 'person-other', 'product-car', 'art-other', 'organization-company', 'other-disease' | 30899  | 4416 | 29014 |\\n| 9    | 'other-biologything', 'person-artist/author', 'location-bodiesofwater', 'art-broadcastprogram', 'other-language', 'person-athlete' | 21794  | 3114 | 31036 |\\n| 10   | 'product-airplane', 'art-music', 'product-software', 'event-other', 'location-park', 'organization-media/newspaper' | 11963  | 1706 | 31874 |\\n| 11   | 'other-currency', 'person-director', 'building-hospital', 'other-law', 'organization-government/governmentagency', 'product-food' | 9787   | 1443 | 32565 |\\n\\nTable 11: Details of Few-NERD task order 1.\\n\\n| Task | Entity Class | # Train | # Dev | # Test |\\n|------|--------------|---------|------|-------|\\n| 1    | 'location-GPE', 'event-sportsevent', 'organization-showorganization', 'event-attack/battle/war/militaryconflict', 'art-other', 'product-car' | 48730  | 7060 | 13963 |\\n| 2    | 'location-bodiesofwater', 'person-scholar', 'person-artist/author', 'person-politician', 'other-livingthing', 'product-airplane' | 21523  | 3183 | 17477 |\\n| 3    | 'product-other', 'art-music', 'location-island', 'person-athlete', 'building-airport', 'building-hotel' | 15257  | 2169 | 19805 |\\n| 4    | 'person-soldier', 'event-other', 'product-software', 'event-election', 'organization-other', 'organization-politicalparty' | 17967  | 2531 | 22192 |\\n| 5    | 'other-award', 'art-film', 'organization-government/governmentagency', 'other-astronomything', 'person-actor', 'person-director' | 12258  | 1792 | 23841 |\\n| 6    | 'event-protest', 'building-library', 'art-broadcastprogram', 'other-educationaldegree', 'organization-sportsleague', 'location-other' | 11191  | 1620 | 25125 |\\n| 7    | 'product-game', 'event-disaster', 'product-train', 'building-other', 'other-disease', 'building-hospital' | 11243  | 1578 | 26473 |\\n| 8    | 'product-ship', 'other-currency', 'art-painting', 'product-weapon', 'organization-sportsteam', 'person-other' | 28035  | 4055 | 29113 |\\n| 9    | 'other-god', 'art-writtenart', 'other-chemicalthing', 'organization-education', 'other-medical', 'building-restaurant' | 11390  | 1631 | 30302 |\\n| 10   | 'building-sportsfacility', 'building-theater', 'organization-company', 'other-biologything', 'organization-religion', 'other-law' | 16806  | 2333 | 31810 |\\n| 11   | 'location-mountain', 'location-road/railway/highway/transit', 'organization-media/newspaper', 'location-park', 'product-food', 'other-language' | 11256  | 1594 | 32565 |\\n\\nTable 12: Details of Few-NERD task order 2.\"}"}
{"id": "acl-2023-long-328", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task | Entity Class | # Train | # Dev | # Test |\\n|------|--------------|---------|-------|-------|\\n| 1    | `organization-other`, `art-film`, `product-weapon` | 24344   | 3377  | 6860  |\\n| 2    | `person-actor`, `product-other`, `person-athlete` | 18381   | 2603  | 11276 |\\n| 3    | `event-attack/battle/war/militaryconflict`, `organization-showorganization` | 102711  | 1504  | 13318 |\\n| 4    | `other-award`, `location-road/railway/highway/transit`, `event-election` | 25684   | 3748  | 18354 |\\n| 5    | `other-medical`, `other-chemicalthing`, `product-airplane` | 14767   | 2148  | 20948 |\\n| 6    | `other-astronomything`, `building-library`, `organization-sportsteam` | 15831   | 2302  | 23476 |\\n| 7    | `other-biologything`, `location-mountain`, `location-other` | 12075   | 1723  | 25509 |\\n| 8    | `organization-politicalparty`, `product-car`, `building-hotel` | 14431   | 2089  | 27139 |\\n| 9    | `product-train`, `organization-government/governmentagency`, `other-disease` | 9792    | 1388  | 28142 |\\n| 10   | `art-writtenart`, `other-god`, `art-other`, `organization-sportsleague` | 47410   | 6902  | 31564 |\\n| 11   | `product-game`, `product-software`, `person-scholar` | 15549   | 2157  | 32565 |\\n\\nTable 13: Details of Few-NERD task order 3.\\n\\n| Task | Entity Class | # Train | # Dev | # Test |\\n|------|--------------|---------|-------|-------|\\n| 1    | `PRODUCT`, `GPE`, `CARDINAL` | 15119   | 2149  | 2124  |\\n| 2    | `QUANTITY`, `DATE`, `LANGUAGE` | 9561    | 1335  | 2883  |\\n| 3    | `PERSON`, `LAW`, `LOC` | 13424   | 1725  | 3852  |\\n| 4    | `ORDINAL`, `PERCENT`, `EVENT` | 3259    | 460   | 4002  |\\n| 5    | `NORP`, `FAC`, `TIME` | 6913    | 949   | 4291  |\\n| 6    | `MONEY`, `WORK_OF_ART`, `ORG` | 11286   | 1480  | 4624  |\\n\\nTable 14: Details of OntoNotes 5.0 task order 1.\\n\\n| Task | Entity Class | # Train | # Dev | # Test |\\n|------|--------------|---------|-------|-------|\\n| 1    | `ORDINAL`, `PERSON`, `PERCENT` | 14323   | 1814  | 1919  |\\n| 2    | `WORK_OF_ART`, `PRODUCT`, `LAW` | 1634    | 229   | 2073  |\\n| 3    | `CARDINAL`, `EVENT`, `QUANTITY` | 6786    | 904   | 2711  |\\n| 4    | `GPE`, `MONEY`, `TIME` | 12823   | 1887  | 3718  |\\n| 5    | `NORP`, `LANGUAGE`, `DATE` | 13090   | 1820  | 4318  |\\n| 6    | `LOC`, `FAC`, `ORG` | 11127   | 1500  | 4624  |\\n\\nTable 15: Details of OntoNotes 5.0 task order 2.\"}"}
{"id": "acl-2023-long-328", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task | Entity Class          | # Train | # Dev | # Test |\\n|------|-----------------------|---------|------|-------|\\n| 1    | ['ORG', 'CARDINAL', 'QUANTITY'] | 14284   | 1898 | 1867  |\\n| 2    | ['LAW', 'FAC', 'GPE'] | 11335   | 1692 | 2877  |\\n| 3    | ['DATE', 'LANGUAGE', 'WORK_OF_ART'] | 9791    | 1386 | 3522  |\\n| 4    | ['PERCENT', 'NORP', 'EVENT'] | 6927    | 927  | 3815  |\\n| 5    | ['ORDINAL', 'TIME', 'MONEY'] | 4327    | 596  | 3979  |\\n| 6    | ['PERSON', 'LOC', 'PRODUCT'] | 13663   | 1747 | 4624  |\"}"}
{"id": "acl-2023-long-328", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\n8\\n\u25a1 ... NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\\n\\nB Did you use or create scientific artifacts?\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n5\\n\u25a1 ... NER tasks that involve identifying the names of people, thus the data are usually not anonymized.\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC Did you run computational experiments?\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\"}"}
{"id": "acl-2023-long-328", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nA.1\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nA.4, A.5\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nA.1\\n\\nD\\n\\nDid you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nLeft blank.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nNo response.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nNo response.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nNo response.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nNo response.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nNo response.\"}"}
