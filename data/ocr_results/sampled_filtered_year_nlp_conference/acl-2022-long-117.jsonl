{"id": "acl-2022-long-117", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Error analysis of generated examples\\n\\nTable 11 shows examples where the RGF model produced incorrect \\\\((q', a', c')\\\\) triples, selected from the manually-annotated subset described in Section 4.\\n\\nNonsensical Question\\n\\nContext:\\nThe security management process relates to other ITIL - processes. However, in this particular section the most obvious relations are the relations to the service level management, incident management and change management processes. Security management is a continuous process that can be compared to W. Edwards Deming\u2019s Quality Circle (Plan, Do, Check, Act). The inputs are requirements from clients. The requirements are translated into security services and security metrics.\\n\\nAnswer:\\nW. Edwards Deming\\n\\nGenerated Question:\\nthe security management process is similar to the itil?\\n\\nIncomplete Question\\n\\nContext:\\nUsing Cartesian coordinates, inertial motion is described mathematically as: where \u201cx\u201d is the position coordinate and \u201c\u03c4\u201d is proper time. (In Newtonian mechanics, \u201c\u03c4 \u2261 t\u201d, the coordinate time). In both Newtonian mechanics and special relativity, space and then spacetime are assumed to be flat, and we can construct a global Cartesian coordinate system. In general relativity, these restrictions on the shape of spacetime and on the coordinate system to be used are lost. Therefore, a different definition of inertial motion is required.\\n\\nAnswer:\\ngeneral relativity\\n\\nGenerated Question:\\nwhich theory states that all motion is a function of?\\n\\nCorrect Type, but Wrong Entity\\n\\nContext:\\nRuth McDevitt (September 13, 1895 \u2013 May 27, 1976) was an American stage, film, radio and television actress. She was born Ruth Thane Shoecraft in Coldwater, Michigan. After attending the American Academy of Dramatic Arts, she married Patrick McDevitt and decided to devote her time to her marriage. After her husband\u2019s death in 1934, she returned to acting. She performed on Broadway, in particular understudying and succeeding Josephine Hull in \u201cArtsenic and Old Lace\u201d and \u201cThe Solid Gold Cadillac\u201d. She also worked as a radio actor. McDevitt was a familiar face on television during the 1950s, 1960s, and 1970s. She played \u201cMom Peepers\u201d in the 1950s sitcom \u201cMister Peepers\u201d. She was a regular with Ann Sheridan, Douglas Fowley, and Gary Vinson in CBS\u2019s \u201cPistols \u2019n\u2019 Petticoats\u201d, a 1966-67 satire of the Old West.\\n\\nAnswer:\\nAnn Sheridan\\n\\nGenerated Question:\\nwho played the mother on mr peepers?\"}"}
{"id": "acl-2022-long-117", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Retrieval-guided Counterfactual Generation for QA\\nBhargavi Paranjape1\u2217, Matthew Lamm 2, and Ian Tenney 2\\n1Paul G. Allen School of Computer Science & Engineering, University of Washington\\n2Google Research\\nbparan@cs.washington.edu\\n{mrlamm,iftenney}@google.com\\n\\nAbstract\\nDeep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals \u2014 i.e. minimally perturbed inputs \u2014 can help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. To address these challenges, we develop a Retrieve-Generate-Filter (RGF) technique to create counterfactual evaluation and training data with minimal human supervision. Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled. Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings. Moreover, we find that RGF data leads to significant improvements to robustness to local perturbations.\\n\\n1 Introduction\\nModels for natural language understanding (NLU) may outperform humans on standard benchmarks, yet still often perform poorly under a multitude of distributional shifts (Jia and Liang (2017); Naik et al. (2018); McCoy et al. (2019), inter alia) due to over-reliance on spurious correlations or dataset artifacts. This behavior can be probed using counterfactual data (Kaushik et al., 2020; Gardner et al., 2020) designed to simulate interventions on specific attributes: for example, perturbing the movie review \u201cA real stinker, one out of ten!\u201d to \u201cA real classic, ten out of ten!\u201d allows us to discern the effect of adjective polarity on the model\u2019s prediction. Many recent works (Kaushik et al., 2020, 2021; Wu et al., 2021a; Geva et al., 2021, inter alia) have shown that training augmented with this counterfactual data (CDA) improves out-of-domain generalization and robustness against spurious correlations. Consequently, several techniques have been proposed for the automatic generation of counterfactual data for several downstream tasks (Wu et al., 2021a; Ross et al., 2021b,a; Bitton et al., 2021; Geva et al., 2021; Asai and Hajishirzi, 2020; Mille et al., 2021).\\n\\nIn this paper, we focus on counterfactual data for question answering, in both the reading comprehension and open-domain settings (e.g. Rajpurkar et al., 2016; Kwiatkowski et al., 2019). Model inputs consist of a question and optionally a context passage, and the target is a short answer span. Counterfactuals are often considered in the context of a specific causal model (Miller, 2019; Halpern and Pearl, 2005), but in this work we follow Wu et al. (2021a) and Kaushik et al. (2020) and seek a method to generate counterfactuals that may be used...\"}"}
{"id": "acl-2022-long-117", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ful in many different settings. In QA, the set of possible causal features is large and difficult to specify a priori; relevant factors are often instance-specific and exploring them may require world knowledge. For example, going from \u201cWho is the captain of the Richmond Football Club\u201d to a perturbed question \u201cWho captained Richmond\u2019s women\u2019s team?\u201d as in Figure 1 requires knowledge about the club\u2019s alternate teams, and the perturbation \u201cWho was the captain of RFC in 1998?\u201d requires knowledge about the time-sensitive nature of the original question. In the absence of such knowledge, otherwise reasonable edits\u2014such as \u201cWho captained the club in 2050?\u201d\u2014can result in false premises or unanswerable questions.\\n\\nWe develop a simple yet effective technique to address these challenges: Retrieve, Generate, and Filter (RGF; Figure 1). We use the near-misses of a retrieve-and-read QA model to propose alternate contexts and answers which are closely related to\u2014but semantically distinct from\u2014the original question. We then use a sequence-to-sequence question generation model (Alberti et al., 2019) to generate corresponding questions to these passages and answers. This results in fully-labeled examples, which can be used directly to augment training data or filtered post-hoc for analysis.\\n\\nWhile our method requires no supervised inputs besides the original task training data, it is able to generate highly diverse counterfactuals covering a range of semantic phenomena (\u00a74), including many transformation types which existing methods generate through heuristics (Dua et al., 2021), meaning representations (Ross et al., 2021b; Geva et al., 2021) or human generation (Bartolo et al., 2020; Gardner et al., 2020). Compared to alternative sources of synthetic data (\u00a75.1), training augmented with RGF data improves performance on a variety of settings (\u00a75.2, \u00a75.3), including out-of-domain (Fisch et al., 2019) and contrast evaluation sets (Bartolo et al., 2020; Gardner et al., 2020), while maintaining in-domain accuracy. Additionally, we introduce a measure of pairwise consistency, and show that RGF significantly improves robustness to a range of local perturbations (\u00a76).\\n\\n2 Related Work\\n\\n2.1 Counterfactual Generation\\n\\nThere has been considerable interest in developing challenge sets for NLU that evaluate models on a wide variety of counterfactual scenarios. Gardner et al. (2020); Khashabi et al. (2020); Kaushik et al. (2020); Ribeiro et al. (2020) use humans to create these perturbations, optionally in an adversarial setting against a particular model (Bartolo et al., 2020). However, these methods can be expensive and difficult to scale.\\n\\nThis has led to an increased interest in creating automatic counterfactual data for evaluating out-of-distribution generalization (Bowman and Dahl, 2021) and for counterfactual data augmentation (Geva et al., 2021; Longpre et al., 2021). Some work focuses on using heuristics like swapping superlatives and nouns (Dua et al., 2021), changing gendered words (Webster et al., 2020), or targeting specific data splits (Finegan-Dollak and Verma, 2020). More recent work has focused on using meaning representation frameworks and structured control codes (Wu et al., 2021a), including grammar formalisms (Li et al., 2020), semantic role labeling (Ross et al., 2021b), structured image representations like scene graphs (Bitton et al., 2021), and query decompositions in multi-hop reasoning datasets (Geva et al., 2021). Ye et al. (2021) and Longpre et al. (2021) perturb contexts instead of questions by swapping out all mentions of a named entity. The change in label can be derived heuristically or requires a round of human re-labeling of the data. These may also be difficult to apply to tasks like Natural Questions (Kwiatkowski et al., 2019), where pre-defined schemas can have difficulty covering the range of semantic perturbations that may be of interest.\\n\\n2.2 Data Augmentation\\n\\nNon-counterfactual data augmentation methods for QA, where the synthetic examples are not paired with the original data, have shown only weak improvements to robustness and out-of-domain generalization (Bartolo et al., 2021; Lewis et al., 2021). Counterfactual data augmentation is hypothesized to perform better, as exposing the model to mini-pairs should reduce spurious correlations and make the model more likely to learn the correct, causal features (Kaushik et al., 2020). However, Joshi and He (2021) find that methods that limit the structural and semantic space of perturbations can potentially hurt generalization to other types of transformations. This problem is exacerbated in the question answering scenario where there can be multiple semantic dimensions to edit. Our method attempts to address this by targeting a broad range...\"}"}
{"id": "acl-2022-long-117", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of semantic phenomena, thus reducing the chance\\nfor the augmented model to overfit.\\n\\n3 RGF: Counterfactuals for Information-seeking Queries\\n\\nWe define a counterfactual example as an alternative input $x'$ which differs in some meaningful, controlled way from the original $x$, which in turn allows us to reason \u2013 or teach the model \u2013 about changes in the label (the outcome). For question-answering, we take as input triples $(q,c,a)$ consisting of the question, context passage, and short answer, and produce counterfactual triples $(q',c',a')$ where $a' \\\\neq a$. This setting poses some unique challenges, such as the need for background knowledge to identify relevant semantic variables to alter, ensuring sufficient semantic diversity in question edits, and avoiding questions with false premises or no viable answers. Ensuring (or characterizing) minimality can also be a challenge, as small changes to surface form can lead to significant semantic changes, and vice-versa. We introduce a general paradigm for data generation \u2013 Retrieve, Generate and Filter \u2013 to tackle these challenges.\\n\\n3.1 Overview of RGF\\n\\nAn outline of the RGF method is given in Figure 1. Given an input example $x = (q,c,a)$ consisting of a question, a context paragraph, and the corresponding answer, RGF generates a set of new examples $N(x) = \\\\{ (q'_1,c'_1,a'_1), (q'_2,c'_2,a'_2),... \\\\}$ from the local neighborhood around $x$. We first use an open-domain retrieve-and-read model to retrieve alternate contexts $c'$ and answers $a'$ where $a' \\\\neq a$. As near-misses for a task model, these candidates $(c',a')$ are closely related to the original target $(c,a)$ but often differ along interesting, latent semantic dimensions (Figure 2) in their relation to the original question, context, and answer. We then use a sequence-to-sequence model to generate new questions $q'$ from the context and answer candidates $(c',a')$. This yields triples $(q',c',a')$ which are fully labeled, avoiding the problem of unanswerable or false-premise questions.\\n\\nCompared to methods that rely on a curated set of minimal edits (e.g. Wu et al., 2021b; Ross et al., 2021b), our method admits the use of alternative contexts $c' \\\\neq c$, and we do not explicitly constrain our triples to be minimal perturbations during the generation step. Instead, we use post-hoc filtering to reduce noise, select minimal candidates, or select for specific semantic phenomena based on the relation between $q$ and $q'$. This allows us to explore a significantly more diverse set of counterfactual questions $q'(\\\\S C.1)$, capturing relations that may not be represented in the original context $c$.\\n\\nWe describe each component of RGF below; additional implementation details are provided in Appendix A.\\n\\n3.2 Retrieval\\n\\nWe use REALM retrieve-and-read model of (Guu et al., 2020). REALM consists of a BERT-based bi-encoder for dense retrieval, a dense index of Wikipedia passages, and a BERT-based answer-span extraction model for reading comprehension, all fine-tuned on Natural Questions (NQ; Kwiatkowski et al., 2019). Given a question $q$, REALM outputs a ranked list of contexts and answers within those contexts: \\\\{ $(c'_1,a'_1), (c'_2,a'_2),..., (c'_k,a'_k)$ \\\\}. These alternate contexts and answers provide relevant yet diverse background information to construct counterfactual questions. For instance, in Figure 1, the question \\\"Who is the captain of the Richmond Football Club\\\" with answer \\\"Trent Cotchin\\\" also returns other contexts with alternate answers like \\\"Jeff Hogg\\\" ($q' = \\\"Who captained the team in 1994\\\") and \\\"Steve Morris\\\" ($q' = \\\"Who captained the reserve team in the VFL league\\\")). Retrieved contexts can also capture information about closely related or ambiguous entities. For instance, the question \\\"who wrote the treasure of the sierra madre\\\" retrieves passages about the original book Sierra Madre, its movie adaptation, and a battle fought in the Sierra de las Cruces mountains. This background knowledge allows us to perform contextualized counterfactual generation, without needing to specify a priori the type of perturbation or semantic dimension. To focus on label-transforming counterfactuals, we retain all $(c'_i,a'_i)$ where $a'_i$ does not match any of the gold answers $a$ from the original NQ example.\\n\\n3.3 Question Generation\\n\\nThis component generates questions $q'$ that correspond to the answer-context pairs $(c',a')$. We use a T5 (Raffel et al., 2020) model fine-tuned...\"}"}
{"id": "acl-2022-long-117", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"on \\\\((q,c,a)\\\\) triples from Natural Questions, using context passages as input with the answer marked with special tokens. We use the trained model to generate questions \\\\((q'_1, q'_2, \\\\ldots, q'_k)\\\\) for each of the retrieved set of alternate contexts and answers, \\\\(((c'_1, a'_1), (c'_2, a'_2), \\\\ldots, (c'_k, a'_k))\\\\). For each \\\\((c'_i, a'_i)\\\\), we use beam decoding to generate 15 different questions \\\\(q'_i\\\\). We measure the fluency and correctness of generated questions in \u00a74.\\n\\n3.4 Filtering for Data Augmentation\\n\\nNoise Filtering\\n\\nThe question generation model can be noisy, resulting in a question that cannot be answered given \\\\(c'_i\\\\) or for which \\\\(a'_i\\\\) is an incorrect answer. Round-trip consistency (Alberti et al., 2019; Fang et al., 2020) uses an existing QA model to answer the generated questions, ensuring that the predicted answer is consistent with the target answer provided to the question generator. We use an ensemble of six T5-based reading-comprehension \\\\((q,c) \\\\rightarrow a\\\\) models, trained on NQ using different random seeds (Appendix A), and keep any generated \\\\((q'_i, c'_i, a'_i)\\\\) triples where at least 5 of the 6 models agree on the answer. This discards about 5% of the generated data, although some noise still remains; see \u00a74 for further discussion.\\n\\nFiltering for Minimality\\n\\nUnlike prior work on generating counterfactual perturbations, we do not explicitly control for the type of semantic shift or perturbation in the generated questions. Instead, we use post-hoc filtering over generated questions \\\\(q'_i\\\\) to encourage minimality of perturbation. We define a filtering function \\\\(f(q, q'_i)\\\\) that categorizes the semantic shift or perturbation in \\\\(q'_i\\\\) with respect to \\\\(q\\\\). One simple version of \\\\(f\\\\) is the word-level edit (Levenshtein) distance between \\\\(q\\\\) and \\\\(q'_i\\\\). After noise filtering, for each original \\\\((q,c,a)\\\\) triple we select the generated \\\\((q'_i, c'_i, a'_i)\\\\) with the smallest non-zero word-edit distance between \\\\(q\\\\) and \\\\(q'_i\\\\) such that \\\\(a \\\\neq a'_i\\\\). We use this simple heuristic to create large-scale counterfactual training data for augmentation experiments (\u00a75). Over-generating potential counterfactuals based on latent dimensions identified in retrieval and using a simple filtering heuristic avoids biasing the model toward a narrow set of perturbation types (Joshi and He, 2021).\\n\\n3.5 Semantic Filtering for Evaluation\\n\\nTo better understand the types of counterfactuals generated by RGF, we can apply additional filters based on question meaning representations to categorize counterfactual \\\\((q, q'_i)\\\\) pairs for evaluation. Meaning representations provide a way to decompose a question into semantic units and categorize \\\\((q, q'_i)\\\\) based on which of these units are perturbed. In this work, we employ the QED formalism for explanations in question answering (Lamm et al., 2021). QED decompositions segment the question into a predicate template and a set of reference phrases. For example, the question \u201cWho is captain of richmond football club\u201d decomposes into one question reference \u201crichmond football club\u201d and the predicate \u201cWho is captain of X\u201d. A few example questions and their QED decompositions are illustrated in Table 1.\\n\\nWe use these question decompositions to identify the relation between a counterfactual pair \\\\((q, q'_i)\\\\). Concretely, we fine-tune a T5-based model on the QED dataset to perform explanation generation following the recipe of Lamm et al. (2021), and use this to identify predicates and references for the question from each \\\\((q,c,a)\\\\) triple. We use exact match between strings to identify reference changes. As predicates can often differ slightly in phrasing (\u201cwho captained\u201d vs. \u201cwho is captain\u201d), we take a predicate match to be a prefix matching with more than 10 characters. For instance, \u201cWho is the captain of Richmond\u2019s first ever women\u2019s team?\u201d and \u201cWho is the captain of the Richmond Football Club\u201d have the same predicates. We filter generated questions into three perturbation categories \u2014 reference change, predicate change, or both.\"}"}
{"id": "acl-2022-long-117", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Who has won the women\u2019s single Wimbledon tennis tournament in 2018?\\n\\nWho won the women\u2019s singles Australian Open?\\n\\nWho won the women\u2019s doubles at Wimbledon 2015?\\n\\nHow many games in Wimbledon final set tie break?\\n\\nWho won the Wimbledon women\u2019s singles title in 2016?\\n\\nWho won the runner-up in the women\u2019s singles at Wimbledon in 2018?\\n\\nWho did Serena Williams best in the Wimbledon finals 2015?\\n\\nwhat\u2019s the population of Walnut Grove, Minnesota?\\n\\nwhat\u2019s the population of Walnut Grove, Washington?\\n\\nwhat is the population of Apple Valley, Minnesota?\\n\\nhow many students at Walnut Grove Secondary School?\\n\\nhow long has the Walnut Twig Beetle been in California?\\n\\nwhere is Walnut Grove located in Minnesota?\\n\\nwhat is the population of Walnut Grove, BC?\\n\\nFigure 2: Context-specific semantic diversity of perturbations achieved by RGF on questions from NQ. The multiple latent semantic dimensions identified (arrows in the diagram) emerge from our retrieval-guided approach.\\n\\nTable 2: Patterns of semantic change between original queries (O) and RGF counterfactuals (C), corresponding to patterns explored by related works. Along three measures: fluency, correctness, and directionality. Fluency measures whether the generated text is grammatically correct and semantically meaningful. Fluency is very high from RGF, as the generation step leverages a high-quality pretrained language model (T5). We manually annotate a subset of 100 generated questions, and find that 96% of these are fluent.\\n\\nCorrectness measures if the generated question $q'$ and context, alternate answer pairs $(c', a')$ are aligned, i.e., the question is answerable given context $c'$ and $a'$ is that answer. We quantify correctness in the generated dataset by manually annotating a samples of 100 $(q', c', a')$ triples (see Appendix B). The proportion of noise varies from 30% before noise filtering and 25% after noise filtering using an ensemble of models ($\u00a73.4$).\\n\\nDirectionality/Semantic Diversity In Table 2, we show examples of semantic changes that occur in our data, including reference changes (50% of changes), predicate changes (30%), negations (1%), question expansions, disambiguations, and contractions (13%). These cover many of the transformations found in prior work (Gardner et al., 2020; Ross et al., 2021b; Min et al., 2020b), but RGF is able to achieve these without the use of heuristic transformations or structured meaning representations. As shown in Figure 2, the types of relations are semantically rich and cover attributes relevant to each particular instance that would be difficult to capture with a globally-specified schema. Additional examples are shown in Figure 6.\"}"}
{"id": "acl-2022-long-117", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Exact Match results for the reading comprehension task for in-domain NQ development set, out-of-domain datasets from MRQA 2019 Challenge (Fisch et al., 2019), Adversarial QA (Bartolo et al., 2020) and AmbigQA (Min et al., 2020b). RGF improves out-of-domain and challenge-set performance compared to other data augmentation baselines.\\n\\n5 Data Augmentation\\n\\nUnlike many counterfactual generation methods, RGF natively creates fully-labeled \\\\((q',c',a')\\\\) examples which can be used directly for counterfactual data augmentation (CDA). We augment the original NQ training set with additional examples from RGF, shuffling all examples in training. We explore two experimental settings, reading comprehension (\u00a75.2) and open-domain QA (\u00a75.3), and compare RGF-augmented models to those trained only on NQ, as well as to alternative baselines for synthetic data generation. As described in Section 3.4, we use edit-distance based filtering to choose one generated \\\\((q',c',a')\\\\) triple to augment for every original example, \\\\((q,c,a)\\\\).\\n\\n3 Additional training details for all models and baselines are included in Appendix A.\\n\\n5.1 Baselines\\n\\nIn the abstract, our model for generating counterfactuals specifies a way of selecting contexts \\\\(c'\\\\) from original questions, and answers \\\\(a'\\\\) within those contexts, and a way of generating questions \\\\(q'\\\\) from them. RGF uses a retrieval model to identify relevant contexts; here we experiment with two baselines that use alternate ways to select \\\\(c'\\\\). We also compare to the ensemble of six reading comprehension models described in 3.4, with answers selected by majority vote.\\n\\nRandom Passage (Rand. Agen-Qgen)\\n\\nHere, \\\\(c'\\\\) is a randomly chosen paragraph from the Wikipedia index, with no explicit relation with the original question. This setting simulates generation from the original data distribution of Natural Questions. To ensure that the random sampling of Wikipedia paragraphs has a similar distribution, we employ the learned passage selection model from Lewis et al. (2021), 4. This baseline corresponds to the model of Bartolo et al. (2021), which was applied to the SQuAD dataset (Rajpurkar et al., 2016); our version is trained on NQ and omits AdversarialQA.\\n\\nGold Context (Gold Agen-Qgen)\\n\\nHere, \\\\(c'\\\\) is the passage \\\\(c\\\\) containing the original short answer \\\\(a\\\\) from the NQ training set. This baseline specifically ablates the retrieval component of RGF, testing whether the use of alternate passages leads to more diversity in the resulting counterfactual questions.\\n\\nAnswer Generation for Baselines\\n\\nFor both the above baselines for context selection, we select spans in the new passage that are likely to be answers for a potential counterfactual question. We use a T5 (Raffel et al., 2020) model fine-tuned for question-independent answer selection \\\\(c\\\\to a\\\\) on NQ, and select the top 15 candidates from beam search. To avoid simply repeating the original question, we only retain answer candidates \\\\(a'\\\\) which do not match the original NQ answers \\\\(a\\\\) for that example. These alternate generated answer candidates and associated passages are then used for question generation and filtering as in RGF (\u00a73.3). For the Gold Agen-Qgen case, we select based on the longest edit distance between \\\\((q,q')\\\\), which gave significantly better performance than random selection or the shortest edit distance used for RGF.\\n\\n5.2 Reading Comprehension (RC)\\n\\nIn the reading comprehension (RC) setting, the input consists of the question and context and the task is to identify an answer span in the context. Thus, we augment training with full triples \\\\((q',c',a')\\\\) consisting of the retrieved passage \\\\(c'\\\\), generated and filtered question \\\\(q'\\\\), and alternate answer \\\\(a'\\\\).\\n\\nExperimental Setting\\n\\nWe finetune a T5 (Raffel et al., 2020) model for reading comprehension, 4https://github.com/facebookresearch/PAQ.\"}"}
{"id": "acl-2022-long-117", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with input consisting of the question prepended to the context. We evaluate domain generalisation of our RC models on three evaluation sets from the MRQA 2019 Challenge (Fisch et al., 2019). We also measure performance on evaluation sets consisting of counterfactual or perturbed versions of RC datasets on Wikipedia, including SQuAD (Rajpurkar et al., 2016), AQA (adversarially-generated SQuAD questions; Bartolo et al., 2020), and human authored counterfactual examples (contrast sets; Gardner et al., 2020) from the QUOREF dataset (Dasigi et al., 2019). We also evaluate on the set of disambiguated queries in AmbigQA (Min et al., 2020b), which by construction are minimal edits to queries from the original NQ.\\n\\nResults\\nWe report exact-match scores in Table 3; F1 scores follow a similar trend. We observe only limited improvements on the in-domain NQ development set, but we see significant improvements from CDA with RGF data in out-of-domain and challenge-set evaluations compared both to the original NQ model and the Gold and Random baselines. RGF improves by 1-2 EM points on most challenge sets, and up to 7 EM points on the BioASQ set compared to training on NQ only, while baselines often underperform the NQ-only model on these sets. Note that all three augmentation methods have similar proportion of noise (Appendix B), so CDA's benefits may be attributed to improving model's ability to learn more robust features for the task of reading comprehension. Using an ensemble of RC models improves slightly on some tasks, but does not improve on OOD performance as much as RGF. RGF's superior performance compared to the Gold Agen-Qgen baseline is especially interesting, since the latter also generates topically related questions. We observe that RGF counterfactuals are more closely related to the original question compared to this baseline (Figure 5 in Appendix C), since $q'$ is derived from a near-miss candidate $(c',a')$ to answer the original $q$ (S3.1).\\n\\n5.3 Open-Domain Question Answering (OD)\\nIn the open-domain (OD) setting, only the question is provided as input. The pair $(q',a')$, consisting of generated and filtered question $q'$ and alternate answer $a'$, is used for augmentation. Compared to the RC setting where passages change as well, here the edit distance filtering of \u00a73.4 ensures the augmentation data represents minimal perturbations.\\n\\nExperimental Setting\\nWe use the method and implementation from Guu et al. (2020) to finetune REALM on $(q,a)$ pairs from NQ. End-to-end training of REALM updates both the reader model and the query-document encoders of the retriever module. We evaluate domain generalization on popular open-domain benchmarks: TriviaQA (Joshi et al., 2017), SQuAD (Rajpurkar et al., 2016), Catured TREC dataset (Min et al., 2021), and disambiguated queries from AmbigQA (Min et al., 2020b).\\n\\nResults\\nIn the open-domain setting (Table 4), we observe an improvement of 2 EM points over the original model even in-domain on Natural Questions, while also improving significantly when compared to other data augmentation techniques. RGF improves over the next best baseline \u2014 Random Agen-Qgen \u2014 by up to 6 EM points (on TriviaQA). We hypothesize that data augmentation has more benefit in this setting, as the open-domain task is more difficult than reading comprehension, and counterfactual queries may help the model learn better query and document representations to improve retrieval.\\n\\n6 Analysis\\nTo better understand how CDA improves the model, we introduce a measure of local consistency (\u00a76.1) to measure model robustness, and perform a stratified analysis (\u00a76.2) to show the benefits of the semantic diversity available from RGF.\\n\\n6.1 Local Robustness\\nCompared to synthetic data methods such as PAQ (Lewis et al., 2021), RGF generates counterfactual examples that are paired with the original inputs and concentrated in local neighborhoods around them (Figure 2). As such, we hypothesize that augmentation with this data should specifically improve local consistency, i.e. how the model behaves under small perturbations of the input.\\n\\nExperimental Setting\\nWe explicitly measure how well a model's local behavior respects perturbations to input. Specifically, if a model $f:(q,c) \\\\to a$ correctly answers $q$, how often does it also correctly answer $q'$? We define pairwise consistency as accuracy over the counterfactuals $(q',a',c')$, conditioned on correct predictions for the original examples:\\n\\n$$C(D) = \\\\mathbb{E}_D[f(q',c') = a' | f(q,c) = a]$$\"}"}
{"id": "acl-2022-long-117", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Exact Match results on open-domain QA datasets (TriviaQA, AmbigQA, SQuAD and TREC) for data augmentation with RGF counterfactuals and baselines. Open-domain improvements are larger than in the RC setting, perhaps as the more difficult task benefits more from additional data.\\n\\n| Dataset        | Train Size | TriviaQA | AmbigQA | SQuAD v1.0 | TREC |\\n|----------------|------------|----------|---------|-------------|------|\\n| Original NQ    | 90K        | 37.65    | 26.75   | 22.43       | 14.25|\\n| Gold Agen-Qgen | 90K + 90K  | 37.86    | 27.02   | 23.65       | 15.01|\\n| Rand. Agen-Qgen| 90K + 90K  | 37.45    | 29.87   | 24.13       | 14.55|\\n| RGF (REALM-Qgen)| 90K + 90K| 39.11    | 32.32   | 26.98       | 16.94|\\n\\nTable 5: Results for pairwise consistency (\u00a76.1) on reading comprehension, measured for datasets containing pairs of very similar questions. QUOREF-C refers to the QUOREF contrast set from (Gardner et al., 2020). RGF leads to better consistency in RC and open-domain settings (Appendix C.2). Results on effect of perturbation type during training (\u0394Ref. and \u0394Pred.) suggest that perturbation-bias does not degrade consistency over the original model.\\n\\nTo measure consistency, we construct validation sets consisting of paired examples \\\\((q, c, a), (q', c', a')\\\\): one original, and one counterfactual. We use QED to categorize our data, as described in \u00a73.5. Specifically, we create two types of pairs: (a) a change in reference where question predicate remains fixed, and (b) a change in predicate, where the original reference(s) are preserved. We create a clean evaluation set by first selecting RGF examples for predicate or reference change, then manually filtering the data to discard incorrect triples (\u00a74) until we have 1000 evaluation pairs of each type (see Appendix B).\\n\\nWe also construct paired versions of AQA, AmbigQA, and the QUOREF contrast set. For AmbigQA, we pair two disambiguated questions and for QUOREF, we pair original and human-authored counterfactuals. AQA consists of human-authored adversarial questions \\\\(q'\\\\) which are not explicitly paired with original questions; we create pairs by randomly selecting an original question \\\\(q\\\\) and a generated question \\\\(q'\\\\) from the same passage.\\n\\n5 We require that the new reference set is a superset of the original, since predicate changes can introduce additional reference slots (see CF2 in Table 1).\\n\\nResults Training with RGF data improves consistency by 12-14 points on the QED-filtered slices of RGF data, and 5-7 points on AQA, AmbigQA and QUOREF contrast (Table 5). The Gold Agen-Qgen baseline (which contains topically related queries about the same passage) also improves consistency over the original model compared to the Random Agen-Qgen baseline or to the ensemble model, though not by as much as RGF. Consistency improvements on AQA, AmbigQA and QUOREF are especially noteworthy, since they suggest an improvement in robustness to local perturbations that is independent of other confounding distributional similarities between training and evaluation data.\\n\\n6.2 Effect of Perturbation Type QED-based decomposition of queries allows for the creation of label-changing counterfactuals along orthogonal dimensions \u2014 a change of reference or predicate. We investigate whether training towards one type of change induces generalization bias, a detrimental effect which has been observed in tasks such as NLI (Joshi and He, 2021).\\n\\nExperimental Setting We shard training examples into two categories based on whether \\\\(q\\\\) and \\\\(q'\\\\) are compatible. This is done by checking if the original question contains all the information needed to answer the generated question, and if so, \\\\(q\\\\) and \\\\(q'\\\\) are considered compatible (or equal). If the generated question is more informative, \\\\(q\\\\) and \\\\(q'\\\\) are considered incompatible (or different).\"}"}
{"id": "acl-2022-long-117", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"have the same reference (predicate change) or same predicate (reference change), as defined in \u00a73.5. We over-generate by starting with $(q', c', a')$ for each original training example to ensure that we find at least one $q'$ that matches the criterion. We also evaluate on paired evaluation sets from \u00a76.1.\\n\\nResults\\n\\nResults are shown for QED-filtered training in Table 5. Counterfactual perturbation of a specific kind (a predicate or a reference change) during augmentation does not hurt performance on another perturbation type compared to the baseline NQ model, which differs from the observations of Joshi and He (2021) on NLI. Furthermore, similar to the observations of Min et al. (2020a), augmenting with one type of perturbation has orthogonal benefits that improve model generalization on another perturbation type: augmenting with RGF ($\\\\Delta_{\\\\text{Pred.}}$) leads to significant improvement on RGF ($\\\\Delta_{\\\\text{Ref.}}$), and vice-versa.\\n\\nCompared to reference-change examples, augmenting with predicate-change examples leads to greater improvements in local consistency, except for on RGF ($\\\\Delta_{\\\\text{Ref.}}$) and on AmbigQA \u2013 which contains many reference-change pairs. Predicate-change examples may also be more informative to the model, as reference changes can be modeled more easily by lexical matching within common context patterns.\\n\\n6.3 Effect of Training data size\\n\\nJoshi and He (2021) show CDA to be most effective in the low-resource regime. To better understand the role that dataset size plays in CDA in the reading comprehension setting, we evaluate RGF in a cross-domain setting where only a small amount of training data is available.\\n\\nExperimental Setting\\n\\nSince our approach depends on using an open-domain QA model and a question generation model trained on all Natural Questions data, we instead experiment with a low-resource transfer setting on the BioASQ domain, which consists of questions on the biomedical domain. We use the domain-targeted retrieval model from Ma et al. (2021), where synthetic question-passage relevance pairs generated over the PubMed corpus are used to train domain-specific retrieval without any gold supervised data. We fine-tune our question-generation model on (limited) in-domain data, generate RGF data for augmentation, and then use this along with (limited) in-domain data to further fine-tune an RC model, using the NQ-trained weights for initialization. Further training details are provided in Appendix A.\\n\\n| Training Data | Train Size | BioASQ (Dev) |\\n|---------------|------------|--------------|\\n| Original      | 1000       | 42.93 23.67  |\\n| Orig. + RGF   | 500 + 500  | 41.72 23.01  |\\n| Original      | 2000       | 45.88 25.80  |\\n| Orig. + RGF   | 1000 + 1000| 44.64 26.80  |\\n\\nTable 6: Results on the reading comprehension task for Low Resource Transfer setting on BioASQ 2019 dataset. A model trained on 1000 gold BioASQ plus 1000 RGF examples performs nearly as well as a model trained on 2000 gold examples.\\n\\nResults\\n\\nWe observe significant improvements over the baseline model in the low resource setting for in-domain data (< 2000 examples), as shown in Table 6. Compared with the limited gains we see on the relatively high-resource NQ reading comprehension task, we find that on BioASQ, CDA with 1000 examples improves performance by 2% F1 and 3% exact match, performing nearly as well as a model trained on 2000 gold examples. These results suggest that using counterfactual data in lieu of collecting additional training data is especially useful in the low-resource setting.\\n\\n7 Conclusion\\n\\nRetrieve-Generate-Filter (RGF) creates counterfactual examples for QA which are semantically diverse, using knowledge from the passage context and a retrieval model to capture semantic changes that would be difficult to specify a priori with a global schema. The resulting examples are fully-labeled, and can be used directly for training or filtered using meaning representations for analysis. We show that training with this data leads to improvements on open-domain QA, as well as on challenge sets, and leads to significant improvements in local robustness. While we focus on question answering, for which retrieval components are readily available, we note that the RGF paradigm is quite general and could potentially be applied to other tasks with a suitable retrieval system.\\n\\nReferences\\n\\nChris Alberti, Daniel Andor, Emily Pitler, Jacob Devlin, and Michael Collins. 2019. Synthetic QA corpus.\"}"}
{"id": "acl-2022-long-117", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-117", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nitish Joshi and He He. 2021. An investigation of the (in) effectiveness of counterfactually augmented data. arXiv preprint arXiv:2107.00753.\\n\\nDivyansh Kaushik, Eduard Hovy, and Zachary Lipton. 2020. Learning the difference that makes a difference with counterfactually-augmented data. International Conference on Learning Representations.\\n\\nDivyansh Kaushik, Douwe Kiela, Zachary C. Lipton, and Wen-tau Yih. 2021. On the efficacy of adversarial data collection for question answering: Results from a large-scale randomized study. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6618\u20136633, Online. Association for Computational Linguistics.\\n\\nDaniel Khashabi, Tushar Khot, and Ashish Sabharwal. 2020. More bang for your buck: Natural perturbation for robust question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 163\u2013170, Online. Association for Computational Linguistics.\\n\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466.\\n\\nMatthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, and Michael Collins. 2021. QED: A Framework and Dataset for Explanations in Question Answering. Transactions of the Association for Computational Linguistics, 9:790\u2013806.\\n\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096, Florence, Italy. Association for Computational Linguistics.\\n\\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich K\u00fcttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. PAQ: 65 million probably-asked questions and what you can do with them. Transactions of the Association for Computational Linguistics, 9:1098\u20131115.\\n\\nChuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu, Xuhui Zhou, and Shane Steinert-Threlkeld. 2020. Linguistically-informed transformations (LIT): A method for automatically generating contrast sets. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 126\u2013135, Online. Association for Computational Linguistics.\\n\\nShayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7052\u20137063, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nSasha Luccioni, Victor Schmidt, Alexandre Lacoste, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. In NeurIPS 2019 Workshop on Tackling Climate Change with Machine Learning.\\n\\nJi Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot neural passage retrieval via domain-targeted synthetic question generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1075\u20131088.\\n\\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428\u20133448, Florence, Italy. Association for Computational Linguistics.\\n\\nSimon Mille, Thiago Castro Ferreira, Anya Belz, and Brian Davis. 2021. Another PASS: A reproduction study of the human evaluation of a football report generation system. In Proceedings of the 14th International Conference on Natural Language Generation, pages 286\u2013292, Aberdeen, Scotland, UK. Association for Computational Linguistics.\\n\\nTim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267:1\u201338.\\n\\nJunghyun Min, R Thomas McCoy, Dipanjan Das, Emily Pitler, and Tal Linzen. 2020a. Syntactic data augmentation increases robustness to inference heuristics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2339\u20132352.\\n\\nSewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. 2021. Joint passage ranking for diverse multi-answer retrieval. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6997\u20137008, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020b. AmbigQA: Answering ambiguous open-domain questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783\u20135797, Online. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-117", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-117", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Below, we describe the details of different models trained in the RGF pipeline. Unless specified otherwise, we use the T5X library and pre-trained checkpoints from Raffel et al. (2020).\\n\\n**Question Generation**\\nWe use a T5-3B model fine-tuned on Natural Questions (NQ) dataset. We only train on the portion of the dataset that consists of gold short answers and an accompanying long answer evidence paragraph from Wikipedia. The input consists of the title of the Wikipedia article the passage is taken from, a separator ('\u00bb') and the passage. The short answer is enclosed in the passage using character sequences '\u00ab answer =' and '\u00bb' on left and right respectively. The output is the original NQ question. The input and output sequence lengths are restricted to be 640 and 256 respectively. We train the model for 20k steps with a learning rate of $2 \\\\cdot 10^{-5}$, dropout 0.1, and batch size of 128. We decode with a beam size of 15, and take the top candidate as our generated question $q'$. \\n\\n**Answer Generation**\\nWe use a T5-3B model trained on the same subset of Natural Questions (NQ) as question generation with same set of hyper-parameters and model size described above. The input consists of the title of the Wikipedia article the passage is taken from, a separator ('\u00bb') and the passage, while the output sequence is the short answer from NQ.\\n\\n**Reading Comprehension Model**\\nWe model the task of span selection-based reading comprehension, i.e. identifying an answer span given question and passage, as a sequence-to-sequence problem. Input consists of the question, separator ('\u00bb'), and title of Wikipedia article, separator ('\u00bb') and passage. The answer format is simply one of the gold answer strings. The reading comprehension model is a T5-large model trained with batch size of 512 and learning rate $2 \\\\cdot 10^{-4}$ for 20K steps.\\n\\n**Open-domain Question Answering model**\\nThe open domain QA model is based on the implementation from Lee et al. (2019), and initialized with the REALM checkpoint from Guu et al. (2020). Both the retriever and reader are initialized from the BERT-base-uncased model. The query and document representations are 128 dimensional vectors. When finetuning, we use a learning rate of $10^{-5}$ and a batch size of 1 on a single Nvidia V100 GPU. We perform 2 epochs of fine-tuning for Natural Questions.\\n\\n**Noise Filtering**\\nWe train 6 reading comprehension models based on the configurations above with different seed values for randomizing training dataset shuffling and optimizer initialization. We retain examples where more than 5 out of 6 models have the same answer for a question.\\n\\n**QED Training**\\nWe use a T5-large model fine-tuned on the Natural Questions subset with QED annotations (Lamm et al., 2021). We refer the reader to the QED paper for details on the linearization of explanations and inputs in the T5 model. Our model is fine-tuned with batch size of 512 and learning rate $2 \\\\cdot 10^{-4}$ for 20k steps.\\n\\n**Experimental Variability**\\nUnless otherwise stated, results are reported from single runs. However, we used the six RC models discussed in Section 3.4 to estimate cross-run variation. Using the procedure and code of Sellam et al. (2021), we find variation of about 0.5 points (F1). As such, we do not find differences smaller than this significant, and in our results focus only on larger effects.\\n\\n**Computational Budget and Environmental Impact**\\nWe fine-tune all T5 models on Cloud TPU v3 hardware; each takes approximately 4 hours on 16 TPUs in pod configuration. Total compute time is approximately 96 TPU-hours and 192 GPU-hours, which we estimate as 43 kg CO2e using the method of Luccioni et al. (2019).\\n\\n**Evaluation of Fluency and Noise**\\nThe authors sampled 300 examples of generated questions. To annotate for fluency, authors use the following rubric: Is the generated question grammatically well-formed barring non-standard spelling and capitalization of named entities. This noise annotation was done for RGF, as well as Gold Agen-Qgen and Random Agen-Qgen.\\n\\n---\\n\\n8https://github.com/google-research/t5x\\n9https://github.com/google-research-datasets/QED\\n10https://mlco2.github.io/impact/#co2eq\"}"}
{"id": "acl-2022-long-117", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Once again, authors annotate for correctness of counterfactual RGF instances that are paired by reference or predicate, as described in \u00a73.5. Filtering is done until 1000 examples are available under each category.\\n\\n| Data       | Unfiltered | Filtered |\\n|------------|------------|----------|\\n| RGF        | 29.8%      | 25.3%    |\\n| Gold Agen-Qgen | 27.9%    | 20.7%    |\\n| Random Agen-Qgen | 30.7%    | 28.3%    |\\n\\nTable 7: Fraction of noise \\\\((q', c', a')\\\\) in generated data, from 300 examples manually annotated by the authors.\\n\\n### C Additional Experiments\\n\\n#### C.1 Intrinsic Evaluation\\n\\nFigure 3: Distribution of edit distance between original \\\\(q\\\\) and counterfactual \\\\(q'\\\\) for RGF and other baselines for context selection. Note: For Random Wiki Passages, original and generated questions bear no relation to each other and are randomly paired.\\n\\nIn Figure 3, we compare distributions of the edit distance between the original and generated questions for questions generated by our approach, those generated with the gold evidence passage (Gold Agen-Qgen baseline), and those generated from a random Wikipedia passage (\u00a75) (Random Agen-Qgen baseline). We find that RGF counterfactuals undergo minimal perturbations from the original question compared to questions that are generated from random Wikipedia paragraph. This pattern also holds when compared to questions generated from gold NQ passages. We hypothesize that the set of alternate answers retrieved in our pipeline approach are semantically similar to the gold answer \u2014 same entity type, for instance. Random answer spans chosen from the gold NQ passage can result in significant semantic shifts in generated questions.\\n\\nFigure 4: Plot of average edit distance between \\\\(q, q'\\\\) vs. retrieval rank \\\\(r\\\\), where \\\\(q'\\\\) is generated from \\\\(r\\\\)th passage, showing that edit distance and retrieval rank are monotonically related.\\n\\nIn Figure 4, we measure the relation between retrieval rank and edit-distance for RGF. For retrieval rank \\\\(i\\\\), we plot average edit distance between the original question and counterfactual question that was generated using the \\\\(i\\\\)th passage and answer. We observe a monotonic relation between retrieval rank and edit distance (which we use for filtering our training data). We also measure changes in the distribution of question type and predicate type between original NQ data and the generated RGF data.\\n\\nFigure 5: Distribution of top 20 question types for original NQ data, RGF counterfactuals and questions generated from random Wikipedia passage, indicating bias towards popular question types.\\n\\nFigure 5 indicates that counterfactual data exacerbates question-type bias. However, this bias...\"}"}
{"id": "acl-2022-long-117", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Consistency Results for Open-domain QA.\\n\\n| Model          | Train Size | NQ  | SQuAD | TriviaQA | HotpotQA | BioASQ | AQA  | AmbigQA |\\n|----------------|------------|-----|-------|----------|----------|--------|------|---------|\\n| Original NQ    | 90K        | 16.58 | 13.33 | 25.12    | 11.23    |        |      |         |\\n| Random Agen-Qgen | 90K + 90K | 15.80 | 20.00 | 27.94    | 17.16    |        |      |         |\\n| RGF (REALM-Qgen) | 90K + 90K | 17.66 | 28.57 | 31.77    | 19.81    |        |      |         |\\n\\nTable 9: Reading comprehension results with varying training data augmentation sizes (exact match). We do not observe a consistent improvement with additional data. This series of experiments was run using an older version of T5X, so are not exactly comparable to Table 3.\\n\\nC.2 Consistency for Open-Domain QA\\n\\nIn Table 8, we show results on evaluating consistency on paired datasets in the open-domain results, similar to the results shown in \u00a76.1 in the Reading Comprehension setting.\\n\\nC.3 Augmentation with more data\\n\\nIn Table 9, we show results on augmenting with more than one RGF counterfactual triple \\\\((q', a', c')\\\\) for every original example \\\\((q, a, c)\\\\) in NQ. These experiments were run on an older version of T5X, so RGF (1X) values are reported differently from Table 3. We observe that adding more RGF data (3X or more) for augmentation can hurt performance. This may be because of increase in the proportion of noisy to clean examples during training and exacerbation of biases in the question generation model (explored in 5), resulting in diminishing returns. These challenges also occur in the baselines, and may be inherent to augmentation with generated data.\\n\\nC.4 Effect of perturbation type\\n\\nExperimental Setting\\n\\nFor edit distance-based experiments, we shard training examples into three categories by binning word-level edit distance between \\\\(q\\\\) and \\\\(q'\\\\) into three ranges: 1\u20134, 5\u201310, and >10. We similarly categorize RGF data generated for the NQ development set into the same categories. Evaluation sets for edit-distance experiments based were not manually noise filtered. We again report consistency on the reading comprehension model.\\n\\nTable 10: Results on sharding training data based on edit distance between \\\\((q, q')\\\\). Training dataset size for each bin is 90k NQ + 167k generated. Once again, training with all RGF data robustly improves consistency across different amounts of perturbations.\\n\\nResults\\n\\nSimilar to the observations for dataset sharding along QED annotations, when data is sharded by edit distance, we observe that using the full RGF data nearly matches the best performance from training on that shard, suggesting that CDA with the highly diverse RGF data can lead to improved consistency on a broad range of perturbation types.\"}"}
{"id": "acl-2022-long-117", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6 includes more examples from Natural Questions, showing the counterfactual questions generated for different input questions by RGF.\\n\\nwho had the most home runs in june of 2008?\\n\\nwho has the most hits in mets history?\\n\\nwho is the highest paid centerfielder in 2008?\\n\\nwho has the most home runs in one season?\\n\\nwho has the most home runs in dodgers history?\\n\\nwho was the last pirate to hit 20 home runs and steal 20 bases in one season?\\n\\nwho caught the largest alligator gar in the wild?\\n\\nwho is the law making body in india?\\n\\nwho gave the recommendation for the appointment of prime minister?\\n\\nwho meets in the parliament building in india?\\n\\nwho gave the recommendation for the appointment of prime minister?\\n\\nwhich is the upper law making body in india?\\n\\nwho is the law making body in pakistan?\\n\\nwho presides over the joint session of parliament?\\n\\nwho is considered the founder of mother's day?\\n\\nwhat event was honored at the first recorded mother's day in the united states?\\n\\nwhen did father's day start in the u.s.\\n\\nwhen was mother's day first celebrated in czech republic?\\n\\nFigure 6: Context-specific semantic diversity of perturbations achieved by RGF on questions from NQ. The multiple latent semantic dimensions identified (arrows in the diagram) fall out of our retrieval-guided approach.\"}"}
