{"id": "lrec-2024-main-24", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Corpus and Method for Chinese Named Entity Recognition in Manufacturing\\n\\nRuiting Li\u00b9\u00b2, Peiyan Wang\u00b9\u00b2\u2020, Libang Wang\u00b9\u00b2, Danqingxin Yang\u00b9\u00b2, Dongfeng Cai\u00b9\u00b2\\n\\n\u00b9 School of Computer Science, Shenyang Aerospace University, Shenyang, China\\n\u00b2 Liaoning Professional Technology Innovation Center on Knowledge Engineering and Human-Computer Interaction, Shenyang, China\\nliruiting@stu.sau.edu.cn, wangpy@sau.edu.cn\\n{wanglibang, yangdanqingxin}@stu.sau.edu.cn, caidf@vip.163.com\\n\\nAbstract\\n\\nManufacturing specifications are documents entailing different techniques, processes, and components involved in manufacturing. There is a growing demand for named entity recognition (NER) resources and techniques for manufacturing-specific named entities, with the development of smart manufacturing. In this paper, we introduce a corpus of Chinese manufacturing specifications, named MS-NERC, including 4,424 sentences and 16,383 entities. We also propose an entity recognizer named Trainable State Transducer (TST), which is initialized with a finite state transducer describing the morphological patterns of entities. It can directly recognize entities based on prior morphological knowledge without training. Experimental results show that TST achieves an overall 82.05% F1 score for morphological-specific entities in zero-shot. TST can be improved through training, the result of which outperforms neural methods in few-shot and rich-resource. We believe that our corpus and model will be valuable resources for NER research not only in manufacturing but also in other low-resource domains.\\n\\nKeywords: corpus, named entity recognition, information extraction\\n\\n1. Introduction\\n\\nThe application of natural language processing (NLP) in manufacturing has propelled the advancement of smart manufacturing, wherein textual data constitutes a pivotal component. Manufacturing specifications are documents entailing different techniques, processes, and components involved in manufacturing. The accurate recognition of named entities in the manufacturing specifications lay the foundation for downstream tasks such as knowledge graph construction (Buchgeher et al., 2021) and relation extraction (Wang et al., 2020).\\n\\nHowever, unsimilar to domains such as financial (Wu et al., 2020), medical (Tian et al., 2022), and computer code (Tabassum et al., 2020), there is a lack of resources and techniques for recognizing manufacturing-specific named entities (e.g., PART or PART_ID) in manufacturing.\\n\\nThere is no publicly available corpus for Chinese NER in the manufacturing specifications. Corpus annotation demands annotators with a strong background in domain-specific knowledge. The high cost of manual annotation leads to limited corpus. To achieve good performance on NER, conventional neural methods normally rely on a large amount of labeled training data, which is unavailable in manufacturing. Our approach to address the issue is to regard entities' morphological patterns as domain-specific knowledge, which can be used for modeling an entity recognizer.\\n\\nFigure 1: Examples of the sentence with named entities and translations in the manufacturing specifications.\"}"}
{"id": "lrec-2024-main-24", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Annotation guidelines for NER corpus in manufacturing, including annotation instructions and definitions for 16 categories such as PART, PART_ID, FILE, and FIGURE_NOTE. Our guidelines provide a reference for the annotation work in the future.\\n\\n\u2022 A Chinese NER corpus named MS-NERC with 4,424 sentences and 16,383 entities. The MS-NERC is manually annotated by three experts in manufacturing.\\n\\n\u2022 An entity recognizer named Trainable States Transducer (TST) incorporating prior morphological knowledge. The model is initialized with a finite state transducer describing the morphological patterns of entities. It is capable of recognizing related entities based on prior morphological knowledge without training. Besides, TST can be improved through supervised learning.\\n\\nOwing to prior morphological knowledge, TST can directly recognize related entities without training. In zero-shot, TST attains an F1 score of 82.05%. In both few-shot and rich-resource, TST consistently outperforms neural methods as shown in \u00a75.3.\\n\\n2. Related Work\\n\\n2.1. NER in Specific Domains\\n\\nWith the application of NER in specific domains, there is a growing interest in both corpus and techniques.\\n\\nCorpus Annotation\\n\\nThe construction of corpus is gradually attracting attention such as news (Mbu-vha et al., 2023), social media (Jiang et al., 2022), and computer code (Tabassum et al., 2020). Meanwhile, little attention is paid to manufacturing documents (Chen et al., 2021; Zhang et al., 2019; Jia et al., 2022). For example, Chen et al. (2021) define 7 categories of entities and annotate 1,139 sentences for assembly manufacturing documents. However, there is no corpus for manufacturing specifications. In our study, we annotate manufacturing specifications offering broader coverage than the existing corpus in manufacturing. We define 16 fine-grained categories and annotate a total of 4,424 sentences, significantly surpassing other corpus.\\n\\nModels Enhanced by Domain Knowledge\\n\\nAdding domain knowledge to neural networks can be effective with insufficient data. One useful way is to apply dictionaries and rules to refine the results obtained from neural networks in chemistry (Ma et al., 2018), military (Feng et al., 2015) and Uyghur language (Zhu, 2019). However, it is invalid when the entities cannot be described by dictionaries and rules. Jia et al. (2022) used domain dictionaries and entity rules for pre-recognizing entities and use pre-recognization features to guide the model training.\\n\\n2.2. Finite State Transducer in NLP\\n\\nThe majority of previous work combines a finite state transducer (FST) with neural networks. For morphological generation tasks, Rastogi et al. (2016) present a hybrid FST-LSTM architecture. It combines FST and long short-term memory (LSTM), while weights of FST depend on different contexts and LSTM is used to extract the features that determine these weights. Lin et al. (2019) introduce an architecture named NFSTs, which is conditional probability distributions over pairs of strings. In this architecture, FST is used to constrain text generation. Other methods model regular expressions for downstream tasks. FARNN (Jiang et al., 2020) and FSTRNN (Jiang et al., 2021) are designed for intent detection and slot filling. The models above can be converted from sentence-level regular expressions and are interpretable after training. In this way, the advantage of symbolic rules and the neural network can be a combination.\\n\\nOur method is different from these works. In contrast to existing studies, our TST model operates at the character level and elucidates the mapping between Chinese characters and entity labels. Especially, we are the first to model FST as a trainable entity recognizer.\\n\\n3. The MS-NERC Corpus\\n\\n3.1. Text Collection\\n\\nWe collect technical specification documents of an aircraft for annotating. The specifications span four subdomains: assembly manufacturing, composite material processing, mechanical processing, and computer numerical control processing. Each document comprises several chapters, such as material control, equipment control, technical control, procedure control, maintenance control, and quality requirements. From these documents, we extract statements containing manufacturing parameters and manufacturing conditions. We filter out information related to specific products and enterprises due to privacy and legal issues. Ultimately, we obtain a raw dataset of 4,424 sentences.\\n\\n3.2. Annotation Guidelines\\n\\nTo define the category set in the manufacturing specifications, we refer to the Fundamental Terminology of Mechanical Manufacturing (Hongyu et al., 2008). We define a set consisting of 16 categories\"}"}
{"id": "lrec-2024-main-24", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"after consulting two experts in manufacturing. We present entity labels, entity definitions, and examples as follows.\\n\\n- **ACCESSORY** marks substances that play an auxiliary role in the production (e.g., \u53d1\u6ce1\u80f6/polystyrene foam).\\n- **ACCESSORY_ID** marks an identification number assigned to the accessory (e.g., RIX-ZC-106).\\n- **ATTRIBUTE** marks a physical characteristic in manufacturing (e.g., \u5347\u6e29\u901f\u7387/heating rate).\\n- **ATTRIBUTE_VA** marks the specific value of a physical characteristic in manufacturing (e.g., 1.5mm).\\n- **FIGURE_NOTE** marks an identification number of the referenced figure (e.g., \u56fe6-20/Figure 6-20).\\n- **FILE** marks an identification number of cited documents in the manufacturing specifications (e.g., CAR3001).\\n- **HOLE** marks a hole for part machining, assembly, inspection, and mounting (e.g., \u94c6\u9489\u5b54/rivet hole).\\n- **MATERIAL** marks a substance used to manufacture parts or components, including metallic and non-metallic materials (e.g., \u94dd\u5408\u91d1/Aluminum alloy).\\n- **OPERATION** marks an activity such as assembly, inspection, and handover, following certain procedures and technical requirements (e.g., \u80f6\u63a5/bonding).\\n- **PART** marks a basic component unit in manufacturing, including the combination of parts (e.g., \u94c6\u9489/rivet).\\n- **PART_AR** marks the location of a part for machining or assembly operations (e.g., \u8702\u7a9d\u677f\u8868\u9762/the surface of honeycomb panel).\\n- **PART_ID** marks an identification number assigned to the part (e.g., NAS1252).\\n- **PART_NU** marks the number of parts and tools (e.g., \u4e24\u4e2a/two).\\n- **REDUNDANT** marks trimmings and scraps generated in manufacturing (e.g., \u6bdb\u523a/burr).\\n- **TABLE_NOTE** marks an identification number of the referenced table (e.g., \u88687-1/Table7-1).\\n- **TOOL** marks a machining tool used in manufacturing (e.g., \u522e\u677f/drawing strickle).\\n\\nIn addition, our annotation adheres to three specific instructions. First, the entities must be specific rather than generalized (e.g., '\u94c6\u9489'/rivet instead of '\u96f6\u4ef6'/part). Second, the entities should not be accompanied by conjunctions and punctuation marks indicating juxtaposition, except in the case of notes in parentheses (e.g., '1.02mm(0.040in.)'). Lastly, when these specific instructions are met, entities are annotated based on their maximum span without nesting.\\n\\n### 3.3. Annotation Process\\n\\nOur corpus is annotated by three experts in manufacturing. Given the initial inconsistencies in the experts' interpretations of the annotation guidelines, a series of iterative discussions take place during the pre-annotation phase to refine the guidelines. Formal annotation commences once a Cohen's Kappa (Cohen, 1960) score exceeding 0.6 is achieved. During the formal annotation stage, the texts are divided into three groups. Each group is assigned to a different annotator, with a 15% overlap for duplicate assessment. The inter-annotator agreement is subsequently calculated based on this 15% overlap, resulting in a Cohen's Kappa score of 0.68.\\n\\n### 3.4. MS-NERC Statistics\\n\\nThe corpus comprises a total of 4,424 sentences and 16,383 entities. Table 1 shows the statistics of entity numbers, character-based max lengths, min lengths, and mean lengths. Notably, entity lengths have considerable variation due to the annotation based on maximum span. For example, there are instances of multi-unit attribute values, such as the ATTRIBUTE_VA of 50 max length. Additionally, partially abbreviated entities with lengths ranging from 1 to 3 characters are observed (e.g., '\u9489' is the abbreviation for '\u94c6\u9489').\\n\\n| Category       | Number | Max | Min | Mean |\\n|----------------|--------|-----|-----|------|\\n| ACCESSORY      | 1,861  | 12  | 1   | 3.31 |\\n| ACCESSORY_ID   | 876    | 17  | 3   | 7.87 |\\n| ATTRIBUTE      | 2,479  | 19  | 1   | 3.54 |\\n| ATTRIBUTE_VA   | 1,357  | 50  | 1   | 10.85|\\n| FIGURE_NOTE    | 396    | 6   | 3   | 4.21 |\\n| FILE           | 556    | 12  | 3   | 6.82 |\\n| HOLE           | 418    | 12  | 1   | 2.14 |\\n| MATERIAL       | 481    | 13  | 1   | 3.27 |\\n| OPERATION      | 1,933  | 9   | 1   | 2.63 |\\n| PART           | 2,407  | 13  | 1   | 3.27 |\\n| PART_AR        | 1,396  | 14  | 1   | 3.85 |\\n| PART_ID        | 202    | 15  | 2   | 8.75 |\\n| PART_NU        | 131    | 4   | 1   | 1.89 |\\n| REDUNDANT      | 164    | 6   | 1   | 2.27 |\\n| TABLE_NOTE     | 296    | 6   | 3   | 4.05 |\\n| TOOL           | 1,430  | 18  | 1   | 3.65 |\\n\\nTable 1: Entity statistics in MS-NERC.\"}"}
{"id": "lrec-2024-main-24", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Trainable States Transducer\\n\\n4.1. Regular Expressions and Finite State Transducer\\n\\nWe employ regular expressions (REs) to summarise the morphological patterns of entities, with examples of entities and their corresponding REs provided in Table 2. However, regular expressions cannot define the mapping from characters to entity labels, whereas a FST can.\\n\\nSakuma et al. (2012) demonstrate rigorously that a regular expression with capturing groups is equivalent to a FST. We represent each FST textually, utilizing the 'BIOE' tag scheme for the output. In this scheme, each '<:>' separates the character (left) and the output label (right). We employ '$' as a wildcard, and 'OO' denotes a temporary label. The use of '$' and 'OO' serves to provide conditions that the unmatched portion of a sentence can be allowed to match with other FSTs. We show the symbols used in Table 3, which contribute to a concise and precise textual representation of FSTs. Each symbol operates solely on its preceding symbol or sub-expression. To illustrate this process, we depict the regular expression and textual FST of an ATTRIBUTE_VA entity in Table 4.\\n\\n| Symbol | Meaning |\\n|--------|---------|\\n| ^      | number (0-9) |\\n| \u2217      | zero or more occurrence |\\n| | or operator |\\n| ?      | zero or one occurrence |\\n| \u00f7      | capital letter (A-Z) |\\n| +      | one or more occurrence |\\n\\nTable 3: Symbols for FST.\\n\\n4.2. Parameter Tensors for TST\\n\\nWe illustrate the parameter initialization of TST in Figure 2. The parameters of TST are initialized with a FST, which can be formally defined as a 6-tuple: \\n\\n$$ F = \\\\langle S, I, O, \\\\delta, B, E \\\\rangle $$\\n\\n$S$ is a finite set of states, $I$ is a finite set of input characters, $O$ is a finite set of output labels, and $\\\\delta$ is a function describing all transitions. $B$ is a finite set of start states and $E$ is a finite set of final states. $B$ and $E$ in FST can be converted to the start states vector $s$ and final states vector $m$ by one-hot encoding. Specifically, $\\\\delta$ describes the transition in the FST: \\n\\n$$ \\\\delta(\\\\gamma, x_t) = \\\\upsilon, l_t. $$\\n\\nIt means that $F$ accepts a character $x_t$ of input, then transmits from state $\\\\gamma$ to state $\\\\upsilon$, and outputs label $l_t$. To address the high time complexity and space complexity of the 4-order tensor, we decompose it into $W_i$ and $W_o$, following the work of Jiang et al. (2021). The decomposition can be computed by Eq. (1):\\n\\n$$ \\\\delta[x_t, l_t, \\\\gamma, \\\\upsilon] = W_i[x_t, \\\\gamma, \\\\upsilon] \\\\times W_o[l_t, \\\\gamma, \\\\upsilon]. $$\\n\\nTo sum up, TST can be formally defined as a 7-tuple: \\n\\n$$ N = \\\\langle S, I, O, W_i, W_o, s, m \\\\rangle $$\\n\\nAlgorithm 1 Inference in TST\\n\\nInput: $x = (x_1, x_2, ..., x_n)$, $N = \\\\langle S, I, O, W_i, W_o, s, m \\\\rangle$.\\n\\nOutput: the label scores $f_t$ of $x_t$.\\n\\nStep 1: Let $\\\\odot$ denote hadamard product, $N$ denote outer product. Let $\\\\alpha_0 = s^T$, $\\\\beta_n = m^T$.\\n\\nStep 2: calculate forward score $\\\\alpha_t$ for $t = 1 \\\\rightarrow n$ do $\\\\alpha_t = W_i[x_t] \\\\cdot \\\\alpha_{t-1}$ end\\n\\nStep 3: calculate backward score $\\\\beta_t$ for $t = n \\\\rightarrow 1$ do $\\\\beta_t = W_Ti[x_t] \\\\cdot \\\\beta_{t+1}$ end\\n\\nStep 4: calculate bidirectional score bi_scores for $t = 1 \\\\rightarrow n$ do bi_scores = $\\\\alpha_t N \\\\beta_t$ end\\n\\n$$ f_t = \\\\sum_{\\\\|S\\\\| k=1} F_t[\\\\cdot, k] \\\\quad \\\\text{return} f_t$$\\n\\n4.3. TST Inference\\n\\nGiven a sentence $x = (x_1, x_2, ..., x_n)$, the TST algorithm aims to find the output labels $l = (l_1, l_2, ..., l_n)$. The score of the label $l_t$ is the sum of the weights of all acceptance paths in TST that match $x_t$ and $l_t$. Casacuberta and de la Higuera (2000) demonstrate that finding the highest scoring output for a given sentence is NP-hard. Our method uses an approximate inference to independently determine the highest-scoring output label at the current position while ignoring the labels at other positions. We show the algorithm of computing label scores simultaneously in Algorithm 1. TST modifies its own parameters through supervised learning. However, the entities described by prior morphological knowledge in TST may be missing from the training set.\"}"}
{"id": "lrec-2024-main-24", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: A parameter initialization example of TST.\\n\\nTable 4: RE and FST of an ATTRIBUTE_VA entity.\\n\\nIn the few-shot scenario. To make the prior morphological knowledge in TST stable during training, we use the combined outcome of trained f and f\u2032 without training. Since a subset of the TST captured group may also be captured, we use a priority layer to resolve conflicts with the aim of prioritizing \u2019I-\u2019 over \u2019B-\u2019 and \u2019E-\u2019. The probability vector \\\\( P_t \\\\) is calculated to the Eq.(2), where \\\\( W_p \\\\) is the weight and \\\\( b_p \\\\) is the bias in the priority layer.\\n\\n\\\\[\\nP_t = \\\\text{priority} (W_p(f_t + f'_t) + b_p)\\n\\\\]\\n\\nBefore decoding from \\\\( P_t \\\\), OO is still retained in the pending output label, which corresponds to a probability \\\\( p'_k \\\\) of undeterminedness. In Eq.(3), if the probability of all labels is less than \\\\( p'_k \\\\), we assign the corresponding label as O. Otherwise, we output the label with the highest probability.\\n\\n\\\\[\\np'_t = \\\\max (p'_1, p'_k, p'_2, \\\\ldots, p'_{k-1})\\n\\\\]\\n\\nWe use Eq.(4) as a loss function to measure the discrepancy between the prediction and actual labels, where \\\\( y(t) \\\\) is one-hot embedding of the target label at time \\\\( t \\\\), while \\\\( p'_t(j) \\\\) is the probability of label \\\\( j \\\\) for \\\\( x_t \\\\).\\n\\n\\\\[\\n\\\\text{loss} = \\\\sum_{t=1}^{n} |O| \\\\sum_{j=1}^{k-1} y(t) \\\\log p'_t(j)\\n\\\\]\"}"}
{"id": "lrec-2024-main-24", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The coveragerate and statisticsof regular expressions with 100%-sen training set.\\n\\n5.2. Baselines\\n\\nWe train $\\\\text{TST}$ with supervised training sets separated from MS-NERIC and compare $\\\\text{TST}$ with the following baselines. We employ the same decoder for all models.\\n\\n- **Prompt-Slot-Tagging** (Hou et al., 2022) reversely predict slot values based on provided slot types. This approach incorporates training by considering the relationships between different slot types.\\n- **Template-based BART** (Cui et al., 2021) is a template-based method for exploiting the few-shot learning potential of generative pre-trained language models to sequence labeling.\\n- **NNShot** (Yang and Katiyar, 2020) is a method based on nearest neighbor learning and structured inference. This approach uses a supervised NER model trained on the source domain, as a feature extractor.\\n- **PER** (Jia et al., 2022) is a specialized NER method in manufacturing. This method utilizes rules and dictionaries for pre-recognition and employs the pre-recognition results to guide the training of the neural network.\\n- **BILSTM** (Siami-Namini et al., 2019) is one of the prominent deep learning models employed for addressing sequence-related tasks.\\n- **TENER** (Yan et al., 2019) utilizes the Transformer architecture to model information for NER tasks.\\n\\nOur baselines are specifically designed for different data conditions: three (Template-based BART, Prompt Slot Tagging and NNShot) for few-shot and two (BILSTM and TENER) for rich-resources. PER is a specialized NER method in manufacturing, so we conducted comparisons in both few-shot and rich-resources.\\n\\n5.3. Experimental Results\\n\\n**Zero-shot**\\n\\nWe compare the performance of the initial $\\\\text{TST}$ and RES in Table 6. Table 6 presents F1 and micro-average F1 scores calculated for six entity categories, assessing the recognizing ability of morphological-specific entities without training. Our method outperforms the regular expressions by +3.29% in micro-average F1. This can be attributed to the priority layer of $\\\\text{TST}$, which effectively resolves conflicts in output labels and selects the optimal choice.\\n\\n| Type              | $\\\\text{TST}$ | RES |\\n|-------------------|--------------|-----|\\n| ACCESSORY_ID      | 96.81        | 97.08 |\\n| ATTRIBUTE_VALUE   | 63.41        | 56.63 |\\n| FIGURE_NOTE       | 96.97        | 98.20 |\\n| FILE              | 92.98        | 88.98 |\\n| PART_ID           | 58.06        | 75.51 |\\n| TABLE_NOTE        | 97.67        | 97.67 |\\n| **micro-average** | **82.05**    | **78.76** |\\n\\nTable 6: F1 scores (%) for the initial $\\\\text{TST}$ and RES in zero-shot.\\n\\n**Rich-resource**\\n\\nWe show the results of 100%-sen training set in Table 7. The micro-average F1 scores listed are calculated based on all 16 entity categories.\\n\\n| Method          | Precision | Recall | F1    |\\n|-----------------|-----------|--------|-------|\\n| $\\\\text{TST}$    | 65.96     | 61.27  | 63.53 |\\n| PER             | 58.07     |        |       |\\n| BILSTM          | 55.28     | 64.67  | 59.61 |\\n| TENER           | 53.37     | 63.98  | 58.19 |\\n\\nTable 7: Micro-average F1 scores (%) for $\\\\text{TST}$ and baselines in rich-resource.\"}"}
{"id": "lrec-2024-main-24", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Micro-average F1 scores (%) for TST and baselines in few-shot.\\n\\n| Baseline                  | 20-shot | 50-shot | 3%-sen | 6%-sen |\\n|---------------------------|---------|---------|--------|--------|\\n| Template-based BART       | 12.52   | 15.68   | 13.87  | 17.54  |\\n| Prompt Slot Tagging       | 16.38   | 17.08   | 17.44  | 23.2   |\\n| NNShot                    | 27.2    | 30.5    | 25.25  | 29.37  |\\n\\nThe baselines, which shows the equivalent learning ability. TST outperforms PER by +1.23% in F1, demonstrating its effectiveness with full training data. TST achieves a higher precision and a lower recall, adopting a conservative approach in comparison to neural networks. This characteristic renders TST well-suited for NER tasks in manufacturing that prioritize high precision and low error rates.\\n\\nFew-shot Table 8 presents the micro-average F1 scores in few-shot, with init-TST indicating the initial TST\u2019s F1 score across all 16 entity categories in zero-shot. The results demonstrate that our model maintains a lead over the baselines in few-shot and outperforms the init-TST after training. Our model\u2019s strength lies in its utilization of prior morphological knowledge, distinguishing it from the neural baselines. PER performs poorly in few-shot because this method relies on domain knowledge to guide the neural network training. When there is insufficient training data, domain knowledge cannot be effectively leveraged.\\n\\nTST underperforms the init-TST and NNShot in 20-shot. This discrepancy may be due to the differences between the development and testing sets. Through training, we select and evaluate the best performance model based on the development set. The 6%-sen has fewer entities than the 50-shot but gets a better result. This is because that the 6%-sen training set and the testing set are extracted based on the proportions of the sentences and entities. In 6%-sen, some entities have counts exceeding 50, and these entities also constitute a significant portion of the testing set. TST demonstrates more comprehensive learning for such entities, contributing to more accurate recognition during testing and yielding a higher micro-average F1 score.\\n\\n6. Conclusion\\n\\nIn this paper, we investigate the task of named entity recognition in manufacturing. We define 16 categories and develop a Chinese NER corpus named MS-NERC with 4,424 sentences and 16,383 entities in manufacturing. We demonstrate that some entities in MS-NERC conform to morphological patterns. We propose an entity recognizer named TST, which can be initialized with a FST describing the morphological patterns of entities. Our method efficiently exploits prior morphological knowledge, enabling TST to directly recognize related entities, free from the constraints of the training set. Experiments show that TST demonstrates competitive performance in zero-shot, few-shot, and rich-resource. To the best of our knowledge, we are the first to model prior morphological knowledge of the entities as a trainable entity recognizer.\\n\\n7. Acknowledgements\\n\\nThis research was supported by Applied Basic Research Program of Liaoning Province (2022JH2/101300248), Research Programs of China National Committee for Terminology in Science and Technology (YB2022015), and National Natural Science Foundation of China (No.U1908216).\\n\\n8. Bibliographical References\\n\\nGeorg Buchgeher, David Gabauer, Jorge Mart\u00ednez Gil, and Lisa Ehrlinger. 2021. Knowledge graphs in manufacturing and production: A systematic literature review. IEEE Access, 9:55537\u201355554.\\n\\nFrancisco Casacuberta and Colin de la Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In Proceedings of the 2000 International Colloquium on Grammatical Inference (ICGI), pages 15\u201324, Lisbon, Portugal.\\n\\nZhiyu Chen, Jinsong Bao, Xiaohu Deng, Siyi Ding, and Tianyuan Liu. 2021. Semantic recognition method of assembly process based on LSTM. Computer Integrated Manufacturing Systems, 27(6):1582.\\n\\nJacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37\u201346.\\n\\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang. 2021. Template-based named entity recognition using BART. In Findings of\"}"}
{"id": "lrec-2024-main-24", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yuntian Feng, Hongjun Zhang, and Wenning Hao. 2015. Named entity recognition for military text. *Journal of Computer Science*, 42(7):15\u201318.\\n\\nDing Hongyu, Xi Daoyun, Zhang Xiufen, Han Linlin, Xiao Chengxaing, and Yunfeng Wang. 2008. *Fundamental Terminology of Mechanical Manufacturing Processes*. General Administration of Quality Supervision, Inspection and Quarantine of the People's Republic of China; Standardization Administration of China.\\n\\nYutai Hou, Cheng Chen, Xianzhen Luo, Bohan Li, and Wanxiang Che. 2022. Inverse is better! fast and accurate prompt for few-shot slot tagging. In *Findings of the Association for Computational Linguistics (ACL)*, pages 637\u2013647, Dublin, Ireland.\\n\\nMeng Jia, Peiyan Wang, Guiping Zhang, and Dongfeng Cai. 2022. Named entity recognition for process text. *Journal of Chinese Information Processing*, 36(3):54\u201363.\\n\\nChengyue Jiang, Zijian Jin, and Kewei Tu. 2021. Neuralizing regular expressions for slot filling. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 9481\u20139498, Online.\\n\\nChengyue Jiang, Yinggong Zhao, Shanbo Chu, Libin Shen, and Kewei Tu. 2020. Cold-start and interpretability: Turning regular expressions into trainable recurrent neural networks. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 3193\u20133207, Online.\\n\\nHang Jiang, Yining Hua, Doug Beeferman, and Deb Roy. 2022. Annotating the tweebank corpus on named entity recognition and building NLP models for social media analysis. In *Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC)*, pages 7199\u20137208, Marseille, France.\\n\\nChuChengLin, HaoZhu, Matthew R. Gormley, and Jason Eisner. 2019. Neural finite-state transducers: Beyond rational relations. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)*, pages 272\u2013283, Minneapolis, MN, USA.\\n\\nJianhong Ma, Liqin Wang, and Shuang Yao. 2018. Named entity recognition for chemical resource text. *Journal of Zhengzhou University (Natural Science Edition)*, 50(4):14\u201320.\\n\\nRendani Mbuvha, David Ifeoluwa Adelani, Tendani Mutavhatsindi, Tshimangadzo Rakhuhu, Aluwani Mauda, Tshifhiwa Joshua Maumela, Andisani Masindi, Seani Rananga, Vukosi Mariyate, and Tshilidzi Marwala. 2023. Mphayaner: Named entity recognition for tshivenda. *CoRR*, abs/2304.03952.\\n\\nPushpendre Rastogi, Ryan Cotterell, and Jason Eisner. 2016. Weighting finite-state transduction with neural context. In *Proceedings of the 2016 conference of the North American chapter of the Association for Computational Linguistics (NAACL)*, pages 623\u2013633, San Diego California, USA.\\n\\nYuto Sakuma, Yasuhiko Minamide, and Andrei Voronkov. 2012. Translating regular expression matching into transducers. *Journal of Applied Logic*, 10(1):32\u201351.\\n\\nSima Siami-Namini, Neda Tavakoli, and Akbar Siami Namin. 2019. The performance of LSTM and biLSTM in forecasting time series. In *Proceedings of the 2019 IEEE International Conference on Big Data (IEEE BigData)*, pages 3285\u20133292, Los Angeles, CA, USA.\\n\\nJeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. 2020. Code and named entity recognition in StackOverflow. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)*, Online.\\n\\nYuanhe Tian, Han Qin, Fei Xia, and Yan Song. 2022. Chimst: A Chinese medical corpus for word segmentation and medical term recognition. In *Proceedings of the Thirteenth Language Resources and Evaluation Conference (LREC)*, Marseille, France.\\n\\nJun Wang, Mingyan Song, Qiang Ye, and Tong Zhang. 2020. Research on the key technologies of relation extraction from quality text for industrial manufacturing. In *Proceedings of the 2020 International Conference on Computer Science and Management Technology (ICCSMT)*, pages 366\u2013369, Shanghai, China.\\n\\nHaoyu Wu, Qinglei, Xinyue Zhang, and Zhengqian Luo. 2020. Creating a large-scale financial news corpus for relation extraction. In *Proceedings of the 2020 3rd International Conference on Artificial Intelligence and Big Data (ICAIBD)*, pages 259\u2013263, Chengdu, China.\\n\\nHang Yan, Bocao Deng, Xiaonan Li, and Xipeng Qiu. 2019. TENER: adapting transformer encoder for named entity recognition. *CoRR*, abs/1911.04474.\"}"}
{"id": "lrec-2024-main-24", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yi Yang and Arzoo Katiyar. 2020. Simple and effective few-shot named entity recognition with structured nearest neighbor learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6365\u20136375, Online.\\n\\nNana Zhang, Peiyan Wang, and Guiping Zhang. 2019. Named entity deep learning recognition method for process operation description text. Computer Applications and Software, 36(11):188\u2013195.\\n\\nShunle Zhu. 2019. Deep learning based uyghur named entities recognition. Journal of Computer Engineering And Design, 40(10):2874\u20132878+2890.\"}"}
