{"id": "acl-2023-long-827", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nAppendix B\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nAppendix B and Appendix F\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nNot applicable. We do not submit the protocol to an ethics review board because our country has not yet established an ethical committee at the national level.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\n4.5 and Appendix F\"}"}
{"id": "acl-2023-long-827", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing\\n\\nXuekai Zhu2\u2217, Jian Guan1\u2217, Minlie Huang1\u2020 and Juan Liu2\\n\\n1The CoAI group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China\\n2Institute of Artificial Intelligence, School of Computer Science, Wuhan University, Wuhan, 430072, China\\n\\n{xuekaizhu,liujuan}@whu.edu.cn, j-guan19@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn\\n\\nAbstract\\nNon-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an input story into a specified author style while maintaining source semantics. To tackle this problem, we propose a generation model, named StoryTrans, which leverages discourse representations to capture source content information and transfer them to target styles with learnable style embeddings. We use an additional training objective to disentangle stylistic features from the learned discourse representation to prevent the model from degenerating to an auto-encoder. Moreover, to enhance content preservation, we design a mask-and-fill framework to explicitly fuse style-specific keywords of source texts into generation. Furthermore, we constructed new datasets for this task in Chinese and English, respectively. Extensive experiments show that our model outperforms strong baselines in overall performance of style transfer and content preservation.\\n\\n1 Introduction\\nText style transfer aims to endow a text with a different style while keeping its main semantic content unaltered. It has a wide range of applications, such as formality transfer (Jain et al., 2019), sentiment transfer (Shen et al., 2017) and author-style imitation (Tikhonov and Yamshchikov, 2018). Due to the lack of parallel corpora, recent works mainly focus on unsupervised transfer by self-reconstruction. Current methods proposed to disentangle styles from contents by removing stylistic tokens from inputs explicitly (Huang et al., 2021) or reducing stylistic features from token-level hidden representations of inputs implicitly (Lee et al., 2021). This line of work has impressive performance on single-sentence sentiment and formality transfer. However, it is yet not investigated to transfer author styles of long texts such as stories, manifesting in the author\u2019s linguistic choices at the lexical, syntactic, and discourse levels.\\n\\nIn this paper, we present the first study on story author-style transfer, which aims to rewrite a story incorporating source content and the target author style.\\n\\nTable 1: An example that transfers a vernacular story to the martial arts style of JY generated by StyleLM. The orange sentence indicates missing content in source text. The rewritten token is underlined. The red highlights are supplementary short phrases or plots to align with the target style. The English texts below the Chinese are translated versions of the Chinese samples.\\n\\n| Source Text: | Generated Text for JY Style: |\\n|-------------|-----------------------------|\\n| \u90ed\u7ff0\u662f\u53e4\u65f6\u5019\u4e00\u540d\u624d\u5b50\u3002\u4e00\u4e2a\u590f\u65e5\u7684\u665a\u4e0a,\u4ed6\u5728\u9662\u4e2d\u4e58\u51c9\u3002\u5ffd\u7136,\u4e00\u9635\u98ce\u8d77,\u9001\u6765\u4e00\u80a1\u6c81\u4eba\u5fc3\u813e\u7684\u6e05\u9999,\u4e00\u4f4d\u5c11\u5973\u9a7e\u7740\u767d\u4e91\u4ece\u5929\u800c\u964d,\u51fa\u73b0\u5728\u90ed\u7ff0\u773c\u524d\u2026 | Guo Han was a talented man in ancient times. One summer evening, he was enjoying the cool in the courtyard. Suddenly, a gust of wind brought a refreshing fragrance, and a young girl descended from the sky on a white cloud and appeared in front of Guo Han\u2026 |\\n\\n1JinYong is a Chinese martial arts novelist.\"}"}
{"id": "acl-2023-long-827", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"enrich the storyline (e.g., the red highlights). In contrast to the transfer of token-level features like formality, it is more difficult to capture the inter-sentence relations correlated with author styles and disentangle them from contents.\\n\\nThe second challenge is that the author styles tend to be highly associated with specific writing topics. Therefore, it is hard to transfer these style-specific contents to another style. For example, the topic \u201ctalented man\u201d hardly shows up in the novels of JY, leading to the low content preservation of such contents, as shown in the orange text in Table 1.\\n\\nTo alleviate the above issues, we propose a generation framework, named StoryTrans, which learns discourse representations from source texts and then combines these representations with learnable style embeddings to generate texts of target styles. Furthermore, we propose a new training objective to reduce stylistic features from the discourse representations, which aims to pull the representations derived from different texts close in the latent space. To enhance content preservation, we separate the generation process into two stages, which first transfers the source text with the style-specific content keywords masked and then generates the whole text by imposing these keywords explicitly.\\n\\nTo support the evaluation of the proposed task, we collect new datasets in Chinese and English based on existing story corpora. We conduct extensive experiments to transfer fairy tales (in Chinese) or everyday stories (in English) to typical author styles, respectively. Automatic evaluation results show that our model achieves a better overall performance in style control and content preservation than strong baselines. The manual evaluation also confirms the efficacy of our model. We summarize the key contributions of this work as follows:\\n\\nI. To the best of our knowledge, we present the first study on story author style transfer. We construct new Chinese and English datasets for this task.\\n\\nII. We propose a new generation model named StoryTrans to tackle the new task, which implements content-style disentanglement and stylization based on discourse representations, then enhances content preservation by explicitly incorporating style-specific keywords.\\n\\nIII. Extensive experiments show that our model outperforms baselines in the overall performance of style transfer accuracy and content preservation.\\n\\n2 Related Work\\n\\n2.1 Style Transfer\\n\\nRecent studies concentrated mainly on token-level style transfer of single sentences, such as formality or sentiment transfer. We categorize these studies into three following paradigms.\\n\\nThe first paradigm built a style transfer system without explicit disentanglement of style and content. This line of work used additional style signals or a multi-generator structure to control the style. Dai et al. (2019) added an extra style embedding in input for manipulating the style of texts. Yi et al. (2020) proposed a style instance encoding method for learning more discriminative and expressive style embeddings. The learnable style embedding is a flexible yet effective approach to providing style signals. Such a design helps better preserve source content. Syed et al. (2020) randomly dropped the input words, then reconstructed input for each author separately, which obtained multiple author-specific generators. The multi-generator structure is effective but also resource-consuming. However, this paradigm incurs unsatisfactory style transfer accuracy without explicit disentanglement.\\n\\nThe second paradigm disentangled the content and style explicitly in latent space, then combined the target style signal. Zhu et al. (2021) diluted sentence-level information in style representations. John et al. (2019) incorporated style prediction and adversarial objectives for disentangling. Lee et al. (2021) removed style information of each token with reverse attention score (Bahdanau et al., 2015), which is estimated by a pre-trained style classifier. This paradigm utilizes adversarial loss functions or a pre-trained estimator for disentanglement. And experiment results indicate that explicit disentanglement leads to satisfactory style transfer accuracy but poor content preservation.\\n\\nThe final paradigm views style as localized features of tokens in a sentence, which locates style-dependent words and replaces the target-style ones. Xu et al. (2018) employed an attention mechanism to identify style tokens and filter out such tokens. Wu et al. (2019) utilized a two-stage framework to mask all sentimental tokens and then infill them. Huang et al. (2021) aligned words of input and reference to achieve token-level transfer. To sum up, this paradigm maintains all word-level information, but it is hard to apply to the scenarios where styles are expressed beyond token level, e.g., author style.\\n\\nAbsorbing ideas from paradigm 1 and 2, we...\"}"}
{"id": "acl-2023-long-827", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2160. Discourse Representation Transfer\\n\\n\u2161. Content Preservation Enhancing\\n\\nThen at the peace of my soul,\u00b7\u00b7\u00b7\\n\\nData preprocessing\\n\\nStory paragraphs extracted from Chinese and English novels\\n\\nMasked Story Paragraphs\\n\\nStyle-Specific Contents\\n\\nIdentifying style-specific contents and replacing them with mask tokens in the story paragraphs\\n\\nEncoder\\n\\nDecoder\\n\\n\\\"Fusion Module +\\n\\n\\\"Then at the <mask> of my soul,\u00b7\u00b7\u00b7\\\"\\n\\n$Gina was looking for a place to be alone.\u00b7\u00b7\u00b7$\\\\text{[Key 1]} Gina [Key 2] alone \u00b7\u00b7\u00b7$\\n\\nPointer Network\\n\\nEncoder\\n\\nDecoder\\n\\nSentence Order\\n\\nStyle Embedding\\n\\n$\\\\text{Shuffled Unshuffled}$\\n\\nPulling Close for Disentangling\\n\\nGeneration Flow\\n\\nSequence Reconstructing\\n\\nFigure 1: An overview of the generative flow. For discourse representation transfer (the first stage), the encoder employs discourse representations ($\\\\{r_i\\\\}_{i=1}^n$) to contain main semantics of pre-processed input ($x_m$). Then, the fusion module stylizes the discourse representations with target style embedding ($\\\\hat{s}$). For content preservation enhancing (the second stage), our model enhances the content preservation of transferred texts ($x_m$) with style-specific content ($k$).\\n\\n$x$ and $\\\\hat{x}$ denote the original story and the final output, respectively.\\n\\nWe are inspired to use a sentence order prediction task to learn high-level discourse representations.\\n\\n2.2 High-Level Representation\\n\\nPrior works captured the hierarchical structure of natural language texts by learning high-level representations. Li et al. (2015) and Zhang et al. (2019) proposed to learn hierarchical embedding representations by reconstructing masked version of sentences or paragraphs. Reimers and Gurevych (2019) derived semantical sentence embeddings by fine-tuning BERT (Devlin et al., 2019) on downstream tasks. Lee et al. (2020); Guan et al. (2021b) inserted special tokens for each sentence and devised several pre-training tasks to learn sentence-level representations. We are inspired to use a sentence order prediction task to learn high-level discourse representations.\\n\\n2.3 Long Text Generation\\n\\nIn order to generate coherent long texts, recent studies usually decomposed generation into multiple stages. Fan et al. (2018); Yao et al. (2019) generated a premise, then transformed it into a passage. Tan et al. (2021) first produced domain-specific content keywords and then progressively refines them into complete passages. Borrowing these ideas, we adopted a mask-and-fill framework to enhance content preservation in text style transfer.\"}"}
{"id": "acl-2023-long-827", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Encoder Decoder Fusion\\n\\nFig. 2: Illustration of loss functions during training for the first stage (a) and second stage (b). Enc, Fus, Dec and C denote the encoder, the fusion module, the decoder, and style classifier, respectively.\\n\\nLevel guidance for the subsequent generation of the transferred text. As for discourse representations learning, we employ a sentence order prediction loss to capture inter-sentence discourse dependencies. And we use a style classifier loss to control the style of generated texts (Lee et al., 2021). In summary, the first-stage model is trained using the following loss function:\\n\\n\\\\[ L_1 = L_{self} + \\\\lambda_1 L_{dis} + \\\\lambda_2 L_{sop} + \\\\lambda_3 L_{style}, \\\\]\\n\\nwhere \\\\( \\\\lambda_1, \\\\lambda_2 \\\\) and \\\\( \\\\lambda_3 \\\\) are adjustable hyper-parameters. \\\\( L_{self}, L_{dis}, L_{sop} \\\\) and \\\\( L_{style} \\\\) are the self-reconstruction loss, the disentanglement loss, the sequence order prediction loss and the style classifier loss, respectively. Figure 2 shows the workflow of learning objects.\\n\\nIn the second stage, we use a denoising autoencoder (DAE) loss to train another encoder-decoder model for reconstructing \\\\( x_m \\\\):\\n\\n\\\\[ L_2 = -\\\\frac{1}{T} \\\\sum_{t=1}^{T} \\\\log P(x_t | x_t < t, \\\\{r_i\\\\}_{n=1}^i, x_m). \\\\]\\n\\nThis stage is unrelated to author styles, and helps achieve better content preservation.\\n\\n3.2 Discourse Representations Transfer\\n\\nAs described in Figure 2, we propose to learn discourse representations, and then reconstruct the texts from discourse representations. And we perform the disentanglement and stylizing operation based on discourse representations.\\n\\nDiscourse Representations\\n\\nSupposing that \\\\( x_m \\\\) consists of \\\\( n \\\\) sentences, we insert a special token \\\\( \\\\langle \\\\text{Sen} \\\\rangle \\\\) at the end of each sentence in \\\\( x_m \\\\) (Reimers and Gurevych, 2019; Lee et al., 2020; Guan et al., 2021b). Let \\\\( r_n \\\\) denote the hidden state of the encoder at the position of the \\\\( n \\\\)-th special token, \\\\( \\\\{r_i\\\\}_{n=1}^{i} = \\\\text{Encoder}(x_m) \\\\). And \\\\( z_n \\\\) is the output of the fusion module corresponding to \\\\( r_n \\\\). Previous studies have demonstrated that correcting the order of shuffled sentences is a simple but effective way to learn meaningful discourse representations (Lee et al., 2020). As shown in Figure 1, we feed \\\\( z_n \\\\) into a pointer network (Gong et al., 2016) to predict orders. During training, we shuffled the original sentence order and feed the perturbed text into the encoder for calculating \\\\( L_{sop} \\\\).\\n\\nFusion Module\\n\\nTo provide signals of transfer direction, we concatenate the learned discourse representations \\\\( \\\\{r_i\\\\}_{n=1}^{i} \\\\) with the style embedding \\\\( s \\\\) and fuse them using a multi-head attention layer, as illustrated in Figure 1. To capture discourse-level features of texts with different author-styles, we set each style embedding to a vector with the same dimension as \\\\( r_i \\\\). Formally, we derive the style-aware discourse representations \\\\( \\\\{z_i\\\\}_{n+1=1}^{i} \\\\) as follows:\\n\\n\\\\[ \\\\{z_i\\\\}_{n+1=1}^{i} = \\\\text{MHA}(Q=K=V=\\\\{s \\\\parallel \\\\{r_i\\\\}_{n=1}^{i}\\\\}), \\\\]\\n\\nwhere MHA is the multi-head attention layer, \\\\( Q/K/V \\\\) is the corresponding query/key/value, \\\\( \\\\parallel \\\\) is the concatenation operation. Then, the decoder gets access to \\\\( \\\\{z_i\\\\}_{n+1=1}^{i} \\\\) through the cross-attention layer, which serve as a discourse-level guidance for generating the transferred texts. Then, we feed \\\\( \\\\{z_i\\\\}_{n+1=1}^{i} \\\\) into the decoder.\\n\\nPointer Network\\n\\nFollowing Logeswaran et al. (2018); Lee et al. (2020), we use a pointer network to predict the original orders of the shuffled sentences. The each position probability of sentence order is formulated as follows:\\n\\n\\\\[ p_i = \\\\text{softmax}(\\\\{z_i\\\\}_{n=1}^{i}Wz^T), \\\\]\\n\\nwhere \\\\( p_i \\\\) is predicted probabilities of sentence \\\\( i \\\\), \\\\( W \\\\) is a trainable parameter.\\n\\n3.3 First-Stage Training Objectives\\n\\nSelf-Reconstruction Loss\\n\\nWe formulate self-reconstruction loss as follows:\\n\\n\\\\[ L_{self} = -\\\\frac{1}{T} \\\\sum_{i=1}^{T} \\\\log P(x_m_t | x_m < t, \\\\{r_i\\\\}_{n=1}^{i}, s), \\\\]\\n\\nwhere \\\\( s \\\\) is the learnable embedding of \\\\( s \\\\). During inference, we replace \\\\( s \\\\) with the embedding of the target style \\\\( \\\\hat{s} \\\\) (i.e., \\\\( \\\\hat{s} \\\\)), to achieve the style transfer.\"}"}
{"id": "acl-2023-long-827", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Disentanglement Loss\\nWe disentangle the style and content on discourse representations. Inspired by prior studies on structuring latent spaces (Gao et al., 2019; Zhu et al., 2021), we devise an additional loss function $L_{\\\\text{dis}}$ to pull close discourse representations from different examples in the same mini-batch, corresponding to different author styles. $L_{\\\\text{dis}}$ and $L_{\\\\text{self}}$ work as adversarial losses and lead the model to achieve a balance between content preservation and style transfer. We derive $L_{\\\\text{dis}}$ as follows:\\n\\n$$L_{\\\\text{dis}} = \\\\frac{1}{2b^2} \\\\sum_{i=1}^{b} \\\\sum_{j=1}^{b} \\\\|\\\\bar{r}_i - \\\\bar{r}_j\\\\|_2^2,$$\\n\\n(6)\\n\\n$$\\\\bar{r}_i = \\\\frac{1}{n} \\\\sum_{i=1}^{n} r_i,$$\\n\\n(7)\\n\\nwhere $b$ is the size of mini-batch.\\n\\nSentence Order Prediction Loss\\nWe formulate $L_{\\\\text{sop}}$ as the cross-entropy loss between the golden and predicted orders as follows:\\n\\n$$L_{\\\\text{sop}} = -\\\\frac{1}{n} \\\\sum_{i=1}^{n} o_i \\\\log(p_i),$$\\n\\n(8)\\n\\nwhere $o_i$ is a one-hot ground-truth vector of correct sentence position, and $p_i$ is predicted probabilities.\\n\\nStyle Classifier Loss\\nWe expect the transferred text to be of the target style. Hence we train a style classifier to derive the style transfer loss as follows:\\n\\n$$L_{\\\\text{style}} = -\\\\mathbb{E}_{\\\\hat{x}_m \\\\sim \\\\text{Decoder}} \\\\left[ \\\\log P_C(s | \\\\hat{x}_m) \\\\right],$$\\n\\n(9)\\n\\nwhere $P_C$ is the conditional distribution over styles defined by the classifier. We train the classifier on the whole training set with the standard cross-entropy loss. Then, we freeze the weights of style classifier for computing $L_{\\\\text{style}}$. On the other hand, we follow Lee et al. (2021); Dai et al. (2019) to use soft sampling to allow gradient back-propagation.\\n\\n3.4 Content Preservation Enhancing\\nAs aforementioned, author styles have a strong correlation with contents. It is difficult to transfer such style-specific contents to other styles directly. Since we train the model in an auto-encoder manner, it will have no idea how to transfer those content representations that have never seen other style embeddings during training. To address the issue, we propose to mask the style-specific keywords in the source text and perform style transfer on the masked text in the first generation stage. Then, we fill the masked tokens in the second stage.\\n\\nWe follow Xiao et al. (2021) to use a frequency-based method to identify the style-specific keywords. Specifically, we extract style-specific keywords by (1) obtaining the top-10 words with the highest TF-IDF scores from each corpus, (2) retaining only people's names, place names, and proper nouns, (3) and filtering out those words with a high frequency in all corpora.\\n\\nWe denote the resulting word set as $D_s$ for the corpus with the style $s$. We extract the style-specific keywords $k$ from the text $x$ by selecting the words that are in $D_s$. We detail above operation and explain it in Appendix A. In the second stage, we train another model to fill the mask tokens in outputs of the first stage conditioned on the identified style-specific keywords in source inputs. During training, we concatenate the keywords in $k$ with a special token $\\\\langle\\\\text{Key}\\\\rangle$ and feed them into the encoder paired with $x_m$, as shown in Figure 1. The training object is formulated as Equation 2. During inference, the decoder generates the transferred text $\\\\hat{x}$ conditioned on the output of the first stage $\\\\hat{x}_m$ in an auto-regressive manner.\\n\\n4 Experiments\\n4.1 Datasets\\nWe construct stylized story datasets in Chinese and English, respectively. The Chinese dataset consists of three styles of texts, including fairy tales from LOT (Guan et al., 2021a), LuXun (LX), and JinYong (JY). Specifically, LuXun writes realism novels while JinYong focuses on martial arts novels. These texts of different styles have a gap in lexical, syntactic, and semantic levels. Samples of different styles are detailed in Appendix C.\\n\\nIn our experiments, we aim to transfer a fairy tale to the LX or JY style. The English dataset consists\"}"}
{"id": "acl-2023-long-827", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of two styles of texts, including everyday stories from ROCStories (Mostafazadeh et al., 2016) and fragments from Shakespeare's plays. We expect to transfer a five-sentence everyday story into the Shakespeare style. The statistics of datasets are shown in Table 2. The more details are described in Appendix B.\\n\\n4.2 Implementation\\n\\nWe take LongLM\\\\textsubscript{BASE} (Guan et al., 2021a) and T5\\\\textsubscript{BASE} (Raffel et al., 2020) as the backbone model of both generation stages for Chinese and English experiments, respectively. Furthermore, the fusion module and pointer network consist of two and one layers of randomly initialized bidirectional Transformer blocks (Vaswani et al., 2017), respectively.\\n\\nWe conduct experiments on one RTX 6000 GPU. In addition, we build the style classifier based on the encoder of LongLM\\\\textsubscript{BASE} and T5\\\\textsubscript{BASE} for Chinese and English, respectively.\\n\\nWe set $\\\\lambda_1/\\\\lambda_2/\\\\lambda_3$ in Equation 1 to 1/1/1, the batch size to 4, the learning rate to 5e-5, the maximum sequence length of the encoder and decoder to 512 for both generation stages in the Chinese experiments. And the hyper-parameters for English experiments are the same except that $\\\\lambda_1/\\\\lambda_2/\\\\lambda_3$ are set to 0.5/0.5/0.5 and the learning rate to 2.5e-5. More implementation details are presented in Appendix D.\\n\\n4.3 Baselines\\n\\nSince no previous studies have focused on story author-style transfer, we build several baselines by adapting short-text style transfer models. For a fair comparison, we initialize all baselines using the same pre-trained parameters as our model. Specifically, we adopt the following baselines:\\n\\n- **Style Transformer**: It adds an extra style embedding and a discriminator to provide style transfer rewards without disentangling content from styles (Dai et al., 2019).\\n- **StyleLM**: This baseline generates the target text conditioned on the given style token and corrupted version of the original text (Syed et al., 2020).\\n- **Reverse Attention**: It inserts a reverse attention module on the last layer of the encoder, which aims to negate the style information from the hidden states of the encoder (Lee et al., 2021).\\n\\n4.4 Automatic Evaluation\\n\\nEvaluation Metrics\\n\\nPrevious works evaluate style transfer systems mainly from three aspects including style transfer accuracy, content preservation, and sentence fluency. A good style transfer system needs to balance the contradiction between content preservation and transfer accuracy (Zhu et al., 2021; Niu and Bansal, 2018). We use a joint metric to evaluate the overall performance of models. On the other hand, previous studies usually use perplexity (PPL) of a pre-trained language model. However, in our experiments, we found that the PPL of model outputs is lower than human-written texts, suggesting that PPL is not reliable for evaluating the quality of stories. Therefore, we evaluate the fluency through manual evaluation.\\n\\nSpecifically, we adopt the following automatic metrics:\\n\\n1. **Style Transfer Accuracy**: We use two variants of style transfer accuracy following Krishna et al. (2021), absolute accuracy (a-Acc) and relative accuracy (r-Acc). We train a style classifier and regard the classifier score as the a-Acc. And r-Acc is a binary value to indicate whether the style classifier score the output higher than the input (1/0 for a higher/lower score). We train the classifier by fine-tuning the encoder of LongLM\\\\textsubscript{BASE} and T5\\\\textsubscript{BASE} on the Chinese and English training set, respectively. The classifier achieves a 99.6% and 99.41% accuracy on the Chinese and English test sets, respectively.\\n\\n2. **Content Preservation**: We use BLEU-n ($n = 1, 2$) (Papineni et al., 2002) and BERTScore (BS) (Zhang* et al., 2020) between generated and input texts to measure their lexical and semantic similarity, respectively. And we report recall (BS-R), precision (BS-P) and F1 score (BS-F1) for BS.\\n\\n3. **Overall**: We use the geometric mean of a-ACC and BLEU/BS-F1 score (BL-Overall/BS-Overall) to assess the overall performance of models (Krishna et al., 2020; Lee et al., 2021).\\n\\nResults on the Chinese Dataset\\n\\nWe show the overall performance and individual metrics results in Table 3. In terms of overall performance, StoryTrans outperforms baselines, illustrating that StoryTrans can achieve a better balance between style transfer and content preservation.\\n\\nIn terms of style accuracy, StoryTrans achieves the best style transfer accuracy (a-Acc) in LX and comparable performance in JY. The bad performance of baselines indicates the necessity to perform explicit disentanglement beyond the token level. In addition, manual inspection shows that Style Transformer tends to copy the input, accounting for the highest BLEU score and BERTScore.\"}"}
{"id": "acl-2023-long-827", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 3: Automatic evaluation results on the test set of the Chinese and English datasets. Bold numbers indicate best performance. ZH-LX/ZH-JY is the Chinese author LuXun/JinYong, respectively. EN-SP is the English author Shakespeare. StoryTrans achieves the best overall performance (BL/BS-Overall), with a good trade-off between style accuracy (r/a-Acc) and content preservation (BLEU-1/2 and BS-P/R/F1).\\n\\n| Models         | ZH-LX | ZH-JY | EN-SP |\\n|---------------|-------|-------|-------|\\n| **ZH-LX**     |       |       |       |\\n| Style Transformer | 65.84 | 46.77 | 0.34  |\\n| StyleLM       | 97.80 | 79.97 | 57.93 |\\n| Reverse Attention | 98.49 | 94.51 | 20.68 |\\n| StoryTrans    | 97.66 | 84.49 | 88.62 |\\n| **ZH-JY**     |       |       |       |\\n| Style Transformer | 46.77 | 71.00 | 0.34  |\\n| StyleLM       | 79.97 | 71.00 | 57.93 |\\n| Reverse Attention | 94.51 | 94.51 | 20.68 |\\n| StoryTrans    | 97.66 | 84.49 | 88.62 |\\n| **EN-SP**     |       |       |       |\\n| Style Transformer | 0.34  | 0.34  | 0.34  |\\n| StyleLM       | 57.93 | 79.97 | 79.97 |\\n| Reverse Attention | 20.68 | 20.68 | 20.68 |\\n| StoryTrans    | 88.62 | 88.62 | 88.62 |\\n\\n### Table 4: Ablation study results on English datasets. (-) indicates removing the component in proposed model. CE denote content enhancing, which means removing the second stage. More ablation results shown in Appendix E.\\n\\n| Models         | ZH-LX | ZH-JY | EN-SP |\\n|---------------|-------|-------|-------|\\n| **Proposed Model** | 88.62 | 52.41 | 32.20 |\\n| **(-) Ldis**  | 75.86 | 31.37 | 33.49 |\\n| **(-) Lstyle** | 50.68 | 7.93  | 45.00 |\\n| **(-) Lsop**  | 78.96 | 38.96 | 39.45 |\\n| **(-) CE**    | 92.41 | 73.10 | 21.62 |\\n\\n### Results on Human Evaluation\\n\\n| Models         | ZH-LX | ZH-JY | EN-SP |\\n|---------------|-------|-------|-------|\\n| Style Transformer | 1.02  | 2.95**| 2.91**|\\n| StyleLM       | 1.61  | 1.99  | 1.58  |\\n| Reverse Attention | 1.69  | 1.25  | 1.64  |\\n| StoryTrans    | 1.98**| 1.84  | 1.67  |\\n\\n\u03ba denotes Fleiss' kappa (Fleiss, 1971) to measure the inter annotator agreement (all are moderate or substantial). The scores marked with ** mean StoryTrans outperforms the baselines significantly with p-value<0.01 (sign test).\\n\\n### Results on Ablation Study\\n\\nAs shown in Table 4, we observe a significant drop in transfer accuracy without Ldis or Lstyle. Ldis works by disentangling stylistic features from the discourse representations, while Lstyle exerts direct supervision on styles of generated texts. Without Lsop, model can hardly capture discourse-level information and keeps more source tokens, leading to higher BLEU scores and lower accuracy. When removing the second stage, the lowers BLEU scores show the benefit of the mask-and-fill framework for content preservation.\"}"}
{"id": "acl-2023-long-827", "page_num": 8, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u67ef\u91cc\u6559\u6388\u72ec\u81ea\u6500\u767b\u4e86\u4e0a\u53bb\uff0c\u5230\u8fbe\u4e86\u5c71\u8109\u6700\u9ad8\u5cf0\u3002\u53ea\u542c\u5f97\u811a\u6b65\u58f0\u4f3c\u4e4e\u8d8a\u8d70\u8d8a\u8fd1\uff0c\u5176\u540e\u4f3c\u4e4e\u8fd8\u53d1\u51fa\u4e00\u4e9b\u5598\u606f\u4e4b\u58f0\u3002\\n\\n\u90ed\u9756\u53cc\u624b\u6491\u5728\u5899\u4e0a\uff0c\u8fdc\u8fdc\u671b\u53bb\uff0c\u67ef\u91cc\u548c\u6559\u6388\u7ad9\u4f4f\u4e86\uff0c\u53ea\u89c1\u90a3\u9053\u95e8\u7f13\u7f13\u95ed\u4e0a\uff0c\u5927\u96fe\u968f\u5373\u6563\u5f00\uff0c\u4ed6\u8fc8\u5f00\u4e86\u6b65\u5b50\uff0c\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u97f3\u4e5f\u968f\u4e4b\u6d88\u5931\u3002\u67ef\u91cc\u5148\u751f\u9053\uff1a\u201c\u518d\u4e5f\u4e0d\u613f\u72ec\u81ea\u79bb\u5f00\u4f60\u7f62\uff01\u201d\\n\\n\u4ed6\u8fde\u79fb\u52a8\u51e0\u4e0b\uff0c\u4fbf\u5728\u8fd9\u65f6\uff0c\u6731\u5148\u751f\u5927\u58f0\u8bf4\u9053\uff1a\u201c\u6709\u4eba\u5728\u4ed6\u6559\u6388\u7684\u5c71\u8109\u4e0a\u91cd\u91cd\u8e0f\u4e86\u4e00\u811a\uff01\u201d\\n\\n\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u4f46\u8fc7\u4e86\u826f\u4e45\uff0c\u4ed6\u5fae\u5598\u7740\u6c14\uff0c\u770b\u770b\u56db\u5468\uff0c\u53ea\u89c9\u5f97\u6709\u79cd\u602a\u5f02\u7684\u5bd2\u6c14\u4fb5\u5165\u4e86\u4ed6\u7684\u808c\u80a4\u3002\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u7eb5\u8eab\u800c\u8d77\uff0c\u4ece\u90a3\u4ee5\u540e\uff0c\u6731\u5148\u751f\u518d\u6ca1\u51fa\u73b0\u5728\u4ed6\u9762\u524d\u3002\\n\\n\u4ed6\u62ac\u5934\u671b\u4e86\u671b\uff0c\u53ea\u89c1\u4e00\u9053\u5149\u5f71\u98de\u8fc7\uff0c\u67ef\u91cc\u548c\u6559\u6388\u4e5f\u6d88\u5931\u5728\u4e86\u6d53\u96fe\u4e2d\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u7eb5\u8eab\u800c\u8d77\uff0c\u4ece\u90a3\u4ee5\u540e\uff0c\u6731\u5148\u751f\u518d\u6ca1\u51fa\u73b0\u5728\u4ed6\u9762\u524d\u3002\u4ed6\u62ac\u5934\u671b\u4e86\u671b\uff0c\u53ea\u89c1\u4e00\u9053\u5149\u5f71\u98de\u8fc7\uff0c\u67ef\u91cc\u548c\u6559\u6388\u4e5f\u6d88\u5931\u5728\u4e86\u6d53\u96fe\u4e2d\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65\u58f0\u6e10\u6e10\u6d88\u5931\uff0c\u56db\u5468\u9664\u4e86\u4ed6\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u8fd8\u80fd\u542c\u5230\u4ec0\u4e48\u58f0\u97f3\u3002\\n\\n\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa\uff0c\u4e0d\u7531\u5f97\u6492\u5f00\u4e86\u8863\u8896\uff0c\u51c6\u5907\u7528\u8896\u5b50\u6765\u906e\u6321\u3002\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a\uff0c\u6211\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002\u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u811a\u6b65"}
{"id": "acl-2023-long-827", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the number of words. As shown in Figure 3, the texts generated by Reverse Attention and StyleLM have similar stylistic features to source texts. In contrast, StoryTrans can better capture different stylistic features and transfer source texts to specified styles. More details are in Appendix F.\\n\\n5 Conclusion\\n\\nIn this paper, we present the first study for story author-style transfer and analyze the difficulties of this task. Accordingly, we propose a novel generation model, which explicitly disentangles the style information from high-level text representations to improve the style transfer accuracy, and achieve better content preservation by injecting style-specific contents. Automatic evaluations show StoryTrans outperform baselines on the overall performance. Further analysis shows StoryTrans has a better ability to capture linguistic features for style transfer.\\n\\nLimitations\\n\\nIn style transfer, content preservation and style transfer are adversarial. Long texts have richer contents and more abstract stylistic features. We also notice that content preservation is the main disadvantage of StoryTrans in automatics evaluation results. Case studies also indicate that StoryTrans can maintain some entities and the relations between entities. However, strong discourse-level style transfer ability endangered content preservation. In contrast, baselines such as Style Trans-former have better content preservation but hardly transfer the style. We believe that StoryTrans is still a good starting point for this important and challenging task.\\n\\nDuring preliminary experiments, we also manually inspected multiple author styles besides Shakespeare, such as Mark Twain. However, we found that their styles are not as obvious as Shakespeare, as shown in the following example. Therefore, we only selected authors with relatively distinct personal styles for our transfer experiments. In future work, we will expand our research and choose more authors with distinct styles for style transfer. For example, the style distinction between the following examples is not readily apparent.\\n\\n\u2022 Everyday story in our datatset: Ashley wanted to be a unicorn for Halloween. She looked all over for a unicorn costume. She wasn't able to find one.\\n\u2022 \\\"A Double Barrelled Detective Story\\\" by Mark Twain: You will go and find him. I have known his hiding-place for eleven years; it cost me five years and more of inquiry.\\n\\nEthics Statement\\n\\nWe perform English and Chinese experiments on public datasets and corpora. Specifically, English datasets come from ROCstories and Project Gutenberg. Moreover, Chinese datasets include the LOT dataset and public corpora of JY and LX. Automatic and manual evaluation demonstrate that our model outperforms strong baselines on both Chinese and English datasets. In addition, our model can be easily applied to different languages by substituting specific pre-trained language models. As for manual evaluation, we hired three native Chinese speakers as annotators to evaluate generated texts and did not ask about personal privacy or collect the personal information of annotators. We pay 1.8 yuan (RMB) per sample in compliance with Chinese wage standards. Considering it would cost an average of 1 minute for an annotator to score a sample, the payment is reasonable.\\n\\nAcknowledgments\\n\\nThis work was supported by the NSFC projects (Key project with No. 61936010). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2020GQG0005.\\n\\nReferences\\n\\nDzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015.\\n\\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python: analyzing text with the natural language toolkit. \\\"O'Reilly Media, Inc.\\\".\\n\\nNing Dai, Jianze Liang, Xipeng Qiu, and Xuanjing Huang. 2019. Style transformer: Unpaired text style transfer without disentangled latent representation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5997\u20136007, Florence, Italy. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\"}"}
{"id": "acl-2023-long-827", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-827", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Style-Specific Contents\\n\\nWe detail how we extract style-specific contents and explain how they are used from the following three aspects:\\n\\nWhat do we mean by \u201cstyle-specific content\u201d? We refer to \u201cstyle-specific content\u201d as those mainly used in texts with specific styles and should be retained after style transfer. For example, \u201cHarry Potter\u201d and \u201cHorcrux\u201d are style-specific since they are used only in J.K. Rowling-style stories. When transferring J.K. Rowling-style stories to\"}"}
{"id": "acl-2023-long-827", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"other styles, style-specific tokens shouldn't be changed. However, existing models tend to drop style-specific tokens since they are not trained to learn these tokens conditioned on other styles.\\n\\nHow do we extract \u201cstyle-specific contents\u201d? We extract style-specific contents by (1) obtaining top-10 salient tokens using TF-IDF, (2) reserving only people names (e.g., \u201cHarry Potter\u201d), place names (e.g., \u201cLondon\u201d), and proper nouns (e.g., \u201cHorcrux\u201d), and (3) filtering out high-frequency tokens in all corpus (e.g., \u201cLondon\u201d) since these tokens can be learned conditioned on every style. We regard the remaining tokens as style-specific contents.\\n\\nAs mentioned before, we employ the TF-IDF algorithm on the corpus to obtain rough style-specific contents for different styles, respectively. The reason for using TF-IDF: it is necessary to ensure that the extracted tokens are salient to the story plots. We extract style-specific tokens from the salient tokens using the second and third steps. Then, we use a part-of-speech tagging toolkit (e.g., NLTK) to identify function words and prepositions to retain people\u2019s names, place names, and proper nouns. Note that the frequency is an empirical value observed from datasets. However, the TF-IDF algorithm chooses the important words corresponding to the special style based on word frequency. There may be some style-unrelated words that are important to the content. Therefore, we need to filter out style-unrelated words. Concretely, we use Jieba/NLTK (Bird et al., 2009) to collect the word frequency for Chinese and English datasets, respectively. Moreover, we regard the words possessing a high frequency in all styles corpus as style-unrelated words. Specifically, We set tokens appearing in 10% samples in the dataset as high-frequency words. Then we filter out these words to obtain style-specific contents. The frequency value needs to be reset to apply the method to other datasets.\\n\\nHow are the \u201cstyle-specific contents\u201d used? One challenge of long-text style transfer is transferring discourse-level author style while preserving the main characters and storylines. It\u2019s difficult for existing models to transfer style-specific contents since they are not trained to learn these tokens conditioned on other styles. Therefore, we extract \u201cstyle-specific contents\u201d before style transferring and replace them with the special token \u201c<Mask>\u201d. Then, the \u201cstyle-specific contents\u201d will be filled in the second stage, as shown in Figure 1.\\n\\nData Pre-Processing\\nDue to lack of stylized author datasets, we collected several authors\u2019 corpus to construct new datasets. As for Chinese, we extracted paragraphs from 21 novels of LuXun (LX) and 15 novels of JinYong (JY), and fairy tales collected by Guan et al. (2021a). On the other hand, the English dataset consists of everyday stories from ROCStories (Mostafazadeh et al., 2016) and fragments from Shakespeare\u2019s plays. Each fragment of Shakespeare\u2019s plays comprises multiple consecutive sentences and as long as samples in ROCStories. We collect the Shakespeare-style texts from the Shakespeare corpus in Project Gutenberg under the Project Gutenberg License. We use Jieba/NLTK (Bird et al., 2009) for word tokenization for the Chinese/English dataset in data preprocessing. In addition, these data are public corpora, and we also check the information for anonymization.\\n\\nRegarding the limitation of modern language models, the length of samples is also limited. We set the max length as 384 and 90 for Chinese and English, respectively. Each sample has 4 sentences at least. We choose above length to balance the data length of different styles. Additionally, we filtered the texts which are too long to generate or too short to unveil author writing style. As Figure 4 shows, texts in the Chinese dataset span a diverse range of length.\\n\\nDifferent Style Samples\\nIn process of constructing datasets, we try to collect different author corpus who have a gap in writing styles. As shown in Table 8, the JY-style texts mostly describe martial arts actions and construct interesting plots, while the LX-style texts focus on realism with profound descriptive and critical significance. And the fairy tales differ from these texts in terms of topical and discourse features. In the English datasets, the Shakespeare-style texts are flamboyant and contain elaborate metaphors and ingenious ideas, which the everyday stories are written in plain language and without rhetoric.\"}"}
{"id": "acl-2023-long-827", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Once, when Professor Curry was climbing the highest peak of the Cairngo Mountains alone, he heard some huge footsteps and panting behind him. He immediately stopped and looked around. No one could see him. He realized that there might be some terrified beast behind him. He was so frightened that he could not help turning around in one breath. That time, there was a loud bang in front of him, and at the same time, there was a sound of panting and shouting from a distance. He did not dare to turn around but continued to climb forward. He could not see any trace of him.\\n\\nSince the publication of \u201cNew Youth,\u201d everyone has ridiculed the reform in response to it, later approved of it, and then ridiculed the reformers. This year, oranges are hanging on the branches again. The rich man\u2019s daughter was drooling. Then, she couldn\u2019t help picking one, and just after a bite, she was unconscious. The rich man was remorseful, so he plucked all the oranges from the tree and gave them to neighbors and friends. They thought that the current situation was not much different from that at that time, and they could still be preserved, so they edited them for critic. But they were criticized by pen and paper mouthpieces. Therefore, my timely and superficial writing should also be ignored and wiped out. However, a few renowned authors who have a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he immediately went to the side of the hill and found a deep hole. He lifted the horse and jumped into the hole. Flappity, flappity, flappity! His body was thrown up and down as if run by a wheel. He could not help heaving a sigh of relief. The horse\u2019s head reached out to grab the horse\u2019s rein with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nYang Guo grabbed the horse\u2019s reins with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nBoth sides are still there.\\n\\nThis is what I am saddened by. I think any attack on the evils of the times, the writing must perish at the same time as the evils of the times, and this is also the reason why my writing was not seen. My timely and superficial writing should also be ignored and wiped out. However, a few renowned authors who have a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he immediately went to the side of the hill and found a deep hole. He lifted the horse and jumped into the hole. Flappity, flappity, flappity! His body was thrown up and down as if run by a wheel. He could not help heaving a sigh of relief. The horse\u2019s head reached out to grab the horse\u2019s rein with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nYang Guo grabbed the horse\u2019s reins with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nThis is what I am saddened by. I think any attack on the evils of the times, the writing must perish at the same time as the evils of the times, and this is also the reason why my writing was not seen. My timely and superficial writing should also be ignored and wiped out. However, a few renowned authors who have a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he immediately went to the side of the hill and found a deep hole. He lifted the horse and jumped into the hole. Flappity, flappity, flappity! His body was thrown up and down as if run by a wheel. He could not help heaving a sigh of relief. The horse\u2019s head reached out to grab the horse\u2019s rein with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nYang Guo grabbed the horse\u2019s reins with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword.Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nThis is what I am saddened by. I think any attack on the evils of the times, the writing must perish at the same time as the evils of the times, and this is also the reason why my writing was not seen. My timely and superficial writing should also be ignored and wiped out. However, a few renowned authors who have a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he immediately went to the side of the hill and found a deep hole. He lifted the horse and jumped into the hole. Flappity, flappity, flappity! His body was thrown up and down as if run by a wheel. He could not help heaving a sigh of relief. The horse\u2019s head reached out to grab the horse\u2019s rein with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nThis is what I am saddened by. I think any attack on the evils of the times, the writing must perish at the same time as the evils of the times, and this is also the reason why my writing was not seen. My timely and superficial writing should also be ignored and wiped out. However, a few renowned authors who have a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he immediately went to the side of the hill and found a deep hole. He lifted the horse and jumped into the hole. Flappity, flappity, flappity! His body was thrown up and down as if run by a wheel. He could not help heaving a sigh of relief. The horse\u2019s head reached out to grab the horse\u2019s rein with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nThis is what I am saddened by. I think any attack on the evils of the times, the writing must perish at the same time as the evils of the times, and this is also the reason why my writing was not seen. My timely and superficial writing should also be ignored and wiped out. However, a few renowned authors who have a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he immediately went to the side of the hill and found a deep hole. He lifted the horse and jumped into the hole. Flappity, flappity, flappity! His body was thrown up and down as if run by a wheel. He could not help heaving a sigh of relief. The horse\u2019s head reached out to grab the horse\u2019s rein with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nThis is what I am saddened by. I think any attack on the evils of the times, the writing must perish at the same time as the evils of the times, and this is also the reason why my writing was not seen. My timely and superficial writing should also be ignored and wiped out. However, a few renowned authors who have a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he immediately went to the side of the hill and found a deep hole. He lifted the horse and jumped into the hole. Flappity, flappity, flappity! His body was thrown up and down as if run by a wheel. He could not help heaving a sigh of relief. The horse\u2019s head reached out to grab the horse\u2019s rein with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nThis is what I am saddened by. I think any attack on the evils of the times, the writing must perish at the same time as the evils of the times, and this is also the reason why my writing was not seen. My timely and superficial writing should also be ignored and wiped out. However, a few renowned authors who have a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he immediately went to the side of the hill and found a deep hole. He lifted the horse and jumped into the hole. Flappity, flappity, flappity! His body was thrown up and down as if run by a wheel. He could not help heaving a sigh of relief. The horse\u2019s head reached out to grab the horse\u2019s rein with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\\n\\nThis is what I am saddened by. I think any attack on the evils of the times, the writing must perish at the same time as the evils of the times, and this is also the reason why my writing was not seen. My timely and superficial writing should also be ignored and wiped out. However, a few renowned authors who have a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he immediately went to the side of the hill and found a deep hole. He lifted the horse and jumped into the hole. Flappity, flappity, flappity! His body was thrown up and down as if run by a wheel. He could not help heaving a sigh of relief. The horse\u2019s head reached out to grab the horse\u2019s rein with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and he slowly walked to the corner to pick up the long sword. Using soft objects to display strength is the stronger side. King, I do not fear it, I have seen you both: But since he is bettered, we therefore have odds. Later, this is too heavy, let me play the double sword for a while, and then I will come back.\"}"}
{"id": "acl-2023-long-827", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To explore the effect of the proposed component, we also conduct more ablation studies on Chinese datasets. As shown in Table 9, the ablation of $L_{dis}$ leads to better style accuracy, which shows different trends comparing with English dataset. We conjecture that $L_{dis}$ aims to maintain the content and reduce style information. Without $L_{dis}$, the powerful $L_{style}$ leads the StoryTrans to degenerate to style conditional language model. Furthermore, the ablation of $L_{style}$ also confirms the powerful ability of style control as in previous paper. And we find that when removing $L_{sop}$, the model loses the ability to transfer at the discourse level and has only learned token-level copy.\\n\\nIn order to investigate whether our StoryTrans indeed rephrase the expression of texts, we employ surface elements of text to show author writing styles. And the surface element are associated with statistical observations. For example, the small average length of sentences show the author preference to write a short sentence, and more question marks indicate the author accustomed to using questions. To this end, we use the number of (1) commas, (2) colons, (3) sentences in a paragraph, (4) question mark (5) left quotation mark, (6) right quotation mark, and (7) average number of words in a sentence to quantify surface elements into a 7 dimension vector. Then we leverage the t-SNE to visualize the golden texts and transferred texts. As shown in Figure 3, different style distribute separately across the style space. This proves JY, LX and fairy tale in Chinese dataset have a gap in writing style. And Figure 5 shows the transferred texts fall in golden texts in style space, indicating StoryTrans successfully transferred the writing style.\\n\\nIn addition to automatic evaluation, we conduct manual evaluation on generated texts. As mentioned before, we require the annotators to score each aspect from 1 (the worst) to 3 (the best). As for payment, we pay 1.8 yuan (RMB) per sample in compliance with Chinese wage standards. Our annotators consist of undergraduate students who are experienced in reading texts written in the styles of the respective authors (JY and LX). To ensure they fully understand the evaluation metrics, we conducted case analyses with them. Our scoring rubric assigns 1, 2, or 3 points to the transferred text based on the proportion of sentences meeting the following criteria (1/3, 2/3, or 3/3):\\n\\n- Style Accuracy: whether the transferred text conforms to the corresponding style.\\n- Content Preservation: whether the source content, such as character names, are retained.\\n- Coherence: whether the sentences in the transferred text are semantically connected.\\n\\nAnd we compute the final score of each text by averaging the scores of three annotators. As illustrated in manual evaluation, we observe that the results mainly conform with the automatic evaluation. Our StoryTrans obtained the highest score on the style accuracy in both transferred directions by a sign test compared to the other baselines, showing its stable ability of style control.\"}"}
{"id": "acl-2023-long-827", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Target Styles | Model | r-Acc | a-Acc | BLEU-1 | BLEU-2 | BS-P | BS-R | BS-F1 | BL-Overall | BS-Overall |\\n|--------------|-------|-------|-------|--------|--------|------|------|------|------------|------------|\\n| ZH-LX        |       |       |       |        |        |      |      |      |            |            |\\n| Proposed Model | 97.66 | 59.94 | 32.19 | 14.44  | 68.53  | 70.48| 69.45|      | 37.38      | 64.52      |\\n| (--)Ldis     | 99.86 | 92.59 | 20.36 | 5.45   | 63.37  | 62.96| 63.14|      | 34.56      | 76.46      |\\n| (--)Lstyle   | 88.06 | 12.20 | 43.09 | 23.88  | 75.44  | 75.68| 75.53|      | 20.21      | 30.35      |\\n| (--)Lsop     | 87.10 | 2.05  | 54.38 | 32.95  | 81.19  | 79.77| 80.42|      | 9.46       | 12.83      |\\n| ZH-JY        |       |       |       |        |        |      |      |      |            |            |\\n| Proposed Model | 84.49 | 62.96 | 30.71 | 14.5   | 68.76  | 71.69| 70.16|      | 37.72      | 66.46      |\\n| (--)Ldis     | 97.53 | 92.59 | 18.49 | 4.85   | 62.17  | 65.42| 63.73|      | 32.87      | 76.81      |\\n| (--)Lstyle   | 61.86 | 40.87 | 39.78 | 21.97  | 73.73  | 75.42| 74.52|      | 35.52      | 55.18      |\\n| (--)Lsop     | 61.72 | 10.83 | 51.29 | 30.98  | 79.65  | 79.82| 79.72|      | 21.10      | 29.38      |\\n\\nTable 9: More ablation study results on Chinese datasets. (--) indicates removing the component in proposed model.\\n\\nZH-LX/ZH-JY is the Chinese author LuXun/JinYong, respectively.\\n\\nof StoryTrans is comparable with StyleLM and slightly higher than Reverse Attention, demonstrating that StoryTrans can keep the main semantics of input. In terms of coherence, the score of StoryTrans is also comparable with baselines, showing some room for improvement. As discussed before, Style Transformer tends to copy the input, leading to the highest performance in content preservation and coherence. In summary, human evaluation depicts the strength of StoryTrans not only on style control but also on overall performance, indicating a balance of these metrics.\\n\\nMore Case Studies\\n\\nWe show more cases in Table 7. Comparing source text with Style Transformer, Style Transformer copies the input and only changes little tokens. This result also confirms with highest BLEU and BERTScore in automatic results. Like StyleLM, Reverse Attention also incorporates some target author content into generated texts. However, Reverse Attention inserts too much content that overpowers original plots. Furthermore, some critical entities (e.g., character name, \\\"\u67ef\u91cc\u6559\u6388/Professor Curry\\\" \u2192 \\\"\u67ef\u9547\u6076/Ke Zhen'e\\\") are revised to the similar word on in target author corpus. To maintain the story coherence, these important entities should stay the same. In summary, the token-level transfer may destroy the essential plots and damage the coherence.\"}"}
{"id": "acl-2023-long-827", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nB Did you use or create scientific artifacts?\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\u25a1 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC Did you run computational experiments?\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
