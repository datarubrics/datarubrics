{"id": "acl-2023-long-93", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: The train-test overlap of 3 MCD splits for JA and ZH together with their EN source patterns. We present the union of the 3 intersections, from which we observe 3 types of structures (part-of-speech tags in blue) leading to structural \u201csimplification\u201d. We provide concrete examples (green) of the structures and their common SPARQL fragments. EN possesses multiple structures for each of the fragments, while JA and ZH possess only one (considering the specific context).\"}"}
{"id": "acl-2023-long-93", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\nThe Limitations section follows the Conclusion section.\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\nOur work only provides a benchmark to evaluate semantic parsing models and not an application that can be used for potentially risky purposes.\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\nAbstract and Section 1 (Introduction).\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\nChatGPT was used for confirming that some concepts are properly described in the paper (specifically, for appendix A). Hence no specific content in the paper is created by the writing assistants.\\n\\nB \u25a1 Did you use or create scientific artifacts? 3, 4 (created MCWQ-R), 5\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n3 (MCWQ), 4 (URBANS), 5 (mT5, mBART, ZX-PARSE)\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\nMCWQ is released under the CC-BY license. URBANS is released under the Apache 2.0 license.\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n1 7 (the introduction and conclusion specified our intended use of MCWQ-R and the toolkit used to generate the dataset)\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\nNot applicable. Left blank.\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n4\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n4\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-93", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Did you run computational experiments?\\n\\n1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\n2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\n3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\n4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nDid you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\n1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\n2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\n3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\n4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\n5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\"}"}
{"id": "acl-2023-long-93", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On Evaluating Multilingual Compositional Generalization with Translated Datasets\\n\\nZi Wang1,2 and Daniel Hershcovich1\\n\\n1 Department of Computer Science, 2 Department of Nordic Studies and Linguistics\\nUniversity of Copenhagen\\n{ziwa, dh}@di.ku.dk\\n\\nAbstract\\nCompositional generalization allows efficient learning and human-like inductive biases. Since most research investigating compositional generalization in NLP is done on English, important questions remain underexplored. Do the necessary compositional generalization abilities differ across languages? Can models compositionally generalize cross-lingually? As a first step to answering these questions, recent work used neural machine translation to translate datasets for evaluating compositional generalization in semantic parsing. However, we show that this entails critical semantic distortion. To address this limitation, we craft a faithful rule-based translation of the MCWQ dataset (Cui et al., 2022) from English to Chinese and Japanese. Even with the resulting robust benchmark, which we call MCWQ-R, we show that the distribution of compositions still suffers due to linguistic divergences, and that multilingual models still struggle with cross-lingual compositional generalization. Our dataset and methodology will be useful resources for the study of cross-lingual compositional generalization in other tasks.\\n\\n1 Introduction\\nA vital ability desired for language models is compositional generalization (CG), the ability to generalize to novel combinations of familiar units (Oren et al., 2020). Semantic parsing enables executable representation of natural language utterances for knowledge base question answering (KBQA; Lan et al., 2021). A growing amount of research has been investigating the CG ability of semantic parsers based on carefully constructed datasets, typically synthetic corpora (e.g., CFQ; Keysers et al., 2019) generated based on curated rules, mostly within monolingual English scenarios. As demonstrated by Perevalov et al. (2022), resource scarcity for many languages largely preclude their speakers' access to knowledge bases (even for languages they include), and KBQA in multilingual scenarios is barely researched mainly due to lack of corresponding benchmarks.\\n\\nCui et al. (2022) proposed Multilingual Compositional Wikidata Questions (MCWQ) as the first semantic parsing benchmark to address the mentioned gaps. Google Translate (GT; Wu et al., 2016), a Neural Machine Translation (NMT) system trained on large-scale corpora, was adopted in creating MCWQ. We argue that meaning preservation during translation is vulnerable in this methodology especially considering the synthetic nature of the compositional dataset. Furthermore, state-of-the-art neural network models fail to capture structural systematicity (Hadley, 1994; Lake and Baroni, 2018; Kim and Linzen, 2020).\\n\\nSymbolic (e.g., rule-based) methodologies allow directly handling CG and were applied both to generate benchmarks (Keysers et al., 2019; Kim...\"}"}
{"id": "acl-2023-long-93", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and Linzen, 2020; Tsarkov et al., 2021) and to in-\\nject inductive bias to state-of-the-art models (Guo\\net al., 2020; Liu et al., 2021a). This motivates\\nus to extend this idea to cross-lingual transfer of\\nbenchmarks and models. We propose to utilize\\nrule-based machine translation (RBMT) to create\\nparallel versions of MCWQ and yield a robust mul-\\ntilingual benchmark measuring CG. We build an\\nMT framework based on synchronous context-free\\ngrammars (SCFG) and create new Chinese and\\nJapanese translations of MCWQ questions, which\\nwe call MCWQ-R (Multilingual Compositional\\nWikidata Questions with Rule-based translations).\\nWe conduct experiments on the datasets translated\\nwith GT and RBMT to investigate the effect of\\ntranslation method and quality on CG in multilin-\\ngual and cross-lingual scenarios.\\n\\nOur specific contributions are as follows:\\n\u2022 We propose a rule-based method to faithfully\\nand robustly translate CG benchmarks.\\n\u2022 We introduce MCWQ-R, a CG benchmark for\\nsemantic parsing from Chinese and Japanese\\nto SPARQL.\\n\u2022 We evaluate the translated dataset through\\nboth automatic and human evaluation and\\nshow that its quality greatly surpasses that\\nof MCWQ (Cui et al., 2022).\\n\u2022 We experiment with two different semantic\\nparsing architectures and provide an analy-\\nsis of their CG abilities within language and\\nacross languages.\\n\\n2 Related Work\\n\\nCompositional generalization benchmarks.\\nMuch previous work on CG investigated how to\\nmeasure the compositional ability of semantic\\nparsers. Lake and Baroni (2018) and Bastings\\net al. (2018) evaluated the CG ability of sequence-\\nto-sequence (seq2seq) architectures on natural\\nlanguage command and action pairs. Keysers et al.\\n(2019) brought this task to a realistic scenario of\\nKBQA by creating a synthetic dataset of questions\\nand SPARQL queries, CFQ, and further quan-\\ntified the distribution gap between training and\\nevaluation using compound divergence, creating\\nmaximum compound divergence (MCD) splits to\\nevaluate CG. Similarly, Kim and Linzen (2020)\\ncreated COGS in a synthetic fashion following\\na stronger definition of training-test distribution\\ngap. Goodwin et al. (2022) benchmarked CG\\nin dependency parsing by introducing gold\\ndependency trees for CFQ questions. For this\\npurpose, a full coverage context-free grammar over\\nCFQ was constructed benefiting from the synthetic\\ndata generation and splitting strategy, rule-based\\napproaches are commonly adopted for dataset\\ngeneration; as Kim and Linzen (2020) put it, such\\napproaches allow maintaining \u201cfull control over\\nthe distribution of inputs\u201d, the crucial factor for\\nvalid compositionality measurement. In contrast,\\nCui et al. (2022) created MCWQ through a process\\nincluding knowledge base migration and question\\ntranslation through NMT, without full control over\\ntarget language composition distribution. We aim\\nto remedy this in our paper by using RBMT.\\n\\nRule-based machine translation.\\nOver decades\\nof development, various methodologies and tech-\\nnologies were introduced for the task of Machine\\nTranslation (MT). To roughly categorize the most\\npopular models, we can divide them into pre-neural\\nmodels and neural-based models. Pre-neural MT\\n(Wu, 1996; Marcu and Wong, 2002; Koehn et al.,\\n2003; Chiang, 2005) typically includes manipula-\\ntion of syntax and phrases, whereas neural-based\\nMT (Kalchbrenner and Blunsom, 2013; Cho et al.,\\n2014; Vaswani et al., 2017) refers to those em-\\nploying neural networks. However, oriented to\\ngeneral broad-coverage applications, most models\\nrely on learned statistical estimates, even for the\\npre-neural models. The desiderata in our work, on\\nthe other hand, exclude methods with inherent un-\\ncertainty. The most relevant methods were by Wu\\n(1996, 1997) who applied SCFG variants to MT\\n(Chiang, 2006). The SCFG is a generalization of\\nCFG (context-free grammars) generating coupled\\nstrings instead of single ones, exploited by pre-\\nneural MT works for complex syntactic reordering\\nduring translation. In this work, we exclude the sta-\\ntistical component and manually build the SCFG\\ntransduction according to the synthetic nature of\\nCFQ; we specifically call it \u201crule-based\u201d instead of\\n\u201csyntax-based\u201d to emphasize this subtle difference.\\n\\nMultilingual benchmarks.\\nCross-lingual learn-\\ning has been increasingly researched recently,\\nwhere popular technologies in NLP are generally\\nadapted for representation learning over multiple\\nlanguages (Conneau et al., 2020; Xue et al., 2021).\\nMeanwhile, transfer learning is widely leveraged\"}"}
{"id": "acl-2023-long-93", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The pipeline of dataset generation. The circled numbers refer to (1) parsing question text, (2) building the dictionary and revising the source grammar and corresponding transduction rules based on parse trees, (3) replacing and reordering constituents, (4) translating lexical units, (5) post-processing and grounding in Wikidata.\\n\\nTo overcome the data scarcity of low-resource languages (Cui et al., 2019; Hsu et al., 2019). However, cross-lingual benchmarks datasets, against which modeling research is developed, often suffer from \u201ctranslation artifacts\u201d when created using general machine translation systems (Artetxe et al., 2020; Wintner, 2016). Longpre et al. (2021) proposed MKQA, a large-scale multilingual question answering corpus (yet not for evaluating CG) avoiding this issue, through enormous human efforts. In contrast, Cui et al. (2022) adopted Google Translate to obtain parallel versions for CFQ questions while sacrificing meaning preservation and systematicity. We propose a balance between the two methodologies, with automatic yet controlled translation. In addition, our work further fills the data scarcity gap in cross-lingual semantic parsing, being the first CG benchmark for semantic parsing for Japanese.\\n\\n3 Multilingual Compositional Wikidata Questions (MCWQ)\\n\\nMCWQ (Cui et al., 2022) is the basis of our work. It comprises English questions inherited from CFQ (Keysers et al., 2019) and the translated Hebrew, Chinese and Kannada parallel questions based on Google Cloud Translate, an NMT system. The questions are associated with SPARQL queries against Wikidata, which were migrated from Freebase queries in CFQ. Wikidata is an open knowledge base where each item is allocated a unique, persistent identifier (QID).\\n\\nDue to the translation method employed in MCWQ, it suffers from detrimental inconsistencies for CG evaluation (see Figures 1 and 3)\u2014mainly due to the unstable mapping from source to target languages performed by NMT models at both the lexical and structural levels. We discuss the consequences with respect to translation quality in \u00a74.3 and model performance in \u00a76.\\n\\n4 MCWQ-R: A Novel Translated Dataset\\n\\nAs stated in \u00a72, data generation with GT disregards the \u201ccontrol over distribution\u201d, which is crucial for CG evaluation (Keysers et al., 2019; Kim and Linzen, 2020). Thus, we propose to diverge from the MCWQ methodology by translating the dataset following novel grammar of the involved language pairs to guarantee controllability during translation. Such controllability ensures that the translations are deterministic and systematic. In this case, generalization is exclusively evaluated with respect to compositionality, avoiding other confounds. We create new instances of MCWQ in Japanese and Chinese, two typologically distant languages from English, sharing one common language (Chinese) with the existing MCWQ. To make comprehensive experimental comparisons between languages, we also use GT to generate Japanese translations (which we also regard as a part of MCWQ in this paper), following the same method as MCWQ.\\n\\nIn this section, we describe the proposed MCWQ-R dataset. In \u00a74.1 we describe the pro-\"}"}
{"id": "acl-2023-long-93", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Example of an MCWQ (Cui et al., 2022) item in JSON format (top) and two fields of the corresponding MCWQ-R item (bottom). We present part of the fields: the English and SPARQL fields inherited from CFQ and the Chinese fields. Specifically, we show an incorrectly translated example in MCWQ where \u201cexecutive produce\u201d is not translated as a composition while MCWQ-R keeps good consistency with English.\\n\\n4.1 Generation Methodology\\n\\nThe whole process of the dataset generation is summarized in Figure 2. We proceed by parsing the English questions, building bilingual dictionaries, a source grammar and transduction rules, replacing and reordering constituents, translating lexical units, post-processing and grounding in Wikidata.\\n\\nGrammar-based transduction.\\n\\nWe base our method on Universal Rule-Based Machine Translation (URBANS; Nguyen, 2021), an open-source toolkit supporting deterministic rule-based translation with a bilingual dictionary and grammar rule transduction, based on NLTK (Bird and Loper, 2004). We modify it to a framework supporting synchronous context-free grammar (SCFG; Chiang, 2006) for practical use, since the basic toolkit lacks links from non-terminals to terminals preventing the lexical multi-mapping. A formally defined SCFG variant is symmetrical regarding both languages (Wu, 1997), while we implement a simplified yet functionally identical version only for one-way transduction. Our formal grammar framework consists of three modules: a set of source grammar rules converting English sentences to parse trees, the associated transduction rules hierarchically reordering the grammar constituents with tree manipulation and a tagged dictionary mapping tokens into the target language based on their part-of-speech (POS) tags. The tagged dictionary here provides links between the non-terminals and terminals defined in a general CFG (Williams, 2016). Context information of higher syntactic levels is encapsulated in the POS tags and triggers different mappings to the target terms via the links. This mechanism enables our constructed grammar to largely address complex linguistic differences (polysemy and inflection for instance) as a general SCFG does. We construct the source grammar as well as associated transduction rules and dictionaries, resulting in two sets of transduction grammars for Japanese and Chinese respectively.\\n\\nSource grammar.\\n\\nThe synthetic nature of CFQ (Keysers et al., 2019) indicates that it has limited sentence patterns and barely causes ambiguities; Goodwin et al. (2022) leverage this feature and construct a full coverage CFG for the CFQ language, which provides us with a basis of source grammar. We revise this monolingual CFG to satisfy the necessity for translation with an \u201cextensive\u201d strategy, deriving new tags for constituents at the lowest syntactic level where the context accounts for multiple possible lexical mappings.\\n\\nBridging linguistic divergences.\\n\\nThe linguistic differences are substantial between the source language and the target languages in our instances. The synthetic utterances in CFQ are generally cultural-invariant and not entailed with specific language style, therefore the problems here are primarily ascribed to the grammatical differences and lexical gaps. For the former, our grammar performs systematic transduction on the syntactic structures; for the latter, we adopt a pattern match-substitution strategy as post-processing for the lexical units applied in a different manner from the others in the target languages. We describe concrete examples in Appendix A. Without the consequence of probability, the systematic transductions simply bridge the linguistic gaps without further exploration.\"}"}
{"id": "acl-2023-long-93", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of MCWQ-R and the corresponding branches of MCWQ. We count question patterns with mod entities (Keysers et al., 2019), the form directly processed during translation, and question-query pairs comprising the question patterns and the associated SPARQL queries with mod entities.\\n\\n4.2 Dataset Statistics\\n\\nDue to the shared source data, the statistics of MCWQ-R are largely kept consistent with MCWQ. Specifically, the two datasets have the same amounts of unique questions (UQ; 124,187), unique queries (101,856, 82% of UQ) and query patterns (86,353, 69.5% of UQ). A substantial aspect nonetheless disregarded was the language-specific statistics, especially those regarding question patterns. As shown in Table 1, for both MCWQ and MCWQ-R, we observe a decrease in question patterns in translations compared with English and the corresponding pairs coupled with SPARQL queries, i.e., question-query pairs. This indicates that the patterns are partially collapsed in the target languages with both methodologies. Furthermore, as the SPARQL queries are invariant logical representations underlying the semantics, the QA pairs are supposed to be consistent with the question patterns even if collapsed. However, we notice a significant inconsistency (\u2206JA = 240; \u2206ZH = 578) between the two items in MCWQ while there are few differences (\u2206JA = 0; \u2206ZH = 9) in MCWQ-R. This further implicates a resultant disconnection between the translated questions and corresponding semantic representations with NMT.\\n\\n4.3 Translation Quality Assessment\\n\\nFollowing Cui et al. (2022), we comprehensively assess the translation quality of MCWQ-R and the GT counterpart based on the test-intersection set (the intersection of the test sets of all splits) samples. While translation quality is a general concept, in this case, we focus on how appropriately the translation trades off fluency and faithfulness to the principle of compositionality.\\n\\nTable 2: Assessment scores for the translations. MP refers to Meaning Preservation and F refers to Fluency. The prefix avg indicates averaged scores. P(MP, F \u2265 3) refers to the proportion of questions regarded as acceptable.\\n\\nLanguage Reference Manual & Method BLEU avgMP avgF P(MP, F \u2265 3)\\n\\n| Language | RBMT       | GT          |\\n|----------|------------|-------------|\\n| JA       | 97.1       | 45.1        |\\n| ZH       | 94.4       | 47.2        |\\n\\nReference-based assessment. We manually translate 155 samples from the test-intersection set in a faithful yet rigid manner as gold standard before the grammar construction. We calculate BLEU (Papineni et al., 2002) scores of the machine-translated questions against the gold set with sacreBLEU (Post, 2018), shown in Table 2. Our RBMT reached 97.1 BLEU for Japanese and 94.4 for Chinese, indicating a nearly perfect translation as expected. While RBMT could ideally reach a full score, the loss here is mainly caused by samples lacking context information (agnostic of entity...\"}"}
{"id": "acl-2023-long-93", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for instance). In addition, we observe that GT obtained fairly poor performance with 45.1 BLEU for Japanese, which is significantly lower than the other branches in MCWQ (87.4, 76.6, and 82.8 for Hebrew, Kannada, and Chinese, respectively; Cui et al., 2022). The main reason for this gap is the different manner in which we translated the gold standard: the human translators in MCWQ took a looser approach.\\n\\nManual assessment.\\n\\nWe manually assess the translations of 42 samples (for each structural complexity level defined by Keysers et al., 2019) in terms of meaning preservation (MP) and fluency (F) with a rating scale of 1\u20135. As shown in Table 2, our translations have significantly better MP than GT, which is exhibited by the average scores (1.1 and 1.3 higher in avgMP for Japanese and Chinese, respectively). However, the methods obtain similar fluency scores, indicating that both suffer from unnatural translations, partially because of the unnaturalness of original English questions (Cui et al., 2022). RBMT produces only few translations with significant grammar errors and semantic distortions, while GT results in 28.6% of unacceptable translations in this respect. Such errors occur on similar samples for the two languages, suggesting a systematicity in GT failure. We include details of manual assessment in Appendix B.\\n\\n5 Experiments\\n\\nWhile extensive experiments have been conducted on both the monolingual English (Keysers et al., 2019) and the GT-based multilingual benchmarks (Cui et al., 2022), the results fail to demonstrate pure multilingual CG due to noisy translations. Consistent with prior work, we experiment in both monolingual and cross-lingual scenarios. Specifically, we take into consideration both RBMT and GT branches in the experiments for further comparison.\\n\\n5.1 Within-language Generalization (Monolingual)\\n\\nCui et al. (2022) showed consistent ranking among sequence-to-sequence (seq2seq) models for the 4 splits (3 MCD and 1 random splits). We fine-tune and evaluate the pre-trained mT5-small (Xue et al., 2021), which performs well on MCWQ for each monolingual dataset. In addition, we train a model using mBART50 (Tang et al., 2020) as a frozen embedder and learned Transformer encoder and decoder, following Liu et al. (2020). We refer to this model as mBART50\u2217 (it is also the base architecture of ZX-Parse; see \u00a75.2).\\n\\nWe show the monolingual experiment results in Table 3. The models achieve better average performance on RBMT questions than GT ones. This meets our expectations since the systematically translated questions excluded the noise. On the random split, both RBMT branches are highly consistent with English, while noise in GT data lowers accuracy. However, the comparisons on MCD splits show that RBMT branches are less challenging than English, especially for mT5-small. In \u00a76.1, we show this is due to the \u201csimplifying\u201d effect of translation on composition.\\n\\nComparisons across languages demonstrate another interesting phenomenon: Japanese and Chinese exhibited an opposite relative difficulty on RBMT and GT. It is potentially due to the more extensive grammatical system (widely applied in different realistic scenes) of the Japanese language, while the grammatical systems and language styles are unified in RBMT, the GT tends to infer such diversity which nonetheless belongs to another category (natural language variant; Shaw et al., 2021).\\n\\n|          | mT5-small | mBART50\u2217 | Match(%) | MCWQ-R | MCWQ | MCWQ-R | MCWQ |\\n|----------|-----------|----------|----------|--------|------|--------|------|\\n| EN mean  | 38.3      | 39.5     | 55.4 \u00b1 1.6 | 55.2 \u00b1 1.6 | 55.2 \u00b1 1.6 | 55.2 \u00b1 1.6 |\\n| JA mean  | 56.3      | 58.3     | 30.8 \u00b1 3.6 | 30.8 \u00b1 3.6 | 30.8 \u00b1 3.6 | 30.8 \u00b1 3.6 |\\n| ZH mean  | 51.1      | 59.9     | 36.3 \u00b1 3.6 | 36.3 \u00b1 3.6 | 36.3 \u00b1 3.6 | 36.3 \u00b1 3.6 |\\n\\nTable 3: Monolingual experiment results: Exact match accuracies in percentage (%) are shown here. We present the model performance on the two translated datasets, which share the English branch. MCD mean represents the average accuracy across 3 MCD splits, and the detailed results breakdown can be found in Appendix D.1. Random refers to the results on the random split. We run 3 replicates for mBART50\u2217 on EN, which is used for further cross-lingual experiments (see \u00a75.2).\\n\\n5.2 Cross-lingual Generalization (Zero-shot)\\n\\nWe mentioned the necessity of developing multilingual KBQA systems in \u00a71. Enormous efforts required for model training for every language en-\"}"}
{"id": "acl-2023-long-93", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"courage us to investigate the zero-shot cross-lingual generalization ability of semantic parsers which serve as the KBQA backbone. While similar experiments were conducted by Cui et al. (2022), the adopted pipeline (cross-lingual inference by mT5 fine-tuned on English) exhibited negligible predictive ability for all the results, from which we can hardly draw meaningful conclusions.\\n\\nFor our experiments, we retain this as a base-line, and additionally train Zero-shot Cross-lingual Semantic Parser (ZX-Parse), a multi-task seq2seq architecture proposed by Sherborne and Lapata (2022). The architecture consists of mBART50 with two auxiliary objectives (question reconstruction and language prediction) and leverages gradient reversal (Ganin et al., 2016) to align multilingual representations, which results in a promising improvement in cross-lingual SP.\\n\\nWith the proposed architecture, we investigate how the designed cross-lingual parser and its representation alignment component perform on the compositional data. Specifically, we experiment with both the full ZX-Parse and with mBART50, its logical-form-only version (without auxiliary objectives). For the auxiliary objectives, we use bi-text from MKQA (Longpre et al., 2021) as supportive data. See Appendix C for details.\\n\\nTable 4 shows our experimental results. mT5-small fine-tuned on English fails to generate correct SPARQL queries. ZX-Parse, with a frozen mBART50 encoder and learned decoder, demonstrates moderate predictive ability. Surprisingly, while the logical-form-only (mBART50) architecture achieves fairly good performance both within English and cross-lingually, the auxiliary objectives cause a dramatic decrease in performance. We discuss this in \u00a76.2\\n\\n6 Discussion\\n\\n6.1 Monolingual Performance Gap\\n\\nAs Table 3 suggests, MCWQ-R is easier than its English and GT counterparts. While we provide evidence that the latter suffers from translation noise, comparison with the former indicates partially degenerate compositionality in our multilingual sets. We ascribe this degeneration to an inherent property of translation, resulting from linguistic differences: as shown in Table 1, question patterns are partially collapsed after mapping to target languages.\\n\\nTrain-test overlap. Intuitively, we consider training and test sets of the MCD splits, where no overlap is permitted in English under MCD constraints (the train-test intersection must be empty). Nevertheless, we found such overlaps in Japanese and Chinese due to the collapsed patterns. Summing up over 3 MCD splits, we observe 58 samples for Japanese and 37 for Chinese, and the two groups share similar patterns. Chinese and Japanese grammar inherently fail to (naturally) express specific compositions in English, predominantly the possessive case, a main category of compositional building block designed by Keysers et al. (2019). This linguistic divergence results in degeneration in compound divergence between training and test sets, which is intuitively reflected by the pattern overlap. We provide examples in Appendix E.1.\\n\\nLoss of structural variation. Given the demonstration above, we further look at MCWQ and see whether GT could avoid this degeneration. Surprisingly, the GT branches have larger train-test overlaps (108 patterns for Japanese and 144 for Chinese) than RBMT counterparts, among which several samples (45 for Japanese and 55 for Chinese) exhibit the same structural collapse as in RBMT. Importantly, a remaining large proportion of the samples (63 for Japanese and 89 for Chinese) possess different SPARQL representations for training and test respectively. In addition, several ill-formed samples are observed in this intersection.\\n\\nThe observations above provide evidence that the structural collapse is due to inherent linguistic differences and thus generally exists in translation-based methods, resulting in compositional degeneration in multilingual benchmarks. For GT branches, the noise involving semantic and grammatical distortion dominates over the degeneration, and thus causes worse model performance.\\n\\nImplications. While linguistic differences account for the performance gaps, we argue that monolingual performance in CG cannot be fairly compared across languages with translated benchmarks. While \u201ctranslationese\u201d occurs in translated datasets for other tasks too (Riley et al., 2020; Bizzoni and Lapshinova-Koltunski, 2021; Vanmassenhove et al., 2021), it is particularly significant here.\"}"}
{"id": "acl-2023-long-93", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Cross-lingual experiment results. The English results in gray refer to within-language generalization performance. Notice that mBART50\u2217 here is the ablation model of ZX-Parse with the same training paradigm for logical form decoder. We run 3 replicates for mBART50\u2217 and ZX-Parse. The results breakdown for 3 MCD splits can be found in Appendix D.1.\\n\\nFigure 4: Accuracy on MCWQ-R of mBART50\u2217 varies against increasing question complexity, averaged over 3 MCD splits. Dashed lines refer to within-language generalization performance, indicating cross-lingual transfer upper boundaries.\\n\\nmT5 tends to make \u201caccidental translation\u201d errors in zero-shot generalization (Xue et al., 2021), while the representation learned by mBART enables effective unsupervised translation via language transfer (Liu et al., 2020). Another surprising observation is that mBART50\u2217 outperforms the fine-tuned mT5-small on monolingual English (55.2% for MCD mean) with less training.\\n\\nWe present additional results regarding PLM fine-tuning in Appendix D.2.\\n\\nHallucination in parsing. mT5 tends to output partially correct SPARQL queries due to its drawback in zero-shot generative scenarios. From manual inspection, we note a common pattern in these errors that can be categorized as hallucinations (Ji et al., 2023; Guerreiro et al., 2023). As Table 5 suggests, the hallucinations with country entities occur in most wrong predictions, and exhibit a language bias akin to that Kassner et al. (2021) found in mBERT (Devlin et al., 2019), i.e., mT5 tends to predict the country of origin associated with the input language in the hallucinations, as demonstrated in Table 6. Experiments in Appendix D.2 indicate that the bias is potentially encoded in the pre-trained decoders.\\n\\n| Halluc. (%) | MCD mean | Random W/ | country |\\n|------------|----------|-----------|---------|\\n|            |          |           |         |\\n|            |          | W/        |         |\\n| EN         | 38.3     | 55.2\u00b11.6  | 23.9\u00b13.4 |\\n| JA         | 0.1      | 76.1      | 0.1     |\\n| ZH         | 0.12     | 37.3      | 0.1     |\\n| Others     | 4.2      | 1.8       | 0.45    |\\n| Total      | 75.2     | 77.9      | 0.45    |\\n\\nTable 5: Proportion of hallucinations with the specific country entities in the wrong predictions, generated by mT5-small in zero-shot cross-lingual generalization (models trained on English). Within-language results are in gray for comparison. The results on MCWQ-R are shown here. The countries are represented in QID and ISO codes, and the other (12) countries involved in the dataset are summed as others. The predominant parts exhibiting language bias are in bold, for which an example is shown in Table 6.\\n\\nRepresentation alignment. The auxiliary objectives in ZX-Parse are shown to improve the SP performance on MultiATIS++ (Xu et al., 2020) and Overnight (Wang et al., 2015). However, it leads to dramatic performance decreases on all MCWQ and MCWQ-R splits. We include analysis in Appendix E.2, demonstrating the moderate effect of the alignment mechanism here, which nevertheless should reduce the cross-lingual transfer penalty.\\n\\nWe thus ascribe this gap to the natural utterances from MKQA used for alignment resulting in less effective representations for compositional utterances, and hence the architecture fails to bring further improvement.\"}"}
{"id": "acl-2023-long-93", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question (EN) Which actor was M0\u2019s actor\\n\\nQuestion (ZH) M0\u7684\u6f14\u5458\u662f\u54ea\u4e2a\u6f14\u5458\\n\\nInferred (RIR) SELECT DISTINCT ?x0 WHERE lb\\n( M0 ( wdt:P453 ) ( ?x0 ) ) . ( ?x0 ( wdt:P27 ) ( wd:Q148 ) ) rb\\n\\nTable 6: An example of the language-biased hallucinations. The questions are parallel across languages and associated with the same SPARQL query. The inferred queries are in RIR form. The language-biased hallucination triples are highlighted in red, where Q148 is China in Wikidata, and Q17 is Japan.\\n\\nCross-lingual difficulty. As illustrated in Figure 4, while accuracies show similar declining trends across languages, cross-lingual accuracies are generally closer to monolingual ones in low complexity levels, which indicates that the cross-lingual transfer is difficult in CG largely due to the failure in universally representing utterances of high compositionality across languages. Specifically, for low complexity samples, we observe test samples that are correctly predicted cross-lingually but wrongly predicted within English. These several samples (376 for Japanese and 395 for Chinese on MCWQ-R) again entail structural simplification, which further demonstrates that this eases the compositional challenge even in the cross-lingual scenario. We further analyze the accuracies by complexity of MCWQ and ZX-Parse in Appendix E.3.\\n\\n7 Conclusion\\n\\nIn this paper, we introduced MCWQ-R, a robustly generated multilingual CG benchmark with a proposed rule-based framework. Through experiments with multilingual data generated with different translation methods, we revealed the substantial impact of linguistic differences and \u201ctranslationese\u201d on compositionality across languages. Nevertheless, removing of all difficulties but compositional-ity, the new benchmark remains challenging both monolingually and cross-lingually. Furthermore, we hope our proposed method can facilitate future investigation on multilingual CG benchmark in a controllable manner.\\n\\nLimitations\\n\\nEven the premise of parsing questions to Wikidata queries leads to linguistic and cultural bias, as Wikidata is biased towards English-speaking cultures (Amaral et al., 2021). As Cui et al. (2022) argue, speakers of other languages may care about entities and relations that are not represented in English-centric data (Liu et al., 2021b; Hershcovich et al., 2022a). For this reason and for the linguistic reasons we demonstrated in this paper, creating CG benchmarks natively in typologically diverse languages is essential for multilingual information access and its evaluation.\\n\\nAs we mentioned in \u00a74.2, our translation system fails to deal with ambiguities beyond grammar and thus generates wrong translations for a few samples (less than 0.31%). Moreover, although the dataset can be potentially augmented with low-resource languages and in general other languages through the translation framework, adequate knowledge will be required to expand rules for the specific target languages.\\n\\nWith limited computational resources, we are not able to further investigate the impact of parameters and model sizes of multilingual PLM as our preliminary results show significant performance gaps between PLMs.\\n\\nBroader Impact\\n\\nA general concern regarding language resource and data collection is the potential (cultural) bias that may occur when annotators lack representativeness. Our released data largely avoid such issue due to the synthetic and cultural-invariant questions based on knowledge base. Assessment by native speakers ensures its grammatical correction. However, we are aware that bias may still exist occasionally. For this purpose, we release the toolkit and grammar used for generation, which allows further investigation and potentially generating branches for other languages, especially low-resource ones.\\n\\nIn response to the appeal for greater environmental awareness as highlighted by Hershcovich et al. (2022b), a climate performance model card for mT5-small is reported in Table 7. By providing access to the pre-trained models, we aim to support future endeavors while minimizing the need for redundant training efforts.\"}"}
{"id": "acl-2023-long-93", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Model publicly available? Yes\\n2. Time to train final model 21 hours\\n3. Time for all experiments 23 hours\\n4. Energy consumption 0.28kW\\n5. Location for computations Denmark\\n6. Energy mix at location 191gCO2eq/kWh\\n7. CO2eq for final model 4.48 kg\\n8. CO2eq for all experiments 4.92 kg\\n\\nTable 7: Climate performance model card for mT5-small fine-tuned on MCWQ/MCWQ-R. \\\"Time to train final model\\\" corresponds to the training time for a single model of one split and one language, while the remaining models have similar resource consumption.\\n\\nAcknowledgements\\nWe thank the anonymous reviewers for their valuable feedback. We are also grateful to Guang Li, Nao Nakagawa, Stephanie Brandl, Ruixiang Cui, Tom Sherborne and members of the CoAStaL NLP group for their helpful insights, advice and support throughout this work.\\n\\nReferences\\nGabriel Amaral, Alessandro Piscopo, Lucie-aim\u00e9e Kaffee, Odinaldo Rodrigues, and Elena Simperl. 2021. Assessing the quality of sources in Wikidata across languages: A hybrid approach. J. Data and Information Quality, 13(4).\\n\\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2020. Translation artifacts in cross-lingual transfer learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7674\u20137684, Online. Association for Computational Linguistics.\\n\\nJasmijn Bastings, Marco Baroni, Jason Weston, Kyunghyun Cho, and Douwe Kiela. 2018. Jump to better conclusions: SCAN both left and right. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 47\u201355, Brussels, Belgium. Association for Computational Linguistics.\\n\\nSteven Bird and Edward Loper. 2004. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214\u2013217, Barcelona, Spain. Association for Computational Linguistics.\\n\\nYuri Bizzoni and Ekaterina Lapshinova-Koltunski. 2021. Measuring translationese across levels of expertise: Are professionals more surprising than students? In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 53\u201363, Reykjavik, Iceland (Online). Link\u00f6ping University Electronic Press, Sweden.\\n\\nDavid Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), pages 263\u2013270, Ann Arbor, Michigan. Association for Computational Linguistics.\\n\\nDavid Chiang. 2006. An introduction to synchronous grammars.\\n\\nKyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulec, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar. Association for Computational Linguistics.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\\n\\nRuixiang Cui, Rahul Aralikatte, Heather Lent, and Daniel Hershcovich. 2022. Compositional generalization in multilingual semantic parsing over Wikidata. Transactions of the Association for Computational Linguistics, 10:937\u2013955.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2019. Cross-lingual machine reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1586\u20131595, Hong Kong, China. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nYaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. 2016. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096\u20132030.\\n\\nEmily Goodwin, Siva Reddy, Timothy O'Donnell, and Dzmitry Bahdanau. 2022. Compositional generalization in dependency parsing. In Proceedings of the...\"}"}
{"id": "acl-2023-long-93", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-93", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726\u2013742.\\n\\nShayne Longpre, Yi Lu, and Joachim Daiber. 2021. MKQA: A linguistically diverse benchmark for multilingual open domain question answering. Transactions of the Association for Computational Linguistics, 9:1389\u20131406.\\n\\nDaniel Marcu and Daniel Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 133\u2013139. Association for Computational Linguistics.\\n\\nTruong-Phat Nguyen. 2021. Urbans: Universal rule-based machine translation nlp toolkit. https://github.com/pyurbans/urbans.\\n\\nInbar Oren, Jonathan Herzig, Nitish Gupta, Matt Gardner, and Jonathan Berant. 2020. Improving compositional generalization in semantic parsing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2482\u20132495, Online. Association for Computational Linguistics.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nAleksandr Perevalov, Axel-Cyrille Ngonga Ngomo, and Andreas Both. 2022. Enhancing the accessibility of knowledge graph question answering systems through multilingualization. In 2022 IEEE 16th International Conference on Semantic Computing (ICSC), pages 251\u2013256. IEEE.\\n\\nMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013191, Belgium, Brussels. Association for Computational Linguistics.\\n\\nLinlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, and Kristina Toutanova. 2022. Evaluating the impact of model scale for compositional generalization in semantic parsing. arXiv preprint arXiv:2205.12253.\\n\\nParker Riley, Isaac Caswell, Markus Freitag, and David Grangier. 2020. Translationese as a language in \u201cmultilingual\u201d NMT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7737\u20137746, Online. Association for Computational Linguistics.\\n\\nReiko Saegusa. 2006. Hanashi kotoba ni okeru teke (Te form in spoken Japanese language). Hitotsubashi University Center for Student Exchange Journal, 9:15\u201326.\\n\\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2021. Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 922\u2013938, Online. Association for Computational Linguistics.\\n\\nTom Sherborne and Mirella Lapata. 2022. Zero-shot cross-lingual semantic parsing. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4134\u20134153, Dublin, Ireland. Association for Computational Linguistics.\\n\\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2020. Multilingual translation with extensible multilingual pretraining and finetuning. arXiv preprint arXiv:2008.00401.\\n\\nDmitry Tsarkov, Tibor Tihon, Nathan Scales, Nikola Momchev, Danila Sinopalnikov, and Nathanael Sch\u00e4rli. 2021. *-cfq: Analyzing the scalability of machine learning on a compositional task. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 9949\u20139957.\\n\\nEva Vanmassenhove, Dimitar Shterionov, and Matthew Gwilliam. 2021. Machine translationese: Effects of algorithmic bias on linguistic complexity in machine translation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2203\u20132213, Online. Association for Computational Linguistics.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\\n\\nYushi Wang, Jonathan Berant, and Percy Liang. 2015. Building a semantic parser overnight. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1332\u20131342, Beijing, China. Association for Computational Linguistics.\\n\\nPhilip Williams, Rico Sennrich, Matt Post, and Philipp Koehn. 2016. Syntax-based statistical machine translation. Synthesis Lectures on Human Language Technologies, 9(4):1\u2013208.\\n\\nShuly Wintner. 2016. Translationese: Between human and machine translation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Tutorial Abstracts, pages 18\u201319, Osaka, Japan. The COLING 2016 Organizing Committee.\"}"}
{"id": "acl-2023-long-93", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Transduction Grammar Examples\\n\\nInflection in Japanese.\\n\\nWe provide a concrete example regarding the linguistic divergences during translation and how our transduction grammar (SCFG) address it. We take Japanese, specifically its verbal inflection case as an example.\\n\\n**Grammar**\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{VP} & \\\\rightarrow \\\\langle \\\\text{V NP}, \\\\text{NP V} \\\\rangle \\\\\\\\\\n\\\\text{V} & \\\\rightarrow \\\\langle \\\\text{VT and V}, \\\\text{VT and V} \\\\rangle \\\\\\\\\\n\\\\text{and V} & \\\\rightarrow \\\\langle \\\\epsilon \\\\text{V} \\\\rangle \\\\\\\\\\n\\\\text{NP} & \\\\rightarrow \\\\langle \\\\text{a film}, \\\\text{\u6620\u753b} \\\\rangle \\\\\\\\\\n\\\\text{V} & \\\\rightarrow \\\\{ \\\\langle \\\\text{edit , \u7de8\u96c6\u3057\u307e\u3059} \\\\rangle, \\\\langle \\\\text{write , \u66f8\u304d\u307e\u3059} \\\\rangle \\\\} \\\\\\\\\\n\\\\text{VT} & \\\\rightarrow \\\\{ \\\\langle \\\\text{edit , \u7de8\u96c6\u3057} \\\\rangle, \\\\langle \\\\text{write , \u66f8\u304d} \\\\rangle \\\\}\\n\\\\end{align*}\\n\\\\]\\n\\n(1)\\n\\n**Generated String**\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\langle \\\\text{write and edit a film}, \\\\text{\u6620\u753b\u3092\u66f8\u304d\u7de8\u96c6\u3057\u307e\u3059} \\\\rangle \\\\\\\\\\n\\\\langle \\\\text{edit and write a film}, \\\\text{\u6620\u753b\u3092\u7de8\u96c6\u3057\u66f8\u304d\u307e\u3059} \\\\rangle\\n\\\\end{align*}\\n\\\\]\\n\\n(2)\\n\\nIn the string pair of (2), the Japanese verbal inflection is reasoned from its position in a sequence where correspondences are highlighted with different colors. To make it more intuitive, consider a phrase (out of the corpus) \\\"run and run\\\" with repeated verb \\\"run\\\" and its Japanese translation \\\"hashi\u8d70\u308a\u3001hashi\u8d70\u308ama\u307e\u3059\\\", where the repeated \\\"hashi\u8d70\u308a\\\"(which should belong to V if in (1)) refers to a category of verb base, namely conjunctive indicating that it could be potentially followed by other verbs; and the inflectional suffix \\\"ma\u307e\u3059\\\" indicting the end of the sentence. Briefly speaking, in the Japanese grammar, the last verb in a sequence have a different form from the previous ones depending on the formality level.\\n\\nIn this case, the transduction rule of the lowest syntactic level explaining this inflection is \\\\( \\\\text{V} \\\\rightarrow \\\\langle \\\\text{VT and V}, \\\\text{VT and V} \\\\rangle \\\\), therefore the VT with suffix T is derived from V (V exhibit no inflection regarding ordering in English) from this level and carries this context information down to the terminals. Considering questions with deep parse trees where such context information should potentially be carried through multiple part-of-speech symbols in the top-down process, we let the suffix be inheritable as demonstrated in (3).\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{VP} & \\\\rightarrow \\\\langle \\\\text{VPT and VP}, \\\\text{VPT and VP} \\\\rangle \\\\\\\\\\n\\\\text{VPT} & \\\\rightarrow \\\\langle \\\\text{VT NP}, \\\\text{NP VT} \\\\rangle\\n\\\\end{align*}\\n\\\\]\\n\\n(3)\\n\\nwhere suffix T carries the commitment of inflection to be performed at the non-terminal level and is explained by context of VPT and inherited by VT. While such suffix is commonly used in formal grammar, we leverage this mechanism to a large extent to fill the linguistic gap. The strategy is proved to be simple yet effective in practical grammar construction to handle most of the problems caused by linguistic differences such as inflection as mentioned.\\n\\nB Translation Assessment Details\\n\\nSince manual assessment is subjective, the guidelines were stated before assessment: translations resulting in changed expected answer domains are rated 1 or 2 for meaning preservation. Those with...\"}"}
{"id": "acl-2023-long-93", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Manual assessment scores vary against increasing complexity levels with a bin size of 3. The scores are averaged over every 3 complexity levels and 2 languages.\\n\\nAccordingly, we regard questions with a score $\\\\geq 3$ as acceptable in the corresponding aspect.\\n\\nTo make an intuitive comparison, we divide the 42 complexity levels (for each level we sampled 1 sentence) into 14 coarser levels and see the variation of the scores of 2 methods against the increasing complexity. As shown in Figure 5, Our method exhibits uniformly good meaning preservation ability while GT suffers from semantic distortion for certain cases and especially for those of high complexity. For the variation of fluency, the steady performance of our method indicates that the loss is primarily systematic and due to compromise for compositional consistency and parallel principle, while GT generates uncontrollable results with incorrect grammar (and thus illogical) occasionally.\\n\\nWe present imprecise translation example of our method. Adjective indicating nationalities such as \\\"American\\\" is naturally adapted to \\\"\u30a2\u30e1\u30ea\u30ab\u4eba\\\" when modifying a person in Japanese; then for a sample (note that entities are bracketed):\\n\\n**Input:** \\\"Was [Kate Bush] British\\\"\\n\\n**Output:** \\\"[Kate Bush] wa \u306f \u3044 \u30ae\u30ea\u30b9 no de \u3067 \u3057 \u305f ka\\\"\\n\\n**Expected:** \\\"[Kate Bush] wa \u306f \u3044 \u30ae\u30ea\u30b9 jin de \u3067 \u3057 \u305f ka\\\"\\n\\nConsider the bracketed entity [Kate Bush] which is invisible during translation, and also the fact that the sentence still holds if it is alternated with non-human entities. Without the contribution of the entity semantics, the grammar is unable to specify \\\"jin\u4eba\\\" (person) in this case, and results in a less natural expression. We observed a few samples similar to this leading to the error in BLEU scores.\\n\\nFor GT, as we mentioned in \u00a74.3, it causes semantic distortions potentially changing expected answers:\\n\\n**Input:** \\\"What did [human] found\\\"\\n\\n**Output (GT):** \\\"[human] wa \u306f \u306a\u306b \u3092 \u898b \u3064 \u3051 \u3066 \u307e \u3057\u305f ka\\\"\\n\\n**Expected (&Ours):** \\\"[human] ga \u304c \u5275 \u9020 \u3057\u3066 \u305f no de \u306f \u306a\u306b de \u3059 \u304b\\\"\\n\\nDisregarding the sentence patterns, the output of GT distorted the meaning as \\\"What did [human] find\\\", translated back to English.\\n\\n**Input:** \\\"Was a prequel of [Batman: Arkham Knight] \u2019s prequel...\\\"\\n\\n**Output (GT):** \\\"[Batman: Arkham Knight] no \u306e \u524d \u65e5 \u8b5a...\\\"\\n\\n**Expected (&Ours):** \\\"[Batman: Arkham Knight] no \u306e \u524d \u65e5 \u8b5a no \u306e \u524d \u65e5 \u8b5a...\\\"\\n\\nThe example above shows how the 2 methods deal with a compositional phrase occurring in the dataset. GT exhibits reasoning ability which understood that \\\"a prequel of a prequel\\\" indicates \\\"a prequel\\\" thus translating it as \\\"\u524d\u65e5\u8b5a (prequel)\\\", whereas an expected compositionally faithful translation should be \\\"\u524d\u65e5\u8b5a of a prequel of a prequel\\\". The examples demonstrate how GT as a neural model fails in accommodating compositionality even for the well-formed translations: the infinite compositional expression potentially reaches the \\\"fringe area\\\" of the trained neural model distribution, i.e., it overly concerns the possibility that the sentence occurs instead of keeping faithful regarding the atoms and their compositions.\\n\\n### C Training Details\\n\\nmT5-small.\\n\\nWe follow the same setup of mT5-small as in (Cui et al., 2022) with default hyperparameters but a learning rate of $5 \\\\times 10^{-4}$, which is believed to help overcome the local minimum. Each model was trained on 4 Titan RTX GPUs with a batch size of 16. The total training time is 234 hours for 12 models (4 splits for GT-Japanese, RBMT-Chinese and RBMT-Japanese respectively).\\n\\nmBART50 and ZX-Parse.\\n\\nWe follow the searched optimal architecture and parameters by Sherborne and Lapata (2022). The logical-form-only mBART50 includes frozen mBART50-large embedder, 1-layer encoder, and 6-layer decoder, and the full ZX-Parse with additional alignment components: 6-layer decoder (reconstruction) and 2-layer feed-forward networks (language...\"}"}
{"id": "acl-2023-long-93", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"prediction) trained with bi-text that we extract from MKQA. The auxiliary components in ZX-Parse make the encoder align latent representations across languages. Each model was trained on 1 Titan RTX GPU with a batch size of 2. It takes around 17 hours to train a full ZX-Parse and 14 hours an mBART50 model.\\n\\nD Additional Results\\n\\nD.1 MCD Splits\\n\\nThe exact match accuracies on the 3 maximum compound divergence (MCD) splits (Keysers et al., 2019) are shown in Table 8.\\n\\nD.2 mT5\\n\\nIn additional experiments, we freeze the mT5 encoders and train randomly initialized layers as mBART50 on English. The cross-lingual generalization results are shown in Table 9. While training decoder from scratch seemingly slightly ease cross-lingual transfer as also stated by Sherborne and Lapata (2022), the monolingual performance of mT5-small drops without pre-trained decoder. The results of mT5-large is consistent with Qiu et al. (2022) which shows that increasing model size brings moderate improvement. However, the performance is still not comparable with mBART50, indicating that training paradigm does not fully account for the performance gap in Table 4.\\n\\nWhile mT5 still struggle in zero-shot generation, the systematic hallucinations of country of origin mentioned in \u00a76.2 disappear in this setup, due to the absence of pre-trained decoders which potentially encode the language bias.\\n\\n|                | EN | JA  | ZH  |\\n|----------------|----|-----|-----|\\n| mT5-small      | 25.9| 1.0 | 1.2 |\\n| mT5-large      | 28.0| 1.1 | 1.0 |\\n\\nTable 9: Additional experiment results by replacing mBART50 with mT5 encoders: superscript * refers to the training paradigm of freezing pre-trained encoder as embedding layer and training randomly initialized encoder-decoder.\\n\\nE Supplementary Analysis\\n\\nE.1 Structural Simplification\\n\\nThe train-test overlaps intuitively reflect the structural simplification, we show the numbers by structural cases and concrete examples in Table 10.\\n\\nE.2 Representation Alignment in ZX-Parse\\n\\nWe analyze the representations before and after the trained aligning layer with t-SNE visualization as Sherborne and Lapata (2022) do. Figure 6 illustrates an example, the representations of compositional utterances (especially English) are distinct from natural utterances from MKQA, even after alignment, which demonstrates the domain gap between the 2 categories of data. Nonetheless, the mechanism performs as intended to align representations across languages.\\n\\nE.3 Accuracy by Complexity\\n\\nWe present the accuracy by complexity on MCWQ in Figure 7. We notice the gaps between monolingual and cross-lingual generalization are generally smaller than on MCWQ-R (see Figure 4). This is ascribed to the systematicity of GT errors\u2014such (partially) systematical errors are fitted by models in monolingual training, and thus cause falsely higher performance on the test samples possessing similar errors.\\n\\nFigure 8 shows the cross-lingual results of ZX-Parse on both datasets. While the accuracies are averagely lowered, the curves appear to be more aligned due to the mechanism.\"}"}
{"id": "acl-2023-long-93", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|     | EN       | JA       | ZH       |\\n|-----|----------|----------|----------|\\n|     |          |          |          |\\n| **MCD 1** |          |          |          |\\n| **EN**  | | 77.6 \u00b1 0.7 | 35.8 \u00b1 4.4 |\\n| **JA**  | | 75.7 | 43.6 |\\n| **ZH**  | | 74.7 | 52.8 |\\n| **MCD 2** |          |          |          |\\n| **EN**  | | 13.9 \u00b1 0.7 | 35.1 \u00b1 3.4 |\\n| **JA**  | | 32.2 | 18.1 |\\n| **ZH**  | | 31.5 | 21.1 |\\n| **MCD 3** |          |          |          |\\n| **EN**  | | 24.3 \u00b1 3.5 | 54.4 \u00b1 3.5 |\\n| **JA**  | | 61.0 | 30.8 |\\n| **ZH**  | | 47.2 | 34.9 |\\n| **Cross-lingual (Supplement to Table 4)** | | | |\\n| **MCD 1** |          |          |          |\\n| **JA**  | | 0.06 \u00b1 0.15 | 42.6 \u00b1 1.7 |\\n| **ZH**  | | 0.08 | 0.08 |\\n| **MCD 2** |          |          |          |\\n| **JA**  | | 0.07 \u00b1 0.16 | 24.5 \u00b1 1.6 |\\n| **ZH**  | | 0.08 | 0.08 |\\n| **MCD 3** |          |          |          |\\n| **JA**  | | 0.18 \u00b1 0.20 | 39.0 \u00b1 2.9 |\\n| **ZH**  | | 0.20 | 0.40 |\\n\\nTable 8: Detailed experiment results breakdown of 3 MCD splits, as supplement to Table 4 and 3.\"}"}
