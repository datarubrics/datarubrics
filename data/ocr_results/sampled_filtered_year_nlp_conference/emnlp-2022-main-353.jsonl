{"id": "emnlp-2022-main-353", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WeTS: A Benchmark for Translation Suggestion\\n\\nZhen Yang, Fandong Meng, Yingxue Zhang, Ernan Li, and Jie Zhou\\n\\nPattern Recognition Center, WeChat AI, Tencent Inc, Beijing, China\\n\\n{zieenyang, fandongmeng, yxuezhang, cardli, withtomzhou}@tencent.com\\n\\nAbstract\\n\\nTranslation suggestion (TS), which provides alternatives for specific words or phrases given the entire documents generated by machine translation (MT), has been proven to play a significant role in post-editing (PE). There are two main pitfalls for existing researches in this line. First, most conventional works only focus on the overall performance of PE but ignore the exact performance of TS, which makes the progress of PE sluggish and less explainable; Second, as no publicly available golden dataset exists to support in-depth research for TS, almost all of the previous works conduct experiments on their in-house datasets or the noisy datasets built automatically, which makes their experiments hard to be reproduced and compared. To break these limitations mentioned above and spur the research in TS, we create a benchmark dataset, called WeTS, which is a golden corpus annotated by expert translators on four translation directions. Apart from the golden corpus, we also propose several methods to generate synthetic corpora which can be used to improve the performance substantially through pre-training. As for the model, we propose the segment-aware self-attention based Transformer for TS. Experimental results show that our approach achieves the best results on all four directions, including English-to-German, German-to-English, Chinese-to-English, and English-to-Chinese. Codes and corpus can be found at https://github.com/ZhenYangIACAS/WeTS.git.\\n\\n1 Introduction\\n\\nComputer-aided translation (CAT) (Barrachina et al., 2009; Green et al., 2014; Knowles and Koehn, 2016; Santy et al., 2019) has attained more and more attention for its promising ability in combining the high efficiency of machine translation (MT) (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) and high accuracy of human translation (HT). A typical way for CAT tools to combine MT and HT is PE (Green et al., 2013; Zouhar et al., 2021), where the human translators are asked to provide alternatives for the incorrect word spans in the results generated by MT. To further reduce the post-editing time, researchers propose to apply TS into PE, where TS provides the sub-segment suggestions for the annotated incorrect word spans in the results of MT, and their extensive experiments show that TS can substantially reduce translators\u2019 cognitive loads and the post-editing time (Wang et al., 2020; Lee et al., 2021).\\n\\nAs there is no explicit and formal definition for TS, we observe that some previous works similar or related to TS have been proposed (Alabau et al., 2014; Santy et al., 2019; Wang et al., 2020; Lee et al., 2021). However, there are two main pitfalls for these works in this line. First, most conventional works only focus on the overall performance of PE but ignore the exact performance of TS. This is mainly because the golden corpus for TS is relatively hard to collect. As TS is an important sub-module in PE, paying more attention to the exact performance of TS can boost the performance and interpretability of PE. Second, almost all of the previous works conduct experiments on their in-house datasets or the noisy datasets built automatically, which makes their experiments hard to be followed and compared. Additionally, experimental results on the noisy datasets may not truly reflect the model\u2019s ability on generating the right predictions, making the research deviate from the correct direction. Therefore, the community is in dire need of a benchmark for TS to enhance the research in this area.\\n\\nTo address the limitations mentioned above and spur the research in TS, we make our efforts to construct a high-quality benchmark dataset with human annotation, named WeTS, which covers four different translation directions. As collecting the\"}"}
{"id": "emnlp-2022-main-353", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"golden dataset is expensive and labor-consuming, we further propose several methods to automatically construct synthetic corpora, which can be utilized to improve the TS performance through pre-training. As for the model, we for the first time propose the segment-aware self-attention based Transformer for TS, named SA-Transformer, which achieves superior performance to the naive Transformer (Vaswani et al., 2017). The main contributions of this paper are summarized as follows:\\n\\n\u2022 We construct and share a benchmark dataset for TS on four translation directions.\\n\u2022 We provide strong baseline models for this community. Specifically, we make a detailed comparison between the Transformer-based and XLM-based models, and propose the segment-aware self-attention based Transformer for TS, which achieves the best results on the benchmark dataset.\\n\u2022 We thoroughly investigate different ways for building the synthetic corpora. Since constructing the golden corpus is expensive and labor-consuming, it is very essential and promising to build the synthetic corpora by making full use of the parallel corpus of MT.\\n\u2022 We conduct extensive experiments and provide deep analyses about the strengths and weaknesses of the proposed approach, which are expected to give some insights for further researches on TS.\\n\\n2 WeTS\\n\\nThis section introduces the proposed dataset WeTS. To make the constructing process understood easily, we first formally define the task of TS.\\n\\n2.1 Translation Suggestion\\nGiven the source sentence $x = (x_1, ..., x_s)$, the translation sentence $m = (m_1, ..., m_t)$, the incorrect words or phrases $w = m_{i:j}$ where $1 \\\\leq i \\\\leq j \\\\leq t$, and the correct alternative $y$ for $w$, the task of TS is optimized to maximize the conditional probability of $y$ as follows:\\n\\n$$P(y | x, m-w, \\\\theta)$$\\n\\nwhere $\\\\theta$ represents the model parameter, and $m-w$ is the masked translation where the incorrect word span $w$ is replaced with a placeholder. $w$ is null if $i$ equals $j$, and the model will predict whether some words need to be inserted in position $i$.\\n\\n2.2 Dataset\\nThis sub-section describes the annotation guidelines and construction process for WeTS, which is a golden corpus for four translation directions, including English-to-German, German-to-English, Chinese-to-English and English-to-Chinese.\\n\\nAnnotation Guidelines\\nIt is non-trivial for annotators to locate the incorrect word spans in the MT sentence. The main difficulty is that, the concept of \u201ctranslation error\u201d is ambiguous and each translator has his own understanding about translation errors. To easier the annotation workload and reduce the possibility of making errors, we group the translation errors on which we aim to focus into three macro categories:\\n\\n\u2022 Under-translation or over-translation: While the problem of under-translation or over-translation has been alleviated with the popularity of Transformer, it is still one of the main mistakes in NMT and seriously destroys the readability of the translation.\\n\u2022 Semantic errors: For the semantic error, we mean that some source words are incorrectly translated according to the semantic context, such as the incorrect translations for entities, proper nouns, and ambiguous words. Another branch of semantic mistake is that the source words or phrases are only translated superficially and the semantics behind are not translated well.\\n\u2022 Grammatical or syntactic errors: Such errors usually appear in translations of long sentences, including the improper use of tenses, passive voice, syntactic structures, etc.\\n\\nAnother key rule for translators is that annotating the incorrect span as local as possible, as generating correct alternatives for long sequences is much harder than that of shorter sequences.\"}"}
{"id": "emnlp-2022-main-353", "page_num": 3, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u4ed6\u4eec\u4e5f\u8bb8\u5e76\u4e0d\u77e5\u9053\u8fd9\u662f\u4e00\u4e2a\u201c\u5047\u7406\u8d22\u201d\u9a97\u5c40\uff0c\u4f46\u4e5f\u5bdf\u89c9\u5230\u4e86\u8bf8\u591a\u53ef\u7591\u4e4b\u5904\uff0c\u7136\u800c\u6700\u7ec8\u8fd8\u662f\u6309\u7167\u5f20\u9896\u7684\u6307\u4f7f\u8fdb\u884c\u4e86\u8fdd\u6cd5\u8fdd\u89c4\u64cd\u4f5c\u3002\\n\\nTranslation\\nThey may not know this is a \\\"fake financial management\\\" scam, but also aware of many suspicious points, and ultimately conduct illegal operations according to Zhang Ying's instructions.\\n\\nSuggestions\\n1. suspects\\n2. doubtful points\\n3. questionable points\\n\\nData Construction\\nAs the starting point, we collect the monolingual corpora for English and German from the raw Wikipedia dumps, and extract Chinese monolingual corpus from various online news publications. We first clean the monolingual corpora with a language detector to remove sentences belonging to other languages.\\n\\nFor all monolingual corpora, we remove sentences that are shorter than 20 words or longer than 80 words. In addition, sentences which exist in the available parallel corpora are also removed. Then, we get the translations by feeding the cleaned monolingual corpus into the corresponding fully-trained NMT model. The NMT models for English-German language pairs are trained on the parallel corpus of WMT14 English-German. For Chinese-English directions, the NMT models are trained with the combination between the WMT19 English-Chinese and the same amount of in-house corpus.\\n\\nFinally, the translators are required to mark the incorrect word spans in the translation sentence and provide at least one alternative for each incorrect span, by using the annotation guidelines. The team is composed by eight annotators with high expertise in translation and each example has been assigned to three experts. There are two phases of agreement computations. In the first phase, an annotation is considered in agreement among the experts if and only if they capture the same incorrect word spans. If one annotation passes the first agreement computation, it will be assigned to other three experts in charge of selecting the right alternatives from the previous annotation. In the second phase of agreement computation, an annotation is considered in agreement among the experts if and only if they select the same right alternatives. With the two-phase agreement checking, we ensure the high quality of the annotated examples. For the annotated examples with multiple incorrect word spans, we can extract multiple examples which have the same source and translation sentences, but different incorrect word span and the corresponding suggestions. Finally the extracted examples are randomly shuffled and then split into the training, validation and test sets.\\n\\nConstruct Synthetic Corpus\\nSince constructing the golden corpus is expensive and labor-consuming, automatically building the synthetic corpus is very promising. In this section, we describe several ways for constructing synthetic corpus for TS based on the parallel corpus of MT.\\n\\n3.1 Sampling on Golden Parallel Corpus\\nSampling on the golden parallel corpus of MT is the most straightforward and simplest way for constructing synthetic corpus for TS. Given the sentence pair \\\\((x, r)\\\\) in the parallel corpus of MT, where \\\\(x\\\\) is the source sentence and \\\\(r\\\\) is the corresponding target sentence, we denote \\\\(r^i:j\\\\) as a masked version of \\\\(r\\\\) where its fragment from position \\\\(i\\\\) to \\\\(j\\\\) is replaced with a placeholder (\\\\(1 \\\\leq i \\\\leq j \\\\leq |r|\\\\)). The \\\\(r^i:j\\\\) denotes the fragment of \\\\(r\\\\) from position \\\\(i\\\\) to \\\\(j\\\\). We treat \\\\(r^i:j\\\\) and \\\\(r^i:j\\\\) as the correct alternative (\\\\(y\\\\) in Equation 1) and masked versions. To keep the fairness of WeTS, we ensure the examples among the training, validation and test sets have different source and translation sentences.\"}"}
{"id": "emnlp-2022-main-353", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The whole architecture of the proposed SA-Transformer. The \\\\( s \\\\) and \\\\( t \\\\) are the lengths for \\\\( x \\\\) and \\\\( m \\\\) respectively. \\\\( N \\\\) denotes the layers for the encoder and decoder.\\n\\nTranslation (\\\\( m - w \\\\) in Equation 1) respectively. In this approach, the masked translation in each example is part of the golden target sentence. However, in production, the TS model needs to predict the correct suggestions based on the context of the machine translated sentence. Therefore, the mismatch of distribution between the golden target sentence and machine translated sentence is the potential pitfall for this approach.\\n\\n3.2 Sampling on Pseudo Parallel Corpus\\n\\nThe second approach we apply to construct the synthetic corpus for TS is sampling on the pseudo parallel corpus of MT. Given the source sentence \\\\( x \\\\) and the MT model \\\\( T_\\\\theta \\\\), we first get the translated sentence \\\\( \\\\tilde{y} \\\\) by feeding \\\\( x \\\\) into \\\\( T_\\\\theta \\\\), and \\\\((x, \\\\tilde{y})\\\\) is treated as the pseudo sentence pair. Then, we perform sampling on \\\\((x, \\\\tilde{y})\\\\) as what we do on \\\\((x, r)\\\\) in Section 3.1. Compared to the approach of sampling on the golden parallel corpus, sampling on the pseudo parallel corpus can address the problem of distribution mismatch mentioned in Section 3.1 and it works without relying on the golden parallel corpus. However, the suggested alternatives may be in poor quality since they are parts of the translated sentences.\\n\\n3.3 Extracting with Word Alignment\\n\\nConsidering the shortcomings of the two previous approaches, we investigate the third approach where we conduct the word alignment between the machine translation and the golden target sentence, and then extract the synthetic corpus for TS based on the alignment information. Given the sentence triple \\\\((x, \\\\tilde{y}, r)\\\\), we perform word alignment between \\\\( \\\\tilde{y} \\\\) and \\\\( r \\\\), and extract the aligned phrase table. For phrase \\\\( \\\\tilde{y}_i:j \\\\) in \\\\( \\\\tilde{y} \\\\) and its aligned phrase \\\\( r_a:b \\\\) in \\\\( r \\\\), we denote \\\\( \\\\tilde{y}_i:j \\\\cap r_a:b \\\\) as the modified version of \\\\( \\\\tilde{y}_i:j \\\\) where the phrase \\\\( \\\\tilde{y}_i:j \\\\) is replaced with \\\\( r_a:b \\\\). If \\\\( r_a:b \\\\) is not identical to \\\\( \\\\tilde{y}_i:j \\\\) and the perplexity of \\\\( \\\\tilde{y}_i:j \\\\cap r_a:b \\\\) is lower than that of \\\\( \\\\tilde{y}_i:j \\\\) with a margin no less than \\\\( \\\\beta \\\\), we treat \\\\( \\\\tilde{y}_i:j \\\\cap r_a:b \\\\) as the masked translation and the correct alternative respectively. \\\\( \\\\beta \\\\) is a hyper-parameter to control the threshold of the margin. While this approach has achieved much improvement compared to the previous approaches, we still notice that the errors in the extracted alignment information may introduce some noises. Recent advances in neural word aligners may provide more accurate alignments (Jalili Sabet et al., 2020; Lai et al., 2022), and we leave this in our future work.\\n\\n4 The Model\\n\\nIn this section, we describe the proposed model, i.e., SA-Transformer, and the whole architecture is...\"}"}
{"id": "emnlp-2022-main-353", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"illustrated as Figure 2.\\n\\n4.1 Inputs\\nGiven the source sentence $x$ and the masked translation $m - w$, the input to the model is formatted as:\\n\\n$$[x; \\\\langle \\\\text{sep} \\\\rangle; m - w]$$ (2)\\n\\nwhere $[\\\\cdot; \\\\cdot]$ means concatenation, and $\\\\langle \\\\text{sep} \\\\rangle$ is a special token used as a delimiter. The position for each segment in the input is calculated independently and we use the segment embedding to distinguish each segment from others. The representation for each token in the input is the sum of its token embedding, position embedding and segment embedding.\\n\\n4.2 Segment-aware Self-attention\\nThe naive Transformer applies the self-attention to extract the higher-level information from the token representations in the lower layer without distinguishing tokens in each segment from those in other segments explicitly. The attention matrix in the self-attention is typically calculated as:\\n\\n$$QW_Q(QW_KK^T)^{1/2}$$ (3)\\n\\nwhere the $Q$ and $K \\\\in \\\\mathbb{R}^{s \\\\times d_x}$ are identical in the encoder, $W_Q$ and $W_K \\\\in \\\\mathbb{R}^{d_x \\\\times d_x}$ are the projection matrix, $d_x$ is the dimension of the word embedding. However, the inputs for TS contain tokens from different segments, i.e., the source sentence, masked translation, and the hints if provided, and the tokens in each segment are expected to be distinguished from those in other segments since they provide different information for the model's prediction. While the segment embedding in token representations has played the role for distinguishing tokens from different segments, its information has been mixed with the word embedding and diluted with the information flow. With this consideration, we propose the segment-aware self-attention by further injecting the segment information into the self-attention to make it perform differently according to the segment information of the tokens. Formally, the attention matrix in the proposed segment-aware self-attention is calculated as:\\n\\n$$(E_{\\\\text{seg}} \\\\cdot Q)W_Q((E_{\\\\text{seg}} \\\\cdot K)W_K)^{1/2}$$ (4)\\n\\nwhere $E_{\\\\text{seg}} \\\\in \\\\mathbb{R}^{s \\\\times d_x}$ is the segment embedding and $\\\\cdot$ represents dot production.\\n\\n4.3 Two-phase Pre-training\\nWe apply the pretraining-finetuning paradigm for training the proposed model. The pre-training process can be divided into two phases: In the first phase, we follow Lee et al. (2021) to pre-train a XLM-R model with a modified translation language model objective on the monolingual corpus, and then utilize the pre-trained parameters of XLM-R to initialize the encoder of the proposed model. In the second phase, we apply the combination of all the constructed synthetic corpus to pre-train the whole model. After pre-training, we finetune the model on the golden training set of WeTS.\\n\\n5 Experiments and Results\\nWe first describe the experimental settings, including datasets, pre-processing, and hyper-parameters; Then we introduce the baseline systems and report the main experimental results.\\n\\n5.1 Datasets and Pre-processing\\nTo make our results reproducible, we construct the synthetic corpora from the publicly available datasets provided by the WMT2019 and WMT2014 shared translation tasks. We use the full training set of the WMT14 English-German, which contains 4.5M sentence pairs. For the WMT19 Chinese-English dataset, we remove sentences longer than 200 words and get 20M sentence pairs. The NMT models utilized for constructing synthetic corpus are identical to the ones used for constructing WeTS. For each translation direction, the source and target corpus are jointly tokenized into sub-word units with BPE (Sennrich et al., 2016). The source and target vocabularies are extracted from the source and target tokenized synthetic corpus respectively. During fine-tuning, we pre-process the golden corpus with the same tokenizer utilized in pre-training. Sizes for the constructed synthetic corpora and the details about pre-processing can be found in Appendix B.\\n\\n5.2 Hyper-parameters and Evaluation\\nWe take the Transformer-base (Vaswani et al., 2017) as the backbone of our model, and we use\"}"}
{"id": "emnlp-2022-main-353", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: The main results on the four language pairs. The numbers with * indicate the significant improvement over the baseline of naive Transformer with \\\\( p < 0.01 \\\\) under t-test.\\n\\n|                      | Zh\u21d2En | En\u21d2Zh | De\u21d2En | En\u21d2De | Zh\u21d2En | En\u21d2Zh | De\u21d2En | En\u21d2De |\\n|----------------------|--------|--------|--------|--------|--------|--------|--------|--------|\\n| XLM-R                | 21.25  | 32.48  | 27.40  | 25.12  | 40.17  | 52.05  | 37.21  | 36.40  |\\n| Naive Transformer    | 24.20  | 35.01  | 30.08  | 28.15  | 43.32  | 54.60  | 41.05  | 41.21  |\\n| Dual-source Transformer | 24.29 | 35.10  | 30.23  | 28.09  | 43.12  | 54.75  | 42.01  | 40.95  |\\n| SA-Transformer (ours)| 25.51* | 36.28* | 31.20* | 29.48* | 44.67* | 56.48* | 42.66  | 42.13* |\\n\\nFor evaluation, we report the scores of BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020) on the test sets of WeTS. Both of the two scores are calculated between the top-1 generated suggestion against the golden suggestions in the reference. BLEU is the widely used metric for calculating the n-gram precision between candidates and references. For the direction of English-to-Chinese, we report the character-level BLEU. For the other three directions, we report the case-sensitive BLEU on the de-tokenized sentences. In this paper, we utilize the script of multi-bleu.pl as the evaluation tool for BLEU. BLEURT is the recently proposed metric which returns a score that indicates to what extent the candidate is fluent and conveys the meaning of the reference. For calculating BLEURT score, we directly utilized the released toolkit bleurt. We refer the readers to the appendix C for details about the experimental settings.\\n\\n5.3 Baselines\\n\\nXLM-R. The first baseline system we consider is the work of Lee et al. (2021) who propose the TS system based on XLM-R (Conneau et al., 2020). Following Lee et al. (2021), we re-implement the XLM-based TS model based on the open-source toolkit of XLM (CONNEAU and Lample, 2019) with slight modification.\\n\\nNaive Transformer. We take the naive Transformer (Vaswani et al., 2017) as the second baseline and directly apply the implementation of fairseq.\\n\\nDual-source Transformer. We finally consider the dual-source Transformer (Junczys-Dowmunt and Grundkiewicz, 2018) which applies two shared encoders to encode the source sentence and masked translation respectively. We re-implement the model based on the fairseq toolkit. All of the baseline systems mentioned above are trained in the same way as our system.\\n\\n5.4 Main Results\\n\\nTable 2 shows the main results of our experiments. We can find that, compared to all of the baseline systems, the proposed SA-Transformer achieves the best results on all of the four translation directions. Compared with the XLM-based approach (comparing systems 2-4 with system 1), the Transformer-based approach can achieve substantial gains on the final performance. While the dual-source Transformer has a more complex model structure, it only achieves comparable results with the naive Transformer. We conjecture the main reason is that the dual-source Transformer does not model the interaction between the source and translation, as the source and translated sentences are encoded with two separate encoders in the dual-source Transformer. Compared to the naive Transformer, the proposed model achieves the improvement up to +1.3 BLEU points and +1.5 BLEURT points on the Chinese-to-English translation direction.\\n\\n6 Analysis\\n\\nThis section provides detailed analysis about the proposed approach, and performance on two translation directions are presented for most of the following experiments.\\n\\n6.1 Effects of Synthetic Parallel Corpus\\n\\nIn this paper, we propose three different ways for constructing synthetic corpus for the second-phase pre-training. A natural question is that how each of the synthetic corpus affects the performance. We investigate this problem by studying the performance on the English-to-Chinese direction with different synthetic corpus. We report both intermediate and final performances of the model, where fine-tuning is removed and applied respectively. Results are...\"}"}
{"id": "emnlp-2022-main-353", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"presented in Table 3. As shown in Table 3, we can find that the model trained on the combination of all three kinds of synthetic corpus achieves the best performance. The synthetic corpus constructed with word alignment contributes the most to the final performance.\\n\\n| Systems                      | En \u21d2 Zh | En \u21d2 De |\\n|------------------------------|---------|---------|\\n| SA-Transformer               | 29.76   | 26.64   |\\n| w/o fine-tuning              |         |         |\\n| w/o on golden corpus         | 26.04   | 25.03   |\\n| w/o with word alignment      | 21.26   | 21.44   |\\n\\nTable 3: Results on the effects of synthetic corpus.\\n\\n6.2 Study the Training Procedure\\n\\nWe adopt the pretraining-finetuning paradigm for the model training, where the two-phase pre-training enhances the model's ability in modeling the general inputs and the fine-tuning further enhances the performance on the golden test sets. In this section, we aim to investigate how the training procedure affects the final performance. Table 4 shows the experimental results. As Table 4 shows, the model achieves very low BLEU scores, i.e., 6.70 in English-to-Chinese and 5.87 in English-to-German, if pre-training is not applied. This is mainly because that the golden corpus of WeTS is too scarce to train a well-performed TS model. In the two-phase pre-training, the second-phase pre-training plays a more important role for the final performance, with a decrease of almost 20 BLEU score on English-to-Chinese translation direction if removed. Fine-tuning on the golden corpus of WeTS substantially enhances the performance, with an improvement of almost 8 BLEU score on the English-to-German translation direction.\\n\\n| Systems                      | En \u21d2 Zh | En \u21d2 De |\\n|------------------------------|---------|---------|\\n| SA-Transformer               | 36.28   | 29.48   |\\n| w/o fine-tuning              | 29.76   | 21.44   |\\n| w/o pre-training             | 6.70    | 5.87    |\\n| w/o first-phase pre-training | 34.63   | 28.37   |\\n| w/o second-phase pre-training| 16.85   | 14.14   |\\n\\nTable 4: Results on the effects of training strategies.\\n\\n6.3 Ablation Study on Model Structure\\n\\nTo understand the importance of the model components, we perform an ablation study by training multiple versions of the model with some components removed or degenerated into the corresponding ones in the naive Transformer. We mainly test three components, including the independent position encoding, segment embedding, and the segment-aware self-attention. Results are reported in Table 5. We find that the best performance is obtained with the simultaneous use of all test components. The most critical component is the segment-aware self-attention, which enables the model to perform a different calculation of self-attention according to the type of the input tokens. When we remove the segment embedding, we get 0.46 BLEU points decline on the English-to-Chinese translation direction. And when the segment-aware self-attention is removed, the decline can be as large as 0.77 BLEU points. These results indicate that the proposed segment-aware self-attention can provide more useful segment information.\\n\\n| Systems                      | En \u21d2 Zh | En \u21d2 De |\\n|------------------------------|---------|---------|\\n| SA-Transformer               | 36.01   | 29.35   |\\n| w/o independent position     |         |         |\\n| w/o segment embedding        | 35.82   | 29.01   |\\n| w/o segment-aware self-attention| 35.51   | 28.74   |\\n\\nTable 5: Ablation study: \u201cw/o segment embedding\u201d means that the segment embedding is not added into the token representation, but still inserted in the segment-aware self-attention.\\n\\n6.4 Case Study\\n\\nWe perform case study in Chinese-English translation directions, and each case includes the source sentence, translation, incorrect word span, and suggestions. For case 1 in Figure 3, the Chinese word \u201c\u706b\u4e86\u201d (means getting popular) has been wrongly translated into its superficial meaning \u201cfire\u201d, and the proposed model gives the right suggestions when the translator selects \u201cfire\u201d as the incorrect part. Similarly, in case 4, the English word \u201cThursday\u201d has been wrongly translated into \u201c24 \u65e5\u201d, and our model provides three correct alternatives. As for case 2, some important constituents are missed in the translation, which makes the translation not in accordance with the rules of grammar. By selecting the words (\u201cwant to\u201d) neighboring to the position where there are missing constituents, our model can fill in the missed constituents rightly. Case 3 demonstrates that the proposed model can generate more fluent alternatives.\"}"}
{"id": "emnlp-2022-main-353", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inputs Suggestions\\n\\n1 (Zh => En) Src: \u4e00\u9996\u88ab\u79f0\u4e3a\u201c\u795e\u66f2\u201d\u7684\u300a\u751f\u50fb\u5b57\u300b\u5728\u7f51\u4e0a\u706b\u4e86\u3002\\nSrc in pinyin: yi shou bei cheng wei shen qu de sheng pi zi zai wang shang huo le. Translation: A song called \u201cshenqu\u201d \u201crare words\u201d on the internet fire.\\n\\n2 (Zh => En) Src: \u4eca\u5929\u5929\u6c14\u5f88\u4e0d\u9519,\u60f3\u4e00\u8d77\u51fa\u53bb\u901b\u8857\u4e48?\\nSrc in pinyin: jin tian tian qi hen bu cuo, xiang yi qi chu qu guang jie me? Translation: Today is a beautiful day, want to go out shopping together?\\n\\n3 (En => Zh) Src: A new policy was adopted to achieve the peaceful unification of our country\\nTranslation in pinyin: dui yu he ping shi xian zu guo tong yi, yi jing cai qu le xin de zheng ce. Translation in English: For the peaceful reunification of our country, a new policy has been adopted.\\n\\n4 (En => Zh) Src: France would not join a US military invasion of Haiti as part of an effort to restore democratic rule, French Foreign Minister said Thursday.\\nTranslation in pinyin: fa guo wai jiao bu zhang 24 ri biao shi, fa guo bu hui jia ru mei guo dui hai di de jun shi ru qin, zhe shi fa guo hui fu min zhu tong zhi nu li de yi bu fen. Translation in English: French Foreign Minister said on Thursday that France would not join a US military invasion of Haiti as part of an effort to restore democratic rule.\\n\\nRelated Work\\nRelated tasks. Some similar techniques have been explored in CAT. Green et al. (2014) and Knowles and Koehn (2016) study the task of so-called translation prediction, which provides predictions of the next word (or phrase) given a prefix. Huang et al. (2015) and Santy et al. (2019) further consider the hints of the translator in the task of translation prediction. Compared to TS, the most significant difference is the strict assumption of the translation context, i.e., the prefix context, which severely impedes the use of their methods under the scenarios of PE. Lexically constrained decoding which completes a translation based on some unordered words, relaxes the constraints provided by human translators from prefixes to general forms (Hokamp and Liu, 2017; Post and Vilar, 2018; Kajiwara, 2019; Susanto et al., 2020). Although it does not need to re-train the model, its low efficiency makes it only applicable in scenarios where only a few constraints need to be applied. Recently, Li et al. (2021) study the problem of auto-completion with different context types. However, they only focus on the word-level auto-completion, and their experiments are also conducted on the automatically constructed datasets.\\n\\nRelated models. Lee et al. (2021) propose to perform translation suggestion based on XLM-R, where the model is trained to predict the masked span of the translation sentence. During inference, they need to generate multiple inputs for the selected sequence of words, with each input containing a different number of the \u201c[MASK]\u201d token. Therefore, the inference process of XLM-R based model gets complex and time-consuming. With the success on many sequence-to-sequence tasks, Transformer can generate sequences with various lengths. The naive Transformer treats each token in the input sentence without any distinction. Based on Transformer, (Junczys-Dowmunt and Grundkiewicz, 2018) propose the dual-source encoder for the task of PE. Wang et al. (2020) also apply the dual-source encoder to the touch-editing scenario, and they also consider the translator\u2019s actions for PE. In parallel to our work, Zhang et al. (2021) propose a domain-aware self-attention to address the domain adaptation. While their idea is similar to the proposed segment-aware self-attention, they introduce large-scale additional parameters.\\n\\n8 Conclusion and Future work\\nIn this paper, we propose a benchmark for the task of translation suggestion. We construct and share a golden dataset, named WeTS, for the community, and propose several ways for automatically constructing the synthetic corpora which can be used to improve the performance substantially. Additionally, we for the first time propose the segment-aware self-attention based Transformer, named SA-Transformer, which achieves the best performance on all four translation directions. We hope our work will provide a new perspective and spur future researches on TS. There are two promising directions for future work.\"}"}
{"id": "emnlp-2022-main-353", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"directions for the future work. First, we decide to introduce new techniques from recommendation systems to generate more diverse and accurate suggestions. Second, modeling the interactions between the source and translation sentences implicitly may be helpful to improve the performance.\\n\\nLimitations\\nWhile achieving promising performance, the proposed model still has some weaknesses in the real application: 1) The suggestions sometimes have low diversity. This is mainly because that the search space of the beam search is too narrow to extract diverse suggestions (Wu et al., 2020; Sun et al., 2020). This problem could be solved by utilizing some random search strategies during inference or the diverse beam search algorithms (Vijayakumar et al., 2016). 2) The model tends to provide less satisfactory suggestions for the long word spans. Poorer performance for longer sequence is the general weakness for most of the neural models. However, this problem may be more urgent for the proposed TS model since the inputs are much longer than the naive translation model (the concatenation between the source and translation sentences). We believe this problem can be alleviated by using the separate encoders for the source and translation sentences. However, our experimental results show that separate encoders in dual-source Transformer achieves inferior performance as no cross-lingual interactions are modeled. Therefore, a promising direction is that the model encodes the source and translation sentences separately in the lower layers and modeling the interactions in the upper layers. 3) The best suggestion does not always rank in the first position. While the results of the beam search are ranked according to the predicted scores, the ranked positions are not always satisfactory. More features extracted from the source and translation sentences should be considered into the re-ranking process.\\n\\nAnother potential limitation proposed by the anonymous reviewers is that the in-house data used to train the En-Zh model. There are two reasons that we add the in-house En-Zh data: 1) The in-house En-Zh data can be used to train the model as strong as possible, which makes the benchmark solid for the production; 2) While we did not open the in-house corpus, we have released the trained En-Zh NMT model, which makes it still easy to re-produce the results.\"}"}
{"id": "emnlp-2022-main-353", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-353", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Detailed statistics about WeTS\\n\\nThis section presents the detailed statistics about the proposed WeTS. Since the training, validation and test sets are three homogeneous splits from the randomly shuffled annotated corpus, we only report the statistics on the training set.\\n\\nA.1 The number of the incorrect span\\n\\nEach annotated example may contain multiple incorrect spans, we show the number of the incorrect span in each annotated example as Figure 4. We can see that most examples have only a few incorrect spans, and there are more than 70 percent examples containing less than 3 incorrect spans for each translation direction.\\n\\nA.2 The length of the incorrect span\\n\\nFigure 5 represents the length distribution of the incorrect spans. We can find that most of the incorrect spans contain less than 3 words or Chinese characters. This is mainly because of our key rule for annotating the incorrect span as local as possible. Additionally, for all of the four translation directions, the number of the incorrect spans with length 0 ranks top-2 among all the length buckets. This shows that under-translation is still a frequent error of the existing NMT models.\\n\\nA.3 The length of the suggestions\\n\\nFigure 6 shows the length distribution of the suggestions. We can see that in English-to-German, German-to-English and Chinese-to-English, most of the suggestions contain only one word. For English-to-Chinese, most suggestions contain two Chinese characters. Additionally, we can also find that there are quite a few of suggestions with length zero in each translation direction. This shows that over-translation is a non-negligible problem for the existing NMT models.\\n\\nB Pre-processing in detail\\n\\nFor learning the BPE codes on Chinese-English language pairs, the number of the merge operation is set as 64,000. For English-German language pairs, the number of merge operation is 32,000. For constructing the synthetic corpora, we perform randomly sampling on the golden and pseudo parallel corpus. The sizes of the constructed synthetic corpora are listed as Table 6.\\n\\n| Directions                | on golden | on pseudo | with word alignment |\\n|--------------------------|-----------|-----------|---------------------|\\n| En\u21d2De                    | 9.0M      | 9.0M      | 5.8M                |\\n| De\u21d2En                    | 9.0M      | 9.0M      | 5.3M                |\\n| Zh\u21d2En                    | 20M       | 20M       | 19.2M               |\\n| En\u21d2Zh                    | 20M       | 20M       | 18.4M               |\\n\\nTable 6: The sizes about the constructed synthetic corpora. \u201con golden\u201d indicates the method of sampling on the golden parallel corpus.\\n\\nC Experimental settings in detail\\n\\nFollowing the base model in Vaswani et al. (2017), we set the word embedding as 512, dropout rate as 0.1 and the head number as 8. We use beam search with a beam size of 4. The proposed model is implemented based on the open-source toolkit fairseq.\\n\\nhttps://github.com/pytorch/fairseq\"}"}
{"id": "emnlp-2022-main-353", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: The number of incorrect span in each annotated example. The horizontal axis represents the number of incorrect span in each example, and the vertical axis denotes the number of annotated examples.\\n\\nFor generating the synthetic corpus with word alignment, we set $\\\\beta$ as 10. During pre-training, the batch size is set as 81,920 tokens, and the learning rate is set as 0.0008. During fine-tuning, the batch size and learning rate are set as 41,960 and 0.0001 respectively. For the first-phase pre-training, we stop training when the model achieves no improvements for the tenth evaluation on the development set. For the process of second-phase pre-training and fine-tuning, we train the whole model for 200,000 and 100 steps respectively. For calculating the BLEURT score, we use the default models (BLEURT-20) provided by the bleurt toolkit.\"}"}
{"id": "emnlp-2022-main-353", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: The length of the incorrect span. The horizontal axis represents the length of the incorrect span, and \\\"0\\\" means that no incorrect word should be selected but some words should be inserted for the under-translation. The vertical axis denotes the number of the incorrect spans. For Chinese, the length is calculated as the number of Chinese characters included in the incorrect span. For other languages, length is calculated as the number of words.\\n\\nFigure 6: The length of the suggestion. The horizontal axis represents the length of the suggestion, and \\\"0\\\" means that the corresponding incorrect span should be deleted as the over-translation happens. The vertical axis denotes the number of the suggestions. For Chinese, the length is calculated as the number of Chinese characters included in the incorrect span. For other languages, length is calculated as the number of words.\"}"}
