{"id": "emnlp-2023-main-830", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32, a corpus of 32 reasoning-focused text games totalling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 57%. While evaluating simulation fidelity is labor intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high-degree of agreement with expert human ratings. We pose this as a challenge task to spur further development at the juncture of world modeling and code generation.\\n\\n1 Introduction\\n\\nSimulating the world through mental models is a crucial component of human problem solving, inference, and cognition (Barsalou, 1999; Buckner and Carroll, 2007; Addis et al., 2009). Large language models (LLMs) have demonstrated precursors of this ability, such as encoding a wide range of common-sense world knowledge from their training data (Li et al., 2022a). Similarly, LLMs have been used directly as interactive world simulators in text games like AI Dungeon (Walton, 2019), where their capacity to predict tokens in context is leveraged to convert natural language user inputs (e.g. open treasure chest) into plausible environmental observations (e.g. you open the chest and find within a glittering sword).\\n\\nIn this work, we examine instead whether LLMs can be used to generate explicit and task-specific world models expressed as code, providing a more formal and interpretable method to examine a model\u2019s world knowledge. We operationalize this as a problem of generating the complete Python source code of an interactive text game that centers around a particular common-sense task, such as washing dishes with a dishwasher or building a campfire. Although an interactive multi-step simulation of even modest tasks typically requires several hundred lines of code, we show that it is possible for LLMs to generate these simulations using single-shot in-context learning. This is accomplished...\"}"}
{"id": "emnlp-2023-main-830", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"by providing the heavily-templated source code of an existing text game as input, and tasking models with adapting the template to a novel specification, as shown in Figure 1. The template provides a consistent, scalable, and general-purpose code architecture by hierarchically decomposing the simulation into object classes and sub-classes (e.g. device and container), which can be instantiated to make specific game objects (e.g. stove and jug). The template also offers example implementations of common actions (e.g. activating devices or opening containers) and scoring functions that automatically detect task progress.\\n\\nThe contributions of this work are:\\n\\n1. We present BYTE-SIZED 32, a corpus of 32 world models (expressed as text games in PYTHON) centered around tasks that require common-sense reasoning. The corpus includes 20k lines of code (including detailed comments), and is suitable for both in-context learning or producing fine-tuned models.\\n\\n2. We develop a suite of metrics to assess the quality of generated games, including measuring technical aspects of the code, whether a game contains required content, how accurately a game models the physical world, and whether a game is winnable. We show that most of these metrics can be automated with a high agreement to gold human ratings, dramatically reducing the manual labor required to evaluate model-generated simulations.\\n\\n3. We show that a model with a large input context, GPT-4, can produce runnable text games for unseen tasks in 28% of cases using in-context learning alone. When allowed to self-reflect on its own generated code combined with PYTHON interpreter errors that assess syntax issues or API compliance, the model dramatically increases performance, generating runnable simulations in 57% of cases.\\n\\n4. We empirically demonstrate that while current best-generated games frequently include task-critical objects and actions, they only accurately model the physical world in 51% of cases, while being winnable in only 38% of cases. We pose this as a challenge task to spur further development at the juncture of world modeling and code generation.\\n\\n2 Related Work\\n\\nText Games and Virtual Environments: Interactive text environments are an attractive choice for studying embodied agents, owing to their relative simplicity compared to full 3D simulations and ability to model complex and abstract tasks (Jansen, 2021; Li et al., 2021). While early text game research focused on testing agents on a small set of extant \u201cinteractive fiction\u201d games like Zork, recent approaches have leaned towards procedurally generating a wider set of simple text-based games in order to evaluate agents\u2019 ability to generalize (C\u00f4t\u00e9 et al., 2018; Urbanek et al., 2019; Shridhar et al., 2020; Wang et al., 2022). These frameworks typically rely on hand-crafted rules and templates programmatically arranged in novel configurations, though some efforts leverage external data sources (Barros et al., 2016) and generative language models (Fan et al., 2019) as well. In contrast, in this work we require models to produce a novel text game as a complete program, expressed as PYTHON code, using only a single existing game for reference.\\n\\nCode Generation: As large language models have become more capable, interest in their ability to generate working snippets of program code has only grown. Several recent datasets have been proposed to facilitate this research, covering a wide range of programming languages and problem types (Yu et al., 2018; Lin et al., 2018; Austin et al., 2021; Chen et al., 2021). Contemporaneously, improvements in model architecture and training have led to impressive gains in code generation (Chen et al., 2021; Nijkamp et al., 2022; Li et al., 2022b; Fried et al., 2023). The GPT-4 language model (OpenAI, 2023), in particular, has sparked an interest in the use of prompting for code generation tasks, a technique which has led to advancements problem decomposition (Pourreza and Rafiei, 2023) and self-debugging by reflecting on errors (Chen et al., 2023; Olausson et al., 2023). Despite these gains, however, existing code generation benchmarks tend to require short and relatively simple programs. In contrast, here models must generate hundreds of lines of PYTHON code to generate complete and accurate task simulations. Similarly, we show that self-reflection can substantially increase the runnability of even large model-generated simulations.\"}"}
{"id": "emnlp-2023-main-830", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Corpus statistics of BYTE SIZED 32. Values represent average values per game. Tokenization includes comments and was performed with \\\\texttt{tiktoken}.\\n\\n| Statistics              | Value  |\\n|-------------------------|--------|\\n| Lines of PYTHON code    | 618.1  |\\n| Lines of comments       | 198.1  |\\n| Tokens                  | 6792   |\\n| Action verbs            | 9.8    |\\n| Valid actions           | 306.6  |\\n| Object classes          | 5.4    |\\n| Object instances        | 7.4    |\\n| Expert path length      | 12.8   |\\n| Total Games             | 32     |\\n\\nthat can serve as examples in a few-shot generation paradigm. Each game is between 500 and 1000 lines of code (including comments), and encodes a task which requires either scientific or commonsense reasoning to complete. For example, a game might require an agent to apply a bandage, load a dishwasher, or make a campfire. Each game has two components: the game code itself, as well as a detailed task specification in the form of structured comments at the top of each game that provide a detailed summary of the game task, critical objects, actions, distractors, and a high-level solution plan. These components are described below, with corpus summary statistics provided in Table 1, and an example game playthrough shown in Figure 2.\\n\\n3.1 Task Specification\\n\\nThe task specification is a set of structured comments at the start of each game in the corpus that serve as a high-level outline for the critical components of each game. These are intended to provide a high-level scaffold that language models can use to better structure games they generate. The components of the task specification include:\\n\\n- **Task Description**: The task an agent playing the game has to solve \u2013 for example, washing dirty dishes using a dishwasher.\\n- **Task-Critical Objects**: Names of task-critical objects, such as dishes, dish soap, and a dishwasher.\\n- **Actions**: Actions that an agent playing the game can take, such as opening or closing containers, activating or deactivating devices, picking up or putting down objects, and so forth.\\n- **Distractors**: Objects (or actions) that limit or hinder task performance, or that are unrelated to the game task \u2013 for example, adding food that an agent can eat, that creates more dirty dishes.\\n\\n3.2 Game Code\\n\\nTo maximize utility as n-shot training data for code generation tasks, each game in the corpus uses a highly-templated structure consisting of core objects and member functions, shown in Figure 3 and described below. The core architecture and API of these functions mirrors other text game frameworks (Hausknecht et al., 2020) derived from the OPE-NAI GYM specification for reinforcement learning models (Brockman et al., 2016). These include:\"}"}
{"id": "emnlp-2023-main-830", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"World Initialization:\\nInitialize the game world. For example, for the dishwasher game, create a\\nkitchen room that includes dirty dishes, dish soap, a dishwasher, and any other relevant objects.\\n\\nValid Actions:\\nReturn a list of all possible valid actions that an agent could take, given the current\\nenvironment state. For example, take dirty dish, or open dishwasher.\\n\\nTake Action Step:\\nPerform a specific action in the environment. This function returns the observation\\nthat results from that action \u2013 for example, the take dirty dish action might return the observation\\n\\\"the dirty dish is now in your inventory\\\".\\n\\nScoring:\\nReturn an agent's current progress in solving the game task, abstracted to an arbitrary\\nnumerical score, and a set of boolean flags that represent whether the game has been won or lost.\\n\\nWhile the above methods are provided through a main game class (TextGame), each game also in-\\ncludes a large number of classes representing specific game objects. Each game object derives from\\na common class, GameObject, from which generic subclasses that share common methods inherit (e.g.\\nContainers, that can store objects, or Devices, that can be activated), before finally instantiating\\nspecific game objects (e.g. Dish, Dishwasher).\\n\\n4 Evaluating Generated Simulations\\nEvaluating model-generated text games presents a number of challenges. First, because games are\\nlargely open-ended and constrained only by a short task prompt, the evaluation metrics must be robust\\nto a wide range of potential errors and behaviors. Second, evaluating open-ended simulations typi-\\ncally requires manual human evaluation, which is costly and labor intensive. Here, we propose a set\\nof fully-automatic metrics that measure both technical aspects of games \u2013 such as whether the simu-\\nlation runs error-free \u2013 as well as content aspects that measure how well generated simulations ad-\\nhere to task specifications. We then validate these automatic instruments with human ratings.\\n\\n4.1 Metrics\\nEvaluation metrics are described briefly below, with additional details of their computation, val-\\nidation, and prompts described in the Appendix.\\n\\nTechnical Validity:\\nThe technical validity metric measures whether the core member functions of\\n\\n```\\n# Generic parent class for all game objects\\n# Provides getters/setters for object properties\\nclass GameObject:\\n    ...\\n\\n# Parent class for game objects that are containers\\n# Provides methods for adding/removing objects\\nclass Container(GameObject):\\n    ...\\n\\n# Parent class for game objects that are devices\\n# Provides methods for activating/deactivating a device\\nclass Device(GameObject):\\n    ...\\n\\n# Example object: Soap for washing dishes\\nclass DishSoap(GameObject):\\n    ...\\n\\n# Example object: A dish (that can contain food)\\nclass Dish(Container):\\n    ...\\n\\n# Example object: A dishwasher (that can contain dishes, dish soap, and be activated to wash the dishes)\\nclass Dishwasher(Device, Container):\\n    ...\\n```\\n\\nThe main Simulation Class\\n```\\nclass TextGame:\\n    # Creates the game world and populates with game objects (including the kitchen, dishes, dishwasher, etc.)\\n    def initializeWorld(self):\\n        ...\\n\\n    # Returns a string describing the game and task\\n    def getTaskDescription(self):\\n        ...\\n\\n    # Returns an array with all possible valid actions given the current game state\\n    def generateValidActions(self):\\n        ...\\n\\n    # Performs an action (e.g. turn on dishwasher) in the environment, changing the environment state.\\n    def step(self, action):\\n        ...\\n\\n    # Calculate the current game score given progress.\\n    def calculateScore(self):\\n        ...\\n```\\n\\nThe main Entry Point (example of a user playing)\\n```\\nif __name__ == \\\"__main__\\\":\\n    game = TextGame()\\n    print( \\\"Task: \\\" + game.getTaskDescription() )\\n    while not game.gameOver:\\n        actionStr = input( \\\"> \\\" )\\n        observation, score, reward = game.step(actionStr)\\n        print( \\\"Observation: \\\" + observation )\\n        print( \\\"Score: \\\" + score )\\n        print( \\\"Reward: \\\" + reward )\\n    print( \\\"Game Completed. \\\" )\\n    print( \\\"Game Won: \\\" + str(game.gameWon) )\\n```\\n\\nFigure 3: An illustration of the core classes and member functions present in the highly-templated games of the BYTESIZED32 corpus. Each game consists of an average of 618 lines of code, and the example here provides only an overview of a subset of the most important functions.\\n\\na generated text game run without errors by calling them in a PYTHON interpreter and capturing\\nerrors. We measure errors during the game initial-\\nization phase where the simulation environment is being constructed, the valid action generation,\\nwhere the simulation provides a list of all valid actions the agent might take given the current envi-\\nronment state, and the step function, which takes a user-requested action that modifies the environ-\\n\"}"}
{"id": "emnlp-2023-main-830", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The valid action generation and step functions are tested by exhaustively crawling all possible trajectories (i.e., sequences of actions) an agent could take. Because games can have up to 2000 valid actions per step, the path crawling procedure has a limited horizon\u2014typically 3 steps. At each step, we also group actions by their action verb and explore a maximum of 100 actions of each group.\\n\\n**Specification Compliance:**\\nThis metric measures whether the generated game includes the required actions, objects, and distractors required in the task specification. Compliance is measured automatically by supplying the generated game and its task specification to a GPT-4 model, which is then asked a series of true-or-false questions about the presence of each required component. For example, in a boiling water game, one such question is: Does the simulation contain the object \u2018Sink\u2019?\\n\\nTo validate this automatic metric, we compare GPT-4 ratings with gold ratings generated by two expert human annotators, showing moderate-to-strong inter-annotator agreement between GPT-4 and human ratings (Avg. $\\\\kappa = 0.74$; Object: $\\\\kappa = 0.96$; Action: $\\\\kappa = 0.75$; Distractor: $\\\\kappa = 0.50$).\\n\\n**Physical Reality Alignment:**\\nIn addition to technical and specification compliance, we provide a measure of how well the actions in generated games accurately model the constraints of the physical world. For example, a game that lets you take an object out of a closed container (like a fridge) without first opening it is not respecting the constraints of the physical world. Because a simulation necessarily encodes only a small subset of reality, we restrict our measure to only the set of actions implemented by the game and returned by the `POSSIBLE ACTIONS()` function.\\n\\nTo measure physical reality alignment, we crawl a given game up to a depth of 3 steps, then randomly sample 100 trajectories equally distributed across each action a game implements. These trajectories are then provided to GPT-4, with a prompt to provide a binary assessment as to whether the game playthrough up to that point adheres to physical reality, as well as a short text justification for that assessment. To validate this metric, two expert human raters produced gold labels for 200 physical reality alignment judgements. The inter-annotator agreement between human and GPT-4 ratings is strong (Cohen\u2019s $\\\\kappa = 0.89$), demonstrating GPT-4 has a high agreement with humans when making these assessments.\\n\\n**Winnability:**\\nA game is considered winnable if there exists a sequence of actions that, when performed in order, will lead to a winning state of the game. Automatic evaluation of winnability was performed by letting a GPT-4 text game agent play through the games. This agent uses recent prompting techniques such as ReAct (Yao et al., 2023) and Reflexion (Shinn et al., 2023) to provide high-level planning and problem-solving. Manual evaluation was performed by a single human evaluator. Both automatic and manual evaluators attempted to reach the game\u2019s winning state by submitting actions to the game\u2019s `STEP()` function.\\n\\nWe note that this process does not always give an accurate measure of a game\u2019s winnability, as it is possible for an evaluator to fail to find a possible winning trajectory. Nevertheless, we find empirically that in the vast majority of cases a game is either obviously winnable or obviously impossible to win. Overall we find that GPT-4 underestimates game winnability, with inter-annotator agreement between GPT-4 and the human evaluator low (Cohen\u2019s $\\\\kappa = 0.43$). This suggests solving arbitrary text games zero-shot is still beyond the capabilities of GPT-4, a finding consistent with prior research on LLMs as commonsense problem solvers (Bian et al., 2023). As such, we report human evaluations of winnability for our experiments.\\n\\n### 5 Experiments\\n\\nHere we investigate the capacity for a large language model such as GPT-4 to generate the hundreds of lines of code required to generate a working text game simulation centered around unseen tasks on each of the technical and quality metrics described above. Alongside, we examine the extent to which these models can use reflection to increase generation performance.\\n\\n#### 5.1 Model and Prompt\\nAs template games contain up to 10K tokens before including the prompt, we make use of GPT-4 (OpenAI, 2023) with a context window of 32K tokens for each of our experiments. The model prompt includes a 1-shot example (a single PYTHON reference game from the BYTE SIZED 32 corpus), followed by a task specification describing the game the model must generate, drawn from an unseen\\n\\n2See Appendix C for performance of CodeLlama (Rozi\u00e8re et al., 2023) on BYTE SIZED 32 with no finetuning.\"}"}
{"id": "emnlp-2023-main-830", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.2 Evaluation Set\\nIn addition to the 32 training games in the BYTESIZED32 dataset, we also provide an evaluation set in the form of task specifications for 16 additional unseen games. Each game in the evaluation set is explicitly crafted to have highly similar or highly dissimilar characteristics to specific games found in the training set, such as similar or dissimilar objects, actions or distractors.\\n\\n5.3 Reference Game Selection\\nTo improve diversity in generation, we randomly pair each game specification in the evaluation set with six different reference games. Half of these reference games are chosen to have at least some similarity to the evaluation game (i.e., they share a similar object, action, or distractor), while half are chosen to minimize similarity. With 16 game specifications in the test set, this results in a total of 96 model-generated games.\\n\\n5.4 Reflection\\nSimilar to other code generation tasks (Lehman et al., 2022), we hypothesize that self-reflection \u2013 that is, providing the model with error output, and allowing it to iteratively correct its generated code \u2013 will increase overall generation performance. As such, during generation, we provide any error messages generated by the PYTHON interpreter during the technical validity evaluation back to the model, in a self-reflection prompt that requests the model to correct the error. This reflection step is repeated until the game passes all technical validity checks, or a maximum number of reflection steps is reached. In the experiments reported here, the maximum number of reflection steps is 3.\\n\\n6 Results\\nHere, we evaluate all generated games (N = 96) across each metric, reporting results both before and after self-reflection. The results of the technical validity evaluation are shown in Table 2. Model performance on creating game initialization methods is strong overall, beginning at 85%, and increasing to 89% after reflection. Similarly, generating a method that enumerates valid actions for a given step occurs in 80% of generated games before reflection, increasing to 89% after reflection. Generating fully runnable games, which successfully run an exhaustive path crawl of all possible game trajectories up to 3 steps without error, occurs in only 28% of games before reflection, but increases to 57% after reflection \u2013 a substantial increase of 29% gained from the self-reflection process. We show examples of GPT-4 fixing bugs in code via reflection in Appendix Table 7.\\n\\nSelf-reflection also increases physical reality alignment, with a histogram of automatically measured physical reality alignment scores shown in Figure 4. Before reflection, average physical reality alignment across games is 43%, which indicates that GPT-4 finds 43% of randomly sampled paths to fully comply with its expectations of physical reality. After reflection, this increases to 51%, a moderate increase of 8%. Though measured automatically, the strong inter-annotator agreement between human and GPT-4 raters in Section 4 suggests this improvement to be genuine, though it is\"}"}
{"id": "emnlp-2023-main-830", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: A histogram of automatically measured physical reality alignment scores, both before (red) and after (blue) reflection. Asterisks represent average scores (0.43 pre-reflection, 0.51 post-reflection).\\n\\nlikely due \u2013 at least in part \u2013 to an overall increase in non-zero physical reality alignment scores due to more games becoming runnable after reflection. When zero scores are removed from the analysis, the average physical reality alignment scores before and after reflection are 58% (N = 71) and 62% (N = 80), respectively \u2013 or in other words, the effect size of reflection decreases to 4%.\\n\\nSimilarly, self-reflection increases the winnability of games. As shown in Table 3, the winnability of all generated games pre-reflection is 30.2% when measured manually by human experts, with this increasing to 37.5% post-reflection \u2013 a gain of 7.3%.\\n\\nSelf-reflection does not improve every metric we evaluate. Each of the submeasures of specification compliance, including generating task-critical objects, task-critical actions, and game distractors observes a small decrease or no difference post-reflection when measured automatically \u2013 suggesting that self-reflecting on technical validity measures affords limited utility to these measures of simulation content. Still, we observe strong overall performance in specification compliance, with generated games including task-critical objects in nearly every case, task-critical actions in 93.8% of games, while struggling with distractors \u2013 generating these in only 18.8% of games.\\n\\n7 Discussion\\n\\nTo what extent can GPT-4 generate long structured text games using single-shot in-context learning?\\n\\nAt a high level, our results indicate that GPT-4 is frequently capable of generating syntactically valid, templated, and playable programs that are hundreds of lines in length, such as the game shown in Figure 5. Of the generated games, nearly all implement at least one task-critical object, 88% implement at least one task-critical action, and a full 38% allow a user or agent to reach a winning state. A more nuanced interpretation of these results suggests that the model has best learned to successfully replicate the high-level structure of the highly-templated B Tie SIZED 32 game API \u2013 as model performance begins to degrade once we examine the minute details: only 58% of games are robust to a 3-step exhaustive trajectory search, and only 19% of games include a required distractor despite their presence in the reference games. Similarly, while the average training game includes 4396 code tokens, the average model-generated game contains only 3368 code tokens \u2013 or 77% of the length of training games \u2013 suggesting that model-generated games are not yet able to replicate the full level of simulation fidelity provided in the training corpus.\"}"}
{"id": "emnlp-2023-main-830", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How does self-reflection assist game generation?\\n\\nSelf-reflection \u2013 that is, iteratively providing an LLM with error messages from the PYTHON interpreter when running generated games, then asking it to correct those errors (Chen et al., 2023) \u2013 dramatically increases generation performance, most notably in terms of technical validity. We find that three steps of self-reflection increases the generation rate of runnable games from 28% to 57%. Gains from self-reflection are typically largest when detailed and specific error feedback is possible (Olausson et al., 2023). This indicates that similar gains might be achievable on errors in specification compliance, physical reality alignment, and winnability by using the output of our automatic evaluation process. However, the feasibility of this approach is constrained by the time and expense involved in querying the model for reflection \u2013 our results indicate that a single game could include hundreds of small and large errors in total. The cost of reflection might be lowered by the use of code diffs for edits (Lehman et al., 2022) or through batching multiple errors into a single reflection request \u2013 though our pilot experiments on applying these techniques to the long generated programs here indicates that current models might require specialized fine-tuning to do so. Alternatively, open source code generation models are quickly approaching GPT-4 performance in both generation length and accuracy (Li et al., 2023; Luo et al., 2023; Gunasekar et al., 2023; Rozi\u00e8re et al., 2023), suggesting that fine-tuning on the entire BYTEIZED corpus may become viable in the near-term, potentially reducing the number of errors when generating high-fidelity simulations, and reducing dependence on self-reflection.\\n\\nCan we use LLMs to automatically evaluate the output of their own simulations?\\n\\nAutomatic evaluation of model outputs is a vital prerequisite for large-scale experiments in world model generation, where manual evaluation of even a single simulation can require a prohibitive amount of time. The complexity of the task, however, precludes the use of most simple automatic metrics. We find that using language models to automatically crawl and evaluate generated simulations is a viable alternative to time-consuming human annotation for certain measures. This automatic evaluation is valid as long as the inter-annotator agreement between the LLM and human annotators is high. By presenting models with game code or trajectories and requesting targeted, binary judgements, we find it is possible to automatically and reliably rate measures of game specification compliance and physical reality alignment. At the same time, we show that certain metrics remain challenging for automation. Determining winnability of a generated game, in particular, essentially requires a model to solve arbitrary text games \u2013 an active area of research (Jansen, 2021). The automated agent currently underestimates game winnability by about half compared to expert human judgements, though it is plausible that this gap will narrow as the reasoning capabilities of LLMs continue to improve.\\n\\nCan we observe the internal world models of LLMs through the simulations they generate?\\n\\nGenerating world models as code provides a formal and interpretable means to explicitly assess how LLMs understand the world. For instance, the GPT-4 model generated a game that involved burying a box of treasure in a hole but required placing soil back into the hole before placing the treasure box inside. In another generated game, an agent was able to directly place water in its inventory without using any containers. These failure modes indicate situations in which the language model was unable to accurately realize the world knowledge presumably contained within its pretraining data. Even after reflection, only 51% of short 3-step trajectories in GPT-4 generated games accurately modeled the physical world, indicating that constructing correct and explicit world models in code remains a formidable challenge for LLMs.\\n\\nConclusion\\n\\nIn this work, we present BYTEIZED 32, a corpus of small world models expressed as text games centered around specific common-sense tasks. Using a simulation as code generation paradigm, we show that it is possible to use these games, expressed as hundreds of lines of PYTHON code, as templates for in-context learning, and generate novel simulations for unseen tasks. We further show that it is possible to iteratively self-reflect on these large simulations, and improve on technical validity and physical reality alignment metrics by as much as...\"}"}
{"id": "emnlp-2023-main-830", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"29% and 8% respectively. While manually evaluating simulations is labor intensive, we empirically demonstrate that a number of measures of simulation accuracy can be automatically evaluated with moderate-to-strong agreement with expert human ratings. We release this work as open source, and as a challenge task at the intersection of world modeling and code generation, to spur further development in expressing the knowledge contained in language models in more formal and interpretable forms.\\n\\nLimitations\\nThis work examines the ability of LLMs to generate abstract text-based world models and the BYTE-SIZED32 corpus is designed to facilitate that task. As such, games in the corpus are not designed to resemble extant text games or to be entertaining. We perform our experiments in a single-shot regime and do not examine the possibility of including more than one game from the corpus within the 32k token context window of our GPT-4 MODEL. We also do not test recent models with similar context sizes like COLT5 (Ainslie et al., 2023), UNILIMIPER (Bertsch et al., 2023), or CAUDE-100 (PBC, 2023). Both of these are valuable directions for future work.\\n\\nFinally, we perform reflection by regenerating the complete program at each step and target only a single error at a time. This process could be made more efficient by outputting only a code diff and batching multiple errors at once. \\n\\nBroader Impact\\nGenerating Simulations: We provide an initial investigation of formalizing the knowledge captured by language models into interactive simulations expressed as code. This process can be used to inspect and evaluate language models, and the ability to generate simulations on-the-fly has potential applications in games and science.\\n\\nSelf-Evaluation of GPT-4: In spite of recent criticisms of the ability of GPT-4 to evaluate its own output, we empirically validate that this is possible in some cases where strict binary judgements of relatively straightforward common-sense tasks are required. Automatic evaluation is a critical component of any effort at scalable environment generation, as it vastly reduces the amount of human labor required to validate and grade outputs. At the same time, strict binary measures may not be desirable for some metrics, and we leave creating and validating more granular metrics for future work.\\n\\nSelf-Reflection: Self-reflection is a rapidly emerging tool for iteratively improving the accuracy and quality of code generated by large language models. Here we show that with targeted feedback, self-reflection is possible and helpful for large (several hundred line) simulation programs. This potentially enables the generation of increasingly complex programs without sacrificing code quality.\\n\\nIntended Use: The games included in the BYTE-SIZED32 corpus have been designed to study LLMs and there is no guarantee they will be entertaining or useful outside this scope.\\n\\nAcknowledgements\\nWe thank the three anonymous reviewers for their comments. This work supported in part by National Science Foundation (NSF) award #1815948 to PJ, and the Allen Institute of Artificial Intelligence (AI2). We thank Matheus Pereira for help setting up CodeLlama-34b-Instruct and vLLM.\\n\\nReferences\\nDonna Rose Addis, Ling Pan, Mai-Anh Vu, Noa Laiser, and Daniel L Schacter. 2009. Constructive episodic simulation of the future and the past: Distinct sub-systems of a core brain network mediate imagining and remembering. Neuropsychologia, 47(11):2222\u20132238.\\n\\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta\u00f1on, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit Sanghai. 2023. Colt5: Faster long-range transformers with conditional computation.\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.\\n\\nGabriella A.B. Barros, Antonios Liapis, and Julian Toegelius. 2016. Murder mystery generation from open data. In Proceedings of the 7th International Conference on Computational Creativity, ICCC 2016, Proceedings of the 7th International Conference on Computational Creativity, ICCC 2016, pages 197\u2013204. Sony CSL.\"}"}
{"id": "emnlp-2023-main-830", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-830", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022b. Competition-level code generation with alphacode. *Science*, 378(6624):1092\u20131097.\\n\\nXi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D. Ernst. 2018. NL2Bash: A corpus and semantic parser for natural language interface to the Linux operating system. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nZiyang Luo, Can Xu, Pu Zhao, Xubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evolving instruct.\\n\\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. A conversational paradigm for program synthesis. *arXiv e-prints*, pages arXiv\u20132203.\\n\\nTheo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2023. Demystifying GPT self-repair for code generation. *arXiv preprint arXiv:2306.09896*.\\n\\nOpenAI. 2023. GPT-4 technical report. *arXiv preprint arXiv:2303.08774*.\\n\\nAnthropic PBC. 2023. Introducing 100k context windows. https://www.anthropic.com/index/100k-context-windows. [Online; accessed 20-June-2023].\\n\\nMohammadreza Pourreza and Davood Rafiei. 2023. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. *arXiv preprint arXiv:2304.11015*.\\n\\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code.\\n\\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.\\n\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2020. Alfworld: Aligning text and embodied environments for interactive learning. *arXiv preprint arXiv:2010.03768*.\\n\\nJack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rockt\u00e4schel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game.\\n\\nNick Walton. 2019. Ai dungeon. https://github.com/Latitude-Archives/AIDungeon. Accessed: yyyy-mm-dd.\\n\\nRuoyao Wang, Peter Alexander Jansen, Marc-Alexandre C\u00f4t\u00e9, and Prithviraj Ammanabrolu. 2022. Scienceworld: Is your agent smarter than a 5th grader? *ArXiv*, abs/2203.07540.\\n\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models.\\n\\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3911\u20133921, Brussels, Belgium. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-830", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Details on Code Generation\\n\\nIn this work, we make extensive use of OpenAI's API. We use the ChatCompletion mode without system prompt. In all our experiments, we keep the following hyperparameters constant:\\n\\n- temperature=0.0\\n- top_p=1\\n- frequency_penalty=0.0\\n- presence_penalty=0.0\\n\\nA.1 Game Generation\\n\\nTo generate the new games, we use the gpt-4-32k model and the following prompt.\\n\\n**GPT-4 Game Generation Prompt**\\n\\nYou are DeveloperGPT, the most advanced AI developer tool on the planet. You answer any coding question, and provide real useful example code using code blocks. Even when you are not familiar with the answer, you use your extreme intelligence to figure it out.\\n\\nYour task is to write a program that:\\n\\n- is a text-based simulation.\\n- The program should be written in Python.\\n- It should be challenging to the user, testing their common-sense knowledge, and take multiple steps to complete.\\n- If possible, there should be distractor objects and actions that do not help progress, to measure whether the user really knows what they're doing.\\n- You should name all target objects and distractor objects with common-sense names.\\n\\nYour code must contain a class named TextGame. The TextGame class should have the following member functions:\\n\\n- `__init__(self, randomSeed)`\\n- `getTaskDescription(self)`\\n- `generatePossibleActions(self)`\\n- `step(self, actionStr)`\\n- `calculateScore()`\\n\\nHere is a specification of the task that your code should simulate.\\n\\n```python\\n# Task: Create a micro-simulation that models how to heat milk to a temperature that is suitable for a baby using a stove.\\n# Environment: kitchen\\n# Task-critical Objects: Stove, Pot, Milk, Fridge, Thermometer\\n# High-level object classes: Device (Stove, Fridge), Container (Stove, Pot, Fridge)\\n# Critical properties: temperature (Milk), temperature_increase_per_tick (Stove), temperature_decrease_per_tick (fridge), max_temperature (Stove), min_temperature (fridge)\\n# Actions: look, inventory, examine, take/put object, open/close container, turn on/off device, use thermometer on object, feed baby with milk\\n# Distractor Items: None\\n# Distractor Actions: drink milk\\n# High-level solution procedure: open fridge, take pot containing milk, put the pot on the stove, turn on the stove, use the thermometer to monitor the milk temperature till the temperature is suitable for a baby to drink, feed baby\\n```\\n\\nHere is an example of a text-based simulation on a different topic that you can use as a template:\\n\\n```python\\nGAME_CODE\\n```\\n\\nDepending on the length of prompt and the code to generate and the API traffic, each game may require 5-10 minutes to generate. We use stream generation which allows us to recover from a GPT-4 API timeout. The code in the response of GPT-4 is wrapped in a Markdown Python code block (i.e., enclosed with three backticks) which makes it easy to extract. We only keep the code part and discard the rest.\\n\\nA.2 Reflection\\n\\nDuring validity check, when the code encounters an error from the Python interpreter, we use the error message to reflect. For the reflection we use the standard GPT-4 model (i.e., with 8k context) and the following prompt.\\n\\n**GPT-4 Reflection Prompt**\\n\\nYou are DeveloperGPT, the most advanced AI developer tool on the planet. You answer any coding question, and provide real useful example code using code blocks. Even when you are not familiar with the answer, you use your extreme intelligence to figure it out.\\n\\nYour task is to correct a program that is a text-based simulation. Here is the code of the simulation:\\n\\n```python\\nGAME_CODE\\n```\\n\\nHere is the error message from a Python interpreter:\\n\\n```python\\nERROR_MESSAGE\\n```\\n\\nYou should respond all the code with your fix. Do not respond anything else.\\n\\nB Additional Notes on Evaluation Metrics\\n\\nB.1 Technical Validity\\n\\nValidity measurements are reported in order, such that failure of a function called earlier in the API implies failure for all subsequent tests. We note, however, that the game initialization functions are evaluated only once, at the beginning of the game, while the GENERATE POSSIBLE ACTIONS() and the STEP() function are necessarily evaluated at each step.\\n\\nB.2 Specification Compliance\\n\\nThe full GPT-4 prompt used to generate the true-or-false evaluations of specification compliance is provided below:\\n\\n**GPT-4 Specification Compliance Prompt**\\n\\nYou are DeveloperGPT, the most advanced AI developer tool on the planet. You answer any coding question, and provide real useful example code using code blocks. Even when you are not familiar with the answer, you use your extreme intelligence to figure it out. Your task is to evaluate a program that is a text-based simulation.\\n\\nHere is a specification of the simulation: \\n\\n```python\\nGAME_SPEC\\n```\\n\\nHere is the code of the simulation: \\n\\n```python\\nGAME_CODE\\n```\\n\\nAnswer the following question based on the given specification and the simulation code: \\n\\n```python\\nEVAL_QUESTION\\n```\"}"}
{"id": "emnlp-2023-main-830", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 4:\\nManual (human) and automatic (GPT-4) evaluation results of the best-generated games (i.e. games after reflection, \\\\(N = 96\\\\)) for both specification compliance and winnability.\\n\\n|                         | Manual | Automatic | \\\\(\\\\Delta\\\\) |\\n|-------------------------|--------|-----------|------------|\\n| **Specification Compliance** |        |           |            |\\n| Task-critical objects    | 97.2%  | 100.0%    | 2.8%       |\\n| Task-critical actions    | 87.5%  | 93.8%     | 6.3%       |\\n| Distractors              | 37.5%  | 18.8%     | 18.7%      |\\n| Winnability              | 37.5%  | 17.8%     | 19.7%      |\\n\\nDifference scores reflect the difference between automatic and manual ratings, showing a moderate overall agreement for specification compliance (Avg. \\\\(\\\\kappa = 0.74\\\\); Object: \\\\(\\\\kappa = 0.96\\\\); Action: \\\\(\\\\kappa = 0.75\\\\); Distractor: \\\\(\\\\kappa = 0.50\\\\)) and modest agreement for winnability (\\\\(\\\\kappa = 0.43\\\\)).\\n\\nBecause we observe variance in GPT-4's answer on a few games, even when using a temperature of zero, we run the same measure 31 times and take the final result as the majority vote.\\n\\nThe upper part of Table 4 shows the manual and GPT-4 automatic evaluation results of specificaiton compliance. The inter-annotator agreement between GPT-4 and the human evaluators are high (average Cohen's \\\\(\\\\kappa = 0.71\\\\)). We note that the manual evaluation differs from the automatic evaluation in only 8 of the 96 games, indicating that automatic evaluation is a viable substitute for costly human evaluation of these metrics. We also note that the automatic evaluation most frequently differed from the manual evaluation in the distractors section, which is also the section that proved the most difficult in terms of generation.\\n\\n### B.3 Physical Reality Alignment\\n\\nThe process of generating sample trajectories for automatic evaluations occurs in two steps. First, we perform a breadth-first crawl of the game using the action strings returned at each step by the \\\\texttt{GATEPPOSSIBLEACTIONS()} function. At each step we maintain a list of counts for each action \\\"verb,\\\" which is extracted from a valid action string by taking the first token. When we perform a recursive search from a given step, we keep only 10 paths for each action verb. We restrict our search to a maximum depth of 3 actions and stop after 25,000 paths have been generated. In addition, if the game produces an error, then the error message is recorded as the observation from that step and the search continues.\\n\\nAfter the initial set of paths has been generated, we group the set by the last action verb used in each path. We then generate a subsample of 100 paths by taking an even number of paths from each group. For instance, if the actions \\\\texttt{TAKE}, \\\\texttt{PUT}, and \\\\texttt{MOVE} occur as the last actions in our set of 25,000 paths then we subsample 33 paths for each action and 1 path randomly. Each subsampled path is sent to GPT-4, along with the game's task description, which is then asked to determine whether every action in the path and its accompanying observation align with physical reality. The GPT-4 evaluation prompt is provided below:\\n\\n**GPT-4 Physical Reality Alignment Prompt**\\n\\nIn the playthrough of the text game below, I would like you to describe whether the game engine (i.e. the observations it returns in response to actions) are physically accurate models of the world or whether they don't make sense.\\n\\nAn example of not making sense would be being able to take an action from a container (like a fridge) without having opened it first. In addition, if an action produces an error from the game, then it automatically fails to accurately model the world and does not make sense.\\n\\nPlease restrict your evaluation only to the short playthrough, and the specific actions chosen, without speculating about other actions.\\n\\nNote: Objects can be manipulated by the agent without first being explicitly picked up, as long as they are in the environment, and readily accessible (e.g. not in a closed container).\\n\\nThe evaluation should be binary (\\\"yes\\\" or \\\"no\\\"), except in the cases where the code generated an error, when the evaluation should be \\\"error\\\".\\n\\nHere is an example output format: ```{\"evaluation\":\"no\",\"short_justification\": \"could take an object (banana) from the closed fridge without having to first open the fridge\"}```\"}"}
{"id": "emnlp-2023-main-830", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are playing a text-based game. Interact with the environment to solve a task.\\n\\nHere is an example.\\n\\nThe game you are about to play only understands one command from the following list of commands:\\n\\nPrepend your thoughts with 'think:' when planning your next steps. When you think the task is completed, say 'done'. If you think the task can't be completed at all, say 'bug'.\\n\\nThe IN_CONTEXT_EXAMPLE used in the GPT-4 Winnability Prompt\\n\\nTask Description: Your task is to make ice cubes.\\n\\nInitial Observation: You find yourself in a kitchen. In the kitchen, you see:\\n- yourself\\n- a freezer that is currently closed\\n- a sink that is currently off and that is empty\\n- a pot that is empty\\n- a peanut butter\\n- a orange\\n\\n> think: I will need some container to make the ice cubes in. I will need to fill the container with water and then put it in the freezer. I will need to wait for the water to freeze. I will need to remove the ice cubes from the container.\\n\\n> think: I will start by looking for a container.\\n\\n> open freezer\\n\\nThe freezer is now open.\\n\\n> examine freezer\\n\\na freezer that is currently open and contains the following items:\\n- an ice cube tray that is empty\\n\\n> take ice cube tray\\n\\nThe ice cube tray is removed from the freezer. You put the ice cube tray in your inventory.\\n\\n> think: I will need to fill the ice cube tray with water. Based on the possible commands, I have to move the ice cube tray to the sink and then turn on the sink.\\n\\n> put ice cube tray in sink\\n\\nThe ice cube tray is removed from the agent.\\n\\nThe ice cube tray is placed in the sink.\\n\\n> look\\n\\nYou find yourself in a kitchen. In the kitchen, you see:\\n- yourself\\n- a freezer that is currently open and empty\\n- a sink that is currently off and that contains one or more items.\\n- a pot that is empty\\n- a peanut butter\\n- a orange\\n\\n> examine sink\\n\\na sink that is currently off and that contains the following items:\\n- an ice cube tray that is empty\\n\\n> turn on sink\\n\\nThe sink is now turned on.\\n\\n> examine sink\\n\\na sink that is currently on and that contains the following items:\\n- an ice cube tray that looks to have some water in it\\n\\n> turn off sink\\n\\nThe sink is now turned off.\\n\\n> think: I will need to put the ice cube tray in the freezer and wait for the water to freeze.\\n\\n> take ice cube tray\\n\\nThe ice cube tray is removed from the sink. You put the ice cube tray in your inventory.\\n\\n> look\\n\\nYou find yourself in a kitchen. In the kitchen, you see:\\n- yourself\\n- a freezer that is currently open and empty\\n- a sink that is currently off and that is empty\\n- a pot that is empty\\n- a peanut butter\\n- a orange\\n\\n> put ice cube tray in freezer\\n\\nThe ice cube tray is removed from the agent.\\n\\nThe ice cube tray is placed in the freezer.\\n\\n> close freezer\\n\\nThe freezer is now closed.\\n\\n> look\\n\\nYou find yourself in a kitchen. In the kitchen, you see:\\n- yourself\\n- a freezer that is currently closed\\n- a sink that is currently off and that is empty\\n- a pot that is empty\\n- a peanut butter\\n- a orange\\n\\n> look\\n\\nYou find yourself in a kitchen. In the kitchen, you see:\\n- yourself\\n- a freezer that is currently closed\\n- a sink that is currently off and that is empty\\n- a pot that is empty\\n- a peanut butter\\n- a orange\\n\\n> open freezer\\n\\nThe freezer is now open.\\n\\n> examine freezer\\n\\na freezer that is currently open and contains the following items:\\n- an ice cube tray that looks to have some ice in it\\n\\nCurrent step: 18\\nScore: 1\\nReward: 0\\nGame Over: True\\nGame Won: True\\n\\nFigure 6: Prompt used for the automatic evaluation of the winnability of generated games.\"}"}
{"id": "emnlp-2023-main-830", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Technical validity measurements of games generated with CodeLlama before reflection (0), and after up to three reflection steps. Values represent the proportion of games ($N = 96$) passing a given test after a given number of reflection steps.\\n\\n| Reflection Measurement | Before | After | \u2206 |\\n|------------------------|--------|-------|---|\\n| Specification Compliance | 96.9% | 78.1% | -18.8% |\\n| Task-critical objects | 100% | 90.6% | -9.4% |\\n| Task-critical actions | 18.8% | 9.4% | -9.4% |\\n| Winnability | 4.2% | 4.2% | 0% |\\n\\nTable 6: Specification compliance and winnability measurements for games generated with CodeLlama before and after reflection. Specification compliance is measured automatically, while winnability is measured manually by human experts. Overall, each measure shows non-negligible decreases post-reflection due to CodeLlama\u2019s difficulty with performing meaningful reflection on large pieces of code.\\n\\nC Open-Source Language Model\\n\\nWe report here the results when using the open source language model CodeLlama (Rozi\u00e8re et al., 2023) served using vLLM (Kwon et al., 2023) with maximum context length of 32k and a dynamic RoPE scaling factor of 2. We opted for CodeLlama because of its capacity to extrapolate up to context size of 100k tokens. While its one-shot code generation performance is decent with 20.8% runnable games compared to 28.1% for GPT-4, this result is tempered by CodeLlama\u2019s apparent difficulty with performing meaningful reflection on these large pieces of code. As shown in Table 5, some of the revised games degrade right after the first reflection and never fully recover. The two main failure cases for doing reflection with CodeLlama is 1) partial code generation and 2) generating the full code but without including the fix even though CodeLlama\u2019s responses mention the issue and how to fix it.\"}"}
{"id": "emnlp-2023-main-830", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Example of GPT-4 agent detecting an issue\\n\\nHere is the task:\\n\\nYou find yourself in a room. In the room, you see: a vending machine, a quarter coin, a dime coin, a nickel coin, a penny coin, and yourself.\\n\\nYour task is to buy a snack from the vending machine using the coins available in the room.\\n\\n> think: I need to collect the coins and use them in the vending machine to buy a snack. The task does not specify which snack to buy, so any snack will do.\\n\\nOK.\\n\\n> take quarter\\n\\nThe quarter is removed from the room. You put the quarter in your inventory.\\n\\n> take dime\\n\\nThe dime is removed from the room. You put the dime in your inventory.\\n\\n> take nickel\\n\\nThe nickel is removed from the room. You put the nickel in your inventory.\\n\\n> take penny\\n\\nThe penny is removed from the room. You put the penny in your inventory.\\n\\n> think: Now that I have all the coins, I need to use them in the vending machine to purchase a snack.\\n\\nOK.\\n\\n> put quarter in vending_machine\\n\\nYou put the quarter in the vending machine. Total money in the vending machine: $0.25\\n\\n> put dime in vending_machine\\n\\nYou put the dime in the vending machine. Total money in the vending machine: $0.35\\n\\n> put nickel in vending_machine\\n\\nYou put the nickel in the vending machine. Total money in the vending machine: $0.40\\n\\n> put penny in vending_machine\\n\\nYou put the penny in the vending machine. Total money in the vending machine: $0.41\\n\\n> select chocolate\\n\\nNot enough money in the vending machine to buy the chocolate. You need $1.59 more.\\n\\n> select chips\\n\\nNot enough money in the vending machine to buy the chips. You need $0.59 more.\\n\\n> think: I don't have enough money to buy any snack. Task can't be completed.\\n\\nOK.\\n\\n> bug\\n\\nFigure 7: An example of using GPT-4 agent to detect issue with a generated game.\"}"}
{"id": "emnlp-2023-main-830", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Error invalid syntax (line 293)\\n\\n```python\\n293 def step(self, actionStr):\\n338 reward = self.score - lastScore\\n```\\n\\nError 'TextGame' object has no attribute 'actionInventory'\\n\\n```python\\n290 def actionInventory(self):\\n291 inventory = self.agent.contains\\n292 if len(inventory) == 0:\\n293 return \\\"You have nothing in your inventory.\\\"\\n294 else:\\n295 inventory_str = \\\"In your inventory, you have:\\\\n\\\"\\n296 for item in inventory:\\n297 inventory_str += \\\"\\\\t\\\" + item.makeDescriptionStr() + \\\"\\\\n\\\"\\n298 return inventory_str\\n```\\n\\nError local variable 'measuring_cup' referenced before assignment\\n\\n```python\\n457 stone = None\\n458 measuring_cup = None\\n459 scale = None\\n468 if stone is not None and measuring_cup is not None and scale is not None:\\n469 if stone.parentContainer == measuring_cup:\\n470 self.score += 1\\n471 if stone.parentContainer == scale:\\n472 self.score += 1\\n473 if measuring_cup.getProperty(\\\"containsLiquid\\\"):\\n474 self.score += 1\\n475 if self.score == 3:\\n476 self.gameOver = True\\n477 self.gameWon = True\\n```\\n\\nTable 7: Examples of GPT-4 fixing bugs via reflection. Note our reflection approach generates the entire code rather than the patches, we show difference between code before and after reflection for clarity purpose.\"}"}
