{"id": "acl-2022-short-92", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CoDA21: Evaluating Language Understanding Capabilities of NLP Models With Context-Definition Alignment\\n\\nL\u00fctfi Kerem Senel, Timo Schick, and Hinrich Sch\u00fctze\\n\\nCenter for Information and Language Processing (CIS), LMU Munich, Germany\\nlksenel@gmail.com, schickt@cis.lmu.de\\n\\nAbstract\\n\\nPretrained language models (PLMs) have achieved superhuman performance on many benchmarks, creating a need for harder tasks. We introduce CoDA21 (Context Definition Alignment), a challenging benchmark that measures natural language understanding (NLU) capabilities of PLMs: Given a definition and a context each for \\\\( k \\\\) words, but not the words themselves, the task is to align the \\\\( k \\\\) definitions with the \\\\( k \\\\) contexts. CoDA21 requires a deep understanding of contexts and definitions, including complex inference and world knowledge. We find that there is a large gap between human and PLM performance, suggesting that CoDA21 measures an aspect of NLU that is not sufficiently covered in existing benchmarks.\\n\\n1 Introduction\\n\\nIncreasing computational power along with the design and development of large and sophisticated models that can take advantage of enormous corpora has drastically advanced NLP. For many tasks, finetuning pretrained transformer-based language models (Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2018) has improved the state of the art considerably. Language models acquire knowledge during pretraining that is utilized during task-specific finetuning. On benchmarks that were introduced to encourage development of models that do well on a diverse set of NLU tasks (e.g., GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019)), these models now achieve superhuman performance (He et al., 2020). The pretrain-then-finetune approach usually requires a great amount of labeled data, which is often not available or expensive to obtain, and results in specialized models that can perform well only on a single task. Recently, it was shown that generative language models can be applied to many tasks without finetuning when the task is formulated as text generation and the PLM is queried with a natural language prompt (Radford et al., 2019; Brown et al., 2020).\\n\\nMotivated by recent progress in zero-shot learning with generative models as well as the need for more challenging benchmarks that test language understanding of language models, we introduce CoDA21 (CoContext Definition Alignment), a difficult benchmark that measures NLU capabilities of PLMs for the English language. Given a definition and a context each for \\\\( k \\\\) words, but not the words themselves, the task is to align the \\\\( k \\\\) definitions with the \\\\( k \\\\) contexts. In other words, for each definition, the context in which the defined word is most likely to occur has to be identified. This requires (i) understanding the definitions, (ii) understanding the contexts, and (iii) the ability to match the two. Since the target words are not given, a model must be able to distinguish subtle meaning differences between different contexts/definitions to be successful. To illustrate the difficulty of the task, Figure 1 shows a partial example for \\\\( k = 4 \\\\) (see Table 5 in the supplementary for the full example).\"}"}
{"id": "acl-2022-short-92", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ample). We see that both complex inference (e.g., <XXX> can give rise to a cloud by being kicked up \u21d2<XXX> must be dry \u21d2<XXX> can be dust, but not soil) and world knowledge (what materials are typical for monuments?) are required for CoDA21.\\n\\nWe formulate the alignment task as a text prediction task and evaluate, without finetuning, three PLMs on CoDA21: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and GPT-2 (Radford et al., 2019). Poor performance of the PLMs and a large gap between human and PLM performance suggest that CoDA21 is an important benchmark for designing models with better NLU capabilities.\\n\\n2 CoDA21\\n2.1 Dataset\\nWe construct CoDA21 by first deriving a set \\\\( G \\\\) of \\\\( \\\\{G_1, G_2, ... \\\\} \\\\) from Wordnet (Miller, 1995). A synset group \\\\( G_i \\\\) is a group of synsets whose meanings are close enough to be difficult to distinguish (making the task hard), but not so close that they become indistinguishable for human and machine. In a second step, each synset group \\\\( G_i \\\\) is converted into a CoDA21 group \\\\( G_i^+ \\\\) \u2013 a set of triples, each consisting of the synset, its definition, and a corpus context. A CoDA21 group can be directly used for one instance of the CoDA21 task.\\n\\nSynset groups. Each synset group \\\\( G \\\\) consists of \\\\( 5 \\\\leq k \\\\leq 10 \\\\) synsets. To create a synset group, we start with a parent synset \\\\( \\\\hat{s} \\\\) and construct a co-hyponym group \\\\( \\\\bar{G}(\\\\hat{s}) \\\\) of its children: \\\\( \\\\bar{G}(\\\\hat{s}) = \\\\{s | s < \\\\hat{s}, s \\\\notin D \\\\} \\\\) where \\\\(<\\\\) is the hyponymy relation between synsets and \\\\( D \\\\) is the set of synsets that have already been added to a synset group. The intuition for grouping synsets with a common parent is that words sharing a hypernym are difficult to distinguish (as opposed to randomly selected words).\\n\\nWe iterate \\\\( \\\\hat{s} \\\\) through all nouns and verbs in WordNet. At each iteration, we get all hyponyms of \\\\( \\\\hat{s} \\\\) that have not been previously added to a synset group; not reusing a synset ensures that different CoDA21 subtasks are not related and so no such relationships can be exploited.\\n\\nWe extract synset groups from co-hyponym groups by splitting them into multiple chunks of size \\\\( k \\\\). In an initial exploration, we found that the task is hard to solve for human subjects if two closely related hyponyms are included, e.g., \u201cclementine\u201d and \u201ctangerine\u201d. We therefore employ clustering to assemble a set of mutually dissimilar hyponyms. We first compute a sentence embedding for each hyponym definition using the stsb-distilbert-base Sentence Transformer model \\\\( ^2 \\\\).\\n\\nWe then cluster the embeddings using complete-link clustering, combining the two most dissimilar clusters in each step. We stop merging before the biggest cluster exceeds the maximum group size (\\\\( k = 10 \\\\)) or before the similarity between the last two combined clusters exceeds the maximum similarity (\\\\( \\\\theta = 0.8 \\\\)). The largest cluster \\\\( G \\\\) is added to the set \\\\( G \\\\) of synset groups. We then iterate the steps of (i) removing the synsets in the previous largest cluster \\\\( G \\\\) from \\\\( \\\\bar{G}(\\\\hat{s}) \\\\) and (ii) running complete-link clustering and adding the resulting largest cluster \\\\( G \\\\) to \\\\( G \\\\) until fewer than five synsets remain in \\\\( \\\\bar{G}(\\\\hat{s}) \\\\) or no cluster can be formed whose members have a similarity of less than \\\\( \\\\theta \\\\).\\n\\nCoDA21 groups. For each synset \\\\( s \\\\), we extract its definition \\\\( d(s) \\\\) from WordNet and a context \\\\( c(s) \\\\) in which it occurs from SemCor \\\\( ^3 \\\\) (Miller et al., 1994). SemCor is an English corpus tagged with WordNet senses. Let \\\\( C(s) \\\\) be the set of contexts of \\\\( s \\\\) in SemCor. If \\\\( |C(s)| > 1 \\\\), we use as \\\\( c(s) \\\\) the context in which bert-base-uncased \\\\( ^4 \\\\) predicts \\\\( s \\\\) with the highest log probability when it is masked, where \\\\( s \\\\) is the word tagged with the sense \\\\( s \\\\) \u2013 this favors contexts that are specific to the meaning of the synset. Finally, we convert each synset group \\\\( G_i \\\\) in \\\\( G \\\\) to a CoDA21 group \\\\( G_i^+ \\\\):\\n\\n\\\\[\\nG_i^+ = \\\\{ (s_j, d(s_j), c(s_j)) | s_j \\\\in G_i \\\\}\\n\\\\]\\n\\nThat is, a CoDA21 group \\\\( G_i^+ \\\\) is a set of triples of sense, definition and context. In PLM evaluation, each CoDA21 group \\\\( G_i^+ \\\\) gives rise to one context-definition alignment subtask.\\n\\nWe name the resulting dataset CoDA21-noisy-hard: noisy because if \\\\( |C(s)| \\\\) is small, the selected context may not be informative enough to identify the matching definition; hard because the synsets in a CoDA21 group are taxonomic sisters, generally with similar meanings despite the clustering-based limit on definition similarity. We construct a clean version of the dataset by only using synsets with \\\\( |C(s)| \\\\geq 5 \\\\). We also construct an easy version by...\"}"}
{"id": "acl-2022-short-92", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: CoDA21 group (G) statistics, USC: Unique Synset Count\\ntaking the \\\"hyponym grandchildren\\\" \\\\(s\\\\) of a parent synset \\\\(s\\\\) \\\\((s < l \\\\land l < \\\\hat{s})\\\\) instead of its hyponym children. This reduces the similarity of synsets in a CoDA21 group, making the task easier. Table 1 gives dataset statistics.\\n\\n2.2 Alignment\\n\\nRecall the CoDA21 task: given a definition and a context each for \\\\(k\\\\) words (but not the words themselves), align the \\\\(k\\\\) definitions with the \\\\(k\\\\) contexts. That is, we are looking for a bijective function (a one-to-one correspondence) between definitions and contexts. Our motivation in designing the task is that we want a hard task (which can guide us in developing stronger natural language understanding models), but also a task that is solvable by humans. Our experience is that humans can at least partially solve the task by finding a few initial \\\"easy\\\" context-definition matches, removing them from the definition/context sets and then match the smaller remaining number of definitions/contexts.\\n\\nThe number of context-definition pairs scales quadratically \\\\(O(k^2)\\\\) with \\\\(k\\\\) and the number of alignments factorially \\\\(O(k!)\\\\). We restrict \\\\(k\\\\) to \\\\(k \\\\leq 10\\\\) to make sure that we do not run into computational problems and that humans do not find the task too difficult.\\n\\nIn order to connect contexts to definitions without using the target words, we replace the target words by a made-up word. This setup resembles the incidental vocabulary acquisition process in humans. Let \\\\(t\\\\) be a target word, \\\\(c\\\\) a context in which \\\\(t\\\\) occurs and \\\\(m\\\\) a made-up word. To test PLMs on CoDA21, we use the following pattern:\\n\\n\\\\[ Q(c,m) = c \\\\]\\n\\nDefinition of \\\\(m\\\\) is where \\\\(c\\\\) with occurrences of \\\\(t\\\\) replaced by \\\\(m\\\\).\\n\\nWe calculate the match score of a context-definition pair \\\\((c,d)\\\\) as \\\\(\\\\log P(d|Q(c,m))\\\\), i.e.,\\n\\n\\\\[ \\\\log P(d|Q(c,m)) \\\\]\\n\\nWhen the target word is a verb (i.e., verb subset of a CoDA21 dataset), we add \\\"to\\\" at the end of our pattern.\\n\\nWe calculate \\\\(P(d|Q(c,m))\\\\) for a masked language model (MLM) \\\\(M\\\\) and an autoregressive language model (ALM) \\\\(A\\\\) as follows:\\n\\n\\\\[ P_M(d|Q'(c,m)) = \\\\prod_{|d|} P(d_i|Q'(c,m),d_{i-1}) \\\\]\\n\\n\\\\[ P_A(d|Q'(c,m)) = \\\\prod_{|d|} P(d_i|Q'(c,m),d_{i-1},...,d_{i-1}) \\\\]\\n\\nwhere \\\\(Q'(c,m) = Q(c,m)\\\\), \\\\(d_i\\\\) is the \\\\(i\\\\)th word in definition \\\\(d\\\\) and \\\\(d_{i-1}\\\\) is the definition with the \\\\(i\\\\)th word masked.\\n\\nWe evaluate the MLMs BERT and RoBERTa and the ALM GPT-2. We experiment with both base and large versions of BERT and RoBERTa and with all four sizes of GPT-2 (small, medium, large, xl), for a total of eight models, to investigate the effect of model size on performance.\\n\\nThe made-up word \\\\(m\\\\) should ideally be unknown so that it does not bias the PLM in any way. However, there are no truly unknown words for the models we investigate due to the word-piece tokenization they apply to the input. Any made-up word that is completely meaningless to humans will have a representation in the models' input space based on its tokenization. To minimize the risk that the meaning of the made-up word may bias the model, we use \\\\(m = bkatuhla\\\\), a word with an empty search result on Google that most likely never appeared in the models' pretraining corpora.\\n\\nIn addition to PLMs, we also evaluate 2 recent sentence transformer models (Reimers and Gurevych, 2019), paraphrase-mpnet-base-v2 (mpnet) and paraphrase-MiniLM-L6-v2 (MiniLM), and fastText static embeddings (Mikolov et al., 2018). To calculate the match score of a context-definition pair, we first remove the target word from the context and represent contexts and definitions as vectors. For sentence transformers, we obtain these vectors by simply encoding the input sentences. For fastText, we average the vectors of the\"}"}
{"id": "acl-2022-short-92", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"words in contexts and definitions. We then calculate the match score as the cosine similarity of context and definition vectors.\\n\\n3 Results\\n\\nTable 2 presents average accuracy of the investigated models on the four CoDA21 datasets. As can be seen, fastText performs only slightly better than random. MLMs also perform better than random chance by only a small margin. This poor performance can be partly explained by the generation style setup we use, which is not well suited for masked language models. Even the smallest GPT-2 model performs considerably better than RoBERTa-large, the best performing MLM. Performance generally improves with model size. GPT-2 \\\\( \\\\texttt{xl} \\\\) achieves the best results among the LMs on almost all datasets. Interestingly, sentence transformer \\\\texttt{all-mpnet-base-v2} performs comparably to GPT-2 \\\\( \\\\texttt{xl} \\\\) on most datasets despite its simple, similarity based matching compared to generation based matching of GPT-2 models.\\n\\nBased on this observation it can be argued that current state-of-the-art language models fail to perform complex, multi-step reasoning and inference which are necessary to solve the CoDA21 tasks.\\n\\nOverall, MLMs perform slightly better on verbs than nouns while the converse is true for GPT-2. As expected, all models perform better on the \\\\texttt{easy} datasets. Performance on \\\\texttt{noisy} and \\\\texttt{clean} datasets are comparable; this indicates that our contexts are of high quality even for the synsets with only a few contexts.\\n\\nHuman performance on CoDA21. We asked two NLP PhD students to solve the task on S20, a random sample of size 20 from the noun part of CoDA21-\\\\texttt{clean-easy}. Table 2 shows results on S20 for these two subjects and our models. Human performance is 0.86 \u2013 compared to 0.48 for GPT-2 \\\\( \\\\texttt{xl} \\\\), the best performing model. This difference indicates that there is a large gap in NLU competence between current language models and humans and that CoDA21 is a good benchmark to track progress on closing that gap.\\n\\nTo investigate the effect of the made-up word \\\\( m \\\\), we experiment with several other words on the noun part of CoDA21-\\\\texttt{clean-easy}. Specifically, we investigate another nonce word \\\\texttt{opyatzel}, a single letter \\\\texttt{x} and two frequent words \\\\texttt{orange} and \\\\texttt{cloud}. Table 3 shows the results of the models for different made-up words. MLMs do not show significant variability in performance, and perform comparably poor for all words tried. GPT2 versions, which perform considerably better than MLMs on CoDA21, perform similarly for the two nonce words and single letter \\\\texttt{x}, which do not have a strong meaning. Their performance drops significantly when the two frequent words are used as the made-up word, due to the effect of prior knowledge models have about these words.\\n\\nTo investigate the effect of the pattern, we compared our pattern \\\\( Q(c,m) \\\\) with two alternative patterns by evaluating GPT-2 \\\\( \\\\texttt{xl} \\\\) on the noun part of CoDA21-\\\\texttt{clean-easy}. Patterns and the evaluation results are shown in Table 4. The results suggest that the effect of the pattern on performance is minimal.\\n\\nEffect of the alignment setup. We constructed CoDA21 as an alignment dataset which uses the fact that matching between the definitions and contexts is one-to-one. This setup makes the task...\"}"}
{"id": "acl-2022-short-92", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pattern Acc\\ncm Definition of m is 0.49\\nTable 4: Effect of the pattern on the performance of GPT2-xl on the noun part of CoDA21-clean-easy.\\n\\nTable 5 (in the Appendix) presents a sample of size 7 from the noun part of the CoDA21-clean-easy dataset. Figure 2 displays all 49 match scores of the context-definition pairs for this sample obtained using GPT-2-xl model on CoDA21-clean-easy dataset using this simple matching approach which yielded 0.38 average accuracy compared to the 0.49 accuracy achieved with the alignment setup. This result suggests that language models can also make use of the alignment style evaluation, similar to humans.\\n\\nAlignment setup enabled the model to match second and third definitions with their corresponding contexts even though their match scores are not the highest ones.\\n\\nTo get a better sense of why the task is hard for PLMs, we give an example, from the CoDA21 subtask in Figure 1 (also Figure 2 and Table 5 refer to the same subtask), of a context-definition match that is scored highly by GPT-2-xl, but is not correct.\\n\\nContext: \u201cthese bees love a fine-grained <XXX> that is moist\u201d.\\nDefinition: \u201cfine powdery material such as dry earth or pollen\u201d. (context 6 and definition 1 in Figure 2) GPT-2-xl most likely gives a high score because it has learned that bees and pollen are associated. It does not understand that the mutual exclusivity of \u201cmoist\u201d and \u201cpowdery\u201d makes this a bad match.\\n\\n4 Related Work\\nThere are many datasets (Levesque et al., 2012; Rajpurkar et al., 2016; Williams et al., 2018) for evaluating language understanding of models. Many adopt a text prediction setup: Lambada (Paperno et al., 2016) evaluates the understanding of discourse context, StoryCloze (Mostafazadeh et al., 2016) evaluates commonsense knowledge and so does HellaSwag (Zellers et al., 2019), but examples were adversarially mined. LAMA (Petroni et al., 2019) tests the factual knowledge contained in PLMs. In contrast to this prior work, CoDA21 goes beyond prediction by requiring the matching of pieces of text. WIC (Pilehvar and Camacho-Collados, 2019) is also based on matching, but CoDA21 is more complex (multiple contexts/definitions as opposed to a single binary match decision) and is not restricted to ambiguous words. WNLaMPro (Schick and Sch\u00fctze, 2020) evaluates knowledge of subordinate relationships between words, and WDLaMPro (Senel and Sch\u00fctze, 2021) understanding of words using dictionary definitions. Again, matching multiple pieces of text with each other is much harder and therefore a promising task for benchmarking NLU.\\n\\n5 Conclusion\\nWe introduced CoDA21, a new challenging benchmark that tests natural language understanding capabilities of PLMs. Performing well on CoDA21 requires detailed understanding of contexts, performing complex inference and having world knowledge, which are crucial skills for NLP. All models we investigated perform clearly worse than humans, indicating a lack of these skills in the current state of the art in NLP. CoDA21 therefore is a promising benchmark for guiding the development of models with stronger NLU competence.\"}"}
{"id": "acl-2022-short-92", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654.\\n\\nHector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Manzhang Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nTomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Advances in pre-training distributed word representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nGeorge A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u201341.\\n\\nGeorge A Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G Thomas. 1994. Using a semantic concordance for sense identification. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8\u201311, 1994.\\n\\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839\u2013849, San Diego, California. Association for Computational Linguistics.\\n\\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525\u20131534, Berlin, Germany. Association for Computational Linguistics.\\n\\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, Hong Kong, China. Association for Computational Linguistics.\\n\\nMohammad Taher Pilehvar and Jose Camacho-Collados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267\u20131273, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. In Technical Report.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.\"}"}
{"id": "acl-2022-short-92", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"how to fix it by attentive mimicking.\\n\\nProceedings of the AAAI Conference on Artificial Intelligence, 34:8766\u20138774.\\n\\nLutfi Kerem Senel and Hinrich Sch\u00fctze. 2021. Does she wink or does she nod? a challenging benchmark for evaluating word understanding of language models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 532\u2013538, Online. Association for Computational Linguistics.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\\n\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium. Association for Computational Linguistics.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791\u20134800, Florence, Italy. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-short-92", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-short-92", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. He came spurring and whooping down the road, his horse kicking up clouds of dust, shouting:\\n\\n2. Pels also sent a check for $100 to Russell's widow and had a white marble monument erected on his grave.\\n\\n3. The high cost of land and a few operational problems resulting from excessive loadings have created the need for a wastewater treatment system with the operational characteristics of the oxidation pond but with the ability to treat more organic matter per unit volume.\\n\\n4. It was a fine broody hen, white, with a maternal eye and a striking abundance of feathers in the under region of the abdomen.\\n\\n5. It was then distilled at least three times from a trap at -78\u00b0 to a liquid air trap with only a small middle fraction being retained in each distillation.\\n\\n6. The thing is that these bees love a fine-grained soil that is moist; yet the water in the ground should not be stagnant either.\\n\\n7. And the coffee shop on Drexel Street, where the men spent their evenings and Sundays playing cards, had a rose hedge beneath its window.\\n\\nTable 5: A sample CoDA21 question taken from the noun part of the CoDA21-clean-easy dataset. The synsets are grandchildren of the parent synset 'material.n.01' whose definition is \\\"the tangible substance that goes into the makeup of a physical object\\\".\"}"}
{"id": "acl-2022-short-92", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. This was Madden's **suggestion**; the police chief shook his head over it.\\n\\n2. The **concept** of apparent black-body temperature is used to describe the radiation received from the moon and the planets.\\n\\n3. Religion can summate, epitomize, relate, and conserve all the highest **ideals** and values - ethical, aesthetic, and religious - of man formed in his culture.\\n\\n4. That much of what he calls folklore is the result of beliefs carefully sown among the people with the conscious aim of producing a desired mass emotional **reaction** to a particular situation or set of situations is irrelevant.\\n\\n5. He had an uneasy **feeling** about it.\\n\\n6. The Federal program of vocational education merely provides financial aid to encourage the establishment of vocational education **programs** in public schools.\\n\\n7. Indefinite reference also carries double **meaning** where an allusion to one person or thing seems to refer to another.\\n\\n8. Almost nothing is said of Charles' spectacular victories, the central **theme** being the heroic loyalty of the Swedish people to their idolized king in misfortune and defeat.\\n\\n---\\n\\n**Synset Definition**\\n\\n- **suggestion.n.01**: an idea that is suggested\\n- **concept.n.01**: an abstract or general idea inferred or derived from specific instances\\n- **ideal.n.01**: the idea of something that is perfect; something that one hopes to attain\\n- **reaction.n.02**: an idea evoked by some experience\\n- **impression.n.01**: a vague idea in which some confidence is placed\\n- **plan.n.01**: a series of steps to be carried out or goals to be accomplished\\n- **meaning.n.02**: the idea that is intended\\n- **theme.n.02**: a unifying idea that is a recurrent element in literary or artistic work\"}"}
