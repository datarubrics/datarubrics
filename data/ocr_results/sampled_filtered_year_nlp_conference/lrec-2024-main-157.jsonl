{"id": "lrec-2024-main-157", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Two-Stage Prediction-Aware Contrastive Learning Framework for Multi-Intent NLU\\n\\nGuanhua Chen, Yutong Yao, Derek F. Wong\u2020, Lidia S. Chao\\n\\nNLP2CT Lab, Department of Computer and Information Science, University of Macau\\n{nlp2ct.guanhua, nlp2ct.yutong}@gmail.com, {derekfw, lidiasc}@um.edu.mo\\n\\nAbstract\\nMulti-intent natural language understanding (NLU) presents a formidable challenge due to the model confusion arising from multiple intents within a single utterance. While previous works train the model contrastively to increase the margin between different multi-intent labels, they are less suited to the nuances of multi-intent NLU. They ignore the rich information between the shared intents, which is beneficial to constructing a better embedding space, especially in low-data scenarios. We introduce a two-stage Prediction-Aware Contrastive Learning (PACL) framework for multi-intent NLU to harness this valuable knowledge. Our approach capitalizes on shared intent information by integrating word-level pre-training and prediction-aware contrastive fine-tuning. We construct a pre-training dataset using a word-level data augmentation strategy. Subsequently, our framework dynamically assigns roles to instances during contrastive fine-tuning while introducing a prediction-aware contrastive loss to maximize the impact of contrastive learning. We present experimental results and empirical analysis conducted on three widely used datasets, demonstrating that our method surpasses the performance of three prominent baselines on both low-data and full-data scenarios.\\n\\nKeywords: Conversational Systems, Natural Language Understanding, Prediction-Aware Contrastive Learning\\n\\n1. Introduction\\nMulti-intent Natural Language Understanding (NLU) models are fundamental building blocks within task-oriented dialogue systems (Qin et al., 2019; Gangadharaiah and Narayanaswamy, 2019). These systems encompass multi-intent detection (mID) and slot-filling (SF) tasks. However, effectively capturing multiple intents within short utterances presents a formidable challenge, primarily attributed to limited labeled data and the vast spectrum of spoken expressions. Consequently, a plethora of advanced models have emerged to refine the granularity of dialogue content and investigate the relationships among different intents (Qin et al., 2020; Song et al., 2022). Furthermore, recent researches (Qin et al., 2021; Chen et al., 2022a; Cai et al., 2022a; Wu et al., 2022) also proved that the extensive linguistic knowledge embedded within these pre-trained models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), is effective to facilitate the comprehension of multiple intents within brief utterances.\\n\\nHowever, these models often face a trade-off between inference speed and the incorporation of additional modules aimed at capturing more knowledge in the training data. This knowledge assumes a pivotal role in facilitating the model's acquisition of a more distinguishable semantic representation space, thereby yielding substantial benefits for downstream tasks. Consequently, several training strategies have emerged to maximize the utility of existing data without affecting the inference speed, such as contrastive learning (Liu et al., 2021; Basu et al., 2022) or curriculum learning (Chen et al., 2023; Zhou et al., 2020; Zhan et al., 2021). Leveraging these methods empowers the model to construct an improved embedding space while utilizing the same volume of data.\\n\\nRegrettably, existing contrastive learning (CL) methods on multi-intent NLU typically assign fixed roles, either positive or negative, to samples. This potentially disregards the valuable knowledge embedded in the relationships between the shared intents, a factor that has been demonstrated to enhance multi-intent detection (Xu and Sarikaya, 2013). Moreover, fine-tuning the model through equal-weight contrastive learning cannot fully learn the knowledge in the training data.\\n\\nTo tackle the above issues, we propose a two-stage Prediction-Aware Contrastive Learning (PACL) framework. PACL is meticulously designed to effectively harness knowledge emanating from shared intents through two-stage training: word-level pre-taining and prediction-aware contrastive fine-tuning. Building on Xu and Sarikaya (2013) proof of words commonly used to express an intent frequently appear across various instances, we further observed that these words exhibit a distinctive emphasis within the Part-of-Speech (POS) distribution. Hence, we introduce a word-level data augmentation strategy to construct a dataset for self-supervised pre-training. It enables the model to learn the associations between meaningful words...\"}"}
{"id": "lrec-2024-main-157", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and corresponding intents accurately. For fine-tuning stage, we devise an innovative prediction-aware contrastive learning framework. It facilitates automatic role-switching for each sample, allowing it to dynamically alternate between positive and negative roles while adjusting its influence based on the model's confidence. Additionally, we design an intent-slot attention mechanism to establish a strong connection between the mID and SF tasks for contrastive learning, which incentivized to more effectively harness the knowledge derived from shared intents, culminating in an embedding space characterized by heightened distinguishability.\\n\\nWe evaluate our PACL framework on three prominent multi-intent datasets: MixATIS (Qin et al., 2020), MixSNIPS (Qin et al., 2020), and StanfordLU (Hou et al., 2021). In addition, we employ three robust baselines for comparison: RoBERTa (Liu et al., 2019), TFMN (Chen et al., 2022b), and SLIM (Cai et al., 2022a). Our experimental results demonstrate that PACL yields substantial enhancements in model performance while accelerating convergence speed. Our empirical analyses further reaffirm the indispensability of each component within our framework.\\n\\n2. Related work\\n\\n2.1. Multi-Intent NLU\\n\\nCompared to single-intent detection (Zhang et al., 2019; Wu et al., 2020; Cheng et al., 2021), multiple intents appear more common in real-time scenarios. Early research (Kim et al., 2017; Gangadhariah and Narayanaswamy, 2019) attempts to apply traditional CNN or RNN-based methods. Qin et al. (2020) proposed An Adaptive Graph-Interactive Framework (AGIF), and further expanded this technique to a non-autoregressive model (Qin et al., 2021). Cai et al. (2022b) proposed Explicit Slot-Intent Mapping with Bert (SLIM) which transferred JointBERT from single intent to multi-intent task while solving the shared-intent problem. Considering the number of intents is important in the multi-intent NLU task, Chen et al. (2022b) designed a novel threshold-free framework to predict the number of intentions in the utterance before predicting the specific intents.\\n\\n2.2. Contrastive Learning\\n\\nContrastive Learning (CL) has been widely used on NLU tasks, because of its data scarcity and diverse expression. The recent works show the effect of CL on the NLU task (Gunel et al., 2020; Hou et al., 2021; Yehudai et al., 2023). Specifically, for the multi-intent NLU task, Vuli\u0107 et al. (2022) devised a strategy to transform a general sentence-encoder into a task-specific one on multi-intent data through contrastive learning. Tu et al. (2023) proposed a novel bidirectional joint model trained using supervised CL and self-distillation, effectively utilizing intent and slot features to complement each other.\\n\\n3. Proposed Method\\n\\n3.1. Problem Formulation\\n\\nAs illustrated in Figure 1, given an input utterance \\\\( x = (x_1, x_2, ..., x_n) \\\\) with \\\\( n \\\\) tokens, the multi-intent NLU model entails the simultaneous prediction of both multi-label intents for the utterance and the slot-filling for each word. Multi-label intent signifies the presence of more than one distinct intent within the set of possible intents.\\n\\nThe effectiveness of a joint training strategy for NLU task has been proved by Chen et al. (2019). The joint objective functions can be mathematically formulated as follows:\\n\\n\\\\[\\nL_{\\\\text{joint}} = L_{\\\\text{ID}} + L_{\\\\text{SF}}\\n\\\\]\\n\\nwhere \\\\( L_{\\\\text{ID}} \\\\) and \\\\( L_{\\\\text{SF}} \\\\) are the cross entropy loss of intent detection and slot filling tasks.\\n\\nBuilding upon this foundation, we introduce our two-stage prediction-aware contrastive learning (PACL) framework as shown in Figure 2.\\n\\n3.2. Word-level Pre-training\\n\\nAs multi-intent samples tend to be particularly challenging to distinguish within the embedding space, our initial step involves word-level pre-training to bolster the model's adaptability to the specific domain. Based on Cai et al. (2022a), which released the correspondence between each token and sub-intent in the MixATIS (Qin et al., 2020) and MixSNIPS (Qin et al., 2020) datasets, we conducted an analysis focusing on the distribution of Part of Speech (POS) categories associated with tokens linked to distinct intents. Notably, POS categories like \\\"NN\\\", \\\"NNS\\\", and \\\"JJ\\\" accounted for a significant proportion of words connected to intent labels, reaching as high as 87.76% in MixATIS and 78.97% in MixSNIPS. This unveiled a strong correlation between specific POS categories and intents.\\n\\nWe found that the intents are strongly correlated with some words with specific POS.\"}"}
{"id": "lrec-2024-main-157", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thus, we split the original utterance-level dataset into word-level, concentrating on those aligned with the specified POS categories to construct a word-level multi-intent dataset. Firstly, we detect whether a word with the specific POS recurs across multiple utterances. Subsequently, we refined the intent associated with this word by extracting the shared intent from those recurring utterances. For instance, if a term \u201clunch\u201d occurs in utterances linked to both \u201cMeal & Sport\u201d and \u201cMeal & Study\u201d intents, we identify it as belonging to \u201cMeal\u201d. For words that remained connected to multiple intents, we concatenate them with words specifically indicating the associated intent. Since this is word-level pre-training, it is acceptable to have two words associated with the same intent in an input even after concatenation.\\n\\nDue to the absence of sentence structure information in the concatenation of words, our pre-training strategy exclusively focuses on the intent detection task. This phase is dedicated to enabling the model to acquire an understanding of the relationships between individual words and various intents, and facilitates the model\u2019s ability to more effectively capture the presence of multiple intents within an utterance. The final pre-training loss can be formulated as:\\n\\n$$L_{PT} = \\\\lambda_1 L_{ID} + \\\\lambda_2 L_{CL}$$\\n\\nwhere $L_{ID}$ is the cross-entropy loss of intent prediction and $L_{CL}$ is the traditional contrastive loss of the intent logits. {\\\\lambda_1, \\\\lambda_2} are the hyper-parameters to balance each loss.\\n\\n3.3. Prediction-Aware Contrastive Fine-tuning\\n\\nUpon the word-level pre-trained model, we introduce an innovative prediction-aware contrastive loss to fine-tune the model on the original utterance-level dataset. In this fine-tuning process, each instance, which shares common intents with other samples, dynamically alternates its role (either positive or negative).\\n\\nAs depicted in Figure 2, given an anchor sample, samples with incorrectly predicted intents are designated as positive samples, while those with correctly predicted intents serve as negative ones. To illustrate, consider a sample with the intent \u201c_capacity\u201d, once the model successfully predicts part of the intent, like \u201c_capacity\u201d, our model categorizes samples with \u201c_capacity\u201d and \u201c_capacity\u201d as the positive candidates. Simultaneously, any samples with different intents, including \u201c_capacity\u201d, are treated as negative. Because the anchor sample has already learned the knowledge of the relationship between the anchor sample and intent \u201c_capacity\u201d.\\n\\nNoteworthy, those instance pairs with identical or completely different multi-intent labels will be regarded as mutually positive or negative pairs during the entire training. This prediction-aware contrastive learning empowers the model to gain a substantial understanding of the relationships between shared intents while expediting the clustering of samples sharing common intents.\\n\\nIn greater detail, each time we construct the mini-batch, we sample a maximum of $K$ positive samples for each anchor sample. Regarding the negative samples, we randomly sampled them from the mini-batch. In situations involving a multi-intent...\"}"}
{"id": "lrec-2024-main-157", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"label to which only the anchor sample belongs, we forward propagate this sample twice, following SimCSE (Gao et al., 2021), to secure at least one positive sample for each anchor. The original CL objective function can be defined as:\\n\\n$$L_{CL}^i = \\\\sum_{p \\\\in P(i)} \\\\log f(h_i, h_{i,p}) \\\\sum_{k \\\\in H(i)} f(h_i, h_{i,k})$$\\n\\n(3)\\n\\nwhere $f(h_1, h_2) = \\\\exp(\\\\cos(h_1, h_2)/\\\\tau)$. $P(i)$ is the set of positive samples, while $H(i)$ denotes the set of all positive and negative samples for the $i$-th anchor sample. $h_i$ is the representation vector for contrastive learning. The $\\\\tau$ is the temperature value in CL loss. In our experiments, we sample at most $K$ positive or negative samples.\\n\\nNevertheless, contend that positive and negative samples with varying predicted probabilities should exert distinct influences on contrastive learning. We leverage the predicted probabilities to calibrate the impact of each sample within our prediction-aware contrastive loss function. In practice, the higher the average probability of the intents associated with the anchor sample, which matches that of the negative sample, the more challenging it becomes to distinguish the negative sample within the embedding space. Hence, we normalize the probability as the weight of negative samples as represented by the following equation:\\n\\n$$w_{i,n} = \\\\text{Softmax}(\\\\text{Mean}\\\\{p_i|y_{neg}^i\\\\})$$\\n\\n(4)\\n\\nwhere $p_{i,j}$ is the probability that the $i$-th sample in the batch belongs to the $j$-th intent.\\n\\nWith respect to the positive samples, a higher probability of a shared intent between the positive sample and the anchor sample signifies the model's enhanced capacity for precise classification. Consequently, it implies that the model can afford to allocate reduced attention to these samples. Correspondingly, we assign weights to the positive samples as follows:\\n\\n$$w_{i,p} = \\\\text{Softmax}(\\\\alpha_i \\\\frac{\\\\text{Count}(y^{pos}_i)}{\\\\text{Count}(y^i)})$$\\n\\n(5)\\n\\nwhere the Count($\\\\cdot$) means the number of elements that satisfy the condition, while $y^i$ refers to the predicted intent, and $y^{pos}_i$ represents the correctly predicted intent. Combining the equation 4 and 5, the prediction-aware contrastive loss can be rewritten as:\\n\\n$$L_{i,PACL} = \\\\sum_{p \\\\in Pos(i)} \\\\log w_{i,p} f(h_i, h_{i,p}) \\\\sum_{k \\\\in H(i)} w_{i,k} f(h_i, h_{i,k})$$\\n\\n(6)\\n\\nFinally, in order to enhance the connection between multi-intent detection and slot filling tasks, we add a multi-head attention layer to capture the relationship between all the slots and intent representation. The final representation for CL can be generated as follows:\\n\\n$$\\\\text{Attention}(Q, K, V) = \\\\text{Softmax}(QK^T/\\\\sqrt{d})V,$$\\n\\n$$h_{\\\\text{intent}} = \\\\text{Attention}(h_{cls}, H_{slot}, H_{slot})$$\\n\\n(7)\\n\\n$$h'_{\\\\text{intent}} = (W[h_{\\\\text{intent}}, h_{cls}] + b)$$\\n\\n(8)\\n\\nwhere Attention($\\\\cdot$) is the attention mechanism as described by Vaswani et al. (2017). $H_{slot} \\\\in \\\\mathbb{R}^{n \\\\times d}$ is the set of all the word representations of the given utterance. $W$ is the trainable parameters, and $h'_{\\\\text{intent}} \\\\in \\\\mathbb{R}^d$ is the final representation. We feed the $h'_{\\\\text{intent}}$ to the same intent classifier in equation 1 to train the additional layer in equation 7 and 8 only, which has few parameters and will not affect the training speed too much. This allows the latent space used for the CL to be more adapted to the intent classifier in equation 1.\\n\\nOverall, the final loss of the fine-tuning process can be formulated as:\\n\\n$$L_{FT} = \\\\lambda_3 L_{ID} + \\\\lambda_4 L_{SF} + \\\\lambda_5 L_{PACL}$$\\n\\n(9)\\n\\nwhere $\\\\{\\\\lambda_3, \\\\lambda_4, \\\\lambda_5\\\\}$ are the hyper-parameters to balance the impact of each loss.\\n\\n4. Experiments\\n\\n4.1. Dataset and Evaluation\\n\\nWe evaluate our method on three widely used multi-intent datasets, MixATIS (Qin et al., 2020), MixSNIPS (Qin et al., 2020), and StanfordLU (Hou et al., 2021). MixATIS contains 13161/756/828 utterances in train/validation/test set. MixSNIPS contains 39776/2198/2199 utterances in train/validation/test set. As for StanfordLU, it consists of 6428/790/820 train/validation/test data. We sampled 10% of each intent for our low-data scenarios. For evaluation metrics, we use F1 score, intent accuracy, and intent-slot overall accuracy.\\n\\n4.2. Baselines\\n\\nWe compare our proposed approach with previous state-of-the-art models (E et al., 2019; Qin et al., 2019, 2020, 2021), whose results are taken from previous work directly. Besides, we also reproduce three strong baselines: RoBERTa (Liu et al., 2019), TFMN (Chen et al., 2022b), and SLIM (Cai).\\n\\n1 https://huggingface.co/roberta-base\\n2 https://github.com/TRUMANCFY/SLIM\"}"}
{"id": "lrec-2024-main-157", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1: The results (%) on the test set of two datasets.\\n\\n| Model | StanfordLU IC Acc | StanfordLU SF | StanfordLU F1 | StanfordLU Overall Acc | AgIF (Qin et al., 2020) | GL-GIN (Qin et al., 2021) | GISCo (Song et al., 2022) | SDJN+BERT (Chen et al., 2022a) | Low-Data RoBERTa | RoBERTa (PACL) | TFMN (Chen et al., 2022b) | TFMN (PACL) |\\n|-------|-------------------|---------------|---------------|-------------------------|------------------------|--------------------------|--------------------------|----------------------------|----------------|--------------|------------------------|--------------|\\n|       |                   |               |               |                         | 75.8                   | 88.1                     | 44.5                     | 96.5                       | 94.5            | 76.4         | 78.0                   | 87.5         |\\n|       |                   |               |               |                         | 76.3                   | 88.3                     | 43.5                     | 95.6                       | 94.9            | 75.4         | 78.0                   | 87.5         |\\n|       |                   |               |               |                         | -                      | -                        | 48.2                     | -                          | -               | 75.9         | -                      | -            |\\n|       |                   |               |               |                         | 46.7                   | 84.1                     | 24.9                     | 94.0                       | 92.3            | 67.5         | 46.4                   | 71.1         |\\n|       |                   |               |               |                         | 66.2                   | 84.0                     | 36.8                     | 94.8                       | 92.9            | 69.6         | 58.3                   | 72.7         |\\n|       |                   |               |               |                         | 52.8                   | 72.4                     | 16.1                     | 93.9                       | 93.4            | 73.8         | 60.8                   | 75.8         |\\n|       |                   |               |               |                         | 77.5                   | 85.5                     | 47.7                     | 96.5                       | 95.6            | 80.9         | 81.5                   | 86.9         |\\n|       |                   |               |               |                         | 79.1                   | 86.0                     | 48.9                     | 96.5                       | 96.2            | 83.4         | 82.9                   | 86.7         |\\n|       |                   |               |               |                         | 81.5                   | 86.8                     | 48.6                     | 97.1                       | 96.1            | 83.4         | 82.9                   | 86.7         |\\n|       |                   |               |               |                         | 78.9                   | 87.1                     | 46.4                     | 96.8                       | 96.3            | 83.7         | 81.9                   | 87.3         |\\n\\nTable 1: The results (%) on the test set of two datasets. \u2020 means we reproduce this framework as our baselines. Bold numbers indicate the better result for each baseline, meanwhile, underlined numbers show the best performance in the column.\\n\\n---\\n\\n### Table 2: The results (%) on the test set of StanfordLU dataset.\\n\\n| Model | StanfordLU IC Acc | StanfordLU SF | StanfordLU F1 | StanfordLU Overall Acc | Low-Data RoBERTa | RoBERTa (PACL) | TFMN (Chen et al., 2022b) | TFMN (PACL) |\\n|-------|-------------------|---------------|---------------|-------------------------|----------------|--------------|------------------------|--------------|\\n|       |                   |               |               |                         | 21.3           | 29.1         | 21.2                     | -            |\\n|       |                   |               |               |                         | 32.4           | 28.8         | 31.9                     | -            |\\n|       |                   |               |               |                         | 41.7           | 30.0         | 38.8                     | -            |\\n|       |                   |               |               |                         | 46.7           | 31.8         | 40.2                     | -            |\\n|       |                   |               |               |                         | 88.0           | 92.3         | 82.9                     | -            |\\n|       |                   |               |               |                         | 89.0           | 92.5         | 84.1                     | -            |\\n|       |                   |               |               |                         | 88.0           | 93.0         | 83.6                     | -            |\\n|       |                   |               |               |                         | 89.1           | 92.9         | 84.3                     | -            |\\n\\nTable 2: The results (%) on the test set of StanfordLU dataset. \u2020 means we reproduce this framework as our baselines. Bold numbers indicate the better result for each baseline.\\n\\n### 4.3. Implementation Detail\\n\\nWe use the BERT Base model for SLIM (Cai et al., 2022a) and TFMN (Chen et al., 2022b). Besides, we also employ the RoBERTa Base model with 2 feed-forward classifier released by hugging face. The max sequence length and batch size are 50 and 64 for both pre-training and fine-tuning. The sampling number $K$ in our experiments is set to 5. We pre-train the model for $\\\\{1, 3, 5\\\\}$ epochs and fine-tune it for 10 epochs. For Roberta and TFMN models, we set the dropout rate and learning rate to 0.1 and 2e-5. For the SLIM model, we set the dropout rate and learning rate to 0.2 and 5e-5. For more details about training, given an input utterance with $n$ tokens, $x = \\\\{x_1, x_2, \\\\ldots, x_n\\\\}$, and its encoded representation, $h = \\\\{h_{cls}, h_1, h_2, \\\\ldots, h_n, h_{sep}\\\\}$, where the representation of the [CLS] token is utilized for multi-intent detection, and the representations of the other tokens are utilized for slot filling, separately.\\n\\n3: https://huggingface.co/bert-base-uncased\\n4: https://huggingface.co/roberta-base\\n\\n### 4.4. Main Results\\n\\nThe primary experimental findings are summarized in Table 1 and Table 2. Our proposed framework consistently outperforms RoBERTa (Liu et al., 2019), TFMN (Chen et al., 2022b), and SLIM (Cai et al., 2022a) on both low-data and full-data scenarios. Notably, we conducted experiments solely with the RoBERTa and TFMN models on the StanfordLU dataset, as this dataset lacks matching data for intent and individual tokens. Intuitively, the improvements achieved by the PACL framework are more substantial in low-data scenarios compared to high-data scenarios, across all three evaluation.\"}"}
{"id": "lrec-2024-main-157", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The ablation study results (%). None of the \\\"PT\\\", \\\"RS\\\", or \\\"PA\\\" components are used meaning the traditional CL method. More explanations are in Section 5.1.\\n\\nFor instance, on the MixATIS dataset, our PACL method demonstrates a remarkable 11.9% increase in intent detection accuracy and a 7.2% boost in overall accuracy in the low-data scenario, whereas it only achieves an improvement of up to 2.7% in intent detection accuracy and 3.7% in overall accuracy in the high-data scenario. Furthermore, we anticipate a modest enhancement in slot-filling F1 scores, as the primary objective of combining intent and slot representations is to primarily enhance overall accuracy. Consequently, we observe instances where the overall accuracy improved, while the individual scores for intent detection and slot filling exhibited less significant improvements. It is pertinent to mention that the improvement in intent classification accuracy is relatively lower on the MixSNIPS dataset compared to the other two datasets. This divergence may be attributed to the fact that MixATIS and StanfordLU comprise a significantly larger number of multi-intent labels, with 17 intents and 24 intents, in contrast to MixSNIPS, which contains only 6 intents. This discrepancy increases the proportion of overlapping intents, which, in turn, results in our PACL framework performing more effectively on the MixATIS and StanfordLU datasets.\\n\\n5. Analysis\\n\\n5.1. Ablation Study\\n\\nFor a better explanation, we divide the fine-tuning stage into two components in this section: dynamic role selection for each sample and the integration of prediction-aware contrastive loss. We conducted ablation experiments in both low-data and full-data scenarios to assess the efficacy of each component, as summarized in Table 3. In this context, \\\"PT\\\" refers to the word-level pre-training process, \\\"RS\\\" signifies the dynamic selection of roles (positive or negative) for each sample during training, and \\\"PA\\\" indicates the prediction-aware contrastive loss. Our findings unequivocally establish the necessity of each component. Specifically, \\\"RS\\\" and \\\"PA\\\" can both contribute to enhancing the performance across all three metrics, while combining \\\"PA\\\" and \\\"RS\\\" obtains further improvement on overall performance. This indicates that these two components can benefit each other. Regarding \\\"PT\\\", it facilitates the model in acquiring domain-specific precise knowledge at word-level, consequently bolstering the effectiveness of the subsequent prediction-aware contrastive fine-tuning. However, \\\"PT\\\" gets less improvement on full data than on low-data, due to the fact that the greater amount of data in the full-data scenario allows the model to learn the knowledge between shared intents better, which diminishes the effect of \\\"PT\\\".\\n\\n5.2. Different Proportions of Training Data\\n\\nTo further explore the effectiveness of our method on different data volumes, we conducted experiments on MixATIS dataset with \\\\{20%,30%,40%,50%\\\\} data, and the results compared with baselines are shown in Table 4. Our method has an average 3% improvement in intent accuracy as well as an average 3.28% improvement in overall accuracy for data on different proportions. Intuitively, the enhancement of our method slightly decreases as the proportion of data increases. It is because more training data will enable the model to recognize multiple intents more clearly, which in turn diminish the effect of\"}"}
{"id": "lrec-2024-main-157", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: The experimental results with different percentages of the MixATIS dataset. For each percentage, the upper results are the SLIM baseline, while the bottom ones are our PACL framework. Bold numbers indicate the better result for each percentage.\\n\\n| Percentage | SLIM Baseline | PACL Framework |\\n|------------|---------------|----------------|\\n| 10%        | 52.8          | 72.4           |\\n|            | 60.8          | 75.8           |\\n| 20%        | 71.4          | 82.1           |\\n|            | 72.6          | 84.2           |\\n| 30%        | 74.8          | 84.7           |\\n|            | 75.9          | 86.2           |\\n| 40%        | 75.7          | 85.9           |\\n|            | 77.9          | 86.9           |\\n| 50%        | 75.8          | 86.1           |\\n|            | 78.0          | 86.9           |\\n\\nThe Number of Intent\\n\\nFigure 3: The Intent Accuracy of different numbers of intents trained on low- and high-data of MixATIS dataset.\\n\\nThe contrastive learning. Notably, the model performs much worse optimizing 10% of training data than on the other proportions, meanwhile, PACL obtains the maximum performance optimization (8% improvement on Intent Accuracy and 7.2% improvement on Overall Accuracy) on 10% of the data. This also proves that our PACL framework can help the model construct better embedding spaces indirectly, especially for low-data scenarios.\\n\\n5.3. The Effect of Different Number of Intents\\n\\nNext, we conduct a comparative analysis of the impact of the SLIM baseline and our PACL framework on labels with varying numbers of intents. As shown in Figure 3, it is clear that on single-intent test data, there are minimal disparities in model performance across different methods and data volumes. However, the disparity between the performance of models trained on low- and high-data volumes is more pronounced. In low-data scenarios, our approach enhances intent accuracy by 8% for 2-intent test data and by 9% for 3-intent data. In high-data scenarios, our method improves intent accuracy by 3.7% for samples with 2-intent labels and by 4% for samples with 3-intent labels. These results underscore the effectiveness of our PACL framework for multi-intent NLU while enhancing the efficiency of utilizing low-resource data.\\n\\n5.4. Learning Curves\\n\\nIn order to validate the impact of our PACL framework during the training process, we visually analyze the learning curve of SLIM (Cai et al., 2022a) on the test set of the MixATIS dataset (Figure 4). The intent accuracy of the baseline and PACL approaches is denoted by dashed lines, while solid lines represent their respective overall accuracy curves. Our proposed method demonstrates swifter convergence compared to the baseline, showcasing consistent enhancements in both intent accuracy and overall accuracy. Notably, the overall accuracy of PACL in the early stage is lower than baseline. This makes sense because the pre-training process uses word-level inputs, and the model needs to adapt to the utterance-level pattern in the early stage of training process.\\n\\n5.5. Visualization\\n\\nAs shown in Figure 5, we utilized the t-SNE algorithm to visualize the distribution of intent embedding.\"}"}
{"id": "lrec-2024-main-157", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: The distribution of intent embeddings in MixATIS dataset. The left one is trained on SLIM baseline, and the other one is trained with our PACL method.\\n\\nFigure 6: The distribution of several shared intents samples in MixATIS test set. The upper one is original BERT model, while the lower one is the BERT model pre-trained with our word-level pre-training strategy.\\n\\nIn further detail, we randomly extract several samples with the same shared intents from the MixATIS test dataset to visualize the distinctions in embedding spaces between the original BERT model and the BERT model pre-trained using our word-level pre-training strategy. Notably, the visual analysis in Figure 6 reveals that, following our word-level pre-training strategy, the model exhibits the capacity to distinguish between utterances with distinct intents, even when they share some intents. This enhanced capability positions the model for improved performance during the fine-tuning stage.\\n\\n5.6. Case Study\\n\\nIn addition to the t-SNE visualization, we exhibit two illustrative examples to validate the performance of our method more specifically, in which example 1 is drawn from the MixATIS dataset, while example 2 is drawn from the MixSNIPS dataset. As depicted in Table 5, the SLIM baseline encounters challenges in making accurate predictions on both multi-intent detection and slot filling tasks, which primarily stems from the high similarity between multiple intents. In contrast, our method successfully demarcates a more distinct boundary, which ameliorates the mispredictions (Example 1) and underpredictions (Example 2) problems. Furthermore, our method addresses inaccuracies in slot predictions. The consistency of slot and intent corrections also demonstrates the strong correlation between the token and its corresponding intent.\\n\\n6. Conclusion\\n\\nThis paper introduces a novel two-stage prediction-aware contrastive learning framework for multi-intent NLU, which significantly enhances model performance through leveraging word-level pre-training and prediction-aware contrastive fine-tuning. Our method acquires knowledge not only from distinct intents, but also from shared common intents. The experimental results show that our method significantly improved for both low-data and full-data scenarios. As for the future work, we plan to explore how to maximize the impact of contrastive learning with a reduced number of positive samples.\\n\\n7. Limitations\\n\\nThe main limitation of this approach is training efficiency. Although it will not increase the inference time, it has more training cost because of the gradient calculation for contrastive samples, especially since the number $K$ is strongly related to the model performance. Besides, even though our approach...\"}"}
{"id": "lrec-2024-main-157", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: The examples of how our method solves the intent misclassification problem and improves overall accuracy. Red texts indicates the incorrect results, while blue texts indicates the correct results.\\n\\n8. Acknowledgements\\n\\nThis work was supported in part by the Science and Technology Development Fund, Macau SAR (Grant Nos. FDCT/060/2022/AFJ, FDCT/0070/2022/AMJ), National Natural Science Foundation of China (Grant No. 62261160648), Ministry of Science and Technology of China (Grant No. 2022YFE0204900), and the Multi-year Research Grant from the University of Macau (Grant No. MYRG-GRG2023-00006-FST-UMDF). This work was performed in part at SICC which is supported by SKL-IOTSC, and HPCC supported by ICTO of the University of Macau.\\n\\n9. Bibliographical References\\n\\nSamyadeep Basu, Amr Sharaf, Karine Ip Kiun Chong, Alex Fischer, Vishal Rohra, Michael Amoake, Hazem El-Hammamy, Ehi Nosakhare, Vijay Ramani, and Benjamin Han. 2022. Strategies to improve few-shot learning for intent classification and slot-filling. In Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI), pages 17\u201325, Seattle, USA. Association for Computational Linguistics.\\n\\nFengyu Cai, Wanhao Zhou, Fei Mi, and Boi Faltings. 2022a. Slim: Explicit slot-intent mapping with bert for joint multi-intent detection and slot filling. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022, pages 7607\u20137611. IEEE.\\n\\nFengyu Cai, Wanhao Zhou, Fei Mi, and Boi Faltings. 2022b. Slim: Explicit slot-intent mapping with bert for joint multi-intent detection and slot filling. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7607\u20137611. IEEE.\\n\\nGuanhua Chen, Runzhe Zhan, Derek F. Wong, and Lidia S. Chao. 2023. Multi-level curriculum learning for multi-turn dialogue generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:3958\u20133967.\\n\\nLisong Chen, Peilin Zhou, and Yuexian Zou. 2022a. Joint multiple intent detection and slot filling via self-distillation. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7612\u20137616.\\n\\nLisung Chen, Nuo Chen, Yuexian Zou, Yong Wang, and Xinzhong Sun. 2022b. A transformer-based threshold-free framework for multi-intent NLU.\"}"}
{"id": "lrec-2024-main-157", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-157", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"filling with global intent-slot co-occurrence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7967\u20137977, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nNguyen Anh Tu, Hoang Thi Thu Uyen, Tu Minh Phuong, and Ngo Xuan Bach. 2023. Joint multiple intent detection and slot filling with supervised contrastive learning and self-distillation. CoRR, abs/2308.14654.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998\u20136008.\\n\\nIvan Vuli\u00b4c, I\u00f1igo Casanueva, Georgios Spithourakis, Avishek Mondal, Tsung-Hsien Wen, and Pawe\u0142 Budzianowski. 2022. Multi-label intent detection via contrastive task specialization of sentence encoders. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7544\u20137559, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nDi Wu, Liang Ding, Fan Lu, and Jian Xie. 2020. SlotRefine: A fast non-autoregressive model for joint intent detection and slot filling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1932\u20131937, Online. Association for Computational Linguistics.\\n\\nYangjun Wu, Han Wang, Dongxiang Zhang, Gang Chen, and Hao Zhang. 2022. Incorporating instructional prompts into a unified generative framework for joint multiple intent detection and slot filling. In Proceedings of the 29th International Conference on Computational Linguistics, pages 7203\u20137208, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\\n\\nPuyang Xu and Ruhi Sarikaya. 2013. Exploiting shared information for multi-intent natural language sentence classification. In INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association, Lyon, France, August 25-29, 2013, pages 3785\u20133789. ISCA.\\n\\nAsaf Yehudai, Matan Vetzler, Yosi Mass, Koren Lazar, Doron Cohen, and Boaz Carmeli. 2023. QAID: question answering inspired few-shot intent detection. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\n\\nRunzhe Zhan, Xuebo Liu, Derek F Wong, and Lidia S Chao. 2021. Meta-curriculum learning for domain adaptation in neural machine translation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14310\u201314318.\\n\\nChenwei Zhang, Yaliang Li, Nan Du, Wei Fan, and Philip Yu. 2019. Joint slot filling and intent detection via capsule neural networks. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5259\u20135267, Florence, Italy. Association for Computational Linguistics.\\n\\nYikai Zhou, Baosong Yang, Derek F. Wong, Yu Wan, and Lidia S. Chao. 2020. Uncertainty-aware curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6934\u20136944, Online. Association for Computational Linguistics.\\n\\nHou, Yutai and Lai, Yongkui and Wu, Yushan and Che, Wanxiang and Liu, Ting. 2021. Few-shot learning for multi-label intent detection. AAAI Press. PID 10.1609/aaai.v35i14.17541.\\n\\nQin, Libo and Xu, Xiao and Che, Wanxiang and Liu, Ting. 2020. AGIF: An Adaptive Graph-Interactive Framework for Joint Multiple Intent Detection and Slot Filling. Association for Computational Linguistics. PID 10.18653/v1/2020.findings-emnlp.163.\"}"}
