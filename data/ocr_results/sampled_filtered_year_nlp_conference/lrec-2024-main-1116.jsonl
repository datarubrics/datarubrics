{"id": "lrec-2024-main-1116", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PILA: A Historical-Linguistic Dataset of Proto-Italic and Latin\\n\\nStephen Bothwell,\u2217 Brian DuSell,\u2020 David Chiang,\u2217 Brian Krostenko\\n\\n\u2217University of Notre Dame\\n\u2020ETH Z\u00fcrich\\nNotre Dame, Indiana, USA Z\u00fcrich, Switzerland\\n{sbothwel,dchiang,bkrosten}@nd.edu brian.dusell@inf.ethz.ch\\n\\nAbstract\\nComputational historical linguistics seeks to systematically understand processes of sound change, including during periods at which little to no formal recording of language is attested. At the same time, few computational resources exist which deeply explore phonological and morphological connections between proto-languages and their descendants. This is particularly true for the family of Italic languages. To assist historical linguists in the study of Italic sound change, we introduce the Proto-Italic to Latin (PILA) dataset, which consists of roughly 3,000 pairs of forms from Proto-Italic and Latin. We provide a detailed description of how our dataset was created and organized. Then, we exhibit PILA's value in two ways. First, we present baseline results for PILA on a pair of traditional computational historical linguistics tasks. Second, we demonstrate PILA's capability for enhancing other historical-linguistic datasets through a dataset compatibility study.\\n\\nKeywords: historical linguistics, Latin, Proto-Italic, resource\\n\\n1. Introduction\\nAll languages change over time, but much work in computational linguistics views language as a static phenomenon. Most natural language processing models (e.g., for named entity recognition or machine translation) are trained on snapshots of languages at a fixed point in time. In contrast, historical linguistics\u2014and, by extension, computational historical linguistics\u2014attempts to track linguistic shifts across various points in time.\\n\\nThe phonological side of historical linguistics examines the relations among cognate sets: forms in different languages that are related etymologically. If forms have a common ancestor, or etymon (pl. etyma), they are known as reflexes of that etymon. Historical linguists hypothesize systems of sound change to explain the evolution of language over time. However, as we proceed further into the past, reconstruction becomes more difficult or even speculative due to the dearth of existing evidence.\\n\\nConsider the examples presented in Table 1 for the Proto-Italic and Latin languages. Each row corresponds to a sound change pattern. Cluster reduction, for instance, involves the collapse of a longer consonant cluster into a shorter one. Here, the reconstructed *takslos reduces *ksl into l.\\n\\nHow does this collapse occur? Many sequences of rules could be proposed to explain the phenomenon. To reconstruct the true process, historical linguists gather evidence by comparing languages in close temporal and geographic proximity. Yet, to our knowledge, such efforts have not demonstrated the precise sequence of rules that underlies cluster reduction\u2014alongside other sound change patterns.\\n\\nTo scrutinize systems of sound change and build models to capture them, we need sizable datasets. Toward that end, we introduce PILA, the Proto-Italic to Latin dataset.\\n\\nPILA contributes to the study of historical linguistics in many ways. Namely:\\n\\n\u2022 PILA is the first dataset to contain full (and not partial; see Section 4.2.2) reconstructions of Proto-Italic etyma and their Latin reflexes.\\n\u2022 PILA is one of the largest available datasets of etymon\u2013reflex pairs for a single proto-language. (See Table 3.)\\n\u2022 PILA provides multiple inflections for most lemmata, letting phonological studies consider morphology's influence. (See Section 4.2.5.)\\n\u2022 PILA highlights the presence of non-phonological changes (e.g., analogy) through per-entry annotations. (See Table 5.)\\n\\nOur dataset is available at the following location: https://github.com/Mythologos/PILA.\"}"}
{"id": "lrec-2024-main-1116", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dashed lines indicate the omission of some intermediate families and branching structures from the tree. The two points represent the time periods covered by our dataset.\\n\\nAfter situating our work in Sections 2 and 3, we describe our dataset (Section 4.1) and its development (Section 4.2). Then, in Section 5, we exhibit PILA's applicability through strong baseline results on standard historical linguistics tasks and a careful dataset compatibility study. We further provide a table in our dataset to encourage studies which span multiple historical linguistics datasets. (See Section 5.2 for details.)\\n\\n2. Background\\n\\nThe Proto-Indo-European (abbreviated PIE) language is a reconstructed, unattested language. Historical linguists theorize that as the language spread, it split up into branches characterized by shared innovations. One set of innovations characterizes a branch conventionally named Proto-Italic, which is the parent of later Italic and Celtic languages. One branch of that family, characterized by further innovations (de Vaan, 2008, 8), is dubbed Proto-Italic. That language is the parent of a family of attested languages spoken chiefly in the Italian peninsula, which fall into dialect or family groups, generally along geographical lines.\\n\\nThe language family centered on the Apennine spine is Osco-Umbrian, divided into Oscan in the south and Umbrian in the north. Meanwhile, the language family originally spoken in the lower country along the Tyrrhenian coast between Etruria and the Campania is Latino-Faliscan, divided into Faliscan in the north and Latin in the south. As the city-state of Rome developed into an international power, Latin spread widely, eventually supplanting the other Italic languages. Thus, Latin is undoubtedly the best attested of the older Italic languages, and it becomes the source of the Romance languages in turn. We depict a summarized version of this language family tree in Fig. 1.\\n\\nTurning to these languages' phonetic inventories, Proto-Italic inherits a fairly small set of phones from PIE (Beekes, 1995, 124). By the time Proto-Italic splits up, laryngeals\u2014a class of sounds with a guttural articulation\u2014disappear. They leave behind traces of various kinds (e.g., long vowels, the creation of a). Meanwhile, PIE's voiced aspirates generally become fricatives, and their vocalized nasals become nasal stops (em, en). Within Latin the fricatives are heavily reordered, sometimes becoming stops; diphthongs have become or are becoming monophthongs; and the labiovelars sometimes have lost labial or velar articulation.\\n\\nPIE is a highly inflected language; Proto-Italic and Latin follow suit, although these languages gradually simplify PIE's system. Like PIE, Proto-Italic and Latin inflect through patterns of suffixation. PIE has a nominal system with stem classes that fall into three agreement categories, conventionally called genders, and can be suffixed into eight or nine cases. By the time of Latin, only five or six cases (e.g., genitive, accusative) are in full use. As for PIE's verbal system, it employs vowel changes in the stem and suffixation variously to assume different aspects, moods (e.g., indicative, imperative), and persons (1st, 2nd, and 3rd). Latin largely drops the meaning behind vowel changes in verb stems and instead standardizes these changes into four conjugations. Every verb is characterized by number (singular, plural), person, tense (e.g., present, perfect), mood, and voice (e.g., active, passive).\\n\\n3. Related Work\\n\\nRecent interest in computational historical linguistics has spurred an uptick in the number of available datasets\u2014including those with proto-languages. Work from Lexibank (List et al., 2022), a large collection of historical linguistics datasets converted into the Cross-Linguistic Data Format (CLDF) (Forkel et al., 2018), has been central to the increase in accessible datasets across language families. Nevertheless, the number of datasets with proto-languages remains small, and coverage of language families could be greatly improved. Lexibank contains languages from four of Glottolog's 245 language families (Hammarstr\u00f6m et al., 2023). With the inclusion of all non-Lexibank datasets that we know of, a total of eight language families have received attention. We list all datasets containing proto-languages with cognate relationships in Table 2. As this table illustrates, our work fills a gap, being the first dataset to provide explicit coverage for Italic. Table 3 further shows that PILA is one of the largest datasets of its kind.\"}"}
{"id": "lrec-2024-main-1116", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Family Coverage\\nArawakan Purus (de Carvalho, 2021)\\nAustronesian Basic Vocabulary Database (Greenhill et al., 2008), Jambu [Munda] (Arora et al., 2023), Micronesian Comparative Dictionary (Bender et al., 2003a,b), Tukanoan (Chacon, 2014)\\nDravidian Jambu (Arora et al., 2023)\\nIndo-European Indo-European Cognate Relationships (IE-CoR) Database (Heggarty et al., 2023), Germanic (Luo, 2021), Indo-European Lexicon (IELEX) (Linguistics Research Center, 2024), Jambu (Arora et al., 2023), PILA [Italic] (Ours), Slavic (Cathcart and Wandl, 2020)\\nMongolic-Khitan Jambu [Mongolic] (Arora et al., 2023)\\nSino-Tibetan Bai\u2020 (Wang, 2004), Burmish\u2020 (Gong and Hill, 2020), Karen\u2020 (Luangthongkum, 2019), Lalo\u2020 (Yang, 2023), Tujia\u2020 (Zhou, 2020)\\nUto-Aztecan Aztecan\u2020 (Davletshin, 2012), Corachol and N\u00e1huatl\u2020 (Pharao Hansen, 2020)\\nTurkic Jambu (Arora et al., 2023)\\n\\nTable 2: A depiction of proto-form dataset coverage across language families. If datasets are named, their names are presented followed by the language subfamilies which they concern, if applicable, in square brackets. Otherwise, the name(s) of contained language subfamilies are used to represent the dataset.\\n\\nItems are marked with a \u2020 if they are, at the time of writing, made available through Lexibank.\\n\\n| Dataset | Ancestor | Descendant | Pairs |\\n|---------|----------|------------|-------|\\n| IELEX (LRC, 2024) | Proto-Indo-European (indo1319) | English (stan1293) | 14697 |\\n| Luo (2021) | Proto-Germanic (germ1287) | Old English (olde1238) | 4599 |\\n| Jambu (Arora et al., 2023) | Proto-Dravidian (drav1251) | Tamil (tami1289) | 4276 |\\n| PILA (Ours) | Proto-Italic (ital1284) | Latin (lati1261) | 2916 |\\n| Cathcart and Wandl (2020) | Proto-Slavic (slav1255) | Russian (russ1263) | 1572 |\\n| MCD (Bender et al., 2003a,b) | Proto-Chuukic (truk1243) | Chuukese (chuu1238) | 1460 |\\n| Yang (2023) | Proto-Lalo Shuizhuping (west1506) | | 954 |\\n| Wang (2004) | Proto-Bai Enqi (enqi1234) | | 455 |\\n| Luangthongkum (2019) | Proto-Karen (kare1337) | Northern Sgaw | 366 |\\n| Zhou (2020) | Proto-Bizic Cebu (sout2739) | | 324 |\\n| Gong and Hill (2020) | Proto-Burmish (burm1266) | Maru (maru1249) | 264 |\\n| de Carvalho (2021) | Proto-Purus (puru1265) | Yine (yine1238) | 201 |\\n| IECoR (Heggarty et al., 2023) | Proto-Indo-European (indo1319) | Old Polish (poli1260) | 146 |\\n| Chacon (2014) | Proto-Tucanoan (tuca1253) | Tukano (tuca1252) | 131 |\\n| Davletshin (2012) | Proto-Aztecan (azte1234) | Classical Nahuatl (clas1250) | 84 |\\n| Pharao Hansen (2020) | Proto-Nahua (azte1234) | Cora (cora1259) | 55 |\\n\\nTable 3: A comparison of datasets containing proto-languages. Each language pairing presented is the pairing with the maximal number of etymon\u2013reflex pairs for the given dataset. Each language is joined by its provided (or otherwise determinable) Glottocode. Reconstructions are included whether they are full or partial (as defined in Section 4.2.2), as it is nontrivial to distinguish between these reconstruction types.\\n\\nOutside of Lexibank, there are other datasets that document etymon\u2013reflex relations between attested languages. For example, the WikiHan dataset compares Middle Chinese to eight Chinese subgroups (Chang et al., 2022). Another group of related datasets compare Latin with five descendant Romance languages (Ciobanu and Dinu, 2014; Meloni et al., 2021). One of two Coglust (Cognate Clustering) datasets similarly deals with Latin and Romance languages, adding Catalan to their set; the other contains Turkic and six of its descendants (Wu and Yarowsky, 2018).\\n\\nAnd unattested proto-forms, an Old Indo-Aryan dataset (Cathcart and Rama, 2020) is left out from Tables 2 and 3. Yet other datasets have less of a focus on modeling the relationship between specific ancestor and descendant languages and instead identify cognate sets over a wide range of languages. Namely, the Indo-European Lexicon (IELEX) (Linguistics Research Center, 2024), the Indo-European Cognate Relationships (IE-CoR) database (Heggarty et al., 2023), CogNet (Batsuren et al., 2019, 2022), the Jambu database for South Asian languages (Arora et al., 2023), and a pair of cognate datasets descending from Proto-Germanic and Latin from Luo all contain over 100 languages.\"}"}
{"id": "lrec-2024-main-1116", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. PILA: Proto-Italic to Latin Dataset\\n\\n4.1. Overview\\n\\nPILA is intended to document etymon\u2013reflex relationships between Proto-Italic and Latin. However, languages change over time even as they bear the same name. Therefore, to assure consistency in etymon\u2013reflex relationships, PILA specifically captures Proto-Italic and Latin at stages of development we call \u201cPre-Latin Proto-Italic\u201d and \u201cCicero-nian Latin\u201d (see Fig. 1).\\n\\nWe selected the former to focus PILA on phonetic changes, as this stage of Proto-Italic has already undergone many non-phonological changes which would obscure sound change laws. As for Cicero-nian Latin, we chose it not only because it serves as a standard version of Latin (promoted by the famed orator, Cicero) but also because its distance from Pre-Latin Proto-Italic allows for many meaningful phonetic changes to have occurred.\\n\\nTo store our dataset, we use the Cross-Linguistic Data Format (CLDF) (Forkel et al., 2018). This format consists of CSV, JSON, and BibTEX files that conform to a specification built on the CSV on the Web (CSVW) standard. By providing explicit and uniform structural requirements, CLDF enforces a principled style of data table design and allows adherent datasets to readily work with libraries supporting the data format (e.g., the historical linguistics library LingPy (List and Forkel, 2021)).\\n\\nOur dataset builds on CLDF's Wordlist module. Disregarding the dataset's JSON table metadata file and its BibTEX file to store cited sources, seven tables (CSV files) constitute the dataset:\\n\\n1. languages.csv: a collection of languages contained in the dataset and their attributes.\\n2. forms.csv: a collection of phonetic sequences and tokenized phones. It contains identifiers to link forms with all other tables in PILA. Statistics for these forms are collected in Table 4.\\n3. cognates.csv: a collection of numeric identifiers which link each form to its cognate set.\\n4. lemmata.csv: a collection of lemmata to which our forms are morphologically related. Currently, only Latin forms have lemmata.\\n5. glosses.csv: a collection of notes and tabulated irregular phenomena for our forms. For our irregularity categories, see Table 5.\\n6. tags.csv: a collection of groups of morphological tags which correspond to Latin forms in our dataset. See Section 4.2.5 for details.\\n7. overlaps.csv: a collection of identifiers linking forms in PILA and in others' datasets together. See Section 5.2 for more details.\\n\\n4.2. Development\\n\\nIn this section, we describe our dataset development procedure. We worked closely with an expert in historical linguistics for Proto-Italic and Latin (the last author) and a graduate student with a decade of Latin experience (the first author) to scrape, trim, normalize, augment, and annotate the data. We describe each step below. Although this procedure is framed in the context of Latin and Proto-Italic, many of its steps (e.g., our scraping procedure) could be performed with minor contextual changes for any language pair.\\n\\n4.2.1. Scraping\\n\\nWe initially extracted data from Wiktionary. Because of public availability and decent etymological curation, Wiktionary was deemed to be a suitable starting point for PILA. We scraped all pages tagged as \u201cLatin terms derived from Proto-Italic\u201d for their Latin headwords and Proto-Italic forms (tagged with the lang attribute \u201citc-pro\u201d) (Wiktionary contributors, 2017).\\n\\nFor each \u201cetymology\u201d heading in the Latin section of a headword\u2019s page, we extracted an etymon\u2013reflex pair. (Multiple etymologies are possible if multiple Latin words happen to have the same spelling.) We automatically cleaned the natural-language formatting of etymologies. We filtered out affix headwords (as they do not occur in natural speech), mislabeled headwords, and reconstructed Latin headwords from our data.\\n\\n4.2.2. Trimming\\n\\nWe manually classified etymon-reflex pairs as partial or full reconstructions. Partially-reconstructed pairs have an etymon either with mismatching parts of speech or with mismatching or missing morphemes. For example, Wiktionary reported (at the time of data collection) an etymology for the Latin verb audeo: \u2018I dare\u2019 as the Proto-Italic adjective 4.\\n\\nThe scraping was done on February 15th, 2022.\"}"}
{"id": "lrec-2024-main-1116", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Meanwhile, a fully-reconstructed pair has matching parts of speech and morphemes; in PILA, we have such pairs in *awido* and *audeo* and also in *awi\u00f0os* and *avidus* \u2018desirous\u2019. Having fully-reconstructed forms is desirable because these forms fully represent phonological developments with respect to both stems and affixes, whereas partially-reconstructed forms do not. Therefore, we dropped the partially-reconstructed forms, producing a set of 1,205 fully-reconstructed pairs.\\n\\nOnce these pairs were gathered, our experts examined them for quality. All Proto-Italic forms were checked against standard etymological dictionaries and grammars (Sihler, 1995; Meiser, 2010; Leumann, 1977; Walde and Hofmann, 1938; Ernout and Meillet, 2001), such as the Etymological Dictionary of Latin (henceforth EDL) (de Vaan, 2008). This resulted in the deletion of various forms. Such forms largely fell into one of two categories. First, a form could have either fallen out of use before our Ciceronian Latin period or could have become attested only after that period. Examples of these include *aevita* \u2018age, lifetime\u2019, which was superseded by *aeta* and *genimen* \u2018progeny\u2019 for late attestation. Second, a form could have been a proper name. Proper names have a tendency to contain non-Latin or non-Indo-European phonetic sequences, diverging from PILA\u2019s focus on Italic phonetics. An example is the Latin *tiBur*, the name of a town.\\n\\nWe also made minor alterations to some forms to reflect known Latin developments. These tend to have case-by-case explanations, which we store among our notes in the glosses.csv table.\\n\\n### 4.2.3. Normalization\\n\\nWe normalized the Latin and Proto-Italic data to consistent phonetic representations. This step was necessary because Wiktionary contributors followed different standards in different entries. Proto-Italic forms were normalized to EDL\u2019s conventions, as this work is a current exemplar for Latin historical linguistics (de Vaan, 2008). However, a few alterations were made to improve the phonetic representations. First, because voicing assimilation appears to be a consistent feature of Pre-Latin Italic, *s* before a voiced consonant or between vowels was written as *z*. For example, the etymon for the adjective *numerus* \u2018number\u2019 is *nomezos* instead of *nomesos*. Second, fricatives that resulted from the loss of PIE\u2019s voiced aspirates were reconstructed as voiceless in initial position and as voiced between vowels or before a voiced consonant. The word *faber* \u2018craftsman\u2019 exhibits this well: its etymon, *faB*, has an initial voiceless fricative (*f*) and a medial voiced fricative (*B*).\\n\\nMeanwhile, for Latin, we largely preserved its orthography but again made alterations when the orthography hid the actual pronunciation. We changed *n* or *g* to *\u014b* where they were actually pronounced as *\u014b*. For instance, *magnus* \u2018great\u2019 was written as *ma\u014bnus* and *frango* \u2018I shatter\u2019 as *fra\u014bgo*. Similarly, we duplicate *i* when the orthography conceals that it is used both as a vowel and as a consonant, as in *maiior* \u2018greater\u2019 and *cuiius* \u2018whose\u2019.\\n\\n### 4.2.4. Augmentation: Novel Forms\\n\\nWe added a variety of forms to the cleaned, fully-reconstructed set. In consultation with EDL (de Vaan, 2008), we completed some of the partial reconstructions and appended additional forms. Some appended forms were chosen by our experts to provide representation for missing morphological features. Specifically, the scraped pairs did not include any perfect-tense forms. Thus, we added some in both the active and passive voices.\\n\\nThe perfect active \u201cthird principal part\u201d for standard Latin verbs largely falls into one of four main categories: reduplicating perfects (e.g., *fefelli*: \u2018I deceived\u2019 for *fallo*); root aorists (e.g., *ru* : *pi* : \u2018I broke\u2019 for *rumpo*); *s* aorists (e.g., *scri* : *psi* : \u2018I wrote\u2019 for *scribo*); and *vi* : /*ui* : perfects (e.g., *ama* : *vi* : \u2018I loved\u2019 for *amo* or *monui* : \u2018I warned\u2019 for *moneo*). Some of these forms pose problems.\\n\\nRegarding reduplicating perfects, the inherited reduplicating vowel is *e* (e.g., *memordi* : \u2018I bit\u2019 for *mordeo*). Apparently, when a verbal root itself contained an *e*, the generative principle for the reduplicated syllable was reinterpreted to incorporate the vowel of the root instead of *e* with the initial consonant. That became the standard pattern in reduplicated perfects (e.g., *cucurri* : \u2018I ran\u2019\u2014and not *cecurri:\u2014for *curro* : \u2018I run\u2019), and we reconstructed reduplicating perfects accordingly.\\n\\nMeanwhile, concerning *vi*:/*ui*: perfects, their origins are not clear. Latin is the only Italic language that uses that suffix for the perfect. A form like *monawai* producing *monui* may therefore not be a truly Proto-Italic feature but a late, \u201cPre-Latin\u201d one. However, given the importance of these perfects in the Latin system and the Pre-Latin character of our Proto-Italic, we decided to include them.\\n\\nTurning to perfect passive forms, we added the Latin paradigm-defining *to* participle. In some cases this participle\u2019s form is due to paradigm leveling (see Table 5) or other types of analogical change (e.g., perhaps *pulsus* \u2018having been struck\u2019 after the perfect active *pulsi*: \u2018I struck\u2019). Such forms were excluded, and older forms, where known or securely reconstructable, were used (e.g., *pultus*, which must have existed, as it is the base of the frequentative *pulta*: *re* \u2018to knock\u2019). Through this process, we created a set of 1,515 Latin forms and 1,548 Proto-Italic forms.\"}"}
{"id": "lrec-2024-main-1116", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: PILA sound change irregularity categories, definitions, and examples.\\n\\n4.2.5. Augmentation: Inflections\\n\\nWe elected to further augment this data with inflected forms. This seemed crucial for our target languages, as they are rife with inflections. Moreover, sound changes may occur differently in the citation form and inflected forms of a word. For instance, the Latin *cru*\u02d0s *'leg'* has a genitive form *cru*\u02d0ris *'of the leg'*. The final *s* in *cru*\u02d0s and the medial *r* in *cru*\u02d0ris arise from the same original *s*; however, because the genitive suffix, *is* (Proto-Italic), begins with a vowel, the stem's *s* found itself between two vowels, causing it to rhotacize. Many other Latin forms display similar behaviors (e.g., *flo*\u02d0s *'flower'* and *flo*\u02d0ris *'of the flower'*).\\n\\nBecause nouns, adjectives, participles, and verbs are the primary subjects of inflection in Latin, we added inflections for words in those categories.\\n\\nNominal Forms\\n\\nTo most nouns, adjectives, and participles, we added genitive singular forms. In all declensions, these endings pose certain difficulties. In the first or *a* declension, Latin inherited the PIE ending *a*\u02d0s, preserved in archaic phrases and oldertexts. But the analogical ending *a*i\u02d0, with *i*\u02d0 imported from the second declension, became the standard ending. We used that ending, as it is the result of non-phonological changes that lead into Pre-Latin Proto-Italic. Meanwhile, in the second or *o* declension, Latin preserves the inherited ending *osyo* quite rarely. In attested Latin, the ending is *i*\u02d0; thus, we used that ending in Proto-Italic as well. Lastly, the Latin third declension represents a fusion of PIE i-stems and consonant stems. These i-stems affixed consonant stem endings to an ablauting medial *ej*. The earlier Latin i-stem genitive ending was *i*s from *ejes*. But that affix was replaced by the *is* of the consonant stems. That affix, in turn, represents the generalization of the outcome of inherited *es* over the outcome of inherited *os* (attested, very rarely, as *us*). All Proto-Italic i-stems and consonant stems were thus reconstructed with genitive *es*.\\n\\nVerbal Forms\\n\\nThe 3rd-person singular present of all indicative verbs was added. It is, on some accounts, the least marked member of a paradigm. A problem is posed by the so-called *hic et nunc* (*'here and now'*) marker *i*, which tagged present tense forms in PIE but disappeared in Proto-Italic. The contrast in Old Latin third person endings *ti* and/or *t* producing Latin *t* and/or *d* suggests that the origin of *t* is (partly) phonological. Therefore, verbs were reconstructed with that ending.\\n\\nMorphological Tags\\n\\nTo provide a way to analyze PILA with respect to its inflections, we incorporate a set of morphological tags into our dataset through the tags.csv file. This file lists common morphological properties for all our Latin forms. We adapted the tagset used by the Perseus Project's morphological analyzer, Morpheus (Crane, 1991), for our purposes. Namely, we considered the part-of-speech, person, number, tense, mood, voice, gender, case, and degree for each form. Furthermore, we added a tag for each form's inflection class\u2014that is, the paradigm of conjugation or declension to which a word belongs (if any).\\n\\nAlthough we largely adhered to Morpheus's tagset, we made a few adjustments to suit the ambiguity that comes with examining forms devoid of potential (and, thus, syntactic) context. For instance, although many Latin adjectives change their inflection class to conform to the gender of the noun that they modify, some\u2014such as the word *fe*\u02d0li\u02d0x *'fortunate'*\u2014do not change at all. As a result, we include all possible combinations of genders as potential tags. We detail other aspects of our tagging scheme in our dataset repository.\\n\\n4.2.6. Annotation\\n\\nTo account for the influence of irregular (i.e., rare or non-phonological) changes in phonological studies, we annotated PILA's etymon\u2013reflex pairs with tags to flag the presence of such changes. We present the categories for these tags in Table 5. In our...\"}"}
{"id": "lrec-2024-main-1116", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"dataset, a gloss accompanies each tag to justify its attachment to a given form. One application of these tags could be to filter PILA to a set of forms which have undergone a specific kind of change to study the effects of that change.\\n\\n5. Applications\\n\\nSo far, we have detailed the PILA dataset. In this section, we exhibit PILA's applicability to standard computational historical linguistics studies (Section 5.1) and PILA's capacity to enhance existing datasets through overlapping forms (Section 5.2).\\n\\n5.1. Sample Tasks\\n\\nIn this section, we perform two traditional computational historical linguistics tasks. Suppose that we have a pair of languages, an ancestor \\\\( E \\\\) and a descendant \\\\( R \\\\), both considered as sets of forms. Then we can define a set of etymon\u2013reflex pairs \\\\( C \\\\subseteq E \\\\times R \\\\). We note that both \\\\( e \\\\in E \\\\) and \\\\( r \\\\in R \\\\) could be used in more than one pair.\\n\\nFor this set \\\\( C \\\\), we can define two tasks. First, we define \\\\textit{reflex prediction} as the task of predicting \\\\( r \\\\), given \\\\( e \\\\). Second, we define \\\\textit{etymon reconstruction} as the task of predicting \\\\( e \\\\), given \\\\( r \\\\). We perform both of these tasks with PILA below.\\n\\n5.1.1. Procedure\\n\\nTo perform the reflex prediction and etymon reconstruction tasks, we grouped our data by lemma (citation form) to ensure that inflected forms of the same lemma stay together, and we randomly split the lemmata into training (80%), validation (10%), and test (10%) splits. This resulted in sets of 2,331, 298, and 287 etymon\u2013reflex pairs, respectively.\\n\\nWe train a Transformer encoder\u2013decoder model (Vaswani et al., 2017), and we match most of the hyperparameter settings used in that work. We use PyTorch's Transformer layer implementation (Paszke et al., 2019). Unlike Vaswani et al. (2017), we use pre-norm instead of post-norm (Wang et al., 2019; Nguyen and Salazar, 2019) and apply layer normalization to the last layer's output. We use 6 layers in both the encoder and decoder and 8 attention heads per layer. We initialize the output layer with Xavier uniform initialization (Glorot and Bengio, 2010). For layer norm, we initialize all weights to 1 and all biases to 0. We initialize all other parameters by sampling uniformly from \\\\([-0.01,0.01]\\\\).\\n\\nWe optimize parameters with Adam (Kingma and Ba, 2015). We clip gradients with a threshold of 5 using L2 norm rescaling. We train the model by minimizing the decoder's cross-entropy (summed over all timesteps) on the training set. We use label smoothing with a weight of 0.1. We take a checkpoint every 2,000 examples to evaluate the decoder's per-token cross-entropy on the validation set. After two checkpoints with no improvement, we multiply the learning rate by 0.5; after two more such checkpoints, we stop training early. We train for up to 100 epochs. We use the checkpoint with the best validation cross-entropy.\\n\\nFor each epoch, we randomly shuffle examples and group examples of similar lengths into the same minibatch. We limit the number of tokens in the source or target side of a batch to \\\\( B \\\\) tokens, including padding, \\\\text{bos} \\\\text{, and} \\\\text{eos} symbols. We use beam search decoding with a beam size of 4. We apply length normalization to hypothesis probabilities before selecting the top \\\\( k \\\\) hypotheses for the next beam. We do this by dividing the log-probability of each hypothesis by the number of tokens generated by that hypothesis so far (including \\\\text{eos} symbols).\\n\\nWe perform a random hyperparameter search (Bergstra and Bengio, 2012) over 10 runs. For each run, we randomly sample four hyperparameters: the initial learning rate, the batch size \\\\( B \\\\), the model size \\\\( s \\\\), and the dropout rate. See Table 6 for our chosen distributions. With the model size, we set \\\\( d_{model} \\\\) to \\\\( 8 \\\\cdot s \\\\) and the size of the feedforward hidden layers to \\\\( 4 \\\\cdot d_{model} \\\\). We apply dropout as PyTorch does, which follows Vaswani et al. (2017) and also applies it to feedforward sublayers' hidden units and attention probabilities.\\n\\nTo evaluate our models, we use multi-reference word error rate (WER) and phoneme error rate (PER). \\\"Multi-reference\\\" means that when there are multiple references (multiple etyma for one reflex), we take the minimum error across all references. Note that for WER, the number of errors is either 0 or 1, so WER is one minus the exact match rate. For PER, we use micro-averaging: we sum the total number of edits and total number of reference symbols over the whole test set, reporting their ratio.\\n\\n5.1.2. Results\\n\\nIn Table 7, we report results on the test set from the hyperparameter search's best-performing model on the validation set. Corresponding hyperparameters for the best models are shown in Table 8.\"}"}
{"id": "lrec-2024-main-1116", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WecomparetheTransformermodeldiscussedintheprevioussectiontoa\\\"Copying\\\"baseline. Inthis baseline, the input is simply copied to the output. This baseline is somewhat reasonable because, although Proto-Italic phones undergo many sound changes in becoming Latin, some remain recognizably unchanged. Thus, by comparing a model to the \\\"Copying\\\" baseline, we examine whether it learns any nontrivial sound change rules. As Table 7 shows, the Transformer baseline vastly outperforms the \\\"Copying\\\" baseline in both directions, indicating that this is the case\u2014and that, in fact, PILA provides a learnable signal.\\n\\n5.2. Dataset Compatibility\\n\\nIn this section, we show to what extent PILA's entries can be linked to those of other datasets, allowing for models to be built with longer chains of sound changes and additional linguistic metadata. To measure PILA's capacity to link to other datasets, we tally the overlap between PILA's and other datasets' Latin forms. Because datasets organize data differently, it is nontrivial to extract overlap counts. Moreover, because datasets vary in their attention to phonetic features such as vowel length, the legitimacy of matches can be murky (e.g., without vowel length, PILA's adjectives le:vis 'smooth' and levis 'light' would be indistinguishable).\\n\\nTo account for this, we define two categories of overlap. We say that an overlap is direct if phonological or morphological information is not lost in performing the match. Conversely, an overlap is indirect if some such information is lost. For instance, a form may need to be inflected differently, resulting only in a partial compatibility (or perhaps a false positive match due to homography) between the information stored in each dataset.\\n\\n5.2.1. Procedure\\n\\nAlgorithm 1 sketches our overlap computation procedure. This procedure centers around the Hungarian or Kuhn\u2013Munkres algorithm (Kuhn, 1955; Munkres, 1957), which solves the linear sum assignment problem. In our pseudocode, this algorithm is named LSA, and in practice we use SciPy's implementation (Virtanen et al., 2020). Given a weighted bipartite graph, the Hungarian algorithm seeks the one-to-one matching that maximizes the sum of those edges' weights.\\n\\nThe function scoreComp takes as arguments two lists of nodes (one for each dataset), and each node contains one or more forms. We create a bipartite graph whose nodes are the aforementioned nodes, and if nodeu and nodev have some form in common, there is an edge between u and v with weight 1. Applying a linear-sum assignment algorithm results in a list of edges. The number of edges is called the overlap.\\n\\nWe apply this algorithm twice. First, we take both normalized datasets, place each form in its own node, and apply Algorithm 1 to obtain the direct overlap. Then, we remove all entries from both datasets that participated in the first matching. We take each dataset's remaining nodes, remove long marks (:), from each form, and add that form back.\"}"}
{"id": "lrec-2024-main-1116", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Dataset compatibility study results. Columns measure degrees of overlap relative to PILA. Integers are counts; percentages are relative to the number of data points in that row's dataset. The top three datasets are from related works and are comprised of similar data.\\n\\nWe apply Algorithm 1 again to get the indirect overlap. Finally, we add the direct and indirect overlap to obtain the total overlap.\\n\\n6. Conclusion\\nThis paper introduced PILA, a historical-phonological dataset of etymon\u2013reflex pairs in Proto-Italic and Latin. It described PILA's development process and organization. It provided baseline results for PILA on two historical linguistics tasks and showed PILA's capacity to enhance other datasets in a compatibility study.\\n\\nFuture work could expand the scope of PILA to encourage deeper historical linguistics studies. For instance, it could broaden its coverage of languages in the Italic region. Although they have scant extant data, languages like Umbrian (Dehouck, 2022) as well as Cisalpine Celtic, Faliscan, Oscan, and Venetic (Murano et al., 2023) have received some attention in computational literature.\\n\\nIn another direction, PILA could incorporate sets of phonological rules to support the task of automatic sound law induction (ASLI) (Luo, 2021; Chang et al., 2023). However, many issues arise when considering how to select and store such rules. For example, what formalism should be used to organize sound change rules? Although historical linguists have a standard notation for sound laws, certain features and complex conditions do not have agreed-upon, computationally-friendly notation (Luo, 2021). An examination of prior ASLI work and the sound law databases UNIDIA (Hamed and Flavier, 2009) and PBase (Mielke, 2008) could serve as a starting point for this direction.\\n\\n7. Acknowledgements\\nRegarding the datasets used in this work, we would like to thank Todd Krause for providing us with a version of the IELEX dataset (Linguistics Research Center, 2024). We would also like to thank Alina Maria Ciobanu and Liviu P. Dinu for allowing us to use their dataset (Ciobanu and Dinu, 2014), as well as Shauli Ravfogel for providing us with their revisions to it (Meloni et al., 2021).\"}"}
{"id": "lrec-2024-main-1116", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For their helpful comments and discussions concerning this work, we would also like to thank Darcey Riley, Ken Sible, Aarohi Srivastava, Chihiro Taguchi, and Andy Yang.\\n\\n8. Bibliographical References\\n\\nRobert S. P. Beekes. 1995. Comparative Indo-European Linguistics: An Introduction. John Benjamins Publishing Company, Amsterdam; Philadelphia, PA.\\n\\nJames Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281\u2013305.\\n\\nGregory Crane. 1991. Generating and Parsing Classical Greek. Literary and Linguistic Computing, 6(4):243\u2013245.\\n\\nMichiel de Vaan. 2008. Etymological Dictionary of Latin and the Other Italic Languages. Number 7 in Leiden Indo-European Etymological Dictionary Series. Brill, Leiden; Boston.\\n\\nAlfred Ernout and Alfred Meillet. 2001. Dictionnaire \u00e9tymologique de la langue latine. Klinksieck, Paris.\\n\\nRobert Forkel, Johann-Mattis List, Simon J. Greenhill, Christoph Rzymski, Sebastian Bank, Michael Cysouw, Harald Hammarstr\u00f6m, Martin Haspelmath, Gereon A. Kaiping, and Russell D. Gray. 2018. Cross-Linguistic Data Formats, advancing data sharing and re-use in comparative linguistics. Scientific Data, 5(1):180205.\\n\\nXavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), volume 9 of Proceedings of Machine Learning Research, pages 249\u2013256.\\n\\nKyle P. Johnson, Patrick J. Burns, John Stewart, Todd Cook, Cl\u00e9ment Besnier, and William J. B. Mattingly. 2021. The Classical Language Toolkit: An NLP framework for pre-modern languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 20\u201329.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR).\\n\\nH. W. Kuhn. 1955. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1\u20132):83\u201397.\\n\\nManu Leumann. 1977. Lateinsche Laut-Und Formenlehre. C.H. Beck, Munich.\\n\\nJohann-Mattis List and Robert Forkel. 2021. LingPy. A Python library for historical linguistics. Max Planck Institute for Evolutionary Anthropology.\\n\\nGerhard Meiser. 2010. Historische Laut- Und Formenlehre Der Lateinischen Sprache. Wissenschaftliche Buchgesellschaft, Darmstadt.\\n\\nJames Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the Society for Industrial and Applied Mathematics, 5(1):32\u201338.\\n\\nFrancesca Murano, Valeria Quochi, Angelo Mario Del Grosso, Luca Rigobianco, and Mariarosaria Zinzi. 2023. Describing inscriptions of ancient Italy. The ItAnt project and its information encoding process. Journal on Computing and Cultural Heritage, 16(3).\\n\\nToan Q. Nguyen and Julian Salazar. 2019. Transformers without tears: Improving the normalization of self-attention. In Proceedings of the 16th International Conference on Spoken Language Translation.\\n\\nYves Ouvrard and Philippe Verkerk. 2014. Collatinus, un outil polymorphe pour l'\u00e9tude du Latin. Archivum Latinitatis Medii Aevi, 72(1):305\u2013311.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 8024\u20138035.\\n\\nAndrew Sihler. 1995. New Comparative Grammar of Greek and Latin. Oxford University Press, New York; Oxford.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30.\\n\\nPhilippe Verkerk, Yves Ouvrard, Margherita Fantoli, and Dominique Longr\u00e9e. 2020. L.A.S.L.A. and Collatinus: A convergence in lexica. Studi e saggi linguistici, 58(1):95\u2013120.\"}"}
{"id": "lrec-2024-main-1116", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C. J. Carey, \u0130lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental algorithms for scientific computing in Python. Nature Methods, 17:261\u2013272.\\n\\nAlois Walde and J. B. Hofmann. 1938. Lateinisches Etymologisches W\u00f6rterbuch. Carl Winter, Heidelberg.\\n\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning deep Transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810\u20131822.\\n\\n9. Language Resource References\\n\\nAryaman Arora, Adam Farris, Samopriya Basu, and Suresh Kolichala. 2023. JAMBU: A historical linguistic database for South Asian languages. In Proceedings of the 20th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 68\u201377.\\n\\nKhuyagbaatar Batsuren, G\u00e1bor Bella, and Fausto Giunchiglia. 2019. CogNet: A large-scale cognate database. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3136\u20133145.\\n\\nKhuyagbaatar Batsuren, G\u00e1bor Bella, and Fausto Giunchiglia. 2022. A large and evolving cognate database. Language Resources and Evaluation, 56(1):165\u2013189.\\n\\nByron W. Bender, Ward H. Goodenough, Frederick H. Jackson, Jeffrey C. Marck, Kenneth L. Rehg, Ho-min Sohn, Stephen Trussel, and Judith W. Wang. 2003a. Proto-Micronesian reconstructions\u20131. Oceanic Linguistics, 42(1):1\u2013110.\\n\\nByron W. Bender, Ward H. Goodenough, Frederick H. Jackson, Jeffrey C. Marck, Kenneth L. Rehg, Ho-min Sohn, Stephen Trussel, and Judith W. Wang. 2003b. Proto-Micronesian reconstructions\u20132. Oceanic Linguistics, 42(2):271\u2013358.\\n\\nChundra Cathcart and Taraka Rama. 2020. Disentangling dialects: A neural approach to Indo-Aryan historical phonology and subgrouping. In Proceedings of the 24th Conference on Computational Natural Language Learning (CoNLL), volume 24, pages 620\u2013630.\\n\\nChundra Cathcart and Florian Wandl. 2020. In search of isoglosses: Continuous and discrete language embeddings in Slavic historical phonology. In Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, volume 17, pages 233\u2013244.\\n\\nThiago Chacon. 2014. A revised proposal of Proto-Tukanoan consonants and Tukanoan family classification. International Journal of American Linguistics, 80(3):275\u2013322.\\n\\nKalvin Chang, Chenxuan Cui, Youngmin Kim, and David R. Mortensen. 2022. WikiHan: A new comparative dataset for Chinese languages. In Proceedings of the 29th International Conference on Computational Linguistics (COLING), pages 3563\u20133569.\\n\\nKalvin Chang, Nathaniel Robinson, Anna Cai, Ting Chen, Annie Zhang, and David Mortensen. 2023. Automating sound change prediction for phylogenetic inference: A Tukanoan case study. In Proceedings of the 4th Workshop on Computational Approaches to Historical Language Change, pages 129\u2013142.\\n\\nAlina Maria Ciobanu and Liviu Dinu. 2014. Building a dataset of multilingual cognates for the Romanian lexicon. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC), pages 1038\u20131043.\\n\\nAlbert Davletshin. 2012. Proto-Uto-Aztecan on their way to the Proto-Aztecan homeland: Linguistic evidence. Journal of Language Relationship, 8(1):75\u201392.\\n\\nFernando O. de Carvalho. 2021. A comparative reconstruction of Proto-Purus (Arawakan) segmental phonology. International Journal of American Linguistics, 87(1):49\u2013108.\\n\\nMathieu Dehouck. 2022. The IKUVINA treebank. In Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages, pages 38\u201342.\\n\\nXun Gong and Nathan Hill. 2020. Materials for an etymological dictionary of Burmish. Zenodo.\\n\\nSimon J. Greenhill, Robert Blust, and Russell D. Gray. 2008. The Austronesian basic vocabulary database: From bioinformatics to lexomics. Evolutionary Bioinformatics Online, 4:271\u2013283.\"}"}
{"id": "lrec-2024-main-1116", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
