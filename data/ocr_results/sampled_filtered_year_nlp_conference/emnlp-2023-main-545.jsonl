{"id": "emnlp-2023-main-545", "page_num": 17, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "emnlp-2023-main-545", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Interface of human evaluation comparing the two methods: vanilla dialog inpainting and Dialogizer. (2/2)\"}"}
{"id": "emnlp-2023-main-545", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This is a task to evaluate the quality of a conversational question answering dataset. You will be given [context, two candidate questions, answer], and your task is to compare the quality of the candidate questions based on four criteria: contextual relevance, well-formedness, fluency, overall quality. For each criteria, answer which question is better.\\n\\n1. Contextual Relevance: whether the question relevant to the answer/context\\n2. Well-formedness: whether the question is well-formed\\n3. Overall Quality: overall quality of the question\\n\\n- Context:\\n- Question A:\\n- Question B:\\n- Answer:\\n\\nChoose the question which is more relevant to the given answer.\\noptions: [Question A, Equal, Question B]\\n\\nChoose the question which is more well-formed?\\noptions: [Question A, Equal, Question B]\\n\\nChoose the question which has better overall-quality.\\noptions: [Question A, Equal, Question B]\\n\\nTable 6: The template of the prompt used for GPT-4 evaluation.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Grevillea rudis is a shrub of the genus \\\"Grevillea\\\" native to an area along the west coast in the Wheatbelt region of Western Australia.\\n\\nThe loose, spreading to erect shrub typically grows to a height of and has non-glaucous branchlets.\\n\\nIt has simple flat, spathulate, irregularly lobed leaves with a blade that is long and wide.\\n\\nIt blooms sporadically throughout the year and produces a terminal raceme regular inflorescence with cream or yellow flowers and white or cream styles.\\n\\nLater it forms obovoid or ellipsoidal glandular hairy fruit that is long.\\n\\nIt will regenerate from seed only.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How many hospitalizations does Orthostatic Hypotension cause?\\n\\nOrthostatic hypotension causes \\\\( \\\\approx 80000 \\\\) hospitalizations per year in the United States.\\n\\nWhat are the treatment options for Orthostatic Hypotension?\\n\\nTreatments for Orthostatic hypotension include Fludrocortisone, a mineralocorticoid analog that promotes sodium reabsorption; and midodrine, an \\\\( \\\\alpha_1 \\\\) adrenergic agonist that is a direct vasoconstrictor.\\n\\nHow safe are both medications for Orthostatic Hypotension?\\n\\nAlthough both medications are used to treat Orthostatic hypotension, few studies have compared their relative safety.\\n\\nAre there any other interesting aspects about this article?\\n\\nWe compared incidence rates of hospitalizations for all causes, and for congestive heart failure between users of Fludrocortisone and users of midodrine in a retrospective cohort study of Tennessee Medicaid adult enrollees (1995\u20132009).\\n\\nHow were the adjusted incidence rate ratios calculated?\\n\\nAdjusted incidence rate ratios were calculated using negative binomial regression models.\\n\\nWhat is osteoarthritis?\\n\\nOsteoarthritis is a degenerative joint disorder of articular cartilage and is the most common type of arthritis in the elderly.\\n\\nIs hydroxychloroquine used to treat osteoarthritis?\\n\\nThere are only a few reports regarding the use of hydroxychloroquine in the treatment of Osteoarthritis.\\n\\nWhat is the effect of hydroxychloroquine on symptoms of knee-Osteoarthritis?\\n\\nTo investigate the effects of hydroxychloroquine on the symptoms of mild to moderate knee Osteoarthritis (Kellgren and Lawrence grade II and III), we performed a double-blind, placebo-controlled study in 44 patients.\\n\\nHow many hydroxychloroquine pills did the 44 patients receive?\\n\\nThe patients were randomly assigned to two groups: one group received hydroxychloroquine pills (200 mg twice daily) and the other group received placebo pills.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How do you write bug-free software?\\n\\nWriting bug-free software is practically impossible, due to the impracticality of predicting every way in which code might be executed.\\n\\nWhat are some of the hidden flaws in the underlying programming language that can be exploited by hackers?\\n\\nBut even if developers go above and beyond to avoid flaws that can be exploited by hackers, attackers can often still take advantage of vulnerabilities in the design of the underlying programming language.\\n\\nWhat happened at the recent Black Hat Europe conference?\\n\\nAt the recent Black Hat Europe conference, IOActive security services revealed it had identified flaws in five major, interpreted programming languages that could be used by hackers in crafting an attack.\\n\\nWhat are the interpreted programming languages vulnerabilities?\\n\\n\\\"With regards to the interpreted programming languages vulnerabilities, software developers may unknowingly include code in an application that can be used in a way that the designer did not foresee,\\\" it writes.\\n\\nHow many doctors are freed; strike continues?\\n\\nA Kenyan court has released seven doctors who are officials in the medics' union and who were jailed earlier this week for not calling off a strike by doctors working in public institutions.\\n\\nWhen did the court free the seven?\\n\\nThe decision to free the seven was made Wednesday by three judges of the appellate court.\\n\\nHow did the public react to the release of the seven?\\n\\nAbout 1,000 doctors outside the court celebrated the officials' release and held a peaceful march to Parliament and Nairobi's Freedom Park.\\n\\nWhy are the doctors on strike?\\n\\nMore than 5,000 doctors from public hospitals are on strike over pay and to protest Kenya's dilapidated health care system.\\n\\nWhat did the health minister say about the release?\\n\\nHealth minister Dr. Cleopa Mailu told the Senate committee for health that he had agreed to the release of the officials.\\n\\nWhat did the union say about the release of the officials?\\n\\nThe union had said that no negotiations would be held until the seven were freed.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is a biomarker for anticoagulation?\\n\\nThere is clinical need for a laboratory biomarker to identify patients who, following an unprovoked venous thrombosis (VTE), are at low VTE recurrence risk and can discontinue anticoagulation after a limited treatment duration (3\u20136 m).\\n\\nWhat is a secondary analysis of the ExACT study?\\n\\nThis secondary analysis of the ExACT study aimed to evaluate whether quantitation of peripheral blood endothelial progenitor cells (EPCs) could improve prediction of VTE recurrence risk.\\n\\nWas the ExACT study a non-blinded, multicentre RCT?\\n\\nThe ExACT study was a non-blinded, multicentre RCT comparing extended vs discontinued anticoagulation following a first unprovoked VTE.\\n\\nWho was eligible for the study?\\n\\nAdult patients were eligible if they had completed \u22653 months anticoagulation and remained anticoagulated.\\n\\nWhat was the primary outcome?\\n\\nThe primary outcome was time to first recurrent VTE from randomisation.\\n\\nHow long did the study follow up?\\n\\nBlood samples were taken at baseline and results correlated with clinical outcome over 2 years follow up.\\n\\nWhat do B cells do?\\n\\nB cells constitute an essential line of defense from pathogenic infections through the generation of class-switched antibody-secreting cells (ASCs) in germinal centers.\\n\\nHow do B cells start germinal center reactions?\\n\\nAlthough this process is known to be regulated by follicular helper T (TfH) cells, the mechanism by which B cells initially seed germinal center reactions remains elusive.\\n\\nWhat is the role of NKT cells in B cell immunity?\\n\\nWe found that NKT cells, a population of innate-like T lymphocytes, are critical for the induction of B cell immunity upon viral infection.\\n\\nHow do B cells priming by resident macrophages work?\\n\\nThe positioning of NKT cells at the interfollicular areas of lymph nodes facilitates both their direct priming by resident macrophages and the localized delivery of innate signals to antigen-experienced B cells.\\n\\nHow many IL-4-producing cells are in NKT cells?\\n\\nIndeed, NKT cells secrete an early wave of IL-4 and constitute up to 70% of the total IL-4-producing cells during the initial stages of infection.\\n\\nHow is the requirement of this innate immunity arm conserved in Zika-virus-infected macaques?\\n\\nImportantly, the requirement of this innate immunity arm appears to be evolutionarily conserved because early NKT and IL-4 gene signatures also positively correlate with the levels of neutralizing antibodies in Zika-virus-infected macaques.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nTo address the data scarcity issue in Conversational question answering (ConvQA), a dialog inpainting method, which utilizes documents to generate ConvQA datasets, has been proposed. However, the original dialog inpainting model is trained solely on the dialog reconstruction task, resulting in the generation of questions with low contextual relevance due to insufficient learning of question-answer alignment. To overcome this limitation, we propose a novel framework called Dialogizer, which has the capability to automatically generate ConvQA datasets with high contextual relevance from textual sources. The framework incorporates two training tasks: question-answer matching (QAM) and topic-aware dialog generation (TDG). Moreover, re-ranking is conducted during the inference phase based on the contextual relevance of the generated questions. Using our framework, we produce four ConvQA datasets by utilizing documents from multiple domains as the primary source. Through automatic evaluation using diverse metrics, as well as human evaluation, we validate that our proposed framework exhibits the ability to generate datasets of higher quality compared to the baseline dialog inpainting model.\\n\\n1 Introduction\\n\\nDialog systems (Huang et al., 2020b; Ni et al., 2023) are designed to engage in natural language conversations with users, provide relevant information, answer queries, and simulate human interactions. These systems have gained significant attention in both academics and industry owing to their various potential applications, such as online customer service, virtual assistants, and interactive chatbots (Jia, 2004; Ghose and Barua, 2013; Nuruzzaman and Hussain, 2020). However, the scarcity of datasets poses a major challenge in the development of dialog systems. In particular, for information-seeking conversational question-answering (ConvQA) tasks (Stede and Schlangen, 2004; Zaib et al., 2022), creating a high-quality domain-specific dataset is costly because it requires the direct involvement of domain experts in data annotation (Demszky et al., 2021).\\n\\nRecent work (Dai et al., 2022) proposes a dialog inpainting method to address this challenge by automatically generating ConvQA datasets using pre-existing text datasets. The text dataset is segmented into sentence-level units, which are directly utilized as answers, while the trained dialog inpainter generates questions corresponding to these answers to complete the conversation. Dialog inpainting has the potential to address the data scarcity issue owing to the abundance of online documents authored by domain experts and the capability to convert...\"}"}
{"id": "emnlp-2023-main-545", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"these documents into dialogs with well-defined answers. Considering the guaranteed quality of the answers, it is crucial to generate questions that are well-aligned with each corresponding answer (Sun et al., 2018). However, we have observed a low contextual relevance in the questions generated by the dialog inpainter trained solely on a dialog reconstruction task, as it lacks sufficient training of question-answer alignment. For instance, as illustrated in Figure 1, the dialog inpainter tends to generate questions that exhibit a deficiency in answer specificity (e.g., the green case) or are contextually inappropriate (e.g., the blue case). Through experimental analysis, we quantitatively demonstrate that the questions generated by the dialog inpainter have low $R_Q$UGE (Mohammadshahi et al., 2022) scores, indicating a lack of contextual relevance.\\n\\nThis study introduces Dialogizer as a novel framework for generating contextually relevant ConvQA datasets from textual datasets. The framework incorporates two training methodologies, in addition to dialog reconstruction, to address the limitation of generating contextually-irrelevant questions. These methodologies include a question-answer matching (QAM) task and a topic-aware dialog generation (TDG) task. In the QAM task, the model is provided with numerous QA pairs and learns to differentiate between matching and non-matching pairs. This enables the model to discern contextual relevance among QA pairs. In the TDG task, we provide the model with keywords extracted from the target answer using a keyword extractor. Then, the model learns to generate answer-specific questions using these keywords along with the given answer sentence and the dialog history. By incorporating these training tasks, the model becomes capable of generating more specific and answer-relevant questions. Furthermore, during the inference process of generating dialog from the passage, re-ranking is conducted using contextual relevance as a metric, ensuring the generation of high-quality questions.\\n\\nUsing Dialogizer, we compile four ConvQA datasets by leveraging source documents from various domains, such as news and medicine. Through automatic evaluation using multiple reference-free dialog metrics, human evaluation, GPT-4 evaluation (Chiang and Lee, 2023; Liu et al., 2023), and an application to text retrieval tasks, we experimentally demonstrate that our Dialogizer-generated datasets exhibit higher quality and context relevance than those generated by the vanilla dialog inpainter. Furthermore, we present an ablation study that shows the effectiveness of each methodology of the proposed framework. Our results represent evidence supporting the potential impact of the Dialogizer in advancing ConvQA research.\\n\\n2 Related Works\\n\\n2.1 Conversational Question-Answering\\n\\nConversational question-answering (ConvQA) aims to enable machines to effectively answer multiple questions from users based on a given passage. ConvQA datasets must contain accurate information regarding specific domains, maintain consistency across topics, and facilitate the progression of dialog turns hierarchically (Zhu et al., 2021). However, creating ConvQA datasets is labor-intensive, as evidenced by the manual annotations throughout existing ConvQA datasets such as CoQA (Reddy et al., 2019), CSQA (Saha et al., 2018), and ConvQuestions (Christmann et al., 2019). In this work, we present a framework designed to automatically generate high-quality ConvQA datasets and provide empirical evidence that generated datasets may serve as valuable resources for ConvQA tasks.\\n\\n2.2 Dialog Inpainting\\n\\nTo overcome the data scarcity problem in ConvQA, an automatic ConvQA dataset generation framework called dialog inpainting (Dai et al., 2022) has recently been developed. Similar to filling in one side of a phone call conversation by overhearing the other side, this methodology considers sentences from text documents as one person\u2019s utterances to generate the remaining utterances, thereby completing the conversation. The efficiency of dialog inpainting as a ConvQA dataset generation framework is demonstrated by its ability to automatically convert text data into a dialog format without loss of information, as evidenced by the abundance of high-quality documents annotated by experts in domains. However, we have experimentally observed that the questions generated via dialog inpainting lack contextual relevance. In this study, we propose Dialogizer as a framework for generating contextually relevant ConvQA datasets.\\n\\n2.3 Reference-free Dialog Metrics\\n\\nOwing to the one-to-many nature of dialog and question generation tasks (Zhao et al., 2017), reference-based natural language generation metrics fail to yield suitable results.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dialog Reconstruktion\\nRe-ranking\\nwith RQUGE\\nTRAININGINFERENCE\\nTopic-Aware Dialog Generation\\nQuestion-Answer Matching\\n\\nA\\nU3\\nWell, I feel a little tired.\\n\\nD\\nA\\nQ\\nMost of the club's income came from net gate receipts of \u00a33,733.\\n\\nQ\\nAlia Bhatt won the Filmfare Critics Award for Best Actress.\\n\\nS1\\nSean John Combs is an American rapper, singer, and record producer.\\n\\nS1\\nWho did Sean Combs start the company with?\\n\\nQ\\nDid Alia Bhatt receive any awards?\\n\\nFigure 2: An overview of the proposed Dialogizer framework. In the training phase, in addition to the dialog reconstruction task, two novel tasks are incorporated: Question-Answer Matching and Topic-aware Dialog Generation. During the inference phase, autoregressive question generation is performed using textual data to complete the conversation, and for each question generation, re-ranking is conducted based on the contextual relevance.\\n\\nMetrics (Papineni et al., 2002; Lin, 2004) have shown a poor association with human judgment (Liu et al., 2016; Lowe et al., 2017; Gupta et al., 2019). Additionally, these metrics have a limitation as they can only be applied in situations where a reference is available. Therefore, recent studies have focused on developing reference-free metrics (Huang et al., 2020a; Gao et al., 2020; Zhang et al., 2021) that exhibit high correlations with human judgment. Hence, we employ various reference-free metrics to evaluate the quality of generated ConvQA datasets. In addition, the reference-free metrics can be used as re-ranking criteria to enhance the generation quality (Wang et al., 2023b). In this study, we employ a reference-free metric, $R^\\\\text{QUGE}$, in the re-ranking process to enhance question generation performance.\\n\\n3 Dialogizer\\n\\nDialogizer is a novel framework designed to generate contextually relevant ConvQA datasets of high quality from textual sources. In addition to the simple dialog reconstruction (DR) task (\u00a73.1), the framework incorporates two novel training methodologies: Question-Answer Matching (QAM) (\u00a73.2) and Topic-aware Dialog Generation (TDG) (\u00a73.3). During the inference phase, the textual passage is segmented into sentences that serve as answers, and the trained model autoregressively generates questions relevant to each answer to complete the ConvQA dataset (\u00a73.4). Furthermore, to consistently generate stable and high-quality questions, Dialogizer employs re-ranking through beam search during the inference phase, taking into account the contextual relevance of the generated questions (\u00a73.5). Figure 2 provides an illustrative overview of our framework.\\n\\n3.1 Dialog Reconstruction\\n\\nDialogizer is basically trained with the Dialog Reconstruction (DR) task proposed in Dai et al. (2022), which includes the random masking of one utterance $d_m(t) = (u_1, u_2, \\\\ldots, u_{t-1}, \\\\cdot, u_{t+1}, \\\\ldots, u_T)$ in the dialog $d = (u_1, u_2, \\\\ldots, u_{t}, \\\\ldots, u_T)$, with the masked utterance denoted by the symbol $\\\\cdot$. The objective is to reconstruct the missing utterance $u_t$. Specifically, we utilize the T5 (text-to-text transfer transformer) model (Raffel et al., 2020) to implement Dialogizer. Dialogizer can be described as a generative model characterized by parameters $\\\\theta$, which define a probability distribution $p_\\\\theta$ of the target utterance $u_t$ given the masked dialog $d_m(t)$. The following DR training objective is set:\\n\\n$$L_{DR}(\\\\theta) = -\\\\sum_{d \\\\in D} E_{u_t \\\\sim d} \\\\left[ \\\\log p_\\\\theta(u_t | d_m(t)) \\\\right],$$\\n\\nwhere $D$ is a dialog corpus. The vanilla dialog inpainting model is trained exclusively in the DR task. However, our observations have confirmed that questions generated by this model exhibit low contextual relevance. Given that the answers are already well-defined as they generated by this model exhibit low contextual relevance. Given that the answers are already well-defined as they...\"}"}
{"id": "emnlp-2023-main-545", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are extracted from the passage, generating questions that align well with the answer is a crucial concern. Consequently, we identify two primary factors that contribute to this phenomenon. Firstly, since the DR is simply designed to reconstruct masked utterances in the original dialogs, training the model exclusively on the DR task results in insufficient learning of question-answer alignment necessary for ConvQA dataset. Second, the original model's reliance on document title information as a prompt during the inference stage poses challenges in determining the specific information that must be conveyed within the generated question. In this work, we aim to overcome these challenges and enhance the contextual relevance of questions in ConvQA dataset generation.\\n\\n3.2 Question-Answer Matching\\n\\nTo address the challenge of insufficient acquisition of question-answer alignment in vanilla dialog inpainting, we incorporate the Question Answer Matching (QAM) task into the Dialogizer training. This task aims to enhance the model's ability to interpret long-term dependencies in question-answer pairs. Similar to the BERT (Devlin et al., 2018) pre-training technique known as next-sentence prediction, this task focuses on the binary classification of matching QA pairs. During the training phase, we utilize a negative sampling method to extract negative answers for specific questions in ConvQA dialogs. This method involves randomly sampling from the same dialog, which is challenging due to the similarity of context. Accordingly, the objective of QAM is to train the model to classify positive answers as MATCH and negative answers as NOT MATCH, as shown in Figure 2. Therefore, the QAM training objective is to minimize the following loss function:\\n\\n\\\\[ L_{QAM}(\\\\theta) = -\\\\sum_{d \\\\in D} E_{q_t \\\\sim d} \\\\left[ \\\\log p_{\\\\theta}(t_P | q_t, a_{P_t}) + \\\\log p_{\\\\theta}(t_N | q_t, a_{N_t}) \\\\right], \\\\]\\n\\nwhere \\\\( D \\\\) is a corpus of ConvQA dialogs, \\\\( q_t \\\\) is a randomly sampled question from dialog \\\\( d \\\\), \\\\( a_{P_t} \\\\) is a positive answer corresponding to question \\\\( q_t \\\\), \\\\( a_{N_t} \\\\) is a negative answer randomly sampled from the dialog \\\\( d \\\\), and \\\\( t_P \\\\) and \\\\( t_N \\\\) represent positive and negative target texts, respectively. Given the T5 architecture's representation of input and output as text strings, the target output \\\\( t_P \\\\) is set to \\\"The answer matches the question\\\" whereas \\\\( t_N \\\\) is set to \\\"The answer does not match the question\\\". These outputs serve as reference labels for the model to learn and classify alignment within given question-answer pairs.\\n\\n3.3 Topic-aware Dialog Generation\\n\\nIn typical question-generation tasks, the primary objective is to determine what to ask and how to ask (Pan et al., 2019; Ghanem et al., 2022). However, when generating questions through this framework, the answers or contents of the questions are predetermined. Therefore, the model must be able to generate good questions by utilizing hints from the predetermined what to ask. To address this issue, Dai et al. (2022) incorporates the document title within the prompt during the inference phase. However, this approach yields unsatisfactory results as document titles are often excessively abstract. Consequently, the model fails to generate answer-relevant questions, instead producing overly general questions such as \\\"What is {document_title}?\\\" or \\\"Are there any other interesting aspects about this article?\\\". To reduce the reliance on document titles and enable the Dialogizer to generate contextually relevant questions, we facilitate knowledge acquisition through the Topic-aware Dialog Generation (TDG) task. We hypothesize that incorporating extracted keywords as hints about what to ask would enhance the generation of more specific and answer-relevant questions. To ensure that the extracted keywords are effectively incorporated into the prompt during the inference phase, we introduce a TDG task during the training phase to train the utilization of extracted keywords. The TDG loss is computed as\\n\\n\\\\[ L_{TDG}(\\\\theta) = -\\\\sum_{d \\\\in D} E_{q_t \\\\sim d} \\\\left[ \\\\log p_{\\\\theta}(q_t | d_{m(t)}, k_t) \\\\right], \\\\]\\n\\nwhere \\\\( k_t \\\\) denotes the keywords extracted from the answer. \\\\( k_t \\\\) is obtained by applying a keyword extractor to the answer corresponding to \\\\( q_t \\\\) and subsequently utilized in the prompt with the format \\\"Keyword: \\\\( k_t \\\\)\\\". Ultimately, we train our Dialogizer model by aggregating the losses using the following approach:\\n\\n\\\\[ L = L_{DR} + \\\\lambda_{QAM} \\\\ast L_{QAM} + \\\\lambda_{TDG} \\\\ast L_{TDG}. \\\\]\"}"}
{"id": "emnlp-2023-main-545", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Automatic evaluation results on datasets generated using the baseline dialog inpainting framework and our proposed Dialogizer framework, based on four source datasets.\\n\\n\u2020: utilized for re-ranking.\\n\\n1 WikiDialog2, 2 PubmedDialog, 3 CC-newsDialog, 4 ElsevierDialog\\n\\n3.4 Inference: Autoregressive Generation\\n\\nThe inference process of the Dialogizer framework comprises transforming a passage into a ConvQA dialog. In the inference phase, our Dialogizer model fills in the masked utterances in the partial dialog \\\\((s_p, s_1, s_2, \\\\ldots, s_T)\\\\) for a given passage \\\\((s_1, s_2, \\\\ldots, s_T)\\\\) with a prompt \\\\(s_p\\\\) using the document title. We then add the keyword prompt \\\"Keyword: \\\\(k(s_t)\\\\)\\\" before the mask token, where \\\\(k(s_t)\\\\) represents the keywords extracted from the answer utterance \\\\(s_t\\\\). The red block in Figure 2 represents the keyword prompt. Additionally, the model generates utterances auto-regressively to avoid discrepancies with the DR approach that generates one utterance at a time. Namely, Dialogizer generates \\\\(\\\\hat{u}_1\\\\) with \\\\((s_p, \\\"Keyword: k(s_1)\\\\\\\"), s_1)\\\\), and continues generating \\\\(\\\\hat{u}_2\\\\) with \\\\((s_p, \\\\hat{u}_1, \\\"Keyword: k(s_2)\\\\\\\"), s_2)\\\\) to fill in all masks auto-regressively, thereby completing the partial dialog to \\\\((s_p, \\\\hat{u}_1, s_1, \\\\ldots, \\\\hat{u}_T, s_T)\\\\). The overall inference process can be shown at the bottom of Figure 2.\\n\\n3.5 Re-ranking with contextual relevance\\n\\nIn addition, we incorporate re-ranking (RR) in the inference phase to improve the contextual relevance of the generated questions. Relying solely on corpus statistics to generate the most likely output in the decoding stage does not guarantee contextual quality. To ensure the quality of the generated questions, we opt for a re-ranking process based on contextual relevance. Mohammadshahi et al. (2022) have shown that re-ranking with R\\\\(\\\\text{U}G\\\\text{E}\\\\) increases contextual relevance and enhances correlation with human judgment in sentence evaluation. Therefore, we utilize R\\\\(\\\\text{U}G\\\\text{E}\\\\) to re-rank candidate questions. Specifically, the model first generates a set of k-candidate questions using beam search. Then, the model selects the most relevant question to both the passage and the answer based on R\\\\(\\\\text{U}G\\\\text{E}\\\\).\\n\\n4 Experiments\\n\\nWe deploy the Dialogizer framework to generate four datasets, validating the quality of the datasets through diverse evaluations. The experimental details are provided in Appendix C.\\n\\n4.1 Model implementation\\n\\nWe evaluate Dialogizer's performance by comparing it to the original dialog inpainting as a baseline, ensuring identical conditions. The baseline is implemented using the same backbone model and training dataset as Dialogizer for a fair comparison. Both models utilize a T5-base (Raffel et al., 2020) as their backbone. We train both frameworks on two open-domain dialog datasets, namely DailyDialog (Li et al., 2017) and Task Masker (Byrne et al., 2019), along with two ConvQA datasets, OR-QUAC (Qu et al., 2020) and QReCC (Anantha et al., 2020). For both models, we use four datasets for the dialog reconstruction task. For Dialogzier, we also use these two ConvQA datasets for QAM and TDG tasks during training. During the TDG training, keyword extraction is performed using the T5-based model developed by P\u02dbezik et al. (2022).\\n\\n4.2 Generated Datasets\\n\\nUsing Dialogizer, we generate four ConvQA datasets for use in experiments. These datasets are developed by leveraging four source-text datasets from diverse domains: Wikipedia, PubMed, CC-News (Hamborg et al., 2017), and Elsevier OA CC-By (Kershaw and Koeling, 2020). Each dataset is named after its corresponding source dataset, namely WikiDialog2, PubmedDialog, CC-newsDialog, and ElsevierDialog. Detailed statistics...\"}"}
{"id": "emnlp-2023-main-545", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Human evaluation\\n0.23 0.19\\nContextual Relevance\\n0.75 0.03 0.22\\nWell-formed\\n0.58\\n: Dialogizer : Equal : Baseline dialog inpainter\\nGPT-4 Evaluation\\n0.76 0.03 0.21\\nOverall Quality\\n0.22 0.16\\n0.37 0.59 0.04\\n0.61\\n0.71 0.15 0.14\\n\\nFigure 3: The human evaluation result comparing two datasets, each generated by the baseline dialog inpainting and our proposed Dialogizer. Ours obtained positive evaluations across all three criteria (contextual relevance, well-formedness, and overall quality), surpassing the baseline.\\n\\nand sample dialogs for each dataset can be found in Appendices A and H.\\n\\n4.3 Automatic Evaluation\\nEvaluation Metrics\\nFor quantitative comparison, we assess the generated datasets using diverse reference-free metrics that gauge distinct aspects of the dialog or generated questions. First, $R^2$ UGE is a reference-free metric for question generation that measures the quality of a given candidate question based on its corresponding answer and relevant passage. Similarly, $Q_{RelScore}$ (Wang et al., 2022) is a context-aware evaluation metric for question generation that measures word- and sentence-level relevance without additional training or human supervision. This metric consists of $Q_{RelScoreLRM}$ and $Q_{RelScoreGRG}$: the former handles complex reasoning by calculating word-level similarity, while the latter measures factual consistency by comparing confidence in generating context.\\n\\nUSR-DR (Mehri and Eskenazi, 2020) is a dialog evaluation metric specifically developed to evaluate the aspects of context maintenance, interest, and knowledge utilization through a retrieval task. This metric evaluates a dialog using retrieval results from the Ubuntu dialog corpus (Lowe et al., 2015), resulting in two categories: USR-DR($c$) incorporates history and facts, while USR-DR($f$) relies just on fact information for context. Pang et al. (2020) also proposed a GPT-2 based dialog metric that evaluates context coherence between sentences in a dialog.\\n\\nResults\\nWe perform automatic evaluations by comparing the quality of the datasets produced by the baseline model and Dialogizer using four source datasets to demonstrate the effectiveness of the Dialogizer in generating ConvQA datasets. We present the main results in Table 1. Our findings demonstrate that the datasets generated by Dialogizer surpass those created by baseline across all metrics. When evaluating the results for five metrics excluding $R^2$ UGE, which is used for re-ranking, Dialogizer exhibits average performance improvements of 18.23% for Wikidialog2, 17.52% for PubmedDialog, 23.66% for CC-newsDialog and 38.80% for ElsevierDialog. The results validate that Dialogizer generates datasets of superior quality and enhanced contextual relevance compared to the baseline. The exceptional performance of Dialogizer across diverse source datasets from various domains further amplifies its value as a ConvQA dataset-generation framework.\\n\\n4.4 Human and GPT-4 Evaluation\\nTo comprehensively evaluate the quality of the questions generated by the baseline and Dialogizer, we randomly sample 100 dialogs from the generated datasets and conduct a relative comparison through both human and GPT-4. For both evaluations, we use three criteria, i.e., contextual relevance, well-formedness, and overall quality, according to the characteristics of the ConvQA task and existing research (Liang and Li, 2021).\\n\\nContextual relevance measures the relevance of the question to the context and answer, well-formedness assesses whether the question is well-formed, and overall quality measures the overall quality of the context and question-answer pair. Human evaluation involves three crowd workers per question. Figure 3 shows that Dialogizer outperforms the baseline across all criteria. Additionally, when utilizing GPT-4 as an NLG evaluator (Wang et al., 2023a), our model consistently outperforms the baseline, confirming its superior performance. More detailed information regarding human and GPT-4 evaluations \u2013 inter-annotator agreement, payment details, instructions, and prompts \u2013 can be found in Appendices F and G.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Average values for text retrieval benchmark of Sentence Transformer (ST) (Reimers and Gurevych, 2019) models trained on dialog datasets generated by three frameworks: baseline dialog inpainter, Dialogizer without re-ranking (\u2020), and Dialogizer (Higher is better). Shading indicates a standard deviation across five seeds.\\n\\n| DR QAM TDG RR | R QUGE USR-DR (c) USR-DR (f) QRelScore LRM QRelScore GRG GPT2 |\\n|----------------|------------------------------------------------------------------|\\n| \u2713              | - - -                                                            |\\n| 2.7579 0.9270 0.6455 0.4655 0.5077 0.6046 | \u2713 \u2713 - -                                                         |\\n| 2.7818 0.9437 0.7308 0.4840 0.5643 0.6224 | \u2713 - \u2713                                                          |\\n| 2.8732 0.9454 0.7381 0.4823 0.5283 0.6191 | \u2713 \u2713 \u2713 -                                                          |\\n| 2.9228 0.9472 0.7437 0.5003 0.5859 0.6232 | \u2713 \u2713 \u2713 \u2713                                                         |\\n| 3.6590 0.9585 0.7592 0.4895 0.5749 0.6257 | \u2713 \u2713 -                                                          |\\n| 3.7095 0.9667 0.8141 0.5165 0.5923 0.6338 | \u2713 \u2713 \u2713                                                            |\\n| 3.7200 0.9753 0.8373 0.5210 0.5793 0.6394 | \u2713 \u2713 \u2713 \u2713                                                          |\\n| 3.8303 0.9883 0.9416 0.5303 0.5893 0.6570 | \u2713 \u2713                                                            |\\n\\nTable 2: Results of an ablation study examining the impact of QAM, TDG, and Re-ranking (RR) components in the Dialogizer framework on the improvement of automatic evaluation performance. Wikipedia is used as a source dataset.\\n\\n4.5 Application to Text Retrieval\\n\\nIn this section, we verify the alignment of the questions generated by our framework with the passage through a zero-shot text retrieval task. The coherence of query-passage pairs during training is crucial for improving performance in retrieving relevant information without explicit task training, especially in zero-shot scenarios. To gauge the alignment, we train a text retrieval model using the generated questions as queries paired with the original passage and assess its performance through a zero-shot text retrieval benchmark. We consider three experimental variations: baseline dialog inpainting, Dialogizer without re-ranking during the inference phase, and Dialogizer with re-ranking for detailed understanding. These three methods are applied to the Wiki corpus to construct passage-query pair datasets and generate 10k questions as queries for each method. These queries are then matched with their corresponding original passages to construct a training set for text retrieval.\\n\\nExperimental results for the Ms-Marco benchmark (Nguyen et al., 2016) are shown in Figure 4. The retrieval model trained on passage-question pairs generated by Dialogizer consistently outperforms those trained using the baseline in terms of NDCG (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2017), Mean Average Precision (MAP), and RECALL. Moreover, when Dialogizer incorporates re-ranking during the inference phase, it generates more relevant question pairs within the passage, further improving performance. These experimental results confirm that Dialogizer exhibits exceptional proficiency in generating questions relevant to a given passage. Experimental details and other benchmark results can be found in Appendix D.\\n\\n5 Analysis\\n\\n5.1 Ablation Study\\n\\nTo gain deeper insight into our method, we conduct an ablation study comparing the quality of datasets generated by applying each methodology to Wikipedia. As shown in Table 2, the Dialogizer model trained with TDG and QAM generates more contextually relevant questions, indicating that both proposed additional tasks effectively fa-\"}"}
{"id": "emnlp-2023-main-545", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The distribution of question types in Wikidialog2, accompanied by explanations of their respective descriptions.\\n\\n- **CONCEPT**: Asking for a definition or explanation of a concept (38.49%)\\n- **EXAMPLE**: Asking for examples of a concept (32.91%)\\n- **VERIFICATION**: Seeking confirmation regarding truthfulness of a concept (15.54%)\\n- **JUDGEMENTAL**: Requesting the answerer's own opinions (8.57%)\\n- **COMPARISON**: Asking for comparison between multiple concepts (4.49%)\\n\\nFor instance, the $R_Q^GUE$ and USR-DR metrics, which evaluate relevance by considering context, questions, and answers, indicate that TDG yields greater performance enhancement. In contrast, QRelScore, which evaluates question-answer pairs, demonstrates the effectiveness of QAM. The results obtained from training with both TDG and QAM simultaneously indicate that the two approaches synergistically contribute to generating more contextually relevant data. Furthermore, when performing re-ranking based on $R_Q^GUE$ scores during inference, performance improvements are observed across all scenarios and metrics.\\n\\n### 4.4207 4.2768\\n\\n### 4.3711\\n\\n### 5.2 Question Types\\n\\nWe analyze the question-type distribution of open-ended questions in Wikidialog2. By referring to the 18 question types specified by Olney et al. (2012), we construct a question type ontology by merging ambiguous types (Cao and Wang, 2021). Table 3 shows that Wikidialog2 encompasses a diverse range of types, including 38% concepts, 31% examples, and 14% verifications. Regarding the ConvQA characteristic of information seeking, we note that there are relatively fewer judgemental questions that inquire about the respondent's opinion. Furthermore, as the original passage is segmented into sentence units to construct answers, it is inferred that fewer comparison-type questions that require comparing multiple concepts within a single question-answer pair. The experimental details can be found in the appendix E.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"specific question creation. Our experimental results\\ndemonstrate that Dialogizer produces contextually\\nrelevant ConvQA datasets. The proposed frame-\\nwork holds promise for advancing ConvQA re-\\nsearch and its practical applications.\\n\\nLimitations\\nOur framework demonstrates improved perfor-\\nance compared to the baseline in terms of Con-\\nvQA dataset generation; however, it is computa-\\ntionally more expensive due to the inclusion of the\\nbeam search during the inference phase. When set-\\ntting the beam size to 5, the inference time increased\\nby approximately 2.4 times compared to greedy de-\\ncoding. Since it is a dataset generation framework,\\nthe inference time may not be critical in real-world\\nscenarios. However, when aiming to generate a\\nlarge number of datasets for purposes such as data\\naugmentation, the inference time should also be\\ntaken into consideration as a significant factor.\\n\\nIn addition, unlike the DR task, which can be\\napplied to all dialogs datasets, the novel tasks pro-\\nposed in this study, QAM and TDG, require the\\nConvQA dataset during the training process. This\\nis because these tasks aim to effectively train the\\nmodel in acquiring a comprehensive understanding\\nof question-answer alignment, thus posing the limi-\\ntation that well-matched question-answer pairs are\\nnecessary.\\n\\nEthics Statement\\nTo verify that the generated datasets do not contain\\nany potential ethical problems, crowd workers were\\ninstructed to ascertain that the generated datasets\\ndo not include offensive, sexist, or racist comments;\\ntoxic language; or any instances of sexual behavior.\\nThe crowd workers were fairly compensated for\\ntheir work. Additionally, a detailed description,\\ninterface to collect human evaluations, and further\\ndetails of payment can be found in Appendix F.\\n\\nAdditionally, we utilize the GPT-4 model from\\nofficial website of OpenAI for GPT-4 evaluation.\\n\\nAll models and datasets used in the experiments\\nare from the publicly accessible website or Github\\nrepositories.\\n\\nAcknowledgements\\nThis work was supported by LG AI Research. This\\nwork was partly supported by Institute of Infor-\\nmation & communications Technology Planning\\n& Evaluation (IITP) grant funded by the Korea\\ngovernment(MSIT) [NO.2021-0-01343, Artificial\\nIntelligence Graduate School Program (Seoul Na-\\ntional University) & NO.2021-0-02068, Artificial\\nIntelligence Innovation Hub (Artificial Intelligence\\nInstitute, Seoul National University)], the BK21\\nFOUR program of the Education and Research Pro-\\ngram for Future ICT Pioneers, Seoul National Uni-\\nversity in 2023, and the National Research Foun-\\ndation of Korea (NRF) grant funded by the Korea\\ngovernment (No. 2021R1A2C2008855). K. Jung\\nis with ASRI, Seoul National University, Korea.\\n\\n8814\"}"}
{"id": "emnlp-2023-main-545", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2020. Open-domain question answering goes conversational via question rewriting. arXiv preprint arXiv:2010.04898.\\n\\nVera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. 2016. A full-text learning to rank dataset for medical information retrieval. In Advances in Information Retrieval: 38th European Conference on IR Research, ECIR 2016, Padua, Italy, March 20\u201323, 2016. Proceedings 38, pages 716\u2013722. Springer.\\n\\nBill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Daniel Duckworth, Semih Yavuz, Ben Goodrich, Amit Dubey, Andy Cedilnik, and Kyu-Young Kim. 2019. Taskmaster-1: Toward a realistic and diverse dialog dataset. arXiv preprint arXiv:1909.05358.\\n\\nShuyang Cao and Lu Wang. 2021. Controllable open-ended question generation with a new question type ontology. arXiv preprint arXiv:2107.00152.\\n\\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937.\\n\\nPhilipp Christmann, Rishiraj Saha Roy, Abdalghani Abujabal, Jyotsna Singh, and Gerhard Weikum. 2019. Look before you hop: Conversational question answering over knowledge graphs using judicious context expansion. In Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages 729\u2013738.\\n\\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S Weld. 2020. Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180.\\n\\nZhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao, Aida Amini, Qazi Mamunur Rashid, Mike Green, and Kelvin Guu. 2022. Dialog inpainting: Turning documents into dialogs. In International Conference on Machine Learning, pages 4558\u20134586. PMLR.\\n\\nDorottya Demszky, Jing Liu, Zid Mancenido, Julie Cohen, Heather Hill, Dan Jurafsky, and Tatsunori Hashimoto. 2021. Measuring conversational uptake: A case study on student-teacher interactions. arXiv preprint arXiv:2106.03873.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\\n\\nXiang Gao, Yizhe Zhang, Michel Galley, Chris Brockett, and Bill Dolan. 2020. Dialogue response ranking training with large-scale human feedback data. arXiv preprint arXiv:2009.06978.\\n\\nBilal Ghanem, Lauren Lutz Coleman, Julia Rivard Dexter, Spencer McIntosh von der Ohe, and Alona Fyshe. 2022. Question generation for reading comprehension assessment by modeling how and what to ask. arXiv preprint arXiv:2204.02908.\\n\\nSupratip Ghose and Jagat Joyti Barua. 2013. Toward the implementation of a topic specific dialogue based natural language chatbot as an undergraduate advisor. In 2013 international conference on informatics, electronics and vision (ICIEV), pages 1\u20135. IEEE.\\n\\nPrakhar Gupta, Shikib Mehri, Tiancheng Zhao, Amy Pavel, Maxine Eskenazi, and Jeffrey P Bigham. 2019. Investigating evaluation of open-domain dialogue systems with human generated multiple references. arXiv preprint arXiv:1907.10568.\\n\\nFelix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. 2017. news-please: A generic news crawler and extractor. In 15th International Symposium of Information Science (ISI 2017), pages 218\u2013223.\\n\\nLishan Huang, Zheng Ye, Jinghui Qin, Liang Lin, and Xiaodan Liang. 2020a. Grade: Automatic graph-enhanced coherence metric for evaluating open-domain dialogue systems. arXiv preprint arXiv:2010.03994.\\n\\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020b. Challenges in building intelligent open-domain dialog systems. ACM Transactions on Information Systems (TOIS), 38(3):1\u201332.\\n\\nKalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2017. Ir evaluation methods for retrieving highly relevant documents. In ACM SIGIR Forum, volume 51, pages 243\u2013250. ACM New York, NY, USA.\\n\\nJiyou Jia. 2004. The study of the application of a web-based chatbot system on the teaching of foreign languages. In Society for Information Technology & Teacher Education International Conference, pages 1201\u20131207. Association for the Advancement of Computing in Education (AACE).\\n\\nDaniel Kershaw and Rob Koeling. 2020. Elsevier oa cc-by corpus. arXiv preprint arXiv:2008.00774.\\n\\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\\n\\nJ Richard Landis and Gary G Koch. 1977. An application of hierarchical kappa-type statistics in the assessment of majority agreement among multiple observers. Biometrics, pages 363\u2013374.\\n\\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. arXiv preprint arXiv:1710.03957.\\n\\nHongru Liang and Huaqing Li. 2021. Towards standard criteria for human evaluation of chatbots: A survey. arXiv preprint arXiv:2105.11197.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-545", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. arXiv preprint arXiv:2004.14974.\\n\\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048.\\n\\nXiaoqiang Wang, Bang Liu, Siliang Tang, and Lingfei Wu. 2022. Qrelscore: Better evaluating generated questions with deeper understanding of context-aware relevance. arXiv preprint arXiv:2204.13921.\\n\\nYihe Wang, Yitong Li, Yasheng Wang, Fei Mi, Pingyi Zhou, Jin Liu, Xin Jiang, and Qun Liu. 2023b. History, present and future: Enhancing dialogue generation with few-shot history-future prompt. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE.\\n\\nMunazza Zaib, Wei Emma Zhang, Quan Z Sheng, Adnan Mahmood, and Yang Zhang. 2022. Conversational question answering: A survey. Knowledge and Information Systems, 64(12):3151\u20133195.\\n\\nChen Zhang, Yiming Chen, Luis Fernando D'Haro, Yan Zhang, Thomas Friedrichs, Grandee Lee, and Haizhou Li. 2021. Dynaeval: Unifying turn and dialogue level evaluation. arXiv preprint arXiv:2106.01112.\\n\\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017. Learning discourse-level diversity for neural dialog models using conditional variational autoencoders. arXiv preprint arXiv:1703.10960.\\n\\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4 shows the statistical information for the four datasets generated using Dialogizer, namely WikiDialog2, PubmedDialog, CC-newsDialog, and ElsevierDialog.\\n\\n| Dataset          | Source dataset    | # of Dialogs | Average # of Turns |\\n|------------------|-------------------|--------------|--------------------|\\n| WikiDialog2      | Wikipedia         | 113,678      | 9.85               |\\n| PubmedDialog     | Pubmed-writing    | 22,811       | 9.24               |\\n| CC-newsDialog    | CC-news           | 69,846       | 10.49              |\\n| ElsevierDialog   | Elsevier OA CC-By | 32,053       | 11.37              |\\n\\nTable 4: Statistics of the four ConvQA datasets generated using the Dialogizer.\\n\\nB Reproductability checklists\\n\\nB.1 Dataset and Source code\\nWe provide our experiment source code along with configuration code as supplementary materials. We will publicly release the generated datasets and the full codes with weight parameters.\\n\\nB.2 Computing Resources\\nXeon 4210R (2.40 GHz) with RXT A6000 is used for the experiments. We use four GPUs for our experiments. All codes are implemented on Python 3.7.13 and PyTorch 1.10.1.\\n\\nC Dialogizer Training Details\\n\\nC.1 Training dataset\\nThe statistics of the four training datasets used for baseline dialog inpainter and Dialogizer training can be found in Table 5.\\n\\n| Dataset   | # of Dialogs | Average # of Turns |\\n|-----------|--------------|--------------------|\\n| Daily Dialog | 13,118      | 7.85               |\\n| Task Master | 54,255      | 19.34              |\\n| OR-QuAC   | 5,644        | 14.36              |\\n| QReCC     | 12,219       | 11.94              |\\n\\nTable 5: Statistics of the four training datasets.\\n\\nC.2 Training configuration\\nWe use T5-base model\u2020 as our Dialogizer model. The number of parameters of our model is about 220M. The model trains with batch size 8 with gradient accumulation step size 8 and takes about 10 hours per epoch. We use AdamW (Loshchilov and Hutter, 2017) optimizer with $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.999$, $\\\\epsilon = 1 \\\\times 10^{-8}$. The max gradient norm for gradient clipping is 1.0. In order to find the best-performing model, we conducted experiments on hyper-parameter combinations with 3 epoch steps: $\\\\lambda_{QAM}$: (0.05, 0.1, 0.5), $\\\\lambda_{TDG}$: (0.05, 0.1, 0.5), per _gpu_ _batch_size_ : (1, 2), initial _learning_rate_ : ($1 \\\\times 10^{-4}$, $5 \\\\times 10^{-5}$, $2 \\\\times 10^{-5}$), warmup _step_ : (0, 500). The hyper-parameter was manually tuned, and the best-performing model is with $\\\\lambda_{QAM}$ 0.1, $\\\\lambda_{TDG}$ 0.1, _per_gpu_batch_size_ 2, _initial_learning_rate_ $5 \\\\times 10^{-5}$, and _warmup_step_ 0. We repeatedly conducted all experiments for four seed numbers.\\n\\nD Application to Text Retrieval Details\\n\\nD.1 Experiment Details\\nFigure 6 provides a detailed explanation of the experimental setup for text retrieval. The baseline dialog inpainter and Dialogizer model are employed to perform inference on the Wiki corpora, resulting in the creation of the WikiDialog dataset used for training the retrieval model. For Dialogizer, two versions of the WikiDialog dataset are generated based on the re-ranking criterion. Subsequently, the text retrieval model is trained using the three generated datasets: WikiDialog _baseline_ \u2020, WikiDialog _Dialogizer_ \u2020, and WikiDialog _Dialogizer_.\\n\\nWe utilize the Sentence Transformer (ST) (Reimers and Gurevych, 2019) as the retrieval model and train it using the cosine similarity score as the ranking score through the MultipleNegativesRankingLoss. We also use AdamW optimizer and perform hyper-parameter tuning, as same in the Dialogizer training. The best-performing model is with _batch_size_ 32, _initial_learning_rate_ $1 \\\\times 10^{-4}$, _warmup_step_ 0, and _num_epochs_ 10. We repeatedly conduct all experiments for five seed numbers and report average values and standard deviation values.\\n\\nD.2 Metrics\\nWe use three metrics for the text retrieval experiment: NDCG, MAP, and Recall. First, NDCG (Normalized Discounted Cumulative Gain) is a widely used metric that measures the quality of a ranked list of documents, considering both relevance and ranking position. MAP (Mean Average Precision) measures the average precision score of the top-ranked documents. Recall measures the fraction of relevant documents that are retrieved.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Precision) focuses on precision at different ranks and calculates the average precision across all queries, providing insights into the ability of a system to retrieve relevant documents. Finally, Recall measures the system's ability to retrieve all relevant documents, indicating the completeness of the retrieval process.\\n\\nD.3 Benchmarks\\nIn addition to the Ms-Marco benchmark discussed in the main text, we conduct experiments on three additional benchmarks: Scifact (Wadden et al., 2020), Nfcorpus (Boteva et al., 2016), and Sci-docs (Cohan et al., 2020).\\n\\nD.4 Results\\nThe results for additional benchmarks in the text retrieval experiment can be found in Figure 7. Similar to MS-MARCO, across all benchmarks, ST Dialogizer demonstrates superior results in all metrics compared to ST baseline, indicating that our methodology generates more coherent questions with the passage. Additionally, the effectiveness of re-ranking is further enhanced in these cases.\\n\\nE Question Types Experiment Details\\nThe question type analysis experiment is conducted on the RoBERTa-base (Liu et al., 2019) model, and the training dataset is created by Cao and Wang (2021). The model is trained using the Adam optimizer (Kingma and Ba, 2014), and the loss function used is CrossEntropyLoss. The best-performing classifier model is with \\\\( \\\\text{per}_\\\\text{gpu} \\\\_ \\\\text{batch} \\\\_ \\\\text{size} 4 \\\\), learning rate \\\\( 1 \\\\_ e^{\\\\text{\u2212}5} \\\\), and num \\\\_ epochs 5.\\n\\nF Human Evaluation Details\\nThe recruitment process for the three crowd workers for the purpose of human evaluation was conducted via the university's online community, specifically targeting individuals who possessed fluency in the English language. The crowd workers were provided with task definitions, instructions, and examples, as illustrated in Figure 8 and 9. Furthermore, they were notified that this evaluation is intended for academic purposes. After conducting the sample evaluation and calculating the required time, the crowd workers were fairly compensated to ensure a minimum hourly wage of $12 or higher, as calculated by the coworkers.\\n\\nInter-Annotator Agreement (IAA)\\nWe measure the Inter-Annotator Agreement (IAA) among three crowd workers in the human evaluation process. We observe that Krippendorff's \\\\( \\\\alpha \\\\) and Cohen's kappa score indicate \\\"substantial\\\" or \\\"moderate\\\" agreement according to the referenced guideline (Landis and Koch, 1977). The Krippendorff's \\\\( \\\\alpha \\\\) and Cohen's kappa values for each of the three criteria are as follows:\\n\\n- **Contextual Relevance**\\n  - Krippendorff alpha: 0.617 (Substantial)\\n  - A1-A2 Cohen's kappa score: 0.661 (Substantial)\\n  - A1-A3 Cohen's kappa score: 0.613 (Substantial)\\n  - A2-A3 Cohen's kappa score: 0.537 (Moderate)\\n\\n- **Well-formed**\\n  - Krippendorff alpha: 0.654 (Substantial)\\n  - A1-A2 Cohen's kappa score: 0.599 (Moderate)\\n  - A1-A3 Cohen's kappa score: 0.632 (Substantial)\\n  - A2-A3 Cohen's kappa score: 0.522 (Moderate)\\n\\n(A1, A2, and A3 stands for Annotator1, Annotator2, and Annotator3)\\n\\nG GPT-4 Evaluation Details\\nThe prompt used for GPT-4 evaluation is devised by referencing Liu et al. (2023), and the input template can be found in Table 6.\\n\\nH Generated Dialog Examples\\nSample dialogs for the four datasets created using Dialogizer (WikiDialog2, PubmedDialog, CC-newsDialog, and ElsevierDialog) can be found in Table 7-14.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 6: The overview of the text retrieval experiment.\"}"}
{"id": "emnlp-2023-main-545", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: Average values for text retrieval benchmarks of Sentence Transformer (ST) (Reimers and Gurevych, 2019) models trained on dialog datasets generated by three frameworks: baseline dialog inpainter, Dialogizer without reranking (\u2020), and Dialogizer (Higher is better). Shading indicates a standard deviation across five seeds.\"}"}
