{"id": "acl-2023-long-854", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: We evaluate the ASR repair and interpretation components in isolation. We experiment with a fine-tuned T5 vs. a prompted GPT3 model.\\n\\nIn the setting that we are predicting programs instead of end states, the final 2 lines are replaced with\\n\\n\\\\[\\\\text{Lispress:}\\\\]\\n\\nD Results\\n\\nD.1 Segmentation\\n\\nWe run all the error analyses in this section on a model trained and tested exclusively on the Replicate doc task (where annotators were asked to replicate emails from the Enron Email Dataset).\\n\\nWe do not evaluate the segmentation model on all of the transcripts that arise during a trajectory, many of which are prefixes of one another. Doing so would pay too little attention to the later segments of the trajectory. (F1 measure on the final transcript will weight all of the segments equally, but F1 measure on the earlier transcripts does not consider the later segments at all.)\\n\\nInstead, we create an evaluation set of shorter transcripts. For each trajectory, we form its final full transcript by concatenating all of its final ASR results. Each sequence of up to 4 consecutive gold segments of this full transcript is concatenated to form a short transcript that the segmentation model should split back into its gold segments. For example, if the full transcript consists of 8 gold segments, then it will have 8 + 7 + 6 + 5 evaluation examples of 1 to 4 segments each.\\n\\nD.1.1 Error Analysis\\n\\nBelow, we list some examples of segmentation errors ([\u00b7] is used to specify segment boundaries, yellow highlighted segments correspond to command segments, while non-highlighted segments correspond to dictation segments).\\n\\n1. Input: Take out the word it. Before the word should. And then replace third with three.\\n   True Segmentation: \\\\[\\\\text{Take out the word it. Before the word should. And then replace third with three.}\\\\]\\n   Pred Segmentation: \\\\[\\\\text{Take out the word it. Before the word should. And then replace third with three.}\\\\]\\n\\n2. Input: You learned. You lie not you learned.\\n   True Segmentation: \\\\[\\\\text{You learned. You lie not you learned.}\\\\]\\n   Pred Segmentation: \\\\[\\\\text{You learned. You lie not you learned.}\\\\]\\n\\n3. Input: Skillings calendar is amazingly full! Let\u2019s shoot for one of the following. Skillings should be apostrophe s. Let\u2019s schedule it ASAP.\\n   True Segmentation: \\\\[\\\\text{Skillings calendar is amazingly full! Let\u2019s shoot for one of the following. Skillings should be apostrophe s. Let\u2019s schedule it ASAP.}\\\\]\\n   Pred Segmentation: \\\\[\\\\text{Skillings calendar is amazingly full! Let\u2019s shoot for one of the following. Skillings should be apostrophe s. Let\u2019s schedule it ASAP.}\\\\]\\n\\nThese examples illustrate two prototypical modes of errors: (i) the ASR system making erroneous judgments about sentence boundary locations, leading the segmentation model astray, and (ii) commands being phrased in ways that disguise them as dictations. The first example illustrate error type (i): the ASR system oversegments the input (which should've been a single sentence) into three separate sentences, consequently leading the segmentation system to believe \\\"Take out the word it\\\" and \\\"Before the word should...\\\" are separate commands. The second example illustrates error type (ii): \\\"You lie not you learned.\\\" is supposed to be a replace command indicating \\\"You learned.\\\" should be replaced with \\\"You lie\\\", but it is not phrased as an explicitly command. Finally, the third example illustrates both error types: we see that the ASR system undersegments the input and combines the sentence \\\"Skillings should be apostrophe s\\\" with the sentence \\\"Let\u2019s schedule it ASAP\\\" without a period. Combined with the fact that \\\"Skillings should be apostrophe s\\\" is not issued explicitly as a command, this confuses the segmentation model into thinking that it is in fact part of the dictation.\\n\\nD.1.2 Success Cases: Fixing Erroneous Segmentations\\n\\nThe above examples illustrated certain cases where the segmentation model was misled by erroneous ASR judgments about sentence boundary locations.\"}"}
{"id": "acl-2023-long-854", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In some cases, however, the segmentation model is able to fix these judgements, as shown below:\\n\\n1. Input: Take out the extra space. In between the two words, but and should.  \\n   True/pred Segmentation:\\n   \\\\[ \\\\text{Take out the extra space. In between the two words, but and should.} \\\\]\\n\\n2. Input: Replace the period. With a comma after restructuring.  \\n   True/pred Segmentation:\\n   \\\\[ \\\\text{Replace the period. With a comma after restructuring.} \\\\]\\n\\nD.2 ASR Repair\\n\\nTo measure the ASR repair step in isolation, we take noisy utterances corresponding to each command and measure to what extent we are able to reconstruct the ground-truth utterance. We measure the percent of utterances for which our predicted repaired utterance exactly matches the ground-truth utterance (EM).\\n\\nResults\\n\\nFrom Table 5, we see that the GPT3 model is much better at repairing speech disfluencies and ASR errors than the T5 model, achieving 70% EM. We suspect this is due to the fact that GPT3 was pretrained on much more (English) language data than T5, giving GPT3 a greater ability to produce grammatically coherent and permissible English sentences, and likely also a better sense of common disfluencies.\\n\\nQualitative Analysis\\n\\nRecall that we designed the ASR repair step to condition on not just the utterance but the state. This allows it to take the state into account when repairing.\\n\\nFor example, when given the following utterance:\\n\\nDelete the period after events.\\n\\nAn ASR repair model that looks at ASR alone may not see any issue with this utterance. However, given the document state:\\n\\nEric, I shall be glad to talk to you about it. The first three days of the next week would work for me. Vince. (note the word events does not appear anywhere in this text), the ASR repair model realizes that the actual utterance should've been, Delete the period after Vince.\\n\\nIndeed, the T5 ASR repair model is able to make the appropriate correction to this utterance.\\n\\nD.3 Interpretation\\n\\nMetrics\\n\\nTo measure the interpretation step in isolation, we take normalized utterances corresponding to each command and measure to how well the interpretation model is able to reconstruct the ground-truth final state for the command.\\n\\nWe use the same set of metrics described in \u00a76.2 (state EM, program EM). However, instead of feeding the interpretation model ASR repair results, we feed in ground-truth utterances. Results are reported in Table 5.\\n\\nWe can also compare these isolated interpretation results with the joint ASR and interpretation results reported in Table 2. Due to error propagation, the T5 model is \\\\( \\\\sim 5\u20138\\\\% \\\\) worse when asked to jointly perform ASR repair and interpretation from noisy ASR, than when simply asked to interpret normalized utterances. Surprisingly however, the GPT3 model performs nearly as well in the joint evaluation as the isolated evaluation. We suspect that even if the GPT3 ASR repair model does return the exactly normalized utterances, it is still able to reconstruct a semantically equivalent/similar command.\\n\\nE Infrastructure and Reproducibility\\n\\nWe trained 220M-parameter T5-base model on a single NVIDIA Tesla A100 GPU machine. Each training run for each component of the model took at most a few hours (\\\\( \\\\langle 8 \\\\)). We also prompted a 12B-parameter GPT3 model.\\n\\nWe used PyTorch (Paszke et al., 2019) and Huggingface Transformers (Wolf et al., 2020) for implementing and training T5-base models. We use OpenAI's API\\\\(^ \\\\text{27} \\\\) for querying GPT3. We use the text-davinci-003 model.\\n\\n\\\\(^ \\\\text{27}\\\\)https://beta.openai.com/\"}"}
{"id": "acl-2023-long-854", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nB Did you use or create scientific artifacts?\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\\nWe are unsure of the license.\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\nEnron was intended for research purposes, according to http://www.cs.cmu.edu/enron/\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\\nThe Enron email dataset creators took steps to anonymize. We did not take further steps.\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC Did you run computational experiments?\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-854", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nSection 5, 6, Appendix B\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nSection 6\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nAppendix D\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nSection 4\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nSection 4, Appendix A.2\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nDisclosing this information may risk anonymity\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nSection 4.1\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nThe dataset collection process posed no risks to annotators.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nSection 4.3\"}"}
{"id": "acl-2023-long-854", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Toward Interactive Dictation\\n\\nBelinda Li \u2660 \u2217 Jason Eisner 3 Adam Pauls 3 Sam Thomson 3\\n\u2660 MIT CSAIL 3 Microsoft Semantic Machines\\n\\nbzl@mit.edu 3 {jason.eisner, adam.pauls, samuel.thomson}@microsoft.com\\n\\nAbstract\\n\\nVoice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must incrementally segment and classify spans of speech as either dictation or command, and interpret the spans that are commands. We experiment with using large pre-trained language models to predict the edited text, or alternatively, to predict a small text-editing program. Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 28% single-command interpretation accuracy with 1.3 seconds of latency, while a larger model achieves 55% with 7 seconds of latency.\\n\\n1 Introduction\\n\\nSpeech can be preferable for text entry, especially on mobile devices or while the user\u2019s hands are occupied, and for some users for whom typing is always slow or impossible. While fast and accurate automatic speech recognition (ASR) is now ubiquitous (Kumar et al., 2012; Xiong et al., 2016; Chiu et al., 2018; Radford et al., 2022), ASR itself only transcribes speech. In practice, users may also wish to edit transcribed text. The ASR output might be incorrect; the user might have misspoken; or they might change their mind about what to say or how to phrase it, perhaps after seeing or hearing their previous version. Azenkot and Lee (2013) found that users with visual impairment spent 80% of time editing text vs. 20% dictating it.\\n\\nJust wanted to ask about the event on Friday the 23rd. Is the event still on?\\n\\nJust wanted to ask about the event on the 23rd, on Friday the 23rd. Is the event still on? Change \u201cthe event\u201d to \u201cit\u201d in the last sentence.\\n\\nJust wanted to check in about the event on Friday the 23rd. Is it still on?\\n\\nFigure 1: A user writes an email using speech input, interleaving dictation (a,c) and commanding (b,d). Top shows the continuous user utterance, while bottom shows the document state at each point of the utterance. Dictations are transcribed verbatim, while commands are interpreted and executed. Our system supports open-ended commanding (i.e., b,d both invoke a replace operation but use vastly different phrasing).\\n\\nIn this work, we study the task of interactive dictation, in which users can both perform verbatim dictation and utter open-ended commands in order to edit the existing text, in a single uninterrupted speech stream. See Figure 1 for an example. Unlike commercial systems like Dragon (DNS; Nuance, 1997, 2022) and dictation for Word (Microsoft, 2022) that require reserved trigger words for commanding, the commands in our data are invoked using unrestricted natural language (NL). For example, in Figure 1, both (b) and (d) invoke replace commands, but (d) uses nested syntax to specify both an edit action and location, while (b) is implicit (as natural speech repairs often are). In interactive dictation, users do not need to memorize a list of specific trigger words or templates in order to invoke their desired functionality. A dictation system should be as intuitive as dictation systems that exist today.\"}"}
{"id": "acl-2023-long-854", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tating to a human assistant\u2014a situation in which people quite naturally and successfully intersperse speech repairs and commands with their dictation. Beyond eliminating the learning curve, letting users speak naturally should also allow them to focus on what they want to say, without being repeatedly distracted by the frustrating separate task of getting those words into the computer.\\n\\nBecause we accept unrestricted NL for commands, both segmentation and interpretation become nontrivial for a system to perform. Segmentation requires capturing (sometimes subtle) changes in intent, and is especially difficult in cases where command boundaries do not align with ASR boundaries.\\n\\nWe collect a dataset of 1320 documents dictated in an interactive environment with live, incremental ASR transcription and Wizard-of-Oz-style interpretation of user commands. Annotators were not told a set of editing features they were allowed to use, but simply instructed to make their commands understandable and executable by a hypothetical human helper. Collection required designing a novel data collection interface. Both the interface and dataset will be publicly released to help unlock further work in this area.\\n\\nFinally, we experiment with two strategies for implementing the proposed system: one that uses a pre-trained language model to directly predict the edited text given unedited text and a command, and another that interprets the command as a program specifying how to edit. Predicting intermediate programs reduces latency because the programs are short, at the expense of accuracy. This strategy also requires additional work to design and implement a set of editing functions and annotate commands with programs that use these functions.\\n\\nFor each strategy, we also experimented with two choices of pre-trained language model: a small fine-tuned T5 model and a large prompted GPT3 model. Using the smaller model significantly improves latency, though again at the cost of accuracy. In summary, our contributions are: (1) a novel task (interactive dictation), (2) a novel data collection interface for the task, with which we collect a new dataset, and (3) a system that implements said task, with experiments and analysis.\\n\\n2 Background & Related Work\\n\\nMany modern speech input tools only support direct speech-to-text (e.g., Radford et al., 2022). Occasionally, these models also perform disfluency correction, which includes removing filler words (e.g., um), repeated words, false starts, etc. (e.g., Microsoft Azure, 2022). One form of disfluency that has received particular attention is speech repair, where the speaker corrects themself mid-utterance. For example, let's chat tomorrow uh I mean Friday contains a speech repair, where the user corrects \\\"tomorrow\\\" with \\\"Friday.\\\" The repaired version of this should be let's chat Friday.\\n\\nPrior work has collected datasets and built systems specifically for speech repair (Heeman and Allen, 1994, 1999; Johnson and Charniak, 2004). Additionally, ASR systems themselves make errors that humans may like to correct post-hoc; there has been work on correcting ASR errors through respeaking misdetected transcriptions (McNair and Waibel, 1994; Ghosh et al., 2020; Vertanen and Kristensson, 2009; Sperber et al., 2013).\\n\\nBeyond disfluencies that were not automatically repaired but were transcribed literally, humans must fix many other mistakes while dictating. They often change their mind about what to say\u2014the human writing process is rarely linear\u2014and ASR itself commonly introduces transcription errors. Most systems require the user to manually fix these errors through keyboard-and-mouse or touchscreen editing (e.g., Kumar et al., 2012), which can be inconvenient for someone who already relies on voice for dictation. Furthermore, most commercial systems that support editing through speech (DNS, Word) require templated commands. Thus, while speech input is often used to write short-form, imprecise text (e.g., search queries or text messages), it is not as popular as it might be, and it is used less when writing longer and more precise documents.\\n\\nIn our work, we study making edits through spoken natural language commands. Interpreting flexible natural language commands is a well-studied problem within NLP, with work in semantic parsing (Zelle and Mooney, 1993; Zettlemoyer and Collins, 2009; Artzi and Zettlemoyer, 2013), instruction-following (Chen and Mooney, 2011;\"}"}
{"id": "acl-2023-long-854", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Branavan et al., 2009; Tellex et al., 2011; Anderson et al., 2018; Misra et al., 2017), and task-oriented dialogue (Budzianowski et al., 2018). Virtual assistants like Siri (Apple, 2011), Alexa (Amazon, 2014), and Google Assistant (Google, 2016) have been built to support a wide range of functionalities, including interacting with smart devices, querying search engines, scheduling events, etc. Due to advances in language technologies, modern-day assistants can support flexible linguistic expressions for invoking commands, accept feedback and perform reinterpretation (Semantic Machines et al., 2020), and work in an online and incremental manner (Zhou et al., 2022). Our work falls in this realm but: (1) in a novel interactive dictation setting, (2) with unrestricted commanding, and (3) where predicting boundaries between dictations and commands is part of the task.\\n\\nRecently, a line of work has emerged examining how large language models (LLMs) can serve as collaborative writing/coding assistants. Because of their remarkable ability to generate coherent texts over a wide range of domains and topics, LLMs have proven surprisingly effective for editing, elaboration, infilling, etc., across a wide range of domains (Malmi et al., 2022; Bavarian et al., 2022; Donahue et al., 2020). Though our system also makes use of LLMs, it supports a different mode of editing than these prior works. Some works use edit models for other types of sequence-to-sequence tasks (e.g. summarization, text simplification, style transfer) (Malmi et al., 2019; Dong et al., 2019; Reid and Zhong, 2021), while others use much coarser-grained editing commands than we do, expecting the LLM to (sometimes) generate new text (Bavarian et al., 2022; Zhang et al., 2023). In addition to these differences, our editing commands may be misrecognized because they are spoken, and may be misdetected/missegmented because they are provided through the same channel as text entry.\\n\\n3 Task Framework\\n\\nWe now formalize our interactive dictation setting. A user who is editing a document speaks to a system that both transcribes user dictation and responds to user commands. This process results in an interactive dictation trajectory\u2014a sequence of timestamped events: the user keeps speaking, several trained modules keep making predictions, and the document keeps being updated.\\n\\nSupervision could be provided to the predictive modules in various ways, ranging from direct supervision to delayed indirect reward signals. In this paper, we collect supervision that can be used to bootstrap an initial system. We collect gold trajectories in which every prediction is correct\u2014except for ASR predictions, where we preserve the errors since part of our motivation is to allow the user to fix dictation errors.\\n\\n4 All predictions along the trajectory are provided in the dataset. Our dataset is not completely generic, since it assumes that certain predictive modules will exist and interact in particular ways, although it is agnostic to how they make their predictions. It is specifically intended to train a system that is a pipeline of the following modules (Figure 2):\\n\\n(a) ASR\\n\\nAs the user speaks, the ASR module proposes transcripts for spans of the audio stream. Due to ASR system latency, each ASR result normally arrives some time after the end of the span it describes. The ASR results are transcripts of successive disjoint spans of the audio, and we refer to their concatenation as the current transcript ($U$ in Figure 2(a)).\\n\\n(b) Segmentation\\n\\nWhen the current transcript changes, the system can update its segmentation. It does so by partitioning the current transcript $U$ into a sequence of segments $u_i$, labeling each as being either a dictation or a command.\\n\\n(c) Normalization\\n\\n(Optional) Each segment $u_i$ can be passed through a normalization module, which transforms it from a literal transcript into clean text that should be inserted or interpreted. This involves speech repair as well as text normalization to handle orthographic conventions such as acronyms, punctuation, and numerals.\\n\\nWhile the module (a) may already attempt some version of these transformations, an off-the-shelf ASR module does not have access to the document state or history. It may do an incomplete job and there may be no way to tune it on gold normalized results. This normalization module can be trained to finish the job. Including it also ensures that our gold trajectories include the intended normalized text of the commands.\\n\\n(d) Interpretation\\n\\nGiven a document state $d_{i-1}$ and a segment $u_i$, the interpretation module predicts the new document state $d_i$ that $u_i$ is meant to perform.\\n\\n4 In module (c) below, we predicted repairs for command segments, so the gold trajectory interprets accurate clean text for commands. But we did not predict repairs for dictation segments, so their errors persist even in the gold trajectories.\"}"}
{"id": "acl-2023-long-854", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attached are the espeak events. Capitalize the S&E speak. Please review.\\n\\n1. Segmentation Model\\n2. ASR Repair Step\\n3. Execution Engine (EE)\\n4. Interpretation Step\\n\\nFigure 2: Diagram of an interactive dictation system. First, the ASR system (a) transcribes speech, which the segmentation system (b) parses into separate dictation and command segments. Next, an optional normalization module (c) fixes any ASR or speech errors in the segment. Finally, the interpretation system (d) returns the result of each operation. On the right is the concrete instantiation of our system.\\n\\n6 When $u_i$ is a dictation segment, no prediction is needed: the state update simply inserts the current segment at the cursor. However, when $u_i$ is a command segment, predicting the state update that the user wanted requires a text understanding model. Note that commands can come in many forms. Commonly they are imperative commands, as in Figure 1d. But one can even treat speech repairs such as Figure 1b as commands, in a system that does not handle repairs at stage (a) or (c).\\n\\nRather than predict $d_i$ directly, an alternative design is to predict a program $p_i$ and apply it to $d_{i-1}$ to obtain $d_i$. In this case, the gold trajectory in our dataset includes a correct program $p_i$, which represents the intensional semantics of the command $u_i$ (and could be applied to different document states).\\n\\nThis prediction can also condition on earlier segments, which provide some context for interpreting $u_i$. It might also depend on document states other than $d_{i-1}$\u2014such as the state or states that were visible to the user while the user was actually uttering $u_i$, for example.\\n\\nThe cursor may have different start and end positions if a span of text is selected, but otherwise has width 0. For example, the document state $d_1$ in Figure 2 is $(\\\"Attached are the espeak events.\\\", (31, 31))$. Change Propagation\\n\\nThe ASR engine we use for module (a) sometimes revises its results. It may replace the most recent of the ASR results, adding new words that the user has spoken and/or improving the transcription of earlier words. The engine marks an ASR result as partial or final according to whether it will be replaced.\\n\\nTo make use of streaming partial and final ASR results, our pipeline supports change propagation. This requires the predictive modules to compute additional predictions. If a module is notified that its input has changed, it recomputes its output accordingly. For example, if module (a) changes the current transcript, then module (b) may change the segmentation. Then module (c) may recompute normalized versions of segments that have changed. Finally, module (d) may recompute the document state $d_i$ for all $i$ such that $d_{i-1}$ or $u_i$ has changed.\\n\\nThe visible document is always synced with the last document state. This sync can revert and replace the effects on the document of previous incorrectly handled dictations and commands, potentially even from much earlier segments. To avoid confusing the user with such changes, and to reduce computation, a module can freeze its older or more confident inputs so that they reject change notifications (Appendix B). Modules (b)\u2013(d) could also adopt the strategy of module (a)\u2014quickly return provisional results from a \u201cfirst-pass\u201d system with the freedom to revise them later. This could further...\"}"}
{"id": "acl-2023-long-854", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To our knowledge, no public dataset exists for the task of interactive dictation. As our task is distinct from prior work in a number of fundamental ways (\u00a72), we create a new dataset, TERTiUS.\\n\\nOur data collection involves two stages. First, a human demonstrator speaks to the system and provides the gold segmentations, as well as demonstrating the normalizations and document state updates for the command segments. Later, for each command segment, an annotator fills in a gold program that would yield its gold state update.\\n\\nFor a command segment, we update the document during demonstration using the demonstrated state updates\u2014that is, they do double duty as gold and actual state updates. Thus, we follow a gold trajectory, as if the demonstrator is using an oracle system that perfectly segments their speech into dictations (though these may have ASR errors) versus commands, and then perfectly interprets the commands. A future data collection effort could instead update the document using the imperfect system that we later built (\u00a75), in which case the demonstrator would have to react to cascading errors.\\n\\n### 4.1 Collecting Interactive Dictation\\n\\nWe build a novel data collection framework that allows us to collect speech streams and record gold and actual events.\\n\\nWe used an existing ASR system, Microsoft Speech Services (MSS; Microsoft Azure, 2022). We asked the demonstrator to play both the role of the user (issuing the speech stream), and also the roles of the segmentation, normalization, and interpretation parts of the system (Figures 2b\u2013d). Thus, we collect actual ASR results, while asking the demonstrator to demonstrate gold predictions for segmentation, normalization, and interpretation.\\n\\nThe demonstration interface is shown in Figure 3. Demonstrators were trained to use the interface, and told during training how their data would be used.\\n\\nA demonstrator is given a task of dictating an email into our envisioned system (shown in the yellow textbox). We collected data in three scenarios:\\n\\n1. **Replicate doc:** Exactly recreate an email from the Enron Email Dataset (Klimt and Yang, 2004).\\n2. **Elaborate doc:** Expand a terse description of an email into a full email. The exact wording of the full email is up to the demonstrator.\\n3. **Replicate segment:** Exactly recreate the post-state of a single command segment (randomly sampled from the already-collected Replicate doc and Elaborate doc data), starting from its pre-state\\\\(^{-1}\\\\). This does not have to be done with a single command.\\n\\nA demonstrator must then reach the target state (either exactly for Replicate doc or Replicate segment, or to their satisfaction for Elaborate doc), following these three steps:\\n\\n**Step 1 (ASR, segmentation)** The demonstrator starts speaking, which gets transcribed in real time by the built-in ASR system into ASR results. As they speak, they demonstrate what the segmentation system should do by holding down a key whenever they are speaking a command (as opposed to dictating). They can specify consecutive commands by quickly releasing and re-pressing the key. This gives us a list of time intervals when the key was held down. By matching these to the ASR timestamps, we identify the gold command segments in the ASR transcript. The remaining segments of the transcript are labeled as dictation.\\n\\n**Step 2 (normalization)** All labeled segments are displayed in the right column of the UI. After the demonstrator has finished speaking, they fill in the normalized text for each command segment. (The segment shows original and normalized text in the ASR and Gold ASR fields.)\\n\\n**Step 3 (interpretation)** Finally, for each command segment, the demonstrator manually carries out the gold state update. They do this by clicking on a command segment in the right column, which pulls up the associated document state in the left column. Initially, it is set to equal the pre-state, and the demonstrator edits it with their actual operation.\\n\\nPreprocessing details can be found in Appendix A.3. We do not allow two dictation segments to be adjacent\u2014that would be equivalent to one concatenated segment. More details on how the ASR results are combined/segmented can be found in Appendix A.1. For dictation segments, the system automatically computes the gold state update by inserting the segment at the selection. This segment is an actual ASR result and may contain errors.\"}"}
{"id": "acl-2023-long-854", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Data collection UI. Demonstrator speech is transcribed by a built-in ASR system. Demonstrators specify gold segmentations by pressing a key to initiate a command segment (editText) and releasing the key to initiate a dictation segment (insertText). The resulting transcribed segments appear in the ASR fields of the boxes in the right column. For a command segment, the demonstrator specifies the normalized version in the Gold ASR field, and demonstrates the command interpretation by editing the document post-state. Document states are shown in the left column: selecting a segment makes its post-state (and pre-state) appear there.\\n\\nMouse and keyboard until it reflects the desired post-state after applying command ui. For reference, the UI also displays the pre-state di\u22121 and a continuously updated visual diff \u2206(di\u22121,di).\\n\\nDemonstrators can move freely among these steps, editing normalizations or state updates at any time, or appending new segments by speaking.\\n\\nWe believe our framework is well-equipped to collect natural, flexible, and intuitive dictation and commanding data, for several reasons: (1) We do not restrict the capabilities of commands or the forms of their utterances, but instead ask demonstrators to command in ways they find most natural. (2) We simulate natural, uninterrupted switching between segments by making it easy for demonstrators to specify segment boundaries in real-time. (3) We collect a realistic distribution over speech errors and corrections by using an existing ASR system and asking demonstrators to replicate real emails. In future, the distribution could be made more realistic if we sometimes updated the document by using predicted normalizations and state updates rather than gold ones, as in the DAgger imitation learning method (Ross et al., 2011).\\n\\nThey are also allowed to back up and remove the final segments, typically in order to redo them.\\n\\n4.2 Annotating Programs for Commands\\n\\nAfter obtaining sequences of demonstrated dialogues using the above procedure, we extract each command segment and manually annotate it with a program pi that represents the intensional semantics of the command. This program should in theory output the correct di when given di\u22121 as input. Program annotation is done post-hoc with a different set of annotators from \u00a74.1.\\n\\nWe design a domain-specific Lisp-like language for text-manipulating programs, and an execution engine for it. We implement a library consisting of composable actions, constraints, and combinators.\\n\\nA program consists of actions applied to one or more text targets, which are specified by constraints. Combinators allow us to create complex constraints by composing them. For example, in Figure 2, Capitalize the S in eSpeak, has the program (capitalize (theText (and (like \\\"S\\\") (in (theText (like \\\"eSpeak\\\")))))) where capitalize is the action, (like \\\"S\\\") and (like \\\"eSpeak\\\") are constraints, and and is a combinator. More examples are in Appendix A.4.\"}"}
{"id": "acl-2023-long-854", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Dataset size statistics.\\n\\n| Task              | Dict. | Cmd. | Total |\\n|-------------------|-------|------|-------|\\n| Replicate doc     | 372   | 473  | 1453  |\\n| Elaborate doc     | 343   | 347  | 473   |\\n| Replicate segment | 605   | 139  | 1299  |\\n| Total             | 1320  | 959  | 3225  |\\n\\n4.3 Handling of partial ASR results\\n\\nThe current transcript sometimes ends in a partial ASR result and then is revised to end in another partial ASR result or a final ASR result. All versions of this transcript\u2014\\\"partial\\\" and \\\"final\\\"\u2014will be passed to the segmenter, thanks to change propagation. During demonstration, we record the gold labeled segmentations for all versions, based on the timing of the demonstrator's keypresses. However, only the segments of the \\\"final\\\" version are shown to the demonstrator for further annotation. A segment of a \\\"partial\\\" version can simply copy its gold normalized text from the segment of the \\\"final\\\" version that starts at the same time.\\n\\nThese gold data will allow us to train the normalization model to predict a normalized command based on partial ASR results, when the user has not yet finished speaking the command or the ASR engine has not yet finished recognizing it.\\n\\nIn the same way, a command segment of the \\\"partial\\\" version could also copy its gold document post-state and its gold program from the corresponding \\\"final\\\" segment. However, that would simply duplicate existing gold data for training the interpretation module, so we do not include gold versions of these predictions in our dataset.\\n\\n4.4 Dataset details & statistics\\n\\nIn the first stage (\u00a74.1), eleven human demonstrators demonstrated 1372 interactive dictation trajectories (see Table 1 for details). In the second stage (\u00a74.2), two human annotators annotated programs for 868 commands. The dataset was then split into training, validation, and test sets with 991 training trajectories (consisting of 3199 demonstrated segments), 173 validation trajectories (562 segments), and 156 test trajectories (423 segments).\\n\\nAll demonstrators and annotators were native English speakers. The dataset is currently only English, and the editor supports unformatted plain text. However, the annotation framework could handle other languages that have spoken and written forms, and could be extended to allow formatted text.\\n\\nA key goal of our system is flexibility. We quantify how well TERTiUS captures flexibility by measuring the diversity of natural language used to invoke each state change.\\n\\n15 We count the number of distinct first tokens (mainly verbs) used to invoke each action. These results are reported in Table 4 in the Appendix, alongside a comparison with DNS.\\n\\n16 We see that TERTiUS contains at least 22 ways to invoke a correction, while DNS supports only 1. In short, these results show that doing well on TERTiUS requires a much more flexible system that supports a wider array of functions and ways of invoking those functions than what existing systems provide.\\n\\n5 Modeling & Training\\n\\nThe overall system we build for interactive dictation follows our pipeline from Figure 2 and \u00a73:\\n\\n1. A segmentation model \\\\( M_{SEG} \\\\) takes the current transcript \\\\( U \\\\), and predicts a segmentation \\\\( u_1, \\\\ldots, u_n \\\\), simultaneously predicting whether each \\\\( u_i \\\\) corresponds to a dictation or command segment.\\n\\n2. Each dictation segment is directly spliced into the document at the current cursor position.\\n\\n3. For each command segment:\\n   - \\\\( M_{NOR} \\\\) predicts the normalized utterance \\\\( u'_i \\\\), repairing any ASR misdetections.\\n   - An interpretation model, \\\\( M_{INT(state)} \\\\) or \\\\( M_{INT(program)} \\\\), either: 1. directly predicts the end state of the command \\\\( d_i \\\\), or 2. predicts the command program \\\\( p_i \\\\), which is then executed to \\\\( d_i \\\\) by the server.\\n\\n17 The system we build can theoretically support more flexibility than what is captured in TERTiUS. However, for TERTiUS to be a useful testbed (and training set) for flexibility, we would like it to be itself diverse.\\n\\n18 We also measure the diversity of state changes captured by TERTiUS in Appendix A.5.\"}"}
{"id": "acl-2023-long-854", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We experiment with both types of interpretation model. Below we describe the specific models we use.\\n\\n5.1 Segmentation\\n\\nThe segmentation model partitions $U$ into segments $u_i$, each of which is labeled by $m_i$ as being either dictation or command:\\n\\n$$M_{SEG}(U) = [(u_0, m_0), \\\\ldots, (u_n, m_n)]$$\\n\\nConcretely, the segmentation model does this using BIOES tagging (Jurafsky and Martin, 2009, Chapter 5). Here each command is tagged with a sequence of the form \\\\text{BI} \\\\ast \\\\text{E} (\\\"beginning, inside, \\\\ldots, inside, end\\\") or with the length-1 sequence \\\\text{S} (\\\"singleton\\\"). Maximal sequences of tokens tagged with \\\\text{O} (\\\"outside\\\") then correspond to the dictation segments. Note that two dictation segments cannot be adjacent. We implement the segmentation model as a T5-base encoder (Raffel et al., 2022) followed by a two-layer MLP prediction module. More details on why each tag is necessary and how we trained this model can be found in Appendix C.1.\\n\\n5.2 Normalization and Interpretation\\n\\nFor each $u_i$ that is predicted as a command segment, we first predict the normalized utterance $u'_i$:\\n\\n$$M_{NOR}(d_{i-1}, u_i) = u'_i.$$  \\\\hspace{1cm} (2)\\n\\nWe then interpret $u'_i$ in context to predict either the document state $d_i$ or an update program $p_i$:\\n\\n$$M_{INT(state)}(d_{i-1}, u'_i) = d_i,$$  \\\\hspace{1cm} (3)\\n\\n$$M_{INT(program)}(d_{i-1}, u'_i) = p_i.$$  \\\\hspace{1cm} (3)\\n\\nWe then update the document state accordingly.\\n\\nWe experiment with two ways of implementing the two steps: we either fine-tune two separate T5-base models (Raffel et al., 2022) that run in a pipeline for each command, or we prompt GPT3 (Brown et al., 2020) to generate both the normalized utterance and the interpretation output in a single inference step. Training and prompting details can be found in Appendix C.2.\\n\\n6 Results\\n\\nWe evaluate the segmentation model in isolation, and the normalization and interpretation steps together. (Appendices D.2 and D.3 evaluate the normalization and interpretation steps in isolation.) For simplicity, we evaluate the models only on current transcripts $U$ that end in final ASR results (though at training time and in actual usage, they also process transcripts that end in partial ones).\\n\\n6.1 Segmentation\\n\\nMetrics\\n\\nExact match (EM) returns 0 or 1 according to whether the entire labeled segmentation of the final transcript $U$ is correct. We also evaluate macro-averaged labeled F1, which considers how many of the gold labeled segments appear in the model's output segmentation and vice versa. Two labeled segments are considered to be the same if they have the same start and end points in $U$ and the same label (dictation or command).\\n\\nResults\\n\\nSegmentation results on an evaluation dataset of transcripts $U$ (see Appendix D.1) are shown in the top section of Table 2. All results are from single runs of the model. The model performs decently on TERTiUS, and in some cases is even able to fix erroneous sentence boundaries detected by the base ASR system (Appendix D.1.2). However, these cases are also difficult for the model: a qualitative analysis of errors finds that, generally, errors arise either when the model is misled by erroneous over- and under-segmentation by the base ASR system, or when commands are phrased in ways similar to dictation. Examples are in Appendix D.1.1.\\n\\n6.2 Normalization & Interpretation\\n\\nMetrics\\n\\nWe evaluate normalization and interpretation in conjunction. Given a gold normalized command utterance $u_i$ and the document's gold pre-state $d_{i-1}$, we measure how well we can reconstruct its post-state $d_i$. We measure state exact match (EM) between the predicted and gold post-states. If the interpretation model predicts given in Appendix D.2.\\n\\n20Specifically, the text-davinci-003 model.\\n\\n21Although the normalized utterance is not used for the final state prediction, early experiments indicated that this auxiliary task helped the model with state prediction, possibly due to a chain-of-thought effect (Wei et al., 2022).\\n\\n22See Appendix D for details.\\n\\n23We disregard the cursor position in this evaluation.\"}"}
{"id": "acl-2023-long-854", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"|       | Metric | T5 | GPT3 |\\n|-------|--------|----|------|\\n|       | Segmentation | F1 | 90.9% | - |\\n|       |               | EM | 85.3% | - |\\n|       | Runtime (s/it) |    | 0.097 | - |\\n\\n**Table 2:** We evaluate segmentation (top) and the ASR repair and interpretation components jointly (bottom). We report accuracy metrics (F1, EM) as well as runtime (in seconds per example). Segmentation is relatively fast and performs decently. For ASR repair and interpretation, we experiment with a fine-tuned T5 vs. a prompted GPT3 model, each outputting either the end state (state) or a program to carry out the command (prog).\\n\\n**Figure 4:** Runtime vs. State EM. GPT3 models produce more accurate state updates than T5, but use an unreasonable amount of time. Directly predicting the updated documents is more often correct than predicting update programs, again at the cost of time.\\n\\nInterpretation Program EM 28.3% - 41.9% -\\n\\nRuntime (s/it) 1.28 3.46 5.32 6.92\\n\\nResults The bottom of Table 2 shows these results. All results are from single runs of the model.\\n\\nGPT3 generally outperforms T5, likely due to its larger-scale pretraining. When we evaluated ASR repair and interpretation separately in Appendices D.2 and D.3, we found that GPT3 was better than T5 at both ASR repair and interpretation. Furthermore, we find that both GPT3 and T5 are better at directly generating states (55.1% vs. 38.6% state EM and 29.5% vs. 28.3% state EM). However, the gap is larger for GPT3. We suspect that GPT3 has a better prior over well-formed English text and can more easily generate edited documents directly, without needing the abstraction of an intermediate program. T5-base, on the other hand, finds it easier to learn the distinctive (and more direct) relationship between u and the short program p.\\n\\nOther than downstream data distribution shift, we hypothesize that program accuracy is lower than state accuracy because the interpretation model is trained mostly on auto-generated program annotations, and because the execution engine is imperfect. We anticipate that program accuracy would improve with more gold program annotations and a better execution engine.\\n\\n**6.3 Efficiency** Table 2 reports runtimes for each component. This allows us to identify bottlenecks in the system and consider trade-offs between model performance and efficiency. We see that segmentation is generally quick and the ASR repair and interpretation steps are the main bottlenecks. The T5 model also runs much faster than the GPT3 model, 24 despite performing significantly worse, indicating a trade-off between speed and accuracy.\\n\\nFigure 4 shows that by generating programs instead of states, we achieve faster runtimes (as the programs are shorter), at the expense of accuracy.\\n\\n**7 Conclusion** Most current speech input systems do not support voice editing. Those that do usually only support a narrow set of commands specified through a fixed vocabulary. We introduce a new task for flexible invocation of commands through natural language, which may be interleaved with dictation. Solving this task requires both segmenting and interpreting commands. We introduce a novel data collection framework that allows us to collect a pilot dataset, TERTiUS, for this task. We explore trade-offs between model accuracy and efficiency. Future work can examine techniques to push out the Pareto frontier, such as model distillation to improve speed and training on larger datasets to improve accuracy.\\n\\nFuture work can also look at domains outside of (work) emails, integrate other types of text transformation commands (e.g., formatting), and may allow the system to respond to the user in ways beyond updating the document.\\n\\n24 Note that GPT3 is called via an external API, while T5 is run on a local GPU. GPT3 runtimes thus include an unknown communication overhead, which will not be present when run on local hardware.\"}"}
{"id": "acl-2023-long-854", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nTERTiUS is a pilot dataset. In particular, its test set can support segment-level metrics, but is not large enough to support reliable dialogue-level evaluation metrics. Due to resource constraints, we also do not report inter-annotator agreement measurements. While we made effort to make our interface low-friction, the demonstration setting still differs from the test-time scenario it is meant to emulate, and such a mismatch may also result in undesired data biases. Because our dialogues were collected before having a trained interpretation model, trajectories always follow gold interpretations. Because of this, the main sources of errors are ASR misdections or user speech errors. In particular, TERTiUS contains data on: 1. misdetections and speech errors in transcription, and how to fix them through commands, 2. misdetections and speech errors in edits, and what intent they correspond to. We leave to future work the task of addressing semantic errors and ambiguities which result from incorrect interpretation of user intent. Some of these limitations can be addressed by incorporating trained models into the demonstration interface, which will allow faster demonstration, and capture trajectories that include actual system (non-gold) interpretations.\\n\\nThough the trained system runs, we have not done user studies with it because it is not production-ready. The T5-base models are efficient enough, but the prompted GPT3 model is too slow for a responsive interactive experience. Neither model is accurate enough at interpretation. We welcome more research on this task!\\n\\nWhen a human dictates to another human, interleaved corrections and commands are often marked prosodically (by pitch melody, intensity, and timing). Our current system examines only the textual ASR output; we have given no account of how to incorporate prosody, a problem that we leave to future work. We also haven\u2019t considered how to make use of speech lattices or n-best lists, but they could be very useful if the user is correcting our mistranscription\u2014both to figure out what text the user is referring to, and to fix it.\\n\\nImpact Statement\\n\\nThis work makes progress toward increasing accessibility for those who cannot use typing inputs. The nature of the data makes it highly unlikely that artifacts produced by this work could be used (intentionally or unintentionally) to quickly generate factually incorrect, hateful, or otherwise malignant text.\\n\\nThe fact that all speakers in our dataset were native speakers of American English could contribute to exacerbating the already present disparity in usability for English vs. non-English speakers. Future work should look to expand the diversity of languages, dialects, and accents covered.\\n\\nReferences\\n\\nAmazon. 2014. Amazon Alexa.\\n\\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. 2018. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3674\u20133683.\\n\\nApple. 2011. Siri.\\n\\nYoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1:49\u201362.\\n\\nShiri Azenkot and Nicole B. Lee. 2013. Exploring the use of speech input by blind people on mobile devices. Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility.\\n\\nMohammad Bavarian, Angela Jiang, Heewoo Jun, and Henrique Pond\u00e9. 2022. New gpt-3 capabilities: Edit & insert. [Online; posted 15-March-2022].\\n\\nS.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 82\u201390, Suntec, Singapore. Association for Computational Linguistics.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\"}"}
{"id": "acl-2023-long-854", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-854", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-854", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Dataset\\nA.1 ASR results\\n\\nTypes of segments\\nBelow we describe the types of ASR results we collect in TERTiUS. As dialogues are uttered, we obtain a stream of times-tamped partial and full ASR results from MSS. Examples of partial and full ASR results can be found below:\\n\\n0:00.00: attached\\n0:00.30: attached is\\n0:00.60: attached is the\\n0:01.05: attached is the draft\\n0:02.15: Attached is the draft.\\n\\nThe first four lines are partial ASR results that are computed quickly and returned by MSS in real time as the user is speaking. The last line is the final ASR result, which takes slightly longer to compute, but represents a more reliable and polished ASR result. After a final result has been computed, it obsolesces prior partial ASR results.\\n\\nWhile not used in present experiments, collecting partial ASR results enables building an incremental system that can be faster and more responsive in real time; rather than waiting for ends of sentences to execute commands, a system can rely on partial ASRs to anticipate commands ahead of time (akin to Zhou et al. (2022)). Collecting timing information is also helpful for evaluating the speed of our system: the system runtime contingues on the rate at which it obtains new ASR results and how long it takes to process them.\\n\\nFurthermore, MSS additionally returns n-best lists for each final ASR results. These are a list of candidate final ASRs that may feasibly correspond with user audio, e.g.,\\n\\nAttached is the draft.\\nAttached his draft.\\nAttacked is the draft.\\n\u00b7\u00b7\u00b7\\n\\nAggregation segments\\nFor long user audio streams, partial and final results are returned sequentially, each describing roughly a single sentence. The most recent ASR result is concatenated together with the previous history of final ASR results, to return the full partial or final ASR result for the entire stream. For example, after the user utters the first sentence in the example above, the user may continue by saying:\\n\\nplease re\\nplease review\\nplease review win\\nplease review when pause\\nPlease review when possible.\\n\\nWe concatenate each of these new ASR results with the previous final ASR results to obtain the current transcript, which evolves over time as follows:\\n\\nAttached is the draft. please\\nAttached is the draft. please re\\nAttached is the draft. please review\\nAttached is the draft. please review win\\nAttached is the draft. please review when\\npause\\nAttached is the draft. please review when\\npossible\\nAttached is the draft. Please review when\\npossible.\\n\\nSegmenting ASR results into Segments During Annotation\\nDuring annotation (\u00a74.1), all these partial and final ASR results get mapped to segments, forming and . This is done by identifying the timestamp of each token within each partial and final result. For example, in the example ASR results sequence at the beginning of this section A.1, suppose the user specifies an segment boundary at time 0:00.45, (separating \\\"Attached is\\\" from \\\"the draft.\\\"). We get the following ASR results for the first segment:\\n\\nattached\\nattached is\\nAttached is\\n\\n(we refer to the first two as partial ASRs for the segment, as they are derived from partial ASR, and the third as the final ASR for the segment), and the following ASR results for the second segment:\\n\\nthe\\ndraft\\ndraft.\\n\\nA.2 Annotation Instructions (\u00a74.1)\\nThe full text of written instructions given to annotators during the first round of annotation (\u00a74.1) is provided below:\"}"}
{"id": "acl-2023-long-854", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Transcribing\\nYour goal is to replicate the prompt in the target box verbatim / expand the prompt in the yellow textbox into a coherent email, starting from the given (potentially non-empty) starting document in the 'Transcription output' box. You are expected to do so using a series of speech-to-text transcriptions and commands. Try to use the starting document as much as possible (i.e. do not delete the entire document and start over).\\n\\nYou can easily see what changes are to be made by toggling the 'See Diff View' button. Once that mode is on, the text you need to add will be highlighted in green, while the text you need to delete will by highlighted in red. Once there is no colored text, your text box matches the target text box and you are done.\\n\\nBegin this process by hitting the 'Begin transcription' button. This will cause a new 'insertText' command to appear in the command log on the right.\\n\\nYou are now in transcription mode. Whatever you say will appear in the 'Transcription output' box.\\n\\n2. Editing\\nYou can fix mistakes in transcription, add formatting, etc. through adding 'editText' commands.\\n\\nHold down 'ctrl' on your keyboard to issue a new 'editText' command.\\n\\nWhile holding down 'ctrl' you will be in edit mode. In this mode, you can manually use mouse-and-keyboard to change the output. However, you must describe the edit you are making before you make it.\\n\\nBegin by describing your edit using your voice. Whatever you say now will appear in the editText ASR box, but not in the 'Transcription output' box.\\n\\nBecause the ASR system is imperfect, the textual description may be faulty. Fix any mistakes in the detected speech in the 'Gold ASR' box.\\n\\nFinally, manually edit the 'Transcription output' box to correspond the effect of your edit command.\\n\\nNote: It is important that you vocalize your change before making any edits to either 'Gold ASR' or 'Transcription output', as the ASR system stops recording as soon as you click into either one of these boxes.\\n\\n3. Undoing, Reseting, Submitting, & Saving\\nYou can click on previous commands in the command log to revisit them. Note that if you edit the output associated with a 'editText' prior in the history, you will erase the changes associated with subsequent 'editText' operations.\\n\\nIf you would like to undo some portion of command log, you can use the 'Delete Selected Command & Afterwards' button. Simply click on the first command you would like to remove, then click the button to remove that command and all commands after it.\\n\\nYou can clear the entire command log by hitting \\\"Reset\\\".\\n\\nIf you would like to work on transcribing another target, use the green arrow keys below the target. This will present you with a new target while saving progress on your current target. To delete a target prompt, press the red 'X'.\\n\\nOnce you are done editing, click \\\"Submit\\\" button.\\n\\nPlease double-check each command before submission! In particular, commands will appear red if they are potentially problematic (e.g. they are not associated with any change to the underlying text). Please check to make sure there are no red commands that you do not intend to be there!\\n\\nA.3 Target Text Preprocessing\\nFor replicating Enron emails, we process emails from the Enron Email Dataset to create our target final states. We break the email threads into individual emails, filtering out email headers and non-well-formed emails (emails that are either less than 50 characters or more than 5000 characters long, or contain too many difficult-to-specify non-English symbols). Annotators also had the option to skip annotating certain emails, if they found the email too difficult to annotate.\\n\\nA.4 Annotation Programs\\nExamples of programs can be found below:\"}"}
{"id": "acl-2023-long-854", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.5 Dataset Analysis\\n\\nTo assess the diversity of state changes, we quantify the number of distinct actions, constraints, and constraint combinators (see \u00a74.2) that appear in the annotated programs. In Table 3, we list out all actions, constraints, and constraint combinators present in TERTiUS. TERTiUS contains at least 15 types of actions (and allows for action composition with sequential chaining operation do), with 34 types of constraint and constraint combinators.\\n\\nIn Table 4, we approximate the invocation diversity represented in TERTiUS, by measuring the number of distinct first tokens used to invoke each type of actions. For actions that overlap in function, we also report a similar diversity metric against the full set of trigger words supported by DNS.\\n\\nTable 4: Number of ways to invoke various commands, in terms of number of distinct first tokens used to invoke that command. Second column shows the number of distinct first invocation tokens as present in TERTiUS, while third column shows the number of distinct first invocation tokens for comparable commands supported by DNS.\\n\\n*Counting undo, backspace, and scratch that as delete commands, despite being less general than our delete functionality (can only delete most recent tokens).\\n\\n25 https://www.nuance.com/asset/en_us/collateral/dragon/command-cheat-sheet/ct-dragon-naturally-speaking-en-us.pdf\\n\\nB Running Online\\n\\nWhen running the system online in real time, we must consider efficiency and usability. We introduce a \u201ccommit point\u201d that signifies that the system cannot re-segment, re-normalize, or re-interpret anything before that point. We only want to consider recent ASR results because the system quickly becomes inefficient as the dialogue length grows (the interpretation step, which is the bottleneck of the system, must run for every single command.) Furthermore, users often refer to and correct only recent dictations and commands; reverting early changes can have potentially large and undesirable downstream effects, leaving users potentially highly confused and frustrated.\\n\\nConcretely, the commit point is implemented as the system treating the document state at that point as the new \u201cinitial state,\u201d so that it is unable to access segments and the history of document states from before that point. We implement this...\"}"}
{"id": "acl-2023-long-854", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"point so that it must coincide with the end of a final ASR result. We feed into the system this state as the initial state, and the entire sequence of ASR results starting from that point. All dictations and command segments returned by the model are executed in sequence from the commit point.\\n\\nWe decide to set a commit point based on system confidence and time since last commit. System confidence is derived from the confidences of each component model at each step of the prediction. We measure the system confidence of the end state predicted by the system, by summing the log-probabilities of: 1. the segmentation model result, (summing the log-probabilities of each BIOES tag predicted for each token), 2. the ASR repair model result for each command (log-probability of the resulting sentence), 3. the interpretation model result for each command (the log-probability of the end state or program). Once the system confidence exceeds a threshold \\\\( \\\\tau \\\\), we decide to commit immediately at that point. Otherwise, if we have obtained more than 4 final ASR results since the last commit, we must commit at our most confident point from within the last 4 turns.\\n\\nC Model Training Details\\n\\nIn this section, we describe how we trained each component of the system. See \u00a75 for a description of the inputs, outputs, and architecture of each model. Our final system is incremental, able to process both partial and final ASR results.\\n\\nC.1 Segmentation Model\\n\\nWe use BIOES for the segmentation model. Note that we cannot just predict a binary command/dictation tag for each token, because it would be unable to discern two consecutive commands from one continuous command. Thus, we need to use \\\\( B \\\\) to specify the beginning of a new command segment. \\\\( E \\\\) is also necessary for the model to predict whether the final segment, in particular, is an incomplete and ongoing (requiring the ASR repair model to predict the future completion) or complete (requiring the ASR repair model to only correct errors).\\n\\nWe expect in the final online version of the end-to-end system, the segmentation model will: 1. run often, being able to accept and segment both partial and final ASR results, 2. run on only the most recent ASR, to avoid completely resegmenting an entire document that's been transcribed. Thus, we construct the training data for this model in a way to simulate these conditions. We extract all sequences of turns of length between 1 \u2013 4 from TERRIUS (capping to at most 4 for condition 2), take their segments \\\\( U \\\\), and concatenate them to simulate \\\\( U \\\\), asking the model to segment them back into their individual \\\\( U \\\\). For the final turn of each chosen sequence, we include in the training data both the final ASR result and all partial ASR results. We fine-tune on this data with a learning rate of 1e-4 and batch size of 4 until convergence.\\n\\nC.2 ASR Repair & Interpretation Models\\n\\nBelow we describe the concrete implementations and training details of each model:\\n\\nT5\\n\\nIn the T5 implementation, both \\\\( M_{\\\\text{NOR}} \\\\) and \\\\( M_{\\\\text{INT}} \\\\) are T5-base encoder-decoder models. As described in \u00a74.4, we do not have annotations of programs for the full training split. Thus, we automatically generate the missing programs using GPT3.\\n\\nWe have an initial training reservoir that consists solely of data points with program annotations \\\\( D_{\\\\text{annot}} \\\\). For each example in the remaining training set, we retrieve a subset of samples from \\\\( D_{\\\\text{annot}} \\\\) to form the prompt. We also use GPT3 for this retrieval step.\\n\\nWe then annotate programs in the remaining training set in an iterative manner: as new programs are annotated, we use the execution engine to check whether it executes to the correct end state, and if so, we add it to \\\\( D_{\\\\text{annot}} \\\\), such that future examples can include these programs in their prompt.\\n\\nGPT3\\n\\nIn the GPT3 implementation, both the ASR repair and interpretation steps occur in a single inference step, with GPT3 being prompted to predict both outputs in sequence. Specifically, it is prompted with:\\n\\n\\\\[\\n\\\\text{[Input State:]} \\\\ d_{i-1} \\\\ \\\\text{[Utterance ASR:]} \\\\ u_i' \\\\ \\\\text{[Gold Utterance:]} \\\\ u_i \\\\ \\\\text{[Final State:]} \\\\ d_i\\n\\\\]\\n\\nThe model is shown demonstrations in this format from the training data, then asked to infer, for each test sample, the highlighted portions from the non-highlighted portions.\\n\\n\\\\( \\\\text{similarity} \\\\)\\\\( \\\\text{computed similarity between two prompts by looking at the similarity over next-token distributions when conditioned on each of the prompts.} \\\\)\"}"}
