{"id": "emnlp-2024-main-1211", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nIn spite of the recent progress in speech processing, the majority of world languages and dialects remain uncovered. This situation only furthers an already wide technological divide, thereby hindering technological and socio-economic inclusion. This challenge is largely due to the absence of datasets that can empower diverse speech systems. In this paper, we seek to mitigate this obstacle for a number of Arabic dialects by presenting Casablanca, a large-scale community-driven effort to collect and transcribe a multidialectal Arabic dataset. The dataset covers eight dialects: Algerian, Egyptian, Emirati, Jordanian, Mauritanian, Moroccan, Palestinian, and Yemeni, and includes annotations for transcription, gender, dialect, and code-switching. We also develop a number of strong baselines exploiting Casablanca. The project page for Casablanca is accessible at: https://www.dlnlp.ai/speech/casablanca.\\n\\n1 Introduction\\n\\nSelf-supervised learning (SSL) has significantly advanced the field of speech processing, impacting everything from speech recognition to speech synthesis and speaker verification. However, the success of these methods heavily relies on the availability of large datasets, which are primarily available for a select few languages. This bias towards resource-rich languages leaves behind the majority of the world\u2019s languages (Bartelds et al., 2023; Talafha et al., 2023; Meelen et al., 2024; Tonja et al., 2024). In this work, we report our efforts to alleviate this challenge for Arabic\u2014a collection of languages and dialects spoken by more than 450 million people. We detail a year-long community effort to collect and annotate a novel dataset for eight Arabic dialects spanning both Africa and Asia. This new dataset, dubbed Casablanca, is rich with various layers of annotation. In addition to speech transcriptions, we include speaker gender, dialect, and code-switching information. Notably, to the best of our knowledge, some of the dialects included in Casablanca have not been featured in any prior speech or broader NLP research. In addition to describing our dataset, we develop baseline systems for automatic speech recognition (ASR). To summarize, our contributions are as follows:\\n\\n1. We introduce Casablanca, the largest fully supervised speech dataset for Arabic dialects, labeled with transcriptions, code-switching, dialect, and gender.\\n2. We evaluate SoTA multilingual ASR models and four Arabic-centered Whisper models across the eight dialects in Casablanca to assess their adaptability and performance, particularly in handling the linguistic nuances of Arabic dialectal variation.\\n3. We assess the performance of the best-performing model in code-switching scenarios, analyzing the segments using both the original Latin characters and their transliterated counterparts.\\n\\n2 Related Work\\n\\nArabic.\\n\\nArabic encompasses a diverse array of linguistic varieties, many of which are nearly mutually unintelligible (Watson, 2007; Abdul-Mageed et al., 2024). This diversity includes three primary categories: Classical Arabic, historically used in literature and still employed in religious contexts; Modern Standard Arabic (MSA), used in media, education, and governmental settings; and numerous colloquial dialects, which are the main forms of daily communication across the Arab world and often involve code-switching (Abdul-Mageed et al., 2020; Mubarak et al., 2021). The significant differences between these varieties pose challenges in adapting technologies from one variety to another.\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Early efforts to develop Egyptian Arabic speech datasets began in 1996 with the CallHome task (Pallett, 2003) under the National Institute of Standards and Technology's (NIST) evaluations, focusing on the Egyptian and Levantine dialects. In 2006, the DARPA-led Global Autonomous Language Exploitation (GALE) (Soltau et al., 2009) and the Spoken-Language Communication and Translation System for Tactical Use (TRANSTAC) programs (Weiss et al., 2008) aimed to develop Iraqi dialect dataset, driven by U.S. military needs (Olive et al., 2011). The Multi-Genre Broadcast (MGB) Challenge has later introduced several datasets aimed at advancing speech recognition, speaker diarization, alignment, and dialect identification using content from TV and YouTube. MGB-2 (Ali et al., 2016) provides 1,200 hours of speech with lightly supervised transcriptions, derived from Aljazeera Arabic news broadcasts with MSA making up 78% of the total content. MGB-3 (Ali et al., 2017) compiles video clips from Egyptian YouTube channels while MGB-5 (Ali et al., 2019) focuses on Moroccan Arabic ASR. Additionally, the QASR project (Mubarak et al., 2021), sourced from Aljazeera's archives between 2004 and 2015, features over 4,000 episodes across various topics, including extensive code-switched transcriptions from multiple dialects. Further details of the MGB and QASR datasets are provided in Table 1.\\n\\nNon-Arabic ASR data. Similar efforts exist for collecting diverse speech datasets across various language varieties and dialects. For instance, STT4SG-350 (Pl\u00fcss et al., 2023) introduces a Swiss German corpus divided into seven dialect regions, annotated with Standard German transcriptions. AfriSpeech (Olatunji et al., 2023) also offers 200 hours of Pan-African English speech, featuring 67,577 audio clips from speakers across 13 countries, encompassing 120 indigenous accents for both clinical and general ASR applications. The ManDi Corpus (Zhao and Chodroff, 2022) provides a detailed spoken database of regional Mandarin dialects and Standard Mandarin, with 357 recordings totaling about 9.6 hours from 36 speakers across six major regions.\\n\\nAdditional information on Arabic ASR can be found in Appendix A.1. Casablanca is the largest fully supervised Arabic dialects dataset with 48 hours of human-transcribed data, surpassing MGB-3 and MGB-5. Although MGB-2 and QASR are larger in size, they utilize light supervision (using ASR systems for transcribing and aligning human transcripts) rather than manual transcriptions. This light supervision method accounts for potential inaccuracies in human transcripts, such as omissions, errors, and variations from factors like corrections, spelling errors, foreign language use, and overlapping speech, leading to possible mismatches between the transcriptions and actual spoken content (Mubarak et al., 2021). Casablanca is also the most fine-grained and diverse corpus available: while datasets such as MGB-2 and QASR focus on broad regional dialects like the Gulf, the Levant, and North Africa (including Egypt), Casablanca targets country-level variation focusing on eight countries belonging to different areas in the Arab world. To the best of our knowledge, our dataset is also the first to introduce zero-resourced dialects in addition to the low-resource ones (specifically the Emirati, Yemeni, and Mauritanian dialects), thus filling a significant need in the research landscape. Furthermore, Casablanca is rich with several layers of annotation: beyond speech transcription, each segment is also labeled with speaker gender and country, which provide valuable demographic information and can be exploited for downstream tasks involving gender and dialect identification. Table 1 provides a comparison between Casablanca and a number of notable Arabic datasets. Finally, with Casablanca, we are advancing the benchmarking efforts to encompass eight dialects and include evaluations on four multilingual models: Whisper (Radford et al., 2023) (both versions 2 and 3), SeamlessM4T (Barrault et al., 2023), and MMS (Pratap et al., 2023) under zero-shot and Arabic-enhanced settings. This expansion strengthens our analysis by incorporating advanced models, offering a comprehensive evaluation of their capacity to handle diverse dialects.\\n\\n3 Corpus Collection\\n3.1 Data Selection\\nWe assembled a team of 15 native speakers (each with a research background) and assigned them the task of manually curating a list of YouTube videos.\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Casablanca in comparison to notable Arabic speech datasets.\\n\\n| Dialect Label | \u2713 | \u2713 | \u2713 |\\n|---------------|---|---|---|\\n| GLF, LEV, NOR, EGY (MSA: majority) | \u2713 | \u2713 | \u2713 |\\n| ALG, EGY, JOR, MOR, UAE, PAL, MAU, YEM | \u2713 | \u2713 | \u2713 |\\n\\nSegmentation:\\n- Lightly: lightly supervised (labeling is performed using a pre-trained model).\\n- Fully: fully supervised (all annotations are carried out manually by humans).\\n- Test: fully: only the test set is labeled manually.\\n- \u2717: does not support.\\n- N/A: not applicable as those datasets have one dialect only.\\n\\nTranscription:\\n- Lightly: fully.\\n- Fully: fully.\\n\\nCode-switching:\\n- \u2717: does not support.\\n\\nGender:\\n- \u2713: 82% data.\\n- \u2713: 100% data.\\n\\nNotes:\\n- Figure 1: Geographic distribution of participants and data in Casablanca.\\n\\n3. Data Segmentation\\n\\nWe segment the episodes into shorter utterances, thereby simplifying transcription and enabling task distribution among annotators for a more streamlined process. We use the voice activity detection model (VAD) of Bredin and Laurent (2021); Bredin et al. (2020), available through the pyannotate library, to detect speech and remove non-speech segments such as music. We then use AudioSegment to extract the identified speech segments. We refer to these extracted audio segments as 'snippets'. It is important to note that an output snippet may contain multiple utterances, often involving various speakers. We put the snippets on the LabelStudio platform (Tkachenko et al., 2020) for annotation. See more details about annotation in Appendix A.2.\\n\\n3.2 Code-switching\\n\\nWe utilize the model with its default hyperparameters (onset: 0.8104, offset: 0.4806, min_duration_on: 0.055, min_duration_off: 0.097).\\n\\n4. Data\\n\\nThe project page for Casablanca is accessible at: https://www.dlnlp.ai/speech/casablanca.\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Data Annotation\\n\\n4.1 Annotators\\n\\nOur community-driven dataset, Casablanca, is created with the help of 27 annotators from the Arab world, each annotating their respective dialects. All annotators either have or are pursuing graduate degrees in natural language processing, making them well-positioned for the task. We involve at least two annotators per dialect, each coming from a different region within the respective country for an enhanced knowledge of sub-dialects, which adds a layer of linguistic richness and diversity to the orthographic representation of each dialect. Table 8 (Appendix A.4) illustrates lexical variation within the eight dialects in Casablanca, showcasing its linguistic diversity.\\n\\n4.2 Tasks\\n\\nWe provided annotators with written guidelines explaining the annotation tasks. During weekly meetings with team members, we discussed, improved, and iteratively extended these guidelines. Annotators are also able to communicate with one another and ask questions through a Slack channel dedicated to the project. The main annotation tasks are:\\n\\nTask 1: Segment Selection\\n\\nWe introduced three annotation options as shown in Figure 3: Dialect for dialect-specific content, MSA for Modern Standard Arabic, and Other for segments containing non-verbal sounds. Selected segments, whether dialectal or MSA, are required to be \u201cclear segments.\u201d They must feature only one speaker to avoid voice overlap, be audibly clear and transcribable despite potential background noise, and contain a minimum of three words without surpassing 30 seconds in length. Moreover, each segment must capture the complete utterance, from beginning to end, accurately representing every phoneme component of the first and last words to preserve speech boundaries.\\n\\nTask 2: Transcription\\n\\nGiven the absence of a standardized orthographic system for Arabic dialects, we asked annotators to transcribe in the manner they usually write in their daily lives. Furthermore, for a faithful representation of the speech signal, we encouraged the incorporation of Tanweens and Hamzat in the transcriptions. We also asked annotators to render numbers in alphabetical format (e.g., /char09/chare1/char4b/char0a/char51/chare5/char11/char84/charab/) instead of numerical symbols (e.g., /char10/chare9/char10/charaf/char41/chara2/char1d/char2e/char32/char30/char09/char50/charf0/char41/charab/), since this allows for reflecting inflections these numbers can have (e.g., /char09/chare1/char4b/char0a/char51/chare5/char11/char84/charab/ vs. /char09/chare0/charf0/char51/chare5/char11/char84/charab/). For code-switching (CS), we asked annotators to provide two versions of the transcript, one with the foreign words in Arabic script (e.g., /charc8/char41/char09/char4a/char1c/char0a/char11/char82/char1c/char0a/char09/charaf/charf0/char51/char4b/char2e/) and another in Latin script (e.g., \u201cprofessional\u201d); see Table 9 in Appendix A.5.\\n\\nTask 3: Gender\\n\\nAnnotators label speaker gender based on perceived biological sex from the set {male, female}. This makes our dataset suited for studying gender-specific speech patterns across dialects.\\n\\nTask 4: Validation\\n\\nIn this task, each team engages in a peer validation process, with annotators reviewing and ensuring the accuracy of one another\u2019s transcriptions, focusing on correcting spelling errors while preserving dialectal orthographic variations.\\n\\nOur annotation process utilized an agile methodology (Cohen et al., 2004) with work divided into weekly sprints, allowing for focused objectives and regular review sessions to refine strategies. We also gave annotators a guideline document and a document on special cases to standardize dialect scenarios and document linguistic variations. See Appendix A.6 for examples. Overall, the annotation project ran for a total duration of six months.\\n\\n5 Dialects Description\\n\\nCasablanca is a detailed collection of around 48 hours of data covering eight Arabic dialects from regions like the Levant, Gulf, Yemen, and North Africa, including Algerian, Egyptian, Emirati, Jordanian, Mauritanian (Hassaniya), Moroccan, Palestinian, and Yemeni. Casablanca involves sub-dialects from these countries as well. In addition, to the best of our knowledge, we are among the first to offer annotated data for the less-represented Emirati, Mauritanian, and Yemeni dialects, addressing a gap in linguistic research.\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Corpus statistics\\n\\nEpisode Coverage. As spelled out earlier, we annotate approximately 48 hours of content across eight dialects. The average annotation duration per episode is about four minutes, constituting roughly 14.71% of the average episode length. Dialects represented by a larger number of episodes typically exhibit lower per-episode annotation durations. This distribution allows annotators to engage with a more diverse range of content. For instance, Mauritanian episodes, totaling 247, feature an average of only one minute and 25 seconds (8.23% of the total episode length). Conversely, the Palestinian subset, with 22 episodes, averages 16 minutes and 30 seconds per episode, which is about 53.72% of the total episode length.\\n\\nAverage Duration. As detailed in Table 2, the average duration of segments across all dialects stands at 4.24 seconds, with the Moroccan having the shortest average duration and the Palestinian the longest. We define the speed rate as the average number of words per second (WPS) and the average number of characters per second (CPS). Interestingly, based on our analysis of the episodes, the Moroccan dialect stands out as the fastest spoken dialect in Casablanca, both in terms of WPS and CPS with 3.2 WPS and 15.7 CPS, respectively. Conversely, Jordanian dialect is the slowest in our dataset, yielding 1.2 WPS and 6.14 CPS.\\n\\nThe average transcript length across all dialects is 8.64 words, with Jordanian transcripts being the shortest and Palestinian the longest. These differences, even between closely related dialects, stem from episode script lengths and annotator preferences for word separation, including prefixes and suffixes. For instance, in the Jordanian dialect, the phrase (\\\"I sent it to her\\\") transcribed by some annotators as a single word (\\\"/char41/chareb/char41/char4b/char0a/char40 /char41/charea/charca/char10/char4a/char11/char4a/charaa/char4b/char2e\\\"), while others split it into two (\\\"/char41/chareb/char41/char4b/char0a/char40 /char41/charea/charcb/char40/char10/char49/char11/char4a/charaa/char4b/char2e\\\") or even three words (\\\"/char41/chareb/char41/char4b/char0a/char40 /char41/charea/charcb/char40/char10/char49/char11/char4a/charaa/char4b/char2e\\\"). This highlights the subjectivity among annotators across the various dialects that influence word count and segment length differences. This subjectivity, in addition to the episodes' topic diversity, influence the unique word count per dialect as detailed in Table 2. For all dialects combined, the unique word count is 85,176 words. On a country level, the Moroccan dialect has the highest number of unique words per hour with 4,458 words, while the Algerian dialect has the smallest at 3,518 words. This indicates that, besides Moroccan being the fastest dialect, it also has the greatest word diversity compared to other dialects.\\n\\nCode-Switching. Among all dialects in Casablanca, Algerian and Moroccan demonstrate a notably high usage of code-switching. Namely, as Table 2 shows, these dialects feature 500+ segments with code-switching. These North African dialects, in addition to Mauritanian, uniquely blend French into their code-switching. Other dialects in our dataset, such as Egyptian and Jordanian, involve switching into English. This linguistic diversity mirrors the historical colonial impact on languages in these regions. Overall, Casablanca includes 234 English code-switching segments (totaling \u224822 minutes) and 1,220 French code-switching segments (one hour and 44 minutes). Examples are shown in Table 10 in Appendix A.5. Conversely, we observe less code-switching in the other dialects. We suspected this may be due to episodes from other countries being relatively older as use of code-switching has become more prevalent among younger Arab generations (Brown, 2005). To test this hypothesis, we manually labeled the episodes for their time coverage. We found the following: Egypt (1997-2018), Jordan (1985-2000), and UAE (1995-2009) with 72, 52, and 59 code-switching instances, respectively. In contrast, newer episodes show higher instances: Algeria (2004-2017), and Morocco (2016-2018) with 586 and 598 cases, respectively. To summarize, our analysis shows that (i) French code-switching is more common than English and, even within the same dialect, (ii) newer episodes involve more code-switching than older ones.\\n\\nGender Bias. Despite our efforts to balance gender representation, a clear male dominance is observed across all dialects as demonstrated in Figure 1. The disparity is most notable in the Palestinian dialect, where male voices constitute 92.31%, leaving a mere 7.69% for female representation. In contrast, the Moroccan dialect exhibits a more gender balanced setup (with 57.08% male and 42.92% female). We now describe baseline models we developed exploiting our dataset.\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dialect   | Total Dur | Avg Dur | Avg U-Wds/hr | Skips | Avg WPS / CPS | CS |\\n|----------|-----------|---------|--------------|-------|---------------|----|\\n| Algeria  | 4:37:35   | 4.15    | 2,537        | 769   | 2.662         | 586|\\n| Egypt    | 7:04:16   | 4.29    | 2,962        | 715   | 2.858         | 72 |\\n| Jordan   | 6:00:16   | 4.23    | 4,255        | 5,257 | 1.286         | 52 |\\n| Mauritania | 5:49:40  | 3.67    | 3,099        | 5,556 | 1.631         | 36 |\\n| Morocco  | 6:15:02   | 3.54    | 4,119        | 504   | 3.206         | 598|\\n| Palestine| 6:02:59   | 5.30    | 2,543        | 720   | 2.264         | 50 |\\n| UAE      | 6:00:06   | 4.25    | 2,780        | 853   | 2.362         | 59 |\\n| Yemen    | 6:03:26   | 4.49    | 4,861        | 3,825 | 1.517         | 1 |\\n\\n**Total**\\n|             | 47:53:20  | 4.24    | 40,793       | 18,199| 2.223         | 17 |\\n\\n---\\n\\nTable 2: Distribution of data in Casablanca.\\n\\n- **Total Dur**: total duration for each dialect.\\n- **Avg Dur**: total duration divided by number of segments.\\n- **A VT**: average transcript length.\\n- **U-Wds**: number of unique words.\\n- **Avg U-Wds/hr**: average number of unique words per hour.\\n- **Skips**: number of skipped snippets.\\n- **WPS**: words per second.\\n- **CPS**: characters per second.\\n- **CS**: Number of code-switching segments. For Total, we take the average for average columns and sums for other columns.\\n\\n---\\n\\n7 Baseline models\\n\\nWe split Casablanca into Train, Dev, and Test, keeping the latter two splits each at one hour of the data per country. We perform a number of ASR experiments on the Dev and Test splits of Casablanca. First, we evaluate general speech models under a zero-shot condition. Then, we evaluate models that were finetuned on MSA or other dialects. Finally, we report experiments on our code-switched data only. We report results in WER and CER, both with and without preprocessing of the data. Details of our preprocessing pipeline are in Appendix A.7.\\n\\n7.1 Evaluation of General Models\\n\\nWe evaluated SoTA multilingual speech models on each dialect to understand their generic adaptability and performance across the eight dialects. Particularly, we evaluated two versions of Whisper (Radford et al., 2023) (whisper-large-v2 and whisper-large-v3, 1550M), SeamlessM4T (Barraud et al., 2023) (seamless-m4t-v2-large, 2.3B), and MMS (Pratap et al., 2023) (mms-1b-all, 1B). For this scenario, we report WER and CER of four different multilingual models on the eight novel dialects, which we hypothesize may not have been incorporated into the training data of these models. As shown in Table 3, all models exhibited high WER and CER across each dialect, indicating their inability to effectively generalize to entirely novel conditions. On average, whisper-large-v3 recorded lower WER and CER compared to other models, both with preprocessing (63 WER and 28.17 CER) and without (69.49 WER and 31.16 CER). In terms of dialects, without any preprocessing, only on the Jordanian dialect we achieved a WER of less than 50, as recorded by both Whisper models and SeamlessM4T. After preprocessing, the Palestinian and Egyptian dialects approached a WER of around 50 with these models. On average, mms-1b-all yielded the lowest performance compared to others, which can be attributed to the significant difference in domains between MMS data, a closed domain focusing on religious texts in MSA, and the Youtube series, an open domain featuring dialectal content.\\n\\n7.2 Evaluation of Dedicated Models\\n\\nHere we evaluate models that were finetuned by Talafha et al. (2023) on MSA, Egyptian, and Moroccan. Since the models were not released, we follow the same approach in Talafha et al. (2023) and regenerate four Arabic Whisper models based on whisper-large-v2: whisper-msa on Common Voice 11.0 (CV11) for MSA, whisper-mixed on MGB-2 targeting a blend of MSA and dialects, whisper-egyptian on MGB-3 focused on the Egyptian dialect, and whisper-moroccan on MGB-5 for the Moroccan dialect. Then, we evaluate these models on all dialects in Casablanca. As reported in Table 4, whisper-egyptian is notably superior for all dialects except Moroccan and Algerian. The superior performance of whisper-egyptian can be attributed to the domain adaptation.\\n\\n---\\n\\nRegenerate here means that we did the same finetunings in (Talafha et al., 2023)\\n\\nhttps://huggingface.co/datasets/mozilla-foundation/common_voice_11_0\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results for dialect evaluation, scenario-1 on the Test set. Results are reported in WER and CER (/ separated).\\n\\n| Country      | whisper-lg-v2 | whisper-lg-v3 | seamless-m4t-v2-large | mms-1b-all |\\n|--------------|---------------|---------------|------------------------|------------|\\n| Algeria      | 82.61 / 38.95 | 80.47 / 36.82 | 83.49 / 40.47          | 84.14 / 39.99 |\\n| Egypt        | 61.99 / 26.38 | 52.38 / 21.71 | 59.11 / 24.77          | 48.95 / 19.86 |\\n| Jordan       | 49.47 / 16.34 | 41.13 / 13.64 | 48.44 / 16.18          | 39.68 / 13.47 |\\n| Mauritania   | 87.85 / 52.34 | 85.74 / 49.76 | 87.44 / 50.19          | 85.68 / 48.08 |\\n| Morocco      | 88.55 / 46.57 | 84.52 / 44.02 | 87.2 / 44.41           | 83.05 / 42.09 |\\n| Palestine    | 57.06 / 20.02 | 48.64 / 17.24 | 58.02 / 21.05          | 50.2 / 18.38 |\\n| UAE          | 61.82 / 22.93 | 52.03 / 19.15 | 62.31 / 24.04          | 52.88 / 20.37 |\\n| Yemen        | 71.31 / 29.8  | 60.65 / 24.49 | 69.94 / 28.17          | 59.45 / 23.19 |\\n| AVG          | 70.08 / 31.66 | 63.195 / 28.35 | 69.49 / 31.16         | 63.00 / 28.17 |\\n\\nTable 4: Results for dialect evaluation, scenario-2 on the Test set. Results are reported in WER and CER (/ separated).\\n\\n| Pre-proc: preprocessing (+ with, - without) |\\n|-------------------------------------------|\\n\\nwhisper-msa is closely aligned with conversational domains that focus on everyday topics, a characteristic shared across all dialectal datasets. In comparison with whisper-moroccan, from a vocabulary perspective, as shown in Figure 2, the Egyptian dialect shares more vocabulary with Yemen, Jordan, UAE, Egypt, Palestine, and Mauritania than with the Moroccan dialect. Conversely, the Moroccan and Algerian dialects demonstrate a closer vocabulary alignment since these two North African dialects share more linguistic similarities than with other dialects. This correlation is consistent with the patterns observed in our experimental results. Therefore, whisper-moroccan performed better for Moroccan and Algerian compared to other models. Despite having the most extensive Arabic content (MGB-2), the whisper-mix model showed the weakest performance overall. This is attributed to two main reasons: firstly, the data was recorded in studio settings; and secondly, the content domain of the MGB-2 dataset (which includes politics, economy, society, culture, media, law, and science) differs significantly from daily conversation topics. This suggests that even though over 70% of the MGB-2 data is MSA, the remainder in dialects also does not accurately represent everyday speech, leaning more towards these specific close-domains. The evidence from the dialectal models supports the argument, showing that the MGB-3 and MGB-5 datasets, which were collected from YouTube (not including TV series), represent a wider range of real-life domains. Although these datasets are smaller in size compared to MGB-2, the relevance of the domain directly influenced their performance.\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This effect is also noticeable in the comparison of the whisper-msa and whisper-mixed models. Both performed well with MSA, as reported in Talafha et al. (2023), yet whisper-msa yielded better outcomes on dialects than whisper-mixed, even though MGB-2 (1200hrs) has a much larger volume of data than CV11 (89hrs). This is also related to the domains covered by CV11 being more open than MGB-2. To further investigate the domain's effect, we juxtaposed the outcomes of whisper-lg-v2 from scenario-1 with those of whisper-msa and whisper-mixed from scenario-2. It was observed that whisper-lg-v2 outperformed both models across all dialects, despite being the foundational model for the latter two. However, in the case of whisper-egyptian and whisper-morrocan, each surpassed whisper-lg-v2 within their respective dialects as well as in Algerian with the Moroccan model. These findings highlight the significance of incorporating models that are both open-domain and dialect-specific. Moreover, they highlight a clear gap between the current multilingual and SOTA Arabic models on one hand, and actual world dialects on the other. We hope that Casablanca contributes to bridging this gap.\\n\\nTo further explore the effectiveness of Casablanca, we fine-tune Whisper-v3 using combined training splits from each dialect (Whisper-Casablanca) and conducted an evaluation on the Algerian dialect as a case study. We compare this model to Whisper-lg-v3 as the baseline, Whisper-mixed, which was pre-trained on the largest dataset, and Whisper-Moroccan, the top-performing model for the Algerian dialect. The results displayed in Table 5 demonstrate a notable performance improvement over previous models. In comparison with Whisper-Moroccan, Whisper-Casablanca shows a 14.06 point reduction in WER before preprocessing and a 16.55 point reduction after preprocessing.\\n\\n| Model                | Pre-proc | Pre-proc |\\n|----------------------|----------|----------|\\n| Whisper-lg-v3        | 83.49    | 40.47    |\\n| Whisper-mixed        | 129.63   | 79.63    |\\n| Whisper-Moroccan     | 74.39    | 29.50    |\\n| Whisper-Casablanca   | 60.33    | 26.92    |\\n\\nTable 5: Results for evaluating different Whisper models on the Algerian Test set. Results are reported in WER and CER (/ separated).\\n\\n7.3 Evaluation on Code-Switched Data Only\\n\\nFor code-switching evaluation, we specifically focused on whisper-large-v3, selected for its overall superior performance compared to other models, as aforementioned (See Table 3). We conducted evaluations first on the original segments containing code-switching with Latin characters, and subsequently on their transliterated counterparts. Due to the relatively small number of code-switching segments, we consolidated all instances into one collective set for this focused evaluation. In the experiments, we evaluated Whisper's performance with inputs featuring either code-switching (CS-) or transliteration (Transliterated-), under three distinct decoding scenarios: (1) decoding without specifying the language (-Auto), (2) decoding with English identified as the language (-EN), and (3) decoding with Arabic recognized as the language (-AR). As reported in Table 6, the WER/CER scores are high in all settings, however identifying the target language makes the prediction worse. For a deeper comprehension of these findings, Table 12 and Table 13 detail the outputs for each condition, specifically for inputs involving code-switching and transliteration, respectively. With code-switched inputs, Table 12, Whisper failed to produce any code-switched words in all scenarios. Notably, even when the decoding language was set to English, Whisper performed a translation task even when specifying the task as \\\"transcription\\\". For the Auto and Arabic settings, Whisper outputted only transliterations. This issue is also observable with the transliterated inputs, see Table 13. This highlights a limitation in Whisper's capacity to transcribe data containing code-switching.\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7.4 Evaluation on Other Tasks\\nIn addition to the main ASR evaluations, we also performed a zero-shot benchmark on two additional tasks: Arabic dialect identification (ADI) and gender recognition. For ADI, we use the best-performing HuBERT-based model from (Sullivan et al., 2023) and perform a zero-shot evaluation on Casablanca's eight dialects. The results in Table 7 reflect similar challenges observed in their study, where the model underperformed on the \\\"YouTube Dramas\\\" domain. In addition to providing dialect labels, Casablanca also includes gender information, as mentioned in Section 4.2. This allows for an evaluation of the gender recognition task. Therefore, we fine-tuned XLS-R (Babu et al., 2021) on Librispeech-clean-100 (Panayotov et al., 2015), as an out-of-domain dataset, and subsequently evaluated its performance on our dataset.\\n\\n| Task          | Accuracy | Precision | Recall | F1 Score |\\n|---------------|----------|-----------|--------|----------|\\n| ADI           | 36.44    | 54.68     | 36.44  | 39.24    |\\n| Gender Rec.   | 83.56    | 89.23     | 83.56  | 84.32    |\\n\\nTable 7: Zero-shot results of ADI and gender recognition tasks on Casablanca.\\n\\n8 Conclusion\\nIn this paper, we introduced Casablanca, the largest supervised dataset for Arabic dialects, featuring a diverse representation across eight dialects. Casablanca includes underrepresented dialects such as Emirati, Yemeni, and Mauritanian. Encompassing 48 hours of data, the dataset also involves detailed annotations on transcriptions, speaker gender, and code-switching. Initial experiments with SoTA models demonstrate the Casablanca's utility for enhancing Arabic speech processing, especially in ASR, gender identification, and dialect identification. A subset of Casablanca is publicly available, aiming to support further research and innovation in both speech processing as well as linguistic research targeting dialects.\\n\\n9 Limitations\\nWhile we believe Casablanca will have a significant impact on a wide range of tasks in Arabic speech, it is important to acknowledge some limitations. Although Casablanca includes eight dialects, substantially more than previous datasets, the Arabic language comprises several other dialects that we do not cover. In addition to dialects, there is also diversity within each dialect. Therefore, we hope to expand the dataset to encompass a broader range of dialects in the future. Furthermore, as Figure 1 illustrates, for all dialects, the majority of speakers in Casablanca are male (over 60%, except for Morocco), potentially introducing gender biases. We recommend caution when working with gender-sensitive tasks. Finally, we provide only a YouTube URL for the source videos instead of the videos themselves due to copyright considerations. This could lead to availability issues if the videos are removed by their authors.\\n\\n10 Ethical Considerations\\nIn developing Casablanca, we adhere to ethical principles to ensure responsible and respectful use of data. Our dataset, sourced from publicly available TV series episodes on YouTube, is curated with careful consideration for privacy, omitting any personal identifiable information beyond what is publicly accessible. We try our best to ensure diverse representation in terms of gender and dialects to mitigate biases and promote inclusivity in ASR systems. All annotations and evaluations were conducted with linguistic and cultural sensitivity. While aiming to share the dataset to advance research, we implement access policies that require responsible use and proper citation. Our commitment to ethical standards is ongoing, and we welcome community feedback to continuously improve our practices.\\n\\nAcknowledgments\\nWe acknowledge support from Canada Research Chairs (CRC), the Natural Sciences and Engineering Research Council of Canada (NSERC; RGPIN-2018-04267), the Social Sciences and Humanities Research Council of Canada (SSHRC; 895-2020-1004; 895-2021-1008), Canadian Foundation for Innovation (CFI; 37771), Digital Research Alliance of Canada, and UBC Advanced Research Computing-Sockeye.\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Muhammad Abdul-Mageed, Amr Keleg, Abdelrahim Elmadany, Chiyu Zhang, Injy Hamed, Walid Magdy, Houda Bouamor, and Nizar Habash. 2024. Nadi 2024: The fifth nuanced Arabic dialect identification shared task. In Proceedings of The Second Arabic Natural Language Processing Conference, pages 709\u2013728.\\n\\nMuhammad Abdul-Mageed, Chiyu Zhang, Abdelrahim Elmadany, and Lyle Ungar. 2020. Toward micro-dialect identification in diaglossic and code-switched environments. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5855\u20135876.\\n\\nAbdelrahman Ahmed, Yasser Hifny, Khaled Shaalan, and Sergio Toral. 2019. End-to-end lexicon free Arabic speech recognition using recurrent neural networks. In Computational Linguistics, Speech And Image Processing For Arabic Language, pages 231\u2013248. World Scientific.\\n\\nAhmed Ali, Peter Bell, James Glass, Yacine Messaoui, Hamdy Mubarak, Steve Renals, and Yifan Zhang. 2016. The mgb-2 challenge: Arabic multi-dialect broadcast media recognition. In 2016 IEEE Spoken Language Technology Workshop (SLT), pages 279\u2013284. IEEE.\\n\\nAhmed Ali, Suwon Shon, Younes Samih, Hamdy Mubarak, Ahmed Abdelali, James Glass, Steve Renals, and Khalid Choukri. 2019. The mgb-5 challenge: Recognition and dialect identification of dialectal Arabic speech. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 1026\u20131033. IEEE.\\n\\nAhmed Ali, Stephan Vogel, and Steve Renals. 2017. Speech recognition challenge in the wild: Arabic mgb-3. In 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 316\u2013322. IEEE.\\n\\nRosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber. 2019. Common voice: A massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670.\\n\\nArun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al. 2021. Xls-r: Self-supervised cross-lingual speech representation learning at scale. arXiv preprint arXiv:2111.09296.\\n\\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449\u201312460.\\n\\nMohammed Bakheet. 2021. Improving speech recognition for Arabic language using low amounts of labeled data.\\n\\nLo\u00efc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. 2023. Seamlessm4t-massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596.\\n\\nMartijn Bartelds, Nay San, Bradley McDonnell, Dan Jurafsky, and Martijn Wieling. 2023. Making more of little data: Improving low-resource automatic speech recognition using data augmentation. arXiv preprint arXiv:2305.10951.\\n\\nHerv\u00e9 Bredin and Antoine Laurent. 2021. End-to-end speaker segmentation for overlap-aware resegmentation. In Proc. Interspeech 2021, Brno, Czech Republic.\\n\\nHerv\u00e9 Bredin, Ruiqing Yin, Juan Manuel Coria, Gregory Gelly, Pavel Korshunov, Marvin Lavechin, Diego Fustes, Hadrien Titeux, Wassim Bouaziz, and Marie-Philippe Gill. 2020. pyannote.audio: neural building blocks for speaker diarization. In ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing, Barcelona, Spain.\\n\\nKeith Brown. 2005. Encyclopedia of language and linguistics, volume 1. Elsevier.\\n\\nDavid Cohen, Mikael Lindvall, and Patricia Costa. 2004. An introduction to agile methods. Advances in computational theories, 62(03):1\u201366.\\n\\nGeorge E Dahl, Dong Yu, Li Deng, and Alex Acero. 2011. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on audio, speech, and language processing, 20(1):30\u201342.\\n\\nYousif A El-Imam. 2004. Phonetization of Arabic: rules and algorithms. Computer Speech & Language, 18(4):339\u2013373.\\n\\nNizar Habash. 2022. Arabic natural language processing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 9\u201310.\\n\\nAmir Hussein, Shinji Watanabe, and Ahmed Ali. 2022. Arabic speech recognition by end-to-end, modular systems and humans. Computer Speech & Language, 71:101272.\\n\\nSameer Khurana and Ahmed Ali. 2016. Qcri advanced transcription system (qats) for the Arabic multi-dialect broadcast media recognition: Mgb-2 challenge. In 2016 IEEE Spoken Language Technology Workshop (SLT), pages 292\u2013298. IEEE.\\n\\nMarieke Meelen, Alexander O\u2019Neill, and Rolando Coto-Solano. 2024. End-to-end speech recognition for endangered languages of Nepal. In Proceedings of the 21754...\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Historically, the Hidden Markov Model (HMM) combined with Gaussian Mixture Models (GMM) has been the dominant approach for achieving top results in large vocabulary continuous speech recognition (LVCSR). The first HMM-DNN hybrid for LVCSR was introduced by Dahl et al. (2011), outperforming traditional HMM-GMM systems. In the MGB2 challenge, Khurana and Ali (2016) utilized a combination of TDNN, LSTM, and BLSTM models, achieving a notable word error rate (WER) of 14.2%. End-to-end (E2E) models, mapping...\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"speech directly to text, gained popularity, simplifying ASR pipelines. Ahmed et al. (2019) introduced an E2E ASR model for Arabic, leveraging BRNNs with CTC for alignment. The introduction of an E2E transformer model addresses the morphological complexity and dialectal variations inherent in Arabic using self-attention mechanism and sub-word tokenization. Hussein et al. (2022) advanced Arabic ASR by employing a transformer-based encoder-decoder with a TDNN-LSTM language model, using Mel filter banks for acoustic features and training on MGB3 and MGB5 corpora, achieving leading performance with WERs of 27.5% for MGB3 and 33.8% for MGB5. In the era of large speech models, Arabic speech is still in its early stages. The XLS-R model (Babu et al., 2021), a large-scale model designed for cross-lingual speech representation learning, utilizing the wav2vec 2.0 framework (Baevski et al., 2020), was utilized on the Mozilla Common Voice dataset for MSA (Zouhair, 2021; Bakheet, 2021). The study of Ardila et al. (2019) benchmarks foundational models on Arabic ASR tasks, focusing on the performance of OpenAI's Whisper (Radford et al., 2023), Google's USM (Zhang et al., 2023), and the KANARI ASR model. These models were evaluated against a variety of datasets, emphasizing their efficacy across different Arabic dialects and speaking styles. Notably, USM typically surpassed Whisper, while KANARI demonstrated exceptional capability, especially in code-switching contexts between MSA and Egyptian dialect. The performance of Whisper across various Arabic dialects for ASR tasks was explored by Talafha et al. (2023). This evaluation spanned most publicly available datasets, utilizing n-shot (zero-, few-, full) fine-tuning approaches. The study also assessed Whisper's adaptability to novel scenarios, including dialect-accented MSA and previously unseen dialects. While Whisper demonstrated competitive results with MSA in zero-shot settings, its ability to adjust to different dialects was limited, showing inadequate performance and random output generation when encountering unfamiliar dialects.\\n\\nA.2 Annotation Tool\\n\\nWe employed Label-Studio, a widely supported open-source labeling platform, as our choice for an annotation tool. We centrally hosted it on our servers and provided online access, allowing for remote and adaptable involvement from annotators across various locations. Within the tool we used the 'Automatic Speech Recognition using Segments' template, enabling annotators to select multiple spans from each snippet and write their transcriptions accompanied by additional metadata. We also customized the tool to allow annotators to specify the gender of the speaker for each segment. We randomly shuffled the data to guarantee each snippet's independence, effectively reducing potential bias and sequencing effects that could impact annotators' perceptions during the annotation process.\\n\\nA.3 Transcribing a segment\\n\\nFigure 3 shows the process of transcribing a speech segment from a snippet based on its category (Dialect, MSA, and Other).\\n\\n![Figure 3: Example of transcribing a segment.](image)\\n\\nA.4 Inter-dialect diversity\\n\\nTable 8 demonstrates how the same words can be written differently within the same dialect, showcasing the inter-dialect diversity and the rich nuances that this brings to dialectical expression.\\n\\nA.5 Code-switching transcription\\n\\nTable 9 shows the code-switching transcription process.\"}"}
{"id": "emnlp-2024-main-1211", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Examples of dialect variation along with their translations in MSA and English.\\n\\n| Dialect | Var-1 | Var-1 | Var-3 | MSA          | English        |\\n|---------|-------|-------|-------|--------------|----------------|\\n| Algeria | /char42/char40/charf1/char11/char83/char11/char80/char40/charf0 ... | original dialect text. |\\n| Egypt   | /charf0/char58/char51/char4b/char2e/chare9/char09/char93/char51/char4b/char2e/charf1/char09/char93/char51/char4b/char2e/char41/char09/char92/char1d/char0a/char0d/char40 |   |\\n| Jordan  | /chare9/charca/char10/char4a/char4a/char0a/charba/char6b/chare9/charcb/char10/char49/char4a/char0a/charba/char6b/chare9/charcb/char10/char49/charca/char10/charaf |   |\\n| Morocco | /charf1/charca/char10/char4b/charf1/char10/charaf/chare9/charab/chare9/char4a/char0a/charca/char10/char4a/charc3 .../chara1/char10/charae/char09/charaf/chare9/charcb/char10/char49/charca/char10/charaf |   |\\n| Mauritania | /charf1/char6a/char2e/char4a/char2e/chard3/char0d/char40/charf1/char4a/char0a/char4a/char2e/chard3/char0d/char40/charf0/char59/char4a/char2e/chard3/char0d/char40/char09/charac/char41/char6d/charcc |   |\\n| Palestine | /char09/char90/char41/chareb/chare9/char41/chareb/char09/chara0/char41/chareb/char40/char09/char59/chareb |   |\\n| UAE      | /chare9/charca/char10/char4a/char10/charaf/chare9/charca/char10/char4a/charca/char10/charaf/chare9/charcb/char10/char49/charca/char10/charaf |   |\\n| Yemen    | /char10/char48/char51/chare5/char94/char1d/char2e/char40/chbc/char51/chare5/char94/char1d/char2e/char40/char10/char48/char51/chare5/char84/char1d/char2e/char40/char10/char49/char4b/char0a/char0d/char40/char50/char0d/char40 | did you see? |\\n\\nTable 9: Examples of code-switching in transcription.\\n\\nTable 10 shows examples of code-switching segments for each dialect, along with their transliterated versions. Code-switched terms are provided in teal color.\\n\\n| Dialect | Example |\\n|---------|---------|\\n| Algeria | l'affaire/chare9/chard3/charf0/char59/char09/chare0/char41/charbf/chare8/char41/char09/charae/char4a/char0a/charbb |\\n| Egypt   | program/chae9/chard3/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/chard2/char6a/char2e/chare9/chard2/char6a/char2e/chare0/char40/chbd/chare9/ch"}
{"id": "emnlp-2024-main-1211", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dialect Description\\n\\nEgyptian Some speakers tend to use \\\"/chara8\\\" in the beginning of the words instead of \\\"/chare8\\\", so we agreed on writing it as \\\"/chare8\\\". Others use the letter \\\"/char68\\\" as in \\\"/charbd/charcb/charf1/char10/charae/char6b\\\" instead of \\\"/charbd/charcb/charf1/char10/charae/chareb\\\". We suggested writing it the way we hear.\\n\\nSome segments in the Egyptian dialect include urban upper Egyptian other than the Cairene one, so I wrote it as I heard. For example, a word like \\\"/charbd/charcb/charf1/char10/charaf/char0d/char40\\\" in Cairene would be \\\"/charbd/charcb/charf1/char6b/char2e/char0d/char40\\\" in Upper Egyptian.\\n\\nJordanian The word \\\"/char41/char82/chareb\\\" is sometimes pronounced as \\\"/chara9/char82/chareb\\\", so I transcribe it based on the last letter; if \\\"/chara8\\\" is clear, I write \\\"/chara9/char82/chareb\\\" otherwise, I write \\\"/char41/char82/chareb\\\".\\n\\nThe word \\\"Tomorrow\\\" has two forms: /char40/char51/charba/char4b/char2e and /chare8/char51/charba/char4b/char2e. I decided to write /char40/char51/charba/char4b/char2e to be distinguished from /chare8/char51/charba/char4b/char2e which also means \\\"I hate\\\".\\n\\nUAE In many pronunciations, some Emaratis (depending on the region and tribe they belong to) put emphasis on some letters in a word. The word \\\"/charfa/char0a/charce/charab\\\" which means on top of me, can also be pronounced with an emphasis on the letter \\\"/charf8/char0a\\\". Another instance is where the letter \\\"/chare8\\\" is added at the end of the word \\\"/chare9/char4a/char0a/charca/charab\\\". Emiratis use the word \\\"/charc9/char4a/char0a/charab\\\" mainly meaning \\\"/char3f /char40/char09/char58/char41/chard3 /char40/char09/char58/char40/char0d\\\" or what else? However, the word has a less frequent use that means to be the cause of an issue \\\"/chare9/char4a/char0a/charca/charab /charc9/char4a/char0a/charab /charc8/char41/charab\\\", but with a slightly different pronunciation.\\n\\nTable 11: Illustrations of special cases unique to each dialect.\\n\\n| Code-switching input | CS_reference |\\n|----------------------|-------------|\\n| /chara9/chard2/char6a/char2e/char10/char4a/char10/char4b | maximum |\\n| /chara9/charaa/char4a/char2e/char83 | maybe |\\n| /charfa/char0a/char09/chare6/charaa/char4b/char0a/char10/chare9/char4a/char0a/char09/char4b/char41/chard6/char11/chardf | Maximum |\\n| /charf0/char10/chare9/charaa/char4a/char2e/char82/charcb/char40/char09/chare1/char1e/char0a/char4b/char2e | Signature |\\n\\nTable 12: Results of whisper-lg-v3 on input having code-switching (Latin letters).\\n\\n| CS_reference | CS - Auto | CS - EN | CS - AR |\\n|-------------|-----------|--------|--------|\\n| /chara9/chard2/char6a/char2e/char10/char4a/char10/char4b | 8 | Maybe 7, maximum 8 | 8 |\\n| /charf1/char4a/char2e/char4a/char0a/chard3 | 7 | Maximum 8 | 7 |\\n| /char91/char09/char4a/charcb/char40 | Translated |\\n\\nTable 13: Results of whisper-lg-v3 on input having transliterated words (Arabic letters).\\n\\n| Transliterated reference | Transliterated - Auto | Translated - EN | Transliterated - AR |\\n|--------------------------|-----------------------|-----------------|--------------------|\\n| /chara9/chard2/char6a/char2e/char10/char4a/char10/char4b | 8 | Maybe 7, maximum 8 | 8 |\\n| /charf1/char4a/char2e/char4a/char0a/chard3 | 7 | Maximum 8 | 7 |\\n| /char91/char09/char4a/charcb/char40 | Translated |\\n\\nCase # Reference/Prediction\\n\\nCase 1 Reference: \u06be\ufe8e\u0645 \u0627\ufedf\ufea3\u0631\u0627\ufbfe\u0631 \ufedb\u0644 \ufebb\ufe91\ufe8e\u0639 \ufe91\ufebb\ufee7\ufecc\ufe94 Prediction: \u0623\ufecb\ufee3\u0644 \ufea3\u0631\u0627\ufbfe\ufe8e \ufe91\ufedb\u0644 \u0627\ufedf\ufebb\ufe91\ufe8e\u0628 \ufee3\u0646 \ufebb\u0646\\n\\nCase 2 Reference: \ufea7\ufefc\u0635 \u0631\u0648\u062d \ufedf\ufee0\ufe91\u0648\ufe97\ufbfe\u0643 \ufe97\ufe8e\ufecb\u0643 \ufe91\ufee0\u0648\u0637\ufba5 \u0631\u0648\u062d Prediction: \ufed3\ufed8\u0637 \u0627\u0630\u06be\u0628 \u0625\ufedf\ufef0 \ufe91\u0648\ufe97\ufbfe\ufedb\u0643\\n\\nCase 3 Reference: \ufee3\ufe8e \u06be\u062f\u0631\ufe97\u0634 \ufecb\ufee0\ufbfe\u0643 \ufee3\u0648\ufefc\u064a Prediction: M\u0103 d\u0103rcea, nicmunei!\\n\\nCase 4 Reference: \u0627\ufedf\ufee0\ufbab \ufbfe\ufeb3\ufee0\ufee3\u0643 Prediction: \ufe9f\ufbfe\u062f \ufe9f\u062f\u0627\\n\\nTable 14: Samples from high error rates in the prediction of the Algerian dialect.\"}"}
