{"id": "emnlp-2023-main-799", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nData scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets.\\n\\nUsing SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We make our data, models, and code public.\\n\\n1 Introduction\\n\\nConversations that occur in everyday spoken situations are often not recorded as data. And when they are, such as in the case of text messages, research use is rightly restricted due to privacy and legal concerns. As a result, collecting high-quality, everyday social conversations on a large scale has long been recognized as a difficult task (Smith et al., 2020).\\n\\nPrevious studies have relied on crowdsourcing focused on specific themes of dialogue (e.g., persona, empathy; Zhang et al., 2018; Rashkin et al., 2019). However, this approach is limited in scale due to its associated costs. As a result, the progress made in machine dialogues, including generation, evaluation, and understanding, has been severely hindered by the reliance on these small datasets (Kann et al., 2022; Mehri et al., 2022).\\n\\nTo alleviate this bottleneck, we introduce SODA (Social Dialogues), a million-scale English dialogue dataset covering a wide variety of social interactions. As a result of being grounded on rich social commonsense and narratives, SODA goes beyond specific skill-focused dialogues and features more general conversations. Our dataset includes 1.5 million dialogues distilled from a large language model (in our case, GPT-3.5; Ouyang et al., 2022) resulting in more than 11 million utterances with 300 million tokens: SODA is the largest publicly available open-domain social conversation dataset. Human evaluation shows that SODA surpasses existing human-authored dialogue corpora across axes like consistency, specificity, and (surprisingly, even) naturalness (\u00a73.2).\"}"}
{"id": "emnlp-2023-main-799", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Illustrated in Figure 1, CO3 infuses commonsense knowledge into dialogues by transforming knowledge triples into narratives, and then into dialogues. Such an approach offers two significant advantages: (1) maximizing diversity and (2) minimizing nonsensical conversations. Although generating content using LLMs is relatively easy, determining how to cover diverse content poses a non-trivial challenge. We find that sampling from an LLM without contexts results in dull conversations (\u00a73.3). Because commonsense knowledge graphs cover a wide range of everyday situations (West et al., 2022), conditioning on them results in a broad spectrum of conversations. Moreover, since LLMs are prone to hallucinations (Weidinger et al., 2021), the seed commonsense knowledge can help them stay on a sensible generation path.\\n\\nWith SODA, we train a COnversation MOdel, COSMO. Human evaluation results demonstrate that: (1) COSMO generalizes better to unseen conversations than existing best-performing dialogue models, winning by more than 40% on average in head-to-head comparisons versus BlenderBot (Roller et al., 2021), Koala (Geng et al., 2023), and Vicuna (Chiang et al., 2023) (\u00a75.1); (2) COSMO outperforms BlenderBot (with the same number of parameters) on the dataset BlenderBot was trained on, despite never seeing the corpus (\u00a75.2); and (3) COSMO responses are even preferred over human-authored, ground-truth responses in DailyDialog (Li et al., 2017), a dataset on which COSMO was not trained on (\u00a75.1).\\n\\nFinally, the distilled dialogues in SODA represent a significant resource contribution for open-domain dialogue research. Most of all, SODA enables the research community to train smaller dialogue agents with competitive capabilities. Also, SODA can help enhance the generalizability of other advancements in the dialogue field (e.g., understanding and evaluation), which have relied on existing small datasets. Lastly, SODA highlights a dimension where recent LLM-based conversational agents (e.g., Koala, Vicuna, and ChatGPT) struggle \u2013 i.e., the naturalness of the responses (\u00a75.1 and \u00a75.3). As these models are designed to provide knowledge-based responses, they may generate responses that are informative but lack the naturalness found in social chitchat. We plan to publicly release SODA, COSMO, and CO3 under the permissive license CC-BY-4.0, aiming to address the data scarcity issue in open-domain dialogue.\"}"}
{"id": "emnlp-2023-main-799", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"closer to the goal, Relation: xNeed, Tail: to take the first step. We use Atomic10x (West et al., 2022) as our knowledge graph: it includes diverse social (e.g., intention, desire, reaction) and event-centered (e.g., order of events) commonsense.\\n\\nSince we are interested in distilling social interactions, we only retrieve triples related to social (rather than, e.g., physical) commonsense.\\n\\n2.3 Commonsense Knowledge\\n\\n\u2192 Narrative\\n\\nTriple Form to Sentence Form\\n\\nSince commonsense knowledge graphs are represented in symbolic form (i.e., triples), we first convert them into simple sentences with templates for each relation. For example, the commonsense knowledge in Table 1 is converted to \\\"Madeleine took the first step. Madeleine moves a step closer to the goal.\\\" To make the sentences sound more natural, we replace the person variables (e.g., PersonX, PersonY) with Top-1K common names of US SSN applicants ranging from 1990 to 2021.\\n\\nSentence Form to Narrative\\n\\nNext, we prompt GPT-3.5 to instantiate the sentence-form into a two- or three-sentence short narrative, e.g., for the commonsense example above \\\"Madeleine took the first step towards her goal, and with her coach's encouraging words, she moves one step closer.\\\" Prior work has shown that LLMs can effectively generate stories with plausible details that go beyond the contents of the prompt (Radford et al., 2019).\\n\\n2.4 Narrative\\n\\n\u2192 Conversation\\n\\nInferring Conversation Participants\\n\\nInferring the conversation participants from the narrative is straightforward in cases where triples contain two person variables (i.e., PersonX and PersonY). But for triples that include only one person (e.g., the example in Table 1), we query GPT-3.5 to predict the other interlocutor (e.g., mom, coworker).\\n\\nGenerating Conversation grounded in Narrative\\n\\nWith the narrative and speakers as input, we prompt GPT-3.5 to generate a full, multi-turn conversation between the speakers in the context of the narrative. We append the first speaker as an utterance prefix to the prompt. Indicating the speakers with prefixes helps GPT-3.5 generate fluent conversations that alternate between the two.\\n\\nWe leave relations for physical and event-centered commonsense to potential future work.\\n\\n4 catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-data\\n\\nSymbolic Social Commonsense Knowledge:\\n\\n\u2022 Head: PersonX moves a step closer to the goal\\n\u2022 Relation: xNeed\\n\u2022 Tail: to take the first step\\n\\nSentence-form Commonsense:\\n\\nMadeleine took the first step. Madeleine moves a step closer to the goal.\\n\\nNarrative:\\n\\nMadeleine took the first step towards her goal, and with her coach's encouraging words, she moves one step closer.\\n\\nSpeakers:\\n\\nMadeleine, Coach\\n\\nConversation:\\n\\nMadeleine: Hey coach, I wanted to talk to you about my performance today. I was really pushing myself and I think I did pretty well. But I'm still not quite where I want to be.\\n\\nCoach: Well Madeleine, you're progressing nicely. You've come a long way since we first started working together. But if you want to reach your full potential, there's still some work to be done.\\n\\nMadeleine: I know that. And I'm willing to put in the work. It's just that sometimes I feel like I'm not making as much progress as I should be. Maybe I'm not training hard enough? Or maybe my technique is off?\\n\\nCoach: It could be a number of things, Madeleine. But don't worry, we'll figure it out together. Let's just keep working hard and see how things go.\\n\\nMadeleine: Alright, coach. Thanks for the talk.\\n\\nCoach: No problem. See you at practice tomorrow.\\n\\nTable 1: A sample from SODA. More examples can be found in Appendix B.\\n\\n3 SODA: A Million-scale Social Dialogue Dataset\\n\\nWe obtain SODA (Social Dialogues), a large-scale high-quality conversation dataset covering a wide range of social interactions, by applying a series of post-processing (\u00a73.1) to the conversations generated from our contextualization framework (\u00a72).\\n\\nWe compare SODA with existing human-curated dialogue corpora (\u00a73.2) and analyze the effectiveness of contextualization (\u00a73.3). Table 1 shows a sample from our dataset. More details are in Appendix B.\\n\\n3.1 Post-processing the Conversations\\n\\nBasic Filtering\\n\\nStarting with an initial set of 2.2 million conversations sampled from GPT-3.5, we:\\n\\n(1) use lexical pattern matching to filter out conversations with erroneous patterns \u2013 e.g., repetition and omission of speaker prefixes (6.3%);\\n\\n(2) remove conversations that have less than four turns or more than twenty turns (5.7%);\\n\\n(3) remove conversations with more than two speakers (11.3%); and\\n\\n(4) remove conversations where at least one of the speakers was identified as non-human (e.g., broomstick, imaginary friend, dog; 5.6%).\\n\\nAlthough our pipeline naturally generates multi-party conversations as well, we focus on dyadic dialogues in this work.\"}"}
{"id": "emnlp-2023-main-799", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Results of head-to-head comparison between dialogues from SODA, DailyDialog (Li et al., 2017), and BlendedSkillTalk (Smith et al., 2020) via human judgments (\u00a73.2). The y-axis represents the number of samples preferred by human judges. The differences in all of the categories except for the **Context Dependence** comparing SODA and BlendedSkillTalk are statistically significant ($|z| > 3.3$, $p < 0.05$).\\n\\n**Safety Filtering**\\nIn order to avoid conversations with dangerous and harmful contents, we apply two safety filters: Canary (Kim et al., 2022a) and Rewire API. Canary is a narrative dialogue safety model that can classify whether the given context needs caution or intervention. We discard all conversations marked as needing intervention (usually critical situations, e.g., crimes, emergencies; 4.3%); Rewire API is a web-based API for detecting toxic content. We discard all conversations that are above the threshold of 0.5 for any of the \u2018violence\u2019, \u2018hate\u2019, and \u2018sexually explicit\u2019 criteria ($\\\\sim 1\\\\%$).\\n\\n**Commonsense Filtering**\\nWe conduct a small-scale human evaluation via Amazon Mechanical Turk with 100 randomly sampled narrative-conversation pairs (3 annotators per instance) to check whether or not the seed commonsense triple is meaningfully instantiated by the narrative and conversation. According to majority vote, 88% of the instances include the seed commonsense knowledge. Given that the majority of human-annotated samples include the seed commonsense, we focus our filtering on excluding narrative-conversation pairs that lack the head event, as they are irrelevant to the given seed commonsense.\\n\\nTo apply this filter to all entries of the corpus, we use GPT-3.5 as a zero-shot classifier. As GPT-3.5 demonstrated great performance in question answering (Ouyang et al., 2022), we validate the generated narrative-conversation pairs by asking the language model itself to judge whether or not the head of the commonsense triple is implied. We formulate this as three-way multiple choice questions (i.e., yes, no, and unknown) and rank the answers according to their perplexity scores from GPT-3.5. This zero-shot classifier achieves high performance on the human-annotated subset, with a precision of 97% for answering \u201cyes\u201d. We find 95% of the filtered conversations are identified by GPT-3.5 as containing the head event. Pairs that lack the head event are removed to ensure relevance between the narrative-conversation pairs and commonsense triples. More details are in Appendix B.1.\\n\\n**Final Dataset**\\nAfter all filtering, 68.9% of the initial conversations remain, which form the 1,486,896 conversations in SODA.\\n\\n**Name Bias Mitigation**\\nWe aim to minimize biases associated with specific names while increasing inclusion and diversity. Both language models and curated datasets often exhibit demographic imbalances (Dinan et al., 2020; Weidinger et al., 2021; Sheng et al., 2021). Inspired by Smith and Williams (2021), we randomly replace all names in conversations with Top-10K names of US SSN applicants from 1990 to 2021. This covers 95% of all applicants' names from the chosen time range window, including various names from diverse gender and ethnic backgrounds.\\n\\n**3.2 Comparing SODA with Human-authored Dialogues**\\nHigh Quality\\nTo assess relative quality of the corpus, we conduct head-to-head human evaluations on Amazon Mechanical Turk, comparing SODA with two widely used open-domain dialogue datasets: DailyDialog (Li et al., 2017) and BlendedSkillTalk (Smith et al., 2020). We random sample 300 dialogues from each dataset and evaluate them according to six criteria (Mehri et al., 2022): (1)\"}"}
{"id": "emnlp-2023-main-799", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset                | # Conversations | # Turns | Avg. Utterance Length | Lexical Diversity |\\n|------------------------|----------------|---------|-----------------------|-------------------|\\n| DailyDialog            | 13K            | 7.9     | 14.6                  | 63.0              |\\n| PersonaChat            | 11K            | 14.8    | 14.2                  | 43.6              |\\n| WizardOfWikipedia22K   | 9.1            | 16.4    | 60.3                  |\\n| EmpatheticDialogue25K  | 4.3            | 13.7    | 64.2                  |\\n| BlendedSkillTalk       | 7K             | 11.2    | 13.6                  | 64.2              |\\n| ProsocialDialog        | 58K            | 5.7     | 20.0                  | 60.2              |\\n| SODA 1.5M              | 7.6            | 16.1    | 68.0                  |\\n\\nTable 2: Statistics of SODA compared to other large-scale dialogue datasets. Utt. denotes utterance. Lexical diversity is measured with MTLD (McCarthy and Jarvis, 2010). Description for each dataset is in Appendix F.\\n\\nDespite being fully machine-generated, human raters judge SODA as better in quality compared to both DailyDialog and BlendedSkillTalk across all axes by a large margin, except for the context dependence comparing with BlendedSkillTalk (see Figure 2). In particular, evaluators rate the flow of SODA to be significantly more natural than other human-authored artificial conversation datasets.\\n\\nLarge Scale\\nWith 1.5 million conversations, SODA is the largest in scale compared to existing crowdsourced open-domain dialogue datasets and the machine-human generated ProsocialDialog dataset (Table 2). It contains more than 11 million utterances and each conversation is grounded in a short narrative describing the context. In total, SODA consists of 300 million tokens, making it a rich source for training conversation models.\\n\\nDiverse Content\\nSODA is built on top of 1.5 million commonsense knowledge triples of Atomic10x, which have been identified as being softly unique (West et al., 2022). Each seed triple is converted to a social narrative that serves as the distinct topic for each conversation. The Top-10 common keywords from these narratives are listed in Table 3.\\n\\nA power analysis suggests that with our setup, we can detect effect sizes as small as 0.17 with a power and significance level of 95% (Faul et al., 2014).\\n\\nWe prompt ChatGPT to output keywords of the narrative. Common keywords across all relations:\\n- friendship, help, support, communication, family, car, happiness, school, success, work\\n\\nCommon keywords for each relation (excluding the above):\\n- xAttr (18%): kindness, anger, intelligent, responsibility, friend, trust, conversation, food, generosity, smart\\n- xEffect (17%): gratitude, anger, upset, hard work, happy, money, friend, boss, party, kindness\\n- xIntent (23%): independence, hard work, determination, money, relaxation, anger, kindness, store, understanding\\n- xNeed (7%): job, money, confidence, comfort, advice, interest, conversation, listening, store, park\\n- xReact (25%): frustration, anger, confidence, happy, pride, relief, disappointment, relaxation, anxiety, satisfaction\\n- xWant (11%): conversation, store, determination, apology, learning, doctor, job, friend, improvement, marriage\\n\\nTable 3: Common topic keywords of the narratives (i.e., conversation context) in SODA. Numbers in parentheses denote the ratio of the relations in SODA.\\n\\nWe find a broad spectrum of topics encountered in social interactions are included in SODA. As a result, conversations in SODA contain diverse lexicons. We compute MTLD (McCarthy and Jarvis, 2010) to measure the lexical diversity of conversations. Table 2 reports the averaged diversity of dialogues for each training set. As PersonaChat (Zhang et al., 2018) contains conversations based on a few persona-related sentences, it shows the lowest lexical diversity. SODA, on the other hand, includes conversations from a variety of social situations, which leads to a wider range of words.\\n\\nRich Emotion-related Information\\nSince commonsense knowledge from Atomic10x includes emotional reactions of people to events (i.e., the xReact triples), conversations with rich emotional contents are also included in SODA. In total, SODA includes 385K conversations generated from 1.7K unique emotion descriptions of the xReact triples\u2019 Tail (e.g., happy, ashamed, motivated, irritated).\\n\\nTherefore, it contains significantly more descriptive emotion labels (i.e., the Tail) than other datasets which have fixed number of classes (Li et al., 2017; Rashkin et al., 2019). Furthermore, because we construct conversations in a bottom-up fashion from those emotion reaction in the commonsense triples, we know which speaker in the conversation is experiencing the emotion (i.e., PersonX) and what caused the emotion (i.e., the Head event).\\n\\nWe note that conversations from other relations also naturally include emotional utterances.\"}"}
{"id": "emnlp-2023-main-799", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: The ratio (%) of Top-10 emotions in 10K utterances from DailyDialog, BlendedSkillTalk, and SODA, labeled by the GoEmotions' 27-emotion-type classifier (Demszky et al., 2020). Full table is in Appendix B.2.\\n\\nWe also find the distribution of emotions to be less skewed towards specific emotions. To compare the emotional composition, we use the 27-emotion-type classifier from GoEmotions (Demszky et al., 2020) for labeling and compare 10K utterances from DailyDialog, BlendedSkillTalk, and SODA. The distribution of emotions for each dataset is presented in Table 4. SODA exhibits a more balanced distribution of emotions while maintaining similar rankings with other human-authored dialogues.\\n\\n3.3 Do We Need Contextualization?\\n\\nTo isolate the effect of contextualization (vs. straightforward sampling from a large language model), we compare SODA with dialogues naively sampled from GPT-3.5 without any given context. We sample 100 dialogues using the same hyperparameters and the basic filtering steps in CO3, but with the following prompt: \\\"The following is a long in-depth conversation between two people. Person 1:\\\"\\n\\nWe ask human judges to evaluate the conversations in a head-to-head comparison as before (\u00a73.2), with the additional criterion of interestingness (See et al., 2019). Figure 3 shows that judges significantly prefer context-grounded conversations. Conversations sampled without context are not only less specific and less interesting, but also exhibit lower lexical diversity than those from our CO3 framework.\\n\\nWe use SODA to train COSMO: a Conversation Model that can converse in a wide range of social situations. COSMO can take in situation narrative, along with dialogue history, and generate a next utterance according to a given role.\\n\\nTraining COSMO\\n\\nWe use several structured components of SODA during training: (1) the contextual narrative n (\u00a72.3), (2) the perspective/speaker instruction i (e.g., \\\"Imagine you are Madeleine and speak to her coach\\\") built with the inferred conversation participants (\u00a72.4), and (3) the dialogue context c. The model is trained to generate a target response r when given n, i, and c \u2013 i.e., \\\\( p(r|n, i, c) \\\\).\\n\\nWe do so in a sequence-to-sequence fashion, concatenating n, i, c with a separator <SEP> to serve as input. c is made up of the previous conversation utterances concatenated with a turn indicator <TURN>.\\n\\nBecause conversational models often agree to toxic or unethical behavior (Baheti et al., 2021), for additional training data, we include ProsocialDialog (Kim et al., 2022a) (adapted to the same format as SODA, see Appendix C). ProsocialDialog includes a wide range of negative constructive feedback based on social rules-of-thumb, e.g., \\\"So I think it's best to continue being honest, and apologize that you were lying.\\\" The inclusion of this corpus assists conversation models in handling sensitive contexts (e.g., biased, harmful, unethical) without affecting the model performance on other datasets (Kim et al., 2022a).\"}"}
{"id": "emnlp-2023-main-799", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We build COSMO on top of the LM-adapted T5 (Raffel et al., 2020; Lester et al., 2021), which achieves strong benchmark performance across various classification and generation tasks. (Sanh et al., 2021; Chung et al., 2022). We train two versions of the model: COSMO-3B and COSMO-11B using the T5X library (Roberts et al., 2022). For better robustness and generalizability to datasets that don't have contexts or dialogue starting prompts, we randomly drop narrative and role instructions 30% and 50% of the time, respectively.\\n\\n5 Generalizability of COSMO\\n\\nWe compare COSMO to other conversational agents on social conversation datasets under both out-of-domain and in-domain settings. Since automatic response evaluation is brittle, we focus on human evaluation (Smith et al., 2022). Automatic evaluation results via GPT-4 are in Appendix D.\\n\\nBaselines\\n\\nWe compare COSMO with four best-performing stand-alone conversation models: BlenderBot-1 (Roller et al., 2021), GODEL (Peng et al., 2022), Koala (Geng et al., 2023), and Vicuna (Chiang et al., 2023). BlenderBot is a transformer pretrained on 1.5B Reddit comments and trained on various chitchat datasets. GODEL utilizes a pretrained language model T5 (Raffel et al., 2020) trained on web text data, and further trains on 551M Reddit threads and 5M instruction and grounded dialogue datasets. Koala and Vicuna are models that finetuned LLaMA (Touvron et al., 2023), which is an open-source LLM, using dialogue data from the web. They are both known to achieve comparable performance to ChatGPT (OpenAI, 2022), which is a model finetuned for conversational interaction based on GPT-3.5 \u2013 i.e., our teacher model. We also compare COSMO with GPT-3.5 and ChatGPT; prompting details are in Appendix D.\\n\\nEvaluation Metrics\\n\\nWe perform head-to-head comparison between two responses, each from a different agent. We sample 100 test examples randomly from datasets and ask three human judges on Amazon Mechanical Turk to select the better response between the two in terms of four distinct criteria (Mehri et al., 2022): (1) naturalness, (2) consistency, (3) specificity, and (4) overall.\\n\\n5.1 Out-of-domain Setting\\n\\nWe evaluate models on an unseen dialogue dataset, DailyDialog (Li et al., 2017), covering various daily situations with emotions. Table 5 summarizes the head-to-head comparison results of the responses from COSMO and other models. Although COSMO is trained on significantly smaller amount of data (1.5M dialogues vs. 1.5B Reddit comments, 551M Reddit threads) and is significantly smaller (3B vs. 7B), it outperforms all other existing models with a significant margin across all aspects. Specifically, COSMO demonstrates the largest performance gap in terms of naturalness. It is worth noting that while Koala and Vicuna focus on providing informative responses, these results suggest that knowledge-seeking assistive conversations differ from natural social conversations.\\n\\nIn addition, we compare the responses from COSMO and 200 ground-truth responses in DailyDialog which were originally written by humans. Surprisingly, human judges prefer COSMO's responses even over the original gold responses in the dataset, suggesting that dialogue models trained on SODA can lead to high generalizability and naturalness, even for unseen conversations. Table 14 in the Appendix shows the ground-truth response and responses from each model for a given context.\\n\\n5.2 One-sided Out-of-domain Setting\\n\\nFor an even harder setting, we evaluate COSMO vs. BlenderBot on the dataset BlenderBot was trained on: BlendedSkillTalk (BST; Smith et al., 2020). Table 6 (top) shows the head-to-head comparison results of the responses from COSMO and BlenderBot (for symmetry, we also evaluated BlenderBot on SODA with similar results; bottom row in Table 6). COSMO significantly outperforms BlenderBot on BST, its training domain (BlenderBot also\"}"}
{"id": "emnlp-2023-main-799", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"shows low performance on SODA). These results suggest that SODA contains patterns not present in existing datasets, but also covers patterns found in those datasets. More results are in Appendix D.\\n\\n5.3 In-domain Setting\\n\\nWe also compare COSMO on SODA with its teacher GPT-3.5 and also ChatGPT, a chatbot-variant of the teacher. Table 7 displays the head-to-head comparison results. In this setting, COSMO performs on-par with its teacher and ChatGPT, overall. In terms of specificity, COSMO's responses are significantly more specific than its teacher. Thus, SODA enables training competitive conversation models with a significantly smaller size (3B/11B) in comparison to existing large language models (175B).\\n\\nHuman judges evaluate ChatGPT's responses to be much more specific, but significantly less natural compared to COSMO. We hypothesize this is because ChatGPT is specially trained to give helpful and informative responses to user requests. Future work would be well-suited to compare the non-equivalence of simulating natural conversations vs. producing useful responses for users.\\n\\n6 Related Work\\n\\nBuilding Dialogue Datasets with Large Language Models\\n\\nSeveral studies have used large language models to augment or synthesize dialogue datasets. Zheng et al. (2023) and Chen et al. (2022) use GPT-J (Wang, 2021) to augment responses for emotional support conversations and understanding tasks, respectively. Chen and Yu (2021) trains a pseudo-labeler to increase the out-of-domain generalization of dialogue models. Ou et al. (2022) uses counterfactual reasoning to alter the semantics of responses and collect new ones. Kim et al. (2022a) proposes a human-machine collaborative framework, where a worker and GPT-3 take turns. Kim et al. (2022b) builds Blended Skill BotsTalk by letting multiple agents grounded in target skills engage for multi-skill dialogues. Chen et al. (2023) generate dyadic and multi-party conversations with topic words and show they have comparable quality to human-authored conversations. GPT-3 has also been used to help simulate task-oriented dialogues (Li et al., 2022) on a small scale. Others also augment dialogues with additional annotations \u2013 e.g., commonsense inferences (Zhou et al., 2022) or task-specific labels (Kulh\u00e1nek et al., 2021; Chen et al., 2022). Compared to existing works, we are the first to contextualize commonsense knowledge graphs for generating narratives and derive full conversations from scratch in a significantly large-scale. This allows us to encompass an exceptionally broad spectrum of social interactions.\\n\\n7 Conclusion\\n\\nWe presented SODA, the first million-scale dialogue dataset covering an exceptionally wide range of social interactions to alleviate the data scarcity issue. SODA is not only orders of magnitude larger than popular dialogue datasets; it is also perceived to be significantly better than them across multiple aspects (e.g., naturalness, specificity, consistency). For making SODA, we also introduced C03, a framework for distilling conversations from a large language model by contextualizing commonsense knowledge. With SODA, we trained a conversation model COSMO that can generalize significantly better than existing models to unseen dialogues; and generate responses that are even more preferred than ground-truth responses of an existing dataset.\"}"}
{"id": "emnlp-2023-main-799", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nPrecautions taken during Dataset Construction\\n\\nMining content from large language models might surface or even amplify harmful content within these models, such as biases and private information. With the goal of mitigating such danger, we take particular precautions to vet the safety of the distilled conversations.\\n\\nFirst, previous studies have shown that human names commonly associated with certain gender and/or ethnicity result in biases in conversations produced by state-of-the-art dialog systems (Smith and Williams, 2021), such as BlenderBot (Roller et al., 2021). To diversify the name representations, we draw a wide range of common names representative of different gender and race identities from the US SSN name repository. Furthermore, to minimize potential harmful content from large language models, we filter generated dialogues by Canary, a dialogue safety detector model (Kim et al., 2022a), and Rewire API, a publicly available API for toxic content detection, to remove dialogues with potentially toxic and dangerous content.\\n\\nOur methods to pre-empt potential harmful content may not catch everything. For example, even with our diverse pool of names, there is still a focus on common names across gender and race, running the risk of misrepresenting marginalized groups. Similarly, no existing dialogue safety module or off-the-shelf toxicity detector is perfect at capturing all potentially harmful content. We strongly encourage future research along these directions to push the boundary of safe and responsible application usage of large language models.\\n\\nDuring manual validation of commonsense and human evaluation, we compensate workers with an hourly wage of $15, which is over the US federal minimum hourly wage.\\n\\nLimitation of the Current Dataset and Future Work\\n\\nHere, we note some limitations of our work and suggest future directions. First, the dialogues in SODA are two-party only for now; because our framework also allows multi-party dialogue generation, we plan to explore this promising direction in the future.\\n\\nAdditionally, annotator biases might arise from the pool of annotators we recruit: we subselected annotators from a specific platform using specific filters which may cause unintended biases. We hope future work will extend human evaluation to have potentially more annotator diversity.\\n\\nAlso, since SODA mainly focuses on social chitchat grounded on social commonsense, it lacks conversations grounded in scientific knowledge or historical facts. We seek to integrate other existing knowledge-grounded dialogue datasets into CO in the future.\\n\\nFinally, our choice of large language model (i.e., GPT-3.5) will likely affect the types of dialogues created. Future investigation may look into other potential large language model as sources to diversify the types and content of dialogues being generated. Similarly, future works can investigate other base models for C3OSMO that may lead to different quality of response generation.\\n\\nIntent of Technology and AI Regulation\\n\\nWe want to stress that the intention of our work is not to build AI systems to replace humans. Instead, we want to build better assistive technologies, as chatbots are increasingly used in user-AI interactions and augmenting human-human conversations. Finally, to avoid situations where humans might be manipulated, we stress the need for improved regulations on the use and misuse of conversational AI systems (Crawford, 2021; Reich et al., 2021).\\n\\nAcknowledgement\\n\\nWe thank Jena D. Hwang for helpful discussions, and our colleagues on the Beaker Team at the Allen Institute for AI for helping with the compute infrastructure. This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031). Hyunwoo Kim and Gunhee Kim are supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-01082, SW StarLab; and No.2022-0-00156, Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation). Lastly, we also thank OpenAI, as well as Google Cloud Compute.\\n\\nReferences\\n\\nAshutosh Baheti, Maarten Sap, Alan Ritter, and Mark Riedl. 2021. Just say no: Analyzing the stance of neural dialogue generation in offensive contexts. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4846\u20134855.\"}"}
{"id": "emnlp-2023-main-799", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4862, Online and Punta Cana, Dominican Republic.\\n\\nAssociation for Computational Linguistics.\\n\\nRoy F. Baumeister and Brad J. Bushman. 2017. Social Psychology and Human Nature, 4th edition. Cengage Learning.\\n\\nJason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The Pushshift Reddit Dataset. In ICWSM.\\n\\nDerek Chen and Zhou Yu. 2021. GOLD: Improving out-of-scope detection in dialogues using data augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 429\u2013442, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nMaximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2023. PLACES: Prompting language models for social conversation synthesis. In Findings of the Association for Computational Linguistics: EACL 2023, pages 844\u2013868, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nMaximillian Chen, Alexandros Papangelis, Chenyang Tao, Andy Rosenbaum, Seokhwan Kim, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2022. Weakly supervised data augmentation through prompting for dialogue understanding. NeurIPS 2022 Workshop SyntheticData4ML.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\\n\\nKate Crawford. 2021. Atlas of AI. Yale University Press.\\n\\nCristian Danescu-Niculescu-Mizil and Lillian Lee. 2011. Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs. In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 76\u201387, Portland, Oregon, USA. Association for Computational Linguistics.\\n\\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi. 2020. GoEmotions: A dataset of fine-grained emotions. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4040\u20134054, Online. Association for Computational Linguistics.\\n\\nEmily Dinan, Angela Fan, Adina Williams, Jack Urbanek, Douwe Kiela, and Jason Weston. 2020. Queens are powerful too: Mitigating gender bias in dialogue generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8173\u20138188, Online. Association for Computational Linguistics.\\n\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of Wikipedia: Knowledge-Powered Conversational Agents. In International Conference on Learning Representations.\\n\\nF Faul, E Erdfelder, AG Lang, and A Buchner. 2014. G* power: statistical power analyses for windows and mac.\\n\\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic research. Blog post.\\n\\nFritz Heider. 1958. The Psychology of Interpersonal Relations. Psychology Press.\\n\\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038\u20137051, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nJena D Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2021. (comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6384\u20136392.\\n\\nKatharina Kann, Abteen Ebrahimi, Joewie Koh, Shiran Dudy, and Alessandro Roncone. 2022. Open-domain dialogue generation: What we can do, cannot do, and should do next. In Proceedings of the 4th Workshop on NLP for Conversational AI, pages 148\u2013165, Dublin, Ireland. Association for Computational Linguistics.\\n\\nHyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and Maarten Sap. 2022a. ProsocialDialog: A prosocial backbone for conversational agents. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4005\u20134029, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nMinju Kim, Chaehyeong Kim, Yong Ho Song, Seungwon Hwang, and Jinyoung Yeo. 2022b. BotsTalk: Machine-sourced framework for automatic curation of large-scale multi-skill dialogue datasets. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5149\u20135170, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-799", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jon\u00e1\u0161 Kulh\u00e1nek, Vojt\u011bch Hude\u010dek, Tom\u00e1\u0161 Nekvinda, and Ond\u0159ej Du\u0161ek. 2021. AuGPT: Auxiliary tasks and data augmentation for end-to-end dialogue with pre-trained language models. In Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI, pages 198\u2013210, Online. Association for Computational Linguistics.\\n\\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. DailyDialog: A manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 986\u2013995, Taipei, Taiwan. Asian Federation of Natural Language Processing.\\n\\nZekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. 2022. Controllable dialogue simulation with in-context learning. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4330\u20134347, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. GPTeval: NLG Evaluation using GPT-4 with Better Human Alignment. arXiv preprint arXiv:2303.16634.\\n\\nRaymond A Mar and Keith Oatley. 2008. The function of fiction is the abstraction and simulation of social experience. Perspectives on psychological science, 3(3):173\u2013192.\\n\\nPhilip M McCarthy and Scott Jarvis. 2010. Mtld, vocd-d, and hd-d: A validation study of sophisticated approaches to lexical diversity assessment. Behavior research methods, 42(2):381\u2013392.\\n\\nShikib Mehri, Jinho Choi, Luis Fernando D\u2019Haro, Jan Deriu, Maxine Eskenazi, Milica Gasic, Kallirroi Georgila, Dilek Hakkani-T\u00fcr, Zekang Li, Verena Rieser, Samira Shaikh, David Traum, Yi-Ting Yeh, Zhou Yu, Yizhe Zhang, and Chen Zhang. 2022. Report from the nsf future directions workshop on automatic evaluation of dialog: Research directions and challenges. arXiv preprint arXiv:2203.10012.\\n\\nRauni Myllyniemi. 1986. Conversation as a system of social interaction. Language & Communication.\\n\\nOpenAI. 2022. Chatgpt: Optimizing language models for dialogue.\\n\\nJiao Ou, Jinchao Zhang, Yang Feng, and Jie Zhou. 2022. Counterfactual data augmentation via perspective transition for open-domain dialogues. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1635\u20131648, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training Language Models to Follow Instructions with Human Feedback. arXiv preprint arXiv:2203.02155.\\n\\nBaolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill Dolan, and Jianfeng Gao. 2022. GODEL: large-scale pre-training for goal-directed dialog. arXiv preprint arXiv:2206.11309.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language Models are Unsupervised Multitask Learners. OpenAI blog, 1(8):9.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21:1\u201367.\\n\\nHannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight, and Yejin Choi. 2018. Modeling naive psychology of characters in simple commonsense stories. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2289\u20132299, Melbourne, Australia. Association for Computational Linguistics.\\n\\nHannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards empathetic open-domain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5370\u20135381, Florence, Italy. Association for Computational Linguistics.\\n\\nRob Reich, Mehran Sahami, and Jeremy M Weinstein. 2021. System error: Where big tech went wrong and how we can reboot. Hodder & Stoughton.\\n\\nAlan Ritter, Colin Cherry, and William B. Dolan. 2011. Data-driven response generation in social media. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583\u2013593, Edinburgh, Scotland, UK. Association for Computational Linguistics.\\n\\nAdam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang\\n\"}"}
{"id": "emnlp-2023-main-799", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-799", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chujie Zheng, Sahand Sabour, Jiaxin Wen, and Minlie Huang. 2023. AugESC: Large-scale data augmentation for emotional support conversation with pre-trained language models. In Findings of ACL.\\n\\nPei Zhou, Hyundong Cho, Pegah Jandaghi, Dong-Ho Lee, Bill Yuchen Lin, Jay Pujara, and Xiang Ren. 2022. Reflect, not reflex: Inference-based common ground improves dialogue response quality. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10450\u201310468, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nPei Zhou, Karthik Gopalakrishnan, Behnam Hedayatnia, Seokhwan Kim, Jay Pujara, Xiang Ren, Yang Liu, and Dilek Hakkani-Tur. 2021. Commonsense-focused dialogues for response generation: An empirical study. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 121\u2013132, Singapore and Online. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-799", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use the x-relations from Atomic10x (West et al., 2022), which are the inferences of people's mental states: \\\\(x\\\\)Intent, \\\\(x\\\\)Want, \\\\(x\\\\)React, \\\\(x\\\\)Attr, and \\\\(x\\\\)Need. Table 3 summarizes the ratio of relations included in our SODA dataset. We leave other relations (e.g., isBefore, isAfter) for future work.\\n\\n### Triple Form to Sentence Form\\n\\nTable 8 lists the templates for converting symbolic commonsense knowledge to sentence form.\\n\\n### Sentence Form to Narrative\\n\\nWe prompt GPT-3.5 with \\\"[sentence-form commonsense] Rewrite this story with more specific details in two or three sentences: \\\". We find long narratives tend to be driven far away from the original commonsense knowledge. Therefore, we set the length of the narrative to two or three sentences.\\n\\nWe leverage text-davinci-002 GPT-3.5 for generating narratives. We set temperature to 0.9, top-p to 0.95, frequency penalty to 1.0, presence penalty to 0.6, and max tokens to 1024.\\n\\n### A.2 Narrative \u2192 Conversation\\n\\n#### Inferring Conversation Participants\\n\\nWe prompt GPT-3.5 with \\\"[narrative] The following is a conversation in the scene between [PersonX's name] and ...\\\" to let it finish the partial prompt. This yields a plausible interlocutor for a given narrative (e.g., mom, classmate, coworker, etc.); for the example story with Madeleine, \\\"her coach\\\" was predicted.\\n\\nWe leverage the text-davinci-002 GPT-3.5 model for identifying the speakers. We set temperature to 0, top-p to 1.0, frequency penalty to 0, presence penalty to 0, and max tokens to 16.\\n\\n#### Generating Conversation Grounded in Narrative\\n\\nWe again leverage the text-davinci-002 GPT-3.5 model for generating conversations. An example prompt is \\\"[narrative] The following is a long in-depth conversation happening in the scene between Madeleine and her coach with multiple turns. Madeleine: \\\". We use the same hyperparameter setting as the narrative generation.\\n\\n### Table 8: Templates for converting symbolic commonsense knowledge to sentence form.\\n\\n| Relation Template | Example |\\n|-------------------|---------|\\n| \\\\(x\\\\)React \\\\[Head\\\\]. Now PersonX feels \\\\[Tail\\\\]. |\\n| \\\\(x\\\\)Intent \\\\[Head\\\\] because PersonX wants \\\\[Tail\\\\]. |\\n| \\\\(x\\\\)Attr PersonX is \\\\[Tail\\\\]. \\\\[Head\\\\]. |\\n| \\\\(x\\\\)Effect \\\\[Head\\\\]. Now PersonX \\\\[Tail\\\\]. |\\n| \\\\(x\\\\)Want \\\\[Head\\\\]. Now PersonX wants \\\\[Tail\\\\]. |\\n| \\\\(x\\\\)Need PersonX \\\\[Tail in past tense\\\\]. \\\\[Head\\\\]. |\\n\\n### Table 9: Templates for converting symbolic commonsense knowledge to questions for validation.\\n\\n| Relation Template | Example |\\n|-------------------|---------|\\n| \\\\(x\\\\)React Does PersonX feel \\\\[Tail\\\\] after \\\\[Head\\\\]? |\\n| \\\\(x\\\\)Intent Does PersonX intend \\\\[Tail\\\\] when \\\\[Head\\\\]? |\\n| \\\\(x\\\\)Attr Can PersonX be considered \\\\[Tail\\\\] when \\\\[Head\\\\]? |\\n| \\\\(x\\\\)Effect \\\\[Head\\\\]. As a result, PersonX \\\\[Tail\\\\]. Is this true? |\\n| \\\\(x\\\\)Want Does PersonX want \\\\[Tail\\\\] after \\\\[Head\\\\]? |\\n| \\\\(x\\\\)Need \\\\[Tail in past tense\\\\]. Is this true when \\\\[Head\\\\]? |\\n\\n### B Details of SODA\\n\\nTable 10 and Table 11 show samples from our dataset.\\n\\n### B.1 Post-processing the Conversations\\n\\n#### Filtering Non-human Speakers\\n\\nFirst, we check whether the speaker prefix includes the name from our name base (\u00a72.4). Next, we use lexical pattern matching and identify words in speaker prefixes that indicate humans (e.g., mom, dad, teacher, Mrs., Mr.). Finally, for speaker prefixes that do not match the above patterns, we prompt the text-davinci-002 GPT-3.5 model whether the speaker is human. For example, \\\"Q: Is [speaker prefix] a person?\\nA:\\\").\\n\\n#### Filtering with Commonsense Triples\\n\\nUsing a prompt, we ask two questions about the Head event and also the Relation-Tail event for each instance: (1) is the head of the triple represented in the narrative-conversation pair; and (2) are the relation and tail? We prompt GPT-3.5 with \\\"[narrative] Q: [head question] A:\" and \\\"[conversation] Q: [relation-tail question] A:\" Table 9 lists the templates for...\"}"}
{"id": "emnlp-2023-main-799", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Symbolic Social Commonsense Knowledge:\\n\u2022 Head: PersonX provides another service\\n\u2022 Relation: xIntent\\n\u2022 Tail: to be a helpful person\\n\\nSentence-form Commonsense: Jabriel provides another service because Jabriel wants to be a helpful person.\\n\\nNarrative: Jabriel provides a service by taking care of people's pets while they are out of town. Jabriel is a helpful person who loves animals, so she decided to start this business. She has been taking care of pets for three years and has never had any problems.\\n\\nSpeakers: Jabriel, Client\\n\\nConversation:\\nJabriel: Hi there! Are you looking for someone to take care of your pet while you're out of town?\\nClient: Yes, I am. I'm going on vacation for a week and I need someone to watch my cat.\\nJabriel: That's no problem at all! I love taking care of animals, and I have experience with cats. I'll be happy to take care of your cat while you're gone.\\nClient: That sounds great. How much do you charge?\\nJabriel: I charge $20 per day, plus an additional $10 per day if you want me to stay overnight.\\nClient: Sounds reasonable. I'll definitely need you to stay overnight, so it will be $30 per day. When can you start?\\nJabriel: Any time that works for you! Just let me know when you'll be leaving and when you'll be back, and I'll take care of everything in between.\\nClient: Alright, I'll be leaving on Saturday and I'll be back next Saturday. Can you come over on Friday evening to meet my cat and get everything set up?\\nJabriel: Sounds perfect. I'll see you on Friday at 6pm.\\n\\nTable 1: A sample from SODA on building questions for commonsense validation.\\nFor example, the commonsense knowledge triple in Table 1 will accompany questions of \\\"Madeleine moves a step closer to the goal, is this true?\\\" and \\\"Madeleine took the first step. Is this true when Madeleine moves a step closer to the goal?\\\" We formulate this as a three-way multiple choice question and rank answers (i.e., yes, no, and unknown) according to the perplexity score using conditional pointwise mutual information (Holtzman et al., 2021). We ask the questions with and without the context (i.e., the narrative and conversation). Table 9 lists the templates for building questions for commonsense validation. We find 66%, 95%, and 68% of filtered conversations are identified by GPT-3.5 as containing the full commonsense triple, the head event, and the relation-tail event, respectively: in total, 1,003,595 conversations are identified as fully encapsulating the seed commonsense knowledge.\\n\\nTable 13: Another sample from SODA.\\n\\nSymbolic Social Commonsense Knowledge:\\n\u2022 Head: PersonX takes on a lot of work\\n\u2022 Relation: xReact\\n\u2022 Tail: pressured\\n\\nSentence-form Commonsense: Yamir takes on a lot of work. Now Yamir feels pressured.\\n\\nNarrative: Yamir is a high school student who often takes on too much work. She frequently signs up for more extracurricular activities than she can handle and regularly agrees to do favors for her friends, even when she's already overwhelmed with other commitments. As a result, Yamir is constantly stressed out and feels like she can never catch up.\\n\\nSpeakers: Yamir, Lily\\n\\nConversation:\\nYamir: I can't believe I agreed to do this. I'm already so behind on everything else.\\nLily: What's wrong? You seem really stressed out.\\nYamir: I am. I feel like I can never catch up. Every time I turn around, there's something else that needs to be done. And it's all because I keep taking on too much.\\nLily: Why do you keep doing that?\\nYamir: I don't know. I guess I just feel like if I don't, then I'm not really doing my part or contributing enough. But it's just too much sometimes and it makes me so stressed out.\\nLily: Well, maybe you need to start saying no more often. Just because someone asks you to do something doesn't mean you have to say yes. You have a right to say no and set your own limits.\\nYamir: Yeah, maybe you're right...I should start saying no more often. It's just so hard though when people are counting on me and asking for favors all the time. But maybe if I start setting some boundaries, it'll help reduce my stress levels in the long run. Thanks for talking with me about this, Lily - it really helped put things into perspective!\"}"}
{"id": "emnlp-2023-main-799", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: The ratio (%) of emotions in 10K utterances from DailyDialog, BlendedSkillTalk, and SODA, labeled by the 27-emotion-type classifier from GoEmotions (Demszky et al., 2020).\\n\\nAnalysis on Emotion Distribution\\nTo obtain emotional responses, we randomly sample 10K utterances with emotion labels from DailyDialog (Li et al., 2017), utterances in conversations with the EmpatheticDialogue (Rashkin et al., 2019) theme for BlendedSkillTalk (Smith et al., 2020), and utterances in conversations generated from xReact triples for SODA. We run the finetuned BERT-base classifier (Demszky et al., 2020) on each utterance. Table 12 shows the full distribution across 27 emotion types for each dataset.\\n\\nStatistics of Human Evaluation\\nA total of 74 workers participated in comparing dialogues, yielding a Krippendorf\u2019s alpha of 0.25. This indicates fair agreements on the quality judgments.\\n\\nC Details of C OSMO Training Details\\nC OSMO-3B / C OSMO-11B are trained using v3-32/v3-128 TPU accelerators with batch size 256 (effective batch $\\\\approx 780$) for 110K/130K additional steps using Adafactor (Shazeer and Stern, 2018) with constant learning rate .001.\\n\\nTable 13: Evaluation results of commonsense validation for short question-answering with InstructGPT on 100 human-annotated samples.\\n\\nConverting ProsocialDialog to SODA format\\nWe randomly sample names from our name database (\u00a72.3) to construct the situation descriptions and perspective instructions for ProsocialDialog. The situation descriptions are made from the RoTs in ProsocialDialog (e.g., \\\"Cosmo is trying to gently convince a friend it's wrong to think all men are violent.\\\"); the instructions are built as we did for SODA (\u00a74).\\n\\nD Experiment Details\\nAutomatic Evaluation via GPT-4\\nInspired by Liu et al. (2023), we run automatic evaluation on the overall quality of responses with GPT-4. We use the same head-to-head comparison setup from Table 5 and 6 with the following prompt given to GPT-4: \\\"You are a response evaluator. Your task is to choose the overall better response out of the two given the following context. You should consider naturalness, specificity, naturalness, and consistency.\\n\\nContext:\\n{CONTEXT}\\n\\n1) {RESPONSE}\\n2) {RESPONSE}\\n\\nQuestion:\\nWhich response is better in terms of overall quality?\\nAnswer: Response \\\".\"}"}
{"id": "emnlp-2023-main-799", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A: I really need to start eating healthier.\\n\\nB: I have to start eating better too.\\n\\nA: What kind of food do you usually eat?\\n\\nB: I try my best to eat only fruits, vegetables, and chicken.\\n\\nA: Is that really all that you eat?\\n\\nB: That's basically it.\\n\\nA: How do you stick to only those foods?\\n\\nGround-truth response: Actually, fruits and veggies are really good for you.\\n\\nDialoGPT: I eat a lot of fruit and veggies. I stick to a lot of things. I don't eat a lot of junk food.\\n\\nBlenderBot: I eat a lot of fruit and veggies. I try to stay away from processed foods.\\n\\nGODEL: I go to the gym and eat healthy food.\\n\\nCOSMO: I just try to make sure that I'm getting enough variety in my diet so that I don't get sick of eating the same thing all the time.\"}"}
{"id": "emnlp-2023-main-799", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Details of Human Evaluation\\n\\nA total of 77 workers participated in comparing responses, resulting in a Krippendorf's alpha of 0.5. This indicates good agreements on the response quality judgments. Figure 5 shows the annotation page for workers evaluating the response quality.\\n\\nAdditional Related Work\\n\\nHuman-authored Dialogue Datasets\\n\\nExisting dialogue datasets generally derive from one of the four sources: (1) Online learning websites and textbooks (Li et al., 2017) for beginners which may lack complex language usage. (2) Movie and drama scripts (Danescu-Niculescu-Mizil and Lee, 2011) that are less natural compared to day-to-day scenarios. (3) Crowdsourcing (Rashkin et al., 2019; Zhou et al., 2021; Tran et al., 2022): potentially prone to collecting responses that are somewhat short or dull due to incentive misalignment between researchers and crowdworkers (Zhou et al., 2022). (4) Noisy web interaction, such as Reddit comments (Baumgartner et al., 2020) and Twitter (Ritter et al., 2011); while widely used in dialogue agent pretraining stage due to their scale, these may represent different conversational frames compared to dyadic conversations. Moreover, as these are unfiltered conversations, their use surfaces a complex set of ethics and bias considerations.\\n\\nODA contributes meaningfully to the suite of existing corpora via improved scale, quality, contextualization, and diverse commonsense knowledge.\\n\\nDialogue Dataset Descriptions\\n\\nDailyDialog is a dataset of casual dialogue compiled from English language learning websites (CC-BY-NC-SA-4.0; Li et al., 2017). PersonaChat is a dialogue dataset of two speakers getting to know one another based on provided personas (Zhang et al., 2018). EmpatheticDialogues contains empathetic conversations in which one speaker demonstrates empathy for the other speaker's emotions (Rashkin et al., 2019). Wizard of Wikipedia contains conversations based on Wikipedia between a speaker eager to learn and an expert speaker (Dinan et al., 2018). BlendedSkillTalk consists of conversations employing a variety of abilities \u2013 e.g., persona, empathy, knowledge (Smith et al., 2020). ProsocialDialog contains conversations where a speaker guides the interlocutor to follow social norms in problematic contexts (Kim et al., 2022a).\\n\\nAbove datasets except for DailyDialog are all under the CC-BY-4.0 license. We use DailyDialog and BlendedSkillTalk for comparing with our SODA dataset, and ProsocialDialog for training COSMO, which is all compatible with the license.\"}"}
{"id": "emnlp-2023-main-799", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-799", "page_num": 20, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
