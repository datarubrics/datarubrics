{"id": "acl-2022-long-84", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension\\n\\nHuibin Zhang1, Zhengkun Zhang1, Yao Zhang1, Jun Wang2,\u2217 Yufan Li1\\nNing Jiang3, Xin Wei3, Zhenglu Yang1\u2217\\n\\n1TKLNDST, CS, Nankai University, China, 2Ludong University, China, 3Mashang Consumer Finance Co., Ltd., China\\n\\n{zhanghuibin.cc,zhangzk2017,yaozhang,junwang,1811486}@mail.nankai.edu.cn,{ning.jiang02,xin.wei02}@msxf.com, yangzl@nankai.edu.cn\\n\\nAbstract\\n\\nProcedural Multimodal Documents (PMDs) organize textual instructions and corresponding images step by step. Comprehending PMDs and inducing their representations for the downstream reasoning tasks is designated as Procedural MultiModal Machine Comprehension (M\u2083C). In this study, we approach Procedural M\u2083C at a fine-grained level (compared with existing explorations at a document or sentence level), that is, entity. With delicate consideration, we model entity both in its temporal and cross-modal relation and propose a novel Temporal-Modal Entity Graph (TMEG). Specifically, a heterogeneous graph structure is formulated to capture textual and visual entities and trace their temporal-modal evolution. In addition, a graph aggregation module is introduced to conduct graph encoding and reasoning. Comprehensive experiments across three Procedural M\u2083C tasks are conducted on a traditional dataset RecipeQA and our new dataset CraftQA, which can better evaluate the generalization of TMEG.\\n\\n1 Introduction\\n\\nMultiModal Machine Comprehension (M\u2083C) is a generalization of machine reading comprehension by introducing multimodality. Due to its differences from Visual Question Answering (VQA) (Antol et al., 2015) in the form of understanding multimodal contexts and conducting multimodal questions and answers, there has been a lot of attention in recent years devoted to this field. In this paper, we investigate a task that has been typical of M\u2083C recently, named Procedural M\u2083C, a task of reading comprehension of Procedural Multimodal Documents (PMDs).\\n\\nAs shown in Figure 1, a recipe that contains successive multimodal instructions is a typical PMD. Reading a recipe seems trivial for humans but is still complex for a machine reading comprehension system before it can comprehend both textual and visual contents and capture their relations. Current Procedural M\u2083C studies (Yagcioglu et al., 2018; Liu et al., 2020) comprehend PMDs by encoding text and images at each procedure step. These efforts, however, only scratch the surface and lack deep insight into the elementary unit of PMDs, that is, entity. From now on, we use entity to refer uniformly to entity in text and object in image. For instance, the recipe in Figure 1 involves multiple entities, i.e., Strawberry and Sugar Cookie Dough, etc. In this work, we target at approaching the Procedural M\u2083C task at a fine-grained entity level.\\n\\nWe observe that a PMD essentially assembles an evolution process of entities and the relations between them. Specifically, the relation between entities can be summarized in the following two categories:\\n\\n\u2022 Temporal Relation. The state of an entity may change as steps progress. Still looking at Figure 1, strawberries are complete at step 1 and...\"}"}
{"id": "acl-2022-long-84", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"by step 4 they are washed and cut into pieces.\\n\\nWe use temporal relation to depict the association between an entity's changing visual signals in images or changing contexts in text.\\n\\n- Multimodal Relation. An entity is naturally and powerfully associated with other entities within a single modality. Meanwhile, the cross-modal association of an entity is worth exploiting to contribute to distinct modality understanding. For example, the visual signal and context about sugar cookie dough can be interpreted by each other at step 2. We generalize the intra- and inter-modal associations of entities with the multimodal relation.\\n\\nBased on the above observations, we believe that simultaneously modeling entities and the temporal and multimodal relations is a key challenge in understanding PMDs. Recent efforts (Amac et al., 2019; Huang et al., 2021) are devoted to encoding temporal relations of entities, while it neglects the multimodal relation. Heterogeneous graphs have become the preferred technology for representing, sharing, and fusing information to modern AI tasks, e.g., relation extraction (Christopoulou et al., 2019) and recommendation (Fan et al., 2019). Inspired by this, we construct a heterogeneous graph, with entities as nodes and relations as edges. Therefore, the research goal of the procedural M$^3$C task will shift from understanding unstructured PMDs to learning structured graph representations.\\n\\nIn this work, we propose a novel Temporal-Modal Entity Graph model, namely TMEG. Our model approaches Procedural M$^3$C at a fine-grained entity level by constructing and learning a graph with entities and temporal and multimodal relations. Specifically, TMEG consists of the following components:\\n\\n1) **Node Construction**, which extracts the token-level features in text and object-level features in images as the initial entity embeddings;\\n2) **Graph Construction**, which constructs the temporal, intra-modal, and cross-modal relations separately to form a unified graph;\\n3) **Graph Aggregation**, which utilizes the graph-based multi-head attention mechanism to perform fusion operations on graphs to model the evolution of entities and relations. Finally, the graph representation is fed into a graph-based reasoning module to evaluate the model's understanding ability.\\n\\nIn addition, in order to further advance the research of Procedural M$^3$C, we release CraftQA, a multimodal semantically enriched dataset that contains about 27k craft product-making tutorials and 46k question-answer pairs for evaluation. We evaluate three representative subtasks, i.e., visual cloze, visual coherence, and visual ordering, on CraftQA and a public dataset RecipeQA (Yagcioglu et al., 2018). The quantitative and qualitative results show the superiority of TMEG compared to the state-of-the-art methods on all three tasks. The main contributions of this paper can be summarized as follows:\\n\\n- We innovatively study the Procedural M$^3$C task at a fine-grained entity level. We comprehensively explore the relations between entities in both temporal and multimodal perspectives.\\n- We propose a Temporal-Modal Entity Graph model TMEG, which constructs a graph with entities as nodes and relations as edges, and then learns the graph representation to understand PMDs.\\n- We release a dataset CraftQA. The experimental results on CraftQA and RecipeQA show TMEG outperforms several state-of-the-art methods.\\n\\n**2 Related Work**\\n\\n**Procedural Text Comprehension.** Procedural text comprehension requires an accurate prediction for the state change and location information of each entity over successive steps. Several datasets have been proposed to evaluate procedural text comprehension, e.g., Recipe (Bosselut et al., 2018), ProPara (Mishra et al., 2019), and OPENPI (Tandon et al., 2020). To model entity evolution, KG-MRC (Das et al., 2019) constructs knowledge graphs via reading comprehension and uses them for entity location prediction. DYNAPRO (Amini et al., 2020) introduces a pre-trained language model to dynamically obtain the contextual embedding of procedural text and learn the attributes and transformations of entities. ProGraph (Zhong et al., 2020) enables state prediction from context by constructing a heterogeneous graph with various knowledge inputs. KoalA (Zhang et al., 2021b) utilizes the external commonsense knowledge injection and data enhancement to reason the states and locations of entities. TSLM (Faghihi and Kordjamshidi, 2021) formulate comprehension task as a question answering problem and adapt pre-trained transformer-based language models on other QA benchmarks. REAL (Huang et al., 2021) builds...\\n\\n- **We innovatively study the Procedural M$^3$C task at a fine-grained entity level.** We comprehensively explore the relations between entities in both temporal and multimodal perspectives.\\n- **We propose a Temporal-Modal Entity Graph model TMEG, which constructs a graph with entities as nodes and relations as edges, and then learns the graph representation to understand PMDs.**\\n- **We release a dataset CraftQA.** The experimental results on CraftQA and RecipeQA show TMEG outperforms several state-of-the-art methods.\"}"}
{"id": "acl-2022-long-84", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Temporal Modal Graph Aggregation\\nNode Encoding\\nGraph-Based Multi-Head Attention\\nAdd & Norm\\nFeed Forward\\n\\nText Input\\nStep 1:\\nIngredients: 16 oz strawberry, 9 oz blueberries, \u2026\\nStep 2:\\nMake or buy your sugar cookie dough. \u2026\\n\\nFigure 2: Overview of our proposed TMEG framework. We reuse the example in Figure 1. Initial nodes are generated from input text and images. Considering the temporal and cross-modal relation of entities, we apply various types of edges to construct a unified temporal-modal entity graph. By combining the encoding of nodes and edges, a graph aggregation module is designed to conduct graph encoding and reasoning.\\n\\nInspired by these previous works, we propose a temporal-modal entity graph model, which is designed with temporal encoding and modal encoding to model multiple types of entities.\\n\\nMultimodal Graph. In recent multimodal research, graph structure has been utilized to model the semantic interaction between modalities. (Yin et al., 2020) propose a graph-based multimodal fusion encoder for neural machine translation, which converted sentence and image in a unified multimodal graph. (Khademi, 2020) convert image regions and the region grounded captions into graph structure and introduced graph memory networks for visual question answering. (Zhang et al., 2021a) propose a multimodal graph fusion approach for named entity recognition, which conducted graph encoding via multimodal semantic interaction. (Yang et al., 2021) focus on multimodal sentiment analysis and emotion recognition, which unified video, audio, and text modalities into an attention graph and learned the interaction through graph fusion, dynamic pruning, and the read-out technique.\\n\\nIn contrast to the above methods, we formulate our multimodal graph on temporal entities and successfully deploy it in Procedural M\u00b3C.\\n\\n3 Proposed Method\\nIn this section, we introduce: (1) problem definition of Procedural M\u00b3C in Section 3.1; (2) The homogeneous graph of each textual instruction (image) and our TMEG in Section 3.2 and Section 3.3, respectively; and (3) graph aggregation module to conduct graph encoding and reasoning in Section 3.4. Figure 2 gives a high-level overview of TMEG.\\n\\n3.1 Problem Definition\\nHere, we define the task of Procedural M\u00b3C, given:\\n\\n\u2022 Context \\\\( S = \\\\{s\\\\}^N_{nt=1} \\\\) in textual modality, which represents a series of coherent textual instructions to perform a specific skill or task (e.g., multiple steps to complete a recipe or a craft product).\\n\u2022 Question \\\\( Q \\\\) and Answer \\\\( A \\\\), which is either a single image or a series of images in a reasoning task (e.g., visual cloze, visual coherence, or visual ordering).\\n\\nFollowing (Liu et al., 2020), we combine the images contained in \\\\( Q \\\\) and \\\\( A \\\\) to form \\\\( N_c \\\\) candidate image sequences \\\\( \\\\{a_1, \\\\ldots, a_j, \\\\ldots, a_{N_c}\\\\} \\\\). Let \\\\( N_a \\\\) be the length of the \\\\( j \\\\)-th candidate image sequence \\\\( a_j = \\\\{I_{j,1}, \\\\ldots, I_{j,N_a}\\\\} \\\\). Take the visual cloze task as an example, we fill the placeholder of the question with candidate answers to form \\\\( N_c \\\\) image sequences with length \\\\( N_a = 4 \\\\). The model requires to select the most relevant candidate by calculating the similarity between text sequence \\\\( S = \\\\{s\\\\}^N_{nt=1} \\\\) and each image sequence \\\\( a_j \\\\).\\n\\n3.2 Homogeneous Graph Construction\\nAs shown in Figure 2, we first extract the tokens (objects) in text (image) as the initial nodes of homogeneous graph, respectively.\\n\\nTextual Node. Let \\\\( N_t \\\\) be the number of textual instructions \\\\( S = \\\\{s\\\\}^N_{nt=1} \\\\). First, each instruction \\\\( s_t \\\\) is encoded as a textual token and added to the textual node set. Then, we add to each textual node \\\\( s_t \\\\) an edge connecting to all visual nodes \\\\( I_{j,1}, \\\\ldots, I_{j,N_a} \\\\) that mention the same object in the image sequence. The textual node set \\\\( N_t \\\\) includes all instructions and their objects.\\n\\nVisual Node. Let \\\\( N_v \\\\) be the number of visual nodes. Each visual node \\\\( I_{j,1}, \\\\ldots, I_{j,N_a} \\\\) in an image sequence is encoded as a visual token and added to the visual node set. Furthermore, a temporal edge is added between each pair of visual tokens to capture the temporal relation.\\n\\nTemporal Edge. The temporal edge \\\\( e_{ij} \\\\) connects two visual tokens \\\\( I_{j,1}, I_{j+1,1} \\\\) if there is a temporal relation between them. For example, in the recipe task, we can add a temporal edge to indicate the order of steps.\\n\\nGraph Construction. The graph structure of the homogeneous graph is constructed by connecting textual nodes to visual nodes with edges. The homogeneous graph is a directed graph with textual nodes and visual nodes as vertices and edges as temporal relations.\\n\\nHomogeneous Graph. In the homogeneous graph, each textual instruction \\\\( s_t \\\\) is connected to the visual nodes \\\\( I_{j,1}, \\\\ldots, I_{j,N_a} \\\\) that mention the same object. The edge set \\\\( E \\\\) includes all textual-to-visual edges.\\n\\nHeterogeneous Graph. In the heterogeneous graph, we add multi-modal edges to connect textual nodes and visual nodes based on the cross-modal relation. The edge set \\\\( E \\\\) includes all textual-to-visual edges and cross-modal edges.\\n\\nGraph Aggregation Module. The graph aggregation module is designed to conduct graph encoding and reasoning. It takes the homogeneous graph and heterogeneous graph as input and outputs the final representation of the graph. The module includes a graph encoding layer, a graph reasoning layer, and a graph fusion layer.\\n\\nGraph Encoding. The graph encoding layer is a graph neural network (GNN) that encodes the node and edge information in the homogeneous graph and heterogeneous graph. The GNN takes the node features and edge features as input and outputs the node representations.\\n\\nGraph Reasoning. The graph reasoning layer is a message-passing network (MPN) that propagates the node representations in the graph. The MPN takes the node representations as input and outputs the updated node representations.\\n\\nGraph Fusion. The graph fusion layer combines the node representations from the homogeneous graph and heterogeneous graph. The fusion layer takes the node representations from both graphs as input and outputs the final node representations.\\n\\nThe TMEG framework can be used in various applications, such as visual question answering, visual sentiment analysis, and visual coherence tasks. By modeling the temporal and cross-modal relation of entities, the TMEG framework can effectively capture the semantic interaction between modalities and improve the performance of multimodal tasks.\"}"}
{"id": "acl-2022-long-84", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"struction st is tokenized into a token sequence \\\\{et[CLS] ,et1 ,...et[SEP] \\\\}, where [CLS] and [SEP] are the special tokens introduced to mark the start and the end of each instruction. Then, we utilize an off-the-shelf POS tagger (Akbik et al., 2018) to identify all nouns and noun phrases in the token sequence. Finally, we concatenate all the token sequences of textual instructions $S$ and feed the token embedding into the graph aggregation module.\\n\\nVisual Node. For each image sequence $a_j$, we employ a pre-trained Faster-RCNN to extract a set \\\\{ev[CLS] ,ev1 ,...evk\\\\} with $k$ object features as visual tokens. Following (Messina et al., 2020; Dosovitskiy et al., 2021), we reserve [CLS] as the beginning token for each image whose final embedding is regarded as the representation of the whole image. The operation of visual node is in a similar manner as textual and any two nodes in the same instruction (image) are connected to construct a homogeneous graph.\\n\\n3.3 Heterogeneous Graph Construction\\n\\nBased on the homogeneous graph of each textual instruction (image), we introduce various types of edges to construct our heterogeneous graph TMEG.\\n\\n3.3.1 Temporal Edge\\n\\nIt is essential to model the temporal evolution of entities for comprehending procedural content. Let us revisit the example in Figure 1. When a human reads step 4, the connection between entities (e.g., strawberries and oranges) and their descriptions in step 1 is naturally established. We design the temporal edge to model the evolution of entities in text and image. It can be seen that the temporal edge describes the evolution at different steps. For the textual nodes, the same entity appearing in different steps are connected by a textual temporal edge (node-based). While for the visual nodes, we directly calculate the Euclidean Distance between object features due to the absence of accurate object detection. Following (Song et al., 2021), if the distance between node (object) $i$ and node (object) $j$ is less than a threshold $\\\\lambda_t$, we treat them as the same object and connect node $i$ to node $j$ via a visual temporal edge (node-based). Meanwhile, we consider that there may also be temporal evolution for edges, such as changes in the relationship between entities. Therefore, we also introduce temporal edge (edge-based) to characterize the change of edges.\\n\\n3.3.2 Modal Edge\\n\\nAs shown in Figure 1, the textual instruction of each image can be viewed as a noisy form of image annotation (Hessel et al., 2019). The association between image and sentence can be inferred through entity representations under different modalities. Correspondingly, we design the intra-modal edge and the inter-modal edge to represent the modal interactions. In Section 3.2, any two nodes in the same modality and the same instruction (image) are connected by an intra-modal edge. It is worth noting that for each instruction (image), the special [CLS] node is connected to all other nodes in order to aggregate graph-level features. On the other hand, the textual node representing any entity and the corresponding visual node are connected by an inter-modal edge. We employ a visual grounding toolkit (Yang et al., 2019) to detect visual objects for each noun phrase. Specifically, we predict the bounding box corresponding to the text entity and compute the Intersection over Union (IoU) between all visual objects. If the IoU between the prediction box and the visual box exceeds a threshold $\\\\lambda_m$, the textual node and the corresponding visual node are connected by an inter-modal edge (node-based). Similar to section 3.3.1, considering the influence of entity-relationship under different modalities, we also introduce inter-modal edge (edge-based) to characterize the interaction between edges.\\n\\n3.4 Graph Aggregation\\n\\n3.4.1 Node Encoding\\n\\nAs described in Section 3.2, we have obtained the embeddings of the textual tokens and visual objects. Similar to (Li et al., 2020), all embeddings are mapped to a set of initial node embeddings, and each node embedding is the sum of 1) a textual token embedding or visual object embedding; 2) a position embedding that identifies the position of the token or object in the image; and 3) a segment embedding generated from the step number in PMD which indicates different textual instructions or images.\\n\\n3.4.2 Edge Encoding\\n\\nTo encode the structural information into TMEG, we consider the temporal encoding and the modal encoding separately. For any two nodes $v_i$ and $v_j$ in $G$, we represent them as $v_i = (\\\\text{node}_i, \\\\text{representation}_i)$ and $v_j = (\\\\text{node}_j, \\\\text{representation}_j)$, where $\\\\text{node}_i$ and $\\\\text{node}_j$ are the node types (e.g., textual or visual), and $\\\\text{representation}_i$ and $\\\\text{representation}_j$ are the representations of the tokens or objects.\\n\\nFor textual edges, we represent them as $e_{ij} = (\\\\text{node}_i, \\\\text{node}_j, \\\\text{representation}_{ij})$, where $\\\\text{node}_i$ and $\\\\text{node}_j$ are the node types, and $\\\\text{representation}_{ij}$ is the representation of the textual edge. For visual edges, we represent them as $e_{ij} = (\\\\text{node}_i, \\\\text{node}_j, \\\\text{representation}_{ij})$, where $\\\\text{node}_i$ and $\\\\text{node}_j$ are the node types, and $\\\\text{representation}_{ij}$ is the representation of the visual edge.\\n\\nWe exploit the bounding box feature extracted by Faster-RCNN as the position embedding of the object.\"}"}
{"id": "acl-2022-long-84", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TMEG, we construct two mappings: $\\\\phi_t(v_i,v_j) \\\\rightarrow \\\\mathbb{R}$ and $\\\\phi_m(v_i,v_j) \\\\rightarrow \\\\mathbb{R}$ which encode the temporal edge and the modal edge between them. The temporal encoding and the modal encoding of the total graph are fed into the graph-based aggregation module.\\n\\n3.4.3 Graph-Based Fusion\\n\\nAs shown in the right part of Figure 2, we first introduce two multi-layer perceptrons (MLP) with Tanh activation function to project different node embeddings from two modalities into the same space. Then, we extend the VisualBERT (Li et al., 2020) to the graph-based fusion layer, which concatenates the node embeddings from MLPs as input and outputs their graph-based joint representations. Specifically, in each fusion layer, updating the hidden states of textual node and visual node mainly involve the following steps.\\n\\nFirstly, we exploit a graph-based multi-head attention mechanism to generate contextual representations of nodes. Formally, the output of the $h$-th attention head in the $(l-1)$ layer can be obtained as follows:\\n\\n$$A(q, k, v)_{h,l}^{j} = \\\\sum_{i=1}^{N} v_{h,i}(\\\\text{Softmax}(e_{h,i,j})),$$\\n\\n$$e_{h,i,j} = q_h j (k_h i)^T \\\\sqrt{d_k} + b_h \\\\phi_m(i,j) + b_h \\\\phi_t(i,j),$$\\n\\nwhere $q, k, v$ are the query matrix, key matrix, and value matrix generated from the hidden state $H_{l-1}$ of nodes in the $(l-1)$ layer. $\\\\phi_t(i,j)$ and $\\\\phi_m(i,j)$ denote the temporal encoding and the modal encoding of TMEG, which serve as bias terms in the attention module. It is worth noting that each head in the multi-head attention mechanism exhibits a broad range of behaviors (Vig, 2019); thus, we add different temporal encoding and modal encoding separately for each attention head. Meanwhile, in order to model the relationship of edges, the temporal encoding and the modal encoding are learned separately for each layer.\\n\\nWe concatenate the output of each head and pass them to a position-wise Feed Forward Networks (FFN) which is preceded and succeeded by residual connections and normalization layer (LN),\\n\\n$$\\\\hat{H}_l = \\\\text{LN}(W[A_1,...,A_h] + H_{l-1})$$\\n\\n$$H_l = \\\\text{LN}(\\\\text{FFN}(\\\\hat{H}_l) + \\\\hat{H}_l),$$\\n\\nwhere $W$ is a learnable parameter and $[\\\\cdot]$ denotes the concatenation manipulation. Finally, based on TMEG, we stack multi-layer graph-based fusion layers to conduct graph encoding. Algorithm 1 shows the aggregation of TMEG in detail.\\n\\n3.4.4 Graph-Based Reasoning\\n\\nAs mentioned in Section 3.2, we regard the hidden state of [CLS] as the representations of each instruction (image), where their final hidden states $H_T$ and $H_V$ are passed into the graph reasoning module for task completion. Firstly, we leverage the one-to-one correspondence between instruction and image, e.g., each instruction has an image to visualize it (Alikhani et al., 2021). TMEG involves a Contrastive Coherence Loss for keeping the alignment between instruction and image. Let $H_{V,+}$ and $H_{V,-}$ represent the positive and negative examples, the loss $L_{\\\\text{Coh}}$ of the $i$-th step can be defined as follows:\\n\\n$$L_{\\\\text{Coh}}^i = -\\\\log \\\\exp\\\\left\\\\{ \\\\frac{\\\\text{sim}(H_T^i, H_{V,+}^i)}{\\\\tau} \\\\right\\\\} \\\\sum_{j=1}^{K} \\\\exp\\\\left\\\\{ \\\\frac{\\\\text{sim}(H_T^i, H_{V,-}^j)}{\\\\tau} \\\\right\\\\},$$\\n\\nwhere $K$ is the total number of negative samples (He et al., 2020) generated from the min-batch, $\\\\text{sim}(\\\\cdot, \\\\cdot)$ and $\\\\tau$ are the standard cosine similarity function and temperature.\\n\\nIn a downstream reasoning task, the model needs to predict the correct candidate $a_j = \\\\{I_j, 1, ... I_j, N\\\\}$ based on the instructions $S = \\\\{s_t\\\\}_{N_t=1}$. Referring to the sentence image prediction...\"}"}
{"id": "acl-2022-long-84", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In (Li et al., 2020), we concatenate all representations of each candidate image sequence to generate a instruction candidate pair as:\\n\\n\\\\[(S, a_j) = [\\\\text{CLS}, H_T^1, \\\\ldots, H_T^N_t, \\\\text{SEP}, H_V^1, \\\\ldots, H_V^N_a]\\\\]\\n\\nwhere [CLS] and [SEP] are special tokens as used in (Li et al., 2020). We pass this input through a shallow transformer followed by a fully connected layer to obtain the prediction score \\\\(P(S, a_j)\\\\) for the \\\\(j\\\\)-th candidate, and the prediction loss can be defined as:\\n\\n\\\\[\\nL_{\\\\text{Pre}} = -\\\\log \\\\exp \\\\left( P(S, a_j) \\\\right) \\\\sum_{i=1}^{N_a-1} \\\\exp \\\\left( P(S, a_i) \\\\right),\\n\\\\]\\n\\n(5)\\n\\nwhere \\\\(a_j\\\\) is the correct candidate and \\\\(N_a\\\\) is the number of candidates. We get the final loss function and optimize it through the Adam optimizer:\\n\\n\\\\[\\nL = L_{\\\\text{Pre}} + \\\\lambda b L_{\\\\text{Coh}},\\n\\\\]\\n\\n(6)\\n\\nwhere \\\\(\\\\lambda b\\\\) is the balance parameter. Unless otherwise specified, all the results in this paper use \\\\(\\\\lambda b = 0.1\\\\) which we find to perform best.\\n\\n4 Experiments\\n\\n4.1 Datasets and Metrics\\n\\nRecipeQA.\\n\\nRecipeQA (Yagcioglu et al., 2018) is a multimodal comprehension dataset with 20K recipes approximately and more than 36K question-answer pairs. Unlike other multimodal reading comprehension datasets (Tapaswi et al., 2016; Iyyer et al., 2017; Kembhavi et al., 2017) analyze against movie clips or comics, RecipeQA requires reasoning real-world cases.\\n\\nCraftQA.\\n\\nWe collect CraftQA from Instructables, which is an online community where people can share their tutorials for accomplishing a task in a step-by-step manner. Specifically, we collect the most visited tutorials and remove those that contain only text or video. For question and answer generation, we also remove the tutorials that contain less than 3 images. To construct the distractor of each task, we compute the Euclidean distance between the image features that are extracted from a pretrained ResNet-50 (He et al., 2016). Taking the visual cloze task as an example, the distractor is sampled from the nearest neighbors of the ground-truth image based on Euclidean distance. Finally, CraftQA contains about 27k craft product-making tutorials and 46k question-answer pairs. We employ CraftQA to evaluate the reading comprehension performance of TMEG in different domains as well as its domain transfer capability. More statistics about these two datasets are shown in Table 1.\\n\\n| Dataset   | Statistics               |\\n|-----------|--------------------------|\\n|           | Train | Valid | Test |\\n| RecipeQA  | # of recipes | avg. # of steps | avg. # of words | avg. # of images |\\n|           | 15,847 | 5.99  | 443.01 | 12.67  |\\n| CraftQA   | # of tutorials | avg. # of steps | avg. # of words | avg. # of images |\\n|           | 21,790 | 7.53  | 535.88 | 20.14  |\\n\\nTable 1: Statistics of RecipeQA and CraftQA dataset. Each dataset is split into training, validation, and test sets based on the number of recipes or craft-making tutorials. We also provide the average count of steps, images, and words contained in each split dataset.\\n\\nMetric.\\n\\nIn three Procedural M^3C tasks that are tested in the following experiments (visual cloze, visual coherence, and visual ordering), we use classification accuracy as the evaluation metric, which is defined as the percentage of yielding the ground-truth answer during testing (Yagcioglu et al., 2018; Amac et al., 2019; Liu et al., 2020).\\n\\n4.2 Implementation Details\\n\\nFor visual node construction, we employ the pretrained Faster R-CNN (Ren et al., 2015) model provided by Detectron2 (Wu et al., 2019) and limit the number of objects to 36 for each image. Following (Yang et al., 2019; Song et al., 2021), we set the thresholds \\\\(\\\\lambda t\\\\) and \\\\(\\\\lambda m\\\\) as 7 and 0.5, respectively, for the temporal and the modal edge constructions.\\n\\nThe framework of the graph-based fusion module is built on VisualBERT (Li et al., 2020) with its initialized parameters and tokenizer implemented by HuggingFace's transformers library (Wolf et al., 2020). The shallow transformer in the graph-based reasoning module is designed as 2 hidden layers with a size of 512 and 8 attention heads. During the training stage, the batch size is fixed to 16 and the number of negative samples \\\\(K\\\\) is set to 8. The temperature parameter \\\\(\\\\tau\\\\) in Eq.(4) is set to 0.07. The balance parameter \\\\(\\\\lambda b\\\\) in Eq.(6) is set to 0.1. Adam with the learning rate \\\\(5 \\\\times 10^{-5}\\\\) is used to update parameters. We introduce an early stopping mechanism and set the patience value to 5, which means the training will stop if the model performance is not improved in five consecutive times. Our source code will be released online.\"}"}
{"id": "acl-2022-long-84", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Experimental comparison of procedural multimodal machine comprehension on RecipeQA and CraftQA:\\n\\n\\\"w/o Temporal Encoding\\\" and \\\"w/o Modal Encoding\\\" denote to remove the temporal or modal edges respectively and \\\"w/o Both Encoding\\\" denotes to remove both of them. \\\"w/o Contrastive Loss\\\" denotes excluding the contrastive coherence loss $L_{Coh}$ from the final loss. Similar to RecipeQA, 100 questions in CraftQA are extracted from its validation set to evaluate the \\\"Human\\\" performance.\\n\\n4.3 Baselines\\n\\nWe compare our model with the following models: (1) Hasty Student (HS) (Yagcioglu et al., 2018) discards textual context and directly exploits the similarities and dissimilarities between answer images to rank candidates. (2) PRN (Amac et al., 2019) introduces external relational memory units to keep track of textual entities and employs a bi-directional attention mechanism to obtain a question-aware embedding for prediction. (3) MLMM-Trans (Liu et al., 2020) modifies the framework of the transformer (Vaswani et al., 2017) and conducts an intensive attention mechanism at multiple levels to predict correct image sequences. (4) VisualBERT (Li et al., 2020) consists of a stack of transformer layers that extend the traditional BERT (Devlin et al., 2019) model to a multimodal encoder. The performance of some baselines on RecipeQA has been previously reported in (Amac et al., 2019; Liu et al., 2020).\\n\\n4.4 Experimental Results\\n\\n4.4.1 Comparison Analysis\\n\\nAs shown in Table 2, TMEG shows favorable performance in different reasoning tasks, with an average accuracy of 69.73 and 49.54 on RecipeQA and CraftQA, following behind the \\\"Human\\\" performance. Besides, the performance on the visual ordering task exceeds human accuracy for the first time, which proves that the temporal and modal analysis in TMEG is effective in comprehending PMDs. MLMM-Trans performs comparably with VisualBERT while inferior to TMEG, which may be attributed to their superficial consideration of entities. MLMM-Trans ignores the entity information contained in text (e.g., the correspondence between entities in text and images) and VisualBERT directly fuses textual and visual features without considering entity evolution. In TMEG, we explicitly identify and model entity evolution in PMD, whereas MLMM-Trans and VisualBERT assume entity information to be learned implicitly along with other data.\\n\\nMeanwhile, CraftQA has more images (20.14 vs 12.67) and tokens (535.88 vs 443.01) on average than RecipeQA. More diverse complex cases in CraftQA require better comprehension and reasoning capacities for both models and humans. We believe this can explain the lower results on CraftQA. This emphasizes the necessity of comprehending entity coherence in a multimodal context.\\n\\n4.4.2 Ablation Study\\n\\nWe evaluate the effects of temporal encoding, modal encoding, and contrastive coherence loss...\"}"}
{"id": "acl-2022-long-84", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Choose the best image for the missing blank to correctly complete the recipe.\\n\\n**Step 1:** Ingredients: 16 oz Strawberry, 9 oz Blueberries, 11 oz can of Mandarin Oranges, 4-5 Kiwifruits, Sugar Cookie Dough, 2 tsp. Vanilla, 1/2 c. Sugar, 1 pkg. Cream Cheese - softened. Supplies: Supplies to bake cookies,\\n\\n**Step 2** Make or buy your sugar cookie dough. Cut into cookie sizes and bake according to your directions.\\n\\n**Step 3** Time to mix up your sauce/frosting. Take your cream cheese, sugar and vanilla and mix them all together. Spread on cookies now or wait until before you want to serve them.\\n\\n**Step 4** Mandarin Oranges: Drain them. Blueberries: Wash them. Strawberries: Wash them. Cut off the stems. Kiwifruit: Wash them. Go all the way through and gently drag it around the kiwi.\\n\\n**Step 5** Time to put your fruits on your little pizzas! Just follow along with the pictures to see how I did it, but you can decorate your pizzas however you want. For the mandarin oranges, I used...\\n\\n**Step 6** This batch was made earlier in the day on New Years Eve and kept till that night. I'm not sure if my fruit that I used this time was just more juicy.\\n\\n---\\n\\nVisualBERT\\nMLMM-Trans\\nTMEG\\nC\\nD\\n\\nFigure 4: An illustrated example of the visual cloze task on RecipeQA to clarify the workflow of TMEG when dealing with the downstream tasks.\\n\\nL Coh to examine the three modules.\\n\\n**Edge Encoding.** Table 2 also shows the ablation results of our model when each module is respectively removed. In terms of edge encoding, removing temporal encoding makes more negative effects on TMEG than moving the modal encoding, reflecting the significance of modeling temporal entity evolution for procedural M3C.\\n\\n**Contrastive Coherence Loss.** As shown in the last row of Table 2, we find that L Coh can indeed improve TMEG. The reason is that L Coh helps enhance the learning of textual entity representation and visual entity representation in Procedural M3C.\\n\\n---\\n\\n**4.5 Analysis of TMEG**\\n\\n**4.5.1 Balance Parameter**\\n\\nIn Figure 3, we illustrate the influence of the balance parameter \u03bbb in Eq.(6), which balances the contrastive coherence loss L Coh and the candidate prediction loss L Pre. We tune \u03bbb from 0 to 0.2 with 0.05 as the step size. We observe that the model beats the highest accuracy when \u03bbb = 0.1. Generally, (1) introducing the contrastive coherence loss can improve TMEG for better fitting downstream tasks, and (2) appropriately balancing the prediction loss L Pre and contrastive coherence loss L Coh helps TMEG comprehend PMDs.\\n\\n---\\n\\n**Model R2C C2R Average**\\n\\n|                         | R2C  | C2R  | Average |\\n|-------------------------|------|------|---------|\\n| MLMM-Trans (Liu et al., 2020) | 33.98 | 40.14 | 37.06   |\\n| VisualBERT (Li et al., 2020)     | 35.24 | 42.15 | 38.69   |\\n| TMEG (Our Model)           | 39.06 |      |         |\\n| TMEG (w/o Edge Encoding)   | 36.02 | 43.17 | 39.60   |\\n\\nTable 3: Results of the domain transfer experiments, where \\\"R2C\\\" denotes training models on RecipeQA while testing on CraftQA, and \\\"C2R\\\" represents the opposite.\\n\\n**4.5.2 Cross-Domain Investigation**\\n\\nTo study the domain transfer capability of our framework, we evaluate TMEG in different domains, as shown in Table 3. Specifically, The model trained on RecipeQA is evaluated on CraftQA, and the reverse is true for CraftQA. Results show that compared with other baselines, our model achieves more generalized and better comprehension performance on domain transfer by incorporating TMEG.\\n\\n**4.5.3 Case Study**\\n\\nFigure 4 further presents a visual cloze example on RecipeQA which requires a correct image in the missing piece after reading the context. We compare the highest-scored candidate images respectively picked out by MLMM-Trans (Liu et al., 2020), VisualBERT (Li et al., 2020), and TMEG.\"}"}
{"id": "acl-2022-long-84", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"By considering the temporal-modal entity evolution, TMEG can capture the salient entities (e.g., Strawberry and Sugar Cookie Dough) and trace their evolution at each step, thereby inferring the ground-truth answer.\\n\\n5 Conclusion\\nIn this paper, we propose a novel temporal-modal entity graph (TMEG) to approach Procedural M$^3$C. Based on TMEG, we introduce graph-based fusion module and reasoning module, which are used to aggregate node features and solve downstream reasoning tasks.\\n\\nWhat's more, we introduce another Procedural M$^3$C dataset called CraftQA to assist in evaluating the generalization performance of TMEG in different domains and domain transfer. Extensive experiments on the RecipeQA and CraftQA validate the superiority of TMEG. A promising future direction is to introduce temporal-modal entity graphs into the video understanding task (Lin et al., 2020; Xu et al., 2020), which also calls for an enhancement of the temporal and the cross-modal reasoning capability.\"}"}
{"id": "acl-2022-long-84", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-84", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"dataset for tracking entities in open domain procedural text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 6408\u20136417.\\n\\nMakarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016. Movieqa: Understanding stories in movies through question-answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4631\u20134640.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, pages 5998\u20136008.\\n\\nJesse Vig. 2019. A multiscale visualization of attention in the transformer model. In Proceedings of the 57th Conference of the Association for Computational Linguistics, pages 37\u201342.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierre Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345.\\n\\nYuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. 2019. Detectron2. https://github.com/facebookresearch/detectron2.\\n\\nFrank F. Xu, Lei Ji, Botian Shi, Junyi Du, Graham Neubig, Yonatan Bisk, and Nan Duan. 2020. A benchmark for structured procedural knowledge extraction from cooking videos. CoRR, abs/2005.00706.\\n\\nSemih Yagcioglu, Aykut Erdem, Erkut Erdem, and Nazli Ikizler-Cinbis. 2018. Recipeqa: A challenge dataset for multimodal comprehension of cooking recipes. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1358\u20131368.\\n\\nJianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, Soujanya Poria, and Louis-Philippe Morency. 2021. MTAG: modal-temporal attention graph for unaligned human multimodal language sequences. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1009\u20131021.\\n\\nZhengyuan Yang, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, and Jiebo Luo. 2019. A fast and accurate one-stage approach to visual grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4682\u20134692.\\n\\nYongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, and Jiebo Luo. 2020. A novel graph-based multi-modal fusion encoder for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3025\u20133035.\\n\\nDong Zhang, Suzhong Wei, Shoushan Li, Hanqian Wu, Qiaoming Zhu, and Guodong Zhou. 2021a. Multi-modal graph fusion for named entity recognition with targeted visual guidance. In Proceedings of the 35th AAAI Conference on Artificial Intelligence and 33rd Conference on Innovative Applications of Artificial Intelligence and 7th Symposium on Educational Advances in Artificial Intelligence, pages 14347\u201314355.\\n\\nZhihan Zhang, Xiubo Geng, Tao Qin, Yunfang Wu, and Daxin Jiang. 2021b. Knowledge-aware procedural text understanding with multi-stage training. In Proceedings of the International World Wide Web Conference 2021, pages 3512\u20133523.\\n\\nWanjun Zhong, Duyu Tang, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020. A heterogeneous graph with factual, temporal and logical knowledge for question answering over dynamic contexts. CoRR, abs/2004.12057.\"}"}
