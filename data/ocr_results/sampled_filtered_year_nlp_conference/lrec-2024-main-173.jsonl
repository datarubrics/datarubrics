{"id": "lrec-2024-main-173", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Automatic Identification of COVID-19-related Narratives in German Telegram Channels and Chats\\n\\nPhilipp Heinrich\u2020, Andreas Blombach\u2020, Bao Minh Doan Dang\u2020, Leonardo Zilio\u2020, Linda Havenstein\u2021, Nathan Dykes\u2020, Stephanie Evert\u2020, Fabian Sch\u00e4fer\u2021\\n\\nChair of Computational Corpus Linguistics\\nChair of Japanese Studies\\nFriedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg\\n\u2020 Bismarckstr. 6, 91054 Erlangen\\n\u2021 Artilleriestr. 70, 91052 Erlangen\\n{firstname.lastname}@fau.de\\n\\nAbstract\\nWe are concerned with mapping the discursive landscape of conspiracy narratives surrounding the COVID-19 pandemic. In the present study, we analyse a corpus of more than 1,000 German Telegram posts manually tagged with 14 conspiracy and conspiracy-related narrative labels by three independent annotators. Since emerging narratives on social media are short-lived and notoriously hard to track, we experiment with different state-of-the-art approaches to few-shot and zero-shot text classification. We report performance in terms of ROC-AUC and in terms of optimal $F_1$, and compare fine-tuned methods with off-the-shelf approaches and human performance.\\n\\nKeywords: COVID-19, User-generated Content, Zero-shot Text Classification, Few-shot Text Classification\\n\\n1. Introduction and Related Work\\nIn early 2020, shortly after declaring the spread of the new coronavirus SARS-CoV-2 a pandemic, the WHO also warned about an \u2018infodemic\u2019, a surge of disinformation, conspiracy narratives and misrepresentation of medical facts and political processes surrounding COVID-19. This came as no surprise, as \u201cbelief in conspiracy theories is stronger under conditions of uncertainty\u201d and \u201cwhen events are especially large-scale or significant\u201d (Douglas et al., 2019); it thrives in times of crises and information vacuums. In addition, the \u2018connectedness\u2019 of the internet and especially social media have contributed to the spread of conspiracy narratives, as it turns the conspiracy narrative baseline of \u2018everything [being] connected\u2019 into reality and the \u201cinterpretative logic of conspiracy theories [mirror] the ordering principle of the World Wide Web\u201d (Butter, 2018).\\n\\nStudies in Germany confirm a prevalence of conspiracy beliefs (Kuhn et al., 2021), with one in five citizens believing that the dangers of SARS-CoV-2 have been intentionally exaggerated to deceive the public (dimap, 2020) and the same proportion of people agreeing that \u2018Many numbers and statistics concerning COVID-19 are forged\u2019 (Institut f\u00fcr Demoskopie Allensbach, 2022). As a large part of the related conspiracy narrative and disinformation discussion has migrated to largely unmoderated platforms such as Telegram, it is difficult to oversee radicalisation processes and potentially hazardous developments within the scene and beyond.\\n\\nAutomatically identifying misinformation such as fakenews, conspiracynarratives, or general drivel is notoriously difficult. One of the major bottlenecks is the lack of suitable training (and evaluation) data, especially in a discursive landscape where narratives are evolving quickly. In the present study, we use a collection of 1099 posts scraped from openly accessible and popular COVID-19-themed Telegram channels and chat groups, which has been labelled manually by domain experts (see Section 2. To bypass the problem of sparse categories, we experiment with approaches that leverage label descriptions created by domain experts and with approaches to zero-shot and few-shot classification (i.e., techniques that use no examples or just a few examples of training data, see Section 3).\\n\\nRelated work in Natural Language Processing (NLP) often focuses on identifying fake news and \u201crumours\u201d (Li and Zhou, 2020), which are related to, but different from, conspiracy theories. Moreover, the task is usually a yes/no classification (\u201cdrivel\u201d vs. \u201cnodrivels\u201d) to assist moderation on social media (Moffitte et al., 2021). For deeper linguistic or computational social science analyses, or in order to apply counter-measures targeted to specific narratives, this binary approach is insufficient. Thus, our aim is to identify different groups of conspiracy-related or conspiracy-adjacent content. Previous work also attempted to automatically identify new conspiracy theories early on (Shahsavari et al., 2020; Marcilino et al., 2021).\\n\\nWith the advent of large language models (LLMs), the language of conspiracy theories is increasingly automated and accessible. This is reflected in the German term Geschwurbel, meaning conspiracy-related or conspiracy-adjacent content. We use drivel as a cover term for any such content in this paper.\"}"}
{"id": "lrec-2024-main-173", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"especially BERT-like models (Devlin et al., 2019) and generative models such as the GPT series (Radford et al., 2018), researchers noticed that these LLMs contain a wealth of information about language and lexical semantic relations. This rich lexical information meant that textual relations can be predicted even without directly training the model for a task and gave rise to new approaches to zero-shot text classification.\\n\\nAn early benchmark for zero-shot text classification based on natural language inference (NLI) was proposed by Yin et al. (2019). By using entailment predictions between texts and hypotheses, the probability of the entailment can serve as a proxy for classification. Using this approach, Barker et al. (2021) achieved good results for zero-shot single-label classification of English texts. Similarly, large generative models can be used to predict text classifications based on a given prompt (Han et al., 2022).\\n\\nIn our study, we adopt an NLI-based approach using models for multi-label text classification based on DeBERTa (He et al., 2020) and RoBERTa (Liu et al., 2019). We also adopt a sentence-similarity-based approach (Reimers and Gurevych, 2019) to evaluate similarities between posts and label descriptions. Finally, we test the generative capacity of ChatGPT4 (OpenAI, 2023) in a zero-shot setting.\\n\\nThe main contributions of this study are the following:\\n\\n- The collection of a large corpus of Telegram posts and the annotation of a sample with different COVID-19-related narratives, as presented in Section 2. The whole data set was used for training a Bert-based masked language model adapted to user-generated content related to the COVID-19 pandemic.\\n- A battery of text classification experiments on user-generated content, including classic machine-learning algorithms, zero-shot and few-shot classification (Section 3). We also provide a stratified split into train, development, and test sets that can be used for text classification.\\n- A comparison of the performance of machine learning algorithms with each other and with that of human annotators on the task of detecting COVID-19-related narratives (Section 4).\\n\\n2. Corpus and Categorisation\\n\\nIn 2020 \u2013 as YouTube, Facebook, and others became more aggressive in cracking down on the spread of disinformation \u2013 sceptics, lockdown critics and conspiracy theorists found themselves in need of a new social media network. While new platforms and hosting services were set up for video streaming, a large part of the text- and image-based discussion migrated to the messaging and microblogging platform Telegram (Lamberty et al., 2022; Holnburger et al., 2022), widely known for its lack of moderation. Telegram channels and groups have thus become one of the most important data sources for studying conspiracy theories.\\n\\nTo build our corpus, we first scraped the channels of several well-known figures in the COVID-19 conspiracy scene using Telegram\u2019s own export function. Since channels often interact with each other (e.g. by forwarding messages), we proceeded to scrape frequently mentioned channels with large numbers of followers, thus iteratively increasing the scope and size of the corpus. This approach was supplemented by channel statistics available on the web.\\n\\nOur full corpus contains over 200 different Telegram channels (with follower counts ranging from a few thousand to over 300,000), as well as over 100 public group chats from January 2020 up to and including July 2022. These figures translate to a total of over 13 million posts, amounting to almost 400 million tokens. Upon request, interested researchers can be given access to search the corpus online.\\n\\n2.1. A masked language model of conspiratorial talk\\n\\nWith several hundreds of million tokens of running text, the corpus itself can be used to adapt a masked language model to the domain of conspiratorial talk on German Telegram. We use gbart-large as a base model and fine-tuned it using the transformers library in Python. The model is available via Huggingface hub and can be used for fine-tuning to a task such as text classification. Note that we do not use the model here, since we do not have a suitable data set for fine-tuning (fine-tuning a masked language model from scratch for text classification needs more examples than the couple of examples we provide with our annotation below).\\n\\n2.2. Sample\\n\\nIn order to obtain a sample for the manual annotation of conspiracy narratives and related content, we first excluded forwarded messages and posts containing images, videos or polls (as we are only...\"}"}
{"id": "lrec-2024-main-173", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"concerned with content in textual form). Furthermore, we required a minimum length from the posts (\u2265 400 characters, excluding URLs). We then drew a sample from the filtered corpus, stratified by month, channel/group and number of messages, resulting in 1099 posts by 343 individual users in 143 different channels/groups from January 2020 to March 2022. This set of posts contains an average of 180 tokens distributed across 11.4 sentences.\\n\\n2.3. Narratives\\n\\nIn order to annotate relevant narratives, it was necessary to develop a categorisation scheme. Our scheme is based on previous research (Institut f\u00fcr Demoskopie Allensbach, 2022; Kuhn et al., 2021), domain knowledge and close reading of excerpts from our corpus prior to sampling and was further refined during the annotation process. The categorisation scheme is hierarchical and contains a total of 18 narrative groups subdivided into 63 fine-grained narratives. It includes descriptions and examples for each narrative and is available online.\\n\\n6. Description sentences were derived from the annotation guidelines by domain experts and are meant to represent concise summaries of the narratives. We include narratives specific to COVID-19, such as \u2018COVID-19 is no more dangerous than the common flu\u2019 or \u2018The pandemic serves to implement the Great Reset\u2019 as well as previously existing narratives such as \u2018New World Order\u2019 or \u2018sheeple\u2019.\\n\\nSince many fine-grained narratives are very infrequent in the sample (and a few are not present at all), we only use the (slightly adapted) narrative groups for the classification task in this paper. To give an idea of the narrative contents, brief descriptions in English are provided below; see also Table 1 for an overview.\\n\\nPseudo-pandemic: narratives that downplay the danger of COVID-19, deny its existence or feed doubts about the official narrative of the pandemic.\\n\\nCriticism of countermeasures: narratives claiming that pandemic response efforts are illegal, more dangerous than the virus (e.g. masks causing illness) or that they discriminate against sceptics and people who refuse to wear masks, be tested or vaccinated.\\n\\nAlternative treatments: narratives about repurposed drugs or other \u2018miracle cures\u2019 against COVID-19 that are allegedly withheld from the population.\\n\\nVaccine hazards: narratives portraying COVID-19 vaccines as insufficiently tested, unsafe, or even dangerous.\\n\\nCOVID-19 conspiracies: conspiracy theories claiming that some hidden agenda is behind the pandemic, e.g. Bill Gates aiming to reduce the world population or to inject people with microchips to control them, or the pandemic serving to destroy the economy, to achieve climate change goals or to produce profit for big corporations and powerful elites.\\n\\nOther conspiracies: pre-existing conspiracy theories not specific to COVID-19 \u2013 chemtrails, claims about false flag operations, mind control, all-powerful secret societies etc.\\n\\nQAnon: narratives about an anonymous individual called Q and his claims of insider knowledge about highly classified U.S. government documents, a Satanic cabal operating a global child sex trafficking ring, and Donald Trump\u2019s secret fight against this cabal.\\n\\nGroup-focused enmity: all forms of racism, xenophobia, Islamophobia, homophobia, misogyny etc., as well as the far-right conspiracy theories Great Replacement (claiming a plot to replace the ethnic white population with non-white immigrants, especially Muslims) and BRDGmbH (claiming that Germany never ceased to be controlled by the Allies after World War II, and rejecting the constitution and legitimacy of the modern German state in favor of the German Reich).\\n\\nTable 1: List of narrative groups with number of description sentences.\\n\\n| Narrative Group            | Number of Description Sentences |\\n|---------------------------|---------------------------------|\\n| Pseudo-pandemic           | 13                              |\\n| Criticism of countermeasures | 12                             |\\n| Alternative treatments     | 6                               |\\n| Vaccine hazards            | 16                              |\\n| COVID-19 conspiracies      | 26                              |\\n| Other conspiracies        | 9                               |\\n| QAnon                     | 6                               |\\n| Group-focused enmity      | 19                              |\\n| Sheeple                   | 2                               |\\n| Millenarianism            | 4                               |\\n| State as an enemy         | 5                               |\\n| Indoctrination            | 7                               |\\n| Esotericism & pseudo-science | 8                              |\\n| Other drivel              | 9                               |\\n| \u201cno drivel\u201d               | 0                               |\"}"}
{"id": "lrec-2024-main-173", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sheeple: covers the single narrative (often accompanying conspiracy narratives) that most people have no idea what is really going on, because they are brainwashed or choose to live in ignorance.\\n\\nMillenarianism: narratives about an upcoming day or time of great change or reckoning when the group's beliefs will be validated and/or its enemies will be defeated.\\n\\nState as an enemy: narratives questioning the status of democracy \u2013 by assuming a deep state that holds the real power, by accusing free actors of being covert agents of the system, by doubting election results, or by accusing the state of having dictatorial features.\\n\\nIndoctrination: narratives claiming that the (mainstream) media is controlled by the state or a powerful group and/or that it lies, censors information and indoctrinates the population, as well as narratives about cancel culture and the death of free speech.\\n\\nEsotericism & pseudo-science: pseudo-scientific claims about medicine (e.g. disbelief in the existence of viruses in general) or alternative medicine, as well as various esoteric practices and beliefs (healing crystals, mediums, auras etc.).\\n\\nOther drivel: includes narratives with very low prevalence: climate change denial and narratives about a man-made origin of COVID-19.\\n\\n2.4. Manual annotation\\n\\nThree of the authors individually annotated the full sample on two levels: whether a post contains drivel (conspiracy-related or conspiracy-adjacent content), and if so, which specific narratives it contains. Note that multiple categories can be assigned to the same post. In a subsequent adjudication process, we resolved all disagreements to arrive at a final gold standard.\\n\\nFigure 1 shows the prevalence of each narrative group in the sample, as well as the number of posts containing no (or no clearly identifiable) drivel (52.5%). If several narratives belonging to the same narrative group occur in the same post, they are counted as only one instance of the group.\\n\\nTable 2: Pairwise Cohen's $\\\\kappa$ for the initial classification as drivel or not (Fleiss' $\\\\kappa$ excluding the gold standard: 0.61).\\n\\nTable 2 shows pairwise inter-annotator agreement for the first level of annotation. As evident from the moderate to good values, even this binary classification is often difficult for domain experts.\\n\\nFleiss $\\\\kappa$ for individual narrative groups ranges from 0.32 to 0.83, with a mean of 0.59 and a standard deviation of 0.13.\\n\\n3. Automatic Classification of Posts\\n\\nAutomatic classification of posts is operationalised as a multi-label document classification problem: the task is to identify which narratives, if any, are mentioned in a post. Since the annotation of training data is very resource-intensive and our categorisation scheme contains a large number of narratives, only a handful of positive examples can be used for training (\\\"few-shot classification\\\", see Section 3.1). Alternatively, we can use zero-shot classification (i.e. classification using no training data, Section 3.2), which derives its predictions from the text to be classified and the semantics of the category label (in our case: the description of a narrative). Note that the descriptions can also be used in supervised approaches by including them as positive examples in the training data.\\n\\nFor evaluation, we treat the problem as separate binary document classification task for each narrative group, so we can quantify performance per group by area under the receiver operating characteristic curve (ROC-AUC). This is a reasonable choice since the classification threshold determining sensitivity (i.e. recall) and specificity (or, alternatively, precision) can be set in a task-specific way, e.g. opting for high recall when using the classifier as a filter whose results are checked manually afterwards.\\n\\nThe complete annotated corpus is split in a stratified fashion into training, development and test sets, with a ratio of 60 : 15 : 25. All models that require training are trained on the training split. Suitable cut-off values for optimal $F_1$ are found on the development set. Measures provided in Table 3 (and Table 4 in the appendix) are derived from the test set.\\n\\nSee https://github.com/fau-klue/infodemic for the complete corpus and the split.\"}"}
{"id": "lrec-2024-main-173", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1. Supervised prediction\\n\\nIn order to have proper baselines for our few-shot and zero-shot approaches, we first perform several multi-label experiments with standard machine learning classifiers. We use logistic regression (LR) and a support vector machines (SVM) with a tf.idf weighted unigram bag-of-words feature matrix and perform experiments using scikit-learn (Pedregosa et al., 2011).\\n\\nAn additional question relating to supervised machine learning classification is whether adding a small number of description texts into training data can improve model performance (allowing a bag-of-words model to directly learn keywords from the descriptions). Therefore, we conduct our experiments as follows: in one round we use the training set (posts only), and in the other we extend training data with description sentences (posts+descriptions) to train our classification models.\\n\\nFew-shot classification\\n\\nSince it is difficult and time-consuming to manually annotate narratives, and our data set is therefore comparatively small, few-shot learning is an obvious approach to generalise from only a small amount of training data for each label.\\n\\nTunstall et al. (2022) proposed SetFit, a framework for few-shot fine-tuning. A pre-trained Sentence Transformers model is first fine-tuned on a number of contrastive pairs of labelled texts. This model is then used to encode the training data. Finally, a text classification head is trained using the encoded data. While state-of-the-art methods such as T-Few (Liu et al., 2022) may attain even better few-shot results, SetFit is competitive and has the advantages of not requiring prompts and being easier to train.\\n\\nIn our experiments, we fine-tuned different Sentence Transformers models, but quickly found paraphrase-multilingual-mpnet-base-v2 to be the best available base model. We also found that using the description sentences for our target labels as additional training data significantly improved model performance. We therefore report only these results for the SetFit approach. We include both \u201cout of the box\u201d results and results after (time-consuming) hyperparameter optimisation.\\n\\n3.2. Zero-shot prediction\\n\\nFor zero-shot classification, we split the posts and narrative descriptions into sentences. For a post $p$ and a narrative $n$, we can calculate one score $s(i, j)$ for each sentence pair $(s_i, s_j)$ with $s_i \\\\in S_p, s_j \\\\in S_n$, where $S_p$ comprises the sentences of post $p$ and $S_n$ the sentences of narrative $n$; see below for the exact procedures to get scores.\\n\\nThe individual sentences of a narrative description usually represent different ways in which the narrative can be expressed. Since we are primarily interested in whether a given conspiracy narrative is present in the post or not (rather than how it is\"}"}
{"id": "lrec-2024-main-173", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In 1937, 1937, 1937, formulated or whether the post is exclusively about this narrative, it seems reasonable to take the maximum over the scores of all sentence pairs as the overall score for post $p$ and narrative $n$:\\n\\n$$\\\\text{score} (p, n) = \\\\max_{s_i \\\\in S_p, s_j \\\\in S_n} (s(s_i, s_j))$$\\n\\nNote that we could also opt for comparing entire posts to each sentence of the narrative descriptions or all sentences in a post to the whole narrative. However, these strategies were consistently outperformed by the approach above, and we exclude those results for the sake of brevity.\\n\\n### Sentence-similarity zero-shot\\n\\nA cheap approach to zero-shot classification can be constructed by looking at similarities between posts and narratives at the sentence level. We encode all sentences using a multi-lingual SBERT model (Reimers and Gurevych, 2019). Here, we use the Python `SentenceTransformer` library and opt for `distiluse-base-multilingual-cased-v1` (Reimers and Gurevych, 2020), which yielded good results for German CMC in a pre-study, even compared to specialised German embeddings. We then use the cosine similarity between the SBERT sentence embeddings $e_{s_i}$ and $e_{s_j}$ as a score:\\n\\n$$s(s_i, s_j) = \\\\cos(e_{s_i}, e_{s_j})$$\\n\\n### NLI zero-shot\\n\\nYin et al. (2019) pioneered the use of natural language inference (NLI) models for zero-shot text classification. The main idea is that if a model can predict whether a hypothesis is semantically entailed from a text, it can also be used for text classification with previously unseen labels. In this study, instead of just using single- or few-word labels, we can make use of a detailed textual description of the label, consisting of several sentences. We tested entailment hypotheses for each sentence from the description against each sentence from a given post. The following hypothesis template was used: \u201cIn diesem Satz geht es um das Thema {}.\u201d [This sentence is about {}], where \"{}\" was replaced with sentences from label descriptions.\\n\\nWe used four different models pre-trained for NLI, all of which are available on Huggingface and are either specifically trained for the German language or, in the case of multilingual models, include German in their training data:\\n\\n- **gbert-large-nli**: This is the only model specifically fine-tuned for German. It uses the 10KG-NAD data set (Schabus et al., 2017) on top of the German BERT-large model (Chan et al., 2020).\\n- **xlm-roberta-large-xnli**: This model was fine-tuned on xlm-roberta-large (Conneau and Lample, 2019) using the XNLI Corpus to perform NLI for 15 languages (Conneau et al., 2018).\\n- **mDeBERTa-v3-base-xnli**: This model was fine-tuned for NLI on top of mDeBERTa-base v3 (He et al., 2022), also using the XNLI Corpus as basis.\\n- **mDeBERTa-v3-base-xnli-2mil7**: This model also used mDeBERTa-base v3, but fine-tuned it for NLI using the XNLI Corpus translated to 26 languages, containing a total of 2.7 million text pairs.\\n\\n### ChatGPT4\\n\\nTo obtain predictions for our test set from ChatGPT, we used OpenAI\u2019s (paid) API via Python. The model (gpt-4-0613) first received a system prompt (in English) to steer its classification behaviour. This included information about the task, the expected input and output format, as well as the same label description sentences (in German) used in our previous experiments.\\n\\nYou will be provided with German Telegram posts from people who are potentially spreading COVID-19 misinformation and conspiracy theories. Posts will be delimited with `\u223c\u223c\u223c\u223c` characters. Each post will be preceded by a unique numeric id on the first line.\\n\\nYour job is a multi-label classification task. Each post can belong to one or more of 14 different classes. If none of these classes apply, a post is to be labelled \u201ckein_Geschwurbel.\u201d For each class or label, there are multiple description sentences in German in the...\"}"}
{"id": "lrec-2024-main-173", "page_num": 7, "content": "{\"primary_language\":\"de\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1938\\n\\nForm \\\"label: description\\\". Diese Label-Alternativen, einer Beschreibung pro Zeile, versuchen, die Kernideen von verschiedenen Subklassen zu kapseln.\\n\\nDie Labels und ihre Beschreibungen sind:\\n\\n**Impfung_ist_gef\u00e4hrlich**: F\u00fcr die Herstellung neuartiger mRNA-Impfstoffe gegen das Coronavirus (wie von BioNTech oder Moderna) werden menschliche Embryonen oder abgetriebene F\u00f6ten benutzt.\\n\\n**Impfung_ist_gef\u00e4hrlich**: Die Impfung gegen COVID-19 ist nicht ausreichend getestet worden und deshalb nicht sicher. M\u00f6gliche Langzeitsch\u00e4den durch die Impfung lassen sich nicht ausschlie\u00dfen.\\n\\n**Millenarismus**: Am Tag X wird das Volk sich erheben, die Regierung st\u00fcrzen und zur Rechenschaft ziehen.\\n\\n**Schlafschafe**: Ein gro\u00dfer Teil der Bev\u00f6lkerung ist gehirngewaschen und hat keine Ahnung, was wirklich in der Welt vor sich geht.\\n\\n**Schlafschafe**: Schlafschafe lassen sich von den Medien blenden und erdulden alles wie brave Schafe, anstatt sich zu wehren.\\n\\nKlassifizieren Sie jede Post entsprechend den oben genannten Labels und Beschreibungen. Es ist m\u00f6glich, mehrere Labels pro Post zu verwenden.\\n\\nSchaffen Sie Ihre Ausgabe in JSON-Format mit IDs und allen f\u00fcr die entsprechende Post zutreffenden Labels.\\n\\nPosten werden in Schenken eingereicht, indem Benutzer Anforderungen stellen. Da der gew\u00e4hlte Modelltyp nur 8.192 Tokens pro Runde verarbeiten konnte und die Systemvorlage, die vor jedem Batch von Posten kommt, mehr als 5.500 Tokens ben\u00f6tigte (aufgrund der Anzahl und L\u00e4nge der Beschreibungen), konnten wir nur wenige Posten pro Runde einreichen. Mit OpenALI's Rate Limit von 10.000 Tokens pro Minute und API-Unterbrechungen sowie unvollst\u00e4ndigen Output-Results von der Modellausgabe, war dieses Vorgehen nicht so glatt wie urspr\u00fcnglich erwartet.\"}"}
{"id": "lrec-2024-main-173", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Macro-average ROC-AUC values for different models (first result column): We report macro averages of all 14 narratives excluding the negative category \\\"no drivel\\\". The cheap approach using sentence similarity outperforms all other models. xlm-roberta-large-xnli yields by far the best results among the NLI zero-shot models. All other models are outperformed by our baselines (in particular SVM leveraging descriptions). Average optimal $F_1$ scores are reported on the right two result columns; the whole table is sorted by micro-average $F_1$. We include student annotators and ChatGPT4 (top), who outperform our models in terms of optimal $F_1$; here, only the few-shot approach (SetFit) beats our baselines.\\n\\nIn order to compare our systems in terms of precision and recall as well (or $F_1$, the harmonic mean of precision and recall), we have to determine a cut-off value for binary prediction of each narrative. We thus use the development set to choose a threshold that maximises $F_1$. In this scenario, we can also assign the label \\\"no drivel\\\" for all zero-shot classifiers: a post is classified as \\\"no drivel\\\" by a model if and only if no other label has been given to this post by the model. Table 3 (right) shows micro- and macro-average $F_1$-scores for all models across all 15 labels (using the optimal thresholds).\\n\\nTable 3 shows that zero-shot approaches are very good at the detection of narratives: the sentence similarity approach beats all other systems in terms of average ROC-AUC and is clearly the best-performing model for predicting e.g. label 'Indoctrination', cf. Figure 2. Supervised ML approaches clearly optimise trade-off between precision and recall and reach highest ranks when compared at optimal $F_1$; they can take the actual prevalence of each narrative into account, which zero-shot models cannot. Note that ChatGPT4 outperforms our models, likely because it is a much larger pre-trained model. Also note that all models are still far from human performance; this difference shows that this sort of prediction is a challenging task, and quantifies how much room for improvement is still left. We invite the research community to engineer better systems for solving the task on our data set.\"}"}
{"id": "lrec-2024-main-173", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Firstly, we did not systematically analyse the influence of description sentences on our classification procedure. Initial experiments indicate that a moderate amount of very specific sentences yields the best results. Similarly, we did not systematically experiment with different SBERT models in the sentence-similarity and few-shot approaches, and only included a handful of models in the NLI zero-shot approach. At this point, it seems that the general multi-language, multi-purpose models yield good results for our procedure, but fine-tuned embeddings might be a promising step. By providing a masked language model adapted to the whole corpus, we lay the foundation for further experiments.\\n\\nLast but not least, the ML classifiers improve with increasing numbers of training examples. A closer look at learning curves would thus be necessary in order to determine the point where supervised techniques start outperforming zero-shot classification techniques.\\n\\nLastly, our approach lends itself to a simple extension: descriptions of narratives can be very abstract on the one side (\u201carm-chair\u201d descriptions) or very concrete on the other (actual surface realisations sampled from the corpus). From a practical point of view, it thus seems reasonable to start with basic descriptions of categories (such as the ones we used here) and extend the descriptions with sentences found in the classification procedure. Since no cut-off for inclusion can be determined a priori, this process should ideally be supervised (i.e., experts can select additional description sentences from n-best lists generated by the classifier).\\n\\n6. Acknowledgements\\n\\nThis work has partially been funded by Volkswagen Foundation (project no. 98 776) and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 466328567.\\n\\n7. Supplementary Materials\\n\\n7.1. Results per narrative group\\n\\nWe report complete results per narrative group in Table 4.\\n\\n7.2. Ethical considerations\\n\\nWe collected posts from public Telegram channels which can be accessed by anyone with access to the world wide web, even without an account or a subscription. Users can thus not expect anonymity. Users generally do not use their real names in Telegram groups (for public figures running their own channel, like Boris Reitschuster or Eva Herman, this is obviously different). In a few cases, however, users did post their real identities and/or phone numbers (in terms of so-called \u201cv-cards\u201d). We stripped our dataset of these obvious identifiers before processing them further.\\n\\nThe manually annotated dataset, which we publish online, only contains channel and chat group names, no individual user names. The full corpus, on the other hand, is only available to other researchers upon request. Sharing the raw data and derived models among the research community is possible under German law (\u00a760d UrhG). Note that a working system for automatic narrative classification could be used for filtering or steering discussions (but given the current performance, such a system would sensibly only flag suspicious posts for human moderators). However, compared to the impact of current generative LLMs such as ChatGPT, practical relevance for malicious applications seems vanishingly low in practice. Ideally, such a model would be used by official moderators (of e.g. forums or comment sections).\\n\\nLastly, the present contribution contains academic experiments. For a productive deployment of any of the systems presented here, one would have to completely anonymise the training data beforehand.\"}"}
{"id": "lrec-2024-main-173", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Narrative Group          | ROC-AUC | Pseudo-Pandemic | Crit. of Countermeasures | Alternative Treatments | Vaccine Hazards | COVID-19 Conspiracies | Other Conspiracies | QAnon | Group-Focused Enmity | Sheeple | Millenarianism | State as an Enemy | Indoctrination | Esot. & Pseudo-Science | Other Drivel | \\\"No Drivel\\\" |\\n|-------------------------|---------|-----------------|--------------------------|------------------------|-----------------|-----------------------|-------------------|-------|----------------------|---------|-----------------|------------------|---------------|----------------------|------------|-------------|\\n|                         |         |                 |                          |                        |                 |                       |                   |       |                      |         |                 |                  |               |                      |            |             |\\n| *micro average*         |         |                 |                          |                        |                 |                       |                   |       |                      |         |                 |                  |               |                      |            |             |\\n|                         | 0.70    | 0.65            | 0.44                     | 0.34                   | 0.33            | 0.32                  | 0.29              | 0.28  | 0.23                 | 0.67   | 0.57            | 0.27             | 0.81          | 0.83                  | 0.60       |             |\\n|                         | 0.76    | 0.71            | 0.52                     | 0.51                   | 0.50            | 0.48                  | 0.47              | 0.45  | 0.40                 | 0.84   | 0.81            | 0.71             | 0.78          | 0.79                  | 0.72       |             |\\n| *macro average*         | 0.71    | 0.65            | 0.41                     | 0.34                   | 0.33            | 0.30                  | 0.25              | 0.29  | 0.23                 | 0.64   | 0.57            | 0.27             | 0.81          | 0.83                  | 0.60       |             |\\n|                         | 0.72    | 0.66            | 0.43                     | 0.37                   | 0.36            | 0.33                  | 0.28              | 0.32  | 0.21                 | 0.84   | 0.81            | 0.71             | 0.78          | 0.79                  | 0.72       |             |\\n\\nTable 4: Complete results: ROC-AUC and (optimal) F1 scores for each narrative group. We report micro and macro averages both for excluding and including prediction of \\\"no drivel\\\" (where applicable). Bold numbers represent best-performing systems for each type of model in terms of the respective score.\"}"}
{"id": "lrec-2024-main-173", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. Bibliographical References\\n\\nKen Barker, Parul Awasthy, Jian Ni, and Radu Florian. 2021. IBM MNLP IE at CASE 2021 task 2: NLI reranking for zero-shot text classification. In Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021), pages 193\u2013202.\\n\\nMichael Butter. 2018. \u00bbNichts ist, wie es scheint\u00ab: \u00dcber Verschw\u00f6rungstheorien. Suhrkamp Verlag.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186.\\n\\nInfratest dimap. 2020. Corona und die Medien \u2013 Eine Studie im Auftrag des NDR \u2013 Magazin zapp \u2013 Mai 2020.\\n\\nKaren M. Douglas, Joseph E. Uscinski, Robbie M. Sutton, Aleksandra Cichocka, Turkay Nefes, Chee Siang Ang, and Farzin Deravi. 2019. Understanding conspiracy theories. Political Psychology, 40(S1):7.\\n\\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2022. PTR: Prompt tuning with rules for text classification. AI Open, 3:182\u2013192.\\n\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. DeBERTa: Decoding-enhanced BERT with disentangled attention. In International Conference on Learning Representations.\\n\\nJosef Holnburger, Maheba Goedeke Tort, and Pia Lamberty. 2022. Q vadis? Zur Verbreitung von QAnon im deutschsprachigen Raum.\\n\\nInstitut f\u00fcr Demoskopie Allensbach. 2022. Politischer Radikalismus und die Neigung zu Verschw\u00f6rungstheorien.\\n\\nSarah Anne Kezia Kuhn, Roselind Lieb, Daniel Freeman, Christina Andreou, and Thea Zander-Schellenberg. 2021. Coronavirus conspiracy beliefs in the German-speaking general population: endorsement rates and links to reasoning biases and paranoia. Psychological Medicine, pages 1\u201315.\\n\\nPia Lamberty, Josef Holnburger, and Maheba Goedeke Tort. 2022. Das Protestpotential w\u00e4hrend der COVID-19-Pandemie.\\n\\nQifei Li and Wangchunshu Zhou. 2020. Connecting the dots between fact verification and fake news detection. In Proceedings of the 28th International Conference on Computational Linguistics, pages 1820\u20131825, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nWilliam Marcellino, Todd C. Helmus, Joshua Kerrigan, Hilary Reininger, Rouslan I. Karimov, and Rebecca Ann Lawrence. 2021. Detecting conspiracy theories on social media: Improving machine learning to detect and understand online conspiracy theories. Technical report, RAND Corporation.\\n\\nJ. D. Moffitt, Catherine King, and Kathleen M. Carley. 2021. Hunting conspiracy theories during the COVID-19 pandemic. Social Media + Society, 7(3):1\u201317.\\n\\nOpenAI. 2023. GPT-4 technical report.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.\\n\\nShadi Shahsavari, Pavan Holur, Tianyi Wang, Timothy R. Tangherlini, and Vwani Roychowdhury. 2020. Conspiracy in the time of corona: automatic detection of emerging COVID-19 conspiracy theories in social media and the news. Journal of Computational Social Science, 3:279\u2013317.\"}"}
{"id": "lrec-2024-main-173", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lewis Tunstall, Nils Reimers, Unso Eun Seo Jo, Luke Bates, Daniel Korat, Moshe Wasserblat, and Oren Pereg. 2022. Efficient few-shot learning without prompts. arXiv preprint arXiv:2209.11055.\\n\\nWenpeng Yin, Jamaal Hay, and Dan Roth. 2019. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, pages 3914\u20133923. Association for Computational Linguistics.\\n\\n9. Language Resource References\\n\\nChan, Branden and Schweter, Stefan and M\u00f6ller, Timo. 2020. German's Next Language Model. International Committee on Computational Linguistics.\\n\\nConneau, Alexis and Lample, Guillaume. 2019. Cross-lingual language model pretraining. Conneau, Alexis and Rinott, Ruty and Lample, Guillaume and Williams, Adina and Bowman, Samuel R. and Schwenk, Holger and Stoyanov, Veselin. 2018. XNLI: Evaluating Cross-lingual Sentence Representations. Association for Computational Linguistics.\\n\\nHe, Pengcheng and Gao, Jianfeng and Chen, Weizhu. 2022. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing.\\n\\nLaurer, Moritz and van Atteveldt, Wouter and Casas, Andreu and Welbers, Kasper. 2023. Less Annotating, More Classifying: Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT-NLI. Cambridge University Press.\\n\\nReimers, Nils and Gurevych, Iryna. 2020. Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. Association for Computational Linguistics.\\n\\nDietmar Schabus and Marcin Skowron and Martin Trapp. 2017. One Million Posts: A Data Set of German Online Discussions.\"}"}
