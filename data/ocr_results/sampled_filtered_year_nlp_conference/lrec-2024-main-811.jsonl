{"id": "lrec-2024-main-811", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I Remember You!: SUI Corpus for Remembering and Utilizing Users' Information in Chat-oriented Dialogue Systems\\n\\nYuiko Tsunomori, Ryuichiro Higashinaka\\n\\nGraduate School of Informatics, Nagoya University, Japan\\n{tsunomori.yuiko.u9@s.mail, higashinaka@i}.nagoya-u.ac.jp\\n\\nAbstract\\n\\nTo construct a chat-oriented dialogue system that will be used for a long time by users, it is important to build a good relationship between the user and the system. To achieve a good relationship, several methods for remembering and utilizing information on users (preferences, experiences, jobs, etc.) in system utterances have been investigated. One way to do this is to utilize user information to fill in utterance templates for use in response generation, but the utterances do not always fit the context. Another way is to use neural-based generation, but in current methods, user information can be incorporated only when the current dialogue topic is similar to that of the user information. This paper tackled these problems by constructing a novel corpus to incorporate arbitrary user information into system utterances regardless of the current dialogue topic while retaining appropriateness for the context. We then fine-tuned a model for generating system utterances using the constructed corpus. The result of a subjective evaluation demonstrated the effectiveness of our model. Furthermore, we incorporated our fine-tuned model into a dialogue system and confirmed the effectiveness of the system through interactive dialogues with users.\\n\\nKeywords:\\nDialogue corpus, Chat-oriented dialogue system, User information\\n\\n1. Introduction\\n\\nThe demand for chat-oriented dialogue systems has been increasing in both research and commercial fields (Adiwardana et al., 2020; Shuster et al., 2022). To construct a chat-oriented dialogue system that will be used for a long time by users, it is important to build a good relationship between the user and the system, which requires that the user and the system know each other well (Richards and Bransky, 2014; Bickmore and Picard, 2005).\\n\\nIn human-to-human dialogue, it is effective to remember and utilize information on the dialogue partner, such as preferences and experiences disclosed by the other party, for building a good relationship (Hall, 2019). To build a good relationship between the system and the user, several methods that remember and utilize information on users (called \\\"user information\\\" in this paper) in system utterances have been investigated. Tsunomori et al. (2019) constructed a chat-oriented dialogue system that remembers and utilizes user information obtained from past dialogue and experimentally confirmed that incorporating user information into system utterances improves users' familiarity with chat-oriented dialogue systems. However, in their work, the system utterances were generated using templates to be filled with user information, which often caused inappropriate utterances with regard to the context.\\n\\nXu et al. (2022b) used neural-based models with dialogue context and user information as input to generate system utterances. However, in their method, user information can be incorporated only when the current dialogue topic is similar to that of the user information. This limits the opportunities for systems to utilize user information because user information similar to the current dialogue topic may not always be available in real-world settings.\\n\\nWe aim to realize a personalized chat-oriented dialogue system that builds a good relationship with users by remembering and utilizing arbitrary user information naturally and actively. Figure 1 shows a dialogue example from the personalized chat-oriented dialogue system that we aim to achieve. In the figure, the system references the user information extracted from its previous interaction with the user and then weaves it into its utterance.\\n\\nIn this paper, to realize such a personalized chat-oriented dialogue system, we constructed\"}"}
{"id": "lrec-2024-main-811", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the System utterance based on User Information corpus (SUI corpus) by extending an existing dialogue corpus. The language of the corpus is Japanese. The SUI corpus contains triplets formed of \\\\langle user information, dialogue context, system utterance based on the user information and dialogue context (expanded system utterance) \\\\rangle.\\n\\nWith this corpus as a basis, we constructed a model for generating system utterances. Our contributions are as follows.\\n\\n1. We constructed the SUI corpus; this is a novel corpus consisting of utterances incorporating various kinds of user information regardless of the current dialogue topic. The SUI corpus is publicly available.\\n\\n2. We fine-tuned a model to generate system utterances using the SUI corpus and conducted a subjective evaluation. The results showed that our model could incorporate arbitrary user information into system utterances regardless of the current dialogue topic while retaining appropriateness for the context.\\n\\n3. We incorporated our fine-tuned model into a dialogue system and confirmed the effectiveness of the system through a live interactive evaluation.\\n\\nRelated Work\\n\\nThere are several studies on personalizing system utterances in chat-oriented dialogue systems by utilizing user information extracted from dialogues with heuristic rules. Sugo and Hagiwara (2014) used rules to extract user information and used them in system utterances to show the user that the system can remember user information. Their system selects its utterances on the basis of the acquired preferences of the user.\\n\\nTsunomori et al. (2019) constructed a chat-oriented dialogue system that extracts and uses user information and confirmed the effectiveness of the system through evaluations of interactive dialogue with users. They reported that remembering and utilizing user information were important to make users feel familiar with a dialogue system. These studies use rules and templates for system utterance generation, which often makes it difficult to generate utterances while retaining appropriateness to the context.\\n\\nRecently, neural-based methods for utilizing user information for system utterance generation have been proposed. These methods can generate more natural utterances on the basis of dialogue contexts. Xu et al. (2022a) constructed a dialogue model that creates user information summaries from dialogue histories and uses them as the dialogue contexts for utterance generation. However, they did not incorporate arbitrary user information into system utterances. Similarly, Xu et al. (2022b) constructed a neural-based chat-oriented dialogue system that incorporates user information into system utterances. The model selects stored user information close to the current dialogue topic and uses the user information and dialogue context as input to generate system utterances. While it is reasonable to bring up user information related to previous system utterances when the topic is similar, we consider this to severely limit opportunities for the system to utilize user information. Therefore, this paper focuses on utterance generation using user information regardless of the current dialogue topic. It is also worth noting that Xu et al. (2022b) only performed evaluations using a user simulator; it is not known if the model will work effectively in interactive dialogue systems with users. We verify our model's effectiveness by incorporating the model into dialogue systems and evaluating the systems with users through a live interactive evaluation.\\n\\n3. System Utterance Based on User Information Corpus (SUI Corpus)\\n\\nTo achieve utterance generation that utilizes arbitrary user information while retaining appropriateness for the context, we constructed the SUI corpus on the basis of dialogue contexts and user information.\\n\\n3.1. Overview\\n\\nThe SUI corpus was constructed by extending the existing Osaka University Multimodal Dialogue Corpus (Hazumi) (Komatani et al., 2019). The Hazumi corpus is a person-to-system multimodal corpus in Japanese consisting of spoken dialogues between users and systems operating under the Wizard-of-Oz (WoZ) method. The wizard selects the system's responses through a dedicated interface and changes the topic in accordance with the user's interests when selecting utterances in the dialogues. For each of the dialogues, the wizard does not repeat the same topics. We used speech transcriptions from Hazumi1911, which consists of 30 dialogues by 30 users (2,859 turns in total). We chose this corpus because it contains dialogues in which a system talks about topics related to the user's interests, so we assumed it would contain a lot of user information.\\n\\nFigure 2 shows the flow for constructing the SUI corpus, which consisted of two tasks: (1)......\"}"}
{"id": "lrec-2024-main-811", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Pairs of Dialogue-1 and Dialogue-2\\n\\nWe created pairs of dialogue-1 and dialogue-2 (which are on different topics) to collect expanded system utterances. Dialogue-1 was used to extract user information, and dialogue-2 was used for dialogue context. First, we removed fillers and misspellings from the Hazumi1911 transcriptions by using heuristic rules, and then we divided the dialogues into topic segments using fixed utterances (\u201cLet\u2019s talk about [topic word]!\u201d, \u201cNow, I would like to move on to the next topic,\u201d and \u201cSo, this is the last question.\u201d) as delimiters that the wizard used to change topics. Approximately five topics were discussed per dialogue. We removed short topic segments with less than 15 utterances and created a total of 152 topic segments.\\n\\nFigure 3 illustrates the procedure for creating pairs of dialogue-1 and dialogue-2, which is described as follows.\\n\\n1. We select two dialogues from different topic segments by the same speaker. In chronological order, the one spoken earlier is dialogue-1, and the latter is a dialogue-2 source.\\n\\n2. We extract a portion of the dialogue-2 source to create dialogue-2. Specifically, let \\\\( U_{user}^1 \\\\) be the first user utterance that appears after the sixth utterance in the dialogue-2 source. From \\\\( U_{user}^1 \\\\), we extract a total of six utterances going back in time and name that portion of utterances dialogue-2. We extract dialogue-2 from \\\\( U_{user}^2 \\\\) in the same way. This is repeated until we reach \\\\( U_{user}^N \\\\), where \\\\( N \\\\) is the index of the last user utterance in the dialogue-2 source.\\n\\n3. Repeat 1\u20132 for all topic segments by the same speaker.\\n\\nAfter completing the above procedure for the data of all speakers, we had a total of 1,594 pairs of dialogue-1 and dialogue-2. Note that \u201cdialogue-2\u201d and \u201cdialogue-2 source\u201d are different entities. Dialogue-2 source refers to a topic segment obtained by dividing a dialogue in the Hazumi corpus by dialogue topics, whereas dialogue-2 is a dialogue obtained by dividing dialogue-2 source into groups of six utterances each.\\n\\nTo confirm whether the SUI corpus consists of user information with varied relevance to the dialogue context, we investigated the similarity between user information and dialogue contexts in the corpus. Specifically, we calculated the similarity of topic words between pairs of dialogue-1 and dialogue-2. We extracted word embeddings of the topic words using FastText (Bojanowski et al., 2017) trained by Wikipedia and then calculated the cosine similarity between them. The similarity score range between 0.2 and 0.3 had the highest frequency. The highest similarity score was 0.48 (\u201cmovie\u201d and \u201cmusic\u201d), and the lowest score was 0.09 (\u201csport\u201d and \u201cbook\u201d).\"}"}
{"id": "lrec-2024-main-811", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This result indicates that dialogue-1 contains topics with various degrees of similarity to dialogue-2. Using the pairs, we can expect to collect expanded system utterances incorporating various user information with a wide degree of relevance to the dialogue context.\\n\\n3.3. Collecting Expanded System Utterances\\n\\nWe used Lancers, a crowdsourcing service in Japan, to collect expanded system utterances for the SUI corpus. The workers created seven utterances for each pair of dialogue-1 and dialogue-2 in the following steps. The number of utterances to create was set to seven in consideration of the load on workers.\\n\\n1. From dialogue-1, as user information, extract as many user utterances that contain self-disclosures as possible. If the user utterance alone is not self-contained, the previous system utterance should also be extracted as part of the user information. For example, consider a situation where the system asks, \\\"What is your favorite food?\\\" and the user answers, \\\"Apple.\\\" In this case, since the user utterance alone is insufficient as self-disclosure, the previous system utterance should also be extracted.\\n\\n2. Select seven of the extracted user information items. If there are less than seven, select a total of seven overlapping items that can be used to create expanded system utterances.\\n\\n3. For each of the user information items selected in step 2, create an expanded system utterance by considering both the user information and dialogue-2 (dialogue context). When using the same user information, create different utterances. Note that it is not allowed to create utterances that forcibly incorporate user information items by using phrases such as \\\"By the way,\\\" \\\"Speaking of,\\\" and so on.\\n\\nIn total, 34 workers participated, and 10,801 expanded system utterances were collected.\\n\\nTable 1 shows example data from the SUI corpus, where the user talked about \\\"drinking alcohol\\\" in the past dialogue (user information) and is talking about \\\"listening to classical music\\\" in the current dialogue (dialogue context). The expanded system utterance, \\\"Do you ever enjoy your favorite classical music while drinking alcohol?\\\" naturally associated \\\"listening to classical music\\\" with \\\"drinking alcohol.\\\" Table 2 shows the statistics of the SUI corpus. Here, MeCab was used for word segmentation. Compared with Hazumi1911 (original), the expanded system utterances were longer and contained more words, reflecting the fact that the user information was incorporated.\\n\\n3.4. Quality Assessment\\n\\nWe conducted a quality assessment of the SUI corpus by using CrowdWorks, a crowdsourcing service in Japan. We randomly selected 1,000 expanded system utterances, and then each utterance was evaluated by three workers. We presented the workers with the user information, dialogue context, and expanded system utterances for assessment and had them judge each of the following three items on a binary scale of \\\"Yes/No\\\" for each expanded system utterance.\\n\\n- Dialogue context reflection: Is the expanded system utterance based on the dialogue context?\\n- User information reflection: Is the expanded system utterance based on the user information?\\n- Naturalness: Do you feel that the expanded system utterance is natural?\\n\\nNote that we imposed a binary decision here based on our preliminary study that indicated the difficulty of judging the degree of how much user information is included in an utterance.\\n\\nTable 3 lists the results of the quality assessment. The annotation agreement statistics (Fleiss') show that the agreement rate exceeded 0.5 for dialogue context reflection and user information reflection, which indicates moderate agreement. In contrast, the agreement rate for naturalness was poor, which suggests that the judgment of naturalness is highly subjective. This is in accordance with previous work that shows that subjective single-turn evaluations in dialogues tend to show low agreement (Higashinaka et al., 2015; Ghandeharioun et al., 2019).\\n\\nThe proportions of \\\"Yes\\\" responses to dialogue context reflection and user information reflection were both over 70%. This indicates that a high proportion of expanded system utterances were based on both user information and dialogue contexts. In contrast, the proportion of \\\"Yes\\\" for naturalness was less than 60%, reflecting the possible difficulty of incorporating user information smoothly even for humans.\\n\\nWe further assessed the quality of the expanded system utterances to ascertain whether they could be good enough references for training generation models even if not very natural. For this assessment, we performed dialogue breakdown annotations and investigated whether the naturalness...\"}"}
{"id": "lrec-2024-main-811", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do you like to drink alcohol? (\u0353{Name:26}\u07b7{Name:26}\u0356{Name:26}\u0281)\\n\\nYes, I drink beer, sake, shochu, and most other alcohol. ({Name:26}\u0366{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name:26}{Name"}
{"id": "lrec-2024-main-811", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1. Fine-tuning Settings\\n\\nWe fine-tuned an existing pre-trained model using the SUI corpus. We used an encoder-decoder model based on Transformer (Adiwardana et al., 2020; Roller et al., 2021) as a pre-trained model. Specifically, we used a Japanese Transformer encoder-decoder dialogue model trained with a large amount of Twitter reply pairs and the Japanese PersonaChat corpus (Sugiyama et al., 2023) as the pre-trained model. The number of parameters is 1.6B. The SUI corpus was divided into train/dev/test datasets. We split the 30 dialogues in Hazumi1911 into train:dev:test = 24:3:3 by avoiding overlapping dialogues by the same speaker.\\n\\nSentencePiece (Kudo and Richardson, 2018) was used for tokenization. We used an NVIDIA Tesla V100 as the GPU. As the hyperparameters used in training, the batch size was 8, the optimizer was Adam, and the loss function was a label-smoothed cross-entropy. The learning rate was 1e-04 with a minimum learning rate of 1e-09. The learning rate schedule used inverse sqrt. We applied an early stopping strategy with a patience of 5 and evaluated the model on the dev data at each epoch. The model with the lowest validation loss was utilized for the evaluation.\\n\\n4.2. Evaluation Settings\\n\\nThe two models we compared are as follows.\\n\\n- Fine-tuned (ours)\\n  A model was fine-tuned with the SUI corpus. The input was dialogue context and user information. The input format was \\\"tokenized user information [SEP] tokenized dialogue context.\\\"\\n\\n- Vanilla\\n  A vanilla Japanese Transformer encoder-decoder dialogue model, not fine-tuned with the SUI corpus. The input was only dialogue context.\\n\\nWe used Lancers to evaluate the generated system utterances. We created an evaluation dataset that included a total of 200 utterances broken down into 100 system utterances generated by each of the two models for the same input. Three workers evaluated each utterance. The other evaluation settings were the same as those for the quality assessment described in Section 3.4.\\n\\n4.3. Results and Analysis\\n\\nTable 5 lists the results of the evaluation. We used the Wilcoxon signed-rank test (Wilcoxon, 1945) for the statistical test. The fine-tuned model had a higher average score than the vanilla model, especially for the user information reflection score. These results indicate that these models can effectively generate utterances on the basis of user information.\\n\\n| | Vanilla | Fine-tuned |\\n|---|---|---|\\n| Ave. | 0.81 | 0.83 |\\n| b | 0.64 | 0.71 |\\n\\nTable 5: Results of subjective evaluation. Percentage of \\\"Yes\\\" for each item is shown. \\\"Ave.\\\" means average of three values. Superscripts a\u2013b next to numbers indicate systems with which that value was statistically better (p < :). Bold font represents top score for each evaluation criterion.\\n\\n| | No. of letters | No. of words |\\n|---|---|---|\\n| Vanilla | 14.05 | 8.50 |\\n| Fine-tuned | 32.06 | 19.02 |\\n| Gold | 34.69 | 20.16 |\\n\\nTable 6: Statistics of system utterances generated by models used for comparison. Gold means expanded system utterances (manually created).\\n\\nThe naturalness score for the fine-tuned model was slightly lower than for the vanilla model, but we believe that this is acceptable because the model was forced to incorporate new information.\\n\\nCompared with manually created utterances (Gold), the naturalness score for the fine-tuned model was 0.71, and that of Gold was 0.56 (Table 3), which means that when considering naturalness alone, the fine-tuned model exceeded human-level performance, possibly because the model placed more emphasis on generating fluent utterances, rather than incorporating user information.\\n\\nAlthough the size of the SUI corpus is not very large, we found that the model fine-tuned with the SUI corpus successfully generated reasonable utterances.\\n\\nTable 6 shows the statistics of the system utterances generated by each model in the test set, and we found that the utterances generated by the fine-tuned model had almost the same length as the expanded system utterances (i.e., Gold). Table 7 shows examples of system utterances generated by each model. In the utterances generated by the fine-tuned model and Gold, \\\"drama\\\" from the dialogue context and \\\"cake\\\" or \\\"sweets\\\" from the user information were used in the generated content. This example shows that the fine-tuned model successfully generated natural utterances on the basis of user information and dialogue context.\\n\\nTo investigate whether the quality of generated utterances varies in accordance with the similarity between contexts and user information, we analyzed the relationship between user evaluation...\"}"}
{"id": "lrec-2024-main-811", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If you have any drama recommendations, please let me know. Marigold. What is the drama about? It is a story about a man who sees the future when he puts his hand in contact with a person who has been wounded and tries to change it. What do you like about this drama? Because of the beautiful ending.\\n\\nUser information\\nDo you have any recommended sweets? Cake. Vanilla I see. I'll watch it sometime.\\n\\nVanilla I see. I'll watch it sometime. (\u03b5\u03a0\u0294\u03c0\u05e5\u0294\u03bf\u03c5)\\n\\nFine-tuned\\nI am sure you watched that drama while eating your favorite cake. (\u03b5\u03a0\u0294\u03c0\u05e5\u0294\u03bf\u03c5)\\n\\nGold Do you watch dramas where the theme is sweets or gourmet food? (\u03b5\u03a0\u0294\u03c0\u05e5\u0294\u03bf\u03c5)\\n\\nTable 7: Example system utterances generated by models used for comparison. All utterances were originally in Japanese. English translations were done by authors.\\n\\nTable 8: Statistics of average naturalness evaluation scores and cosine similarity scores between contexts and user information. We extracted the sentence embeddings of contexts and user information using Sentence-BERT (Reimers and Gurevych, 2019) (we used stsb-xlm-r-multilingual) and then calculated the cosine similarity between their embeddings. Table 8 shows the relationship between the average naturalness evaluation scores of the fine-tuned model given by three workers and cosine similarity scores between contexts and user information. As a result, we found that the evaluation scores had little relevance to the similarity scores between contexts and user information. This is a good indication that the quality of generated utterances does not depend on the similarity between contexts and user information. As opposed to the work by Xu et al. (2022b) that incorporates user information only when the topics are similar, this confirms that our models can incorporate arbitrary user information into system utterances regardless of the degree of similarity between contexts and user information.\\n\\n5. Live Interactive Experiment\\nDespite the positive results in the previous section, it was still not clear if our fine-tuned model would work effectively in an interactive dialogue system with users. Therefore, in this section, we developed a chat-oriented dialogue system incorporating our fine-tuned model and evaluated its effectiveness through a live interactive evaluation.\\n\\n5.1. Systems for Comparison\\nWe developed three chat-oriented dialogue systems; two of them were baselines, and one was a system based on our fine-tuned model.\\n\\nVanilla This model does not utilize user information to generate system utterances. Utterances are generated by the vanilla Japanese Transformer encoder-decoder dialogue model (Sugiyama et al., 2023). Although we used a model fine-tuned with Japanese PersonaChat in Section 4.1, here we used a model fine-tuned with Japanese EmpatheticDialogues because this leads to more coherent dialogue.\\n\\nUInfoRule This model is a replication of the work by Tsunomori et al. (2019). Utterances are randomly generated by hand-crafted rules with a probability of 30% and by Vanilla with a probability of 70%. These ratios were determined in line with (Tsunomori et al., 2019). The rules use handcrafted templates such as \\\"By the way, you talked about [word], didn\u2019t you? Let\u2019s talk more about it.\\\" to be filled in with a word from user information. For example, given the user information \\\"I go to concerts,\\\" the generated utterance would be \\\"By the way, you talked about concerts, didn\u2019t you? Let\u2019s talk more about it.\\\" We manually selected words to be used from user information for the experiment.\\n\\nUInfoGen (ours) Utterances are randomly generated by our fine-tuned model in Section 4 with a probability of 30% and by Vanilla with a probability of 70%. UInfoGen generates utterances regardless of the degree of similarity between dialogue contexts and user information.\"}"}
{"id": "lrec-2024-main-811", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Results of interaction evaluation (7 is highest). Superscripts a\u2013c next to numbers indicate systems with which that value was statistically better. Double letters (e.g., aa) mean $p < 0.01$; otherwise, $p < 0.05$.\\n\\n5.2. Evaluation Settings\\n\\nWe used CrowdWorks to recruit 50 workers who conducted dialogues in a text-chat interface with the three systems. The order of the systems was randomized. The workers were instructed to read a dialogue displayed in the chat interface as their own past dialogue with the system. The dialogues covered five items of user information selected randomly from the SUI corpus (test set in Section 4.1). Then, the workers conducted a dialogue with each system lasting 15 turns (30 utterances in total). After each dialogue session, they evaluated the system by indicating their degree of agreement with the following questions using a seven-point Likert scale. The questions were modified versions of those used in (Tsunomori et al., 2019).\\n\\nQ1: The utterances of this dialogue system are easy to understand.\\nQ2: The utterances of this dialogue system are interesting and informative.\\nQ3: This dialogue system sounds familiar.\\nQ4: This dialogue system remembers the contents of the past dialogue.\\nQ5: This dialogue system appropriately uses the contents of the past dialogue.\\nQ6: The utterances of this dialogue system are natural.\\nQ7: I want to talk to this dialogue system again.\\nQ8: I am satisfied with this dialogue.\\n\\nWe mainly added questions concerning the system's ability to remember and use user information.\\n\\n5.3. Results and Analysis\\n\\nWhen we compare UInfoGen and UInfoRule, both had high scores for Q4 (remembering). This indicates that UInfoRule and UInfoGen made users feel remembered by the system. However, UInfoRule did not appropriately use past dialogue because it had a lower score for Q5 (using past dialogues). In addition, it had significantly lower scores for all questionnaire items except for Q1 (understanding) and Q4. We found that incorporating user information into utterances using simple templates without considering dialogue context lowers the overall score.\\n\\nWhen we compare UInfoGen and Vanilla, both had equally high scores, and there was no significant difference between them for all questionnaire items except for Q4 (remembering) and Q5 (using past dialogue). For Q4 and Q5, UInfoGen was significantly better. This indicates that our model, fine-tuned by the SUI corpus, enabled a dialogue system to remember and utilize user information; our model worked effectively in an interactive dialogue system with users. In addition, UInfoGen was better for Q2 (informative). By utilizing user information, UInfoGen succeeded in incorporating more information into utterances.\\n\\nFor system utterances generated by UInfoGen using user information, we calculated the cosine similarity between the dialogue context (up to the last three turns) and each piece of user information in the same manner as Section 4.3. The average similarity score was 0.29. The cosine similarity threshold in the work by (Xu et al., 2022b) was set to 0.7. However, in this experiment, the percentage of similarity score above 0.7 was just 2%, and that above 0.5 was 10%, indicating the importance of being able to incorporate arbitrary user information in system utterances. Note that, although Xu et al. (2022b) used ERNIE (Sun et al., 2020) embeddings instead of Sentence-BERT to calculate cosine similarity, the range of similarity values should fall in a similar range.\"}"}
{"id": "lrec-2024-main-811", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conclusion and Future Work\\n\\nIn this paper, to build a good relationship between systems and users by remembering and utilizing arbitrary user information naturally and actively, we constructed a novel corpus, the System utterance based on User Information corpus (SUI corpus). This corpus takes into account both user information on various topics and dialogue context. We fine-tuned a model to generate system utterances using the SUI corpus and conducted a subjective evaluation. The results showed that our fine-tuned model could incorporate arbitrary user information into system utterances regardless of the current dialogue topic while retaining appropriateness for the context. In addition, we found that our fine-tuned model was effective in a live interactive dialogue system.\\n\\nThere is still much room for improvement, especially in incorporating our fine-tuned model into dialogue systems. Our model sometimes generates unnatural utterances to incorporate arbitrary user information. We want to analyze the timing for effectively incorporating user information in dialogues since we only had random choice, which is obviously not the optimal solution. In fact, there were instances where the generated utterances disrupted the conversation flow. We would like to explore methods for reducing transitions that clearly cause problems, aiming to mitigate their impact. We would also like to enable the automatic extraction of user information and evaluate the system in real-world settings.\\n\\nWe also want to test the application of large language models (LLMs). Recently, few-shot learning using pre-trained LLMs has been applied successfully in generating natural sentences while taking into account the given information (Kasahara et al., 2022; Lee et al., 2022; Liu et al., 2022; Han et al., 2022). Thus, we believe that LLMs applied with the SUI corpus by few-shot learning methods could generate more natural utterances while incorporating user information.\"}"}
{"id": "lrec-2024-main-811", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"description, datasets, and evaluation metrics. In Proceedings of the Language Resources and Evaluation Conference (LREC), pages 3146\u20133150.\\n\\nRyuichiro Higashinaka, Masahiro Mizukami, Kotaro Funakoshi, Masahiro Araki, Hiroshi Tsukahara, and Yuka Kobayashi. 2015. Fatal or not? finding errors that lead to dialogue breakdowns in chat-oriented dialogue systems. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2243\u20132248.\\n\\nTomohito Kasahara, Daisuke Kawahara, Nguyen Tung, Shengzhe Li, Kenta Shinzato, and Toshinori Sato. 2022. Building a personalized dialogue system with prompt-tuning. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL -HLT) Student Research Workshop, pages 96\u2013105.\\n\\nKazunori Komatani, Shogo Okada, Haruto Nishimoto, Masahiro Araki, and Mikio Nakano. 2019. Multimodal dialogue data collection and analysis of annotation disagreement. In Proceedings of the International Workshop on Spoken Dialogue Systems Technology (IWSDS), pages 201\u2013213.\\n\\nTaku Kudo and John Richardson. 2018. Sentence-Piece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 66\u201371.\\n\\nYoung-Jun Lee, Chae-Gyun Lim, Yunsu Choi, Ji-Hui Lm, and Ho-Jin Choi. 2022. PERSONACHATGEN: Generating personalized dialogues using GPT-3. In Proceedings of the Workshop on Customized Chat Grounding Persona and Knowledge, pages 29\u201348.\\n\\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2122\u20132132.\\n\\nZihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai Prabhumoye, Wei Ping, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Multi-stage prompting for knowledgeable dialogue generation. In Findings of the Association for Computational Linguistics (ACL), pages 1317\u20131337.\\n\\nBilyana Martinovsky and David Traum. 2006. The error is the clue: Breakdown in human-machine interaction. In Proceedings of the ISCA Workshop on Error Handling in Spoken Dialogue Systems, pages 11\u201316.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992.\\n\\nDeborah Richards and Karla Bransky. 2014. ForgetMeNot: What and how users expect intelligent virtual agents to recall and forget personal conversational content. International Journal of Human-Computer Studies, 72(5):460\u2013476.\\n\\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. 2021. Recipes for building an open-domain chatbot. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 300\u2013325.\\n\\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. 2022. BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage. CoRR, arXiv:2208.03188v3.\\n\\nHiroaki Sugiyama, Masahiro Mizukami, Tsunehiro Arimoto, Hiromi Narimatsu, Yuya Chiba, Hideharu Nakajima, and Toyomi Meguro. 2023. Empirical analysis of training strategies of transformer-based Japanese chit-chat systems. In Proceedings of IEEE Spoken Language Technology Workshop (SLT), pages 685\u2013691.\\n\\nKensuke Sugo and Masafumi Hagiwara. 2014. A dialogue system with knowledge acquisition ability from user's utterance. Journal of Japan Society of Kansei Engineering, 13(4):519\u2013526. (In Japanese).\\n\\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0: A continual pre-training framework for language understanding. In Proceedings of the AAAI conference on artificial intelligence (AAAI), volume 34, pages 8968\u20138975.\"}"}
{"id": "lrec-2024-main-811", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yuiko Tsunomori, Ryuichiro Higashinaka, Takeshi Yoshimura, and Yoshinori Isoda. 2019. Chat-oriented dialogue system that uses user information acquired through dialogue and its long-term evaluation. In Proceedings of the International Workshop on Spoken Dialogue Systems Technology (IWSDS), pages 227\u2013238.\\n\\nFrank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80\u201383.\\n\\nJing Xu, Arthur Szlam, and Jason Weston. 2022a. Beyond goldfish memory: Long-term open-domain conversation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 5180\u20135197.\\n\\nXinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. 2022b. Long time no see! Open-domain conversation with long-term persona memory. In Findings of the Association for Computational Linguistics (ACL), pages 2639\u20132650.\\n\\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT: Large-scale generative pre-training for conversational response generation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL): System Demonstrations, pages 270\u2013278.\"}"}
