{"id": "acl-2022-long-303", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\\nVolume 1: Long Papers, pages 4419 - 4431, May 22-27, 2022\\n\u00a9 2022 Association for Computational Linguistics\\n\\nFiNER: Financial Numeric Entity Recognition for XBRL Tagging\\n\\nLefteris Loukas1,2, Manos Fergadiotis1,2, Ilias Chalkidis3, Eirini Spyropoulou1, Prodromos Malakasiotis1,2, Ion Androutsopoulos1,2, George Paliouras1,\\n\\nInstitute of Informatics and Telecommunications, NCSR \u201cDemokritos\u201d\\nDepartment of Informatics, Athens University of Economics and Business\\nDepartment of Computer Science, University of Copenhagen\\n\\nAbstract\\n\\nPublicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (xbrl) word-level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce xbrl tagging as a new entity extraction task for the financial domain and release finer-139, a dataset of 1.1M sentences with gold xbrl tags. Unlike typical entity extraction datasets, finer-139 uses a much larger label set of 139 entity types. Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself. We show that subword fragmentation of numeric expressions harms bert\u2019s performance, allowing word-level bilstm s to perform better. To improve bert\u2019s performance, we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting original token shapes and numeric magnitudes. We also experiment with finerbert, an existing bert model for the financial domain, and release our own bert (sec-bert), pre-trained on financial filings, which performs best. Through data and error analysis, we finally identify possible limitations to inspire future work on xbrl tagging.\\n\\n1 Introduction\\n\\nNatural language processing (nlp) for finance is an emerging research area (Hahn et al., 2019; Chen et al., 2020; El-Haj et al., 2020). Financial data are mostly reported in tables, but substantial information can also be found in textual form, e.g., in company filings, analyst reports, and economic news. Such information is useful in numerous financial intelligence tasks, like stock market prediction (Chen et al., 2019; Yang et al., 2019), financial sentiment analysis (Malo et al., 2014; Wang et al., 2013; Akhtar et al., 2017), economic event detection (Ein-Dor et al., 2019; Jacobs et al., 2018; Zhai and Zhang, 2019), and causality analysis (Tabari et al., 2018; Izumi and Sakaji, 2019). In this work, we study how financial reports can be automatically enriched with word-level tags from the eXtensive Business Reporting Language (xbrl), a tedious and costly task not considered so far.\\n\\nTo promote transparency among shareholders and potential investors, publicly traded companies are required to file periodic financial reports. These comprise multiple sections, including financial tables and text paragraphs, called text notes. In addition, legislation in the us, the uk, the eu and elsewhere requires the reports to be annotated with tags of xbrl, an xml-based language, to facilitate the processing of financial information. The annotation of tables can be easily achieved by using company-specific pre-tagged table templates, since the structure and contents of the tables in the reports of a particular company rarely change. On the other hand, the unstructured and dynamic nature of text notes (Figure 1) makes adding xbrl tags to them much more difficult. Hence, we focus on automatically tagging text notes. Tackling this task could facilitate the annotation of new and old reports (which may not include xbrl tags), e.g.,\\n\\n1See https://www.xbrl.org/the-standard/what/an-introduction-to-xbrl/ for an introduction to xbrl.\"}"}
{"id": "acl-2022-long-303", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards this direction, we release finer-139, a new dataset of 1.1M sentences with gold xbrl tags, from annual and quarterly reports of publicly traded companies obtained from the US Securities and Exchange Commission (sec). Unlike other entity extraction tasks, like named entity recognition (ner) or contract element extraction (Table 1), which typically require identifying entities of a small set of common types (e.g., persons, organizations), xbrl defines approx. 6k entity types. As a first step, we consider the 139 most frequent xbrl entity types, still a much larger label set than usual.\\n\\nAnother important difference from typical entity extraction is that most tagged tokens (\u223c91%) in the text notes we consider are numeric, with the correct tag per token depending mostly on context, not the token itself (Figure 1). The abundance of numeric tokens also leads to a very high ratio of out-of-vocabulary (oov) tokens, approx. 10.4% when using a custom word2vec (Mikolov et al., 2013a) model trained on our corpus. When using subwords, e.g., in models like bert (Devlin et al., 2019), there are no oov tokens, but numeric expressions get excessively fragmented, making it difficult for the model to gather information from the fragments and correctly tag them all. In our experiments, this is evident by the slightly better performance of stacked bilstm (Graves et al., 2013; Lample et al., 2016) operating on word embeddings compared to bert. The latter improves when using a crf (Lafferty et al., 2001) layer, which helps avoid assigning nonsensical sequences of labels to the fragments (subwords) of numeric expressions.\\n\\nTo further improve bert\u2019s performance, we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting the original token shapes and magnitudes. We also experiment with finer-bert (Yang et al., 2020), an existing bert model for the financial domain, and release our own family of bert models, pre-trained on 200k financial filings, achieving the best overall performance.\\n\\nOur key contributions are:\\n\\n1. We introduce xbrl tagging, a new financial nlp task for a real-world need, and we release finer-139, the first xbrl tagging dataset.\\n2. We provide extensive experiments and baselines with generic or in-domain pre-training, which establish strong baseline results for future work on finer-139.\\n3. We show that replacing numeric tokens with pseudo-tokens reflecting token shapes and magnitudes significantly boosts the performance of bert-based models in this task.\\n4. We release a new family of bert models (sec-bert, sec-bert-num, sec-bert-shape) pre-trained on 200k financial filings that obtains the best results on finer-139.\\n\\n**Table 1** Examples of previous entity extraction datasets. Information about the first four from Tjong Kim Sang and De Meulder (2003); Pradhan et al. (2012); Doddington et al. (2004); Kim et al. (2003).\\n\\n**Domain** | **Entity Types** | **References**\\n--- | --- | ---\\n**conll-2003** | Generic | 4\\n**ontonotes-v5** | Generic | 18\\n**ace-2005** | Generic | 7\\n**genia** | Biomedical | 36\\n**Chalkidis et al. (2019)** | Legal | 14\\n**Francis et al. (2019)** | Financial | 9\\n**finer-139 (ours)** | Financial | 139\\n\\n**Related Work**\\n\\nEntity extraction: xbrl tagging differs from ner and other previous entity extraction tasks (Table 1), like contract element extraction (Chalkidis et al., 2019). Crucially, in xbrl tagging there is a much larger set of entity types (6k in full xbrl, 139 in finer-139), most tagged tokens are numeric (\u223c91%), and the correct tag highly depends on context. In most ner datasets, numeric expressions are classified in generic entity types like 'amount' or 'date' (Bikel et al., 1999); this can often be achieved with regular expressions that look for common formats of numeric expressions, and the latter are often among the easiest entity types in ner datasets.\\n\\nBy contrast, although it is easy to figure out that the first three highlighted expressions of Figure 1 are amounts, assigning them the correct xbrl tags requires carefully considering their context. Contract element extraction (Chalkidis et al., 2019) also requires considering the context of dates, amounts etc. to distinguish, for example, start dates from end dates, total amounts from other mentioned amounts, but the number of entity types in finer-139 is an order of magnitude larger (Table 1) and the full tag set of xbrl is even larger (6k).\\n\\n---\\n\\n3 https://huggingface.co/nlpaueb/sec-bert-base\\n4 https://huggingface.co/nlpaueb/sec-bert-num\\n5 https://huggingface.co/nlpaueb/sec-bert-shape\"}"}
{"id": "acl-2022-long-303", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Previous financial applications use at most 9 (generic) class labels. Salinas Alvarado et al. (2015) investigated financial in finance to recognize organizations, persons, locations, and miscellaneous entities on 8 manually annotated financial agreements using CRFs. Francis et al. (2019) experimented with transfer learning by unfreezing different layers of a BILSTM with a CRF layer, pre-trained on invoices, to extract 9 entity types with distinct morphological patterns (e.g., IBAN, company name, date, total amount). Also, Hampton et al. (2015, 2016) applied a Maximum Entropy classifier, CRFs, and handcrafted rules to London Stock Exchange filings to detect 9 generic entity types (e.g., person, organization, location, money, date, percentages). Finally, Kumar et al. (2016) extended the work of Finkel et al. (2005) and built a financial entity recognizer of dates, numeric values, economic terms in sec and non-sec documents, using numerous handcrafted text features. By contrast, fi-ner-139 uses a specialized set of 139 highly technical economic tags derived from the real-world need of xbrl tagging, and we employ no handcrafted features.\\n\\nNumerical reasoning:\\nNeural numerical reasoning studies how to represent numbers to solve numeracy tasks, e.g., compare numbers, understand mathematical operations mentioned in a text, etc. Zhang et al. (2020) released NumBERT, a Transformer-based model that handles numerical reasoning tasks by representing numbers by their scientific notation and applying subword tokenization. On the other hand, GenBERT (Geva et al., 2020) uses the decimal notation and digit-by-digit tokenization of numbers. Both models attempt to deal with the problem that word-level tokenization often turns numeric tokens to OOVs (Thawani et al., 2021). This is important, because numerical reasoning requires modeling the exact value of each numeric token. In fin-ner-139, the correct xbrl tags of numeric tokens depend much more on their contexts and token shapes than on their exact numeric values (Fig. 1). Hence, these methods are not directly relevant.\\n\\n3 Task and Dataset\\nTraditionally, business filings were simply rendered in plain text. Thus, analysts and researchers needed to manually identify, copy, and paste each amount of interest (e.g., from filings to spreadsheets). With xbrl-tagged filings, identifying and extracting amounts of interest (e.g., to spreadsheets or databases) can be automated. More generally, xbrl facilitates the machine processing of financial documents. Hence, xbrl-tagged financial reports are required in several countries, as already noted (Section 1). However, manually tagging reports with xbrl tags is tedious and resource-intensive. Therefore, we release fi-ner-139 to foster research towards automating xbrl tagging.\\n\\nFine-ner-139 was compiled from approx. 10k annual and quarterly English reports (filings) of publicly traded companies downloaded from sec\u2019s edgar system. The downloaded reports span a 5-year period, from 2016 to 2020. They are annotated with xbrl tags by professional auditors and describe the performance and projections of the companies. We used regular expressions to extract the text notes from the Financial Statements Item of each filing, which is the primary source of xbrl tags in annual and quarterly reports. xbrl taxonomies have many different attributes, making xbrl tagging challenging even for humans (Baldwin et al., 2006; Hoitash and Hoitash, 2018). Furthermore, each jurisdiction has its own xbrl taxonomy. Since we work with US documents, our labels come from us-gaap. Since this is the first effort towards automatic xbrl tagging, we chose to work with the most essential and informative attribute, the tag names, which populate our label set. Also, since xbrl tags change periodically, we selected the 139 (out of 6,008) most frequent xbrl tags with at least 1,000 appearances in finer-139.\\n\\n6https://www.sec.gov/edgar/\\n7www.xbrl.us/xbrl-taxonomy/2020-us-gaap/\"}"}
{"id": "acl-2022-long-303", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: \\\\( f_{i-139} \\\\) statistics, using SpaCy's tokenizer and the 139 tags of this work (\u00b1 standard deviation).\\n\\nThe distribution of these tags seems to follow a power law (Figure 2), hence most of the 6k xbrl tags that we did not consider are very rare. We used the iob2 annotation scheme to distinguish tokens at the beginning, inside, or outside of tagged expressions, which leads to 279 possible token labels.\\n\\nWe split the text notes into 1.8M sentences, the majority of which (\u223c90%) contained no tags. The sentences are also html-stripped, normalized, and lower-cased. To avoid conflating trivial and more difficult cases, we apply heuristic rules to discard sentences that can be easily flagged as almost certainly requiring no tagging; in a real-life setting, the heuristics, possibly further improved, would discard sentences that do not need to be processed by the tagger. The heuristic rules were created by inspecting the training subset and include regular expressions that look for amounts and other expressions that are typically annotated. Approx. 40% of the 1.8M sentences were removed, discarding only 1% of tagged ones. We split chronologically the remaining sentences into training, development, and test sets with an 80/10/10 ratio (Table 2).\\n\\n4 Baseline Models\\n\\nSpacy (Honnibal et al., 2020) is an open-source nlp library. It includes an industrial ner that uses word-level Bloom embeddings (Serr\u00e0 and Karpitzoglou, 2017) and residual Convolutional Neural Networks (cnn) (He et al., 2016). We trained SpaCy's ner from scratch on \\\\( f_{i-139} \\\\).\\n\\nBilstm: This baseline uses a stacked bidirectional Long-Short Term Memory (lstm) network (Graves et al., 2013; Lample et al., 2016) with residual connections. Each token \\\\( t_i \\\\) of a sentence \\\\( S \\\\) is mapped to an embedding and passed through the bilstm stack to extract the corresponding contextualized embedding. A shared multinomial logistic regression (lr) layer operates on top of each contextualized embedding to predict the correct label. We use the word2vec embeddings (Mikolov et al., 2013a,b) of Loukas et al. (2021).\\n\\nBert: This is similar to bilstm, but now we fine-tune bert-base (Devlin et al., 2019) to extract contextualized embeddings of subwords. Again, a multinomial lr layer operates on top of the contextualized embeddings to predict the correct label of the corresponding subword.\\n\\nCrf: In this case, we replace the lr layer of the previous two models with a Conditional Random Field (crf) layer (Lafferty et al., 2001), which has been shown to be beneficial in several token labeling tasks (Huang et al., 2015; Lample et al., 2016; Chalkidis et al., 2020b).\\n\\n5 Baseline Results\\n\\nWe report micro-F1 (\\\\( \\\\mu \\\\)-F1) and macro-F1 (m-F1) at the entity level, i.e., if a gold tag annotates a multi-word span, a model gets credit only if it tags the exact same span. This allows comparing more easily methods that label words vs. subwords.\\n\\nTable 3 shows that SpaCy performs poorly, possibly due to the differences from typical token labeling tasks, i.e., the large amount of entity types, the abundance of numeric tokens, and the fact that in \\\\( f_{i-139} \\\\) the tagging decisions depend mostly on context. Interestingly enough, bilstm (with word embeddings) performs slightly better than bert. However, when a crf layer is added, bert achieves the best results, while the performance of bilstm (with word embeddings) deteriorates significantly, contradicting previous studies.\\n\\nBaseline methods\\n\\n| Model | \u00b5-F1 | m-F1 |\\n|-------|------|------|\\n| SpaCy (words) | 48.6 \u00b1 0.4 | 37.6 \u00b1 0.2 |\\n| Bilstm (words) | 77.3 \u00b1 0.6 | 73.8 \u00b1 1.8 |\\n| Bilstm (subwords) | 71.3 \u00b1 0.2 | 68.6 \u00b1 0.2 |\\n| Bert (subwords) | 75.1 \u00b1 1.1 | 72.6 \u00b1 1.4 |\\n| Bilstm (words) + crf | 69.4 \u00b1 1.2 | 67.3 \u00b1 1.6 |\\n| Bilstm (subwords) + crf | 76.2 \u00b1 0.2 | 73.4 \u00b1 0.3 |\\n| Bert (subwords) + crf | 78.0 \u00b1 0.5 | 75.2 \u00b1 0.6 |\\n\\nTable 3: Entity-level \u00b5-F1 and m-F1 (% avg. of 3 runs with different random seeds, \u00b1 std. dev.) on test data.\\n\\nWe hypothesize that the inconsistent effect of crf is due to tokenization differences. When using bert's subword tokenizer, there are more decisions that need to be all correct for a tagged span to be correct (one decision per subword) than when using word tokenization (one decision per word). Thus, it becomes more difficult for subword models to avoid nonsensical sequences of token labels,\"}"}
{"id": "acl-2022-long-303", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"e.g., labeling two consecutive subwords as beginning and inside of different entity types, especially given the large set of 279 labels (Table 1). The crf layer on top of subword models helps reduce the nonsensical sequences of labels. On the other side, when using words as tokens, there are fewer opportunities for nonsensical label sequences, because there are fewer tokens. For instance, the average number of subwords and words per gold span is 2.53 and 1.04, respectively. Hence, it is easier for the bilstm to avoid predicting nonsensical sequences of labels and the crf layer on top of the bilstm (with word embeddings) has less room to contribute and mainly introduces noise (e.g., it often assigns low probabilities to acceptable, but less frequent label sequences). With the crf layer, the model tries to maximize both the confidence of the bilstm for the predicted label of each word and the probability that the predicted sequence of labels is frequent. When the bilstm on its own rarely predicts nonsensical sequences of labels, adding the crf layer rewards commonly seen sequences of labels, even if they are not the correct labels, without reducing the already rare nonsensical sequences of labels.\\n\\nTo further support our hypothesis, we repeated the bilstm experiments, but with subword (instead of word) embeddings, trained on the same vocabulary with bert. Without the crf, the subword bilstm performs much worse than the word bilstm (6 p.p drop in \u00b5-F1), because of the many more decisions and opportunities to predict nonsensical label sequences. The crf layer substantially improves the performance of the subword bilstm (4.9 p.p. increase in \u00b5-F1), as expected, though the word bilstm (without crf) is still better, because of the fewer opportunities for nonsensical predictions. A drawback of crfs is that they significantly slow down the models both during training and inference, especially when using large label sets (Goldman and Goldberger, 2020), as in our case. Hence, although bert with crf was the best model in Table 3, we wished to improve bert\u2019s performance further without employing crfs.\\n\\n6 Fragmentation in bert\\n\\nIn Figure 3, the majority (91.2%) of the gold tagged spans are numeric expressions, which cannot all be included in bert\u2019s finite vocabulary; e.g., the token \u20189,323.0\u2019 is split into five subword units, [\u20189\u2019, \u2018##\u2019, \u2018##323\u2019, \u2018##\u2019, \u2018##0\u2019], while the token \u201812.78\u2019 is split into [\u201812\u2019, \u2018##\u2019, \u2018##78\u2019]. The excessive fragmentation of numeric expressions, when using subword tokenization, harms the performance of the subword-based models (Table 3), because it increases the probability of producing nonsensical sequences of labels, as already discussed. We, therefore, propose two simple and effective solutions to avoid the over-fragmentation of numbers.\\n\\nbert+num: We detect numbers using regular expressions and replace each one with a single [num] pseudo-token, which cannot be split. The pseudo-token is added to the bert vocabulary, and its representation is learned during fine-tuning. This allows handling all numeric expressions in a uniform manner, disallowing their fragmentation.\\n\\nbert+shape: We replace numbers with pseudo-tokens that cannot be split and represent the number\u2019s shape. For instance, \u201853.2\u2019 becomes \u2018[XX.X]\u2019, and \u201840,200.5\u2019 becomes \u2018[XX,XXX.X]\u2019. We use 214 special tokens that cover all the number shapes of the training set. Again, the representations of the pseudo-tokens are fine-tuned, and numeric expressions (of known shapes) are no longer fragmented. The shape pseudo-tokens also capture information about each number\u2019s magnitude; the intuition is that numeric tokens of similar magnitudes may require similar xbrl tags. Figure 3 illustrates the use of [num] and [shape] pseudo-tokens.\\n\\n7 In-domain Pre-training\\n\\nDriven by the recent findings that pre-training language models on specialized domains is beneficial for downstream tasks (Alsentzer et al., 2019; Belt-\"}"}
{"id": "acl-2022-long-303", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"development test\\n\\n|                | \u00b5-P   | \u00b5-R   | \u00b5-F1  | \u00b5-P   | \u00b5-R   | \u00b5-F1  |\\n|----------------|-------|-------|-------|-------|-------|-------|\\n| bert           | 74.9\u00b11.5 | 82.0\u00b11.3 | 78.2\u00b11.4 | 71.5\u00b11.1 | 79.6\u00b11.4 | 75.1\u00b11.1 |\\n| bert+crf       | 78.3\u00b10.8 | 83.6\u00b10.4 | 80.9\u00b10.3 | 75.0\u00b10.9 | 81.2\u00b10.2 | 78.0\u00b10.5 |\\n| bert+num       | 79.4\u00b10.8 | 83.0\u00b10.9 | 81.2\u00b10.9 | 76.0\u00b10.6 | 80.7\u00b10.8 | 78.3\u00b10.7 |\\n| bert+shape     | 82.1\u00b10.6 | 82.6\u00b10.4 | 82.3\u00b10.2 | 78.7\u00b10.5 | 80.1\u00b10.2 | 79.4\u00b10.2 |\\n| fin-bert       | 73.9\u00b11.3 | 81.4\u00b10.7 | 77.5\u00b11.0 | 70.2\u00b11.2 | 78.7\u00b10.7 | 74.0\u00b11.1 |\\n| fin-bert+num   | 81.1\u00b10.1 | 82.5\u00b11.2 | 81.8\u00b10.1 | 77.9\u00b10.1 | 79.9\u00b10.7 | 78.8\u00b10.3 |\\n| fin-bert+shape | 82.3\u00b11.7 | 84.0\u00b11.2 | 83.2\u00b11.4 | 79.0\u00b11.6 | 81.2\u00b11.1 | 80.1\u00b11.4 |\\n| sec-bert       | 75.2\u00b10.4 | 82.7\u00b10.5 | 78.8\u00b10.1 | 71.6\u00b10.4 | 80.3\u00b10.5 | 75.7\u00b10.1 |\\n| sec-bert+num   | 82.5\u00b12.1 | 84.4\u00b11.2 | 83.7\u00b11.7 | 79.0\u00b11.9 | 82.0\u00b10.9 | 80.4\u00b11.4 |\\n| sec-bert+shape | 84.8\u00b10.2 | 85.8\u00b10.2 | 85.3\u00b10.0 | 81.0\u00b10.2 | 83.2\u00b10.1 | 82.1\u00b10.1 |\\n\\nTable 4: Entity-level micro-averaged P, R, F1 \u00b1 std. dev. (3 runs) on the dev. and test data for bert-based models.\\n\\nagy et al., 2019; Yang et al., 2020; Chalkidis et al., 2020b), we explore this direction in our task which is derived from the financial domain.\\n\\nfin-bert: We fine-tune fin-bert (Yang et al., 2020), which is pre-trained on a financial corpus from sec documents, earnings call transcripts, and analyst reports. The 30k subwords vocabulary of fin-bert is built from scratch from its pre-training corpus. Again, we utilize fin-bert with and without our numeric pseudo-tokens, whose representations are learned during fine-tuning.\\n\\nsec-bert: We also release our own family of bert models. Following the original setup of Devlin et al. (2019), we pre-trained bert from scratch on edgar-corpus, a collection of financial documents released by Loukas et al. (2021). The resulting model, called sec-bert, has a newly created vocabulary of 30k subwords. To further examine the impact of the proposed [num] and [shape] special tokens, we also pre-trained two additional bert variants, sec-bert-num and sec-bert-shape, on the same corpus, having replaced all numbers by [num] or [shape] pseudo-tokens, respectively. In this case, the representations of the pseudo-tokens are learned during pre-training and they are updated during fine-tuning.\\n\\n8 Improved bert Results\\n\\nTable 4 reports micro-averaged precision, recall, and F1 on development and test data. As with Table 3, a lr layer is used on top of each embedding to predict the correct label, unless specified otherwise.\\n\\n11 We use the finbert-finvocab-uncased version from https://github.com/yya518/FinBERT.\\n\\nFocusing on the second zone, we observe that the [num] pseudo-token improves bert's results, as expected, since it does not allow numeric expressions to be fragmented. The results of bert+[num] are now comparable to those of bert+crf. Performance improves further when utilizing the shape pseudo-tokens (bert+[shape]), yielding 79.4 \u00b5-F1 and showing that information about each number's magnitude is valuable in xbrl tagging.\\n\\nInterestingly, fin-bert (3rd zone) performs worse than bert despite its pre-training on financial data. Similarly to bert, this can be attributed to the fragmentation of numbers (2.5 subwords per gold tag span). Again, the proposed pseudo-tokens ([num], [shape]) alleviate this problem and allow fin-bert to leverage its in-domain pre-training in order to finally surpass the corresponding bert variants, achieving an 80.1 \u00b5-F1 test score.\\n\\nOur new model, sec-bert (last zone), which is pre-trained on sec reports, performs better than the existing bert and fin-bert models, when no numeric pseudo-tokens are used. However, sec-bert is still worse than bert with numeric pseudo-tokens (75.7 vs. 78.3 and 79.4 test \u00b5-F1), suffering from number fragmentation (2.4 subwords per gold tag span).\\n\\nsec-bert (without pseudo-tokens) also performs worse than the bilstm with word embeddings (75.7 vs. 77.3 \u00b5-F1, cf. Table 3). However, when the proposed pseudo-tokens are used, sec-bert-num and sec-bert-shape achieve the best overall performance, boosting the test \u00b5-F1 to 80.4 and 82.1, respectively. This indicates that learning to handle numeric expressions during model pre-training is a better strategy than trying to acquire this knowledge only during fine-tuning.\"}"}
{"id": "acl-2022-long-303", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"9 Additional Experiments\\n\\n9.1 Subword pooling\\nAn alternative way to bypass word fragmentation is to use subword pooling for each word. \u00c1cs et al. (2021) found that for ner tasks, it is better to use the first subword only, i.e., predict the label of an entire word from the contextualized embedding of its first subword only; they compared to several other methods, such as using only the last subword of each word, or combining the contextualized embeddings of all subwords with a self-attention mechanism. Given this finding, we conducted an ablation study and compare (i) our best model (sec-bert) with first subword pooling (denoted sec-bert-first) to (ii) sec-bert with our special tokens (sec-bert-num, sec-bert-shape), which avoid segmenting numeric tokens.\\n\\nTable 5 shows that, in xbrl tagging, using the proposed special tokens is comparable (sec-bert-num) or better (sec-bert-shape) than performing first pooling (sec-bert-first). It might be worth trying other pooling strategies as well, like last-pooling or subword self-attention pooling. It's worth noting, however, that the latter will increase the training and inference times.\\n\\n| Model                  | \u00b5-F 1 | m-F 1 |\\n|------------------------|-------|-------|\\n| sec-bert               | 78.8\u00b10.1 | 72.6\u00b10.4 |\\n| sec-bert-first         | 79.9\u00b11.2 | 77.1\u00b11.7 |\\n| sec-bert-num           | 80.4\u00b11.4 | 78.9\u00b11.3 |\\n| sec-bert-shape         | 82.1\u00b10.1 | 80.1\u00b10.2 |\\n\\nTable 5: Entity-level \u00b5-F 1 and m-F 1 (%, avg. of 3 runs with different random seeds, \u00b1 std. dev.) on test data using different ways to alleviate fragmentation.\\n\\n9.2 Subword bilstm with [num] and [shape]\\nTo further investigate the effectiveness of our pseudo-tokens, we incorporated them in the bilstm operating on subword embeddings (3rd model of Table 3). Again, we replace each number by a single [num] pseudo-token or one of 214 [shape] pseudo-tokens, for the two approaches, respectively. These replacements also happen when pre-training word2vec subword embeddings; hence, an embedding is obtained for each pseudo-token. Table 6 shows that bilstm-num outperforms the bilstm subword model. bilstm-shape further improves performance and is the best bilstm subword model overall, surpassing the subword bilstm with crf, which was the best subword bilstm model in Table 3. These results further support our hypothesis that the [num] and [shape] pseudo-tokens help subword models successfully generalize over numeric expressions, with [shape] being the best of the two approaches, while also avoiding the over-fragmentation of numbers.\\n\\n| Model                  | \u00b5-F 1 | m-F 1 |\\n|------------------------|-------|-------|\\n| bilstm (subwords)       | 71.3\u00b10.2 | 68.6\u00b10.2 |\\n| bilstm (subwords) + crf | 76.2\u00b10.2 | 73.4\u00b10.3 |\\n| bilstm-num (subwords)   | 75.6\u00b10.3 | 72.7\u00b10.4 |\\n| bilstm-shape (subwords) | 76.8\u00b10.2 | 74.1\u00b10.3 |\\n\\nTable 6: Entity-level \u00b5-F 1 and m-F 1 (%, avg. of 3 runs with different random seeds, \u00b1 std. dev.) on test data for bilstm models with [num] and [shape] tokens.\\n\\n9.3 A Business Use Case\\nSince xbrl tagging is derived from a real-world need, it is crucial to analyze the model's performance in a business use case. After consulting with experts of the financial domain, we concluded that one practical use case would be to use an xbrl tagger as a recommendation engine that would propose the k most probable xbrl tags for a specific token selected by the user. The idea is that an expert (e.g., accountant, auditor) knows beforehand the token(s) that should be annotated and the tagger would assist by helping identify the appropriate tags more quickly. Instead of having to select from several hundreds of xbrl tags, the expert would only have to inspect a short list of k proposed tags.\"}"}
{"id": "acl-2022-long-303", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the tokens to be annotated are known. If the correct tag is among the top $k$, we increase the number of hits by one. Finally, we divide by the number of tokens to be annotated. Figure 4 shows the results for different values of $k$. The curve is steep for $k = 1$ to $5$ and saturates as $k$ approaches 10, where Hits@$k$ is nearly perfect (99.4%). In practice, this means that a user would have to inspect 10 recommended xbrl tags instead of hundreds for each token to be annotated; and in most cases, the correct tag would be among the top 5 recommended ones.\\n\\n9.4 Error Analysis\\nWe also performed an exploratory data and error analysis to unveil the peculiarities of finer-139, extract new insights about it, and discover the limitations of our best model. Specifically, we manually inspected the errors of sec-bert-shape in under-performing classes (where $F_1 < 50\\\\%$) and identified three main sources of errors.\\n\\nSpecialized terminology: In this type of errors, the model is able to understand the general financial semantics, but does not fully comprehend highly technical details. For example, Operating Lease Expense amounts are sometimes missclassified as Lease And Rental Expense, i.e., the model manages to predict that these amounts are about expenses in general, but fails to identify the specific details that distinguish operating lease expenses from lease and rental expenses. Similarly, Payments to Acquire Businesses (Net of Cash Acquired) amounts are mostly misclassified as Payments to Acquire Businesses (Gross). In this case, the model understands the notion of business acquisition, but fails to differentiate between net and gross payments.\\n\\nFinancial dates: Another interesting error type is the misclassification of financial dates. For example, tokens of the class Debt Instrument Maturity Date are mostly missclassified as not belonging to any entity at all (\u2018O\u2019 tag). Given the previous type of errors, one would expect the model to misclassify these tokens as a different type of financial date, but this is not the case here. We suspect that errors of this type may be due to annotation inconsistencies by the financial experts.\\n\\nAnnotation inconsistencies: Even though the gold xbrl tags of finer-139 come from professional auditors, as required by the Securities & Exchange Commission (sec) legislation, there are still some discrepancies. We provide an illustrative example in Figure 5. We believe that such inconsistencies are inevitable to occur and they are a part of the real-world nature of the problem. We hope that this analysis inspires future work on xbrl tagging. For example, the specialized terminology and financial date errors may be alleviated by adopting hierarchical classifiers (Chalkidis et al., 2020a; Manginas et al., 2020), which would first detect entities in coarse classes (e.g., expenses, dates) and would then try to classify the identified entities into finer classes (e.g., lease vs. rent expenses, instrument maturity dates vs. other types of dates). It would also be interesting to train classifiers towards detecting wrong (or missing) gold annotations, in order to help in quality assurance checks of xbrl-tagged documents.\\n\\n10 Conclusions and Future Work\\nWe introduced a new real-word nlp task from the financial domain, xbrl tagging, required by regulatory commissions worldwide. We released finer-139, a dataset of 1.1M sentences with xbrl tags. Unlike typical entity extraction tasks, finer-139 uses a much larger label set (139 tags), most tokens to be tagged are numeric, and the correct tag depends mostly on context rather than the tagged token. We experimented with several neural classifiers, showing that a bilstm outperforms bert due to the excessive numeric token fragmentation of the latter. We proposed two simple and effective solutions that use special tokens to generalize over the shapes and magnitudes of numeric expressions. We also experimented with fin-bert, an existing...\"}"}
{"id": "acl-2022-long-303", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"bert model for the financial domain, which also\\nbenefits from our special tokens. Finally, we pre-\\ntrained and released our own domain-specific\\nbert model, sec-bert, both with and without the spe-\\ncial tokens, which achieves the best overall results\\nwith the special tokens, without costly crf\\nlayers.\\n\\nIn future work, one could hire experts to re-\\nannotate a subset of the dataset to measure human\\nperformance against the gold tags. Future work\\ncould also consider less frequent xbrl\\ntags (few-\\nand zero-shot learning) and exploit the hierarchical\\ndependencies of xbrl\\ntags, possibly with hierar-\\nchical classifiers, building upon our error analysis.\"}"}
{"id": "acl-2022-long-303", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), pages 363\u2013370, Ann Arbor, Michigan.\\n\\nSumam Francis, Jordy Van Landeghem, and Marie-Francine Moens. 2019. Transfer learning for named entity recognition in financial and biomedical documents. Information, 10(8):248.\\n\\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 946\u2013958.\\n\\nXavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, pages 249\u2013256, Chia Laguna Resort, Italy.\\n\\nEran Goldman and Jacob Goldberger. 2020. CRF with deep class embedding for large scale classification. Computer Vision and Image Understanding, 191:102865.\\n\\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. 2013. Speech recognition with deep recurrent neural networks. In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 6645\u20136649.\\n\\nUdo Hahn, V\u00e9ronique Hoste, and Zhu Zhang, editors. 2019. Proceedings of the Second Workshop on Economics and Natural Language Processing. Association for Computational Linguistics, Hong Kong.\\n\\nPeter Hampton, Hui Wang, William Blackburn, and Zhiwei Lin. 2016. Automated sequence tagging: Applications in financial hybrid systems. In Research and Development in Intelligent Systems XXXIII, pages 295\u2013306. Springer.\\n\\nPeter John Hampton, Hui Wang, and William Blackburn. 2015. A hybrid ensemble for classifying and repurposing financial entities. In Research and Development in Intelligent Systems XXXII, pages 197\u2013202. Springer.\\n\\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778.\\n\\nRani Hoitash and Udi Hoitash. 2018. Measuring accounting reporting complexity with XBRL. The Accounting Review, 93:259\u2013287.\\n\\nMatthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrial-strength Natural Language Processing in Python.\\n\\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991.\\n\\nKiyoshi Izumi and Hiroki Sakaji. 2019. Economic causal-chain search using text mining technology. In Proceedings of the First Workshop on Financial Technology and Natural Language Processing, pages 61\u201365, Macao, China.\\n\\nGilles Jacobs, Els Lefever, and V\u00e9ronique Hoste. 2018. Economic event detection in company-specific news text. In Proceedings of the First Workshop on Economics and Natural Language Processing, pages 1\u201310, Melbourne, Australia.\\n\\nJ-D Kim, Tomoko Ohta, Yuka Tateisi, and Jun\u2019ichi Tsujii. 2003. Genia corpus\u2014a semantically annotated corpus for bio-textmining. Bioinformatics, 19:i180\u2013i182.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, USA, May 7-9, 2015, Conference Track Proceedings.\\n\\nAman Kumar, Hassan Alam, Tina Werner, and Manan Vyas. 2016. Experiments in candidate phrase selection for financial named entity extraction - a demo. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 45\u201348, Osaka, Japan.\\n\\nJohn D. Latofferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML'01, page 282\u2013289, San Francisco, USA.\\n\\nGuillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 260\u2013270, San Diego, California.\\n\\nLefteris Loukas, Manos Fergadiotis, Ion Androutsopoulos, and Prodromos Malakasiotis. 2021. EDGAR-CORPUS: Billions of tokens make the world go round. In Proceedings of the Third Workshop on Economics and Natural Language Processing, pages 13\u201318, Punta Cana, Dominican Republic.\\n\\nPekka Malo, Ankur Sinha, Pekka J. Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. J. Assoc. Inf. Sci. Technol., 65(4):782\u2013796.\\n\\nNikolaos Manginas, Ilias Chalkidis, and Prodromos Malakasiotis. 2020. Layer-wise guided training for BERT: Learning incrementally refined document...\"}"}
{"id": "acl-2022-long-303", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-303", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Experimental Setup\\n\\nFor spaCy, we followed the recommended practices. All other methods were implemented in tensorflow. Concerning bert models, we used the implementation of huggingface (Wolf et al., 2020). We also use Adam (Kingma and Ba, 2015), Glorot initialization (Glorot and Bengio, 2010), and the categorical cross-entropy loss. Hyper-parameters were tuned on development data with Bayesian Optimization (Snoek et al., 2012) monitoring the development loss for 15 trials. For the bilstm encoders, we searched for \\\\{1, 2, 3\\\\} hidden layers, \\\\{128, 200, 256\\\\} hidden units, \\\\{1e-3, 2e-3, 3e-3, 4e-3, 5e-3\\\\} learning rate, and \\\\{0.1, 0.2, 0.3\\\\} dropout. We trained for 30 epochs using early stopping with patience 4. For bert, we used grid-search to select the optimal learning rate from \\\\{1e-5, 2e-5, 3e-5, 4e-5, 5e-5\\\\}, fine-tuning for 10 epochs, using early stopping with patience 2. All final hyper-parameters are shown in Table 7. Training was performed mainly on a dgx station with 4 nvidia v100 gpus and an Intel Xeon cpu e5-2698 v4 @ 2.20 ghz.\\n\\n| Method            | Params | L | U   | P drop | LR  |\\n|-------------------|--------|---|-----|--------|-----|\\n| bilstm (words)    | 21M    | 2 | 128 | 0.1    | 1e-3|\\n| bilstm (subwords) | 8M     | 1 | 256 | 0.2    | 1e-3|\\n| bilstm (words) + crf | 21M | 2 | 128 | 0.1    | 1e-3|\\n| bilstm (subwords) + crf | 8M | 1 | 256 | 0.2    | 1e-3|\\n| bilstm - num     | 1M     | 1 | 256 | 0.2    | 1e-3|\\n| bilstm - shape   | 0.8M   | 2 | 128 | 0.1    | 1e-3|\\n| bert             | 110M   | - | -   | -      | 1e-5|\\n| bert + [num]    | 110M   | - | -   | -      | 1e-5|\\n| bert + [shape]  | 110M   | - | -   | -      | 1e-5|\\n| bert + crf      | 110M   | - | -   | -      | 1e-5|\\n| fin - bert      | 110M   | - | -   | -      | 2e-5|\\n| fin - bert + [num] | 110M | - | -   | -      | 2e-5|\\n| fin - bert + [shape] | 110M | - | -   | -      | 2e-5|\\n| sec - bert      | 110M   | - | -   | -      | 1e-5|\\n| sec - bert - num | 110M   | - | -   | -      | 1e-5|\\n| sec - bert - shape | 110M | - | -   | -      | 1e-5|\\n\\nTable 7: Number of total parameters (Params) and the best hyper-parameter values for each method; i.e., number of recurrent layers (L), number of recurrent units (U), dropout probability (P drop), learning rate (LR).\\n\\nB Additional Results\\n\\nTable 8 shows micro-averaged Precision, Recall, and F1 for the development and test data, using all baseline methods. The macro-averaged scores are similar and we omit them for brevity. Using a logistic regression (lr) classification layer, bilstm (words) surpasses bert both in Precision and F1 score. However, when using a crf layer on top, bert outperforms bilstm (words) in all measures. Table 9 shows the micro-averaged Precision, Recall, and F1 for the development and test data using the bilstm models operating on subwords with the proposed tokenizations. [num] and [shape] tokens help the model to bypass the word fragmentation problem, increasing its scores in all metrics.\"}"}
{"id": "acl-2022-long-303", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | P \u00b1 std. dev | R \u00b1 std. dev | F1 \u00b1 std. dev |\\n|------------------------|--------------|--------------|---------------|\\n| bilstm (words)         | 78.6 \u00b1 2.4   | 80.3 \u00b1 1.2   | 79.4 \u00b1 1.0    |\\n| bilstm (subwords)      | 73.4 \u00b1 0.1   | 77.2 \u00b1 0.0   | 75.2 \u00b1 0.1    |\\n| bert (subwords)        | 74.9 \u00b1 1.5   | 82.0 \u00b1 1.3   | 78.2 \u00b1 1.4    |\\n| bilstm + crf           | 73.4 \u00b1 2.0   | 69.3 \u00b1 0.9   | 71.3 \u00b1 1.2    |\\n| bilstm (subwords) + crf| 80.0 \u00b1 0.3   | 78.7 \u00b1 0.5   | 79.3 \u00b1 0.4    |\\n\\nTable 8: Entity-level micro-averaged P, R, F1 \u00b1 std. dev. (3 runs) on the dev. and test data for our baselines.\\n\\nTable 9: Entity-level micro-averaged P, R, F1 \u00b1 std. dev. (3 runs) on the dev. and test data for the bilstm models using the [num] and [shape] tokens.\"}"}
