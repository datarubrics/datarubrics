{"id": "acl-2024-long-848", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nTheory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind, formalizing cognitive processes as a chained structure. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. In addition, we further generalize COKE using LLMs and build a powerful generation model COLM tailored for cognitive reasoning. Experimental results in both automatic and human evaluation demonstrate the high quality of COKE, the superior ToM ability of COLM, and its potential to significantly enhance social applications. We release our code and data at https://github.com/jincenziwu/COKE.\\n\\n1 Introduction\\n\\nIn social environments, human beings must be able not only to react to what others are doing, but also to anticipate what they will do. This ability to understand and infer human goals is typically described as Theory of Mind (ToM) (Premack and Woodruff, 1978). One way of accomplishing ToM is to observe what others do in various situations, and derive a set of affective and behavioral rules. When the same or highly similar things arise again, we can bring out plausible predictions accordingly (Call and Tomasello, 2011). Figure 1 presents an example that someone will deliver a talk at the university tomorrow (a social circumstance), and he has substantial public speaking experience (a trigger factor). We can plausibly anticipate that he will deliver an impressive speech (a mental activity), feels joyful (an affective response), and has a restful sleep tonight (a behavioral response). Here ToM is instantiated as a chained cognitive process that derives from our knowledge, experiences, and memories (Harris et al., 1989). ToM is indispensable to humans since it allows us to leverage our own minds to simulate others', so as to achieve efficient communication (Rabinowitz et al., 2018). Despite its importance for social intelligence, ToM is not well internalized by modern AI and NLP systems. Shapira et al. (2023) illustrates a significant decline in performance and outright failure of Large Language Models (LLMs) in ToM tasks, particularly evident when confronted with adversarial samples. The main reason is that learning-based systems are usually trained on superficial text corpora.\"}"}
{"id": "acl-2024-long-848", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"pora, while lacking access to the underlying human mental state and cognitive process (Sap et al., 2022). In other words, NLP systems rely on the maximum likelihood to understand and generate texts, but do not go beneath the surface to the desires, beliefs, and intentions of humans.\\n\\nIn this paper, we introduce COKE: the first COgnitive KnowledgE graph formalizing cognitive process as a chained structure for machine theory of mind. Our goal is to formalize ToM and make it accessible and learnable for AI systems. In COKE, we instantiate ToM as a collection of manually verified cognitive chains that characterize humans' mental activities in specific social circumstances along with their behavioral and affective responses (Meinhardt-Injac et al., 2018; Mehl et al., 2020).\\n\\nEach cognitive chain involves five types of nodes: 1) situations denote the social circumstances; 2) clues denote the trigger factors; 3) thoughts denote the mental activities; 4) actions denote the behavioral responses; 5) emotions denote the affective responses. Moreover, as shown in Figure 1, individuals react differently to the same situation due to the diversified cognitive processes. Therefore, for each situation, we derive multiple cognitive chains and further label them as positive (means optimistic) or negative (means pessimistic) to mark the chain polarity. We propose to induce the raw data from LLMs, and then recruit educated workers majoring in psychology for manual selection and revision. The resulting knowledge graph constitutes 62,328 nodes and 45,369 cognitive chains.\\n\\nThe construction of COKE offers the basic ToM ability to understand and infer the human goals in already collected situations (Call and Tomasello, 2011). But obviously, it is impossible to enumerate all situations in the real world. Thus we move one step further and build a cognitive language model COLM to cope with unseen situations that have not appeared in the knowledge graph. Specifically, we decompose the construction of cognitive chains into four cognitive generation tasks, then finetune LLMs using the manually collected data in COKE. By this means, we combine the commonsense knowledge embedded in LLMs and the ToM ability provided by COKE, enabling COLM to infer cognitive chains for unseen situations.\\n\\nWe summarize our contributions in this work as follows.\\n\\n1) We propose the first cognitive knowledge graph for machine theory of mind, formalizing cognitive process as a chained structure. We instantiate human theory of mind as a collection of 45k+ manually verified cognitive chains, which provides a basic ToM ability for accessing and learning.\\n\\n2) We build a powerful cognitive language model COLM by associating COKE with LLaMA-2 (Touvron et al., 2023), so as to predict cognitive chains for out-of-KG situations.\\n\\n3) We conduct extensive experiments to evaluate the ToM ability of COLM and typical LLMs. The results show that COLM outperforms strong baseline models such as GPT-4 (Achiam et al., 2023) in both zero-shot and few-shot settings, proved by automatic and human evaluations in all cognitive generation tasks, which in turn demonstrates the high quality of COKE.\\n\\n4) We further substantiate the potential of COKE in enhancing social applications, and prove its effectiveness on downstream emotional support conversation task.\\n\\n2 COKE: Cognitive Knowledge Graph\\n\\n2.1 Preliminaries\\n\\nWhat is Theory of Mind?\\n\\nTheory of Mind refers to the ability of humans to understand and infer other people's desires, beliefs, and intentions (Premack and Woodruff, 1978). Under specific social circumstances (situations), the core mechanism by which ToM empowers us is to ascribe others' mental activities (thoughts), and predict their corresponding behavioral responses (actions) and affective responses (emotions) (Leslie et al., 2004; Apperly, 2010). Furthermore, the acquisition of ToM enables us to realize and appreciate that people can have different cognitive responses in the same situation due to various trigger factors (clues) like personality and experience (Meinhardt-Injac et al., 2018).\\n\\nWhy Do AI Systems Need ToM?\\n\\nToM has been a persistent yet elusive goal of artificial intelligence for decades (Choi, 2022). AI systems need ToM to understand a user's situations and predict subsequent reactions, so as to provide effective responses or operations that meet the user's needs (Le et al., 2019; Dhelim et al., 2021; Langley et al., 2022). However, recent studies (Shapira et al., 2023) show that today's LLMs still lack ToM and social intelligence. The main reason is that AI systems can only learn from text corpora in training, but cannot access the human mental state and cognitive process that determine what and why to say. Now, there are neither public resources that contain all the concepts in ToM, nor commonsense knowledge graphs that depict the structure of cognitive chains.\"}"}
{"id": "acl-2024-long-848", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This motivates us to propose the first cognitive knowledge graph for machine Theory of Mind.\\n\\n2.2 Data Structure of COKE\\n\\nAccording to the above-mentioned psychology research on theory of mind, we specify five types of nodes in COKE: situations, clues, thoughts, actions, and emotions. We here define the basic unit of COKE as the following cognitive chain:\\n\\nSituation $\\\\Rightarrow$ Clue $\\\\Rightarrow$ Thought $\\\\Rightarrow$ (Action $\\\\pm$ Emotion).\\n\\nThis structure depicts the intact cognitive process of ToM: When a person faces a situation, some clues trigger his/her thoughts, along with his/her actions and emotions. Furthermore, to distinguish whether a cognitive chain is optimistic or pessimistic under the specific situation, we further define its polarity as positive or negative.\\n\\nIn practice, the polarity of the cognitive chain is determined by its thought node. Notice that, in COKE, we omit the definition of edges (i.e., the connections between nodes) since they can be easily inferred when the types of nodes are already known. We then illustrate the nodes in detail.\\n\\nSituations in COKE denote the social circumstances in which individuals (potentially) interact with others. By referring to DailyDialog (Li et al., 2017), a widely used daily social dialogue dataset, we select the five most common social topics: School (what happened at school), Work (what happened at work), Tourism (travel and entertainment), Relationship (social activities between individuals), Ordinary Life (what happened in families). Detailed information about each topic can be found in Appendix A.\\n\\nClues in COKE denote the trigger factors that direct and concretize the cognitive process. In a specific situation, humans' mental activities are triggered and directed by relevant subjective and objective factors (Meinhardt-Injac et al., 2018). According to the taxonomy from (Baldwin, 1992), clues mainly involve the particular information about personality, knowledge, experience, education, objective facts, social rules, and so on.\\n\\nThoughts in COKE denote the mental activities that act as the cognitive responses to situations. Thoughts serve as the bridge between the external environment and individual cognition, thus can be considered as the core of ToM (Westbrook et al., 2011). As mentioned before, the polarity of a cognitive chain is anchored to its thought node. In other words, an optimistic thought marks the entire cognitive chain as positive, and a pessimistic thought marks it as negative.\\n\\nActions in COKE denote the behavioral responses to situations after specific thoughts. Notice that the semantic meaning of actions may not conform to their polarity annotations. For example, in Figure 1, the action \\\"I practice my script in front of the mirror\\\" is a neutral sentence. However, it is the consequence of the negative thought \\\"I may mess up the speech\\\", so it is still labeled as negative.\\n\\nEmotions in COKE denote the affective responses to situations after specific thoughts. Without loss of generality, we restrict the emotions to six basic categories (Phillip et al., 1987): Love, Surprise, Joyful, Sad, Angry, and Fearful. The first three appear in positive cognitive chains, while the last three appear in negative cognitive chains.\\n\\n2.3 Data Creation and Selection\\n\\nRecent studies have proved that LLMs trained on huge text corpora can naturally serve as a repository for data collection (West et al., 2023; Kim et al., 2023). Inspired by their successful attempts, we propose a two-step data collection approach for constructing COKE. As shown in Figure 2, 1) we first manually design suitable few-shot prompts to induce GPT-3.5 (i.e., text-davinci-002 ) Ouyang et al. (2022) to automatically generate raw data for five types of nodes in a pipeline manner. 2) We then recruit and train eight graduate students majoring in social psychology as annotators to select and revise the outputs of GPT-3.5. Next, we illustrate the details of how to prompt GPT-3.5, then introduce the data statistics after human annotation. More detailed parameters and templates used for prompting are provided in Appendix B and C.\"}"}
{"id": "acl-2024-long-848", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The two-step data collection approach for constructing COKE.\\n\\nsonY\u201d, thereby losing most of the interpersonal information that is indispensable for social situations. For example, if someone misses calls from his employer, he may worry that something is wrong with his work. But if the caller changes to his girlfriend, he may sense love because she cares about him.\\n\\n2) ATOMIC events omit most of the social context information, making the background environments where the events occur not available. For example, we don\u2019t know if the calls in the above event happen at work or on vacation, so we have no idea how to reify subsequent cognitive processes.\\n\\nTranslate [Event] to [Situation]:\\n\\n[Event] PersonX misses calls from PersonY\\n\\n\u21d2\\n\\n[Situation] I miss multiple calls from my girlfriend because I\u2019m playing Project Zomboid.\\n\\n\u2026(Other demonstrations)\u2026\\n\\n[Event] PersonX delivers PersonX\u2019s message\\n\\n\u21d2\\n\\n[Situation] I will deliver a talk at the university tomorrow.\\n\\nPrompting LLMs for situations.\\n\\nTo address the problem, we choose to prompt GPT-3.5 to rewrite ATOMIC events to qualified COKE situations. Specifically, we first manually create several demonstration (input, output) pairs such as (\u201cPersonX misses calls from PersonY\u201d, \u201cI miss multiple calls from my girlfriend because I\u2019m playing Project Zomboid\u201d). Then we wrap the demonstrations with the task instruction (\u201cTranslate [Event] to [Situation]\u201d) and an arbitrary event query (\u201cPersonX delivers PersonX\u2019s message\u201d) to form an input template as shown in Figure 3. As a result, we collect the output situation from GPT-3.5 like \u201cI will deliver a talk at the university tomorrow\u201d. In practice, we manually select 400 ATOMIC events that are general and easy to adapt, then rewrite each event to 5 situations with different topics. Finally, we obtain 2,000 raw situations.\\n\\nPrompting LLMs for Thoughts\\n\\nThoughts in COKE denote the mental activities that act as the cognitive responses to situations. As mentioned above, thoughts serve as the bridge between the external environment and individual cognition, and can be seen as the core of ToM. Since thoughts are triggered by certain clues, they come after clues in cognition chains. However, our pilot experiments show that directly using situations to prompt LLMs can better stimulate their generation ability and get more diverse thoughts, which is beneficial to manual selection and revision. Therefore, here we temporarily reverse the order of thoughts and clues, i.e., we first prompt LLMs using situations to generate thoughts, and then prompt LLMs using thoughts to generate clues.\\n\\nSimilar to the instruction prompts for collecting situations, we manually construct the demonstration as \u201cWhen [Situation], I feel great/terrible since I think [Thought]\u201d and again wrap it with the task instruction and an arbitrary situation query to form the input template. Notice that we use different sentiment terms (\u201cgreat\u201d and \u201cterrible\u201d) to control the polarity (positive and negative) of the generated thoughts. Since the polarity of a cognitive chain is anchored to its thought, we have also controlled the polarity of the entire cognitive chain that will be generated. After prompting, we obtain 14,400 raw thoughts, half of which are positive and the other half are negative.\\n\\nPrompting LLMs for Clues, Actions, and Emotions\\n\\nClues in COKE denote the trigger factors that direct and concretize the cognitive process. Thus we construct the demonstrations as \u201cComplete the sentence: When [Situation], I think [Thought] since [Clue]\u201d to prompt LLMs for clues.\\n\\nActions and emotions in COKE denote the behavioral and affective responses to situations after specific thoughts. We hence construct the demonstrations as \u201cWhen [Situation], I think [Thought] so [Action]\u201d and \u201cWhen [Situation], I think [Thought] and I feel [Emotion]\u201d to prompt LLMs for actions and emotions. Since emotion belongs to predefined\u2026\"}"}
{"id": "acl-2024-long-848", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We further modify its corresponding prompt to the question-answering form. Finally, we obtain 29,364 raw clues, 29,364 raw actions, and 14,400 raw emotions. Their polarities are already determined by previously generated thoughts.\\n\\nHuman Selection and Revision\\n\\nAfter collecting the raw data, we manually annotate a small amount of data and formulate several rules to distinguish good data from bad data. Subsequently, we use the detailed definition of nodes in COKE and the filtering rules as a tutorial to train the eight annotators. After passing an annotation qualification test, they are asked to select and revise the raw data of five types of nodes. More details about human annotation can be found in Appendix D.\\n\\nAs shown in Table 1, the final reserved data contains 1,200 situations, 9,788 thoughts, 21,677 clues, 19,875 actions, and 9,788 emotions, resulting in an overall retention rate of around 70%. This statistic proves that ToM ability exhibited by most powerful LLMs like GPT-3.5 is still not satisfactory enough even with delicate prompting, thus further emphasizing the necessity of our construction of COKE.\\n\\nAfter linking and ordering the obtained nodes, we instantiate ToM with a total of 45,369 cognitive chains in COKE, containing 23,252 positive chains and 22,117 negative chains in English. Example COKE data can be found in Appendix E.\\n\\n| Dimension | Raw | Final | Avg. Len. | Retention Rate |\\n|-----------|-----|-------|-----------|----------------|\\n| Situation | 2,000 | 1,200 | 11.5 | 60.00% |\\n| Thought   | 14,400 | 9,788 | 6.6 | 67.97% |\\n| Clue      | 29,364 | 21,677 | 7.3 | 73.82% |\\n| Action    | 29,364 | 19,875 | 6.8 | 67.68% |\\n| Emotion   | 14,400 | 9,788 | 1.0 | 67.97% |\\n\\nTable 1: The statistics of COKE.\\n\\n3 Cognitive Language Model COLM\\n\\nBy consulting the constructed cognitive knowledge graph COKE, we can obtain the basic ability of theory of mind (ToM) via matching the faced situation to a similar situation in KG, and then inspecting the involved cognitive chains (a.k.a, entity linking). But obviously, COKE only collects finite situations and cannot cover the infinite and diverse situations in the real world. Inspired by the methods for automatic knowledge graph completion like COMET (Bosselut et al., 2019), we propose a cognitive language model COLM to cope with unseen situations and expand the scope of application of COKE.\\n\\nCOLM is built upon LLMs, aiming to integrate the commonsense knowledge inside LLMs and the ToM ability from COKE. To this end, we first decompose the cognitive process into a sequence of cognitive generation tasks, and then finetune LLaMA-2 (Touvron et al., 2023) using the collected data from COKE.\\n\\n3.1 Cognitive Generation Tasks\\n\\nIn COKE, a cognitive process towards theory of mind (ToM) is instantiated as a sequenced cognitive chain containing situation, clue, thought, action, and emotion. Therefore, given a situation, we can decompose the cognitive process into four generation tasks as shown in Table 2. These four tasks work in a pipeline manner, and the complete cognitive chain can be restored by linking their generated results.\\n\\n1) Clue Generation. When facing a specific situation, humans can automatically distinguish the factors that may influence beliefs and trigger thoughts.\\n\\n2) Thought Generation. In a specific situation, the related clues trigger and arouse diversified human mental activities, i.e., thoughts.\\n\\n3) Action Generation. Driven by specific thoughts, we humans will take corresponding actions to realize our beliefs and achieve our goals. Notice that we omit clues here since their impacts are largely covered by the triggered thoughts.\\n\\n4) Emotion Generation. After forming a specific thought in a certain situation, humans will naturally generate corresponding emotions to express attitudes and views on the situation. Since emotions are limited to 6 categories, this task is a classification task.\\n\\nBy linking the above four tasks in a pipeline manner, we can restore the cognitive chain \\n\\n\\\\[ \\\\text{Situation} \\\\Rightarrow \\\\text{Clue} \\\\Rightarrow \\\\text{Thought} \\\\Rightarrow (\\\\text{Action} + \\\\text{Emotion}) \\\\]\\n\\nthus preserving the complete cognition process of ToM. Since each cognitive chain in COKE is labeled with polarity, each cognitive generation task can be further divided into positive and negative subtasks (e.g., positive thought generation and negative thought generation).\\n\\n3.2 Training COLM\\n\\nAfter decomposing the cognitive chain into four cognitive generation tasks, we can process the data in COKE accordingly to obtain training samples. For computational efficiency, we design COLM as a multi-task controllable generation model, so that it can simultaneously accomplish four cognitive generative tasks and further control the polarity (i.e., positive or negative) of the outputs. As shown...\"}"}
{"id": "acl-2024-long-848", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Decomposition of the cognitive process to four cognitive generation tasks for training COLM. The complete outputs of four tasks can be restored to cognitive chains. Here \\\\([\\\\text{Neg}.*]\\\\) denotes special tokens for controllable generation in negative cognitive chains. When coming to positive cognitive chains, we use \\\\([\\\\text{Pos}.*]\\\\).\\n\\nFor each task and polarity, we insert specific tokens (i.e., \\\\([\\\\text{NegClue}]\\\\), \\\\([\\\\text{PosClue}]\\\\)) in the input \\\\(X\\\\) to guide the generation process of COLM. For implementation, COLM is built as a decoder-only architecture and initialized with the LLaMA-2 (Touvron et al., 2023). The model is trained with each input-output pair \\\\(X-Y\\\\) from any task.\\n\\n4 Experiments\\n\\nIn this section, we construct a dataset from COKE to evaluate ToM ability of our cognitive language model COLM, and compare it with advanced LLMs including GPT-3.5 Turbo, GPT-4, LLaMA-2-7B (our backbone) and Mistral-7B. We first illustrate the experimental setup, then analyze the experimental results for automatic and human evaluation, and finally validate COKE\u2019s effectiveness to empower social applications.\\n\\n4.1 Experimental Setup\\n\\nTo evaluate the ToM ability of different models, we randomly split 1,200 social situations into 1,080 (90%) training and 120 (10%) validation situations. Then we can automatically split samples for different cognitive generation tasks according to the situations, and obtain training/validation splits as 19,409/2,268 (clue), 8,746/1,042 (thought), 17,982/1,893 (action), and 8,746/1,042 (emotion).\\n\\nBased on this setting, we ensure that the cognitive chains in the validation set all occur in UNSEEN situations, which is crucial for ToM evaluation. For COLM, we use the Hugging Face implementation 1 of LLaMA-2, and train it for 20 epochs using the AdamW optimizer (Loshchilov and Hutter, 2019) and LoRA (Hu et al., 2021) with learning rate 3e-4 and batch size 32 in Five Tesla V100 GPUs. The whole training process costs about 9 GPU hours. For baseline models, we construct manual prompts to enable them to complete the cognitive generation tasks. We evaluate 0-shot, 2-shot, and 5-shot performance of all baseline LLMs. More details can be found in Appendix F. Due to the data deficiency, we report the performance on the validation set.\\n\\n4.2 Main Results\\n\\nAutomatic Evaluation\\n\\nTo evaluate the clue/thought/action generation tasks, we use METEOR (Lavie and Denkowski, 2009), ROUGE (Lin, 2004), BLEU-1, BLEU-2 (Papineni et al., 2002) and BERTScore (Zhang et al., 2019) as metrics. In these three tasks, each input may be mapped to multiple ground truth outputs. Therefore, following Mostafazadeh et al. (2020), we compute the average scores between all predicted sequences and all ground truth outputs 2. Moreover, to evaluate the emotion generation (classification) task, we compute the classification accuracy as the metric. We present all automatic evaluation results in Table 3.\\n\\nCompared to baseline models like LLaMA-2 and Mistral, which face challenges in following task instructions, COLM shows significant performance enhancements across various cognitive generation tasks with COKE. While leveraging additional prompts boosts performance for powerful models like GPT-3.5 Turbo and GPT-4 via in-context learning in clue/thought/action generation tasks, these LLMs still struggle with complex emotional understanding (Wang et al., 2023) and fail in the emotion classification task. Compared to these powerful LLMs, COLM maintains substantial advantages across all evaluation metrics. We hereby have two observations: 1) The data we collected in COKE is of high quality and can empower the model with strong ToM ability. 2) COLM, which is designed as a controllable generative model for multiple cognitive tasks, can effectively internalize the ToM ability and cope with unseen situations.\\n\\nHuman Evaluation\\n\\nBeyond automatic evaluation, we also wonder how humans perceive the cognitive chains generated by different models since ToM is essentially a human ability. Therefore, for each model, we sample a cognitive chain for each...\"}"}
{"id": "acl-2024-long-848", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Automatic evaluation results for cognitive generation tasks. The highest scores are highlighted in color, and second best results are underlined. All results are average scores of 3 runs with random seeds.\\n\\n| Situation | Model       | METEOR | ROUGE  | BLUE-1 | BLEU-2 | BertScore |\\n|-----------|-------------|--------|--------|--------|--------|-----------|\\n| Clue Gen. | COLM        | 0.370  | 0.381  | 0.340  | 0.011  | 0.876     |\\n|          | GPT-3.5 Turbo| 0.325  | 0.343  | 0.310  | 0.007  | 0.872     |\\n|          | GPT-4       | 0.325  | 0.343  | 0.310  | 0.007  | 0.872     |\\n|          | LLaMa-7B    | 0.295  | 0.305  | 0.280  | 0.007  | 0.872     |\\n|          | Mistral-7B  | 0.275  | 0.285  | 0.260  | 0.007  | 0.872     |\\n\\nTable 4: Human evaluation results of COLM and baseline LLMs with 5 shots. \\\"Overall\\\" determines if the response is relevant, correct, and logical to continue the cognitive chains. \\\"Valid.\\\" refers to the validation percentage of outputs.\\n\\n| Situation | Model       | Overall | Valid. |\\n|-----------|-------------|---------|--------|\\n| Clue Gen. | COLM        | 4.83    | 90%    |\\n|          | GPT-3.5 Turbo| 4.72    | 85%    |\\n|          | GPT-4       | 4.79    | 78%    |\\n|          | LLaMa-7B    | 4.83    | 90%    |\\n|          | Mistral-7B  | 4.83    | 90%    |\\n\\n4.3 Empowering Social Application\\n\\nAs COKE is a cognitive knowledge graph for machine theory of mind, we further substantiate its effectiveness in empowering social applications with ToM capabilities. Our chosen testing ground is the emotional support conversation (ESC) task (Liu et al., 2021). The prime objective of ESC is to generate empathetic and effective responses in social dialogues, with the aim of mitigating users' emotional distress and fostering improved mental states. ESC requires ToM because it involves understanding and responding to a user's emotional state and underlying challenges by understanding their mental processes and intentions.\"}"}
{"id": "acl-2024-long-848", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The ESC dataset has already provided the situation behind each dialogue. We then treat each user's utterance in the dialogue as a clue, and employ COLM to infer two thoughts (1 positive and 1 negative) via thought generation. Based on the generated thoughts, we can accordingly infer actions for each thought via action generation. We simply extract verbs and nouns in thoughts or actions as the ToM knowledge keywords, and append them to the end of dialogue history, serving as an enhanced context. Based on the context, the ESC system is trained to generate the corresponding response. We here use the Blenderbot (Roller et al., 2021) as the generation model, which is the same as the original ESC paper.\\n\\nESC Performance with ToM Knowledge\\n\\nIn our experiments, we compare three models: Vanilla ESC, ESC with COLM Actions, and ESC with COLM Thoughts. We conduct both automatic and human evaluations, following the same methodology as the original ESC paper. We sample 30 dialogues in the validation set and let two experts rate Fluency, Identification, Comfort, and Suggestions on a 3-point Likert scale. The final score is the average of these ratings. The experimental results shown in Table 5 demonstrate that, when incorporating the ToM knowledge into the response generation process, both the ESC with COLM Actions and ESC with COLM Thoughts models significantly outperform the Vanilla ESC model across all metrics. For clarity, we further present a case study and how COKE offers a more nuanced and empathetic approach to AI-driven emotional support in Appendix H.\\n\\n| Metric       | Vanilla | + Action | + Thought |\\n|--------------|---------|----------|-----------|\\n| PPL          | \u2193       | \u2193        | \u2193         |\\n| BLEU-2       | \u2191       | \u2191        | \u2191         |\\n| ROUGE-L      | \u2191       | \u2191        | \u2191         |\\n| Extrema      | \u2191       | \u2191        | \u2191         |\\n| Fluency      | \u2191       | \u2191        | \u2191         |\\n| Identification | \u2191     | \u2191        | \u2191         |\\n| Comforting   | \u2191       | \u2191        | \u2191         |\\n| Suggestion   | \u2191       | \u2191        | \u2191         |\\n\\nTable 5: Automatic and human evaluation on ESC.\\n\\nRelated Work\\n\\nCognitive knowledge is essential for modeling intuitive tasks, such as reasoning, in language models. Previous work mainly focuses on realizing cognitive knowledge as an accessory to commonsense knowledge. They construct text-based knowledge graphs that include numerous instances of Head-Relation-Tail triplets regarding different events and objects. One of the widely-used examples of such knowledge graphs is ConceptNet (Speer et al., 2016), which provides a graph of concepts connected by relations, covering a variety of taxonomic facts. Sap et al. (2019) proposed ATOMIC, a graph of if-then inferences that models social commonsense in daily life events. Hwang et al. (2021) expanded upon ATOMIC by incorporating two additional categories of commonsense relations: physical-entity commonsense relations and event-centered commonsense relations. West et al. (2021) prompted GPT-3 to distill commonsense knowledge and create a machine-generated corpus ATOMIC. Kim et al. (2023) further leveraged it to generate a large-scale dataset focused on socially-grounded conversations. Furthermore, NOV ATOMIC (West et al., 2023) employs natural language queries as open relations to link the commonsense knowledge, enabling its application in general reasoning tasks.\\n\\nGenerally, existing KGs consider event-centered commonsense relations, such as temporal and causal relationships, represented as $\\\\text{Event} \\\\xrightarrow{\\\\text{IsBefore}} \\\\text{Event} \\\\xrightarrow{\\\\text{Causes}} \\\\text{Event}$. However, they have not explicitly addressed ToM concepts and relations, which are crucial for accessing and interpreting human mental states and cognitive processes. In contrast, COKE delineates ToM concepts and structures them as a cognitive chain: $\\\\text{Situation} \\\\Rightarrow \\\\text{Clue} \\\\Rightarrow \\\\text{Thought} \\\\Rightarrow (\\\\text{Action} + \\\\text{Emotion})$. This chained structure is designed to mirror human cognitive processes, enabling a deeper understanding of how individuals infer others' mental states in specific social circumstances along with their behavioral and affective responses. Moreover, the ATOMIC family, which uses short phrases with generic placeholders like \\\"PersonX\\\", significantly constrains the social and interpersonal information critical for complex ToM reasoning (Zalla and Korman, 2018). COKE addresses this limitation by providing a richer and more nuanced context for each cognitive concept in the chain. This approach broadens the scope of cognitive knowledge accessible to machine systems and enhances their capability in social cognition, hereby fostering downstream applications.\"}"}
{"id": "acl-2024-long-848", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we present COKE, the first cognitive knowledge graph for machine theory of mind. We instantiate ToM as a series of cognitive chains to describe human mental activities and behavioral/affective responses in social situations. Through prompting GPT-3.5 and manual annotation, we collect 62k+ nodes and construct 45k+ cognitive chains. Based on COKE, we build a powerful cognitive language model COLM. COLM can handle unseen situations and predict complete cognitive chains in a pipeline manner. Automatic and human evaluations show that COLM effectively internalizes the ToM ability and outperforms strong baseline models like GPT-4.\\n\\nWe further demonstrate that COKE can empower LLMs with a nuanced understanding of human affective and cognitive reasoning, resulting in superior performance in ESC. By integrating COKE into LLMs, they gain the ability to comprehend and predict human cognitive processes with greater depth. We believe that COKE and COLM can facilitate social applications such as dialogue systems and autonomous agents.\\n\\nAcknowledgments\\nThis work was supported by the National Science Foundation for Distinguished Young Scholars (No. 62125604) and the NSFC Key Project (No. 61936010). This work was also supported by Tsinghua Precision Medicine Foundation, Tsinghua University - Beijing Tsingshang Architectural Decoration Engineering Co., Ltd. Joint Institute for Smart Scene Innovation Design, and China Postdoctoral Science Foundation (No. 2023M741944).\\n\\nLimitations\\nIn this work, we introduce COKE, the first cognitive knowledge graph for machine Theory of Mind, which aims to empower AI systems with cognitive capabilities. However, we acknowledge the following omissions and inadequacies in our work.\\n\\nData Coverage\\nWe acknowledge that there is a limitation in topic coverage of proposed COKE, which only covers five social topics that are commonly discussed. Besides, COKE has a relatively small data scale in comparison with other relative knowledge graphs, due to its more specific node contents and more complicated construction process. Consequently, COKE cannot cover all situations in deployment, and the cognitive reasoning models constructed on this basis may have unreliable predictions in out-of-domain situations.\\n\\nCognitive Inference Ability\\nAs a pioneer in integrating Theory of Mind into AI systems, the proposed cognitive language model COLM still has a lot of room for improvement in inferring cognitive chains when deployed in practice. In COLM, we take LLaMA-2 as the backbone model to validate the gain of COKE on the cognitive inference ability of language models. We acknowledge that adopting larger backbone models would contribute to a more powerful inference model, which is worth exploring in more depth in the future.\\n\\nEthics Statement\\nWe present the first cognitive knowledge graph COKE for machine theory of mind. Our graph is built based on public datasets and model generations. We strictly adhere to data source usage protocols and ensure that the proposed COKE can be released and used LEGALLY. The construction of COKE has gone through the steps of manual selection and revision. We manually filtered the data containing potentially private information, such as phone numbers and email addresses, to protect user PRIVACY further. We also carefully delete abusive, offensive, biased, and other inappropriate content to avoid unpredictable ETHICAL hazards.\\n\\nOur knowledge graph is designed to empower AI systems with the ability of cognitive inference. We are aware that this capability could be misused in malicious scenarios in the future. However, we believe the value of this work outweighs the risks, and we also call for more socially responsible research in this field.\\n\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nIan Apperly. 2010. Mindreaders: the cognitive basis of \\\"theory of mind\\\".\\n\\nMark W Baldwin. 1992. Relational schemas and the processing of social information. Psychological bulletin, page 461. \"}"}
{"id": "acl-2024-long-848", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-848", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We identify the top 8 keywords in five topics using TF-IDF. As indicated in Table 6, the information provided by situations in each topic is distinct and relevant to the theme. According to the keywords \\\"boss\\\" or \\\"work\\\", we can locate information such as \\\"I attend my boss's meeting\\\" in the Work topic. The project deadline dilemma like \\\"I bet my friend that I can finish my project before the deadline\\\" is shared in the School topic. Besides, we can find the travel schedule like \\\"I asked my best friend for advice on what to pack for my upcoming trip to Europe\\\" in the Tourism topic. Date sharing like \\\"I brought my date back to my place after the movie\\\" can be found in the Relationship topic. Daily diary like \\\"My friend is helping me plan my surprise party\\\" appears in the Ordinary Life topic.\\n\\nTopic Keywords extracted\\n\\nSchool: asked, friend, friends, dad, project, mom, help, go\\n\\nWork: boss, ask, work, asked, new, friend, job, meeting\\n\\nTourism: asked, trip, travel, go, friend, friends, sister, brother\\n\\nRelationship: asked, new, date, friends, best, girl, guy, girlfriend\\n\\nOrdinary Life: asked, day, friend, new, friends, go, wanted, going\\n\\nTable 6: Keywords in five topics.\\n\\nB Parameters for GPT-3.5 API\\n\\nUtilization\\n\\nDuring the data collection process, we use the GPT-3.5 API offered by OpenAI. We read the terms of service and follow the usage policies. We give the parameter details of the GPT-3.5 API utilized in data collection and the experiments in Table 7. Our data collection was finalized before the release of GPT-3.5 Turbo and GPT-4. Therefore, we opted for the powerful model text-davinci-002.\\n\\nC Prompts for Data Collection\\n\\nIn Table 11, we present the detailed prompts for how to extend the base event to the situation in our COKE. When we design the prompt template, we find that the base events prefixed with the token [Sentence] in the prompt lead to better generation than those with the token [Event]. Table 12 and Table 13 show the specific prompts used to collect data in the negative and positive chains for each generation task.\"}"}
{"id": "acl-2024-long-848", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Parameter | Situation | Clue | Thought | Action | Emotion\\n---|---|---|---|---|---\\n1 | 3 | 3 | 3 | 1 | 1\\nbest_of | 1 | 3 | 3 | 3 | 1\\nmodel | text-davinci-002 | text-davinci-002 | text-davinci-002 | text-davinci-002 | text-davinci-002\\n\\nTable 7: Parameters for GPT-3.5 generation.\\n\\nD Instruction for Human Selection and Revision\\n\\nSince our cognitive knowledge graph is closely related to psychology, we choose and train eight graduate students majoring in social psychology as annotators. They are evenly distributed by gender (four males and four females) and come from various regions. We pay the annotators approximately $12 per hour, which exceeds the local minimum wage, and the total cost for annotation is about $6,000. We illustrate the relevant background and how the data would be used clearly in the beginning.\\n\\nTo construct the knowledge graph for machine Theory of Mind (ToM), we have researched relevant papers and books and discussed it with the social psychology professor several times over three months. As a result, we decompose the ToM inference into four inference tasks and clearly define the five types of nodes. In the annotation instructions, explicit definitions of nodes are provided.\\n\\nIn order to effectively train the annotators, we first have a testing annotation on 160 situations for each task ourselves. Afterwards, we determine what types of incorrect data annotators may encounter and utilize these annotation examples to provide specific guidance. Several common types of bad data in all tasks are 1) repetitive context, 2) unsafe words, and 3) offensive content. Therefore, we automatically filter out the repetitive context and let the annotator manually remove the unsafe terms and offensive content. Besides, the specific types of incorrect data for each task are depicted in the details instructions in Figure 5, Figure 6, Figure 7, and Figure 8. In order to ensure data diversity, we ask workers to modify repetitive data into different and reasonable data.\\n\\nDuring training, the annotators work on ten testing situations and receive feedback after following the instructions. Then, based on their annotations of the second 10 situation examples, we evaluate if students have a solid grasp of the task. With specific guidance, students all do well in the subsequent annotation. In addition, to maintain the high quality of the data, we allow them to highlight data with which they are confused and work as the experts to make the ultimate decision. For example, in emotion data revision, the annotator assigns additional emotion labels to the illogical inference, and the experts make the final decision.\\n\\nE Example Cognitive Chains in COKE\\n\\nWe present the cognitive chains in COKE from five topics in Figure 9 (School), Figure 10 (Work), Figure 11 (Tourism), Figure 12 (Relationship), and Figure 13 (Ordinary Life), respectively. The data in COKE is in English.\\n\\nF Prompts for PLMs Generation in Evaluation\\n\\nAs shown in Table 8, we present the prompts that lead the LLaMA-2, Mistral, GPT-3.5 Turbo and GPT-4 to make inferences on the validation dataset.\\n\\nG Case Study of Generated Cognitive Chains\\n\\nTo have a close look, in Table 9, we present ten cognitive chains generated by COLM in five situations covering different topics. It can be observed that the proposed cognitive generation model COLM can generate smooth and effective cognitive chains in unseen situations.\\n\\nH Empowering Emotional Support Conversation with COKE\\n\\nIn this section, we discuss how COKE can be used to enhance social applications, specifically focusing on Emotional Support Conversation (ESC) (Liu et al., 2021). This task represents a key arena in which Theory of Mind (ToM) can play a pivotal role, given the need to understand and respond to a user's emotional state and underlying challenges.\\n\\nEmotional Support Conversation\\n\\nThe ESC task is structured around a user in a negative emotional state, potentially due to a specific problem or challenge they are confronting. The user's emotional state is characterized by a negative emotion label...\"}"}
{"id": "acl-2024-long-848", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Prompts for LLaMa/Mistral/GPT-3.5-Turbo/GPT-4 generation on the validation data in the negative and positive cognitive chains. (e), an intensity level of the emotion (1, on a scale of 1 to 5), and the underlying challenge causing their distress. The goal of the supporter (or AI system) is to comfort the user in a conversation, deploying support skills to reduce the intensity of the user\u2019s negative emotions. The conversation\u2019s effectiveness is gauged by the extent to which the user\u2019s emotional intensity is reduced, and the ability of the supporter to accurately identify the problem, comfort the user, and offer constructive solutions or suggestions.\\n\\nExperimental Results\\nIn experiments, we compare three models: Vanilla ESC, ESC w/ COLM Actions, and ESC w/ COLM Thoughts. We conduct both automatic evaluation (PPL, BLEU-2, ROUGE-L, Extrema) and human evaluation (Fluency, Indentification, Comforting, Suggestion), same as the original ESC paper. The experimental results are presented in Table 5. To have a close look, we also present a case study in Table 10. The results demonstrate the potential of COKE to empower social applications, offering a more nuanced and empathetic approach to AI-driven emotional support. By integrating COKE\u2019s cognitive chains into dialogue history, AI systems can achieve a more sophisticated understanding of human emotions and behavior, leading to more effective interactions in emotionally charged contexts such as the ESC task. Interestingly, the ESC with COLM Thoughts model shows a slight advantage over the ESC with COLM Actions model. This could be attributed to the fact that the thoughts generated by COLM offer a more direct reflection of the user\u2019s mental state.\"}"}
{"id": "acl-2024-long-848", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Case study. Given situations from different topics, the proposed cognitive generation model COLM can predict corresponding clues, thoughts, actions, and emotions in a pipeline manner.\\n\\nDialogue History\\n\\n**SITUATION:** I can't seem to make any friends - always left on my own and no-one to talk to.\\n\\n**SYSTEM:** Hello how are you doing today? I'd love to chat with you! What's been on your mind?\\n\\n**USER:** oh hello I'm having problems with friends - or more precisely the lack of them. I don't have any.\\n\\n**SYSTEM:** Oh I see. You feel as though you don't have any friends currently? I totally get that. I feel like that a lot too, especially in these weird times we are going through. It's so hard to see people and keep relationships right now.\\n\\n**USER:** Well it's more that I think I've never had friends. Not anyone I can confide in or laugh with.\\n\\n**SYSTEM:** That's a really hard thing to be going through. I'm sorry to hear that. Do you have any really close family members that you are able to confide in?\\n\\n**USER:** I don't think I'd want to let them know how unhappy I am to be honest.\\n\\n**SYSTEM:** Oh I see. Well personally, when I was going through a hard time and I felt like I didn't have any friends, I found a couple pretty cool apps that helped me out! IT sounds crazy, but there are apps specifically for finding friends. They are kind of like dating apps but it's 100% meant for friends so everyone on them has the same intentions you do. I actually found a couple close friends through apps like that. Have you heard about those or looked into them?\\n\\n**USER:** I've never heard of friendship apps to be honest.\\n\\n**SYSTEM:** Yeah! They are a really good way to connect with people that you share a lot of similar interests with. It's a very popular app and there are tons of sites like facetime and facetime. ESC w/ COLM: well, they are apps that provide a way to meet people from different cultures.\\n\\nTable 10: Case study of emotional support conversation.\"}"}
{"id": "acl-2024-long-848", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: Prompts for collecting Situations under five common daily topics. Each prompt input consists of 6 lines of content. The first line L1 is an Introduction, lines L2 - L5 are Demonstration, and the last line L6 is Query.\"}"}
{"id": "acl-2024-long-848", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Type Prompt inputs for collecting Thoughts\\n(a)\\nThought in the negative cognitive Chain\\nL1 Complete the sentence:\\nL2 When I will give a presentation at college, I feel terrible since I think I will freeze on stage.\\nL3 When I was waiting in the waiting room for my last job interview, I felt terrible since I thought I would not be selected.\\nL4 When I lost the wallet, I felt terrible since I thought I would be blamed.\\nL5 When Situation, I feel terrible since I think \\n\\n(b)\\nThought in the positive cognitive Chain\\nL1 Complete the sentence:\\nL2 When I will give a presentation at college, I feel great since I think I am going to ace this presentation.\\nL3 When I was waiting in the waiting room for my last job interview, I felt great since I thought I would land my dream job.\\nL4 When I gave a house key to the girl I had gone on two dates with, I felt great since I thought she would fall in love with me.\\nL5 When Situation, I feel great since I think \\n\\nType Prompt inputs for collecting Clues\\n(a)\\nClue in the negative cognitive chain\\nL1 Complete the sentence:\\nL2 When I will give a presentation at college, I think I will freeze on stage since I haven't given a presentation before.\\nL3 When I was waiting in the waiting room for my last job interview, I thought I would not be selected since too many people were interviewing for the job.\\nL4 When my college entrance exam is tomorrow, I think I will fail the exam since I didn't prepare well.\\nL5 When Situation, I think Thought since \\n\\n(b)\\nClue in the positive cognitive chain\\nL1 Complete the sentence:\\nL2 When I will give a presentation at college, I think I will win the appreciation of my teachers and colleagues since they clap their hands happily.\\nL3 When my boss takes me to a nice restaurant, I think he likes me and respects my opinion since I have gone out with him before and he has always seemed to enjoy my company.\\nL4 When I will have a meeting with my boss, I think it is an opportunity to show myself since I have great knowledge about my work.\\nL5 When Situation, I think Thought since \\n\\nType Prompt inputs for collecting Actions\\n(a)\\nAction in the negative cognitive chain\\nL1 Complete the sentence:\\nL2 When I will give a presentation at college, I think I will freeze on stage, so I rehearse in front of the mirror.\\nL3 When I was waiting in the waiting room for my last job interview, I thought I would not be selected, so I took a deep breath repeatedly.\\nL4 When my college entrance exam is tomorrow, I think I will fail the exam, so I scan through some old papers to review.\\nL5 When Situation, I think Thought, so \\n\\n(b)\\nAction in the positive cognitive chain\\nL1 Complete the sentence:\\nL2 When I will give a presentation at college, I think I am going to ace this presentation, so I take good sleep at night.\\nL3 When I was waiting in the waiting room for my last job interview, I thought I would land my dream job, so I talked to other candidates confidently.\\nL4 When my boss took me to a nice restaurant, I thought it was a good sign of a promotion, so I talked to him about my career plan.\\nL5 When Situation, I think Thought, so \\n\\nTable 12: Prompts for collecting Thoughts, Clues and Actions in negative and positive cognitive chains. Each prompt input consists of 5 lines of content. The first line L1 is an Introduction, lines L2 - L4 are Demonstration, and the last line L5 is Query.\"}"}
{"id": "acl-2024-long-848", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Type Prompt inputs for collecting Emotions\\n\\n(a) Emotion generation in the negative chain\\n\\nL1 Situation: I arranged an interview for my friend with the CEO.\\nL2 Thought: It will go badly.\\nL3 Question: Choose one word to describe my feeling about the Situation when I have the Thought.\\nL4 Choice: Angry, Fearful, Sad\\nL5 Answer: Fearful\\n\\nL6 Situation: The boss asserted his authority by yelling at the employees.\\nL7 Thought: He's a terrible person.\\nL8 Question: Choose one word to describe my feeling about the Situation when I have the Thought.\\nL9 Choice: Angry, Fearful, Sad\\nL10 Answer: Angry\\n\\nL11 Situation: I asked my daughter to help me with the dishes.\\nL12 Thought: she will refuse.\\nL13 Question: Choose one word to describe my feeling about the Situation when I have the Thought.\\nL14 Choice: Angry, Fearful, Sad\\nL15 Answer: Sad\\n\\n(b) Emotion generation in the positive chain\\n\\nL1 Situation: I arranged an interview for my friend with the CEO.\\nL2 Thought: I will help my friend's career.\\nL3 Question: Choose one word to describe my feeling about the Thought when I am in the Situation.\\nL4 Choice: Joyful, Love, Surprised\\nL5 Answer: Joyful\\n\\nL6 Situation: I also took out the new girl on a date.\\nL7 Thought: She will call me her boyfriend.\\nL8 Question: Choose one word to describe my feeling about the Situation when I have the Thought.\\nL9 Choice: Joyful, Love, Surprised\\nL10 Answer: Love\\n\\nL11 Situation: I broke my leg skiing.\\nL12 Thought: I will get a snowboard for my birthday.\\nL13 Question: Choose one word to describe my feeling about the Thought when I am in the Situation.\\nL14 Choice: Joyful, Love, Surprised\\nL15 Answer: Surprised\\n\\nSituation Extended from Atomic Event\\nL17 Thought: Thought automatically generated and manually evaluated.\\nL18 Question: Choose one word to describe my feeling about the Thought when I am in the Situation.\\nL19 Choice: Joyful, Love, Surprised\\nL20 Answer:\"}"}
{"id": "acl-2024-long-848", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theory of mind refers to humans\u2019 ability to understand and infer the desires, beliefs, and intentions of others. People in the same situation will think differently due to their different personal experiences and the different characteristics of the personalities of others involved in the situations. Then, people will have different behavioral and affective responses to the same situation due to their different thoughts.\\n\\nWe want to collect individual thoughts, clues, actions, and emotions when they are in the situation. Therefore, we can use these data to predict others\u2019 mental states and behavioral response affective response to make the Theory of Mind inference.\\n\\n**Thought Definition**\\n\\n- **Negative Thought**: thoughts trigger negative emotion\\n- **Positive Thought**: thoughts trigger positive emotion\\n\\nWe just find out what they think when people are in a situation.\\n\\n**Task Description:** Find the reasonable thoughts from the \u2018thought1\u2019 \u2018thought2\u2019 \u2018thought3\u2019\\n\\n**Annotation Examples**:\\n\\n1. **Situation**: I will give a presentation to my classmates and teachers tomorrow.\\n   - **Negative Thought**: I think it is difficult for me to give a good speech\\n\\n2. **Situation**: When I am in the waiting room preparing for the next interview\\n   - **Positive Thought**: I feel sure that I will get the job.\\n\\n3. **Situation**: I\u2019m interviewing for a new job today.\\n   - **Positive Thought**: I feel sure I will get the job position.\\n   - **Negative Thought**: I am not favored by the interviewer.\\n\\n**Incorrect Data**:\\n\\n1. **Polarity in consistency**: the polarity of the thought candidate is inconsistent with the chain.\\n2. **Containing specific emotions**: happy, sad, anxious, etc.\\n3. **A specific action rather than a thought**: e.g. I sighed (action)\\n4. **Conflicting with the fact**: Situation: The dummy model is broken. Negative Thought: The blood stains clothes.\"}"}
{"id": "acl-2024-long-848", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Clue Definition\\n\\nClue concretizes and differentiates Thought.\\n\\nTask Description:\\n\\nFind the reasonable Clue from the 'clue1' 'clue2' 'clue3'\\n\\nCategories\\n\\nClues can be divided into several categories.\\n\\nOther: others' character, others' experience, others' appearance, others' behavior, etc.\\n\\nPerson: my character, my appearance, my experience.\\n\\nEvent: event characteristics, event flow.\\n\\nObject: characteristics of the object, etc.\\n\\nSocial: social knowledge, social norms\\n\\nAnnotation Examples:\\n\\n1. Situation: I am doing a presentation to my classmates and teachers.\\n   Positive Thought: I think they must have enjoyed my presentation.\\n   Clue: Because they all applauded. (Others' Action)\\n\\n2. Situation: I'm interviewing for a new job today.\\n   Positive Thought: I feel sure I will get the job position.\\n   Clue: I have a lot of work experience. (My Clue)\\n\\n   Negative Thought: I am not favored by the interviewer.\\n   Clue: The interviewer kept looking at the computer screen and did not look at me. (Others' Action)\\n\\n3. Situation: My boss invites me to dinner.\\n   Positive Thought: He will respect my choice.\\n   Clue: He is a polite person. (Others' Personality)\\n\\n4. Situation: I dropped my phone on the floor\\n   Positive Thought: I will have a new phone\\n   Clue: My phone is still under warranty. (Object Clue)\\n\\nIncorrect Data:\\n\\n1. Clues conflict with the Situation or cannot trigger the Thought. (Filter out these Clues)\\n\\n2. Some irrelevant content with the reasonable Clue content. (Delete the irrelevant part)\"}"}
{"id": "acl-2024-long-848", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instruction for Action Selection and Revision\\n\\nAction Definition\\n\\nThe actions denote the humans' behavioral responses which are controlled and guided by the thoughts in specific situations.\\n\\nTask Description:\\n\\nFind the reasonable actions from the 'action1' 'action2' 'action3'\\n\\nAnnotation Examples:\\n\\n1. Situation: I will give a presentation to my classmates and teachers tomorrow.\\n   Negative Thought: I think it is difficult for me to give a good speech\\n   Action: I keep repeating the speech in front of the mirror.\\n\\n2. Situation: When I am in the waiting room preparing for the next interview\\n   Positive Thought: I feel sure that I will get the job.\\n   Action: I am confident talking to the other candidates\\n\\n3. Situation: When I am in the waiting room preparing for the next interview\\n   Negative Thought: I am not favored by the interviewer.\\n   Action: I am in the hallway constantly recalling answers to pre-prepared interview questions.\\n\\n4. Situation: I dropped my phone on the floor\\n   Negative Thought: I need to spend a lot of money to buy a new phone\\n   Action: I go to check my bank card balance.\\n\\n5. Situation: I have an exam tomorrow.\\n   Negative Thought: I am worried that I will fail the exam tomorrow\\n   Behavior: I browse a lot of exam-related past exam questions\\n\\nIncorrect Data:\\n\\n1. Others' action (Keep only my action after having the thought)\\n2. Action conflict with the Situation or Thought. (Filter out these actions)\\n3. Some irrelevant content with the reasonable action content. (Delete the irrelevant part)\"}"}
{"id": "acl-2024-long-848", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instruction for Emotion Selection and Revision\\n\\nTask Description:\\nGive the emotion label to describe the feeling that the Thought evoke.\\n\\nThere are two class:\\nPositive (Joyful, Love, Surprised)\\nNegative (Angry, Fearful, Sad)\\n\\nThe feelings that each label describes:\\nAngry: Disgusting Envious Angry\\nFearful: Horror Nervous\\nSad: Disappointed Ashamed Sad\\nJoyful: Optimistic Proud Relieved Joyful\\nLove: Loveful\\nSurprised: Surprised\\n\\nAnnotation Examples:\\n1. I arranged an interview for my friend with the CEO.\\n   \u2022 It will go badly. Sad\\n   \u2022 he may not get the job. Sad\\n2. I asked the cashier for directions to the nearest post office\\n   \u2022 He refuses to let me know. Angry\\n3. I broke my leg skiing.\\n   \u2022 I will have to stay home all winter. Sad\\n   \u2022 I will have to have surgery. Fearful\\n   \u2022 I will get a lot of attention. Love\\n   \u2022 I will get a snowboard for my birthday. Surprised\\n   \u2022 It'll heal soon. Joyful\\n4. Incorrect Data:\\n   Filter Out:\\n   1. Inference not in the emotion labels. =====> Give a label\\n   2. Inference cannot describe the feeling of thought. ====> Give a new label\\n\\nTips:\\nThe emotion can be evoked by the specific factors:\\nFearful:\\nNegative impact of rejection/ Potential for failure /Loss of control, loss of integrity\\n/Unfamiliar things, new things /Being alone /Being in the dark /Death /Surgery\\nSad:\\nNot Expected Results/ Separation / Rejected by someone /Not getting what you want /\\nLosing power /Sadness of a loved one /Death of a loved one\\nAngry:\\nBeing treated differently/ Forced to feel pain/Expected to be disappointed\"}"}
{"id": "acl-2024-long-848", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Examples of cognitive chains in the situation from the School topic in COKE.\\n\\nFigure 10: Examples of cognitive chains in the situation from the Work topic in COKE.\"}"}
{"id": "acl-2024-long-848", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Examples of cognitive chains in the situation from the Tourism topic in COKE.\\n\\nFigure 12: Examples of cognitive chains in the situation from the Relationship topic in COKE.\"}"}
{"id": "acl-2024-long-848", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 13: Examples of cognitive chains in the situation from the Ordinary Life topic in COKE.\\n\\n16007\"}"}
