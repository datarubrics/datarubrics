{"id": "acl-2022-short-49", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WLASL-LEX: a Dataset for Recognising Phonological Properties in American Sign Language\\n\\nFederico Tavella and Viktor Schlegel\\n\\nAphrodite Galata and Angelo Cangelosi\\n\\n{name.surname}@manchester.ac.uk\\n\\nDepartment of Computer Science, The University of Manchester\\n\\nAbstract\\n\\nSigned Language Processing (SLP) concerns the automated processing of signed languages, the main means of communication of Deaf and hearing impaired individuals. SLP features many different tasks, ranging from sign recognition to translation and production of signed speech, but it has been overlooked by the NLP community thus far. In this paper, we bring attention to the task of modelling the phonology of sign languages. We leverage existing resources to construct a large-scale dataset of American Sign Language signs annotated with six different phonological properties. We then conduct an extensive empirical study to investigate whether data-driven end-to-end and feature-based approaches can be optimised to automatically recognise these properties. We find that, despite the inherent challenges of the task, graph-based neural networks that operate over skeleton features extracted from raw videos are able to succeed at the task to a varying degree. Most importantly, we show that this performance pertains even on signs unobserved during training.\\n\\n1 Introduction\\n\\nAround 200 languages in the world are signed rather than spoken, featuring their own vocabulary and grammatical structures. For example, the American Sign Language (ASL) is not a mere translation of English into signs and is unrelated to the British Sign Language (BSL). Their non-textual nature introduces many challenges to their automated processing, compared with purely textual NLP. Research on Sign Language Processing (SLP) encompasses tasks such as sign language detection, i.e., recognising if and which signed language is performed (Moryossef et al., 2020) and sign language recognition (SLR) (Koller, 2020), i.e., the identification of signs either in isolation or in continuous speech. Other tasks concern the translation from signed to spoken (or written) (Camgoz et al., 2018) language or the production of signs from text (Rastgoo et al., 2021). With the recent success of deep learning-based approaches in computer vision (CV), as well as advancements in\u2014 from the CV perspective\u2014related tasks of action and gesture recognition (Asadi-Aghbolaghi et al., 2017), SLP is gaining more attention in the CV community (Zheng et al., 2017).\\n\\nDue to the complexity of the tasks, some recent approaches to various SLP tasks implicitly rely on phonological features (Tornay, 2021; Metaxas et al., 2018; Gebre et al., 2013; Tavella et al., 2021). Surprisingly, however, little work has been carried out on explicitly modelling the phonology of signed languages. This presents a timely opportunity to investigate signed languages from the perspective of computational linguistics (Yin et al., 2021). In the context of signed languages, phonology typically distinguishes between manual features, such as usage, position and movement of hands and fingers, and non-manual features, such as facial expressions. Sign language phonology is a matured field with well-developed theoretical frameworks (Liddell and Johnson, 1989; Fenlon et al., 2017; Sandler, 2012). These phonological features, or phonemes, are drawn from a fixed inventory of possible configurations which is typically much smaller than the vocabulary of signed languages (Borg and Camilleri, 2020). For example, there is only a limited number of fingers that can be used...\"}"}
{"id": "acl-2022-short-49", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to perform a sign due to anatomical constraints. Hence, different signs share phonological properties and well performing classifiers can be used to predict those properties for signs unseen during training. This potentially holds even across different languages, because, while different languages may dictate different combinations of phonemes, there are also significant overlaps (Tornay et al., 2020).\\n\\nFinally, these phonological properties have a strong discriminatory power when determining signs. For example, in ASL-Lex (Caselli et al., 2017), a lexicon which also captures phonology information, the authors report that more than 50% of its 994 described signs have a unique combination of only six phonological properties and more than 80% of the signs share their combination with at most two other signs. By relying on this phonological information from resources such as ASL-Lex, many signs can be uniquely determined. This means that well performing classifiers can leverage this information to predict signs without having encountered them during training. This is a capability that current data-driven approaches to SLR lack by design (Koller, 2020). Thus, in combination, mature approaches to phonology recognition can facilitate the development of sign language resources, for example by providing first-pass silver annotations for new sign languages based on their phonological properties. This is an important task for both documenting low-resource sign languages as well as rapid developing of large-scale datasets, and for fully harnessing data-driven CV approaches.\\n\\nTo spur research in this direction, we extend the preliminary work by Tavella et al. (2021) and introduce the task of Phonological Property Recognition (PPR). More specifically, with this paper, we contribute (i) WLASLLex2001, a large-scale, automatically constructed PPR dataset, (ii) an analysis of the dataset quality, and (iii) an empirical study of the performance of different deep-learning based baselines thereon.\\n\\n2 Methodology\\n\\nWe address PPR as a classification problem based on features extracted from videos of people speaking SL. Although manual annotation approaches are widely adopted, these are time consuming and require expert knowledge. Instead, we rely on automated dataset construction. On a high level, we cross-reference a large-scale ASL SLR dataset with an ASL Lexicon and annotate videos of signs with their corresponding phonological properties. We then extract skeletal features, by taking advantage of pre-trained deep models from the computer vision community (Rong et al., 2021; Wang et al., 2019). Finally, we train several deep models to classify them as phonological classes.\\n\\n2.1 Dataset construction\\n\\nAs previously mentioned, ASL-Lex (Caselli et al., 2017) contains phonological features of American Sign Language, such as where the sign is executed, the movement performed by the hand and the number of hands and fingers involved. The latter properties were coded by 3 ASL-versed people. In our work, we are interested in recognising phonological properties from videos of people speaking ASL. Consequently, we aim to construct a dataset, suitable for supervised learning, containing videos labelled with six phonological properties. Specifically, we choose the manual properties with the strongest discriminatory power to determine signs based on their configuration (Caselli et al., 2017): (i) flexion: aperture of the selected fingers of the dominant hand at sign onset, (ii) major location: general location of the dominant hand at sign onset, (iii) minor location: specific location of the dominant hand at sign onset, (iv) movement: the first movement path of the sign, (v) selected fingers: fingers that are moving or are foregrounded during that movement, and (vi) sign type: symmetry of the hands according to Battison (1978). A detailed description of all the properties is provided in the appendix.\\n\\nOne of the limitations of ASL-Lex is the small number of examples and lack of variety: its first iteration (ASL-Lex 1.0) contains less than 1000 videos, all signed by the same person. While sufficient for educational purposes, these videos are of limited suitability for developing robust classifiers that can capture the diversity of ASL speakers (Yin et al., 2021). To this end, we source videos from WLASL (Li et al., 2020) (Word Level-ASL), one...\"}"}
{"id": "acl-2022-short-49", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of the largest available SL datasets, featuring more than 2000 glosses demonstrated by over 100 people, for a total of more than 20000 videos. Each sign is performed by at least 3 different signers, which implies greater variability compared to having one gloss performed by only one user. By cross-referencing ASL-Lex and WLASL2000 based on corresponding glosses, we can increase the number of samples available to train our models.\\n\\nFinally, to leverage state of the art SLR architectures that operate over structured input, we enrich each raw video with its extracted keypoints that represent the joints of the speaker. To do so, we use two pretrained models, FrankMocap (Rong et al., 2021) and HRNet (Wang et al., 2019). While these tracking algorithms follow different paradigms, the former extracting 3D coordinates based on a predicted human model and the latter predicting keypoints as coordinates from videos directly, they produce similar outputs. An important distinction is that while FrankMocap estimates the 3D keypoints, HRNet outputs 2D keypoints with associated prediction confidence scores. We use these different models to explore whether different tracking algorithms affect the recognition of phonological classes. We select a subset of features of the upper body, namely: nose, eyes, shoulders, elbows, wrists, thumbs and first/last knuckles of the fingers. These manual features were determined to be the most informative while performing sign language recognition (Jiang et al., 2021b).\\n\\nOur final dataset, WLASL-Lex2001 (WLASL2000 + ASL-Lex 1.0), is composed of 10017 videos corresponding to 800 glosses, 3D skeletons ($x$, $y$, $z$ from FrankMocap and $x$, $y$ and score from HRNet) labelled with their phonological properties. A characteristic of this dataset is that it follows a long tailed distribution. Due to the nature of language, some phonological properties are more common than others, which means that some classes are more represented than others. On the one hand, the training setup for our models should take this factor into account, but on the other hand, the advantage of training over phonological classes instead of glosses is that different glosses can share phonological classes. 2.2 Models\\n\\nTo estimate the complexity of the dataset, we use the majority-class baseline and the Multi-Layer Perceptron (MLP) as basic deep models. We further use Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) as models capable of capturing the temporal component of videos. As state-of-the-art SLP architectures that have been used to perform SLR, we use the I3D 3D Convolutional Neural Network (Carreira and Zisserman, 2017; Li et al., 2020) able to learn from raw videos, and the Spatio-Temporal Graph Convolutional Network (STGCN) (Jiang et al., 2021b) that captures both spatial and temporal components from the extracted keypoints.\\n\\n2.3 Experimental Setup\\n\\nFor each phonological property we generate dataset splits and train dedicated models separately. While a multi-class multi-label approach could achieve higher scores, by relying on potential interdependencies of different properties, we chose to model the properties in isolation, to disentangle the factors that affect the learnability of each property. From now on, when we mention the dataset, we refer to an instance of the WLASL-Lex 2001 dataset, where labels are the values of a single phonological class. We make this distinction because we produce six different train, validation and test splits (with a 70 : 15 : 15 ratio) stratifying on the corresponding phonological property (Phoneme). By doing so, we make sure that (a) all splits contain all possible labels for a classification target (i.e. phonological property) and (b) follow the same distribution. Since we source the videos from WLASL, we have multiple videos representing each gloss, therefore, randomly splitting our data will result in the fact that glosses in the test set might appear in the training set as well, signed by a different speaker. Thus, to investigate how well the models can predict properties on unseen glosses, we also produce label-stratified splits on gloss-level (Gloss), such that videos of glosses in the validation and test set do not appear in training data and vice versa. Thus, to summarise, experiments in the Phoneme setting aim to evaluate the capability to recognise phonological properties of signs that were already encountered in the training data, but are performed by a different speaker in the test set. Conversely, experiments in the Gloss setting aim to evaluate the ability to recognise phonological properties of signs completely unseen during training.\"}"}
{"id": "acl-2022-short-49", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To investigate the former, we measure the agreement on videos that all models misclassify. We omit error margins for balanced accuracy as the low number of classes results in a small sample size. Additional movement when predicting the phonological properties. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe other models are trained from scratch using MLP and fine-tune it on raw videos from our datasets. We select the best performing model based on performance on the validation set and for the shorter sequences are looped to reach the fixed length. We keypoint as input. We fix the length of all input to and operate over HRNet-extracted keypoints has been shown to be the largest contributor to the SLR performance measures are reported in the appendix. We measure the error margin as a confidence interval at.\\n\\nThe lower half of Table 1 shows the performance of all models to reach the baseline for some properties (for balanced accuracy as the low number of classes results in a small sample size. Additional movement when predicting the phonological properties. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties. Often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe lower half of Table 1 shows the performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge and do not exhibit easily exploitable regularities. Due to its simplicity, it is barely able to perform in general, and class-balanced accuracy to ensure both accuracy, to investigate how well models take into account how well they are able to model different classes of the phonological properties.\\n\\nThe performance of all models to reach the baseline for some properties (often, crowd sourced (Polonio et al., 2018) or automatically constructed datasets such as ours, it is the only challenge"}
{"id": "acl-2022-short-49", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"using Fleiss' \u03ba. Intuitively, if models consistently agree on a label different than the ground truth, the ground truth label might be wrong. We find that averaged across the six tasks, the agreement is negligible: 0.09 \u00b1 0.06 and 0.11 \u00b1 0.09 for Phoneme and Gloss split, respectively.\\n\\nSimilarly, for the latter, if all models consistently fail to assign any correct label for a given video (e.g. all models err on a video appearing in the test sets of movement and flexion), this can hint at low quality of the input, making it impossible to predict anything correctly. We find that this is not the case with WLASL-LEX2001, as videos appearing in test sets of different tasks tend to have a low mutual misclassification rate: 1% and 0.7% of videos appearing in test sets of two and three tasks were misclassified by all models for all associated tasks for the Phoneme split. For the Gloss split the numbers are 3 and 0% for two and three tasks, respectively. Together, these observations suggest that the models presented in this paper are unlikely to reach the performance ceiling on WLASL-Lex2001 and more advanced approaches could obtain even higher accuracy scores.\\n\\n4 Conclusion\\n\\nIn this paper, we discuss the task of Phonological Property Recognition (PPR). We automatically construct a dataset for the task featuring six phonological properties and analyse it extensively. We find that there is potential for improvement over our presented data-driven baseline approaches. Researchers pursuing this direction can focus on developing better-performing models, for example by relying on jointly learning all properties, as labels for different properties can be mutually dependent. Another possible avenue is to investigate the feasibility of using PPR to perform tokenisation of continuous sign language speech, by decomposing it into multiple phonemes, which is identified as one of the big challenges of SLP (Yin et al., 2021).\\n\\nAcknowledgements\\n\\nThe authors would like to acknowledge the use of the Computational Shared Facility at The University of Manchester. The work was partially supported by the UKRI TAS Node on Trust, the US Air Force project THRIVE++ and the H2020 projects TRAINCREASE, eLADDA and PERSEO.\"}"}
{"id": "acl-2022-short-49", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Oscar Koller. 2020. Quantitative Survey of the State of the Art in Sign Language Recognition.\\n\\nDongxu Li, Cristian Rodriguez, Xin Yu, and Hongdong Li. 2020. Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison. In The IEEE Winter Conference on Applications of Computer Vision, pages 1459\u20131469.\\n\\nScott K. Liddell and Robert E. Johnson. 1989. American Sign Language: The Phonological Base. Sign Language Studies, 1064(1):195\u2013277.\\n\\nB.W. Matthews. 1975. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA) - Protein Structure, 405(2):442\u2013451.\\n\\nDimitris Metaxas, Mark Dilsizian, and Carol Neidle. 2018. Scalable ASL sign recognition using model-based machine learning and linguistically annotated corpora.\\n\\nAmit Moryossef, Ioannis Tsochantaridis, Roee Aharoni, Sarah Ebling, and Srini Narayanan. 2020. Real-Time Sign Language Detection Using Human Pose Estimation. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 12536 LNCS:237\u2013248.\\n\\nDavide Polonio, Federico Tavella, Marco Zanella, and Armir Bujari. 2018. Ghio-ca: An android application for automatic image classification. In Smart Objects and Technologies for Social Good, pages 248\u2013257, Cham. Springer International Publishing.\\n\\nRazieh Rastgoo, Kourosh Kiani, and Sergio Escalera. 2021. Sign Language Recognition: A Deep Survey. Expert Systems with Applications, 164:113794.\\n\\nYu Rong, Takaaki Shiratori, and Hanbyul Joo. 2021. Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration. In IEEE International Conference on Computer Vision Workshops.\\n\\nWendy Sandler. 2012. The Phonological Organization of Sign Languages. Language and Linguistics Compass, 6(3):162\u2013182.\\n\\nViktor Schlegel, Marco Valentino, Andr\u00e9 Andre Freitas, Goran Nenadic, and Riza Batista-Navarro. 2020. A Framework for Evaluation of Machine Reading Comprehension Gold Standards. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5359\u20135369, Marseille, France. European Language Resources Association.\\n\\nFederico Tavella, Aphrodite Galata, and Angelo Cangeli. 2021. Phonology recognition in American Sign Language.\\n\\nSandrine Tornay. 2021. Explainable Phonology-based Approach for Sign Language Recognition and Assessment. Ph.D. thesis, Lausanne, EPFL.\\n\\nSandrine Tornay, Marzieh Razavi, and Mathew Magimai.-Doss. 2020. Towards Multilingual Sign Language Recognition. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6304\u20136308. IEEE.\\n\\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. 2019. Deep high-resolution representation learning for visual recognition. TPAMI.\\n\\nKayo Yin, Amit Moryossef, Julie Hochgesang, Yoav Goldberg, and Malihe Alikhani. 2021. Including Signed Languages in Natural Language Processing. pages 7347\u20137360.\\n\\nLihong Zheng, Bin Liang, and Ailian Jiang. 2017. Recent Advances of Deep Learning for Sign Language Recognition. DICTA 2017 - 2017 International Conference on Digital Image Computing: Techniques and Applications, 2017-Decem:1\u20137.\"}"}
{"id": "acl-2022-short-49", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2 contains all the hyperparameters explored during our experiment over each different model. The best model is the one that maximises the Matthew\u2019s correlation coefficient\\n\\n\\\\[ MCC = \\\\frac{TP \\\\cdot TN - FP \\\\cdot FN}{\\\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} \\\\]\\n\\nwith \\\\(TP, TN, FP, FN\\\\) being true/false positive/negative. For the STGCN we use hyperparameters chosen by Jiang et al. (2021a), because initial experiments on our data showed a difference of at most 2% accuracy, which is within the uncertainty estimate. To find the optimal hyperparameters for the other models, we perform Bayesian optimisation over a pre-defined set. We maximise Matthews correlation coefficient (MCC) (Matthews, 1975) on the validation sets of all six tasks. We choose MCC as it provides a good trade-off between overall and class-level accuracy which is necessary due to the unbalance inherently present in our dataset.\\n\\n| Model | Parameters |\\n|-------|------------|\\n| MLP   | number of layers, hidden dimension, dropout, learning rate, scheduler step size, gamma |\\n| RNN   | number of RNN layers, RNN hidden dimension, RNN dropout |\\n| STGCN | learning rate, number of groups, block size, window size, scheduler step size, dropout, warmup epochs |\\n| 3D CNN| dropout, learning rate, gamma, scheduler step size, window size |\\n\\nTable 2: Set of explored hyperparameters for each different model\\n\\nTable 3 illustrates the performance on the test set for each model with respect to chance as measured by training 5 models from different random seeds. The performance difference is negligible suggesting that model training is largely stable with regard to chance.\\n\\n| Model   | Accuracy |\\n|---------|----------|\\n| MLP     | 74.39 \u00b1 0.35 |\\n| RNN     | 79.12 \u00b1 0.46 |\\n| STGCN   | 84.12 \u00b1 0.29 |\\n| 3D CNN  | 69.23 \u00b1 0.93 |\\n\\nTable 3: Mean and standard deviation of accuracy of all architectures trained with the HRNet output, measured on the SIGNETYPE test set and averaged over 5 different random seeds. Results for the 3D CNN are obtained from the validation set.\\n\\nC Phonological classes description\\n\\nTables 4 to 9 describe in detail the meaning of values for all the phonological classes according to ASL-Lex (Caselli et al., 2017). The cardinality is calculated on WLASL-Lex, which is why some classes that are in ASL-Lex are not represented (i.e., cardinality equal to 0).\\n\\nD Additional results\\n\\nTable 10 illustrates additional results for several different metrics. In particular, we report micro- and macro precision/recall and Matthews correlation coefficient. These metrics help to give a better understanding of the classification results, as they are affected more by data imbalance when compared to accuracy.\"}"}
{"id": "acl-2022-short-49", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 4: Values and relative definitions for selected fingers\\n\\n| Value Definition                  | Cardinality |\\n|----------------------------------|-------------|\\n| Value Definition                 |             |\\n| imrp index, middle, ring, pinky  | 4824        |\\n| imr index, middle, ring finger   | 95          |\\n| mrp middle, ring, pinky finger   | 28          |\\n| im index, middle finger          | 1296        |\\n| ip index, pinky finger           | 51          |\\n| mr middle, ring finger           | 0           |\\n| mp middle, pinky finger          | 0           |\\n| rp ring, pinky finger            | 0           |\\n| i index finger                   | 2547        |\\n| m middle finger                  | 259         |\\n| r ring finger                    | 0           |\\n| p pinky                         | 407         |\\n| thumb                            | 510         |\\n\\n### Table 5: Values and relative definitions for major location\\n\\n| Value Definition                  | Cardinality |\\n|----------------------------------|-------------|\\n| Head Sign                        | 3137        |\\n| Arm Sign                         | 219         |\\n| Body Sign                        | 1019        |\\n| Hand Sign                        | 2194        |\\n| Neutral Sign                     | 3448        |\\n| Other Sign                       | 0           |\\n\\n### Table 6: Values and relative definitions for flexion\\n\\n| Value Definition                  | Cardinality |\\n|----------------------------------|-------------|\\n| Value Definition                 |             |\\n| 1 Fully open: no joints          | 5037        |\\n| 2 Bent (closed): non-base joints | 693         |\\n| 3 Flat-open: base joints         | 909         |\\n| 4 Flat-closed: base joints       | 507         |\\n| 5 Curved open: base and non-base | 1130        |\\n| 6 Curved closed: base and non-base joints | 642 |\\n| 7 Fully closed: base and non-base joints | 795 |\\n| Stacked                          | 123         |\\n| Crossed                          | 181         |\"}"}
{"id": "acl-2022-short-49", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Value         | Definition                              | Cardinality |\\n|--------------|-----------------------------------------|-------------|\\n| HeadTop Sign | is produced on top of the head           | 20          |\\n| Forehead Sign| is produced at the forehead              | 246         |\\n| Eye Sign     | is produced near the eye                 | 616         |\\n| CheekNose Sign| is produced on the cheek or nose         | 511         |\\n| UpperLip Sign| is produced on the upper lip             | 53          |\\n| Mouth Sign   | is produced on the mouth                 | 431         |\\n| Chin Sign    | is produced on the chin                  | 717         |\\n| UnderChin Sign| is produced under the chin               | 74          |\\n| UpperArm Sign| is produced on the upper arm             | 39          |\\n| ElbowFront Sign| is produced in the crook of the elbow    | 0           |\\n| ElbowBack Sign| is produced on the outside of the elbow  | 13          |\\n| ForearmBack Sign| is produced on the outside of the forearm | 32         |\\n| ForearmFront Sign| is produced on the inside of the forearm | 10         |\\n| ForearmUlnar Sign| is produced on the ulnar side of the forearm | 56         |\\n| WristBack Sign| is produced on the back of the wrist     | 23          |\\n| WristFront Sign| is produced on the front of the wrist    | 0           |\\n| Neck Sign    | is produced on the neck                  | 68          |\\n| Shoulder Sign| is produced on the shoulder              | 101         |\\n| Clavicle Sign| is produced on the clavicle              | 419         |\\n| TorsoTop Sign| is produced in the upper third of the torso | 0         |\\n| TorsoMid Sign| is produced in the middle third of the torso | 0         |\\n| TorsoBottom Sign| is produced in the bottom third of the torso | 19     |\\n| Waist Sign   | is produced at the waist                 | 34          |\\n| Hips Sign    | is produced on the hips                  | 59          |\\n| Palm Sign    | is produced on the palm of the non-dominant hand | 925     |\\n| FingerFront Sign| is produced on the front of the fingers of the non-dominant hand | 99     |\\n| PalmBack Sign| is produced on the back of the palm of the non-dominant hand | 218 |\\n| FingerBack Sign| is produced on the back of the fingers of the non-dominant hand | 186     |\\n| FingerRadial Sign| is produced on the radial side of the non-dominant hand | 410     |\\n| FingerUlnar Sign| is produced on the ulnar side of the non-dominant hand | 40     |\\n| FingerTip Sign| is produced on the tip of the fingers of the non-dominant hand | 158    |\\n| Heel Sign    | is produced on the heel of the non-dominant hand | 88     |\\n| Other Sign   | is produced in an unspecified location on the body | 707 |\\n| Neutral Sign | is not produced on or near the body      | 3390        |\"}"}
{"id": "acl-2022-short-49", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Value Definition Cardinality\\n\\nOne Handed Sign only recruits one hand 3939\\n\\nSymmetrical\\nOr Alternating\\nSign recruits both hands\\nPhonological specifications for both hands are identical\\nMovement of both hands is either symmetrical or alternating 3358\\n\\nAsymmetrical\\nSame Handshape\\nSign recruits both hands\\nOnly the dominant hand moves\\nThe location and orientation of the hands may differ,\\nbut the other specifications of handshape are the same\\nNon-Dominant hand must be an unmarked handshape (B A S 1 C O 5) 938\\n\\nAsymmetrical\\nDifferent Handshape\\nSign recruits both hands\\nOnly the dominant hand moves\\nThe location and orientation of the hands may differ,\\nand the other specifications of handshape are not the same\\nNon-Dominant hand must be an unmarked handshape (B A S 1 C O 5) 1639\\n\\nOther Sign violates Battison's Symmetry and Dominance Conditions 143\\n\\nTable 8: Values and relative definitions for sign type\\n\\nValue Definition Cardinality\\n\\nTable 9: Values and relative definitions for movement\\n\\nValue Definition Cardinality\\n\\nStraight\\nStraight movement of the dominant hand through xyz space 1938\\n\\nCurved\\nSingle arc movement of the dominant hand through xyz space\\nHands may or not make contact with multiple locations 1255\\n\\nBackAndForth\\nSequence of more than one straight or curved movements 3549\\n\\nCircular\\nCircular movement of the dominant hand through space\\nRotation alone does not constitute a circular movement 1129\\n\\nNone\\nEntire sign (or first free morpheme) does not have a path movement 1748\\n\\nOther\\nSign has another unspecified path movement 398\"}"}
{"id": "acl-2022-short-49", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| FLEXION | LOCATION | MOVEMENT | FINGERS | SIGNTYPE |\\n|---------|----------|----------|---------|-----------|\\n| P\u00b5 | PM | R\u00b5 | RM | MCC |\\n\\nBaseline.\\n\\n3 11 0 88 4 20 87 0 6 33 87 12 0 0 48 17 11 0 32 20 0 32 | 1 7 14 8 0 6 2 41 5 28 5 26 9 5 6 59 5 6 59 5 5 7 9 54 1 42 8 7 3 52 | 54 | 44 H | 49 3 6 50 3 8 52 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 3 3 8 46 | 3 "}
