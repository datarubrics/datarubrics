{"id": "acl-2023-long-51", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\\n\\nAll images generated by stable diffusion discord services are under the CC0 1.0 License, and therefore so are images in this dataset. In addition, the distribution of the dataset is under the Terms of Use (StabilityAI, 2022b) posed by Stability AI, the company that both made Stable Diffusion and runs the official Discord server.\\n\\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.\\n\\nAll images in this dataset have a CC0 1.0 License and follows the Stability AI\u2019s Terms of Use (StabilityAI, 2022b).\\n\\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.\\n\\nNo.\\n\\nAny other comments? None.\\n\\nMaintenance\\n\\nWho will be supporting/hosting/maintaining the dataset? The authors of this paper will be supporting and maintaining the dataset.\\n\\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)? The contact information of the curators of the dataset is listed on the project website: https://poloclub.github.io/diffusiondb.\\n\\nIs there an erratum? If so, please provide a link or other access point. There is no erratum for our initial release. Errata will be documented in future releases on the dataset website.\\n\\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub).\\n\\nYes, we will monitor the Google Form where users can report harmful images and creators can remove their images. We will update the dataset bimonthly. Updates will be posted on the project website https://poloclub.github.io/diffusiondb.\\n\\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced.\\n\\nPeople can use a Google Form linked on the project website to remove specific instances from DIFFUSION DB.\\n\\nWill older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers.\\n\\nWe will continue to support older versions of the dataset.\\n\\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description.\\n\\nAnyone can extend/augment/build on/contribute to DIFFUSION DB. Potential collaborators can contact the dataset authors.\\n\\nAny other comments? None.\"}"}
{"id": "acl-2023-long-51", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\nSection 7 and Appendix A\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\nSection 8 and Appendix A\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\nSection 1\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\nLeft blank.\\n\\nB\\n\u25a1 B1. Did you use or create scientific artifacts?\\nWe created a new dataset, described in Section 2.\\n\\n\u25a1 B2. Did you cite the creators of artifacts you used?\\nSection 2\\n\\n\u25a1 B3. Did you discuss the license or terms for use and / or distribution of any artifacts?\\nSection 2.5.\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\nSection 2.5.\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\nAbstract, section 2\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\nSection 2 and 3\\n\\nC\\n\u25a1 C1. Did you run computational experiments?\\nLeft blank.\\n\\n\u25a1 C2. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\nNot applicable. Left blank.\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-51", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\nNot applicable. Left blank.\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\nNot applicable. Left blank.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\nNot applicable. Left blank.\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\nLeft blank.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\nNot applicable. Left blank.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\nNot applicable. Left blank.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\nNot applicable. Left blank.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\nNot applicable. Left blank.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\nNot applicable. Left blank.\"}"}
{"id": "acl-2023-long-51", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DIFFUSION DB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models\\nZijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, Duen Horng Chau\\nCollege of Computing, Georgia Tech. \\n{jayw|emontoya30|david.munechika|alexanderyang|bhoov|polo}@gatech.edu\\n\\nAbstract\\nWith recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DIFFUSION DB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models. DIFFUSION DB is publicly available at: https://poloclub.github.io/diffusiondb.\\n\\n1 Introduction\\nRecent diffusion models have gained immense popularity by enabling high-quality and controllable image generation based on text prompts written in natural language (Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022). Since the release of these models, people from different domains have quickly applied them to create award-winning artworks (Roose, 2022), synthetic radiology images (Chambon et al., 2022), and even hyper-realistic videos (Ho et al., 2022).\\n\\nHowever, generating images with desired details is difficult, as it requires users to write proper prompts specifying the exact expected results. Developing such prompts requires trial and error, and can often feel random and unprincipled (Liu and Chilton, 2022). Willison et al. (2022) analogize writing prompts to wizards learning \u201cmagical spells\u201d: users do not understand why some prompts work, but they will add these prompts to their \u201cspell book.\u201d For example, to generate highly-detailed images, it has become a common practice to add special keywords such as \\\"trending on artstation\\\" and \\\"unreal engine\\\" in the prompt.\\n\\nPrompt engineering has become a field of study in the context of text-to-text generation, where researchers systematically investigate how to construct prompts to effectively solve different downstream tasks (Branwen, 2020; Reynolds and McDonell, 2021). As large text-to-image models are relatively new, there is a pressing need to understand how these models react to prompts, how to write effective prompts, and how to design tools to help users generate images (Liu and Chilton, 2022). Our work helps researchers tackle these critical challenges, through three major contributions:\\n\\n\u2022 DIFFUSION DB (Fig. 1), the first large-scale prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion (Rombach et al., 2022) using 1.8 million unique prompts along with their accompanying hyperparameters.\\n\\nFig. 1: DIFFUSION DB is the first large-scale dataset featuring 6.5TB data including 1.8 million unique Stable Diffusion prompts and 14 million generated images with accompanying hyperparameters. It provides exciting research opportunities in prompt engineering, deepfake detection, and understanding large generative models.\"}"}
{"id": "acl-2023-long-51", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DIFFUSION DB contains 14 million Stable Diffusion images, 1.8 million unique text prompts, and all model hyperparameters: seed, step, CFG scale, sampler, and image size. Each image also has a unique filename, a hash of its creator's Discord username, and a creation timestamp. To help researchers filter out potentially unsafe or harmful content, we employ state-of-the-art models to compute an NSFW score for each image and prompt.\\n\\nWe construct this dataset by collecting images shared on the Stable Diffusion public Discord server (\u00a7 2). We release DIFFUSION DB with a CC0 1.0 license, allowing users to flexibly share and adapt the dataset for their use. In addition, we open-source our code that collects, processes, and analyzes the images and prompts.\\n\\n- Revealing prompt patterns and model errors. The unprecedented scale of DIFFUSION DB paves the path for researchers to systematically investigate diverse prompts and associated images that were previously not possible. By characterizing prompts and images, we discover common prompt patterns and find different distributions of the semantic representations of prompts and images. Our error analysis highlights particular hyperparameters and prompt styles can lead to model errors. Finally, we provide evidence of image generative models being used for potentially harmful purposes such as generating misinformation and nonconsensual pornography (\u00a7 3).\\n\\n- Highlighting new research directions. As the first-of-its-kind text-to-image prompt dataset, DIFFUSION DB opens up unique opportunities for researchers from natural language processing (NLP), computer vision, and human-computer interaction (HCI) communities. The scale and diversity of this human-actuated dataset will provide new research opportunities in better tooling for prompt engineering, explaining large generative models, and detecting deepfakes (\u00a7 4).\\n\\nWe believe DIFFUSION DB will serve as an important resource for researchers to study the roles of prompts in text-to-image generation and design next-generation human-AI interaction tools.\\n\\n1 Code: https://github.com/poloclub/diffusiondb\\n\\n2 Constructing DIFFUSION DB\\nWe construct DIFFUSION DB (Fig. 2) by scraping user-generated images from the official Stable Diffusion Discord server. We choose Stable Diffusion as it is currently the only open-source large text-to-image generative model, and all generated images have a CC0 1.0 license that allows uses for any purpose (StabilityAI, 2022b). We choose the official public Discord server as it has strict rules against generating illegal, hateful, or NSFW (not suitable for work, such as sexual and violent content) images, and it prohibits sharing prompts with personal information (StabilityAI, 2022a).\\n\\nOur construction process includes collecting images (\u00a7 2.1), linking them to prompts and hyperparameters (\u00a7 2.2), applying NSFW detectors (\u00a7 2.3), creating a flexible file structure (\u00a7 2.4), and distributing the dataset (\u00a7 2.5). We discuss DIFFUSION DB\u2019s limitations and broader impacts in \u00a7 7, \u00a7 8, and a Data Sheet (Gebru et al., 2020) (\u2021 A).\"}"}
{"id": "acl-2023-long-51", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"images as a grid (e.g., a $3 \\\\times 3$ grid of $n = 9$ images); these images have the same prompt and hyperparameters but different seeds. We use Pillow (Clark, 2015) to split a collage into $n$ individual images and assign them with the correct metadata and unique filenames. Finally, we compress all images in DIFUSION-DB using lossless WebP (Google, 2010).\\n\\n### 2.3 Identifying NSFW Content\\n\\nThe Stable Diffusion Discord server prohibits generating NSFW images (StabilityAI, 2022a). Also, Stable Diffusion has a built-in NSFW filter that automatically blurs generated images if it detects NSFW content. However, we find DIFUSION-DB still includes NSFW images that were not detected by the built-in filter or removed by server moderators. To help researchers filter these images, we apply state-of-the-art NSFW classifiers to compute NSFW scores for each prompt and image. Researchers can determine a suitable threshold to filter out potentially unsafe data for their tasks.\\n\\n#### NSFW Prompts\\n\\nWe use a pre-trained multilingual toxicity prediction model to detect unsafe prompts (Hanu and Unitary team, 2020). This model outputs the probabilities of a sentence being toxic, obscene, threat, insult, identity attack, and sexually explicit. We compute the text NSFW score by taking the maximum of the probabilities of being toxic and sexually explicit (Fig. 3 Top).\\n\\n#### NSFW Images\\n\\nWe use a pre-trained EfficientNet classifier to detect images with sexual content (Schuhmann et al., 2022). This model predicts the probabilities of five image types: drawing, hentai, neutral, sexual, or porn. We compute the image NSFW score by summing the probabilities of hentai, sexual, and porn. We use a Laplacian convolution kernel with a threshold of 10 to detect images that have already been blurred by Stable Diffusion and assign them a score of 2.0 (Fig. 3 Bottom). As Stable Diffusion's blur effect is strong, our blurred image detector has high precision and recall (both 100% on 50k randomly sampled images).\\n\\n#### NSFW Detector Accuracy\\n\\nTo access the accuracy of these two pre-trained state-of-the-art NSFW detectors, we randomly sample 5k images and 2k prompt texts and manually annotate them with two binary NSFW labels (one for image and one for prompt) and analyze the results. As the percentage of samples predicted as NSFW (score > 0.5) is small, we up-sample positive samples for annotation, where we have an equal number of positive and negative examples in our annotation sample. After annotation, we compute the precisions and recalls. Because we have up-sampled positive predictions, we adjust the recalls by multiplying false negatives by a scalar to adjust the sampling bias. The up-sampling does not affect precisions. Finally, the precisions, recalls and adjusted recalls are 0.3604, 0.9565, and 0.6661 for the prompt NSFW detector, and 0.315, 0.9722, and 0.3037 for the image NSFW detector. Our results suggest two detectors are progressive classifiers. The lower adjusted recall of the prompt NSFW detector can be attributed to several potential factors, including the use of a fixed binary threshold and the potential discrepancy in the definition of NSFW prompts between the detector and our annotation process.\\n\\n### 2.4 Organizing DIFUSION-DB\\n\\nWe organize DIFUSION-DB using a flexible file structure. We first give each image a unique filename using Universally Unique Identifier (UUID, Version 4) (Leach et al., 2005). Then, we organize images into 14,000 sub-folders\u2014each includes 1,000 images. Each sub-folder also includes a JSON file that contains 1,000 key-value pairs mapping an image name to its metadata. An example of this image-prompt pair can be seen in Fig. 2. This modular file structure enables researchers to flexibly use a subset of DIFUSION-DB. We create a metadata table in Apache Parquet format (Apache, 2013) with 13 columns: unique image name, image path, prompt, seed, CFG scale, sampler, width, height, username hash, timestamp, image NSFW score, and prompt NSFW.\"}"}
{"id": "acl-2023-long-51", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 4: The distribution of token counts for all 1.8 million unique prompts in DIFFUSION DB. It is worth noting that Stable Diffusion truncates prompts at 75 tokens.\\n\\n2.5 Distributing DIFFUSION DB\\n\\nWe distribute DIFFUSION DB by bundling each image sub-folder as a Zip file. We collect Discord usernames of image creators (\u00a7 2.2), but only include their SHA256 hashes in the distribution\u2014as some prompts may include sensitive information, and explicitly linking them to their creators can cause harm. We host our dataset on a publicly accessible repository under a CC0 1.0 license. We provide scripts that allow users to download and load DIFFUSION DB by writing two lines of code. We discuss the broader impacts of our distribution in \u00a7 7, \u00a7 8, and the Data Sheet (\u2021 A). To mitigate the potential harms, we provide a form for people to report harmful content for removal. Image creators can also use this form to remove their images.\\n\\n3 Data Analysis\\n\\nTo gain a comprehensive understanding of the dataset, we analyze it from different perspectives. We examine prompt length (\u00a7 3.1), language (\u00a7 3.2), characteristics of both prompts (\u00a7 3.3) and images (\u00a7 3.4). We conduct an error analysis on misaligned prompt-image pairs (\u00a7 3.5) and provide empirical evidence of potentially harmful uses of image generative models (\u00a7 3.6).\\n\\n3.1 Prompt Length\\n\\nWe collect prompts from Discord, where users can submit one prompt to generate multiple images and experiment with different hyperparameters. Our dataset contains 1,819,808 unique prompts. We tokenize prompts using the same tokenizer as used in Stable Diffusion (Platen et al., 2022). This tokenizer truncates tokenized prompts at 75 tokens, excluding special tokens <|startoftext|>\\n\\nWe measure the length of prompts by their tokenized length. The prompt length distribution (Fig. 4) indicates that shorter prompts (e.g., around 6 to 12 tokens) are the most popular. The spike at 75 suggests many users submitted prompts longer than the model's limit, highlighting the need for user interfaces guiding users to write prompts within the token limit.\\n\\n3.2 Prompt Language\\n\\nWe use a pre-trained language detector (Joulin et al., 2017) to identify the languages used in prompts. 98.3% of the unique prompts in our dataset are written in English. However, we also find a large number of non-English languages, with the top four being German (5.2k unique prompts), French (4.6k), Italian (3.2k), and Spanish (3k). The language detector identifies 34 languages with at least 100 unique prompts in total. Stable Diffusion is trained on LAION-2B(en) (Schuhmann et al., 2022) that primarily includes images with English descriptions, thus our findings suggest that expanding the training data's language coverage to improve the user experience for non-English communities.\\n\\n3.3 Characterizing Prompts\\n\\nIn this section, we explore the characteristics of prompts in DIFFUSION DB. We examine the syntactic (\u00a7 3.3.1) and semantic (\u00a7 3.3.2) features of prompt text via interactive data visualizations. Lastly, We discuss the implications of our findings and suggest future research directions.\\n\\n3.3.1 Prompt Syntactic Features\\n\\nTo characterize the composition of prompts, we parse phrases from all 1.8M unique prompts. We split each prompt by commas and then extract named entities (NE) and noun phrases (NP) from each separated component using use Spacy (Honnibal et al., 2020). If there is no noun phrase in a comma-separated component, we extract the whole component (C) as a phrase. We keep track of each NP's root to create a hierarchy of noun phrases. For example, for the prompt \\\"draw baby yoda in a loading screen for grand theft auto 5, highly detailed, digital art, concept art,\\\" we extract six phrases: \\\"baby yoda\\\" (NE), \\\"a loading screen\\\" (NP with root \\\"screen\\\"), \\\"grand theft auto 5\\\" (NE), \\\"highly detailed\\\" (C), \\\"digital art\\\" (NP with root \\\"art\\\"), and \\\"concept art\\\" (NP with root \\\"art\\\"). We group...\"}"}
{"id": "acl-2023-long-51", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 5: We identify and group popular phrases in prompts through named entity recognition and dependency parsing. Our interactive circle-packing visualization highlights the distribution and hierarchy of these phrases.\\n\\n(A) The Overview visualizes each phrase as a circle, with its size representing the phrase's frequency. In this example, a viewer clicks a circle to zoom into the \u201cpainting\u201d phrase.\\n\\n(B1) The Detail View shows all noun phrases that use \u201cpainting\u201d as their root. Similarly, it shows all phrases that include \u201coil painting\u201d when the viewer zooms into \u201coil painting.\u201d \u201cdigital art\u201d and \u201cconcept art\u201d into the same hierarchy as they share the same NP root \u201cart.\u201d\\n\\nVisualizing Prompt Phrases. We create an interactive circle packing visualization to gain an understanding of the distribution and relationships between different phrases (Fig. 5). Circle packing (Wang et al., 2006) is a technique to visualize hierarchical data, and each phrase is represented as a circle whose size encodes the phrase's frequency in the dataset. We position sibling noun phrases (e.g., phrases sharing the same NP root) inside their parent phrase's circle through a front-chain packing algorithm (Wang et al., 2006). Viewers can hover over a circle to see the corresponding phrase and its frequency. Viewers can also click a circle (Fig. 5A) to zoom into that sub-tree to see more details about a phrase (Fig. 5-B1) or a sub-phrase (Fig. 5-B2).\\n\\nInsights and implications. Our interactive visualization reveals that key phrases such as \u201chighly detailed,\u201d \u201cintricate,\u201d and \u201cgreg rutkowski\u201d are commonly used in prompts (Fig. 5A). The hierarchical visualization also surfaces popular image styles specified by users, such as \u201cdigital painting,\u201d \u201coil painting,\u201d and \u201cportrait painting\u201d for painting styles (Fig. 5-B1) and \u201cstudio lighting,\u201d \u201cvolumetric lighting,\u201d and \u201catmospheric lighting\u201d for lighting. These phrases can be unfamiliar to Stable Diffusion users, especially beginners, which highlights the importance of helping users develop prompting vocabularies. Researchers can leverage DIFFUSION DB and our visualization to design tutorials and user interfaces that integrate exemplar prompts to guide users in describing their desired images.\\n\\n3.3.2 Prompt Semantic Features\\nIn addition to analyzing the syntactic characteristics of prompts, we also analyze their semantic features. We use a pre-trained CLIP model (Radford et al., 2021) to extract semantic features (Ramesh et al., 2022). We use a frozen CLIP ViT-L/14 text encoder (the same model used in Stable Diffusion) to convert prompts into 768-dimension vectors.\"}"}
{"id": "acl-2023-long-51", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 6: An interactive contour plot of 1.8M prompts' CLIP embeddings, created with UMAP and kernel density estimation. Text labels show the top keywords of prompts in a grid tile. It reveals popular prompt topics. \\n\\nVisualizing Prompt Embeddings. To study the distribution of prompts in high-dimensional space, we use UMAP (McInnes et al., 2020) to project 768-dimensional vectors into 2-D vectors for easy visualization. UMAP is a popular dimensionality reduction technique that is better at preserving the global structure of data and more scalable to large datasets compared to t-SNE (van der Maaten and Hinton, 2008) and PCA (Hotelling, 1936). We use grid search to fine-tune hyperparameters $n_{neighbors}$ (60) and $min\\\\_dist$ (0.1) so that prompts are more spread out in a 2-D space. \\n\\nWe develop an interactive visualization tool to explore prompts' semantic embeddings (Fig. 6). We use Kernel Density Estimation (KDE) (Rosenblatt, 1956) with a standard multivariate Gaussian kernel and Silverman bandwidth (Silverman, 2018) to estimate the distribution of prompts' UMAP representations. Then, we visualize the estimated distribution as a contour plot. To summarize prompts that are in the same region, we create four grids with varying granularity and pre-compute keywords for each grid tile, by treating all prompts in the tile as a document and selecting the top 4 keywords with the highest TF-IDF scores.\\n\\nInteractions. Our visualization shows keywords of tiles that are close to high-density regions and prompt clusters by default. Viewers can hover over a tile to see its keywords, pan and zoom in to see more details of specific regions, and click a button to display each prompt as a small dot that viewers can hover over to read its prompt text.\\n\\n4 Prompt embedding visualization: https://poloclub.github.io/diffusiondb/explorer/#prompt-embedding\\n\\nFig. 7: CLIP embeddings of 2M randomly selected images shown as a contour plot, with text labels being keywords of prompts in the grid tiles. It shows images have a different embedding distribution from prompts.\\n\\nInsights and implications. Our semantic embedding visualization (Fig. 6) highlights two popular prompt categories: art-related prompts (left in the plot) and photography-related prompts (dark blue regions on the right). These two groups appear distant from each other in the UMAP space, suggesting that the prompts for art and photography typically have distinct semantic representations. Interestingly, photography prompts appear to contain two clusters: one for non-human objects (top right) and another for celebrities (bottom right). Small prompt clusters outside the central area often feature artist names. Our findings suggest that future researchers can leverage the prompt usage distribution to fine-tune generative models to tailor to specific popular prompt categories.\\n\\n3.4 Characterizing Images\\n\\nWe visualize the CLIP embedding distribution of 2 million unique image instances randomly sampled from DIFFUSIONDB (Fig. 7) by defining the unique key as the combination of the image's prompt and hyperparameters CFG scale, step, size, and seed. We use the UMAP model that was previously trained on the prompt embeddings to project the image embeddings into the same 2-D space. Finally, we apply the same method we used for our prompt embedding visualization ($\\\\S$ 3.3.2) to generate a contour plot and grid label overlays.\\n\\nInsights and implications. Our image embedding visualization reveals that generated images have a different distribution from their prompts in the CLIP embedding space. For example, the\\n\\n5Image embedding visualization: https://poloclub.github.io/diffusiondb/explorer/#image-embedding\"}"}
{"id": "acl-2023-long-51", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u201cmovie\u201d cluster in the prompt embedding has been replaced by the \u201cportrait\u201d cluster in the image embedding. This suggests the semantic representations of prompts and their generated images may not be perfectly aligned. One hypothesis is that large image generative models face limitations when generating photorealistic human faces (Borji, 2022), and therefore some images generated with movie-related prompts appear to be closer to art and portrait regions in the embedding space.\\n\\n3.5 Stable Diffusion Error Analysis\\n\\nWe leverage DIFFUSIONDB to discover Stable Diffusion generation failure cases and examine potential causes. To surface poor image generations, we compute CLIP embeddings for all prompts and images in DIFFUSIONDB. We then select prompt-image pairs with a large cosine distance (\\\\(d\\\\)) between their embeddings. The cosine distances have a normal distribution (\\\\(N(0.7123, 0.04132)\\\\)).\\n\\nIn this analysis, we focus on 13,411 \u201cbad\u201d prompt-image pairs (1) with a distance that is larger than 4 standard deviations from the mean and (2) the image was not blurred by Stable Diffusion (\u00a7 2.3).\\n\\nImpacts of hyperparameters.\\n\\nWe conduct a logistic regression test to analyze the relationship between Stable Diffusion hyperparameter values (e.g., CFG scale, step, width, and height) and the likelihood of generating an image that is semantically different from its prompt. The results reveal that all four hyperparameters are negatively correlated with the likelihood of generating a bad image. The correlation is statistically significant with a \\\\(p\\\\)-value of less than 0.0001 for all four variables. Furthermore, we find the distribution of selected sampler options when generating bad images is significantly different from the overall distribution (\\\\(X^2 = 40873.11, p < 0.0001\\\\)).\\n\\nCFG scale controls how much the generated image looks like the prompt. We find some users specify negative CFG scales that make images look different from their prompts (large cosine distance \\\\(d\\\\)). In the example shown on the right, a user generates an image using a prompt about \u201csuperman\u201d with all default hyperparameters values, except for setting CFG scale to -1. This results in an image featuring a bowl of soup instead of \u201csuperman\u201d.\\n\\nA small step could also generate underdeveloped images that look different from the specified prompts. As demonstrated in the example on the right, a user generates an image about \u201cplague doctor\u201d with all default hyperparameter values, except for setting step to 2, which leads to a blurry image.\\n\\nStable Diffusion struggles with generating images with a small size or large aspect ratios. The dissimilar image shown on the right is generated with default hyperparameters except for a size of \\\\((64, 512)\\\\).\\n\\nImpacts of prompts.\\n\\nDespite controlling all hyperparameters to be close to default values, we still find 1.1k unique bad image-prompt pairs. Most of these instances have non-English prompts, very short prompts, or prompts consisting primarily emojis (see an example on the right). The token lengths of these instances are significantly lower than the overall token length (one-tailed \\\\(t = -23.7203, p < 0.0001\\\\)). The English prompt frequency among these instances is also significantly lower than the overall frequency (\\\\(X^2 = 1024.56, p < 0.0001\\\\)).\\n\\nInterestingly, we also find that Stable Diffusion sometimes generates unexpected images even when prompts are meaningful English sentences. Future researchers can use our error analysis and failure cases to check potentially mislabeled training data.\\n\\nImplications.\\n\\nOur study reveals Stable Diffusion can make mistakes when generating images with certain hyperparameter values or prompt styles. Negative CFG scales, small steps, or small sizes contributes to generating images dissimilar to prompts. Short and non-English prompts can also lead to errors. To improve the quality of future generative models, researchers can expand the training data to cover these edge cases. There are opportunities for researchers to design user interfaces that can help users understand the impact of different hyperparameters and guide them in choosing values that fit their specific use cases.\\n\\n3.6 Potentially Harmful Uses\\n\\nTo identify potentially malicious uses of Stable Diffusion, we use named entity recognition to analyze prompts. We find that many prompts include names...\"}"}
{"id": "acl-2023-long-51", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of influential politicians, such as over 65k images generated with a prompt including \\\"Donald Trump\\\" and over 48k images with \\\"Joe Biden.\\\" Some prompts portray these politicians in negative lights, ranging from depicting them \\\"as Gollum with hair\\\" to \\\"arrested in handcuffs.\\\" Additionally, we find female celebrities are frequently used in prompts, with a high frequency after artists and influential politicians. Some of these prompts are presented in a sexual context that could be considered nonconsensual pornography.\\n\\nThrough keyword search, we discover prompts generating misinformation that could cause harm. For example, the prompt \\\"scientists putting microchips into a vaccine\\\" may harm public trust in medical institutions by potentially validating conspiracy theories. Similarly, the prompt \\\"Russian soldiers in gas masks found the last surviving ukrainian after a nuclear war to liberate ukraine\\\" depicts false images of the Russo-Ukrainian War and could lead to new forms of propaganda. Our findings highlight the crucial need for further research on the broader impacts of large generative models and ways to regulate and mitigate their harms.\\n\\n4 Enabling New Research Directions\\n\\nThe unprecedented scale and diversity of DIFFUSION DB bring new exciting research opportunities to help users generate images more effectively and efficiently, and enable researchers to improve, explain, and safeguard generative models.\\n\\nPrompt Autocomplete. With DIFFUSION DB, researchers can develop an autocomplete system to help users construct prompts. For example, one can use the prompt corpus to train an n-gram model to predict likely words following a prompt part. Alternatively, researchers can use semantic autocomplete (Hyv\u00f6nen and M\u00e4kel\u00e4, 2006) by categorizing prompt keywords into ontological categories such as subject, style, quality, repetition, and magic terms (Oppenlaender, 2022). This allows the system to suggest related keywords from unspecified categories, for example suggesting style keyword \\\"depth of field\\\" and a magic keyword \\\"award-winning\\\" to improve the quality of generated images. Additionally, researchers can also use DIFFUSION DB to study prompt auto-replace by distilling effective prompt patterns and creating a \\\"translation\\\" model that replaces weaker prompt keywords with more effective ones.\\n\\nGeneration through Search. As DIFFUSION DB contains 14 million images, this dataset might have already included images with a user's desired effects. Thus, a user can quickly search images in DIFFUSION DB instead of running Stable Diffusion, which can be slow and costly. Lexica (Shameem, 2022), an AI start-up, provides such a search engine, where users can search Stable Diffusion images by natural language or images. Researchers can also construct a structured index of images and prompts, such as building a semantivisual image hierarchy of images (Li et al., 2010) or a hierarchical topic model of prompts (Griffiths et al., 2003), to help users easily discover and explore images and prompts with similar styles.\\n\\nImproving Generative Models. With DIFFUSION DB, a large and diverse collection of Stable Diffusion usage logs, researchers not only can identify weak points and failure modes of Stable Diffusion but also gain insights into user preferences. For example, we demonstrate that researchers can use joint text-image embeddings between prompts and images to detect generation misalignments (\u00a7 3.5). Additionally, DIFFUSION DB provides important metadata such as username hash and timestamp for each generated image. By analyzing these metadata fields, researchers can trace the evolution chain of prompts, parameters, and images, which offers valuable insights into how users develop mental models of large generative models and their preferences of generated images. This understanding can inform future researchers to enhance generative models and design interfaces that facilitate better image-generation experiences.\\n\\nExplainable Generation. As generative models have been gaining immense popularity, there is a call for explainable creativity (Llano et al., 2022). Many explanation techniques use input permutation that computes feature attribution scores by running a model on slightly-modified input values (Lundberg and Lee, 2017). DIFFUSION DB contains 14 million prompt-image pairs including similar prompts with minor differences, such as \\\"a happy dog\\\" and \\\"a sad dog,\\\" allowing researchers to investigate how individual keywords affect the generation process.\\n\\nDeepfake Detection. Breakthroughs in generative models raise concerns about deepfakes\u2014fake images of real individuals for unethical purposes (Wiggers, 2022). DIFFUSION DB is valuable...\"}"}
{"id": "acl-2023-long-51", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"able for detecting deepfakes, as it contains a large-scale collection of model-generated images and their metadata. Researchers can use this collection to train ML models to identify synthetic artifacts and train classifiers that classify synthetic images from real images (Mirsky and Lee, 2022).\\n\\n5 Related Work\\n\\nText-to-text Prompting. Researchers have been studying prompt engineering for text-to-text generation (e.g., Liu et al., 2022; Lu et al., 2022; Rubin et al., 2022). To facilitate this line of research, researchers develop PromptSource (Bach et al., 2022), a dataset of 2k text prompts along with a framework to create and share prompts. In contrast, our work focuses on text-to-image prompting, and DIFFUSION DB has an unprecedented scale of 14 million real prompt-image pairs.\\n\\nText-to-image Prompting. There is a growing interest in text-to-image prompt engineering research from NLP, Computer Vision, and HCI communities (e.g., Qiao et al., 2022; Pavlichenko and Ustalov, 2022). For example, Oppenlander (2022) identifies six types of prompt modifiers through an ethnographic study, and Liu and Chilton (2022) propose design guidelines for text-to-image prompt engineering by experimenting with 1,296 prompts. Closest in spirit to DDIFFUSION DB is Lexica (Shameem, 2022) which allows users to search over 5 million Stable Diffusion images with their prompts, but it does not release its internal database. In comparison, DDIFFUSION DB is open-source and publicly available to everyone.\\n\\n6 Conclusion\\n\\nWe present DDIFFUSION DB, the first large-scale text-to-image prompt dataset, containing 14 million images with their prompts and hyperparameters collected from the Stable Diffusion discord server. We release the dataset with a CC0 1.0 license and open source all collection and analysis code, broadening the public's access to cutting-edge AI technologies. We discuss findings on prompt and image patterns. We hope our work will serve as a cornerstone for the future development of large generative models and tools that help users use these modes.\\n\\n7 Limitations\\n\\nWe discuss four limitations of our work: the inclusion of unsafe content, potential biases in data sources, a limited measure of image quality and generalizability to different generative models.\\n\\n\u2022 Inclusion of unsafe images and prompts. We collect images and their prompts from the Stable Diffusion Discord server (\u00a7 2). The Discord server has rules against users generating or sharing harmful or NSFW (not suitable for work, such as sexual and violent content) images. The Stable Diffusion model used in the server also has an NSFW filter that blurs the generated images if it detects NSFW content. However, we observe that DDIFFUSION DB includes some NSFW images that were not detected by the NSFW filter or removed by the server moderators. To mitigate the potential harm, we compute and share the likelihood of an image or a prompt containing unsafe content using the state-of-the-art NSFW detectors (\u00a7 2.3). In addition, we provide a Google Form on the DDIFFUSION DB website where users can report harmful or inappropriate images and prompts. We will closely monitor this form and remove reported images and prompts from DDIFFUSION DB.\\n\\n\u2022 Potential biases of the data source. The 14 million images in DDIFFUSION DB have diverse styles and categories. However, Discord can be a biased data source. Our images come from channels where early users could use a bot to use Stable Diffusion before release. As these users had started using Stable Diffusion before the model was public, we hypothesize that they are AI art enthusiasts and are likely to have experience with other text-to-image generative models. Therefore, the prompting style in DDIFFUSION DB might not represent novice users. Similarly, the prompts in DDIFFUSION DB might not generalize to domains that require specific knowledge, such as medical images (Chambon et al., 2022).\\n\\n\u2022 Limited measure of image quality. We use joint text-image CLIP embeddings between prompts and images to detect generation misalignments (\u00a7 3.5). While the CLIP embedding distance can indicate the degree of alignment between the prompts and generated images, it does not provide a measure of the overall image quality. When constructing our dataset, we have considered including image properties such as entropy, variance, and the most common colors to help users gauge image qualities. However, these metrics do not provide a good measure of the overall image quality as well. To better measure...\"}"}
{"id": "acl-2023-long-51", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sure image quality, future researchers can recruit annotators to rate images in DIFFUSION DB.\\n\\n\u2022 Generalizability. Previous research has shown a prompt that works well on one generative model might not give the optimal result when used in other models (Borji, 2022). Therefore, different models can need users to write different prompts. For example, many Stable Diffusion prompts use commas to separate keywords, while this pattern is less seen in prompts for DALL-E 2 (Ramesh et al., 2022) or Midjourney (Holz, 2022). Thus, we caution researchers that some research findings from DIFFUSION DB might not be generalizable to other text-to-image generative models.\\n\\n8 Ethics Statement\\nIn this section, we discuss two main ethical considerations of DIFFUSION DB.\\n\\n\u2022 Copyright. By using the Stable Diffusion Discord server, all users agree to the entirety of CC0 1.0 Universal Public Domain Dedication. This includes waiving any intellectual property rights related to any content shared on the server (StabilityAI, 2022b). All prompts and images in the Discord server are considered to be public domain and can be used by anyone for any purpose. Also, we release DIFFUSION DB under the CC0 1.0 license (\u00a7 2.5).\\n\\n\u2022 Privacy. While it is possible that some prompts may contain sensitive information, this is not common because the Stable Diffusion Discord has strict rules against writing personal information in the prompts and has moderators in place to remove violative messages. To further protect user privacy, we have anonymized the usernames of all users in our dataset (\u00a7 2.4). Users also have the option to remove their prompts and images from our dataset through an online form (\u00a7 2.5).\\n\\nWe provide a thorough discussion on the limitations and broader impacts of DIFFUSION DB in its Data Sheet (Gebru et al., 2020) (\u2021 A).\\n\\nAcknowledgements\\nWe thank Stability AI for releasing Stable Diffusion and hosting the Stable Diffusion Discord server. We especially appreciate the Stable Diffusion Discord moderators and users for creating an open and friendly online community that makes our work possible. We also extend our appreciation to Hugging Face for hosting our dataset. Lastly, we would like to acknowledge the anonymous reviewers for their valuable feedback and insightful comments that helped improve our paper. This work was supported in part by a J.P. Morgan PhD Fellowship, NSF grants IIS-1563816, DARPA GARD, gifts from Cisco, Bosch, and NVIDIA. Use, duplication, or disclosure is subject to the restrictions as stated in Agreement number HR00112030001 between the Government and the Performer.\\n\\nReferences\\nApache. 2013. Apache Parquet: Open Source, Column-oriented Data File Format Designed for Efficient Data Storage and Retrieval.\\n\\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Alshaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. 2022. Prompt-Source: An Integrated Development Environment and Repository for Natural Language Prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations.\\n\\nAli Borji. 2022. Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2. arXiv 2210.00586.\\n\\nGwern Branwen. 2020. GPT-3 Creative Fiction.\\n\\nPierre Chambon, Christian Bluethgen, Curtis P. Lagnlotz, and Akshay Chaudhari. 2022. Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains. arXiv 2210.04133.\\n\\nAlex Clark. 2015. Pillow: Python Imaging Library (Fork).\\n\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. 2020. Datasheets for Datasets. arXiv:1803.09010 [cs].\\n\\nGoogle. 2010. Comparative Study of WebP, JPEG and JPEG 2000.\\n\\nThomas Griffiths, Michael Jordan, Joshua Tenenbaum, and David Blei. 2003. Hierarchical topic models and the nested chinese restaurant process. In Advances in Neural Information Processing Systems, volume 16.\\n\\nLaura Hanu and Unitary team. 2020. Detoxify: Toxic Comment Classification with Pytorch Lightning and Transformers.\"}"}
{"id": "acl-2023-long-51", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-51", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning To Retrieve Prompts for In-Context Learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Maddavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. arXiv 2205.11487.\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. LAION-5B: An open large-scale dataset for training next generation image-text models. arXiv 2210.08402.\\n\\nSharif Shameem. 2022. Lexica: Building a Creative Tool for the Future.\\n\\nBernard W Silverman. 2018. Density Estimation for Statistics and Data Analysis.\\n\\nStabilityAI. 2022a. Stable Diffusion Discord Server Rules.\\n\\nStabilityAI. 2022b. Stable Diffusion Dream Studio beta Terms of Service.\\n\\nLaurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research, 9.\\n\\nWeixin Wang, Hui Wang, Guozhong Dai, and Hongan Wang. 2006. Visualization of large hierarchical data by circle packing. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.\\n\\nKyle Wiggers. 2022. Deepfakes for all: Uncensored AI art model prompts ethics questions.\\n\\nSimon Willison, Adam Stacoviak, and Jerod Stacoviak. 2022. Stable Diffusion Breaks the Internet.\"}"}
{"id": "acl-2023-long-51", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The **DIFFUSION DB** project was inspired by important needs in research focused on diffusion models and prompt engineering. As large text-to-image models are relatively new, there is a pressing need to understand how these models work, how to write effective prompts, and how to design tools to help users generate images. To tackle these critical challenges, we present **DIFFUSION DB**, the first large-scale prompt dataset with 14 million real prompt-image pairs.\\n\\n**Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?**\\n\\nThe dataset was created by Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau at the Georgia Institute of Technology.\\n\\n**Who funded the creation of the dataset?** If there is an associated grant, please provide the name of the grantor and the grant name and number.\\n\\nFunded in part by J.P. Morgan PhD Fellowship, NSF grants IIS-1563816, DARPA GARD, and gifts from Cisco, Bosch, and NVIDIA.\\n\\n**Any other comments?**\\n\\nNone.\\n\\n**Composition**\\n\\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)?\\n\\nPlease provide a description.\\n\\nEach instance consists of an image generated by the Stable Diffusion model and the prompt as well as parameters that were input into the model to generate the image. The input parameters include seed, CFG scale, sampler, width, height, username hash, timestamp, image NSFW score and prompt NSFW score.\\n\\nHow many instances are there in total (of each type, if appropriate)?\\n\\nThere are 14 million instances in total.\\n\\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).\\n\\nThe dataset is a sample of instances. It represents a sample of images from the Stable Diffusion discord server. No tests were run to determine representativeness.\\n\\nWhat data does each instance consist of? \\\"Raw\\\" data (e.g., unprocessed text or images) or features? In either case, please provide a description.\\n\\nEach instance consists of the image generated by the Stable Diffusion model (with a unique id), along with the prompt used to generate the image and the model parameters as a JSON file.\\n\\nIs there a label or target associated with each instance?\\n\\nIf so, please provide a description.\\n\\nThe labels associated with each image are the prompt and other input parameters.\\n\\nIs any information missing from individual instances?\\n\\nIf so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.\\n\\nEverything is included. No data is missing.\\n\\nAre relationships between individual instances made explicit (e.g., users' movie ratings, social network links)?\\n\\nIf so, please describe how these relationships are made explicit.\\n\\nNot applicable.\\n\\nAre there recommended data splits (e.g., training, development/validation, testing)?\"}"}
{"id": "acl-2023-long-51", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"so, please provide a description of these splits, explaining the rationale behind them.\\n\\nNo. This dataset is not for ML benchmarking. Researchers can use any subsets of it.\\n\\nAre there any errors, sources of noise, or redundancies in the dataset?\\n\\nIf so, please provide a description.\\n\\nNo. All images and prompts are extracted as is from the Discord chat log.\\n\\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\\n\\nThe dataset is entirely self-contained.\\n\\nDoes the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals\u2019 nonpublic communications)?\\n\\nIf so, please provide a description. Unknown to the authors of the datasheet.\\n\\nIt is possible that some prompts contain sensitive information. However, it would be rare, as the Stable Diffusion Discord has rules against writing personal information in the prompts, and there are moderators removing messages that violate the Discord rules.\\n\\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\\n\\nIf so, please describe why.\\n\\nWe collect images and their prompts from the Stable Diffusion discord server. Even though the discord server has rules against users sharing any NSFW (not suitable for work, such as sexual and violent content) and illegal images, DIFFUSIONDB still contains some NSFW images and prompts that were not removed by the server moderators.\\n\\nDoes the dataset identify any subpopulations (e.g., by age, gender)?\\n\\nIf so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.\\n\\nNo.\\n\\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?\\n\\nIf so, please describe how.\\n\\nNo.\\n\\nAny other comments?\\n\\nNone.\\n\\nHow was the data associated with each instance acquired?\\n\\nWas the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.\\n\\nThe data was directly observed from the Stable Diffusion Discord Channel. It was gathered from channels where users can generate images by interacting with a bot, which consisted of messages of user generated images and the prompts used to generate those images.\\n\\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated?\\n\\nThe data was gathered using a DiscordChatExporter (Holub, 2017), which collected images and chat messages from each channel specified. We then extracted and linked prompts to images using Beautiful Soup (Richardson, 2007). Random images and prompts were selected and manually verified to validate the prompt-image mapping.\\n\\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\\n\\nDIFFUSIONDB does not sample from a larger set. However, DIFFUSIONDB-2M is a sample from a larger set. For certain messages, there would exist a collage of \\\\( n \\\\) images (e.g., \\\\( n = 2, 4, 9 \\\\)) with identical prompts consolidated into a single image. These images were split and a single image would be randomly selected to include in DIFFUSIONDB-2M from \\\\( n \\\\) images with equal probability of any image being selected. This saved space and prioritized unique prompts.\"}"}
{"id": "acl-2023-long-51", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\\n\\nStudents conducted the data collection process and were compensated with stipend or course credits.\\n\\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.\\n\\nAll messages were generated in August 2022 and messages were collected between October 18th and 24th 2022. DIFFUSION DB includes the generation timestamps of all images.\\n\\nWere any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.\\n\\nThere were no ethical review processes conducted.\\n\\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\\n\\nThe data was directly obtained from individual messages in the Discord server.\\n\\nWere the individuals in question notified about the data collection?\\n\\nIf so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.\\n\\nUsers of the channel were not notified about this specific gathering of data but agree to forfeit any intellectual property rights claims by using Stable Diffusion. In addition, users are instructed that the images are public domain and can be used by anyone for any purpose. The exact language is as follows (StabilityAI, 2022b):\\n\\n\u201cNote, that while users have forfeited copyright (and any/all intellectual property right claims) on these images, they are still public domain and can be used by anyone for any purpose, including by the user. Feel free to use images from DreamStudio Beta and the Stable Diffusion beta Discord service for anything, including commercial purposes.\u201d\\n\\nDid the individuals in question consent to the collection and use of their data?\\n\\nIf so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.\\n\\nBy using the server and tools, users consented to the regulations posed by Stability AI LTD, the company that both made Stable Diffusion and runs the Discord server. This implies consent by using the tool. The exact wording is as follows:\\n\\n\u201cBy your use of DreamStudio Beta and the Stable Diffusion, you hereby agree to forfeit all intellectual property rights claims, worldwide, and regardless of legal jurisdiction or intellectual property law applicable therein, including forfeiture of any/all copyright claim(s), to the Content you provide or receive through your use of DreamStudio Beta and the Stable Diffusion beta Discord service.\u201d\\n\\nThis message is contained in the rules and terms of service section of the Stable Diffusion Discord (StabilityAI, 2022a,b). In conjunction with the previous statement about images being public domain (CC0 1.0 license), it is established that the images made by using Stable Diffusion can be used for other purposes.\\n\\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?\\n\\nIf so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).\\n\\nUsers will have the option to report harmful content or withdraw images they created through a Google Form listed on the DIFFUSION DB website: https://github.com/poloclub/diffusiondb.\\n\\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?\\n\\nIf so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.\"}"}
{"id": "acl-2023-long-51", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"No analysis has been conducted.\\n\\nAny other comments?\\nNone.\\n\\nPreprocessing\\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section.\\n\\nThe Discord chat logs include collage images, where each collage contains a grid of images that share the same prompt but have different seeds. We use Pillow (Clark, 2015) to split a collage into individual images. For DIFFUSION, we include all split images. However, for DIFFUSION-2M, we only include one randomly selected split image to save space and prioritize unique prompts.\\n\\nWas the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the \u201craw\u201d data.\\n\\nRaw data was not saved.\\n\\nIs the software that was used to preprocessed/cleaned/labeled the data available? If so, please provide a link or other access point.\\n\\nAll our data collection and preprocessing code is available at: https://github.com/poloclub/diffusiondb.\\n\\nAny other comments?\\nNone.\\n\\nUses\\nHas the dataset been used for any tasks already? If so, please provide a description.\\n\\nNo.\\n\\nIs there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\\n\\nNo.\\n\\nWhat (other) tasks could the dataset be used for?\\nThis dataset can be used for (1) prompt autocomplete, (2) generating images through search, (3) detecting deepfake, (4) debugging image generation, (5) explaining image generation, and more.\\n\\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description.\\n\\nThere is minimal risk for harm: the data were already public. Personally identifiable data (e.g., discord usernames) were removed during the collection/preprocessing phases.\\n\\nAre there tasks for which the dataset should not be used? If so, please provide a description.\\n\\nAll tasks that utilize this dataset should follow the licensing policies and the regulations (StabilityAI, 2022b) posed by Stability AI, the company that both made Stable Diffusion and runs the official Discord server.\\n\\nAny other comments?\\nNone.\\n\\nDistribution\\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description.\\n\\nYes, the dataset is publicly available on the internet.\\n\\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)?\\n\\nThe dataset is distributed on the project website: https://poloclub.github.io/diffusiondb.\\n\\nThe dataset shares the same DOI as this paper.\\n\\nWhen will the dataset be distributed?\\nThe dataset is released on October 25th, 2022.\"}"}
