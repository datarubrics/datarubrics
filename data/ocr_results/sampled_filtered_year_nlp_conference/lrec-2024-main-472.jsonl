{"id": "lrec-2024-main-472", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do Neural Language Models Compose Concepts the Way Humans Can?\\n\\nAmilleah Rodriguez 1,2, Shaonan Wang 1,2, Liina Pylkkanen 1,2\\n\\n1 Department of Linguistics, New York University\\n2 Department of Psychology, New York University\\n{amilleah.rodriguez, shaonan.wang, liina.pylkkanen}@nyu.edu\\n\\nAbstract\\n\\nWhile compositional interpretation is the core of language understanding, humans also derive meaning via inference. For example, while the phrase \u201cthe blue hat\u201d introduces a blue hat into the discourse via the direct composition of \u201cblue\u201d and \u201chat,\u201d the same discourse entity is introduced by the phrase \u201cthe blue color of this hat\u201d despite the absence of any local composition between \u201cblue\u201d and \u201chat.\u201d Instead, we infer that if the color is blue and it belongs to the hat, the hat must be blue. We tested the performance of neural language models and humans on such inferentially driven conceptual compositions, eliciting probability estimates for a noun in a syntactically composing phrase, \u201cThis blue hat\u201d, following contexts that had introduced the conceptual combinations of those nouns and adjectives either syntactically or inferentially. Surprisingly, our findings reveal significant disparities between the performance of neural language models and human judgments. Among the eight models evaluated, RoBERTa, BERT-large, and GPT-2 exhibited the closest resemblance to human responses, while other models faced challenges in accurately identifying compositions in the provided contexts. Our study reveals that language models and humans may rely on different approaches to represent and compose lexical items across sentence structure.\\n\\nAll data and code are accessible at https://github.com/wangshaonan/BlueHat.\\n\\nKeywords: Neural Language Models, Composition, Inference, Dataset Construction\\n\\n1. Introduction\\n\\nLanguage comprehension involves combining word meanings according to the structure of a sentence and yet, it also encompasses a wide range of inferential processing. Neuroscience research has revealed a uniform brain basis for conceptual combinations that align with syntax and those that result through inference (Parrish et al., 2022). Building on this finding, we tested human and language model performance on inferentially vs. syntactically arising conceptual combinations. Can neural language models, proficient in diverse language tasks (Han et al., 2021) and mirroring patterns of human brain activity (Sun et al., 2019; Wang et al., 2020; Caucheteux and King, 2022) create combined concepts even when the combinations are not obvious from the syntax? To investigate this phenomenon in neural language models, we introduce a novel dataset introducing pairs of sentences matched in syntactic structure but which vary in whether they introduce a conceptual combination of an adjective and noun or not.\\n\\n1. The blue color of this hat is lovely. This blue hat..\\n2. The blue lamp near this hat is lovely. This blue hat..\\n\\nDespite maintaining the same syntactic distance between \u201cblue\u201d and \u201chat\u201d in both context sentences, humans only obtain a \u201cblue hat\u201d interpretation in the first case. Therefore, only in the first case is a subsequent reference to a blue hat natural. This study tests whether the same information is available for language models: can they represent combined concepts even when the two elements do not syntactically merge and the combination arises via inference?\\n\\nPrevious research has explored neural language models\u2019 syntactic and semantic capabilities, investigating elements like filler-gap relationships (Wilcox et al., 2018; Linzen and Baroni, 2021), subject-verb agreements (Linzen et al., 2016; Jawahar et al., 2019), anaphor binding (Hu et al., 2020), non-syntactic factors like politeness effects (Lee and Wang, 2023), semantic prowess like quality of phrase representations (Yu and Ettinger, 2020; Garcia et al., 2021), and the capacity to recognize semantic roles and possess event knowledge (Ettinger, 2020; Pavlick, 2022; Kauf et al., 2022). The current study adds to this body of work by asking whether humans and language models diverge in their ability to combine concepts in the absence of a local syntactic relationship.\\n\\n2. Methodology\\n\\nIn our study, we provided a context sentence using color and object descriptors, for example, \u201cThe blue color of this hat is lovely.\u201d This was followed by a probe expression like \u201cThis blue hat...\u201d When the initial sentence combined the concepts of color and object as in the probe, the term \u2018hat\u2019 is a natural...\"}"}
{"id": "lrec-2024-main-472", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ral and predictable continuation after 'blue' in the probe sentence. In contrast, after a context sentence such as \\\"The blue lamp near this hat is lovely,\\\" which does not introduce the combined concept of the probe expression, the predictability of the noun \\\"hat\\\" is effectively zero after blue. Thus, human participants who effortlessly discern the long-distance semantic relationship between 'blue' and 'hat' in the context sentence are expected to assign a higher score to the naturalness of 'hat' as the next word after 'blue' in the probe when the context sentence introduced the combined concept of a blue hat via inference. Similarly, a language model that can effectively capture the long-distance semantic connections in our context sentence will assign probabilities based on whether or not the context introduces the combined concept of a blue hat.\\n\\nAltogether, our context sentences introduced eight distinct compositional contexts for the probe expression, as shown in Section 3.1. Human judgments were log-transformed naturalness ratings of the noun in the probe expression. To assess whether the language model inferred a 'blue hat' from each preceding context, we evaluate surprisal (Hale, 2016) as the log-transform of output probabilities at the word 'hat' in our probe expression. Additionally, since our context manipulation also may affect how predictable the determiner 'This' is at the beginning of the probe, we subtracted the probability of 'This' from the probability of 'hat' in our analysis. This adjustment helps us account for how each context influences the interpretation of the probe phrase. We then contrasted surprisal across minimal pairs of combinatory and non-combinatory contexts within matched target items and syntactic distance. We used a binary measure to categorize language model performance across these minimal pairs, correctly determining 'hat' in the probe as less surprising when the context introduced a blue hat, than when the context did not. We used this binary measure to calculate percent accuracy across each condition of combinatory and non-combinatory pairs as shown in Table 1.\\n\\nTo further investigate how language models represented and used these semantic relationships over linear and syntactic distances, we also examined the information retention of the target adjective on the noun (or noun on the adjective, depending on word order) within our context sentences. We computed the cosinesimilaritybetween the adjective and noun at each word position within the context sentence, with the assumption that features relevant for composition would be maintained across syntactic distance. The results for our best performing model, GPT-2, are displayed in Figure 2., with the rest of the language models results in Figure 3.\\n\\n3. Experimental Setting and Dataset\\n3.1. BlueHat dataset\\nWe followed established psycholinguistic methods to minimize the effects of word frequency and sentence structure variations using a controlled vocabulary of five color adjectives and five nouns sorted into sentence templates. We included sets of prepositions, verbs, adverbs, intensifiers, and color descriptors to avoid the repetitive use of a small vocabulary. The frequencies of these elements were balanced within each template item. This method produced 100 sentence pairs across eight distinct conditions (800 context sentences and corresponding probes).\\n\\n**1. Local(ADJ)**\\nThe color of the blue hats is lovely.\\n(Combinatory)\\nThe lamps are blue, hats red, and socks gray.\\n(Non-combinatory)\\n\\n**2. Local(NOUN)**\\nYou'll see that the hat is blue in color.\\n(Combinatory)\\nYou will see a hat, a blue lamp and socks.\\n(Non-combinatory)\\n\\n**3. Non-local(ADJ)**\\nThe blue color of this hat is lovely.\\n(Combinatory)\\nThe blue lamp near this hat is lovely.\\n(Non-combinatory)\\n\\n**4. Non-local(NOUN)**\\nThe hat is surely a lovely blue color.\\n(Combinatory)\\nThe hat is near a lovely blue lamp.\\n(Non-combinatory)\\n\\nIn these examples, Non-local refers to the inferential, long-distance context the target adjective and noun appear in, while Local indicates local contexts of syntactic composition. We use (ADJ) or (NOUN) to indicate which of the target words, the adjective or noun, appears first within the sentence. Combination, either combinatory or non-combinatory refers to whether the target adjective and noun can be composed within the sentence.\\n\\n3.2. Human behavioral experiment\\nWe conducted a behavioral experiment involving 40 participants recruited from Prolific (Palan and Schitter, 2018). These participants were tasked with evaluating the naturalness of 120 sentences, comprising 15 pairs of context sentences and corresponding probes randomly selected from a pool of 100 sentence pairs. Participants utilized a 5-point scale, ranging from 1 (Very Unnatural) to 5 (Very Natural).\"}"}
{"id": "lrec-2024-main-472", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Human and Language Model Accuracy across Word Order and Distance.\\n\\n|                  | Overall | Local (ADJ) | Local (NOUN) | Non-local (ADJ) | Non-local (NOUN) |\\n|------------------|---------|-------------|--------------|-----------------|------------------|\\n| Humans           |         | 94.3        | 95.4         | 97.1            | 89.8             |\\n| GPT              |         | 48.25       | 66.0         | 44.0            | 41.3             |\\n| GPT-2            |         | 71.75       | 96.0         | 100.0           | 86.0             |\\n| GPT-2-large      |         | 22.0        | 72.0         | 1.67            | 16.0             |\\n| BERT             |         | 43.75       | 20.0         | 16.0            | 68.7             |\\n| BERT-large       |         | 58.5        | 68.0         | 60.0            | 44.0             |\\n| DistilBERT       |         | 34.75       | 62.0         | 30.0            | 20.7             |\\n| RoBERTa          |         | 58.0        | 100.0        | 94.0            | 19.3             |\\n| XLNet            |         | 26.75       | 10.0         | 2.0             | 36.0             |\\n\\n3.3. Language models\\n\\nThe present study aims to identify strategies neural language models use to represent and compose meaning in adjective-noun phrases. We tested three distinct model groups: 1) BERT Family: We utilized BERT, BERT-large, distillBERT, and RoBERTa that employ bidirectional learning. Notably, RoBERTa's singular objective function sets it apart from other BERT variants. 2) GPT Family: This group includes autoregressive training models like GPT, GPT-2, and GPT-2-large, valuable for sequential text generation tasks requiring creativity. 3) XLNet: Positioned at the crossroads of BERT and GPT families, XLNet combines bidirectional architecture and autoregressive training methods.\\n\\nGPT-2 and RoBERTa exhibit surprisal that most closely resembles human naturalness ratings collected in our experiment. Their performance suggests they may hold more flexible representations to extend the meaning of a sentence to a novel combination. The other models we evaluated scored below chance accuracy comparing combinatory and noncombinatory sentence minimal pairs.\\n\\n4. Results\\n\\nTable 1 reveals a striking disparity: all language models underperform in comparison to human judgments.\"}"}
{"id": "lrec-2024-main-472", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Humans consistently identify the probe in non-local (NOUN) conditions. Surprisal results for combinatory (blue) and non-combinatory (red) contexts, indicating difficulty in extracting the concept of a blue hat from the phrase \\\"blue color combination.\\\" However, the models' behavior towards this hat is lovely, making it unexpected to directly mention a 'blue hat.'\\n\\nAmong the language models, GPT-2 exhibited the highest overall accuracy at 72%. RoBERTa had the lowest accuracy of only 19.3% across the Non-local (NOUN) condition. However, it struggled with identifying the probe's minimal construction when the items combined. BERT, GPT, and Elmo performed similarly, though the other models revealed that BERT, GPT, and Elmo had difficulty in maintaining long-range dependencies across context sentences. GPT-2-large failed to correctly assign higher probabilities to our combinatory, Local control sentences.\\n\\nThe participants' ability to reliably identify when sentences were forming the intended adjective-noun structure occurred in linear order. This disparity highlights a key difference between human understanding and the key difference between human understanding and the models' tendency towards non-combinatory word positioning.\\n\\nInformation retention across context sentences and target words was measured using cosine similarity at each hidden layer (right) for combinatory (blue) and non-combinatory (red) contexts. For GPT-2, an autoregressive model, this difference in performance was notable, indicating that they were unable to detect when the target noun and target adjective in the context sentence. We measured the cosine similarity between information retained at each point in the context sentence indicating that they were unable to detect when the target noun and target adjective were maintained or accessed at later points in the context sentence. We measured the cosine similarity between information retained at each point in the context sentence indicating that they were unable to detect when the target noun and target adjective were maintained or accessed at later points in the context sentence.\"}"}
{"id": "lrec-2024-main-472", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Language model results. Cosine similarity of target words across sentence for combinatory structures (denoted by distances 1 to 5) and across conditions, the combinatory context generally reduces accuracy. This analysis explains the decrease in accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between composing and non-composing sentences, indicating how the model distinguished between the word \\\"hat\\\" from the context across different sentence conditions. The combinatory context generally reduces accuracy for the representation of the composing items, considering how the model maintained and used compositional capabilities. Distances higher layers suggest that the network's deeper layers might be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. Wetested. GPT-2's correct assignment of higher cosine similarity across sentence structure, but higher layers suggest that thenetwork's deeper layers may be helpful in further investigations of the deep linguistic capabilities exhibited by neural language models. For GPT-2, we were also interested in understanding how the model progresses from maintaining and using compositional items, even if the model has difficulty evaluating the probe. This shows an understanding of the context sentence, which correctly has a higher cosine similarity between"}
{"id": "lrec-2024-main-472", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\nCharlotte Caucheteux and Jean-R\u00e9mi King. 2022. Brains and algorithms partially converge in natural language processing. *Communications biology*, 5(1):134.\\n\\nAllyson Ettinger. 2020. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models. *Transactions of the Association for Computational Linguistics*, 8:34\u201348.\\n\\nMarcos Garcia, Tiago Kramer Vieira, Carolina Scarlton, Marco Idiart, and Aline Villavicencio. 2021. Assessing the representations of idiomaticity in vector models with a noun compound dataset labeled at type and token levels. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2730\u20132741.\\n\\nJohn Hale. 2016. Information-theoretical complexity metrics. *Language and Linguistics Compass*, 10(9):397\u2013412.\\n\\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, et al. 2021. Pre-trained models: Past, present and future. *AI Open*, 2:225\u2013250.\\n\\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, and Roger Levy. 2020. A systematic assessment of syntactic generalization in neural language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1725\u20131744.\\n\\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah. 2019. What does bert learn about the structure of language? In *ACL 2019-57th Annual Meeting of the Association for Computational Linguistics*.\\n\\nCarina Kauf, Anna A Ivanova, Giulia Rambelli, Emmanuele Chersoni, Jingyuan S She, Zawad Chowdhury, Evelina Fedorenko, and Alessandro Lenci. 2022. Event knowledge in large language models: the gap between the impossible and the unlikely. *arXiv preprint arXiv:2212.01488*.\\n\\nSoo-Hwan Lee and Shaonan Wang. 2023. Do language models know how to be polite? *Proceedings of the Society for Computation in Linguistics*, 6(1):375\u2013378.\\n\\nTal Linzen and Marco Baroni. 2021. Syntactic structure from deep learning. *Annual Review of Linguistics*, 7:195\u2013212.\\n\\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of lstms to learn syntax-sensitive dependencies. *Transactions of the Association for Computational Linguistics*, 4:521\u2013535.\\n\\nStefan Palan and Christian Schitter. 2018. Proflific. ac\u2014a subject pool for online experiments. *Journal of Behavioral and Experimental Finance*, 17:22\u201327.\\n\\nAlicia Parrish, Amilleah Rodriguez, and Liina Pylkk\u00e4nen. 2022. Non-local conceptual combination. *bioRxiv*.\\n\\nEllie Pavlick. 2022. Semantic structure in deep learning. *Annual Review of Linguistics*, 8:447\u2013471.\\n\\nJingyuan Sun, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. 2019. Towards sentence-level brain decoding with distributed representations. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 33, pages 7047\u20137054.\\n\\nShaonan Wang, Jiajun Zhang, Haiyan Wang, Nan Lin, and Chengqing Zong. 2020. Fine-grained neural decoding with distributed word representations. *Information Sciences*, 507:256\u2013272.\\n\\nEthan Wilcox, Roger Levy, Takashi Morita, and Richard Futrell. 2018. What do rnn language models learn about filler\u2013gap dependencies? In *Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 211\u2013221.\\n\\nLang Yu and Allyson Ettinger. 2020. Assessing phrasal representation and composition in transformers. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 4896\u20134907.\"}"}
