{"id": "emnlp-2024-main-343", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nLarge language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work has elicited confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present SaySelf, a novel training framework that teaches LLMs to express more fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. The generated self-reflective rationales are also reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.\\n\\n1 Introduction\\n\\nWhile large language models (LLMs) exhibit remarkable proficiency in reasoning and generating effective responses (OpenAI, 2023; Touvron et al., 2023; Jiang et al., 2023; Wang et al., 2024), they often produce fabricated information (a.k.a, hallucination) and typically hesitate to indicate their uncertainty when faced with unfamiliar questions (Ye et al., 2023; Liu et al., 2023). Determining how to accurately obtain reliable confidence estimates from LLMs is essential (Xiong et al., 2023; Zhou et al., 2023), particularly when the responses are not limited to single tokens.\\n\\nPrevious work on eliciting confidence from LLMs includes prompting-based and training-based approaches. Prompting-based methods, such as direct prompting and self-consistency prompting in Figure 1, employ specific prompts to generate confidence scores or use answer consistency as a confidence indicator, even though these can have poor calibration performance or significantly increase inference latency (Tian et al., 2023; Xiong et al., 2023; Diao et al., 2023). Training-based approaches, such as group-based calibration training and R-Tuning in Figure 1, develop specialized datasets for fine-tuning that encourage LLMs to express confidence. However, these methods often provide suboptimal or binary confidence estimates, failing to accurately reflect the models' confidence levels (Lin et al., 2022; Zhang et al., 2023a; Yang et al., 2023). In conclusion, previous work tends to suffer from the following problems: (1) Poor calibration performance; (2) Coarse-grained confidence levels; (3) Long inference latencies.\\n\\nIn this work, we present SaySelf, a training framework that teaches LLMs to generate more accurate and fine-grained confidence estimates. It successfully tackles the aforementioned problems in previous work. More importantly, SaySelf also goes beyond the confidence elicitation in previous work, and further enables LLMs to\"}"}
{"id": "emnlp-2024-main-343", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Direct Prompting / Group-based Calibration Training\\nSelf-Consistency\\nPrompting\\nPrevious Work\\n\\nWhat is the name of the younger son of the current President of the United States?\\n\\nRobert Hunter Biden. My overall confidence is 3.\\n\\nRobert Hunter Biden.\\nAccording to my knowledge, there is a slight possibility that the current President is Trump.\\nMy overall confidence is 8.\\n\\nSaySelf\\n\\nGround Truth\\n\\nRobert Hunter Biden.\\nRobert Hunter Biden.\\nRobert Hunter Biden.\\nRobert Hunter Biden.\\n\\nR-Tuning\\n\\nRobert Hunter Biden.\\nRobert Hunter Biden.\\nRobert Hunter Biden.\\nRobert Hunter Biden.\\n\\nSampling\\n\\nRobert Hunter Biden.\\nRobert Hunter Biden.\\nRobert Hunter Biden.\\nRobert Hunter Biden.\\n\\nFigure 1: The comparison between SaySelf and previous work.\\n\\nSaySelf can produce the self-reflective rationale that explains why the model is uncertain and the fine-grained and accurate confidence estimates. This simple example is constructed for illustration purposes, and the reasoning chain is omitted for brevity.\\n\\ngenerate self-reflective rationales that indicate their knowledge gap and explain their confidence estimates (Figure 1). We accomplish this by automatically generating a model-specific dataset for supervised fine-tuning using an off-the-shelf LLM (e.g., GPT-4 (OpenAI, 2023)). Specifically, for each question, we sample multiple reasoning chains from LLMs. We then perform clustering of the reasoning chains based on the semantic similarity and retain one instance per cluster. GPT-4 is then tasked with analyzing these instances from various clusters, summarizing uncertainties in natural language from a first-person perspective, which is subsequently used for fine-tuning.\\n\\nFor accurate and fine-grained confidence estimates, we employ reinforcement learning to calibrate LLMs' confidence estimate in each response. We design a reward function that incentivizes LLMs to produce accurate, high-confidence predictions and imposes penalties for overconfidence in incorrect responses. In addition, the self-reflective rationales and confidence estimates are generated without multiple sampling, significantly reducing the inference time.\\n\\nWe evaluate SaySelf on multiple knowledge-extensive question-answering tasks. We show that SaySelf significantly reduces the confidence calibration error and maintains the task performance. The generated self-reflective rationales effectively capture the internal uncertainty and can further improve the calibration performance. In addition, we find that SaySelf enables LLMs to produce low confidence in unanswerable questions.\\n\\nOur research has the potential to exert influence on both related academic research and real-world applications, including but not limited to the following cases: (1) A clear confidence expression with explanations can promote trustworthiness in AI, from the perspective of LLMs' alignment. (2) The self-reflective rationales can guide LLMs to perform subsequent steps, like invoking external tools or asking clarification questions, for better interaction and performance. (3) We also anticipate promising developments in training protocols once LLMs are trained with SaySelf, including proactive learning algorithms that enhance LLMs' interactions with humans for continued learning.\\n\\n2 Related Work\\n\\nLLMs' Confidence Elicitation\\n\\nEliciting accurate confidence estimates for LLM-generated answers that contain multiple tokens is challenging (Borji, 2023; Zhou et al., 2024). Previous work can be categorized into prompting-based and training-based approaches. Prompting-based approaches use a specific prompt to guide LLMs to generate confidence scores for their predictions (Tian et al., 2023; Kadavath et al., 2022), or prompt LLMs to generate the answers multiple times and use the consistency levels as indicators of their confidence (Xiong et al., 2023; Lyu et al., 2024; Diao et al., 2023; Yang et al., 2023). These approaches can cause inferior performance or lead to extensive inference-time latency. Training-based approaches construct a specialized dataset for supervised fine-tuning, encouraging LLMs to express their uncertainty. Lin et al. (2022) first group examples based on their types of question as the label, then obtains the confidence score for each example using the empirical accuracy for the whole group. This approach can lead to suboptimal confidence estimates since not all examples in the same group...\"}"}
{"id": "emnlp-2024-main-343", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are equal. R-Tuning (Zhang et al., 2023a) reconstructs the SFT data to add \\\"I am sure/unsure\\\" at the end of the correct/incorrect responses, which can only generate binary uncertainty estimates. As mentioned in Section 1, SaySelf addresses the limitations of the previous methods and guides LLMs to generate more accurate and fine-grained confidence estimates.\\n\\nLLMs' Hallucination & Uncertainty Expression\\n\\nLLMs' hallucination refers to instances where these models generate information that is not supported by their training data or the input provided (Zhang et al., 2023b; Liang et al., 2024; Agrawal et al., 2023). Numerous research is dedicated to exploring the causes of hallucination (Dziri et al., 2022; McKenna et al., 2023; Han et al., 2024) and developing methods to detect or mitigate hallucination (Varshney et al., 2023; Rawte et al., 2023; Andriopoulos and Pouwelse, 2023). Besides the hallucination, the reluctance of LLMs to express uncertainty when they are unable to solve tasks can further erode trust in these systems (Ji et al., 2023; Zhou et al., 2024). Existing research identifies the tendency in LLMs to fabricate information when addressing unknown questions (Liu et al., 2023; Hu et al., 2023; Amayuelas et al., 2023). This inability can be traced back to the supervised instruction finetuning (SFT) stage, which trains LLMs on human-written or GPT-synthesized (instruction, response) pairs (Wang et al., 2022; Peng et al., 2023). This paradigm neglects the discrepancy between pretraining and SFT data, potentially inducing hallucinations by instructing LLMs to appear helpful, even when they are unable to solve the problem, and discouraging them from expressing uncertainty or declining responses (Zhang et al., 2023a).\\n\\nIn this work, we propose SaySelf to train LLMs to express accurate confidence estimates and self-reflective rationales as an efficient method against hallucinations, as they can guide end users to verify information in responses and help them regain confidence in LLMs.\\n\\nLLMs' Explainability\\n\\nOur work is also related to explainability for LLMs regarding the self-reflective rationales generation (Zhao et al., 2024; Singh et al., 2024). Previous work on natural language explanations for LLMs provides motivation to explain the models' decision-making process for a prediction (Costa et al., 2018; Cambria et al., 2023). The typical approaches to producing natural language explanations involve training LLMs with the ground-truth labels and the human-annotated explanations that can serve as effective augmented supervision that guide LLMs to reason in a right way (Rajani et al., 2019; Luo et al., 2021; Yordanov et al., 2021). Another line of research adopts chain-of-thought (CoT) reasoning as natural language explanations (Wei et al., 2022; Lyu et al., 2023; Xu et al., 2024; Chen et al., 2023a). Compared to previous methods, SaySelf significantly departs from existing methods by generating rationales that not only justify the predictions but also elucidate the confidence estimates. Most importantly, SaySelf adopts LLMs' internal reasoning process to generate self-reflective rationales, instead of human-annotated explanations, which may not be faithful to specific LLMs. Unlike CoT, which primarily clarifies the rationale behind predictions, SaySelf also explicates the sources of uncertainty.\\n\\n3 SaySelf\\n\\nWe present SaySelf, a training framework to teach LLMs to express fine-grained confidence with self-reflective rationales (see Figure 2). SaySelf consists of 2 essential stages: (1) Supervised Fine-Tuning: We establish a model-specific dataset containing self-reflective rationales and confidence estimates. This dataset is built from multiple sampled responses from LLMs. (2) Reinforcement Learning from Task Supervision: We use reinforcement learning with a carefully designed reward function to further calibrate the confidence estimates for each instance. For both 2 stages, we adopt the training samples in HotpotQA (Yang et al., 2018), which typically require multi-step reasoning on knowledge facts to derive the answer. After the two-stage training, the trained models can directly answer questions with confidence estimates and self-reflective rationales without additional computational overhead.\\n\\n3.1 Supervised Fine-Tuning\\n\\nIn this stage, our goal is to construct a supervised dataset $D$, where each sample contains a question $q$, an answer with the reasoning chain $s$, the self-reflective rationale $r$, and the confidence estimate $c$. Basically, $r$ summarizes specific knowledge that the LLM is uncertain about, and is generated by analyzing the inconsistency in multiple selective responses sampled from the vanilla LLM $M$. $c$ is an integer from 1 to 10, and is derived based on the consistency of $s$. \\n\\n5987\"}"}
{"id": "emnlp-2024-main-343", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Current president: Joe Biden\\nHis younger son: Robert Hunter Biden.\\n\\nThe younger son of the current president: Robert Hunter Biden.\\n\\nCurrent president: Donald Trump\\nHis younger son: Barren Trump\\n\\nThe younger son of the current president: Barren Trump.\"}"}
{"id": "emnlp-2024-main-343", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The summary is thus taken as the self-reflective rationale. The prompt is provided in Appendix A.\\n\\nWe train the vanilla $M$ on $D$ via supervised fine-tuning. The objective function is:\\n\\n$$\\\\max_{\\\\Theta} \\\\sum_{(q,s,r,c') \\\\in D} \\\\left[ \\\\log P(s|q; \\\\Theta) + \\\\log P(r|s,q; \\\\Theta) + \\\\log P(c'|s,r,q; \\\\Theta) \\\\right]$$\\n\\nwhere $\\\\Theta$ represents the parameters of $M$, $c'$ is the natural language expression of the confidence estimate (a.k.a., \\\"My confidence is $c\\\"). The objective function is meant to maximize the sum of these log probabilities over all the tuples $(q,s,r,c')$ in the dataset.\\n\\n3.2 Reinforcement Learning from Task Supervision\\n\\nDue to the nature of supervised fine-tuning, the model tends to produce homogeneous confidence levels, such as relatively lower confidence levels for correct responses and higher levels for incorrect responses. To address this issue, we use reinforcement learning to further calibrate LLMs' fine-grained confidence estimates and guide the model to produce more accurate and differentiated values.\\n\\nDuring the sampling phase, LLMs are prompted to produce responses, self-reflective rationales, and confidence levels. To optimize the model, we compare the generated response with the ground truth. Subsequently, we formulate a reward function considering answer accuracy and model confidence. To encourage the model towards more differentiated values, the reward function has a quadratic output:\\n\\n$$R = 1 - 2 \\\\left( I(response) - confidence \\\\right)^2$$\\n\\nwhere $I()$ is the indicator function, which returns 1 if the generated response is correct, else 0. The confidence level is normalized between 0 and 1. This reward function reinforces LLMs for high confidence in accurate samples while penalizing them for being overconfident in incorrect ones.\\n\\nWe utilize the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017) to train LLMs based on this defined reward function. The optimization objective is expressed as:\\n\\n$$\\\\max_{\\\\Theta} \\\\mathbb{E}_t \\\\left[ \\\\min \\\\left( r_t(\\\\Theta) \\\\hat{A}_t, \\\\text{clip} \\\\left( r_t(\\\\Theta), 1-\\\\epsilon, 1+\\\\epsilon \\\\right) \\\\hat{A}_t \\\\right) \\\\right]$$\\n\\nwhere $r_t(\\\\Theta)$ calculates the probability ratio of the newly proposed policy to the old policy. The advantage estimate $\\\\hat{A}_t$, crucial for directing updates, is calculated from the difference between the anticipated future rewards under the current policy and the baseline or value function. This advantage estimate is directly influenced by the reward $R$, which in turn ties the optimization process closely with both response accuracy and confidence level.\\n\\n3.3 Implementation Details\\n\\nFor the supervised dataset collection, the sampling time $N$ is set to 100 and the temperature is 1.2. The similarity threshold $T$ is set to 0.9. For supervised fine-tuning, the learning rate is set to $7e^{-5}$ and the batch size is set to 8. For the reinforcement learning stage, the learning rate is set to $1e^{-5}$ and the batch size is set to 8. To check the correctness of the responses, we utilize a verification method where annotated answers must be present within the responses. This heuristic demonstrates high precision in knowledge-based QA tasks.\\n\\n4 Experiments\\n\\n4.1 Evaluation Setting\\n\\nEvaluation Datasets\\n\\nWe follow Zhang et al. (2023a) to evaluate LLMs on knowledge-extensive QA tasks. We include the following datasets:\\n\\n- HotpotQA (Yang et al., 2018), a dataset of multi-hop reasoning question-answer pairs;\\n- TruthfulQA (Lin et al., 2021), a dataset that tests whether models generate truthful answers to questions specifically designed to induce false answers;\\n- StrategyQA (Geva et al., 2021), a dataset of true/false questions requiring multi-hop reasoning;\\n- FEVER (Thorne et al., 2018), a dataset used to assess the ability of models to verify the factuality of statements against Wikipedia documents;\\n- HaluEval (Li et al., 2023), a dataset that evaluates the hallucination of models;\\n- ParaRel (Elazar et al., 2021), a dataset that measures the model's performance in understanding paraphrased relational facts.\\n\\nEvaluation Environments\\n\\nThe experiments are run on a server with 4 Nvidia A6000 GPUs and 256GB RAM. The models are implemented with the Huggingface Transformers (https://huggingface.co/) library. The reported data are all average values of three runs. Both stages take approximately 1 hour to train during the two-stage training process.\\n\\nEvaluation Metrics\\n\\nWe measure various approaches from 3 aspects. (1) Confidence Calibration Performance: We adopt 2 calibration metrics.\\n\"}"}
{"id": "emnlp-2024-main-343", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"First, we use the ECE score to measure the confidence calibration error (Guo et al., 2017; Chen et al., 2023b). Basically, ECE evaluates the correlation between the confidence scores assigned by LLMs and their corresponding correctness. For responses from LLMs $A$, it can be calculated as:\\n\\n$$ECE = \\\\frac{1}{|A|} \\\\sum_{a \\\\in A} |I(a) - \\\\text{conf}(a)|,$$\\n\\n(4)\\n\\nwhere $I()$ is the indicator function defined in Equation 2, and $\\\\text{conf}()$ returns the confidence level of LLMs. Second, we adopt the AUROC score following (Hendrycks and Gimpel, 2016). It measures the ability of LLMs to distinguish between correct and incorrect responses across different threshold settings. It can be calculated as:\\n\\n$$\\\\text{AUROC} = \\\\int_{0}^{1} \\\\text{TPR} (FPR - \\\\frac{1}{x}) \\\\, dx,$$\\n\\n(5)\\n\\nwhere $x$ is the threshold confidence level, $\\\\text{TPR}$ is the true positive rate under this threshold confidence level, and $\\\\text{FPR}$ is the false positive rate under the threshold.\\n\\n### Task Performance\\n\\nWe measure the typical accuracy on the test split of the datasets. (3)\\n\\n### Faithfulness of the Generated Self-Reflective Rationales\\n\\nWe make the first effort to measure the faithfulness of the provided self-reflective rationales. We suggest employing the same intuition utilized in SaySelf. For each question, we sample multiple responses (answers with reasoning chains) from the LLM, and perform clustering to retain several representative responses. Subsequently, we utilize a proficient LLM (GPT-4) to examine whether the provided self-reflective rationales can faithfully express the uncertainty demonstrated in the sampled responses, and give a score from 1 to 10. The final faithfulness score is the average over all samples.\\n\\n### Baselines\\n\\nWe compare with the following approaches: (1) Direct prompting for confidence extraction ($DP$): We directly ask the vanilla LLMs to give a confidence score from 1 to 10 in their previous response (Tian et al., 2023). (2) Self-consistency-based confidence estimate ($SC$): We use the self-consistency-based approach to derive the confidence estimates of LLMs. Confidence is calculated as the ratio of response frequency to the number of samples (Xiong et al., 2023). (3) Prompting for correctness ($PC$): We ask the vanilla LLMs to judge whether their responses are correct or not (Kadavath et al., 2022). (4) R-Tuning: We train LLMs to generate binary confidence estimates (sure vs. unsure) using a model-specific dataset (Zhang et al., 2023a). (5) Aligning with self-consistency-based confidence ($AS$): We train LLMs to generate the confidence estimates derived from self-consistency prompting (Yang et al., 2023). (6) Grouping-based confidence estimates for calibration training ($GCE$): We group the samples in HotpotQA via clustering, and use the accuracy of samples in the group as the confidence estimates for all samples within that group. The constructed dataset is thus used for fine-tuning (Lin et al., 2022). We implement the baseline approaches and SaySelf on Mistral-7B (Jiang et al., 2023) for fair comparison. To prove that SaySelf can generalize on multiple models, we also implement the baseline approaches and SaySelf on Llama 3 8B (Team, 2024) in Appendix D.\\n\\n### 4.2 Main Experimental Results\\n\\n#### Confidence Calibration Performance\\n\\nWe show the ECE results (Table 1) and the AUROC results (Table 5 in the Appendix) that measure the correlation between the expressed confidence and the actual performance. We observe that SaySelf significantly outperforms all baseline approaches in reducing the calibration error (ECE) and improving the distinction of confidence in correct and incorrect responses (AUROC). This conclusion holds in both in-distribution (HotpotQA) and out-of-distribution datasets, which demonstrates the general applicability of SaySelf. Also, the difference of SaySelf from other baselines is mostly statistically significant ($p < 0.05$), further demonstrating its capability to provide effective confidence estimates.\\n\\n#### Task Performance\\n\\nWe show the accuracy results in Table 2. SC, which uses multiple sampling, achieves overall better performance compared to other approaches. However, this results in high inference latency. Compared to other baseline approaches, SaySelf can overall maintain the original task performance. This indicates that the task of confidence estimates doesn't conflict with the original task, consistent with previous work (Chen et al., 2023b; Zhang et al., 2023a).\\n\\n#### Faithfulness of the Generated Self-Reflective Rationales\\n\\nThe evaluation prompt for GPT-4 is shown in Appendix A. We show the faithfulness results in Table 3. Due to the budget limits for GPT-4 evaluation, we sample 100 instances from each dataset for evaluation. The instances with multiple\"}"}
{"id": "emnlp-2024-main-343", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method  | Dataset       | HotpotQA | TruthfulQA | StrategyQA | FEVER | HaluEval | ParaRel |\\n|---------|---------------|----------|------------|------------|-------|----------|---------|\\n| DP      |               | 0.6667   | 0.3437     | 0.5357     | 0.4529| 0.6746   | 0.5129  |\\n| SC      |               | 0.3830   | 0.5204     | 0.3957     | 0.4537| 0.4242   | 0.5458  |\\n| PC      |               | 0.5515   | 0.4963     | 0.4379     | 0.4659| 0.3080   | 0.5071  |\\n| R-Tuning|               | 0.4141   | 0.4111     | 0.4477     | 0.4007| 0.2777   | 0.6797  |\\n| AS      |               | 0.3833   | 0.4308     | 0.4125     | 0.3973| 0.4344   | 0.3926  |\\n| GCE     |               | 0.3597   | 0.3639     | 0.4474     | 0.4473| 0.5819   | 0.4634  |\\n| SaySelf |               | 0.3558   | *0.3368*   | *0.3907*   | *0.3704*| *0.2661*| *0.3272*|\\n| w/o RL  |               | 0.3704   | 0.3887     | 0.3951     | 0.3903| 0.2804   | 0.3628  |\\n| w/o R & CE |           | 0.5063   | 0.4286     | 0.4195     | 0.4313| 0.4143   | 0.3972  |\\n| w/o R  |               | 0.3750   | 0.3609     | 0.3938     | 0.3854| 0.4294   | 0.4730  |\\n| w/ Naive RF |             | 0.6129   | 0.4356     | 0.4062     | 0.4238| 0.2812   | 0.3316  |\\n\\nTable 1: The ECE evaluation results of baselines, SaySelf, and various ablations. Lower is better. HotpotQA is the only in-distribution dataset. $p$-Values are the $p$-values comparing SaySelf over other methods. In this table, DP denotes direction prompting, SC denotes self-consistency, PC denotes prompting for correctness, AS denotes aligning with self-consistency-based confidence, GCE denotes grouping-based confidence estimates for calibration training; w/o RL denotes SaySelf without reinforcement learning, w/o R & CE denotes SaySelf without self-reflective rationales and confidence estimates, w/o R denotes SaySelf without self-reflective rationales, w/ Naive RF denotes using another naive reward function. The numbers with asterisk marks (*) mean significant advantage with the statistical significance threshold of $p$-value 0.05 in the paired t-test comparing with baselines.\\n\\n| Method  | Dataset       | HotpotQA | TruthfulQA | StrategyQA | FEVER | HaluEval | ParaRel |\\n|---------|---------------|----------|------------|------------|-------|----------|---------|\\n| DP      |               | 0.1562   | 0.5125     | 0.3904     |       |          |         |\\n| SC      |               | 0.3288   |            |            | 0.5713|          |         |\\n| PC      |               | 0.3281   | 0.5546     | 0.4450     | 0.4968|          |         |\\n| R-Tuning|               | 0.3664   | 0.5216     | 0.5318     | 0.5530|          |         |\\n| AS      |               | 0.3379   | 0.4861     | 0.3670     | 0.4539|          |         |\\n| GCE     |               | 0.3635   | 0.4425     | 0.5504     | 0.5506|          |         |\\n| SaySelf |               | 0.3585   | 0.5353     | 0.5956     | 0.5393|          |         |\\n| w/o RL  |               | 0.3708   | 0.4667     | 0.5340     |       |          |         |\\n| w/o R & CE |           | 0.3411   | 0.4623     | 0.3811     | 0.4004|          |         |\\n| w/o R  |               | 0.3650   | 0.4964     | 0.4224     |       |          |         |\\n| w/ Naive RF |             | 0.3715   |            |            | 0.5721|          |         |\\n\\nTable 2: The accuracy evaluation results of baselines, SaySelf, and various ablations.\\n\\n| Method  | Dataset       | HotpotQA | TruthfulQA | StrategyQA | FEVER | HaluEval | ParaRel |\\n|---------|---------------|----------|------------|------------|-------|----------|---------|\\n| DP / SC |               | 6.5      | 7.8        | 5.9        | 6.2   | 7.5      | 7.0     |\\n| PC      |               | 6.4      | 7.3        | 5.2        | 6.0   | 7.1      | 7.2     |\\n| R-Tuning|               | 6.7      | 7.4        | 6.0        | 6.2   | 6.7      | 6.1     |\\n| AS      |               | 5.1      | 6.6        | 8.1        | 7.6   | 8.0      | 6.6     |\\n| GCE     |               | 5.7      | 6.1        | 4.2        | 5.6   | 5.9      | 5.2     |\\n| SaySelf |               | 8.3      | 8.6        | 5.5        | 7.8   | 8.5      | 7.4     |\\n\\nTable 3: The faithfulness evaluation results for self-reflective rationales. Selective reasoning chains are chosen for priority. For all baseline approaches, we explicitly instruct LLMs to provide reasoning for the confidence levels assigned to their prior responses. The results for DP and SC are combined, as both methods sample from the same LLM.\\n\\nThe experimental results show that SaySelf can generate more reasonable self-reflective rationales that indicate the internal uncertainty in LLMs as evidenced by inconsistencies across multiple sampled responses. One exception is the StrategyQA dataset, which only contains True/False questions. Consequently, typically only one or two responses are selected for each question, resulting in high variance in the evaluation.\\n\\nWe conduct human annotations to justify the use of GPT-4 for automatic evaluation. The details are described in Appendix C. We observe a Spearman\u2019s rank correlation coefficient of 0.89 between the ratings given by GPT-4 and humans, which demonstrates...\"}"}
{"id": "emnlp-2024-main-343", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Howard Centre is located in Letchworth, England. Letchworth was founded as the world's first garden city in 1903 by Ebenezer Howard. Therefore, the year Letchworth was founded as a garden city is 1903.\\n\\nThe Howard Centre is located in the town of Welwyn Garden City in Hertfordshire, England. The town was founded in 1920 as a model garden city, designed by the Ebenezer Howard's Garden Cities and Town Planning Association. Therefore, Welwyn Garden City was founded in 1920.\\n\\nThe Howard Centre is located in Cambridge, United Kingdom. Cambridge was founded as a garden city in 1841 by Sir Charles Barry and Ebenezer Howard. The garden city concept... (omitted)\\n\\nIn what year was the town where the Howard Centre is located founded as a garden city?\"}"}
{"id": "emnlp-2024-main-343", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SaySelf's strong ability to detect and summarize internal uncertainties. For example, in the first case, SaySelf expresses uncertainty about the exact location of the Howard Centre, identifying strong indications that it is likely in Letchworth and not Welwyn Garden City, with Cambridge being an unlikely option. This rationale acknowledges the mixed information leading to different founding years based on the location\u20141903 for Letchworth and 1920 for Welwyn Garden City, dismissing the 1841 Cambridge claim as highly improbable. This capability for self-reflective generation has a profound impact on improving the reliability of LLM-based systems.\\n\\n5 Conclusion\\n\\nThis paper presents a training framework SaySelf for eliciting more accurate and fine-grained confidence estimates and self-reflective rationales from LLMs. SaySelf involves supervised finetuning with a model-specific dataset constructed by summarizing the difference between multiple reasoning chains and reinforcement learning with a properly designed reward function. Our evaluations across diverse datasets confirm that SaySelf reduces calibration errors, maintains performance, and generates insightful rationales.\\n\\nLimitations\\n\\nA potential limitation of SaySelf is its dependence on multiple sampled chains of reasoning to develop self-reflective rationales for training. There is still an ongoing debate regarding the faithfulness of CoT reasoning, specifically questioning whether it authentically represents the thinking process of LLMs (Lanham et al., 2023; Bentham et al., 2024; Turpin et al., 2024). The unfaithful CoT reasoning can cause unfaithful self-reflective rationales. Nonetheless, our ablation study demonstrates that these self-reflective rationales substantially enhance calibration performance. Further improvements in the effectiveness and faithfulness of SaySelf could potentially be achieved by integrating methods from recent research aimed at increasing the faithfulness of CoT reasoning, as suggested by Lyu et al. (2023).\\n\\nEthical Considerations\\n\\nThis work aims to improve the performance of LLMs in eliciting more fine-grained confidence estimates and self-reflective rationales. In the case of this work, it involves the use of Mistral 7B and GPT-4, so the same risks from LLMs research are also applicable to this work (Bender et al., 2021). While SaySelf aims to enhance trust in AI by providing clear confidence expressions and self-reflective rationales, there is a risk that users might over-rely on these confidence estimates. If the self-reflective rationales are not accurate or fail to capture the true uncertainty of the model, it could lead to potentially harmful decisions based on the model's outputs. Therefore, users are advised to check important information before making crucial decisions.\\n\\nThis paper works on several publicly available datasets including HotpotQA, TruthfulQA, StrategyQA, FEVER, HaluEval, and ParaRel. They are available for the research community to study under Apache 2.0, Apache 2.0, MIT, CC-BY-SA 3.0, Apache 2.0, and MIT licenses respectively. Data is anonymized, thus our work does not propagate any privacy problems about any specific entities. Finally, we carried out human annotations for analysis purposes. Since the amount of work is small, we and the annotators agree to consider it as a voluntary service. We have sufficiently discussed the specific use of the annotations and potential risks to annotators before the work.\\n\\nAcknowledgments\\n\\nThis work is supported in part by the US National Science Foundation under grant NSF-IIS2226108. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.\\n\\nReferences\\n\\nAyush Agrawal, Lester Mackey, and Adam Tauman Kalai. 2023. Do language models know when they're hallucinating? references. arXiv preprint arXiv:2305.18248.\\n\\nAlfonso Amayuelas, Liangming Pan, Wenhu Chen, and William Wang. 2023. Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. arXiv preprint arXiv:2305.13712.\\n\\nKonstantinos Andriopoulos and Johan A. Pouwelse. 2023. Augmenting llms with knowledge: A survey on hallucination prevention. arXiv, abs/2309.16459.\\n\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021\"}"}
{"id": "emnlp-2024-main-343", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Oliver Bentham, Nathan Stringham, and Ana Marasovi\u0107. 2024. Chain-of-thought unfaithfulness as disguised accuracy. arXiv preprint arXiv:2402.14897.\\n\\nAli Borji. 2023. A categorical archive of chatgpt failures. arXiv preprint arXiv:2302.03494.\\n\\nErik Cambria, Lorenzo Malandri, Fabio Mercorio, Mario Mezzanzanica, and Navid Nobani. 2023. A survey on xai and natural language explanations. Information Processing & Management, 60(1):103111.\\n\\nYangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. 2023a. Measuring and improving chain-of-thought reasoning in vision-language models. arXiv preprint arXiv:2309.04461.\\n\\nYangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. 2023b. A close look into the calibration of pre-trained language models. ACL.\\n\\nFelipe Costa, Sixun Ouyang, Peter Dolog, and Aonghus Lawlor. 2018. Automatic generation of natural language explanations. In Proceedings of the 23rd international conference on intelligent user interfaces companion, pages 1\u20132.\\n\\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023. Active prompting with chain-of-thought for large language models. arXiv preprint arXiv:2302.12246.\\n\\nNouha Dziri, Sivan Milton, Mo Yu, Osmar R Zaiane, and Siva Reddy. 2022. On the origin of hallucinations in conversational models: Is it the datasets or the models? In North American Chapter of the Association for Computational Linguistics.\\n\\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Sch\u00fctze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models. Transactions of the Association for Computational Linguistics, 9:1012\u20131031.\\n\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346\u2013361.\\n\\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. Preprint, arXiv:1706.04599.\\n\\nTianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang. 2024. The instinctive bias: Spurious images lead to hallucination in mllms. CoRR, abs/2402.03757.\\n\\nDan Hendrycks and Kevin Gimpel. 2016. A baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136.\\n\\nShengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, and Maosong Sun. 2023. Won't get fooled again: Answering questions with false premises. arXiv preprint arXiv:2307.02394.\\n\\nJiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. 2023. Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852.\\n\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Menisch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.\\n\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.\\n\\nTamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. 2023. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702.\\n\\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A large-scale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449\u20136464.\\n\\nYuxin Liang, Zhuoyang Song, Hao Wang, and Jiaxing Zhang. 2024. Learning to trust your feelings: Leveraging self-awareness in llms for hallucination mitigation. arXiv preprint arXiv:2401.15449.\\n\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.\\n\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. ArXiv preprint, abs/2205.14334.\\n\\nGenglin Liu, Xingyao Wang, Lifan Yuan, Yangyi Chen, and Hao Peng. 2023. Prudent silence or foolish babble? examining large language models' responses to the unknown. arXiv preprint arXiv:2311.09731.\\n\\nSiwen Luo, Hamish Ivison, Soyeon Caren Han, and Josiah Poon. 2021. Local interpretations for explainable natural language processing: A survey. ACM Computing Surveys.\"}"}
{"id": "emnlp-2024-main-343", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. 2023. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2301.13379.\\n\\nQing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, and Chris Callison-Burch. 2024. Calibrating large language models with sample consistency. arXiv preprint arXiv:2402.13904.\\n\\nNick McKenna, Tianyi Li, Liang Cheng, Mohamad Javad Hosseini, Mark Johnson, and Mark Steedman. 2023. Sources of hallucination by large language models on inference tasks. ArXiv, abs/2305.14552.\\n\\nOpenAI. 2023. Gpt-4 technical report. Preprint, arXiv:2303.08774.\\n\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.\\n\\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361.\\n\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822.\\n\\nVipula Rawte, A. Sheth, and Amitava Das. 2023. A survey of hallucination in large foundation models. ArXiv, abs/2309.05922.\\n\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.\\n\\nChandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. 2024. Rethinking interpretability in the era of large language models. arXiv preprint arXiv:2402.01761.\\n\\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, and Tao Yu. 2022. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741.\\n\\nLlama Team. 2024. The Llama 3 Herd of Models. Preprint, arXiv:2407.21783.\\n\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355.\\n\\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Preprint, arXiv:2305.14975.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n\\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. 2024. Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting. Advances in Neural Information Processing Systems, 36.\\n\\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. Preprint, arXiv:2307.03987.\\n\\nXingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030.\\n\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837.\\n\\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint arXiv:2306.13063.\\n\\nLin Xu, Ningxin Peng, Daquan Zhou, See-Kiong Ng, and Jinlan Fu. 2024. Chain of thought explanation for dialogue state tracking. arXiv preprint arXiv:2403.04656.\\n\\nYuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2023. Alignment for honesty. arXiv preprint arXiv:2312.07000.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600.\\n\\nHongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023. Cognitive mirage: A review of hallucinations in large language models. Preprint, arXiv:2309.06794.\"}"}
{"id": "emnlp-2024-main-343", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA Prompt\\n\\nA.1 GPT-4 Summarization Prompt\\nYour task is to analyze a question provided to you along with a set of correct and incorrect responses generated by my model. Your objective is to identify and summarize the inconsistency in the models' responses that can explain why my model is uncertain about the correct answer.\\n\\nPlease note that:\\n1. You should give the reasons from a first-person perspective, as if you are my model that gives the provided responses and confidence scores.\\n2. Limit your explanation to the knowledge and facts the model possesses about the question.\\n3. Keep your summary brief, aiming for 1-3 sentences.\\n4. Each response is paired with a confidence score at the beginning. Include the confidence score that accompanies each response in your summary.\\n5. Please directly provide the summarized reason without any greetings or other unnecessary information. If you find the incorrect responses are consistent with the correct response regarding the question, please directly return N/A.\\n6. Importantly, my model doesn't have access to the ground truth. Therefore, the summarized reason should not have any statement about correctness or incorrectness of the responses. You should only focus on discussing the uncertainty in the knowledge and facts based on the inconsistency in the responses.\\n7. Importantly, my model only has access to the correct response. Thus, the summary should not include any statement like \\\"My different responses have ...\\\", \\\"my responses about ...\\\", \\\"the multiple responses ...\\\", etc. You should not say \\\"my responses\\\" or \\\"the responses\\\" anywhere in the summary.\\n\\nHere is an example:\\nQuestion: Sky High starred the actress who is married to which actor?\\nCorrect Response: (6% confidence) The actress who starred in \\\"Sky High\\\" (2005) and is married to an actor is Kelly Preston. Her husband is John...\"}"}
{"id": "emnlp-2024-main-343", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 5: The AUROC evaluation results of baselines, SaySelf, and various ablations.\\n\\n| Method      | Dataset | HotpotQA | TruthfulQA | StrategyQA | FEVER | HaluEval | ParaRel |\\n|-------------|---------|----------|------------|------------|-------|----------|---------|\\n| DP          |         | 0.3222   | 0.5667     | 0.5193     | 0.5371| 0.5278   | 0.5291  |\\n| SC          |         | 0.5765   | 0.4939     | 0.5498     | 0.5472| 0.5843   | 0.5546  |\\n| PC          |         | 0.6636   | 0.5244     | 0.5385     | 0.5139| 0.5544   | 0.6181  |\\n| R-Tuning    |         | 0.6529   | 0.5980     | 0.5406     | 0.5688| 0.5330   | 0.5424  |\\n| AS          |         | 0.4955   | 0.4835     | 0.5391     | 0.5101| 0.5569   | 0.5713  |\\n| GCE         |         | 0.5042   | 0.4966     | 0.5043     | 0.4942| 0.4907   | 0.5031  |\\n| SaySelf     |         | 0.7156   | 0.6107     | 0.6074     | 0.6481| 0.7318   | 0.6816  |\\n| w/o RL      |         | 0.6524   | 0.5675     | 0.5910     | 0.5798| 0.5929   | 0.6003  |\\n| w/o R & CE  |         | 0.5256   | 0.5724     | 0.5738     | 0.6059| 0.6002   | 0.5823  |\\n| w/o R       |         | 0.4928   | 0.4952     | 0.4567     | 0.4831| 0.4893   | 0.4853  |\\n| w/ Naive RF |         | 0.5140   | 0.4907     | 0.5091     | 0.5137| 0.5147   | 0.5053  |\\n\\n### Table 6: The ECE and accuracy evaluation results of baselines and SaySelf on a different base model, Llama 3 8B.\\n\\n| Method      | Dataset | ECE | Accuracy |\\n|-------------|---------|-----|----------|\\n| DP          |         | 0.4744 | 0.4762  |\\n| SC          |         | 0.4798 | 0.3475  |\\n| PC          |         | 0.4604 | 0.3678  |\\n| R-Tuning    |         | 0.4094 | 0.4348  |\\n| AS          |         | 0.3525 | 0.4230  |\\n| GCE         |         | 0.4056 | 0.3480  |\\n| SaySelf     |         | 0.3296 | 0.4284  |\\n\\n### Incorrect Responses:\\n1. (33% confidence) The actress who starred in \u201cSky High\u201d (2005) and is married to an actor is Kristen Bell. Bell voiced the main character, Layla, in \u201cSky High,\u201d and she is married to Dax Shepard, who is also an actor.\\n2. (17% confidence) The actress who starred in \u201cSky High\u201d (2005) and is married to an actor is Kelly Clarkson. Her acting debut was in this film, and she married singer and actor Brandon Blackstock in 2013.\\n\\nThe output can be: I am uncertain about the correct actress in \u201cSky High\u201d. There is a high probability that the actress is Kristen Bell, instead of Kelly Preston. I am confused about her voice acting roles with on-screen appearances. There is also some probability that the actress is Kelly Clarkson.\\n\\nNow consider the following case:\\n\\n**Question:** {}\\n\\n**Correct Response:** {}\\n\\n**Incorrect Responses:** {}\"}"}
{"id": "emnlp-2024-main-343", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zuffa was founded in 2001 by Lorenzo Ferlitta and Frank Fertitta III. Frank Fertitta III currently serves as the CEO of Station Casinos, a gaming and hospitality company based in Las Vegas. Therefore, Frank Fertitta III is the current CEO of Station Casinos.\\n\\nZuffa was founded in 2001 by Lorenzo Ferlitta and another American entrepreneur who is the current CEO of what?\"}"}
