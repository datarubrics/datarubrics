{"id": "emnlp-2024-main-331", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts\\n\\nJaewook Lee*, Yeajin Jang*, Hongjin Kim, Woojin Lee, Harksoo Kim\u2020\\n\\nKonkuk University\\n{benecia428, dpwls258, jin3430, shes100, nlpdrkim}@konkuk.ac.kr\\n\\nAbstract\\nEmotional intelligence (EI) in artificial intelligence (AI), which refers to the ability of an AI to understand and respond appropriately to human emotions, has emerged as a crucial research topic. Recent studies have shown that large language models (LLMs) and vision large language models (VLLMs) possess EI and the ability to understand emotional stimuli in the form of text and images, respectively. However, factors influencing the emotion prediction performance of VLLMs in real-world conversational contexts have not been sufficiently explored. This study aims to analyze the key elements affecting the emotion prediction performance of VLLMs in conversational contexts systematically. To achieve this, we reconstructed the MELD dataset, which is based on the popular TV series Friends, and conducted experiments through three sub-tasks: overall emotion tone prediction, character emotion prediction, and contextually appropriate emotion expression selection. We evaluated the performance differences based on various model architectures (e.g., image encoders, modality alignment, and LLMs) and image scopes (e.g., entire scene, person, and facial expression). In addition, we investigated the impact of providing persona information on the emotion prediction performance of the models and analyzed how personality traits and speaking styles influenced the emotion prediction process. We conducted an in-depth analysis of the impact of various other factors, such as gender and regional biases, on the emotion prediction performance of VLLMs. The results revealed that these factors significantly influenced the model performance.\\n\\n1 Introduction\\nEmotional intelligence (EI) in artificial intelligence (AI), which refers to the ability of an AI to understand and respond appropriately to human emotions, is a crucial topic in AI research. EI involves the ability to interpret and manage the emotions that are embedded in information and is essential for various cognitive tasks, from problem solving to behavior regulation (Salovey et al., 2009). Human emotions play a significant role in various domains such as academics, competitive sports, and daily life and are shaped by internal and external factors (Koole, 2010; Pekrun et al., 2002; Lazarus, 2000; Li et al., 2023b). Equipping AI systems with EI enhances the quality of human-AI interactions, improves user experience, and enables more natural and effective communication based on emotional empathy.\\n\\nLarge language models (LLMs), which are considered a crucial step towards achieving artificial general intelligence, have exhibited exceptional performance in various fields (Bubeck et al., 2023). As a result, there has been growing interest in whether LLMs possess EI. Wang et al. (2023) evaluated the EI of LLMs through psychological measurements and discovered that GPT-4 achieved high EQ scores. Moreover, studies by Li et al. (2023b) and Li et al. (2023c) showed that LLMs can understand emotional stimuli in the form of text and images, perceiving emotions in a manner similar to humans. However, these studies have limitations as they focus on a single modality, whereas various factors such as verbal cues, visual cues, and contextual information interact in a complex manner in real-world conversational situations.\\n\\nVision large language models (VLLMs) have recently gained attention to overcome the above limitations. As VLLMs can process text and images simultaneously, they have the potential to solve more complex and multifaceted emotion prediction tasks. For example, VLLMs can infer emotional states by comprehensively analyzing the facial expressions and verbal cues of conversation participants or predict appropriate emotional responses considering the context.\"}"}
{"id": "emnlp-2024-main-331", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the conversational context. However, despite their potential, the key factors influencing the emotion prediction of VLLMs in conversational situations have not yet been sufficiently explored.\\n\\nThis study aimed to analyze the factors influencing the emotion prediction of VLLMs, such as the model architecture, persona information, and biases, systematically to explore means of improving emotion prediction performance in conversational situations. To achieve this, we reconstructed the MELD dataset (Poria et al., 2018) based on the popular TV series *Friends* and augmented it by integrating various elements, such as images, conversational context, and persona information, to evaluate the performance of VLLMs comprehensively. We conducted an extensive assessment of the emotion understanding and expression performance of VLLMs through three sub-tasks: overall emotion tone prediction, character emotion prediction, and contextually appropriate emotion expression selection.\\n\\nThe experimental results showed that differences in the model architecture had a distinct impact on the emotion prediction performance. This suggests that the structural characteristics of VLLMs, such as the method of integrating image and text information and the LLM Backbone, play crucial roles in emotion prediction performance. In addition, models that included persona information exhibited notable differences in the emotion prediction process. This implies that information on the personality traits and speaking styles of an individual significantly influences the emotion understanding and response performance of the model. We also conducted an in-depth analysis of the impact of various factors related to emotion prediction, such as gender and regional biases, on the emotion prediction performance of VLLMs. The analysis revealed that factors such as gender and regional biases significantly influenced the emotion prediction process of VLLMs, revealing the biases and limitations that may arise in this process.\\n\\n2 Related Work\\n\\nThe rapid development of LLMs has led to substantial progress in language generation, knowledge utilization, and complex reasoning tasks. However, as these models are being integrated into various application domains, enhancing their EI and mitigating social biases have become increasingly important. Wang et al. (2023) explored the EI of LLMs using psychological methods, thereby laying the foundation for further research on how these models perceive and respond to emotional stimuli. Building on this work, Li et al. (2023b) and Li et al. (2023c) investigated the ability of LLMs to understand emotional content and demonstrated that current models can react to emotional stimuli similarly to humans. Paech (2023) introduced a new criterion for evaluating the EI of LLMs through EQ-Bench, which is a benchmark that measures the ability of a model to predict the emotional states of characters within conversations. Sabour et al. (2024) proposed EMOBENCH, which is a benchmark that is designed to evaluate the EI of LLMs comprehensively by assessing not only emotion recognition, but also emotional regulation and the application of emotional understanding.\\n\\nAlong with research on the EI of LLMs, addressing the social biases that are inherent in these models is a crucial task for the development of ethical AI. Sheng et al. (2019) and Schick et al. (2021) emphasized the importance of recognizing and mitigating gender stereotypes and other biases in the training data. Nadeem et al. (2021) measured stereotypical biases using the StereoSet benchmark, while Parrish et al. (2022) evaluated biases in question-answering tasks using the BBQ dataset. These studies provided important insights into the EI and social biases of LLMs. Building on this foundation, the present study aimed to analyze the key factors influencing the emotion prediction of VLLMs in conversational contexts systematically. Specifically, we intended to investigate the impact of factors such as persona, gender, and regional biases on the emotion prediction processes of VLLMs in depth.\\n\\n3 Dataset and Task Overview\\n\\nWe reconstructed the MELD dataset (Poria et al., 2018), which is based on popular TV series *Friends*, to investigate the key factors influencing the emotion prediction performance of VLLMs in conversational contexts. The MELD dataset provides full-scene images for each scene and the corresponding conversational context, along with the names of the characters who engage in the dialogue and the emotion and sentiment labels for the feelings of each character. The dataset includes emotion and sentiment labels for each utterance. Emotions are categorized into seven types: \u201cfear,\u201d \u201cdisgust,\u201d \u201cjoy,\u201d \u201csadness,\u201d \u201csurprise,\u201d \u201canger,\u201d and...\"}"}
{"id": "emnlp-2024-main-331", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overview of the data reconstruction process for evaluating the emotion prediction performance of VLLMs using the MELD dataset. The process involved three main stages: (1) dialogue selection, which filtered and adjusted dialogues based on the number of turns and presence of characters with personal information; (2) image scope reconstruction, which extracted images from video frames and categorized them into three scopes (entire scene, person, and facial expression) to capture different aspects of emotional information; and (3) incorrect sentence selection, which selected distractor sentences for each sub-task using SBERT.\\n\\nNeutral,\\\" whereas sentiments are divided into three categories: \\\"positive,\\\" \\\"negative,\\\" and \\\"neutral.\\\"\\n\\nAppendix A presents the overall statistics.\\n\\n3.1 Persona Information\\nAs the characteristics of an individual greatly influence emotion expression and understanding, we constructed additional persona information for the MELD dataset. The persona information consisted of the personality traits and speaking styles of each character. Personality traits influence how individuals perceive and express emotions, and play a crucial role in understanding and modeling emotional responses in conversational contexts. We carefully defined the personality traits of the characters of Friends to provide comprehensive persona information. By including these personality traits in the model, we could investigate their impact on the emotion prediction performance.\\n\\nSpeaking styles affect how individuals convey their emotions and intentions. Each character of Friends has a unique manner of speaking. By integrating these speaking styles into the model, we could analyze their influence on emotion prediction performance.\\n\\n3.2 Quantitative Evaluation\\nTo evaluate the emotion prediction performance of VLLMs comprehensively, we approached the problem by selecting the most appropriate emotion expression in each conversational turn, beyond simply recognizing the emotions of the speaker. We used a multiple-choice question format in which each question consisted of one correct utterance and three incorrect utterances. The three sub-tasks were designed to assess different aspects of the emotion understanding and expression abilities of the model, as follows:\\n\\nOverall emotion tone prediction task assessed the ability of the model to predict the overall emotional tone of the dialogue by selecting the most appropriate utterance from the options with different sentiments.\\n\\nCharacter emotion prediction task evaluated the ability of the model to predict the emotions of specific characters in a given context by selecting the most appropriate utterance from the options expressing different emotions.\\n\\nContextually appropriate emotion expression selection task assessed the ability of the model to understand the context in depth and to select the\"}"}
{"id": "emnlp-2024-main-331", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"most appropriate emotion expression by identifying the correct utterance from options with the same emotion but different expressions.\\n\\n4. Dataset Construction\\n\\nWe reconstructed the existing MELD dataset to align it with the objectives of this study. The data reconstruction process consisted of three stages: 1) dialogue selection, 2) image scope reconstruction, and 3) incorrect sentence selection. Using these stages, we constructed data that fit the purpose and removed unnecessary data. The entire process is illustrated in Figure 1.\\n\\n4.1 Dialogue Selection\\n\\nThe original dataset includes various characters and sentences that are commonly used in real-life conversations. These characteristics are useful for identifying the key elements that influence the emotion prediction of VLLMs in conversational contexts. For data selection, we removed samples that either included dialogue with very long turns or could not reflect persona information.\\n\\nAdjusting dialogues with very long turns. In conversations, instances arise in which very long turns appear. In such situations, models generally rely more heavily on the previous conversational context than on facial expressions or gestures. This can significantly affect emotion prediction, particularly for VLLMs that use LLMs as their backbone models. This is because these models may prioritize the textual context over visual cues. This can act as noise when identifying the key factors that influence the emotion prediction of the model. Therefore, we decided to reduce dialogues exceeding 15 turns randomly, to between 9 and 15 turns. The reason for randomly adjusting the number of turns rather than fixing them was to prevent bias associated with the number of turns. In addition, the randomization ensured the inclusion of samples with various dialogue lengths within the dataset to aid in evaluating the model performance in real-life conversational scenarios with varying lengths.\\n\\nRemoving characters lacking persona information. We also aimed to evaluate the emotion prediction performance of VLLMs based on the inclusion or exclusion of persona information. To this end, we structured the dialogue data such that characters to whom persona information could be assigned appeared during the final utterance turn. However, collecting persona information for some characters (e.g., hosts, customers, and airline employees), is difficult or impossible. Therefore, dialogues involving such characteristics were excluded from the dataset. The final dataset included a pool of characters consisting of six main characters with persona information and 18 surrounding characters.\\n\\n4.2 Image Scope Reconstruction\\n\\nText-based information is often effective for explicit communication, but has limitations in conveying complex emotional states or atmospheres. In contrast, images enrich these emotional nuances through nonverbal elements and visual context. Particularly in human conversations, emotions vary significantly depending on the context and environment. Therefore, the visual information contained in images, such as the posture, facial expressions, and gestures of the conversation partners, can capture the subtleties of emotions that are difficult to discern from text alone.\\n\\nFor image processing, the original videos were divided into frames and image information was extracted from each relevant frame. The most suitable frame was selected and used for the entire scene image. Person and facial expression images were extracted separately from the selected image, and the entire process was performed manually by the authors. At the end of each stage, cross-validation was performed to improve the image accuracy and ensure strict quality control.\\n\\n4.3 Incorrect Sentence Selection\\n\\nThe final stage of the data construction involved the selection of incorrect sentences for each dialogue. In this stage, we selected incorrect sentences corresponding to the multiple-choice questions. We selected sentences with sentiments or emotions that differed from the correct sentences for the overall emotion tone and character emotion prediction tasks. For the contextually appropriate emotional expression selection task, we selected sentences with the same emotion as the correct sentence.\\n\\nThe selected sentences were filtered using SBERT (Reimers and Gurevych, 2019). Some sentences may have high semantic similarity and can be used interchangeably with the correct sentence; therefore, we removed sentences that received semantic similarity scores above a certain level to eliminate such cases. In addition, we constructed the dataset with two difficulty levels (easy and hard).\"}"}
{"id": "emnlp-2024-main-331", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model     | LLM          | Tone Emotion Context | Avg.       |\\n|-----------|--------------|----------------------|------------|\\n| All Person Face | All Person Face | All Person Face |            |\\n| Prompt type | Original     |                      |            |\\n| InstructBLIP | Vicuna (13B) | 40.23 40.15 39.10 40.28 40.83 40.45 40.72 41.09 41.16 | 40.45 |\\n| LLaV A-1.5  | Vicuna (13B) | 50.39 50.15 49.77 48.98 48.94 48.75 48.69 48.39 47.50 | 49.06 |\\n| LLaV A-Next | Vicuna (13B) | 52.00 51.42 51.28 49.87 49.18 48.56 49.97 49.76 49.13 | 50.13 |\\n| InstructBLIP | FLAN (11B)   | 56.10 56.36 56.59 56.75 56.75 56.92 55.35 55.34 55.73 | 56.20 |\\n| LLaV A-1.5  | Vicuna (7B)  | 38.11 38.41 37.45 36.49 36.63 36.67 35.72 35.48 34.82 | 36.65 |\\n| LLaV A-Next | Vicuna (7B)  | 46.41 46.06 46.11 45.82 45.76 45.51 45.16 44.98 44.53 | 45.59 |\\n| LLaV A-Next | Mistral (7B) | 47.86 47.58 47.48 46.64 46.47 46.53 46.50 46.03 45.79 | 46.76 |\\n| Qwen-VL-Chat | Qwen (7B)    | 39.78 39.49 40.17 38.88 38.92 38.52 37.75 37.97 37.56 | 37.86 |\\n| MiniGPT-4   | Vicuna (7B)  | 27.58 27.89 28.57 27.82 27.85 27.22 26.45 26.77 27.07 | 27.47 |\\n| Otter      | MPT (7B)     | 38.00 37.65 37.92 38.58 38.37 38.89 37.11 37.23 37.00 | 38.78 |\\n| InstructBLIP | FLAN (3B)    | 51.91 51.91 51.37 51.86 51.57 51.52 50.75 50.64 50.06 | 51.28 |\\n| Prompt type | Personality traits |                      |            |\\n| InstructBLIP | Vicuna (13B) | 39.95 39.86 39.00 40.41 40.27 39.57 39.80 39.59 39.14 | 39.73 |\\n| LLaV A-1.5  | Vicuna (13B) | 51.09 51.00 50.50 49.33 49.60 49.27 49.44 49.75 48.59 | 49.84 |\\n| LLaV A-Next | Vicuna (13B) | 49.94 49.05 48.55 47.23 46.73 46.33 48.55 48.47 47.28 | 48.02 |\\n| InstructBLIP | FLAN (11B)   | 54.87 55.00 54.20 54.95 54.95 54.54 53.05 53.05 53.45 | 54.23 |\\n| Prompt type | Speaking styles |                      |            |\\n| InstructBLIP | Vicuna (13B) | 40.18 40.12 39.46 40.33 40.69 39.59 39.62 39.59 39.49 | 39.90 |\\n| LLaV A-1.5  | Vicuna (13B) | 50.48 50.24 49.34 49.17 49.49 48.79 48.77 48.91 47.84 | 49.23 |\\n| LLaV A-Next | Vicuna (13B) | 50.45 49.66 49.67 46.87 46.95 46.23 47.47 47.00 47.12 | 47.94 |\\n| InstructBLIP | FLAN (11B)   | 55.71 56.50 56.13 55.38 55.91 55.95 54.60 54.47 54.93 | 55.51 |\\n| Prompt type | CoT |                      |            |\\n| InstructBLIP | Vicuna (13B) | 40.53 40.47 39.17 41.52 41.02 40.24 41.17 41.19 40.41 | 40.63 |\\n| LLaV A-1.5  | Vicuna (13B) | 49.33 49.04 48.94 49.15 49.13 49.06 48.13 47.94 47.22 | 48.66 |\\n| LLaV A-Next | Vicuna (13B) | 51.21 50.41 49.70 48.36 47.71 47.12 49.87 49.60 48.48 | 49.16 |\\n| InstructBLIP | FLAN (11B)   | 55.94 56.11 56.02 55.84 55.94 56.17 54.93 55.06 55.05 | 55.67 |\\n| Prompt type | CoT |                      |            |\\n\\n**Table 1:** Comprehensive performance comparison of VLLM models with varied prompt types (original, personality traits, speaking styles, and CoT). The results, shown as the accuracy scores averaged across three distinct prompts per type, indicate the mean performance on the easy and hard levels. \u201cAll\u201d denotes entire scene scope, \u201cPerson\u201d refers to individual character scope, and \u201cFace\u201d refers to facial expression scope. \u201cTone,\u201d \u201cEmotion,\u201d and \u201cContext\u201d correspond to the overall emotion tone prediction task, character emotion prediction task, and contextually appropriate emotion expression selection task, respectively.\\n\\nFor \u201ceasy,\u201d we randomly selected sentences from the top 20 sentences with semantic similarity scores of 0.1 or lower. For \u201chard,\u201d to introduce more complexity than the easy level, we adjusted the semantic similarity score criterion to 0.4 and randomly selected sentences from the top 20 sentences with the highest scores.\\n\\n5 Experiments and Results\\n\\nWe measured the performance using three different prompts, considering their influence. The detailed prompts can be found in Appendix D.\\n\\n5.1 Baselines\\n\\nThe experiments were conducted using various open-source VLLMs. Specifically, factors such as modality alignment, model size, and LLMs were considered in the model selection. Modality alignment is a technique for effectively integrating and processing various types of data, such as text and images, in VLLMs. We analyzed key modality alignment techniques, including Direct Mapping (Liu et al., 2023), Q-Former (Li et al., 2023d), and Customization Perceiver (Alayrac et al., 2022; Awadalla et al., 2023). In addition, following gen...\"}"}
{"id": "emnlp-2024-main-331", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"erally known scaling laws, we thoroughly investigated how the emotion prediction performance of VLLMs interacted with other factors. To this end, we also conducted experiments on models with the same architecture but different sizes. The selected VLLMs included LLaV A-1.5 (Liu et al., 2023), MiniGPT-4 (Zhu et al., 2023), InstructBLIP (Dai et al., 2024), Qwen-VL-Chat (Bai et al., 2023), LLaV A-Next (Liu et al., 2024), and Otter (Li et al., 2023a). We selected various LLMs and model sizes and performed experiments on 11 VLLMs. Some high-performance models, such as GPT-4V, were excluded from the detailed analysis because their internal workings and parameter configurations have not been disclosed.\\n\\n5.2 Main Results\\n\\nTable 1 presents the performance based on the average values of the easy and hard difficulty levels. The individual performances for easy and hard can be found in Appendix B. We provide answers to the following questions according to the main experimental results:\\n\\nQ1: What is the most influential factor in the emotion prediction performance of the model?\\n\\nAnswer: LLM. Our experiments show that the most important factor in the emotion prediction performance of a model is the LLM itself. In particular, we observed that as the size of the LLM increased, the performance consistently improved across all models used in the experiments (InstructBLIP, LLaV A-1.5, LLaV A-Next, etc.). This trend was evident across all prompt types, including original, personality traits, speaking styles, and chain of thought (CoT). These results suggest that the LLM Backbone plays a more crucial role in predicting human emotions than focusing on specific image regions does. This aligns with existing research, indicating that the architecture and scaling of LLMs enhance the performance.\\n\\nQ2: What is the most outstanding model architecture for emotion prediction?\\n\\nAnswer: InstructBLIP (FLAN 11B). InstructBLIP (FLAN 11B) consistently achieved the highest performance in most cases. To verify whether these results were simply owing to the instruction-tuning dataset, we conducted a comparative experiment with InstructBLIP (Vicuna 13B), which was trained using the same data. Consequently, FLAN exhibited superior performance over Vicuna, indicating that the architecture of FLAN itself, rather than merely the tuning data, provides excellent emotion prediction performance.\\n\\nQ3: Do additional persona information and CoT affect the emotion prediction performance of the model?\\n\\nAnswer: Yes. The integration of persona information and CoT prompts influenced the emotion prediction performance of the model. The experimental results indicated that the effects of these elements varied depending on the model. Some models (e.g., LLaV A-1.5 and Qwen-VL-Chat) exhibited slight performance improvements when persona information or CoT prompts were added, whereas other models (e.g., InstructBLIP and LLaV A-Next) showed no significant differences or performance degradation. This suggests that persona information and CoT prompts may have different effects depending on the model architecture or pre-training data. However, considering that the overall performance improvement was not substantial, the effects of these elements appear to be limited. Therefore, future research should explore means of using persona information and CoT prompts more effectively.\\n\\n6 Analysis\\n\\n6.1 How do different prompts affect the overall emotion prediction performance?\\n\\nWe analyzed the performance for each emotion in the emotion prediction. As shown in Figure 2, all prompt types showed the highest performance in predicting the \u201cjoy\u201d emotion, with the speaking styles prompt achieving the best result of 50.82%. This suggests that the tone and style of conversation play an important role in predicting positive emotions. The personality traits prompt also showed high performance in predicting \u201cjoy,\u201d at 49.98%, indicating that individual personality traits are crucial elements in understanding and expressing joy. These results demonstrate that the model can predict positive emotions more accurately based on the personality and speaking style of the speaker.\\n\\nIn contrast, all prompt types showed relatively lower performance in predicting \u201cfear\u201d than other emotions. In the case of the speaking styles prompt, the performance for predicting the \u201cfear\u201d emotion was the lowest among all emotions, at 39.58%, and similar trends were observed for the other prompt types. This indicates that predicting negative emotions such as fear is challenging. Fear may require complex and subtle contexts and the limitations of the model may be exposed when accurately predicting such emotions.\"}"}
{"id": "emnlp-2024-main-331", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Comparison of emotion prediction performance across different prompt types (original, personality traits, speaking styles, and CoT).\\n\\nIn addition, the prediction of the \\\"neutral\\\" emotion showed relatively low performance across all prompt types, particularly in the personality traits prompt, which had the lowest performance at 39.44%. This suggests that individual personality traits may add complexity to the process of discerning emotional neutrality. Neutral emotions are difficult to predict owing to the absence of clear positive or negative signals, indicating that additional research is required to improve the model performance to respond in situations in which clear emotional signals are lacking.\\n\\n6.2 How does emotion prediction performance differ based on the image scope?\\n\\nWe demonstrated the differences in emotion prediction performance based on the image scope, as shown in Figure 3. For \\\"joy,\\\" high performance was observed across all scopes, with the \\\"all\\\" scope achieving the best result at 48.69%. However, the performance of the \\\"face\\\" and \\\"person\\\" scopes was not significantly different, at 48.61% and 48.24%, respectively. This suggests that various cues, such as facial expressions, individuals, and the overall context, may be equally important when predicting joy.\\n\\nFor \\\"sadness,\\\" the \\\"face\\\" scope showed the highest performance at 47.34%, suggesting that facial expressions are a crucial factor in predicting sadness. However, for \\\"fear,\\\" the \\\"all\\\" scope exhibited the highest performance, at 41.64%. This implies that the overall image information can be helpful in the prediction of fear because it is an emotion that arises in complex contents.\\n\\nFor \\\"disgust,\\\" the \\\"face\\\" scope achieved the highest performance at 44.50%, whereas for \\\"surprise,\\\" the \\\"person\\\" scope showed the highest performance at 44.75%. This indicates that facial expressions and posture or movements of an individual can play important roles in predicting disgust and surprise, respectively. For \\\"anger,\\\" the performance difference between the image scopes was not significant, with the \\\"face\\\" scope showing a slightly higher level at 44.84.\\n\\nIn contrast, \\\"neutral\\\" showed relatively low performance across all scopes, particularly in the \\\"face\\\" scope, which had the lowest performance at 40.78%. This suggests that facial expressions alone may not provide sufficient cues for predicting neutral emotions. The \\\"all\\\" scope showed the highest performance at 41.40%, but this low level suggests that more sophisticated context analysis may be necessary to predict neutral emotions accurately.\\n\\n6.3 Is the emotion prediction performance of the model influenced by gender?\\n\\nIn this section, we analyze whether differences in emotion prediction performance occurred based on gender. The experimental results presented in Figure 4 clearly show how the emotion prediction performance varied depending on the gender of the subject that the model aimed to predict. The results revealed that the emotion prediction performance of female was higher than that of male for most emotions. Notably, for the \\\"disgust\\\" emotion, the prediction performance for females (54.21%) was significantly superior to that for males (34.78%). A detailed analysis is provided in Appendix E.\\n\\nFor the \\\"joy\\\" and \\\"surprise\\\" emotions, the prediction performance for females was also higher at 50.59% and 46.19%, respectively, compared to males (46.63% and 41.68%, respectively). This...\"}"}
{"id": "emnlp-2024-main-331", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"implies that positive emotions such as \u201cjoy\u201d and \u201csurprise\u201d may be more prominently expressed by females. In contrast, when recognizing the \u201csadness\u201d emotion, the performance for males (48.77%) was higher than that for females (45.90%). This suggests that male emotional expressions may be better recognized by the model when identifying \u201csadness.\u201d In the recognition of the \u201canger\u201d and \u201cneutral\u201d emotions, the performance difference between males and females was not significant, indicating that the expression differences of \u201canger\u201d and \u201cneutral\u201d based on gender may be relatively small.\\n\\n6.4 Do regional biases influence the emotion prediction performance?\\n\\nThe experimental results presented in Figure 5 clearly demonstrate the impact of regional biases on the emotion prediction performance of the model. According to the analysis, most regions showed a tendency for the model performance to degrade when regional persona information was added. In particular, for the Middle East and Africa, the performance decreased by -2.40% and -2.20%, respectively, compared to the original prompt, indicating that regional biases had a negative impact on the emotion prediction performance. Performance degradation was also observed for East Asia (-1.90%), South Asia (-1.87%), and Nordic countries (-1.71%).\\n\\nIn contrast, North America was the only region for which the performance improved by +0.07%. This suggests that the data used to train the model reflect the characteristics of the North American region relatively well. For Latin America and Western Europe, the performance decreases were relatively small, at -1.28% and -1.02%, respectively; however, they still appeared to be influenced by regional biases. Additional details are provided in Appendix F.\\n\\n7 Conclusion\\n\\nThis study has systematically analyzed the key factors influencing the emotion prediction performance of VLLMs. The experimental results showed that the model architecture and size, particularly the LLM Backbone, had the most significant impact. The integration of persona information and CoT prompts exhibited varying effects depending on the model, and differences in the prediction performance were observed based on the image scope for each emotion. However, biases in emotion prediction performance based on gender and region were identified, indicating the need for efforts to mitigate these biases. Future research should focus on developing emotionally intelligent VLLMs by minimizing data and model biases using advanced dataset composition and model training methods.\"}"}
{"id": "emnlp-2024-main-331", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThis work was supported by Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (RS-2024-00343989, Enhancing the Ethics of Data Characteristics and Generation AI Models for Social and Ethical Learning) and (RS-2024-00398115, Research on the reliability and coherence of outcomes produced by Generative AI).\\n\\nLimitations\\n\\nAlthough this study provides valuable insights into the factors that influence the emotion prediction performance of VLLMs, it has some limitations that should be acknowledged. First, we excluded high-performing models, such as GPT-4, from our detailed analysis because their internal structures and model sizes have not been publicly disclosed. Although these models are likely to employ advanced architectures that can further our understanding of emotion prediction, their lack of transparency makes it difficult to analyze the specific factors that contribute to their performance systematically. However, as more information on these models becomes available, future research should investigate their emotion prediction capabilities in relation to the factors identified in this study.\\n\\nSecond, although our experiments revealed the presence of gender and regional biases in the emotion predictions of VLLMs, proposing comprehensive solutions to these biases is beyond the scope of this study. Addressing these biases is crucial for developing fair and unbiased VLLMs, and we strongly encourage future research to focus on mitigating these issues.\\n\\nFinally, it is important to note that although the MELD dataset, which is based on the TV series Friends, reflects many real-world emotional situations, it may not capture the full range of emotions and contexts that are present in human interactions. Although TV shows are designed to mirror real life, they are ultimately scripted and may not always represent the spontaneity and complexity of real-world conversations. Future research could expand the scope of this study by incorporating datasets from diverse sources, such as real-world conversations, to validate and generalize our findings further.\\n\\nDespite these limitations, we believe that our study provides a solid foundation for understanding the factors that influence the emotion prediction performance of VLLMs. We have identified key areas for future research and development in this field by systematically analyzing the effects of the model architecture, persona information, and various biases. As VLLMs continue to advance, it will be crucial to address these limitations and build emotionally intelligent models that can understand and respond to human emotions in a fair and unbiased manner.\\n\\nReferences\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716\u201323736.\\n\\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. 2023. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390.\\n\\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.\\n\\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.\\n\\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. 2024. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in Neural Information Processing Systems, 36.\\n\\nSander L Koole. 2010. The psychology of emotion regulation: An integrative review. Cognition and emotion, pages 138\u2013177.\\n\\nRichard S Lazarus. 2000. How emotions influence performance in competitive sports. The sport psychologist, 14(3):229\u2013252.\\n\\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. 2023a. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425.\\n\\nCheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. 2023b. Large language models understand and can be enhanced by emotional stimuli. arXiv preprint arXiv:2307.11760.\"}"}
{"id": "emnlp-2024-main-331", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Dataset Statistics\\nWe maintained the emotion ratios used in the original MELD dataset and sampled emotions to construct a final dataset of 1,112 instances. The proportions of sentiments and emotions used in the data composition can be found in Figure 6 and Figure 7.\\n\\nB Main Results for Easy and Hard Difficulty Levels\\nIn this section, we present the performance variations of the models based on the difficulty levels of the emotion prediction tasks. Table 2 showcases the detailed results for both the easy and hard difficulty levels. These findings provide valuable insights into the capabilities of the models in understanding and processing emotions in diverse situational complexities.\\n\\nC Model Hyperparameter Configuration\\nWe conducted experiments on all models using the same hyperparameter values. Specifically, we set the do_sample parameter uniformly to False across all experiments.\\n\\nD Prompt details\\nWe conducted tests using the following three prompts to reduce the influence of prompts on the models:\\n\\n\u2022 Given a conversation involving multiple speakers and an associated image, select the most appropriate statement for the final speaker in the conversation. Consider the context, sentiment, and emotions conveyed in the dialogue.\"}"}
{"id": "emnlp-2024-main-331", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In addition, we used corresponding persona information as an additional input to analyze the differences in emotional prediction performance according to personality traits and speaking styles. We utilized the Friends Fandom Wiki to generate persona information, providing the relevant data as input to GPT-4 to create the persona information. Considering the maximum token limit for specific models, we only added two persona information inputs.\\n\\nThe overall prompt can be found in Figure 9.\\n\\nE Analysis of \u201cDisgust\u201d Emotion Prediction Differences by Gender\\n\\nThe conversational samples in Figures 10 and 11 reveal notable differences in how males and females express the emotion of \u201cdisgust.\u201d In the female samples, disgust is often expressed through strong exclamations such as \u201cOh my God!\u201d and \u201cEwww!\u201d (Figure 10, Samples 1 and 3). These expressions suggest a more overt and emphatic display of the \u201cdisgust\u201d emotion by females. In addition, female characters tend to provide more detailed descriptions of the disgusting situation, such as \u201cShe\u2019s got her tongue in his ear\u201d (Figure 10, Sample 2), which vividly conveys their sense of revulsion.\\n\\nIn contrast, the male samples show a relatively more subdued expression of \u201cdisgust.\u201d For instance, in Figure 11, Sample 1, the male character expresses his aversion to drinking breast milk in a more matter-of-fact manner, stating, \u201cNot even if Carol\u2019s breast had a picture of a missing child on it.\u201d While still conveying disgust, the expression is less emotionally charged compared to the female samples. Similarly, in Figure 11, Sample 3, the comment by the male character, \u201cOK, is there a mute button on this woman?\u201d suggests annoyance and disgust, but lacks the same level of overt expression that appears in the female samples.\\n\\nThese differences in the expression of disgust between males and females could potentially explain the higher performance in predicting \u201cdisgust\u201d for female (54.21%) compared to male (34.78%).\"}"}
{"id": "emnlp-2024-main-331", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"more explicit and emphatic expressions of disgust by females may provide clearer cues for the VLLMs to identify the emotion accurately. However, it is important to acknowledge the limitations of this analysis. The conversational samples provided, while based on a TV show reflecting many real-world situations, do not fully capture the entire spectrum of \\\"disgust\\\" emotion expressions that occur in real-life interactions. In addition, the differences observed in these specific samples may be influenced by individual character traits and situational contexts, rather than being solely attributable to gender.\\n\\nA more comprehensive study with a larger and more diverse dataset would be necessary to draw more definitive conclusions regarding gender-based differences in \\\"disgust\\\" emotion expression. Such a study should consider various factors, including individual personality traits, cultural backgrounds, and conversational contexts, to determine whether the observed differences are truly representative of gender-based patterns or whether other factors play a more significant role.\\n\\nIn summary, while the analysis of the provided conversational samples suggests potential differences in how males and females express disgust, further research is required to establish the extent to which these differences are generalizable across a wider population and to determine the relative influence of gender compared to other factors in shaping \\\"disgust\\\" emotion expression.\\n\\nRegional Bias Problem in Emotion Prediction of VLLMs\\n\\nThis study identified a general trend towards decreased emotion prediction performance when persona prompts containing regional information were provided to the models. This suggests that the models may inherently hold prejudices or stereotypes towards specific regions.\\n\\nThe prompts used in the experiments were structured as follows:\\n\\n- Last speaker's characteristics:\\n  1. The last speaker has lived in ## throughout their life, deeply rooted in the language, religion, and customs of that region.\\n  2. The last speaker uses the communication style commonly employed in ## to interact with others.\\n\\nIn the above, ## was replaced with the corresponding region name. These prompts provided the model with the information that the last speaker is deeply connected to the language, religion, customs, and communication style of that region.\\n\\nHowever, the research results showed that the emotion prediction performance of the model deteriorated when such regional information was provided, demonstrating the possibility that the models harbor stereotypes or biases towards specific regions. These models may make inappropriate assumptions based on the regional information provided through prompts, leading to inaccurate emotion predictions.\\n\\nThis finding is directly related to the fairness and bias issues in VLLMs. If the models make biased predictions about specific regions, this can lead to unfair treatment of individuals in those regions. Therefore, future research is necessary to minimize such biases and enhance the fairness of the models.\\n\\n**How do different prompts affect the overall sentiment prediction performance?**\\n\\nWe analyzed the impact of various prompt types on the overall sentiment prediction performance of the models systematically, based on the experimental results presented in Figure 8. The results revealed that the inclusion of persona information, such as personality traits and speaking styles, influenced the sentiment prediction performance significantly, with notable variations observed across different sentiments. For the recognition of \\\"positive\\\" sentiments, the models exhibited a substantial improvement in performance when persona information was incorporated. In contrast, for the recognition of \\\"negative\\\" sentiments, the original prompt, which did not include any persona information, recorded the highest performance. When it comes to \\\"neutral\\\" sentiments, the models showed mixed performance, with some prompts leading to better predictions than others.\\n\\nFigure 8: Changes in sentiment prediction performance according to various prompts from a specific region and is deeply connected to the language, religion, customs, and communication style of that region.\"}"}
{"id": "emnlp-2024-main-331", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tral\\\" sentiments, the CoT prompt, which involved a systematic thought process, demonstrated the highest performance.\\n\\nThe experimental results showed that the inclusion of persona information had mixed effects on the sentiment prediction performance of the models, with the overall performance improvement being limited. While the personality prompt showed promising results for positive sentiments, it did not benefit all sentiment prediction tasks consistently. Similarly, the speaking styles prompt, although effective for positive sentiments, did not yield significant improvements in the recognition of negative or neutral sentiments. These findings suggest that the impact of persona information on sentiment prediction performance varies depending on the specific emotion being analyzed.\\n\\nOur analysis highlights the importance of considering the interplay between persona information and sentiment prediction in conversational contexts. While the inclusion of personality traits and speaking styles can enhance the models' understanding of certain sentiments, such as positive sentiments, its impact is not uniform across all sentiment categories. Further research is needed to explore more sophisticated approaches for integrating persona information into sentiment prediction tasks, taking into account the nuances and challenges associated with different emotional expressions.\"}"}
{"id": "emnlp-2024-main-331", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Example of the prompt template used for testing. This figure illustrates the detailed structure of the prompt used in our experiments, including sections for instruction, historical content, personality traits, speaking styles, response options, and the CoT. This comprehensive prompt format ensures that the model evaluates multiple aspects of context and persona information to determine the most appropriate response.\"}"}
{"id": "emnlp-2024-main-331", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model   | Prompt type | Original | InstructBLIP | Vicuna (13B) | Vicuna (7B) | Mistral (7B) | Qwen (7B) | MiniGPT-4 | Otter | FLAN (3B) | FLAN (11B) | Qwen-VL-Chat |\\n|---------|-------------|----------|--------------|--------------|--------------|--------------|-----------|-----------|-------|-----------|-----------|-------------|\\n|         |             |          |              |              |              |              |           |           |       |           |            |             |\\n| **Tone Emotion Context** |             |          |              |              |              |              |           |           |       |           |            |             |\\n| All Person Face |              |          |              |              |              |              |           |           |       |           |            |             |\\n| All Person Face |              |          |              |              |              |              |           |           |       |           |            |             |\\n| All Person Face |              |          |              |              |              |              |           |           |       |           |            |             |\\n| **Prompt type** |             |          |              |              |              |              |           |           |       |           |            |             |\\n| **Personality traits** |             |          |              |              |              |              |           |           |       |           |            |             |\\n| InstructBLIP |              |          |              |              |              |              |           |           |       |           |            |             |\\n| **Speaking styles** |             |          |              |              |              |              |           |           |       |           |            |             |\\n| InstructBLIP |              |          |              |              |              |              |           |           |       |           |            |             |\\n| **CoT** |             |          |              |              |              |              |           |           |       |           |            |             |\\n| InstructBLIP |              |          |              |              |              |              |           |           |       |           |            |             |\\n| **Easy** |             |          |              |              |              |              |           |           |       |           |            |             |\\n| InstructBLIP |              |          |              |              |              |              |           |           |       |           |            |             |\\n| **Hard** |             |          |              |              |              |              |           |           |       |           |            |             |\\n| InstructBLIP |              |          |              |              |              |              |           |           |       |           |            |             |\\n\\nTable 2: Comparative performance analysis of VLLM models using different prompt types (original, personality traits, speaking styles, and CoT) for both easy and hard difficulty levels. The results, presented as accuracy scores, are averaged across three distinct prompts for each prompt type and are reported separately for Easy and Hard difficulties, allowing for a more detailed comparison of model performance across different complexity levels.\"}"}
{"id": "emnlp-2024-main-331", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Example dialogues in which female characters express \\\"disgust\\\" in the final utterance.\\n\\nFigure 11: Example dialogues in which male characters express \\\"disgust\\\" in the final utterance.\"}"}
