{"id": "emnlp-2023-main-275", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Standardizing Distress Analysis: Emotion-Driven Distress Identification and Cause Extraction (DICE) in Multimodal Online Posts\\n\\nGopendra Vikram Singh*, Soumitra Ghosh*, Atul Verma, Chetna Painkra and Asif Ekbal\\nDepartment of Computer Science and Engineering, IIT Patna, India\\n{gopendra_1921cs15,asif}@iitp.ac.in, {ghosh.soumitra2,atul.verma.a3,chetnapaikra55}@gmail.com\\n\\nAbstract\\nDue to its growing impact on public opinion, hate speech on social media has garnered increased attention. While automated methods for identifying hate speech have been presented in the past, they have mostly been limited to analyzing textual content. The interpretability of such models has received very little attention, despite the social and legal consequences of erroneous predictions. In this work, we present a novel problem of Distress Identification and Cause Extraction (DICE) from multimodal online posts. We develop a multi-task deep framework for the simultaneous detection of distress content and identify connected causal phrases from the text using emotional information. The emotional information is incorporated into the training process using a zero-shot strategy, and a novel mechanism is devised to fuse the features from the multimodal inputs. Furthermore, we introduce the first-of-its-kind Distress and Cause annotated Multimodal (DCaM) dataset of 20,764 social media posts. We thoroughly evaluate our proposed method by comparing it to several existing benchmarks. Empirical assessment and comprehensive qualitative analysis demonstrate that our proposed method works well on distress detection and cause extraction tasks, improving F1 and ROS scores by 1.95% and 3%, respectively, relative to the best-performing baseline. The code and the dataset can be accessed from the following link: https://www.iitp.ac.in/~ai-nlp-ml/resources.html#DICE.\\n\\n1 Introduction\\nThe exponential expansion of microblogging sites and social media not only empowers free expression and individual voices, but also allows individuals to exhibit anti-social conduct (ElSherief et al., 2018), such as cyberbullying, online rumours, and spreading hate remarks (Ribeiro et al., 2018). Abusive speech based on race, religion, and sexual orientation is becoming more common (Karim et al., 2020). Automatic identification of hate speech and raising public awareness are critical tasks (Karim et al., 2020). Manually evaluating and validating a large volume of web information, on the other hand, is time-consuming and labor-intensive.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"distressed and extracting the reasons for the classification decision (for the Distressed class) from the textual input. The prime focus of this study is to comprehend the causes associated with any form of offensive content (hate, offensive, abusive, etc.). We club all the connotations of offensive content under the category distressed.\\n\\nThe main contributions are summarized below:\\n\\n1. We propose the novel task of Unified Distress Identification and Cause Extraction (DICE) from multimodal online posts.\\n2. We develop a multi-task deep framework for the simultaneous detection of distress content and identify connected causal phrases from the text using emotional information.\\n3. We devise a zero-shot strategy to dynamically incorporate emotional information into training and propose a novel fusion mechanism to infuse the features of multimodal inputs.\\n4. The first Distress and Cause annotated Multimodal (DCaM) corpus is created consisting over 20,764 social media posts.\\n5. Resources are open-sourced to aid research.\\n\\nThe rest of the paper is organized as follows. Section 2 summarises some previous works in this area. We discuss the dataset preparation in Section 3. Section 4 addresses our proposed methodology in depth, followed by the results and analysis in Section 5. Finally, we conclude our discussion in Section 6 and define the scope of future work.\\n\\n2 Related Work\\n\\nSeveral approaches have been suggested to identify online hate speech (Burnap and Williams, 2016; Zhang et al., 2018; Qian et al., 2018). The current interest in hate speech research has led to the availability of datasets in several languages (Sanguinetti et al., 2018; Ousidhoum et al., 2019) and different computational ways to counteract online hate (Mathew et al., 2019; Aluru et al., 2020). Text-, user-, and network-based traits and characteristics that identify bullies have been extracted in (Chatzakou et al., 2017). Deep learning Lundberg and Lee (2017); Founta et al. (2019) has been used extensively to identify hate speech keyword identification, sexism, bullying, trolling, and racism. Recent research on identifying hate speech has made use of deep learning techniques, including neural networks (Han and Eisenstein, 2019) and word embedding techniques (McKeown and McGregor, 2018). Recent models based on Transformers (Vaswani et al., 2017) have had extraordinary success. Since this is essentially a classification problem, BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018) has found widespread use in the field of hate speech identification. Ranasinghe et al. (2019) showed that a BERT-based model performed better than models based on recurrent neural networks (RNNs). Zaidan et al. (2007) first proposed the use of rationales, where human annotators highlight text that supports their classification decision. This work was enhanced by Yessenalina et al. (2010) to provide self-generating rationales. An encoder-generator system for quality rationales without annotations was presented in Lei et al. (2016). Mathew et al. (2021) used dataset rationales to fine-tune BERT to address bias and explainability. Recent research has shifted towards accommodating multimodal content, with a focus on detecting hate speech and objectionable material in various media. Gandhi et al. (2019) developed a computer vision-based technique for identifying offensive and non-offensive images in large datasets. Kiela et al. (2020) introduced a novel challenge for multimodal hate speech detection in Facebook memes. Rana and Jha (2022) employed the Hate Speech Recognition Video Dataset to identify emotion-based hate speech in a multimodal context. Karim et al. (2022) presented a dataset for detecting hate speech in Bengali memes and text. Fersini et al. (2022) discussed SemEval-2022 Task 5, focusing on identifying misogynous memes through text and images, including sub-tasks for recognizing misogynous content and categorizing types of misogyny. Hee et al. (2022) investigated multimodal hateful meme detection models and their ability to capture derogatory references in both images and text. Additionally, Cao et al. (2022) introduced PromptHate, a model that leverages pre-trained language models with specific prompts and examples for hateful meme classification.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets | Labels | Total Size | Language | Multimodal? | Rationales?\\n---|---|---|---|---|---\\nWaseem and Hovy (2016) | Racist, Sexist, Normal | 16,914 | English | x | x\\nDavidson et al. (2017) | Hate Speech, Offensive, Normal | 24,802 | English | x | x\\nFounta et al. (2018) | Abusive, Hateful, Normal, Spam | 80,000 | English | x | x\\nOusidhoum et al. (2019) | Labels for five different aspects | 13,000 | English, French, Arabic | x | x\\nMathew et al. (2021) | Hate Speech, Offensive, Normal | 20,148 | English | x | \u2713\\nDCaM (ours) | Distressed (Hate-Offensive-Abusive), Non-distressed (causes) | 20,764 | English | \u2713 | \u2713\\n\\n### 3 Dataset\\n\\nWe discuss the data collection and annotation details in the following subsections.\\n\\n#### 3.1 Data Collection\\n\\nWe collect our dataset from sources where previous studies (Davidson et al., 2017; Zannettou et al., 2018; Mathew et al., 2021) on hate speech have been conducted: Twitter and Gab. The data was scraped from the top 5 trending topics on Twitter using selenium to reduce the effects of sample bias. As for Twitter, we selected the top 10 percent of all collected tweets between October 2022 and December 2022. Using the textual mode of scraped tweets, we generated a list of the most frequent words, which we then used as tags to gather the posts from Gab. Please refer to Appendix Section A.1 for details on data collection from Gab, including keywords used for the DCaM dataset (see Table 8). To compile this data, we scoured Gab for posts between November and December 2022. Posts that have been deleted and reposted are not considered. We also remove links from posts to ensure that annotators can access all relevant information. A number of distress datasets are compared in Table 1.\\n\\n#### 3.2 Data Annotation\\n\\nTo ensure the dataset consists of only English posts, we used the TextBlob library for language detection and included only those identified as English. Additionally, non-English posts were flagged and excluded during annotation. Annotators were informed about the presence of hate or offensive content beforehand. Annotation guidelines from Poria et al. (2021); Ghosh et al. (2022c) were provided to assist annotators in understanding the classification and span annotation tasks. Each post was annotated by five annotators (DI task), and then majority voting was applied to decide the final label.\\n\\nThere are two kinds of annotations in our dataset. First, whether the post is Distressed or Non-distressed post. Second, if the text is considered as Distressed by majority of the annotators, we ask the annotators to highlight parts of the text that include terms that might be a plausible basis for the provided annotation. These span annotations help us to delve further into the manifestations of hatred or offensive speech.\\n\\n| Dataset | Distressed | Non-distressed | Total |\\n|---|---|---|---|\\n| Twitter | 3248 | 5210 | 8458 |\\n| Gab | 7066 | 5240 | 12306 |\\n| Total | 10314 | 10450 | 20764 |\\n\\nTable 2: Dataset details\\n\\nFor the Distressed Identification task, the Krippendorff\u2019s $\\\\alpha$ for the inter-annotator agreement is 0.66 which is much higher than other hate speech tasks.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Methodology\\n\\nIn this section, we illustrate our proposed DICE framework, which is a multitask system for Depression Identification and Cause Extraction from multimodal social media posts. The system employs a zero-shot strategy to dynamically incorporate emotional information into training and presents a novel fusion mechanism to infuse the features from the multimodal inputs. The overall architecture of the proposed method is shown in Figure 3a.\\n\\n4.1 Problem Formulation\\n\\nGiven a post \\\\( P = [s_1, \\\\cdots, s_i, \\\\cdots, s_p] \\\\) composed of a sequence of sentences (s), and each utterance can be further decomposed into a sequence of words. \\\\( p \\\\) indicates the number of sentences in the post. The objective is to determine if the post is distressed or not (0 or 1) and to extract every plausible causal span that supports the prediction.\\n\\n4.2 Proposed DICE Framework\\n\\nTextual Encoder. Our textual encoder uses BERT followed by an ontology-based word graph. BERT extracts local information from a text. Ontology is the backbone of knowledge graphs (KGs) (Song et al., 2022), which give meta-data descriptions to guide the creation and completion of knowledge graphs. Additionally, relation descriptions contain semantic information that can be used to represent relations. During Graph Neural Network (GNN) message transmission, we embed text within ontology nodes. First, all the nodes are embedded using node embedding and text embedding as follows:\\n\\n\\\\[\\nh_o = h_o W_e \\\\]\\n\\n\\\\[\\nh_t = \\\\sum_{n=1}^{N} x_i W_e (1)\\n\\\\]\"}"}
{"id": "emnlp-2023-main-275", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $W_E$ is word text embedding (BERT), $W_o$ is graph embedding, $x_i$ depicts a node (representing a word), $h_0$ is a concept in ontology. Figure 3b illustrates the interaction between the vocab graph and BERT embedding to establish relationships. Our method enriches the text-embedding and graph-embedding space, enabling the identification of previously unseen relationships between graph embeddings of the head and tail.\\n\\n$$r_a = \\\\sum_{n=1}^{N} g(h)$$\\n\\nwhere, $r_a$ is aggregate relationship, $g(*)$ is aggregate function, and $N$ is neighboring nodes for the missing node.\\n\\nImage Encoder. We use ResNet 5 to capture facial expressions and visual surroundings for rich emotional indicators from the image in the input post. We separated the embedding dimensions and image data into groups to simplify the problem and make better use of the complete embedding space. Each learner will create a unique distance metric using just a subspace of the original embedding space and a portion of the training data. By segmenting the network's embedding layer into $D$ consecutive slices, we are able to isolate $D$ unique learners inside the embedding space. After learner solutions converge, we aggregate them to obtain the whole embedding space. The merging is accomplished by recombining the slices of the embedding layer that correspond to the $D$ learners. To ensure uniformity in the embeddings produced by various learners, we then perform fine-grained tuning across the entire dataset. The merged embeddings may be hampered by the gradients, which resemble white noise and would hinder training performance. This is called the \\\"shattered gradients problem\\\". To address this, residual weights (Balduzzi et al., 2017) provide the gradients with some spatial structure, which aids in training, as shown in Figure 3b.\\n\\nInter-modal Fusion (IMF). The IMF module exchanges information and aligns entities across modalities (text and image) to learn joint inter-modality representations. Figure 4 illustrates the mechanism of inter-modal fusion. Text infused visual features (and vice-versa). We use an external word embedding model to build high-level representations ($T'_i$) for an image-text pair consisting of $I_i$ and $T_i$. Cross attention is employed to combine the textual and visual features to create the Text infused visual features ($T_V$). Taking into account the spatial properties of the channel-wise features, the query vectors (Q) are generated by convolution with $N*k$ kernels on each channel of $I_i$ and then averaging (avg pooling) the feature maps as illustrated in Figure 4. Similarly, we construct the Visual infused textual features ($V_T$) by exchanging $I_i$ and $T_i$. In particular, the key vectors (K) are produced by convolution with $N*k$ kernels on each channel of $I_i'$ and then averaging (average pooling) the feature maps.\\n\\nCross-Attention. First, we take the query vector from one modality (say image, $I_i$) and the key/value pair from the other (say text, $T_i$). To examine how text affects the image vector, we feed the query ($I_q$) and textual key/value to self-attention ($selfAtt(T_k, T_v, I_q)$).\\n\\n$$I_q = Query(I)$$\\n$$T_k, T_v = Key(T), Value(T)$$\\n$$S_A = selfAtt(T_k, T_v, I_q)$$\\n\\nWe filter noise from the output of the self-attention using the forget gate ($\\\\sigma$) and concatenate it with the linear layer's residual (c.f. Figure 4).\\n\\n$$G_{TI} = Concat(linear(S_{TI}), \\\\sigma(linear(S_{TI})))$$\\n\\nFinally, we pass the representations of all the modalities (i.e., text, and image) through another self-attention to know how much the image vector will be impacted by text [$Cross_{TI} = SA(G_{TI}, I_q)$]. Please note that bolded $I$ in $Cross_{TI}$ represents the impacted modality (i.e., $I$). Similarly, we compute $Cross_{IT}$ and concatenate all of them to obtain the cross-attentive multimodal features.\\n\\nFinal Fusion. Although, the $T_V$ and $V_T$ can independently conduct image-text multimodal recognition, to further enhance the model's performance, we apply self-attention to fuse the two aforementioned feature vectors.\\n\\nClass Penalty. The inter-modal fusion unit receives a class penalty value to help the model understand the link between a unified distress label and the input post. This improves the prediction of start and end tokens. The equations below originate from...\"}"}
{"id": "emnlp-2023-main-275", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ L = -\\\\frac{1}{b_s} \\\\sum_{i=1}^{b_s} \\\\log \\\\exp(W_{l_i} + b_i) \\\\sum_{j=1}^{N} \\\\exp(W_{l_j} + b_j) \\\\] (5)\\n\\nWhere, \\\\( l_i \\\\in \\\\mathbb{R}^d \\\\) is the feature of \\\\( i \\\\)th sample; \\\\( b_s \\\\) is batch size; \\\\( b_i \\\\) and \\\\( b_j \\\\) denote the bias; and \\\\( W \\\\in \\\\mathbb{R}^{d \\\\times n} \\\\) denotes the weight matrix. Information extraction tasks are notoriously difficult to find the decision boundary for the start and end markers of a span, and a basic softmax/sigmoid classifier cannot manage this distinction. Some samples may be misclassified due to the classification boundary's ambiguity. This may require a faster convergence rate. We use the Insightface loss technique (Deng et al., 2019) to normalize the feature and weight matrices. It assesses feature similarity based on the angle difference by which it maps the vector more closely. To converge the feature, it adds a penalty value to the angle.\\n\\n\\\\[ L_u^1 = -\\\\frac{1}{b_s} \\\\sum_{i=1}^{b_s} \\\\log \\\\exp(a(\\\\cos(\\\\theta + x))) \\\\sum_{j=1}^{N} \\\\exp(a(\\\\cos(\\\\theta))) \\\\] (7)\\n\\n\\\\[ L_u^2 = -\\\\frac{1}{b_s} \\\\sum_{i=1}^{b_s} \\\\exp(a(\\\\cos(\\\\theta + x))) + \\\\exp(a(\\\\cos(\\\\theta))) \\\\] (8)\\n\\nwhere \\\\( L_u^1 \\\\) and \\\\( L_u^2 \\\\) is updated loss functions for softmax and sigmoid, respectively, \\\\( \\\\theta \\\\) denotes the angle between weight \\\\( W \\\\) and feature \\\\( l_i \\\\) and \\\\( a \\\\) denotes the amplifier function.\\n\\n**Emotion Features.** We consider Ekman's (Ekman, 1992) emotion classes and initialize them with the BERT (Devlin et al., 2018) vectors to represent their semantic features.\\n\\n**Reconstruction Loss.** An auto-encoder reconstructs adjective-noun pair (ANP) features and produces latent features while maintaining emotion information in the learned latent space to match label and ANP feature structures. By optimizing the following loss function, the auto-encoder input \\\\( A \\\\) and output \\\\( \\\\hat{A} \\\\) must be sufficiently close to identify its parameters.\\n\\n\\\\[ L_{re} = \\\\left| \\\\left| \\\\hat{A}(\\\\text{IMF}(a,t)) - A(\\\\text{IMF}(a,t)) \\\\right| \\\\right|^2 \\\\] (9)\\n\\nAlso, optimizing this loss results in lower-dimensional input features and high-accuracy feature reconstruction.\\n\\n**Adversarial loss.** Our objective is to maintain the discriminative capacity of the combined features. To begin, we employ mid-level semantic representations of ANP features for the creation of an intermediary latent space. When provided with a training image, we opt for the application of the pre-trained ANP detector, DeepSentiBank (Chen et al., 2014), to extract the ANP feature. To establish a proficient latent space conducive to a concise representation of the original affective features, we embrace the utilization of an auto-encoder model.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results of the DICE framework on the DCaM dataset. Values in bold are the maximum scores attained.\\n\\n| Modality | Distress Identification | Cause Extraction |\\n|----------|-------------------------|------------------|\\n|          | F1 (%)                  | ACC. (%)         |\\n| DICE (T) | 86.12                   | 86.54            |\\n|          | FM                      | 38.74            |\\n|          | PM                      | 40.51            |\\n|          | HD                      | 0.66             |\\n|          | JF                      | 0.82             |\\n|          | ROS                     | 0.84             |\\n| DICE (I) | 70.15                   | 72.11            |\\n|          | FM                      | 28.41            |\\n|          | PM                      | 31.28            |\\n|          | HD                      | 0.52             |\\n|          | JF                      | 0.71             |\\n|          | ROS                     | 0.71             |\\n| ViL-BERT CC | 83.75               | -                |\\n|          | -                       | -                |\\n|          | -                       | -                |\\n|          | -                       | -                |\\n|          | Visual BERT COCO | 85.27             |\\n|          | -                       | -                |\\n|          | -                       | -                |\\n|          | -                       | -                |\\n|          | DICE (T+I)              | 87.71            |\\n|          | FM                      | 41.31            |\\n|          | PM                      | 45.48            |\\n|          | HD                      | 0.69             |\\n|          | JF                      | 0.85             |\\n|          | ROS                     | 0.88             |\\n\\n(a) Results across different modalities. Here, T: Text, I: Image\\n\\n(b) Results of human evaluation. Here, KC: Knowledge Consistency, Inf: Informativeness, F: Fluency\\n\\nTable 3 shows the results of the proposed DICE framework on the introduced DCaM dataset. Specifically, we show the modality-varying results of the proposed DICE framework on the introduced DCaM dataset.\\n\\n4.2.1 Calculation of Final Loss\\nAs illustrated in equation 9, the model is trained using a unified loss function. For both the DI and CE tasks, we employ binary cross-entropy loss.\\n\\n\\\\[ L = \\\\sum_\\\\omega W_\\\\omega L_\\\\omega \\\\] (9)\\n\\nHere, \\\\( \\\\omega \\\\) represents the two tasks, DI and CE. The weights (\\\\( W_\\\\omega \\\\)) are updated using back-propagation for specific losses for each task.\\n\\n5 Experiments and Results\\nThis section discusses the results and the analysis. Due to space constraints, we discuss the experimental setup in Section A.3 and the evaluation metrics in Section A.5.1 in the Appendix.\\n\\n5.1 Baselines\\nOur framework combines distress identification and cause extraction into a single automated system, utilizing classification and span detection. Due to the lack of suitable multimodal baselines with similar objectives, existing automated systems were used for evaluation. We compare our proposed DICE approach and the presented DCaM dataset against various baselines, including BiRNN-Attn (Liu and Lane, 2016), CNN-GRU (Zhang et al., 2018), BiRNN-HateXplain (Mathew et al., 2021), BERT (Liu et al., 2019a), BERT-HateXplain (Mathew et al., 2021), SpanBERT (Liu et al., 2019b), and CMSEKI (Ghosh et al., 2022b).\\n\\nTo thoroughly evaluate our approach on multimodal inputs, we employed two widely-used multimodal baselines, ViLBERT CC (Lu et al., 2019) and Visual BERT COCO (Li et al., 2019), to assess the distress identification task in our dataset. We discuss the baselines briefly in Section A.4 of the Appendix.\\n\\n5.2 Results and Analysis\\nTable 3 shows the results of the proposed DICE framework on the introduced DCaM dataset.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Results from the DICE model and the various baselines. Here, the bolded values indicate maximum scores.\\n\\nTable 3a. The bi-modal (Text+Image) configuration yields the best results, followed by the uni-modal network. The textual modality outperforms the others when compared independently, as texts have less background noise than visual sources.\\n\\nTable 5: Results of ablation experiments. The % fall in scores are shown in brackets. IMF: Inter-Modal Fusion, AE: Autoencoder, DS: DeepSentiBank, VG: Vocab Graph, TV: Text infused visual features, VT: Visual infused textual features.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Colorado school bus driver faces criminal charges for slapping a 10-year-old student in the face for not wearing a mask.\\n\\nIf you need #violence to defend against #Jewish ideas, your ideas aren't #terrorism they are #SelfDefence. Food For Thought.\\n\\nWe observe from Table 5 that removing the text-infused visual features (T_V) has a more detrimental effect on the system's performance compared to removing the visual infused text features (V_T). Next, we remove DeepSentiBank sahi kya h(DS) alongside IMF ([T+I]-IMF+DS), and, finally, we substitute the proposed IMF, DS and AE mechanism by linear concatenation to fuse multimodal features ([T+I]-IMF+DS+AE). We observe a notable fall in scores when either of these modules is removed from the DICE approach, especially when we remove the IMF+DS+AE module. This establishes that all components of the DICE model developed for multimodal data contribute to the success of the defined tasks in a zero-shot environment.\\n\\nWe thoroughly examined the predictions made by the different systems. Consider the examples in Table 6. The top row displays the tokens (or 'causes') that human annotators noted and that they consider representing the causes for the post being Distressed. The next four rows show the extracted tokens from the various models. We observe that the proposed DICE model correctly categorizes the examples as distressed and also extracts good-quality causal spans. In the second example, we observe that although the SpanBERT model extracts a partial causal span correctly, it assigns the wrong label (Non-distressed).\\n\\nIn this work, we present a novel problem of Distress Identification and Cause Extraction (DICE) from multimodal online posts. We develop a multi-task, deep framework for detecting distress content and identifying associated causal phrases from text using emotional information. We devise a zero-shot strategy to dynamically incorporate emotional information into training and propose a novel fusion mechanism to infuse the features of multimodal inputs. Furthermore, we introduce the first Distress and Cause annotated Multimodal (DCaM) corpus, consisting of over 20,764 social media posts. We illustrate the effectiveness of our method by comparing it to several state-of-the-art baselines. When compared to human performance, the present state-of-the-art models perform poorly, which serves to emphasize the difficulty of the task at hand. We believe our work will advance multimodal reasoning and comprehension while also assisting in the resolution of a significant real-world problem.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations and Future Scope\\n\\nDue to the low prevalence of hate speech on social media (approximately 3% of messages are hateful), (Fortuna and Nunes, 2018), we scrape posts by searching for hate words to increase the likelihood of encountering hate-offensive content. This may have invited some undesired sampling bias while constructing the dataset. Additionally, emoticons and other non-standard symbols like $ are often used in current online interactions. One potential research direction is to use these neglected visual features of text information to adapt to more realistic settings.\\n\\nEthical Consideration\\n\\nWe created our resource using publicly accessible social media postings. We adhered to the data use guidelines and did not infringe on any copyright problems. Our Institutional Review Board also reviewed and approved this research. We make the code and data accessible for research purposes through an appropriate data agreement mechanism.\\n\\nReferences\\n\\nSai Saketh Aluru, Binny Mathew, Punyajoy Saha, and Animesh Mukherjee. 2020. Deep learning models for multilingual hate speech detection. arXiv preprint arXiv:2004.06465.\\n\\nDavid Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. 2017. The shattered gradients problem: If resnets are the answer, then what is the question? In International Conference on Machine Learning, pages 342\u2013350. PMLR.\\n\\nPete Burnap and Matthew L Williams. 2016. Us and them: identifying cyber hate on twitter across multiple protected characteristics. EPJ Data science, 5:1\u201315.\\n\\nRui Cao, Roy Ka-Wei Lee, Wen-Haw Chong, and Jing Jiang. 2022. Prompting for multimodal hateful meme classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 321\u2013332.\\n\\nDespoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini, and Athena Vakali. 2017. Mean birds: Detecting aggression and bullying on twitter. In Proceedings of the 2017 ACM on web science conference, pages 13\u201322.\\n\\nTao Chen, Damian Borth, Trevor Darrell, and Shih-Fu Chang. 2014. Deepsentibank: Visual sentiment concept classification with deep convolutional neural networks. arXiv preprint arXiv:1410.8586.\\n\\nThomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Proceedings of the international AAAI conference on web and social media, pages 512\u2013515.\\n\\nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. 2019. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690\u20134699.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\\n\\nPaul Ekman. 1992. An argument for basic emotions. Cognition & emotion, 6(3-4):169\u2013200.\\n\\nMai ElSherief, Vivek Kulkarni, Dana Nguyen, William Yang Wang, and Elizabeth Belding. 2018. Hate lingo: A target-based linguistic analysis of hate speech in social media. In Proceedings of the International AAAI Conference on Web and Social Media, volume 12.\\n\\nElisabetta Fersini, Francesca Gasparini, Giulia Rizzi, Aurora Saibene, Berta Chulvi, Paolo Rosso, Alyssa Lees, and Jeffrey Sorensen. 2022. Semeval-2022 task 5: Multimedia automatic misogyny identification. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 533\u2013549.\\n\\nPaula Fortuna and S\u00e9rgio Nunes. 2018. A survey on automatic detection of hate speech in text. ACM Computing Surveys (CSUR), 51(4):1\u201330.\\n\\nAntigoni Maria Founta, Despoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Athena Vakali, and Ilias Leontiadis. 2019. A unified deep learning architecture for abuse detection. In Proceedings of the 10th ACM conference on web science, pages 105\u2013114.\\n\\nAntigoni Maria Founta, Constantinos Djouvas, Despoina Chatzakou, Ilias Leontiadis, Jeremy Blackburn, Gianluca Stringhini, Athena Vakali, Michael Sirivianos, and Nicolas Kourtellis. 2018. Large scale crowdsourcing and characterization of twitter abusive behavior. In Twelfth International AAAI Conference on Web and Social Media.\\n\\nShreyansh Gandhi, Samrat Kokkula, Abon Chaudhuri, Alessandro Magnani, Theban Stanley, Behzad Ahmadi, Venkatesh Kandaswamy, Omer Ovenc, and Shie Mannor. 2019. Image matters: scalable detection of offensive and non-compliant content/logo in product images. arXiv preprint arXiv:1905.02234.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-275", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We discuss the implementation details and present supporting details of the considered baselines and the human evaluation metrics. We also discuss a vivid qualitative analysis that compares our model's predictions with the best-performing baselines.\\n\\nFigure 5: Word Cloud from Distressed posts.\\n\\nA.1 Word characteristics\\nWe generate word clouds to graphically represent the word frequencies that appear more frequently in the Distressed and Non-distressed posts. The\"}"}
{"id": "emnlp-2023-main-275", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"bigger the term in the visual, the more often it appeared in user descriptions. Figures 5 and 6 show the word clouds generated from the 100 most frequent words of each class. The difference in word choices for the distinct classes is evident from the figures. Table 8 shows some keywords used for crawling posts from Twitter and Gab to develop the DCaM dataset. Initially, we randomly crawled around 5000 posts each for a period of 1 week from both Twitter and Gab and performed topic modeling to fetch the trending topics. We randomly use a subset of these topics to crawl posts for our dataset. From the collected posts, we create a bag of frequently occurring hashtags and use the generated set to crawl further posts. We take care of non-repetition in the collected posts by maintaining the post IDs. Lastly, to supplement the lack of offensive posts being crawled, we use the synonyms of the words 'hate', and 'offensive' and use them as tags (like for the word 'offensive' an example synonym could be 'insult' and gab URL that can be used: https://gab.com/tags/insult). Figure 6: Word Cloud from Non-distressed posts.\\n\\nA.2 Annotation Guidelines\\n\\nOur annotation guidelines are rooted in the works of (Poria et al., 2021; Ghosh et al., 2022c). The annotators were instructed to identify the set of causal spans that accurately depict the reasons for a post being tagged as distressed given an input post with that label. The annotators annotated a post with the No_cause tag if the cause of the post was latent.\\n\\nTwo human experts\u2014graduate students with adequate task knowledge\u2014annotated every post. We used the union of candidate spans from distinct annotators as the final causal span only when the size of their intersection was at least 50% of the size of the smallest candidate span. A third annotator was brought in if the final span could not be determined from the previous spans. This third annotator was similarly told to choose shorter spans over larger spans where they could adequately depict the reason without losing any information.\\n\\nA.3 Experimental Setup\\n\\nWe use PyTorch, a Python-based deep learning package, to develop our proposed model. We conduct experiments with the BERT import from the huggingface transformers package. To establish the ideal value of the additive angle $x$, which affects performance, five values ranging from 0.1 to 0.5 were examined. The default value for $x$ is 0.30. We set amplification value $a$ as 64. All experiments are carried out on an NVIDIA GeForce RTX 2080 Ti GPU. We conducted a grid search across 200 epochs. We find empirically that our Embedding...\"}"}
{"id": "emnlp-2023-main-275", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use Adam (Kingma and Ba, 2015) for optimization. The learning rate is 0.05, and the dropout is 0.5. The auto-latent encoder\u2019s dimension is fixed at 812. The discriminator $D$ consists of two completely linked layers and a ReLU layer and accepts 812-D input features. Stochastic gradient descent has a learning rate of 1e-4 and a weight decay of 1e-3 with a momentum of 0.5. We perform 5 cross-validations of the DCaM dataset for training and testing purposes. We run our experiments for 200 epochs and report the averaged scores after 5 runs of the experiments to account for the non-determinism of Tensorflow GPU operations.\\n\\nA.4 Baselines\\n\\nWe discuss the details of the considered baselines below. Similar to the DICE approach, to adapt the baselines to our multi-task scenario, we add a linear layer on top of the hidden-states output in the output layer of the CE task to calculate span start and end logits. The output layer for the CE task employs sigmoid activation, in which the threshold value is set at 0.4.\\n\\nA.4.1 BiRNN-Attention\\n\\nThe only difference between this model and the BiRNN model is the addition of an attention layer (Liu and Lane, 2016) after the sequential layer. In order to further train the attention layer outputs, we calculate the cross entropy loss between the attention layer output and the ground truth attention.\\n\\nA.4.2 CNN-GRU\\n\\nZhang et al. (2018) employed CNN-GRU to achieve state-of-the-art on several hate speech datasets. We add convolutional 1D filters of window sizes 2, 3, and 4, with 100 filters per size, to the existing architecture. We employ the GRU layer for the RNN component and max-pool the hidden layer output representation. This hidden layer is routed via a fully connected layer to yield prediction logits.\\n\\nA.4.3 BERT\\n\\nWe fine-tune BERT (Liu et al., 2019a) by adding a fully connected layer, with the output corresponding to the CLS token in the input. Next, to add attention supervision, we try to match the attention values corresponding to the CLS token in the final layer to the ground truth attention. This is calculated using a cross-entropy between the attention values and the ground truth attention vector, as detailed in (Mathew et al., 2021).\\n\\nA.4.4 ViL-BERT CC\\n\\nViL-BERT CC (Lu et al., 2019) is a variant of the ViL-BERT model that has been pre-trained on the Conceptual Captions (CC) dataset. Conceptual Captions is a large-scale dataset containing image-caption pairs sourced from the web. By leveraging the rich and diverse data in CC, ViL-BERT CC is designed to understand and generate captions for images, enabling tasks such as image captioning, visual question answering, and image retrieval.\\n\\nA.4.5 Visual BERT COCO\\n\\nVisual BERT COCO (Li et al., 2019) is a variant of the Visual BERT model that has been pre-trained on the Common Objects in Context (COCO) dataset. COCO is a widely used dataset for object detection, segmentation, and captioning tasks. By pre-training on COCO, Visual BERT COCO learns to encode visual features and understand the context of images, enabling tasks such as object recognition, image captioning, and visual question answering. Visual BERT COCO enhances the model\u2019s ability to analyze visual content and perform various vision-related tasks.\\n\\nA.4.6 BiRNN-HateXplain and BERT-HateXplain\\n\\nWe fine-tune the models made available by Mathew et al. (2021) on our DCaM dataset by changing the output layers as described earlier to suit our task\u2019s objective.\\n\\nA.4.7 SpanBERT\\n\\nSpanBERT (Joshi et al., 2020) follows a different pre-training objective compared to traditional BERT system (e.g. predicting masked contiguous spans instead of tokens) and performs better on question-answering tasks. Following the work in (Ghosh et al., 2022c) where SpanBERT is used to solve a mix of classification and cause extraction tasks, we fine-tune the SpanBERT base model on our DCaM dataset to meet our objective.\\n\\nA.4.8 Cascaded Multitask System with External Knowledge Infusion (CMSEKI)\\n\\nWe contrast the performance of our model with the state-of-the-art CMSEKI system presented in https://github.com/punyajoy/HateXplain.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CMSEKI leverages commonsense knowledge in the learning process to address multiple tasks simultaneously.\\n\\n**A.5 Metric Definitions**\\n\\nThe following metrics collectively provide a quantitative assessment of how well our model performs in the task of extracting causal spans for manifestations and determinants.\\n\\n**A.5.1 Evaluation Metrics**\\n\\n- **Full Match (FM):** This metric measures the percentage of predicted outputs that exactly match the ground truth outputs. In the context of span extraction, it would indicate the proportion of extracted causal spans that are completely correct.\\n- **Partial Match (PM):** This metric evaluates the similarity between the predicted outputs and the ground truth outputs, but it allows for some degree of variation. It takes into account cases where only a portion of the prediction matches the ground truth. This can be useful when the extracted causal spans are almost correct but might have minor variations.\\n- **Hamming Distance (HD):** Hamming Distance is a measure of the difference between two strings of equal length. It counts the number of positions at which the corresponding symbols in the two strings are different. In the context of causal extraction, it could represent the number of positions where the predicted and ground truth causal relationships differ.\\n- **Jaccard Similarity (JS):** Jaccard Similarity is a measure of set similarity that calculates the ratio of the size of the intersection of two sets to the size of their union. In the context of causal extraction, it would assess the similarity between the sets of tokens (or other elements) in the predicted and ground truth sequences.\\n- **Ratcliff-Obershelp Similarity (ROS):** The ROS is a sequence comparison metric that measures the similarity between two sequences by identifying the common substrings between them. It calculates a similarity score based on the length of the longest common subsequence between the sequences. This metric would quantify how much of the predicted causal spans match the ground truth causal spans in terms of shared subsequence patterns.\\n\\n**A.5.2 Human Evaluation-based Metrics**\\n\\n1. **Fluency:** This determines whether or not the extracted span is fluent and natural. Natural and regular answers get a score of 5, whereas inarticulate ones receive a 0.\\n2. **Knowledge consistency:** This determines whether or not the produced answer has used the appropriate knowledge. If the model generates responses based on irrelevant information, it must get a score of 0, while the selection of pertinent knowledge must receive a score of 5.\\n3. **Informativeness:** This metric is used to assess how informative the produced replies are. Here, a score of 0 means that the replies are uninformative, and a score of 5 means that they are.\\n\\n**A.6 Error Analysis**\\n\\nAlthough our proposed DICE framework performs well in the majority of the test cases, still there are certain scenarios where it fails to make the correct predictions. We show some sample predictions from the test set in Table 9. In the first two instances, our model is able to partially predict the causal spans; however, in the first example, it fails to categorize the post as Distressed. It is also to be noted that the model extracted span in the second example seems to be more appropriate than the actual annotation by the human annotator. The model rightfully ignores the irrelevant information 'Video shows' and focuses on the relevant action part of the post. This illustrates our model's strong ability to comprehend offensive reasoning among diverse test cases. In the third and fourth examples, our model fails to extract any relevant cause from the given input. Moreover, in the third example, the model wrongly categorizes the post as Non-distressed. This can be due to the lack of sufficient context that hindered our model's comprehension ability for the given input.\"}"}
{"id": "emnlp-2023-main-275", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Rutgers Professor On White People: \u201cWe Gotta Take These MF\u2019ers Out!\u201d\\n\\n2. EXCLUSIVE: Video shows terrifying ambush-style robbery, shooting in San Francisco.\\n\\n3. Apu reads a children\u2019s book about a transsexual Nazi.\\n\\n4. Speaking with Raymond Ibrahim about #Christian Persecution.\\n\\nTable 9: Error Analysis from the Proposed Systems. Color Coding: Blue- Correct, Red: Incorrect; Teal: Incomplete. Highlighted text in pink shows the human-annotated causal spans.\"}"}
