{"id": "emnlp-2023-main-890", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nTheory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANTOM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANTOM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.\\n\\nIntroduction\\n\\nExisting evaluations for language models\u2019 theory of mind (ToM) \u2013 i.e., the ability to understand the mental states (e.g., thoughts, beliefs, and intentions) of others (Premack and Woodruff, 1978), is primarily focused on using situation descriptions (i.e., narratives) as the target domain (Nematzadeh et al., 2018; Le et al., 2019; Sap et al., 2022; Shapira et al., 2023a). However, ToM capabilities play an even more important role in understanding dynamic social interactions, as they form a crucial component of effective communication (Frith, 1994; Schober, 2005). Furthermore, as narratives condense situation information into short texts, reporting biases can cause them to include spurious correlations or surface cues (Gordon and Van Durme, 2013). These can be exploited by large language models (LLMs) to display illusory ToM \u2013 i.e., a false sense of robust social reasoning by models.\\n\\nIn this work, we introduce FANTOM, an English benchmark for stress-testing machine ToM in interactions \u2013 i.e., conversations. As conversations present interactions in their raw form, they are much less susceptible to reporting biases, and are more aligned with real-world scenarios requiring ToM reasoning. FANTOM consists of 10K questions covering 256 multiparty conversations around a certain topic while characters enter and leave the discussion, leading to distinct mental states between characters due to information asymmetry. The goal of FANTOM is to effectively measure how well models can track the belief of multiple participants in a conversation. To evaluate performance, we formulate the following four types of questions:\\n\\n\u2022 Answerability Questions (about the Fact Question):\\n\\nQ: Who knows the correct answer to this question?\\nA: Linda, David, Sally\\n\\nQ: Does David know the correct answer to this question?\\nA: Yes\\n\\n\u2022 Info Accessibility Questions (about the Full Fact Answer):\\n\\nQ: Who knows about this information?\\nA: Linda, David, Sally\\n\\n\u2022 Belief Question:\\n\\nQ: What breed would Kailey think Linda\u2019s dog is?\\n\\nOmniscient-view Belief: Kailey believes Linda has a golden retriever.\\n\\nKailey-centric Belief: Kailey does not know the breed.\\n\\nQ: Does Sally know about this information?\\nA: Yes\\n\\nTheory of Mind Questions\\n\\nFigure 1: An example question set in FANTOM.\"}"}
{"id": "emnlp-2023-main-890", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"characters in conversations where some information may be inaccessible to some participants. For example, in Figure 1, Kailey briefly steps away from the conversation to get a cup of coffee, while the others continue discussing Linda's new dog. The information exchanged during Kailey's absence remains unknown to Kailey, and only the information shared after Kailey's return becomes accessible. We convert factual question-answer pairs to obtain multiple challenging questions about characters' beliefs concerning the inaccessible information. Our aim is to design questions at different levels that evaluate a model's capability for a coherent understanding of others' mental states. In doing so, we are particularly interested in identifying instances of illusory ToM, which we define as situations where a model may answer some questions correctly but fails to answer others that require the same type of ToM reasoning.\\n\\nThe analysis of evaluation results on FANTOM reveals several interesting findings (\u00a74): (1) First, existing neural models score significantly lower than humans on individual questions and on the full set of questions by more than 70% on average. (2) While chain-of-thought reasoning (CoT) does improve performance in most models, it does not substantially bridge the gap with human performance. (3) Although our benchmark is not meant for training, we observe that fine-tuning can help models achieve scores higher than human performance on individual question types. However, when it comes to metrics that require coherent responses across multiple question types, the fine-tuned model still significantly underperforms compared to humans. (4) Additionally, we find that models exhibit different error types depending on the format of questions, despite all questions requiring the same underlying reasoning. (5) Moreover, our results indicate that CoT has a selective impact on performance, showing improvement only in specific scenarios.\\n\\nTo the best of our knowledge, FANTOM is the first benchmark to introduce conversation-based ToM evaluation for language-based models. Our benchmark design and experiment results yield important insights into the debate around ToM (Whang, 2023) and the development of artificial general intelligence (Metz, 2023) in LLMs. We release our benchmark to spark further discussions on evaluating the ToM capabilities of LLMs.\\n\\n2 Design Considerations for FANTOM\\n\\nWe go over the important design choices that we made when constructing FANTOM. Our goal is to incorporate (1) social interactions that necessitate natural theory of mind (ToM) reasoning (\u00a72.1), (2) essential theoretical prerequisites for validating ToM from psychology (\u00a72.2), and (3) empirical findings that must be taken into account when evaluating large language models (\u00a72.3).\\n\\n2.1 Grounding in Social Interactions\\n\\nTo capture the interactive aspect of ToM, we ground our task in natural social interactions \u2013 i.e., conversations. By doing so, we gain two key benefits: (1) minimizing reporting bias (Gordon and Van Durme, 2013) and (2) aligning with real-world scenarios. Since narratives are condensed descriptions of interactions, the process of deciding what to include or exclude can introduce reporting bias, resulting in artifacts that models exploit. For instance, including \\\"Carlos did not see this, so he does not know currently where the apple is.\\\" in a narrative for ToM evaluation provides a significant clue about the other's mental state. However, such explicit hints are rarely present in real-world interactions. Conversations, on the other hand, present interactions in their raw form, without those explicit hints about others' mental states. During conversations, we reason through the intermediate steps from scratch, thereby grounding the benchmark in conversations enables a more realistic and unbiased assessment of ToM.\\n\\n2.2 Meeting Theoretic Requirements\\n\\nWe follow the two important criteria outlined by Quesque and Rossetti (2020) that must be met when designing a task to validate ToM: \\\"non-merging\\\" and \\\"mentalizing\\\". (1) \\\"Non-merging\\\": Evaluation should require the respondent to maintain a distinction between the others' mental state and its own. For example, suppose someone is asked about the other's belief regarding the location of the TV remote controller, and both are believing it to be on the sofa. If the respondent answers that the other believes it is on the sofa, it becomes unclear whether the response is based on the respondent's own belief or the other's (i.e., merging mental states). Such merging scenario is unsuitable for validating ToM. Since machines lack emotions or intentions (Gros et al., 2022), we exploit information asymmetry...\"}"}
{"id": "emnlp-2023-main-890", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"try when constructing our benchmark to simulate the non-merging mental state scenarios. We design multiparty conversations where specific information is inaccessible to certain characters. While machines do not possess their own point of view, they act as omniscient observers during our evaluation since we provide the entire conversation as input. As a result, the mental states of the model and the character can be regarded as distinct with respect to that information.\\n\\n(2) \u201cMentalizing\u201d: Lower-level processes should not be accounted for successful performance of ToM tasks. If a simpler process can explain a phenomenon, it should always be preferred over a more complex one when interpreting the results. For instance, recognizing joy by observing laughter is more of a visual discrimination than reasoning mental representations.\\n\\nIf the correct answer for a ToM task has a high degree of word correlation with a salient part of the given input, it becomes difficult to determine whether the model is accurately ascribing the other's mental state or simply following a shortcut pattern matching (i.e., the lower-level process). Therefore, such cases should be discouraged when evaluating ToM in neural language models. In FANTOM, we create false answers that have high word correlation with the input to verify whether the models can overcome the shortcut pattern matching when reasoning mental states.\\n\\n2.3 Seeking Comprehensive Evaluation\\n\\nSince the performance of LLMs varies significantly based on given prompts (Webson and Pavlick, 2022), we adopt a series of reiterative questions at various levels for the same input context, including free-form response questions, multiple-choice questions, and straightforward yes or no questions. The inclusion of free-form response questions is important as it aligns with the common usage of LLMs in contrast to multiple-choice questions that are prevalent in existing benchmarks (Sakaguchi et al., 2021; Hendrycks et al., 2021). Although their formats are different, all questions in FANTOM fundamentally aim to ascertain the same underlying reasoning: \u201cwho is aware of the information?\u201d As a result, FANTOM enables us to identify illusory ToM instances wherein models deliver accurate responses for one format but struggles to do so for another format.\\n\\n3 FANTOM Overview\\n\\nFollowing the success of previous works (Kim et al., 2022; Chen et al., 2023), we automatically construct full conversations using the large language model (LLM) InstructGPT davinci-003 (Ouyang et al., 2022). We also generate theory of mind (ToM) question-answer pairs related to the conversation participants' beliefs using a specially designed pipeline. In preliminary explorations, we find off-the-shelf LLMs struggle with directly generating ToM question-answer pairs for a given conversation. Our pipeline consists of three steps: (1) generate conversations with information asymmetry (\u00a73.1), (2) generate fact question-answer (QA) pairs (\u00a73.2), and (3) construct ToM (e.g., belief) QA pairs from the fact QA pairs (\u00a73.3). We use different evaluation methods for each question types (\u00a73.4), and validate the final dataset (\u00a73.5).\\n\\n3.1 Information-Asymmetric Conversations\\n\\nFANTOM consists of small talk conversations involving multiple characters, with each conversation centered around a topic (e.g., pets, risk-taking, personal growth). Each topic has several subtopics, e.g., the topic \u201cpets\u201d may include subtopics \u201cbreed\u201d and \u201cspecial moves.\u201d Initially, the conversation begins with two or three characters. As the conversation progresses, characters join and leave the discussion and the conversation's subtopic changes over time. Conversations include explicit indications of leaving and joining, such as utterances like \u201cHey guys, I'll go grab a coffee.\u201d or \u201cHey, I'm back, what are you guys discussing now?\u201d shown in Figure 1.\\n\\nDuring the absence of a character, the conversation continues and information is shared among the remaining participants, creating a natural information asymmetry that reflects real-life interactions. After a series of utterances, the character who was absent (re)joins the conversation, unaware of the information that was previously shared with other participants. More details are in Appendix A.1.\\n\\nMany existing ToM tasks involve some form of asymmetry between characters (Bra\u00fcner et al., 2020). For example, in the Sally-Anne task, Sally does not know that Anne relocated the object, while the observer is aware of the action. In the Smarties task, the character in the story does not know the label changed, whereas the observer is fully aware of this situation. This inherent asymmetry ensures two distinct mental states (i.e., the non-merging criterion; \u00a72.2) to be present during the experiments.\"}"}
{"id": "emnlp-2023-main-890", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Factual Question-Answer (QA) Pairs\\n\\nThe conversations in FANTOM include factual question-answer pairs (FACT Q) about the inaccessible information\u2014i.e., the information that a specific character is unaware of. An example question would be \\\"What is the breed of Linda\u2019s dog?\\\" in Figure 1. More details are in Appendix A.2.\\n\\nThere are two distinct types of answers for each FACT Q: (1) FULL FACT A and (2) LIMITED FACT A. The FULL FACT A incorporates the full information in the preceding conversation where the character PersonX was absent. On the other hand, LIMITED FACT A relies only on the conversation in which PersonX participated. The former answer is based on information that PersonX does not have access to, while the latter answer only takes into account the accessible information for PersonX. For cases where no information was shared regarding the FACT Q, the LIMITED FACT A indicates that no information has been provided. Examples of these answers are in Figure 1. \\\"Linda has a golden retriever.\\\" is based on the preceding conversation where Kailey was absent, while \\\"There is no information on the breed of Linda\u2019s dog\\\" is based on the conversation Kailey participated.\\n\\n3.3 ToM QAs based on Fact QAs\\n\\nFor each FACT Q, we build six types of ToM QA. Construction details can be found in Appendix A.3. (1) BELIEF Q[DIST.] and (2) BELIEF Q[CHOICE]: These questions are created by rephrasing the FACT Q to ask beliefs of characters in the conversation. We are particularly interested in PersonX\u2019s belief about the inaccessible information from the previous conversation, in which PersonX did not participate. For example, the FACT Q of \\\"What is the breed of Linda\u2019s dog?\\\" in Figure 1 is converted to \\\"What breed would Kailey think Linda\u2019s dog is?\\\" The BELIEF Q[DIST.] requires free-form response, while BELIEF Q[CHOICE] provides multiple-choice options for the same question. The options for BELIEF Q are created by rephrasing the FULL FACT A and LIMITED FACT A. For example, the \\\"Linda has a golden retriever.\\\" in Figure 1 is converted to \\\"Kailey believes Linda has a golden retriever.\\\" Since the FULL FACT A reflects information that is not accessible to PersonX and the LIMITED FACT A incorporates only the information accessible to PersonX, we label the converted FULL FACT A and LIMITED FACT A as \\\"OMNISCIENT - VIEW BELIEF A\\\" and \\\"PERSON X - CENTRIC BELIEF A\\\", respectively.\\n\\n(3) ANSWERABILITY Q[LIST]: Given the FACT Q, we ask models \\\"List all the characters who know the correct answer to this question\\\". In essence, we are interested in whether the model can identify who among the participants can correctly answer the FACT Q. This is a meta-question that necessitates two-step reasoning: first determining the answer itself, and second, identifying the characters who have access to this knowledge.\\n\\n(4) INFO ACCESS Q[LIST]: Here, we provide the FULL FACT A with the FACT Q and ask the model \\\"List all the characters who know this information\\\". Essentially, this question aims to identify the individuals who have knowledge or access to this information. Since the information is explicitly provided to the model, only the second reasoning step of the ANSWERABILITY Q[LIST] is required.\\n\\n(5) ANSWERABILITY Q[Y/N] and (6) INFO ACCESS Q[Y/N]: We ask models to determine, through a simple binary response (yes or no), whether each character is capable of answering the question or knows the information. For example, we ask models \\\"Does David know the correct answer to this question?\\\" and \\\"Does Sally know about this information?\\\" (Figure 1).\\n\\n3.4 Evaluation\\n\\nEach question is provided to the model along with the conversation as input. This makes the model an omniscient observer, having access to all information shared in the conversation. On the other hand, PersonX was absent for a while, thereby an information asymmetry naturally arises between the model and PersonX. Responses that include inaccessible information for PersonX indicate a lack of ToM in the model.\\n\\nInput context types\\n\\nFANTOM comprises two types of input conversations: short and full. In the case of short input, the model is provided with the conversation that only includes the part where the specific speaker left and (re)joined, while excluding the other earlier and later parts of the conversation. On the other hand, a full conversation encompasses the entire discussion on the main topic, including all subtopics. As a result, this is significantly longer than the short input.\"}"}
{"id": "emnlp-2023-main-890", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When given a belief question regarding PersonX, the model should generate a response that incorporates only the information accessible to PersonX. We use cosine similarity to measure the distance between SentenceBERT (Reimers and Gurevych, 2019) embeddings of each option and response. A correct response should always be closer to the PERSONX-CENTRIC BELIEF A than the OMNISCIENT-BELIEF A.\\n\\nTo accurately assess the performance of the response, we also calculate the token F1 score for responses that are considered correct based on the distance metric, following the convention of various QA tasks (Rajpurkar et al., 2016, 2018). When comparing distances in the embedding space, nonsensical responses (e.g., repetition of character names) can be deceptively closer to PERSONX-CENTRIC BELIEF A, resulting in misleading accuracy. Therefore, models must score high on both the distance and F1 metrics for the BELIEF Q[DIST.]\\n\\nThe model should choose between the OMNISCIENT-BELIEF A and the PERSONX-CENTRIC BELIEF A. The correct answer is the PERSONX-CENTRIC BELIEF A.\\n\\nANSWERABILITY Q[LIST] and INFO ACCESS Q[LIST] A correct response must include all characters who have access to the answer or information while excluding all characters who do not. No partial marks are assigned.\\n\\nANSWERABILITY Q[Y/N] and INFO ACCESS Q[Y/N] The model should respond with \\\"yes\\\" or \\\"true\\\" for all characters who have access to the answer or information, and with \\\"no\\\" or \\\"false\\\" for all characters who do not. More details are in Appendix A.4.\\n\\n3.5 Dataset Validation & Statistics\\n\\nValidation To ensure the quality of our benchmark, we go through a manual validation process for all conversations and question-answer pairs using Amazon Mechanical Turk (MTurk). We conduct validation on the entire conversations in our dataset using 32 annotators who passed a qualification test for assessing conversation coherence. We ask workers to flag conversations that are incoherent or unsafe (e.g., unethical, biased, harmful, dangerous, or offensive). Each conversation is validated by three workers. While 10 conversations received votes for incoherence, none achieved a majority vote indicating they were incoherent. We refine all 10 conversations. As for safety, no conversations were voted as being unsafe. We also request workers to verify the answers provided for BELIEF Q[CHOICE]. We remove all question sets that were marked as erroneous by the worker (\u223c8.6%).\\n\\nStatistics FANT OMM is composed of 256 conversations with 1,415 BELIEF Q[DIST.]s and BELIEF Q[CHOICE]s, 703 FACT Qs, ANSWERABILITY Q[LIST]s, and INFO ACCESS Q[LIST]s, respectively. Additionally, there are 2,689 ANSWERABILITY Q[Y/N]s and INFO ACCESS Q[Y/N]s. Given that the ANSWERABILITY Q[Y/N]s and INFO ACCESS Q[Y/N]s iterate over all characters present in the conversations, they have the highest count among all the question types.\\n\\nThe average number of turns in the input context is 13.8 (short conversation), and the average number of words in each turn is 21.9. For reference, the corresponding statistics for ToMi (Le et al., 2019) are 4.9 and 4.2, respectively. More statistics can be found in Appendix A.5.\\n\\n4 Experiments\\n\\nBaseline Models We test a total of thirteen recent instruction-tuned neural language models: GPT-4 (gpt-4-0613 and gpt-4-0314; OpenAI, 2023), ChatGPT (gpt-3.5-turbo-0613; OpenAI, 2022), InstructGPT (davinci-003 and curie-001; Ouyang et al., 2022), Flan-T5-XL and Flan-T5-XXL (Chung et al., 2022), Flan-UL2 (Tay et al., 2023), Falcon Instruct (7B and 40B; Almazrouei et al., 2023), Mistral Instruct 7B (Jiang et al., 2023), Zephyr 7B (HuggingFace, 2023), and Llama-2 Chat 70B (Touvron et al., 2023). Descriptions for each model are in Appendix B.\\n\\nAlthough our benchmark is not meant for training, we also fine-tune Flan-T5-XL (Chung et al., 2022) by randomly splitting FANT OMM according to the conversation's main topics. We then test the model on unseen conversation topics. More details can be found in Appendix B.\\n\\nHuman Performance We also measure human performance by asking graduate students in computer science.\\n\\nAs it is redundant to ask human testers binary questions when they have already been asked ANSWERABILITY Q[LIST] and INFO ACCESS Q[LIST], we do not ask ANSWERABILITY Q[Y/N]\"}"}
{"id": "emnlp-2023-main-890", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 Results\\n\\nAll the models exhibit scores that are significantly worse than human performance. Table 9 shows the full results of state-of-the-art large language models (LLMs) on FANTOM. We break down the table and highlight each discussion point below.\\n\\nIllusory Theory of Mind\\n\\nFigure 2 shows the results of a few selected models. We find models perform significantly better on BELIEF Q [CHOICE] compared to ANSWERABILITY Q [LIST] and INFO ACCESS Q [Y/N]. Despite the ANSWERABILITY Q [LIST] and INFO ACCESS Q [Y/N] being prerequisites for solving BELIEF Q [CHOICE], they are much more challenging for models. Furthermore, models' performance sharply drops when evaluated for coherent reasoning across multiple question types with the same underlying theory of mind (ToM) reasoning (i.e., All Question Types). These findings suggest that some instances of successful LLM ToM reasoning in FANTOM should be interpreted as illusory.\\n\\nA Chain-of-thought and Fine-tuning\\n\\nTable 1 summarizes the results when we apply zero-shot chain-of-thought (CoT) reasoning or fine-tuning to models. For CoT, we follow Kojima et al. (2022) and use the prompt \\\"let's think step by step\\\". We observe an improvement in scores with CoT applied. However, there are still significant score gaps compared to human performance.\\n\\nWe also find fine-tuned Flan-T5 XL still falls short of human performance in metrics that demand consistent accuracy across multiple questions\u2014i.e., the All scores.\\n\\n3 Although our benchmark is not intended for training purposes, developing models with a coherent ToM reasoning remains challenging, even with explicit training on the data.\\n\\nComprehending Facts vs. Distinct Beliefs\\n\\nFigure 3 shows the token F1 scores for FACT Q and answerability Q [Y/N]. We find fine-tuning achieves scores comparable with human performance on individual question types (see Table 9).\"}"}
{"id": "emnlp-2023-main-890", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Results of FactQ and BeliefQ [Dist.] for models given the short conversation context. Full results with all models, input types, and metrics are in Table 9.\\n\\nAccuracy for BeliefQ [Dist.] The token F1 scores for FactQ can be seen as a measure of a model's basic comprehension capability for interactions. Scoring high in FactQ indicates the model is good at identifying the most relevant information piece to answering the question. Despite its small size, Mistral Instruct 7B shows the strongest performance among the open-source models.\\n\\nOn the other hand, BeliefQ [Dist.] aims to measure a model's understanding of individual characters' perspective of a particular information\u2014i.e., belief. To meet the mentalizing criterion (see \u00a72.2), we deliberately design the incorrect answers in BeliefQ [Dist.] to have greater word overlap with the context than correct answers. Also, BeliefQ [Dist.] are rephrased questions inquiring about PersonX's belief for the facts in FactQ, thereby the two question types share significant word overlap. However, the same information that was used to answer FactQ should not be included in the response for BeliefQ [Dist.] on PersonX as it is from the conversation that PersonX missed. As a result, certain models with higher token F1 scores for FactQ have lower scores for BeliefQ [Dist.] compared to models that perform worse on FactQ (e.g., InstructGPT davinci-003 vs. Llama-2 Chat and Mistral Instruct). This suggests the models lack the ability to comprehend distinct perspectives of individual characters, leading them to reproduce similar responses to FactQ for BeliefQ [Dist].\\n\\nFree-Response vs. Choice We observe a pattern where models score significantly worse in free-response questions than choice questions (BeliefQ [Dist.] vs. BeliefQ [Choice]; Figure 3 and 2). However, many of them still achieve scores either below or around 50, which is the random baseline for those binary choice questions.\\n\\nTable 2 compares models' performance between AnswerabilityQs [Y/N] and InfoAccessQs [Y/N]. As AnswerabilityQs require an additional step of reasoning compared to InfoAccessQs, models consistently perform worse on AnswerabilityQs compared to InfoAccessQs. However, this pattern is not consistent across models for AnswerabilityQs [List] and InfoAccessQs [List] (see Figure 2). This may be because models significantly struggle with AnswerabilityQs [List] and InfoAccessQs [List], potentially resulting in the absence of meaningful performance patterns.\\n\\nShort vs. Full Conversations When a model is provided with the full conversation (Table 9, bottom), its performance noticeably decreases compared to when it is given only the relevant parts of the conversation (Table 9, top). The decrease can be attributed to the model's need to identify the relevant information within the full conversation, whereas it does not have to do so for the short conversations. This indicates theory of mind reasoning becomes even more challenging for models when it needs to be combined with different types of reasoning (e.g., search).\\n\\n4.2 In-depth Analysis What types of errors do models make? Figure 4 and 5 summarize the error types of AnswerabilityQ and InfoAccessQ for each model with and without chain-of-thought (CoT) reasoning. For list-type questions, models make more errors by including characters who are unaware of the information in the responses, rather than excluding characters who are aware. Interestingly, when CoT is applied, the error of including unaware characters decreases, whereas the error of excluding characters who are aware increases for most models.\"}"}
{"id": "emnlp-2023-main-890", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the case of binary questions, false positives and false negatives correspond to including characters who are unaware and excluding characters who are aware in the response for list-type questions, respectively. If the model fails to generate a yes or no response, we mark it as irrelevant. Models tend to exhibit false negative responses more frequently for binary questions compared to list-type questions. Similarly, CoT primarily helps the model in reducing the false positive error rates, but the reduction in false negative error rates is not consistent across models. This suggests that CoT selectively improves reasoning specifically for determining characters who are unaware of the information, rather than characters who are aware.\\n\\nHow accurate and consistent are models\u2019 answers for a given character?\\n\\nFor accuracy, we report the \\\\textit{All For Each Character} score which is determined by whether the models are able to answer all six types of ToM questions correctly regarding the specific character. For consistency, we measure the ratio of consistent model responses across \\\\textit{Answerability} and \\\\textit{Inference} for each character. Table 3 shows the accuracy and consistency of the models\u2019 responses for each character within the given conversation context. Overall, we observe a pattern where models that score low in accuracy also show low consistency. While CoT generally improves model performance (see Table 9), we find that it does not always lead to improved accuracy and consistency. The decrease in \\\\textit{All For Each Character} score when CoT is applied suggests that CoT has a selective impact on different question types.\\n\\nAre there differences in performance in terms of the order of ToM beliefs?\\n\\nTable 4 presents the results of \\\\textit{BELIEF Q} with respect to different order of ToM beliefs.\"}"}
{"id": "emnlp-2023-main-890", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"orders of ToM beliefs. Similar to Le et al. (2019), models perform better on the second-order belief questions than those with first-order beliefs. To further investigate the performance on second-order belief questions, we analyze the results based on the cyclic and acyclic patterns in them. The cyclic second-order belief questions inquire about Character 1\u2019s belief regarding Character 2\u2019s belief about Character 1 (e.g., What does Linda think about Kailey\u2019s belief on the breed of Linda\u2019s dog?); while the acyclic second-order questions focus on Character 1\u2019s belief about Character 2\u2019s belief regarding Character 3 (e.g., What does David think about Kailey\u2019s belief on the breed of Linda\u2019s dog?). Models show better performance on the cyclic questions than acyclic ones, which include more characters to track. However, when CoT is applied, the increase in score for acyclic questions is greater than that of cyclic ones, suggesting CoT helps multi-tracking.\\n\\n5 Related Work\\n\\nExisting Theory of Mind Benchmarks\\n\\nMany theory of mind (ToM) benchmarks, inspired by the false belief test from psychology (Wimmer and Perner, 1983), evaluate models on reasoning beliefs about object locations with narratives (Grant et al., 2017; Nematzadeh et al., 2018; Le et al., 2019). Other works such as Shapira et al. (2023b) build benchmarks based on the Faux Pas Test (Baron-Cohen et al., 1999). Also, ToM-related benchmarks focus on reasoning emotions and mental states in narratives (Rashkin et al., 2018; Sap et al., 2019).\\n\\nTheory of Mind in Large Language Models\\n\\nAlthough qualitative assessments might imply a degree of ToM in large language models (LLMs; Whang, 2023), more comprehensive quantitative investigations reveal that they have yet to achieve human-level ToM across various benchmarks (Sap et al., 2022; Shapira et al., 2023a). LLMs struggle to reason ToM robustly (Ullman, 2023), though their performance can be improved through few-shot samples and chain-of-thought prompting (Sap et al., 2022; Moghaddam and Honey, 2023) as well as specific inference methods (Sclar et al., 2023).\\n\\n6 Conclusion & Discussion\\n\\nWe introduced FANTOM, a new benchmark for stress-testing theory of mind (ToM) capabilities of neural language models in conversations via question answering. Our benchmark is built upon essential theoretical requisites and empirical considerations required for validating ToM in large language models (LLMs). The conversations in our benchmark involve information asymmetry, with characters joining and leaving the discussion while it continues, to simulate distinct mental states. To identify illusory ToM, we crafted multiple types of challenging belief questions regarding the conversation participants\u2019 mental states by converting factual questions. Our evaluation results show that coherent ToM reasoning is challenging for current LLMs, performing significantly worse than humans even when using chain-of-thought reasoning or fine-tuning.\\n\\nAlthough there has been recent debates around whether current LLMs possess ToM capabilities or not (Whang, 2023), our results indicate that this capacity has not yet emerged in any manner. Previous instances of success on well-known psychology ToM tests may be attributed to exposure during the pretraining phase (Ullman, 2023). Our work highlights the need for novel interaction-oriented benchmarks that introduce scenarios not encountered during training, and also aligning more closely with real-world use cases as LLMs are increasingly being deployed in interactive settings.\\n\\nOur results also shed light on a broader issue in neural models \u2013 the lack of internal consistency (Elazar et al., 2021). We find they often fail to provide consistent answers to questions requiring the same underlying ToM reasoning. To address this concern, future works can explore various directions, such as grounding reasoning in pragmatics (Kim et al., 2020), visual information (Bisk et al., 2020), or belief graphs (Sclar et al., 2023).\\n\\nAnother issue that our work touches upon is the reporting biases inherent in language models. We observed that models often exhibit biases in their responses, showing a tendency to overly rely on the information they are conditioned on, such as preferring answers that have high overlap with the context (Sugawara et al., 2018). However, to achieve successful ToM reasoning, it is crucial to distinguish between accessible and inaccessible information for a particular agent, rather than blindly using all information available to the model. One potential approach to mitigate this is to combine pretraining with interactive learning (Sap et al., 2022).\\n\\nIn the spirit of encouraging future research in this direction, we make our benchmark publicly available at https://hyunw.kim/fantom.\"}"}
{"id": "emnlp-2023-main-890", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Although Fantom is the first benchmark, to the best of our knowledge, to cover theory of mind (ToM) reasoning in conversational interactions, it is currently limited to small talks on specific topics. Additionally, our benchmark only considers only a single type of relationship between conversation participants, where they do not have prior knowledge of each other. However, social reasoning can become much more dynamic when variables such as relationships (e.g., family, friends, co-workers) are introduced. ToM is essential in all conversational interactions, hence we strongly encourage future works to evaluate ToM in a wider range of diverse conversation scenarios.\\n\\nOur evaluation solely focuses on language-based models. However, it is important to note that ToM extends beyond a single modality (Piaget, 1956; Wu and Keysar, 2007). For instance, the well-known Sally-Anne test (Wimmer and Perner, 1983; Baron-Cohen et al., 1985) is typically conducted as a face-to-face experiment, where visual cues affect the performance of the participants. Therefore, interesting future work will involve examining the capabilities of multi-modal models in relation to ToM reasoning.\\n\\nLastly, as we generate full conversations with large language models, conversations may contain offensive contents (Weidinger et al., 2021). However, we specifically select casual topics for small talks (e.g., pets, personal growth, traveling) to minimize the likelihood of offensive content generation. Also, we manually validate all conversations in our benchmark with crowdworkers from Amazon Mechanical Turk.\\n\\nWe acknowledge that the term \\\"theory of mind\\\" (ToM) may evoke anthropomorphic connotations regarding AI models. However, we emphasize that the purpose of our work is not to promote anthropomorphism of AI models. Rather, our focus lies in exploring the limitations of existing language models in social reasoning. While the concept of ToM attempts to capture the ability to attribute mental states to oneself and others (Premack and Woodruff, 1978), it is important to clarify that AI models do not possess subjective consciousness or true understanding of intentions, beliefs, or desires. Our experiment results also demonstrate that current large language models do not exhibit any coherent ToM reasoning; instead, they primarily rely on word correlations.\\n\\nAcknowledgement\\nWe thank the participants who contributed to the human performance measurement. We also appreciate our colleagues on the Beaker Team at the Allen Institute for AI for helping with the compute infrastructure. This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031). Hyunwoo Kim and Gunhee Kim are supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-01082, SW StarLab; and No.2022-0-00156, Fundamental research on continual meta-learning for quality enhancement of casual videos and their 3D metaverse transformation). Lastly, we also thank OpenAI, as well as Google Cloud Compute.\\n\\nReferences\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamis, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Helllow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.\\nSimon Baron-Cohen, Alan M Leslie, and Uta Frith. 1985. Does the autistic child have a \\\"theory of mind\\\"? Cognition, 21(1):37\u201346.\\nSimon Baron-Cohen, Michelle O\u2019riordan, Valerie Stone, Rosie Jones, and Kate Plaisted. 1999. Recognition of faux pas by normally developing children and children with asperger syndrome or high-functioning autism. Journal of autism and developmental disorders, 29(5):407\u2013418.\\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. Experience grounds language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8718\u20138735, Online. Association for Computational Linguistics.\\nTorben Bra\u00fcner, Patrick Blackburn, and Irina Polyanetskaya. 2020. Being deceived: Information asymmetry in second-order false belief tasks. Topics in Cognitive Science, 12(2):504\u2013534.\\nMaximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu, and Dilek Hakkani-Tur. 2023. PLACES:\"}"}
{"id": "emnlp-2023-main-890", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-890", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jean Piaget. 1956. Child's Conception of Space. Routledge.\\n\\nDavid Premack and Guy Woodruff. 1978. Does the chimpanzee have a theory of mind? Behavioral and brain sciences, 1(4):515\u2013526.\\n\\nFran\u00e7ois Quesque and Yves Rossetti. 2020. What do theory-of-mind tasks actually measure? theory and practice. Perspectives on Psychological Science, 15(2):384\u2013396.\\n\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013789, Melbourne, Australia. Association for Computational Linguistics.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin, Texas. Association for Computational Linguistics.\\n\\nHannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight, and Yejin Choi. 2018. Modeling naive psychology of characters in simple commonsense stories. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2289\u20132299, Melbourne, Australia. Association for Computational Linguistics.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics.\\n\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106.\\n\\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. 2022. Neural theory-of-mind? on the limits of social intelligence in large LMs. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3762\u20133780, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019. Social IQa: Commonsense reasoning about social interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4463\u20134473, Hong Kong, China. Association for Computational Linguistics.\\n\\nMichael F Schober. 2005. Conceptual alignment in conversation. Other minds: How humans bridge the divide between self and others, pages 239\u2013252.\\n\\nMelanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. 2023. Minding language models' (lack of) theory of mind: A plug-and-play multi-character belief tracker. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13960\u201313980, Toronto, Canada. Association for Computational Linguistics.\\n\\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2023a. Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763.\\n\\nNatalie Shapira, Guy Zwirn, and Yoav Goldberg. 2023b. How well do large language models perform on faux pas tests. In Findings of the Association for Computational Linguistics: ACL 2023.\\n\\nSaku Sugawara, Kentaro Inui, Satoshi Sekine, and Akiko Aizawa. 2018. What makes reading comprehension questions easier? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208\u20134219, Brussels, Belgium. Association for Computational Linguistics.\\n\\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. 2023. Ul2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n\\nTomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399.\\n\\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300\u20132344, Seattle, United States. Association for Computational Linguistics.\\n\\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\"}"}
{"id": "emnlp-2023-main-890", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to create the conversations in our benchmark, we use a predefined set of subtopics for each main topic and employ templates to generate scripts. For example, for the topic \\\"pets\\\" subtopics may include \\\"breed\\\", \\\"special moves\\\", and \\\"favorite food\\\". Following Kim et al. (2022), we use specific speaker prefixes with English names sampled from the Top-1K names in the US SSN database for more natural conversations. We append each utterance with speaker prefixes. We randomly shuffle the subtopics for each topic and generate conversations for each subtopic. We generate the first conversation with the following prompt: \\\"{Character 1}, {Character 2}, ... {Character n} met for the first time at this social event. They are having a conversation on their {topic}. They now discuss {subtopic}.\\n{Character 1}:\" The initial conversation starts with two or three characters and there can be up to five characters who are participating in the conversation at the same time. Then, for each subtopic, we randomly select characters to join or leave the conversation. We use the following prompt when a character is selected to leave: \\\"Now, {leaving character} leaves the conversation because of the reason '{leaving reason}'. They now discuss {subtopic}. Remember to indicate that {leaving character} is leaving the conversation. {Conversation history}\\n{leaving character}: \\\" We use a predefined list of 64 reasons for leaving the conversation. Table 7 shows all reasons for leaving. We append the previous conversation history to the input prompt to make the conversation continue from the previous one.\\n\\nWe use the following prompt when a character is selected to join: \\\"Now {joining character} comes back after leaving the conversation because of the reason {leaving reason}. They now discuss {subtopic}. Remember to indicate that {joining character} is joining the conversation. Do not mention the details in the previous conversations. {Conversation history}\\n{joining character}: \\\" Extracting the inaccessible information for PersonX Whenever a character (re)joins the conversation, we extract the inaccessible information by asking GPT-4 (gpt-4-0314) what information was shared in the preceding conversation where the character PersonX did not participate. We provide the previous conversation and the current one as input to GPT-4 with the prompt \\\"What information was shared before PersonX joined, but was not mentioned after PersonX joined?\\\" appended to it. To ease the task, the joining of the character is explicitly denoted by inserting a script between the conversations, as follows:\\n\\n\\\"Previous conversation\\n[PersonX joined the conversation]\\nCurrent conversation\\\"\\n\\nWe observe quality improvements for the output generated by GPT-4 with the inclusion of the hint script. The returned result can be viewed as a conversation summary explicitly covering the previous context.\\n\\nA.2 Generating Factual QA Pairs We construct factual question-answer (QA) pairs related to the inaccessible information. First, we generate three non-yes-or-no questions and denote these as \\\"FACT Qs\\\" and obtain them by prompting GPT-4, given the inaccessible information text. We obtain \\\"FACT Qs\\\" by prompting GPT-4 with the following: \\\"{inaccessible information}\\n\\nBased on this, formulate three non-yes-or-no questions that can be answered by this conversation summary.\\\" Next, we generate two distinct types of answers for each FACT Q with GPT-4. (1) First, we generate\"}"}
{"id": "emnlp-2023-main-890", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sabrina: So, what was the most challenging workout experience you ever had?\\nAnna: Definitely when I decided to try out CrossFit. I'm not going to lie, it kicked my butt!\\nSabrina: Wow, that sounds intense. What kind of exercises did you do?\\nAnna: We did a lot of different things like high intensity interval training and Olympic lifting with barbells and dumbbells.\\nSabrina: That definitely takes dedication! How did you stay motivated during it?\\nAnna: It was tough but I kept reminding myself why I wanted to get fit in the first place and that helped me stay focused on my goals.\\nGina: Hey, I'm back! What were you guys talking about?\\nSabrina: We were just discussing our most challenging workout experiences. What do you think about when it comes to making a good workout playlist?\\nAnna: Music is really important when it comes to getting in the zone while working out. For me, I like upbeat and energetic songs that get me going.\\nGina: Yeah, something with a high tempo can really help push you during those tough workouts! I also like adding in some of my favorite classic songs that give me extra motivation to keep going.\"}"}
{"id": "emnlp-2023-main-890", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"UL2 are open-source (i.e., Apache 2.0) models from Google trained on instruction-phrased datasets. They are based on the encoder-decoder transformer architecture. Falcon Instruct is another open-source (i.e., Apache 2.0) model trained on RedefinedWeb (Penedo et al., 2023) and Baize (Xu et al., 2023). Llama-2 Chat (Touvron et al., 2023) is a fine-tuned 70B large language model, optimized for following user requests in dialogue format. Mistral Instruction (Jiang et al., 2023) is a 7B language model fine-tuned to follow instructions, which is reported to surpass the Llama-2 Chat 13B model. Zephyr (HuggingFace, 2023) is a model based on Mistral, further fine-tuned on UltraChat (Ding et al., 2023) and aligned with UltraFeedback (Cui et al., 2023).\\n\\nResults of other models\\n\\nTable 9 shows the results for other large language models not included in Figure 2. Given the random baseline score is 50 for BELIEF Q [C HOICE Q [Y/N], and INF A CCESS Q [Y/N], most of the models show low performance on our benchmark.\\n\\nFine-tuning details\\n\\nWe fine-tune Flan-T5-XL with learning rate=2e-5 and weight decay=0.01, evaluating per epoch and using early stopping with patience 1 (batch size = 3 for Flan-T5-XL). We observe an increase in validation loss after the first epoch. We also add special tokens before and after the completions to prevent the model from over-generating, which we find in early experiments. We also fine-tune text-curie-001 (Ouyang et al., 2022) for two epochs using standard parameters from the OpenAI API.\\n\\nZachary: Have you guys thought about how much money you'll need for retirement?\\nHazel: I'm still trying to figure that out. I know it's important to save, but it can be hard when there are so many other expenses.\\nCory: Yeah, that's true. It's also good to plan for how you want your lifestyle to look when you retire too. Will you travel or stay close to home?\\nHazel: That's a tough one! I think a combination of both might work best for me - travel while we're younger and then spend more time closer to family as we get older.\\nZachary: Sounds like a great plan! What kind of investments have each of you made?\\nCory: Well, I've started investing in some mutual funds and my 401(k). How about the two of you?\\nHazel: I've been contributing regularly to my IRA and also investing in index funds. Zachary?\\nZachary: Same here - mutual funds and an IRA with occasional stock investments as well.\\nHazel: Sorry guys, but my pet needs attention. I need to go check on him.\\nCory: Oh, okay. Have a good one!\\nHazel: See you later.\\nCory: Well, now that Hazel is gone, we can talk about the psychology of retirement. What are some of the emotional aspects to consider when planning for retirement?\\nZachary: It can be hard to think about the future and envision what life will be like in retirement without having any prior experience with it. People also tend to underestimate how much money they'll need for a comfortable lifestyle and overestimate their ability to save for it over time.\\nAlec: Hey, I'm back! What were you guys talking about?\\nCory: We were discussing retirement planning. Zachary was just mentioning how difficult it can be to think about the future and envision what life will be like in retirement without having any prior experience with it.\\nAlec: Yeah, that's true. It's important to save as much as possible now so you have enough money for retirement later. Have you guys started investing yet?\\nZachary: Yes, we both have investments in mutual funds and IRAs, and Cory has a 401(k). How about you Alec?\\nAlec: I've been saving some money into a savings account and I'm looking into investing in index funds soon. What other advice do you have for someone starting out with their retirement planning?\\nCory: Do your research on different investment options available to figure out which ones are best for your goals. There are also some tax benefits associated with certain types of investments that could make them more attractive than others depending on your situation.\\n\\nTable 6: Another sample from FANTOM.\"}"}
{"id": "emnlp-2023-main-890", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"bathroom break\\ncoffee break\\nforgot something important\\nforgot to print some documents\\nforgot to receive a package\\nforgot to run errands\\nforgot to submit documents\\nhave a meeting starting soon that I need to prepare for\\nhave a previous engagement that I need to attend to quickly\\nhave a work-related emergency that requires my immediate attention\\nhave an unexpected visitor at my door\\nhave errands to run\\nhave to attend to someone who just walked in\\nhave to check on something\\nhave to go to the restroom\\nhave to pick up a prescription\\nhave to pick up dry cleaning\\nhave to print or scan documents\\nhave to receive a delivery\\nhave to recharge laptop\\nhave to return a borrowed item\\nhave to take care of a family matter\\nhave to take care of an unexpected task\\nhave unexpected visitor\\nhis/her pet needs attention\\nhis/her family is calling\\nincoming delivery\\nmust respond to a phone call\\nneed to check on a friend or family member who needs assistance\\nneed to finish a task that's time-sensitive\\nneed to get a phone call\\nneed to get some coffee\\nneed to go to the toilet\\nneed to grab a snack or a drink\\nneed to have a quick chat with someone else\\nneed to make a phone call\\nneed to make a quick trip to the drug store\\nneed to make a quick trip to the grocery store\\nneed to pick up a package\\nneed to receive a parcel\\nneed to recharge cellphone\\nneed to register for an event\\nneed to schedule a haircut or salon appointment\\nneed to schedule another appointment\\nneed to step away for a moment to stretch and clear my mind\\nneed to step out for a moment\\nneed to submit some papers\\nneed to take care of some paperwork or documents\\nneed to take care of some personal matters\\nneed to take care of something related to my health\\nneed to take care of something urgent\\nneed to troubleshoot something\\nparking meter expiring\\nremembered something that needs to be taken care of\\nremembered to receive a package\\nremembered to submit some papers\\nremembered to take care of some paperwork or documents\\nremembered to take care of some personal matters\\nremembered to take care of something urgent\\nwant to go grab a drink\\nwant to go grab a coffee\\nwant to go take some fresh air\\nwant to go to the bathroom\\n\\nTable 7: Predefined reasons for characters leaving the conversation.\\n\\n| Dataset   | ToMi 6K | FANTOM 10K |\\n|-----------|---------|------------|\\n| #Questions| 6.0     | 12.9       |\\n| Avg. #Questions per Context| 4.9 | 13.8 |\\n| Avg. #Turns (Partial) | 4.7 | 24.5 |\\n| Avg. #Turns (Full) | 5.0 | 21.9 |\\n| Avg. Turn Length | 6.0 | 12.9 |\\n\\nTable 8: Statistics of FANTOM and ToMi (Le et al., 2019).\"}"}
{"id": "emnlp-2023-main-890", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | Question Types                  | Answerability Questions | Info Access Questions | Faith Questions | CoT Models |\\n|------------------------|---------------------------------|-------------------------|-----------------------|----------------|------------|\\n|                       | All Question Types              |                         |                       |                |            |\\n| Human                 | 87.5                            | 93.8                    | 90.6                  | 90.6           |            |\\n| Flan-T5-XL            | 0.0                             | 0.1                     | 30.5                  | 40.1           | 3.2        |\\n| Flan-T5-XXL           | 0.1                             | 0.3                     | 27.3                  | 42.1           | 2.2        |\\n| Flan-UL2              | 0.0                             | 0.1                     | 23.0                  | 47.6           | 2.9        |\\n| Mistral Instruct 7B   |                                 |                         |                       |                |            |\\n| Mistral Instruct 7B   | 0.0                             | 0.1                     | 27.6                  | 26.2           |            |\\n| Flan-T5-XL + CoT      | 0.0                             | 0.0                     | 43.0                  | 26.4           |            |\\n| Flan-UL2 + CoT        | 0.0                             | 0.0                     | 24.7                  | 32.4           |            |\\n| Mistral Instruct 7B + CoT | 0.0                        | 0.4                     | 58.5                  | 31.5           |            |\\n| Zephyr 7B + CoT       | 0.0                             | 0.0                     | 49.0                  | 69.6           |            |\\n| Falcon Instruct 7B + CoT | 0.0                          | 0.0                     | 42.4                  | 45.3           |            |\\n| Falcon Instruct 40B + CoT | 0.0                          | 0.0                     | 51.7                  | 72.1           |            |\\n| Llama-2 Chat 70B + CoT | 0.0                             | 0.0                     | 38.4                  | 17.8           |            |\\n| InstructGPT curie-001 + CoT | 0.0                         | 0.0                     | 21.0                  | 14.7           |            |\\n| InstructGPT davinci-003 + CoT | 0.0                         | 0.4                     | 17.7                  | 16.5           |            |\\n| ChatGPT 0613 + CoT     | 0.0                             | 0.1                     | 53.5                  | 26.2           |            |\\n| GPT-4 0314 + CoT      | 0.4                             | 0.6                     | 39.0                  | 29.3           |            |\\n| GPT-4 0613 (June) + CoT | 8.2                            | 12.3                    | 73.3                  | 65.3           |            |\\n| GPT-4 0613 (October) + CoT | 2.4                           | 4.1                     | 68.4                  | 56.1           |            |\\n| InstructGPT curie-001 + FT  | 0.0                             | 0.0                     | 56.6                  | 54.7           |            |\\n| InstructGPT davinci-003 + FT  | 1.3                            | 6.2                     | 39.8                  | 22.2           |            |\\n| ChatGPT 0613 + FT      | 2.1                             | 3.7                     | 58.5                  | 45.2           |            |\\n| GPT-4 0613 (June) + FT | 10.1                            | 15.4                    | 70.1                  | 61.2           |            |\\n| GPT-4 0613 (October) + FT | 0.9                           | 1.4                     | 60.9                  | 46.0           |            |\\n\\nTable 9: Zero-shot results from humans and large language models on FANTOM with the same instructions. CoT denotes chain-of-thought reasoning and FT denotes fine-tuning.\"}"}
