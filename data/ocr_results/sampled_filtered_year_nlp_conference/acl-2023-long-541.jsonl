{"id": "acl-2023-long-541", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-541", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How did Switzerland stay out of WWII?\\n\\nThey were literally the bankers of the war. The Nazis and the allies both kept their assets there. This is how they stayed neutral, because if either side invaded, that side's assets would either be seized by the other side, or seized by the Swiss.\\n\\nWhy do some people vomit when they see a corpse and/or witness a homicide?\\n\\nWe essentially vomit at the sight of gory or bloody death as a defense mechanism. In the face of corpses or death, we are often at risk ourselves, and therefore vomit to remove possible biohazards from our system that may have been spread by the dead, as blood and gore are often good at transmitting biohazards. It also prevents us from possibly ingesting any biohazards by forcing everything out of the mouth that may have been headed for the stomach (i.e. blood).\\n\\nHow does the MLS all star game work?\\n\\nThe Major League Soccer All-Star Game is an annual soccer game held by Major League Soccer featuring select players from the league against an international club. MLS initially adopted a traditional all-star game format used by other North American sports leagues where the Eastern Conference squared off against the Western Conference. This eventually evolved into the current system where the league annually invites a club from abroad to play against a league all-star team. The MLS All-Stars hold an 8\u20134 record in the competition marking the season's midpoint. Players are awarded rosters spots through a combination of fan voting and selections by the appointed manager and league commissioner.\"}"}
{"id": "acl-2023-long-541", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACL 2023 Responsible NLP Checklist\\n\\nA For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\n\\nLimitations section on Page 10, right after Section 6 (Conclusion)\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\\nEthics Statement section on Page 10, after the Limitations section\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\\nThe abstract is on the first page and the Introduction is the first section\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nLeft blank.\\n\\nB \u25a1 B1. Did you use or create scientific artifacts?\\n\\nIn section 3 we refer to a dataset collected in Xu et al. (2022) which provided input to collect our annotations and in section 4, we mention all the pre-trained models used and their sources.\\n\\n\u25a1 B2. Did you cite the creators of artifacts you used?\\n\\nSections 3.2 and 3.3 contain citations for the datasets used and Sections 4.1 and 4.2 contain citations for the models used.\\n\\n\u25a1 B3. Did you discuss the license or terms for use and / or distribution of any artifacts?\\n\\nIn section 1 we mention that we plan to open source all of our models, data, and annotations at time of publication, we will distribute with the CC BY-SA 4.0 license. Our code/data can be found at https://github.com/acpotluri/lfqa_summary/tree/main and https://huggingface.co/datasets/abhilashpotluri/lfqa_summary\\n\\n\u25a1 B4. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\nWe didn't explicitly discuss it in the paper but we only use publicly available models and question answering datasets and we build our dataset for research processes so it is compatible with the intentions of the existing artifacts.\\n\\n\u25a1 B5. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\n\\nIn sections 3 and 5 we discuss the data collection process (which was done through MTurk) and we also have screenshots of the annotation pages which show that we do not collect any personal information. We also have the annotation template for both the data and the user study in the public Github repository that we released.\\n\\n\u25a1 B6. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\nIn section 3.2 we discuss the datasets which our data is sourced from and their domains.\\n\\n\u25a1\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-541", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Did you run computational experiments?\\nSections 3.5 and 4.3 along with Tables 4 and 5\\n\\nDid you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\nSection 4.1/4.2 has model details and Appendix B has the remaining model details and computing infrastructure/budget descriptions.\\n\\nDid you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\nThe experimental setup is provided in section 4.3 and hyperparameters for fine-tuned models is in Appendix section B.\\n\\nDid you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\nExperimental statistics for the test set are in section 4.3 and results on the development set are in the appendix.\\n\\nIf you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\nSection 4 contains details of the models used for evaluation and any parameters which were set (as well as details of which evaluation packages were used).\\n\\nDid you use human annotators (e.g., crowdworkers) or research with human participants?\\nSection 3 has the dataset annotation details and section 5 has the human evaluation study\\n\\nDid you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\nAppendix sections A.1, A.2, and figures 3,4,5,6 contain full details and screenshots of the annotation instructions.\\n\\nDid you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\nSections 3.3 and 5.2 have the details of the participants for each annotation task and the hourly pay rates.\\n\\nDid you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\nThe instructions (which are attached in the appendix) for the annotation explain that we are trying to understand multi-sentence answers to complex queries for research purposes.\\n\\nWas the data collection protocol approved (or determined exempt) by an ethics review board?\\nWe discuss this in the ethics statement.\\n\\nDid you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\nSections 3.3 and 5.2 have the details of the participants for each annotation task.\"}"}
{"id": "acl-2023-long-541", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Concise Answers to Complex Questions:\\nSummarization of Long-form Answers\\nAbhilash Potluri\u2217 Fangyuan Xu\u2217 Eunsol Choi\\nDepartment of Computer Science\\nThe University of Texas at Austin\\n{acpotluri, fangyuan, eunsol}@utexas.edu\\n\\nAbstract\\nLong-form question answering systems provide rich information by presenting paragraph-level answers, often containing optional background or auxiliary information. While such comprehensive answers are helpful, not all information is required to answer the question (e.g. users with domain knowledge do not need an explanation of background). Can we provide a concise version of the answer by summarizing it, while still addressing the question? We conduct a user study on summarized answers generated from state-of-the-art models and our newly proposed extract-and-decontextualize approach. We find a large proportion of long-form answers (over 90%) in the ELI5 domain can be adequately summarized by at least one system, while complex and implicit answers are challenging to compress. We observe that decontextualization improves the quality of the extractive summary, exemplifying its potential in the summarization task. To promote future work, we provide an extractive summarization dataset covering 1K long-form answers and our user study annotations. Together, we present the first study on summarizing long-form answers, taking a step forward for QA agents that can provide answers at multiple granularities.\\n\\n1 Introduction\\nLong-form answers (Fan et al., 2019), as compared to span-based short answers (Rajpurkar et al., 2016), can provide comprehensive answers to a broader set of questions (Cao and Wang, 2021; Fan et al., 2019). While providing comprehensive information in multiple sentences is helpful, users often prefer short and concise answers to their questions when possible (Choi et al., 2021). Today\u2019s search engines already present concise answers by highlighting the most relevant parts from the passage excerpts. In this paper, we present the first study on summarizing long-form answers.\\n\\nSummarizing long-form answers introduces a new challenge in addition to the faithfulness and fluency challenges of generic summarization which mostly focus on news articles (Nallapati et al., 2016; Narayan et al., 2018): the summary output should still provide a reasonable answer to the original question. We take inspiration from a recent study (Xu et al., 2022) that reports that up to 40% of sentences in long-form answers contain non-essential information, such as providing background information or examples (Wang et al., 2022), which demonstrates the potential for compressing long-form answer.\\n\\nWe first aim for an extractive summarization model and collect sentence-level annotations on long-form answers, where annotators identify sentences that address the question directly and can serve as the \u201csummary\u201d.\\n\\nWe collect a dataset covering 1,134 examples, each consisting of a question, a long-form answer, and a set of summary sentences. To improve the extractive summaries collected, we propose a simple and yet novel summarization approach, extract-and-decontextualize, which first extracts summary sentences and rewrites them to stand-alone (Choi et al., 2021).\\n\\nCompared to abstractive summarization models trained on noisy distantly supervised datasets (e.g. CNN/DM (Nallapati et al., 2016) and XSum (Narayan et al., 2018)) which encourage paraphrasing but also hallucinations (Kryscinski et al., 2019; Cao et al., 2018; Kang and Hashimoto, 2020), decontextualization makes minimal edits to the original sentence, preserving its meaning while improving its fluency.\\n\\n1This is a simplified annotation task compared to the original discourse study of Xu et al. (2022).\"}"}
{"id": "acl-2023-long-541", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q: Why does car sickness seem to hit the hardest when you look down at your phone, book, etc.?\\nA: The brain perceived motion because it receives information from the eyes, ears, and muscles. When these parts send conflicting information, the brain doesn't know which is right and which is wrong, and this is what causes motion sickness. An example of this is when reading a book while you are in a moving car. To your eyes, the book is stationary while your inner ear and the rest of your body can feel a sense of motion. This would likely cause car sickness.\\n\\nAbstractive: The brain gets confused when it receives conflicting information about motion from different parts of the body, and this can cause car sickness.\\n\\nPartially: When these parts send conflicting information, the brain doesn't know which is right and which is wrong, and this is what causes motion sickness.\\n\\nGold: So, for about 20 working days, the Senate Democrats could have broken a filibuster if you could get every single one of them to agree on something. This did not go well.\\n\\nTable 1: We present two examples of questions, long-form answers, their summarized answers produced by different systems, and human evaluation results (\u201csummary adequacy\u201d and \u201cfaithfulness\u201d). We highlight the gold extractive summaries we collected. We find vanilla extractive approach, even with gold sentences, presents inadequate summaries but decontextualizing them makes them on par with GPT-3 abstractive answers. While none of the systems consistently present high-quality summaries (GPT-3 records a 67% success rate), most questions (95%) have at least one system that can generate a valid summary, showing the potential for successful compression of long-form answers. Together, we present the first corpus and study on summarizing long-form answers, opening doors for developing more flexible QA systems which provide answers with varying amounts of information. We release our data, code, and user study templates at https://github.com/acpotluri/lfqa_summary.\"}"}
{"id": "acl-2023-long-541", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to roles (e.g., auxiliary information) that are not necessary to answer the question, suggesting summarizing existing answers is viable. We follow their study and collect larger-scale data focusing on the \\\"answer summary\\\" role to study the summarization of long-form answers.\\n\\nSummarizing existing answers will support providing a consistent answer set of different granularities, where the users can expand condensed answer to see a more detailed version of the same answer. Consistent answers at multiple granularities are harder to enforce with a controllable generation approach. For instance, if we generate a five-sentence answer from the raw evidence set, the five-sentence answer can contain information absent in the ten-sentence answer.\\n\\nLastly, retrieval-augmented long-form QA models (Nakano et al., 2021) resemble query-focused summarization. Query-focused summarization (Xu and Lapata, 2020; Kulkarni et al., 2020) often studies challenging multi-document settings, where the input text is summarized focusing on a particular query, provided at inference time content control. A difference to our setting is that a long-form answer is written for the question \\\\( q \\\\), presenting already synthesized information tailored for the question.\\n\\n### 3 Extractive Summary for Long-form Answers\\n\\nWe first introduce our annotation task of identifying key sentences for long-form answers, which will be used as an extractive summary. Extractive summaries allow easier data collection and evaluation but can suffer from disfluency and incoherence. Thus, we manually evaluate our collected gold extractive summaries in Section 5.\\n\\n#### 3.1 Task\\n\\nGiven a question \\\\( q \\\\) and its long-form answer consisting of \\\\( n \\\\) sentences \\\\( a_1, a_2, \\\\ldots, a_n \\\\), the model makes a binary decision on whether each sentence \\\\( a_i \\\\) should be included in the summary. This setup differs from general summarization in having question \\\\( q \\\\) as an additional input.\\n\\n#### 3.2 Source Data\\n\\nWe use long-form answer data, (question, answer) pairs, from prior study (Xu et al., 2022) which compiled three existing LFQA datasets. ELI5 (Fan et al., 2019) consists of question answer pairs extracted from the subreddit Explain Like I'm Five. Natural Questions (NQ) (Kwiatkowski et al., 2019): NQ contains Google search queries as the questions, paired with paragraph-level answers from Wikipedia passages identified by annotators. WebGPT (Nakano et al., 2021) contains answers written by trained human annotators, with the questions sourced from ELI5. The annotator first searches for related documents using a search engine and then constructs the answers with direct references to those documents. We only take answers that passed their validity annotation, which excludes questions with false presupposition, ill-formed queries, and answers that do not provide valid answers. Their preprocessing step also filters answers with more than 15 sentences or less than 3 sentences.\\n\\n#### 3.3 Annotation Task\\n\\nGiven a question and its long-form answer, annotators select a set of summary sentences containing salient information addressing the question. The annotator interface and instructions are in the appendix. As saliency is somewhat subjective, we collect three-way annotations for each example. We recruited crowd workers from Amazon Mechanical Turk. We recruited workers from English-speaking countries, with at least a 95% acceptance rate on 1000+ HITs. Each worker was paid $0.50 per annotation, translating to an hourly rate of $15. We recorded reasonable agreement (Fleiss' Kappa 0.53) for the annotations.\\n\\n#### 3.4 Dataset Statistics\\n\\nTable 2 contains our collected dataset statistics, comparing it to a popular news summarization dataset (Nallapati et al., 2016) and a query-focused summarization dataset, AQuaMuSE (Kulkarni et al., 2020). To compute the summary length in our dataset, we randomly choose one of three summary annotations. The average number of sentences chosen as summaries by a single annotator was 1.6 out of 6.2 sentences in long-form answers. The statistics show that our data handles shorter texts and compress less than existing datasets. On average, our dataset contains 3.4 sentences per question, compared to 10.6 sentences per question in the AQuaMuSE dataset and 7.7 sentences per question in the NewsSumm dataset.\\n\\nXu et al. (2022) hired expert annotators (undergraduate linguistics students), as they required annotators to provide sentence-level labels among six functional roles. The expert annotators reached a similar agreement (0.52 Fleiss' kappa) for the \\\"summary\\\" role.\"}"}
{"id": "acl-2023-long-541", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Summarization dataset statistics, showing the number of examples (#), the length of question \\\\( q \\\\), document to summarize \\\\( d \\\\), and summary \\\\( s \\\\). For length, we report the average number of tokens and the average number of sentences in the parenthesis.\\n\\nLong-form answers were compressed to about one-third of their original length, with a slightly higher compression rate for ELI5 answers. This aligns with the prior discourse study (Xu et al., 2022) which reports ELI5 contains sentences that serve other functional roles (like providing an example) more frequently (23% compared to 5% and 8% in NQ/WebGPT datasets), neither of which are likely to be included in the summary.\\n\\n3.5 Automatic Extractive Summarization\\n\\nHaving collected a new dataset, we evaluate existing extractive summarization models on it. Is it easy for models to identify key sentences from long-form answers?\\n\\nSetting\\n\\nWe aggregate all data from three datasets (ELI5, NQ, WebGPT) and split them into 70% train, 15% validation, and 15% test set. We report classification metrics (precision, recall, \\\\( F_1 \\\\) scores) with summary sentences being the positive class. For each long-form answer, metrics are computed against each of the three references, with the results from the reference with the maximum \\\\( F_1 \\\\) score reported. We also report exact-match (EM), whether the model-predicted summary sentence set matches any of the three annotations. The training details and hyperparameters can be found in Appendix B.\\n\\n**PreSumm**\\n\\nWe use PreSumm (Liu and Lapata, 2019), a BERT-based extractive summarization model, which was trained on the CNN/DailyMail (Nallapati et al., 2016) dataset. It encodes the document with pre-trained BERT (Devlin et al., 2018) and outputs a score for each sentence. We select a threshold for the score at which it is considered a summary sentence to maximize \\\\( P \\\\), \\\\( R \\\\), \\\\( F_1 \\\\), \\\\( EM \\\\) %.\\n\\n| Setting | \\\\( P \\\\) | \\\\( R \\\\) | \\\\( F_1 \\\\) | \\\\( EM \\\\) % |\\n|---------|--------|--------|--------|--------|\\n| LEAD-2  | 0.41   | 0.74   | 0.51   | 11.4   |\\n| LEAD-3  | 0.46   | 0.83   | 0.56   | 5.3    |\\n| PreSumm-cnn (A) | 0.46 | 0.77 | 0.55 | 11.7 |\\n| PreSumm-cnn (Q+A) | 0.53 | 0.78 | 0.60 | 11.0 |\\n| PreSumm-cnn+ours (A) | 0.55 | 0.81 | 0.61 | 36.0 |\\n| PreSumm-cnn+ours (Q+A) | 0.55 | 0.88 | 0.63 | 30.9 |\\n| T5-ours (A) | 0.67 | 0.71 | 0.65 | 20.5 |\\n| T5-ours (Q+A) | 0.70 | 0.78 | 0.69 | 25.0 |\\n| Human | \u2217 | 0.77 | 0.79 | 0.77 | 41.3 |\\n\\nTable 3: Binary classification accuracy of extractive summarization models on the test set.\\n\\nThe \\\\( F_1 \\\\) score on the validation set. We evaluate both the original model (trained on CNN/DM dataset) and the model fine-tuned on our dataset.\\n\\n**T5**\\n\\nWe use a sequence-to-sequence model, T5-large (Raffel et al., 2019), to classify whether a sentence belongs to the summary or not. This was the best performing model for fine-grained role classification of long-form answers in Xu et al. (2022). For question prepending input, the input sequence to the model would be: \\\\( q[1]a_{1}[2]a_{2}[3]...[n]a_{n} \\\\).\\n\\nThe output sentence would then be of the form: \\\\( [1]r_{1}[2]r_{2}[3]...[n]r_{n} \\\\), where \\\\( r_{i} \\\\) was a binary class label whether \\\\( i \\\\)-th answer sentence \\\\( a_{i} \\\\) belongs to the summary or not.\\n\\n**Results**\\n\\nTable 3 reports model performances on the test set. The result on the validation set can be found in Table 8 in the appendix. With in-domain fine-tuning, both models are able to accurately predict which sentences belong to the summary. Fine-tuned T5 model shows a strong performance, though underperforming human, especially in exact match. We also find all trained classifiers benefit from having questions as additional input, signifying that questions provide important signals for content selection. While there is room for improvement, results suggest that predicting key sentence sets is not a major hurdle for state-of-the-art language models. Thus, we use the gold extractive summary for our user study (Section 5).\\n\\n4 Abstractive Summaries for Long form Answers\\n\\nWhile we have gold extractive summaries at hand, they often suffer from disfluencies and factual errors (Zhang et al., 2022). We aim to improve this in two ways, (1) by introducing a decontextualization (Choi et al., 2021) model to edit extractive summaries and (2) by using abstractive summarization.\"}"}
{"id": "acl-2023-long-541", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"models. We explore zero-shot transfer from an abstractive summarization model (Zhang et al., 2019) and prompting an instruction-tuned large language model (Brown et al., 2020). We experiment with two types of input sequences: (1) long-form answer only as an input (2) the question followed by a separation token and the long-form answer, whenever applicable. In the latter setting, models sometimes output the question as a part of the summary, which we remove with postprocessing.\\n\\n4.1 Editing Extractive Summary with Decontextualization\\n\\nThe disfluencies and lack of coherence of extractive summaries are well-known issues, motivating a flurry of abstractive summarization models (Rush et al., 2015; See et al., 2017). While abstractive models can provide coherent and fluent summaries, one of their major issues is hallucination (Kryscinski et al., 2019; Cao et al., 2018). Recent work explores extract-and-abstract approaches (Hsu et al., 2018; Liu et al., 2018; Pilault et al., 2020), aiming to take the best of both worlds. Most of these approaches are fine-tuned on an abstractive summarization dataset. As we don't have an abstractive summary of long-form answers at hand, we opt to use a decontextualization model to rewrite the extractive summary.\\n\\nDecontextualization (Choi et al., 2021) is a text editing task, which aims to rewrite the target sentence in a document such that the edited target sentence can be interpreted when presented alone while preserving its meaning. While its use cases in QA and text retrieval (Gao et al., 2022) have been explored, its use case in summarization has not been explored. Earlier prior work (Clarke and Lapata, 2010; Durrett et al., 2016) have studied discourse constraints for summarization \u2013 that for each pronoun included in the summary, the pronoun's antecedent should be included or the pronoun to be rewritten as a full mention to make summary coherent and clear. Decontextualization is well-suited to prevent these common errors of pronouns/concepts being \u201corphaned\u201d in extractive summary.\\n\\nFor extractive models, we exclude the question if it is chosen as the summary. For abstractive models, we remove the first sentence of the summary if it has high lexical overlap (over 75% unigram overlap) with the question (which happened for roughly 38% of the dataset).\\n\\n| Method            | Prediction Category Distribution | Decontextualization Attempted |\\n|-------------------|----------------------------------|-------------------------------|\\n| Wiki (NQ Short)   |                                  |                               |\\n| model             | Unnecessary: 14.7 Inf: 26.3 Done: 59.0 | 13%                           |\\n| Wiki (NQ Long)    |                                  |                               |\\n| model             | Unnecessary: 66.8 Inf: 13.9 Done: 19.3 | 28%                           |\\n| LFQA Answers      |                                  |                               |\\n| Wiki (NQ Long)    |                                  |                               |\\n| model             | Unnecessary: 49.3 Inf: 34.3 Done: 16.4 | 34%                           |\\n| Web-GPT model     |                                  |                               |\\n|                   | Unnecessary: 66.6 Inf: 14.6 Done: 18.8 | 29%                           |\\n\\nTable 4: Decontextualization output statistics. The second column block represents prediction category distribution, where Un represents unnecessary (no edit is necessary), Inf represents infeasible (stand-alone not feasible), and Done represents decontextualization attempted.\\n\\nWe use an off-the-shelf decontextualization system from recent work (Chen et al., 2021), which trained a T5 3B model on the original decontextualization dataset (Choi et al., 2021) on Wikipedia text. This model takes the concatenation of the Wikipedia page title and a paragraph with the sentence to be decontextualized as input. For ELI5 and WebGPT answers which lack a page title, we consider the question as the title.\"}"}
{"id": "acl-2023-long-541", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are edited. For these edited sentences, we report the length increase (\\\\(\\\\Delta\\\\)), or the average value of \\\\(\\\\frac{\\\\text{len}(\\\\text{decontext}) - \\\\text{len}(\\\\text{original})}{\\\\text{len}(\\\\text{original})}\\\\), following the original study.\\n\\nWhile decontextualization is attempted less frequently when it is decontextualized the length of the sentence increases more substantially. More ELI5 sentences were classified as Infeasible. We hypothesize that the sentences in ELI5 could be deemed more challenging because of the narrative nature of Reddit posts. We include sample decontextualization outputs in Table 9 in the appendix.\\n\\nWe manually examine decontextualization outputs from ELI5 and Web-GPT to evaluate their performance on out-of-domain, non-Wikipedia texts. We (the authors of this paper) randomly sample 50 examples where the model has made changes, and 50 examples from the entire set. Out of 50 edits, 42 edits were meaning preserving (without introducing factually incorrect contents), and 44 edits successfully decontextualized the sentence (without unresolved or unclear references). On a randomly sampled set of 50 examples, we evaluate whether the category assigned is correct (infeasible, unnecessary, done), finding 45 examples were assigned the correct category. Overall, we found the zero-shot performance of the decontextualization system on the new domain was surprisingly robust.\\n\\nRecent work (Eisenstein et al., 2022) also showed large language model can perform decontextualization robustly when prompted carefully. We will evaluate decontextualized summaries with a user study in Section 5.\\n\\n### 4.2 Abstractive Models\\n\\nIn this section, we explore abstractive models for summarization to improve fluency. Pegasus (Zhang et al., 2019) shows promising performance across diverse summarization benchmarks. We examine a suite of Pegasus fine-tuned on various summarization datasets and chose a model fine-tuned on the CNN/DailyMail as it showed the most promising results upon manual inspection. We do not fine-tune it with our extractive dataset to preserve its abstract nature.\\n\\nGPT-3 Recent work (Goyal et al., 2022a) has found that GPT-3 (Brown et al., 2020) exhibits strong zero-shot performance on several news summarization benchmarks. Unlike fine-tuned abstractive models, prompted language models would not inherit issues from noisy distant supervision training datasets. Thus, we investigate its ability to perform zero-shot long-form answer summarization. Specifically, we used the text-davinci-002 model. We explore two settings: with and without length control in the prompt, following prior work (Goyal et al., 2022a). The prompt with length control is \\\\(\\\\text{Q: } \\\\{\\\\text{question text} \\\\} \\\\text{ A: } \\\\{\\\\text{answer text}\\\\} \\\\text{ Summarize the above answer in } \\\\{\\\\text{length of gold summary}\\\\} \\\\text{ sentences}\\\\), and the prompt without length control is \\\\(\\\\text{Q: } \\\\{\\\\text{question text} \\\\} \\\\text{ A: } \\\\{\\\\text{answer text}\\\\} \\\\text{ Summarize the above answer.}\\\\)\\n\\n### 4.3 Automatic Evaluation\\n\\nWe first aim to perform an automatic evaluation of abstractive systems, using gold extractive summaries as references. While this would not evaluate fluency, automatic metrics measure the content selection of generated abstractive summaries.\\n\\n**Setting**\\n\\nWe use the same data split as in Section 3.5, and repeat lead baselines: LEAD-2 and LEAD-3. We use established automatic summarization evaluation metrics ROUGE (Lin, 2004) and BERTScore (Zhang* et al., 2020).\\n\\nAs our dataset is 3-way annotated, we report the highest ROUGE-L \\\\(F_1\\\\) score among the three reference answers and use the same reference answer to compute BERTScore \\\\(F_1\\\\). The Human baseline is computed by choosing one extractive summary annotation at random as the reference and doing a pairwise computation of ROUGE and BERTScore with the other two annotations for that example.\\n\\n**Results**\\n\\nTable 5 reports model performances on the test set. The results on the development set are in Table 7 in the appendix. Similar to other domains, lead baselines show strong performances, outperforming models trained on out-of-domain data (Pegasus, GPT3). Yet, they are inherently limited, covering only 73% of the summary sentences. We see that the abstractive models show better performance with the BERTScore metric compared to the ROUGE-L metric, potentially due to the ROUGE-L metric punishing for paraphrasing. Having the question in addition to the answer improves the performance of the Pegasus model. Having length control also improves the zero-shot performance of GPT-3, similar to the finding from Goyal et al. (2022a).\"}"}
{"id": "acl-2023-long-541", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Automatic evaluation results on the test set. For the \u201cInput\u201d column, A refers to a long answer while Q+A refers to (question, long answer) as an input to the model and L refers to the length of the gold extractive summary in sentences. For length, we present the number of tokens, with the number of sentences in the parenthesis.\\n\\nPrior work (Goyal et al., 2022b). This is a semi-oracle setting as the model is given the summary length.\\n\\n5. Human Evaluation of Summary Answers\\n\\nSo far we have evaluated summarized answers against the gold extractive summary. Yet, we are aware extractive answers themselves are limited and automatic evaluation of summary is non-trivial. To properly evaluate summarizing long-form answers, we launch a user study evaluating four different types of answer summaries: a gold extractive summary, a gold extractive summary that is decontextualized, an abstract summary from Pegasus, and an abstract summary from GPT3. Can the summarized answer present a useful, concise answer that preserves the original meaning of the long-form answer, without producing incoherent discourse structure (e.g., orphaned anaphora)?\\n\\n5.1 User Study Design\\n\\nWe design a two-stage interface to evaluate the summarized answer. The exact wording and interface can be found in the appendix (Figures 5, 6, 7, and 8). First, they are shown the summary answer and the question alone, and then, the original long-form answer will be shown to them.\\n\\nStage 1: The annotators first measure the quality of the summary answer itself.\\n\\n- Fluency (choices: Yes/No): if the answer is grammatical and fluent. We do not distinguish coherence and fluency as prior study (Fabbri et al., 2021) reports that annotators often confuse those two dimensions.\\n- Adequacy (choices: Yes/Partially/No): if the summary adequately answers the original question.\\n\\nStage 2: The annotators then measure both the summary and original long-form answer.\\n\\n- Faithfulness (choices: Yes/No): if the summary accurately captures the main idea of a long-form answer regarding the question.\\n- Long-Answer Adequacy (choices: Yes/Partially/No): if the long-form answer addresses the question adequately. This annotation only evaluates the original long-form answer, as a control to avoid blaming the summarization system when the long answer itself is not adequate. As we filtered out invalid long answers during pre-processing, most answers should be labeled as adequate.\\n\\n5.2 User Study Setting\\n\\nData\\n\\nWe annotate 175 long-form answers paired with four types of summary: (1) summary generated from our best abstractive model (Pegasus), (2) gold extractive summary (GOLD), (3) gold extractive summary that is decontextualized with automatic decontextualizer system (GOLD++) and (4) GPT-3 zero shot summaries with length restriction. We sample 150 examples at random and additionally sample 25 examples where the decontextualization process made edits to the gold extractive summary. The average length of the tokens for the four summary settings were 43.4, 40.9, 47.6, and 31.3 for Pegasus, GOLD, GOLD++, GPT3.\\n\\nAnnotators\\n\\nHuman evaluation was done on the Amazon Mechanical Turk platform. We required the workers to be from English-speaking countries and have at least a 95% acceptance rate on 1000+ HITs. Each worker was paid $0.50 per annotation, translating to an hourly rate of $15. We set up the task that each annotator will see only one variant of the summary per each long-form answer. The annotators were not aware of which summarization system provided the summary. A small subset of data is annotated by the authors, following the same setup. We had 561 unique annotators for this task.\\n\\n5.3 Results\\n\\nTable 6 presents the results from the user study. We report two numbers \u2013 one on all 175 examples, and one on a subset of 63 examples where decontextualization changed the extractive summary.\"}"}
{"id": "acl-2023-long-541", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: User study results. The first row shows Fleiss\u2019 kappa for each question. The rest of the rows present the percentage of examples in each category, with results on the subset of 63 examples where decontextualization modified the extractive summary presented in parenthesis. The last column presents the percentage of functional short answers, meaning they are adequate, fluent, and meaning-preserving.\\n\\nWe include the inter-annotator agreement for each question in the first row. We observed moderate to high agreement for all four questions. Evaluating the quality of answers (summary adequacy and long answer adequacy) was more subjective than evaluating fluency or faithfulness, revealing the challenge of open-ended long-form answer evaluation as pointed out in prior work (Krishna et al., 2021). We also see high agreement among annotators by comparing long answer adequacy distributions across four rows, which are very similar as expected.\\n\\nCan a summarized version of long-form answers provide an adequate answer to the original question? We see somewhat mixed results \u2013 while the annotators said the summaries provide at least a partial answer to the question most of the time (over 90%), only about 60% of answers per system provide adequate answers. Again, we find that decontextualization helps \u2013 on about 10% examples, annotators labeled extractive answers as partially adequate, but their decontextualized versions are adequate.\\n\\nGPT-3 produces adequate summaries the most, showcasing its powerful zero-shot summarization ability (Goyal et al., 2022a). Further analysis showed that summary adequacy is highly system dependent rather than question dependent \u2013 for 90% of the questions, there is at least one system whose outputs are adequate according to the majority of the annotators.\\n\\nWe find fluency is not a major issue, both for extractive and abstractive systems. The large-scale language model (GPT3), in particular, provides the most fluent answers. For the extractive summaries, we see a substantial gain (about 10% on 63 examples where decontextualization changed the input) in fluency by introducing contextualization. The fluency gap between Gold and Gold++ was statistically significant on McNemar\u2019s test with $p < 0.05$.\\n\\nWe observe a slightly lower performance on faithfulness across four summary systems compared to fluency. While the weaker abstractive model (Pegasus) ranks slightly lower than the extractive model, GPT-3 somewhat surprisingly outperforms extractive approaches in meaning preservation. This mirrors findings from a recent study (Zhang et al., 2022) about how extractive summary can also introduce factual errors. Overall, faithfulness has been extensively studied in summarization literature (Fabbri et al., 2022a) but mostly in the news domain.\\n\\nWhen can we use summarized answers? In the last column, we report the percentage of summary answers that are fluent, adequate, and faithful to the original long-form answer. Decontextualized answers (GOLD++) and GPT-3 zero-shot summary achieve more promising results than the other two approaches. Of the 168 long answers considered \u201cadequate\u201d by a majority of the annotators, 160 (95%) of them has at least one summary that was considered functional by a majority of the annotators. We examine error cases in the next section.\\n\\n5.4 What makes it hard for models to summarize long-form answers? As we have identified fluency as a minor issue, we specifically look at 60 examples that satisfy all the following conditions: (1) summary is fluent, (2) summary answer is not fully adequate nor faithful, and (3) long-form answer is adequate.\\n\\nWe identify a few patterns of why the summary answers fall short: (1) around 10% of them contain summarization errors (e.g. not properly resolving anaphora or hallucination). (2) for around 60% of examples, adding a few more sentences to the summary was necessary to provide a coherent answer to the question. This is particularly true in cases.\"}"}
{"id": "acl-2023-long-541", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q: Why do most restaurants sell Pepsi instead of Coke, and yet Coke is seen to be a bigger competitor?\\n\\nA: Coke sells way more soda by volume than Pepsi. As a response, Pepsi offers its products to restaurants at a reduced cost, which is why many restaurants carry it. But only up to midscale places \u2013 no nice restaurant serves Pepsi, because Coke has more cachet, and also you need it for mixed drinks. Note also that McDonald\u2019s, the single biggest restaurant chain in the world, serves Coke.\\n\\nQ: How is it that the human brain/body sometimes wakes up seconds before an alarm goes off?!\\n\\nA: Your body does have internal regulation mechanisms, I'm not a doctor and there are plenty who are who can talk more intelligently about the circadian rhythm of the body etc. The other component is psychological. What's happening is an example of confirmation bias. You've woken up a few times almost on the clock (relative to the total number of days you've ever slept in your life). Though this number is astronomical low, you only remember the times you did wake up on the minute. You bias yourself to count those times and subconsciously ignore the other times and thus you feel as though you have an ability to wake up on time. This also happens when people think that they can catch when people are looking at them. You sometimes do and sometimes don't, but the times you don't are not out of the ordinary so you forget them. Thus you only remember catching them and get a false sense of confirmation.\\n\\nGPT-3 summary: The human brain/body sometimes wakes up seconds before an alarm goes off because of the body's internal regulation mechanisms and the psychological phenomenon of confirmation bias.\\n\\nFigure 1: Examples with inadequate summaries: In the first example, the highlighted extractive summaries need further decontextualization. In the second example, the long-form answer is too complex. Where the answers are multifaceted (e.g., providing multiple reasons for some phenomena, and the current summary contains only one of them). We also noticed a few cases where disclaimers (e.g., \u201cI'm talking about poverty in U.S.\u201d) or counterexamples in the long-form answer that were not included in the summary, potentially misleading the readers. (3) Some long-form answers (around 25%) are tricky to summarize without massive rewriting as it is explaining a complex procedure (e.g., why the Obama administration could not pass legislation, see the full example in Table 1). Figure 1 presents two representative failure cases. Future QA models can actively identify questions that require comprehensive v.s. concise answers.\\n\\n6 Related Work\\n\\nQuery/Aspect-focused summarization\\nOur task is relevant to query-focused summarization, which studies controllable summarization with respect to a query (Xu and Lapata, 2020; Deng et al., 2020; Zhu et al., 2020; Vig et al., 2021) or aspect (Angelidis et al., 2021; Hayashi et al., 2021; Ahuja et al., 2022; Kulkarni et al., 2020). Recently proposed MASH-QA (Zhu et al., 2020) dataset on the medical domain presents a question, context document, and extractive answer sentences. Compared to these works which summarize documents written independently of the question into a summary, we aim to compress long-form answers written with respect to the question. Another line of work (Fabbri et al., 2022b; Song et al., 2017) studies generating summaries of multiple answers to the same question. Lastly, Deng et al. (2019) looks into the same task formulation of summarizing long-form answers, but their evaluation is limited to distantly supervised data.\\n\\nDecontextualization for summarization\\nSlobodkin et al. (2022) proposes the task of controllable text reduction, which rewrites chosen sentences from a document in a coherent manner using existing summarization datasets. They cover longer documents and involve multiple sentences to be decontextualized whereas we reuse a single-sentence decontextualization model (Choi et al., 2021).\\n\\n7 Conclusion and Future Work\\nWe present the first study on generating concise answers to complex questions. We collect an extractive summarization dataset in the new summarization domain of long-form answers to support future research. To address this new task, we deploy diverse summarization models, including zero-shot abstractive summarization models and a new decontextualization postprocessing method, which is applied to extractive summaries. Through our comprehensive user study, we find that around 70% of the summaries can serve as functional, concise answers to the original questions. Our work shows potential for building QA systems that generate answers at different granularities, as well as using decontextualization to improve the faithfulness and fluency of extractive summaries. Future work can also look into applying controllable generation techniques (Yang and Klein, 2021; Li et al., 2022; Qin et al., 2022) to generate answers with different lengths to generate concise answers.\"}"}
{"id": "acl-2023-long-541", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nOur study is limited in scope, studying only English question-answering data. We also acknowledge that the long-form answers we study are not always factually correct, as they can be outdated (Zhang and Choi, 2021) or incorrect as they are crawled from web forums (Fan et al., 2019).\\n\\nFurther, our user study is limited in its scale, evaluating 175 instances, and does not carefully study potentially diverging interpretations from annotators of different demographics. We also do not extensively explore all summarization models, such as the extract-and-abstract approaches mentioned in related work.\\n\\nEthics Statement\\n\\nOur data collection and user study protocols do not collect identifiable private information from annotators.\\n\\nThe question-answering data we annotated comes from an English online forum and might contain biased information. Our annotation is done by crowd-workers recruited from an online platform. We make use of pre-trained language models to generate abstractive summaries, which could suffer from hallucinating unfactual contents (Kang and Hashimoto, 2020) and perpetuating bias (Field et al., 2021). Thus, more post-processing steps are required before presenting these contents to users. Our user study shows that our proposed method, extract-and-decontextualize, could be one effective post-processing step to reduce hallucination.\\n\\nAcknowledgements\\n\\nWe thank Tanya Goyal, Jessy Li, Jiacheng Xu, and members of the UT Austin NLP community for their helpful feedback on the draft. We thank Ji-fan Chen for sharing the decontextualization model with us. We also thank the reviewers and meta-reviewer of the ACL community for helpful comments and feedback on the earlier draft of the paper. Lastly, we would like to thank the crowdworkers for their help with our data annotation and user study. The work is partially supported by a gift from Google Faculty Research Award.\\n\\nReferences\\n\\nOjas Ahuja, Jiacheng Xu, Akshay Kumar Gupta, Kevin Horecka, and Greg Durrett. 2022. Aspectnews: Aspect-oriented summarization of news documents. In ACL.\\n\\nStefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021. Extractive opinion summarization in quantized transformer spaces. Transactions of the Association for Computational Linguistics, 9:277\u2013293.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. ArXiv, abs/2005.14165.\\n\\nShuyang Cao and Lu Wang. 2021. Controllable open-ended question generation with a new question type ontology. ArXiv, abs/2107.00152.\\n\\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. ArXiv, abs/1711.04434.\\n\\nJifan Chen, Eunsol Choi, and Greg Durrett. 2021. Can nli models verify qa systems' predictions? ArXiv, abs/2104.08731.\\n\\nEunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. 2021. Decontextualization: Making sentences stand-alone. CoRR, abs/2102.05169.\\n\\nJames Clarke and Mirella Lapata. 2010. Discourse constraints for document compression. Computational Linguistics, 36:411\u2013441.\\n\\nYang Deng, Wai Lam, Yuexiang Xie, Daoyuan Chen, Yaliang Li, Min Yang, and Ying Shen. 2019. Joint learning of answer selection and answer summary generation in community question answering. In AAAI Conference on Artificial Intelligence.\\n\\nYang Deng, Wenxuan Zhang, and Wai Lam. 2020. Multi-hop inference for question-driven summarization. In Conference on Empirical Methods in Natural Language Processing.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.\\n\\nGreg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein. 2016. Learning-based single-document summarization with compression and anaphoricity constraints. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1998\u20132008, Berlin, Germany. Association for Computational Linguistics.\"}"}
{"id": "acl-2023-long-541", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael Collins, and David Mimno. 2022. Honest students from untrusted teachers: Learning an interpretable question-answering pipeline from a pretrained language model. ArXiv, abs/2210.02498.\\n\\nA. R. Fabbri, Wojciech Kryscinski, Bryan McCann, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391\u2013409.\\n\\nAlexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022a. QAFactEval: Improved QA-based factual consistency evaluation for summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2587\u20132601, Seattle, United States. Association for Computational Linguistics.\\n\\nAlexander Fabbri, Xiaojian Wu, Srini Iyer, Haoran Li, and Mona Diab. 2022b. AnswerSumm: A manually-curated dataset and pipeline for answer summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2508\u20132520, Seattle, United States. Association for Computational Linguistics.\\n\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: long form question answering. CoRR, abs/1907.09190.\\n\\nAnjalie Field, Su Lin Blodgett, Zeerak Waseem, and Yulia Tsvetkov. 2021. A survey of race, racism, and anti-racism in NLP. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1905\u20131925, Online. Association for Computational Linguistics.\\n\\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, N. Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2022. Rarr: Researching and revising what language models say, using language models.\\n\\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022a. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356.\\n\\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022b. Snac: Coherence error detection for narrative summarization. ArXiv, abs/2205.09641.\\n\\nHiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham Neubig. 2021. Wikiasp: A dataset for multi-domain aspect-based summarization. Transactions of the Association for Computational Linguistics, 9:211\u2013225.\\n\\nWan Ting Hsu, Chieh-Kai Lin, Ming-Ying Lee, Kerui Min, Jing Tang, and Min Sun. 2018. A unified model for extractive and abstractive summarization using inconsistency loss. ArXiv, abs/1805.06266.\\n\\nDaniel Kang and Tatsunori B. Hashimoto. 2020. Improved natural language generation via loss truncation. In ACL.\\n\\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4940\u20134957, Online. Association for Computational Linguistics.\\n\\nWojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural text summarization: A critical evaluation. In EMNLP.\\n\\nSayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. 2020. Aquamuse: Automatically generating datasets for query-based multi-document summarization. arXiv preprint arXiv:2010.12694.\\n\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenneth Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452\u2013466.\\n\\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. 2022. Diffusion-lm improves controllable text generation. ArXiv, abs/2205.14217.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\\n\\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating verifiability in generative search engines. ArXiv, abs/2304.09848.\\n\\nPeter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam M. Shazeer. 2018. Generating wikipedia by summarizing long sequences. ArXiv, abs/1801.10198.\\n\\nYang Liu and Mirella Lapata. 2019. Text summarization with pretrained encoders. CoRR, abs/1908.08345.\\n\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332.\"}"}
{"id": "acl-2023-long-541", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ramesh Nallapati, Bing Xiang, and Bowen Zhou. 2016. Sequence-to-sequence RNNs for text summarization. CoRR, abs/1602.06023.\\n\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. CoRR, abs/1808.08745.\\n\\nJonathan Pilault, Raymond Li, Sandeep Subramanian, and Christopher Joseph Pal. 2020. On extractive and abstractive neural document summarization with transformer language models. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), page 9308\u20139319.\\n\\nLianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. 2022. Cold decoding: Energy-based constrained text generation with langevin dynamics. ArXiv, abs/2202.11705.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In EMNLP.\\n\\nAlexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization. In EMNLP.\\n\\nA. See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer-generator networks. ArXiv, abs/1704.04368.\\n\\nAviv Slobodkin, Paul Roit, Eran Hirsch, Ori Ernst, and Ido Dagan. 2022. Controlled text reduction.\\n\\nHongya Song, Zhaochun Ren, Shangsong Liang, Piji Li, Jun Ma, and M. de Rijke. 2017. Summarizing answers in non-factoid community question-answering. Proceedings of the Tenth ACM International Conference on Web Search and Data Mining.\\n\\nJesse Vig, Alexander R. Fabbri, Wojciech Kry\u015bci\u0144ski, Chien-Sheng Wu, and Wenhao Liu. 2021. Exploring neural models for query-focused summarization.\\n\\nShufan Wang, Fangyuan Xu, Laure Thompson, Eunsol Choi, and Mohit Iyyer. 2022. Modeling exemplification in long-form question answering via retrieval. In North American Chapter of the Association for Computational Linguistics.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierre Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface\u2019s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.\\n\\nFangyuan Xu, Junyi Jessy Li, and Eunsol Choi. 2022. How do we answer complex questions: Discourse structure of long-form answers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.\\n\\nFangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. 2023. A critical evaluation of evaluations for long-form question answering.\\n\\nYumo Xu and Mirella Lapata. 2020. Query focused multi-document summarization with distant supervision. ArXiv, abs/2004.03027.\\n\\nKevin Yang and Dan Klein. 2021. FUDGE: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3511\u20133535, Online. Association for Computational Linguistics.\\n\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2019. PEGASUS: pre-training with extracted gap-sentences for abstractive summarization. CoRR, abs/1912.08777.\\n\\nMichael J.Q. Zhang and Eunsol Choi. 2021. Situatedqa: Incorporating extra-linguistic contexts into qa. ArXiv, abs/2109.06157.\\n\\nShiyue Zhang, David Wan, and Mohit Bansal. 2022. Extractive is not faithful: An investigation of broad unfaithfulness problems in extractive summarization. ArXiv, abs/2209.03549.\\n\\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.\\n\\nMing Zhu, Aman Ahuja, Da-Cheng Juan, Wei Wei, and Chandan K. Reddy. 2020. Question answering with long multiple-span answers. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3840\u20133849, Online. Association for Computational Linguistics.\"}"}
{"id": "acl-2023-long-541", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3 Dataset Compression Statistics\\n\\nFigure 2 plots the token-level compression ratio (% of tokens included in the summary) on the three different types of long-form answers we study.\\n\\nB Model Training Details\\n\\nAll models are trained/evaluated on NVIDIA Quadro RTX 8000 GPUs. We use pytorch-transformers Wolf et al. (2019) to implement our models. The hyperparameters are manually searched by the authors.\\n\\nPreSumm\\n\\nWe use the checkpoint of BertSumExt from https://github.com/nlpyang/PreSumm. We use the same hyperparameter in the original paper, using a batch size of 16 and a learning rate of \\\\(2 \\\\times 10^{-3}\\\\). On two GPUs, fine-tuning on the training set and then evaluating on the test set takes between 1 to 2 hours.\\n\\nT5\\n\\nWe use the T5-large checkpoint with 770 million parameters and fine-tune for 30 epochs with a batch size of 16 and learning rate of \\\\(1 \\\\times 10^{-4}\\\\). On two GPUs, fine-tuning on the training set and then evaluating on the test set takes between 2 to 3 hours.\\n\\nB.1 Validation Set Results\\n\\nTables 7 and 8 show our automatic evaluation results on the validation set for the extractive and abstractive models (computed in the same way that the test set values were).\\n\\nB.2 Decontextualization Sample Output\\n\\nTable 9 gives three examples of the modifications that the decontextualization models made to the extractive gold label summaries.\\n\\n| Model Input | ROUGE | BERTScore | Length |\\n|-------------|-------|-----------|--------|\\n| LEAD-2 A    | 0.541 | 0.677     | 38.36 (2.00) |\\n| LEAD-3 A    | 0.641 | 0.710     | 58.29 (3.00) |\\n| Pegasus A   | 0.571 | 0.750     | 43.50 (2.77) |\\n| Pegasus Q+A | 0.572 | 0.752     | 41.11 (2.64) |\\n| GPT3 (length) | 0.517 | 0.681 | 32.29 (1.68) |\\n| GPT3       | 0.507 | 0.683     | 48.03 (2.24) |\\n| Human      | 0.815 | 0.883     | 39.62 (1.95) |\\n\\nTable 7: Automatic summary evaluation results on the validation set. For the \u201cInput\u201d column, A refers to using only the long answer as an input to the model while Q+A provides the long answer with the question prepended as an input. The length is computed in the number of tokens/words and the number in parenthesis represents the average number of sentences.\\n\\n| PRF1 EM | % |\\n|--------|---|\\n| LEAD-2 | 0.42 0.74 0.51 11.3 |\\n| LEAD-3 | 0.47 0.81 0.55 5.6 |\\n| PreSumm-cnn (A) | 0.47 0.75 0.55 11.7 |\\n| PreSumm-cnn (Q+A) | 0.52 0.78 0.60 21.8 |\\n| PreSumm-cnn+ours (A) | 0.56 0.89 0.65 28.1 |\\n| PreSumm-cnn+ours (Q+A) | 0.58 0.91 0.68 35.9 |\\n| T5-ours (A) | 0.70 0.73 0.66 20.0 |\\n| T5-ours (Q+A) | 0.73 0.78 0.71 26.3 |\\n| Human | 0.76 0.80 0.77 40.8 |\\n\\nTable 8: Binary classification accuracy of extractive summarization models on the validation set.\"}"}
{"id": "acl-2023-long-541", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-541", "page_num": 15, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-541", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: User study instructions (Step 1)\"}"}
