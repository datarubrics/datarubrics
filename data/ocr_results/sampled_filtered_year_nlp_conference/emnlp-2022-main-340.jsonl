{"id": "emnlp-2022-main-340", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SUPER-NATURAL INSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks\\n\\nYizhong Wang\\nSwaroop Mishra\\nPegah Alipoormolabashi\\nYeganeh Kordi\\nAmirreza Mirzaei\\nAnjana Arunkumar\\nArjun Ashok\\nArut Selvan Dhanasekaran\\nAtharva Naik\\nDavid Stap\\nEshaan Pathak\\nGiannis Karamanolakis\\nHaizhi Gary Lai\\nIshan Purohit\\nIshani Mondal\\nJacob Anderson\\nKirby Kuznia\\nKrima Doshi\\nMaitreya Patel\\nKuntal Kumar Pal\\nMehrad Moradshahi\\nMihir Parmar\\nMirali Purohit\\nNeeraj Varshney\\nPhani Rohitha Kaza\\nPulkit Verma\\nRavsehaj Singh Puri\\nRushang Karia\\nShailaja Keyur Sampat\\nSavan Doshi\\nSiddhartha Mishra\\nSujan Reddy\\nSumanta Patro\\nTanay Dixit\\nXudong Shen\\nChitta Baral\\nYejin Choi\\nNoah A. Smith\\nHannaneh Hajishirzi\\nDaniel Khashabi\\n\\n1 Introduction\\nThe NLP community has witnessed great progress in building models for generalization to unseen tasks via in-context instructions (Mishra et al., 2022b) using large pretrained language models (Raffel et al., 2020; Brown et al., 2020). As remarkable as models like InstructGPT (Ouyang et al., 2022) are, the contribution of various design choices to their success is opaque. In particular, the role of supervised data has remained understudied due to limited data released by the corporate entities behind major models. In addition, it is nearly impossible for the research community to extend and re-train these gigantic models. Addressing these two challenges is critical for advancing the state of the art in NLP.\\n\\n2 Dataset\\n\\nSUPER-NATURAL INSTRUCTIONS represents a super-sized expansion of NATURAL INSTRUCTIONS (Mishra et al., 2022b) which had 61 tasks. The dataset, models, and a leaderboard can be found at https://instructions.apps.allenai.org.\\n\\n\u2022 Input: \\\"Context: \u2026 That's fantastic, I'm glad we came to something we both agree with.\\\" Utterance: 'Me too. I hope you have a wonderful camping trip.'\\n\u2022 Output: \\\"Yes\\\"\\n\u2022 Explanation: \\\"The participant engages in small talk when wishing their opponent to have a wonderful trip.\\\"\\n\\n\u2022 Input: \\\"Context: \u2026 Sounds good, I need food the most, what is your most needed item?!\\\" Utterance: 'My item is food too'.\\n\u2022 Output: \\\"Yes\\\"\\n\u2022 Explanation: \\\"The utterance only takes the negotiation forward and there is no side talk. Hence, the correct answer is 'No'.\\\"\\n\\nDefinition\\n\\nGiven an utterance and recent dialogue context containing past 3 utterances (wherever available), output 'Yes' if the utterance contains the small-talk strategy, otherwise output 'No'. Small-talk is a cooperative negotiation strategy. It is used for discussing topics apart from the negotiation, to build a rapport with the opponent.\\n\\nFigure 1: An example task from SUPER-NATURAL INSTRUCTIONS adopted from Chawla et al. (2021). A successful model is expected to use the provided instructions (including task definition and demonstration examples) to output responses to a pool of evaluation instances.\\n\\nEvaluation\\n\\nTk-Instruct\\n\u2022 Input: \\\"Context: \u2026 I am excited to spend time with everyone from camp!\\\" Utterance: 'That's awesome! I really love being out here with my son. Do you think you could spare some food?'\\n\u2022 Expected Output: \\\"Yes\\\"\\n\\nPositive Examples\\n\\nNegative Examples\\n\\nEvaluation Instances\\n\\nTk-Instruct\\n\\n2022b; Sanh et al., 2022; Wei et al., 2022) using large pretrained language models (Raffel et al., 2020; Brown et al., 2020). As remarkable as models like InstructGPT (Ouyang et al., 2022) are, the contribution of various design choices to their success is opaque. In particular, the role of supervised data has remained understudied due to limited data released by the corporate entities behind major models. In addition, it is nearly impossible for the research community to extend and re-train these gigantic models. Addressing these two challenges is critical for advancing the state of the art in NLP.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This work comprises a wide variety of NLP tasks with their corresponding instruction types. Evaluating models that can generalize to unseen instructions is a critical challenge. T5 (Sanh et al., 2022) instead. T0 (Sanh et al., 2022) consists of a new task given the instruction, outperforming crossfit (Ye et al., 2021) and promptsource (Bach et al., 2020) in terms of training of the T5 model (Raffel et al., 2020) over all the task instructions in our training set, and is evaluated on in-context instructions (task definition or task type) from other datasets.\\n\\n- Resource: SUP-NATINST\\n- (this work)\\n- NATINST: (Mishra et al., 2022b)\\n- CROSSFIT: (Ye et al., 2021)\\n- PROMPTSOURCE: (Bach et al., 2020)\\n\\n### Consistency Table\\n\\n| Is public? | Has non-English tasks? | Resource | Task Definition | Task Type | Has Task Instructions? | Number of annotated task types |\\n|------------|------------------------|----------|----------------|-----------|------------------------|-----------------------------|\\n| \u2713          | \u2717                      | \u2713        | \u2713              | \u2713         | \u2717                      | 13                          |\\n| \u2713          | \u2717                      | \u2713        | \u2713              | \u2713         | \u2717                      | 13                          |\\n| \u2717          | \u2713                      | \u2713        | \u2713              | \u2713         | \u2717                      | 13                          |\\n\\n### Task Details\\n\\n- Avg. task definition length (words): 56.6\\n- 134.4\\n- 24.8\\n- 8.2\\n\\n- Has task instructions: Yes\\n- Has task types: Yes\\n- Is public: Yes\\n- Has non-English tasks: No\\n\\n### Example Tasks\\n\\n1. **Answer Generation**\\n   - Description: Generates an answer to a given question or prompt.\\n   - Example: Given a question, generate a detailed, coherent answer.\\n\\n2. **Structure to Text**\\n   - Description: Converts a hierarchical or structured format into a text representation.\\n   - Example: Convert a JSON file into a readable text.\\n\\n3. **Translation**\\n   - Description: Translates text from one language to another.\\n   - Example: Translate a sentence from English to French.\\n\\n4. **Answer Verification**\\n   - Description: Verifies the correctness of an answer.\\n   - Example: Verify if the generated code satisfies the specified requirements.\\n\\n5. **Question Rewriting**\\n   - Description: Reformulates a question into a new, more natural or concise form.\\n   - Example: Rewrite a complex question into a simpler, more understandable version.\\n\\n6. **Overlap Extraction**\\n   - Description: Identifies overlapping information between two or more texts.\\n   - Example: Extract overlapping phrases from a set of documents.\\n\\n7. **Intent Identification**\\n   - Description: Determines the underlying intent or purpose behind a given text.\\n   - Example: Identify the intentions behind a set of user comments.\\n\\n8. **Data to Text**\\n   - Description: Converts data into a textual format.\\n   - Example: Convert a spreadsheet into a narrative text.\\n\\n9. **Story Composition**\\n   - Description: Generates a coherent story or narrative.\\n   - Example: Create a compelling story from a set of given elements.\\n\\n10. **Language Identification**\\n    - Description: Determines the language of a given text.\\n    - Example: Identify the language of a document.\\n\\n11. **Other**\\n    - Description: Miscellaneous tasks not categorized under the above lists.\\n    - Example: A task that does not fit into any of the predefined categories.\\n\\n### Other Aspects\\n\\n- **Translation**\\n- **Named Entity Recognition**\\n- **Natural language inference**\\n- **Misc.**\\n- **Text to Code**\\n- **Word Analogy**\\n- **Misc.**\\n- **QA**\\n- **Extractive Entity Relation Classification**\\n- **Paraphrase**\\n- **Fact Verification**\\n- **Open QA**\\n- **Summarization**\\n- **Sentence Ordering**\\n- **Cause Effect Classification**\\n- **Stance Detection**\\n- **Minimal Text Modification**\\n- **Language Identification**\\n- **Other**\\n- **Text Categorization**\\n- **Text Matching**\\n- **Word Relation Classification**\\n- **Negotiation Strategy Detection**\\n- **Answerability Classification**\\n- **Code to Text**\\n- **Incorrect Answer Generation**\\n- **Ethics Classification**\\n- **Text Completion**\\n- **Dialogue Act Recognition**\\n- **Gender Classification**\\n- **Data to Text**\\n- **Story Composition**\\n- **Question Rewriting**\\n- **Answer Verification**\\n- **Overlap Extraction**\\n- **Intent Identification**\\n- **Text Matching**\\n- **Word Relation Classification**\\n- **Negotiation Strategy Detection**\\n- **Answerability Classification**\\n- **Code to Text**\\n- **Incorrect Answer Generation**\\n- **Ethics Classification**\\n- **Text Completion**\\n- **Dialogue Act Recognition**\\n- **Gender Classification**\\n- **Data to Text**\\n- **Story Composition**\\n- **Question Rewriting**\\n- **Answer Verification**\\n- **Overlap Extraction**\\n- **Intent Identification**\"}"}
{"id": "emnlp-2022-main-340", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Interesting, an 11B-parameter \\\\textit{Tk-INSTRUCT} can outperform the 175B-parameter \\\\textit{InstructGPT} model by 9.9 ROUGE-L points when evaluated on 119 unseen English tasks, and the multilingual variant \\\\textit{mTk-INSTRUCT} outperforms \\\\textit{InstructGPT} by 13.3 points on 35 non-English tasks (\u00a76.1). According to human evaluation, \\\\textit{Tk-INSTRUCT} generates responses at least as well as the ground truth for 77% of the testing instances (\u00a76.2), confirming its strong generalization to unseen tasks.\\n\\nThe compelling empirical performance of \\\\textit{Tk-INSTRUCT} confirms the importance of super-sized meta datasets such as our \\\\textit{SUP-NAT} to facilitate research towards generalizable NLP models. We conduct extensive analysis to understand the important factors for this generalization (\u00a77). Our analysis shows that scaling up the diversity of training tasks and the model size are both important for strong generalization to unseen tasks. Finally, we estimate performance upper bounds, suggesting further room for improvement.\\n\\n2 Related Work\\n\\nLanguage instructions are a versatile way of defining goals, which is why they have been studied in the context of a variety of applications, such as instructions in grounded environments (Shridhar et al., 2020; Stepputtis et al., 2020; Min et al., 2022b; Weir et al., 2022) and database commands (Kim et al., 2020). Here, we focus on applications of instructions for general NLP tasks.\\n\\nRecent literature has been motivated by building models that are generalizable across a variety of NLP tasks, when prompted with either a few examples (Ye and Ren, 2021; Bragg et al., 2021) or language definitions (Efrat and Levy, 2020; Weller et al., 2020; Zhong et al., 2021; Mishra et al., 2022b,a; Parmar et al., 2022). Our work is related to the existing benchmarks in this line of work, as delineated in Table 1 along various dimensions. Our benchmark extends \\\\textit{NAT} (Mishra et al., 2022b) with 26 \u00d7 more tasks and greater variety of task types (Fig. 2). While \\\\textit{CROSSFIT} (Ye et al., 2021) focuses on benchmarking with a few in-context examples, our benchmark also offers task instructions.\\n\\nConcurrent to our work, \\\\textit{PROMPTSOURCE} (Bach et al., 2022) is another benchmark of tasks and their language instructions (prompts). An important distinction between this benchmark and ours is the phrasing of the task definitions: while \\\\textit{PROMPTSOURCE} task definitions are relatively concise, our task definitions are collected with the intention of providing a complete definition of each task and therefore are longer (24 tokens vs. 56 tokens on average; Table 1). More recently, \\\\textit{BIGBENCH} (Srivastava et al., 2022) introduces a collection of 204 tasks and also provides short task descriptions and input prefixes that can be used for prompting LMs. With little overlap to our collection of tasks, they focus more on finding challenging tasks that can be used to test different behaviors of current LMs. Nevertheless, we believe that all these efforts in collecting different tasks as well as the task instructions are complementary, and the community will benefit from considering different benchmarks. Finally, the well-adopted \\\\textit{InstructGPT} model (Ouyang et al., 2022) is partially enabled by a large dataset of prompts that are collected via various synthetic data augmentation which, unfortunately, is not publicly available.\\n\\nBeyond cross-task generalization, our benchmark can also be used to study multi-task learning more broadly, which is a longstanding goal for AI (Caruana, 1997). Traditionally, this literature focuses on setups that involve evaluation on tasks that are observed during training (Collobert and Weston, 2008; Hashimoto et al., 2017). More recent studies show promise that large-scale multi-task learning can enable strong generalization to similar tasks via unified encoding (Khashabi et al., 2020; Xie et al., 2022) or better finetuning results on downstream tasks (McCann et al., 2018; Aribandi et al., 2022). Our proposed benchmark provides diverse tasks for studying multi-tasking at a massive scale.\\n\\n3 \\\\textit{SUP-NATURAL INSTRUCTIONS}\\n\\n\\\\textit{SUP-NATURAL INSTRUCTIONS} is a metadataset (Triantafillou et al., 2019) consisting of a variety of NLP tasks (see Fig. 2a) and instructions that describe them in plain language.\\n\\nInstruction schema.\\n\\nAll task instructions follow the same uniform schema (see Fig. 1) which is composed of the following parts:\\n\\n\u2022 \\\\textit{DEFINITION} defines a given task in natural language. This is a complete definition of how an input text (e.g., a sentence or a document) is expected to be mapped to an output text.\\n\\n\u2022 \\\\textit{POSITIVE EXAMPLES} are samples of inputs and their correct outputs, along with a short explanation for each.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NEGATIVE EXAMPLES are samples of inputs and their incorrect/invalid outputs, along with a short explanation for each. The above schema is based on that of Mishra et al. (2022b), though it is simplified. See Appendix C for the comparison.\\n\\nTask instances. Given the instructions for each task, a model is expected to solve instances of that task. We use a unified format to organize the instances of all our tasks. More precisely, each instance consists of a textual input and a list of acceptable textual outputs. We limit the number of instances in each task to 65K to avoid an imbalanced distribution of instances between tasks.\\n\\nBenchmark collection. The benchmark was collected through a large community effort on GitHub. 3 Tasks were collected and contributed by NLP practitioners who were either responding to our public invitation or students who were encouraged to contribute as part of their class project. Contributors were encouraged to be creative and source the tasks from several resources: (a) existing public NLP datasets, (b) available intermediate annotations in crowdsourcing experiments (e.g., paraphrasing questions or rating their quality during crowdsourcing a QA dataset), or (c) synthetic tasks that can be communicated to an average human in a few sentences (e.g., basic algebraic operations like number comparison, finding the longest palindrome substring, etc.). When using existing datasets or crowdsourcing annotations, contributors were encouraged to adopt the instructions used to create this dataset whenever available. This was done to ensure that the instructions were sufficient to define the tasks to average human readers. Tasks along with instructions and other meta information were contributed as JSON files via GitHub pull requests, which were reviewed by automated checks and peers. We had 88 contributors from diverse locations and backgrounds contribute to our repository.\\n\\nQuality control. Controlling the quality of this community-contributed data was done in several phases: (1) Upon creating a GitHub pull request of the proposed task, it immediately went through an automatic test. This process verified that the introduced file contained the expected fields and adhered to our desired properties (e.g., no duplicate instances, the output labels are not heavily imbalanced, etc.) and (2) The proposed task was then peer-reviewed by 1\u20132 other expert contributors to ensure the clarity and sufficiency of instruction content. The review process was done iteratively until the reviewers were content with the quality of the proposed instruction. Specifically, reviewers were asked to verify that the instruction is clear and sufficient for an average language speaker to solve the underlying task (evaluation instances) while being grammatical, fluent, and concise. On average, the review of each GitHub pull request took about 4\u20136 iterations over the span of multiple days before being merged. (3) Lastly, the added tasks were presented to crowdworkers in order to collect feedback on the quality of the provided instructions, such as typos, clarity, or other issues (details in \u00a7A). Subsequently, one of the authors used this feedback to improve the task definitions of the instances. This feedback was done only for English tasks, as finding high-quality crowdworkers in other languages is nontrivial (Pavlick et al., 2014).\\n\\nDiversity of tasks. Collecting tasks for SUP-NAT INST was carefully supervised to cover a wide variety of natural language understanding tasks, domains, and languages. To better understand this diversity, we comprehensively categorize tasks along three different dimensions:\\n\\n- **Task Type** defines the nature of the mapping from instance inputs to outputs (e.g., question answering, classification, etc.).\\n- **Language** indicates the language(s) of the instances.\\n- **Domain** indicates the domain(s) to which the text of the tasks belong to (e.g., politics, medicine, dialogue, etc.).\\n\\nThese different measures of categorization can be used to study different senses of generalization. In our empirical studies (\u00a75), we study generalization along the axis of task types. We refer the reader to Fig. 10 in the appendix for the distribution of tasks among different task types, languages, and domains.\\n\\nStatistics. Table 2 shows various statistics for the benchmark. In total, the dataset includes 1616 tasks and 5M instances. On average, each instruction is paired with 2.8 positive and 2.4 negative examples. The average definition length is 56.6 in words.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Statistics of S\\\\textsuperscript{UP-NAT}. \\n\\n4 T\\\\textsubscript{KINSTRUCT}: Learning to Follow Instructions at Scale\\n\\nDefining Generalization to Unseen Tasks. Each task \\\\( t \\\\) is defined via its natural language instruction \\\\( I_t \\\\), and each task has a set of input/output instances \\\\( (X_t, Y_t) \\\\). A model \\\\( M \\\\) is expected to produce the output \\\\( y \\\\), given the input \\\\( x \\\\) and the task instruction \\\\( I_t \\\\):\\n\\n\\\\[\\nM(I_t, x) = y, \\\\quad \\\\text{for} \\\\quad (x, y) \\\\in (X_t, Y_t).\\n\\\\]\\n\\nIn particular, we would like to evaluate model \\\\( M \\\\) on tasks that are not observed (i.e., their instances were not used for training \\\\( M \\\\)). The only source of signal for learning the task at inference time is in-context instructions \\\\( I_t \\\\) that contain a definition and demonstration examples of the task.\\n\\nT\\\\textsubscript{KINSTRUCT}. We introduce T\\\\textsubscript{KINSTRUCT}, a model that is meta-trained on S\\\\textsuperscript{UP-NAT} for solving tasks given their in-context instructions. Previous work has shown the effectiveness of such meta-training in improving model's ability to do in-context learning with either prompts (Zhong et al., 2021; Sanh et al., 2022) or demonstration examples (Min et al., 2022a). Because of the large variety of tasks in S\\\\textsuperscript{UP-NAT}, we are able to do this multi-task meta-training at a larger scale than before. We conduct our experiments and analysis based on the T5 model (Raffel et al., 2020). Since each instruction \\\\( I_t \\\\) consists of multiple elements as described in our instruction schema (\u00a73), we map these elements to textual format and append them before the input instance. Fig. 8 in the appendix shows how we encode the full instructions. We study different combinations of these instruction elements in \u00a77.2. By default, we will use our most effective instruction elements (i.e., task definition and two positive examples) unless otherwise specified. In the same manner, we train the multilingual variant mT\\\\textsubscript{KINSTRUCT} based on the mT5 model (Xue et al., 2021).\\n\\n5 Benchmarking Cross-Task Generalization with S\\\\textsuperscript{UP-NAT}\\n\\nHere we provide our recommended recipe for benchmarking generalization via S\\\\textsuperscript{UP-NAT}.\\n\\n5.1 Evaluation Setup\\n\\nAn Evaluation Split of Unseen Tasks. We split the large collection of tasks in S\\\\textsuperscript{UP-NAT} into two subsets: one for evaluation and the other for supervision. For evaluation tasks, we fix a manually-selected collection of 12 categories that represent 154 tasks. The large variety of tasks in S\\\\textsuperscript{UP-NAT} enables us to choose a diverse set of tasks for evaluation \u2013 such as those at word, sentence, and document levels, covering both classification and generation formats. Appendix G lists our evaluation tasks with examples for representative tasks.\\n\\nFor an efficient evaluation, we sample a maximum of 100 instances for each task, which results in 15,310 testing instances in total. The remaining tasks are used for training models.\\n\\nDivided Tracks for English and X-lignual Tasks. S\\\\textsuperscript{UP-NAT} consists of tasks across multiple languages, which enables evaluating the model's generalization to unseen tasks not only in English but also in other languages. Therefore, we divide our evaluation tasks into two tracks: one for English cross-task generalization (119 tasks) and the other for cross-lingual cross-task generalization (35 tasks). To the best of our knowledge, this is the first study in cross-lingual cross-task generalization (i.e., generalization to unseen tasks in different languages). Fig. 11 and Fig. 12 in the appendix contain the evaluation tasks for each track.\\n\\nEvaluation Metrics. Due to the diversity of our tasks and the open-ended generation nature of our formulation, we adopt ROUGE-L (Lin, 2004) for reporting aggregated performance results. This is a soft string overlap metric that can be applied to a wide range of text generation tasks. We show that the ranking from this metric correlates well with accuracy for classification tasks in Appendix E. We also conduct a human evaluation in \u00a76.2.\\n\\n6 To avoid data leakage, we exclude tasks from the training set if they are sourced from the same dataset as any test task. This results in 757 training tasks for the English track and 1271 training tasks for the cross-lingual track.\\n\\n7 Unlike Sanh et al. (2022) and Wei et al. (2022), who evaluate their models on classification tasks via option ranking (i.e., scoring the correct answer(s) higher than other candidate answers), we evaluate our models in an open-ended generation setting with no task-specific assumptions. We believe this is a more realistic measure of generalization to unseen tasks.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.2 Baselines and Existing Models\\n\\nHere we discuss a variety of baselines and competitive models for our target application. See Appendix D for implementation details.\\n\\nHeuristic baselines. We first evaluate the following heuristics to evaluate the possible shortcuts in the data.\\n\\n- **Copying Demo Output**: copies the output of a random demonstration example. Since we balance the labels for our test tasks, the performance of this baseline will roughly equal a random guess or a majority baseline for classification tasks.\\n\\n- **Copying Instance Input**: copies the given instance input. This strategy performs well on tasks where the target output largely overlaps with the input (e.g., question rewriting, grammar error correction).\\n\\nOff-the-shelf pretrained language models. We evaluate existing LMs that are not fine-tuned with instruction-specific data. Specifically, we evaluate the 11B-parameter T5 (Raffel et al., 2020) as a direct counterpart of T\\\\textsubscript{k-I}\\\\textsubscript{NSTRUCT}. Due to the infilling pretraining objective of the original T5 model, it cannot continue text well. Therefore, we evaluate its \u201cLM-adapted\u201d version, which is further trained with a language modeling objective (Lester et al., 2021). Additionally, we evaluate GPT-3 (Brown et al., 2020), a 175B-parameter autoregressive LM that has shown remarkable ability in following demonstrations provided in its prompt.\\n\\nInstruction-tuned models. In addition to our T\\\\textsubscript{k-I}\\\\textsubscript{NSTRUCT} (\u00a74), we evaluate existing models that are fine-tuned to follow language instructions. In particular, we evaluate InstructGPT (Ouyang et al., 2022) which uses reinforcement learning to incorporate human preferences into a GPT-3 pretrained model, and T0 (Sanh et al., 2022) which finetunes T5 on a collection of task prompts in PROMPT-SOURCE (Bach et al., 2022).\\n\\nUpper bound estimates. We estimate an upper bound on models\u2019 generalization to unseen tasks by fine-tuning an oracle model on the tasks\u2019 labeled instances. Since this model observes the hidden instances of the evaluation tasks, it is, by definition, an estimated upper bound to our generalization-based models. Specifically, we fine-tune a T5-11B model on the 119 English evaluation tasks, and a mT5-13B model on the 35 non-English tasks, with 1K random training instances per task, without overlap with the evaluation instances.\\n\\n| Method             | ROUGE-L | Human |\\n|--------------------|---------|-------|\\n| Copying Demo Output | 28.5    | 50.3  |\\n| Copying Instance Input | 14.2   | 5.4   |\\n| T0 (11B)          | 32.3    | 52.1  |\\n| InstructGPT (175B) | 52.1    | 52.8  |\\n| T\\\\textsubscript{k-I}\\\\textsubscript{NSTRUCT} (ours, 11B) | 62.0    | -     |\\n| mT\\\\textsubscript{k-I}\\\\textsubscript{NSTRUCT} (ours, 13B) | 57.1    | 66.1  |\\n| Supervised Training | 74.3    | 94.0  |\\n\\nTable 3: The overall performance of different methods on unseen tasks in the test set of S\\\\textsubscript{UP-N}\\\\textsubscript{INST} (\u00a76.1).\\n\\nWe report ROUGE-L here as our aggregated metric. Models that leverage instructions show stronger generalization to unseen tasks. In particular, our model that is fine-tuned on a diverse set of tasks outperforms InstructGPT and T0 by a large margin.\\n\\n![Human evaluation vs. ROUGE-L for several methods](image)\\n\\nFigure 3: Human evaluation vs. ROUGE-L for several methods (\u00a76.2). The trends of these two metrics are highly correlated with a Pearson coefficient of 0.998.\\n\\n6 Experimental Results\\n\\n6.1 Overall Results\\n\\nTable 3 summarizes our overall benchmarking results. We use the same input encoding that contains the most effective instructional elements (task definition and two positive examples without the negative examples and explanations) for all the methods. To better understand models\u2019 generalization to different tasks, we also break down the performance according to the task categories in Fig. 4. We refer the reader to Appendix H for more detailed analysis on each individual task.\\n\\nInstruction-tuning enables stronger generalization to unseen tasks. Generally instruction-tuned models perform better compared to their untuned LM counterparts (T\\\\textsubscript{k-I}\\\\textsubscript{NSTRUCT} vs. T5-LM, InstructGPT vs. GPT-3) and heuristic baselines. This indicates models do learn to follow instructions by finetuning on instruction data, and this can generalize to new instructions for unseen tasks. T0 is an exception, which is only slightly better than...\"}"}
{"id": "emnlp-2022-main-340", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task Type                        | ROUGE-L | 0   | 25  | 50  | 75  | 100 |\\n|---------------------------------|---------|-----|-----|-----|-----|-----|\\n| Textual Entailment              | 50      | 10  | 46  | 62  | 75  | 79  |\\n| Cause Effect Classification     |         |     |     |     |     |     |\\n| Coreference Resolution          |         |     |     |     |     |     |\\n| Dialogue Act Recognition        |         |     |     |     |     |     |\\n| Answerability Classification    |         |     |     |     |     |     |\\n| Word Analogy                    |         |     |     |     |     |     |\\n| Overlap Extraction              | 11      | 16  | 45  | 57  | 67  | 75  |\\n| Keyword Tagging                 | 13      | 63  | 34  | 65  | 70  | 73  |\\n| Question Rewriting              |         |     |     |     |     |     |\\n| Title Generation                |         |     |     |     |     |     |\\n| Data to Text                    |         |     |     |     |     |     |\\n| Grammar Error Correction        |         |     |     |     |     |     |\\n\\nFigure 4: Performance per evaluation task type.\\n\\nTk-Instruct consistently performs better than other generalization-based methods on all task types, while there is still a sizable gap compared to supervised training.\\n\\nT5-LM. We suspect this is because the style of prompting in T0's training data is very different from our style of instructions.\\n\\nOur Tk-Instruct outperforms InstructGPT.\\n\\nOur Tk-Instruct and mTk-Instruct models, which are trained with a variety of tasks, generalize best to unseen tasks for both English and non-English tasks in all evaluation task categories.\\n\\nInstructGPT also shows a great extent of generalization to our evaluation tasks. However, we want to note it is not clear if InstructGPT's training data overlaps with our evaluation tasks since their data is unavailable.\\n\\nThere is a sizable gap for improvement. Despite the impressive performance of current models, there is a sizable gap between the generalization of instruction-based models and the supervised training approach, leaving more room for improvement.\\n\\n6.2 Human Evaluation\\n\\nFor language generation tasks, automatic metrics are only an approximation of human judgments; we conduct a human evaluation to confirm the findings so far. Specifically, we ask crowdworkers to indicate if they prefer the predicted answer by the model or the ground truth outputs for each instance with ties being allowed (see Appendix B for details). The resulting human evaluation metric indicates how often model predictions were rated as at least as good as our ground truth labels. The theoretical upper bound of this metric is 100% when the model is rated at least as good as the ground truth for all the instances. The results of human evaluation (shown in Fig. 3) align quite well with our automatic metrics and confirm the human-perceived quality of our models.\\n\\n7 Further Analysis\\n\\nWe conduct further analysis to understand the important factors for models to generalize across tasks. Due to the computational cost, this analysis is done on the English track and using the T5-3B checkpoint, except for the experiments on model sizes.\\n\\n7.1 Scaling Trends of Generalization\\n\\nWe study Tk-Instruct's generalization performance with respect to three scaling factors: the number of training tasks, the number of instances per task, and the model sizes. Fig. 5 presents the performance change by scaling each of them.\\n\\nMore observed tasks improve the generalization.\\n\\nWe fine-tune Tk-Instruct with different numbers of tasks that are randomly sampled from the whole training set (Fig. 5a). The model generalization performance grows log-linearly as we increase the set of tasks used for training. Previous work (Mishra et al., 2022b; Sanh et al., 2022; Wei et al., 2022) has made similar observations on a much smaller scale, while we show that this trend holds even with 757 diverse training tasks.\\n\\nA large number of training instances do not help generalization.\\n\\nWe then vary the number of instances per task that are used for finetuning (Fig. 5b). While the conventional wisdom in supervised learning is that more training instances usually helps (Banko and Brill, 2001; Sun et al., 2017; Hestness et al., 2017), in our setup, the model's performance saturates when only 64 instances per task are used for training. A large number of training instances would instead lead to longer training time and risk overfitting to the training tasks.\\n\\n8 A linear function of an exponential increase of parameters, i.e., growth at a constant multiplicative rate.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Scaling trends of models performance (\u00a77.1) as a function of (a) the number of training tasks; (b) the number of instances per training task; (c) model sizes. The x-axes are in log scale. The linear growth of model performance with exponential increase in observed tasks and model size is a promising trend. Evidently, the performance gain from more instances is limited.\\n\\nTable 4: Performance (ROUGE-L) of models trained and evaluated with various encodings. Diagonal numbers (underlined) represent performances of models trained and evaluated with the same instruction encoding. Each encoding is a combination of the elements in the instructions (Fig. 1). Task ID is a short string composed of dataset name and task category; Def represents the task definition; Pos (k) represents k positive examples; Neg (k) represents k negative examples; Expl represents explanation. These results (a) show the gains from various instructional elements, and (b) indicate surprising reliability of the models to various input encoding. A model trained with definition and positive examples (i.e., the last row) remains robust for different encodings. Tuning larger models with instructions consistently lead to gains.\\n\\nWe study the effect of model scaling by initializing T_k-INSTRUCT from different sizes of pretrained T5 checkpoints, including the small, base, large, xl and xxl sizes (Fig. 5c). We found that increasing the model sizes consistently bring significant improvement (log-linearly with parameter size). This finding contradicts the claim in Xu et al. (2022) that \\\"model size has little impact on performance with an extremely large number of tasks.\\\" Combining Fig. 5(a) and Fig. 5(c), one can create a correspondence between model size and task size. For example, a T5-large model trained with 757 tasks can achieve comparable performance (48.0 ROUGE-L) to the T5-3B model trained with 128 tasks (48.4 ROUGE-L), indicating that increasing the diversity of training tasks is an alternative to scaling model sizes.\\n\\n7.2 Instructing with Different Elements\\n\\nWe evaluate the performance of T_k-INSTRUCT under different instructional elements. The benefit of different instructional elements. As shown in Fig. 1, SUP-NAT-INST provides multiple elements for instructing a task. We train multiple models with different combinations of these elements. The diagonal cells of Table 4 show the performance of our models when trained and evaluated on a particular instruction encoding. Based on the diagonal numbers, including the task definition consistently helps the model to generalize better. Moreover, combining the task definition with positive demonstration examples yields further improvement. However, adding more demonstration examples is negligible. Negative examples help a little bit; explanations decrease performance, which is consistent with the observations of Mishra et al. (2022b) and Lampinen et al. (2022).\"}"}
{"id": "emnlp-2022-main-340", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the model is not large enough. Future work can\\nexplore whether more powerful models can benefit\\nfrom these elements.\\n\\nGeneralization to different input encodings. We\\nfurther investigate whether a model trained on a par-\\nticular encoding can generalize to other encodings.\\nThis can be read from the non-diagonal cells of Table 4. The negative result here is that definition-\\nonly models cannot generalize to example-only\\ntest encodings; and similarly, example-only models\\ncannot generalize to definition-only test encodings.\\nHowever, models trained on encodings that con-\\ntain both definition and examples are surprisingly\\nrobust across different encoding variations.\\n\\n8 Conclusion\\nWe construct a large-scale benchmark consisting\\nof a diverse set of NLP tasks and their instructions.\\nThis benchmark can serve as a rich playground for\\ntraining or evaluation of models that can generalize\\nto unseen tasks by following instructions. Further-\\nmore, we train T-k-I-STRUCT using this data, and\\ndemonstrate its capability to perform unseen tasks\\nto a surprising extent. We provide extensive anal-\\nysis to understand the important factors for such\\ngeneralization. We hope our data and model will fa-\\ncilitate future work towards more general-purpose\\nmodels.\\n\\n9 Limitations\\nWhile the presented data offers a notable variety\\n(e.g., diverse task types), its underlying distribu-\\ntions suffer from skews which should be addressed\\nin future work (see Appendix F). On language di-\\nversity, the proposed benchmark is biased toward\\nEnglish. On output diversity, the collected tasks\\nare generally still skewed to short responses, which\\nmight reflect the distribution of the available tasks\\nin the field. This under-representation of the long-\\ntail of tasks poses a challenge for building general-\\npurpose models in the future. We hope future work\\naddresses such distributional imbalances. More-\\nover, we see natural extensions of the instruction-\\nfollowing setup here in the context of other modali-\\nties such as vision or speech.\\n\\nAutomatic evaluation of models' performance\\nis another challenge, considering the diverse set of\\ntasks in our benchmark, and many of them being\\nopen-ended generation tasks. We use ROUGE-L as\\nan aggregated metric in this paper and find it as a\\ngood proxy for the overall performance of the mod-\\nels, aligning well with human evaluation. However,\\nthere are specific tasks for which ROUGE-L might\\nnot serve as an effective proxy of quality (such\\nas rewriting tasks or error correction tasks where\\ncopying the input can result in a high ROUGE-L\\nscore). We hope these issues will be addressed\\nwith the development of more powerful evaluation\\nmetrics for text generation.\\n\\nIn terms of computing power, we have experi-\\nenced with models that were accessible to us and\\nhave made the resulting models publicly available.\\nWe also acknowledge that there are larger models\\nthat we were not able to train due to the limitations\\nof our computational budget.\\n\\nAcknowledgments\\nWe thank the anonymous reviewers, our colleagues\\nfrom AI2 and UWNLP, especially Matthew Peters\\nfor his encouraging conversations that motivated\\nthis project. We also thank the student contributors\\nof Arizona State University's CSE 576 \\\"Topics\\nin NLP\\\" course and all other contributors to our\\ndata repository. All experiments were run on AI2's\\nBeaker GPU clusters and Google's research TPUs.\\nThis work was supported in part by ONR MURI\\nN00014-18-1-2670, ONR N00014-18-1-2826, and\\nDARPA MCS N66001-19-2-4031 grants.\\n\\nReferences\\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,\\nHuaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-\\nglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni,\\net al. 2022. ExT5: Towards Extreme Multi-Task\\nScaling for Transfer Learning. In International Con-\\nference on Learning Representations (ICLR).\\n\\nStephen H Bach, Victor Sanh, Zheng-Xin Yong, Al-\\nbert Webson, Colin Raffel, Nihal V Nayak, Abheesht\\nSharma, Taewoon Kim, M Saiful Bari, Thibault\\nFevry, et al. 2022. PromptSource: An Integrated\\nDevelopment Environment and Repository for Nat-\\nural Language Prompts. In Annual Meeting of the\\nAssociation for Computational Linguistics (ACL)\\n- System Demonstrations.\\n\\nMichele Banko and Eric Brill. 2001. Scaling to Very\\nVery Large Corpora for Natural Language Disam-\\nbiguation. In Annual Meeting of the Association for\\nComputational Linguistics (ACL).\\n\\nMax Bartolo, Alastair Roberts, Johannes Welbl, Sebas-\\ntian Riedel, and Pontus Stenetorp. 2020. Beat the ai:\\nInvestigating adversarial human annotation for read-\\ning comprehension. Transactions of the Association\\nfor Computational Linguistics (TACL), 8:662\u2013678.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-340", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-340", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Sao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In Conference on Empirical Methods in Natural Language Processing (EMNLP) - System Demonstrations.\\n\\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. 2022. UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. arXiv preprint arXiv:2201.05966.\\n\\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-gang Wang, Haiyu Li, and Zhilin Yang. 2022. Zero-Prompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization. arXiv preprint arXiv:2201.06910.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer. In Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).\\n\\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP. In Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nQinyuan Ye and Xiang Ren. 2021. Learning to Generate Task-Specific Adapters from Task Description. In Annual Meeting of the Association for Computational Linguistics (ACL).\\n\\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021. Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. In Conference on Empirical Methods in Natural Language Processing (EMNLP) - Findings.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Crowdsourcing Human Feedback\\n\\nWe use Amazon Mechanical Turk (AMT) to crowdsource feedback on the quality of the collected instructions. We limit our crowdworkers to predominantly English-speaking countries (USA, UK, Canada, and Australia), and to those who have finished over 1K HITs with an approval rating of over 99%.\\n\\nFig. 6 shows the crowdsourcing template used for collecting crowdworker feedback on our instructions. We show the instructions (the task definition, along with positive and negative examples) followed by forms for their feedback. We allow the crowdworkers to give us a qualitative measure of their perceived quality as well as text boxes for more concrete items (such as typos or phrasings that may benefit from more clear articulation). For each task, we solicit the feedback of 3 crowdworkers and then use this feedback to improve the task definitions or the examples for each task.\\n\\nCrowdsourcing Human Judgements of Generation Quality\\n\\nWe perform a crowdsourcing experiment on Amazon Mechanical Turk (AMT) to assess the quality of the generated responses of models. Specifically, we ask crowdworkers to indicate if they prefer the predicted answer by the model or the ground truth outputs for each instance. The annotation interface is shown in Fig. 7. It is essentially the same template used for the quality assessment of the dataset (\u00a7A), except that here the crowdworkers are shown a pair of responses for each instance\u2014the reference text (from our benchmark) and the one generated by the model\u2014turning the task into a comparative evaluation.\\n\\nFor each instance, we obtain annotations from an annotator as to whether they prefer either response over the other or they would rate them equally (\u201ctie\u201d). The model receives a credit of 1.0 if the worker favors the model\u2019s prediction at least as well as the ground truth label (otherwise, the model would receive a credit of 0.0). The overall accuracy score for the model is computed by averaging instance-level scores. To reduce the costs, the human evaluation of our models is done on 60 randomly selected tasks (about half of our evaluation tasks), and on 10 random instances of each task.\\n\\nSince it is non-trivial to find non-English speaking crowdworkers (Pavlick et al., 2014), this evaluation was restricted to English language tasks. Therefore, since our task is focused on English tasks, we required workers to be based in a country with a population predominantly of native English speakers (USA, Canada, UK, and Australia) and have completed at least 5000 HITs with \u226599% assignment approval rate.\\n\\nThe resulting human-evaluation metric indicates how often were model predictions equal or preferred to our ground truth labels. In this evaluation, the theoretical upper bound is 100% where the model is rated at least as well as the ground truth. The results of human evaluation are shown in the bottom row of Fig. 3.\\n\\nC Instruction Schema\\n\\nOur instruction schema is based on that of NAIINST (Mishra et al., 2022b), but we simplify it to make data collection easier. Our DEFINITION field serves as the union of Mishra et al. (2022b)\u2019s DEFINITION, THINGS TO AVOID, and EMPHASIS & CAUTION. Additionally, we drop their TITLE and PROMPT as their content is most often covered by DEFINITION.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When fine-tuning models, we train them for two epochs with a batch size of 16 and a constant learning rate of 1e-5. The maximum input length is set to 1024, and the maximum output length is set to 128. These experiments are conducted with 8 A100 GPUs with 48GB GPU memory per each. We use DeepSpeed for model parallelization, with bfloat16 precision enabled to save the GPU memory. Each training run takes 6 hours to complete.\\n\\nWe use the OpenAI API for conducting the GPT-3 experiments. We use their \u201cdavinci\u201d engine for the GPT-3 language model experiments and their \u201ctext-davinci-001\u201d engine for the InstructGPT experiments. When making the requests, we set the temperature as 0, top_p as 1 and the maximum generation length as 50.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Due to the high cost, we randomly sample 20 instances from each of our 119 test tasks to estimate the performance of GPT-3 and InstructGPT. All API requests were made on May 30, 2022.\\n\\nFor every problem setup, we map a given instruction $I_t$ and an input instance $x$ into a textual format, obtaining $\\\\text{enc}(I_t, x)$. Each instruction $I_t$ consists of multiple elements as described in our instruction schema (\u00a73). We map each element of the instruction to a textual format and prepend it to the input instance. Fig. 8 shows how we encode the full instruction.\\n\\nWe study different combinations of these instruction elements in \u00a77.2. The encoded instance is then fed to an encoder-decoder model to predict $y$: $M: \\\\text{enc}(I_t, x) \\\\rightarrow y$.\\n\\n**Definition:**\\n\\nPositive Example 1 - input: $p_{ex1}.input$; output: $p_{ex1}.output$; explanation: $p_{ex1}.exp$.\\n\\nNegative Example 1 - input: $n_{ex1}.input$; output: $n_{ex1}.output$; explanation: $n_{ex1}.exp$.\\n\\nNegative Example 2 - ... (omitted for brevity).\\n\\nNow complete the following example - input: $x.input$; output: (omitted for brevity).\\n\\n**Evaluation Metrics**\\n\\nWe adopt ROUGE-L as our automatic evaluation metric in this work. However, it remains a question for how much ROUGE-L can reflect model's performance on different tasks. Although we cannot test ROUGE-L's correlation with each task-specific metric of the tasks included in our data, we do investigate whether ROUGE-L can be used for classification tasks. Fig. 9 plots the ROUGE-L scores and accuracy of several models on different types of tasks. These task types are usually regarded as classification tasks and have very short ground truth output. We can see that for all these task types, the trend of ROUGE-L correlates well with the trend of accuracy. For some task types, we do see some gap between these two metrics. The reason is because there are some generation tasks categorized into these types. These results indicate that ROUGE-L is a good proxy for accuracy for classification tasks.\\n\\n**Distribution of Tasks**\\n\\nAs is described in \u00a73, SUP-NATIST provides the annotation for categorizing tasks along three different dimensions: task type, language, and domain. Fig. 10 shows the distribution of tasks among these three dimensions. This meta-information can be used to study model's generalization ability in different senses. Despite the diversity of the data, we acknowledge the skew toward certain tasks and languages, which we leave to be addressed by future work.\\n\\n**Evaluation Tasks**\\n\\nTable 5 lists the 12 task categories used for our evaluation and all the tasks included in each category (introduced in \u00a75.1). To provide a better sense of what those tasks look like, we also select one representative task from each category and list them in Tables 6\u201317. Due to the large number of tasks in our dataset, we cannot list all 1,616 tasks in this paper. We refer the reader to our dataset.\\n\\n**Performance Improvement per Evaluation Task**\\n\\nTo provide more detailed analysis of Tk-INSTRUCT on each individual task, Fig. 11 presents the per-task improvement of our Tk-INSTRUCT (3B) model over the best of two heuristic baselines on the English evaluation tasks, and Fig. 12 presents the per-task improvement of the mTk-INSTRUCT model on the cross-lingual evaluation tasks. For most of the evaluation tasks, we see a notable extent of generalization by Tk-INSTRUCT.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 10: Distribution of SUP-NATINST tasks in terms of their (a) task types (b) languages (c) domains.\\n\\n- **Task Types**: The distribution of tasks across different categories.\\n- **Languages**: The distribution of tasks across various languages.\\n- **Domains**: The distribution of tasks across different domains.\\n\\nThe y-axes are in log scale.\\n\\n- **# of Tasks**: The number of tasks in each category.\\n- **Rouge-L**: The Rouge-L score for the tasks.\\n- **Accuracy**: The accuracy of the tasks.\\n\\nThe trends of these two metrics are highly correlated with a Pearson coefficient of 0.970.\\n\\nFigure 9: Rouge-L v.s. Accuracy for task types that are usually regarded as classification tasks. The trends of these two metrics are highly correlated with a Pearson coefficient of 0.970.\"}"}
{"id": "emnlp-2022-main-340", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task Category | Metric | List of Tasks |\\n|---------------|--------|---------------|\\n| Textual Entailment | Exact Match | task202_multinli_textual_entailment, task936_defeasible_nli_atomic_textual_entailment, task1344_rte_textual_entailment, task1615_sick_textual_entailment, task1385_anli_textual_entailment, task199_multinli_textual_entailment, task1388_cb_textual_entailment, task1554_scitail_textual_entailment, task640_e_snli_textual_entailment, task534_farstail_textual_entailment, task201_multinli_textual_entailment, task1386_anli_textual_entailment, task463_pasinlu_textual_entailment, task1387_anli_textual_entailment, task738_perspectrum_textual_entailment, task1529_scitailv1.1_textual_entailment, task190_snli_textual_entailment, task200_multinli_textual_entailment, task1612_sick_textual_entailment, task970_sherliic_textual_entailment, task890_gwsd_textual_entailment, task464_pasinlu_textual_entailment, task1516_imppres_textual_entailment, task642_e_snli_textual_entailment |\\n| Cause Effect | Exact Match | task391_cod3s_cause_effect_classification, task939_indicnlp_cause_effect_classification, task392_cod3s_cause_effect_classification, task938_indicnlp_cause_effect_classification, task1168_xcopa_cause_effect_classification, task828_copa_cause_effect_classification, task1628_copa_hr_cause_effect_classification, task943_indicnlp_cause_effect_classification, task1182_xcopa_cause_effect_classification, task1171_xcopa_cause_effect_classification, task968_xcopa_cause_effect_classification, task942_indicnlp_cause_effect_classification, task1181_xcopa_cause_effect_classification, task1174_xcopa_cause_effect_classification, task1177_xcopa_cause_effect_classification, task614_glucose_cause_effect_classification, task1629_copa_hr_cause_effect_classification, task1175_xcopa_cause_effect_classification, task827_copa_cause_effect_classification, task1173_xcopa_cause_effect_classification, task1180_xcopa_cause_effect_classification, task1170_xcopa_cause_effect_classification, task1183_xcopa_cause_effect_classification, task969_xcopa_cause_effect_classification, task941_indicnlp_cause_effect_classification, task1626_copa_hr_cause_effect_classification, task940_indicnlp_cause_effect_classification, task393_cod3s_cause_effect_classification, task1169_xcopa_cause_effect_classification, task1179_xcopa_cause_effect_classification |\\n| Coreference Resolution | Exact Match | task1391_winogrande_coreference_resolution, task1664_wino_bias_coreference_resolution, task304_numeric_fused_head_coreference_resolution, task892_gap_coreference_resolution, task891_gap_coreference_resolution, task330_gap_coreference_resolution, task401_numeric_fused_head_coreference_resolution, task033_winogrande_coreference_resolution, task133_winowhy_coreference_resolution, task329_gap_coreference_resolution, task249_enhanced_wsc_coreference_resolution, task648_winograd_wsc_coreference_resolution, task1390_wsc_fiexed_coreference_resolution, task893_gap_coreference_resolution |\\n| Dialogue Act Recognition | Exact Match | task879_schema_guided_dstc8_dialogue_act_recognition, task362_spolin_dialogue_act_recognition, task1533_dailydialog_dialogue_act_recognition, task1534_dailydialog_dialogue_act_recognition, task880_schema_guided_dstc8_dialogue_act_recognition, task1531_dailydialog_dialogue_act_recognition, task1394_meta_woz_dialogue_act_recognition |\\n| Answerability Classification | Exact Match | task020_mctaco_answerability_classification, task050_multirc_answerability_classification, task1439_doqa_answerability_classification, task233_iirc_answerability_classification, task226_stack_overflow_answerability_classification, task396_persianqa_answerability_classification, task1640_adverserial_qa_answerability_classification, task232_iirc_answerability_classification, task1442_doqa_answerability_classification, task242_tweetqa_answerability_classification, task1624_disfl_qa_answerability_classification, task520_aquamuse_answerability_classification, task290_tellmewhy_answerability_classification, task349_squad2.0_answerability_classification |\\n| Word Analogy Exact Match | task1155_bard_word_analogy, task1152_bard_word_analogy, task1158_bard_word_analogy, task1156_bard_word_analogy, task1157_bard_word_analogy, task1159_bard_word_analogy, task1153_bard_word_analogy |\\n| Overlap Extraction | ROUGE-L | task039_qasc_overlap_extraction, task281_points_of_correspondence_overlap_extraction |\\n| Keyword Tagging | ROUGE-L | task613_liar_keyword_tagging, task645_wiki_auto_all_data_keyword_tagging, task620_ohsumed_keyword_tagging, task036_qasc_keyword_tagging, task623_ohsumed_keyword_tagging |\\n| Question Rewriting | ROUGE-L | task670_ambigqa_question_rewriting, task121_zest_question_rewriting, task1195_disfl_qa_question_rewriting, task442_com_qa_question_rewriting, task1345_qqp_question_rewriting, task035_winogrande_question_rewriting, task671_ambigqa_question_rewriting, task1562_zest_question_rewriting, task1622_disfl_qa_question_rewriting, task034_winogrande_question_rewriting, task402_grailqa_question_rewriting |\\n| Title Generation | ROUGE-L | task1356_xlsum_title_generation, task1540_peer_read_title_generation, task1659_billsum_title_generation, task569_recipe_nlg_title_generation, task1342_amazon_us_reviews_title_generation, task220_rocstories_title_generation, task1561_clickbait_news_bg_title_generation, task418_persent_title_generation, task1358_xlsum_title_generation, task769_qed_title_generation, task219_rocstories_title_generation, task602_wikitext_title_generation, task1586_scifact_title_generation, task743_eurlex_title_generation, task500_scruples_title_generation, task619_ohsumed_title_generation, task510_reddit_tifu_dataset_title_generation, task288_gigaword_title_generation, task1161_coda_19_title_generation |\\n| Data to Text | ROUGE-L | task957_e2e_data_to_text, task1631_open_pi_data_to_text, task1598_nyc_data_to_text, task1728_web_nlg_data_to_text, task102_commongen_data_to_text, task677_ollie_data_to_text, task1407_dart_data_to_text, task1409_dart_data_to_text, task760_msr_sqa_data_to_text |\\n| Grammar Error Correction | ROUGE-L | task1557_jfleg_grammar_error_correction |\"}"}
{"id": "emnlp-2022-main-340", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task Type Textual Entailment\\nTask ID task1344_rte_textual_entailment\\nDefinition In this task, you're given two sentences. Indicate if the first sentence clearly entails the second sentence (i.e., one can conclude the 2nd sentence by reading the 1st one). Indicate your answer with \\\"1\\\" if the first sentence entails the second sentence, otherwise answer with \\\"0\\\".\\n\\nPositive Example\\nInput: Sentence 1: No Weapons of Mass Destruction Found in Iraq Yet. Sentence 2: Weapons of Mass Destruction Found in Iraq.\\nOutput: 0\\nExplanation: In our first statement we clearly say that Iraq does not have any weapon of mass destruction but the second sentence says that weapon of mass destruction is found in Iraq which is a contradiction. Hence output will be 0 for non entailment.\\n\\nNegative Example\\nInput: Sentence 1: Valero Energy Corp., on Monday, said it found \\\"extensive\\\" additional damage at its 250,000-barrel-per-day Port Arthur refinery. Sentence 2: Valero Energy Corp. produces 250,000 barrels per day.\\nOutput: 0\\nExplanation: The first statement mentions that there was damage found in the 250,000 barrel-per-day Port Arthur refinery. Which means that they produce 250,000 barrels a day. Hence the output should have been 1 for entailment.\\n\\nInstance\\nInput: Sentence 1: Like the United States, U.N. officials are also dismayed that Aristide killed a conference called by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the feuding parties together. Sentence 2: Aristide had Prime Minister Robert Malval murdered in Port-au-Prince.\\nValid Output: \\n\\nTable 6: An example task in the Textual Entailment category of our dataset, adopted from RTE (Dagan et al., 2005; Bentivogli et al., 2008).\\n\\nTask Type Cause Effect Classification\\nTask ID task828_copa_cause_effect_classification\\nDefinition In this task your given two statements. You must judge whether the second sentence is the cause or effect of the first one. Label the instances as \\\"cause\\\" or \\\"effect\\\" based on your judgment. The sentences are separated by a newline character.\\n\\nPositive Example\\nInput: The women met for coffee. They wanted to catch up with each other.\\nOutput: cause\\nExplanation: The women met for coffee because they wanted to catch up with each other.\\n\\nNegative Example\\nInput: My body cast a shadow over the grass. The sun was rising.\\nOutput: effect\\nExplanation: The rising of the sun isn't an effect of casting a shadow over the grass.\\n\\nInstance\\nInput: The woman tolerated her friend's difficult behavior. The woman knew her friend was going through a hard time.\\nValid Output: \\n\\nTable 7: An example task in the Cause Effect Classification category of our dataset, adopted from COPA (Roemmele et al., 2011).\\n\\nTask Type Coreference Resolution\\nTask ID task1391_winogrande_coreference_resolution\\nDefinition In this task, you are given a question containing a blank (_) and two options. You should pick the best option to answer the question. Please answer with \\\"A\\\" or \\\"B\\\".\\n\\nPositive Example\\nInput: Katrina gave Christine a stuffed animal for their birthday, but _ already had this one. (A) Katrina (B) Christine\\nOutput: B\\nExplanation: Since the blank is someone who received the gift and already had a stuffed animal, the answer must be \\\"Christine\\\".\\n\\nNegative Example\\nInput: Kevin had to use less sunscreen when at the beach tanning than Justin because _ had less sensitive skin. (A) Kevin (B) Justin\\nOutput: (A)\\nExplanation: Here, an additonal parentheses has been added to the answer. Note that, a valid answer must be \\\"A\\\" or \\\"B\\\".\\n\\nInstance\\nInput: Benjamin hated being in the sand and just watched Nelson make castle since _ hated to be messy. (A) Benjamin (B) Nelson\\nValid Output:\\n\\nTable 8: An example task in the Cause Effect Classification category of our dataset, adopted from WinoGrande (Sakaguchi et al., 2020).\"}"}
{"id": "emnlp-2022-main-340", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task Type: Dialogue Act Recognition\\n\\nTask ID: task1394_meta_woz_dialogue_act_recognition\\n\\nDefinition: In this task, you are given four sentences: a bot task sentence, a bot role sentence, a user task sentence, and a user role sentence. Your job is to classify given sentences into one of the 47 different domains. The domains are: \\\"UPDATE_CALENDAR\\\", \\\"PRESENT_IDEAS\\\", \\\"MOVIE_LISTINGS\\\", \\\"AUTO_SORT\\\", \\\"GAME_RULES\\\", \\\"CONTACT_MANAGER\\\", \\\"BANK_BOT\\\", \\\"MUSIC_SUGGESTER\\\", \\\"CHECK_STATUS\\\", \\\"PET_ADVICE\\\", \\\"HOW_TO_BASIC\\\", \\\"NAME_SUGGESTER\\\", \\\"QUOTE_OF_THE_DAY_BOT\\\", \\\"GUINESS_CHECK\\\", \\\"INSURANCE\\\", \\\"RESTAURANT_PICKER\\\", \\\"MAKE_RESTAURANT_RESERVATIONS\\\", \\\"WEDDING_PLANNER\\\", \\\"SKI_BOT\\\", \\\"HOME_BOT\\\", \\\"PLAY_TIMES\\\", \\\"BUS_SCHEDULE_BOT\\\", \\\"WHAT_IS_IT\\\", \\\"PHONE_PLAN_BOT\\\", \\\"DECIDER_BOT\\\", \\\"PHONE_SETTINGS\\\", \\\"TIME_ZONE\\\", \\\"LIBRARY_REQUEST\\\", \\\"UPDATE_CONTACT\\\", \\\"CATALOGUE_BOT\\\", \\\"PROMPT_GENERATOR\\\", \\\"SCAM_LOOKUP\\\", \\\"SPORTS_INFO\\\", \\\"POLICY_BOT\\\", \\\"CITY_INFO\\\", \\\"APARTMENT_FINDER\\\", \\\"EVENT_RESERVE\\\", \\\"SHOPPING\\\", \\\"EDIT_PLAYLIST\\\", \\\"LOOK_UP_INFO\\\", \\\"ORDER_PIZZA\\\", \\\"WEATHER_CHECK\\\", \\\"APPOINTMENT_REMINDER\\\", \\\"GEOGRAPHY\\\", \\\"STORE_DETAILS\\\", \\\"AGREEMENT_BOT\\\", \\\"ALARM_SET\\\".\\n\\nPositive Example: Input: Bot's task: Inform the user that the topping they are asking for is unavailable. Bot's role: You are a bot designed to help customers order pizza. User's task: Ask if a certain pizza topping is available. User's role: You are interacting with a pizza restaurant bot. Output: ORDER_PIZZA Explanation: According to the descriptions of the four sentences, we know that the type of task is ORDER_PIZZA.\\n\\nNegative Example: Input: Bot's task: Help the user with their pizza order. Bot's role: You are a bot designed to help customers order pizza. User's task: Ask the bot for three different pizzas. User's role: You are interacting with a pizza restaurant bot. Output: UPDATE_CALENDAR Explanation: According to the descriptions of the tasks and roles, we know that the type of task is ORDER_PIZZA, but the output is UPDATE_CALENDAR, so it is incorrect.\\n\\nInstance: Input: Bot's task: Tell the user when the movie is playing on Friday night. Bot's role: You are a bot designed to provide movie listings. User's task: Ask the bot for the movie times for a movie on Friday night. User's role: You are interacting with a bot designed to provide movie listings. Valid Output: [\\\"MOVIE_LISTINGS\\\"]\\n\\nTable 9: An example task in the Dialogue Act Recognition category of our dataset, adopted from MetaLWOZ (Shalydanov et al., 2020).\\n\\nTask Type: Answerability Classification\\n\\nTask ID: task1640_adverserial_qa_answerability_classification\\n\\nDefinition: Given a paragraph from a wikipedia article about some topic, and a question related to the topic, determine whether the question is answerable from the paragraph. If the question is answerable, answer \\\"True\\\", otherwise, answer \\\"False\\\".\\n\\nPositive Example: Input: Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood-brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior. Question: What is surrounded by cerebrospinal fluid? Output: True Explanation: The paragraph comes from the wikipedia page on the brain. The answer to the question is the brain which can be found in the paragraph.\\n\\nNegative Example: Input: NASCAR (headquartered in Daytona Beach) begins all three of its major auto racing series in Florida at Daytona International Speedway in February, featuring the Daytona 500, and ends all three Series in November at Homestead-Miami Speedway. Daytona also has the Coke Zero 400 NASCAR race weekend around Independence Day in July. The 24 Hours of Daytona is one of the world's most prestigious endurance auto races. The Grand Prix of St. Petersburg and Grand Prix of Miami have held IndyCar races as well. Question: What is the starting time of NASCAR's big events? Output: False Explanation: This paragraph comes from the wikipedia article on Florida. The answer to the given question is February which can be found in the paragraph, however the output is given as False.\\n\\nInstance: Input: Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood-brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior. Question: What are the benefits of the blood-brain barrier? Valid Output: [\\\"True\\\"]\\n\\nTable 10: An example task in the Answerability Classification category of our dataset, adopted from AdversarialQA (Bartolo et al., 2020).\"}"}
{"id": "emnlp-2022-main-340", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Task Type:** Word Analogy  \\n**Task ID:** task1156_bard_word_analogy  \\n**Definition:** Two analogies that relate actions to the tools used to perform the action is given in the form \\\"A : B. C : ?\\\". \\\"A : B\\\" relates action A to tool B. Your task is to replace the question mark (?) with the appropriate tool for the given action C, following the \\\"A : B\\\" relation.\\n\\n**Positive Example**  \\n**Input:** eat : fork. cook : ?  \\n**Output:** pan  \\n**Explanation:** The given analogy relates actions to the tools used to perform them. A fork can be used to eat. To cook, a pan can be used.\\n\\n**Negative Example**  \\n**Input:** dig : shovel. wash : ?  \\n**Output:** sink  \\n**Explanation:** The given analogy relates actions to the tools used to perform them. A knife can be used to cut. To wash, a sink CANNOT be used.\\n\\n**Instance**  \\n**Input:** cut : knife. wash : ?  \\n**Valid Output:** [\\\"soap\\\", \\\"washcloth\\\", \\\"detergent\\\", \\\"rag\\\"]\\n\\n---\\n\\n**Task Type:** Overlap Extraction  \\n**Task ID:** task281_points_of_correspondence_overlap_extraction  \\n**Definition:** You will be given three sentences. Read them, then identify a noun phrase (person, place, or thing) or event that is shared between all three sentences. As the output, write the span of the text corresponding to that phrase in each sentence. Keep the order of the sentences, that is, your answer should look like: 1: *a phrase from sentence 1* 2: *a phrase from sentence 2* 3: *a phrase from sentence 3*.\\n\\n**Positive Example**  \\n**Input:** 1: Four employees of the store have been arrested, but its manager \u2013 herself a woman \u2013 was still at large Saturday, said Goa police superintendent Kartik Kashyap. 2: If convicted, they could spend up to three years in jail, Kashyap said. 3: The four store workers arrested could spend 3 years each in prison if convicted.  \\n**Output:** 1: Four employees of the store 2: they 3: The four store workers  \\n**Explanation:** All three mentioned parts refer to the same entity, the four employees. \\\"of the store\\\" in first sentence must be included, since it is part of the same noun phrase describing the employees.\\n\\n**Negative Example**  \\n**Input:** 1: But an Arizona official told CNN Bates never trained with the agency. 2: He didn't come to Arizona, the official from the Maricopa County Sheriff's Office said, and he certainly didn't train with us. 3: Maricopa County Sheriff's Office in Arizona says Robert Bates never trained with them.  \\n**Output:** 1: the agency 3: Maricopa County Sheriff's Office in Arizona  \\n**Explanation:** The two noun phrases given in this example are correct, but there's no noun phrase from sentence 2. You should include all three sentences in your response.\\n\\n**Instance**  \\n**Input:** 1: The President is headed to Panama for a regional summit, and Julie Pace of The Associated Press reports one of the big questions is whether he'll make history and have a face-to-face meeting with Cuban leader Raul Castro. 2: And so what the White House is going to be weighing is whether this meeting would be a way to generate more progress or whether it would be a premature reward for the Castros. 3: White House weighing whether Obama should meet with Raul Castro.  \\n**Valid Output:** [\\\"1: Cuban leader Raul Castro 2: the Castros 3: Raul Castro\\\", \\\"1: face-to-face meeting 2: this meeting 3: meet\\\"]\\n\\n---\\n\\nTable 11: An example task in the Word Analogy category of our dataset, adopted from BARD (Fulda et al., 2017).\\n\\nTable 12: An example task in the Overlap Extraction category of our dataset, adopted from PointsOfCorrespondence (Lebanoff et al., 2020).\"}"}
{"id": "emnlp-2022-main-340", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-340", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task Type: Title Generation\\nTask ID: task418_persent_title_generation\\nDefinition: Given a document, generate a short title of the document. The title should convey the main idea/event/topic about which the document is being written. Note that URLs in the text have been replaced with [Link].\\n\\nPositive Example:\\nInput: In a letter posted on the White House web site Tuesday John Brennan assistant to President Barack Obama for homeland security and counterterrorism said Schmidt will have regular access to the president and play a vital role in the country's security. Schmidt's selection comes more than 10 months after Obama declared cyber security a priority and ordered a broad administration review. A senior White House official said Obama was personally involved in the selection process and chose Schmidt because of his unique background and skills. Schmidt will have regular and direct access to the president for cyber security issues the official said. The official spoke on the condition of anonymity to discuss the selection process. At the same time cyber experts and potential job candidates have complained that the position lacks the budgetary and policy-making authority needed to be successful. Schmidt will report to the National Security Council and closely support the National Economic Council on cyber issues. Schmidt's selection suggests that economic and business interests in the White House held more sway in the selection process. Schmidt president and CEO of the Information Security Forum a nonprofit international consortium that conducts research in information security has served as chief security officer for Microsoft and as cyber security chief for online auction giant eBay. He was reportedly preferred by Lawrence Summers director of the economic council. A good format for the title can be the simple subject + object + verb.\\nOutput: White House picks new cyber coordinator\\nExplanation: The title is relevant to the main topic of document, that is, the selection of Schmidt as the cyber security chief.\\n\\nNegative Example:\\nInput: Lauren Cohan's Walking Dead fate may be written in the Whiskey Cavalier. While the show hasn't been picked up yet Cohan hasn't yet signed on for season 9 of Walking Dead and rumors have circulated for months about her possible exit from the zombie show. Just two days ago show runner Scott Gimple told TV Line that he is incredibly hopeful about working out a new deal with Cohan who has played Maggie since the second season. \\\"These things do happen in TV\\\" he said. But we are talking.\\nOutput: Scott Gimple signed Lauren Cohan for season 9\\nExplanation: Though the topic is relevant and have correct names from the document, this is incorrect fact, as it is mentioned that Cohan hasn't yet signed.\\n\\nInstance:\\nInput: Days after at least 58 people were killed in a Las Vegas mass shooting, Hillary Clinton called for better gun control. Clinton also had some words for President Trump, particularly of his handling of Hurricane Maria and the devastation in Puerto Rico. Clinton, on her book tour for \\\"What Happened,\\\" called her memoir \\\"a story of resilience.\\\" Fallon also had female staff writers write thank you notes to Clinton. \\\"Thank you, Miley, tonight's show writers and all of the women and young girls out there who are smart, strong and deserving of every opportunity,\\\" Clinton said. As for election night, Clinton said she was disappointed both that she lost and that President Trump won.\\nValid Output: [\\\"Hillary Clinton calls for gun control after Las Vegas shooting\\\"]\\n\\nTable 15: An example task in the Title Generation category of our dataset, adopted from PerSenT (Bastan et al., 2020).\\n\\nTask Type: Data to Text\\nTask ID: task957_e2e_data_to_text\\nDefinition: In this task, we ask you convert a data table of restaurant descriptions into fluent natural-sounding English sentences. The input is a string of key-value pairs; the output should be a natural and grammatical English sentence containing all the information from the input.\\n\\nPositive Example:\\nInput: name[Aromi], eatType[restaurant], food[English], area[city centre]\\nOutput: Aromi is an English restaurant in the city centre.\\nExplanation: The output sentence faithfully converts the data in the input into a natural-sounding sentence.\\n\\nNegative Example:\\nInput: name[Blue Spice], eatType[coffee shop], priceRange[more than\u02d800a330], customer rating[5 out of 5], area[riverside], family-friendly[yes], near[Avalon]\\nOutput: Blue Spice is a Colombian coffee shop located by the riverside, near Avalon in Boston. Its prices are over\u02d800a330. Its customer ratings are 5 out of 5.\\nExplanation: While the output contains most of the information from the input, it hallucinates by adding ungrounded information such as \\\"Colombian\\\" and \\\"Boston\\\".\\n\\nInstance:\\nInput: name[The Mill], eatType[restaurant], area[riverside], near[The Rice Boat]\\nValid Output: [\\\"A restaurant called The Mill, can be found near the riverside next to The Rice Boat.\\\"]\\n\\nTable 16: An example task in the Data to Text category of our dataset, adopted from E2E (Novikova et al., 2017).\"}"}
{"id": "emnlp-2022-main-340", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task Type: Grammar Error Correction\\nTask ID: task1557_jfleg_grammar_error_correction\\nDefinition: In this task, you will be shown an incorrect English sentence. You need to generate a corrected form of the input sentence.\\n\\nPositive Example:\\nInput: The car's wheel are loose.\\nOutput: The car's wheel is loose.\\nExplanation: The instance of are is replaced by the word is. This makes the sentence grammatically correct.\\n\\nNegative Example:\\nInput: This way is the way to go.\\nOutput: This way may be the way to go.\\nExplanation: The example does not correct the misuse of the word way. Instead, it should shorten the sentence to: this is the way to go.\\n\\nInstance:\\nInput: I think it's harder for successful person to risk something, thay coluld lost much more then others.\\nValid Output: \\n\\\"I think it's harder for a successful person to risk something because they could lose much more than others.\\\"\\n\\nTable 17: An example task in the Grammar Error Correction category of our dataset, adopted from JFLEG (Napoles et al., 2017).\"}"}
{"id": "emnlp-2022-main-340", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task Description                                      | Task Code |\\n|------------------------------------------------------|-----------|\\n| task 133 winowhy_coreference_resolution               |           |\\n| task 190 snli_textual_entailment                      |           |\\n| task 760 msr_sqa_data_to_text                         |           |\\n| task 281 points_of_correspondence_overlap_extraction  |           |\\n| task 201 multinli_textual_entailment                   |           |\\n| task 1385 anli_textual_entailment                     |           |\\n| task 738 perspectrum_textual_entailment               |           |\\n| task 642 e_snli_textual_entailment                    |           |\\n| task 1386 anli_textual_entailment                     |           |\\n| task 1407 dart_data_to_text                           |           |\\n| task 670 ambigqa_question_rewriting                   |           |\\n| task 1387 anli_textual_entailment                     |           |\\n| task 671 ambigqa_question_rewriting                   |           |\\n| task 1388 cb_textual_entailment                       |           |\\n| task 1439 doqa_answerability_classification           |           |\\n| task 199 multinli_textual_entailment                   |           |\\n| task 641 e_snli_textual_entailment                    |           |\\n| task 1562 zest_question_rewriting                     |           |\\n| task 121 zest_question_rewriting                      |           |\\n| task 1345 qqp_question_rewriting                      |           |\\n| task 1557 jfleg_grammar_error_correction              |           |\\n| task 233 iirc_answerability_classification            |           |\\n| task 1534 dailydialog_dialogue_act_recognition        |           |\\n| task 232 iirc_answerability_classification            |           |\\n| task 362 spolin_dialogue_act_recognition              |           |\\n| task 393 cod3s_cause_effect_classification             |           |\\n| task 1390 wsc_fiexed_coreference_resolution           |           |\\n| task 290 tellmewhy_answerability_classification       |           |\\n| task 614 glucose_cause_effect_classification          |           |\\n| task 937 defeasible_nli_atomic_textual_entailment     |           |\\n| task 304 numeric_fused_head_coreference_resolution    |           |\\n| task 1195 disfl_qa_question_rewriting                 |           |\\n| task 1622 disfl_qa_question_rewriting                 |           |\\n| task 392 cod3s_cause_effect_classification            |           |\\n| task 442 com_qa_question_rewriting                    |           |\\n| task 391 cod3s_cause_effect_classification            |           |\\n| task 1342 amazon_us_reviews_title_generation          |           |\\n| task 677 ollie_data_to_text                           |           |\\n| task 020 mctaco_answerability_classification          |           |\\n| task 1533 dailydialog_dialogue_act_recognition        |           |\\n| task 226 stack_overflow_answerability_classification  |           |\\n| task 957 e2e_data_to_text                             |           |\\n| task 288 gigaword_title_generation                    |           |\\n| task 1728 web_nlg_data_to_text                        |           |\\n| task 1358 xlsum_title_generation                      |           |\\n| task 1529 scitailv1.1_textual_entailment              |           |\\n| task 349 squad2.0_answerability_classification        |           |\\n| task 1442 doqa_answerability_classification           |           |\\n| task 640 e_snli_textual_entailment                    |           |\\n| task 648 winograd_wsc_coreference_resolution           |           |\\n| task 035 winogrande_question_rewriting                |           |\\n| task 102 commun_gen_data_to_text                      |           |\\n| task 1598 nyc_data_to_text                            |           |\\n| task 1516 imppres_textual_entailment                  |           |\\n| task 034 winogrande_question_rewriting                |           |\\n| task 500 scruples_title_generation                    |           |\\n| task 935 defeasible_nli_atomic_textual_entailment     |           |\\n| task 1659 billsum_title_generation                    |           |\\n| task 1391 winogrande_coreference_resolution           |           |\\n| task 1531 dailydialog_dialogue_act_recognition        |           |\\n| task 892 gap_coreference_resolution                   |           |\\n| task 602 wikitext_title_generation                    |           |\\n| task 743 eurlex_title_generation                      |           |\\n| task 1624 disfl_qa_answerability_classification       |           |\\n| task 1154 bard_word_analogy                           |           |\\n| task 1598 nyc_data_to_text                            |           |\\n| task 1393 copa_cause_effect_classification             |           |\\n| task 1540 peer_read_title_generation                  |           |\\n| task 1615 sick_textual_entailment                     |           |\\n| task 1152 bard_word_analogy                           |           |\\n| task 970 sherliic_textual_entailment                  |           |\\n| task 329 gap_coreference_resolution                   |           |\\n| task 936 defeasible_nli_atomic_textual_entailment     |           |\\n| task 418 persent_title_generation                     |           |\\n| task 1161 coda_19_title_generation                    |           |\\n| task 1155 bard_word_analogy                           |           |\\n| task 613 liar_keyword_tagging                         |           |\\n| task 1586 scifact_title_generation                    |           |\\n| task 1640 adverserial_qa_answerability_classification |           |\\n| task 401 numeric_fused_head_coreference_resolution    |           |\\n| task 879 schema_guided_dstc8_dialogue_act_recognition |           |\\n| task 880 schema_guided_dstc8_dialogue_act_recognition |           |\\n| task 1393 copa_cause_effect_classification             |           |\\n| task 1540 peer_read_title_generation                  |           |\\n| task 1612 sick_textual_entailment                     |           |\\n| task 1158 bard_word_analogy                           |           |\\n| task 620 ohsumed_keyword_tagging                      |           |\\n| task 242 tweetqa_answerability_classification         |           |\\n| task 220 rocstories_title_generation                  |           |\\n| task 249 enhanced_wsc_coreference_resolution          |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 1156 bard_word_analogy                           |           |\\n| task 200 multinli_textual_entailment                   |           |\\n| task 033 winogrande_coreference_resolution             |           |\\n| task 891 gap_coreference_resolution                   |           |\\n| task 1394 meta_woz_dialogue_act_recognition           |           |\\n| task 330 gap_coreference_resolution                   |           |\\n| task 936 defeasible_nli_atomic_textual_entailment     |           |\\n| task 402 grailqa_question_rewriting                   |           |\\n| task 039 qasc_overlap_extraction                      |           |\\n| task 1664 wino_bias_coreference_resolution            |           |\\n| task 827 copa_cause_effect_classification             |           |\\n| task 1158 bard_word_analogy                           |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 620 ohsumed_keyword_tagging                      |           |\\n| task 242 tweetqa_answerability_classification         |           |\\n| task 220 rocstories_title_generation                  |           |\\n| task 249 enhanced_wsc_coreference_resolution          |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 1156 bard_word_analogy                           |           |\\n| task 200 multinli_textual_entailment                   |           |\\n| task 033 winogrande_coreference_resolution             |           |\\n| task 891 gap_coreference_resolution                   |           |\\n| task 1394 meta_woz_dialogue_act_recognition           |           |\\n| task 330 gap_coreference_resolution                   |           |\\n| task 936 defeasible_nli_atomic_textual_entailment     |           |\\n| task 402 grailqa_question_rewriting                   |           |\\n| task 039 qasc_overlap_extraction                      |           |\\n| task 1664 wino_bias_coreference_resolution            |           |\\n| task 827 copa_cause_effect_classification             |           |\\n| task 1158 bard_word_analogy                           |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 620 ohsumed_keyword_tagging                      |           |\\n| task 242 tweetqa_answerability_classification         |           |\\n| task 220 rocstories_title_generation                  |           |\\n| task 249 enhanced_wsc_coreference_resolution          |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 1156 bard_word_analogy                           |           |\\n| task 200 multinli_textual_entailment                   |           |\\n| task 033 winogrande_coreference_resolution             |           |\\n| task 891 gap_coreference_resolution                   |           |\\n| task 1394 meta_woz_dialogue_act_recognition           |           |\\n| task 330 gap_coreference_resolution                   |           |\\n| task 936 defeasible_nli_atomic_textual_entailment     |           |\\n| task 402 grailqa_question_rewriting                   |           |\\n| task 039 qasc_overlap_extraction                      |           |\\n| task 1664 wino_bias_coreference_resolution            |           |\\n| task 827 copa_cause_effect_classification             |           |\\n| task 1158 bard_word_analogy                           |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 620 ohsumed_keyword_tagging                      |           |\\n| task 242 tweetqa_answerability_classification         |           |\\n| task 220 rocstories_title_generation                  |           |\\n| task 249 enhanced_wsc_coreference_resolution          |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 1156 bard_word_analogy                           |           |\\n| task 200 multinli_textual_entailment                   |           |\\n| task 033 winogrande_coreference_resolution             |           |\\n| task 891 gap_coreference_resolution                   |           |\\n| task 1394 meta_woz_dialogue_act_recognition           |           |\\n| task 330 gap_coreference_resolution                   |           |\\n| task 936 defeasible_nli_atomic_textual_entailment     |           |\\n| task 402 grailqa_question_rewriting                   |           |\\n| task 039 qasc_overlap_extraction                      |           |\\n| task 1664 wino_bias_coreference_resolution            |           |\\n| task 827 copa_cause_effect_classification             |           |\\n| task 1158 bard_word_analogy                           |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 620 ohsumed_keyword_tagging                      |           |\\n| task 242 tweetqa_answerability_classification         |           |\\n| task 220 rocstories_title_generation                  |           |\\n| task 249 enhanced_wsc_coreference_resolution          |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 1156 bard_word_analogy                           |           |\\n| task 200 multinli_textual_entailment                   |           |\\n| task 033 winogrande_coreference_resolution             |           |\\n| task 891 gap_coreference_resolution                   |           |\\n| task 1394 meta_woz_dialogue_act_recognition           |           |\\n| task 330 gap_coreference_resolution                   |           |\\n| task 936 defeasible_nli_atomic_textual_entailment     |           |\\n| task 402 grailqa_question_rewriting                   |           |\\n| task 039 qasc_overlap_extraction                      |           |\\n| task 1664 wino_bias_coreference_resolution            |           |\\n| task 827 copa_cause_effect_classification             |           |\\n| task 1158 bard_word_analogy                           |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 620 ohsumed_keyword_tagging                      |           |\\n| task 242 tweetqa_answerability_classification         |           |\\n| task 220 rocstories_title_generation                  |           |\\n| task 249 enhanced_wsc_coreference_resolution          |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 1156 bard_word_analogy                           |           |\\n| task 200 multinli_textual_entailment                   |           |\\n| task 033 winogrande_coreference_resolution             |           |\\n| task 891 gap_coreference_resolution                   |           |\\n| task 1394 meta_woz_dialogue_act_recognition           |           |\\n| task 330 gap_coreference_resolution                   |           |\\n| task 936 defeasible_nli_atomic_textual_entailment     |           |\\n| task 402 grailqa_question_rewriting                   |           |\\n| task 039 qasc_overlap_extraction                      |           |\\n| task 1664 wino_bias_coreference_resolution            |           |\\n| task 827 copa_cause_effect_classification             |           |\\n| task 1158 bard_word_analogy                           |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 620 ohsumed_keyword_tagging                      |           |\\n| task 242 tweetqa_answerability_classification         |           |\\n| task 220 rocstories_title_generation                  |           |\\n| task 249 enhanced_wsc_coreference_resolution          |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 1156 bard_word_analogy                           |           |\\n| task 200 multinli_textual_entailment                   |           |\\n| task 033 winogrande_coreference_resolution             |           |\\n| task 891 gap_coreference_resolution                   |           |\\n| task 1394 meta_woz_dialogue_act_recognition           |           |\\n| task 330 gap_coreference_resolution                   |           |\\n| task 936 defeasible_nli_atomic_textual_entailment     |           |\\n| task 402 grailqa_question_rewriting                   |           |\\n| task 039 qasc_overlap_extraction                      |           |\\n| task 1664 wino_bias_coreference_resolution            |           |\\n| task 827 copa_cause_effect_classification             |           |\\n| task 1158 bard_word_analogy                           |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 620 ohsumed_keyword_tagging                      |           |\\n| task 242 tweetqa_answerability_classification         |           |\\n| task 220 rocstories_title_generation                  |           |\\n| task 249 enhanced_wsc_coreference_resolution          |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 1156 bard_word_analogy                           |           |\\n| task 200 multinli_textual_entailment                   |           |\\n| task 033 winogrande_coreference_resolution             |           |\\n| task 891 gap_coreference_resolution                   |           |\\n| task 1394 meta_woz_dialogue_act_recognition           |           |\\n| task 330 gap_coreference_resolution                   |           |\\n| task 936 defeasible_nli_atomic_textual_entailment     |           |\\n| task 402 grailqa_question_rewriting                   |           |\\n| task 039 qasc_overlap_extraction                      |           |\\n| task 1664 wino_bias_coreference_resolution            |           |\\n| task 827 copa_cause_effect_classification             |           |\\n| task 1158 bard_word_analogy                           |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 620 ohsumed_keyword_tagging                      |           |\\n| task 242 tweetqa_answerability_classification         |           |\\n| task 220 rocstories_title_generation                  |           |\\n| task 249 enhanced_wsc_coreference_resolution          |           |\\n| task 1157 bard_word_analogy                           |           |\\n| task 1156 bard_word_analogy                          "}
{"id": "emnlp-2022-main-340", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: \\\\( mTk-I\\\\textsc{NSTRUCT} \\\\)'s per-task performance improvement over the best of two heuristic baselines on the 35 evaluation tasks of the cross-lingual track.\"}"}
