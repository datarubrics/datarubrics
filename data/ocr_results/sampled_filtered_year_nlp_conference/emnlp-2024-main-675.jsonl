{"id": "emnlp-2024-main-675", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unown Claims: Generation of Fact-Checking Training Examples from Unstructured and Structured Data\\n\\nJean-Flavien Bussotti \u2217\\nLuca Ragazzi \u2217\\nGiacomo Frisoni \u2217\\nGianluca Moro \u2217\\nPaolo Papotti \u2217\\nEURECOM, France\\n{bussotti, papotti}@eurecom.fr\\nDepartment of Computer Science and Engineering, University of Bologna, Italy\\n{l.ragazzi, giacomo.frisoni, gianluca.moro}@unibo.it\\n\\nAbstract\\nComputational fact-checking (FC) relies on supervised models to verify claims based on given evidence, requiring a resource-intensive process to annotate large volumes of training data. We introduce UNOWN, a novel framework that generates training instances for FC systems automatically using both textual and tabular content. UNOWN selects relevant evidence and generates supporting and refuting claims with advanced negation artifacts. Designed to be flexible, UNOWN accommodates various strategies for evidence selection and claim generation, offering unparalleled adaptability. We comprehensively evaluate UNOWN on both text-only and table+text benchmarks, including FEVEROUS, SCAF, and MMFC, a new multi-modal FC dataset. Our results prove that UNOWN examples are of comparable quality to expert-labeled data, even enabling models to achieve up to 5% higher accuracy. The code, data, and models are available at https://github.com/disi-unibo-nlp/unown\\n\\n1 Introduction\\nThe spread of false information on social media threatens public trust. For example, during the COVID-19 pandemic, misinformation led to vaccine hesitancy, straining public health systems and informed decision-making (Saakyan et al., 2021; Carey et al., 2022; Carrieri et al., 2023). Computational fact-checking (FC) is a vital tool for verifying claims against diverse evidence types, including unstructured text and structured tabular data. Diversity increases task complexity, requiring advanced NLP methods to cross-reference information accurately (Guo et al., 2022).\\n\\nTraditional FC verification models (i.e., those making final predictions over evidence, without retrieving it) heavily rely on training samples manually annotated by experts, who meticulously review and pair claims with corresponding evidence, and\\n\\n\u2217 Equal contribution (co-first authorship).\\n\\nFigure 1: UNOWN pipeline. Given a corpus of documents, the Example Generation module (investigated in this work) outputs training instances.\\n\\nInference\\nTest\\nData\\n\\nEvidence: Claim \u2705:\\nLabel: Coliseum is in Rome\\nSupports Rome Italy\\nColiseum is in Italy\\n\\nTrain\\nCities\\n Evidence\\nSelection\\nCorpus\\nCity Country\\nRome Italy\\nNice France\\n\\nEvidence:\\nClaim \u274c:\\nLabel:\\nColiseum is in France\\nRefutes Rome Italy\\nColiseum is in Rome\\n\\nFact-Checking Model\\n\\nSupports\\nRefutes\\n\\nExample Generation\\n\\nInference\\nTest\\nData\\n\\nEvidence: Claim \u2705:\\nLabel: Coliseum is in Rome\\nSupports Rome Italy\\nColiseum is in Italy\\n\\nTrain\\nCities\\n Evidence\\nSelection\\nCorpus\\nCity Country\\nRome Italy\\nNice France\\n\\nEvidence:\\nClaim \u274c:\\nLabel:\\nColiseum is in France\\nRefutes Rome Italy\\nColiseum is in Rome\\n\\nFact-Checking Model\\n\\nSupports\\nRefutes\\n\\nExample Generation\\n\\nInference\\nTest\\nData\\n\\nEvidence: Claim \u2705:\\nLabel: Coliseum is in Rome\\nSupports Rome Italy\\nColiseum is in Italy\\n\\nTrain\\nCities\\n Evidence\\nSelection\\nCorpus\\nCity Country\\nRome Italy\\nNice France\\n\\nEvidence:\\nClaim \u274c:\\nLabel:\\nColiseum is in France\\nRefutes Rome Italy\\nColiseum is in Rome\\n\\nFact-Checking Model\\n\\nSupports\\nRefutes\\n\\nExample Generation\\n\\nInference\\nTest\\nData\\n\\nEvidence: Claim \u2705:\\nLabel: Coliseum is in Rome\\nSupports Rome Italy\\nColiseum is in Italy\\n\\nTrain\\nCities\\n Evidence\\nSelection\\nCorpus\\nCity Country\\nRome Italy\\nNice France\\n\\nEvidence:\\nClaim \u274c:\\nLabel:\\nColiseum is in France\\nRefutes Rome Italy\\nColiseum is in Rome\\n\\nFact-Checking Model\\n\\nSupports\\nRefutes\"}"}
{"id": "emnlp-2024-main-675", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Summary of works on the automatic generation of training samples for fact-checking systems.\\n\\n| Work                      | Human Eval | Pan et al. (2021) | FEVER (2018) | Wright et al. (2022) | Ours  |\\n|---------------------------|------------|-------------------|--------------|----------------------|-------|\\n|                           |            | \u2717                 | \u2713            | \u2717                    | \u2717     |\\n|                           |            | \u2717                 | \u2717            | \u2713                    | \u2713     |\\n|                           |            | \u2713                 | \u2713            | \u2717                    | \u2713     |\\n|                           |            | \u2717                 | \u2717            | \u2717                    | \u2717     |\\n\\n\u2020 The study combines unstructured and structured data as evidence.\\n\u2021 The study includes human examination of the generated examples.\\n\\nWe validate our approach by comparing the accuracy of FC models trained on examples generated by U\\\\textsuperscript{OWN} versus those labeled by humans. To achieve this, we conduct extensive experiments on text-only and text+table evidence scenarios using three public FC datasets targeting general and scientific content: FEVEROUS (Aly et al., 2021), SCIFACT (Wadden et al., 2020), and MMFC, our new multi-modal and multi-domain fact-checking dataset.\\n\\n4 MMFC complements FEVEROUS as the second existing corpus featuring textual and tabular evidence, distinguishing it from SCIFACT, which exclusively focuses on text.\\n\\nThe main findings of our study are as follows:\\n\\n\u2022 In text-only evidence scenarios, training on U\\\\textsuperscript{OWN} data yields lower accuracy, showing an 8% gap compared to human-labeled samples. However, this gap diminishes to just 2% with the inclusion of only 100 human-labeled instances. Conversely, in text+table scenarios, we achieve up to 5% higher accuracy.\\n\\n\u2022 SLMs and LLMs produce synthetic data of comparable quality, with just a 1% gap in downstream FC accuracy.\\n\\n\u2022 By transcending traditional reliance on external KBs, U\\\\textsuperscript{OWN} adeptly generates refuting claims with sophisticated negation artifacts.\\n\\n2 Related Work\\n\\nComputational FC has been an active area of research for decades (Dagan et al., 2005; Guo et al., 2022). Recently, the rise of LLMs has advanced the development of FC pipelines (Schulman et al., 2022), but their effectiveness is still inferior to human experts (Saeed et al., 2022; Caramancion, 2023). Specialized models are currently the most effective approach (Li et al., 2023), despite they require large labeled datasets for training.\\n\\nExisting methods for automatically generating FC training examples have been approached through both unsupervised and supervised techniques. Unsupervised solutions, typically employed in the absence of labeled data, leverage PLMs to create textual claims from a given text, e.g., by using template prompts (Meng et al., 2022). Supervised approaches rely on specific resources, e.g., an annotated taxonomy to train an LSTM model for sentence generation (Meng et al., 2019). Several works have investigated the generation of claims from textual evidence (see Table 1). Pan et al. (2021) produce question\u2013answer pairs using answer replacement to assemble the refuting claim. Wright et al. (2022) create supporting claims with a generative PLM and ER over a domain-specific KB for evidence refusal in the biomedical field.\\n\\nResearch on generating claims specifically from tabular data remains limited. While some studies...\"}"}
{"id": "emnlp-2024-main-675", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ies have explored template-based methods (Wang et al., 2021; Veltri et al., 2023), Bussotti et al. (2023) demonstrated improved results by producing claims based on human-provided examples. Artificial text passages have recently demonstrated greater effectiveness than human-written ones for reasoning-demanding QA (Frisoni et al., 2024), but FC tasks have not yet been studied. To the best of our knowledge, no existing work has addressed the generation of FC training examples from structured and unstructured data as input.\\n\\n3 Problem Formulation\\n\\nLet $d$ represent a semi-structured document (e.g., a Wikipedia page) containing $n$ sentences and $m$ tables. We define evidence $e = \\\\{e_s, e_t\\\\}$ as a non-empty subset of sentences $e_s = \\\\{s_1, ..., s_{|e_s|} \\\\}$ and, optionally, cell values $e_t = \\\\{c_1, ..., c_{|e_t|} \\\\}$ extracted from a table within $d$, where $p$ is the total number of cells. A supervised FC model $F$ evaluates whether a textual claim $c$ is supported or contradicted by the given evidence $e$. Specifically, $F$ takes as input a data pair $<e, c>$ and outputs a verdict from the set $L = \\\\{Supports, Refutes\\\\}$.\\n\\nConsequently, our goal is to automatically generate labeled examples $E = <e, c, l \\\\in L>$ to train $F$.\\n\\nChallenge I: Refuting Claims. There have been proposals to generate artificial claims by synthesizing $e$ in a sentence. Abstractive summarization has been explored with text-only evidence (Tonguz et al., 2021; Wright et al., 2022) and scenarios centered on cell values only (Bussotti et al., 2023). In contrast, our goal is to create claims that incorporate evidence from both structured and unstructured data, as illustrated in Figure 1. However, while a $Supports$ claim naturally aligns with the provided $e$, we also require examples with a $Refutes$ label to train FC models effectively, which entails claims that are in conflict with $e$. Technically, refuting samples should go beyond basic negations such as \\\"Rome is not in Italy.\\\" They should instead be adept at capturing nuanced factual contradictions, e.g., \\\"Rome is in France\\\", \\\"There are two Coliseums in Rome.\\\" Obtaining such variety in claims remains an open research question.\\n\\nChallenge II: Low-Budget Environment. In low-resource settings, restrictions such as commoditity hardware infrastructure can affect model supervision and performance (Parida and Motl\u00edcek, 2019; Moro and Ragazzi, 2022, 2023; Huh and Ko, 2023; Moro et al., 2023a,b,c). In the era of LLMs, the investigation of flexible and scalable solutions is being neglected despite their high social impact (Tamkin et al., 2021). Developing FC systems capable of scaling and adapting to diverse user needs and scenarios is imperative.\\n\\n4 Method\\n\\nWe introduce $UNOWN$ (Figure 3), a novel framework to automate the production of FC training data. In a first step, $e$ is created from the input $d$ (evidence selection). Then, $e$ is used to generate supporting or refuting claims (claim generation).\\n\\n4.1 Evidence Selection\\n\\nAnchor Creation. The evidence construction process begins by creating a textual anchor $a$. We distinguish two settings. Text-only: $a$ is a randomly selected sentence from the document $d$. Text + Table: we combine textual and tabular data to determine $a$. In alignment with the text-centric vision of previous works (Berrios et al., 2023; Tan et al., 2023; Zeng et al., 2023), we fine-tune T5-large (780M parameters) (Raffel et al., 2020) on TOT (Parikh et al., 2020), a table-to-text dataset. We sample table cells following a distribution based on the observed tabular evidence size in the FEVEROUS training set (i.e., $[2, 3, 3, 4, 4, 4, 5, 5, 6, 6, 7, 8]$) to generate $a$ (text) by inference, unifying the data modalities. The prompt uses cell values and includes contextual details such as table headers and the document title to maintain coherence (see Figure 4). This approach eases claim generation but still leaves the question of how to select evidence.\\n\\nEvidence Completion. Once $a$ is created, we propose two alternative strategies to complete the evidence.\\n\\nRandom: we pick $k$ random sentences from $d$. Various topics may exist within $e$, as the information chosen may not be aligned.\\n\\nSemantic Consistency: $e_s$ is built by concatenating the $k$ sentences from $d$ that semantically align the most with $a$, preserving the topic coherence. As in Liu et al. (2023), we use cosine similarity after T5 encoding. We expand on important clarifications.\\n\\n1. In text-only scenarios, $e_t = \\\\emptyset$ and $e_s$ consists of a set of sentences. In text + table scenarios, $e_t$ is non-empty.\"}"}
{"id": "emnlp-2024-main-675", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: UNOWN pipeline. The input document $d$ consists of sentences and optional tables. (1) When both modalities are used, we obtain $e$ with a cell sampling and verbalization process. From $e$, different strategies can be used to determine $e$ and complete $e$; in a text-only approach ($e = \\\\emptyset$), $e$ is established after sentence sampling. (2) We generate supporting and refuting claims using PLMs. Non-continuous lines and arrows delineate alternatives.\\n\\n| City Name | Function |\\n|-----------|----------|\\n| London    | Mike     |\\n| New York  | Luke     |\\n\\n\u201cWorking People\u201d\\n\\nMike works as a teacher in London.\\n\\nFigure 4: Verbalization of a subset of tabular cells.\\n\\ncomprises sentences and a verbalized representation of tabular cells. We overwrite $e$ by prefixing the table title for context with special $<$title> and $<$evidence> token delimiters. Concatenation enables cross-attention among the page title, cells, and sentences.\\n\\n2. $k$ is drawn randomly from a distribution of $[1, 1, 2, 2, 2, 3, 3, 4, 5]$, selected based on patterns observed in the EVEROUS training set.\\n\\n3. We emphasize that constructing $e$ from $e$ to $e$ using a single verbalization step is the most practical approach, avoiding the complexities of reverse operations.\\n\\n4.2 Claim Generation\\n\\nFine-tuning models on data aligned with the target task has proven effective in enhancing performance (Gururangan et al., 2020). Practically, users can expect access to external data from related FC applications and a limited number (e.g., 10, 100) of internal human samples specific to the downstream task. Given this context, we define the following concepts to guide our methodology.\\n\\nWarm-start: external examples are available for preliminary training (i.e., $e \\\\rightarrow c$).\\n\\nCold-start: no external data is available.\\n\\nFew-shot learning: internal examples are accessible for specialized fine-tuning (regardless of warm/cold start).\\n\\nRefuting Claims. Generating refuting claims comes with additional intricacies. We recognize two main paths to avoid introducing a strong lexical bias in the artificial training samples, such as basic\"}"}
{"id": "emnlp-2024-main-675", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Direct Refusal: we use a PLM that can directly transform $e$ into a refuting claim, ensuring a direct and straightforward method.\\n\\nTwo-Step Approach: we summarize $e$ into a supporting claim and apply a targeted modification to flip its meaning. This involves either using Direct Refusal with the supporting claim or employing ER, where keywords are strategically swapped with antonyms or related terms from a KB.\\n\\n5 Experimental Setup\\n\\nOur focus is on evaluating the veracity component of the FC process during test time, where models are provided with gold evidence alongside the claim for verification. To achieve this, we address the following research questions:\\n\\nQ1 Are our generated artificial examples effective for training FC models?\\n\\nQ2 Which evidence selection strategy yields the best performance?\\n\\nQ3 What method is recommended for generating refuting claims?\\n\\nQ4 To what extent does the efficacy of synthetic examples generalize across various domains?\\n\\nQ5 How many internal dataset-specific samples are necessary for few-shot learning to bootstrapped the downstream FC model successfully?\\n\\nDatasets.\\n\\nIn warm-start scenarios, we use a subset of 10K positive and 10K negative human examples from FEVER (Thorne et al., 2018), a collection of claim\u2013evidence pairs based on Wikipedia. As the leading FC benchmark, we take FEVEROUS (Aly et al., 2021), an extension of FEVER with more complex claims enriched with tabular evidence (with no overlap between the two corpora). Since the original test set is private and lacks gold labels and evidence for the claims, we used the provided validation set as our test set for evaluation. We then divided this set into two sub-sets: one containing claims based solely on textual evidence, and another containing claims that require both textual and tabular evidence (we exclude claims relying only on tables). To assess generality, we include SCIFACT (Wadden et al., 2020), a dataset of expert-written claims paired with evidence from scientific papers abstracts. For the same rationale applied to FEVEROUS, we used the original validation set as our test set. Finally, we release MMFC, a new multi-modal FC corpus.\\n\\nMechanically, we sample 2000 instances from MULTIMODAL QA (Talmor et al., 2021), a QA dataset requiring joint reasoning over text, table, and images. In our sampling procedure, we filter out instances requiring visual grounding. Then, we perform few-shot in-context learning with GPT-4-TURBO to transform each question\u2013answer pair into a claim paired with text + table evidence. Finally, we carefully review all examples through human verification to ensure that all reference claims were qualitatively accurate and correctly labeled. Dataset statistics are provided in Table 2. See the Appendix for details.\\n\\nMetrics.\\n\\nWe assess FC predictions using Accuracy and F1 scores ([0, 1]; higher is better), distinguishing between Supports and Refutes labels. We validate models on the test sets after training with artificial and human examples. We finally evaluate the logical relationship between each evidence\u2013claim pair with a DEBERTA cross-encoder (Reimers and Gurevych, 2019) pretrained on natural language inference (NLI) tasks to classify pairs as Entailment, Contradiction, or Neutral.\\n\\nClaim Generation Models.\\n\\nAs SLM, we use models built on BART (Lewis et al., 2020). For supporting claims, we employ the large version (400M parameters). For refuting claims, we utilize two variants: BART-large and BARTNEG (Lee et al., 2021), a specialized BART-base model (140M parameters) trained on parallel and opposing claims from the WIKIFACTCHECK dataset (Sathe et al., 2020).\\n\\nAs LLM, we operate with LLM-2-7B (Touvron et al., 2023), opting for QLoRA (Dettmers et al., 2023).\"}"}
{"id": "emnlp-2024-main-675", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6 Results and Discussion\\n\\n6.1 Quality of Generated Claims\\n\\nSLMs. We evaluate how UNOWN training examples generated by small models contribute to a downstream FC system by measuring performance on the FEVEROUS test set (Figure 5). In the worst-case scenario (cold-start, zero-shot learning), the highest accuracy achievable by UNOWN is 86.7 with BART-large used for the generation of both supporting and refuting claims. When leveraging human training instances, the results show a consistent boost in performance. In fact, accuracy climbs to 92.3 with warm start and just 100 internal target examples, using BART-large for supporting claims and BART-NEG for direct refusal\u2014close to the accuracy achievable with human-annotated data (94.5).\\n\\nLLMs. Figure 6 looks at how the claims generated by LLAMA-2 stack up against those inferred by the best SLM setup. The accuracy propelled by LLAMA-2 claims, after training on 100 internal examples, is 93.3, outperforming the small solution by a single point. Therefore, incorporating LLMs does not appear essential in the UNOWN pipeline, favoring BART-based models for their...\"}"}
{"id": "emnlp-2024-main-675", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To gain additional insight into the generated claims, we compute the NLI prediction score between claims and evidence. Table 3 shows that, for supporting claims, UNOWN\u2019s examples closely resemble the score distribution in their human-written counterparts. Yet, in the refuting examples generated by UNOWN, the percentage of entailed claims surpasses that of human-generated ones, highlighting the greater difficulty in creating refuting examples compared to supporting ones. We observe that the ER baseline performs the worst.\\n\\nChallenges of Claim Verification.\\nWe evaluate the effectiveness of our data generation method across challenge categories defined by Aly et al. (2021). Specifically, we compare the performance of an FC model trained on UNOWN data versus human-crafted data on different subsets of the FEVEROUS test set, each focused on a particular challenge. As shown in Table 4, the FC model trained on our data performs competitively in several categories, such as \u201cCombining Tables and Texts\u201d and \u201cSearch Terms Not in Claim,\u201d even outperforming the model trained on human-generated data. While the FEVEROUS-trained model holds a slight advantage in areas like \u201cMulti-hop Reasoning,\u201d \u201cNumerical Reasoning,\u201d and \u201cEntity Disambiguation,\u201d our approach radically reduces the need for expensive and time-consuming human annotation.\\n\\nHuman Evaluation.\\nWe perform a qualitative analysis to investigate the quality of the claims generated by UNOWN. We randomly sample 50 instances from the FEVEROUS training data (25 supporting, 25 refuting). Taking into account the expense associated with careful human evaluation and the central role of text as our unified modality, we accord priority to text-only evidence. Each instance is presented with its original human-selected evidence and the corresponding claim. To maintain fairness, we condition our models on this evidence and generate synthetic claims using our best-performing models: the warm-started BART-large for supporting claims and BART-NEG for refuting claims. After manually verifying the correctness of the assigned label, which were accurate for all 50 claims, we enlist the expertise of three external annotators with strong NLP and FC backgrounds to evaluate the claims. In a blind review process, we provide them with the evidence and the two claims (original and generated) in randomized order. Following a direct comparison assessment, which has proven to be more reliable and sensitive than rating scales (Kiritchenko and Mohammad, 2017) and has been used to evaluate abstractive summaries (Fabbri et al., 2019; Moro et al., 2023d; Ragazzi et al., 2024) and answers (Moro et al., 2024), we ask the annotators to determine which claim is the best with respect to two dimensions: clarity (effective communication of the intended meaning with a good sentence structure, fluency, and English precision) and coherence (semantic connection to the evidence). They are also given the option to declare a tie if they perceive the quality of the claims to be comparable. To aggregate the annotations, we employ a majority voting approach and calculate Cohen\u2019s $\\\\kappa$ coefficient to gauge the agreement between annotators and the majority voting label. The coefficient value of 0.613 indicates a substantial level of agreement, enhancing the reliability of our analysis.\"}"}
{"id": "emnlp-2024-main-675", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Overall, the generated examples (see the Appendix) prove to be sufficiently effective for training FC models, yielding quantitative results in a 2-point margin in absolute accuracy compared to those achieved by a crowd of annotators.\\n\\n### 6.2 Evidence Selection\\n\\nWe study the impact of alternative evidence selection methods. We report two experiments using FEVEROUS training examples: one with text-only evidence and another with text+table evidence; test datasets are filtered according to the scenario. For every human example, referred to as \\\"gold,\\\" we execute our best BART model with four alternative evidence selection strategies:\\n\\n- **Human evidence**, where we use the original evidence handpicked by the annotators.\\n- **Random with gold**, where the number of selected sentences matches the human example, but the actual cells and sentences are chosen randomly from \\\\( d \\\\).\\n- **Random without gold**, where the number of retrieved sentences \\\\( k \\\\), after anchor definition, is drawn from the distribution presented in Section 4.1.\\n- **Semantic consistency**, where textual evidence is retrieved using embedding similarity to the table verbalization (see Figure 4).\\n\\nTable 5 shows accuracy and F1 results. The influence of evidence is evident. The use of human evidence allows UNOWN to produce examples that match nearly the human upper bound. In the text+table setting, we achieve even higher scores for supporting claims, confirming the quality of our claim generator. In the text-only scenario, performance is optimal when guided by the cardinality of human gold evidence, with random selection surpassing semantic consistency. In text+table, semantic consistency outperforms both random selection and original human examples in all metrics. We observe that human annotators struggle to annotate tabular data accurately, making mistakes that mislead the classifier. This is also reflected in the generally lower results for text+table compared to the text-only scenario.\\n\\nTable 5 also shows the results for MMFC. In this dataset, all claims involve text and tabular data and we only have human gold evidence for the original claim. We explain the lower quality results for UNOWN because the warm start includes examples from FEVER, which are different from those in MMFC (see the Appendix for examples), possibly introducing a negative bias.\\n\\n### 6.3 Refuting Claims\\n\\nWe show how the FC performance varies with different types of Refutes generated claims in a quantitative analysis and then in a qualitative user study.\\n\\n**Quantitative.** We identify FAN as the best ER method (the results are shown in Figure 12 in the Appendix); unless otherwise specified, we use ER to denote this baseline approach. Figure 5 includes the impact of various negation strategies on the accuracy of the target task. In cold start, the combination of BART and BARTNEG using the two-step approach is effective, while the results are subpar with ER, which fails to make refuting claims, possibly due to limitations in content replacement without adequate rewording. As anticipated, starting with a warm start is beneficial, resulting in the highest accuracy with 0 and 100 training samples.\\n\\n**Qualitative.** We perform a human analysis to evaluate the negation techniques used to refute claims. We adhere to the negation taxonomy outlined in previous studies (Zafra et al., 2020; Dobreva and Keller, 2021). Rigorously, we use two main negation strategies:\"}"}
{"id": "emnlp-2024-main-675", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Strategies for refuting claim generation on SciFact; models use BART to create supporting claims.\\n\\nIn warm scenarios, models are fine-tuned on FEVER.\\n\\n| Strategy Type          | Accuracy | Model | Accuracy | Model | Accuracy |\\n|------------------------|----------|-------|----------|-------|----------|\\n| Verbal Negation (V)    |          |       |          |       |          |\\n| Morphological (M)      |          |       |          |       |          |\\n| Replacement (R)        |          |       |          |       |          |\\n\\nGiven these classes, three annotators (selected among the authors) evaluated 30 refuting claims from the original FEVEROUS training dataset and 30 refuting claims generated by HUMAN. The final category is identified by majority voting over the three suggested labels; the Cohen's \u03ba coefficient is 0.91, which shows very high agreement among annotators. The results of the study are illustrated in Figure 8, allowing a comparison of annotation distributions between the two sets of examples (HUMAN vs. human). HUMAN produces an even distribution of refuting claims, encompassing both noun phrases and verbal structures, whereas humans tend to prefer noun phrases. Both HUMAN and humans favor the replacement strategy for noun phrases and the lexical strategy for verbs. In both scenarios, the ranking of classes and subclasses remains consistent, indicating that HUMAN produces a range of negation types comparable to those observed in a human-crafted corpus.\\n\\n6.4 Checking Scientific Claims\\n\\nWe measure the quality of the FC system trained with HUMAN examples in a different domain. Due to the lack of heterogeneous datasets such as FEVEROUS, we use the text-only scientific corpus SciFact. Table 6 confirms the analysis outcome on FEVEROUS. Human data achieve the best results, followed by HUMAN with the warm-started BART.\\n\\nWe explain the greater result gap between humans and HUMAN because the warm start includes only examples from FEVER. Again, BART-NEG leads to better results with respect to ER. We posit that low F1 refuting scores (i.e., 1.98, 16.51) stem from FLAN-T5's pre-knowledge bias, which may not adequately align with scientific subjects.\\n\\n6.5 Bootstrapping: Cold vs. Warm Start\\n\\nWe measure the impact of the examples used to fine-tune the models. As shown in Figure 5, Figure 6, and Table 6, a warm-start approach improves the quality of the generated data. More precisely, Figure 9 shows the average \u2206accuracy improvement when shifting from cold to warm in FEVEROUS. We observe a decrease in \u2206as the number of internal samples from the target dataset increases, highlighting the beneficial contribution of using external related data as a guide source of knowledge. Also SciFact exhibits an increase in accuracy for BART-NEG in the warm approach.\\n\\n7 Conclusion\\n\\nWe introduced HUMAN, a domain-agnostic framework to automatically generate training examples for fact-checking systems, bypassing the costly task of manually annotating large volumes of data. HUMAN fits both structured and unstructured data to compile textual claims that support or refute the evidence provided. It also accommodates several solutions for evidence selection and claim generation to adapt to different scenarios. We evaluated our framework using three datasets that deal with general-domain and scientific contexts. The results indicate that our synthetic examples exhibit a quality comparable to that of expert-labeled data, showing the practicality and efficacy of our framework. Quantitative and human evaluation also register that our refuting examples have high variety, comparable to human-generated ones.\\n\\nLimitations\\n\\nAlthough HUMAN is a promising step forward, some research directions remain unexplored. First, our generation process lacks coverage of certain...\"}"}
{"id": "emnlp-2024-main-675", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"examples within the long tail, e.g., mathematical operations, such as the premise \\\"Paul is 2 years younger than Mary.\\\" We consider using a solution in which more intricate patterns are generated as queries over relational tables (Bussotti et al., 2023). Second, once models have been trained with instances from U\\\\textsubscript{NOWN}, we could set up active learning algorithms to guide our methods in generating examples that effectively enhance performance on the test set (Zhang et al., 2022). Third, while the considered datasets contain well-crafted claims, real-world claims can often be incomplete\u2014lacking context and presenting ambiguity in relation to the evidence (Glockner et al., 2024)\u2014or require multi-modal evidence that extends beyond text and tables (Akhtar et al., 2023). Furthermore, reasoning over multiple pieces of evidence from diverse sources may also be necessary. Finally, the selection of a specific model for generating supporting or refuting claims can result in diverse fact-checking challenges that may vary in their alignment with the target dataset. For instance, the EVEROUS test set contains instances that demand robust multi-hop reasoning abilities, whereas other benchmarks might require advanced numerical reasoning skills. This observation helps explain why, despite using identical models for synthetic data generation, the text + table performance achieved by ROBERTA on MMFC after training on synthetic data is less promising compared to its performance on EVEROUS. These findings underscore the importance of future research efforts to explore methods for better aligning synthetic data with the characteristics of specific target datasets. Future endeavors could also consider the evidence retrieval stage (Frisoni et al., 2022), cross-domain FC (Kao and Yen, 2024; Domeniconi et al., 2014), and knowledge extracted from unlabeled corpora (Frisoni and Moro, 2020) to force the generation of cross-document claims.\\n\\nEthics and Impact Statement\\nAlthough fact-checking systems like U\\\\textsubscript{NOWN} enhance information integrity and combat misinformation, it is essential to ensure their responsible and beneficial use in society. U\\\\textsubscript{NOWN} aims to efficiently generate training instances, yet it is needed to rigorously validate and supervise the synthetic examples to ensure that they accurately represent real-world scenarios without introducing inadvertent biases. Moreover, high-resource language models demonstrate limited effectiveness when applied to low-resource language data (Huang et al., 2023). Similarly to various domains within NLP that depend on meticulously constructed datasets, fact-checking contributions have mainly focused on a few high-resource languages, such as English and Chinese (Zarharan et al., 2021). As this could skew perceptions of automated fact-checking advancements, future studies should prioritize advances in false claim detection for low-resource languages.\\n\\nAcknowledgements\\nThis research is partially supported by (i) the ANR project ATTENTION (ANR-21-CE23-0037), (ii) the AI-PACT project (CUP B47H22004450008 and B47H22004460001), (iii) the Complementary National Plan PNC-I.1 \\\"Research initiatives for innovative technologies and pathways in the health and welfare sector\\\" D.D. 931 of 06/06/2022, DARE\u2014DigitAl lifelong pRevEntion initiative, code PNC0000002, CUP B53C22006450001, (iv) the PNRR\u2014M4C2\u2014Investment 1.3, Extended Partnership PE00000013, FAIR\u2014Future Artificial Intelligence Research, Spoke 8 \\\"Pervasive AI,\\\" funded by the European Commission under the NextGeneration EU program, (v) the European Commission and the Italian MIMIT through the Chips JU TRISTAN project (G.A. 101095947).\\n\\nReferences\\nMubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo, Oana Cocarascu, Elena Simperl, and Andreas Vlachos. 2023. Multimodal automated fact-checking: A survey. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 5430\u20135448. Association for Computational Linguistics.\\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. FEVEROUS: fact extraction and verification over unstructured and structured information. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual.\\nWilliam Berrios, Gautam Mittal, Tristan Thrush, Douwe Kiela, and Amanpreet Singh. 2023. Towards language models that can see: Computer vision through the LENS of natural language. CoRR, abs/2306.16410.\\nJean-Flavien Bussotti, Enzo Veltri, Donatello Santoro, and Paolo Papotti. 2023. Generation of training examples within the long tail, e.g., mathematical operations, such as the premise \\\"Paul is 2 years younger than Mary.\\\" We consider using a solution in which more intricate patterns are generated as queries over relational tables (Bussotti et al., 2023).\"}"}
{"id": "emnlp-2024-main-675", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-675", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"by meta-transfer learning and pointer-generator networks.\\nWei-Yu Kao and An-Zi Yen. 2024. MAGIC: Multi-argument generation with self-refinement for domain\\ngeneralization in automatic fact-checking. In Proceedings of the 2024 Joint International Conference\\non Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 10891\u2013\\n10902, Torino, Italia. ELRA and ICCL.\\nSvetlana Kiritchenko and Saif M. Mohammad. 2017. Best-worst scaling more reliable than rating scales:\\nA case study on sentiment intensity annotation. In Proceedings of the 55th Annual Meeting of the As-\\nsociation for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 2:\\nShort Papers, pages 465\u2013470. Association for Com-\\nputational Linguistics.\\nMinwoo Lee, Seungpil Won, Juae Kim, Hwanhee Lee,\\nCheon-Eum Park, and Kyomin Jung. 2021. Crossaug:\\nA contrastive data augmentation method for debias-\\ning fact verification models. In CIKM '21: The 30th\\nACM International Conference on Information and\\nKnowledge Management, Virtual Event, Queensland,\\nAustralia, November 1 - 5, 2021, pages 3181\u20133185.\\nACM.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\\nBART: denoising sequence-to-sequence pre-training\\nfor natural language generation, translation, and com-\\nprehension. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\nACL 2020, Online, July 5-10, 2020, pages 7871\u20137880.\\nAssociation for Computational Linguistics.\\nMiaoran Li, Baolin Peng, and Zhu Zhang. 2023. Self-\\nchecker: Plug-and-play modules for fact-checking\\nwith large language models. CoRR, abs/2305.14623.\\nJiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A.\\nSmith, Yejin Choi, and Hannaneh Hajishirzi. 2023.\\nVera: A general-purpose plausibility estimation\\nmodel for commonsense statements. CoRR, abs/2305.03695.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\\nRoberta: A robustly optimized BERT pretraining\\napproach. CoRR, abs/1907.11692.\\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\\n2022. Generating training data with language mod-\\nels: Towards zero-shot language understanding. In\\nAdvances in Neural Information Processing Systems\\n35: Annual Conference on Neural Information Pro-\\ncessing Systems 2022, NeurIPS 2022, New Orleans,\\nLA, USA, November 28 - December 9, 2022.\\nYu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.\\n2019. Weakly-supervised hierarchical text classifi-\\ncation. In The Thirty-Third AAAI Conference on\\nArtificial Intelligence, AAAI 2019, The Thirty-First\\nInnovative Applications of Artificial Intelligence Con-\\nference, IAAI 2019, The Ninth AAAI Symposium on\\nEducational Advances in Artificial Intelligence, EAAI\\n2019, Honolulu, Hawaii, USA, January 27 - February\\n1, 2019, pages 6826\u20136833. AAAI Press.\\nGeorge A. Miller. 1995. Wordnet: A lexical database\\nfor english. Commun. ACM, 38(11):39\u201341.\\nGianluca Moro and Luca Ragazzi. 2022. Semantic self-\\nsegmentation for abstractive summarization of long\\ndocuments in low-resource regimes. In Thirty-Sixth\\nAAAI Conference on Artificial Intelligence, AAAI\\n2022, Thirty-Fourth Conference on Innovative Ap-\\nplications of Artificial Intelligence, IAAI 2022, The\\nTwelveth Symposium on Educational Advances in Ar-\\ntificial Intelligence, EAAI 2022 Virtual Event,\\nFebruary 22 - March 1, 2022, pages 11085\u201311093. AAAI\\nPress.\\nGianluca Moro and Luca Ragazzi. 2023. Align-then-\\nabstract representation learning for low-resource sum-\\nmarization. Neurocomputing, 548:126356.\\nGianluca Moro, Luca Ragazzi, and Lorenzo Valgimigli.\\n2023a. Carburacy: Summarization models tuning\\nand comparison in eco-sustainable regimes with a\\nnovel carbon-aware accuracy. In Thirty-Seventh\\nAAAI Conference on Artificial Intelligence, AAAI\\n2023, Thirty-Fifth Conference on Innovative Applica-\\ntions of Artificial Intelligence, IAAI 2023, Thirteenth\\nSymposium on Educational Advances in Artificial In-\\ntelligence, EAAI 2023, Washington, DC, USA, Febru-\\nary 7-14, 2023, pages 14417\u201314425. AAAI Press.\\nGianluca Moro, Luca Ragazzi, Lorenzo Valgimigli.\\n2023b. Graph-based abstractive summarization of\\nextracted essential knowledge for low-resource sce-\\nnarios. In ECAI 2023 - 26th European Conference\\non Artificial Intelligence, September 30 - October 4,\\n2023, Krak\u00f3w, Poland - Including 12th Conference\\non Prestigious Applications of Intelligent Systems\\n(PAIS 2023), volume 372 of Frontiers in Artificial In-\\ntelligence and Applications, pages 1747\u20131754. IOS\\nPress.\\nGianluca Moro, Luca Ragazzi, Lorenzo Valgimigli,\\nGiacomo Frisoni, Claudio Sartori, and Gustavo Marfia.\\n2023c. Efficient memory-enhanced transformer\\nfor long-document summarization in low-resource\\nregimes. Sensors, 23(7):3542.\\nGianluca Moro, Luca Ragazzi, Lorenzo Valgimigli,\\nand Lorenzo Molfetta. 2023d. Retrieve-and-rank end-to-\\nend summarization of biomedical studies. In Simi-\\nlarity Search and Applications - 16th International\\nConference, SISAP 2023, A Coru\u00f1a, Spain, October\\n9-11, 2023, Proceedings, volume 14289 of Lecture\\nNotes in Computer Science, pages 64\u201378. Springer.\\nGianluca Moro, Luca Ragazzi, Lorenzo Valgimigli,\\nFabian Vincenzi, and Davide Freddi. 2024. Reve-\\nlio: Interpretable long-form question answering. In\\nThe Second Tiny Papers Track at ICLR 2024, Tiny\"}"}
{"id": "emnlp-2024-main-675", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-675", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-675", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nMulti-Modal Evidence.\\n\\nWe conduct an ablation study aimed at evaluating the importance of each evidence modality for table+text FC instances (Table 7). When text or cells are excluded from the evidence in the test data, accuracy and F1 scores for the FC model drop significantly.\\n\\n|                | Test set Accuracy | F1     |\\n|----------------|-------------------|--------|\\n| STANDARD TEST DATA | 86.9              | 91.7   |\\n| ABLATED TABLES   | 57.6              | 66.0   |\\n| ABLATED SENTENCES| 62.8              | 71.5   |\\n\\nTable 7: Results on three different test sets: the gold test set, the same test set with ablated tables in evidence, and the same test set with ablated sentences in evidence.\\n\\nTraining data is always based on the warm start and the BART/BERART combination.\\n\\nEnvironment.\\n\\nWe run each experiment on a cluster of OS Linux workstations with a single Nvidia GeForce RTX3090 Turbo GPU of 24 GB VRAM. UNOWN is developed using PyTorch (Paszke et al., 2019) and the HuggingFace library (Wolf et al., 2019) (seed set to 42 for reproducibility).\\n\\nExperimental Setting.\\n\\nTo train BART, we set the following hyperparameters: learning_rate = 1e\u22124, batch_size = 16, and epochs = 20; for LLAMA-2, we use 4-bit nested quantization, r = 8, \u03b1 = 32, batch_size = 1, and epochs = 3. For inference, we adopt beam search (num_beams = 5) and nucleus sampling (top_p = 0.01, top_k = 40, temp = 0.15) for BART and LLAMA-2, respectively.\\n\\nExecution Times.\\n\\nTable 8 reports the train and inference time per claim for the claim generation task. The benefit of smaller models is evident during inference. We also report the average time required to generate an example in terms of evidence selection. The total time of about 6 seconds per claim is in contrast to the time and effort required by a human to craft a comparable example.\\n\\nExamples.\\n\\nTables 9, 10, and 11 report examples of textual claims generated by our system with different models given the same original evidence. The human-written claim is provided for comparison. We note that many claims generated by BART/BERART with Refutes labels do not contain the word \\\"never\\\". To illustrate:\\n\\n\u2022 \u201cIn the 2006-07 San Jose Sharks season, the team scored 107 goals, 183 assists, and 1...\\\"\"}"}
{"id": "emnlp-2024-main-675", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Time consumption for different tasks.\\n\\n- **Model Task**\\n  - **Claim Generation**\\n    - BART Train/Infer.: 1.92 / 0.12\\n    - BART NEG Train/Infer.: 1.01 / 0.08\\n    - LLAMA-2 Train/Infer.: 1.98 / 2.10\\n\\n- **Table-to-Text**\\n  - T5-TOO Infer.: 0.75\\n\\n- **Evidence Selection (Semantic Consistency)**\\n  - T5 Tokeniz. + Distance: 5.43\\n\\nThese examples showcase the variability of our generated claims, ensuring that the models trained on our data must learn robust patterns beyond simple negations and manage hard negative cases from a semantic viewpoint. Additionally, we acknowledge the presence of several generated claims with **Supports** labels that contain the word \u201cnever\u201d, further requiring the ability to capture diverse linguistic patterns. For instance, \u201cBruce Johnston\u2019s song \u2018I Write the Songs\u2019 never charted.\u201d\\n\\nClaim Generation Prompts.\\n\\nPrompt tuning experiments proved the marginal role of few-shot in-context learning. We then opted for a simpler and reproducible zero-shot approach, also fairer to small models, as reported in Figure 10.\\n\\n**Figure 10**: Instruction tuning prompt template for claim generation. The highlighted part is used for loss computation. Green and red colors denote alternative instructions for supporting and refuting targets, respectively.\\n\\n**Numerical Reasoning.**\\n\\nThe FORIOUS datasets contain several reasoning examples. Examining its test set for table + text, we reviewed 100 claims and identified only 7 instances requiring reasoning through cell aggregation. Consequently, we investigated how our system was able to deal with them.\\n\\nWe compared some examples of human-written claims versus **UNOWN** generated ones, using the same evidence. We showcase them in Table 12. For each example, we present the claim generated, along the intermediate text it generated from the table. In the first example, we can see that both the table to text system and the final **UNOWN** simply gave a description of the routes without making any counting. In the second example, even though it appears that our system counted the points, the truth is that this number is present in the original table. In the meantime, the human leveraged this information to create a superlative \u201cscored the most points\u201d. In the last examples, again, the T5 Verbalizer simply reports \u201c57%\u201d without trying to convert it to a more subtle information such as \u201cmore than half\u201d, as the human did. Our system even discards this information and prefers to write a claim about the number of votes.\\n\\nWe emphasize the role played by the T5-large table verbalizer in this observation. The inclusion or exclusion of operations involved in generating textual descriptions associated with the content of sampled cells is largely determined by the training dataset used. The T5 verbalizer is trained on ToTTo (Parikh et al., 2020). An analysis of the distribution of various linguistic phenomena conducted by the dataset\u2019s authors reveals that reasoning (including logical, numerical, and temporal) is present in only 21% of the instances. As a second note, even if we provided claims that require operations to verify their accuracy, we cannot expect the final predictor model to handle these operations effectively. As several experiments have demonstrated (Chang et al., 2023), even recent LLMs struggle with basic tasks like averaging. The most recent approach to address this issue is to use external modules, such as Python, to handle the mathematical computations (Yin et al., 2024).\"}"}
{"id": "emnlp-2024-main-675", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Eagle AC-7 Eagle 1 (USAF designation YE-5) is an aircraft that was manufactured by Windecker Industries. Data from Jane's All the World's Aircraft 1971.\\n\\nSupporting Claims\\n\\n**HUMAN** The Eagle AC-7 Eagle 1 is a military aircraft that was manufactured by Windecker Industries.\\n\\n**LAMA** Windecker Eagle is listed in Jane's All the World's Aircraft 1971-72.\\n\\nIn 1896, Friedrich Baume established a legal practice with A. E. Whitaker, a son of Frederick Whitaker, and won the election.\\n\\nTable 9: Example from FEVEROUS of UNOWN's supporting claims with textual evidence.\\n\\n| Player       | Points |\\n|--------------|--------|\\n| Bubba Watson | 1,498  |\\n\\n**Supporting Claims**\\n\\n**HUMAN** The player who had 1,498 points in the regular season of the 2010 FedEx Cup Playoffs attended Milton High School.\\n\\n**BART** Bubba Watson Watson played in the 2010 FedEx Cup playoffs. Watson was born and raised in Bagdad, Florida, near Pensacola. He played on the golf team at Milton High School, which had featured future PGA Tour members Heath Slocum and Boo Weekley.\\n\\nTable 10: Example from FEVEROUS of UNOWN's supporting claims, with tabular and textual evidence.\\n\\n**Evidence (d title = Stefan Lessard)**\\n\\nLessard was born in Anaheim, California, to musician Ron and Jacqueline Lessard. During the 2007 European Tour, Lessard also began a video blog on YouTube.\\n\\nIn October 2005 he married Jaclyn Burton, and together they have three daughters, Hazel Bailey (born March 6, 2004), Flora Jude (born July 13, 2006), and India (born November 2015).\\n\\n**Refuting Claims**\\n\\n**HUMAN** Stefan Kahil Lessard (born on June 4, 1974 outside Anaheim, California) completed his junior year of high school, passed the GED examinations in the summer of 1991, and was accepted into the Jazz Studies program at Virginia Commonwealth University.\\n\\n**LAMA** Stefan Lessard has been married to Jaclyn Burton since October 2005. They have three sons named Hazel Bailey (born March 6, 2004), Flora Jude (born July 13, 2006) and India (born November 2015).\\n\\n**BART** Stefan Lessard never began a video blog on YouTube.\\n\\nTable 11: Example from FEVEROUS of UNOWN's refuting claims, highlighting negation artifacts (bold) with respect to evidence excerpts (underline).\\n\\n**Example 1**\\n\\n**HUMAN** Lindfield railway station has 3 bus routes, in which the first platform services routes to Emu Plains via Central and Richmond and Hornbys via Strathfield.\\n\\n**GENERATED** Lindfield railway station is on the Northern Line (T9), a historical landmark where it has a little bit of accessibility.\\n\\n**VERBALIZED** Lindfield railway station is served by services to Emu Plains via the Central Railway Station and Richmond via the Northern Railway Station.\\n\\n**Example 2**\\n\\n**HUMAN** The 2006-07 San Jose Sharks season, the 14th season of operation (13th season of play) for the National Hockey League (NHL) franchise, scored the most points in the Pacific Division.\\n\\n**GENERATED** In the 2006-07 San Jose Sharks season, the team scored 183 goals and had a total of 46 Shutouts.\\n\\n**VERBALIZED** The Anaheim Ducks had 110 points and the San Jose Sharks had 107 points.\\n\\n**Example 3**\\n\\n**HUMAN** During the 2003 Ottawa municipal elections, more than half of the votes in the 8th Zone for the Eastern Ontario Public School Board Trustees seat went to Chantal Lecours.\\n\\n**GENERATED** In the 2003 Ottawa municipal election Denis Chartrand was elected with 760 votes.\\n\\n**VERBALIZED** Chantal Lecours received 57.84% of the vote in the 2003 Ottawa municipal election.\"}"}
{"id": "emnlp-2024-main-675", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are generated by prompting GPT-4-TURBO (gpt-4-turbo-2024-04-09) as detailed in Figure 13. The examples employed in the few-shot learning process are structured as follows:\\n\\n\u2022 input contains the question\u2013answer pair.\\n\u2022 not optimal output shows a type of answer to avoid.\\n\u2022 better output provides the reference claim.\\n\\nRefuting claims are generated with the prompt reported in Figure 14. A why field clarifies the expected negation behavior and makes explicit the difference between the not optimal output, incorrect output, and better output fields.\\n\\nWe conducted in-depth prompt engineering and manually checked the generated claims, revising them as needed to correct errors.\\n\\nCan you make a claim out of this Question/Answer pair? Your answer should only contain the claim. You should add no other information.\\n\\nHere are some examples of things not to do:\\n\\nInput: Is the religion with 16.27% of the Canadian Census of 1871 the same religion as the Church of England? No\\nNot optimal output: The religion constituting 16.27% of the Canadian Census of 1871 is not the Church of England.\\nBetter output: The religion constituting 16.27% of the Canadian Census of 1871 is a religion other than the Church of England.\\n\\nInput: Which team was Sebastian Sv\u00e4rd on in 2004-05 that played in the 2017 FA Cup final? Arsenal\\nNot optimal output: Sebastian Sv\u00e4rd was on the Arsenal team in 2004-05.\\nBetter output: Arsenal, the team Sebastian Sv\u00e4rd was on in 2004-05, played in the 2017 FA Cup final.\\n\\nSUPPORTS\\n\\nCan you make a refuted claim out of this Question/Answer pair? Your answer should only contain the claim. The claim should not be based on basic negation.\\n\\nHere are some examples of things not to do and why:\\n\\nInput: Is mobil 1 the official sponsor for the constructor that had a time/retired of electrical in the Australian Grand Prix race of 2016 F1 team? Yes\\nNot optimal output: Mobil 1 was not the official sponsor for the constructor that had a time/retired of electrical in the Australian Grand Prix race of the 2016 F1 team\\nWhy: The boolean answer should not cause a poor negation, containing a simple negation\\nBetter output: Google was the official sponsor for the constructor that had a time/retired of electrical in the Australian Grand Prix race of the 2016 F1 team.\\n\\nInput: in the Season victories of 2017 Astana season, where was the grand depart for the 2017 Race when the Location was La Planche des Belles Filles city and country?\\nD\u00fcsseldorf, Germany\\nNot optimal output: The 2017 Race grand depart from Astana season in La Planche des Belles Filles was in D\u00fcsseldorf, Germany.\\nWhy: the way the claim is refuted is too subtle\\nBetter output: The 2017 Race grand depart from Astana season was in Paris, France.\\n\\nInput: When did the home team that had a score of 20-34 in round 7 of the 2018 NRL season enter the NRL? 1988\\nIncorrect output: The home team that scored 20-34 in round 7 of the 2018 NRL season entered the NRL in 1988.\\nWhy: the generated claim is not false with regards to the question/answer pair\\nBetter output: The home team that scored 20-34 in round 7 of the 2018 NRL season entered the NRL in 1975.\\n\\nInput: For the religion that has 3,304 females in the Moscow Governorate, what is its primary literary work? Talmud\\nIncorrect output: The religion with 3,304 female adherents in the Moscow Governorate predominantly follows the Talmud as its primary literary work.\\nWhy: the generated claim is not false with regards to the question/answer pair\\nBetter output: The religion with 3,304 female adherents in the Moscow Governorate predominantly follows the Bible as its primary literary work.\\n\\nIn any case, the text you generate must be false in light of the initial question/answer pair.\\n\\nREFUTES\\n\\nCan you make a refuted claim out of this Question/Answer pair? Your answer should only contain the claim. The claim should not be based on basic negation.\\n\\nInput: in the Season victories of 2017 Astana season, where was the grand depart for the 2017 Race when the Location was La Planche des Belles Filles city and country?\\nD\u00fcsseldorf, Germany\\nNot optimal output: The 2017 Race grand depart from Astana season in La Planche des Belles Filles was in D\u00fcsseldorf, Germany.\\nWhy: the way the claim is refuted is too subtle\\nBetter output: The 2017 Race grand depart from Astana season was in Paris, France.\\n\\nInput: When did the home team that had a score of 20-34 in round 7 of the 2018 NRL season enter the NRL? 1988\\nIncorrect output: The home team that scored 20-34 in round 7 of the 2018 NRL season entered the NRL in 1988.\\nWhy: the generated claim is not false with regards to the question/answer pair\\nBetter output: The home team that scored 20-34 in round 7 of the 2018 NRL season entered the NRL in 1975.\\n\\nInput: For the religion that has 3,304 females in the Moscow Governorate, what is its primary literary work? Talmud\\nIncorrect output: The religion with 3,304 female adherents in the Moscow Governorate predominantly follows the Talmud as its primary literary work.\\nWhy: the generated claim is not false with regards to the question/answer pair\\nBetter output: The religion with 3,304 female adherents in the Moscow Governorate predominantly follows the Bible as its primary literary work.\\n\\nIn any case, the text you generate must be false in light of the initial question/answer pair.\\n\\nREFUTES\\n\\nFigure 14: Prompt for the generation of refuting claims from question\u2013answer pairs in MMFC.\"}"}
