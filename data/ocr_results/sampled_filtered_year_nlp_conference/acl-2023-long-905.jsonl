{"id": "acl-2023-long-905", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LENS: A Learnable Evaluation Metric for Text Simplification\\nMounica Maddela\u2217, Yao Dou \u2217, David Heineman, Wei Xu\\nSchool of Interactive Computing\\nGeorgia Institute of Technology\\n{mmaddela3, douy, david.heineman}@gatech.edu; wei.xu@cc.gatech.edu\\n\\nAbstract\\nTraining learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimPEVAL corpus that contains:\\n\\n- SimPEVALPAST, comprising 12K human ratings on 2.4K simplifications of 24 past systems,\\n- SimPEVAL2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text.\\n\\nTraining on SimPEVAL, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce RANK & RATE, a human evaluation framework that rates simplifications from several models in a list-wise manner using an interactive interface, which ensures both consistency and accuracy in the evaluation process and is used to create the SimPEVAL datasets.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\\n\\n1 Introduction\\nText simplification is a text-to-text generation task that aims to make a text easier to read while preserving its original meaning (Saggion, 2017). Automatic evaluation of text simplification is challenging because a sentence can be simplified in many ways, such as paraphrasing complex words, deleting insignificant information, and splitting long sentences into shorter ones. An ideal automatic metric should accommodate these diverse choices while capturing semantic similarity and fluency. However, existing metrics such as SARI (Xu et al., 2016) and BERTScore (Zhang et al., 2020) struggle to capture all the aspects and achieve a high correlation with human evaluation (Alva-Manchego et al., 2021) (see Figure 1). These metrics fail even more when evaluating high-quality systems that have close performance, calling for a more robust and accurate metric for text simplification.\\n\\nPrior work on machine translation (Rei et al., 2020; Sellam et al., 2020) has seen the success of using language models as an automatic metric by training them on human judgments, such as Direct Assessment (DA) that rates generations on a 0-100 scale. However, existing human evaluation datasets (Alva-Manchego et al., 2021; Sulem et al., 2018) for text simplification are not suitable for training metrics because they include a limited number of annotations or systems. Besides, these datasets do not include state-of-the-art generation models such as GPT-3.5.\"}"}
{"id": "acl-2023-long-905", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we introduce LENS, a learnable evaluation metric for text simplification. LENS is the first supervised metric for the task and uses an adaptive ranking loss to promote fair comparison of simplifications that have undergone different edits (i.e., splitting, paraphrasing, and deletion).\\n\\nTo train LENS, we collect 12K human judgments on 2.4K simplifications by 24 simplification systems from the literature, which we name as the SIMPEVALPAST dataset. We also create SIMPEVAL2022 to evaluate LENS and other metrics in a realistic and challenging setting of assessing simplifications by state-of-the-art language models on the more recent, complex, and longer sentences published on Wikipedia after Oct 22nd, 2022. SIMPEVAL2022 contains over 1K human ratings on 360 simplifications generated by 4 SOTA models, including GPT-3.5.\\n\\nEmpirical experiments show that LENS achieves a higher correlation of 0.331 with human ratings on SIMPEVAL2022, which is more than twice as high as the correlation scores of 0.112 and 0.149 by BERTScore and SARI, respectively. We further demonstrate that incorporating LENS into decoding process, using minimum Bayes risk framework (Fernandes et al., 2022), can directly improve the automatic text simplification system's performance.\\n\\nWe expect that our data collection method, including RANK & RATE, a list-wise human evaluation interface, can be easily adapted to other text generation tasks.\"}"}
{"id": "acl-2023-long-905", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"3 Automatic Evaluation Metric\\n\\nTo tackle the challenge of limited human evaluation data, we curate SIMPEVAL, a corpus containing over 13K human judgements on 2.8K simplification from 26 systems. This facilitates the training and evaluation of LENS (ALearnable Evaluation Metric for Text Simplification), the first supervised automatic metric for text simplification evaluation.\\n\\nIn this section, we first describe the creation of SIMPEVAL datasets in \u00a73.1 and then LENS in \u00a73.2.\\n\\n### 3.1 Collecting Human Judgements\\n\\nWe collect SIMPEVALPAST, containing 12K human ratings on 2.4K simplifications from 24 systems on sentences from TurkCorpus (Xu et al., 2016), to train LENS. For evaluating LENS and other simplification metrics, we create SIMPEVAL2022 that consists of 1,080 human ratings on 360 simplifications from both humans and SOTA models, including GPT-3.5. It features more complex sentences from Wikipedia written after Oct 22nd, 2022, very recent to the time we conduct the experiments, to reduce the risk of \\\"data contamination\\\" (i.e., appearing in the training data of LLMs) and serve as a more challenging test bed for large language models. Table 1 shows the summary of both datasets.\\n\\n#### A Diverse Set of Simplification Systems.\\n\\nWe consider the following systems (further details in Appendix C):\\n\\n- (i) two GPT-3.5 outputs under zero-shot and 5-shot settings;\\n- (ii) eight fine-tuned Transformer-based systems of varied sizes and parameter settings (Sheang and Saggion, 2021; Raffel et al., 2020; Martin et al., 2020; Maddela et al., 2021);\\n- (iii) three supervised BiLSTM-based systems that use vanilla RNN, reinforcement learning (Zhang and Lapata, 2017), or explicit editing (Dong et al., 2019);\\n- (iv) one unsupervised and one semi-supervised system utilizing auto-encoders (Surya et al., 2019);\\n- (v) two systems that apply statistical machine translation approaches to simplification (Wubben et al., 2012; Xu et al., 2016);\\n- (vi) a rule-based system (Kumar et al., 2020);\\n- (vii) three hybrid systems that combine linguistic rules with data-driven methods (Narayan and Gardent, 2014; Sulem, 2018; Maddela et al., 2021);\\n- (viii) two naive baselines that copy the input or scramble 5% of the input words; and\\n- (ix) six human-written simplifications, including two from ASSET (Alva-Manchego et al., 2020).\\n\\n| System Arch. Data Human Avg. | Raw Z-Score |\\n|-----------------------------|-------------|\\n| SIMPEVALPAST (24 systems on 100 original sentences) |             |\\n| Human-1 (2020) Human ASSET | 86.69        |\\n| 0.783                      |             |\\n| Human-2 (2020) Human ASSET | 86.12        |\\n| 0.711                      |             |\\n| MUSS (2022) BART-largeWikiLarge + Mined Data | 84.48 |\\n| 0.653                      |             |\\n| ControlT5 (2021) T5-base WikiLarge | 84.70 |\\n| 0.650                      |             |\\n| T5-3B (2020) T5 WikiAuto | 82.79        |\\n| 0.492                      |             |\\n| T5-large T5 WikiAuto | 81.86        |\\n| 0.453                      |             |\\n| T5-base T5 WikiAuto | 82.15        |\\n| 0.443                      |             |\\n| Transformer (2017) BERT-TF WikiAuto | 79.42 |\\n| 0.366                      |             |\\n| Controllable (2021) BERT-TF WikiAuto | 79.44 |\\n| 0.323                      |             |\\n| Human-3 (2016) Human TurkCorpus | 79.54 |\\n| 0.281                      |             |\\n| Human-4 Human Simple Wiki | 78.36        |\\n| 0.249                      |             |\\n| DRESS (2017) BiLSTM+RL WikiLarge | 77.18 |\\n| 0.206                      |             |\\n| Copy \u2013 \u2013 | 76.81        |\\n| 0.103                      |             |\\n| ACCESS (2020) Transformer WikiLarge | 73.25 |\\n| 0.001                      |             |\\n| SBMT-SARI (2016) Statistic MT PWKP | 73.66 |\\n| -0.014                     |             |\\n| PBMT-R (2012) Statistic MT PWKP | 72.44 |\\n| -0.066                     |             |\\n| EditNTS (2019) BiLSTM WikiLarge | 70.15 |\\n| -0.162                     |             |\\n| BiLSTM BiLSTM WikiLarge | 68.35        |\\n| -0.245                     |             |\\n| SEMosses (2018) LSTM WikiLarge | 62.84 |\\n| -0.565                     |             |\\n| UNTS (2019) RNN Wiki dump | 62.66        |\\n| -0.596                     |             |\\n| UNMT (2018) RNN Wiki dump | 60.43        |\\n| -0.673                     |             |\\n| Rule-based (2020) \u2013 \u2013 | 60.15        |\\n| -0.687                     |             |\\n| Hybrid (2014) Statistic MT PWKP | 55.36 |\\n| -0.925                     |             |\\n| Scramble \u2013 \u2013 | 35.17        |\\n| -1.954                     |             |\\n\\n### Table 1: SIMPEVALPAST and SIMPEVAL2022 datasets of human evaluation data that covers a wide range of simplification systems. MUSS is the best-performing model in SIMPEVALPAST with a small gap to humans but is much worse on the more challenging sentences in SIMPEVAL2022 where GPT-3.5 performs better. BERT-TF: BERT-base initialized Transformer. Z-scores (Graham et al., 2013; Akhbardeh et al., 2021) are standardized based on each rater's mean and standard deviation.\\n\\nGPT is pre-trained on a vast amount of web text, and\"}"}
{"id": "acl-2023-long-905", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the sentences in TurkCorpus/ASSET are derived from the 12-year-old Parallel Wikipedia Simplification corpus (Zhu et al., 2010), which may have already been encountered by GPT-3.5. To address this \u201cdata contamination\u201d issue and provide a more challenging benchmark for state-of-the-art models, we manually select 60 long, complex sentences covering recent world events such as the Twitter acquisition and World Cup for SIMPEVAL2022.\\n\\nThese sentences are from new revisions or articles added to Wikipedia between October 22nd and November 24th, 2022. They have an average length of 38.5 tokens, much longer than the 19.7-token average of TurkCorpus and ASSET.\\n\\nData Annotation.\\nWe ask in-house annotators to rate each simplification on a single 0-100 overall quality scale using our RANK & RATE framework (more details in \u00a76). We provided an extensive tutorial and two training sessions to the annotators, which involved rating system outputs for five input sentences (screenshots in Appendix F). We periodically inspected the annotations to prevent the deterioration of quality over time. All annotators are English speakers who are university students. Each simplification in SIMPEVALPAST receives 5 ratings, while each simplification in SIMPEVAL2022 is rated by 3 annotators. We follow WMT (Akhbardeh et al., 2021) to normalize the raw scores by the mean and standard deviation of each rater, also known as the z-score, which are later used to train LENS. The inter-annotator agreement for SIMPEVALPAST is 0.70 using the interval Krippendorff\u2019s \u03b1 on z-scores, and 0.32 for SIMPEVAL2022, partly because it contains GPT-3.5 outputs that are quite competitive with human. Both are considered fair to good agreement (Krippendorff, 2004).\\n\\n3.2 A New Learnable Metric \u2013 LENS\\nGiven an input text c, the corresponding system output s, and a set of n references \\\\( R = \\\\{ r_1, r_2, ..., r_n \\\\} \\\\), LENS produces a real-valued score \\\\( z_{\\\\text{max}} = \\\\max_{1 \\\\leq i \\\\leq n} (z_i) \\\\) that maximizes over the quality scores \\\\( z_i \\\\) of \\\\( s \\\\) in regards to each reference \\\\( r_i \\\\). Our model encodes all texts into vectors \\\\( (c, s, r_i) \\\\) using Transformer-based encoders such as RoBERTa (Liu et al., 2019), then combines them into an intermediate representation \\\\( H = [s; r_i; s \\\\odot c; s \\\\odot r_i; |s - c|; |s - r_i|] \\\\). For example, \u201cMusk stated that Twitter Blue\u2019s pricing would be raised to around US$8.00 per-month, and include reduced advertising on the Twitter service, the ability to post longer audio and video files, and verified account status.\u201d by concatenation \\\\([; ;]\\\\) and element-wise product \\\\( \\\\odot \\\\), which is then fed to a feedforward network to predict \\\\( z_i \\\\).\\n\\nFor training, besides considering all references equally (i.e., LENSall when \\\\( k = n \\\\) in Eq. (1)), we also adopt a reference-adaptive loss that selects a subset of references closer to \\\\( s \\\\) in terms of edit operations rather than the entire set \\\\( R \\\\). It encourages the metric to consider that different simplifications (e.g., paraphrasing-focused, deletion-focused, with or without splitting) can be acceptable, as long as they are close to some (not necessarily all) of the human references. We compute this loss \\\\( L_{\\\\text{adapt}} \\\\) as:\\n\\n\\\\[\\nL_{\\\\text{adapt}} = \\\\frac{1}{km} \\\\sum_{j=1}^{m} \\\\sum_{z_l \\\\in Z'} (h_j - z_l)^2\\n\\\\]\\n\\nwhere \\\\( h \\\\) is human rating and \\\\( m \\\\) is the training data size. We compute the set of predicted scores \\\\( Z = \\\\{ z_1, z_i, ..., z_n \\\\} \\\\) corresponding to references in \\\\( R \\\\) and then choose top \\\\( k \\\\) (\\\\( k \\\\leq n \\\\)) values from \\\\( Z \\\\) to form a subset \\\\( Z' \\\\subseteq Z \\\\). Finally, we calculate the mean squared error (MSE) between the human rating \\\\( h \\\\) and each score in \\\\( Z' \\\\). By selecting top \\\\( k \\\\) scores in \\\\( Z \\\\), we focus the training of metric on references similar to \\\\( s \\\\) in terms of writing style or editing operations. This loss also aligns the training step with the inference step, where we select the best-predicted score \\\\( z_{\\\\text{max}} \\\\) corresponding to \\\\( R \\\\). Although multiple references are ideal, the proposed loss can also use a single reference, similar to the standard MSE loss. We train LENS on our SIMPEVALPAST dataset (details in \u00a73.1). We provide further implementation details in Appendix A.\\n\\nSimilar to the COMET (Rei et al., 2020) and BLEURT (Sellam et al., 2020) metrics for MT evaluation, LENS is trained on human ratings after z-score normalization, which are real values predominantly lie between \\\\([-3, 3]\\\\). To make LENS more interpretable, we rescale the predicted scores to the range between \\\\([0, 100]\\\\) as the percentage of area under a normal curve of mean 0 and standard deviation 1 corresponding to \\\\( z_i \\\\). We present the rescaled scores for experiments and analyses in this paper. Besides, we use RoBERTa-large as the underlying model of LENS throughout the paper, unless otherwise specified (\u00a75).\\n\\n4 Experiments\\nWe benchmark LENS metric against the existing automatic metrics (i) for evaluating text simplification system outputs and (ii) for training better\"}"}
{"id": "acl-2023-long-905", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Correlation results between automatic metrics and three human ratings datasets: SIMPEVAL 2022 (this work), WIKI-DA (Alva-Manchego et al., 2021), and NEWSELA-LIKERT (Maddela et al., 2021).\\n\\n\u03c4para, \u03c4spl, and \u03c4all represent the Kendall Tau-like correlation for paraphrase-focused, split-focused, and all simplifications, respectively.\\n\\nWe report the Pearson correlation coefficients along three dimensions for WIKI-DA and NEWSELA-LIKERT. The best values are marked in bold and the second best values are underlined.\\n\\nFigure 3: The 95% confidence intervals for Kendall Tau-like correlation (\u03c4all) on SIMPEVAL 2022 and for Pearson correlation with simplicity ratings on WIKI-DA and NEWSELA-LIKERT, calculated by bootstrapping (Deutsch et al., 2021). LENS is more reliable with smaller intervals and has higher correlation with human judgments.\\n\\nWe demonstrate that LENS correlates better with human judgments than the existing metrics.\\n\\nEvaluation Datasets. We compare LENS to the existing metrics, namely SARI (Xu et al., 2016), BERTScore (Zhang et al., 2020), BLEU, and Flesch-Kincaid Grade Level readability (FKGL) (Kincaid, 1975) on three datasets: SIMPEVAL 2022 (\u00a73.1), WIKI-DA released by Alva-Manchego et al. (2021) with 0-100 continuous scale ratings on fluency, meaning preservation, and simplicity for 600 simplifications across six systems, and NEWSELA-LIKERT collected by Maddela et al. (2021) with 5-point Likert scale ratings on the same dimensions for 500 simplifications across five systems. While SIMPEVAL 2022 and WIKI-DA are derived from Wikipedia, NEWSELA-LIKERT is derived from news articles in Newsela (Xu et al., 2015), a widely used corpus for text simplification. When calculating metric scores for SIMPEVAL 2022 that contains two human simplifications, we use one as the reference and the other as the oracle simplification system. We remove the complex sentences in WIKI-DA that overlap with SIMPEVAL PAST, the training dataset for LENS.\\n\\nEvaluation Setup. We report Kendall Tau-like correlation for SIMPEVAL 2022 to capture the ability of metrics to distinguish two systems, which is close to the real-world scenario. Kendall Tau-like correlation is predominantly used in machine translation metric evaluation at WMT (Bojar et al., 2017; Ma et al., 2018) for the same reason as it focuses on the relative ranking of the outputs. Given an input c and its simplifications from N systems $S = \\\\{s_1, \\\\ldots, s_m, \\\\ldots, s_n, \\\\ldots, s_N\\\\}$, we extract $(s_m, s_n)$ pairs, where $1 \\\\leq m < n \\\\leq N$, and calculate Kendall Tau-like coefficient $\\\\tau$:\\n\\n$$\\\\tau = \\\\frac{|\\\\text{Concordant}| - |\\\\text{Discordant}|}{|\\\\text{Concordant}| + |\\\\text{Discordant}|}$$\\n\\nwhere Concordant is the set of pairs where the metric ranked $(s_m, s_n)$ in the same order as humans ranked and Discordant is the set of the pairs where the metric and humans disagreed. We report separate coefficients for paraphrase- ($\\\\tau_{para}$), split-focused ($\\\\tau_{spl}$), and all simplifications ($\\\\tau_{all}$).\"}"}
{"id": "acl-2023-long-905", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and split-focused ($\\\\tau_{spl}$) simplifications, along with ($\\\\tau_{all}$) for all of them. Following the literature, we only used the ($s_m, s_n$) pairs for which the difference in ratings is more than 5 out of the 100 points, and all the annotators agreed on the ranking order. To make results comparable to existing work (Alva-Manchego et al., 2021, 2020) that evaluated on WIKI-DA and NEWSELA-LIKERT, we report Pearson correlation ($\\\\rho$) between the metric scores and the human ratings.\\n\\nResults. Table 2 shows that LENS outperforms the existing metrics on SIMEVAL2022 and WIKI-DA belonging to the Wikipedia domain, and NEWSELA-LIKERT based on newswire domain. The difference is more substantial on SIMEVAL2022 consisting of similar performing SOTA systems, where the $\\\\tau_{all}$ of LENS$_k=3$ exceeds the $\\\\tau_{all}$ of BERTScore by 0.22 points. Training using top $k$ references (LENS$_k=3$) has improved $\\\\tau_{all}$ on SIMEVAL2022 and $\\\\rho$ along the simplicity dimension on the rest when compared to using all the references (LENS$_{all}$). Figure 3 shows the 95% confidence intervals for $\\\\tau_{all}$ on SIMEVAL2022 and $\\\\rho$ for simplicity, which is deemed to be the most important dimension in prior work (Alva-Manchego et al., 2021; Xu et al., 2016), on WIKI-DA and NEWSELA-LIKERT calculated using bootstrapping methods by Deutsch et al. (2021). A smaller interval indicates that the metric exhibits lower variance and higher reliability. LENS exhibits smaller intervals than the other metrics.\\n\\n4.2 LENS as Training or Decoding Objectives\\n\\nWe also incorporate LENS into training as an alternative reward function in minimum risk training framework (Kumar and Byrne, 2004; Smith and Eisner, 2006) and into decoding as a reranking objective in minimum Bayes risk decoding (Fernandes et al., 2022; Freitag et al., 2022) to improve the quality of generated simplifications.\\n\\nMinimum Risk Training (MRT). Given the input $c$ and reference $r$, we generate a set of candidates $S$ for each training step and calculate the expected risk ($L_{\\\\text{risk}}$) as follows:\\n\\n$$L_{\\\\text{risk}} = \\\\sum_{s \\\\in S} \\\\text{cost}(c, r, s) \\\\cdot P(s | c) \\\\sum_{s' \\\\in S} P(s' | c).$$\\n\\n(3)\\n\\n$$P(s | c) = T \\\\prod_{t=1}^{T} P(s_t | c, s_{<t}; \\\\theta).$$\\n\\n$$\\\\text{cost}(c, r, s) = 1 - \\\\text{Metric}(c, r, s).$$\\n\\nWe choose $\\\\gamma = 0.3$ and $|S| = 10$ for our experiments.\\n\\nMinimum Bayes Risk Decoding (MBR). We adopt the MBR framework proposed by Fernandes et al. (2022), where we first generate a set of candidates $S$ for input $c$ during inference then rerank them by comparing each candidate $s \\\\in S$ to all the other candidates in the set:\\n\\n$$\\\\hat{u}_{\\\\text{MBR}} = \\\\arg \\\\max_{s \\\\in S} 1/|S| \\\\sum_{s' \\\\in S} \\\\text{Metric}(c, s, s').$$\\n\\n(5)\\n\\nFor our experiments, we generate the candidates using beam search with beam size $|S|$.\\n\\nExperiment Setup. We fine-tune a T5 model that prepends control tokens to the input (Sheang and Saggion, 2021) to control various aspects of the generated simplification and has shown state-of-the-art performance for the task. We use WIKI-AUTO (Jiang et al., 2020) for training, ASSET and split-focused ($\\\\tau_{spl}$) simplifications, along with ($\\\\tau_{all}$) for all of them. Following the literature, we only used the ($s_m, s_n$) pairs for which the difference in ratings is more than 5 out of the 100 points, and all the annotators agreed on the ranking order. To make results comparable to existing work (Alva-Manchego et al., 2021, 2020) that evaluated on WIKI-DA and NEWSELA-LIKERT, we report Pearson correlation ($\\\\rho$) between the metric scores and the human ratings.\"}"}
{"id": "acl-2023-long-905", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Automatic evaluation results for minimum Bayes risk decoding (MBR) with different model sizes on SIMEVAL2022. For standard MLE, we use beam search with beam sizes of 10 and 100.\\n\\nTable 5: Human evaluation results for the T5-11B and close-sourced LLMs on SIMEVAL2022. T5-11B with MBR-LENS decoding achieves the state-of-the-art open-source model performance, on par with GPT-3.5.\\n\\nGiven the success of using LENS as the utility function of MBR decoding on T5-base, we further apply it to larger models, including T5-3B and T5-11B. As displayed in Table 4, MBR-LENS with 100 candidates improves over standard beam search by an increase of over 11 points of LENS score across all model sizes. Although MBR may inflate the results of the utility function it uses (Fernandes et al., 2022), our human evaluations solidify the assertion that T5-11B with MBR-LENS decoding exceeds standard beam search, thereby establishing state-of-the-art (SOTA) performance among open-source models. When compared to close-source large language models, T5-11B with MBR-LENS achieves on-par performance with GPT-3.5.\\n\\nTable 6: Kendall Tau-like correlation ($\\\\tau_{para}$, $\\\\tau_{spl}$, and $\\\\tau_{all}$) of LENS metric, when based on different encoder models, with human ratings in SIMEVAL2022.\\n\\nFigure 4: Pearson correlation of LENS on WIKIADATASET using a varied number of references. They are very comparable: MRT-LENS generates 28% better, 28% worse, and 44% equal quality simplifications compared to MRT-SARI. Additionally, BERTScore and SARI show a decrease in quality when used in decoding than standard maximum likelihood estimation (MLE). It is noteworthy that we use a beam size of 10 for MLE rather than a large search space of beam size 100 because generation quality degrades with increased beam size as shown in Table 4 as well as in existing literature (Stahlberg and Byrne, 2019; Meister et al., 2020).\"}"}
{"id": "acl-2023-long-905", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: RANK & RATE framework consists of three steps: (1) classifying the generations, (2) annotating the edits performed by the system, and (3) rating and ranking the generations. For the first two steps, the annotators verify the automatically extracted categories and edits instead of annotating from scratch.\\n\\n5 LENS Analysis\\n\\nIn this section, we delve into the impact of the underlying model architecture and the number of references on the performance of LENS.\\n\\nModel Architecture.\\n\\nTable 6 shows the Kendall Tau-like correlation ($\\\\tau_{\\\\text{para}}$, $\\\\tau_{\\\\text{spl}}$, and $\\\\tau_{\\\\text{all}}$) of LENS metric trained on various encoders on SIM-P-EVAL 2022. LENS metrics trained on RoBERTa encoders (Liu et al., 2019) perform better than their respective LENS metrics trained on the BERT encoders (Devlin et al., 2019). Among all models, LENS trained on RoBERTa-large achieves the highest overall correlation. It has a substantial improvement in $\\\\tau_{\\\\text{spl}}$ with a trade-off in $\\\\tau_{\\\\text{para}}$, in comparison to RoBERTa-base.\\n\\nNumber of References.\\n\\nFigure 4 shows the Pearson correlation of LENS metric on WIKI-DA for a varied number of references used during inference. Although the metric performs the best with 10 references, we see only a slight drop with one reference, demonstrating that LENS is capable of evaluating with single or multiple references.\\n\\n6 RANK & RATE Framework\\n\\nTo facilitate consistent evaluation and enable the training of LENS, we develop RANK & RATE, a human evaluation framework to assist annotators in efficiently comparing and rating many (>20) system outputs at once in a list-wise ranking manner.\\n\\n6.1 Methodology\\n\\nWe describe the three-step annotation methodology for RANK & RATE (Figure 5) as follows:\\n\\nStep 1 - Categorizing System Outputs. As there are different acceptable ways to simplify the input text, we first display the system outputs in groups as split-, deletion-, or paraphrase-focused based on the following criteria: (i) outputs with multiple sentences are split-focused, (ii) outputs with a compression ratio less than 0.5 or generated by deleting words from the original sentence are deletion-focused, (iii) the rest are paraphrase-focused. Annotators can adjust the initial automatic categorization during the annotation process.\\n\\nStep 2 - Highlighting Edits. To help annotators notice the changes made by each system and subsequently improve the accuracy of their ratings, we use a state-of-the-art word alignment model by Lan et al. (2021) to highlight the edits performed by the system and classify them into three types: (i) Deletion-edits are phrases or words with no match in the modified output, (ii) Paraphrase-edits are new phrases or words added or modified, and (iii) Split-edits are any added periods (\\\\text{.}). Deletion-edits are marked with a red caret (\\\\text{\u2227}), paraphrase-edits with bolded text, and split-edits with two vertical bars (\\\\text{||}) (see Figure 5). We ask annotators to correct the misclassified edits using an interactive pop-up window (see Appendix F).\\n\\nStep 3 - Ranking and Rating System Outputs. Following the machine translation evaluation at WMT (Ma et al., 2018, 2019; Barrault et al., 2020; Akhbardeh et al., 2021), we ask annotators to rate the quality of system outputs on a 0-100 continuous scale instead of the 5-point Likert scale because the former was shown to provide higher levels of inter-annotator consistency (Novikova et al., 2018) and...\"}"}
{"id": "acl-2023-long-905", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Model and human simplification quality under different human evaluation methods. Following Sulem et al. (2018), SIMPLIKERT 2022 uses a 1 to 5 scale for fluency and adequacy, and -2 to 2 for simplicity. SIMPDA 2022 rates on a continuous 0-100 scale. All three methods show similar rankings of systems.\\n\\n6.2 Human Evaluation Comparison\\n\\nWe compare RANK & RATE with the existing human evaluation methods: 5-point Likert (Sulem et al., 2018) and Direct Assessment with a continuous scale of 0 to 100 (Alva-Manchego et al., 2021), which were both conducted on three dimensions: fluency, adequacy, and simplicity. For a fair comparison, we annotate the same set of simplifications using each method, resulting in SIMPLIKERT 2022 and SIMPDA 2022. Table 7 shows similar rankings of the systems by the three methods with very slight differences. We also calculate the inter-annotator agreement using Krippendorff\u2019s $\\\\alpha$ (Krippendorff, 2011). RANK & RATE achieves an $\\\\alpha$ of 0.32, which is higher than the 0.23 $\\\\alpha$ by Likert and 0.25 $\\\\alpha$ by DA. All values are considered fair (Krippendorff, 2004).\\n\\n7 Other Related Work\\n\\nText-to-text Generation Metrics.\\n\\nThere is currently no single automatic metric to evaluate all the text-to-text generation tasks that revise text. SARI (Xu et al., 2016) is the main metric for text simplification and has been used by other generation tasks such as sentence splitting and fusion (Rothe et al., 2020; Kim et al., 2021), decontextualization (Choi et al., 2021), and scientific rewriting (Du et al., 2022). Style transfer tasks (Hu et al., 2017; Rao and Tetreault, 2018; Prabhumoye et al., 2018; Krishna et al., 2020; Xu et al., 2012; Ma et al., 2020) use different automatic metrics to measure each aspect of text: (i) text similarity metrics such as BLEU, METEOR (Lavie and Agarwal, 2007), and BERTScore to measure content preservation, (ii) text classification models (Sennrich et al., 2016; Luo et al., 2019; Krishna et al., 2022b) or embedding-based edit distance metrics such as MoverScore (Zhao et al., 2019; Mir et al., 2019) to evaluate target style, and (iii) perplexity or pseudo log likelihood (Salazar et al., 2020) to measure fluency.\\n\\nIncorporating Evaluation Metrics into Training and Inference.\\n\\nPrior studies have improved MLE-trained generation systems with alternative training approaches that integrate evaluation metrics based on reinforcement learning (Ranzato et al., 2015; Li et al., 2016; Gong et al., 2019), minimum risk (Smith and Eisner, 2006; Shen et al., 2016), and ranking (Hopkins and May, 2011; Xu et al., 2016; Krishna et al., 2022a). Incorporating metrics into decoding has also been explored by reranking the generations using discriminative rankers (Shen et al., 2004; Lee et al., 2021), energy-based rankers (Bhattacharyya et al., 2021), and minimum risk (Kumar and Byrne, 2004; Freitag et al., 2022). We use minimum risk as it has been shown to help machine translation systems (Wieting et al., 2019; Fernandes et al., 2022; Amrhein and Sennrich, 2022).\\n\\n8 Conclusion\\n\\nWe introduce LENS, the first supervised automatic metric for text simplification. We show that LENS exhibits higher human correlation than other automatic metrics. We also introduce RANK & RATE framework, which allows annotators to evaluate multiple systems\u2019 outputs at once in a list-wise manner. Using it, we create SIMPEVAL 2022 to train LENS, and SIMPEVAL 2022 as a new metric evaluation benchmark for text simplification. We hope our metric, data, and framework will facilitate future research in text simplification evaluation.\"}"}
{"id": "acl-2023-long-905", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we show that LENS shows better human correlation than other metrics on Wikipedia and news domains. Future research can further experiment and extend LENS to other domains, such as medical and children's books, as the preference for different simplification operations can vary depending on the domain and user. Additionally, our work focuses on sentence-level simplification, and future work can extend LENS to evaluating paragraph- and document-level simplification.\\n\\nThe SIMPEVAL dataset and LENS are also limited to the English language.\\n\\nEthics Statement\\nWe used in-house annotators to collect human ratings in SIMPEVAL datasets and write simplifications in SIMPEVAL. The annotators are university-level undergraduate and graduate students, including both native and non-native speakers of English. We did not collect any personal information from the annotators. We paid each annotator $15 per hour, which is above the US federal minimum wage. We ensured that the content shown to the annotators was not upsetting and let them know that they could skip the task if they felt uncomfortable at any point. We also let the annotators know the purpose of the collected data. The original complex sentences in the SIMPEVAL datasets are from the publicly available Wikipedia. The simplifications are either from the existing work or human simplifications collected from our annotators. We used the author-released simplification outputs if they are available.\\n\\nFor T5 (base, large, and 3B) systems, we trained our own simplification models using open-sourced code from the Hugging Face Transformers library.\\n\\nAcknowledgments\\nWe thank Nghia T. Le, Tarek Naous, Yang Chen, and Chao Jiang as well as three anonymous reviewers for their helpful feedback on this work. We also thank Marcus Ma, Rachel Choi, Vishnesh J. Ramanathan, Elizabeth Liu, Alex Soong, Govind Ramesh, Ayush Panda, Anton Lavrouk, Vinayak Athavale, and Kelly Smith for their help with human evaluation. This research is supported in part by the NSF awards IIS-2144493 and IIS-2112633, ODNI and IARPA via the BETTER program (contract 2019-19051600004) and the HIATUS program (contract 2022-22072200004). The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of NSF, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.\\n\\nReferences\\nFarhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ond\u0159ej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-Juss\u00e0, Cristina Espa\u00f1a-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Augustine Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, pages 1\u201388, Online. Association for Computational Linguistics.\\n\\nFernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Beno\u00eet Sagot, and Lucia Specia. 2020. ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations. In Proceedings of the Association for Computational Linguistics.\\n\\nFernando Alva-Manchego, Carolina Scarton, and Lucia Specia. 2021. The (un)suitability of automatic evaluation metrics for text simplification. Computational Linguistics, 47(4):861\u2013889.\\n\\nChantal Amrhein and Rico Sennrich. 2022. Identifying weaknesses in machine translation metrics through minimum Bayes risk decoding: A case study for COMET. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).\\n\\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. 2018. Unsupervised neural machine translation. ArXiv, abs/1710.11041.\"}"}
{"id": "acl-2023-long-905", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-905", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sentence alignment in text simplification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943\u20137960, Online. Association for Computational Linguistics.\\n\\nDavid Kauchak. 2013. Improving text simplification language modeling using unsimplified text data. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1537\u20131546, Sofia, Bulgaria. Association for Computational Linguistics.\\n\\nJoongwon Kim, Mounica Maddela, Reno Kriz, Wei Xu, and Chris Callison-Burch. 2021. BiSECT: Learning to split and rephrase sentences with bitexts. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6193\u20136209, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nKincaid. 1975. Derivation of new readability formulas (automated readability index, fog count and Flesch reading ease formula) for navy enlisted personnel. Research Branch Report.\\n\\nKlaus Krippendorff. 2004. Reliability in content analysis: Some common misconceptions and recommendations. Human communication research, 30(3):411\u2013433.\\n\\nKlaus Krippendorff. 2011. Computing Krippendorff\u2019s alpha-reliability.\\n\\nKalpesh Krishna, Yapei Chang, John Wieting, and Mohit Iyyer. 2022a. Rankgen: Improving text generation with large ranking models.\\n\\nKalpesh Krishna, Deepak Nathani, Xavier Garcia, Bidisha Samanta, and Partha Talukdar. 2022b. Few-shot controllable style transfer for low-resource multilingual settings. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7439\u20137468, Dublin, Ireland. Association for Computational Linguistics.\\n\\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020. Reformulating unsupervised style transfer as paraphrase generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 737\u2013762, Online. Association for Computational Linguistics.\\n\\nDhruv Kumar, Lili Mou, Lukasz Golab, and Vechtomova Olga. 2020. Iterative edit-based unsupervised sentence simplification. In Proceedings of the Association for Computational Linguistics.\\n\\nShankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 169\u2013176, Boston, Massachusetts, USA. Association for Computational Linguistics.\\n\\nWuwei Lan, Chao Jiang, and Wei Xu. 2021. Neural semi-markov CRF for monolingual word alignment.\\n\\nAlon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228\u2013231, Prague, Czech Republic. Association for Computational Linguistics.\\n\\nAnn Lee, Michael Auli, and Marc\u2019Aurelio Ranzato. 2021. Discriminative reranking for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7250\u20137264, Online. Association for Computational Linguistics.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\\n\\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. 2016. Deep reinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1192\u20131202, Austin, Texas. Association for Computational Linguistics.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandal Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. ArXiv, abs/1907.11692.\\n\\nIlya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. ArXiv preprint arXiv:1711.05101.\\n\\nFuli Luo, Peng Li, Pengcheng Yang, Jie Zhou, Yutong Tan, Baobao Chang, Zhifang Sui, and Xu Sun. 2019. Towards fine-grained text sentiment transfer. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013\u20132022, Florence, Italy. Association for Computational Linguistics.\\n\\nQingsong Ma, Ond\u0159ej Bojar, and Yvette Graham. 2018. Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 671\u2013688, Belgium, Brussels. Association for Computational Linguistics.\\n\\nQingsong Ma, Johnny Wei, Ond\u0159ej Bojar, and Yvette Graham. 2019. Results of the WMT19 metrics.\"}"}
{"id": "acl-2023-long-905", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"shared task: Segment-level and strong MT systems pose big challenges. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 62\u201390, Florence, Italy. Association for Computational Linguistics. \\n\\nXinyao Ma, Maarten Sap, Hannah Rashkin, and Yejin Choi. 2020. PowerTransformer: Unsupervised controllable revision for biased language correction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7426\u20137441, Online. Association for Computational Linguistics.\\n\\nMounica Maddela, Fernando Alva-Manchego, and Wei Xu. 2021. Controllable text simplification with explicit paraphrasing. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3536\u20133553, Online. Association for Computational Linguistics.\\n\\nLouis Martin, \u00c9ric de la Clergerie, Beno\u00eet Sagot, and Antoine Bordes. 2020. Controllable sentence simplification. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4689\u20134698, Marseille, France. European Language Resources Association.\\n\\nLouis Martin, Angela Fan, \u00c9ric de la Clergerie, Antoine Bordes, and Beno\u00eet Sagot. 2022. MUSS: Multilingual unsupervised sentence simplification by mining paraphrases. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1651\u20131664, Marseille, France. European Language Resources Association.\\n\\nClara Meister, Ryan Cotterell, and Tim Vieira. 2020. If beam search is the answer, what was the question? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2173\u20132185, Online. Association for Computational Linguistics.\\n\\nRemi Mir, Bjarke Felbo, Nick Obradovich, and Iyad Rahwan. 2019. Evaluating style transfer for text. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 495\u2013504, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nShashi Narayan and Claire Gardent. 2014. Hybrid simplification using deep semantics and machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 435\u2013445, Baltimore, Maryland. Association for Computational Linguistics.\\n\\nJekaterina Novikova, Ond\u02c7rej Du\u0161ek, and Verena Rieser. 2018. RankME: Reliable human ratings for natural language generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 72\u201378, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\\n\\nRichard Yuanzhe Pang and Kevin Gimpel. 2019. Unsupervised evaluation metrics and learning criteria for non-parallel textual transfer. In Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 138\u2013147, Hong Kong. Association for Computational Linguistics.\\n\\nShrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, and Alan W Black. 2018. Style transfer through back-translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 866\u2013876, Melbourne, Australia. Association for Computational Linguistics.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367.\\n\\nMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level training with recurrent neural networks.\\n\\nSudha Rao and Joel Tetreault. 2018. Dear sir or madam, may I introduce the GY AFC dataset: Corpus, benchmarks and metrics for formality style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 129\u2013140, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online. Association for Computational Linguistics.\\n\\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 8:264\u2013280.\\n\\nHoracio Saggion. 2017. Automatic text simplification. Synthesis Lectures on Human Language Technologies.\"}"}
{"id": "acl-2023-long-905", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. 2020. Masked language model scoring. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699\u20132712, Online. Association for Computational Linguistics.\\n\\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational Linguistics.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Controlling politeness in neural machine translation via side constraints. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 35\u201340, San Diego, California. Association for Computational Linguistics.\\n\\nKim Cheng Sheang and Horacio Saggion. 2021. Controllable sentence simplification with a unified text-to-text transfer transformer. In Proceedings of the 14th International Conference on Natural Language Generation, pages 341\u2013352, Aberdeen, Scotland, UK. Association for Computational Linguistics.\\n\\nLibin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Discriminative reranking for machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 177\u2013184, Boston, Massachusetts, USA. Association for Computational Linguistics.\\n\\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Minimum risk training for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1683\u20131692, Berlin, Germany. Association for Computational Linguistics.\\n\\nDavid A. Smith and Jason Eisner. 2006. Minimum risk annealing for training log-linear models. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787\u2013794, Sydney, Australia. Association for Computational Linguistics.\\n\\nFelix Stahlberg and Bill Byrne. 2019. On NMT search errors and model errors: Cat got your tongue? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3356\u20133362, Hong Kong, China. Association for Computational Linguistics.\\n\\nElior Sulem. 2018. Semantic structural evaluation for text simplification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers).\\n\\nElior Sulem, Omri Abend, and Ari Rappoport. 2018. Simple and effective text simplification using semantic and neural methods. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 162\u2013173, Melbourne, Australia. Association for Computational Linguistics.\\n\\nSai Surya, Abhijit Mishra, Anirban Laha, Parag Jain, and Karthik Sankaranarayanan. 2019. Unsupervised neural text simplification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2058\u20132068, Florence, Italy. Association for Computational Linguistics.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems.\\n\\nJohn Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig. 2019. Beyond BLEU: training neural machine translation with semantic similarity. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4344\u20134355, Florence, Italy. Association for Computational Linguistics.\\n\\nSander Wubben, Antal van den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1015\u20131024, Jeju Island, Korea. Association for Computational Linguistics.\\n\\nWei Xu, Chris Callison-Burch, and Courtney Napoles. 2015. Problems in current text simplification research: New data can help. Transactions of the Association for Computational Linguistics (TACL).\\n\\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. 2016. Optimizing statistical machine translation for text simplification. Transactions of the Association for Computational Linguistics, 4:401\u2013415.\\n\\nWei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. 2012. Paraphrasing for style. In Proceedings of COLING 2012, pages 2899\u20132914, Mumbai, India. The COLING 2012 Organizing Committee.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In International Conference on Learning Representations.\\n\\nXingxing Zhang and Mirella Lapata. 2017. Sentence simplification with deep reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.\"}"}
{"id": "acl-2023-long-905", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563\u2013578, Hong Kong, China. Association for Computational Linguistics.\\n\\nZhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353\u20131361, Beijing, China. Coling 2010 Organizing Committee.\"}"}
{"id": "acl-2023-long-905", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A LENS Metric - Implementation Details\\n\\nWe leverage Transformers and PyTorch Lightning libraries to implement the metric. We used the RoBERTa Large model (Liu et al., 2019) as the encoder. We fine-tuned the metric with the style-adaptive loss for 20 epochs and selected the checkpoint with the best validation loss. We used the ten human references from ASSET to calculate metric scores while training LENS using SIMPEVALPAST.\\n\\nWe used a batch size of 8, Adam optimizer with a learning rate of 3e-05 for the encoder and 1e-05 for other layers. We use dropout of 0.15 and freeze the encoding layers for 1 epoch. We train the model on one Quadro RTX 6000 GPU with 25GB RAM, which takes around 3 hours.\\n\\nB Incorporating Evaluation Metrics into Training and Inference - Implementation Details\\n\\nWe implement the controllable sentence simplification system proposed by Sheang and Saggion (2021), a T5-base model. The input sentence is prepended with four control tokens, namely character length ratio, dependency tree depth ratio, character-level Levenshtein similarity, and inverse frequency ratio, to control various aspects of the generated simplification. During training, each control value is calculated using the corresponding training pair and discretized into bins of width 0.05. During inference, we set the control tokens to the average value of the training set. We used 0.9 for the length ratio and 0.75 for the rest. We also fine-tune the controllable T5-3B and T5-11B versions using the same approach.\\n\\nWe use the Hugging Face Transformers library for implementing the base model and the MRT framework. We first train the model for 5 epochs using MLE loss and then fine-tune it for 5 epochs using MRT. We use Adam optimizer with a learning rate of 1e-4, linear learning rate warmup of 4k steps, weight decay of 0.1, epsilon of 1e-8, and batch size of 16. During MRT, we use a beam size of 8 to generate candidates. The rest of the parameters are left with default values from the library. During inference, we use a beam size of 10.\\n\\nOur models are trained on 2 A40 GPUs with 45GB RAM for 48 hours.\\n\\nWe used the code released by Fernandes et al. (2022) for the MBR framework. We generated candidates using beam search of size 100 and selected the top 100 candidates for reranking. We used the above T5 model fine-tuned using MLE to generate the candidates. The rest of the parameters are left with default values from the library.\\n\\nC Systems in SIMPEVAL Datasets\\n\\nData-driven Neural Models. We use ten supervised models: (i) three fine-tuned T5 (Raffel et al., 2020) models of various sizes, namely T5-base, T5-large, and T5-3B, (ii) a controllable T5-base model that prepends tokens to the input to control the lexical and syntactic complexity of the output (Sheang and Saggion, 2021), (iii) two BART (Lewis et al., 2020) models that also use control tokens where one is trained on Wikipedia (Martin et al., 2020), and the other is fine-tuned on a combination of Wikipedia and web-mined paraphrases (Martin et al., 2022), (iv) a BERT-initialized Transformer (Maddela et al., 2021), (v) a BiLSTM edit-based approach that first generates the edit operations, and then the simplification (Dong et al., 2019), (vi) a BiLSTM that directly generates simplifications using reinforcement learning (Zhang and Lapata, 2017), and (vii) a vanilla BiLSTM model. In addition, we also include one unsupervised model and one semi-supervised model by Surya et al. (2019) that uses an auto-encoder with adversarial and denoising losses.\\n\\nFew-shot Methods. We include simplifications generated by GPT-3.5 under zero-shot and 5-shot settings (prompts are provided in Appendix D.2).\\n\\nData-driven Statistical Methods. We incorporate two systems that applied statistical machine translation (MT) approaches to text simplification: (i) a phrase-based MT system that reranks the outputs based on their dissimilarity with the input (Wubben et al., 2012), and (ii) a syntactic MT system that uses paraphrase rules for lexical simplification (Xu et al., 2016).\\n\\nRule-based Methods. Kumar et al. (2020) iteratively generates candidates using rules and ranks them with a linguistic scoring function.\\n\\nHybrid Methods. We utilize three hybrid systems that combine linguistic rules with data-driven methods: (i) Narayan and Gardent (2014) uses semantic structure to predict sentence splitting and\\n\\nSpecifically, we use the text-davinci-003 model which is the most recent variant with 175B parameters.\"}"}
{"id": "acl-2023-long-905", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"paraphrases with a phrase-based MT system, (ii) Sulem et al. (2018) performs splitting and deletion using linguistic rules and paraphrasing using a BiLSTM, and (iii) Maddela et al. (2021) generates candidates with different amounts of splitting and deletion and then paraphrases the best candidate with a BERT-initialized Transformer.\\n\\nNaive Baseline Methods.\\n\\nExisting metrics are biased towards conservative systems because their outputs are generally fluent and exhibit high lexical overlap with the input/reference (Pang and Gimpel, 2019; Krishna et al., 2020). We add two conservative systems that are challenging for automatic metrics: (i) a system that always copies the input and (ii) a content-preserving but nonsensical system that scrambles 5% of the input words.\\n\\nHumans.\\n\\nWe also add human-written simplifications using different instructions from ASSET (Alva-Manchego et al., 2020) and TurkCorpus (Xu et al., 2016), two widely used evaluation benchmarks for sentence simplification, and an auto-aligned one from the SimpleWiki (Kauchak, 2013).\\n\\nD Implementation Details for Simplification Systems\\n\\nD.1 T5 Setup\\n\\nWe use the Hugging Face Transformers library. We fine-tune T5-base, T5-large, and T5-3B on 4 A40 GPUs of a total batch size of 64 for 20 epochs (10.4K steps), 16 epochs, and 8 epochs, respectively. We use a learning rate of 3e-4. We save checkpoints every 5K steps and select the best one by performing a manual inspection on a set of 60 simplifications from the development set, resulting in 80K, 60K, and 25K steps for T5-base, T5-large, and T5-3B respectively, which are in a similar range to FLAN-T5 (Chung et al., 2022). We use AdamW (Loshchilov and Hutter, 2017) as the optimizer.\\n\\nD.2 GPT-3.5 Setup\\n\\nHyperparameters. We use the text-davinci-003 GPT-3.5 model from OpenAI API. To generate simplification, we use the following hyperparameters: temperature=1 and top-p=0.95.\\n\\nPrompts. We use the instruction from ASSET (Alva-Manchego et al., 2021) to prompt GPT-3.5.\\n\\nZero-shot setting: Please rewrite the following complex sentence in order to make it easier to understand by non-native speakers of English. You can do so by replacing complex words with simpler synonyms (i.e. paraphrasing), deleting unimportant information (i.e. compression), and/or splitting a long complex sentence into several simpler ones. The final simplified sentence needs to be grammatical, fluent, and retain the main ideas of its original counterpart without altering its meaning.\\n\\nInput: {input}\\n\\nOutput:\\n\\n5-shot setting: Please rewrite the following complex sentence in order to make it easier to understand by non-native speakers of English. You can do so by replacing complex words with simpler synonyms (i.e. paraphrasing), deleting unimportant information (i.e. compression), and/or splitting a long complex sentence into several simpler ones. The final simplified sentence needs to be grammatical, fluent, and retain the main ideas of its original counterpart without altering its meaning.\\n\\nExamples:\\n\\nInput: {random sampled from ASSET}\\n\\nOutput: {random sampled human reference of the Input from ASSET}\\n\\nInput: {random sampled from ASSET}\\n\\nOutput: {random sampled human reference of the Input from ASSET}\\n\\nInput: {random sampled from ASSET}\\n\\nOutput: {random sampled human reference of the Input from ASSET}\\n\\nInput: {random sampled from ASSET}\\n\\nOutput: {random sampled human reference of the Input from ASSET}\\n\\nInput: {random sampled from ASSET}\\n\\nOutput: {random sampled human reference of the Input from ASSET}\"}"}
{"id": "acl-2023-long-905", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"replacing complex words with simpler synonyms (i.e. paraphrasing), deleting unimportant information (i.e. compression), and/or splitting a long complex sentence into several simpler ones. The final simplified sentence needs to be grammatical, fluent, and retain the main ideas of its original counterpart without altering its meaning.\\n\\nInput: {input}\\nOutput: 16400\"}"}
{"id": "acl-2023-long-905", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Metric       | WIKI-DA | NEWSELA-LIKERT |\\n|--------------|---------|----------------|\\n| **FKGL**     | -0.556  | 0.333          |\\n| **BLEU**     | 0.048   | 0.460          |\\n| **SARI**     | 0.206   | 0.238          |\\n| **BERTScore**| 0.238   | 0.238          |\\n\\n| **Original** | **Rescaled** |\\n|--------------|--------------|\\n| LENS all     | 0.333        | 0.333          |\\n| LENS k=1     | 0.460        | 0.460          |\\n| LENS k=3     | 0.429        | 0.429          |\\n\\nTable 8: Metric evaluation results on SIMPLEVAL2022, WIKI-DA (Alva-Manchego et al., 2021), and NEWSELA-LIKERT (Maddela et al., 2021) human ratings datasets. We include the results for both original and rescaled versions of LENS. \\\\(\\\\tau_{para}\\\\), \\\\(\\\\tau_{spl}\\\\), and \\\\(\\\\tau_{all}\\\\) represent the Kendall Tau-like correlation for paraphrase-focused, split-focused, and all simplifications respectively. We report the Pearson correlation coefficients along three dimensions for WIKI-DA and NEWSELA-LIKERT. The best values are marked in bold and the second best values are underlined. As Kendall Tau measures pairwise rankings, we see the same results for the original and rescaled versions of LENS.\"}"}
{"id": "acl-2023-long-905", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"F.1. Step 1: System Output Categorization.\\n\\nFigure 6: Annotation interface for categorizing system outputs. The outputs can be moved up and down or to other categories.\"}"}
{"id": "acl-2023-long-905", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Step - 2: Highlighting System Edits.\\n\\nFigure 7: Span-fixing interface provided by annotators to fix the highlighted changes between the original and simplified sentences. Annotators could click and drag to modify the bounds of a paraphrase label or highlight text to add a paraphrase level. The deletion (\\\"\u2227\\\") and split (\\\"||\\\") can also be dragged and can be added using buttons on the bottom left of the modal.\\n\\nFigure 8: Annotation interface for span fixing. Clicking on \\\"Fix\\\" button opens up a pop up window.\"}"}
{"id": "acl-2023-long-905", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"F.3 Step - 3: Rating and Ranking System Outputs.\\n\\nFigure 9: Sentence ranking and rating interface provided to annotators. The annotator can enter the ratings for each sentence and is able to re-order sentences by clicking and dragging their mouse.\"}"}
{"id": "acl-2023-long-905", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-905", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-905", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper\u2019s main claims?\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nB \u25a1 Did you use or create scientific artifacts?\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC \u25a1 Did you run computational experiments?\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-905", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No. This study does not meet the definition of \\\"human subjects\\\" research by our institutional IRB review board, as we did not: \\\"(1) Interact with, intervene with, or obtain/access private, identifiable information or data about, a living individual (includes online surveys) or (2) Conduct research on a drug, biologic, or medical device\\\". The annotations are linguistic judgements provided by hired in-house employees.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\"}"}
