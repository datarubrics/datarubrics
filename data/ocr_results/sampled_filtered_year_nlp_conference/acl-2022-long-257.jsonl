{"id": "acl-2022-long-257", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NEGATIVE: \u201cThis has been like the worst week ever im so done with everything. sick tired\u201d\\n\\nPOSITIVE: \u201cI made it to the end of the most challenging week ever!\u201d\\n\\nNEGATIVE: \u201cUgh my mac is starting to slow up and I need to figure out how to defragment the hard drive...\u201d\\n\\nPOSITIVE: \u201cI need to defragment the hard drive to speed up my mac. Good thing I\u2019m smart, and I know I can do this.\u201d\\n\\nNEGATIVE: \u201cI am SO stressed with all my exams and my lit review hanging over my head this week.\u201d\\n\\nPOSITIVE: \u201cOnly one more week until my exams and lit review are all done!\u201d\\n\\nNEGATIVE: \u201cI am the only person I know who writes a healthy grocery list and plans meals when I am stressed:( Can\u2019tSleep\u201d\\n\\nPOSITIVE: \u201cI\u2019m so thankful that I am still able to eat healthy even when I\u2019m stressed.\u201d\\n\\nExample Reframes\\n\\nIn Table 5, we compare examples of model-generated reframes from different models. The examples are structurally and semantically diverse, which may suggest that different architectures could serve as complementary systems in a broader effort to introduce a range of positive perspectives in text. However, the generations are not perfect. In this particular example, CopyNMT and GPT-2 fail to integrate the key concept of the \u201cheavy workload\u201d into their reframe generations. There is still significant room to improve upon these models in future work.\\n\\nWe were also interested in the success of the Controlled generations. Do models appropriately follow a given strategy when the generation is conditioned on that strategy? Table 6 shows different reconstructions of the same text using different controlling strategies. Qualitatively, we can see that controlled generation is a reasonable method for influencing the most prominent strategy. The \u2018thankfulness\u2019, \u2018growth\u2019 and \u2018self-affirmation\u2019 strategies are particularly distinct.\"}"}
{"id": "acl-2022-long-257", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Always stressing and thinking about loads of things at once, I need to take it one at a time and overload is stressed. Need to rant.\\n\\nGround Truth\\n\\nLoads of things on my mind, I need to make a list, prioritise and work through it all calmly and I will feel much better.\\n\\nModel Generation\\n\\nRandom\\n\\nAnnoyed because the USPS lost 1 of the 8 flat rate boxes of books I shipped from California to Kentucky. Hope it shows up :-/\\n\\nSBERT\\n\\nReally stressed at the moment but I know that this will pass and I'll get everything sorted out.\\n\\nSeq2seq-LSTM\\n\\nI have a lot of things to do. It is a good night's sleep and I can get through it.\\n\\nCopyNMT\\n\\nAlways stressing and thinking of things at once, but I know that I need it.\\n\\nGPT\\n\\nI'm so relieved that I'm going to get to organize my thoughts in such a short time! I hope I am done with the list of things I have to do today, so that I can go back to bed.\\n\\nGPT-2 No-pretrain\\n\\nI'm going to try to seek a new job to get some perspective through today.\\n\\nGPT-2\\n\\nI should be more energetic in my thoughts and not worry too much.\\n\\nT5\\n\\nI am stressed and thinking about loads of things at once, I need to take it one at a time and stressed need to rant. I'm sure I'll be able to focus on it.\\n\\nBART\\n\\nI am stressed and thinking about loads of things at once, but I am going to take it one at a time.\\n\\nTable 5:\\n\\nA model comparison for reframing the same text. Here, we observe significant diversity in the generations, suggesting that these different architectures could serve as complementary systems in an effort to restructure and transform negative text on the web.\\n\\nOriginal Text\\n\\nI'm doing really well in some class and bad in other classes. I hate this.\\n\\nGPT-2 Generations\\n\\nGrowth Mindset\\n\\nI'm doing really well in some class and bad in other classes. I hate this, but I'll be able to do it again.\\n\\nImpermanence\\n\\nI'm doing really well in some class and bad in other classes. I hate this. But I'll be able to do my best in the future.\\n\\nNeutralizing\\n\\nI'm doing really well in some class and bad in other classes. I hate this. But I can't wait to see if I can do it.\\n\\nOptimism\\n\\nI'm doing really well in some class and bad in other classes. I hate this. I hope I'll be able to do better in the future.\\n\\nSelf-affirmation\\n\\nI'm doing really well in some class and bad in other classes. I hate this. It's a good thing to do, but I'll be able to do it.\\n\\nThankfulness\\n\\nI'm doing really well in some class and bad in other classes. I hate this. But I can't wait to see if I can do it.\\n\\nT5 Generations\\n\\nGrowth Mindset\\n\\nI'm doing really well in some class and bad in other classes. I hate this, but I'll be able to do it again.\\n\\nImpermanence\\n\\nI'm doing really well in some class and bad in other classes. I hate this. But I'll be able to do my best in the future.\\n\\nNeutralizing\\n\\nI'm doing really well in some class and bad in other classes. I hate this. But I can't wait to see if I can do it.\\n\\nOptimism\\n\\nI'm doing really well in some class and bad in other classes. I hate this. I hope I'll be able to do better in the future.\\n\\nSelf-affirmation\\n\\nI'm doing really well in some class and bad in other classes. I hate this. It's a good thing to do, but I'll be able to do it.\\n\\nThankfulness\\n\\nI'm doing really well in some class and bad in other classes. I hate this. But I can't wait to see if I can do it.\\n\\nBART Generations\\n\\nGrowth Mindset\\n\\nI'm doing really well in some class and bad in other classes. I'm going to try to improve my grades.\\n\\nImpermanence\\n\\nI'm doing really well in some class and bad in other classes, but I'm sure it will all work out in the end.\\n\\nNeutralizing\\n\\nI'm doing really well in some class and bad in other classes. I don't like this.\\n\\nOptimism\\n\\nI'm doing really well in some class and bad in other classes. But I'm sure it will all work out.\\n\\nSelf-affirmation\\n\\nI'm doing really well in some class and bad in other classes. But I know I can do better.\\n\\nThankfulness\\n\\nI'm doing really well in some class and bad in other classes. But I'm thankful that I have the opportunity to study.\\n\\nTable 6:\\n\\nA model comparison for reframing the same text using different controlling strategies. Here, we observe that models can learn some information from the input strategy label and make distinctive generations, especially for the 'thankfulness', 'growth' and 'self-affirmation' strategies.\"}"}
{"id": "acl-2022-long-257", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Amazon Mechanical Turk interface used to collect positive reframes (in Section 4.1).\\n\\nFigure 5: Amazon Mechanical Turk interface used to find inter-annotator agreement for the taxonomy (in Section 4.2).\"}"}
{"id": "acl-2022-long-257", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inducing Positive Perspectives with Text Reframing\\n\\nCaleb Ziems \u22c6\u2020 Minzhi Li \u22c6\u22c4 Anthony Zhang \u2020 Diyi Yang \u2020\\n\\n\u22c6 Equal contribution.\\n\\nAbstract\\n\\nSentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, POSITIVE FRAME, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work. To download the data, see https://github.com/GT-SALT/positive-frames\\n\\n1 Introduction\\n\\nGratitude is not only the greatest of virtues, but the parent of all the others. \u2014 Marcus Tullius Cicero\\n\\nText style transfer (TST) has received much attention from the language technologies community (Hovy, 1987; Jin et al., 2020), where the goal is to change some attribute, like the sentiment of the text, without changing any attribute-independent content (Mir et al., 2019; Fu et al., 2018; Logeswaran et al., 2018). Some TST applications such as de-biasing (Pryzant et al., 2020; Ma et al., 2020) and paraphrasing (den Bercken et al., 2019; Xu et al., 2012) require meaning-preserving transformations, while political leaning (Prabhumoye et al., 2018), sentiment (Shen et al., 2017; Hu et al., 2017), and topical transfer (Huang et al., 2020) allow for a change in the underlying meaning. For instance, for a negative review, \\\"this was a bland dish,\\\" we can use a sentiment TST model to create a more positive \\\"this was a tasty dish,\\\" by swapping the word bland with tasty. Although the input's structure and attribute-independent content are preserved, the truth-conditional meaning is clearly altered.\\n\\nIn this work, we introduce a closely related task\u2014positive reframing\u2014that differs from sentiment TST in important ways. We effectively reframe negative text by inducing a complementary positive viewpoint (e.g. glass-half-full), which nevertheless supports the underlying content of the original sentence. The reframe should implicate rather than contradict the source (see Figure 1), and the transformation should be motivated by theoretically justified strategies from positive psychology (Harris et al. 2007; see Section 3).\\n\\nTo use the example from before, we could reframe \\\"this was a bland dish\\\" with the self-affirmation strategy and say \\\"I've made dishes that are much tastier than this one.\\\" This reframed one still communicates the author's original intention by conversationally implicating that the dish was unsatisfying (Grice, 1975), but it shifts the focus away from the negative judgment and onto a positive and...\"}"}
{"id": "acl-2022-long-257", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"self-affirming perspective. Numerous studies have shown the positive effects of this and other reframing strategies on well-being and cognitive performance (Martens et al., 2006; Cohen et al., 2006; Good et al., 2003), which motivate this work. Our main contribution is the design and implementation of a new positive reframing task. To facilitate research in this space, we introduce a parallel corpus of 8,349 reframed sentence pairs and 12,755 structured annotations for six theoretically-motivated re-write strategies. This is a significant contribution, especially since rich parallel corpora are scarce in TST tasks. Some related datasets exist for politeness (Madaan et al., 2020) and sentiment transfer (Shen et al., 2017; He and McAuley, 2016), but they lack this parallel structure. With only unaligned corpora, researchers are limited to unsupervised training paradigms, which notoriously fail to disentangle style from content, and thus also fail to preserve meaning (Lample et al., 2019). Using our parallel corpus, we examine how current state-of-the-art neural models work for positive reframing. We find that, supervised transformer-based neural models appear capable of rewriting a negative text without contradicting the original premise of that text. However, these models still struggle to generate reasonable positive perspectives, suggesting that our dataset will serve as a useful benchmark for understanding psychologically well-motivated strategies for augmenting text with positive perspectives.\\n\\n2 Related Work\\n\\n2.1 Style-Transfer\\n\\nThere is a longstanding interest in style transfer, starting with the early days schema-based systems (McDonald and Pustejovsky, 1985; Hovy, 1987), and then syntax-based (Zhu et al., 2010; Xu et al., 2016) and phrase-based machine translation (Xu et al., 2012; Wubben et al., 2012), into the age of end-to-end neural models. Recent works include supervised seq2seq tasks on parallel data (Rao and Tetreault, 2018; Fu et al., 2018) or pseudo-parallel data (Jin et al., 2019; Zhang et al., 2020b), as well as unsupervised generative modeling on non-parallel data (Hu et al., 2017; Shen et al., 2017), and semi-supervised techniques (Shang et al., 2019). Other ideas include domain adaptation (Li et al., 2019) or multi-task learning (Niu et al., 2018), zero-shot translation (Korotkova et al., 2019), unsupervised \u201cdelete and generate\u201d approaches (Li et al., 2018; Sudhakar et al., 2019; Malmi et al., 2020; Madaan et al., 2020), and reinforcement learning (Zhang and Lapata, 2017; Wang et al., 2016).\\n\\nMany existing datasets lack parallel structure, so the unsupervised setting is common in TST. Unfortunately, many of these methods still fail to disentangle style from content and adequately preserve the meaning of the original text (Lample et al., 2019). Autoencoders are particularly vulnerable to this shortcoming (Hu et al., 2017; Zhao et al., 2018), but some unsupervised machine translation techniques appear less vulnerable (Artetxe et al., 2018; Lample et al., 2018). In contrast, our positive reframing task requires source meaning-preservation and the introduction of new content and new perspectives, posing a unique challenge to unsupervised methods. We also provide a parallel corpus to train supervised models for this task.\\n\\n2.2 Language and Positive Psychology\\n\\nPositivity is contagious and can spread quickly across social networks (Coviello et al., 2014; Hatfield et al., 1993). Positive contagion in teams can reduce group conflict and improve group cooperation and even task performance (Barsade, 2002). Effective leaders also harness the power of positive reframing to promote company growth (Sy and Choi, 2013; Sy et al., 2005; Johnson, 2009; Masters, 1992) and beneficially shape negotiations (Filipowicz et al., 2011), customer relations (Dietz et al., 2004), decision making (G\u00e4chter et al., 2009; Druckman, 2001) and policy outcomes (Erisen et al., 2014). At an individual level, people who express optimism and gratitude are less likely to have depressive symptoms (Lambert et al., 2012) and more likely to experience emotional and psychological well-being (Carver et al., 1999; Watkins et al., 2008; Scheier et al., 2001).\\n\\nOn the other hand, fake expressions of positivity are correlated with negative brain activity (Ekman et al., 1990) and may actually be more harmful than helpful (Fredrickson, 2000; Fredrickson and Losada, 2005; Gross, 2013; Logel et al., 2009). That is why in our task it is essential that any positively reframed rephrased text remain true to the original premise of the source. In this way, our task is most similar to meaning-preserving transformations via parallel corpora from domains such as political argumentation (Chakrabarty et al., 2021), de-biasing (Pryzant et al., 2020; Ma et al., 2020), politeness (Madaan et al., 2020), and paraphrasing...\"}"}
{"id": "acl-2022-long-257", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we present our psychologically-motivated taxonomy of positive reframing strategies. Instead of merely swapping antonyms for negative words or inserting unfounded positive language into a sentence, these strategies work to more fundamentally reconstruct the author's fixed, global, and ultimately harmful self-narratives, which are known in the literature as cognitive distortions (Burns, 1981; Abramson et al., 2002; Walton and Brady, 2020). Cognitive distortions include many exaggerated or irrational self-focused thoughts (Nalabandian and Ireland, 2019), such as dichotomous \u201call-or-nothing\u201d thinking (Oshio, 2012), over-generalization (Muran and Motta, 1993), and catastrophizing (Sullivan et al., 2001). We can reconstruct these ideas using strategies from positive psychology (Harris et al., 2007). Each strategy is designed to promote a beneficial shift in perspective without distorting the underlying context of the author's situation.\\n\\n**Growth Mindset** or, alternatively, the incremental theory of personality (Yeager et al., 2014; Burdette and Finkel, 2012), is the belief that one\u2019s skills and abilities are not immutable but can instead be changed and improved over time (Dweck, 2016); that one\u2019s willpower is an abundant rather than limited or exhaustible resource (Job et al., 2010, 2015); and that apparent setbacks like stress can be enhancing rather than debilitating (Crum et al., 2013). Instead of saying \u201cI\u2019m such a lazy procrastinator,\u201d a growth-mindset would say \u201cI\u2019m determined to learn better time management.\u201d This mindset has demonstrable benefits like improved performance on school tests (Good et al., 2003; Blackwell et al., 2007; Dweck and Yeager, 2019; Yeager et al., 2014).\\n\\n**Impermanence** means understanding that negative experiences are finite and temporary, and that others have also experienced or even overcome similar forms of adversity. Someone might say \u201csince I failed this test, I must be too stupid for school.\u201d An impermanence reframe could be \u201cThis wasn\u2019t the test score I hoped for, but everyone slips up now and then.\u201d This category is also related to those proposed by Walton and Brady (2020): (1) focus on the \u201cpossibility of improvement,\u201d (2) recognize \u201cspecific, normal causes,\u201d and (3) understand \u201cyou\u2019re not the only one.\u201d\\n\\n**Neutralizing** involves removing or rewriting negative phrases and terms so they are more neutral (Pryzant et al., 2020). Someone might complain that \u201cWendy\u2019s customer service is terrible.\u201d A neutralized reframe could be \u201cWendy\u2019s customer service could use some improvement.\u201d\\n\\n**Optimism** does not mean to negate or deny the negative aspects of a situation, but instead to shift the emphasis to the more positive aspects of the situation, including expectations for a bright future (Carver et al., 2010). For example, if there is a negative emphasis, like in the sentence, \u201cI\u2019ve completely worked myself to the bone this week, burning the candle at both ends... TGIF,\u201d we can use optimism to shift the emphasis towards the positive as follows: \u201cIt\u2019s been a long week, but now I can kick back, relax, and enjoy my favorite shows because it\u2019s the weekend.\u201d\\n\\n**Self-affirmation** means to assert a more holistic or expansive version of oneself by listing one\u2019s values, skills, and positive characteristics (Cohen and Sherman, 2014; Silverman et al., 2013). Positive psychology gives many examples like love, courage, hope, gratitude, patience, forgiveness, creativity, and humor (Harris et al., 2007). Reflecting on these values can bolster one\u2019s sense of integrity (see Self-Affirmation Theory; Steele 1988), can reduce depressive affect (Enright and Fitzgibbons, 2000), and can translate to increased performance on measurable tasks like exams (Martens et al., 2006; Cohen et al., 2006; Sherman et al., 2009).\\n\\n**Thankfulness** can also be described more broadly as an \u201cattitude of gratitude\u201d (Emmons and Shelton, 2002). Adding more positive words that convey thankfulness or gratitude (e.g. appreciate, glad that, thankful for). For example, we can reframe the rhetorical question, \u201cIs it sad that I don\u2019t wanna be at home and wish that work could call me in early?\u201d by expressing gratitude for career: \u201cI am thankful that I have a job that makes me want to get out of bed everyday.\u201d\"}"}
{"id": "acl-2022-long-257", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Summary statistics for POSITIVE PSYCHOLOGY FRAMES. (Left) Distribution of the non-exclusive labels across all 8,349 annotations shows a preference for optimism and neutralizing strategies. (Right) The quality of annotations is shown by moderate Intra-class Correlation (ICC), with reasonable genuineness (Gen) metrics for 100 randomly sampled datapoints.\\n\\nInstead of using a specific list of cognitive distortions, we developed a list of six keyword pairs, each of which signal the form of one of the cognitive distortions, and the literature on distortion classification was still relatively unexplored (Simms et al., 2017; Shickel et al., 2020). We instead chose the simple keyword #stressed to signal the anxiety, negative affect, and hopelessness that has been shown to accompany cognitive distortions by prior work (Sears and Kraus, 2009). Our decision to use Twitter was also motivated by the 280 character limit, which ensured that samples were short, focused expressions of relatively atomic ideas, as opposed to longer narrative-style texts from discussion platforms like Reddit's /r/rant.\\n\\nOur filtered collection of negative texts comes from a collection of over 1 million #stressed tweets written between 2012 and 2021, and it excludes any replies and retweets, any insubstantial tweets less than 30 characters, and any text containing a URL, which is often associated with spam (Zhang et al., 2012; Grier et al., 2010). After we removed other hashtags or Twitter handles from the text, we used TextBlob (Loria, 2018) to exclude any overtly positive texts with a non-negative sentiment score. Finally, to reduce any confounds between cognitive distortions and hate speech, and to make the human annotation task more agreeable for crowd-workers, we excluded examples that were flagged as offensive with over 80% confidence according to HateSonar (Davidson et al., 2017).\\n\\nWe also considered pet peeve, fml, and other keywords but manual inspection revealed that these tweets were unlikely to contain cognitive distortions. In contrast, stressed hashtag provides a high precision data collection. We acknowledge this as a limitation and urge readers to keep this mind when interpreting our findings.\\n\\n4.1 Annotation\\nWe recruited crowdworkers to reframe 8,687 randomly-sampled texts with two workers assigned to each task, so we had two unique reframe annotations for every tweet. The annotators were encouraged to decide independently which reframing strategy to use, and they could combine multiple strategies in the same reframe. We simply asked annotators to record the strategies they selected. Additionally, they gave us, on a scale from 1-5, a score indicating how positive the original text was, and separately, how positive the text had become after they reframed it. Finally, we asked workers to mark advertisements, spam, or any text they felt they could not understand or effectively reframe. These examples were later removed from the corpus (see Appendix A for details).\\n\\nIn total, 204 workers participated in this task. Before they worked on the task, workers were asked to be familiar with our task by reading our provided reframing examples for each of the six strategies (Section 3), along with detailed annotation instructions. Then they had to pass a qualification test to show they can recognize different strategies in different reframing examples, with at least 5 out of 6 multiple-choice questions answered correctly.\\n\\nWe paid all annotators a fair wage above the federal minimum and both manually and programmatical inspected their work for quality (see Appendix A). After removing any poor-quality data, we were left with 8,349 reframed sentences. The strategy label distribution is given on the left side of Table 1, where a single reframe can have more than one strategy label.\"}"}
{"id": "acl-2022-long-257", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Data Quality\\nTo determine the reliability of the reframing strategy constructs, we randomly sampled 100 annotations from Section 4.1 and asked three annotators to consider both the original text and the reframed text, and then the annotators marked which of the six strategies were used in the given reframe. This allowed us to compute inter-annotator agreement scores for the strategy labels in Table 1. We observe the Intra-class Correlation for one-way random effects between the three raters and find moderate inter-rater agreement across these attribute categories (min 0.32; max 0.68). We also asked this second round of annotators to evaluate the genuineness of the reframes on a scale from 1-5. Our instructions explain that, with a more genuine reframe, it is more likely that someone in the original situation would say something similar. We find that, across all strategy labels, the average genuineness score is $\\\\sim 4$ out of 5, so we know the data conforms reasonably well to our task instructions.\\n\\n5 Positive Reframing\\nWith POSITIVE PSYCHOLOGY FRAMES, we then examine how generative models work to automatically suggest a negatively-oriented self-narrative with a more positive shift in perspective without distorting any of the underlying meaning of that text. To do so will make use of encoder-decoder or conditional language models, as well as the six positive psychology strategies outlined in Section 3.\\n\\n5.1 Task Formulation\\nLet $(s,t,\\\\psi_t)$ be a single annotation tuple in POSITIVE PSYCHOLOGY FRAMES for original source text $s$ and positive reframe target $t$, which uses positive psychology strategies given by the multi-hot encoded vector $\\\\psi_t$. In the Positive Reframing task, our goal is to encode $s$ and, at decoding time, produce $t$ which makes use of $\\\\psi_t$ strategies and preserves the underlying meaning of $s$. Therefore, we formulate the problem as conditional generation and, during training, we maximize the standard language modeling objective\\n\\n$$\\\\frac{1}{N} \\\\sum_{i=0}^{N} \\\\log p(g_i | g_0: i-1)$$\\n\\nover the string $g = \\\\{s, \\\\psi_t, t\\\\} = \\\\{\\\\langle BOS \\\\rangle, s_1, s_2, ..., s_n, \\\\langle STRG \\\\rangle, \\\\psi_{grow}, \\\\psi_{imp}, ..., \\\\psi_{thank}, \\\\langle REFR \\\\rangle, t_1, t_2, ..., t_m, \\\\langle EOS \\\\rangle\\\\}$ where $g_i$ is the $i$th token in the string of length $N$, which contains the start token $\\\\langle BOS \\\\rangle$, the tokenized source $s_1:n$, the tokenized reframe target $t_1:m$, and the binary tokens $\\\\psi_{grow}, \\\\psi_{imp}, ...$ indicating whether a particular strategy (e.g. grow mindset) was used in reframe $t$.\\n\\nAt decoding time, we consider three settings: Unconstrained generation $p(t | s)$, Controlled generation $p(t | s, \\\\psi_t)$, and a strategy Prediction form of generation $p(t, \\\\psi_t | s)$. Unlike in the Unconstrained setting, the Controlled generation is conditioned on the desired strategies $\\\\psi_t$. In the Prediction setting, the model will concurrently predict the strategies it used to generate its own reframe.\\n\\nNote that, we introduce three different model settings here to capture how positive reframing assistance might be used by people in the real world. Specifically, the Unconstrained setting models reframe text directly without being aware of any specific strategy to use. The Prediction setting extends the unconstrained mode, i.e., produce the reframed text and also output the reframing strategies used in the reframing process spontaneously. The Controlled setting simulates the scenario of producing a reframed text with the help of concrete positive reframing strategies.\\n\\n5.2 Experimental Setup\\nFor ground truth training, development, and testing, we randomly partition the annotations using an 8:1:1 ratio, with 6,679 train, 835 development and 835 test data. We fine-tune the GPT and GPT-2 language models (Radford et al., 2019) as well as two Seq2Seq neural machine translation models \u2013 LSTM (Hochreiter and Schmidhuber, 1997) and CopyNMT (See et al., 2017) \u2013 and finally, two encoder-decoder models, BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). For all models, we use greedy decoding. As an ablation in the Unconstrained setting, we also test a No-pretrain condition for GPT-2 in which we randomly initialize the model parameters before fine-tuning.\\n\\nRetrieval: We test two simple retrieval systems: Random retrieval of a reframed sentence from the training set, and SBERT (Reimers and Gurevych,\"}"}
{"id": "acl-2022-long-257", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Positive reframing results measured by Meaning including ROUGE-1 (R-1), ROUGE-1 (R-2), ROUGE-L (R-L), BLEU, BERTScore (BScore), Positivity via \\\\( \\\\Delta \\\\text{TextBlob} \\\\) (\\\\( \\\\Delta \\\\text{TB} \\\\)) and Fluency. State-of-the-art models can generate meaning-preserving reframes in the unconstrained setting \\\\( p(t|s) \\\\) and strategy-predictive setting \\\\( p(t, \\\\psi_t|s) \\\\) as well as when we condition the generation to use the reframing strategy from the ground truth \\\\( p(t|s, \\\\psi_t) \\\\). The best in-category performance is bolded; best overall performance is highlighted.\\n\\n| Model                  | R-1  | R-2  | R-L  | BLEU | BScore | \\\\( \\\\Delta \\\\text{TB} \\\\) | Avg. Len | Meaning | Positivity | Fluency |\\n|------------------------|------|------|------|------|--------|------------------------|----------|---------|------------|---------|\\n| Retrieval              | 9.6  | 3.6  | 8.4  | 0.17 | 84.8   | 0.36                   | 20.0     | 2.79    | 3.03       | 3.60    |\\n| SBERT                  | 15.2 | 1.9  | 12.8 | 1.47 | 87.6   | 0.36                   | 17.7     | 3.45    | 3.97       | 4.16    |\\n| Few-shot GPT-3         | 18.3 | 3.4  | 15.5 | 2.9  | 88.2   | 0.44                   | 17.3     | 3.73    |            |         |\\n| GPT-Neo                | 18.7 | 3.4  | 16.0 | 3.0  | 88.2   | 0.40                   | 17.6     | 3.69    | 4.16       | 4.21    |\\n| Unconstrained \\\\( p(t|s) \\\\) | 13.3 | 1.8  | 11.3 | 1.1  | 86.4   | 0.37                   | 21.1     | 3.55    | 3.91       | 4.08    |\\n| GPT-2 No-pretrain      | 13.2 | 1.3  | 11.4 | 0.66 | 89.6   | 0.37                   | 16.9     | 3.11    | 3.66       | 3.96    |\\n| GPT-2                  | 20.9 | 4.6  | 17.7 | 4.2  | 88.5   | 0.35                   | 20.0     | 3.58    | 4.01       | 4.18    |\\n| Seq2Seq-LSTM           | 15.7 | 1.4  | 12.4 | 0.73 | 85.6   | 0.49                   | 25.8     | 3.33    | 4.15       | 4.10    |\\n| CopyNMT                | 20.8 | 5.0  | 18.0 | 4.0  | 85.7   | 0.32                   | 16.1     | 3.57    | 3.69       | 3.91    |\\n| T5                     | 27.4 | 9.8  | 23.8 | 8.7  | 88.7   | 0.38                   | 35.3     | 4.09    | 3.79       | 4.06    |\\n| BART                   | 27.7 | 10.8 | 24.3 | 10.3 | 89.3   | 0.23                   | 24.4     | 4.13    | 3.81       | 4.15    |\\n| Predict \\\\( p(t, \\\\psi_t|s) \\\\) | 27.5 | 10.5 | 24.0 | 11.0 | 89.0   | 0.23                   | 25.1     | 4.10    |            |         |\\n| Control \\\\( p(t|s, \\\\psi_t) \\\\) | 27.7 | 10.0 | 23.9 | 8.8  | 88.8   | 0.36                   | 35.0     | 4.11    | 3.89       | 4.07    |\\n| BART                   | 28.8 | 10.9 | 25.1 | 9.85 | 89.6   | 0.27                   | 23.4     | 4.09    | 3.95       | 4.11    |\\n| Human                  | 100  | 100  | 100  | 100  | 100    | 0.35                   | 17.4     | 3.80    | 3.82       | 4.18    |\\n\\n5.3 Evaluation\\n\\nFollowing other style transfer work with a parallel corpus (Jhamtani et al., 2017; Xu et al., 2012), we evaluate our models for semantic similarity with the ground truth using the BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and BERTScore (Zhang et al., 2020a). Since there are two ground truth annotations per tweet, we take the maximum of the two scores and report the average across these maxima. We also report \\\\( \\\\Delta \\\\text{TextBlob} \\\\) or the average change in sentiment score according to TextBlob (Loria, 2018). Finally, we conduct human evaluation in which 50 items are distributed to 3 raters who score the reframed sentences for three criteria, each on a scale from 1 to 5. The criteria include Meaning Preservation (Shang et al., 2019), our task-specific objective, as well as the Positivity and Fluency of the generated text, following the sentiment style transfer literature (Luo et al., 2019).\\n\\n5.4 Results\\n\\nAutomatic Evaluation\\n\\nAcross these metrics (Table 2, left) in the unconstrained generation setting, the BART model provided the highest quality of positive reframes, while GPT provided the worst quality with results similar to the No-pretrain version of GPT-2. The pre-trained version of GPT-2 was trained on English web text, while GPT was trained on works of fiction, so it appears that pre-training decisions can affect performance.\\n\\nWe tested the two best-performing models, T5 and BART, on the controlled generation and strategy-prediction settings as well and found that the both models performed reasonably. Overall, controlled generation boosts performance, since the model can target the gold standard's strategies, but these improvements are only slight (see the Controlled part in Table 2). This warrants further\"}"}
{"id": "acl-2022-long-257", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"investigation: in Section 5.6, we explore models\u2019 ability to identify the underlying strategies given an existing reframe to understand whether models can make sense of these underlying constructs. Unsurprisingly, all supervised models outperformed our simple retrieval baselines. Most interestingly, few-shot GPT-3 and GPT-Neo also could not match the supervised models in terms of overlap with the ground truth (ROUGE, BLEU, BERTScore), but they still achieved a comparable positive shift in sentiment ($\\\\Delta$ TextBlob).\\n\\nHuman Evaluation\\nHuman judgments both support and elaborate on the automatic evaluation findings. For our best performing BART and T-5 models, the average scores are very high, even surpassing the quality of the human gold standard in all of the unconstrained, predictive, and controlled settings. These systems most effectively induce a natural-sounding positive reframe while also preserving the meaning of the original text. This is critical: controlled BART model scored 4.07 in Positivity and 4.27 in Fluency while also achieving the winning Meaning preservation score.\\n\\nIn contrast with BART, the few-shot systems fail to preserve the meaning of the original sentence, despite their ability to articulately induce a more positive sentiment (Positivity scores up to 4.17; Fluency scores up to 4.27). Meaning preservation is absolutely critical for this task. From these results, we can conclude that, at the present time, supervised learning may be the most viable option for achieving reliable positive reframing results. POSITIVE PSYCHOLOGY will facilitate ongoing efforts in this direction.\\n\\nQualitative Investigation\\nTable 3 shows example reframes generated by our best controlled BART model, with one example for each strategy (for a similar comparison between models, see Table 5 in Appendix D). We see that, even without explicit lexical overlap between the generation and ground truth, the model reframes can still shift the cognitive distortions and negative outlook to a more positive perspective. In each of these examples, the model does so without losing the underlying meaning of the original text. Transformer-based models appear to be capable of solving our task with reasonable success. However, success can be highly variable (as evidenced by Table 5), so there is still room for significant improvement.\\n\\n5.5 Error Analysis\\nWe manually go through 100 randomly sampled model generations by our best controlled BART model, and summarize the main error classes here. We manually investigated 100 randomly sampled model generations by our best controlled BART model, and summarize the four largest error classes here. First, 26% of generations contained (1) insubstantial changes. These were especially prominent in the neutralizing strategy where the model would swap only a few negative words, like changing the phrase \\\"I hate it\\\" to \\\"I don't like it.\\\" On the other hand, some reframed generations were so drastically modified they contained (2) contradictions to the premise (9% of instances). For example, \\\"Feel like crying, this math class is impossible to pass\\\" was transformed into \\\"This math class is hard, but I know I can pass it\\\" \u2013 a failure of meaning preservation. More concerningly, the system can generate (3) self-contradictions (6%) like the phrase, \\\"I don't like opening up to people, but I'm glad I have the courage to do it.\\\" Finally, like many other NLG systems, our system can produce (4) hallucinations (2%) with unmotivated perspectives, like mentioning a good night sleep when the original post was about nosebleeds in the bath.\\n\\n5.6 Frame Strategy Classification\\nIn Section 5.4, we observed only slight performance gains when conditioning the generation based on the ground-truth reframing strategy (Control section in Table 2). For this reason, we take a closer look at whether models can reliably understand and classify the reframe strategies underlying a given source-reframe text pair. We formulate this problem as a multi-label multi-class classification task over sentence pairs $\\\\langle s, t \\\\rangle$. Given both the source text and positive reframe target in the annotation tuple $\\\\langle s, t \\\\rangle$ from POSITIVE PSYCHOLOGY, we predict the multi-hot encoded strategy vector $\\\\psi_t = [s_{\\\\text{grow}}; s_{\\\\text{imp}}; \\\\ldots; s_{\\\\text{thank}}]$ using transformer models. We experiment with a set of state-of-the-art classifiers, including BERT (Devlin et al., 2019), RoBERTA (Liu et al., 2019), and XLNet (Yang et al., 2019).\\n\\nAs shown in Table 4, all of the classification models can learn to recognize the thankfulness, optimism, and growth mindset strategies with moderate reliability ($F_1 > 0.60$). Although XLNet model cannot identify the neutralizing strategy very well, BERT and RoBERTa models can achieve an\"}"}
{"id": "acl-2022-long-257", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Could someone just give me like $1000? It would change my life, stressing about rent, bills and food money is just the worst.\\n\\nI need to learn how to manage my money better so that I don't have to stress about rent, bills and food money.\\n\\nTime to focus on making a budget, so I don't have to stress about rent, bills, and food money.\\n\\n(b) I just went back to school today and I'm already stressed because we have midterms next week and this weekend \u2013 JAM PACKED :-(\\n\\nWell, just think by the end of next week we'll be all done with the midterms!\\n\\n(c) The caravans at Talacre beach need to sort out their check-in process, so the kids don't get stuck in the car.\\n\\nTalacre beach's check in process could do with some improvement to reduce the wait time for customers.\\n\\n(d) I'm glad that tomorrow is Friday. This week has been long, but I'm looking forward to the weekend.\\n\\nI'm glad the weekend is coming up, so I can rest.\\n\\n(e) Sometimes I get these impulses to just throw a tantrum. Like throw/break things, cry and scream.\\n\\nBut I'm strong, and I know I can handle it.\\n\\nIt's normal for me to feel overwhelmed sometimes but I know I am strong to handle and go through it.\\n\\n(f) I'm really lucky to have such a caring mum who is willing to call me every night when I'm having a hard time.\\n\\nMy mom has been calling me every night to calm me down from school. I've needed it these past few days. I'm thankful for her.\\n\\nTable 3:\\n\\n| Strategy  | BERT | RoBERTA | XLNet | Support |\\n|-----------|------|---------|-------|---------|\\n| Thankfulness | 0.71 | 0.69    | 0.71  | 109     |\\n| Neutralizing  | 0.59 | 0.60    | 0.49  | 302     |\\n| Optimism     | 0.72 | 0.71    | 0.72  | 400     |\\n| Impermanence | 0.55 | 0.55    | 0.54  | 157     |\\n| Growth       | 0.61 | 0.63    | 0.67  | 221     |\\n| Self Affirmation | 0.43 | 0.44    | 0.39  | 76      |\\n\\nTable 4: Strategy classification F1 scores\\n\\nF1 score of around 0.6. The impermanence and self-affirmation strategies appear more challenging for all three models to identify. Overall, the results here show that this task is tractable: reframe strategies are learnable by various classification models.\\n\\nThis further supports the reliability of our Positive Psychology framework, confirming what we found with human reliability metrics in Section 4.2.\\n\\nAlthough we mainly treat this frame strategy classification as a robustness check and deep dive into the role of framing strategies, this task can also be a novel NLP or computational social science application on its own, i.e., determining the positive reframing relation between a pair of sentences.\\n\\n6 Discussion and Conclusion\\n\\nThis work introduces a new and challenging NLG task called positive reframing. The objective is to construct a more positive outlook as a way of rephrasing a negative source text such that the meaning of that source is preserved. Our parallel dataset, POSITIVE PSYCHOLOGY FRAMES, will serve as a benchmark that will enable sustained work on this task. We experiment with many of the leading style-transfer models and show that these models can learn to shift from a negative to a more positive perspective using a combination of strategies from positive psychology. Importantly, the best models are fluent and effective reframing systems that can learn to largely preserve the meaning of the original text, even under a perspective shift. However, these models still struggle to generate reasonable positive perspectives, and even the best models are still prone to errors. We discuss four key error classes: insubstantial changes, contradictions to the premise, self-contradictions, and hallucinations, as shown in Error Analyses in Section 5.5. Overall, this suggests that our dataset can serve as a useful benchmark for understanding well-motivated positive reframing strategies and equipping natural language generation systems with positive perspectives.\\n\\nFuture work can dive deeper into these issues by enforcing a stronger level of semantic equivalence between the generation and the source text.\"}"}
{"id": "acl-2022-long-257", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Even with semantic equivalence constraints, it would be necessary to also allow for the injection of new positive perspectives. Methods ranging from guided sequence generation (Krause et al., 2020) or semantic attention-guided decoding (Nie et al., 2019) to pragmatic reconstruction (Shen et al., 2019) and persona consistency (Kim et al., 2020) may all be applicable in follow-up studies.\\n\\nAcknowledgements\\nThe authors would like to thank reviewers for their helpful insights and feedback. CZ is supported by the NSF Graduate Research Fellowship under Grant No. DGE-2039655 and DY is supported by the Microsoft Research Faculty Fellowship. This work is funded in part by a grant from Amazon.\\n\\nEthics\\nAnnotation.\\nWe followed the guidelines for ethical annotation practices and crowdsourcing that are outlined in (Sheehan, 2018), including paying workers a fair wage above the federal minimum. If workers contacted us with any questions or concerns, we responded promptly to them within 24 hours. In the task interface, in the header, we warned annotators that the content might be upsetting, and we gave the following recommendation: \\\"if any point you do not feel comfortable, please feel free to skip the HIT or take a break.\\\"\\n\\nDeployment.\\nAlthough this data is designed for pro-social outcomes (i.e. increasing positivity in text), there may be unexpected use-cases for this data, such as obfuscating impolite or even hateful data to avoid detection (ElSherief et al., 2021). The parallel structure of the data means it is also possible to invert the direction of the seq2seq task to introduce more negative or pessimistic perspectives into a positive source. This is not a particularly new risk, since sentiment style transfer can accomplish a similar outcome in this direction. Still, we will require interested parties to sign a data-use agreement that encourages only ethical uses of positive psychology frames.\\n\\nReferences\\nLyn Y Abramson, Lauren B Alloy, Benjamin L Hankin, Gerald J Haeffel, Donal G MacCoon, and Brandon E Gibb. 2002. Cognitive vulnerability-stress models of depression in a self-regulatory and psychobiological context.\\n\\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. Unsupervised statistical machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3632\u20133642, Brussels, Belgium. Association for Computational Linguistics.\\n\\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2895\u20132905, Florence, Italy. Association for Computational Linguistics.\\n\\nSigal G Barsade. 2002. The ripple effect: Emotional contagion and its influence on group behavior. Administrative science quarterly, 47(4):644\u2013675.\\n\\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow.\\n\\nLisa S Blackwell, Kali H Trzesniewski, and Carol Sorich Dweck. 2007. Implicit theories of intelligence predict achievement across an adolescent transition: A longitudinal study and an intervention. Child development, 78(1):246\u2013263.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\n\\nJeni L Burnette and Eli J Finkel. 2012. Buffering against weight gain following dieting setbacks: An implicit theory intervention. Journal of Experimental Social Psychology, 48(3):721\u2013725.\\n\\nDavid D Burns. 1981. Feeling good. Signet Book.\\n\\nCharles S Carver, Christina Pozo, Suzanne D Harris, Victoria Noriega, Michael F Scheier, David S Robinson, Alfred S Ketcham, Frederick L Moffat Jr, and Kimberley C Clark. 1999. How coping mediates the effect of optimism on distress: a study of women with early stage breast cancer.\\n\\nCharles S Carver, Michael F Scheier, and Suzanne C Segerstrom. 2010. Optimism. Clinical psychology review, 30(7):879\u2013889.\"}"}
{"id": "acl-2022-long-257", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-257", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-257", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-257", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-257", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Benjamin Shickel, Scott Siegel, Martin Heesacker, Sherry Benton, and Parisa Rashidi. 2020. Automatic detection and classification of cognitive distortions in mental health text. In 2020 IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE), pages 275\u2013280. IEEE.\\n\\nArielle Silverman, Christine Logel, and Geoffrey Cohen. 2013. Self-affirmation as a deliberate coping strategy: The moderating role of choice. Journal of Experimental Social Psychology, 49(1):93\u201398.\\n\\nTaetem Simms, Clayton Ramstedt, Megan Rich, Michael Richards, T Martinez, and C Giraud-Carrier. 2017. Detecting cognitive distortions through machine learning text analytics. In 2017 IEEE international conference on healthcare informatics (ICHI), pages 508\u2013512. IEEE.\\n\\nClaude M Steele. 1988. The psychology of self-affirmation: Sustaining the integrity of the self. In Advances in experimental social psychology, volume 21, pages 261\u2013302. Elsevier.\\n\\nAkhilesh Sudhakar, Bhargav Upadhyay, and Arjuna Mather. 2019. \\\"transforming\\\" delete, retrieve, generate approach for controlled text style transfer. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3269\u20133279, Hong Kong, China. Association for Computational Linguistics.\\n\\nMichael JL Sullivan, Wendy M Rodgers, and Irving Kirsch. 2001. Catastrophizing, depression and expectancies for pain and emotional distress. Pain, 91(1-2):147\u2013154.\\n\\nThomas Sy and Jin Nam Choi. 2013. Contagious leaders and followers: Exploring multi-stage mood contagion in a leader activation and member propagation (lamp) model. Organizational Behavior and Human Decision Processes, 122(2):127\u2013140.\\n\\nThomas Sy, St\u00e9phane C\u00f4t\u00e9, and Richard Saavedra. 2005. The contagious leader: impact of the leader's mood on the mood of group members, group affective tone, and group processes. Journal of applied psychology, 90(2):295.\\n\\nGregory M Walton and Shannon T Brady. 2020. \\\"bad\\\" things reconsidered. In Applications of social psychology, pages 58\u201381. Routledge.\\n\\nTong Wang, Ping Chen, John Rochford, and Jipeng Qiang. 2016. Text simplification using neural machine translation. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pages 4270\u20134271. AAAI Press.\\n\\nPhilip C Watkins, Lilia Cruz, Heather Holben, and Russell L Kolts. 2008. Taking care of business? grateful processing of unpleasant memories. The Journal of Positive Psychology, 3(2):87\u201399.\\n\\nSander Wubben, Antal van den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1015\u20131024, Jeju Island, Korea. Association for Computational Linguistics.\\n\\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. 2016. Optimizing statistical machine translation for text simplification. Transactions of the Association for Computational Linguistics, 4:401\u2013415.\\n\\nWei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. 2012. Paraphrasing for style. In Proceedings of COLING 2012, pages 2899\u20132914, Mumbai, India. The COLING 2012 Organizing Committee.\\n\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 5754\u20135764.\\n\\nDavid Scott Yeager, Rebecca Johnson, Brian James Spitzer, Kali H Trzesniewski, Joseph Powers, and Carol S Dweck. 2014. The far-reaching effects of believing people can change: implicit theories of personality shape stress, health, and achievement during adolescence. Journal of personality and social psychology, 106(6):867.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020a. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\\n\\nXianchao Zhang, Shaoping Zhu, and Wenxin Liang. 2012. Detecting spam and promoting campaigns in the twitter social network. In 2012 IEEE 12th international conference on data mining, pages 1194\u20131199. IEEE.\\n\\nXingxing Zhang and Mirella Lapata. 2017. Sentence simplification with deep reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584\u2013594, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nYi Zhang, Tao Ge, and Xu Sun. 2020b. Parallel data augmentation for formality style transfer. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3221\u20133228, Online. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-257", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, and Yann LeCun. 2018. Adversarially regularized autoencoders. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 5897\u20135906. PMLR.\\n\\nZhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353\u20131361, Beijing, China. Coling 2010 Organizing Committee.\"}"}
{"id": "acl-2022-long-257", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Data Quality-Control Methods\\n\\nWe used programmatic methods to ensure high-quality reframing annotations at submission time. Workers could not submit their task if the reframe: (1) contained fewer than 3 word types; (2) had a length less than 25% of the original text; (3) had more than 3 repetitions of a single bigram; or (4) was too similar to the original text, with a token Jaccard Similarity greater than 90%. Furthermore, we used the LanguageTool API to prompt workers to fix any grammatical mistakes in their writing. Cumulatively, these heuristics greatly improved the annotation quality. Later, in the post-processing stage, we employed additional programmatic measures as well as manual quality-checks to filter out the unsatisfactory examples. This process was iterated after each batch, with a batch size of 100. First, one of the authors manually checked any sentences where annotators had scored the original text with a postivity score greater than 3 (out of 5). If that author found that the text was not negative enough or did not contain the requisite cognitive distortions to warrant a substantial reframing, the sentence was removed from the corpus. Next, we considered all neutralizing reframes with a score less than 4 (out of 5). If the text was not effectively neutralized, we removed the sentence from the corpus. Then we considered all annotations containing the first-person pronoun you. If the text abandoned the author's first-person voice and shifted into a 3rd-person critique or commentary (e.g. \\\"I feel hopeless\\\" \u2192 \\\"you should find hope\\\"), then we removed this from the corpus. Finally, we grouped the annotations by Worker ID and, for each worker, scanned the top 10 annotations. If the annotator produced poor quality work, we removed the examples and blocked the worker from future tasks. After a last pass through the data to manually correct noticeable punctuation and grammar errors, we were left with our cleaned corpus of 8,349 reframed sentences.\\n\\nB Task Interface\\n\\nFigure 2 shows the Instructions we gave to the Amazon Mechanical Turk (MTurk) workers. Figure 3 shows the examples we displayed for each reframe strategy. Figure 4 shows the MTurk HIT interface that we used for the Section 4.1 task to collect positive reframes with their associated strategies as well as the positivity scores for both the original TEXT and the REFRAME. Figure 5 shows the interface for the Section 4.2 task where we collected new strategy labels for prior annotations to compute inter-annotator agreement scores.\\n\\nC Few-shot Learning Setting\\n\\nFollowing (Han et al., 2018; Baldini Soares et al., 2019) and others, we consider 5-shot learning. We pull 5 representative exemplars from the training set to indicate a range of strategies:\\n\\nNEGATIVE: \\\"I have a huge project due tomorrow morning. But where do I have to be, a stupid basketball game dumb\\\"\\n\\nPOSITIVE: \\\"I should plan ahead next time so that my basketball game does not conflict too closely with my projects.\\\"\"}"}
