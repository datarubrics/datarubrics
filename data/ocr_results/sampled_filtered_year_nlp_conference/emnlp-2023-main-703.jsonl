{"id": "emnlp-2023-main-703", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Method       | R1    | R2    | RL    | METEOR | BARTScore | BERTScore |\\n|--------------|-------|-------|-------|--------|-----------|-----------|\\n| Lead-2       | 0.21  | 0.06  | 0.15  | 0.26   | -3.23     | 0.86      |\\n| Lexrank      | 0.18  | 0.06  | 0.12  | 0.31   | -2.89     | 0.86      |\\n| SBERT        | 0.25  | 0.09  | 0.18  | 0.33   | -2.90     | 0.87      |\\n\\nTable 7: Results for the extractive summarization methodologies on FullFact dataset.\\n\\nGold Verdict\\nThe network has been attacked, but the idea that it was Islamic State appears to be based on speculation from a single, unnamed student.\\n\\nFullFact\\nclaim Islamic State has hacked a major computer network used by universities.\\n\\nmodel This is not the case. The network provider, Jisc, has told us that \u201cthe identity of the perpetrators remains subject to an ongoing criminal investigation\u201d\\n\\nSMP-style\\nclaim Just heard Islamic State has breached a large computer network utilized by universities!\\n\\nmodel It\u2019s definitely alarming to hear about the breach of a university\u2019s computer network. However, it\u2019s important to note that there is currently no evidence linking Islamic State to the attack. Hopefully authorities can resolve the issue quickly.\\n\\nEmotional\\nclaim As a student, I am infuriated by the news that ISIS has hacked into a major university computer network. This is a violation of our education and a threat to our safety. #stopISIS\\n\\nmodel It\u2019s important to note that the news about ISIS hacking into a major university computer network has not been confirmed by any official source. Nonetheless, it\u2019s understandable to be concerned about the potential impact this may have on our education and safety. #stopISIS\\n\\nTable 8: Examples of generated verdicts for each fine-tuning configuration tested in-domain.\\n\\nsentences of a document based on their importance by means of eigenvector centrality. We tested these approaches by extracting two sentence-long summaries from Fullfact articles. Subsequently, these summaries were evaluated against the gold verdicts. The results of this evaluation are shown in Table 7.\\n\\nSBERT outperforms both Lead-2 and LexRank for all the metrics employed.\\n\\nC.2 Fine-Tuning Configuration\\nWhen fine-tuning, PEG base was trained for 5 epochs with a batch size of 4 and a random seed set to 2022. To this end, we employed the Hugging-face Trainer using the default hyperparameter settings, with the exception of the Learning Rate values and the optimisation method. Instead, we used the Adafactor stochastic optimisation method (Shazeer and Stern, 2018) and a Learning Rate value of 3e-05. The training was performed on a single Tesla V100 GPU, while the testing was performed on a single Quadro RTX A5000 GPU. The checkpoint with minimum evaluation loss was employed for testing.\\n\\n13 https://huggingface.co/docs/transformers/main_classes/trainer\\n\\nC.3 Decoding Configuration\\nAt inference time, we employed nucleus sampling decoding strategy, setting the probability at 0.9, and repetition penalty, set at 2.0, for the verdict generation.\\n\\nD Examples of Generated Verdicts\\nIn Table 8 we report examples of verdicts generated with PEGASUS model (Zhang et al., 2020) fine-tuned on the three different stylistic versions of our dataset, i.e. FullFact, SMP-style and emotional style. In particular, we report the generations obtained in the in-domain experiments.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Countering Misinformation via Emotional Response Generation\\n\\nDaniel Russo1,2, Shane Peter Kaszefski-Yaschuk1,2, Jacopo Staiano2, Marco Guerini1\\n\\n1 Fondazione Bruno Kessler, Via Sommarive 18, Povo, Trento, Italy\\n2 University of Trento, Italy\\n{drusso, skaszefskiyaschuk, guerini}@fbk.eu, jacopo.staiano@unitn.it\\n\\nAbstract\\nThe proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive dialogue with users who spread\u2014often in good faith\u2014misleading messages. Although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in conversations on social media. Thereby, significant effort has been made to automate the use of fact-checker material in social correction; however, no previous work has tried to integrate it with the style and pragmatics that are commonly employed in social media communication. To fill this gap, we present VerMouth, the first large-scale dataset comprising roughly 12 thousand claim-response pairs (linked to debunking articles), accounting for both SMP-style and basic emotions, two factors which have a significant role in misinformation credibility and spreading. To collect this dataset we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data. We also provide comprehensive experiments showing how models trained on our proposed dataset have significant improvements in terms of output quality and generalization capabilities.\\n\\n1 Introduction\\nSocial media platforms (SMP) represent one of the most effective mediums for spreading misleading content (Lazer et al., 2018). Social media users interact with potentially false claims on a daily basis and contribute (whether intentionally or not) to their spreading. Several techniques are commonly employed to construct false but convincing content: mimicking reliable media posts, as in the case of so-called \u201cfake news\u201d; impersonating trustworthy public figures; leveraging emotional language (Basol et al., 2020; Martel et al., 2020). Among the different countermeasures adopted, one of the most employed is fact-checking, i.e. the task of assessing a claim\u2019s veracity. Although the work of professional fact-checkers is crucial for countering misinformation (Wintersieck, 2017), it has been shown that most debunking on SMP is carried out by ordinary users through direct replies to misleading messages (Micallef et al., 2020). In the literature, this phenomenon is called social correction (Ma et al., 2023).\\n\\nIn order to keep up with the massive amount of fake news constantly being produced, Natural Language Processing techniques have been proposed as a viable solution for the automation of fact-checking pipelines (Vlachos and Riedel, 2014). Researchers have focused on both the automatic\"}"}
{"id": "emnlp-2023-main-703", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"prediction of the truthfulness of a statement (a classification task, often called veracity prediction) and the generation of a written rationale (a generation task called verdict production; Guo et al., 2022).\\n\\nWhile generating a rationale is more challenging than stating a claim veracity, previous research has proven that it is more persuasive (Lewandowsky et al., 2012). Thus, automating the verdict generation process has been deemed crucial (Wang et al., 2018) as an aid for both fact-checkers and for social media users (He et al., 2023).\\n\\nAn effective explanation (verdict) is characterised as being accessible (i.e. adopting a language directly and easily comprehensible by the reader) and by containing a limited number of arguments to avoid the so-called overkill backfire effect (Lombrizo, 2007; Sanna and Schwarz, 2006).\\n\\nIn this paper, we contribute to automated fact-checking by introducing VerMouth, the first large-scale and general-domain SMP-style dataset grounded in trustworthy fact-checking articles, comprising ~12 thousand examples for the generation of personalised explanations. VerMouth was collected via an efficient and effective data augmentation pipeline which combines instruction-based Large Language Models (LLMs) and human post-editing.\\n\\nStarting from harvested journalistic-style claim-verdict pairs, we ran two data collection sessions: first, we focused on claims by rewriting them in a general SMP-style and then adding emotional/personalisation aspects to better mimic content which can be found online. Then, in the second session, the verdicts were rewritten according to predefined criteria (e.g. displaying empathy) to match the new claims obtained in the first session. This process is summarised in Figure 1.\\n\\nFinally, we tested the capabilities and robustness of generative models fine-tuned over VerMouth: automatic and human evaluation, as well as qualitative analysis of the generated verdicts, suggest that for social media claims, verdicts generated through models trained on VerMouth are widely preferred and that those models are more robust to the changing of claim style.\\n\\nOur analyses show that generated verdicts are deemed less effective if they are (i) either too long and filled with a high number of arguments or (ii) if they are excessively empathetic. Generally, despite these limitations, our results show that verdicts written in a social and emotional style hold greater sway and effectiveness when dealing with claims presented in an SMP-style.\\n\\n2 Related Work\\n\\nThe fact-checking process is comprised of two main tasks: first, given a news story, the truthfulness/veracity of a statement has to be determined; then, an explanation (verdict) has to be produced.\\n\\nIn the literature, the problem of determining a claim's veracity, has been framed as a binary (Nakashole and Mitchell, 2014; Potthast et al., 2018; Popat et al., 2018) or multi-label (Wang, 2017; Thorne et al., 2018) classification task, and occasionally addressed under a multi-task learning paradigm (Augenstein et al., 2019). Given the supervised nature of these methodologies, significant efforts have been directed towards the development of datasets for evidence-based veracity prediction, such as FEVER (Thorne et al., 2018), SciFact (Wadden et al., 2020), COVID-fact (Saakyan et al., 2021), and PolitiHop (Ostrowski et al., 2021).\\n\\nFor the more challenging task of Verdict Production, several methodologies have been explored, ranging from logic-based approaches (Gad-Elrab et al., 2019; Ahmadi et al., 2019) to deep learning techniques (Popat et al., 2018; Yang et al., 2019; Shu et al., 2019; Lu and Li, 2020). More recently, He et al. (2023) introduced a reinforcement learning-based framework which generates counter-misinformation responses, rewarding the generator to enhance its politeness, credibility, and refutation attitude while maintaining text fluency and relevancy. Previous works have shown how casting this problem as a summarization task \u2013 starting from a claim and a corresponding fact-checking article \u2013 appears to be the most promising approach (Kotonya and Toni, 2020a). Under such framing, the explanations are either extracted from the relevant portions of manually written fact-checking articles (Atanasova et al., 2020) or generated ex novo (Kotonya and Toni, 2020b); these two approaches correspond, respectively, to extractive and abstractive summarization. Finally, Russo et al. (2023) proposed a hybrid approach for the generation of explanation, by employing both extractive and abstractive approaches combined into a unique pipeline.\\n\\nExtractive and abstractive approaches suffer from known limitations: on the one hand, extractive\"}"}
{"id": "emnlp-2023-main-703", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The abstractive approach remains the most promising, also in light of the current advances in LLMs development, but the problem of collecting an adequate amount of training examples persists: the few datasets available for explanation production are limited in size, domain coverage or quality. The most commonly used datasets are either machine-generated, e.g. e-FEVER by Stammbach and Ash (2020), or silver data as for LIAR-PLUS by Alhindi et al. (2018). To the best of our knowledge, only three datasets include gold explanations, i.e. P Ub HEALT by Kotonya and Toni (2020b), the MisinfoCorrect's crowdsourced dataset by He et al. (2023), and FULLFACT by Russo et al. (2023). However, PUBHEALT and MisinfoCorrect datasets are domain-specific (respectively, health and COVID-19), and only the latter comprises textual data written in an SMPs style (informal, personal, and empathetic if required), even if limited in size (591 entries). This style is very different from a journalistic style, more direct and concise, meant for the general public. Other datasets, based on community-oriented fact-checking derived from Birdwatch (Pr\u00f6llochs, 2022; Allen et al., 2022), do not fit well our scenario, as users' corrections were proven to be often driven by political partisanship (Allen et al., 2022).\\n\\nIn this work, we introduce VerMouth, a new large-scale dataset for the generation of explanations for misinformation countering that are anchored to fact-checking articles. To build this dataset we adapted the author-reviewer pipeline presented by Tekiro\u02d8glu et al. (2020), wherein a large language model (the author component) produces novel data while humans (the reviewer) filter and eventually post-edit them (Figure 1). Differently from their approach, based on GPT-2, we used an instruction-based LLM that does not require fine-tuning and applied it to the source data taken from a popular fact-checking website. We leveraged the author-reviewer pipeline for a style transfer task, so to generate new data in an SMP-style rather than in a journalistic one.\\n\\nEach entry in our dataset includes a triplet comprising: a claim (i.e. the factual statement under analysis), a fact-checking article (i.e. a document containing all the evidence needed to fact-check a claim), and a verdict (i.e. a short textual response to the claim which explains why it might be true or false). Both the claims and the verdicts were rewritten according to the desired style using the author-reviewer pipeline. Still, given the different nature and purpose of claims and verdicts, we instructed the LLMs with different specific requirements during two different sessions of data collection. For the first session, we further considered two phases. The goal of the first phase was to obtain claims with a generic \u201cSMP-style\u201d, i.e. something that resembles a post which can be found online, rather than the more journalistic and neutral style. In the second phase, we added an emotional component to the LLM\u2019s instruction.\\n\\nWe considered Paul Ekman\u2019s six basic emotions: anger, disgust, fear, happiness, sadness, and surprise (Ekman, 1992). Verdicts were generated in a second session as responses to each newly generated claim, using the same author-reviewer pipeline but different instructions for the LLM and different guidelines for the reviewer. This was done to account for the characteristics a verdict should have, e.g. politeness, attacking the arguments and not the person, and empathy (Malhotra et al., 2022; Thorson et al., 2010). In Table 1 we give an example of the obtained outputs using our methodology.\\n\\n3.1 Source Data\\nWe leveraged FullFact data (FF henceforth; Russo et al., 2023) as a human-curated data source for the derivation of our dataset. The FF data was acquired from the FULLFACT website. FF comprises all the data published on the website from 2010 and 2021, accounting for a total of 1838 entries. FF triplets were labelled with one or more topic labels: including crime (10.50%), economy (27.80%), education (11.15%), Europe (20.46%), health (32.37%), and law (8.05%). FF data are written in a journalistic style, dry and formal, very different from the style employed on SMPs.\\n\\n3.2 Author: LLM Instructions\\nTo provide more natural and realistic claims and more personalised verdicts resembling the SMP-style, we performed data augmentation on the original data. Each entry in our dataset includes a triplet comprising: a claim (i.e. the factual statement under analysis), a fact-checking article (i.e. a document containing all the evidence needed to fact-check a claim), and a verdict (i.e. a short textual response to the claim which explains why it might be true or false). Both the claims and the verdicts were rewritten according to the desired style using the author-reviewer pipeline. Still, given the different nature and purpose of claims and verdicts, we instructed the LLMs with different specific requirements during two different sessions of data collection. For the first session, we further considered two phases. The goal of the first phase was to obtain claims with a generic \u201cSMP-style\u201d, i.e. something that resembles a post which can be found online, rather than the more journalistic and neutral style. In the second phase, we added an emotional component to the LLM\u2019s instruction.\\n\\nWe considered Paul Ekman\u2019s six basic emotions: anger, disgust, fear, happiness, sadness, and surprise (Ekman, 1992). Verdicts were generated in a second session as responses to each newly generated claim, using the same author-reviewer pipeline but different instructions for the LLM and different guidelines for the reviewer. This was done to account for the characteristics a verdict should have, e.g. politeness, attacking the arguments and not the person, and empathy (Malhotra et al., 2022; Thorson et al., 2010). In Table 1 we give an example of the obtained outputs using our methodology.\\n\\n3.1 Source Data\\nWe leveraged FullFact data (FF henceforth; Russo et al., 2023) as a human-curated data source for the derivation of our dataset. The FF data was acquired from the FULLFACT website. FF comprises all the data published on the website from 2010 and 2021, accounting for a total of 1838 entries. FF triplets were labelled with one or more topic labels: including crime (10.50%), economy (27.80%), education (11.15%), Europe (20.46%), health (32.37%), and law (8.05%). FF data are written in a journalistic style, dry and formal, very different from the style employed on SMPs.\\n\\n3.2 Author: LLM Instructions\\nTo provide more natural and realistic claims and more personalised verdicts resembling the SMP-style, we performed data augmentation on the original data. Each entry in our dataset includes a triplet comprising: a claim (i.e. the factual statement under analysis), a fact-checking article (i.e. a document containing all the evidence needed to fact-check a claim), and a verdict (i.e. a short textual response to the claim which explains why it might be true or false). Both the claims and the verdicts were rewritten according to the desired style using the author-reviewer pipeline. Still, given the different nature and purpose of claims and verdicts, we instructed the LLMs with different specific requirements during two different sessions of data collection. For the first session, we further considered two phases. The goal of the first phase was to obtain claims with a generic \u201cSMP-style\u201d, i.e. something that resembles a post which can be found online, rather than the more journalistic and neutral style. In the second phase, we added an emotional component to the LLM\u2019s instruction.\\n\\nWe considered Paul Ekman\u2019s six basic emotions: anger, disgust, fear, happiness, sadness, and surprise (Ekman, 1992). Verdicts were generated in a second session as responses to each newly generated claim, using the same author-reviewer pipeline but different instructions for the LLM and different guidelines for the reviewer. This was done to account for the characteristics a verdict should have, e.g. politeness, attacking the arguments and not the person, and empathy (Malhotra et al., 2022; Thorson et al., 2010). In Table 1 we give an example of the obtained outputs using our methodology.\\n\\n3.1 Source Data\\nWe leveraged FullFact data (FF henceforth; Russo et al., 2023) as a human-curated data source for the derivation of our dataset. The FF data was acquired from the FULLFACT website. FF comprises all the data published on the website from 2010 and 2021, accounting for a total of 1838 entries. FF triplets were labelled with one or more topic labels: including crime (10.50%), economy (27.80%), education (11.15%), Europe (20.46%), health (32.37%), and law (8.05%). FF data are written in a journalistic style, dry and formal, very different from the style employed on SMPs.\\n\\n3.2 Author: LLM Instructions\\nTo provide more natural and realistic claims and more personalised verdicts resembling the SMP-style, we performed data augmentation on the original data. Each entry in our dataset includes a triplet comprising: a claim (i.e. the factual statement under analysis), a fact-checking article (i.e. a document containing all the evidence needed to fact-check a claim), and a verdict (i.e. a short textual response to the claim which explains why it might be true or false). Both the claims and the verdicts were rewritten according to the desired style using the author-reviewer pipeline. Still, given the different nature and purpose of claims and verdicts, we instructed the LLMs with different specific requirements during two different sessions of data collection. For the first session, we further considered two phases. The goal of the first phase was to obtain claims with a generic \u201cSMP-style\u201d, i.e. something that resembles a post which can be found online, rather than the more journalistic and neutral style. In the second phase, we added an emotional component to the LLM\u2019s instruction.\\n\\nWe considered Paul Ekman\u2019s six basic emotions: anger, disgust, fear, happiness, sadness, and surprise (Ekman, 1992). Verdicts were generated in a second session as responses to each newly generated claim, using the same author-reviewer pipeline but different instructions for the LLM and different guidelines for the reviewer. This was done to account for the characteristics a verdict should have, e.g. politeness, attacking the arguments and not the person, and empathy (Malhotra et al., 2022; Thorson et al., 2010). In Table 1 we give an example of the obtained outputs using our methodology.\\n\\n3.1 Source Data\\nWe leveraged FullFact data (FF henceforth; Russo et al., 2023) as a human-curated data source for the derivation of our dataset. The FF data was acquired from the FULLFACT website. FF comprises all the data published on the website from 2010 and 2021, accounting for a total of 1838 entries. FF triplets were labelled with one or more topic labels: including crime (10.50%), economy (27.80%), education (11.15%), Europe (20.46%), health (32.37%), and law (8.05%). FF data are written in a journalistic style, dry and formal, very different from the style employed on SMPs.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The vaccine manufacturers do not have liability. BREAKING: According to a recent court ruling, vaccine makers can\u2019t be held accountable for any issues that may arise from their products.\\n\\nAs someone who has lost a loved one due to vaccine complications, it makes my blood boil to think that the vaccine manufacturers have zero liability. How is this fair? They should be held accountable for any harm caused by their products.\\n\\nCovid-19 vaccine manufacturers are immune to some, but not all, civil liability. Actually, while it\u2019s true that vaccine manufacturers are protected from some liability, they are still subject to civil liability for certain issues. It\u2019s very important to be aware of this fact.\\n\\nI can\u2019t even begin to imagine the pain your loss has caused you. It\u2019s important to note that Covid-19 vaccine manufacturers do have some immunity from civil liability, but this is not absolute. Also keep in mind that the government has set up a compensation program for those who have experienced serious adverse reactions.\\n\\nI hope this information helps, and I hope you do better soon.\\n\\nTable 1: An example of claim (first row) and verdict (second row): original versions from FullFact, then SMP-style and emotional versions, obtained via our author-reviewer approach. The original claim and verdict have a dry and neutral style, while the variations we obtain resemble the content found on SMPs with hashtags, sensationalist expressions and informal style (in yellow). The emotional claim clearly contains emotional expressions, as well as, sometimes, personal stories grounding the emotional component (red) while the verdict also contains the qualities required by a social response such as politeness and empathy (green).\\n\\nThis approach has the advantage of avoiding privacy concerns (since no real SMP data is collected) and prevents dataset ephemerality (Klubicka and Fern\u00e1ndez, 2018). As an author module, we tested instruction-based LLMs such as GPT3 (Brown et al., 2020) and ChatGPT.\\n\\nTo set the proper prompt/instruction, we run preliminary experiments by testing several textual variants, providing the annotators with a sample of the data generated for quality evaluation. We evaluated the prompts according to the following factors: generalisability, variability, originality, coherence, and post-editing effort. Details on configurations and methodology of the quality evaluation are given in Appendix A.1. The final instructions for claim and verdict generation are reported in Table 2.\\n\\nInstructions for claim generation: PROMPT (A) for SMP-style; PROMPT (B) for the emotional style.\\n\\nInstructions for verdict generation: PROMPT (C).\\n\\nSee Appendix B for the full guidelines.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.4 Session 1: Claim Augmentation\\n\\nThrough our LLM-based pipeline and the available FF claims, two sets of claims were generated: the \u201cSMP-style\u201d claims, and the \u201cemotional style\u201d claims. What makes a claim \u201cgood\u201d can often be counter-intuitive since they do not need to be truthful. The generated claims exhibited specific characteristics which were accounted while creating the post-editing guidelines. Some of the most relevant phenomena and the resulting post-editing actions follow:\\n\\n1. The generated texts occasionally copy the entire original claim verbatim, despite the model was prompted not to. In these instances, manual paraphrasing is necessary.\\n\\n2. Sometimes, the generated claim debunks the original claim. For example, if the original claim says that \u201cvaccines do not work\u201d, but the generated claim says the opposite, then it needs to be changed to match the intent of the original claim.\\n\\n3. Hallucinated information is usually undesired. However, since the claims might be misleading or completely inaccurate, hallucinations can actually be useful for our task, making the claim seem more authoritative or convincing by adding new false facts and arguments. For example, the model rewrote \u201cAlmost 300 people under 18 were flagged up ...\u201d in \u201c291 young people identified ...\u201d making the potential author of the post appear knowledgeable due to the precision in the stated number.\\n\\n4. For emotional claims specifically, we need to ensure that the emotion matches the claim and is reasonable. For example, being happy that people are dying from vaccines is not something reasonable. A plausible correction can be that a person is \u201chappy as people are finally seeing the truth about the fact that the vaccine is causing deaths\u201d. If the correction is not possible, then the claim can be discarded.\\n\\n3.5 Session 2: Verdict Augmentation\\n\\nThe verdict augmentation process was conducted similarly to Session 1. However, in this case, the prompt included both the original FF verdict and the post-edited claim, since the generated verdicts are intended to be a specific response to it. A different approach was required when post-editing verdicts, as they must follow stricter standards of quality: they have to be always true, address the arguments made by the claim, avoid political polarisation, and they must be empathetic and polite. It is important to highlight that LLM was required to rewrite a gold verdict and not to write a debunking from scratch, as can be seen in Table 2. For this reason, the main task of the annotators was to check whether there were discrepancies between the gold and the generated verdicts, and, in case, to correct them. We took for granted that the gold verdicts are trustworthy (as they were manually written by professional fact-checkers), thus we are sure that a new verdict that differs only in style but not in content is trustworthy too.\\n\\nSome of the characteristics of the generated verdicts as well as actions which must be taken to post-edit them effectively are listed below.\\n\\n1. Recurrent patterns, e.g. \u201cthank you for.. \u201d, \u201cI understand your concern about... \u201d, \u201cIt\u2019s important to... \u201d, were reworded or removed entirely.\\n\\n2. The generated verdicts often include \u201ccalls to action\u201d, i.e. exhortative sentences which call upon the reader to take some form of action (e.g., \u201cit\u2019s important to continue advocating for fair treatment and stability in employment.\u201d). To avoid potentially polarising verdicts \u2013 as the main objective of a verdict is to simply provide factual arguments in favour or against a given claim \u2013 it was also necessary to neutralise or avoid overtly political or polarising calls to action.\\n\\n3. Consistency regarding who exactly is \u2018responding\u2019 to a claim was necessary. Sometimes the first-person plural was used (\u201cwe understand that you\u2019re... \u201d), and in other cases, the first-person singular was used (\u201cI agree that... \u201d). We decided that the verdicts should appear to have been written by a single person, rather than a group, as we considering the case of social correction by single users. In some instances, the first-person plural can be used, but only when referring to a group that includes both the writer and the reader (\u201cas a society, we should... \u201d).\\n\\n4. Sometimes the generated verdicts lack information or statistics contained in the original verdict. If whatever is missing is crucial to the argument being made, then including it is\"}"}
{"id": "emnlp-2023-main-703", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Average length of articles, claims, and verdicts in our dataset.\\n\\n3.6 Dataset Analysis\\n\\nAfter the data augmentation process, we obtained ~12 thousand examples (11990 claim-verdict pairs, 1838 written in a general SMP-style and 10152 also comprising an emotional component). Post-editing details can be found in Appendix A.2. In Table 3 we report the average number of words, sentences, and BPE tokens for the articles, the claims and the verdicts of each stylistic version of our dataset.\\n\\n6 Then, to quantitatively assess the quality of the post-edited data we employed two measures: the Human-targeted Translation Edit Rate (HTER; Snover et al., 2006) and the Repetition Rate (RR; Bertoldi et al., 2013).\\n\\nHTER measures the minimum edit distance, i.e. the smallest amount of edit operations required, between a machine-generated text and its post-edited version. HTER values greater than 0.4 account for low-quality generations; in this case, writing a text anew or post-editing would require a similar effort (Turchi et al., 2013). In Table 4 we report the HTER of the post-edited claims and verdicts.\\n\\n6 We employed Spacy (https://spacy.io) for extracting words and sentences, and the sentence-piece tokenizer used in Pegasus (Zhang et al., 2020) for the BPE tokens.\\n\\n7 HTER values were averaged over the entire samples under analysis (including the non-post-edited data).\\n\\nRR measures the repetitiveness of a text, by computing the geometric mean of the rate of n-grams occurring more than once in it. A fixed-size sliding window while processing the text ensures that the differences in documents' size do not impact the overall scores. For our analysis, we computed the rate of word n-grams (with n ranging from 1 to 4) with a sliding window of 1000 words. Following previous works (Bertoldi et al., 2013; Tekiro\u011flu et al., 2020), the RR values reported in this paper range between 0 and 100.\\n\\nAs can be seen in Table 4, the HTER values computed on the claims are very low, always less than 0.1, suggesting good quality machine-generated texts. In particular, the data generated according to a general SMP-style were less post-edited. Machine-generated claims, which comprise also an emotional component, required more post-editing than SMP-style claims, as shown by the higher HTER values.\\n\\nMoreover, HTER values for the post-edited verdicts are higher than those for the claims. This can be explained by the need to ensure verdicts' truthfulness, by adjusting or removing calls to action, possible model hallucinations or repeated patterns.\\n\\nHowever, even though HTER values vary across the single emotions, on average they are lower than the 0.4 threshold. This is corroborated by the RR of the verdicts: a substantial decrease in repetitiveness was obtained after post-editing at the expense of more editing operations. The average RR for the data comprising an emotional component is comparable to the one obtained on the corresponding claims. However, this does not apply to the SMP-style data: in this case, the RR for the claims is more than 2 points lower than that on the verdicts. This can be explained by the tendency of the LLMs employed to produce more recurrent patterns when the instructions are enriched with specific details, such as the emotional state.\\n\\nIn summary, our pipeline facilitated the acquisition of high-quality claims and verdicts.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: For each dataset (column-wise): number of samples, HTER and Repetition Rate (RR) values for both the post-edited claims and verdicts.\\n\\nThe quality of the generated verdicts was assessed with both an automatic and a human evaluation. We present and discuss the results in Section 5.\\n\\n4.1 Extractive Approaches\\nUnder an extractive summarization framing, we defined the task of verdict generation as that of extracting 2-sentence long verdicts from FullFact articles, and 3-sentences long for the SMP and emotional data. Such lengths were decided according to the averaged length of the verdicts, reported in Table 3. We employed SBERT (Reimers and Gurevych, 2019), a BERT-based siamese network used to encode the sentences within an article as well as the claim and to score their similarity via cosine distance (SBERT-k henceforth, with k denoting the number of sentences). Under our experimental design, the top-k sentences with a latent representation closer to that of the claim would be selected to construct the output verdict.\\n\\nWe used a semantic retrieval approach rather than other common unsupervised methods for extractive summarization, such as LexRank (Erkan and Radev, 2004), since the latter has no visibility into the claim itself. Nonetheless, we tested those approaches in preliminary analyses (reported in Appendix C.1) and verified that the performance was significantly lower than that obtained with SBERT. We will consider SBERT-k as a baseline for the following experiments.\\n\\n4.2 Abstractive Models\\nWe employed PEGASUS (Zhang et al., 2020), a language model pretrained with a summarization objective. In all the experiments, the length of the articles was reduced through extractive summarization with SBERT (Reimers and Gurevych, 2019) in order to fit the maximum input length of the model (i.e. 1024). We opted for SBERT in light of its higher performances with respect to other extractive methods (see Appendix C.1). We explored four different configurations and tested them on all the versions of our dataset, i.e. FullFact, SMP, and emotional version (see Appendix C.2 and C.3 for fine-tuning and decoding details):\\n\\n- **PEG base**: Zero-shot experiments with PEGASUS fine-tuned on CNN/Daily Mail, with the goal of summarizing the debunking article.\\n- **PEG FF**: Fine-tuning of PEG base on FF data. A claim and its corresponding debunking article were concatenated and used as input, with the verdict as target.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Results for each configuration, for both the in-domain and cross-domain experiments.\\n\\n- **PEGsmp**: Fine-tuning of PEG base on the SMP-style data. Training input data were processed as in the PEG FF configuration.\\n- **PEGemo**: Fine-tuning of PEG base on the emotional data. Training input data were as in the PEG FF configuration.\\n\\n### 5 Results\\n\\nWe assessed the potential of our proposed dataset in terms of generation capabilities via both automatic and human evaluation.\\n\\n#### 5.1 Automatic Evaluation\\n\\nWe adopted the following automatic measures:\\n\\n- **ROUGE**: Recall-Oriented Understudy for Gisting Evaluation (Lin, 2004) measures the overlap between two distinct texts by examining their shared units. We include ROUGE-N ($RN$, $N=1,2$) and ROUGE-L ($RL$), a modified version that considers the longest common substring (LCS) shared by the two texts.\\n- **METEOR**: Determined by mapping the unigrams in the generated verdict with those in the reference gold verdict, accounting for factors such as stemming, synonyms, and paraphrastic matches.\\n- **BERTScore**: Computes token-level semantic similarity between two texts using BERT (Devlin et al., 2019).\\n- **BARTScore**: Built upon the BART model (Lewis et al., 2020), frames the evaluation as a text generation task by computing the weighted probability of the generation of a target sequence given a source text.\\n\\nTable 5 reports the results of all the experiments we carried out. For all metrics, the higher scores were obtained after fine-tuning the model in both in-domain and cross-domain experimental scenarios. Indeed, zero-shot experiments with PEG base resulted in scores even lower than the SBERT baseline. This suggests that summarising the article is not enough by itself to obtain quality verdicts.\\n\\nInterestingly, the PEGASUS models fine-tuned on the SMP-style and emotional style samples appear to generalise better. In fact, when tested against the other test subsets, they have a similar overall performance and a smaller decrease in cross-domain settings compared to PEG FF.\\n\\n#### 5.2 Human Evaluation\\n\\nWe adapted the methodology proposed by He et al. (2023) to our scenario: three participants were asked to analyse 180 randomly sampled items; each item comprises the claim and three verdicts produced by PEG FF, PEGsmp, and PEGemo over that claim, compounding to 60 claims for each stylistic configuration present in VerMouth.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We asked to evaluate the model-generated verdicts by answering the following question: Consider a social media post, which response is better when countering the possible misinformation within the post (the claim)? Rank the following responses from the most effective (1) to the least effective (3). Ties are allowed.\\n\\nAfter collecting the responses, we run a brief interview to understand the main elements that drove the annotators' decisions. These interviews highlighted some crucial aspects: (i) verdicts comprising too much data and information induced a negative perception of their effectiveness (overkill backfire effect); (ii) verbose explanations are generally not appreciated; (iii) there was a positive appreciation for the empathetic component in the response, however (iv) \u201cover-empathising\u201d was negatively perceived.\\n\\nTable 6 shows how PEG\\\\textsubscript{FF} is highly preferred for in-domain cases, possibly because it avoids (i) excessively long verdicts and (ii) the stylistic/empathetic discrepancy between a journalistic claim from FF and other systems' output with a more SMP-like style. Still, PEG\\\\textsubscript{FF} performs the worst in cross-domain settings. Conversely, PEG\\\\textsubscript{smp} and PEG\\\\textsubscript{emo} are somewhat more stable (consistently with the automatic evaluation). In general, style and emotions in the verdict have a greater impact if the starting claim has style and emotions. Users reported that empathy mitigates the length effect. From a manual analysis, PEG\\\\textsubscript{smp} shows the ability to provide slightly empathetic responses, so it sometimes ended up being preferred for its empathetic (but not overly so) responses.\\n\\nTo sum up: for social claims, which resemble those found online, social verdicts are widely preferred to FullFact journalistic claims.\\n\\n|     | FF | SMP | EM |\\n|-----|----|-----|----|\\n| 1.55| 2.08| 2.00|\\n| 1.93| 1.97| 1.75|\\n| 1.90| 1.92| 1.80|\\n\\nTable 6: Average rankings obtained via human evaluation. The ranks range from 1 (most effective) to 3 (least effective). The best results are highlighted in blue.\\n\\n6 Conclusion\\n\\nProducing a verdict, i.e. a factual explanation for a claim's veracity and doing so in a constructive and engaging manner is a very demanding task. On social media platforms, this is usually done by ordinary users, rather than professional fact-checkers. In this context, automated fact-checking can be very beneficial. Still, to fine-tune and/or evaluate NLG models, high-quality datasets are needed. To address the lack of large-scale and general-domain SMP-style resources (grounded in trustworthy fact-checking articles) we created VerMouth, a novel dataset for the automatic generation of personalised explanations. The provided resource is built upon debunking articles from a popular fact-checking website, whose style has been altered via a collaborative human-machine strategy to fit realistic scenarios such as social-media interactions and to account for emotional factors.\\n\\nLimitations\\n\\nThere are some known limitations of the work presented in this paper. First, the resource is limited to only English language only; nonetheless, the author-reviewer approach we adopted for data collection is language-agnostic and can be transferred as-is to other languages, assuming the availability of (i) a seed set of \\\\texttt{<article,claim,verdict>} triples for (or translated in) the desired target language, and (ii) an instruction based LLM for the desired language. Furthermore, this dataset is limited in the sense that it only covers a particular style of language most commonly seen on specific Social Media Platforms \u2013 short and informal posts (such as those typically found on Twitter and Facebook), rather than longer or more formal posts (which may be more typical on sites such as Reddit or on internet forums). We leave efforts to tackle such limitations to future iterations of this work.\\n\\nEthics Statement\\n\\nThe debate on the promise and perils of Artificial Intelligence, in light of the advancements enabled by LLM-based technologies, is ongoing and extremely polarising. A common concern across the community is the potential undermining of democratic processes when such technologies are coupled with social media and used with malicious/destabilising intent. With this work, we provide a resource aiming at countering such nefarious dynamics while integrating the capabilities of LLMs for social good.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This work was partly supported by the AI4TRUST project - AI-based-technologies for trustworthy solutions against disinformation (ID: 101070190).\\n\\nAcknowledgements\\n\\nReferences\\n\\nNaser Ahmadi, Joohyung Lee, Paolo Papotti, and Mohammed Saeed. 2019. Explainable fact checking with probabilistic answer set programming. arXiv preprint arXiv:1906.09198.\\n\\nTariq Alhindi, Savvas Petridis, and Smaranda Mure\u0219an. 2018. Where is your evidence: Improving fact-checking by justification modeling. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 85\u201390, Brussels, Belgium. Association for Computational Linguistics.\\n\\nJennifer Allen, Cameron Martel, and David G Rand. 2022. Birds of a feather don't fact-check each other: Partisanship and the evaluation of news in twitter's birdwatch crowdsourced fact-checking program. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI '22, New York, NY, USA. Association for Computing Machinery.\\n\\nPepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. Generating fact checking explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7352\u20137364, Online. Association for Computational Linguistics.\\n\\nIsabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, and Jakob Grue Simonsen. 2019. MultiFC: A real-world multi-domain dataset for evidence-based fact checking of claims. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4685\u20134697, Hong Kong, China. Association for Computational Linguistics.\\n\\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372.\\n\\nMelisa Basol, Jon Roozenbeek, and Sander Van der Linden. 2020. Good news about bad news: Gamified inoculation boosts confidence and cognitive immunity against fake news. Journal of cognition, 3(1).\\n\\nNicola Bertoldi, Mauro Cettolo, and Marcello Federico. 2013. Cache-based online adaptation for machine translation enhanced computer assisted translation. In Proceedings of Machine Translation Summit XIV: Papers, Nice, France.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nPaul Ekman. 1992. Facial expressions of emotion: New findings, new questions. Psychological Science, 3(1):34\u201338.\\n\\nG\u00fcnes Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Int. Res., 22(1):457\u2013479.\\n\\nMargherita Fanton, Helena Bonaldi, Serra Sinem Tekiro\u011flu, and Marco Guerini. 2021. Human-in-the-loop for data collection: a multi-target counter narrative dataset to fight online hate speech. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3226\u20133240, Online. Association for Computational Linguistics.\\n\\nMohamed H. Gad-Elrab, Daria Stepanova, Jacopo Urbani, and Gerhard Weikum. 2019. Exfakt: A framework for explaining facts over knowledge graphs and text. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM '19, page 87\u201395, New York, NY, USA. Association for Computing Machinery.\\n\\nZhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. A survey on automated fact-checking. Transactions of the Association for Computational Linguistics, 10:178\u2013206.\\n\\nBing He, Mustaque Ahamad, and Srijan Kumar. 2023. Reinforcement learning-based counter-misinformation response generation: a case study of covid-19 vaccine misinformation. In Proceedings of the ACM Web Conference 2023, pages 2698\u20132709.\\n\\nFilip Klubicka and Raquel Fern\u00e1ndez. 2018. Examining a hate speech corpus for hate speech detection and popularity prediction. In 4REAL 2018 Workshop on\"}"}
{"id": "emnlp-2023-main-703", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neema Kotonya and Francesca Toni. 2020a. Explainable automated fact-checking: A survey. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5430\u20135443, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nNeema Kotonya and Francesca Toni. 2020b. Explainable automated fact-checking for public health claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7740\u20137754, Online. Association for Computational Linguistics.\\n\\nDavid M. J. Lazer, Matthew A. Baum, Yochai Benkler, Adam J. Berinsky, Kelly M. Greenhill, Filippo Menczer, Miriam J. Metzger, Brendan Nyhan, Gorndon Pennycook, David Rothschild, Michael Schudson, Steven A. Sloman, Cass R. Sunstein, Emily A. Thorson, Duncan J. Watts, and Jonathan L. Zittrain. 2018. The science of fake news. Science, 359(6380):1094\u20131096.\\n\\nStephan Lewandowsky, Ullrich K. H. Ecker, Colleen M. Seifert, Norbert Schwarz, and John Cook. 2012. Misinformation and its correction: Continued influence and successful debiasing. Psychological Science in the Public Interest, 13(3):106\u2013131. PMID: 26173286.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\\n\\nTania Lombrozo. 2007. Simplicity and probability in causal explanation. Cognitive psychology, 55(3):232\u2013257.\\n\\nYi-Ju Lu and Cheng-Te Li. 2020. GCAN: Graph-aware co-attention networks for explainable fake news detection on social media. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 505\u2013514, Online. Association for Computational Linguistics.\\n\\nYingchen Ma, Bing He, Nathan Subrahmanian, and Srijan Kumar. 2023. Characterizing and predicting social correction on Twitter. arXiv preprint arXiv:2303.08889.\\n\\nPranav Malhotra, Kristina Scharp, and Lindsey Thomas. 2022. The meaning of misinformation and those who correct it: An extension of relational dialectics theory. Journal of Social and Personal Relationships, 39(5):1256\u20131276.\\n\\nCameron Martel, Gordon Pennycook, and David G Rand. 2020. Reliance on emotion promotes belief in fake news. Cognitive research: principles and implications, 5:1\u201320.\\n\\nNicholas Micallef, Bing He, Srijan Kumar, Mustaque Ahamad, and Nasir D. Memon. 2020. The role of the crowd in countering misinformation: A case study of the COVID-19 infodemic. CoRR, abs/2011.05773.\\n\\nNdapandula Nakashole and Tom M. Mitchell. 2014. Language-aware truth assessment of fact candidates. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1009\u20131019, Baltimore, Maryland. Association for Computational Linguistics.\\n\\nWojciech Ostrowski, Arnav Arora, Pepa Atanasova, and Isabelle Augenstein. 2021. Multi-hop fact checking of political claims. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 3892\u20133898. International Joint Conferences on Artificial Intelligence Organization. Main Track.\\n\\nKashyap Popat, Subhabrata Mukherjee, Andrew Yates, and Gerhard Weikum. 2018. DeClarE: Debunking fake news and false claims using evidence-aware deep learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 22\u201332, Brussels, Belgium. Association for Computational Linguistics.\\n\\nMartin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, and Benno Stein. 2018. A stylometric inquiry into hyperpartisan and fake news. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 231\u2013240, Melbourne, Australia. Association for Computational Linguistics.\\n\\nNicolas Pr\u00f6llochs. 2022. Community-based fact-checking on Twitter\u2019s Birdwatch platform. In Proceedings of the International AAAI Conference on Web and Social Media, volume 16, pages 794\u2013805.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics.\\n\\nDaniel Russo, Serra Sinem Tekiro\u011flu, and Marco Guerini. 2023. Benchmarking the Generation of Fact Checking Explanations. Transactions of the Association for Computational Linguistics, 11:1250\u20131264.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-703", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Data Augmentation Details\\n\\nA.1 Prompt and Model Selection\\n\\nThe first step when it comes to effectively leveraging LLMs for one's specific use case is prompt engineering. In our case, a carefully crafted prompt allows LLMs to produce quality claims and verdicts systematically, minimising the amount of post-editing required. Since ChatGPT's API was not yet available to the public when we began our experiments, the initial prompt testing was performed using GPT3.\\n\\nInitial tests focused on finding the optimal prompt and parameters for our specific use case. The parameters we tested were the temperature $T$ and the cumulative probability $p$ for nucleus sampling (Top-P).\\n\\nThe $T$ hyperparameter ranges between 0 and 1 and controls the amount of randomness used when sampling: a value of 0 corresponds to a deterministic output (i.e. picking exclusively the top-probability token from the vocabulary); conversely, a value of 1 provides maximum output diversity. Finding a balance between not being overly deterministic while remaining coherent was the goal when testing different temperature values - 0.7 and 1 were used when performing these initial tests.\\n\\nThe Top-P parameter also ranges between 0 and 1 and determines how much of the probability distribution of words is considered during generation. It was necessary to find a value which was not overly deterministic and that avoid using very rare words that may reduce coherence. During these initial tests, we tried using the default value of 0.5 as well as a Top-P value of 1, which includes all words in the probability distribution. We also tested using a Top-P of 0.9.\\n\\nDetermining the \\\"optimal\\\" prompt and parameters can be challenging because what makes a \\\"good\\\" personalised claim or verdict is subjective. Several factors were taken into account when selecting our prompts and parameters:\\n\\n1. Generalisability: Do they perform well on a variety of claims, or do they only work well on specific ones (i.e.: it produces quality output for Covid-related claims, but struggles with claims about Brexit)?\\n2. Variability: Do the generated claims and verdicts vary between one another, or do they all follow similar patterns?\\n3. Originality: Do the generated claims and verdicts resemble too much the original? Do they contain the original claim or verdict verbatim?\\n4. Coherence: Do the generated claims and verdicts make sense? Are they coherent? Are they saying what the original claims and verdicts are, or do they instead say something unrelated?\\n5. Amount of Post-Editing: On average, how much post-editing is required for each of the generated claims and verdicts? What proportion of these claims and verdicts requires any post-editing at all?\\n\\nEventually, we opted for the following parameters: a temperature of 1 and a Top-P of 0.9. These parameters were used with OpenAI's Davinci model during initial tests. No changes were made when we switched to ChatGPT after their public API was released.\\n\\nA.2 Post-Editing\\n\\nThe post-editing of the data and the prompt evaluation were carried out by two annotators either native English speakers or fluent in English. The time needed for post-editing was heavily dependent on the type of data (claim versus verdict) and on the configuration (SMP-style versus emotional style). On average, the annotators were able to process 250 SMP-style claims and 200 emotional claims per hour. For the emotional data, the time required to post-edit varied greatly depending on the emotion. Since verdicts are usually longer than claims and much more constrained, the post-editing process was much longer: on average, 150 SMP-style verdicts and 70 emotional verdicts were able to be post-edited per hour.\\n\\nNot every claim and verdict required post-editing. Only ~19% of the generated SMP-style claims and ~68% of the emotional style claims were post-edited. Keep in mind that some of the generated emotional claims were discarded if a specific emotion and the content of the claim were mismatched, as this resulted in forced and unnatural combinations. Figure 2 displays the distribution of post-edited, non-post-edited, and discarded claims.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"amount of discarded data, but they also required the most post-editing.\\n\\nAs mentioned before, post-editing verdicts were a much longer process. Since verdicts are subject to stricter standards of quality (because they must be truthful and polite, for example) and are much longer on average, many more of them required post-editing. Figure 3 shows this disparity: for some emotions such as disgust and fear, there were fewer than 10 verdicts which did not require at least minimal post-editing. SMP-style verdicts also required post-editing at a much higher rate than SMP-style claims, although less than emotional style verdicts. In total, there were 1838 SMP-style verdicts and 2609 emotional style verdicts, resulting in post-editing rates of 91.4% and 96.3% respectively.\\n\\nFigure 2: Graph representing the number of ChatGPT generated claims that were deleted, post-edited, or not post-edited\\n\\nFigure 3: Graph representing the number of ChatGPT generated verdicts that were post-edited versus those which were not post-edited\\n\\nA.3 Timing benefits of post-editing\\n\\nWe carried out an extra experiment to assess whether post-editing machine-generated data is more effective in terms of time than writing new data from scratch. To this end, we provided one of the annotators with 60 claims and asked to write from scratch new tweets, 30 in SMP-style and 30 emotional tweets. In both cases, it took the annotator (expert in the field) around 23 minutes to create 30 new tweets (thus, roughly 80 claims per hour as compared to the 250 SMP-style and 200 emotional tweets obtained with our pipeline). If in the creation of claims the time differences are considerable, we assume that this also applies to verdicts, which is a task that requires more constraints.\\n\\nB Detailed Guidelines\\n\\nB.1 Claim Guidelines\\n\\n1. Generated claims copying original claims verbatim: Sometimes the wording of the original claim is copied verbatim in the generated claim. These should be rewritten to avoid resembling the original dataset. Determining whether a generated claim resembles the original claim \u201ctoo much\u201d can be subjective, so discretion must be used.\\n\\n2. Reoccurring Patterns: Since the SMP-style claims have a lot more freedom to decide what sort of tone to adopt, they are much more diverse. With emotional style claims, the emotional component is an extra constraint which is applied during generation. This means that there are often reoccurring patterns which appear in the resulting generated claims: \u201cI\u2019m livid!\u201d \u201c\u2018So sad to hear that X\u201d \u201cDisgusting!\u201d etc. If a pattern can be removed while preserving the overall emotional intent, then it is better to remove it entirely. Conversely, if removing a pattern also removes any \u201cemotion\u201d from the generated claim, then rewriting is preferred. In rare cases, the pattern can be kept, keeping in mind that too many occurrences may result in degraded performance during training.\\n\\n3. Hashtags: Due to the prompt used, hashtags often appear in the generated claims. We noted two different phenomena which may occur and which require post-editing:\\n\\n(a) Debunking hashtags: There are some occurrences where a hashtag debunks a claim or works against the claim\u2019s intent. If a claim is about how vaccines are not effective, having the hashtag \u201c#VaccinesSaveLives\u201d is not appropriate. These hashtags...\"}"}
{"id": "emnlp-2023-main-703", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"can either be removed or edited to match the original intent.\\n\\n(b) Unnecessary hashtags: There are some hashtags which are so vague that they diminish the overall quality of the claim (such as \\n\\nemotional claims specifically, the emotion given in the prompt is turned into a hashtag (such as \\n\\nAny hashtags which fit these criteria are to be removed.\\n\\n4. Generated claims debunking original claims: There are some instances where the generated claim actually debunks the original claim. These must be changed to reflect the intent behind the original claim. For example: if the original claim says that \\n\\n\\\"wearing masks causes dementia and hypoxia\\\" and the generated claim says \\n\\n\\\"False info alert: wearing a mask doesn't cause dementia and hypoxia\\\" it should be rewritten to match the original claim.\\n\\n5. Dates and places: The original claims contained many references to dates and places. These could be vague references (\\\"last year\\\", \\\"in our nation\\\", etc.) or specific references (\\\"21 July 2021\\\", \\\"in England\\\", etc.). Any dates and places in the generated claims should match the original claim's level of specificity.\\n\\n6. Hallucinations: Since claims do not necessarily need to be true, hallucinations can often be beneficial. Consider an example where the original claim says \\n\\n\\\"almost 300 people have died from the vaccine\\\" but the generated claim contains a hallucination which states that \\n\\n\\\"291 people have died from the vaccine\\\" - this number is more specific, and this may give off the impression that the person knows what they are talking about.\\n\\n7. General formatting issues: While rare, there are cases in which grammatical errors, typos, malformed hashtags (such as \\n\\n\\\"#endrape culture\\\") or other formatting issues occur in generated claims. These should simply be corrected.\\n\\nB.2 Verdict Guidelines\\n\\n1. Reoccurring Patterns: As with generated claims, there are often patterns which occur often in generated verdicts. These should be removed or rewritten. Some examples of common patterns include \\n\\n\\\"thank you for X\\\", \\n\\n\\\"I understand that you feel X\\\", \\n\\n\\\"Let's continue to follow the recommended guidelines\\\", etc.\\n\\n2. Calls to action: As stated before, a \\\"call to action\\\" is a phrase or sentence which, as the name implies, calls upon the reader of the verdict to take action in some way. For example: \\n\\n\\\"I understand your frustration, and while the proportion of BME students at Oxbridge has actually increased, I agree that more needs to be done to address the lack of diversity from disadvantaged areas. It's important that we continue examining the root causes of this inequality and work towards equal opportunities for all. #diversitymatters #educationforall\\\"\\n\\nTo avoid overtly political or polarising verdicts, many considerations need to be kept in mind when a call to action in a generated verdict is encountered.\\n\\n(a) Is the call to action well-integrated into the verdict? - If a call to action does not make a meaningful contribution to the overall quality of the verdict, then it will be removed. An example of a poorly-integrated call to action is \\n\\n\\\"let's focus on promoting peaceful and respectful discourse. \\\" This call to action is broad and vague and should be removed.\\n\\n(b) Is the call to action political or polarising? - One must determine whether or not the call to action is actually political or polarising. With certain topics, it is simply impossible to avoid having a call to action which contains political elements (such as a claim about a politician, or a new law). We decided upon two possible approaches one can take when post-editing political calls to action.\\n\\nThe common sense approach (or the \\\"reasonable person\\\" approach) is employed for a call to action expressing a political opinion on which the most agree (such as \\n\\n\\\"demanding transparency and accountability from our government\\\"). This call to action can be kept.\\n\\nThe empathetic approach is employed when dealing with opinions or thoughts that simply have no \\\"correct\\\" answers, but rely on one's own beliefs. It was decided that the call to action should be changed to empathise with the claim writer's beliefs.\"}"}
{"id": "emnlp-2023-main-703", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\" Without agreeing or disagreeing with them.\\n\\nFor example, if a claim expresses pro-life opinions, and a call to action such as \u201cit's important to acknowledge the magnitude of lives affected by this issue\u201d exists, changing it to \u201cit's important to acknowledge the magnitude of lives affected by this issue no matter what you believe\u201d is empathetic, but also explicitly avoids picking one side or the other in a polarising situation like this.\\n\\n(c) How strong is a call to action, and who is the focus?\\n\\n- Different actions require different amounts of effort. If a call to action asks for too much from someone, then it should be changed. Asking someone, for example, to \u201cadvocate for stricter testing protocols\u201d may be asking too much of them, but asking them to \u201chope that stricter testing protocols are implemented\u201d is not.\\n\\nIf a call to action is too strong, then it can either be weakened or neutralised. Weakening a call to action involves changing what is expected of the reader of the verdict: rather than \u201cwe must personally take action to end child poverty immediately,\u201d one can post-edit the call to action to say \u201clet\u2019s try and do our part together to hopefully end child poverty one day.\u201d Neutralising a call to action takes the focus off the reader entirely. This involves either putting the onus on someone else who may be more capable of solving the issue or not demanding action from anyone at all. Thus, \u201cwe must continue to keep an eye out for potential side effects of the vaccines\u201d can be rewritten to \u201cthe experts must continue to keep an eye out for potential side effects of the vaccines.\u201d\\n\\nAn example of not demanding action from anyone at all is: \u201cwe must continue to advocate for those struggling to make ends meet.\u201d It can be post-edited as \u201ccompassion and understanding for those struggling to make ends meet is crucial.\u201d\\n\\n3. Pronouns and Grammatical Personhood:\\n\\nAs the original verdicts sometimes contain first-person plural pronouns (\u201cwe have contacted them for more clarification\u201d), and at other times contained first-person singular pronouns (\u201cI understand your frustration\u201d), there are inconsistencies regarding \u201cwho\u201d is writing the verdict. The assumption one should take when post-editing is that each verdict is written by a single person. Therefore, if first-person plural pronouns are encountered, they should be changed to first-person singular pronouns. One exception exists: when the reader and writer of the verdict are grouped together, then first-person plural pronouns can be kept: \u201csurely we can all agree that this is a serious issue.\u201d\\n\\n4. Confirmations:\\n\\nSometimes a claim is fully or partially correct, and the original FullFact verdict notes this with a simple \u201ccorrect,\u201d or \u201cthis is right, but...\u201d, but the generated verdict does not. In this case, adding a quick confirmation such as \u201cyes, you\u2019re right, but...\u201d or \u201cabsolutely, it\u2019s a serious issue\u201d can be done as long as it does not reduce the overall readability of the verdict.\\n\\n5. Missing information:\\n\\nSometimes the generated verdicts do not include information from the original verdicts. This can make the generated verdict easier to read without reducing its persuasiveness. If missing information negatively impacts how effective a verdict is, then it should be added.\\n\\n6. New claims:\\n\\nConversely, there are cases in which the generated verdicts actually include information which is not contained in the original verdict, but which is either objectively true or is a subjective statement. If the claims made in the generated verdict are provably false, then removing them is necessary. If they are provably true, or if they are a subjective statement or opinion which can not be concretely proven true or false, they can be kept or removed at the post-editors discretion.\\n\\n7. General formatting issues:\\n\\nAs with generated claims, general formatting issues such as grammatical errors, typos, malformed hashtags, etc. should be corrected.\\n\\nC Experimental Details\\n\\nC.1 Extractive Summarization Methods\\n\\nBesides SBERT, we also considered other extractive summarization methodologies, i.e. Lead-k which extract the first k sentences from the article, and LexRank (Erkan and Radev, 2004), a graph-based unsupervised methodology which ranks the\"}"}
