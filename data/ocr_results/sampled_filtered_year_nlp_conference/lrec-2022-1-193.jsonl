{"id": "lrec-2022-1-193", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Annotating Attribution in Czech News Server Articles\\n\\nBarbora Hladk\u00e1, Ji\u0159\u00ed M\u00edrovsk\u00fd, Maty\u00e1\u0161 Kopp, V\u00e1clav Moravec\\n\\n1 Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics\\n2 Charles University, Faculty of Social Sciences, Institute of Communication Studies and Journalism\\n\\nhladka, mirovsky, kopp,\\nvaclav.moravec@fsv.cuni.cz\\n\\nAbstract\\nThis paper focuses on detection of sources in the Czech articles published on a news server of Czech public radio. In particular, we search for attribution in sentences and we recognize attributed sources and their sentence context (signals). We organized a crowdsourcing annotation task that resulted in a data set of 2,167 stories with manually recognized signals and sources. In addition, the sources were classified into the classes of named and unnamed sources.\\n\\nKeywords: media bias, news server, article, attribution, signal, source, annotation, crowdsourcing\\n\\n1. Introduction\\nAutomated journalism refers to the use of algorithms to automatically generate news from structured data, see e.g., Lepp\u00e4nen et al. (2017). Artificial Intelligence (AI) journalism is a broader concept that includes not only automation but machine learning and data processing of various newsrooms related tasks as well. In his survey, Becket (2019) reports that news gathering, news production, and news distribution are the three most common areas for his respondents' future AI-tool wishlist. Marconi (2020) provides readers with a more detailed discussion on how journalism will change through the AI-based processes. No doubt Natural Language Processing (NLP) plays (and will play) a key role in AI journalism.\\n\\nOne of the areas that is currently receiving a lot of attention is the area of a systematic, empirically-based, and historical-comparative understanding of media bias. Wikipedia defines media bias as \\\"the perceived bias of journalists and news producers within mass media in the selection of events and stories that are reported, and how they are covered.\\\"\\n\\nQuite surprisingly, there are still only few NLP studies systematically analyzing media bias, see e.g., Hamborg et al. (2019). Currently, significant attention is being paid to fact-checking, see e.g., Thorne and Vlachos (2018), Lazarski et al. (2021). In social sciences, the news production process is an established model that defines nine forms of media bias and describes where these forms originate from, see e.g., Baker (1996), Park et al. (2009).\\n\\nAn informative, balanced article should provide the background of a story, including naming sources. This paper aims at a news server of Czech public radio and focuses on detection of sources in its articles. In particular, we search for attribution in sentences and we extract attributed sources from them. This is a task from the area of text understanding and, at least to our knowledge, an NLP system automatically processing attribution in newspapers articles has not been implemented yet. Prasad et al. (2006) have proposed and described an annotation scheme for marking the attribution in the Penn Discourse TreeBank. However, we plan to use morphological and syntactic relations to detect attributed sources.\\n\\nAttribution\\nThe Macmillan Dictionary defines attribution as \\\"the act of attributing something to a particular cause or person, especially the act of saying that something was written, said, painted etc. by a particular person.\\\"\\n\\nOur primary task is to automatically detect sources that journalists credit in newspaper stories. We will approach it by detecting a sentence context in which sources are attributed and therefore we formalize the definition of attribution as follows:\\n\\n\\\\[ \\\\text{attribution} = \\\\text{source} + \\\\text{information} + \\\\text{signal} \\\\]\\n\\nwhere \\\\( \\\\text{source} \\\\) originally provided \\\\( \\\\text{information} \\\\) and \\\\( \\\\text{signal} \\\\) is a textual marker that identifies the \\\\( \\\\text{source} \\\\) of the \\\\( \\\\text{information} \\\\). We use mathematical notation intentionally, namely to emphasize that the order of \\\\( \\\\text{source} \\\\), \\\\( \\\\text{information} \\\\) and \\\\( \\\\text{signal} \\\\) in the sentence does not play a role which is analogous to the commutative property of addition. For illustration, we recognize the \\\\( \\\\text{information} \\\\) there are 7.77 million Internet users over the age of ten in the Czech Republic and the \\\\( \\\\text{source} \\\\) Netmonitor attributed using the \\\\( \\\\text{signal} \\\\) according to in the sentence\\n\\n\\\\[ \\\\text{According to} \\\\ \\\\text{NetMonitor}, \\\\ \\\\text{there are 7.77 million Internet users over the age of ten in the Czech Republic.} \\\\]\\n\\nIn English grammar, a signal phrase is a phrase, clause, or sentence that introduces a quotation, paraphrase, or summary, e.g., Marianne Egeland, Professor of Comparative Literature at the University of Oslo, argues that. We found it in several teaching materials that clearly explain their motivation to use the word signal:\\n\\n2 https://www.macmillandictionary.com/dictionary/british/attribution\"}"}
{"id": "lrec-2022-1-193", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"They signal to a reader that the writer is using an outside source. Signal phrases inspired us and we modified the signal definition so that we consider signal and source separately. The rest of the paper is organized as follows: A classification hierarchy of sources is presented in Section 2. In Section 3, we describe an automatic pipeline for processing the iRozhlas collection of stories published by the news server of Czech public radio. A subset of this collection, the SIR 1.0 corpus, was annotated in the annotation task described in Section 4 and evaluated in Section 5. We conclude by summarizing future plans to analyse the annotated data in great detail and to generate signal patterns to detect sources in the complete iRozhlas collection.\\n\\n2. Source Classification\\n\\nNamed sources\\nTheir attribution is as descriptive as possible. They can be further divided according to affiliated institutions into:\\n\u2013 Official sources\\n  One of their main characteristics is their authority and importance hierarchy. These sources not only have access to information, but make political, economic and social decisions as well. They usually have a dominant position among journalistic sources since both journalists and news consumers attribute higher information quality to them, which need not always be a legitimate expectation. Typically their positions and institutions are mentioned. They can be further classified as:\\n  \u2013 Political sources\\n    include political actors according to their political party affiliation. We can also include politicians representing executive and legislative bodies (i.e. president, prime minister, ministers, senators, deputies, etc.), e.g., member of Parliament Jaroslav Falt\u00fdnek/ANO/, ODS chairman Petr Fiala.\\n  \u2013 Non-political sources\\n    include sources usually connected to specific institutions and positions, e.g., director of the war museum.\\n\u2013 Unofficial sources\\n  do not have as much authority as official sources. They are often \u201cordinary people\u201d as witnesses of important events or confidential information providers (e.g., experts, most scientists). Unofficial sources are essential for the development of investigative journalism, which often guarantees their anonymity and confidentiality. In this case, as compared with official sources, editorial routines are associated with a more careful verification of information.\\n\\n3. iRozhlas Collection\\n\\niRozhlas is a news server of the Czech public radio launched on April 18, 2017. The iRozhlas collection where we will detect sources contains 63,325 articles from the period April 18, 2017\u2013June 24, 2021 and written by 927 authors. All the articles are Czech. Originally, the iRozhlas collection was represented in the JSON format containing the following items for each story: Story identifier, URL, Date of publication and change, Domicile, List of authors, List of sections, List of tags, Headline, Leading paragraph (Lead), and Text. We chose the TEI format as a target format for the collection namely because of the following three reasons (1) The format is standardized and widely recognized by the community of corpus linguistics, (2) We use it in the ParCzech (Kopp et al., 2021) and ParlaMint (Erjavec et al., 2022) projects compiling parliamentary data into corpora, and thus we can directly use the existing scripts for e.g., linguistic processing, and (3) The data can be visualized and queried in the TEITOK web service. Most of the item values were converted into TEI XML elements' values without any subsequent modification. The Lead and Text items contain not only the story itself but an HTML code of the original page including Javascript codes as well. We normalized sequences of spaces and removed/replaced problematic Unicode characters. Further, we removed text formatting (e.g., bold text) because it would subsequently make linguistic processing difficult, mainly tokenization. All the scripts are available in the GitHub repository https://github.com/ufal/media-irozhlas. The linguistic processing scripts use the API of LINDAT services UDPipe and NameTag for morphological and syntactic analysis (incl. tokenization and lemmatization) and named-entity recognition, resp. For internal purposes, we uploaded the iRozhlas collection into TEITOK which is an online system for (1) making corpora available and searchable, and (2)...\"}"}
{"id": "lrec-2022-1-193", "page_num": 3, "content": "{\"primary_language\":\"cs\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Italsk\u00e1 ekonomika se vymanila z recese. V prvn\u00edm \u010dtvrtlet\u00ed se jej\u00ed HDP zv\u00fd\u0161il o 0,2 procenta. Tamn\u00ed statistick\u00fd \u00fa\u0159ad ISTAT v \u00fater\u00fd ozn\u00e1mil, \u017ee hrub\u00fd dom\u00e1c\u00ed produkt se oproti p\u0159edchoz\u00edm t\u0159em m\u00e1lo zv\u00fd\u0161il o 0,2 procenta. It\u00e1lie je t\u0159et\u00ed nejv\u011bt\u0161\u00ed ekonomikou euroz\u00f3ny po N\u011bmecku a Francii. Ve t\u0159et\u00edm i \u010dtvrt\u00e9m \u010dtvrtlet\u00ed lo\u0148sk\u00e9ho roku vyk\u00e1zal italsk\u00fd HDP pokles o 0,1 procenta. Ekonomika se tak dostala do recese, kter\u00e1 se obvykle definuje jako alespo\u0148 dv\u011b \u010dtvrtlet\u00ed hospod\u00e1\u0159sk\u00e9ho poklesu za sebou. ISTAT rovn\u011b\u017e ozn\u00e1mil, \u017ee m\u00edra nezam\u011bstnanosti v It\u00e1lii se v b\u0159eznu sn\u00ed\u017eila na 10,2 procenta z \u00fanorov\u00fdch 10,5 procenta. Tato \u010d\u00edsla dokazuj\u00ed solidnost a stabilitu italsk\u00e9 ekonomiky, uvedl italsk\u00fd ministr hospod\u00e1\u0159stv\u00ed Giovanni Tria. Hospod\u00e1\u0159sk\u00fd r\u016fst v prvn\u00edm \u010dtvrtlet\u00ed p\u0159ekonal o\u010dek\u00e1v\u00e1n\u00ed analytik\u016f, kte\u0159\u00ed podle pr\u016fzkumu agentury Reuters p\u0159edpokl\u00e1dali, \u017ee HDP se zv\u00fd\u0161\u00ed pouze o 0,1 procenta.\"}"}
{"id": "lrec-2022-1-193", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Number of annotated signals per 100 sentences for the sections in a period of five years\\n\\nTable 4: Annotations of the source classes\\n\\nFigure 2: A sample attribution annotation in Brat\\n\\nFigure 2 illustrates an attribution annotation of the sentence\\n\\nPhilosopher Damon Young claims that this is an escape from the boredom of daily life.\\n\\nTable 5: Number of annotated attribution links per 100 sentences\"}"}
{"id": "lrec-2022-1-193", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Co-occurrence of source classes in the Czech Republic News section\\n\\nCzech is a typical pro-drop language, which omits the subject if it can be easily reconstructed from the pre-previous context. Therefore we see a difference between the number of the annotated signals and sources. In the following three sentences, we detect one signal Luk\u00e1\u0161 Krp\u00e1lek and two signals vysv\u011btluje (explains) and popisuje (describes), i.e. two attributions: Mistrem sv\u011bta se stal Luk\u00e1\u0161 Krp\u00e1lek ji\u017e podruh\u00e9. Moje judo je zalo\u017eeno na kondici... vysv\u011btluje. \u010clov\u011bk je mus\u00ed unavit... popisuje.\\n\\nAlso, a lack of annotators' attention causes some differences. For each source class, Table 4 displays the number of annotations and Figure 3 shows these numbers for the individual sections. For example, there is a clear evidence that official political sources occur rarely in the Sports and Culture sections.\\n\\nThe average number of annotated attribution links per 100 sentences is 22.6. The Sport section has the lowest attribution \u201cdensity\u201d (14.4) while the World News section has the highest one (34.3), see Table 5.\\n\\nThe paper (Duffy and Williams, 2011) presents a six-decade longitudinal quantitative analysis on how unnamed sourcing in the Washington Post and The New York Times has changed over time. As for our data, Table 3 displays the number of annotated signals per 100 sentences for each section in a period of five years. For example, we observe with surprise that the number of annotated signals in the CR News section significantly increased while the number of annotated signals in the World News section significantly decreased. A discussion with the news server editor helps to interpret all these data.\\n\\nTable 6 visualizes co-occurrences of source classes annotated in the Czech Republic News section. Each cell represents two types of sources that appeared in the same article.\\n\\nTable 7: Inter-annotator agreement in recognition of signals, sources and source classes by two annotators\\n\\n| Source Class          | F1 measure | Percentage Agreement | Cohen's Kappa |\\n|-----------------------|------------|----------------------|---------------|\\n| Official-non-political | 0.67       | 74.0%                | 0.58          |\\n| Official-political    | 0.60       |                      |               |\\n| Unofficial            |            |                      |               |\\n| Anonymous-partial     | 0.60       |                      |               |\\n| Anonymous             | 0.60       |                      |               |\\n\\nTable 8: Frequency of source classes in the headlines\\n\\n| Source Class         | % Signal |\\n|----------------------|----------|\\n| Official-non-political| 36.3     |\\n| Official-political   | 26.4     |\\n| Unofficial           | 19.7     |\\n| Anonymous-partial    | 14.4     |\\n| Anonymous            | 3.2      |\\n\\nTable 9: Top-6 signals in the headlines and lead+texts\\n\\n| Signal | % Signal |\\n|--------|----------|\\n| \u0159\u00edkat  | 19.5     |\\n| podle  | 15.5     |\\n| tvrdit | 7.4      |\\n| uv\u00e9st  | 8.7      |\\n| \u0159\u00edci    | 4.3      |\\n| varovat| 3.1      |\\n| dodat   | 3.1      |\\n| informovat| 2.8    |\\n\\nInter-Annotator Agreement was measured on 170 files (not all of the 220 files selected for double annotation contained any annotation in the end), each of which was independently double-annotated by two annotators. Table 7 shows F1 measure for recognition of citation sources and citation phrases, and a percentage agreement and Cohen's kappa for classification of sources recognized by both annotators.\\n\\nWe expected a higher agreement at the beginning of the annotation period. The students are not experienced with this type of tasks and this fact certainly contributed to the given results. But at the same time, we are worried that there is a lack of understanding of what attribution is and how to recognize it in text.\\n\\nHeadlines vs. Leads and Texts\\n\\nTerentieva et al. (2020) studied the attribution technique across headlines in the electronic editions of five leading Spanish mass media outlets between 2010 and 2018. Besides other findings, they concluded that headlines with attribution comprise approximately 15% of the total number of headlines in the given media. It is perfectly in line with our findings: in our annotated dataset, 13% of the total number of headlines contain annotation. Table 8 shows how often attribution occurs in headlines and leading paragraphs and texts. It is also interesting to see in Table 9 which signals are the most common: preposition podle (according to) clearly dominates the leading paragraphs and texts, while it is less frequent in headlines. At the top of the lists there are typically words with neutral polarity. However, in the headlines, there is the verb varovat (to warn) with negative polarity in order to attract readers.\\n\\nAuthors vs. Sources\\n\\nThe analysis of sources for individual authors is interesting. For illustration, we extracted the annotated sources in the articles written by one author. There are 950 such articles (out of 1,947) written by 294 authors. The horizontal axis of the histogram in Figure 4 displays the number of authors and\"}"}
{"id": "lrec-2022-1-193", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Conclusion\\n\\nMedia bias includes, beside others, bias that concerns an analysis of sources attributed in news articles. We focus on sources in the Czech articles published on the iRozhlas web news server being operated by Czech public radio. Namely, we explore the iRozhlas collection of more than 60 thousand articles. In the future, we will perform source detection and classification automatically as a combination of rule-based approach and machine learning. Given that, a very first task is to create a golden data set. Therefore we organized an annotation task. We designed it as a crowdsourcing task that engages typically a large number of individuals to achieve a given goal. In our case, more than 200 bachelor students of the Faculty of Social Sciences, Charles University participated in the annotation. The annotation was one of the course completion conditions and the students were not paid for their work. Based on the annotation analysis presented in this paper we summarize several facts and observations:\\n\\nFirst, we set the annotation time to two hours. Then we estimated the length of text to be annotated in two hours and finally we set the number of articles to be annotated by an annotator to 9 or 10. Further, we decided each annotator to annotate articles from all the newspaper sections (see Table 1). Since the students had very little experience with text annotation we did not apply any other criteria for file selection. In total, 1,947 stories from the iRozhlas server were randomly assigned to the students; including the double-assigned files for measuring the inter-annotator agreement, the resulting collection contains 2,167 files.\\n\\nWe set the annotation period to continuous five weeks. An e-mail helpline was active during this period to discuss any topic related to the annotation. Only 5% of the students took advantage of this opportunity and they typically asked questions of a technical nature. Once the annotation period ended, we extracted the annotated signals and sources, lemmatized them and represented them as a frequency list. To check how the annotators understand the task, we checked the low-frequency items in this list. No doubt some mistakes are due to the annotators' inattention, but the others show that some students do not recognize attributions in texts at all. This leads us to organize this annotation task again next year and split the annotation period into two parts. The annotation evaluation after the first part will show annotator agreement that we can use in the process of file assignment.\\n\\nWe will focus on more rigorous evaluation of the annotation task using statistical hypothesis testing. We will discuss its results with journalists and news editors.\\n\\n11 https://ufal.mff.cuni.cz/ano-tace-citacnich-frazi-v-datech-irozhlas\"}"}
{"id": "lrec-2022-1-193", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Based on the annotated signals and sources, we will generate queries in e.g., the Corpus WorkBench Query Language that enables searching data analysed by UDPipe and NameTag in TEITOK. For illustration, the query \\\\[ \\\\text{form=\\\"podle\\\"} <\\\\text{name type=\\\"PER\\\">} \\\\] searches for the signal podle (according to) and sources being persons in the genitive case.\\n\\nAcknowledgement\\nWe would like to thank the students for their efforts and Eva Haji\u02c7cov\u00b4a for her valuable comments on this article. This work was supported by the Technological Agency of the Czech Republic (grant number TL05000057). This work has been using language resources and tools developed and/or stored and/or distributed by the LINDAT/CLARIAH-CZ project of the Ministry of Education, Youth and Sports of the Czech Republic.\\n\\n7. Bibliographical References\\nBaker, B. (1996). How To Identify, Expose And Correct Liberal Media Bias. Media Research Center, Alexandria, V A.\\nBecket, C. (2019). New powers, new responsibilities. https://drive.google.com/file/d/1utmAMCmd4rfJHrUfLLfSJ-clpFTjyef1/view. Online.\\nDuffy, M. J. and Williams, A. E. (2011). Use of unnamed sources drops from peak in 1960s and 1970s. Newspaper Research Journal, 32(4):6\u201321.\\nErjavec, T., Ogrodniczuk, M., Osenova, P., Ljube\u02c7si\u00b4c, N., Simov, K., Pan\u02c7cur, A., Rudolf, M., Kopp, M., Barkarson, S., Steingr \u00b4\u0131msson, S., C \u00b8 \u00a8oltekin, C \u00b8 ., de Does, J., Depuydt, K., Agnoloni, T., Venturi, G., P \u00b4erez, M. C., de Macedo, L. D., Navarretta, C., Luxardo, G., Coole, M., Rayson, P., Morkevi\u02c7cius, V ., Krilavi\u02c7cius, T., Dar \u00b4gis, R., Ring, O., van Heusden, R., Marx, M., and Fi\u02c7ser, D. (2022). The ParlaMint Corpora of Parliamentary Proceedings. Language Resources and Evaluation. https://link.springer.com/article/10.1007/s10579-021-09574-0.\\nHamborg, F., Donnay, K., and Gipp, B. (2019). Automated identification of media bias in news articles: an interdisciplinary literature review. International Journal on Digital Libraries, 20(4):391\u2013415.\\nKopp, M., Stankov, V ., Kr\u02dauza, J. O., Stra\u02c7n\u00b4ak, P., and Bojar, O. (2021). ParCzech 3.0: A Large Czech Speech Corpus with Rich Metadata. In 24th International Conference on Text, Speech and Dialogue, pages 293\u2013304, Cham, Switzerland. Springer.\\nLazarski, E., Al-Khassaweneh, M., and Howard, C. (2021). Using nlp for fact checking: A survey. Designs, 5(3).\\nLepp\u00a8anen, L., Munezero, M., Granroth-Wilding, M., and Toivonen, H. T. (2017). Data-driven news generation for automated journalism. In INLG.\\nMarconi, F. (2020). Newsmakers: Artificial Intelligence and the Future of Journalism. Columbia University Press.\\nPark, S., Kang, S., Chung, S., and Song, J. (2009). NewsCube: Delivering Multiple Aspects of News to Mitigate Media Bias. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI '09, page 443\u2013452, New York, NY , USA. Association for Computing Machinery.\\nPrasad, R., Dinesh, N., Lee, A., Joshi, A., and Webber, B. (2006). Annotating attribution in the Penn Discourse TreeBank. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 31\u201338, Sydney, Australia, July. Association for Computational Linguistics.\\nTerentieva, E., Khimich, G., and Veselova, I. (2020). The analysis of citation in headlines in the spanish press. Heliyon, 6(1):e03155.\\nThorne, J. and Vlachos, A. (2018). Automated fact checking: Task formulations, methods and future directions. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3346\u20133359, Santa Fe, New Mexico, USA, August. Association for Computational Linguistics.\"}"}
