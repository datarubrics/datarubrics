{"id": "acl-2023-long-846", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exploring Large Language Models for Classical Philology\\nFrederick Riemenschneider\\nDept. of Computational Linguistics\\nHeidelberg University\\n69120 Heidelberg\\nriemenschneider@cl.uni-heidelberg.de\\nAnette Frank\\nDept. of Computational Linguistics\\nHeidelberg University\\n69120 Heidelberg\\nfrank@cl.uni-heidelberg.de\\n\\nAbstract\\nRecent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5\u2019s decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology. Our models and resources are available at https://github.com/Heidelberg-NLP/ancient-language-models.\\n\\n1 Introduction\\nSince the beginning of the creation of the Index Thomisticus in 1946 (Busa, 1980) and the publication of the Concordance to Livy (Packard, 1968), Classical Philology has been revitalized by the \u201cdigital revolution\u201d (Berti, 2019). Today, numerous efforts have been undertaken to make Classical texts digitally available, annotate, and automatically process them. E.g., the Classical Language Toolkit (CLTK, Johnson et al., 2021) offers various tools to process pre-modern languages, in particular Latin and pre-modern Greek.\\n\\n1 Recently, we see a surge of the first pre-trained contextualized language models (PLMs) for Classical languages: Latin BERT has been proposed by Bamman and Burns (2020), Ancient Greek (AG) BERT by Singh et al. (2021). Lately, a second AG BERT has been proposed by Yamshchikov et al. (2022). However, both AG BERT models have been pre-trained on a comparatively small pre-training dataset. Moreover, they have been initialized from Modern Greek BERT (Koutsikakis et al., 2020), which limits them to the modern Greek alphabet, ignoring the diacritics of Ancient Greek. Although numerous richly annotated treebanks are available for Latin and AG, systems have, by now, not been evaluated on a shared benchmark. Given that two popular treebanks for AG have been integrated into Universal Dependencies (de Marneffe et al., 2021), it is surprising that researchers working on AG do not compare to benchmarking results of, e.g., Straka (2018). Hence, a thorough assessment of the performance of the existing models is necessary in order to compare and evaluate their effectiveness for this underexplored language. While BERT models are known to achieve high performance on a wide range of tasks, encoder-decoder models or multilingual models may often be a better choice, depending on the task. In this work, we explore a variety of language models for Classics in general and Ancient Greek in particular: We introduce GReTA, GReBERTA, PHILBERTA, and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020) models trained on Ancient Greek texts, respectively. PHILBERTA and PHILTA, four PLMs for Classics. GReBERTA and GReTA are RoBERTa (Liu et al., 2"}
{"id": "acl-2023-long-846", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PHILTA are their trilingual counterparts pre-trained on Greek as well as Latin and English data. We explore the advantages of (i) the two model architectures in (ii) mono- and multilingual pre\u2013training for the mid-resource language Ancient Greek on a variety of morphological, syntactic, and semantic tasks, helping to answer questions, such as: When to choose one architecture over the other? or: How does multilinguality affect a language model?\\n\\nMoreover, we publish the first wide-ranging benchmark results to compare our models for AG and Latin to the relevant prior work, establishing new SoTA results for both languages.\\n\\nIn summary, we aim to unify and push forward the current research landscape at the intersection of Classics and NLP with the following contributions:\\n\\n(i) We introduce four pre-trained language models for Classics: GRE (BERT|T)A and PHL (BERT|T)A. To our knowledge, we are the first to develop encoder-decoder models for Classics, and multilingual models tailored to both Latin and Greek.\\n\\n(ii) We evaluate the already existing and our proposed models on several tasks, making many of them comparable for the first time. Furthermore, we outperform the existing Ancient Greek BERT models by a notable margin.\\n\\n(iii) Our evaluation sheds light on the differences between encoders like ROBERTA and encoders of encoder-decoder models like T5 as well as on the influence of multilinguality on the mid-resource language Ancient Greek. By offering novel model types for AG, we aim to inspire new research and application tasks.\\n\\n(iv) We develop and publish a large-scale, high-quality pre-training corpus for AG as a contribution to the community.\\n\\n2 Related Work\\n\\nPre-training Data for Ancient Greek. Pre-trained language models require large amounts of unlabeled pre-training data. Ancient Greek and Latin being historical languages, the number of available texts is inherently limited, which makes the creation of a high-quality pre-training corpus even more important. To circumvent this problem, Singh et al. (2021) and Yamshchikov et al. (2022) pre-trained their AG BERT model from a Modern Greek BERT (Koutsikakis et al., 2020). But this approach has two weaknesses: First, there is an important cultural gap between modern and ancient texts that we do not want to introduce into our models. A Modern Greek BERT is familiar with contemporary concepts like cell phones or communism, which are unknown to antiquity, while we intend to use PLMs as a \u201cwindow\u201d to ancient cultures. Also the style of modern internet documents is fundamentally different from the transmitted ancient texts. Second, and more importantly, continuing pre-training of the Modern Greek BERT prevents us from adapting its tokenizer. AG, however, uses more diacritics, which host important information. By contrast, in our work, we build a tokenizer from scratch that is optimized for Ancient Greek.\\n\\nIn order to boost the data needed to train \u201cpure\u201d models of Ancient Greek, we put special effort into the curation of a large, but high-quality pre-training corpus for AG, leveraging previously unused textual sources. Finally, we evaluate the effect of using additional multilingual pre-training data.\\n\\nEvaluating Models for Ancient Languages. Morphological and syntactic tasks, such as PoS tagging, dependency parsing, and lemmatization, have always been of interest to researchers of Latin and Ancient Greek. The standard tool for AG morphological analysis is Morpheus (Crane, 1991), a rule-based system, that has also been integrated into many more recent approaches. PoS Tagging has also been performed by various language-agnostic systems trained on AG data (Celano et al., 2016), but their success depends heavily on the chosen dataset: a winning system on one dataset (Celano et al., 2016) achieves the worst results on another (Keersmaekers, 2019). More recently, the CLTK (Johnson et al., 2021) provides a variety of taggers for many tasks. Surprisingly, although numerous richly annotated treebanks are available, systems have, by now, not been evaluated on a common benchmark.\\n\\n2 E.g., Singh et al. (2021) test their proposed AG BERT on random splits from three popular treebanks, which we cannot compare against. The second AG BERT (Yamshchikov et al., 2022) has only been evaluated on authorship attribution. As for lemmatization, Vatri and McGillivray (2020) provide an evaluation of three different lemmatizers. However, one of the evaluated candidates was partly trained on test data, which may have influenced its performance. It is noteworthy that, although numerous richly annotated treebanks are available, systems have, by now, not been evaluated on a common benchmark.\\n\\n2 Cf. also Johnson et al. (2021): \u201cThe CLTK lacks formal evaluations of its models\u2019 accuracies. [...]. Unfortunately, [outside] benchmarks do not yet exist for pre-modern languages.\u201d\"}"}
{"id": "acl-2023-long-846", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Form: \u03c4\u1f78\u03bd \u03b4\u1fbf \u1f04\u1fe4 \u1f51\u03c0\u03cc\u03b4\u03c1\u03b1 \u1f30\u03b4\u1f7c\u03bd \u03c0\u03c1\u03bf\u03c3\u03ad\u03c6\u03b7 \u03c0\u03cc\u03b4\u03b1\u03c2 \u1f60\u03ba\u1f7a\u03c2 \u1fbf\u0391\u03c7\u03b9\u03bb\u03bb\u03b5\u03cd\u03c2 \u00b7\\nLemma: \u1f41 \u03b4\u03ad \u1f04\u03c1\u03b1 \u1f51\u03c0\u03cc\u03b4\u03c1\u03b1 \u03b5\u1f36\u03b4\u03bf\u03bd \u03c0\u03c1\u03cc\u03c3\u03c6\u03b7\u03bc\u03b9 \u03c0\u03bf\u03cd\u03c2 \u1f60\u03ba\u03cd\u03c2 \u1fbf\u0391\u03c7\u03b9\u03bb\u03bb\u03b5\u03cd\u03c2 \u00b7\\nROOT\\nobj\\ncc\\nadvmod\\nadvmod advcl nmod amod\\nnsubj\\npunct\\n\\nFigure 1: Example sentence (Hom. Il. 1.148) with corresponding dependency, lemma, UPoS, and XPoS labels.\\nTranslation: \\\"Then watching him grimly from under his brows, swift-footed Achilles spoke to him.\\n\\ndespite the integration of two popular treebanks for AG into Universal Dependencies (UD, de Marneffe et al., 2021), many groups working on AG systems have not compared their models against the results of models benchmarked on UD data, such as Straka (2018). We remedy these issues by evaluating our systems and existing AG BERT models on the two authoritative treebanks covered by UD. The tasks we consider \u2013 dependency parsing, lemmatization, coarse, universal (UPoS) PoS tagging and fine-grained, language-specific (XPoS) tagging \u2013 are visualized in Figure 1.\\n\\nFor Latin, the issue does not arise thanks to the EvaLatin 2022 campaign (Sprugnoli et al., 2022), which has enabled direct comparison of models and has engendered strong models for Latin. Yet, despite the impressive results achieved in EvaLatin, our trilingual models outperform the existing systems on PoS tagging and lemmatization.\\n\\nLanguage Model Architectures. Language models can be categorized into three classes: encoder-only, decoder-only, and encoder-decoder models. Encoder-only models such as BERT (Devlin et al., 2019) and \\\\textsc{RoBERTa} (Liu et al., 2019) are best suited for tasks that aim to analyze complete sequences by sequence or token classification. Encoder-decoder models, on the other hand, are typically employed for conditional generation tasks, such as machine translation. Currently, all three models for ancient languages are BERT and thus encoder-only architectures.\\n\\nWe argue that an encoder-decoder model, such as \\\\textsc{T5} (Raffel et al., 2020), is a useful addition to this encoder-only landscape. First, it enlarges the space of possible NLP tasks for AG, enabling us, e.g., to cast lemmatization as a sequence-to-sequence task and to explore machine translation for ancient languages. Second, it allows us to compare the encoder of an encoder-only model with that of an encoder-decoder architecture, as they are both trained on the same data with a similar pre-training objective. Finally, commonly used multilingual encoder-decoder models like \\\\textsc{M}T5 (Xue et al., 2021) and \\\\textsc{BY}T5 (Xue et al., 2022) are not pre-trained on Ancient Greek texts. As we aim for optimally trained encoder-only models, we chose \\\\textsc{RoBERTa} over BERT: its dynamic masking strategy exploits the pre-training data better, and it has been shown that BERT's NSP objective can be detrimental (Liu et al., 2019).\\n\\n3 Pre-trained Language Models for Ancient Greek and Latin\\n3.1 \\\\textsc{Gr}\u20ac\\\\textsc{BERT}A and P\\\\textsc{HIL}BERTA\\n\\\\textsc{Gr}\u20ac\\\\textsc{BERT}A and P\\\\textsc{HIL}BERTA are \\\\textsc{RoBERTa} base-sized models. Both models are pre-trained using a masked language modeling (MLM) objective. Specifically, in the case of \\\\textsc{RoBERTa}, wordpieces are masked during the pre-training process, while for \\\\textsc{T5}, spans are masked. Although it has been shown that multilingual pre-training can lead to gains for low-resource languages through cross-lingual transfer, it remains an open question when exactly it is preferable to use a multilingual instead of a monolingual model (Doddapaneni et al., 2021). To explore the implications of multilinguality for AG language models, we test different capabilities and possible interferences by comparing the different model types.\\n\\n3.2 PLM Fine-tuning for Downstream Tasks\\nPoS Tagging. PoS tagging for Ancient Greek typically aims for a complete morphological analysis: 3 The following descriptions remain neutral to different PLM types by referring to basic transformer components. Where necessary, we will distinguish specific PLM types.\"}"}
{"id": "acl-2023-long-846", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Next to the word class, the model has to predict eight fine-grained morphological attributes. We frame this sequence labeling task as a multi-task classification problem applied to each token, with nine different classification heads per token on top of one shared encoder: We denote a sequence of tokens $S = w_1, w_2, \\\\ldots, w_n$ and refer to the contextualized embedding of each token as $e_i = Encoder(w_1:n, i)$. As Byte Pair Encoding (Sennrich et al., 2016) splits words into subword units, we represent each token using its first sub-word embedding in the encoder. Each of the nine attributes is predicted using a feed-forward network applied to the last encoding layer, followed by a softmax function. The total loss is calculated as:\\n\\n$$L_{total} = \\\\sum_{m=0}^{1} \\\\frac{1}{9} L_m$$\\n\\nWe use this approach for the Perseus XPoS dataset. For the other, less-detailed tagsets, we employ a single classification head.\\n\\nDependency Parsing. We follow Zhang et al. (2017) who cast dependency parsing as head selection. The model predicts a unique head for each token considered as a dependent. Since the model makes independent predictions, the obtained dependency graph can (in a few cases) be unconnected and is then completed by the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) for building non-projective trees \u2013 given that AG allows free word order. While Zhang et al.'s DESS parser was based on a bi-directional LSTM, we define the model on top of the final hidden states of the transformer encoders. Following Zhang et al. (2017), we add an artificial ROOT token $w_0$ and calculate the probability of the word $w_j \\\\in \\\\{w_0, w_1, \\\\ldots, w_N\\\\}$ being the head of the word $w_i \\\\in \\\\{w_1, w_2, \\\\ldots, w_n\\\\}$ in $S$ as:\\n\\n$$p_{head}(w_j|w_i, S) = \\\\frac{\\\\exp(f(e_j, e_i))}{\\\\sum_{k=0}^{N} \\\\exp(f(e_k, e_i))}$$\\n\\nwhere $f$ predicts the score of an edge $(w_j, w_i)$ as follows:\\n\\n$$f(e_j, e_i) = v^\\\\top \\\\cdot \\\\tanh(U \\\\cdot e_j + W \\\\cdot e_i)$$\\n\\nHere, $v$ is a weight vector and $U$, $W$ weight matrices. Dependency labels are predicted in a similar fashion: Let $g$ be a single hidden-layer rectifier network that takes as input the concatenation $[e_i; e_j]$. The probability for the label $l$ is then computed as:\\n\\n$$p_{label}(l|w_j, w_i, S) = \\\\frac{\\\\exp(g(e_j, l, e_i))}{\\\\sum_{l' \\\\in L} \\\\exp(g(e_j, l', e_i))}$$\\n\\nWhile Zhang et al. (2017) use the representations of their trained DESS model as input for the label classifier, we resort to the pre-trained embeddings.\\n\\nLemmatization. Current systems for lemmatization of AG, such as UDPipe (Straka, 2018) or GLEM (Bary et al., 2017), are rule-based or use a classifier to predict editing rules that modify a token's pre- and suffixes. However, these complex scripts are not well-suited for a language like AG, which has many irregular forms that involve modifications of the word stem. An alternative approach is to utilize an encoder-decoder model that receives the inflected form, the PoS tag, and (optionally) additional information such as morphological features, as demonstrated for different languages by Schmid (2019) or Wr\u00f3bel and Nowak (2022). Yet, these earlier encoder-decoder-based lemmatization models are purely word-based and rely on pre-computed PoS tags or morphological features in a pipeline setting. By contrast, we propose a novel T5-based lemmatization model that is (i) contextualized, so that relevant morphological indicators can be inferred by the model on the fly from the token's surrounding context. (ii) The model works end-to-end: it receives the surface form of the word to be lemmatized in its full sentence context and predicts its lemma without receiving or predicting PoS tags or morphological features. 5 We mark the t(arget) token to be lemmatized in its context using delimiter tokens $<t_tok_begin>$ and $<t_tok_end>$. For instance, for the input sentence \u03be\u03cd\u03bd\u03bf\u03b9\u03b4\u03b1 $<t_tok_begin>$\u1f10\u03bc\u03b1\u03c5\u03c4\u1ff7$<t_tok_end>$ \u03bf\u1f50\u03b4\u1f72\u03bd \u1f10\u03c0\u03b9\u03c3\u03c4\u03b1\u03bc\u03ad\u03bd\u1ff3 with the marked target token \u1f10\u03bc\u03b1\u03c5\u03c4\u1ff7, we expect as output the lemma \u1f10\u03bc\u03b1\u03c5\u03c4\u03bf\u1fe6. We also experiment with providing, in addition, the target word as a sequence of individual characters, delimited by an additional separator token $<t_tok_sep>$: \u03be\u03cd\u03bd\u03bf\u03b9\u03b4\u03b1 $<t_tok_begin>$\u1f10\u03bc\u03b1\u03c5\u03c4\u1ff7$<t_tok_sep>$\u1f10 \u03bc \u03b1 \u03c5 \u03c4 \u1ff7$<t_tok_end>$ \u03bf\u1f50\u03b4\u1f72\u03bd \u1f10\u03c0\u03b9\u03c3\u03c4\u03b1\u03bc\u03ad\u03bd\u1ff3.\"}"}
{"id": "acl-2023-long-846", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tactic tasks. However, to evaluate the models more comprehensively, it is necessary to also test their semantic and world knowledge. Since such benchmarks do not exist for AG or Latin, we create two small datasets to evaluate these aspects. Inspired by Talmor et al. (2020), we test whether the language models can distinguish synonyms from antonyms. For this task, we input a sentence, e.g., \\\\( \\\\tau \\\\text{\u03cc \u03c7\u03c1\u03ae\u03c3\u03b9\u03bc\u03bf\u03bd \u03ba\u03b1\u1f76 \u03c4\u03cc \u1f00\u03b3\u03b1\u03b8\u03cc\u03bd:} \\\\) \\\\(<\\\\text{mask}>\\\\) \\\\(\\\\text{\u1f41\u03bc\u03bf\u1fd6\u03ac} \\\\) \\\\(\\\\text{\u1f10\u03c3\u03c4\u03b9\u03bd}\\\\) (\\\"the useful and the good: they are <mask> similar\\\"), and the model has to predict either \\\\(\\\\text{\u03bf\u1f50\u03c7}\\\\) (\\\"not\\\") or \\\\(\\\\text{\u03c0\u03ac\u03bd\u03c4\u03c9\u03c2}\\\\) (\\\"very\\\"). Talmor et al. (2020) cast a similar task for English as a zero-shot MLM prediction problem using BERT and ROBERTA. However, with our prompt, the models always predict \\\\(\\\\text{\u03bf\u1f50\u03c7}\\\\) (\\\"not\\\"), regardless of the provided word pairs. Experiments with variations of the prompt have led to similar difficulties. Hence, we evaluate this task in a few-shot setting, fine-tuning the MLM-head on 10 to 50 shots of synonyms and antonyms each, to prepare them for the task.\\n\\nSimilarly, we construct a dataset of family relationships between (mythical) heroes and gods. Here, the model is given a phrase, such as \\\\(\\\\text{T\u03b7\u03bb\u03ad\u03bc\u03b1\u03c7\u03bf\u03c2 \u1f41 \u03c4\u03bf\u1fe6 <mask> \u03c0\u03b1\u1fd6\u03c2}\\\\) (\\\"Telemachus, son of <mask>\\\"), and has to predict the correct entity (in this case, Odysseus). For this task, we test the models in a zero-shot setting. However, this task cannot be solved by most encoder-only models, as the masked names typically consist of more than a single wordpiece. Thus, for this task, we evaluate only GRET and PHIL, which can predict full entity names. By comparing the mono- and multilingual variants, we assess the models' acquired world knowledge as well as potential effects that may be induced by multilingual training: Given that Greek and Roman mythology share many of these gods, yet by different names, the multilingual model may be able to acquire additional knowledge from the Latin pre-training data, to solve the task formulated in Ancient Greek. We describe both datasets in Appendix B.2.\\n\\n3.3 Acquisition of Pre-training Data\\n\\nAncient Greek. To cover a wide range of dialects, topics, and time periods of Ancient Greek, we make use of four different data sources: (the Greek part of) the Open Greek & Latin Project, 6 the CLARIN corpus Greek Medieval Texts, 7 the Patrologia Graeca, 8 and the Internet Archive (IA). 9 While the first three sources contain born-digital textual data, the IA online library provides books in the public domain along with their OCR transcriptions. However, we found the partition of texts labeled as Ancient Greek in the IA to be incomplete and noisy: only a small fraction of the books containing AG text was labeled as such, and only few of them were transcribed with OCR settings supporting Greek characters. We hence extracted a novel data partition from the IA that was then fully OCRred by the Internet Archive to ensure correct transcription. To select a large number of high-quality texts, we applied a complex retrieve and filter procedure, focusing not only on (i) text quality, but also on (ii) collecting purely Ancient Greek texts, avoiding inclusion of texts in different languages, such as Latin, English, or German that can co-occur in the same book, and on (iii) filtering duplicates.\\n\\nLatin and English. Acquiring pre-training data for Latin was facilitated by the Corpus Corporum project, 10 a meta-repository of Latin corpora that offers a comprehensive representation of the Latin language. All this data was kindly offered to us. For English, we collect pre-training data from various sources, aiming for texts that are related to antiquity, by being focused on the same topics that we find in ancient texts \u2013 as opposed to modern themes. To this end, we utilize English translations of Latin and Ancient Greek texts as pre-training data. Furthermore, we ensure that the amount of English data is of similar size as the ancient texts, to prevent the models from being overwhelmed by a large number of English texts.\\n\\nStatistics of pre-training data in Table 1. More details on corpus creation and statistics in Appendix C.\\n\\n3.4 Pre-training Process\\n\\nEven though our filtering of the IA corpus resulted in high-quality texts, the corpus is necessarily noisier than the born-digital texts. We therefore start pre-training on the IA data, and continue with the born-digital texts. Our tokenizers and the multilingual variants are trained on the born-digital texts only. For further pre-training details, see Appendix A.\\n\\n---\\n\\n6 https://opengreekandlatin.org/.\\n7 https://inventory.clarin.gr/corpus/890.\\n8 https://patristica.net/graeca/.\\n9 https://archive.org/.\\n10 https://www.mlat.uzh.ch/.\"}"}
{"id": "acl-2023-long-846", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of the pre-training datasets. Only Open Greek & Latin is used by Singh et al. (2021) and Yamshchikov et al. (2022) for their AGBERT models. Token counts determined by UNIX command `wc -w`.\\n\\n4 Experiments\\n\\nWe run the experiments outlined in Section 3.2 to provide insight into the performances achieved by different model types and in relation to prior SoTA.\\n\\n4.1 Datasets\\n\\nAncient Greek. For the PoS tagging, dependency parsing, and lemmatization tasks, we evaluate the PLMs for AG on the data provided by the Perseus and the PROIEL datasets, which are both integrated into Universal Dependencies 2.10 (de Marneffe et al., 2021).\\n\\nTo probe our models for semantic and world knowledge (see Section 3.2), we use our newly constructed datasets, described in Appendix B.2.\\n\\nLatin. For Latin, we resort to the treebank used in EvaLatin 2022 (Sprugnoli et al., 2022), which covers three tasks: PoS tagging, lemmatization, and feature identification. Since no data for dependency parsing is provided, we restrict our evaluation to PoS tagging and lemmatization. In EvaLatin, instead of constructing test data by drawing samples from the initial data set, the test data exhibits different degrees of distribution differences in relation to the training data. For each task, three test sets are provided: The Classical set belongs to the same genre and time period as the training data, but comes from an author not included in the training data. The Cross-genre data includes two works that belong to different genres, yet being written during roughly the same time period. The Cross-time test set is based on text written in the 15th century, which is significantly later than the texts of the training data.\\n\\nIn Table 2, we summarize the diverse tasks under consideration with their corresponding metrics, the used evaluation datasets, the model architectures, and the pre-trained language models that are applicable to the respective task. Further details, including dataset statistics, are provided in Appendix B.1.\\n\\n4.2 Models and Baselines\\n\\nAncient Greek. To showcase the capabilities of a recent system tailored to AG, we report the results of the taggers provided by the Classical Language Toolkit (Johnson et al., 2021).\\n\\nAs a baseline, we use the currently best-performing system, UDP-IPE (Straka et al., 2019), a transformer-based multi-task architecture that utilizes multilingual BERT, trainable word embeddings, and character embeddings.\\n\\nIn addition, to directly assess the benefits of using our monolingual model, we replace this multilingual BERT with our $\\\\text{GR}_\\\\epsilon$BERT model. For PoS tagging and dependency parsing, we further compare to both prior encoder models trained on AG texts. We use the PoS tagger and DESEN (Section 3.2) to evaluate both AGBERT models as well as our $\\\\text{GR}_\\\\epsilon$BERT and PTHILBERT models. We apply the same approach to $\\\\text{GR}_\\\\epsilon$T's encoder ($\\\\text{GR}_\\\\epsilon$T-ENC) to investigate its behavior.\\n\\nFor lemmatization, we compare the performance of CLTK and UDP-IPE with that of our full-fledged T5 models. To predict a lemma during inference, we use beam search with a width of 20.\\n\\nLatin. For Latin, we report the results of both teams that participated in the EvaLatin 2022 competition: Team KRAK\u00d3W (Wr\u00f3bel and Nowak, 2022) utilizes the XLM-RoBERTA large (Conneau et al., 2020) model for PoS tagging, team KU-LIEUVEN (Mercelis and Keersmaekers, 2022) employs an ELECTRA model. For lemmatization, Wr\u00f3bel and Nowak (2022) use BYT5 small (Xue et al., 2022), a multilingual encoder-decoder model similar to M-T5 (Xue et al., 2021) that operates on UTF-8 bytes instead of subwords. Mercelis and Keersmaekers (2022) implement a cascaded approach that resembles the Greek lemmatizer GLEM (Bary et al., 2017): If a rule-based lookup...\"}"}
{"id": "acl-2023-long-846", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PoS Tagging\\nDependency Parsing\\nLemmatization\\nUPoS\\nXPoS\\nUnlabeled\\nLabeled\\nTask Description\\nPoS tagging with universally applicable, coarse PoS tags\\nPoS tagging with language-specific, fine-grained tags; complete morphological analysis in the case of Perseus\\npredicting the head of each token in text\\npredicting the head and relation type of each token in text\\npredicting the lemma of each token in text\\nMetric\\nAccuracy\\nAccuracy\\nUAS\\nLAS\\nAccuracy\\nDatasets\\nPerseus\u2713\\nPROIEL\u2713\\nEvaLatin\u2713\\nPerseus\u2713\\nPROIEL\u2713\\nEvaLatin\u2717\\nPerseus\u2713\\nPROIEL\u2713\\nEvaLatin\u2717\\nPerseus\u2713\\nPROIEL\u2713\\nEvaLatin\u2713\\nModel Architecture\\nEncoder + Classification Head\\nEncoder + Classification Head(s)\\nEncoder + DENSE\\nEncoder + DENSE\\nEncoder-decoder\\nPLM Instances\\n(GR|PHIL)BERT\\nA,\\n(GR|PHIL)TAA-ENC\\n(GR|PHIL)BERT\\nA,\\n(GR|PHIL)TAA-ENC\\n(GR|PHIL)BERT\\nA,\\n(GR|PHIL)TAA-ENC\\n(GR|PHIL)TAA-ENC\\nTable 2: Summary of the tasks under consideration.\\nreturns multiple lemmata, the system tries to disambiguate between these possibilities by means of the predicted PoS tag. To further clarify any remaining ambiguities, a classifier is trained to select the correct lemma from the available options.\\n\\n5 Results\\nAncient Greek.\\nWe present the results for PoS tagging and dependency parsing for Ancient Greek on the Perseus dataset in Table 3. The PROIEL dataset seems to be easier to solve, as all models achieve performances that are much closer to each other (see Appendix D). Since the overall trends are consistent across both datasets, we focus our discussion on the results on the Perseus dataset.\\n\\nAs seen in Table 3, the CLTK performs clearly below all other models on both tasks. While the CLTK is not directly comparable to the other models (see fn. 11), the evaluation still provides a perspective on the capabilities of the de facto only available framework for processing AG text.\\n\\nUDPipe provides a strong baseline, which AG BERT (Yamshchikov et al., 2022) is unable to consistently outperform. By contrast, all other PLMs show clear gains over UDPipe. The monolingual, encoder-only GreBERT A model consistently performs best on all tasks. Interestingly, the performance of GreBERT TAA-ENC on PoS tagging is slightly worse than that of PhilBERT A, while it achieves better results for dependency parsing. This trend has also been observed in initial experiments. We elaborate on the behavior of GreBERT TAA-ENC and PhilBERT A in Section 6.\\n\\nResults for Lemmatization are shown in Table 4. Here, augmenting UDPipe with GreBERT A's pre-trained embeddings does not lead to better scores. We attribute this to the tokenization process and refer to our discussion in Section 6. GreBERT TAA, on the other hand, demonstrates strong encoder-decoder capabilities and significantly outperforms UDPipe. Providing GreBERT TAA with the individual characters of the target word leads to a small gain.\\n\\nThe results of the Synonym/antonym disambiguation task are visualized in Figure 2. Again, GreBERT A and PhilBERT A demonstrate higher scores compared to the AG BERT models. We observe the same for GreBERT TAA and PhilBERT TAA (cf. Figure 4 in Appendix D). Our monolingual models and their multilingual counterparts perform almost equally, especially taking into account the overlapping standard deviation bands. We see a minimal trend for PhilBERT TAA to gain over GreBERT TAA in Figure 4, but our small datasets do not allow drawing firm conclusions on their relative performance.\\n\\nFinally, we report zero-shot results for the Family relationship task in Table 5. As the T5-based models have been pre-trained to predict multiple masked spans at once, they tend to predict, for each sample, more than a single entity. We interpret the output as a ranked list and report recall@k, evaluation.\"}"}
{"id": "acl-2023-long-846", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: PoS tagging and dependency parsing results on the Ancient Greek Perseus dataset. The results are averaged over three runs with different random seeds, and the standard deviation is indicated in parentheses, except for the CLTK and UDP IPE (reported results). Note also that the CLTK is not trained on exactly the same data as the other models and therefore not strictly comparable.\\n\\n| Model           | Accuracy |\\n|-----------------|----------|\\n| CLTK            | 76.10    |\\n| UDP IPE (official) | 86.70 |\\n| UDP IPE (ours)  | 84.50 (0.09) |\\n| UDP IPE + GRBERT A | 85.56 (0.06) |\\n| PHIL TA         | 90.02 (0.02) |\\n| PHIL TA + Chars | 90.66 (0.01) |\\n| GR\u03b5T A          | 90.80 (0.10) |\\n| GR\u03b5T A + Chars  | 91.14 (0.10) |\\n\\nTable 4: Lemmatization results for Ancient Greek on the Perseus dataset. Results are averaged over three runs, with standard deviation in parentheses, except for the CLTK and UDP IPE (reported results).\\n\\n| Model           | k = 1 | k = 5 | k = 10 | k > 10 |\\n|-----------------|-------|-------|--------|--------|\\n| GR\u03b5T A          | 4.39  | 9.65  | 10.53  | 10.96  |\\n| PHIL TA         | 3.07  | 8.33  | 11.40  | 11.84  |\\n\\nTable 5: Zero-shot family relationships task (recall@k).\\n\\n| Family | k = 1 | k = 5 | k = 10 | k > 10 |\\n|--------|-------|-------|--------|--------|\\n| Classical | 97.99  | 96.06  | 92.97  | 92.97  |\\n| Cross-genre | 97.61  | 94.62  | 92.69  | 92.69  |\\n| Cross-time | 96.33  | 92.31  | 92.11  | 92.11  |\\n\\nLatin. The PoS tagging and lemmatization scores on EvaLatin 2022 are reported in Table 6. While the performance scores of all models are rather close to each other, our trilingual models consistently outperform the EvaLatin 2022 participant systems across all three subtasks. PHIL BERT reaches even higher scores than KRAK\u00d3W-OPEN on PoS tagging, which leverages additional annotated data. On lemmatization, PHIL TA similarly outperforms KRAK\u00d3W-CLOSED on the Classical, Cross-genre, and Cross-time subtasks by 2.25, 1.78, and 0.23 percentage points, respectively, but does not outperform KRAK\u00d3W-OPEN on the Cross-genre and the Cross-time subtask.\\n\\n| Subtask            | Classical | Cross-genre | Cross-time |\\n|--------------------|-----------|-------------|------------|\\n| UPoS               | KRAK\u00d3W-OPEN | 97.99 | 96.06 | 92.97 |\\n| XPoS               | KRAK\u00d3W-OPEN | 97.61 | 94.62 | 92.69 |\\n| UAS                | KRAK\u00d3W-OPEN | 96.33 | 92.31 | 92.11 |\\n| LAS                | KRAK\u00d3W-OPEN | 95.08 | 91.08 | 91.68 |\\n| Lemmatiz.          | KRAK\u00d3W-OPEN | 97.26 | 96.45 | 92.15 |\\n| Lemmatiz.          | KRAK\u00d3W-OPEN | 95.44 | 96.48 | 84.60 |\\n| Lemmatiz.          | PHIL TA + Chars | 97.33 | 93.40 | 91.91 |\"}"}
{"id": "acl-2023-long-846", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Validation accuracy (AG XPoS Tagging on Perseus) for GreBERTa and GreTA-Enc and random counterparts over various training epochs. By contrast, GreBERTa reaches a high validation accuracy already after one epoch. We see the same trend with different random seeds and for dependency parsing, but it is most apparent in Perseus XPoS tagging.\\n\\nLemmatization as a Character-based Task. As seen in Table 4, augmenting UDP IPE with GreBERTa does not lead to significant improvement for lemmatization. This we attribute to the tokenization process. GreBERTa uses wordpieces, which contain little information about individual characters. We hypothesize that UDP IPE ignores the GreBERTa embeddings for lemmatization and instead relies on its own additional character embeddings. Accordingly, explicitly providing GreTA with the individual characters of the inflected word form leads to a slight increase in performance. This explanation can also shed light on the success of the UTF-8 bytes-based BYT5 model for lemmatization in Latin. This model was chosen by Wr\u00f3bel and Nowak (2022), after initial experiments with the wordpiece-based MT5 (Xue et al., 2021) had underperformed. Future work on (AG) lemmatization could therefore investigate whether Byte Pair Encoding-based models can be augmented with character embeddings as additional input.\\n\\nEffect of Multilinguality. Table 3 shows that PHILBERTa consistently performs slightly worse compared to monolingual GreBERTa on morphological and syntactic tasks. We attribute this to the curse of multilinguality (Conneau et al., 2020): the capacity of the trilingual models is split between three languages. Still, both models achieve strong results on AG and Latin tasks and can be especially useful in tasks that require multilingual knowledge, as in MT or glossing tasks. Our small-sized knowledge probing tasks show very similar performance for both model types. While the size of our data does not allow for firm conclusions, this is in line with Kassner et al. (2021), who find no improved knowledge representation in multilingual PLMs.\\n\\n7 Conclusion\\nWe introduce four strong language models for Classical Philology, including the first encoder-decoder PLMs for Ancient Greek and Latin. We rigorously benchmark our models and prior work on various tasks, demonstrating strong improvement over the SoTA. We showcase the versatility of encoder-decoder models, (i) by offering a novel end-to-end contextualized lemmatization model for AG and Latin, with a greatly simplified architecture that clearly outperforms prior work; (ii) while MLM in encoder-only models is restricted to single-token predictions, our T5-based models exhibit great flexibility for formulating probing tasks, which help exploring what models learn from pre-training data. Considering the two investigated model dimensions, our work (iii) sheds light on differences between the encoders of T5 vs. ROBERTA, where the former tends to exhibit slower learning curves; (iv) our monolingual models outperform the multilingual ones in monolingual morphological and syntactic tasks, without clear trends on small-scale semantic and knowledge probing tasks.\\n\\nLimitations\\nWhile we aim for a comprehensive analysis of existing methods (such as lemmatization) and model types for Ancient Greek and other Classical languages, there are limits to exhaustively exploring the full space of variations and rigorously evaluating their impact on model performance. For example, we could not comprehensively evaluate the effects of (i) the pre-training corpora, as we did not re-train a BERT model for Ancient Greek, to pin down the exact difference between prior BERT models (which were trained on smaller data before) and our own models, which are based on inherently stronger model types; similarly, we did not induce Latin ROBERTA and T5 models, to confirm the differences between mono- and multilingual models for language-specific Latin tasks. (ii) In a similar vein, we did not compare different model sizes. However, we studied prior work and scaling laws and believe that the base model is appropriate for the size of our training data. Further factors of this type concern (iii) hyperparameter settings and (iv) other factors in isolation. Not only do we miss sufficient computational\"}"}
{"id": "acl-2023-long-846", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"resources to perform such manifold ablations and comparative assessments, we also considered the carbon footprint that such experiments cause and which does not stand up to the insights that could possibly be gained from more experiments.\\n\\nFor these reasons, we focused on two selected dimensions of variants that we believe to be valuable for a community interested in Classical languages:\\n\\n(i) We tried to answer questions as to when multilingual models can be profitably used, and\\n(ii) aimed to showcase various potential advantages of encoder-decoder models, which by now have not been considered in studies on Classical languages.\\n\\nAnother clear limitation lies in the size of the demonstrated semantic and knowledge probing tasks. (i) They are of small size, and we cannot, therefore, draw firm conclusions as to, e.g., the effect of multilinguality. Also, the synonym/antonym disambiguation task is presumably the most difficult one. As a counter-balance, we used a more tangible task for knowledge probing, by choosing family relationships, which we expect to be frequently found in the pre-training corpora.\\n\\n(ii) A further limitation we find for the knowledge probing tasks resides in the size of our trained models and the underlying pretraining data. This limitation could be one that is not easy to overcome. But we still encourage the community to create similar probing task datasets. Future work may find appropriate ways of data augmentation, or transfer learning methods that are applicable to historical languages so that further progress and insight will be possible.\\n\\nEthics Statement\\n\\nIt is a computationally demanding task to pre-train large language models. However, transfer learning opens the possibility to fine-tune our pre-trained models, which showed strong performances, in a reasonable amount of time.\\n\\nThe texts utilized for pre-training the models may well exhibit biases related to ancient perspectives of the world. We do not view this as an issue, as the proposed language models for historical languages are intended for academic use and do not have practical, everyday applications.\\n\\nAcknowledgments\\n\\nWe are deeply indebted to the Internet Archive team for their continuous support by creating new OCR transcriptions of the misclassified Greek books, and to our anonymous reviewers for their comments, which have helped to significantly improve the paper. We thank Nina Stahl and Thomas Kuhn-Treichel for their help in creating our semantic and knowledge probing tasks, as well as Jan Ctibor and Philipp Roelli for providing us with the invaluable Corpus Corporum data. Finally, we acknowledge and thank for crucial support from the Google TPU Research Cloud program, for granting us access to their TPUs.\\n\\nReferences\\n\\nDavid Bamman and Patrick J Burns. 2020. Latin BERT: A contextual language model for classical philology. arXiv preprint arXiv:2009.10053.\\n\\nCorien Bary, Peter Berck, and Iris Hendrickx. 2017. A Memory-Based Lemmatizer for Ancient Greek. In Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage, DAT-ech2017, page 91\u201395, New York, NY , USA. Association for Computing Machinery.\\n\\nM. Berti. 2019. Digital Classical Philology: Ancient Greek and Latin in the Digital Revolution. Age of access? Grundfragen der Informationsgesellschaft. De Gruyter.\\n\\nThorsten Brants. 2000. TnT \u2013 a statistical part-of-speech tagger. In Sixth Applied Natural Language Processing Conference, pages 224\u2013231, Seattle, Washington, USA. Association for Computational Linguistics.\\n\\nR. Busa. 1980. The Annals of Humanities Computing: The Index Thomisticus. Computers and the Humanities, 14(2):83\u201390.\\n\\nGiuseppe G. A. Celano, Gregory Crane, and Saeed Majidi. 2016. Part of Speech Tagging for Ancient Greek. Open Linguistics, 2(1).\\n\\nYoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest arborescence of a directed graph. Scientia Sinica, 14(10):1396.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\\n\\nGregory Crane. 1991. Generating and Parsing Classical Greek. Literary and Linguistic Computing, 6(4):243\u2013245.\"}"}
{"id": "acl-2023-long-846", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marie-Catherine de Marneffe, Christopher D. Manning, Joakim Nivre, and Daniel Zeman. 2021. Universal Dependencies. *Computational Linguistics*, 47(2):255\u2013308.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nSumanth Doddapaneni, Gowtham Ramesh, Mitesh M Khapra, Anoop Kunchukuttan, and Pratyush Kumar. 2021. A primer on pretrained multilingual language models. *arXiv preprint arXiv:2107.00676*.\\n\\nJack Edmonds. 1967. Optimum branchings. *Journal of Research of the National Bureau of Standards B*, 71(4):233\u2013240.\\n\\nKyle P. Johnson, Patrick J. Burns, John Stewart, Todd Cook, Cl\u00e9ment Besnier, and William J. B. Mattingly. 2021. The Classical Language Toolkit: An NLP framework for pre-modern languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 20\u201329, Online. Association for Computational Linguistics.\\n\\nNora Kassner, Philipp Dufter, and Hinrich Sch\u00fctze. 2021. Multilingual LAMA: Investigating knowledge in multilingual pretrained language models. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3250\u20133258, Online. Association for Computational Linguistics.\\n\\nAlek Keersmaekers. 2019. Creating a richly annotated corpus of papyrological Greek: The possibilities of natural language processing approaches to a highly inflected historical language. *Digital Scholarship in the Humanities*, 35(1):67\u201382.\\n\\nManfred Kern, Alfred Ebenbauer, and Silvia Kr\u00e4mer-Seifert. 2003. *Lexikon der antiken Gestalten in den deutschen Texten des Mittelalters*. Walter de Gruyter.\\n\\nJohn Koutsikakis, Ilias Chalkidis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2020. GREEK-BERT: The Greeks Visiting Sesame Street. In 11th Hellenic Conference on Artificial Intelligence, SETN 2020, page 110\u2013117, New York, NY, USA. Association for Computing Machinery.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. *arXiv preprint arXiv:1907.11692*.\\n\\nWouter Mercelis and Alek Keersmaekers. 2022. An ELECTRA model for Latin token tagging tasks. In Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages, pages 189\u2013192, Marseille, France. European Language Resources Association.\\n\\nD. W. Packard. 1968. *A Concordance to Livy*. Harvard University Press.\\n\\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A Python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. *Journal of Machine Learning Research*, 21(140):1\u201367.\\n\\nHelmut Schmid. 2019. Deep Learning-Based Morphological Taggers and Lemmatizers for Annotating Historical Texts. In Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage, DATeCH2019, page 133\u2013137, New York, NY, USA. Association for Computing Machinery.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics.\\n\\nPranaydeep Singh, Gorik Rutten, and Els Lefever. 2021. A pilot study for BERT language modelling and morphological analysis for ancient and medieval Greek. In Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 128\u2013137, Punta Cana, Dominican Republic (online). Association for Computational Linguistics.\\n\\nRachele Sprugnoli, Marco Passarotti, Flavio Massimo Cecchini, Margherita Fantoli, and Giovanni Moretti. 2022. Overview of the EvaLatin 2022 evaluation campaign. In Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages, pages 183\u2013188, Marseille, France. European Language Resources Association.\\n\\nMilan Straka. 2018. UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 197\u2013207, Brussels, Belgium. Association for Computational Linguistics.\\n\\nMilan Straka, Jana Strakov\u00e1, and Jan Haji\u02c7c. 2019. Evaluating contextualized embeddings on 54 languages in...\"}"}
{"id": "acl-2023-long-846", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"pos tagging, lemmatization and dependency parsing.\\n\\narXiv preprint arXiv:1908.07448.\\n\\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. oLMpics\u2014on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743\u2013758.\\n\\nAlessandro Vatri and Barbara McGillivray. 2020. Lemmatization for Ancient Greek: An experimental assessment of the state of the art. Journal of Greek Linguistics, 20(2):179 \u2013 196.\\n\\nKrzysztof Wr\u00f3bel and Krzysztof Nowak. 2022. Transformer-based part-of-speech tagging and lemmatization for Latin. In Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages, pages 193\u2013197, Marseille, France. European Language Resources Association.\\n\\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291\u2013306.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.\\n\\nIvan Yamshchikov, Alexey Tikhonov, Yorgos Pantis, Charlotte Schubert, and J\u00fcrgen Jost. 2022. BERT in Plutarch's Shadows. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6071\u20136080, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nXingxing Zhang, Jianpeng Cheng, and Mirella Lapata. 2017. Dependency parsing as head selection. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 665\u2013676, Valencia, Spain. Association for Computational Linguistics.\"}"}
{"id": "acl-2023-long-846", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Pre-training hyperparameters.\\n\\n| Hyperparameter          | Value                  |\\n|-------------------------|------------------------|\\n| Adam \u03f5                 | 1 \u00b7 10^{-8}            |\\n| Adam \u03b2\u2081                | 0.9                    |\\n| Adam \u03b2\u2082                | 0.999                  |\\n| Batch Size              | 32                     |\\n| Early Stopping Patience | 5                      |\\n| Learning Rate           | 1 \u00b7 10^{-4}            |\\n| Learning Rate Scheduler | linear                 |\\n| Random Seeds            | 42, 1, 2               |\\n| Train Epochs            | 50                     |\\n| Weight Decay            | 1 \u00b7 10^{-5}            |\\n\\nFurther details in Table 7.\\n\\nWe pre-train the monolingual models for 50 epochs on the Internet Archive corpus and continue pre-training for 100 epochs on the born-digital texts, the trilingual models were trained for 100 epochs on the born-digital texts. The tokenizers were trained on the born-digital data only. G_{BERT} and P_{BERT} were trained on an NVIDIA A100-PCIE-40GB, G_{TA} and P_{TA} on a Google TPU v2-8. Training took between 3 and 7 days.\\n\\nA.2 Fine-tuning Details\\n\\nWe train every Greek model for 50 epochs on an NVIDIA GeForce GTX 1080 Ti, evaluating the model after every epoch on the validation set and using early stopping with a stopping patience of 5. As the EvaLatin dataset does not provide a validation set, we use 2% of the training data as the validation set. Furthermore, since the EvaLatin dataset is larger than the Greek datasets, we set the maximum number of training epochs to 20 for the Latin models. Depending on the treebank and the task, training the models took approximately 1 hour (PoS tagging), 5\u20137 hours (dependency parsing), and 1\u20133 days (lemmatization). Further details in Table 8. We did not experiment with different hyperparameter settings, as our main goal was to provide comparable and wide-ranging benchmarking results.\\n\\nTable 8: Fine-tuning hyperparameters.\\n\\n| Hyperparameter          | Value                  |\\n|-------------------------|------------------------|\\n| Learning Rate           | 1 \u00b7 10^{-4}            |\\n| Learning Rate Scheduler | linear                 |\\n| Random Seeds            | 42, 1, 2               |\\n| Train Epochs            | 50                     |\\n| Weight Decay            | 1 \u00b7 10^{-5}            |\\n\\nFurther details in Table 8.\\n\\nTable 9: Statistics of the Perseus, PROIEL, and EvaLatin datasets.\\n\\n| Dataset      | Sentences (train) | Sentences (dev) | Sentences (test) | Sentences (total) |\\n|--------------|-------------------|-----------------|------------------|-------------------|\\n| Perseus      | 11 476            | 1137            | 1306             | 13 919            |\\n| PROIEL       | 15 014            | 1019            | -                | 17 080            |\\n| EvaLatin     | 15 785            | -               | 1960             | 17 745            |\\n\\n| Dataset      | Tokens (train)    | Tokens (dev)    | Tokens (test)    | Tokens (total)    |\\n|--------------|-------------------|-----------------|------------------|-------------------|\\n| Perseus      | 159 895           | 22 135          | 20 959           | 202 989           |\\n| PROIEL       | 187 033           | 13 652          | 13 314           | 213 999           |\\n| EvaLatin     | 316 573           | -               | 45 544           | 362 117           |\\n\\n| Dataset      | Lemmata           | Forms           | UPoS Tags        | XPoS Tags         |\\n|--------------|-------------------|-----------------|------------------|------------------|\\n| Perseus      | 13 413            | 41 304          | 14                | -                |\\n| PROIEL       | 9 354             | 32 591          | -                | 13                |\\n| EvaLatin     | 10 357            | 54 133          | -                | 27                |\\n\\n| Dataset      | Dependency Relations | | |\\n|--------------|----------------------| | |\\n| Perseus      | 25                   | |\\n| PROIEL       | 33                   | |\\n| EvaLatin     | -                    | |\\n\\nB Downstream Task Details\\n\\nB.1 Universal Dependencies and EvaLatin 2022\\n\\nFor PoS tagging, UD provides universal PoS tags (UPoS) and language-specific PoS tags (XPoS). UPoS consists of 17 tags used for all languages covered by UD. XPoS tags, on the other hand, can follow a dataset-specific annotation scheme. While the XPoS tags of the PROIEL dataset are similar to the UPoS tags, the Perseus dataset aims for a complete morphological analysis (cf. Section 3.2). See Table 9 for further details and Table 2 for an overview. In line with common convention, we report the accuracy for both PoS tag sets. For dependency parsing, we report the unlabeled attachment score (UAS) and the labeled attachment score (LAS). The UAS indicates the percentage of tokens that have been assigned the correct head, whereas for the LAS, both the predicted head and the dependency label have to be correct. All results are obtained from the official evaluation script.\\n\\nIn the case of AG, 3 of these 17 tags are not used.\\n\\n13 https://universaldependencies.org/conll18/conll18_ud_eval.py and https://github.com/CIRCSE/LT4HALA/blob/master/2022/data_and_doc/conll18_ud_eval_EvaLatin_2022_rev2.py.\"}"}
{"id": "acl-2023-long-846", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Semantic Knowledge. We asked a graduate student and a doctoral candidate in the field of Classics to gather synonym and antonym pairs. Such word pairs can be nouns and substantivized adjectives or substantivized infinitives. We then utilized a predefined template to generate sentences that incorporate the collected pairs. As this template does not ensure grammaticality, the annotators manually edited the sentences. Subsequently, the sentences were independently reviewed by both annotators, deduplicated, and then verified by a professor of Ancient Greek. All three annotators participated on a voluntary basis and were not compensated for their contributions. One of the annotators is also a co-author of this paper.\\n\\n141 synonym and 146 antonym pairs were collected. While we publish all 287 examples, we drop 5 randomly selected antonym pairs in our experiments to ensure that the number of synonym and antonym pairs is equal. We train all language models for 10 epochs using a batch size of 4 and report the averaged, cross-validated results.\\n\\nWorld Knowledge. This dataset was compiled by one of the previous annotators who is not a co-author of this paper. The annotator gathered 228 examples with 11 different relations by reading through Hesiod's *Theogony* and by drawing inspiration from Kern et al. (2003), a lexicon that contains comprehensive information about mythological figures.\\n\\nC Acquisition of Pre-training Data\\n\\nC.1 Ancient Greek Pre-training Data\\n\\nOpen Greek & Latin Project. The Open Greek & Latin Project is an umbrella project covering various subprojects that aim toward the development of open-access corpus linguistic resources for Latin and Classical Greek. Two of them, the Perseus Digital Library and the First Thousand Years of Greek project, contain Ancient Greek texts, mostly covering texts that are typically associated with classical antiquity, such as Homer, Plato, Herodotus, Euripides, and Plutarch. Already in this corpus, we find a wide variety of dialects and language stages. The Open Greek & Latin Project contains approximately 30 million tokens.\\n\\nGreek Medieval Texts. The Greek Medieval Texts corpus offered by CLARIN covers writings from the fourth to the sixteenth century AD. It contains religious, poetical-literary and political-historical texts as well as hymns and epigrams. Strictly speaking (and as the name suggests) the corpus contains texts of late antiquity, and in particular, Medieval Greek. We argue, however, that Ancient Greek and Medieval Greek, although different language stages, are strongly connected to each other and that our language models benefit from seeing more diverse data during pre-training. This corpus contains about 3,3 million tokens and is licensed under the CC BY-NC 4.0 license.\\n\\nPatrologia Graeca. The Patrologia Graeca is a large collection of important Christian texts written in Greek, dating from the first until the fifteenth century AD. Since not all texts are machine-readable and available, we are restricted to those out of copyright texts that are made accessible (around 28.5 million tokens).\\n\\nInternet Archive. The Internet Archive is an online library that provides texts obtained from public domain books via OCR. We found out that only a small fraction of the books containing Ancient Greek text was labeled as such. Moreover, we discovered that even less books were transcribed with OCR settings that allowed Greek characters. As a result, many high-quality scans of Ancient Greek texts were transcribed into incomprehensible sequences of non-Greek characters. For example, the verse \u1f62 \u03b3\u03cd\u03bd\u03b1\u03b9 \u1f26 \u03bc\u03ac\u03bb\u03b1 \u03c4\u03bf\u1fe6\u03c4\u03bf \u03bd\u03b7\u03bc\u03b5\u03c1\u03c4\u1f72\u03c2 \u1f14\u03b5\u03b9\u03c0\u03b5\u03c2 is transcribed as \u03b6\u03c5vvai, ff pdXa \u03c4ovto Stto\u02c6 vrjpepTe\u02c6 e. Even though transcriptions of this nature may seem useless at first glance, they are nevertheless helpful in identifying documents that have been incorrectly treated as non-Greek texts, for many common words are relatively consistently transcribed. \u03c4\u03bf\u1fe6\u03c4\u03bf (\u201cthis\u201d), for example, is often transcribed into \u03c4ovto. By searching for all books that contain the word \u03c4ovto, we can identify potential Greek texts. This approach allows us to avoid the computationally intensive task of applying Greek OCR to every book in the Internet Archive, and instead focus our efforts on a more targeted search. All candidates are then filtered more aggressively: If...\"}"}
{"id": "acl-2023-long-846", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a candidate contains the five (presumably) Greek stopwords tovto (\u03c4\u03bf\u1fe6\u03c4\u03bf), kal (\u03ba\u03b1\u03af), tov (\u03c4\u03cc\u03bd), to (\u03c4\u03cc), and yap (\u03b3\u03ac\u03c1) more than ten times, the candidate is considered to contain Greek text.\\n\\nWe argue that this method effectively minimizes false positives while retaining a high recall: Since Greek stopwords like \u03c4\u03bf\u1fe6\u03c4\u03bf (\\\"this\\\") and \u03ba\u03b1\u03af (\\\"and\\\") should be present often enough in every book with a significant amount of text, our approach should correctly classify them as Greek. Non-Greek texts, on the other hand, should hardly contain all five stopwords more than ten times.\\n\\nThis procedure yields 25378 books, on which the Internet Archive applies OCR with Ancient Greek as a target language. While our method reliably detects Greek texts, it does not ensure a high scan (and therefore also text) quality. In order to use solely high-quality data, we keep only lines in which more than 90% of tokens are also present in the born-digital vocabulary. A similar approach is used by Bamman and Burns (2020), who use Latin texts from the Internet Archive as pre-training data for Latin BERT. They \\\"retain only those books where at least 40% of tokens are present in a vocabulary derived from born-digital texts\\\". We argue that it is more sensible to include or disregard individual lines instead of whole books: Almost every Greek text contains a Latin or English introduction, and many books are equipped with a translation. Thus, our method not only ensures high-quality data but also removes non-Greek text parts.\\n\\nFinally, to ensure that our dataset does not contain any significant duplications, we remove all instances of repeated text exceeding 300 characters. After this aggressive filtering, we have approximately 123.3 million tokens left. To demonstrate its quality, we show 40 samples randomly drawn from the dataset in Table 10.\\n\\nC.2 English Pre-training Data\\n\\nBy collecting English translations of ancient texts, we focus on texts that are strongly connected to antiquity. We gather these texts from various sources: The Perseus Digital Library and the Internet Classics Archive provide born-digital open-access translations of Classical Greek and Latin texts. Similarly, the Documenta Catholica Omnia database contains a large amount of primarily catholic texts in many languages, of which we select the English...\"}"}
{"id": "acl-2023-long-846", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"restricted to providing translations of Latin and Greek texts, the Project Gutenberg offers a more diverse range of literature. Therefore, we use only books from Project Gutenberg that are tagged with the keyword \u201cLatin\u201d. We report detailed statistics in Table 11.\\n\\n| Language                  | Dataset                          | Number of Tokens       |\\n|---------------------------|----------------------------------|------------------------|\\n| Ancient Greek             | Open Greek & Latin               | 30.0 million           |\\n| Greek Medieval Texts      |                                  | 3.3 million            |\\n| Patrologia Graeca         |                                  | 28.5 million           |\\n| Internet Archive          |                                  | 123.3 million          |\\n| Overall                   |                                  | 185.1 million          |\\n| English                   | Perseus                          | 10.8 million           |\\n| Classics Archive          |                                  | 4.9 million            |\\n| Lexundria                 |                                  | 2.8 million            |\\n| Loebulus                  |                                  | 14.0 million           |\\n| Project Gutenberg         |                                  | 28.7 million           |\\n| Documenta Catholica Omnia |                                  | 151.7 million          |\\n| Overall                   |                                  | 212.8 million          |\\n\\nTable 11: Statistics of the pre-training datasets. Only Open Greek & Latin is used by Singh et al. (2021) and Yamshchikov et al. (2022) for their AGBERT models. Token counts determined by UNIX command `wc -w`.\\n\\nFurther Results\\n\\n| Model                  | Accuracy  |\\n|------------------------|-----------|\\n| CLTK                   | 96.51     |\\n| UDP IPE (official)     | 94.71     |\\n| UDP IPE (ours)         | 93.87 (0.05) |\\n| UDP IPE + GREEBERT     | 94.17 (0.05) |\\n| GREEFTA                 | 97.40 (0.02) |\\n| GREEFTA + Chars        | 97.48 (0.02) |\\n\\nTable 12: Lemmatization results on the Ancient Greek PROIEL dataset. The results are averaged over three runs with different random seeds, and the standard deviation is indicated in parentheses, except for the CLTK and UDP IPE (reported results).\\n\\n![Figure 4](image_url): Synonym/antonym disambiguation accuracy scores for different few-shot training set sizes, for GREEFTA and PHILTA against AGBERT models. The models are always given equal amounts of synonyms and antonyms, e.g., when using 20 training instances, the models are given 10 synonyms and 10 antonyms. We evaluate all models using k-fold cross-validation and report standard deviation as error bars.\"}"}
{"id": "acl-2023-long-846", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 13: PoS tagging and dependency parsing results on the Ancient Greek PROIEL dataset.\\n\\nThe results are averaged over three runs with different random seeds, and the standard deviation is indicated in parentheses, except for the CLTK and UDP IPE (reported results). Note also that the CLTK is not trained on exactly the same data as the other models and therefore not directly comparable.\\n\\n| Model       | PoS Tagging | Dependency Parsing |\\n|-------------|-------------|--------------------|\\n|             | UPoS        | XPoS               | UAS  | LAS  |\\n| CLTK        | 97.10       | 97.47              | 76.81 | 73.39 |\\n| UDPIPE (official) | 97.77   | 98.05              | 85.64 | 81.70 |\\n| UDP IPE (ours) | 97.99 (0.05) | 98.68 (0.06) | 88.50 (0.09) | 84.72 (0.18) |\\n| AG BERT (Singh et al., 2021) | 97.98 (0.02) | 98.14 (0.05) | 89.75 (0.09) | 86.59 (0.15) |\\n| AG BERT (Yamshchikov et al., 2022) | 97.19 (0.06) | 97.42 (0.08) | 86.61 (0.21) | 82.12 (0.15) |\\n| GR\u03b5TA - ENC | 98.16 (0.02) | 98.31 (0.03) | 89.93 (0.08) | 86.48 (0.08) |\\n| PHILBERT A | 98.15 (0.16) | 98.45 (0.05) | 90.32 (0.13) | 86.43 (0.61) |\\n| GR\u03b5BERT A | 98.60 (0.03) | 98.70 (0.04) | 90.28 (0.03) | 86.84 (0.12) |\"}"}
{"id": "acl-2023-long-846", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\nWe discuss general limitations in a section titled \\\"Limitations\\\". Furthermore, we acknowledge that the experiments on our small probing datasets do not allow to draw firm conclusions in Section 5 and Section 6.\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\nNot applicable. Left blank.\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper\u2019s main claims?\\nThe paper\u2019s claims are summarized in the Abstract and explicitly listed at the end of the Introduction (Section 1).\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\nLeft blank.\\n\\nB \u25a1 Did you use or create scientific artifacts?\\nWe use pre-trained Language Models for Ancient Greek, introducing them in Section 1, elaborating on them in Section 2 as Related Work, and using them in our experiments in Section 5 and Section 6. Furthermore, we pre-train Language Models, elaborating on them in Section 3 and using them in our experiments (Section 5 and 6) as well. Finally, we create a pre-training corpus for Ancient Greek, described in Section 3 and Section C. The downstream task datasets that we use are introduced in Section 4 and Section B.\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\nWe cite the creators of the datasets we used in Sections 1 and 2. We cite relevant prior work and language models that we compare to in Sections 1 and 2. We elaborate on the datasets we use in Section 4.1 and specify the version.\\n\\n\u25a1 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\\nThe licenses for the data are discussed in Section C.\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\nWe specify the intended use for our pre-training dataset in Section 1 and the intended use for our language models in Section 1 and 2.\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\nGiven that we create our dataset utilizing open-domain texts from antiquity, we do not consider anonymization to be a significant concern.\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\nOur pre-training corpus for Ancient Greek is described in Section 3 and Section C. Our language models are documented in Section 3 and Section A.\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-846", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not.\\n\\nFor the Universal Dependencies and EvaLatin datasets that we used, we report the statistics in Appendix B. We report the creation of the pre-training and probing corpora and their statistics in Appendix B and C.\\n\\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? We report pre-training and fine-tuning details in Appendix A.\\n\\nC2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? No response.\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? No response.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? We use the official evaluation scripts for our dataset, mentioned in Section B.\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants? The collection of our probing task data is described in Section B.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? They were informed orally in a brief introductory session.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? We report details about how the annotators were paid and how they were recruited in Appendix B.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Details about consent are reported in Appendix B.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. Left blank.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? We report the characteristics of the annotator population in Appendix B.\"}"}
