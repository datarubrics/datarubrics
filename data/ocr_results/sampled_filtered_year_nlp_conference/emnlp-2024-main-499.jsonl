{"id": "emnlp-2024-main-499", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents\\n\\nLiyan Tang \u2662 Philippe Laban \u2660 Greg Durrett \u2662\\n\u2662The University of Texas at Austin \u2660 Salesforce AI Research\\nlytang@utexas.edu\\n\\nAbstract\\n\\nRecognizing if LLM output can be grounded in evidence is central to many tasks in NLP: retrieval-augmented generation, summarization, document-grounded dialogue, and more. Current approaches to this kind of fact-checking are based on verifying each piece of a model generation against potential evidence using an LLM. However, this process can be very computationally expensive, requiring many calls to a model to check a single response. In this work, we show how to build small fact-checking models that have GPT-4-level performance but for 400x lower cost. We do this by constructing synthetic training data with GPT-4, which involves creating realistic yet challenging instances of factual errors via a structured generation procedure. Training on this data teaches models to check each fact in the claim and recognize synthesis of information across sentences. For evaluation, we unify datasets from recent work on fact-checking and grounding LLM generations into a new benchmark, LLM-A\\\\textsubscript{GGRE}\\\\textsubscript{FACT}. Our best system MiniCheck- FT5 (770M parameters) outperforms all systems of comparable size and reaches GPT-4 accuracy. We release LLM-A\\\\textsubscript{GGRE}\\\\textsubscript{FACT}, code for data synthesis, and models.\\n\\n1 Introduction\\n\\nFreeform generation of responses is a flexible way to employ large language models (LLMs) for question answering, summarization, and beyond. However, this kind of generation can lead to factual errors, the \u201challucination\u201d problem in LLMs (Falke et al., 2019; Maynez et al., 2020; McKenna et al., 2023; Zhang et al., 2023a). Such errors arise in generation settings where an LLM is prompted closed-book, but its parametric knowledge may be insufficient to produce the right facts (Min et al., 2023; see latest leaderboard at llm-aggrefact.github.io).\\n\\nFigure 1: We unify the task of fact-checking across various settings that rely on grounding documents. We train a small sentence-level fact-checker by leveraging new synthetically generated data, which demonstrates strong performance on a new unified benchmark LLM-A\\\\textsubscript{GGRE}\\\\textsubscript{FACT}, comparable to GPT-4 but 400x cheaper.\\n\\nPast work has largely dealt with these problems separately. We can post-hoc verify closed-book generated answers by retrieving supporting documents and checking the answer against them (Gao et al., 2023; Malaviya et al., 2024; Jacovi et al., 2024), which requires precise checking as many statements are not exactly supported or may have conflicting information available (Wang et al., 2023; Glockner et al., 2024). When the grounding is known in settings like summarization, the attribution problem can be cleanly framed as document-level textual entailment (Nie et al., 2020a; Yin et al., 2021) and has been studied extensively for smaller language models (Falke et al., 2019; Goyal and Durrett, 2020, 2021; Laban et al., 2022; Tang et al., 2024). Different but related errors occur in grounded generation settings where evidence is already available, like summarization of input documents or retrieval-augmented question answering, where an LLM can blend information incorrectly (Liu et al., 2023; Tang et al., 2024).\"}"}
{"id": "emnlp-2024-main-499", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The problems in this space have a shared primitive operation: the need to check a statement against grounding documents, either retrieval-augmented content or post-hoc retrieved evidence. We call this primitive fact-checking on grounding documents, shown in Figure 1. Implementations of this primitive need to be accurate, spotting subtle errors while maintaining a low false positive rate, as most generated statements are correct. They also need to be efficient: a single LLM response may contain dozens of facts to verify, and self-verification with an LLM may increase cost by an order of magnitude (Weng et al., 2023; Gero et al., 2023). For instance, the 110-150 word biographies in FActScore (Min et al., 2023) contain 26-41 atomic facts that are checked against 5 documents each, resulting in 130-205 entailment checks.\\n\\nIn this work, we build an efficient system for fact-checking on grounding documents. Our key insight is to develop a new synthetic training dataset which is tailored to the complexities of the fact-checking task. Unlike standard distillation from LLMs (Taori et al., 2023; Hsieh et al., 2023), this setting differs in that we do not necessarily have access to task instances that we can label with strong LLMs. For instance, in datasets like ExpertQA (Malaviya et al., 2024), even the inputs to the LLM are expert-written questions. As a result, we synthesize challenging fact-checking instances from the ground up, as a scalable way to teach a small model how to simultaneously verify multiple facts in a sentence against multiple sentences in grounding documents.\\n\\nOur system, MiniCheck, is an instance of Flan-T5 (Chung et al., 2022) fine-tuned on this data plus standard entailment data (Nie et al., 2020a).\\n\\nFor our experiments, we introduce a new unified benchmark, LLM-AGGREGACT, which aggregates 10 existing datasets for both closed-book and grounded generation settings. In each constituent dataset, sentence-level factual errors are labeled by human annotators. We show that MiniCheck can perform as well as GPT-4 in aggregate and substantially outperform past fine-tuned systems like AlignScore (Zha et al., 2023). Moreover, we find that decomposition of sentences into atomic facts, which has been explored in past work (Kamoi et al., 2023; Gao et al., 2023; Wang et al., 2023), is not necessary to achieve this high performance.\\n\\nOur contributions are as follows: (1) Two synthetic data generation methods to address the challenges of fact-checking on grounding documents. (2) A new benchmark unifying factual evaluation on closed-book and grounded generation settings. (3) Evaluation shows that our MiniCheck system can beat previous specialized systems by 4% to 10% in absolute values, despite using less fine-tuning data, and is on par with GPT-4 with a much smaller model size, faster inference speed, and 400 times less cost. Furthermore, we can do this without a separate claim decomposition step.\\n\\n2 Background and Motivation\\n\\nProblem Setup: Claim Verification\\n\\nWe assume a collection of statements to be checked consisting of sentences $c = [c_1, \\\\ldots, c_k]$. Typically, this will be a sequence of sentences produced by an LLM. Each sentence $c_i$ has an associated set of grounding documents $D_i = \\\\{D_{i,1}, \\\\ldots, D_{i,n}\\\\}$. These different $D_i$ per sentence accommodate post-hoc retrieval settings where each sentence has different retrieved evidence; however, some settings may use shared evidence across all sentences or even a single grounding document for tasks like single-document summarization (i.e., all $D_i$ only contain the document being summarized).\\n\\nWe view these sentences as claims. Our goal in this work is to build a system that can validate each claim against the documents. Following Laban et al. (2022), we define a discriminator $M(D_{i,j}, c) \\\\in \\\\{0, 1\\\\}$, that classifies each claim $c_i$ into unsupported, 0, or supported, 1, according to a provided document $D_{i,j}$.\\n\\nThis process makes two assumptions. First, we assume that each supported claim can be validated against a single document; that is, claims are \u201catomic enough\u201d. Our methodology can be generalized to handle claims supported by multiple documents by simply appending multiple documents in the context of $M$, but we did not find it necessary in any of the datasets we studied.\\n\\nSecond, we assume that we can perform our entailment checks on each sentence $c_i$ on its own, without context $c_{<i}$. In general, sentences do need context to be understood (e.g., most sentences starting with pronouns), but this can be resolved through the use of a decontextualization step (Choi et al., 2021). Section 7 examines whether such a step improves performance of our system.\\n\\nFollowing past work (Kamoi et al., 2023; Sanyal et al., 2024), we disregard the usual \u201ccontradiction\u201d class from textual entailment, as contradictions are rare in our benchmark.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aricle\u2026 LIN: Well, and some airlines are going to say that so many factors are out of their control: like weather and now labor problems. \u2026\\n\\nTRIPPLER: \u2026 I believe those are in their control. Weather, I understand. Labor: Come on, airlines, let\u2019s get it together. \u2026\\n\\nSome airlines argue that factors like weather and labor problems are beyond their control, but experts disagree.\\n\\nAtomic Facts:\\n1. Some airlines argue that factors like weather are beyond their control.\\n2. Some airlines argue that factors like labor problems are beyond their control.\\n3. Experts disagree that factors like weather are beyond airlines\u2019 control.\\n4. Experts disagree that factors like labor problems are beyond airlines\u2019 control.\\n\\nFigure 2: An example dialogue snippet with an LLM-generated summary sentence, from the TofuEval dataset.\\n\\nWe judge a sentence $c_i$ by taking $\\\\max_{j} M(D_{i,j}, c_i)$: it is supported if and only if there exists some document that supports it.\\n\\nChallenges of verification\\n\\nTwo aspects of the task make this process challenging. First, there may be several individual facts in $D_{i,j}$ which are necessary to validate a claim $c_i$. For example, the LLM-generated sentence in Figure 2 can be broken down into four atomic facts. Each fact must be checked even if they are not explicitly materialized.\\n\\nSecond, and relatedly, the claim, or a fact in the claim, may require making inferences that span multiple sentences within $D_{i,j}$. In Figure 2, Lin initially argues that airlines have no control over either weather or labor issues. However, Trippler\u2019s later statement, \u201cWeather, I understand. Labor: Come on, airlines, let\u2019s get it together,\u201d implies agreement that weather is uncontrollable but suggests that labor problems are within the airlines\u2019 control. This indicates that the third atomic fact is unsupported by the document.\\n\\nWe argue that existing specialized fact-checkers fall short in effectively considering all atomic facts within a claim to be verified and struggle with reasoning across multiple sentences. Results in Appendix A.1 support this characterization. To address this issue, we come up with two synthetic data generation methods (Section 3) to enhance the models\u2019 ability in these areas. We discuss the relation to prior work in Section 8.\\n\\n3 Methodology: Training Data Synthesis\\n\\nTo address these challenges, new data is required. Existing datasets like MNLI (Williams et al., 2018) and ANLI (Nie et al., 2020a) do not feature instances that reflect the complexity of LLM fact-checking. Annotation of real errors is challenging to scale; datasets of such errors (including those in LLM-A and LLM-GRE) are largely test-only.\\n\\nOur goal is to construct a dataset $\\\\{(D_i, c_i, y_i)\\\\}_{i=1}^N$ of $N$ instances of documents $D_i$ paired with claims $c_i$ with label $y_i \\\\in \\\\{0, 1\\\\}$, using two novel synthetic data generation methods (Figure 3). Statistics about our final synthetic training data can be found in Table 1. A small-scale human evaluation of the synthetic data quality can be found in Appendix A.3. Additional details, including the sources of claims and documents and examples of generated data, can be found in Appendix D. We provide all prompts and quality assurance details in Appendix H.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Our synthetic data generation pipeline: C2D (upper) and D2C (lower). We illustrate with a claim that contains two atomic facts. Examples of generated data can be found in Appendix D.\\n\\nculty of the task by ensuring that multiple-sentence reasoning is required to correctly classify a claim.\\n\\nStep 4: Nonsupporting document generation\\n\\nBy construction, an atomic fact $a_i$ in the claim $c$ is supported by the sentence pair $(s_i, 1, s_i, 2)$ mentioned in the generated document $D$. Therefore, by omitting one of the sentences from the pair in a newly generated document $D'$, it is likely that $a_i$, and consequently $c$, is no longer supported by $D'$ (except in cases of redundancy in the sentences $s_i$).\\n\\nMore formally, we can construct a document $D'_a$ that probably cannot support fact $a_i$ in $c$ (and hence $c$) by removing sentence $s_{i,j}$ from its sentence pair:\\n\\n$$D'_a = \\\\text{PassageGen}(s_i, s_{i,j})$$\\n\\nfor all $i \\\\in \\\\{1, \\\\ldots, l\\\\}$ and $j \\\\in \\\\{1, 2\\\\}$ (Figure 3; top right). To collect documents that do not support the claim $c$, we retain $D'_a$ if $a_i$ cannot be supported by the information combined from the remaining sentence from its sentence pair and other atomic facts $(s_i, 3 - j \\\\cup \\\\{a_{-a_i}\\\\})$ via an entailment check by GPT-4. Note that this entailment check is again more accurate than directly checking $a_i$ against $D'_a$ due to the shorter context.\\n\\nStep 5: Pairing subclaims and generated documents\\n\\nWe have collected tuples $(D, c, 1)$ and $(D'_a, c, 0)$ for some $i$ and $j$. We can further augment this data to produce more examples. We first generate a power set $\\\\text{Power}(a)$, that consists of all possible subsets of atomic facts $a$ in $c$, but excludes the empty set. We then create a set of augmented subclaims $\\\\text{Aug}(c)$ by merging atomic facts from each subset:\\n\\n$$\\\\text{Aug}(c) = \\\\{\\\\text{Merge}(a'_i) : \\\\forall a'_i \\\\in \\\\text{Power}(a)\\\\}.$$\\n\\nIt follows that we obtain tuples $(D, c', 1)$ for every $c' \\\\in \\\\text{Aug}(c)$. Similarly, for each $D'_a$, we generate tuples $(D'_a, \\\\text{Merge}(a'_i), 1)$ if $a_i / \\\\in a'_i$, indicating that the document still supports the sub-claim absent the atomic fact $a_i$. Conversely, we have $(D'_a, \\\\text{Merge}(a'_i), 0)$ if $a_i \\\\in a'_i$, suggesting that the document does not support the sub-claim due to the absence of $a_i$.\\n\\nBecause the same sub-claim is supported by certain documents and unsupported by others depending on the presence or absence of specific atomic facts, we achieve the same benefits that training on contrast sets provides (Cao and Wang, 2021; Liu et al., 2022; Tang et al., 2023b), namely making the model more sensitive to the specifics of the decision boundary and encouraging it to consider all atomic facts within a claim during prediction.\\n\\n3.2 Doc to Claim (D2C) Generation\\n\\nIn the D2C method, our objective is to enhance the diversity of documents and ensure that the documents are more realistic than those in C2D, thereby reducing the distribution shift between synthetic documents used during training and real documents at test time. To achieve this, we assume that we have access to a set of human-written documents to start with. The goal is to generate claims and pair them with portions of these human written documents, which, once again, require multi-sentence, multi-fact reasoning to check the claims.\\n\\nStep 1: Chunk-level summarization\\n\\nWe first divide a human written document into three chunks \\\\{$D_1, D_2, D_3$\\\\} with approximately equal length. We then use GPT-4 to generate a summary sentence for each chunk, resulting in a set of summary sentences $c = \\\\{c_1, c_2, c_3\\\\}$. We assume these generated summary sentences are factually consistent with respect to their corresponding chunks, i.e. $(D_i, c_i, 1)$ for all $i$, as each chunk is short and LLMs can almost always generate factual summaries in this setting (Zhang et al., 2024).\\n\\nStep 2: Claim decomposition and subclaim augmentation\\n\\nSimilar to the C2D method, for a summary sentence $c_i$ in $c$, we decompose it into atomic facts $a_i = \\\\{a_{i,1}, \\\\ldots, a_{i,l}\\\\}$, and create a set of augmented subclaims $\\\\text{Aug}(c_i) = \\\\{\\\\text{Merge}(a'_i) : \\\\forall a'_i \\\\in \\\\text{Power}(a_i)\\\\}$.\\n\\nStep 3: Document-claim augmentation\\n\\nThis step aims to do data augmentation on a $(D_i, c_i)$ pair. Given a chunk $D_i = \\\\text{Concat}(s)$, which is the\"}"}
{"id": "emnlp-2024-main-499", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Type   | Data Size | Uniq. Claim | Uniq. Doc | Doc Len | Claim Len | % of Neg |\\n|--------|-----------|-------------|-----------|---------|-----------|----------|\\n| C2D    | 7076      | 2004        | 1188      | 189     | 19        | 42%      |\\n| D2C    | 7319      | 1392        | 4967      | 164     | 12        | 65%      |\\n\\nTable 1: Statistics of synthetic training data.\\n\\nWe construct new documents by iteratively removing each sentence $s_{i,j}$ from $s_i$:\\n\\n$$D'_i \\\\setminus j = \\\\text{Concat}(s \\\\{s_{i,j}\\\\})$$\\n\\nWe then determine the entailment label for each atomic fact $a_{i,k}$ in $c_i$, where $k \\\\in \\\\{1, \\\\ldots, l\\\\}$:\\n\\n$$L^{-j}(a_{i,k}) = \\\\text{Ent}(D'_i \\\\setminus j, a_{i,k}) \\\\in \\\\{0, 1\\\\}$$\\n\\nSimilar to step 5 in C2D, if $L^{-j}(a_{i,k}) = 1$ for all $a_{i,k} \\\\in a'_i$, we create tuples $(D'_i \\\\setminus j, \\\\text{Merge}(a'_i), 1)$.\\n\\nConversely, if there exists any $a_{i,k} \\\\in a'_i$ such that $L^{-j}(a_{i,k}) = 0$, we then create tuples $(D'_i \\\\setminus j, \\\\text{Merge}(a'_i), 0)$.\\n\\nStep 4: Cross-document-claim augmentation\\n\\nThe objective of this step is to perform data augmentation on a $(D_j, c_i)$ pair, where $j \\\\neq i$. The rationale behind this is that the important information in a document can be conveyed multiple times in various ways. Given that each chunk $D_i$ has an associated summary $c_i$, it is probable that the summary $c_i$ conveys some information that can be indirectly supported by other chunks $D_j$ within the document, even if $D_j$ are not used to generate $c_i$.\\n\\nTherefore, we consider chunks $D_j$, where $j \\\\neq i$, as more challenging chunks to either support or refute the claim $c_i$ or its atomic facts $a_{i,k}$.\\n\\nMore formally, we determine the entailment label for each atomic fact $a_{i,k}$ in $c_i$, using the document chunk $D_j$, where $k \\\\in \\\\{1, \\\\ldots, l\\\\}$, and $j \\\\neq i$:\\n\\n$$L_{D_j}(a_{i,k}) = \\\\text{Ent}(D_j, a_{i,k}) \\\\in \\\\{0, 1\\\\}$$\\n\\nIf $L_{D_j}(a_{i,k}) = 1$ for all $a_{i,k} \\\\in a'_i$, we create tuples $(D_j, \\\\text{Merge}(a'_i), 1)$.\\n\\nConversely, if there exists any $a_{i,k} \\\\in a'_i$ such that $L_{D_j}(a_{i,k}) = 0$, we then create tuples $(D_j, \\\\text{Merge}(a'_i), 0)$.\\n\\n3.3 MINICHECK Models\\n\\nWe fine-tune three models with various model architectures by leveraging our synthetic data. We use the standard cross-entropy loss for all models. See Appendix G for training details.\\n\\nMiniCheck-D\\n\\n$\\\\text{DeBERTa-v3-large}$ (He et al., 2021) and $\\\\text{flan-T5-large}$ (Chung et al., 2022). We take a subset (21K) of the ANLI training data, selecting examples where their trained entailment models made incorrect predictions during dataset construction. Training on more of ANLI was not effective.\\n\\nCombining these 21K datapoints with our 14K-sized dataset, we have 35K training datapoints in total. We map the labels $\\\\text{contradiction}$ and $\\\\text{neutral}$ from ANLI to $\\\\text{unsupported}$.\\n\\nMiniCheck-R\\n\\nWe also explore whether it is possible to improve upon the previous $\\\\text{AlignScore}$ (Zha et al., 2023) system, the existing SOTA specialized fact-checking model. We fine-tune the tuned $\\\\text{roberta-large}$ (Liu et al., 2019) model from $\\\\text{AlignScore}$ with a binary classification head, on our 14K synthetic datapoints.\\n\\nProducing classification decisions\\n\\nAlthough our task is framed as binary classification, in reality the models we have are of the form $\\\\mathbf{M}(D_i, c_i) \\\\rightarrow z \\\\in \\\\mathbb{R}$, mapping each $(document, claim)$ pair to a score in the range $z \\\\in [v_{\\\\text{min}}, v_{\\\\text{max}}]$. Following Laban et al. (2022); Zha et al. (2023); Tang et al. (2023a), we convert each method into a binary classifier $\\\\mathbf{M}(D_i, c_i) \\\\rightarrow \\\\{0, 1\\\\}$ by picking a threshold $t$ such that we predict 1 if $\\\\mathbf{M}(D_i, c_i) > t$ and 0 otherwise. Unless otherwise specified, we set $t = 0$.  \\n\\n5. LLM-AGREEFACT Benchmark\\n\\nWe construct a fact verification benchmark, LLM-AGREEFACT, by aggregating 10 of the most up-to-date publicly available datasets on factual consistency evaluation across both closed-book and grounded generation settings.\\n\\nCharacteristics\\n\\nIn LLM-AGREEFACT, all datasets contain human-annotated $(document, claim, label)$ tuples. The documents come from diverse sources, including Wikipedia paragraphs, interviews, web text, covering domains such as news, dialogue, science, and healthcare. The claims to be verified are mostly generated from recent generative models (except for one dataset...\"}"}
{"id": "emnlp-2024-main-499", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: 10 datasets in LLM-A\\n\\nValidation/Test set split\\nFor the datasets from A\\\\textsc{GGRE}\\\\textsc{F}\\\\textsc{ACT} and T\\\\textsc{OFU}\\\\textsc{E}\\\\textsc{VAL}, as well as W\\\\textsc{ICE} and C\\\\textsc{LAIM}\\\\textsc{V}\\\\textsc{ERIFY}, we directly use the existing validation and test splits from the original work. For R\\\\textsc{E}\\\\textsc{VEAL}, F\\\\textsc{ACT}\\\\textsc{C}\\\\textsc{HECK}-GPT, E\\\\textsc{XPERT}\\\\textsc{QA} and L\\\\textsc{FQA}, we randomly divide each of them into validation and test sets (50%/50%), assuring that responses to unique queries do not appear in both sets.\\n\\nOne potential use of the validation data is to allow for per-dataset threshold tuning. This setting is used in substantial past work (Laban et al., 2022; Luo et al., 2023; Zha et al., 2023; Tang et al., 2023a, 2024). However, we do not follow this trend in order to focus on building systems that can be deployed zero-shot across multiple downstream tasks, without any additional hyperparameter tuning. Instead, for $M(d, c) \\\\rightarrow z \\\\in [v_{min}, v_{max}]$, the threshold is set as the midpoint of the output score range $t = (v_{min} + v_{max})/2$, which is 0.5 for most fact-checkers. In practice, most fact-checkers return scores at the extremes of the range, so small tweaks on this procedure have little effect. See Appendix B.1 for the results of the threshold tuning setting, which yields qualitatively similar results.\\n\\nEvaluation Metric\\nFollowing Laban et al. (2022); Fabbri et al. (2022); Tang et al. (2023a), we evaluate the performance of fact-checkers using balanced accuracy (BAcc):\\n\\n$$BAcc = \\\\frac{1}{2} \\\\left( \\\\frac{TP}{TP+FN} + \\\\frac{TN}{TN+FP} \\\\right)$$\\n\\nwhere TP, TN, FP, and FN represent true/false positives/negatives.\\n\\n5 Experimental Setup\\nWe include the following specialized fact-checkers:\\n\\n- T5-NLI-Mixed (Honovich et al., 2022)\\n- DAE (Goyal and Durrett, 2021)\\n- QAFactEval (Fabbri et al., 2022)\\n- SummaC-ZS and SummaC-CV (Laban et al., 2022)\\n- AlignScore (Zha et al., 2023)\\n- FT5-ANLI-L that fine-tunes flan-t5-large on the full ANLI training set. A meta-comparison of those specialized fact-checkers and our models can be found in Table 3. More inference details can be found in Appendix E.2.\\n\\nWe also include the following LLMs as fact-checkers:\\n\\n- Gemini-Pro (Team et al., 2023)\\n- PaLM2-Bison (Thoppilan et al., 2022)\\n- Mistral-8x7B, Mistral-Large (Jiang et al., 2024)\\n- Claude 2.1, Claude 3 Opus (Bai et al., 2022)\\n- GPT-3.5 and GPT-4 (OpenAI, 2023).\\n\\nMore details about the models can be found in Appendix E.1. For the LLM-based fact-checkers, we adapt a prompt from Luo et al. (2023) for zero-shot prediction, which can be found in Appendix H.\\n\\n6 Results\\n6.1 Main Results\\nOur synthetic data improves performance across diverse model architectures. Table 2 demonstrates that our synthetic data gives strong performance when used in three different backbone models: RoBERTa, DeBERTa, and Flan-T5. These models outperform prior models of a similar scale. Notably, MiniCheck-FT5 achieves a 4.3% overall improvement over AlignScore, outperforming it on 6 out of 10 datasets and matching its performance on the remaining 4. We attribute its additional 2% gain over MiniCheck-R\\\\textsc{BTA} and -D\\\\textsc{BTA} to its larger model size. However, model size alone does not guarantee superior performance, as evidenced by T5-NLI-Mixed and FT5-ANLI-L, which, despite being trained on NLI datasets, underperform on most of the benchmark settings. This underscores the importance of training data selection in addition to model capacity.\\n\\nOur models achieve performance on par with the most capable LLM-based fact-checkers.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Performance (BAcc) of models on the test set of LLM-A\\n\\nModels are split into LLM-based fact-checkers | specialized fact-checkers | Ours. We highlight the best performance for each dataset, where multiple green highlights indicate systems indistinguishable from the best according to a paired bootstrap test with 1000 runs and p-value < 0.05. Details for -Dectx and -Decmp are in Section 7. Our MiniCheck models outperform other specialized evaluators and MiniCheck-FT5 reaches the performance of GPT-4.\\n\\n| Model Name       | Backbone     | Model Size | # FT Data | Cost ($) |\\n|------------------|--------------|------------|-----------|----------|\\n| T5-NLI-Mixed     | T5-XXL       | 11B        | 1,697K    | 7.39     |\\n| FT5-ANLI-L       | Flan-T5-L    | 770M       | 163K      | 0.24     |\\n| DAE              | ELECTRA-B    | 110M       | 95K       | 0.26     |\\n| QAFactEval       | multiple     | \u2217           | 1.4B - 1.87|          |\\n| SummaC-ZS        | ALBERT-XL    | 60M        | 371K      | 0.85     |\\n| SummaC-CV        | ALBERT-XL    | 60M        | 381K      | 0.85     |\\n| AlignScore       | RoBERTa-L    | 355M       | 4,700K    | 0.20     |\\n| MiniCheck-R      | AlignScore   | 355M       | 14K       | 0.20     |\\n| MiniCheck-D      | DeBERTa-L    | 355M       | 35K       | 0.20     |\\n| MiniCheck-FT5    | Flan-T5-L    | 770M       | 35K       | 0.24     |\\n\\n### Table 3: Comparison of specialized fact-checkers on model sizes, training data sizes, and the inference cost ($0.8/GPU-hr) on the 13K LLM-A test set.\\n\\n\u2217QAFactEval contains several model components, which sum up to 1.4B in size.\\n\\nIn the top rows of Table 2, we present the performance of strong LLM-based fact-checkers. We observe that existing specialized fact-checkers achieve similar performance to non-frontier LLM-based fact-checkers like Mistral-8x7B and GPT-3.5. MiniCheck-R and MiniCheck-D can surpass these non-frontier LLM-based fact-checkers by a large margin. MiniCheck-FT5 achieves the same performance as Claude-3 Opus and is close to GPT-4, but with a much smaller model size.\\n\\n### Extended Analysis\\nSee Appendix A for an intrinsic evaluation on our synthetic data and an ablation study on our best model MiniCheck-FT5.\\n\\n### 6.2 Computational Cost Comparison\\nWe compare the computational cost of specialized fact-checkers and LLMs on the test set of LLM-A. For specialized fact-checkers, we use our own hardware and convert the prediction time on our GPUs to the equivalent cost of using cloud computing services (see Appendix E.2.1 for details). For LLM-based fact-checkers, we compute the costs of corresponding API calls. Results are shown in Table 3 and 4. We see that specialized models in general have much lower inference costs. In particular, our most capable model MiniCheck-FT5 has almost the same performance as GPT-4 but is more than 400 times cheaper.\\n\\n### 7 Rethinking LLM Fact-Checking\\nWe now revisit two other stages of the typical LLM fact-checking pipeline: claim decomposition and decontextualization. Surprisingly, we find that claim decomposition is not needed in our settings, contradicting prior work (Yang and Zhu, 2021; Kamoi et al., 2023). Furthermore, we find that decontextualization doesn\u2019t help on our benchmark.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Inference cost comparison for API models on the 13K LLM-A GCRE F\\n\\n| Model Name | Cost ($) |\\n|------------|----------|\\n| Gemini-Pro | 5.24     |\\n| Claude-2.1 | 89.9     |\\n| PaLM2-Bison| 10.9     |\\n| Claude-3 Opus | 165 |\\n| Mistral-8x7B | 7.78   |\\n| GPT-3.5   | 4.75     |\\n| Mistral-Large | 90.2 |\\n| GPT-4     | 107      |\\n| GPT-4-Dectx | 161   |\\n| GPT-4-Decmp | 212    |\\n\\nTable 5: Average performance on the test set of LLM-A GCRE F\\n\\n| Model Name        | Original | After Decontextualization |\\n|-------------------|----------|--------------------------|\\n| GPT-4             | 75.6 (+0.3) | 75.3 (+0.0)        |\\n| SummaC-CV         | 58.8 (-3.3) | 60.8 (+1.3)    |\\n| QAFactEval        | 64.6 (-1.9) | 66.4 (+0.1)    |\\n| SummaC-ZS         | 69.1 (+1.2) | 67.7 (-0.2)    |\\n| AlignScore        | 71.5 (+1.1) | 70.4 (+0.0)    |\\n| MiniCheck-R BTA   | 73.2 (+0.5) | 72.4 (-0.3)    |\\n| MiniCheck-D BTA   | 72.7 (+0.1) | 71.2 (-1.4)    |\\n| MiniCheck-FT5     | 73.3 (-1.4) | 74.1 (-0.6)    |\\n\\n7.1 Claim Decomposition\\nWe also experiment with a setting using claim decomposition. In this setting, we decompose each claim $c_i$ into atomic facts $a_{i,k}$ with the prompt from Kamoi et al. (2023) and use a fact-checker to predict the factuality label for each $(D_i, a_{i,k})$ pair, $k \\\\in \\\\{1, \\\\ldots, l\\\\}$. If all atomic facts are supported by the document, then the claim is supported, and unsupported otherwise. We do this for every dataset except FactCheck-GPT which is already atomic facts. There are typically 2-4 atomic facts per claim across datasets.\\n\\nWe show the results from GPT-4 and a subset of specialized fact-checkers in Table 5. We observe near-zero performance change for GPT-4 and mixed changes for specialized fact-checkers. Overall, there is no clear indication that decomposing claims into atomic facts can consistently improve models' performance. Because this approach increases the inference time and costs by a factor of 2-4 for different datasets, depending on the average number of atomic facts per claim, we believe it should not be used until it provides a clear accuracy benefit.\\n\\n7.2 Claim Decontextualization\\nAs mentioned in Section 2, our approach relies on being able to check each sentence in isolation. However, phenomena like coreference and ellipsis may make sentences difficult to ground out of context. We can address this with an explicit decontextualization step (Choi et al., 2021; Wang et al., 2023; Jacovi et al., 2024). We experiment with TOFUEVAL-MediaS, TOFUEVAL-MeetB, WICE, REVEAL, CLAIMVERIFY, EX-PERTQA and LFLQA, which are the datasets in our benchmark where sentences need to be interpreted in context (FACTCHECK is already decontextualized). We prompt GPT-4 for decontextualization as shown in Appendix H, using the previous claims or response sentences as context to expand the claim. Respectively, 33%, 33%, 39%, 11%, 35%, 47%, and 57% of the claims from those datasets are changed after decontextualization.\\n\\nIn Table 5, we show the average fact-checking performance when using this decontextualization step (see the prompt in Table 24). These results suggest that models may make decent guesses about context-dependent content, particularly when the retrieval stage already implicitly enforces shared context between the claim and the grounding documents. However, for tasks such as retrieval-augmented generation, we believe decontextualization still plays a crucial role in ensuring meaningful document retrieval. Furthermore, as LLMs scale further and their responses get more complex, the level of contextualization they feature may be higher, making this step more necessary.\\n\\n8 Related Work\\nHallucinations in LLMs\\nLLMs are prone to hallucinations across various settings (Huang et al., 2023; Zhang et al., 2023b; Rawte et al., 2023), generating information that cannot be supported by any source. For example, in the closed-book setting, where LLMs rely solely on their parametric knowledge, they may fabricate details when describing biographies or providing Wikipedia entity information (Min et al., 2023; Guan et al., 2023; Mallen...\"}"}
{"id": "emnlp-2024-main-499", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al., 2023). In retrieval-augmented settings, where models have access to external documents to provide responses to user queries, they may generate supplementary information that is not faithful to the provided documents (Chiesurin et al., 2023; Adlakha et al., 2023; Chen et al., 2024). Even when LLMs are provided with gold documents, such as in text summarization and simplification tasks, they still generate factually inconsistent outputs with diverse error types across different domains (Joseph et al., 2023; Shaib et al., 2023; Tang et al., 2024, 2023c). In this work, we construct a new benchmark dataset, LLM-AGREFACT, which unifies human-annotated model responses across all settings, and evaluate the performance of existing fact-checkers and our proposed ones on the benchmark in detecting such errors.\\n\\nMethods in Detecting Hallucinations\\n\\nWhen documents are directly available for model-generated sentences, such as in text summarization (Falke et al., 2019; Kryscinski et al., 2020; Maynez et al., 2020; Fabbri et al., 2021; Tang et al., 2023a) or retrieval-augmented generation (Liu et al., 2023; Malaviya et al., 2024), the entire claims are directly verified against the source documents. However, in cases where such documents are not readily available, such as in close-book generation, Gao et al. (2023); Min et al. (2023); Wang et al. (2023) decompose each generated sentence into atomic facts and then search for relevant documents to support each atomic fact. Alternatively, Malaviya et al. (2024) directly search for relevant documents for each sentence as a whole.\\n\\nThere are two main approaches to verifying sentences against documents. The first involves training specialized fact-checkers specifically designed for factual consistency evaluation, which are primarily evaluated in the context of summarization (Kryscinski et al., 2020; Fabbri et al., 2022; Goyal and Durrett, 2020; Laban et al., 2022). The second approach leverages LLMs as fact-checkers, particularly for evaluating LLM-generated responses from retrieval-augmented generation and closed-book generation (Min et al., 2023; Wang et al., 2023; Malaviya et al., 2024; Gao et al., 2023). In this work, we bridge the gap between these two approaches by evaluating both specialized fact-checkers and LLM-based fact-checkers across all these settings using our new benchmark, LLM-AGREFACT. We show that our best model can match GPT-4 performance and perform well in all settings without doing sentence decomposition.\\n\\nEntailment Datasets\\n\\nOur work contributes a new dataset for training textual entailment models over documents or document-chunks. Most prior entailment datasets have been human-authored (Bowman et al., 2015; Williams et al., 2018), which is known to introduce artifacts (Gururangan et al., 2018), or collected in the wild (Kamoi et al., 2023), which is challenging to scale. Past work has automatically generated contrast sets for NLI (Li et al., 2020). DocNLI (Yin et al., 2021) is a restructure of existing datasets where the length of most training examples cannot fit into the input limit of small models. Our work differs from these in its hand-built, synthetic nature to encourage multi-sentence and multi-fact reasoning, which is important to the task of fact-checking on grounding documents.\\n\\nConclusion\\n\\nIn this work, we introduce two synthetic data generation methods that address key limitations of specialized fact-checkers by encouraging models to verify each atomic fact within a claim and reason across multiple sentences. We also present LLM-AGREFACT, a new factual consistency evaluation dataset covering both closed-book and grounded generation settings. A model fine-tuned on our synthetic data outperforms all prior specialized fact-checkers on LLM-AGREFACT, while being much cheaper than LLM-based fact-checkers.\\n\\nLimitations\\n\\nInterpretation\\n\\nLike many other specialized fact-checking models, our models do not reveal their internal decision-making processes, making it challenging to localize errors to particular mismatched spans of a claim or document. There are two ways to alleviate this issue. The first is to perform claim decomposition and check the models' prediction labels on each atomic fact, thereby localizing the error from the original claim. Our models show better performance compared to other specialized fact-checkers when using this claim decomposition method (Table 5), but this method can give greater interpretability. The second approach, which can be a future research direction, is to enable our best model, MiniCheck-FT5, or generative models in general, to provide reliable explanations in addition to the binary predictions. We believe a model can provide reliable explanations only if it can first correlate the model's predictions with the input sentences.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"directly identify errors in our binary setting, which was the focus of this work.\\n\\nMulti-Document Reasoning\\nWhile our benchmark includes instances that evaluate models' ability to reason across multiple sentences, these datasets do not necessitate reasoning over evidence that is significantly separated or spread across various documents. Future research could focus on evaluating the performance of existing models in such scenarios, creating new labeled datasets of errors to expand our benchmark, and developing better fact-checking models to handle these challenges.\\n\\nSynthetic Data\\nThe effectiveness of our synthetic data is demonstrated by the improved performance when training on it across various model architectures. We find that using a pair of sentences to support a fact is a simple method that yields useful training data for our models. However, there are other possible strategies for how atomic facts or claims could be expanded into multiple sentences. We believe that constructing more complex and higher-quality data could be a future direction not only for this work but also for other related tasks. As LLMs continue to advance, the quality of the synthetic data generated using our approach is also expected to improve.\\n\\nLanguage\\nOur models are trained exclusively on English data. Although the backbone model, Flan-T5, is trained on multilingual data, we have not systematically assessed how well our model's performance extends to other languages due to the absence of a human-annotated factual consistency evaluation dataset for LLM-generated outputs in non-English languages. We believe that developing a fact-checker that can perform well across multiple languages is important for future work.\\n\\nAcknowledgments\\nWe would like to thank Jessy Li for comments on a draft of this work. This work was principally supported by a gift from Amazon as part of the UT-Amazon Science Hub. It was partially supported by NSF CAREER Award IIS-2145280, the NSF AI Institute for Foundations of Machine Learning (IFML), a grant from Open Philanthropy, a grant from the UT Austin Office of the Vice President for Research through the \\\"Creating Connections for National Security Research Grants\\\" program, and Good Systems, a UT Austin Grand Challenge to develop responsible AI technologies.\\n\\nReferences\\nVaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade, and Siva Reddy. 2023. Evaluating correctness and faithfulness of instruction-following models for question answering. arXiv preprint arXiv:2307.16877.\\n\\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conterly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional AI: Harmlessness from AI feedback.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal. Association for Computational Linguistics.\\n\\nMeng Cao, Yue Dong, and Jackie Cheung. 2022. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3340\u20133354, Dublin, Ireland. Association for Computational Linguistics.\\n\\nShuyang Cao and Lu Wang. 2021. CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633\u20136649, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nHung-Ting Chen, Fangyuan Xu, Shane A Arora, and Eunsol Choi. 2023. Understanding retrieval augmentation for long-form question answering. arXiv preprint arXiv:2310.12150.\\n\\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Thirty-Eighth...\"}"}
{"id": "emnlp-2024-main-499", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-499", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-499", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2024. ExpertQA: Expert-curated questions and attributed answers. In 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics.\\n\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9802\u20139822, Toronto, Canada. Association for Computational Linguistics.\\n\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, Online. Association for Computational Linguistics.\\n\\nNick McKenna, Tianyi Li, Liang Cheng, Mohammad Hosseini, Mark Johnson, and Mark Steedman. 2023. Sources of hallucination by large language models on inference tasks. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2758\u20132774, Singapore. Association for Computational Linguistics.\\n\\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076\u201312100, Singapore. Association for Computational Linguistics.\\n\\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence rnns and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. Association for Computational Linguistics.\\n\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Brussels, Belgium. Association for Computational Linguistics.\\n\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020a. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885\u20134901, Online. Association for Computational Linguistics.\\n\\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020b. What can we learn from collective human opinions on natural language inference data? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9131\u20139143, Online. Association for Computational Linguistics.\\n\\nOpenAI. 2023. GPT-4 technical report. ArXiv, abs/2303.08774.\\n\\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812\u20134829, Online. Association for Computational Linguistics.\\n\\nEllie Pavlick and Tom Kwiatkowski. 2019. Inherent disagreements in human textual inferences. Transactions of the Association for Computational Linguistics, 7:677\u2013694.\\n\\nFabio Petroni, Samuel Broscheit, Aleksandra Piktus, Patrick Lewis, Gautier Izacard, Lucas Hosseni, Jane Dwivedi-Yu, Maria Lomeli, Timo Schick, Michele Bevilacqua, Pierre-Emmanuel Mazar\u00e9, Armand Joulin, Edouard Grave, and Sebastian Riedel. 2023. Improving Wikipedia verifiability with AI. Nature Machine Intelligence, 5(10):1142\u20131148.\\n\\nVipula Rawte, Amit Sheth, and Amitava Das. 2023. A survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922.\\n\\nSoumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang, and Xiang Ren. 2024. Minds versus machines: Rethinking entailment verification with language models. arXiv preprint arXiv:2402.03686.\\n\\nChantal Shaib, Millicent Li, Sebastian Joseph, Iain Marshall, Junyi Jessy Li, and Byron Wallace. 2023. Summarizing, simplifying, and synthesizing medical evidence using GPT-3 (with varying success). In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1387\u20131407, Toronto, Canada. Association for Computational Linguistics.\\n\\nLiyan Tang, Tanya Goyal, Alex Fabbri, Philippe Labban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscinski, Justin Rousseau, and Greg Durrett. 2023a. Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11626\u201311644, Toronto, Canada. Association for Computational Linguistics.\\n\\nLiyan Tang, Yifan Peng, Yanshan Wang, Ying Ding, Greg Durrett, and Justin Rousseau. 2023b. Less likely brainstorming: Using language models to generate alternative hypotheses. In Findings of the Association for Computational Linguistics: ACL 2023, pages 12532\u201312555, Toronto, Canada. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Liyan Tang, Igor Shalyminov, Amy Wing mei Wong, Jon Burnsky, Jake W. Vincent, Yu'an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab Mansour, and Kathleen McKeown. 2024. TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization.\\n\\nLiyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G. Nestor, Ali Soroush, Pierre A. Elias, Ziyang Xu, Ying Ding, Greg Durrett, Justin F. Rousseau, Chunhua Weng, and Yifan Peng. 2023c. Evaluating large language models on medical evidence summarization. *npj Digital Medicine*, 6(1).\\n\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).\\n\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, and et al. 2023. Gemini: A family of highly capable multimodal models.\\n\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam M. Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, I. A. Krivokon, Willard James Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Hartz S\u00f8raker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark D\u00edaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, V. O. Kuzmina, Joseph Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Rogers Croak, Ed Huai hsin Chi, and Quoc Le. 2022. LaMDA: Language Models for Dialog Applications. *ArXiv*, abs/2201.08239.\\n\\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 5008\u20135020, Online. Association for Computational Linguistics.\\n\\nYuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, and Preslav Nakov. 2023. Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output. *ArXiv*, abs/2311.09000.\\n\\nYixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. 2023. Large language models are better reasoners with self-verification. In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages 2550\u20132575, Singapore. Association for Computational Linguistics.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nXiaoyu Yang and Xiaodan Zhu. 2021. Exploring decomposition for table-based fact verification. In *Findings of the Association for Computational Linguistics: EMNLP 2021*, pages 1045\u20131052, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nWenpeng Yin, Dragomir Radev, and Caiming Xiong. 2021. DocNLI: A large-scale dataset for document-level natural language inference. In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*, pages 4913\u20134922, Online. Association for Computational Linguistics.\\n\\nYuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. AlignScore: Evaluating factual consistency with a unified alignment function. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 11328\u201311348, Toronto, Canada. Association for Computational Linguistics.\\n\\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. 2023a. How language model hallucinations can snowball. *arXiv preprint arXiv:2305.13534*.\\n\\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2024. Benchmarking large language models for news summarization. *Transactions of the Association for Computational Linguistics*, 12:39\u201357.\\n\\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023b. Siren's song in the AI ocean: a survey on hallucination in large language models. *arXiv preprint arXiv:2309.01219*.\\n\\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful prompting for large language models. In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages 14544\u201314556, Singapore. Association for Computational Linguistics.\\n\\nChenguang Zhu, Yang Liu, Jie Mei, and Michael Zeng. 2021. MediaSum: A large-scale media interview dataset for dialogue summarization. In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 8831\u20138839, Online. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Intrinsic Evaluation on C2D/D2C\\n\\nOur model achieves strong overall performance, but we would like to have more insight as to whether it actually does well at the types of instances in D2C and C2D. We evaluate the performance of QAFactEval, SummaC-ZS, SummaC-CV, AlignScore, FT5-ANLI-L on our held-out synthetic data of C2D and D2C as a way to understand their ability to reason over multiple sentences and consider multiple atomic facts within a claim. There are 2K held-out instances from C2D and D2C, respectively, in the format of (document, claim, label) tuples. Performance is measured by BAcc.\\n\\nSynthetic data ablations\\n\\nBeside those specialized models, we fine-tune flan-t5-large on the training set of C2D (FT5-C2D) and D2C (FT5-D2C), respectively. We evaluate FT5-C2D on D2C as an out-of-distribution (OOD) evaluation set, and evaluate FT5-D2C on C2D.\\n\\nTo demonstrate the necessity of steps in creating our synthetic data, we also create simplified versions of the synthetic data for both methods, denoted as C2D-S_IMP and D2C-S_IMP, each comprising 7K training examples, same as in C2D and D2C. For the C2D-S_IMP method, we ask GPT-4 to directly generate documents that support and not support a provided claim, with the requirement mentioned in the prompt that the inference on the claim should require reasoning over multiple sentences from a document. For the D2C-S_IMP method, we ask GPT-4 to generate summary sentences for Google News articles and come up with unsupported summaries by injecting errors into those summary sentences following the method in SummEdit (Laban et al., 2023). We denote models trained on those simplified synthetic data as FT5-C2D-S and FT5-D2C-S. More details for creating these synthetic datasets can be found in Appendix F.\\n\\nSynthetic data needs to be carefully constructed for a fact-checker to work well.\\n\\nFigure 5 shows the in-distribution performance of FT5-C2D and FT5-D2C as optimal performance in each sub-figure, represented by the black dashed lines. We observe that models trained on synthetic data with simplified construction steps (C2D-S_IMP and D2C-S_IMP) fail to develop the desired properties we expect. FT5-D2C-S performs even worse than random chance on the held-out set of C2D. In contrast, models trained on C2D and D2C outperform all other fact-checkers on the OOD held-out set of D2C and C2D, respectively. Additionally, we note that the model trained on 163K ANLI data points fail to reach the performance of models trained solely on 7K synthetic data. Our synthetic data generation methods can effectively encourage models to pay more attention to multiple atomic facts and reason over multiple sentences, even with a limited amount of training data.\\n\\nA.2 Ablation of D2C/C2D\\n\\nWe observe that there are still gaps between the optimal performance and that achieved by other specialized fact-checkers in Figure 5. Notably, AlignScore demonstrates the best performance among the four specialized metrics, but its performance can still be outperformed by FT5-C2D and FT5-D2C. As shown in Table 2, we can improve AlignScore's performance on the benchmark by fine-tuning it on this small amount of data, effectively equipping it with the desired properties.\\n\\nTo further investigate, we conducted an ablation study on our top-performing model, MiniCheck-FT5, by removing our synthetic data from the training set. The results, presented in Table 6, reveal that the two types of synthetic data complement each other. Notably, the model performs poorly when trained solely on the ANLI subset, with a performance drop of nearly 10% in the absence of threshold tuning. However, the addition of either 7K C2D or 7K D2C to the training data significantly enhances the model's performance and...\"}"}
{"id": "emnlp-2024-main-499", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Ablation study on the training data. Models are evaluated on the test set of LLM-A GGRE FACT (without threshold tuning). We show the average performance downgrade in red. - BOTH drops from 69.1 to 59.9 without threshold tuning.\\n\\nTable 7: Performance of models on the test set of LLM-A GGRE FACT with threshold tuning on the validation set. Balanced accuracy is computed for each model on the 10 datasets in LLM-A GGRE FACT, and the average is computed. In each dataset, a factuality metric selects a threshold based on the performance on the corresponding validation set.\\n\\nA.3 Human Evaluation of Synthetic Data\\n\\nWhile we use automatic entailment checks to ensure label quality in the training data construction pipeline, we conduct a small scale human evaluation to measure the quality of the generated data. We randomly chose 40 (document, claim) pairs from each of the C2D and D2C training data. Three authors of the work independently annotated these 40 \u00d7 2 datapoints as supported or not supported (without seeing the gold label), with an average annotation time of around 1 min and 40 seconds per example. Among the three annotators, we computed an annotation agreement using Fleiss' Kappa, obtaining a score of 0.51 for the C2D samples and 0.70 on the D2C samples, indicating moderate to substantial agreement. The annotators adjudicated cases with disagreement to reach a consensus on the final factuality of the labels. We refer to these as the ground truth labels.\\n\\nCompared to these ground truth labels, the labels given to our training samples have an accuracy of 80% on C2D and 78% on D2C, respectively. We calculated similar accuracy values for the human annotators, and the average across the three annotators' accuracies was 85% on C2D and 88% on D2C. These accuracies demonstrate that the automatic labels are a bit lower-quality than single-annotator labels on D2C, but close on C2D.\\n\\nMany of the disagreement cases are classic cases of subjectivity in NLI (Pavlick and Kwiatkowski, 2019; Chen et al., 2020; Nie et al., 2020b). For example, for the C2D claim \\\"Located in the Scottish Highlands, she lived with and later married James Ballard in Spean Bridge.\\\", the generated passage only references the couple living in the town of Spean Bridge after the marriage, making it unclear whether or not they got married there. However, this is a reasonable supposition to make. Our results show that our data is useful for training in spite of these subjective examples.\\n\\nB Additional Results\\n\\nB.1 Results using Threshold Tuning\\n\\nAs shown in Table 7, AlignScore achieves the highest overall performance (72.2%) on LLM-A GGRE FACT (without threshold tuning).\"}"}
{"id": "emnlp-2024-main-499", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Name | A | G  | R | E | F | ACT | TOFU | VAL | W | ICE | REVEAL | CLAIM | VERIFY | FACT | CHECK | EXPERT | QA | XPERT | L | FQA | Avg |\\n|------------|---|----|---|---|---|-----|------|-----|---|-----|--------|-------|--------|------|-------|--------|---|-------|---|-----|-----|\\n| CNN XSum  |   |    |   |   |   |     |      |     |   |     |        |       |        |      |       |        |   |       |   |     |     |\\n| MediaS    |   |    |   |   |   |     |      |     |   |     |        |       |        |      |       |        |   |       |   |     |     |\\n| MeetB     |   |    |   |   |   |     |      |     |   |     |        |       |        |      |       |        |   |       |   |     |     |\\n| GPT-4     | 70.1 | 74.3 | 70.9 | 79.4 | 77.6 | 87.4 | 73.2 | 79.9 | 59.6 | 84.9 | 75.6 | (+0.3) |       |      |       |        |   |       |   |     |     |\\n| SummaC-CV | 65.2 | 51.2 | 58.0 | 55.2 | 50.9 | 66.2 | 69.6 | 53.4 | 53.5 | 65.0 | 58.8 | (-3.3) |       |      |       |        |   |       |   |     |     |\\n| QAFactEval| 61.7 | 60.4 | 65.0 | 60.2 | 59.1 | 81.7 | 68.6 | 66.1 | 54.6 | 73.0 | 64.6 | (-1.9) |       |      |       |        |   |       |   |     |     |\\n| SummaC-ZS | 62.4 | 64.5 | 64.5 | 70.0 | 64.8 | 85.6 | 70.0 | 75.2 | 56.3 | 77.2 | 69.1 | (+1.2) |       |      |       |        |   |       |   |     |     |\\n| AlignScore | 65.6 | 68.3 | 71.2 | 73.2 | 63.9 | 86.2 | 73.7 | 74.3 | 57.6 | 82.6 | 71.5 | (+1.1) |       |      |       |        |   |       |   |     |     |\\n| MiniCheck-R | 65.0 | 72.1 | 72.6 | 75.1 | 66.9 | 88.5 | 76.1 | 73.3 | 58.0 | 84.3 | 73.2 | (+0.5) |       |      |       |        |   |       |   |     |     |\\n| MiniCheck-D | 59.9 | 73.3 | 67.9 | 74.5 | 75.0 | 88.2 | 72.5 | 73.0 | 57.9 | 84.5 | 72.7 | (+0.1) |       |      |       |        |   |       |   |     |     |\\n| MiniCheck-FT5 | 65.9 | 71.7 | 69.2 | 77.4 | 72.9 | 87.0 | 73.5 | 74.7 | 57.5 | 83.2 | 73.3 | (-1.4) |       |      |       |        |   |       |   |     |     |\\n\\nTable 8: Performance of models on the test set of LLM-A by aggregating predictions on decomposed claims. We include the performance change compared to predicting using original claims from Table 2 (red for worse performance and green for better performance).\\n\\nB.2 Full Results Using Claim Decomposition\\n\\nIn Table 8, we show the performance (and performance changes) of GPT-4 and a subset of specialized fact-checkers across all datasets from LLM-A by doing claim decomposition where it is applicable. We include the performance change compared to predicting using original claims from Table 2.\\n\\nB.3 Full Results Using Claim Decontextualization\\n\\nIn Table 9, we show the performance (and performance changes) of GPT-4 and a subset of specialized fact-checkers across all datasets from LLM-A by doing claim decontextualization when applicable. In particular, we perform claim decontextualization on TOFU EVAL-MediaS, among fact-checkers from prior work. However, by fine-tuning AlignScore's backbone RoBERTa model on our 14K synthetic data (MiniCheck-R BTA), we surpass AlignScore's performance by 1.5%. This improvement is more significant in the setting without threshold tuning (Section 6). Remarkably, this boost in performance is attained using a dataset that constitutes less than 0.3% of the total data on which AlignScore was initially trained (Table 3). This finding highlights the potential of curated synthetic data in enhancing the performance of state-of-the-art fact-checkers.\\n\\nOverall, our models achieve new state-of-the-art among specialized models under the threshold tuning setting.\\n\\nOur synthetic data enhances model robustness and performance in the absence of threshold tuning. Comparing Table 2 with Table 7, we see that the performance of specialized fact-checkers decreases without threshold tuning. However, MiniCheck-FT5 only drops by 0.4% compared to larger drops for other systems, such as 9.2% for SummaC-CV. These results suggest that our synthetic data not only improves overall performance but also enhances the robustness of models across the domains of our benchmark, enabling them to maintain strong performance even without threshold tuning.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Comparison of GPT-4 and GPT-4-Full on TOFU EVAL, a dataset where sentences from an LLM-generated response share the same grounding document.\\n\\n| Model Name | TOFU | EVAL |\\n|------------|------|------|\\n| GPT-4      | 71.4 | 79.9 |\\n| GPT-4-Full | 72.3 | 79.7 |\\n\\nGPT-4-Full achieves performance similar to predicting the factual label for each summary sentence individually. The inference cost on the TOFU EVAL test set can be reduced from $16.7 to $6.72 with this method.\\n\\nHowever, in retrieve-then-generate and post-hoc grounding settings, evidence is typically retrieved for each claim separately, meaning no document can be shared across claims in a single response, and thus the inference cost is barely reduced.\\n\\nC LLM-A GGREFACT Details\\n\\nC.1 Dataset Descriptions\\n\\nA GGREFACT (Tang et al., 2023a) is a factual consistency evaluation benchmark for new summarization, targeting CNN(/DM) (Nallapati et al., 2016) and XSum (Narayan et al., 2018). Our focus is on the SOTA sets within A GGREFACT, where summaries are generated from SOTA fine-tuned summarizers, since their analysis suggests that summaries are more challenging to evaluate for factual consistency compared to summaries generated by pre-SOTA summarizers. Data in A GGREFACT comes from 9 factual consistency evaluation datasets on CNN or XSum, including widely used ones such as SummaC (Laban et al., 2022), FRANK (Pagnoni et al., 2021), and SummEval (Fabbri et al., 2021). Check Appendix C for a complete set of evaluation datasets in A GGREFACT.\\n\\nBecause CNN/DM and XSum feature quite different styles of summaries, we report these numbers separately in our benchmark. However, we do not otherwise report results on the smaller datasets within the A GGREFACT SOTA subset.\\n\\nTOFU EVAL (Tang et al., 2024) is a factual consistency evaluation benchmark for dialogue summarization, targeting MediaS um (interviews, Zhu et al. (2021)) and Meeting Bank (city council meetings, Hu et al. (2023)). It includes topic-focused dialogue summaries generated by 6 LLMs, with sentence-level factual consistency annotations by linguists.\\n\\nW ICE (Kamoi et al., 2023) is a textual entailment dataset that consists of naturally occurring claims from Wikipedia and their cited documents. Based on its cited documents, each claim is labeled as supported, partially-supported, or non-supported.\\n\\nR EV EAL (Jacovi et al., 2024) is a benchmark dataset that evaluates the correctness of reasoning chains generated by LLMs in the context of open-domain question-answering. The dataset includes annotations at the sentence level, covering various aspects of response correctness. For our dataset, we focus on the subset of sentences that have attribution annotations, which indicate whether a sentence in a reasoning chain can be attributed to information retrieved from Wikipedia paragraphs with three label categories: fully attributable, partially attributable, or contradictory.\\n\\nC LAIM VERIFY (Liu et al., 2023) evaluates the correctness of responses from four generative search engines in answering user queries. Similar to W ICE, the dataset contains annotations on whether check-worthy sentences from the engines' responses can be fully supported by their associated cited documents. The dataset contains binary-level factual consistency annotations for each cite-worthy sentence.\\n\\nF ACTHECK-GPT (Wang et al., 2023) contains factual consistency annotations for LLMs' responses to search queries. In this dataset, each sentence from LLMs' responses is first decomposed into atomic facts and those atomic facts are then further analyzed for their factual consistency.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Finally, each worth-checking and decontextualized atomic fact is labeled as completely support, partially support, refute, or irrelevant. We include those decontextualized atomic facts and their corresponding documents in the benchmark. **EXPERT QA** (Malaviya et al., 2024) contains responses from 6 different systems to queries curated by experts from 32 fields. These systems answer queries either in a close-book fashion with/without in-line citations, or based on retrieved document(s). For each sentence in the response, the sentence is verified against the concatenation of cited or retrieved document(s), if any. We include examples where documents are judged as complete, partial, or incomplete in supporting the corresponding sentences. We do not include human edited claims and evidence in our benchmark. **LFFA** (Chen et al., 2023) contains LLM-generated responses to questions from the ELI5 (\\\"Explain Like I'm Five\\\") dataset (Fan et al., 2019). LLMs generate responses based on documents that are either retrieved by humans, models, or randomly selected. Human annotators then evaluate each sentence in the LLM-generated responses against the corresponding document set, classifying them into supported, partially supported, or not supported. **C.2 Label Unification**\\n\\nFor **AGREFACT**, **TOFEVAL**, and **CLAIMVERIFY**, we keep using the binary label from the original work. For the remaining datasets, we map supported, fully attributable, completely support, and complete to supported, and unsupported otherwise. **C.3 Excluded Datasets**\\n\\nWe excluded **HALUEVAL** (Li et al., 2023) and **SUMMEDITS** (Laban et al., 2023) from our benchmark since they are synthetic, with errors in summaries generated via instruction prompts that guide the model to intentionally make errors in summaries. These errors are unnatural and do not fit with our goal of detecting true LLM generation errors. **FACTCORE** (Min et al., 2023) contains naturally generated biographies from LLMs and has human-annotated labels of individual atomic facts. However, humans could potentially search different articles to verify the correctness of those sentences than the ones that models retrieve. As a result, a non-negligible fraction of the claims in the dataset appear mislabeled from the standpoint of the fact-checking on grounded documents task. **C.4 Statistics**\\n\\nThe statistics of LLM-A AGREFACT can be found in Table 11. Our use of these datasets is for research purposes only, which is consistent with their intended use. **A AGREFACT** contains the following 9 factual consistency evaluation datasets on CNN or XSum: FactCC (Kryscinski et al., 2020), Wang'20 (Wang et al., 2020), SummEval (Fabbri et al., 2021), Polytope (Huang et al., 2020), Cao'22 (Cao et al., 2022), XSumFaith (Maynez et al., 2020), FRANK (Pagnoni et al., 2021), Goyal'21 (Goyal and Durrett, 2021), and CLIFF (Cao and Wang, 2021). **D Synthetic Data Details**\\n\\n**Source of Data**\\n\\nIn our C2D method, we choose around 400 claims from Wikipedia that have cited web articles (Kamoi et al., 2023; Petroni et al., 2023) to generate synthetic documents. In our D2C method, we scraped around 300 Google News articles since November 2023 from diverse topic categories.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Length distribution of generated documents.\\n\\nWe use the NLTK package for word tokenization.\\n\\nCategories, including science, politics, world, entertainment, business, and technology. Each document is approximately 500 words. Statistics of our generated data can be found in Table 1 and 12. See Appendix H.1 for how we maintain the quality of our synthetic data.\\n\\nCharacteristics of Synthetic Data\\n\\nIt is worth noting that constructing our synthetic dataset involves using human-written or naturally generated claims, which sets it apart from prior synthetic data generation methods used to train fact-checkers for text summarization. These methods, such as entity swapping and sentence negation (Kryscinski et al., 2020; Goyal and Durrett, 2021), were designed to target specific error types that occurred in claims from earlier summarization models. However, as errors from generative models progress (Tang et al., 2023a) and new error types emerge from LLMs, focusing on specific error types may not generalize well to unseen datasets with potentially novel errors.\\n\\nExamples of synthetic data for C2D and D2C can be found in Table 15 and 16.\\n\\nData Rejection Rate\\n\\nSince we use GPT-4 for filtering out low-quality examples in our C2D method, we report the rejection rate at different steps that require entailment checks.\\n\\nDuring the atomic fact expansion step (step 2), 6% of the final generated sentence pairs, when combined, could not support the original atomic fact. In the supporting document generation step (step 3), 5% of the final documents failed the entailment check. For the non-supporting document generation step (step 4), 53% of the final documents still supported the claim, and these documents were not included in our constructed data. These filtering steps are crucial for improving the training dataset\u2019s quality.\\n\\nTable 13: LLM checkpoints\\n\\n| Model                  | Checkpoint                  |\\n|------------------------|-----------------------------|\\n| Gemini-Pro             | gemini-1.0-pro              |\\n| PaLM2-Bison            | chat-bison@001              |\\n| Mistral-8x7B           | open-mixtral-8x7b           |\\n| Mistral-Large          | mistral-large-2402          |\\n| Claude-2.1             | claude-2.1                  |\\n| Claude-3 Opus          | claude-3-opus-20240229      |\\n| GPT-3.5                | gpt-3.5-turbo-0125          |\\n| GPT-4                  | gpt-4-0125-preview          |\\n\\nE Fact-Checking Model Details\\n\\nE.1 LLM-Based Fact-Checkers\\n\\nWe use the official APIs for LLM-based fact-checkers. The checkpoints we use for LLMs can be found in Table 13. The inference prompt is the same for all LLMs and can be found in Table 23.\\n\\nWe use a temperature of zero to collect deterministic outputs, which is typical from previous work.\\n\\nE.2 Specialized Fact-Checkers\\n\\nQAFactEval (Fabbri et al., 2022) is a QA-based fact-checker with optimized components for answer selection, question answering, question generation, and answer overlap calculation. It selects spans as answers from a summary sentence, generates questions based on these answers, and then answers these questions using the source document. Finally, it computes an overall overlap score for the summary sentence by comparing the selected spans from the summary sentence with the answers derived from the source document, given the generated questions. QAFactEval produces scores on a continuous scale ranging from 0 to 5. In our experiments, we use the default model and hyperparameters as provided by the authors.\\n\\nDAE (Goyal and Durrett, 2020, 2021) is an entailment-based fact-checker that evaluates the factual consistency of each dependency arc in a summary sentence. It independently verifies whether the semantic relationship of each dependency arc is factually supported by the source document. Finally, it aggregates the scores for all dependency arcs to compute an overall sentence-level factuality score ranging from 0 to 1. In our experiments, we use the default model and hyperparameters as provided by the authors in Goyal and Durrett (2021).\"}"}
{"id": "emnlp-2024-main-499", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SummaC-ZS (Laban et al., 2022) is an entailment-based fact-checker. To evaluate a summary sentence $c_i$, it divides the source document $D_i$ into a set of sentences or paragraphs $D_i = \\\\{d_{i,1}, \\\\ldots, d_{i,|d_i|}\\\\}$, and the score for $c_i$ is determined by the highest score among all $(d_{i,j}, c_i)$ pairs, i.e., $\\\\text{score}(c_i) = \\\\max_j M(d_{i,j}, c_i)$. For a multi-sentence summary, the final score is calculated as the average of the individual sentence scores. In our experiments, we do not use the authors' default setting of splitting the document $D_i$ into sentences and instead choose paragraph-level segmentation, as most datapoints in LLM-A require reasoning across multiple sentences. We find this change not only improves the overall performance but also accelerates inference speed. Apart from this adjustment, we adhere to the default model and hyperparameters provided by the authors. SummaC-ZS returns a score between -1 and 1.\\n\\nSummaC-CV (Laban et al., 2022) extends SummaC-ZS by considering all entailment scores for each summary sentence $c_i$. Similar to SummaC-ZS, SummaC-Conv evaluates a summary sentence $c_i$ by dividing the source document $D_i$ into $D_i = \\\\{d_{i,1}, \\\\ldots, d_{i,|d_i|}\\\\}$. However, instead of selecting the maximum score among all $(d_{i,j}, c_i)$ pairs, SummaC-Conv uses a learned convolutional layer to transform the distribution of entailment scores $\\\\{M(d_{i,j}, c_i) : \\\\forall j\\\\}$ into a single score. The final summary score is computed by averaging the scores of individual sentences. As with SummaC-ZS, we use paragraph-level segmentation in our experiments and keep other settings as default. SummaC-Conv outputs a score between 0 and 1.\\n\\nAlignScore (Zha et al., 2023) is an entailment-based model that has been trained on data from a wide range of tasks such as NLI, QA, fact verification, and summarization. It works similarly to SummaC-ZS, with the only difference being that it splits a document $D_i = \\\\{d_{i,1}, \\\\ldots, d_{i,|d_i|}\\\\}$ into sequential chunks at sentence boundaries. Each chunk contains approximately 350 tokens, determined by white space splitting. In our experiments, we use the default model and hyperparameters as provided by the authors. AlignScore outputs a score between 0 and 1.\\n\\nT5-NLI-Mixed (Honovich et al., 2022) is an entailment-based fact-checker built on T5-XXL. It has been trained on a diverse set of NLI datasets and predicts whether a given claim is supported by a document, outputting \\\"1\\\" for supported claims and \\\"0\\\" for unsupported ones. The final entailment score is calculated as the probability of the model predicting the token \\\"1\\\". To optimize its performance on 2 GPUs from our hardware setup, we select a chunk size of 350 tokens according to the T5 tokenizer. T5-NLI-Mixed outputs a score between 0 and 1.\\n\\nMNICHECK-RBTA, MiniCheck also split a document into chunks at sentence boundaries, with a chunk size of approximately 400 tokens according to RoBERTA and DeBERTa tokenizers. This results in approximately the same chunk size as in AlignScore, which has a chunk size of 350 tokens using white space splitting. The output scores fall within the range of 0 to 1. FT5-ANLI-L, MiniCheck-FT5 work the same way as T5-NLI-Mixed, but using only one GPU and setting the chunk size to 500 tokens using white space splitting. The output scores fall within the range of 0 to 1.\\n\\nE.2.1 Machine Configuration for Specialized Fact-Chekers\\nWe use two NVIDIA RTX A6000 GPUs for T5-NLI-Mixed, given its model size, and one GPU for the remaining models, all on our own hardware. According to Lambda,\\n\\n\\\\[ \\\\text{Detailed price specifications are available at https://lambdalabs.com/service/gpu-cloud##pricing.} \\\\]\"}"}
{"id": "emnlp-2024-main-499", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We then ask GPT-4 to minimally modify $D$ to create a new document $D'$, which can support some atomic facts mentioned in $c$ but not all of them. Inspired by the error type definitions from Tang et al. (2023a), we provide four different revision types to help GPT-4 generate diverse non-supporting documents, covering various reasons for not supporting the claim (see Table 26 for the prompt). As the generated supporting documents for a given claim tend to be similar despite adjusting the model temperature, we do not generate multiple supporting documents for $c$. Instead, for each claim, we generate one supporting and pair it with one non-supporting document. To enhance the diversity of the training data and maintain a comparable dataset size to our C2D method, we randomly select 3,500 claims from Wikipedia with cited web articles, resulting in the C2D-SIMP dataset containing 7K datapoints.\\n\\nWe start by directly using the summary sentences generated using the chunk-level summarization step of our D2C method (Section 3.2). That is, for each human written document, we have three document chunks $\\\\{D_1, D_2, D_3\\\\}$ and corresponding supporting summary sentences $\\\\{c_1, c_2, c_3\\\\}$ generated by GPT-4. For each $(D_i, c_i, 1)$ tuple, we ask GPT-4 to modify the summary sentence $c_i$ such that the edited summary sentence $c'_i$ is no longer supported by the document chunk $D_i$. We leverage the editing method from Laban et al. (2023), which is used to construct their SummEdit factual consistency evaluation benchmark. The editing prompt is provided in Table 27. We sample 7K datapoints from the generated data to construct D2C-SIMP.\\n\\n### H. Training Details\\n\\nWe include the training details and hyperparameter details in the section. Unless otherwise specified, we use the default hyperparameters of the backbone models. All models are trained using the standard cross-entropy loss function.\\n\\nFor our baseline models: FT5-C2D, FT5-D2C, FT5-ANLI-L, FT5-C2D-S, and FT5-D2C-S, we fine-tune flan-t5-large for 2 epochs on prepared data described in Section 5 and A, using a batch size of 4 and a learning rate of 5e-5.\\n\\nFor MiniCheck-RBTA, MiniCheck-DBTA and MiniCheck-FT5, we begin by fine-tuning the tuned RoBERTa-Large model from AlignScore, deberta-v3-large, and flan-t5-large on their respective training data (Section 3.3) for 2 epochs, while excluding 7K D2C synthetic data. We use a batch size of 4 with an accumulation step of 2 and a learning rate of 1e-5 for RoBERTa and 5e-5 for the other two models. We then fine-tune these models on 7K D2C synthetic data for 1 epoch, with a batch size of 4 and learning rate of 1e-5.\\n\\nWe observe that following this training pipeline consistently yields higher performance across all three backbone models compared to training on all data simultaneously. We hypothesize that this improvement stems from the fact that the source documents in the D2C dataset are human-written documents, in contrast to the synthetically generated source documents in the C2D dataset. Fine-tuning on these realistic documents at the end helps the models adapt back to a realistic distribution, preventing them from overfitting to synthetic documents and allowing them to perform well on real documents in the benchmark.\\n\\n### H. Prompts\\n\\nIn Table 14, we present the full list of the prompts used throughout our work. We use GPT-3.5 for sentence decomposition and merging atomic facts, and GPT-4 for the remaining prompts. We next elaborate on how we ensure the labeling quality of our synthetically generated data.\\n\\n### H.1 Quality Assurance for Generations\\n\\nSentence decomposition\\nWe adapt a few-shot sentence decomposition prompt from (Kamoi et al., 2023).\\n\\nWe adapt a few-shot sentence decomposition prompt from (Kamoi et al., huggingface.co/microsoft/deberta-v3-large).\"}"}
{"id": "emnlp-2024-main-499", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which can generate complete and correct atomic facts most of the time according to their human evaluation. The prompt (Table 17) is used for both of our synthetic data generation methods and the claim decomposition experiment in Section 7.1.\\n\\nC2D: Atomic fact expansion\\n\\nWe use a 4-shot prompt (Table 18) for this step, where we ask GPT-4 to produce a sentence pair where the atomic fact is supported if and only if the information from both sentences is combined. To ensure the quality of the generation, we verify the correctness of this condition after generation via an entailment check by GPT-4 (Table 20). If the correctness is not met, we iterate the process and regenerate a new sentence pair up to a specified number of attempts. In cases where the correctness criterion remains unmet after the specified attempts, we remove the datapoint from the dataset.\\n\\nC2D: Supporting document generation\\n\\nWe ensure that all sentences $s$ from the generated sentence pairs are mentioned in (and hence are entailed by) the generated document $D$ by using the entailment check by GPT-4. Same as above, if the document fails to mention all sentences from the sentence pairs, we iteratively generate new documents until a specified number of attempts is reached. It is important to note that we only verify whether sentences $s$ are mentioned in $D$, as we believe GPT-4 can perform well on this simple task. By construction, if all sentences are mentioned in $D$, then $c$ is supported by $D$. However, directly performing an entailment check on the $(D, c)$ pair with GPT-4 may introduce many labeling errors, which can negatively impact the performance of the trained models.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"More than 5,000 individuals, part of a caravan that crossed into Mexico last month, are now seeking asylum and have established a temporary encampment at the Tijuana Stadium as of today. The Tijuana Stadium, known for hosting sporting events, recently underwent renovations that doubled its seating capacity. Prior to these changes, the stadium had a capacity to accommodate 1,500 spectators.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With the SAG-AFTRA strike settled, the six-month, multi-guild Hollywood labor disruption has finally ended, but the theatrical damage has only begun to surface. Reviewing the films delayed until next year, a rough estimate suggests that the stoppage cost theaters around $400 million\u2013$600 million in gross\u2014more, when including lost concession revenue. \u201cBarbie,\u201d \u201cOppenheimer,\u201d \u201cSound of Freedom,\u201d and \u201cTaylor Swift: The Eras Tours\u201d kept the damage from being worse. Immediately following the SAG-AFTRA settlement, Disney announced wholesale delays in its upcoming release schedule. Their revived plans includes only one Marvel title for 2024 (\u201cDeadpool 3,\u201d moved to July 26 from May 3), down from the customary three per year from MCU.\\n\\nDisney was among the first studios to announce delays, with Sony already out Wednesday evening with word the third \u201cVenom\u201d film would move from July to November. Related Stories The good news for theaters is despite it all, 2023 should still reach the $9 billion in domestic gross hoped for this year.\\n\\nHowever, any hopes that 2024 might return to 2019 box-office parity are dashed. Grosses from rescheduled titles will help, but production delays will leave substantial gaps. Even so: It could have been worse. \u201cDune: Part 2\u201d (Warner Bros.) and \u201cKraven the Hunter\u201d and \u201cGhostbusters: Frozen Empire\u201d (Sony) will cost this year\u2019s total the most\u2014perhaps $400 million (\u201cGhostbusters\u201d would have had only 12 days of 2023 play). Figure other films, mostly limited/specialized entries like Luca Guadagnino\u2019s \u201cChallengers\u201d and Jeff Nichols \u201cThe Bikeriders\u201d (Disney), Ethan Coen\u2019s \u201cDrive Away Dolls\u201d (Focus), and \u201cThe Book of Clarence\u201d (Sony) could have contributed $100 million or more while riding the awards wave. The biggest unknown is how much the lack of promotion hurt the films released during the strikes. By the time the SAG-AFTRA strike began July 11, most of the summer\u2019s top titles had already been released or were about to be, which meant their promotional pushes were all but complete.\\n\\nSome, like DC Comics\u2019 \u201cBlue Beetle\u201d (WB) and \u201cThe Equalizer 3\u201d (Sony), may have suffered more. Would promotion for all strike-period releases have totaled $100 million? Maybe. Theaters were fortunate that both \u201cBarbie\u201d and \u201cOppenheimer\u201d already had enormous publicity before actors struck, and both had major, Oscar-nominated directors to carry the ball. Their $950 million combined domestic gross more than doubled expectations. Add the mid-summer sleeper success of \u201cSound of Freedom\u201d and July and August both were strong months. A wild card, unknown when the strike began, was Taylor Swift\u2019s concert film. It certainly filled an October void that existed before the strike and its SAG-AFTRA wrangle meant she could promote it. The outside-studio success of \u201cSound of Freedom\u201d and \u201cThe Eras Tour\u201d were not only welcome for their grosses, but also because they show it\u2019s possible to find releases outside the studios.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Segment the following sentence into individual facts:\\nSentence: Other title changes included Lord Steven Regal and The Nasty Boys winning the World Television Championship and the World Tag Team Championship respectively.\\nFacts:\\n- Lord Steven Regal won the World Television Championship.\\n- The Nasty Boys won the World Tag Team Championship.\\n\\nSentence: The parkway was opened in 2001 after just under a year of construction and almost two decades of community requests.\\nFacts:\\n- The parkway was opened in 2001.\\n- The parkway was opened after just under a year of construction.\\n- The parkway was opened after two decades of community requests.\\n\\nSentence: Touring began in Europe in April-June with guitarist Paul Gilbert as the opening act, followed by Australia and New Zealand in July, Mexico and South America in late July-August, and concluding in North America in October-November.\\nFacts:\\n- Touring began in Europe in April-June.\\n- The opening act of the tour was guitarist Paul Gilbert.\\n- The tour was in Australia and New Zealand in July.\\n- The tour was in Mexico and South America in late July-August.\\n- The tour was concluded in North America in October-November.\\n\\nSentence: In March 2018, the company partnered with Amazon Web Services (AWS) to offer AI-enabled conversational solutions to customers in India.\\nFacts:\\n- The company partnered with Amazon Web Services (AWS) in March 2018.\\n- The two companies partnered to offer AI-enabled conversational solutions to customers in India.\\n\\nSentence: The most significant of these is in Germany, which now has a Yazidi community of more than 200,000 living primarily in Hannover, Bielefeld, Celle, Bremen, Bad Oeynhausen, Pforzheim and Oldenburg.\\nFacts:\\n- The most significant of these is in Germany.\\n- Germany now has a Yazidi community of more than 200,000.\\n- Yazidi community in Germany lives primarily in Hannover, Bielefeld, Celle, Bremen, Bad Oeynhausen, Pforzheim and Oldenburg.\\n\\nSentence: A previous six-time winner of the Nations\u2019 Cup, Sebastian Vettel became Champion of Champions for the first time, defeating Tom Kristensen, who made the final for the fourth time, 2-0.\\nFacts:\\n- Sebastian Vettel is a previous six-time winner of the Nations\u2019 Cup.\\n- Sebastian Vettel became Champion of Champions for the first time, defeating Tom Kristensen, 2-0.\\n- Tom Kristensen made the final for the fourth time.\\n\\nSentence: \\nFacts:\\nTable 17: Sentence decomposition prompt adapted from (Kamoi et al., 2023).\"}"}
{"id": "emnlp-2024-main-499", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Your task is to generate a pair of sentences so that the provided claim can be entailed by the sentence pair. You must make sure that the claim can only be deduced by combining the information from the two sentences that contain unique information.\\n\\nExamples:\\nProvided Claim: The investigation is into allegations that his mayoral campaign received illegal foreign funds.\\nSentence 1: During the period leading up to the mayoral election, there was a notable increase in his campaign's financial resources.\\nSentence 2: Investigation shows the funds having origins beyond national boundaries, a detail raising questions under current campaign laws.\\n\\nProvided Claim: Approximately 1,000 fans fainted at the concert.\\nSentence 1: Emergency services reported an unusually high number of calls for medical assistance during the concert with an attendance of 20,000.\\nSentence 2: Venue officials estimated that approximately 5% of the audience required medical attention for fainting.\\n\\nProvided Claim: The interest rate hikes were intended to manage inflation and moderate economic growth.\\nSentence 1: Central bank officials expressed concern over the rising consumer price index and the overheating of the economy.\\nSentence 2: The monetary policy committee decided to adjust the interest rates as a response to these economic indicators.\\n\\nProvided Claim: Several advertisers are considering halting their ads on social media platform X.\\nSentence 1: Some companies are re-evaluating their marketing strategies to avoid association with platforms that fail to address misinformation.\\nSentence 2: Recent reports show that platform X has received criticism for its handling of false information spreading unchecked.\\n\\nPlease make sure that NEITHER sentence alone supports the claim.\\n\\nYour turn:\\nProvided Claim: [CLAIM]\"}"}
{"id": "emnlp-2024-main-499", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Merge the following individual facts into a single sentence:\\n\\nFacts:\\n- Lord Steven Regal won the World Television Championship.\\n- The Nasty Boys won the World Tag Team Championship.\\n\\nSentence: Other title changes included Lord Steven Regal and The Nasty Boys winning the World Television Championship and the World Tag Team Championship respectively.\\n\\nFacts:\\n- The parkway was opened in 2001.\\n- The parkway was opened after just under a year of construction.\\n- The parkway was opened after almost two decades of community requests.\\n\\nSentence: The parkway was opened in 2001 after just under a year of construction and almost two decades of community requests.\\n\\nFacts:\\n- Touring began in Europe in April-June.\\n- The opening act was guitarist Paul Gilbert.\\n- There was a tour in Australia in July.\\n- There was a tour in New Zealand in July.\\n- There was a tour in Mexico in late July-August.\\n- There was a tour in South America in late July-August.\\n- The tour was concluded in North America in October-November.\\n\\nSentence: Touring began in Europe in April-June with guitarist Paul Gilbert as the opening act, followed by Australia and New Zealand in July, Mexico and South America in late July-August, and concluding in North America in October-November.\\n\\nFacts:\\n- The company partnered with Amazon Web Services (AWS) in March 2018.\\n- The two companies partnered to offer AI-enabled conversational solutions to customers in India.\\n\\nSentence: In March 2018, the company partnered with Amazon Web Services (AWS) to offer AI-enabled conversational solutions to customers in India.\\n\\nFacts:\\n- The most significant of these is in Germany.\\n- Germany now has a Yazidi community of more than 200,000.\\n- Yazidi community in Germany lives primarily in Hannover.\\n- Yazidi community in Germany lives primarily in Bielefeld.\\n- Yazidi community in Germany lives primarily in Celle.\\n- Yazidi community in Germany lives primarily in Bremen.\\n- Yazidi community in Germany lives primarily in Bad Oeynhausen.\\n- Yazidi community in Germany lives primarily in Pforzheim.\\n- Yazidi community in Germany lives primarily in Oldenburg.\\n\\nSentence: The most significant of these is in Germany, which now has a Yazidi community of more than 200,000 living primarily in Hannover, Bielefeld, Celle, Bremen, Bad Oeynhausen, Pforzheim and Oldenburg.\\n\\nFacts:\\n- Sebastian Vettel is a previous six-time winner of the Nations' Cup.\\n- Sebastian Vettel became Champion of Champions for the first time.\\n- Sebastian Vettel defeated Tom Kristensen.\\n- Tom Kristensen made the final for the fourth time.\\n- The score was 2-0.\\n\\nSentence: A previous six-time winner of the Nations' Cup, Sebastian Vettel became Champion of Champions for the first time, defeating Tom Kristensen, who made the final for the fourth time, 2-0.\"}"}
{"id": "emnlp-2024-main-499", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-499", "page_num": 30, "content": "{\"revision_type\": \"\", \"revised_article\": \"\"}"}
