{"id": "emnlp-2024-main-904", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nLarge language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from visual illusion. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the visual illusion level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation results aid in better assessments of the MLLMs' robustness in the presence of misleading images. The code and datasets are available at https://github.com/MasaiahHan/CorrelationQA.\\n\\n1 Introduction\\n\\nLarge language models (LLMs) have sparked a transformative shift in the field of artificial intelligence (Zhao et al., 2023; Workshop et al., 2022; Chowdhery et al., 2023; Touvron et al., 2023). Following the development of LLMs, a series of multi-modal large language models (MLLMs) have emerged to enable LLMs with visual processing capabilities (Alayrac et al., 2022; Gong et al., 2023; Yin et al., 2023; Zhu et al., 2023). Typically, current MLLMs process visual inputs by converting them into visual tokens that share the same latent space as language tokens in LLMs. This conversion not only maintains excellent text processing abilities but also enables LLMs with powerful visual semantic understanding capabilities. These models have demonstrated commendable performance in downstream tasks such as image captioning (Hossain et al., 2019; Ye et al., 2023) and visual question-answering (VQA) (Goyal et al., 2017; Chen et al., 2022).\\n\\nDespite the success achieved by state-of-the-art MLLMs, most studies mainly focus on simple VQA. However, MLLMs are usually applied to complex vision reasoning scenarios, where the answers are usually not included in the images, which requires MLLMs to utilize the reasoning ability of LLM to answer. We identify an visual illusion, the instinctive bias, which is widespread in vision reasoning. Existing MLLMs are prone to ignore the semantic information in reasoning quizzes and answer directly to the objects in the pictures instead of utilizing their reasoning ability. In Figure 1, we show a specific example of instinctive bias. Under the text-only condition, LLaV A can...\"}"}
{"id": "emnlp-2024-main-904", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Accuracy of MLLMs on natural spurious images in our proposed benchmark CorrelationQA. The higher accuracy indicates that MLLMs answer correctly when accompanied by spurious images.\\n\\nHowever, when the image only contains the spurious image, LLaVA assumes Eiffel tower to be the corresponding answer and ignores the semantics of the question. This type of illusion affects the widespread use of MLLMs. In scenarios such as shopping recommendations and real-time VQA, users want to be recommended similar styles of schoolbags, or users cannot describe accurately and choose to upload pictures for information supplementation. With the instinctive bias, MLLMs tend to give incorrect answers. Therefore, it is essential to establish a benchmark to quantify the impact of such issues in current MLLMs.\\n\\nTo study the illusion of MLLMs under spurious visual inputs, we design a novel benchmark called CorrelationQA. CorrelationQA collects over 7,000 question-answer (QA) pairs in 13 categories, where each pair contains multiple answer-related images that may mislead MLLMs. We first use GPT-4 (OpenAI, 2023) to generate meaningful QA pairs with five related but incorrect answers and a correct one. Based on the generated answers, we leverage the advanced diffusion model to generate the corresponding spurious images for each question. Specifically, we generate factual images with the correct answers as a comparison. In addition to natural images, we also generate five typographic images for spurious answers, inspired by Liu et al. (2023d). To ensure that the synthetic data is not biased, we collect corresponding realistic images from the Internet via search engine.\\n\\nBased on the designed benchmark, we conducted an in-depth analysis to uncover the instinctive bias present in mainstream MLLMs. Our findings, presented in Figure 2, demonstrate that 9 state-of-the-art MLLMs including GPT-4V suffer from visual illusion when presented with spurious visual inputs. This phenomenon indicates that by providing information related to spurious answers, images can induce MLLMs to instinctively focus on the visual content, resulting in responses that are predominantly based on visual information without proper reasoning and thinking. This is similar to the cases of unconscious decision-making processes observed in human brains (Kahneman, 2011; Booch et al., 2021).\\n\\nOur contributions are summarized as follows:\\n\\n1) We first identify the visual instinctive bias in MLLMs, where spurious visual inputs can cause current MLLMs to delude.\\n2) We propose CorrelationQA to quantify the seriousness of instinctive bias across different types, demonstrating that this issue is universal across MLLMs.\\n3) We provide an in-depth analysis of the recent 9 representative MLLMs on our benchmark, showing their susceptibility to spurious visual inputs under different scenarios.\\n\\n2 Method\\n\\nIn this section, we first present the background of multi-modal large language models (MLLMs) in commonsense question-answering (CQA) and the motivation of our study (2.1). Next, we introduce the proposed automated pipeline to generate our CorrelationQA benchmark (2.2). Finally, we provide the designed evaluation metrics to measure the sensitivity of MLLMs on spurious images (2.3).\\n\\n2.1 Motivation\\n\\nBy projecting the visual tokens into language space, existing MLLMs are able to equip large language models with visual processing ability. However, past studies only demonstrate their \u201cfast thinking\u201d abilities in simple CQA tasks, but have yet explored their \u201cslow reasoning\u201d performance in complicated visual questions-answer tasks, such as when the input image provides relevant but indirect information about the correct answer.\\n\\nOur study is motivated by the observation that current MLLMs, such as GPT-4V (OpenAI, 2023) and LLaVA (Liu et al., 2023c), are prone to inaccurate when presented with answer-correlated but\"}"}
{"id": "emnlp-2024-main-904", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Known for its distinctive black and white stripes, this African equine is closely related to horses and donkeys, .... ... What is it? Question\\n\\nAnswer: Factual + Spurious\\n\\nGiraffe, .... ... x6\\n\\nAssume you give you some examples\\n\\nFew-shot QA\\nExample 1:\\n\\nExample 2:\\n\\nExample 3:\\n\\nCategories\\n\\nAnimal, Color, Plant, City,\\n\\nFood ...\\n\\nStage I: QA generation\\n\\nStage II: Image generation\\n\\nPrompt\\n\\nStable Diffusion\\n\\nNatural synthetic Image\\n\\nPython Script\\n\\nOCR Typography\\n\\nSearch Engine\\n\\nRealistic Image\\n\\nFigure 3: Pipeline of our dataset construction. First, we utilize GPT-4 to generate a set of QA pairs with five spurious answers. Next, we leverage image generators to generate corresponding images based on these answers (natural synthetic and typography). We use the answers as the keywords to obtain realistic images from search engine. Using these images, we construct a set of text and image pairs to evaluate the robustness of MLLMs to spurious images.\\n\\nanswer-contradicted images. Examples depicted in Figure 1 demonstrate that LLaVA would fail spectacularly given a query accompanied by a spurious image. On the other hand, it is able to give correct answers in text-only scenarios. This indicates that the injection of additional image information has a detrimental effect on the capabilities of MLLMs.\\n\\nTo further study the role of the input image, we split the images into the following three types:\\n\\n1) Factual image: the images are relevant and directly correspond to the correct answer,\\n2) Spurious image: the images are related to the question but do not correspond to the correct answer, and\\n3) Random image: the images are unrelated to either the question or answer.\\n\\nWe then construct a set of image-text pairs to evaluate the performance of MLLMs under these three kinds of scenarios.\\n\\n2.2 CorrelationQA\\n\\nIn order to obtain a large dataset of image-text pairs, we have designed a three-step automatic pipeline for generating and collecting the necessary data. The overall pipeline is shown in Figure 3. We first pre-define 13 meta-categories for the proposed dataset, where the distribution of each category is illustrated in Table 1. As we notice MLLMs favor spurious answers that occurred in the images over the semantics of questions, we firstly generate CQA pairs which can be prompted directly to LLMs. Secondly, for each question, we generate 5 images corresponding to five wrong answers and one image corresponding to the correct answer as visual inputs for MLLMs. Additionally, we collect realistic images of the wrong answer for each question.\\n\\nStep1: Text Pairs\\n\\nTo fully utilize the superior language comprehension capabilities of GPT-4, we employ this state-of-the-art language model to assist in data creation. Specifically, we use it to generate around 100 unique question-answer (QA) pairs for each scenario given some QA pair examples. These questions are demonstrated to be neither too simple nor stray from factual accuracy. Then, we also instruct GPT-4 to provide an accurate answer along with five spurious alternatives for each question, serving as the primary entities for subsequent image creation steps. The prompts and some examples are detailed in Figure 7.\\n\\nStep2: Image Generation and Collection\\n\\nGiven the constructed QA-pairs, this step leverages the image generator to create corresponding images.\"}"}
{"id": "emnlp-2024-main-904", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We follow Liu et al. (2023d) to build two kinds of images: natural and typographies. Specifically, we apply the cutting-edge image generation model, Stable Diffusion (SD) (Rombach et al., 2022) as the image generator. We integrate six answers obtained in the first step into a prompt template for SD. Then, we leverage SD to output images with a resolution of 1024x1024 for better detail restoration and later resize the images to 512x512 for storage. Additionally, we utilize a search engine to collect the realistic images corresponding to the answer from the Internet and resize the longest side to 512. We present some image-text pairs of CorreLatioQA in Figure 9.\\n\\nStep 3: Typography Generation\\n\\nThere are numerous scenarios such as road sign recognition and document scanning, where text within images plays a crucial role in practical applications. Additionally, testing with OCR images can better simulate complex real-world data environments, challenging the robustness of MLLMs. Therefore, we generate typography images. Following Liu et al. (2023d), we use the Pillow library to print the answers on a plain white background like OCR images. The image size is set to 512x512, as detailed image refinement is not as critical in this step compared to the previous one. The font size is set to 90 to ensure text legibility and prominence in the images.\\n\\n2.3 Evaluation Metrics\\n\\nSuccessful Answer Rate\\n\\nTo analyze the assessment of CorrelationQA, we employ successful answer rate as the metric to determine MLLMs' susceptibility to the instinctive bias, which is also referred to as Accuracy defined as follows:\\n\\n\\\\[ \\\\text{Acc} = \\\\frac{C}{T}, \\\\]\\n\\nwhere \\\\( C \\\\) denotes the number of image-text pairs correctly answered by the model, and \\\\( T \\\\) represents the total number of image-text pairs. We further impose a word count limit for MLLMs' outputs as all labels in the benchmark do not exceed a length of five words. To count the number of \\\\( C \\\\), we adopt an approximate match approach, where it is acceptable for the response to be an abbreviation of the label or any sentence containing the label. For instance, if the label is \\\"Los Angeles Lakers\\\" then responses such as \\\"Lakers\\\" or \\\"It is Los Angeles Lakers\\\" are both considered correct.\\n\\nAccuracy Drop\\n\\nTo evaluate the sensitivity of MLLMs under spurious images, we further design an Accuracy Drop (AccDrop) metric as follows:\\n\\n\\\\[ \\\\text{AccDrop} = A_f - A_s, \\\\]\\n\\nwhere \\\\( A_f \\\\) and \\\\( A_s \\\\) denote Accuracy on factual and spurious data respectively. A higher AccDrop value indicates superior model performance with factual data and poorer with spurious one, which reflects the sensitivity to deceptive type information.\\n\\n3 Experiments\\n\\n3.1 Dataset Collection\\n\\nAs outlined in section 2, our approach involves several steps. First, we pre-collect a set of demonstrating question-answer (QA) pairs. We then use these pairs to guide GPT-4 in generating additional QA pairs across different categories, each with one correct answer and five incorrect answers. Based on the generated answers, we utilize a state-of-the-art Stable Diffusion model and OCR-generated script to generate corresponding factual and spurious images, respectively. For further details on the collected scenario and dataset statistics, please refer to Table 1.\\n\\n3.2 Experimental Setup\\n\\nModels.\\n\\nWe perform a comprehensive evaluation of 9 recently released MLLMs on our CorrelationQA, including LLaV A-1.5-7B and 13B (Liu et al., 2023c) (referred as LLaV A-7B and LLaV A-13B for convenience), MiniGPT-4 (Zhu et al., 2023), mPLUG-Owl2 (Ye et al., 2023), Qwen-VL (Bai et al., 2023), Idefics (Lauren\u00e7on et al., 2023), GPT-4V (OpenAI, 2023), InstructBlip (Dai et al., 2023) and CogVLM (Wang et al., 2023a).\\n\\nParameter Settings.\\n\\nConsidering the different versions and updates of MLLMs, we choose their latest released weights for testing. All other parameters for each model are set to default values as specified by the original authors. For the open-sourced model, if not specifically mentioned, we adopt the widely-used 7B version of LLM for evaluation. Regarding image generation, playground-v2-1024px-aesthetic checkpoint is adopted in Stable Diffusion. Compared to the commonly used stable-diffusion-xl-base-1.0 checkpoint, this checkpoint enables more realistic image generation quality and avoids simple and counter-intuitive results.\"}"}
{"id": "emnlp-2024-main-904", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Assessments results on accuracy (Acc) and accuracy drop (AccDrop) for MLLMs. The results on the left refer to the natural image and the right one refers to the typography image. Corresponding AccDrop is presented on the right side of each figure. Fac and Spu denote factual and spurious, respectively.\\n\\nPrompt Settings.\\nFor QA pairs generation, we utilize GPT-4 to generate thousands of QA pairs by providing several demonstrating examples. Give you some examples of QA pairs. The content of QA pairs should include truth and commonsense. No repeated examples and answers. The description of the question should be complex as much as possible. Here are some examples: [Q: Sample Question1 A: Correct Answer1], [Q: Sample Question2 A: Correct Answer2], give 100 examples in the format: [Q:, A:, W:], while W means you should also give other 5 wrong confusing answers. Reference these to generate 100 similar examples relevant to [Categories], [Detailed Requirement].\\n\\nFor image generation, we present the correct and spurious answers under the following prompt template to the diffusion model.\\nA photo of [Spurious Answer], detailed, 8k, realistic, trending on artstation.\\n\\nFor visual question answering, we adopt the following prompt template with the questions as the text inputs into MLLMs.\\n[Question] Answer in no more than five words. Each question is along with the generated natural or typography image. To more accurately assess the model responses, we require MLLMs to directly answer the questions. This approach is reasonable since all the correct are less than five words.\\n\\n3.3 Experimental Results\\nEvaluation Results on CorrelationQA\\nIn Figure 4, we first present the overall accuracy (Acc) and accuracy drop (AccDrop) of nine MLLMs on our CorrelationQA. The green color bars in each image represent the AccDrop from the factual image to spurious images, revealing that MLLMs consistently struggle with instinctive bias from spurious images, even for GPT-4V. This instinctive bias problem also occurs on the OCR data, which have higher AccDrop.\\n\\nIt is worth noticing that LLaV A and GPT-4V have higher average accuracy on the spurious images compared with other MLLM. What's more, both LLaV A-7B and LLaV A-13B exhibit almost no fluctuation in both spurious and factual contexts, which we believe can be attributed to its training data. To enhance the model's capabilities across various domains, researchers incorporate datasets like OK-VQA (Marino et al., 2019) and A-OKVQA (Schwenk et al., 2022) which require extensive knowledge to answer the question. Such training data enables LLaV A to reduce the influence of unessential images and leverage the inherent capabilities of LLMs for reasoning, thus leading to similar accuracy for LLaV A in both factual and spurious images. However, other tested MLLMs are mostly trained on image-answer-consistent data, therefore showing a performance drop between factual and spurious images. For GPT-4V, its pronounced proficiency in image-text understanding and language processing logically predicates a diminished propensity for instinctive bias.\\n\\nCompared to different types of image formats, typography exhibits a more serious instinctive bias problem over natural images. One potential reason is that spurious OCR typography might lead...\"}"}
{"id": "emnlp-2024-main-904", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to a more simplistic and crude understanding of MLLMs. OCR images inherently contain limited information due to their simplistic textual content in our cases (e.g., a single word). Because MLLMs are found to possess a certain degree of OCR recognition capability, when MLLMs process information on these inputs, the proportion of spurious elements in the visual information is higher compared to that in generated images, which makes MLLMs suffer from more instinctive bias. Similarly, as the content of OCR typography is easier to understand, MLLMs achieve higher accuracy when along with factual typography.\\n\\nResults on Different Categories\\n\\nTable 2 and Table 3 present AccDrop of 9 MLLMs on each category in detail. The results indicate that MLLMs exhibit varying degrees of sensitivity to different categories. We observe that MLLMs on categories such as animals, colors, food, and plants suffer from larger AccDrop as highlighted. On the contrary, it shows significantly lower AccDrop in categories like history and art. Intuitively, the former categories consist of tangible entities while the latter include concepts like the 'Industrial Revolution' or 'The Lord of the Rings,' which may not be easily represented in generated natural images.\\n\\nOur analysis also shows that the impact of typography images on MLLMs is greater than that of natural data, where each category exhibits larger gap in AccDrop. Interestingly, unlike natural images, AccDrop in typography images does not show a significant difference across different categories. This is reasonable, as the content of typography images typically consists of words, which are easier to interpret compared to natural images.\\n\\nWe hypothesize that the influences of the training data, cross-modal alignment training, and instruction tuning cause MLLMs to focus more on the semantic correlations between the query and the image. Identifying common patterns in the behavior of MLLMs could greatly assist in refining approaches for future work and is therefore an important finding.\\n\\nSpurious Information induces Visual Illusion\\n\\nThe variation in performance among MLLMs also motivates us to analyze the impact of image type on model accuracy. In Figure 5, we present a comprehensive comparison of the average accuracy of four MLLMs under five different conditions. The \\\"Text-only\\\" condition indicates that only the text query is used to prompt the model. Regarding the multi-modality condition, we provide the factual, spurious, and random images, respectively. For the random image, we randomly select an image from another category for a specific question. Notably, only in scenarios with text-only and factual image inputs do MLLMs have comparable performances. It suggests that the strategy of using images as supplementary information does not positively influence the models' responses even if the answer is hidden in the visual inputs. Compared to the other four conditions, spurious data induce more instinctive bias in the selected four MLLMs, particularly evident with OCR typography.\\n\\nFor the text-only scenario, we sample 20% of the questions from each category to test GPT-4 due to its request rate limit. The results indicate that GPT-4 achieves remarkably high accuracy in text-only scenarios with almost all questions being correctly answered. GPT-4V, one of the most ad-\\n\\n### Table 1: The statistics distribution of CorrelationQA.\\n\\n| Category | Questions | Images |\\n|----------|-----------|--------|\\n| Animal   | 105       | 630    |\\n| Art      | 105       | 630    |\\n| Color    | 99        | 594    |\\n| City     | 90        | 540    |\\n| Food     | 100       | 500    |\\n| History  | 104       | 624    |\\n| Human    | 105       | 630    |\\n| Material | 90        | 540    |\\n| Natural  | 100       | 600    |\\n| Objects  | 105       | 630    |\\n| Plant    | 105       | 630    |\\n| Sports   | 95        | 570    |\\n| Technology| 105  | 630    |\\n| Total    | 1,218     | 7,308  |\"}"}
{"id": "emnlp-2024-main-904", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Accuracy Drop (AccDrop) of MLLMs under 12 categories when applied natural images. AccDrop is the accuracy drop from the factual image into the spurious image. A higher value reflects a higher sensitivity to deceptive information. The three most sensitive categories are highlighted in blue background. Bold values are the top performance drop for each model. 0\u2217 represents zero accuracy on both factual and spurious images.\\n\\nTable 3: Accuracy Drop (AccDrop) of MLLMs under 12 categories when applied typography. AccDrop is the accuracy drop from the factual image into the spurious image. A higher value reflects a higher sensitivity to deceptive information. The three most sensitive categories are highlighted in blue background. Bold values are the top performance drop for each model. 0\u2217 represents zero accuracy on both factual and spurious images.\\n\\nIn our main study, we utilized the Stable Diffusion model to synthesize a large number of images for studying the inductive bias problem in MLLMs. Additionally, to better align with MLLMs' real-world applications, we evaluated the accuracy and accuracy drop of MLLMs on realistic factual and spurious images.\\n\\nFollowing the pipeline shown in Figure 3, we first utilize GPT-4 to generate correct and incorrect text answers. Then, we employ a search engine to obtain corresponding realistic images using the search keywords from the correct and incorrect answers. Finally, we resize the images proportionally to ensure the shorter side remained at 512 pixels.\\n\\nTable 4 shows the accuracy and accuracy drop for Qwen-VL and LLaV A-7B on both realistic and natural synthetic images. We observed that MLLMs exhibit similar behavior on both types of images, indicating that the conclusions drawn from the massive amount of synthetic images are generalizable to realistic images. Furthermore, the performance drop between spurious realistic images and synthetic images may be due to the purity of the content in the searched realistic images. Images retrieved through keyword searches may contain information beyond the keywords themselves. \\n\\nIn Figure 8, we provide some examples of real pictures and synthetic pictures under the same spurious answer.\\n\\nQualitative Analysis\\nFigure 6 further visualizes the examples where all 9 MLLMs answer correctly or incorrectly, respectively. For the image from accurate answers, we observe that the image contents do not significantly mislead the answers. For example, an image for \\\"My Fair Lady\\\" might be interpreted by MLLMs as \\\"A woman wearing a medieval-style hat adorned with a flower,\\\" leading to a shift in the relationship by MLLMs.\"}"}
{"id": "emnlp-2024-main-904", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This geological event occurs when there is a sudden release of energy in the Earth's crust, causing seismic waves.\"}"}
{"id": "emnlp-2024-main-904", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"world scenarios and understanding the modality alignment of MLLMs. Through our findings, future work could concentrate on adjusting training strategies, aiding MLLMs in appropriately calibrating their attention to image information based on its relevance in suitable contexts.\\n\\n6 Limitations\\nOur research introduces the widespread instinctive bias in multi-modal large language models (MLLMs) towards deceptive images. We suggest that this may be associated with training data. However, MLLMs supporting other modalities such as video and audio may also exhibit instinctive bias due to their predominant use of data pairs with simple modality relationships in the training process, which is worth exploring in future work. Additionally, our proposed CorrelationQA, which consists of questions whose answers are entities, limits the evaluation to other types of questions. Due to the size of MLLMs, we do not conduct further assessments on larger parameter versions of large language models (i.e., Vicuna-33B). However, we do find that instinctive bias appears to be unrelated to the model scale (Figure 4).\\n\\n7 Acknowledgements\\nWe express our gratitude to Lingyi Zhu, Yongjie Cai, Niya, Peiming Zhang and Han Peng from HKUST for verification and proofreading of the CorrelationQA benchmark.\\n\\nReferences\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736.\\n\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\\n\\nGrady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jonathan Lenchner, Nick Linck, Andreas Loreggia, Keerthiram Murgesan, Nicholas Mattei, Francesca Rossi, et al. 2021. Thinking fast and slow in ai. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 15042\u201315046.\\n\\nNicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. 2023. Poisoning web-scale training datasets is practical. arXiv preprint arXiv:2302.10149.\\n\\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. 2022. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113.\\n\\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instuctblip: Towards general-purpose vision-language models with instruction tuning.\\n\\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. 2023. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790.\\n\\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913.\\n\\nMD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shariatuddin, and Hamid Laga. 2019. A comprehensive survey of deep learning for image captioning. ACM Computing Surveys (CsUR), 51(6):1\u201336.\\n\\nNeel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614.\\n\\nDaniel Kahneman. 2011. Thinking, fast and slow. macmillan.\\n\\nHugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al. 2023. Obelisc: An open web-scale filtered dataset of interleaved image-text documents. arXiv preprint arXiv:2306.16527.\\n\\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. 2020. Bert-attack: Adversarial attack against bert using bert. arXiv preprint arXiv:2004.09984.\\n\\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355.\"}"}
{"id": "emnlp-2024-main-904", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023a. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565.\\n\\nHanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. 2024. A survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253.\\n\\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023b. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744.\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023c. Visual instruction tuning. arXiv preprint arXiv:2304.08485.\\n\\nXin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, and Yu Qiao. 2023d. Query-relevant images jailbreak large multi-modal models. arXiv preprint arXiv:2311.17600.\\n\\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nJohn X Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. arXiv preprint arXiv:2005.05909.\\n\\nOpenAI. 2023. Gpt-4 technical report.\\n\\nRenjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng Kong, and Tong Zhang. 2023a. Detgpt: Detect what you need via reasoning.\\n\\nRenjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang. 2024. Mllm-protector: Ensuring mllm\u2019s safety without hurting performance.\\n\\nRenjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, and Tong Zhang. 2023b. Perceptiongpt: Effectively fusing visual perception into llm.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695.\\n\\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa: A benchmark for visual question answering using world knowledge. arXiv.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\\n\\nWeihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. 2023a. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079.\\n\\nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. 2023b. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.\\n\\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483.\\n\\nBigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\\n\\nFangzhao Wu, Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, and Xing Xie. 2023. Defending chatgpt against jailbreak attack via self-reminder.\\n\\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178.\\n\\nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549.\\n\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.\\n\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592.\\n\\nAndy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043.\"}"}
{"id": "emnlp-2024-main-904", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Accuracy (Acc) of MLLMs on CorrelationQA under twelve categories when applied spurious image. We highlight the top three accuracy categories in blue background.\\n\\nA More Related Works\\n\\nAdversarial Attack on LLMs. Adversarial attacks are inputs that trigger the model to output something undesired (Zou et al., 2023) even when developers impose constraints on model behaviors during the alignment process for safety purpose, such as reinforcement learning from human feedback (RLHF). Existing studies have shown that LLMs are still easily attacked to generate irrelevant or inappropriate outputs through methods like adversarial prompts (Carlini et al., 2023; Wei et al., 2023; Li et al., 2020) and token manipulation (Morriss et al., 2020). On top of that, to bypass safeguarding mechanisms, various attack mechanisms (Wu et al., 2023; Jain et al., 2023) have been proposed to counteract user-driven adversarial behavior in both LLMs and MLLMs aspects. For example, Liu et al. (2023d); Pi et al. (2024) discovered that incorporating relevant images can trigger an image jailbreak in MLLMs, enabling the model to produce harmful information beyond what is achievable in a text-only scenario.\\n\\nB Detailed Prompts Example\\n\\nFigure 7 displays an example of generating question-answer (QA) pairs with GPT-4. We detail the system prompt for the animal category and provide three example QA pairs for GPT-4 as references. Due to the output token limit, GPT-4 could only produce 10 QA pairs once, so we require it to continue generating more examples.\\n\\nC More Experiments\\n\\nC.1 Manual Verification\\n\\nWe randomly sample 20% of the QA pairs from each category and verify if the actual answers match the true answers provided by GPT-4. The authenticity rates for QA pairs in each category are displayed in Table 9. Most of the categories have higher than 90% authenticity rates except the class city. The reason is that in the city category, there are some fictional cities from the movies and novels besides the real world, which results in naming conflict.\\n\\nC.2 Accuracy Results on Spurious Image\\n\\nIn Table 5 and Table 6, we present the accuracy of 9 MLLMs on spurious natural image and spurious typography, respectively. For the spurious natural image, categories like art, human, technology and history reach higher accuracy, which aligns with our analysis in section 3.3. For categories such as art, technology and history, the spurious answers are often non-visualizable concepts (e.g., 5G technology, the Battle of Waterloo), and in the human category, spurious image containing portraits or photographs are unrecognizable to MLLMs, thus failing to significantly deceive or mislead the models.\\n\\nFor the spurious typography, accuracy across all 13 categories is more uniform. Compared to natural image, the application of typography results in a lower average accuracy for each category. We assume that since the content of typography solely consists of OCR text which does not involve understanding the content and is independent of the category, MLLMs are more directly misled by spurious information.\\n\\nC.3 Accuracy Declined Ratio\\n\\nWe additionally defined Accuracy declined ratio to identify which categories experience the largest proportion of accuracy decline in MLLMs. The definition of accuracy declined ratio is as follows:\\n\\n$$\\\\text{ADR} = \\\\frac{\\\\text{AccDrop}}{\\\\text{Acc}}$$\\n\\n(3)\"}"}
{"id": "emnlp-2024-main-904", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Give you some examples of QA pairs. The content of QA pairs should include the truth and commonsense. No repeated examples and answers. The description of question should be complex as much as possible. Here are some examples:\\n\\n[Q]: A welsh person might say: \\\"our biggest and most diverse city, our cultural and media capital, with the most tourist attractions, foreigners, and rich people\\\". What is the name of the city? A: Cardiff\\n\\n[Q]: She's an American visual artist and author of multiple books. Her first name is Hillary. Who is she? A: Hillary Carlip\\n\\n[Q]: This American studied art at Yale and is the author of multiple popular mystery novels. First name is \\\"Hillary\\\". What's the last name? A: Hillary Waugh\\n\\nGive 100 examples in the format: [Q], A, W, while W means you should also give other 5 wrong confusing answers. Reference these to generate 100 similar examples relevant with animals.\\n\\n[Q]: Known for its distinctive black and white stripes, this African equine is actually closely related to horses and donkeys. What is it? A: Zebra, W: Tiger, Giraffe, Leopard, Hyena, Elephant\\n\\n[Q]: This large bear species native to North America is recognized by its prominent shoulder hump, which is a muscle mass used to power its forelimbs in digging and foraging. What is it? A: Grizzly Bear, W: Black Bear, Polar Bear, Panda Bear, Kodiak Bear, Sun Bear\\n\\n[Q]: This flightless bird, the largest by height, is native to Africa and can run at high speeds thanks to its strong legs. What is it? A: Ostrich, W: Emu, Kiwi, Penguin, Cassowary, Rhea\\n\\n[Q]: This insect, noted for the loud chirping sound produced by males, is often heard in the warm months and is a natural indicator of temperature. What is it? A: Cricket, W: Grasshopper, Cicada, Beetle, Ant, Mosquito\\n\\n\u2026\u2026\"}"}
{"id": "emnlp-2024-main-904", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Accuracy (Acc) of MLLMs on CorrelationQA under twelve categories when applied spurious typography.\\n\\n| Category         | CogVLM | Idefics | InstructBlip | MiniGPT-4 | mPLUG-Owl2 | Qwen-VL | LLaV A-7B | LLaV A-13B | GPT-4V | Average |\\n|------------------|--------|---------|--------------|-----------|------------|---------|-----------|------------|--------|---------|\\n| Animal Art       | 45%    | 67%     | 62%          | 82%       | 65%        | 76%     | 3%        | 5%         | 45%    | 50%     |\\n| Color            | 8%     | 1%      | 4%           | 21%       | 10%        | 15%     | 2%        | 0%         | 8%     | 5%      |\\n| City             | 49%    | 33%     | 66%          | 73%       | 69%        | 83%     | 13%       | 16%        | 53%    | 53%     |\\n| Food             | 41%    | 54%     | 55%          | 80%       | 55%        | 68%     | 6%        | 12%        | 45%    | 45%     |\\n| History          | 0%     | -3%     | 8%           | 9%        | 1%         | 3%      | 2%        | 0%         | 3%     | 3%      |\\n| Human            | 20%    | 6%      | 4%           | 4%        | 6%         | 18%     | 1%        | 5%         | 7%     | 7%      |\\n| Material         | 50%    | 61%     | 59%          | 79%       | 49%        | 68%     | 14%       | 14%        | 48%    | 48%     |\\n| Natural Objects  | 71%    | 61%     | 78%          | 86%       | 74%        | 86%     | 43%       | 57%        | 62%    | 62%     |\\n| Plant            | 12%    | 37%     | 16%          | 0%        | 21%        | 28%     | 5%        | 7%         | 13%    | 13%     |\\n| Sports           | 0%     | 6%      | 4%           | 71%       | 52%        | 57%     | 7%        | 7%         | 35%    | 35%     |\\n| Tech.            | 0%     | 37%     | 4%           | 71%       | 44%        | 68%     | 10%       | 10%        | 36%    | 36%     |\\n\\nTable 7: Accuracy declined ratio (the ratio between AccDrop (AccDrop) and Accuracy (Acc) on factual image) in natural image. It reflects the proportion of accuracy decline when models are exposed to spurious image compared to factual ones. We highlight the top three accuracy categories in blue background. Bold values are the maximum AccDrop proportion for each model.\\n\\n| Category         | CogVLM | Idefics | InstructBlip | MiniGPT-4 | mPLUG-Owl2 | Qwen-VL | LLaV A-7B | LLaV A-13B | GPT-4V | Average |\\n|------------------|--------|---------|--------------|-----------|------------|---------|-----------|------------|--------|---------|\\n| Animal Art       | 65%    | 78%     | 76%          | 82%       | 82%        | 87%     | 18%       | 9%         | 72%    | 68%     |\\n| Color            | 45%    | 48%     | 60%          | 51%       | 65%        | 72%     | 13%       | 27%        | 51%    | 46%     |\\n| City             | 67%    | 87%     | 71%          | 73%       | 88%        | 93%     | 13%       | 16%        | 68%    | 68%     |\\n| Food             | 93%    | 96%     | 95%          | 95%       | 100%       | 97%     | 12%       | 12%        | 72%    | 72%     |\\n| History          | 0%     | 34%     | 42%          | 47%       | 61%        | 61%     | 12%       | 13%        | 41%    | 41%     |\\n| Human            | 0%     | 60%     | 42%          | 48%       | 62%        | 62%     | 12%       | 12%        | 41%    | 41%     |\\n| Material         | 91%    | 95%     | 89%          | 97%       | 90%        | 86%     | 12%       | 12%        | 71%    | 71%     |\\n| Natural Objects  | 49%    | 61%     | 83%          | 97%       | 66%        | 71%     | 12%       | 12%        | 47%    | 47%     |\\n| Plant            | 0%     | 60%     | 42%          | 72%       | 76%        | 84%     | 12%       | 12%        | 41%    | 41%     |\\n| Sports           | 49%    | 61%     | 83%          | 97%       | 66%        | 71%     | 12%       | 12%        | 47%    | 47%     |\\n| Tech.            | 0%     | 60%     | 42%          | 72%       | 76%        | 84%     | 12%       | 12%        | 41%    | 41%     |\\n\\nTable 8: Accuracy declined ratio (the ratio between AccDrop (AccDrop) and Accuracy (Acc) on factual image) in typography. It reflects the proportion of accuracy decline when models are exposed to spurious image compared to factual ones. We highlight the top three accuracy categories in blue background. Bold values are the maximum AccDrop proportion for each model.\\n\\nwhere AccDrop denotes the pre-defined Accuracy drop metric of MLLMs, and Acc represents accuracy on factual image. A higher accuracy declined ratio indicates more severely affected by spurious information, which is similar to AccDrop but emphasizes the relative effects.\\n\\nTable 7 and Table 8 display the accuracy declined ratio results for natural image and typography. Our findings are consistent with those in subsection C.2. For natural image, categories like animal, color, and plant which consist of tangible entities experience a higher accuracy decline ratio. With typography, the accuracy decline ratio for all categories exceeds 30%. After applying spurious images, the decline ratio for typography in every category is higher than for natural images.\"}"}
{"id": "emnlp-2024-main-904", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A bird species famous for its ability to mimic sounds, including human speech, often kept as a pet for this unique talent. What is it? Answer no more than five words.\\n\\nIn traffic lights, this color instructs drivers to proceed with caution. What is the color? Answer no more than five words.\\n\\nA famous painting by Leonardo da Vinci, known for the subject's enigmatic smile and currently housed in the Louvre. What is it? Answer no more than five words.\\n\\nThis leafy green vegetable is commonly used in salads and is rich in vitamins. What is it? Answer no more than five words.\\n\\nThis material is often used in outdoor furniture due to its resistance to weather conditions. What is the material? Answer no more than five words.\\n\\nA major 18th-century conflict involving most of the great powers of Europe, it\u2019s often considered the first \u2018world war\u2019 due to its global impact. What is it? Answer no more than five words.\\n\\nThis natural occurrence involves the gradual increase in the Earth\u2019s average surface temperature. What is it? Answer no more than five words.\\n\\nA popular indoor plant known for its long, thin leaves and easy care, often used in Feng Shui to bring good luck. What is it? Answer no more than five words.\"}"}
{"id": "emnlp-2024-main-904", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Class         | Questions | Authenticity Rate |\\n|--------------|-----------|-------------------|\\n| Animal       | 105       | 100%              |\\n| Art          | 105       | 100%              |\\n| City         | 90        | 78%               |\\n| Color        | 99        | 95%               |\\n| Food         | 100       | 95%               |\\n| History      | 105       | 100%              |\\n| Material     | 90        | 90%               |\\n| Natural      | 100       | 100%              |\\n| Objects      | 105       | 100%              |\\n| Plant        | 105       | 91%               |\\n| Sports       | 95        | 95%               |\\n| Technology   | 105       | 100%              |\\n\\nTable 9: We present the total number of questions and the Authenticity rate of CorrelationQA. We randomly sample 20% of QA pairs from each category and manually verify the Authenticity of true answers given by GPT-4.\"}"}
