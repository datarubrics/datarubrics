{"id": "lrec-2024-main-60", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Lifelong Multilingual Multi-granularity Semantic Alignment Approach via Maximum Co-occurrence Probability\\n\\nXin Liu, Hongwei Sun, Shaojie Dai, Bo Lv, Youcheng Pan, Hui Wang\\n\\n1 Peng Cheng Laboratory, 2 Mudanjiang Normal University, 3 University of Chinese Academy of Sciences\\n{hit.liuxin, panyoucheng4}@gmail.com, emailshw0319@yeah.net, {daishaojie22,lvbo19}@mails.ucas.ac.cn, {wangh06,yuy}@pcl.ac.cn\\n\\nAbstract\\nCross-lingual pre-training methods mask and predict tokens in a multilingual text to generalize diverse multilingual information. However, due to the lack of sufficient aligned multilingual resources in the pre-training process, these methods may not fully explore the multilingual correlation of masked tokens, resulting in the limitation of multilingual information interaction. In this paper, we propose a lifelong multilingual multi-granularity semantic alignment approach, which continuously extracts massive aligned linguistic units from noisy data via a maximum co-occurrence probability algorithm. Then, the approach releases a version of the multilingual multi-granularity semantic alignment resource, supporting seven languages, namely English, Czech, German, Russian, Romanian, Hindi and Turkish. Finally, we propose how to use this resource to improve the translation performance on WMT14\u223c18 benchmarks in twelve directions. Experimental results show an average of 0.3\u223c1.1 BLEU improvements in all translation benchmarks. The analysis and discussion also demonstrate the superiority and potential of the proposed approach.\\n\\nKeywords: lifelong, multilingual, multi-granularity, alignment, maximum co-occurrence probability\\n\\n1. Introduction\\nThe alignment between languages is the key message for machine translation, and it encourages the models to learn the correlation of different languages and to achieve multilingual interaction (Mao et al., 2022; Tang et al., 2022; Adjali et al., 2022). Typically, the models could acquire the multilingual alignment information through the multilingual texts during the cross-lingual pre-training (Weiet al., 2021; Chi et al., 2021; Batheja and Bhattacharyya, 2022) or through the parallel corpora during the fine-tuning (Fernandez and Adlaon, 2022).\\n\\nHowever, although most models could learn accurate multilingual alignment information through the parallel corpora, they are usually limited by the insufficient scale of the parallel corpora and thus cannot learn sufficiently (Wang and Li, 2021; Chimoto and Bassett, 2022). In contrast, the cross-lingual pre-training methods based on various training strategies and large-scale multilingual texts alleviate this problem to some extent and inject the alignment information into the models (Yang et al., 2020a; Luo et al., 2021). So these models could present the multilingual correlation of different languages more or less. Benefiting from this alignment information in the pre-training process, these methods have shown promising performances in multilingual machine translation (Lin et al., 2020; Pan et al., 2021). However, constrained by not using explicit multilingual alignment resources during pre-training, these pre-training methods may not be able to explore the multilingual correlation of different tokens in multilingual texts as comprehensively and accurately as in parallel corpora (Yang et al., 2021). Thus, this undoubtedly presents an opportunity and raises an urgent need for a high-quality multilingual alignment resource for the further advances of the cross-lingual pre-training methods.\\n\\nTo address the need, this paper proposes a lifelong multilingual multi-granularity semantic alignment approach via maximum co-occurrence probability in noisy parallel data and uses it to build a semantic alignment resource. A linguistic unit is a sequence of consecutive tokens in a sentence, so it may be a word, phrase, segment, or short sentence. The approach collects a group of noisy pairs that contain the same linguistic unit in one language.\"}"}
{"id": "lrec-2024-main-60", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and computes the co-occurrence probability of one candidate linguistic unit in the other language. The co-occurrence probability is the probability that one linguistic unit appears in all sentences, so one candidate linguistic unit will have a higher probability if it occurs in most sentences. Figure 1 presents the English-German multi-granularity linguistic units in the resource built by this approach.\\n\\nThe approach can satisfy the above need from three aspects, namely the scale and quality, the linguistic diversity, and the lifelong property. Taking the large-scale noisy data as the data source and constraining the aligned unit with maximum co-occurrence probability ensure the scale and quality. The multi-granularity and multilingualism reflect the linguistic diversity. Through a lifelong stream of noisy data, the approach can continuously expand the resource with new languages or aligned units. Additionally, the resource can also be used in multilingual machine translation scenarios to boost translation performance through the combination of pre-training and fine-tuning strategies. In summary, we highlight our contributions as follows:\\n\\n\u2022 This paper proposes a lifelong multilingual multi-granularity semantic alignment approach that only relies on the co-occurrence constraints in the multilingual noisy data, and can identify massive semantically aligned linguistic units at various granularity through the maximum occurrence probability continuously and unsupervised.\\n\\n\u2022 The proposed approach releases a version of the lifelong multilingual multi-granularity semantic alignment resource (called LM\u00b2gSAR). In this version, LM\u00b2gSAR supports the multilingual alignment between seven languages, namely English (en), Czech (cs), German (de), Russian (ru), Romanian (ro), Hindi (hi) and Turkish (tr). Meanwhile, it also supports the continuous expansion in scale, language coverage, and granularity. The resource will be publicly available.\\n\\n\u2022 This paper conducts exhaustive experiments on the aligner comparisons and the bi-directional translation tasks between English and the above six languages. Compared to the other aligners, the approach shows higher alignment accuracy. The models using LM\u00b2gSAR have shown significant improvements in almost all translation directions. In addition, we perform objective analysis and discussion as strong evidence of the value and significance of this work.\\n\\n1 https://github.com/Gdls/MCoPSA\\n\\n2 Related Works\\n\\nThe mainstream pre-training methods rely on different mechanisms, techniques, or tools (Wu et al., 2022; Dou and Neubig, 2021) to learn multilingual alignment information. For example, MARGE (Lewis et al., 2020) learned with an unsupervised multilingual multi-document paraphrasing objective. Luo et al. (2021) plugged a cross-attention module into the Transformer encoder to build language interdependence. mRASP (Lin et al., 2020) introduced a random aligned substitution technique into the pre-training to bridge the semantic space. Yang et al. (2020b) performed lexicon induction with unsupervised word embedding mapping technique to learn the cross-lingual alignment information from monolingual corpus (Bajaj et al., 2022). Tang et al. (2022) specifically highlighted the importance of word embedding alignment by guiding similar words in different languages. Yang et al. (2020a) performed the word alignment with the GIZA++ (Casacuberta and Vidal, 2007) toolkit to code-switch the sentences of different languages to capture the cross-lingual context of words and phrases. Yang et al. (2021) proposed to use FastAlign (Dyer et al., 2013) as the prior knowledge to guide cross-lingual word pre-diction. These mechanisms, techniques, or tools have boosted the capabilities of these methods on generalizing alignment information, but due to the absence of accurate and sufficient alignment resources, there is still a lot of room for improving their capabilities.\\n\\nNormally, the common multilingual alignment resources are the parallel corpora, which come from the public releases (Ziemski et al., 2016), web mining (Tiedemann and Nygaard, 2004), or competitions. These resources are usually aligned at the sentence level and can be used to train the translation models directly. The other resources mainly focus on the word or phrase level (Imani et al., 2022), e.g., the multilingual paraphrase database (Ganitkevitch and Callison-Burch, 2014), the multilingual lexical database (Giguet and Luquet, 2006), the multilingual multi-word expression corpora (Han et al., 2020), automatic similarity-based dataset (Yousef et al., 2022) and unpublished synonym dictionary (Pan et al., 2021). Although these resources could provide multilingual alignment information, the scale or linguistic diversity may not meet the need for the pre-training methods.\"}"}
{"id": "lrec-2024-main-60", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nThe maximum co-occurrence probability based semantic alignment algorithm.\\n\\n1: \\\\textbf{procedure} \\\\texttt{MCoPSA} (\\\\texttt{ul}, \\\\texttt{DN})\\n2: \\\\hspace{1em} \\\\texttt{Assert} \\\\texttt{ul} \\\\in \\\\texttt{X} and \\\\texttt{ul} \\\\in \\\\texttt{Y};\\n3: \\\\hspace{1em} \\\\texttt{Initialize} \\\\texttt{G}(\\\\texttt{ul}) = (p_0, \\\\ldots, p_n);\\n4: \\\\hspace{1em} \\\\texttt{Initialize} \\\\texttt{lists L=}, \\\\texttt{ul}; \\\\texttt{dict D=}\\n5: \\\\hspace{1em} \\\\texttt{Select} \\\\texttt{tY}_0, \\\\texttt{tY}_1 from \\\\texttt{G}(\\\\texttt{ul});\\n6: \\\\hspace{1em} \\\\texttt{G}(\\\\texttt{ul}) = \\\\texttt{G}(\\\\texttt{ul}) - p_0 - p_1;\\n7: \\\\hspace{1em} \\\\texttt{L.append(} \\\\texttt{tY}_0, \\\\texttt{tY}_1\\\\texttt{);}.\\n8: \\\\hspace{1em} \\\\texttt{ul.extend(} \\\\texttt{CSFunc(} \\\\texttt{tY}_0, \\\\texttt{tY}_1\\\\texttt{);}.\\n9: \\\\hspace{1em} \\\\texttt{Update cnt(} \\\\texttt{ul}\\\\texttt{);}.\\n10: \\\\hspace{1em} \\\\texttt{while} \\\\texttt{G(ul)} is not \\\\texttt{\u00d8} \\\\texttt{do}\\n11: \\\\hspace{2em} \\\\texttt{Select} \\\\texttt{tY}_i from \\\\texttt{G(ul)};\\n12: \\\\hspace{2em} \\\\texttt{G(ul)} = \\\\texttt{G(ul)} - p_i;\\n13: \\\\hspace{2em} \\\\texttt{for} \\\\texttt{tY}_j \\\\texttt{in L} \\\\texttt{do}\\n14: \\\\hspace{3em} \\\\texttt{ul.extend(} \\\\texttt{CSFunc(} \\\\texttt{tY}_i, \\\\texttt{tY}_j\\\\texttt{);}.\\n15: \\\\hspace{3em} \\\\texttt{Update cnt(} \\\\texttt{ul}\\\\texttt{);}.\\n16: \\\\hspace{2em} \\\\texttt{L.append(} \\\\texttt{tY}_j\\\\texttt{);}.\\n17: \\\\hspace{2em} \\\\texttt{for} \\\\texttt{ul}_k \\\\texttt{Y}_i \\\\texttt{\u2208 ul} \\\\texttt{do}\\n18: \\\\hspace{3em} \\\\texttt{D[ul}_k\\\\texttt{]} = \\\\texttt{cnt[ul}_k\\\\texttt{]} / \\\\texttt{len(L)};\\n19: \\\\hspace{2em} \\\\texttt{end for}\\n20: \\\\hspace{1em} \\\\texttt{end while}\\n21: \\\\hspace{1em} \\\\texttt{ulY} = \\\\texttt{maxProb(D, \u03f1)};\\n22: \\\\hspace{1em} \\\\texttt{Return (} \\\\texttt{ul}, \\\\texttt{ulY}\\\\texttt{);}.\\n23: \\\\textbf{end procedure}\\n\\nFigure 2 presents an example of English and Romanian pairs to illustrate the processing of Algorithm 1. The linguistic unit \\\\texttt{ul} is from English, and we have \\\\texttt{ul}_\\\\texttt{EN} = \\\\text{calculation error}. There are four \\\\texttt{EN-RO} pairs in its group \\\\texttt{G(ul)} = (p_0, p_1, p_2, p_3). When \\\\texttt{p}_0 and \\\\texttt{p}_1 are selected, the algorithm will output the candidate linguistic units, namely \\\"\\\\text{erori de calcul}\\\", \\\"\\\\text{sunt}\\\", \\\"\\\\text{de}\\\", and their co-occurrence probabilities in the current step. Next, when \\\\texttt{p}_2 comes, the algorithm updates the candidate list and their...\"}"}
{"id": "lrec-2024-main-60", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"there may be a calculation error. The resulting difference is not a calculation error. He had 5 accidents at the same company, he made a calculation error at another company, etc. These events are concerning because they could lead to accidents or calculation errors. There are 5 pairs of aligned linguistic units in the current step: \\n\\n- English: \\\"calculation error\\\" \\n- Romanian: \\\"erori de calcul\\\"\\n\\nThe algorithm repeats the calculation with the coming of new pairs. Finally, when receiving the last one, the algorithm outputs the final candidate linguistic units with the ranking of co-occurrence probability and takes the one with the maximum co-occurrence probability as the aligned linguistic unit, namely \\\"erori de calcul\\\". So, in this example, \\\"calculation error\\\" and \\\"erori de calcul\\\" is the semantic alignment between English and Romanian.\\n\\n3.2. The statistics on LM$_2$gSAR\\n\\nBased on the MCoPSA algorithm, this paper released the first version of the lifelong multilingual multigranularity semantic alignment resource (LM$_2$gSAR). This section will detail the statistics on LM$_2$gSAR from three aspects: the languages, the scale, and the linguistic diversity.\\n\\nIn this release, LM$_2$gSAR supports seven languages, namely English, Czech, German, Russian, Romanian, Hindi and Turkish. Meanwhile, it is built on the noisy bilingual data from published CCMatrix v1 (Schwenk et al., 2021) between English and the other six languages. When building it with the en-XX bilingual data, the MCoPSA algorithm takes the English linguistic units as input and outputs its alignment in language XX, where XX is one of the other six languages.\\n\\nFrom the scale, Table 1 lists the statistics on the scale of the bilingual data used in the MCoPSA algorithm for building LM$_2$gSAR, including the volume of the bilingual data and its percentage in CCMatrix v1. Table 2 shows the statistics on the scale of the aligned linguistic units in LM$_2$gSAR between seven languages.\\n\\nSince the original scale of the bilingual data in CCMatrix v1 varies greatly, we randomly sampled a certain percentage for each en-XX bilingual data. The table below shows the statistics on the scale of the bilingual data from published CCMatrix v1 (Schwenk et al., 2021) used in the MCoPSA algorithm for building LM$_2$gSAR. The magnitude \\\"M\\\" in the second column is million. The third column \\\"PinND\\\" is the final percentage of CCMatrix data used in this work.\\n\\n| Languages | Volume | PinND |\\n|----------|--------|-------|\\n| en-de    | 21.5M  | 8.7   |\\n| en-ru    | 20.6M  | 14.7  |\\n| en-cs    | 15.6M  | 27.7  |\\n| en-ro    | 15.1M  | 27.2  |\\n| en-tr    | 14.2M  | 29.2  |\\n| en-hi    | 5.5M   | 36.4  |\\n\\nTable 1: The scale of the bilingual data from published CCMatrix v1 (Schwenk et al., 2021) used in the MCoPSA algorithm for building LM$_2$gSAR. The magnitude \\\"M\\\" in the second column is million. The third column \\\"PinND\\\" is the final percentage of CCMatrix data used in this work.\\n\\nFrom the scale, Table 1 lists the statistics on the scale of the bilingual data used in the MCoPSA algorithm for building LM$_2$gSAR, including the volume of the bilingual data and its percentage in CCMatrix v1. Table 2 shows the statistics on the scale of the aligned linguistic units in LM$_2$gSAR between seven languages.\\n\\nTable 2: The statistics on the scale of the aligned linguistic units in LM$_2$gSAR between seven languages.\\n\\nIt is 10% for en-de, 20% for en-ru, 30% for en-cs, en-ro, and en-tr, and 40% for en-hi. Table 1 presents the final scale and percentage after the post-processing, e.g., deduplication, discarding, merging, etc. In Table 2, the first column indicates that the scale of the aligned linguistic units between English and each language is in millions, which is an encouraging scale. In the construction, the MCoPSA algorithm takes the English linguistic unit as input and outputs its aligned unit. So we take the English linguistic unit as a bridge and can easily get the aligned linguistic unit between any two languages except English. The remaining columns in Table 2 present the scale of the aligned linguistic units between the other six languages. Obviously, the scale between any two of these languages is more than one hundred thousand, and it is also a promising scale.\\n\\nFrom the linguistic diversity, Table 3 shows the statistics on the granularity distribution of the aligned linguistic units in LM$_2$gSAR on the en-XX alignment. In each row, Table 3 presents the percentage of this granularity in all alignments. For accuracy, we only report the statistics on the en-XX alignment since the granularity in the English linguistic unit is known during the construction. In Table 3, most of the aligned units belong to the phrase or segment level, which is because the phrases and segments are widely used through the word combination in language expression. But the words are usually finite, so the percentage is stable. As for the short-sentence-level granularity, it\"}"}
{"id": "lrec-2024-main-60", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The statistics on the granularity distribution of the aligned linguistic units in LM2gSAR based on the en-XX alignment. 'w-1': word-level by unigram, 'p-2' and 'p-3': phrase-level by bi-gram or tri-gram, 's-4': segment-level by four-gram, and 'ss-5+': short sentence-level.\\n\\n| Language Pair | w-1 | p-2 | p-3 | s-4 | ss-5+ |\\n|---------------|-----|-----|-----|-----|-------|\\n| en-cs         | 3   | 21  | 44  | 31  | 1     |\\n| en-de         | 4   | 23  | 42  | 30  | 1     |\\n| en-ru         | 4   | 20  | 44  | 31  | 1     |\\n| en-ro         | 3   | 19  | 42  | 34  | 2     |\\n| en-tr         | 3   | 30  | 44  | 21  | 2     |\\n| en-hi         | 4   | 28  | 43  | 24  | 1     |\\n| Avg           | 3.5 | 23.5| 43.2| 28.5| 1.3   |\\n\\nAn interesting observation is that the alignment methods using multi-gram approaches (i.e., phrase and segment-level alignments) achieve better performance than those using mono-gram approaches (i.e., word-level alignments). This suggests that using higher granularities in the alignment process can lead to more accurate and efficient alignment results.\\n\\n4. Experiments and Results\\n\\n4.1. Experimental datasets and metrics\\n\\nIn this work, we validate the proposed approach through two experiments, namely the aligner comparison experiment and the machine translation experiment. In the aligner comparison experiment, we select the well-known statistical aligners (GIZA++, Fast-Align) and neural aligner (Awesome-align) to compare with our proposed methods on the same test set. In this process, we first randomly collected a group of en-XX corpora that are not used in Section 3.2 and selected the top 500 language units in each en-XX corpus based on the term frequency and inverse document frequency. Second, we recruited some language experts with English and XX backgrounds, and for each pair, they manually annotated the golden alignment of the English language unit in XX sentences, which serves as the evaluation test set. Next, we applied the proposed MCoPSA algorithm, GIZA++, Fast-Align, and Awesome-align to compute the alignments of the top 500 language units. Finally, we performed the evaluation using the metrics of the alignment error rates (AER).\\n\\nIn the machine translation experiment, we apply the resource that the approach built to the machine translation tasks. We select the WMT datasets including twelve translation directions as the evaluation benchmarks, namely en-de (4.5M), en-ru (1.1M), and en-hi (32K) in WMT14, en-ro (0.6M) in WMT16, en-tr (0.2M) in WMT17, and en-cs (11M) in WMT18. Based on the scale of the training data in each dataset, we follow the division in Tang et al. (2021) and Lin et al. (2020) to divide the datasets into four categories: extremely low resource (<100K), low resource (>100k and <1M), medium resource (>1M and <10M), and high resource (>10M). These datasets are publicly available, and anyone can easily access the same training, validation, and test sets for reproduction or comparison. For all evaluation benchmarks, we take the BLEU score as the metrics and it is computed with the official sacreBLEU (Post, 2018) with default tokenization.\\n\\n4.2. Baseline and comparison methods\\n\\nIn the aligner comparison experiment, the statistical aligners for comparison are GIZA++ (Casacuberta and Vidal, 2007) and Fast-Align (Dyer et al., 2013), and the neural aligner is Awesome-align (Batheja and Bhattacharyya, 2022). In these experiments, we followed the default setting of each method.\\n\\n- **GIZA++** is an extension of the program GIZA (part of the SMT toolkit EGYPT). We used the version released by Och and Ney (2003).\\n\\n- **Fast-align** is a simple, fast, unsupervised word aligner. We used the version released by Dyer et al. (2013) from the Github page.\\n\\n- **Awesome-align** is a tool that can extract word alignments from multilingual BERT. We used the version released by Dou and Neubig (2021) from the Github page.\\n\\nIn machine translation experiments, three famous open-source multilingual models are selected as the baseline, namely mBART (Liu et al., 2020), M2M100 (Fan et al., 2021), and mT5 (Xue et al., 2021). In these experiments, all the codes and checkpoints for these models are from the public Hugging Face hub. One reason is that these models are all multilingual models and cover enough languages to evaluate our LM2gSAR as it grows continuously. Another reason is that these models come from different types of pre-training tasks, which can demonstrate the quality of LM2gSAR from different aspects.\\n\\n- **mBART** is one of the first methods for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages. The initial checkpoint of mBART model we used in this work is mbart-large-cc25 (5).\\n\\n- **M2M-100** is a Many-to-Many multilingual translation model that can translatedirectly between 2 languages.\"}"}
{"id": "lrec-2024-main-60", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"any pair of 100 languages. The initial check-\\npoint of M2M-100 model we used in this work\\nis m2m100_418M.\\n\\n\u2022 mT5 is pre-trained on a new Common Crawl-\\nbased dataset covering 101 languages and\\nhas shown SOTA performance on many multi-\\nlingual benchmarks. The initial checkpoint of\\nmT5 model we used in this work is mt5-base\\n\\n4.3. Experimental setup\\nSome experimental settings or hyperparameters\\nfor the machine translation task in this work are\\nlisted below: all experiments with pre-training\\nor fine-tuning are based on three baseline mod-\\nels. In these experiments, the tokenizer in each\\nmodel is the default one, namely MBartTokenizer,\\nM2M100Tokenizer, and T5Tokenizer. The training\\nbatch size is 4\u223c16 in all experiments. The max se-\\nquencelengthis1024. Thebeamsizefordecoding\\nis 5. Checkpoints are saved every 1000 steps in\\nhigh and medium resource benchmarks, and every\\n100-500 steps for low and extremely-low resource\\nbenchmarks. We use the AdamW (Loshchilov\\nand Hutter, 2017) optimizer with an initial learn-\\ning rate of 5e-5. Early stopping is used when\\nthe training loss converges during the pre-training\\nand fine-tuning process, and we select the hyper-\\nparameters based on the validation set.\\n\\n4.4. Training strategy in machine\\ntranslation experiment\\nWe adopt a new strategy to (pre-)train a baseline\\nmodel with LM\\n2gSAR, called LM\\n2gSAR-based pre-\\ntraining and fine-tuning. In this training strategy,\\nwe first apply the align-\\nment substitution technique (AST) with LM\\n2gSAR to\\nprepare the pre-training corpus. In this part, the\\nmonolingual sentences used to construct the align-\\nment substitution with LM\\n2gSAR come from the cor-\\nresponding bilingual training data. Therefore, dur-\\ning the LM\\n2gSAR pre-training, no additional mono-\\nlingual or bilingual data is introduced; the same\\ndata source is utilized in the fine-tuning phase. A\\nbaselinemodelispre-trainedwiththecorpus. Next,\\nthe pre-trained model is fine-tuned with the training\\ndata. Finally, the trained model is evaluated on the\\ntest set. The AST technique is similar to the previ-\\nous works (Lin et al., 2020; Yang et al., 2020a) that\\ngiven a monolingual sentence\\nS,\\nAST substitutes\\nthelinguisticunitsin\\nS\\nwiththecorrespondingalign-\\nments in LM\\n2gSAR to produce a mixed language\\nsentence\\nSx.\\nDuring the pre-training, the input of\\nthe model is\\nSx\\nand the output is its original sen-\\ntence\\nS.\\nFor example, in pair\\np3\\nof Figure 2, for\\n\\nBenchmark Scale(->/<-) PinT(%) AvgLSu AvgLSe AvgP(%)\\nen-cs 1.2M/1.2M 10.4 2.3 9.7 23.8\\nen-de 3.0M/3.0M 66.5 5.6 23.1 24.1\\nen-ru 0.2M/0.2M 11.8 1.9 15.9 11.9\\nen-ro 0.6M/0.6M 92.9 5.3 23.1 22.3\\nen-tr 0.1M/0.1M 70.1 2.9 21.4 13.5\\nen-hi 5.0K/7.6K 19.2 1.1 2.1 51.2\\n\\nTable 4:\\nThe statistics on the pre-training corpus for\\neach benchmark. \u201cScale(->/<-)\u201d is the pre-training cor-\\npus scale for both translation directions. \u201cPinT\u201d indi-\\ncates the percentage of the pre-training corpus scale\\nin each benchmark to its training data scale. \u201cAvgLSu\u201d\\nis the average length of the substitution, and \u201cAvgLSe\u201d\\nis the average length of the original sentence. \u201cAvgP\u201d is\\nAvgLSu/AvgLSe.\\n\\nGIZA++ Fa-Align Aw-Align MCoPSA\\nen-cs 48.4 42.1 30.8\\n19.8\\nen-de 61.2 58.7 29.0\\n17.2\\nen-hi 67.2 66.0 30.3\\n16.2\\nen-ro 53.4 50.2 24.4\\n16.4\\nen-ru 50.1 46.1 21.8\\n15.2\\nen-tr 63.2 72.6 30.3\\n12.4\\nAvgs 57.3 55.9 24.4\\n16.2\\n\\nTable 5:\\nThe AER scores on each en-XX test set of the\\nMCoPSA, GIZA++, FastAlign (Fa-Align), and Awesome-\\nAlign (Aw-Align).\\n\\nword phrase segment Avgs\\nGIZA++ 52.2 48.4 58.1 52.9\\nFa-Align 54.5 47.4 56.1 52.7\\nAw-Align 24.1 20.2 19.5\\nMCoPSA 25.4 13.1 13.4 17.3\\n\\nTable 6:\\nThe AER score of each method at word, phrase,\\nand segment granularity of the linguistic unit on the en-ru\\ntest set. The \u201cAvgs\u201d column is the average of the three\\ngranularities and is therefore slightly different from that\\nin Table 5.\"}"}
{"id": "lrec-2024-main-60", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The BLEU scores of the baseline models under different training strategies on the test sets of each WMT benchmark. The blocks in \u201cBenchmark\u201d corresponds to the \u201chigh (H)/medium (M)/low (L)/extremely low (El)\u201d resource according to their official training data volume. The bold is the best BLEU score in this direction. Here, \u201c\u2192/\u2190\u201d is the translation direction. \u201cAvg \u2206\u201d: average of the difference of all models between the upper line and below line, and \u201c+\u201d and \u201c\u2191\u201d mean improvement.\\n\\nPreliminary exploration of the correlation of language units between the parallel data based on the co-occurrence constraint. Besides, the performance difference between MCoPSA and the others also indicates that the alignments by MCoPSA are of better quality and more promising for the pre-training stage in real-world scenarios. To further investigate their ability, we report the AER score of each method at word, phrase, and segment granularity of the linguistic unit on the en-ru test set in Table 6, which helps to indicate the ability of each method on different granularity. The GIZA++ and Fast-Align seem to have the similar performance on each granularity, while the Awesome-Align performs best on word-level. However, the proposed MCoPSA show a much better performance on phrase and segment-level, and this may be the main reason why the proposed MCoPSA achieves the best results in Table 5 and 6.\\n\\nTable 7 lists the BLEU scores of the baseline models under different training strategies on the test sets of each WMT benchmark. For each test set, we provide two lines of results from three baseline models. The upper one is the results of fine-tuning the baseline models with the official training data, and the below one is the results of the LM2gSAR-based pre-training and fine-tuning. In table 7, we highlight the lines of fine-tuning with underline and ::::: the lines of ::::::::: LM2gSAR-based pre-training and ::::::::: fine-tuning with wave line, respectively. For clarity, we denote a model with only fine-tuning as model f and that with LM2gSAR-based pre-training and fine-tuning as model p\u00b7f.\\n\\nFrom the results, we have the following observation: 1) Each model with the LM2gSAR pre-training and fine-tuning shows better performances than the only fine-tuning one on all benchmarks, with an average of 0.3 \u223c 1.1 BLEU improvement in the six benchmarks (See \u201cAvg \u2206\u201d column). This is strong evidence that LM2gSAR contributes to the translation task. 2) Almost all models p\u00b7f show some improvement over models f that were only fine-tuned (see results with \u201c\u2191\u201d). In particular, the improvement of the best results in bold on each benchmark is significant. Even though in some directions, such as M2M100 in en<-cs, en->de, en-ro, and mT5 in en<-tr, models p\u00b7f is slightly worse than models f, the results are still very competitive. 3) The table also indicates the LM2gSAR pre-training is somewhat helpful for the high/medium/low/extremely low resource translations, and the results show a consistent improvement trend. 4) In the table, we have bolded the best BLEU scores for both directions, and the best results are almost from mBART p\u00b7f. The M2M100 p\u00b7f and mT5 p\u00b7f also perform better in most cases. It is worth noticing that one minibenchmarks, the M2M100 p\u00b7f far exceeds the others in both directions. This may benefit from the mechanism in M2M100 that its initial parameters are pre-trained with pseudo-parallel data, and the LM2gSAR pre-training in this work can further strengthen its ability.\\n\\nAt present, the LM2gSAR pre-training in this experiment is just an initial attempt, the improvement is still significant. Once we expand the scale of the pre-training, it will be really encouraging. Considering the improvement from multiple dimensions, the experimental results show LM2gSAR has a significant\"}"}
{"id": "lrec-2024-main-60", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.6. Significant test\\n\\nIn this work, we also performed the significance test on machine translation tasks based on the models that report the best BLEU scores in both directions (see the bold results in Table 7). The well-known Wilcoxon signed-rank test was used to measure whether the improvement between the corresponding data distributions in two samples is significant.\\n\\nIn the Wilcoxon signed-rank test, we first randomly sampled 50% data in each test set 20 times and used the model $p \\\\cdot f$ and model $f$ to predict the translations on the sample data. Second, we scored the translation results with the sacreBLEU script to obtain the BLEU score on each direction of each benchmark. After sampling 20 times, we had a sequence of BLEU scores with the length of 20 for model $p \\\\cdot f$ and model $f$, respectively. Finally, the corresponding BLEU score sequences for model $p \\\\cdot f$ and model $f$ were input into the \\\"wilcox.test()\\\" function in R Tutorial, and the function will output the P-value of two sequences to indicate the significance. If P-value < 0.05, the improvement between model $p \\\\cdot f$ and model $f$ is significant, otherwise not.\\n\\nFinally, in Table 7, on each translation direction of each benchmark, the improvement between model $p \\\\cdot f$ and model $f$ on BLEU score is significant (P-value < 0.05).\\n\\n5. Discussion\\n\\nIn section 3.2, this paper has proven the advantages of scale and linguistic diversity of the proposed approach via statistics on LM$_2$gSAR. This section discusses the quality and lifelong property.\\n\\n5.1. Quality control in MCoPSA algorithm\\n\\nIn the MCoPSA algorithm, the function maxProb(\u00b7) provides a series of post-processing operations to output the co-occurrence probability. We find that it is a key point to control the quality of the aligned linguistic units in MCoPSA. To prove the quality, we performed an analysis experiment on en-XX languages. In this experiment, we used the public LaBSE (Feng et al., 2022) to evaluate the aligned linguistic units that MCoPSA extracted with or without the maxProb(\u00b7) function. LaBSE can map languages to a shared vector space and compute their similarities. Given the en-XX aligned linguistic units, LaBSE will output a similarity score.\\n\\nFirst, we collected the aligned linguistic units based on \\\"MCoPSA w/o maxProb(\u00b7)\\\" in en-XX languages. Second, we used LaBSE to compute the similarity scores and ranked them in ascending order. Then, we divided the ranked units into five-folds, and each fold contains 20% of the whole units. Finally, we averaged the LaBSE scores in each fold. The average LaBSE score curves (Dotted curve) for each fold on en-XX languages are presented in Figure 3. The dotted curves indicate that the scores range from 0.2 to 1.0, and a certain percentage ($\\\\approx 40\\\\%$) of data falls into $0.2 \\\\sim 0.6$.\\n\\nBased on our manual statistics, the aligned linguistic units with a LaBSE score of less than 0.6 are quite noisy. Next, we repeated the operations with \\\"MCoPSA w/ maxProb(\u00b7)\\\" to recollect and recompute the LaBSE score of the aligned linguistic units. The solid curves in Figure 3 indicate the distributions of the scores, which range from 0.7 to 1.0, which brings a great improvement for each language. The score ranges between two curves prove that the maxProb(\u00b7) function plays a key role in quality control. Meanwhile, the LaBSE scores are over 0.7 and beyond our statistic of 0.6, indicating that the aligned linguistic units via the MCoPSA\"}"}
{"id": "lrec-2024-main-60", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.2. Lifelong property of the approach\\n\\nThe term \\\"lifelong\\\" is an important property of the proposed approach and a key differentiator from other known methods. It refers to the sustainability and extensibility of the proposed approach, which is mainly reflected in the language extension and continuous scale expansion of the resource.\\n\\nFirst, in this paper, the proposed approach released the first version of LM$_2$gSAR, which supports seven languages. But from Table 1 in Section 3.2, we know that the approach only relies on the en-XX bilingual data. The multilingualism in Table 2 also shows that the approach presents positive effects between languages. So the approach can easily extend new languages into LM$_2$gSAR through their bilingual data, and make connections between the new language and other languages to improve linguistic diversity. Second, Table 1 also shows that the noisy bilingual data used in this paper is only a part of the original library, and the linguistic units are far from reaching the upper bound. Thus, with the expansion of the paralleldata, the approach can expand the scale of the resource, and supplement more linguistic units to perfect its resource.\\n\\n6. Conclusion\\n\\nIn this paper, to alleviate the problem of lacking sufficient alignment resources in the pre-training methods, we proposed a lifelong multilingual multi-granularity semantic alignment approach via maximum co-occurrence probability in the noisy parallel data and released a version of its corresponding resource. We also conducted experiments to prove the ability of the MCoPSA algorithm compared to the traditional aligners and elaborate on how to use the resource to prove its effectiveness in machine translation tasks. The experimental results, analysis, and discussion also prove the superiority of the proposed approach and resource. In the future, we will continue to optimize the approach from the quality and linguistic diversity. Meanwhile, we will release more versions of the resource with the optimized approach to support more languages and provide a bigger scale. Besides, we will explore the strategies for utilizing the resource to contribute to the pre-training methods. At the same time, the approaches and resources will gradually be opened to the public.\\n\\n7. Acknowledgements\\n\\nWe thank all the reviewers for their efforts to make the paper comprehensive and solid. This work is supported by the National Key Research and Development Program of China (Grant No. 2021ZD0112905), the Major Key Project of PCL (Grant No. PCL2023A09), the National Natural Science Foundation of China (Grant No. 62206140), and the China Postdoctoral Science Foundation (Grant No. 2022M711726).\\n\\n8. Bibliographical References\\n\\nOmar Adjali, Emmanuel Morin, and Pierre Zweigenbaum. 2022. Building comparable corpora for assessing multi-word term alignment. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3103\u20133112, Marseille, France. European Language Resources Association.\\n\\nGoonmeet Bajaj, Vinh Nguyen, Thilini Wijesiriwardene, Hong Yung Yip, Vishesh Javangula, Amit Sheth, Srinivasan Parthasarathy, and Olivier Bodenreider. 2022. Evaluating biomedical word embeddings for vocabulary alignment at scale in the UMLS Metathesaurus using Siamese networks. In Proceedings of the Third Workshop on Insights from Negative Results in NLP, pages 82\u201387, Dublin, Ireland. Association for Computational Linguistics.\\n\\nAkshay Batheja and Pushpak Bhattacharyya. 2022. Improving machine translation with phrase pair injection and corpus filtering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5395\u20135400, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nFrancisco Casacuberta and Enrique Vidal. 2007. Giza++: Training of statistical translation models.\\n\\nZewen Chi, Li Dong, Bo Zheng, Shaohan Huang, Xian-Ling Mao, He-Yan Huang, and Furu Wei. 2021. Improving pretrained cross-lingual language models via self-labeled word alignment. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3418\u20133430.\\n\\nEverlyn Chimoto and Bruce Bassett. 2022. Very low resource sentence alignment: Luhya and Swahili. In Proceedings of the Fifth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2022), pages 1\u20138, Gyeongju, Republic of Korea. Association for Computational Linguistics.\"}"}
{"id": "lrec-2024-main-60", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-60", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Matt Post. 2018. A call for clarity in reporting bleu scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013191.\\n\\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, \u00c9douard Grave, Armand Joulin, and Angela Fan. 2021. Ccmatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6490\u20136500.\\n\\nHenry Tang, Ameet Deshpande, and Karthik Narasimhan. 2022. Align-mlm: Word embedding alignment is crucial for multilingual pre-training. arXiv preprint arXiv:2211.08547.\\n\\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3450\u20133466.\\n\\nJ\u00f6rg Tiedemann and Lars Nygaard. 2004. The OPUS corpus - parallel and free: http://logos.uio.no/opus. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04), Lisbon, Portugal. European Language Resources Association (ELRA).\\n\\nMingxuan Wang and Lei Li. 2021. Pre-training methods for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Tutorial Abstracts, pages 21\u201325.\\n\\nXiangpeng Wei, Rongxiang Weng, Yue Hu, Luxi Xing, Heng Yu, and Weihua Luo. 2021. On learning universal representations across languages. In International Conference on Learning Representations.\\n\\nDi Wu, Liang Ding, Shuo Yang, and Mingyang Li. 2022. MirrorAlign: A super lightweight unsupervised word alignment model via cross-lingual contrastive learning. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 83\u201391, Dublin, Ireland (in-person and online). Association for Computational Linguistics.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498.\\n\\nJian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi Wu, Zhoujun Li, and Ming Zhou. 2020a. Alternating language modeling for cross-lingual pre-training. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9386\u20139393.\\n\\nZhen Yang, Bojie Hu, Ambyera Han, Shen Huang, and Qi Ju. 2020b. Csp: Code-switching pre-training for neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2624\u20132636.\\n\\nZiqing Yang, Wentao Ma, Yiming Cui, Jiani Ye, Wanxiang Che, and Shijin Wang. 2021. Bilingual alignment pre-training for zero-shot cross-lingual transfer. In Proceedings of the 3rd Workshop on Machine Reading for Question Answering, pages 100\u2013105.\\n\\nTariq Yousef, Chiara Palladino, David J. Wright, and Monica Berti. 2022. Automatic translation alignment for Ancient Greek and Latin. In Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages, pages 101\u2013107, Marseille, France. European Language Resources Association.\\n\\nMicha\u0142 Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen. 2016. The United Nations parallel corpus v1.0. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 3530\u20133534, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\"}"}
