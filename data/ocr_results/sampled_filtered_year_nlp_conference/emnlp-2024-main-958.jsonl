{"id": "emnlp-2024-main-958", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Medical texts are notoriously challenging to read. Properly measuring their readability is the first step towards making them more accessible. In this paper, we present a systematic study on fine-grained readability measurements in the medical domain at both sentence-level and span-level. We introduce a new dataset MEDREADME, which consists of manually annotated readability ratings and fine-grained complex span annotation for 4,520 sentences, featuring two novel \\\"Google-Easy\\\" and \\\"Google-Hard\\\" categories. It supports our quantitative analysis, which covers 650 linguistic features and automatic complex word and jargon identification. Enabled by our high-quality annotation, we benchmark and improve several state-of-the-art sentence-level readability metrics for the medical domain specifically, which include unsupervised, supervised, and prompting-based methods using recently developed large language models (LLMs). Informed by our fine-grained complex span annotation, we find that adding a single feature, capturing the number of jargon spans, into existing readability formulas can significantly improve their correlation with human judgments. We will publicly release the dataset and code.\\n\\n1 Introduction\\n\\nIf you can't measure it, you can't improve it.\\n\u2013 Peter Drucker\\n\\nTimely disseminating reliable medical knowledge to those in need is crucial for public health management (August et al., 2023). Trustworthy platforms like Merck Manuals and Wikipedia contain extensive medical information, while research papers introduce the latest findings, including emerging medical conditions and treatments (Joseph et al., 2023). However, comprehending these resources can be very challenging due to their technical nature and the extensive use of specialized terminology (Zeng et al., 2005). As the first step to making them more accessible, properly measuring the readability of medical texts is crucial (Rooney et al., 2021; Echuri et al., 2022). However, a high-quality multi-source dataset for reliably evaluating and improving sentence readability metrics for medical domain is lacking.\\n\\nTo address this gap in research, we present a systematic study for medical text readability in this paper, which includes a manually annotated readability dataset (\u00a72), a data-driven analysis to answer \\\"why medical sentences are so hard\\\" (\u00a73), a comprehensive benchmark of state-of-the-art readability metrics (\u00a74.1), a simple yet effective simplification approach (\u00a75), and an annotation study (\u00a76).\"}"}
{"id": "emnlp-2024-main-958", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The distribution of sentence readability (boxplot on the left y-axis) and the average number of jargon spans per category (stacked barplot on the right y-axis) in each sentence across both \u201ccomplex\u201d and \u201csimplified\u201d versions for 15 commonly used resources for medical text simplification. Sentences with higher readability scores require a higher level of education to comprehend. The readability of sentences in different resources varies greatly.\\n\\nOur MEDREAD dataset consists of 4,520 sentences with both sentence-level readability ratings and fine-grained complex span-level annotations (Figure 1). It covers complex-simple parallel article pairs from 15 diverse data resources that range from encyclopedias to plain-language summaries to biomedical research publications (Figure 2). The readability ratings are annotated using a rank-and-rate interface (Maddela et al., 2023) based on the CEFR scale (Arase et al., 2022), which is shown to be more reliable than other methods (Naous et al., 2023). We also ask lay annotators to highlight any words/phrases that they find hard to understand and categorize the reason using a 7-class taxonomy. Considering that \u201cthe majority of people seek health information online began at a search engine\u201d, we introduce two categories of \u201cGoogle-Easy\u201d and \u201cGoogle-Hard\u201d to reflect whether jargon is understandable after a quick Google search, providing a fresh perspective beyond binary or 5-point Likert scales.\\n\\nOur new dataset addresses three limitations in prior work: (1) Existing work with sentence-level ratings mainly covers data from general domains, such as Wikipedia (De Clercq and Hoste, 2016), news (Stajner et al., 2017; Brunato et al., 2018), and textbooks for ESL learners (Arase et al., 2022), which are very different from specialized fields, such as medicine (Choi and Pak, 2007). (2) Prior work separates the research on sentence readability and complex jargon terms, hence missing the possible correlations between them (Kwon et al., 2022; Naous et al., 2023). (3) Previous research on sentence readability uses document-level ratings as an approximation, which is shown to be inaccurate (Arase et al., 2022; Cripwell et al., 2023).\\n\\nOur analysis reveals that compared to various linguistic features, complex spans, especially medical jargon from certain domains, more significantly elevate the difficulty of sentences (\u00a73.1). We also scrutinize the quality of 15 widely used medical text simplification resources (\u00a73.4), and find that there are non-negligible variances in readability among them, as shown by the differences in the height of the box plots in Figure 2. While evaluating various sentence readability metrics, we find that unsupervised methods based on lexical features perform poorly in the medical domain. Prompting large language models such as GPT-4 (Achiam et al., 2023) with 5-shot achieves strong performance, yet is outperformed by fine-tuned models in a much smaller size. Inspired by our analysis, we add a single feature that captures the \u201cnumber of jargon\u201d in a sentence into existing readability formulas, and find it can significantly improve their performance and also make them more stable.\\n\\nConstructing MEDREAD Corpus\\n\\nThis section presents the detailed procedure for constructing the Medical Readability Measurement (MEDREAD) corpus, which consists of 4,520 sentences in 180 complex-simple article pairs randomly sampled from 15 data sources (\u00a72.1).\"}"}
{"id": "emnlp-2024-main-958", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.1 Data Collection and Preprocessing\\n\\nDifferent from prior work (Arase et al., 2022; Naous et al., 2023), our study consists of sentences from complete complex-simple article pairs, enabling a deeper analysis of how professional editors simplify medical documents. The 15 resources that we considered include (1) the abstract sections and plain-language summaries from scientific papers, such as the National Institute for Health and Care Research (NIHR) and Cochrane Review of \u201cthe highest standard in evidence-based healthcare\u201d, for which we use the aligned article pairs released from prior studies (Devaraj et al., 2021a; Goldsack et al., 2022; Guo et al., 2022); and (2) segment and paragraph pairs in the parallel versions of medical references from trusted online platforms, such as Merck Manuals and medical-related Wikipedia articles we extracted. A detailed description of each resource and pre-processing steps is provided in Appendix C.\\n\\nTarget Audience.\\n\\nTo ensure our study reflects the background of a broader audience, our study mainly targets people who have completed high school or are entering college, and our dataset is annotated by college students without medical backgrounds using a six-point Likert scale.\\n\\n2.2 Sentence-level Readability Annotation\\n\\nTo collect ground-truth judgments, we hire three university students with prior linguistic annotation experience to annotate the readability ratings for 4,520 sentences. We utilize the \u201crank-and-rate\u201d interface (Naous et al., 2023) and the CEFR scale (Arase et al., 2022), with several improvements.\\n\\nAnnotation Guidelines.\\n\\nFollowing prior work (Arase et al., 2022), we adopt the Common European Framework of Reference for Languages (CEFR) to annotate the sentence readability. CEFR standards were originally created for language learners. Because the scale is essentially a six-point Likert scale, we believe the findings would be mostly generalizable to a broader audience, including native speakers. Another reason for using the CEFR scale is to make our work comparable to the existing work and datasets which were created using the CEFR standards.\\n\\nCEFR Scale.\\n\\nCEFR is the most widely used international criteria to define learners\u2019 language proficiency, assessing language skills on a 6-level scale with detailed guidelines, from beginners (A1) to advanced mastery (C2), which are denoted as level 1 (easiest) to level 6 (hardest) in our interface. Following prior work (Arase et al., 2022; Naous et al., 2023), a sentence\u2019s readability is determined based on the CEFR level, at which an individual can understand the sentence without assistance. As medical texts naturally concentrate on the harder-to-understand side, we introduce the use of \u201c+\u201d and \u201c-\u201d signs to differentiate the nuance in readability, e.g., \u201c3+\u201d and \u201c3-\u201d, in addition to each integer level. They are treated as 3.3 and 2.7 when converting to the numeric scores.\\n\\n2\\nhttps://www.cochranelibrary.com/\\n\\n3\\nhttps://www.merckmanuals.com/\"}"}
{"id": "emnlp-2024-main-958", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Top representative linguistic features and their Pearson correlation with readability.\\n\\n| Feature                                                                 | Corr. |\\n|------------------------------------------------------------------------|-------|\\n| Number of unique sophisticated lexical words                             | 0.645 |\\n| Corrected type-token-ratio (CTTR)                                       | 0.627 |\\n| Number of syllables                                                     | 0.589 |\\n| Max age-of-acquisition (AoA) of words (2012)                            | 0.576 |\\n| Number of unique words                                                 | 0.574 |\\n| Number of words                                                         | 0.532 |\\n| Average number of characters per token                                 | 0.524 |\\n| Corrected noun variation                                                | 0.513 |\\n| The maximum dependency tree depth                                       | 0.437 |\\n| Cumulative Zipf score for all words (2012)                              | 0.425 |\\n\\n\u2020 Sophisticated lexical words (Lu, 2012) are nouns, non-auxiliary verbs, adjectives, and certain adverbs that are not in the 2,000 most frequent lemmatized tokens in the American National Corpus (ANC). More features and more implementation details are provided in the Appendix B.\\n\\nRank-and-Rate Framework.\\nSix sentences are shown together to an annotator, who is instructed to rank them from most to least readable first, then rate each sentence using the 6-point CEFR standard. The interface is shown in Appendix J. Compared to rating each sentence individually, this method enables annotators to compare and contrast sentences within each set, leading to higher annotator agreement (Maddela et al., 2023) and a more engaging user experience (Naous et al., 2023).\\n\\nQuality Control.\\nFor each medical sentence we annotate for the MEDREAD corpus, we sample another (mostly non-medical) sentence with comparable length from the existing READER++ dataset (Naous et al., 2023) as a \u201ccontrol\u201d. Therefore, each set of sentences shown to the annotator consists of three medical sentences and three control sentences whose ratings are known. Annotators are asked to spend at least three minutes on every set, and their annotation quality is monitored through the use of control sentences. The 1,924 sentences in the dev and test sets are double annotated, and the scores are merged by average. The inter-annotator agreement is 0.742 measured by Krippendorff\u2019s alpha (Krippendorff, 2011). On the control sentences, our annotation achieves a Pearson correlation of 0.771 with the original ratings from READER++.\\n\\n2.3 Fine-trained Complex Span Annotation\\nWe propose a new taxonomy to comprehensively capture 7 different categories of complex spans that appeared in the medical texts, as shown in Table 1. The complete annotation guideline with more examples is provided in Appendix L.\\n\\n| Type             | #Spans | #Tokens | %Tokens |\\n|------------------|--------|---------|---------|\\n| Medical Jargon    | 0.644  | 0.591   | 0.445   |\\n| Abbreviation      | 0.259  | 0.254   | 0.134   |\\n| General Complex   | 0.112  | 0.09    | 0.001   |\\n| Multi-sense       | 0.058  | 0.059   | 0.035   |\\n| All Categories    | 0.656  | 0.617   | 0.584   |\\n\\nTable 3: The impact of 15 features related to complex spans, measured by the Pearson correlation with ground-truth sentence readability on the MEDREAD dataset.\\n\\n\u201cGoogle-Hard\u201d Jargon.\\nIn pilot study, we find that some medical terms, such as \u201cTiotropium bromide\u201d (a drug) and \u201cPlasmodium\u201d (an insect), can be grasped after a quick Google search, although they are outside the vocabulary of many people. Some other phrases, such as \u201canti-tumour necrosis factor failure\u201d and \u201cprocessive nucleases\u201d, will require extensive research before a layperson can possibly (or still not) understand them, even though some of them contain short or common words. This seemingly minor distinction can have great implications in developing technological advances for medical text simplification and health literacy, motivating us to propose a novel category \u201cGoogle-Hard\u201d for medical jargon, which is separate from jargon that is \u201cGoogle-Easy\u201d or \u201cName-Entity\u201d. In total, our dataset captures 698 Google-Hard medical jargon and 5,251 Google-Easy ones.\\n\\nAnnotation Agreement.\\nAfter receiving a two-hour training session, two of our in-house annotators independently annotate each of the 4,520 sentences using a web-based annotation tool, BRAT (Stenetorp et al., 2012). The annotation interface is provided in Appendix K. An adjudicator then further inspects the annotation and discusses any significant disagreements with the annotators. The inter-annotator agreement is 0.631 before adjudication, measured by token-level Cohen\u2019s Kappa (Cohen, 1960).\\n\\n3 Key Findings\\nEnabled by our MEDREAD corpus, we first analyze the sentence readability measurements for medical texts (\u00a73.1 and \u00a73.4), then dive into medical jargon of different complexities (\u00a73.2 and \u00a73.3).\\n\\n3.1 Why Medical Texts are Hard-to-Read?\\nThe readability of a sentence can be impacted by a mixture of factors, including sentence length, grammatical complexity, word choice, etc. We extract 650 linguistic features from each sentence and measure their correlation with ground-truth readability.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"15 additional features are designed to quantify the influence of complex spans. Based on our qualitative analysis, we found that complex spans, such as medical jargon, have a more profound impact on readability compared to other linguistic aspects.\\n\\nImpact of linguistic features. For each sentence, 650 linguistic features are extracted, including syntax and semantics features, quantitative and corpus linguistics features, in addition to psycholinguistic features (Vajjala and Meurers, 2016), such as the age of acquisition (AoA) released by Kuperman et al. (2012), and concreteness, meaningfulness, and imageability extracted from the MRC psycholinguistic database (Wilson, 1988). These features are extracted using a combination of toolkits, each of which covers a different subset of features, including LFTK (Lee and Lee, 2023), LingFeat, Profiling\u2013UD (Brunato et al., 2020a), Lexical Complexity Analyzer (Lu, 2012), and L2 Syntactic Complexity Analyzer (Lu, 2010).\\n\\nWe select and present top-10 representative features in Table 2, and provide a more complete list of the top-50 influential features in Appendix B with more detailed definition of each feature. We found that resource-based methods, such as the count of \\\"sophisticated lexical words\\\" (Lu, 2012) and Zipf score (Powers, 1998), are very useful. Length-related features are also informative.\\n\\nImpact of Complex Spans. Based on our pilot study and feedback from annotators, we observed that the specialized terminology, while allowing for precise and concise communication among experts, significantly affects the difficulty level of texts in specialized domains. With our fine-grained span-level annotations (\u00a72.3), we can directly measure the effects that each type of complex words and jargon have on readability. Specifically, we design three features \\\"number-of-jargon-spans\\\", \\\"number-of-jargon-tokens\\\", and \\\"percentage-of-jargon-tokens\\\" for complex span in each category: medical jargon, abbreviation, general complex terms, and multi-sense words. We then compute their correlation with the sentence-level readability ratings. As shown in Table 3, we find that medical jargon significantly affects readability, and abbreviations follow in influence.\\n\\n3.2 What Makes a Jargon Easy (or Hard)?\\n\\nBased on the feedback from annotators, we identify two major factors that influence the perceived difficulty of medical jargon, as listed below:\\n\\nInherent Complexity of Topics. To analyze the perceived difficulty of medical jargon from different domains, we randomly sample 200 Google-Easy and 200 Google-Hard medical jargon, and manually analyze their topics. The results are presented in Figure 4. Google-Easy terms are more\"}"}
{"id": "emnlp-2024-main-958", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Pearson correlation (\u2191) between human ground-truth readability and each unsupervised readability metric.\\n\\nNIHR and PLOS are aggregations of 5 sources for each. All correlations are statistically significant. \\\"-Jar\\\" denotes adding a \\\"number-of-jargon\\\" feature into existing readability formula (more details in \u00a74.2). Our proposed method significantly improves the correlation over existing metrics, as demonstrated by the average correlation.\\n\\nTable 5: The percentage of explanatory content provided by Google. An annotated screenshot of the webpage is provided in Figure 6 in Appendix I to visually demonstrate \\\"Knowledge Panel\\\" and \\\"Feature Snippets\\\", diversified across different topics, while Google-Hard terms mainly fall under Genetics / Cellular Biology and Biology / Molecular Processes. This suggests that jargon associated with genetics or molecular procedures tends to be more challenging to read, possibly due to the specialized knowledge required to interpret them.\\n\\nVariance in the Explanation. We also observed that the accessibility of medical jargon is greatly improved when search engines offer explanations or visual aids in their results. Search engines may provide the explanation of a medical term in two places: (1) the feature snippets in the answer box; and (2) the knowledge panel, which is powered by a knowledge graph. An annotated screenshot of the search results is provided in Figure 6 in Appendix I to demonstrate each element visually. By parsing the Google search results for 2,731 unique Google-Easy and 504 Google-Hard medical jargon from our corpus, we quantified the existence of these explanations in Table 5. The Google-Easy jargon is more frequently accompanied by explanatory content compared to the Google-Hard category. The use of visual aids also follows a similar pattern; Google-Easy terms are much more likely to be explained by figures compared to Google-Hard ones.\\n\\nTable 6: The distribution of operations to 200 medical jargon (100 in each type), based on our manual analysis. 3.3 How Professional Editors Simplify the Medical Jargon? To study how jargon are handled during the manual simplification process, we randomly sample 200 jargon and manually analyze the operation applied to them. The results are presented in Table 6. We find that the majority part of jargon in both categories got deleted. Compared to Google-Easy, \\\"Google-Hard\\\" jargon got copied less, and are being rephrased and explained more often. This findings indicate that trained editors adopt different strategies to handle jargon with different complexities.\\n\\n3.4 Readability Significantly Varies Across Existing Medical Simplification Corpora To better understand the quality of medical text simplification corpora, in Figure 2, we plot the distribution of sentence readability and numbers of jargon per sentence across 15 different resources. Within each source, the simplified texts are rated as easier to understand than their complex counterparts, though the extent varies. However, when compared across the board, simplified texts from some sources can be even more challenging to read than the complex texts from other sources, suggesting that not all plain texts are equally simple. In addition, some resources, such as \\\"PLOS pathogens\\\", are especially difficult for laypersons without domain-specific knowledge to understand. The current research practice in medical text simplification corpora...\"}"}
{"id": "emnlp-2024-main-958", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"plification often treat all data uniformly, such as concatenating all available corpora into one giant training set. However, we argue for a more cautious approach. For some resources, the \\\"simplified\\\" version remains quite complex, and the topics may not be directly relevant to laypersons. Therefore, the decision to include a corpus or not should be made after considering the intended audiences' desired readability level and their use cases.\\n\\n4 Medical Readability Prediction\\n\\nIn this section, we present a comprehensive evaluation of state-of-the-art readability metrics for medical texts (\u00a74.1), and design a simple yet effective method to further improve them (\u00a74.2).\\n\\n4.1 Evaluating Existing Readability Metrics\\n\\nEnabled by our annotated corpus, we first evaluate commonly used sentence readability metrics.\\n\\nUnsupervised Methods.\\n\\nThe Pearson correlations between ground-truth readability and each unsupervised metric are presented in the left half of Table 4. The metrics we considered include FKGL (Kincaid et al., 1975), ARI (Smith and Sen\\nter, 1967), SMOG (McLaughlin, 1969), and RSRS (Martinc et al., 2021), and their detailed formula-\\ntions are provided in Appendix A. We also add sentence length as a baseline. We find that the unsupervised methods generally do not perform very well. The language model-based RSRS score significantly outperforms the traditional feature-based metrics, among which SMOG performs best.\\n\\nSupervised and Prompt-based Methods.\\n\\nThe results are presented in Table 7. For supervised methods, we fine-tune language models on our dataset and existing corpora (Naous et al., 2023; Arase et al., 2022; Brunato et al., 2018) to predict the sentence readability. We also evaluate the performance of in-context learning by prompting large language models such as GPT-4 and Llama-3 (AI@Meta, 2024) using 5-shot. The prompts are constructed following Naous et al. (2023). More details and the full prompt template are in Appendix H. We find that prompt-based methods achieve competitive results, e.g., GPT-4 outperforms the strongest unsupervised metric RSRS, although they still fall behind supervised methods.\\n\\n5 Fine-grained Complex Span Identification\\n\\nBased on our analysis in \u00a74.2, identifying complex spans in a sentence can help the judgment of its readability. It can also improve the performance of downstream text simplification system (Shardlow, 2014). We formulate this task as a NER-style sequential labeling problem (Gooding and Kochmar, 2019), and utilize our annotated dataset to train and evaluate several models.\\n\\nData and Models.\\n\\nThe 4,520 sentences in our corpus is split into 2,587/784/1,140 for train, dev, and test sets. We mainly consider BERT/RoBERTa-based standard tagging models, initialized with diff-\"}"}
{"id": "emnlp-2024-main-958", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sources 5-shots Trained on Each Corpus The Trained + an Jargon Term\\nGPT-4(Achiam et al.) Llama 3-8b(AI@Meta) ReadMe++(Naous et al.) CEFR-SP(Arase et al.) ComDS(Brunato et al.) MEDREREAD ME (Ours)\\n\\nTable 7: Pearson correlation (\u2191) between human ground-truth readability and each prompting and supervised readability metric. All numbers are averaged over five runs, and all correlations are statistically significant.\\n\\ndenotes RoBERTa-large models. \\\"-Jar\\\" means adding a \\\"jargon\\\" term (more details in \u00a74.2). Prompt-based methods are competitive, while still outperformed by fine-tuned models in much smaller sizes.\\n\\nTable 9 presents the performance for binary complex span identification on the MEDREAD ME test set. The best and second-best scores are highlighted. Models are trained with fine-grained labels in seven categories and evaluated at different granularity.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: F1 on the test set of MEDEREAD for models trained on different datasets. \u201cEntity\u201d and \u201cToken\u201d denote binary entity-/token-level performance. \u201c#Sent\u201d is the number of unique sentences in the training set.\\n\\n6 Related Work\\n\\nReadability Measurement in Medical Domain. Unsupervised metrics, such as FKGL (Kincaid et al., 1975), ARI (Smith and Senter, 1967), SMOG (Mc Laughlin, 1969), and Coleman-Liau index (Coleman and Liau, 1975) have been widely adopted in existing research on the medical readability analysis, as they do not require training data (Fu et al., 2016; Chhabra et al., 2018; Xu et al., 2019; Devaraj et al., 2021a; Kruse et al., 2021; Guo et al., 2022; Kaya and G\u00f6rmez, 2022; Hartnett et al., 2023, inter alia). However, their reliability has been questioned (Wilson, 2009; Jindal and MacDermid, 2017; Devaraj et al., 2021b), as they mainly rely on the combination of shallow lexical features. Unsupervised RSRS score (Martinc et al., 2021) utilizes the log probability of words from a pre-trained language model such as BERT (Devlin et al., 2019), while other supervised metrics rely on fine-tuning LLMs on the annotated corpora (Arase et al., 2022; Naous et al., 2023); however, previously, the performance of these methods on the medical texts were unclear. Enabled by our high-quality dataset, we benchmark existing state-of-the-art metrics in the medical domain (\u00a74.1), and also further improve their performances (\u00a74.2).\\n\\nComplex Span Identification in Medical Domain. Kauchak and Leroy (2016) collects a dataset that consists of the difficulty for 275 words. CompLex 2.0 (Shardlow et al., 2020) consists of complex spans rated on a 5-point Likert scale. However, it only covers spans with one or two tokens. MedJEx corpus (Kwon et al., 2022) consists of binary jargon annotation for sentences in the electronic health record (EHR) notes, whereas the dataset is licensed. Other work on complex word identification mainly focuses on general domains, such as news and Wikipedia, and other specialized domains, e.g., computer science. Due to space limits, we list them in Appendix E. Our data is based on open-access medical resources and contains both sentence-level readability ratings and complex span annotation with a finer-grained 7-class categorization (\u00a72).\\n\\n7 Conclusion\\n\\nIn this work, we present a systematic study for sentence readability in the medical domain, featuring a new annotated dataset and a data-driven study to answer \u201cwhy medical sentences are so hard.\u201d In the analysis, we quantitatively measure the impact of several key factors that contribute to the complexity of medical texts, such as the use of jargon, text length, and complex syntactic structures. Future work could extend to the medical notes from clinical settings to better understand real-time communication challenges in healthcare. Additionally, leveraging our dataset that categorizes complex spans by difficulty and type, further research could develop personalized simplification tools to adapt content to the target audience, thereby improving patients\u2019 understanding of medical information.\\n\\nLimitations\\n\\nDue to the reality that major scientific medical discoveries are mostly reported in English, our study primarily focuses on English-language medical texts. Future research could extend to medical resources in other languages. In addition, the focus of our work is to create readability datasets for general purposes following prior work. We did not study or distinguish the fine-grained differences and nuances between native speakers and non-native speakers (Yimam et al., 2017). The readability ratings of a sentence can be impacted by a mixture of factors, including sentence length, grammatical complexity, word difficulty, the annotator\u2019s educational background, the design and quality of annotation guidelines, as well as the target audience. We choose to use the CEFR standards, which is \u201cthe most widely used international standard\u201d to access learners\u2019 language proficiency (Arase et al., 2022). It has detailed guidelines in 34...\"}"}
{"id": "emnlp-2024-main-958", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"languages and have been widely used in many prior research (Boyd et al., 2014; Rysov\u00e1 et al., 2016; Fran\u00e7ois et al., 2016; Xia et al., 2016; Tack et al., 2017; Wilkens et al., 2018; Arase et al., 2022; Naous et al., 2023, inter alia).\\n\\nEthics Statement\\nDuring the data collection process, we hired under-grad students from the U.S. as in-house annotators. All annotators are compensated at $18 per hour or by credit hours based on the university standards.\\n\\nAcknowledgments\\nThe authors would like to thank Mithun Subhash, Jeongrok Yu, and Vishnu Suresh for their help in data annotation. This research is supported in part by the NSF CAREER Award IIS-2144493, NSF Award IIS-2112633, NIH Award R01LM014600, ODNI and IARPA via the HIATUS program (contract 2022-22072200004). The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of NSF, NIH, ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.\\n\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. ArXiv preprint, abs/2303.08774.\\nAI@Meta. 2024. Llama 3 model card.\\nYuki Arase, Satoru Uchida, and Tomoyuki Kajiwara. 2022. CEFR-based sentence difficulty annotation and assessment. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6206\u20136219, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\nTal August, Katharina Reinecke, and Noah A. Smith. 2022. Generating scientific definitions with controllable complexity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8298\u20138317, Dublin, Ireland. Association for Computational Linguistics.\\nAdriane Boyd, Jirka Hana, Lionel Nicolas, Detmar Meurers, Katrin Wisniewski, Andrea Abel, Karin Sch\u00f6ne, Barbora \u0160tindlov\u00e1, and Chiara Vettori. 2014. The MERLIN corpus: Learner language and the CEFR. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pages 1281\u20131288, Reykjavik, Iceland. European Language Resources Association (ELRA).\\nDominique Brunato, Andrea Cimino, Felice Dell'Orletta, Giulia Venturi, and Simonetta Montemagni. 2020a. Profiling-UD: a tool for linguistic profiling of texts. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 7145\u20137151, Marseille, France. European Language Resources Association.\\nDominique Brunato, Andrea Cimino, Felice Dell'Orletta, Giulia Venturi, and Simonetta Montemagni. 2020b. Profiling-UD: a tool for linguistic profiling of texts. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 7145\u20137151, Marseille, France. European Language Resources Association.\\nDominique Brunato, Lorenzo De Mattei, Felice Dell'Orletta, Benedetta Iavarone, and Giulia Venturi. 2018. Is this sentence difficult? do you agree? In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2690\u20132699, Brussels, Belgium. Association for Computational Linguistics.\\nMarc Brysbaert and Andrew Biemiller. 2017. Test-based age-of-acquisition norms for 44 thousand English word meanings. Behavior research methods, 49:1520\u20131523.\\nMarc Brysbaert, Boris New, and Emmanuel Keuleers. 2012. Adding part-of-speech information to the subtlex-us word frequencies. Behavior research methods, 44:991\u2013997.\\nYixin Cao, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan Liu, and Tat-Seng Chua. 2020. Expertise style transfer: A new task towards better communication between experts and laymen. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1061\u20131071, Online. Association for Computational Linguistics.\\nRosy Chhabra, Deena J Chisolm, Barbara Bayldon, Maheen Quadri, Iman Sharif, Jessica J Velazquez, Karen Encalada, Angelic Rivera, Millie Harris, Elana Levites-Agababa, et al. 2018. Evaluation of pediatric human papillomavirus vaccination provider counseling written materials: a health literacy perspective. Academic Pediatrics, 18(2):S28\u2013S36.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-958", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pranay Jindal and Joy C MacDermid. 2017. Assessing reading levels of health information: uses and limitations of Flesch formula. *Education for Health: Change in Learning & Practice*, 30(1).\\n\\nSebastian Joseph, Kathryn Kazanas, Keziah Reina, Vishnesh Ramanathan, Wei Xu, Byron Wallace, and Junyi Jessy Li. 2023. Multilingual simplification of medical texts. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 16662\u201316692, Singapore. Association for Computational Linguistics.\\n\\nDavid Kauchak and Gondy Leroy. 2016. Moving beyond readability metrics for health-related text simplification. *IT professional*, 18(3):45\u201351.\\n\\nErhan Kaya and Sinan G\u00f6rmez. 2022. Quality and readability of online information on plantar fasciitis and calcaneal spur. *Rheumatology International*, 42(11):1965\u20131972.\\n\\nJ Peter Kincaid, Robert P Fishburne Jr, Richard L Rogers, and Brad S Chissom. 1975. Derivation of new readability formulas (Automated Readability Index, Fog Count, and Flesch Reading Ease formula) for Navy Enlisted Personnel. Technical report, Naval Technical Training Command Millington TN Research Branch.\\n\\nKlaus Krippendorff. 2011. Computing Krippendorff\u2019s alpha-reliability.\\n\\nJessica Kruse, Paloma Toledo, Tayler B Belton, Ericka J Testani, Charlesnika T Evans, William A Grobman, Emily S Miller, and Elizabeth MS Lange. 2021. Readability, content, and quality of COVID-19 patient education materials from academic medical centers in the United States. *American Journal of Infection Control*, 49(6):690\u2013693.\\n\\nVictor Kuperman, Hans Stadthagen-Gonzalez, and Marc Brysbaert. 2012. Age-of-acquisition ratings for 30,000 English words. *Behavior Research Methods*, 44:978\u2013990.\\n\\nSunjae Kwon, Zonghai Yao, Harmon Jordan, David Levy, Brian Corner, and Hong Yu. 2022. MedJEx: A medical jargon extraction model with Wiki\u2019s hyperlink span and contextualized masked language model score. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 11733\u201311751, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nBruce W. Lee, Yoo Sung Jang, and Jason Lee. 2021. Pushing on text readability assessment: A transformer meets handcrafted linguistic features. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 10669\u201310686, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nBruce W. Lee and Jason Lee. 2023. LFTK: Handcrafted features in computational linguistics. In *Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)*, pages 1\u201319, Toronto, Canada. Association for Computational Linguistics.\\n\\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. *Bioinformatics*, 36(4):1234\u20131240.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandal Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. *ArXiv preprint*, abs/1907.11692.\\n\\nXiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. *International Journal of Corpus Linguistics*, 15(4):474\u2013496.\\n\\nXiaofei Lu. 2012. The relationship of lexical richness to the quality of ESL learners\u2019 oral narratives. *The Modern Language Journal*, 96(2):190\u2013208.\\n\\nLi Lucy, Jesse Dodge, David Bamman, and Katharine Keith. 2023. Words as gatekeepers: Measuring discipline-specific terms and meanings in scholarly publications. In *Findings of the Association for Computational Linguistics: ACL 2023*, pages 6929\u20136947, Toronto, Canada. Association for Computational Linguistics.\\n\\nMounica Maddela, Yao Dou, David Heineman, and Wei Xu. 2023. LENS: A learnable evaluation metric for text simplification. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 16383\u201316408, Toronto, Canada. Association for Computational Linguistics.\\n\\nMatej Martinc, Senja Pollak, and Marko Robnik-\u0160ikonja. 2021. Supervised and unsupervised neural approaches to text readability. *Computational Linguistics*, 47(1):141\u2013179.\\n\\nG Harry McLaughlin. 1969. Smog grading\u2014a new readability formula. *Journal of Reading*, 12(8):639\u2013646.\\n\\nTarek Naous, Michael J Ryan, Mohit Chandra, and Wei Xu. 2023. Towards massively multi-domain multilingual readability assessment. *ArXiv preprint*, abs/2305.14463.\\n\\nGottfried E Noether. 1981. Why Kendall tau? *Teaching Statistics*, 3(2):41\u201343.\\n\\nGustavo Paetzold and Lucia Specia. 2016. SemEval-2016 task 11: Complex word identification. In *Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)*, pages 560\u2013569, San Diego, California. Association for Computational Linguistics.\\n\\nNikhil Pattisapu, Nishant Prabhu, Smriti Bhati, and Vasudeva Varma. 2020. Leveraging social media for medical text simplification. In *Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2020)*, pages 1729\u20131730, Toronto, Canada. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David M. W. Powers. 1998. Applications and explanations of Zipf's law. In *New Methods in Language Processing and Computational Natural Language Learning*.\\n\\nMichael K Rooney, Gaia Santiago, Subha Perni, David P Horowitz, Anne R McCall, Andrew J Einstein, Reshma Jagsi, and Daniel W Golden. 2021. Readability of patient education materials from high-impact medical journals: a 20-year analysis. *Journal of patient experience*, 8:2374373521998847.\\n\\nKate\u0159ina Rysov\u00e1, Magdal\u00e9na Rysov\u00e1, and Ji\u0159\u00ed M\u00edrovsk\u00fd. 2016. Automatic evaluation of surface coherence in L2 texts in Czech. In *Proceedings of the 28th Conference on Computational Linguistics and Speech Processing (ROCLING 2016)*, pages 214\u2013228, Tainan, Taiwan. The Association for Computational Linguistics and Chinese Language Processing (ACLCLP).\\n\\nMatthew Shardlow. 2013. The CW corpus: A new resource for evaluating the identification of complex words. In *Proceedings of the Second Workshop on Predicting and Improving Text Readability for Target Reader Populations*, pages 69\u201377, Sofia, Bulgaria. Association for Computational Linguistics.\\n\\nMatthew Shardlow. 2014. Out in the open: Finding and categorising errors in the lexical simplification pipeline. In *Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)*, pages 1583\u20131590, Reykjavik, Iceland. European Language Resources Association (ELRA).\\n\\nMatthew Shardlow, Michael Cooper, and Marcos Zampieri. 2020. CompLex \u2014 a new corpus for lexical complexity prediction from Likert Scale data. In *Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding Difficulties (READI)*, pages 57\u201362, Marseille, France. European Language Resources Association.\\n\\nDaniel Simig, Tianlu Wang, Verna Dankers, Peter Henderson, Khuyagbaatar Batsuren, Dieuwke Hupkes, and Mona Diab. 2022. Text characterization toolkit (TCT). In *Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: System Demonstrations*, pages 72\u201387, Taipei, Taiwan. Association for Computational Linguistics.\\n\\nEdgar A Smith and RJ Senter. 1967. *Automated readability index*, volume 66. Aerospace Medical Research Laboratories, Aerospace Medical Division, Air... Sanja Stajner, Simone Paolo Ponzetto, and Heiner Stuckenschmidt. 2017. Automatic assessment of absolute sentence complexity. In *Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017*, pages 4096\u20134102. ijcai.org.\\n\\nPontus Stenetorp, Sampo Pyysalo, Goran T\u043e\u043f\u0438\u30fbc, Tomoko Ohta, Sophia Ananiadou, and Jun\u2019ichi Tsujii. 2012. brat: a web-based tool for NLP-assisted text annotation. In *Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics*, pages 102\u2013107, Avignon, France. Association for Computational Linguistics.\\n\\nJeniya Tabassum, Wei Xu, and Alan Ritter. 2020. WNUT-2020 task 1 overview: Extracting entities and relations from wet lab protocols. In *Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)*, pages 260\u2013267, Online. Association for Computational Linguistics.\\n\\nAna\u00efs Tack, Thomas Fran\u00e7ois, Sophie Roekhaut, and C\u00e9drick Fairon. 2017. Human and automated CEFR-based grading of short answers. In *Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications*, pages 169\u2013179, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nRobert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Fine-tuning large neural language models for biomedical natural language processing.\\n\\nSowmya Vajjala and Detmar Meurers. 2016. Readability-based sentence ranking for evaluating text simplification. ArXiv preprint, abs/1603.06009.\\n\\nRodrigo Wilkens, Leonardo Zilio, and C\u00e9drick Fairon. 2018. SW4ALL: a CEFR classified and aligned corpus for language learning. In *Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)*, Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nMeg Wilson. 2009. Readability and patient education materials used for low-income populations. *Clinical Nurse Specialist*, 23(1):33\u201340.\\n\\nMichael Wilson. 1988. Mrc psycholinguistic database: Machine-usuable dictionary, version 2.00. *Behavior research methods, instruments, & computers*, 20(1):6\u201310.\\n\\nMenglin Xia, Ekaterina Kochmar, and Ted Briscoe. 2016. Text readability assessment for second language learners. In *Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications*, pages 12\u201322, San Diego, CA. Association for Computational Linguistics.\\n\\nWei Xu, Chris Callison-Burch, and Courtney Napoles. 2015. Problems in current text simplification research: New data can help. *Transactions of the Association for Computational Linguistics*, 3:283\u2013297.\\n\\nM. Xue. 2017. Text readability assessment for second language learners. In *Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications*, pages 12\u201322, San Diego, CA. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhan Xu, Lauren Ellis, and Laura R Umphrey. 2019. The easier the better? comparing the readability and engagement of online pro-and anti-vaccination articles. Health education & behavior, 46(5):790\u2013797.\\n\\nSeid Muhie Yimam, Chris Biemann, Shervin Malmasi, Gustavo Paetzold, Lucia Specia, Sanja \u0160tajner, Ana\u00efs Tack, and Marcos Zampieri. 2018. A report on the complex word identification shared task 2018. In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 66\u201378, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nSeid Muhie Yimam, Sanja \u0160tajner, Martin Riedl, and Chris Biemann. 2017. CWIG3G2 - complex word identification task across three text genres and two user groups. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 401\u2013407, Taipei, Taiwan. Asian Federation of Natural Language Processing.\\n\\nQing Zeng, Eunjung Kim, Jon Crowell, and Tony Tse. 2005. A text corpora-based estimation of the familiarity of health terminology. In Biological and Medical Data Analysis: 6th International Symposium, ISB-MDA 2005, Aveiro, Portugal, November 10-11, 2005. Proceedings 6, pages 184\u2013192. Springer.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we list the formulas for four unsupervised readability metrics.\\n\\n**FKGL.** The Flesch-Kincaid Grade Level formula is a well-known readability test designed to indicate how difficult a text in English is to understand. It is calculated using the formula:\\n\\n\\\\[\\nFKGL = 0.39 \\\\left( \\\\frac{\\\\text{total words}}{\\\\text{total sentences}} \\\\right) + 11.8 \\\\left( \\\\frac{\\\\text{total syllables}}{\\\\text{total words}} \\\\right) - 15.59\\n\\\\]\\n\\n**ARI.** The Automated Readability Index (ARI) is another widely used readability metric that estimates the understandability of English text. It is formulated based on characters rather than syllables. The ARI formula is given by:\\n\\n\\\\[\\nARI = 4.71 \\\\left( \\\\frac{\\\\text{total characters}}{\\\\text{total words}} \\\\right) + 0.5 \\\\left( \\\\frac{\\\\text{total words}}{\\\\text{total sentences}} \\\\right) - 21.43\\n\\\\]\\n\\n**SMOG.** The SMOG (Simple Measure of Gobbledygook) Index is a readability formula that measures the years of education needed to understand a piece of writing. SMOG is particularly useful for higher-level texts. The formula is as follows, where the polysyllables are calculated by counting the number of words in a text that have three or more syllables:\\n\\n\\\\[\\nP = \\\\frac{\\\\text{number of polysyllables}}{S}\\n\\\\]\\n\\n\\\\[\\nSMOG = 1.0430 \\\\sqrt{P \\\\times 30} + 3.1291\\n\\\\]\\n\\n**RSRS.** The RSRS (Ranked Sentence Readability Score) leverages log probabilities from a neural language model and the sentence length feature. It's calculated through a weighted sum of individual word losses. Each word's Negative Log Loss (WNLL) is sorted in ascending order and weighted by its rank. The formula assigns higher weights to the out-of-vocabulary (OOV) words, by setting \\\\( \\\\alpha = 2 \\\\) for all OOV words and 1 for others. The formula for RSRS is:\\n\\n\\\\[\\nRSRS = \\\\sum_{S_i=1}^{S} \\\\left[ \\\\sqrt{i} \\\\right] \\\\alpha \\\\cdot \\\\text{WNLL}(i)\\n\\\\]\\n\\nAnd WNLL can be calculated by:\\n\\n\\\\[\\n\\\\text{WNLL} = -\\\\left( y_t \\\\log y_p + (1 - y_t) \\\\log(1 - y_p) \\\\right)\\n\\\\]\\n\\nHere, \\\\( S \\\\) is sentence length, \\\\( y_p \\\\) is the predicted distribution from the language model, and \\\\( y_t \\\\) is the empirical distribution, where 1 for words that appear in the text, and 0 for all others.\\n\\nIn this section, we provide more results on the influence of linguistic features, including syntax and semantics features, quantitative and corpus linguistics features, in addition to psycho-linguistic features (Vajjala and Meurers, 2016), such as the age of acquisition (AoA) released by Kuperman et al. (2012), and concreteness, meaningfulness, and imageability extracted from the MRC psycholinguistic database (Wilson, 1988).\\n\\nThe features are extracted using a combination of toolkits, each of which covers a different subset of features, including 220 features from the LFTK package (Lee and Lee, 2023), 255 from the LingFeat (Lee et al., 2021), 61 from Text Characterization Toolkit (TCT) (Simig et al., 2022), 119 from Profiling\u2013UD (Brunato et al., 2020a), 33 from the Lexical Complexity Analyzer (LCA) (Lu, 2012) and 23 from the L2 Syntactic Complexity Analyzer (L2SCA) (Lu, 2010). The top 50 most influential features are presented in Table B after skipping the duplicated and nearly equivalent ones, e.g., the typo-token-ratio and root-type-token-ratio.\\n\\nFor each of the listed features, we look into the implementation details from the original toolkit and explain them in the \u201cImplementation Details\u201d column. To facilitate reproducibility, we also include the exact feature name used in the original code in the \u201cOriginal Feature Name\u201d column.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Top 50 most influential linguistic features on readability assessment.\\n\\n| Package Original Feature Name | Pearson Correlation | Implementation Details in the Original Toolkit |\\n|------------------------------|---------------------|-----------------------------------------------|\\n| LCA (2012)                   | 0.6120              | Number of all sophisticated lexical words, which are lexical words (i.e., nouns, non-auxiliary verbs, adjectives, and certain adverbs that provide substantive content in the text) and are also \u201csophisticated\u201d (i.e., not in the list of 2,000 most frequent lemmatized tokens in the ANC corpus). |\\n| LFTK (2023)                  | 0.6083              | Corrected type-token-ratio (CTTR), which is calculated as (number-of-unique-tokens / \u221a(2 \u00d7 number-of-all-tokens)), based on the tokens without lemmatization. |\\n| LCA (2012)                   | 0.6037              | Number of different words in the first Z words. Z is computed as the 20th percentile of word counts from a dataset, resulting in a value of 16 in our case. |\\n| LCA (2012)                   | 0.5996              | Number of unique lexical words. Lexical words include nouns, non-auxiliary verbs, adjectives, and certain adverbs that provide substantive content in the text. |\\n| LCA (2012)                   | 0.5961              | Number of different words expected in random Z words over ten trials. Z is computed as the 20th percentile of word counts from a dataset, resulting in a value of 16 in our case. |\\n| LCA (2012)                   | 0.5750              | Number of lexical words. Lexical words include nouns, non-auxiliary verbs, adjectives, and certain adverbs that provide substantive content in the text. |\\n| LCA (2012)                   | 0.5758              | Max age-of-acquisition (AoA) of words. The AoA of each word is defined by Kuperman et al. (2012). |\\n\\nhttps://anc.org/\"}"}
{"id": "emnlp-2024-main-958", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Package       | Original Feature Name | Pearson Correlation | Implementation Details in the Original Toolkit |\\n|--------------|-----------------------|---------------------|-----------------------------------------------|\\n| LFTK (2023)  | t_uword               | 0.5744              | Number of unique words.                       |\\n| LingFeat (2021) | WTopc20_S             | 0.5686              | The count of distinct topics, out of 200 extracted from Wikipedia, that are significantly represented in a text, showing the breadth of topics it covers. |\\n| LFTK (2023)  | t_syll2               | 0.5607              | Number of words that have more than two syllables. |\\n| LingFeat (2021) | BClar20_S             | 0.5598              | Semantic Clarity measured by averaging the differences between the primary topic's probability and that of each subsequent topic, reflecting how prominently a text focuses on its main topic, based on 200 topics extracted from the WeeBit Corpus. |\\n| LingFeat (2021) | to_AAKuW_C            | 0.5379              | Total age-of-acquisition (AoA) of words. The AoA of each word is defined by Kuperman et al. (2012). |\\n| TCT (2022)   | DESWC                 | 0.5323              | Number of words. |\\n| LingFeat (2021) | BClar15_S             | 0.5294              | Semantic Clarity measured by averaging the differences between the primary topic's probability and that of each subsequent topic, reflecting how prominently a text focuses on its main topic, based on 150 topics extracted from the WeeBit Corpus. |\\n| LingFeat (2021) | at_Chara_C            | 0.5237              | Average number of characters per token. |\\n| LFTK (2023)  | corr_noun_var         | 0.5127              | Corrected noun variation, which is computed as \\\\( \\\\frac{\\\\text{number-of-unique-nouns}}{\\\\sqrt{2 \\\\times \\\\text{number-of-all-nouns}}} \\\\) |\\n| LingFeat (2021) | as_AAKuW_C            | 0.5069              | Average age-of-acquisition (AoA) of words. The AoA of each word is defined by Kuperman et al. (2012). |\\n| LFTK (2023)  | t_bry                 | 0.5046              | Total age-of-acquisition (AoA) of words. The AoA of each word is defined by Brysbaert and Biemiller (2017). |\\n| LFTK (2023)  | t_syll3               | 0.5044              | Number of words that have more than three syllables. |\\n| LingFeat (2021) | WTopc15_S             | 0.4956              | The count of distinct topics, out of 150 extracted from Wikipedia, that are significantly represented in a text, showing the breadth of topics it covers. |\\n| LFTK (2023)  | corr_adj_var          | 0.4764              | Corrected adjective variation, which is computed as \\\\( \\\\frac{\\\\text{number-of-unique-adjectives}}{\\\\sqrt{2 \\\\times \\\\text{number-of-all-adjectives}}} \\\\) |\\n| LFTK (2023)  | n_unoun               | 0.4694              | Number of unique nouns. |\\n| LingFeat (2021) | at_Sylla_C            | 0.4691              | Average number of syllables per token. |\\n| LFTK (2023)  | a_bry_ps              | 0.4586              | Average age-of-acquisition (AoA) of words. The AoA of each word is defined by Brysbaert and Biemiller (2017). |\\n| LFTK (2023)  | n_noun                | 0.4581              | Number of nouns. |\\n| LFTK (2023)  | n_adj                 | 0.4497              | Number of adjectives. |\\n| LFTK (2023)  | n_uadj                | 0.4483              | Number of unique adjectives. |\\n| Profiling\u2013UD (2020b) | avg_max_depth        | 0.4371              | The maximum tree depths extracted from a sentence, which is calculated as the longest path (in terms of occurring dependency links) from the root of the dependency tree to some leaf. |\\n| LingFeat (2021) | WNois20_S             | 0.4362              | Semantic noise, which quantifies the dispersion of a text's topics, reflecting how spread out its content is across different subjects. It is calculated by analyzing the text's topic probabilities on 200 topics extracted from through Latent Dirichlet Allocation (LDA). |\\n| LCA (2012)   | ls1                   | 0.4255              | Lexical Sophistication-I, calculated as the ratio of sophisticated lexical tokens to the total number of lexical tokens. |\"}"}
{"id": "emnlp-2024-main-958", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"t_subtlex_us_zipf 0.4253 Cumulative Zipf score for all words, based on frequency data from the SUBTLEX-US corpus (Brysbaert et al., 2012). Zipf scores are a measure of word frequency, with higher scores indicating more common words.\\n\\nWTopc10_S 0.4242 The count of distinct topics, out of 100 extracted from Wikipedia, that are significantly represented in a text, showing the breadth of topics it covers.\\n\\navg_links_len 0.4167 Average number of words occurring linearly between each syntactic head and its dependent (excluding punctuation dependencies).\\n\\nn_adp 0.4144 Number of adpositions.\\n\\nSquaAjV_S 0.4088 Squared Adjective Variation-1, which is calculated as the \\\\((\\\\frac{\\\\text{number-of-unique-adjectives}^2}{\\\\text{number-of-total-adjectives}})\\\\).\\n\\nn_upunct 0.4053 Number of unique punctuations.\\n\\ncorr_adp_var 0.4031 Corrected adposition variation, which is computed as \\\\((\\\\sqrt{\\\\frac{\\\\text{number-of-unique-adpositions}}{2 \\\\times \\\\text{number-of-all-adpositions}}})\\\\).\\n\\nn_uadp 0.4022 Number of unique adpositions.\\n\\ncorr_propn_var 0.3895 Corrected proper noun variation, which is computed as \\\\((\\\\sqrt{\\\\frac{\\\\text{number-of-unique-proper-nouns}}{2 \\\\times \\\\text{number-of-all-proper-nouns}}})\\\\).\\n\\nWClar20_S 0.3879 Semantic Clarity measured by averaging the differences between the primary topic's probability and that of each subsequent topic, reflecting how prominently a text focuses on its main topic, based on 200 topics extracted from Wikipedia Corpus.\\n\\nSquaNoV_S 0.3864 Squared Noun Variation-1, which is calculated as the \\\\((\\\\frac{\\\\text{number-of-unique-nouns}^2}{\\\\text{number-of-total-nouns}})\\\\).\"}"}
{"id": "emnlp-2024-main-958", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Introduction of Medical Text Simplification Resources\\n\\nOur dataset is constructed on top of open-accessed resources. Each of the resources is detailed below.\\n\\nTable 1 presents the basic statistics of 180 sampled article (segment) pairs.\\n\\n**Biomedical Journals.** The latest advancements in the medical field are documented in the research papers. To improve accessibility, the authors or domain experts sometimes write a summary in lay language, providing a valuable resource for studying medical text simplification. We include five sub-journals from NIHR, five sub-journals from PLOS, and the Proceedings of the National Academy of Sciences (PNAS) compiled by (Guo et al., 2022). In addition, we also include the eLife corpus compiled by (Goldsack et al., 2022), which consists of the paper abstracts and summaries in life sciences written by expert editors.\\n\\n**Cochrane Reviews.** As \u201cthe highest standard in evidence-based healthcare\u201d, Cochrane Review provides systematic reviews for the effectiveness of interventions and the quality of diagnostic tests in healthcare and health policy areas, by identifying, appraising, and synthesizing all the empirical evidence that meets pre-specified eligibility criteria. We use the parallel corpus compiled by (Devaraj et al., 2021a).\\n\\n**Medical Wikipedia.** As their original and simplified versions are created independently in a collaboration process, the two versions are on the same topic but may not be entirely aligned (Xu et al., 2015). We apply the state-of-the-art methods (Jiang et al., 2020) to extract aligned paragraph pairs from Wikipedia, of which we improve the quality and quantity over existing work (Pattisapu et al., 2020). Specifically, we first collect 60,838 medical terms using Wikidata\u2019s SPARQL service by querying unique terms that have 30 specific properties, including UMLS code, medical encyclopedia, and the ontologies for disease, symptoms, examination, drug, and therapy. Then, we extract corresponding articles for each term from Wikipedia and simple Wikipedia dumps, based on title matching using WikiExtractor library, resulting in 2,823 aligned article pairs after filtering the empty pages.\\n\\n| Source of the Publication | Avg. #Sent. | Avg. Sent. Len. |\\n|--------------------------|------------|----------------|\\n| Public Library of Science (PLOS) | 8.3 / 8.2 | 28.2 / 26.8 |\\n| Genetics | 10.2 / 6.2 | 28.9 / 30.3 |\\n| Pathogens | 8.9 / 7.2 | 30.7 / 29.5 |\\n| Computational Biology | 9.1 / 7.2 | 29.3 / 27.4 |\\n| Neglected Tropical Diseases | 10.2 / 8.0 | 29.3 / 26.4 |\\n| National Institute for Health and Care Research (NIHR) | | |\\n| Public Health Research | 23.4 / 14.3 | 26.2 / 20.5 |\\n| Health Technology Assessment | 25.1 / 12.9 | 27.3 / 25.7 |\\n| Efficacy and Mechanism Evaluation | 22.6 / 14.9 | 28.2 / 21.4 |\\n| Programme Grants for Applied Research | 27.6 / 14.2 | 27.6 / 22.6 |\\n| Health Services and Delivery Research | 23.2 / 14.1 | 27.9 / 23.2 |\\n| Medical Wikipedia | 5.4 / 5.8 | 23.3 / 19.4 |\\n| Merck Manuals (medical references) | 5.0 / 5.6 | 23.8 / 16.3 |\\n| eLife (biomedicine and life sciences) | 6.5 / 15.6 | 27.0 / 26.3 |\\n| Cochrane Database of Systematic Reviews | 25.4 / 16.1 | 27.3 / 22.2 |\\n| Proc. of National Academy of Sciences | 9.1 / 5.5 | 27.2 / 24.1 |\\n\\nWe use the state-of-the-art neural CRF sentence alignment model (Jiang et al., 2020) with 89.4 F1 on Wikipedia to perform paragraph and sentence alignment for each complex-simple article pair.\\n\\n**Merck Manuals.** We use the segment pairs from prior work (Cao et al., 2020), which are manually aligned by medical experts.\\n\\n### D Implementation Details for Complex Span Identification Models\\n\\nWe use the Huggingface implementations of the BERT and RoBERTa models. We tune the learning rate in \\\\( \\\\{1e^{-6}, 2e^{-6}, 5e^{-6}, 1e^{-5}, 2e^{-5}\\\\} \\\\) based on F1 on the devset, and find \\\\( 2e^{-6} \\\\) works best for our best performing RoBERTa-large model. All models are trained within 1.5 hours on one NVIDIA A40 GPU.\\n\\n### E More Related work on Complex Span Identification in Medical Domain\\n\\nOther work mainly focuses on the general domains such as news and Wikipedia, including CW corpus in SemEval 2016 shared task (Shardlow, 2013; Paetzold and Specia, 2016) and CWIG3G2 corpus in SemEval 2018 (Yimam et al., 2017, 2018). In addition, Guo et al. (2024) collects a jargon dataset from computer science research papers, Lucy et al. (2023) studies the social implications of jargon usage, and August et al. (2022); Huang et al. (2022) focus on the explanation of jargon.\\n\\n### 13 https://github.com/huggingface/transformers\"}"}
{"id": "emnlp-2024-main-958", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 14 presents the results of the exact match at entity level for the complex span identification task on the MREAD test set. As medical jargon and complex spans have diverse formats in the medical articles, it is challenging for the models to predict the exact matched entities.\\n\\n| Models       | Binary | 3-Class | 7-Category |\\n|--------------|--------|---------|------------|\\n| Large-size Models |       |         |            |\\n| BERT (2019)  | 72.0   | 68.2    | 48.5       |\\n| RoBERTa (2019)| 74.9   | 71.2    | 64.1       |\\n| BioBERT (2020)| 72.4   | 67.6    | 60.5       |\\n| PubMedBERT (2021)| 73.4 | 69.9    | 62.2       |\\n| Base-size Models |       |         |            |\\n| BERT (2019)  | 70.7   | 67.0    | 59.3       |\\n| RoBERTa (2019)| 73.5   | 70.0    | 62.4       |\\n| BioBERT (2020)| 70.5   | 67.1    | 59.8       |\\n| PubMedBERT (2021)| 72.2 | 69.0    | 61.2       |\\n\\nThe best and second best scores within each model size are highlighted. Models are trained with fine-grained labels in seven categories and evaluated at different granularity.\\n\\nWe conducted an additional experiment to study how different complex span identification models used in Section 5 affect the performance of medical readability prediction. We find that using predictions from different complex span prediction models leads to similar improvements in readability prediction, with a \u00b10.015 difference in average Pearson correlation across different resources.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prompts for Sentence Readability\\n\\nRate the following sentence on its readability level. The readability is defined as the cognitive load required to understand the meaning of the sentence. Rate the readability on a scale from very easy to very hard. Base your scores on the CEFR scale for L2 learners. You should use the following key:\\n\\n1 = Can understand very short, simple texts a single phrase at a time, picking up familiar names, words and basic phrases and rereading as required.\\n2 = Can understand short, simple texts on familiar matters of a concrete type\\n3 = Can read straightforward factual texts on subjects related to his/her field and interest with a satisfactory level of comprehension.\\n4 = Can read with a large degree of independence, adapting style and speed of reading to different texts and purpose\\n5 = Can understand in detail lengthy, complex texts, whether or not they relate to his/her own area of speciality, provided he/she can reread difficult sections.\\n6 = Can understand and interpret critically virtually all forms of the written language including abstract, structurally complex, or highly colloquial literary and non-literary writings.\\n\\nEXAMPLES:\\nSentence: \u201c[EXAMPLE 1]\u201d\\nGiven the above key, the readability of the sentence is (scale=1-6): [RATING 1]\\nSentence: \u201c[EXAMPLE 2]\u201d\\nGiven the above key, the readability of the sentence is (scale=1-6): [RATING 2]\\nSentence: \u201c[EXAMPLE 3]\u201d\\nGiven the above key, the readability of the sentence is (scale=1-6): [RATING 3]\\nSentence: \u201c[EXAMPLE 4]\u201d\\nGiven the above key, the readability of the sentence is (scale=1-6): [RATING 4]\\nSentence: \u201c[EXAMPLE 5]\u201d\\nGiven the above key, the readability of the sentence is (scale=1-6): [RATING 5]\\nSentence: \u201c[TARGET SENTENCE]\u201d\\nGiven the above key, the readability of the sentence is (scale=1-6): [RATING]\\n\\nTable 15: Following (Naous et al., 2023) in prompt construction, we utilize the same description of the six CEFR levels that were provided to human annotators, along with five examples and their ratings, randomly sampled from the dev set. Then, the model is instructed to evaluate the readability of a given sentence. The full template is presented above.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: An annotated screenshot of search results from Google. Search engines may provide the explanation of a medical term in two places: (1) the featured snippets in the answer box and (2) the knowledge panel on the right-hand side, which is powered by a knowledge graph.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Jean Valjean remained silent, motionless, with his back towards the door, seated on the chair from which he had not stirred, and holding his breath in the dark.\\n\\nThese bead-like structures are called nucleosomes, and interactions between histones in different nucleosomes can link one nucleosome to another, to package the DNA into a very condensed form.\\n\\nIn a sketch or outline drawing, lines drawn often follow the contour of the subject, creating depth by looking like shadows cast from a light in the artist's position.\\n\\nThe long-term functional outcomes of early administration of RDI of amino acids and the use of SMOFlipid, including neurodevelopment, body composition and metabolic health, should be evaluated.\\n\\nAll these initiatives take hold as they do, from lead pipes being removed from schools and homes, to new factories being built in communities with a resurgence of American manufacturing.\\n\\nThe illumination of the subject is also a key element in creating an artistic piece, and the interplay of light and shadow is a valuable method in the artist's toolbox.\\n\\n| Score | Description and Examples |\\n|-------|--------------------------|\\n| 1     | Can understand very short, simple texts a single phrase at a time, picking up familiar names, words and basic phrases and rereading as required. |\\n|       | Example: For breakfast, I had a pancake and drank a glass of milk. |\\n|       | Example: Well, I'm going to pick up Luz from school. |\\n| 2     | Can understand short, simple texts containing the highest frequency vocabulary, including a proportion of shared international vocabulary items. Can understand short, simple texts on familiar matters of a concrete type which consist of high frequency everyday or job-related language. |\\n|       | Example: A man is reading the paper as he talks with someone on the phone. |\\n|       | Example: The majority of car trips in the world today are less than five miles. |\\n| 3     | Can read straightforward factual texts on subjects related to his/her field and interest with a satisfactory level of comprehension. |\\n|       | Example: Every attempt should be made to keep all teammates as closely matched as possible, especially in the sports where strength, speed and size are factors. |\\n| 4     | Can read with a large degree of independence, adapting style and speed of reading to different texts and purposes, and using appropriate reference sources selectively. Has a broad active reading vocabulary, but may experience some difficulty with low-frequency idioms. |\\n|       | Example: Long-term autoimmunity and variants' interactions are huge questions too. |\\n|       | Example: Our aim is to investigate how predictive processing can aid learning of more effective control policies. |\\n| 5     | Can understand in detail lengthy, complex texts, whether or not they relate to his/her own area of speciality, provided he/she can reread difficult sections. |\\n|       | Example: A being who could have hovered over Paris that night with the wing of the bat or the owl would have had beneath his eyes a gloomy spectacles. |\\n|       | Example: There is the Titanism of the Celt, his passionate, turbulent, indomitable reaction against the despotism of fact; and of whom does it remind us so much as of Byron? |\\n| 6     | Can understand a wide range of long and complex texts, appreciating subtle distinctions of style and implicit as well as explicit meaning. Can understand and interpret critically virtually all forms of the written language including abstract, structurally complex, or highly colloquial literary and non-literary writings. |\\n|       | Example: Therefore, he had a repeat colonoscopy on 11-06 which showed expected mucosal signs of moderate ulcerative colitis, no polyps, w/ 8 mm ulcer at junction of distal descending colon and sigmoid colon. |\\n\\nI have read and understood the notes.\\n\\nContinue\"}"}
{"id": "emnlp-2024-main-958", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jean Valjean remained silent, motionless, with his back towards the door, seated on the chair from which he had not stirred, and holding his breath in the dark.\\n\\nThese bead-like structures are called nucleosomes, and interactions between histones in different nucleosomes can link one nucleosome to another, to package the DNA into a very condensed form.\\n\\nIn a sketch or outline drawing, lines drawn often follow the contour of the subject, creating depth by looking like shadows cast from a light in the artist's position.\\n\\nThe long-term functional outcomes of early administration of RDI of amino acids and the use of SMOFlipid, including neurodevelopment, body composition and metabolic health, should be evaluated.\\n\\nAll these initiatives take hold as they do, from lead pipes being removed from schools and homes, to new factories being built in communities with a resurgence of American manufacturing.\\n\\nThe illumination of the subject is also a key element in creating an artistic piece, and the interplay of light and shadow is a valuable method in the artist's toolbox.\\n\\nScore\\n\\n1\\nCan understand very short, simple texts a single phrase at a time, picking up familiar names, words and basic phrases and rereading as required.\\nExample:\\nFor breakfast, I had a pancake and drank a glass of milk.\\nExample:\\nWell, I'm going to pick up Luz from school.\\n\\n2\\nCan understand short, simple texts containing the highest frequency vocabulary, including a proportion of shared international vocabulary items. Can understand short, simple texts on familiar matters of a concrete type which consist of high frequency everyday or job-related language.\\nExample:\\nA man is reading the paper as he talks with someone on the phone.\\nExample:\\nThe majority of car trips in the world today are less than five miles.\\n\\n3\\nCan read straightforward factual texts on subjects related to his/her field and interest with a satisfactory level of comprehension.\\nExample:\\nEvery attempt should be made to keep all teammates as closely matched as possible, especially in the sports where strength, speed and size are factors.\\n\\n4\\nCan read with a large degree of independence, adapting style and speed of reading to different texts and purposes, and using appropriate reference sources selectively. Has a broad active reading vocabulary, but may experience some difficulty with low-frequency idioms.\\nExample:\\nLong-term autoimmunity and variants' interactions are huge questions too.\\nExample:\\nOur aim is to investigate how predictive processing can aid learning of more effective control policies.\\n\\n5\\nCan understand in detail lengthy, complex texts, whether or not they relate to his/her own area of speciality, provided he/she can reread difficult sections.\\nExample:\\nA being who could have hovered over Paris that night with the wing of the bat or the owl would have had beneath his eyes a gloomy spectacles.\\nExample:\\nThere is the Titanism of the Celt, his passionate, turbulent, indomitable reaction against the despotism of fact; and of whom does it remind us so much as of Byron?\\n\\n6\\nCan understand a wide range of long and complex texts, appreciating subtle distinctions of style and implicit as well as explicit meaning. Can understand and interpret critically virtually all forms of the written language including abstract, structurally complex, or highly colloquial literary and non-literary writings.\\nExample:\\nTherefore, he had a repeat colonoscopy on 11-06 which showed expected mucosal signs of moderate ulcerative colitis, no polyps, w/ 8 mm ulcer at junction of distal descending colon and sigmoid colon.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It has long been proposed that much of the information encoding how a protein folds is contained locally in the peptide chain. Here we present a large-scale simulation study designed to examine the extent to which conformations of peptide fragments in water predict native conformations in proteins.\\n\\nWe perform replica exchange molecular dynamics (REMD) simulations of 872 8-mer, 12-mer, and 16-mer peptide fragments from 13 proteins using the AMBER 96 force field and the OBC implicit solvent model. To analyze the simulations, we compute various contact-based metrics, such as contact probability, and then apply Bayesian classifier methods to infer which metastable contacts are likely to be native vs. non-native.\\n\\nWe find that a simple measure, the observed contact probability, is largely more predictive of a peptide\u2019s native structure in the protein than combinations of metrics or multi-body components. Our best classification model is a logistic regression model that can achieve up to 63% correct classifications for 8-mers, 71% for 12-mers, and 76% for 16-mers.\\n\\nWe validate these results on fragments of a protein outside our training set. We conclude that local structure provides information to solve some but not all of the conformational search problem. These results help improve our understanding of folding mechanisms, and have implications for improving physics-based conformational sampling and structure prediction using all-atom molecular simulations.\\n\\nProteins must fold to unique native structures in order to perform their functions. To do this, proteins must solve a complicated conformational search problem, the details of which remain difficult to study experimentally. Predicting folding pathways and the mechanisms by which proteins fold is thus central to understanding how proteins work.\\n\\nOne longstanding question is the extent to which proteins solve the search problem locally, by folding into sub-structures that are dictated primarily by local sequence.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Assumption:\\n\u25cf ... formed: stool that is solid\\n\u25a0 resident: a physician receiving specialized clinical training in a hospital\\n\\nFigure 10: The annotation guideline for complex span identification.\"}"}
{"id": "emnlp-2024-main-958", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here, HR is defined as the hazard ratio.\\n\\nFigure 11: The annotation guideline for complex span identification (continue).\"}"}
