{"id": "acl-2023-long-36", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When Does Translation Require Context?\\nA Data-driven, Multilingual Exploration\\nPatrick Fernandes1,2,3\u2217 Kayo Yin4\u2217 Emmy Liu1 Andr\u00e9 F. T. Martins2,3,5 Graham Neubig1\\n\\n1 Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA\\n2 Instituto Superior T\u00e9cnico & LUMLIS (Lisbon ELLIS Unit), Lisbon, Portugal\\n3 Instituto de Telecomunica\u00e7\u00f5es, Lisbon, Portugal\\n4 University of California, Berkeley\\n5 Unbabel, Lisbon, Portugal\\n\\npfernand@cs.cmu.edu kayoyin@berkeley.edu\\n\\nAbstract\\nAlthough proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations requiring context. We confirm the difficulty of previously studied phenomena while uncovering others that were previously unaddressed. We find that common context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena.\\n\\n1 Introduction\\nIn order to properly translate discourse phenomena including anaphoric pronouns, lexical cohesion, and discourse markers, a machine translation (MT) model must use information from previous utterances (Guillou et al., 2018; L\u00e4ubli et al., 2018; Toral et al., 2018).\\n\\nHowever, while generating proper translations of these phenomena is important for comprehension, they represent a small portion of words in natural language. Therefore, common metrics such as BLEU (Papineni et al., 2002) cannot be used to judge the quality of discourse translation.\\n\\n\u2217 Equal contribution\\n\\n1 Code available at https://github.com/CoderPat/MuDA.\\nSee \u00a7A for example usages of our released toolkit.\\n\\nDataset\\n\\n| Lang. Phenomena | M\u00fcller et al. (2018) | Bawden et al. (2018) | Voita et al. (2018) |\\n|-----------------|----------------------|----------------------|---------------------|\\n| EN\u2192DE           | Pronouns             | Pronouns, Coherence  | Pronouns           |\\n|                 |                       |                      |                     |\\n|                  | M\u00fcller et al. (2019b)| Pronouns, Ellipsis   | Deixis, Ellipsis    |\\n|                  |                       |                      |                     |\\n|                  | Voita et al. (2020)  | Pronouns, Coherence  | Discourse Connectives |\\n|                  |                       |                      |                     |\\n\\nTable 1: Some representative works on contextual machine translation that perform evaluation on discourse phenomena, contrasted to our work. For a more complete review see Maruf et al. (2021).\\n\\nRecent work on neural machine translation (NMT) models that attempt to incorporate extralinguistic context (Tiedemann and Scherrer, 2017; Miculicich et al., 2018; Maruf and Haffari, 2018, inter alia) often perform targeted evaluation of certain discourse phenomena, mostly focusing on ellipsis, formality (Voita et al., 2019b,a), and pronoun translation (M\u00fcller et al., 2018; Bawden et al., 2018; Lopes et al., 2020). However, only a limited set of discourse phenomena for a few language pairs have been studied (see summary in Table 1).\\n\\nThe difficulty of broadening these studies stems from the reliance of previous work on introspection and domain knowledge to identify the relevant discourse phenomena, frequently involving expert speakers, which then requires engineering complex language-specific methods to create test suites or manually designing data for evaluation.\\n\\nIn this paper, we identify sentences that contain discourse phenomena through a data-driven, semi-automatic methodology. We apply this method to create a multilingual benchmark testing discourse phenomena in the domain of MT. First, we develop P-CXMI (\u00a72) as a metric to identify when context is helpful in MT, or more broadly text generation in general. Then, we perform a systematic analysis of words with high P-CXMI to find categories of translation requiring context.\"}"}
{"id": "acl-2023-long-36", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We identify novel discourse phenomena that to our knowledge have not been addressed previously (e.g. consistency of verb forms), without requiring a-priori language-specific knowledge. Finally, we design a series of methods to automatically tag words belonging to the identified classes of ambiguities (\u00a74) and we evaluate existing translation models for different categories of ambiguous translations (\u00a75).\\n\\nWe examine a parallel corpus spanning 14 language pairs, measuring translation ambiguity and model performance. We find that the context-aware methods, while improving on standard evaluation metrics, only perform significantly better than context-agnostic baselines for certain discourse phenomena in our benchmark. Our benchmark provides a more fine-grained evaluation of translation models and reveals weaknesses of context-aware models, such as verb form cohesion. We also find that DeepL, a commercial document-level translation system, does better in our benchmark than its sentence-level ablation and Google Translate. We hope that the released benchmark and code, as well as our findings, will spur targeted evaluation of discourse phenomena in MT to cover more languages and more phenomena in the future.\\n\\n2 Measuring Context Usage\\n\\n2.1 Cross-Mutual Information\\n\\nPast work on contrastive evaluation has examined correct and incorrect translations of specific discourse phenomena (Bawden et al., 2018; M\u00fcller et al., 2018), but this provides only a limited measure of context usage on phenomena defined by the creators of the dataset. We are therefore interested in devising a metric that is able to capture all context usage by a model, beyond a predefined set. Conditional Cross-Mutual Information (CXMI) (Bugliarello et al., 2020; Fernandes et al., 2021) measures the influence of context on model predictions at the corpus level. CXMI is defined as:\\n\\n$$\\\\text{CXMI} (C \\\\rightarrow Y | X) = H(q_{MTA}(Y | X)) - H(q_{MTC}(Y | X, C)),$$\\n\\nwhere $X$ and $Y$ are a source and target sentence, respectively, $C$ is the context, $H(q_{MTA})$ is the entropy of a context-agnostic MT model, and $H(q_{MTC})$ refers to a context-aware MT model. This quantity can be estimated over a held-out set with $N$ sentence pairs and their respective context as:\\n\\n$$\\\\text{CXMI} (C \\\\rightarrow Y | X) \\\\approx -\\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\log q_{MTA}(y(i)|x(i)) / q_{MT}(y(i)|x(i), C(i)),$$\\n\\nImportantly, the authors find that training a single model $q_{MT}$ as both the context-agnostic and context-aware model ensures that non-zero CXMI values are due to context and not other factors (see Fernandes et al. (2021) and \u00a73.1 for details).\\n\\nAlthough this approach is promising, it is defined only at a corpus level: as the previous equation shows, CXMI is estimated by over a full set of sentences. Since we are interested in measuring how important context is for single sentences or words within a sentence, we extend this definition to capture lower-level context dependency in the next section.\\n\\n2.2 Context Usage Per Sentence and Word\\n\\nPointwise Mutual Information (P-MI) (Church and Hanks, 1990) measures the association between two random variables for specific outcomes. Mutual information can be seen as the expected value of P-MI over all possible outcomes of the variables. Taking inspiration from this, we define the Pointwise Cross-Mutual Information (P-CXMI) for a source, target, context triplet $(x,y,C)$ as:\\n\\n$$\\\\text{P-CXMI} (y,x,C) = -\\\\log q_{MTA}(y|x) / q_{MTC}(y|x,C),$$\\n\\nIntuitively, P-CXMI measures how much more (or less) likely a target sentence $y$ is when it is given context $C$, compared to not being given that context. Note that this is estimated according to the models $q_{MTA}$ and $q_{MTC}$ since, just like CXMI, this measure depends on their learned distributions.\\n\\nWe can also apply P-CXMI at the word level to measure how much more likely a particular word in a sentence is when it is given the context, by leveraging the auto-regressive property of the neural decoder. Given the triplet $(x,y,C)$ and the word index $i$, we can measure the P-CXMI for that particular word as:\\n\\n$$\\\\text{P-CXMI} (i,y,x,C) = -\\\\log q_{MTA}(y_i|y_{t<i}, x) / q_{MTC}(y_i|y_{t<i}, x, C).$$\\n\\nNote that nothing constrains the form of $C$ or even $x$ and P-CXMI can, in principle, be applied to any conditional language modelling problem.\"}"}
{"id": "acl-2023-long-36", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Avelile's mother had HIV virus. Avelile had the virus; she was born with the virus.\\n\\nYour daughter? Your niece?\\n\\nRoger. I got'em. Two-Six, this is Two-Six, we're mobile.\\n\\nOur tools today don't look like shovels and picks. They look like the stuff we walk around with.\\n\\nLouis XIV had a lot of people working for him. They made his silly outfits, like this.\\n\\nThey're the ones who know what society is going to be like in another generation. I don't.\\n\\nTable 2: Examples of high P-CXMI tokens and corresponding linguistic phenomena. Contextual sentences are italicized. The high P-CXMI target token is highlighted in pink, source and contextual target tokens related to the high P-CXMI token are highlighted in blue and green respectively.\\n\\nWe use this metric to find words that are strongly context-dependent, which is to say that their likelihood increases greatly with context relative to other words. These words are the ones that likely correspond to discourse phenomena.\\n\\n3 Which Translation Phenomena Benefit from Context?\\n\\nTo identify salient translation phenomena that require context, we perform a thematic analysis (Braun and Clarke, 2006), examining words with high P-CXMI across different language pairs and manually identifying patterns and categorizing them into phenomena where context is useful for translation.\\n\\nTo do so, we systematically examined (1) the mean P-CXMI per part-of-speech (POS) tag, (2) the words with the highest mean P-CXMI across the corpus, and (3) the individual words with the highest P-CXMI in a particular sentence.\\n\\n3.1 Data & Model\\n\\nTo compare linguistic phenomena that arise during document-level translation across language pairs, we use a dataset consisting of TED talks' transcripts and translations (Qi et al., 2018). We use this dataset due to its abundance of discourse phenomena, as well as its availability across many parallel languages. We study translation between English and Arabic, German, Spanish, French, Hebrew, Italian, Japanese, Korean, Dutch, Portuguese, Romanian, Russian, Turkish and Mandarin Chinese. These 14 target languages are chosen for their high availability of TED talks and linguistic tools, as well as for the diversity of language types in our comparative study (Table 4 in Appendix B). For each language pair, our dataset contains 113,711 parallel training sentences from 1,368 talks, 2,678 development sentences from 41 talks, and 3,385 testing sentences from 43 talks.\\n\\nTo obtain the P-CXMI for words in the data, we train a small Transformer (Vaswani et al., 2017) model for every target language and incorporate the target context by concatenating it with the current target sentence (Tiedemann and Scherrer, 2017). We train the model with dynamic context size (Fernandes et al., 2021), by sampling 0-3 target context sentences and estimating P-CXMI by using this model for $q_{MT}$ and $q_{MT C}$ (details in Appendix G).\\n\\n3.2 Analysis Procedure\\n\\nWe start our analysis by studying POS tags with high mean P-CXMI. In Appendix C, we report the mean P-CXMI for selected POS tags on test data. Some types of ambiguity, such as dual form pronouns, can be linked to a single POS tag and be identified at this step, whereas others require finer inspection.\\n\\nNext, we inspect the vocabulary items with high mean P-CXMI. At this step, we can detect phenomena that are reflected by certain lexical items that consistently benefit from context for translation.\\n\\nFinally, we examine individual tokens that obtain the highest P-CXMI. In doing so, we identify patterns that do not depend on lexical features, but rather on syntactic constructions for example. In Table 2, we provide selected examples of tokens that have high P-CXMI and the discourse phenomenon we have identified from them.\\n\\n3.3 Identified Phenomena\\n\\nThrough our thematic analysis of items with high P-CXMI, we identified various types of translation ambiguity. Unlike previous work, our method requires no prior knowledge of languages and easily...\"}"}
{"id": "acl-2023-long-36", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Although this procedure may find phenomena that are intuitive to the annotators, the data-driven approach makes confirmation bias less severe than works relying on introspection. Hence, our procedure can allow us to discover relevant phenomena that have not been previously addressed, such as verb forms. Examples of each phenomenon are given in Table 2.\\n\\n3.3.1 Lexical Cohesion\\n\\nEntities may have multiple possible translations in the target language, but the same entity should be referred to by the same word in a translated document. This is called lexical cohesion.\\n\\n3.3.2 Formality\\n\\nWe identify two phenomena which fall under the general category of formality. First, several languages we examined have a T-V distinction (Appendix B, \u201cPronouns Politeness\u201d) in which the second-person pronouns a speaker uses to refer to someone depend on the relationship between the speaker and the addressee.\\n\\nSecond, languages such as Japanese and Korean use honorifics to indicate formality, which are special titles or words expressing courtesy or respect for position.\\n\\n3.3.3 Pronoun Choice\\n\\nUnlike in English, many languages use gendered pronouns for pronouns other than the third-person singular, or assign gender based on formal rules rather than semantic ones. In order to assign the correct pronoun, it is therefore necessary to use the previous context to distinguish the grammatical gender of the antecedent.\\n\\n3.3.4 Verb Form\\n\\nWhile English verbs may have five forms (e.g. write, writes, wrote, written, writing), other languages may have a more fine-grained verb morphology. For example, English has only a single form for the past tense, while the Spanish past tense consists of six verb forms. Verbs must be translated using the verb form that reflects the tone, mood and cohesion of the document.\\n\\n3.3.5 Ellipsis\\n\\nEllipsis refers to the omission of superfluous words that are able to be inferred from the context. For instance, in the last row of Table 2, the English text does not repeat the verb know in the second sentence as it can be understood from the previous sentence. However, in Turkish, there is no natural way to translate the verb-phrase ellipsis, so context is important for translating the verb correctly.\\n\\n4 Cross-phenomenon MT Evaluation\\n\\nNext, we develop a series of methods to automatically tag tokens belonging to these classes of ambiguous translations and propose the Multilingual Discourse-Aware (MuDA) benchmark for context-aware MT models.\\n\\n4.1 MT Evaluation Framework\\n\\nGiven a pair of parallel source and target documents \\\\((X, Y)\\\\), our MuDA tagger assigns one or more tags from a set of discourse phenomena \\\\(\\\\{t_1, \\\\ldots, t_n\\\\}\\\\) to each target token \\\\(y_i \\\\in Y\\\\). Using the compare-mt toolkit (Neubig et al., 2019), we compute the mean word f-measure of system outputs compared to the reference for each tag. This allows us to identify which discourse phenomena models can translate more or less accurately.\"}"}
{"id": "acl-2023-long-36", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"word alignments from a parallel corpus\\n\\n\\\\[ D = \\\\{(X_1,Y_1),\\\\cdots,(X_{|D|},Y_{|D|})\\\\}, \\\\]\\n\\nwhere \\\\((X_m,Y_m)\\\\) denote the source and target reference document pair. We use the AWESOME aligner (Dou and Neubig, 2021) to obtain:\\n\\n\\\\[ A_m = \\\\{\\\\langle x_i,y_j \\\\rangle | x_i \\\\leftrightarrow y_j, x_i \\\\in X_m, y_j \\\\in Y_m \\\\}, \\\\]\\n\\nwhere each \\\\(x_i\\\\) and \\\\(y_j\\\\) are the lemmatized content source and target words and \\\\(\\\\leftrightarrow\\\\) denotes a bidirectional word alignment. For each target word \\\\(y_j\\\\) that is aligned to source word \\\\(x_i\\\\), if the alignment pair \\\\(\\\\langle x_i,y_j \\\\rangle\\\\) occurred at least 3 times already in the current document, excluding the current sentence, we tag \\\\(y_j\\\\) for lexical cohesion.\\n\\nFormality\\n\\nFor languages with T-V distinction, we tag the target pronouns containing formality distinction if there has previously been a word pertaining to the same formality level in the same document. Some languages such as Spanish often drop the subject pronoun, and T-V distinction is instead reflected in the verb form. For these languages, we use spaCy (Honnibal and Montani, 2017) and Stanza (Qi et al., 2020) to find POS tags and detect verbs with a second-person subject in the source, and conjugated in the second (T) or third (V) person in the target.\\n\\nFor languages with more complex honorifics systems, such as Japanese, we construct a word list of common honorifics-related words to tag (details in Appendix F.3).\\n\\nPronoun Choice\\n\\nTo find pronouns in English that have multiple translations, we manually construct a list \\\\(P_\\\\ell = \\\\{\\\\langle p_s,p_t \\\\rangle\\\\}\\\\) for each language (Appendix F.2), where each \\\\(p_s\\\\) is an English pronoun and \\\\(p_t\\\\) the list of possible translations of \\\\(p_s\\\\) in the language \\\\(\\\\ell\\\\). Then, for each aligned token pair \\\\(\\\\langle x_i,y_j \\\\rangle\\\\), if \\\\(x_i,y_j\\\\) are both pronouns with \\\\(\\\\langle x_i,p_t|y_j \\\\in p_t \\\\rangle \\\\in P_\\\\ell\\\\), and the antecedent of \\\\(x_i\\\\) is not in current sentence, we tag \\\\(y_j\\\\) as an ambiguous pronoun. To obtain antecedents, we use AllenNLP (Gardner et al., 2017)'s coreference resolution module. This procedure is similar to M\u00fcller et al. (2018).\\n\\nVerb Form\\n\\nFor each target language, we define a list \\\\(V_\\\\ell = \\\\{v_1,\\\\cdots,v_k\\\\}\\\\) of verb forms (Appendix F.3) where \\\\(v_i \\\\in V_\\\\ell\\\\) if there exists a verb form in English \\\\(u_j\\\\) and an alternate verb form \\\\(v_k \\\\neq v_i\\\\) in the target language such that an English verb with form \\\\(u_j\\\\) may be translated to a target verb with form \\\\(v_i\\\\) or \\\\(v_k\\\\) depending on the context. Then, for each target token \\\\(y_j\\\\), if \\\\(y_j\\\\) is a verb of form \\\\(v_j \\\\in V_\\\\ell\\\\), and another verb with form \\\\(v_j\\\\) has appeared previously in the same document, we tag \\\\(y_j\\\\) as ambiguous.\\n\\nEllipsis\\n\\nTo detect translation ambiguity due to VP and NP ellipsis, we look for instances where the ellipsis occurs on the source side, but not on the target side, which means that the ellipsis must be resolved during translation. Since existing ellipsis models are limited to specific types of ellipsis, we first train an English (source-side) ellipsis detection model. To do so, we extract an ellipsis dataset from the English data in the Penn Treebank (Marcus et al., 1993) and train a BERT text classification model (Devlin et al., 2019), which achieves 0.77 precision and 0.73 recall (see Appendix F.4 for training details). Then, for each sentence pair where the source sentence is predicted to contain an ellipsis, we tag the word \\\\(y_j\\\\) in the target sentence \\\\(Y_m\\\\) if: (1) \\\\(y_j\\\\) is a verb, noun, proper noun or pronoun; (2) \\\\(y_j\\\\) has occurred in the previous target sentences of the same document; (3) \\\\(y_j\\\\) is not aligned to any source words, that is, \\\\(\\\\nexists x_i \\\\in X_m \\\\text{s.t.} \\\\langle x_i,y_j \\\\rangle \\\\in A_m\\\\).\\n\\n4.3 Evaluation of Automatic Tags\\n\\nWe apply the MuDA tagger to the reference translations of our TED talk data. We thus obtain an evaluation set of 3,385 parallel sentences for each of the 14 language pairs. In Appendix C we report the mean P-CXMI for each language and MuDA tag. Overall, we find higher P-CXMI on tokens with a tag compared to those without, which provides empirical evidence that models indeed rely on context to predict words with MuDA tags.\\n\\nAppendix D shows that the frequency of tags varies significantly across languages. Overall, only 4.5% of the English sentences have been marked for ellipsis, giving an upper bound for the number of ellipsis tags in other languages. We find that languages from a different family than English have a relatively high number of ellipsis tags. We also find that Korean and especially Japanese have more formality tags than languages with T-V distinction, which reflects that register is more often important when translating to languages with honorifics.\\n\\nManual Evaluation\\n\\nTo evaluate our tagger, we asked native speakers with computational linguistics backgrounds to manually verify MuDA tags for 8 languages on 50 randomly selected utterances as well as all words tagged with ellipsis in our corpus. This allows us to measure how many automatic\"}"}
{"id": "acl-2023-long-36", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Precision of MuDA tags on 50 utterances.\\n\\nFor all languages, we obtain high precision for all tags except ellipsis, confirming that the methodology can scale to languages where no native speakers were involved in developing the tags. For ellipsis, false positives often come from one-to-many or non-literal translations, where the aligner does not align all target words to the corresponding source word. We believe that the ellipsis tagger is still useful in selecting difficult examples that require context for translation; despite the low precision, we find a significantly higher P-CXMI on ellipsis words for many languages (Appendix C).\\n\\n4.4 Extension to New Languages\\n\\nWhile MuDA currently supports 14 language pairs, our methodology can be easily extended to new languages. The lexical and ellipsis tags can be directly applied to other languages provided a word aligner between English and the new target language. The formality tag can be extended by adding a list of pronouns or verb forms related to formality in the new language. Similarly, the pronouns and verb forms tag can also be extended by providing a list of ambiguous pronouns and verb forms.\\n\\nExhaustively listing all relevant phenomena in document-level MT is extremely complex and beyond the scope of our paper. To identify new discourse phenomena on other languages, our thematic analysis can be reused as follows: (1) Train a model with dynamic context size on translation between the new language pair; (2) Use the model to compute P-CXMI for words in a parallel document-level corpus of the language pair; (3) Manually analyze the POS tags, vocabulary items and individual tokens with high P-CXMI; (4) Link patterns of tokens with high P-CXMI to particular discourse phenomena by consulting linguistic resources.\\n\\n4. Workers were paid 20$/hour.\\n\\n5. Exploring Context-aware MT\\n\\nOur MuDA tagger can be applied to documents in the supported languages to create benchmarking datasets for discourse phenomena during translation. We use our benchmark of the TED talk dataset enhanced with MuDA tags to perform an exploration of context usage across languages with 4 models, including commercial systems.\\n\\n5.1 Trained Models\\n\\nWe train a sentence-level and document-level concatenation-based small transformer (base) for every target language. While conceptually simple, concatenation approaches have been shown to outperform more complex models when properly trained. For the context-aware model, the major difference from \u00a73.1 is that we use a static context size of 3, since we are not using these models to measure P-CXMI. (Lopes et al., 2020).\\n\\nTo evaluate stronger models, we additionally train a large transformer model (large) that was pretrained on a large, sentence-level corpora, for German, French, Japanese and Chinese. Further details can be found in Appendix G.\\n\\n5.2 Commercial Models\\n\\nTo assess if commercially available machine translation engines are able to leverage context and therefore do well in MuDA, we consider two engines: (1) the Google Cloud Translation v2 API. In early experiments, we assessed that this model only does sentence-level translation, but included it due to its widespread usage; (2) the DeepL v2 API. This model advertises its usage of context as part of translations and our experiments confirm this. Early experimentation with other providers (Amazon and Azure) indicated that these are not context-aware so we refrained from evaluating them.\\n\\nTo obtain provider translations, we feed the documents into an API request. To re-segment the translation into sentences, we include special marker tokens in the source that are preserved during translation and split the translation on those tokens. We also evaluate a sentence-level version of DeepL where we feed each sentence separately to compare with its document-level counterpart.\\n\\n6 translate.google.com, deepl.com. Translations were obtain from version of engines available in April 2021.\"}"}
{"id": "acl-2023-long-36", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Impact of context on BLEU, COMET, and Word f-measure per tag for base context-aware models. BLEU, COMET and word f-measures statistically significantly higher than no-context (p < 0.05) are marked with *. Languages for which the phenomenon doesn\u2019t exist are marked with \u2205. BLEU scores are normalized between [0,1].\\n\\nFigure 3: Impact of context on BLEU, COMET, and Word f-measure per tag for large models. Values that are statistically significantly higher than no-context (p < 0.05) are marked with *. Languages for which the phenomenon doesn\u2019t exist are marked with \u2205. BLEU scores are normalized between [0,1].\\n\\n5.3 Results and Discussion\\n\\nFigure 2 shows results for base models, trained either without (no-context) or with context, and for the latter with either predicted (context) or reference context (context-gold) during decoding. Results are reported with respect to standard MT metrics BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020), as well as the MuDA benchmark. The corpus-level metrics BLEU and COMET are calculated over the entire corpus, rather than just the sentences tagged by MuDA.\\n\\nFirst, we find that BLEU scores are highest for context-gold models for most language pairs, but context-agnostic models have higher COMET scores. Moreover, in terms of mean word f-measure overall, we do not find significant differences between the three systems. It is therefore difficult to see which system performs the best on document-level ambiguities using only corpus-level metrics.\\n\\nFor words tagged by MuDA as requiring context for translation, context-aware models often achieve significantly higher word f-measure than context-agnostic models on certain tags such as ellipsis and formality, but not on other tags such as lexical and verb form. This demonstrates how MuDA allows us to clarify which inter-sentential ambiguities context-aware models are able to resolve.\\n\\nFor the pretrained large models (Figure 3), context-aware models perform better than the context-agnostic on corpus-level metrics, especially COMET. On words tagged with MuDA, context-aware models generally obtain the high-\"}"}
{"id": "acl-2023-long-36", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Scores for commercial models. DeepL (doc) BLEU, COMET and word f-measures statistically significantly higher than DeepL (sent) are marked with *. Languages for which neither DeepL or Google translations are available are marked with \u2205. BLEU scores are normalized between [0,1] est f-measure as well, particularly when given reference context, especially on phenomena such as lexical and pronouns, but improvements are less pronounced than on corpus-level evaluation.\\n\\nAmong commercial engines (Figure 4), DeepL outperforms Google on most metrics and language pairs. The sentence-level ablation of DeepL performs worse than its document-level system for most MuDA tags. Current context-aware MT systems translate some inter-sentential discourse phenomena well, but are unable to consistently obtain significant improvements over context-agnostic counterparts on challenging MuDA data. Tables with all results can be found in Appendix H.\\n\\n6 Related Work\\n\\nSeveral works have worked on measuring the performance of MT models on contextual discourse phenomena. The first example of this was done by Hardmeier et al. (2010), which evaluated automatically the precision and recall of pronoun translation in statistical MT systems. Jwalapuram et al. (2019) proposed evaluating models on pronoun translation based on a pairwise comparison between translations that were generated with and without context, and later Jwalapuram et al. (2020) extended this work to include more languages and phenomena in their automatic evaluation/test set creation. These works rely on prior domain knowledge and intuition to identify context-aware phenomena, whereas we take a systematic, data-driven approach.\\n\\nMost works have focused on evaluating performance in discourse phenomena through the use of contrastive datasets. M\u00fcller et al. (2018) automatically create a dataset for anaphoric pronoun resolution to evaluate MT models in EN \u2192 DE. Bawden et al. (2018) manually creates a dataset for both pronoun resolution and lexical choice in EN \u2192 FR. Voita et al. (2018, 2019b) create a dataset for anaphora resolution, deixis, ellipsis and lexical cohesion in EN \u2192 RU. However, Yin et al. (2021) suggest that translating and disambiguating between two contrastive choices are inherently different, motivating our approach in measuring direct translation performance.\\n\\n7 Conclusions and Future Work\\n\\nWe investigate types of ambiguous translations where MT models benefit from context using our proposed P-CXMI metric. We perform a data-driven thematic analysis across 14 languages to identify context-sensitive discourse phenomena,\"}"}
{"id": "acl-2023-long-36", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"some of which (such as verb forms) have not been previously addressed in work on MT. In comparison to previous work, our approach is systematic, extensible, and does not require prior knowledge of the language. Additionally, the P-CXMI metric can be used to identify other context-dependent words in generation. We construct the MuDA benchmark that tags words in parallel corpora and evaluates models on 5 context-dependent phenomena. Our evaluation reveals that context-aware and commercial translation systems achieve small improvements over context-agnostic models on our benchmark, and we encourage further development of models that improve on context-aware translation.\\n\\nLimitations\\nWhile MuDA relies on set of hand-crafted rules for tagging specific phenomena, these rules might involve the use of other error-prone systems (such as coreference resolution and alignment models) and these errors might be susceptible to problems (such as lack of out-of-domain generalization) that could limit the applicability of our tagger. However, this could be fixed by extending MuDA to use newer and better versions of these systems.\\n\\nThe use of F-1 per tag with surface-form matching between reference/translation can also lead to penalizing translations that use context correctly but choose other equivalent words. Nevertheless, this should also be mitigable by extending the scoring method to, for example, match synonyms.\\n\\nFinally, the benchmarking of context-aware models might not apply to newer, state-of-the-art translation models, especially if these leverage large language models that were trained on long-context data.\\n\\nAcknowledgements\\nWe would like to thank Uri Alon, Ipek Baris, George Bejinariu, Hiba Belkadi, Chlo\u00e9 Billiotte, Giovanni Campagna, Remi Castera, Volkan Cirik, Taisiya Glushkova, Junxian He, Mert Inan, Alina Karakanta, Benno Krojer, Emma Landry, Chanoung Park, Artidoro Pagnoni, Maria Ryskina, Odette Scharenborg, Melanie Sclar, Jenny Seok, Emma Schippers, Bogdan Vasilescu for advice on various languages and help with manual annotations.\\n\\nWe would also like to thank all the members of DeepSPIN and NeuLab who provided feedback on earlier versions of this work. This work was supported by the European Research Council (ERC StG DeepSPIN 758969), by EU\u2019s Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the P2020 program MAIA (LISBOA-01-0247-FEDER-045909), by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (NextGenAI, Center for Responsible AI), and by the Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia through contracts SFRH/BD/150706/2020 and UIDB/50008/2020.\\n\\nReferences\\nLo\u00efc Barrault, Ond\u0159ej Bojar, Marta R. Costa-Juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 1\u201361, Florence, Italy. Association for Computational Linguistics.\\n\\nRachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. Evaluating discourse phenomena in neural machine translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1304\u20131313, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nAnn Bies, Mark Ferguson, Karen Katz, Robert MacIntyre, Victoria Tredinnick, Grace Kim, Mary Ann Marcinkiewicz, and Britta Schasberger. 1995. Bracketing guidelines for treebank ii style penn treebank project. University of Pennsylvania, 97:100.\\n\\nVirginia Braun and Victoria Clarke. 2006. Using thematical analysis in psychology. Qualitative research in psychology, 3(2):77\u2013101.\\n\\nEmanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, and Naoaki Okazaki. 2020. It\u2019s easier to translate out of English than into it: Measuring neural translation difficulty by cross-mutual information. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1640\u20131649, Online. Association for Computational Linguistics.\\n\\nMauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web inventory of transcribed and translated talks. In Proceedings of the 16th Annual conference of the European Association for Machine Translation, pages 261\u2013268, Trento, Italy. European Association for Machine Translation.\\n\\nKenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22\u201329.\"}"}
{"id": "acl-2023-long-36", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-36", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-36", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language  | Family     | Word Order | Pronouns | Politeness | Gendered Pronouns | Gender Assignment |\\n|-----------|------------|------------|----------|------------|-------------------|------------------|\\n| Arabic    | Afro-Asiatic | VSO        | None     | 1 and/or 2 and 3 | Semantic-Formal   |                  |\\n| English   | Indo-European | SVO        | None     | 3.Sing     | Semantic         |                  |\\n| German    | Indo-European | SOV/SVO    | Binary   | 3.Sing     | Semantic-Formal   | Binary           |\\n| Spanish   | Indo-European | SVO        | Binary   | 1 and/or 2 and 3 | Semantic-Formal   |                  |\\n| French    | Indo-European | SVO        | Binary   | 3.Sing     | Semantic-Formal   |                  |\\n| Hebrew    | Afro-Asiatic | SVO        | None     | 1 and/or 2 and 3 | Semantic-Formal   |                  |\\n| Italian   | Indo-European | SVO        | Binary   | 3.Sing     | Semantic-Formal   |                  |\\n| Japanese  | Japonic    | SOV        | Avoided  | 3 None     |                   |                  |\\n| Korean    | Koreanic   | SOV        | Avoided  | 3.Sing     | None              |                  |\\n| Dutch     | Indo-European | SOV/SVO    | Binary   | 3.Sing     | Semantic-Formal   | Binary           |\\n| Portuguese| Indo-European | SVO        | Binary   | 3.Sing     | Semantic-Formal   |                  |\\n| Romanian  | Indo-European | SVO        | Multiple | 3.Sing     | Semantic-Formal   |                  |\\n| Russian   | Indo-European | SVO        | Binary   | 3.Sing     | Semantic-Formal   |                  |\\n| Turkish   | Turkic     | SOV        | Binary   | None       | None              | None             |\\n| Mandarin  | Sino-Tibetan | SVO        | Binary   | 3.Sing     | None              |                  |\\n\\nTable 4: Properties of the languages in our study.\\n\\nB Language Properties\\n\\nTable 4 summarizes the properties of the languages analyzed in this work.\\n\\nC P-CXMI Results\\n\\nTable 5 presents the average P-CXMI value per POS tag and per MuDA tag.\\n\\nD Tag Numbers\\n\\nTable 6 lists the counts of each tag per language.\\n\\nE Tagging other Document-level Datasets\\n\\nWe report the number of tags found for two other document-level datasets commonly used in the literature: (1) IWSLT-17 (Cettolo et al., 2012) test sets for EN \u2192 DE and EN \u2192 FR and (2) A randomly subsampled portion of the news-commentary dataset for EN \u2192 {AR, DE, ES, FR, NL, PT, RU, ZH} (Barrault et al., 2019). These results can be found respectively in Figure 5 and Figure 6.\"}"}
{"id": "acl-2023-long-36", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: Number of tags for EN \u2192 DE and EN \u2192 FR in the IWSLT17 dataset. Lexical cohesion and verb form are common phenomena in this dataset.\\n\\nFigure 6: Number of tags across languages in the news-commentary dataset. Lexical cohesion and verb form are common phenomena in this dataset.\"}"}
{"id": "acl-2023-long-36", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Tag      | ADJ     | ADP     | ADV     | AUX     | CCONJ   | DET     | INTJ    | NOUN    | NUM     | PART    | PRON    | PRON.1  | PRON.1.Plur | PRON.1.Sing | PRON.2  | PRON.2.Plur | PRON.2.Sing | PRON.3  | PRON.3.Dual | PRON.3.Plur | PRON.3.Sing | PRON.Plur | PRON.Sing | PROPN    | PUNCT    | SCONJ    | SYM   | VERB     | VERB.Fut | VERB.Imp | VERB.Past | VERB.Pres | ellipsis | formality | lexical | no tag | pronouns | verb form | with tag |\\n|----------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|---------|------------|------------|---------|-------------|-------------|---------|-----------|-----------|----------|-----------|----------|---------|----------|---------|---------|---------|---------|---------|\\n|          | 0.017   | 0.017   | 0.038   | 0.053   | 0.044   | 0.006   | -0.066  | 0.012   | 0.011   | 0.006   | -0.001  | 0.019   | 0.027     | 0.029       | 0.009   | 0.009       | 0.017       | 0.018   | 0.016     | 0.016     | 0.014    | 0.047     | 0.009    | 0.007   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | -0.014  | -0.001  | 0.013   | 0.010   | 0.025   | 0.008   | 0.014   | 0.021   | 0.002   | 0.013   | -0.003  | 0.025   | 0.037     | 0.038       | 0.018   | 0.029       | 0.032       | 0.039   | 0.057     | 0.023     | 0.014    | 0.045     | 0.006    | 0.007   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.000   | 0.004   | 0.008   | -0.001  | 0.001   | 0.011   | 0.003   | 0.007   | 0.011   | 0.005   | -0.006  | 0.003   | 0.008     | 0.015       | 0.016   | 0.013       | 0.015       | 0.014   | 0.015     | 0.016     | 0.023    | 0.007     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | -0.037  | -0.008  | 0.001   | 0.000   | 0.001   | 0.005   | 0.002   | 0.007   | 0.011   | 0.005   | -0.013  | 0.006   | 0.007     | 0.005       | 0.006   | 0.007       | 0.006       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   | 0.002   | 0.007   | 0.001   | 0.007   | 0.006   | 0.002   | 0.002   | 0.005     | 0.002       | 0.005   | 0.007       | 0.005       | 0.010   | 0.004     | 0.009     | 0.007    | 0.006     | 0.017    | 0.005   | -0.002   | 0.003   | 0.034    | 0.002   | 0.009    |\\n|          | 0.001   | 0.005   | 0.005   | 0.005   | 0.001   |"}
{"id": "acl-2023-long-36", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Total number of MuDA tags on TED test data. \u20180\u2019 indicates that the phenomenon does not apply to that language.\\n\\nTo address the imbalance in labels, we up-weight the loss for samples tagged as ellipsis by a factor of 100.\\n\\nThe transformer-small model has hidden size of 512, feedforward size of 1024, 6 layers and 8 attention heads. The transformer-large model has hidden size of 1024, feedforward size of 4096, 6 layers, 16 attention heads. As in Vaswani et al. (2017), we train using the Adam optimizer with $\\\\beta_1 = 0.9$ and $\\\\beta_2 = 0.98$ and use an inverse square root learning rate scheduler, with an initial value of $10^{-4}$ for large model and $5 \\\\times 10^{-4}$ for the base and multi models, with a linear warm-up in the first 4000 steps.\\n\\nFor the pretrained models we used Paracrawl (Espl\u00e0 et al., 2019) for German and French, JParacrawl (Morishita et al., 2020) for Japanese and the Backtranslated News from WMT2021 for Chinese.\\n\\nDue to the sheer number of experiments, we use a single seed per experiment. We base our experiments on the framework Fairseq (Ott et al., 2019).\"}"}
{"id": "acl-2023-long-36", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Words related to formality for each target language.\\n\\n| Language | Words |\\n|----------|-------|\\n| de | du | sie |\\n| es | t\u00fa, tu, tus, ti, contigo, tuyo, te, tuya |\\n| usted, vosotros, vuestro, vuestra, vuestras, os |\\n| fr | tu, ton, ta, tes, toi, te, tien, tiens, tienne, tiennes |\\n| vous, votre, vos |\\n| it | tu, tuo, tua, tuoi |\\n| lei, suo, sua, suoi |\\n| ja | \u3060, \u3060\u3063, \u3058\u3083, \u3060\u308d\u3046, \u3060, \u3060\u3051\u3069 |\\n| \u3054\u3056\u3044, \u307e\u3059, \u3044\u3089\u3063\u3057\u3083\u308c, \u3044\u3089\u3063\u3057\u3083\u3044, \u3054, \u89a7, \u4f3a, \u4f3a\u3063, \u5b58, \u3067\u3059, \u307e\u3059 |\\n| ko | \u1166\u1100 \u1161, \u1165\u1112 \u1174, \u1161\u1103 \u1161, \u1161\u110b \u1166, \u1109 \u1166, \u1160\u110c \u116e, \u1161\u110c \u1166, \u1161\u110e \u1161, \u1161\u110b \u1163 \u11a8, \u1161\u110c \u116e |\\n| nl | jij, jouw, jou, jullie, je |\\n| u, men, uw |\\n| pt | tu, tua, teu, teus, tuas, te |\\n| voc\u00ea, sua, seu, seus, suas, lhe |\\n| ro | tu, el, ea, voi, ei, ele, t \u02d8au, ta, tale, tine |\\n| dumneavoastr \u02d8a, dumneata, mata, matale, d\u00e2nsul, d\u00e2nsa dumnealui, dumneaei, dumnealor |\\n| ru | \u0442\u044b, \u0442\u0435\u0431\u044f, \u0442\u0435\u0431\u0435, \u0442\u043e\u0431\u043e\u0439, \u0442\u0432\u043e\u0439, \u0442\u0432\u043e\u044f, \u0442\u0432\u043e\u0438 |\\n| \u0432\u044b, \u0432\u0430\u0441, \u0432\u0430\u043c, \u0432\u0430\u043c\u0438, \u0432\u0430\u0448, \u0432\u0430\u0448\u0438 |\\n| tr | sen, senin |\\n| siz, sizin |\\n| zh | \u4f60, \u60a8 |\\n\\n621\"}"}
{"id": "acl-2023-long-36", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Ambiguous pronouns w.r.t. English for each target language.\"}"}
{"id": "acl-2023-long-36", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | Imperfect | Pluperfect | Future | Past | Future, Pluperfect |\\n|----------|-----------|------------|--------|------|-------------------|\\n| ar       | no-context | BLEU       | 17.25  | 28.02| 35.72             |\\n| de       |            |            |        |      |                   |\\n| es       |            |            |        |      |                   |\\n| fr       | no-context | BLEU       | 37.74  | 32.70| 7.10              |\\n| he       |            |            |        |      |                   |\\n| it       | no-context | BLEU       | 32.30  | 32.30|                  |\\n| ja       |            |            |        |      |                   |\\n| ko       |            |            |        |      |                   |\\n| nl       |            | BLEU       | 32.22  | 39.03| 25.36             |\\n| pt       |            |            |        |      |                   |\\n| ro       | no-context | BLEU       | 39.10  | 25.37| 17.00             |\\n| ru       |            |            |        |      |                   |\\n| tr       |            |            |        |      |                   |\\n| zh       |            |            |        |      |                   |\\n\\nTable 9: Ambiguous verb forms w.r.t. English for each target language.\\n\\nTable 10: BLEU, COMET, and Word f-measure per tag for base context-aware models. BLEU, COMET and word f-measures statistically significantly higher than no-context (p < 0.05) are underlined.\"}"}
{"id": "acl-2023-long-36", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 11: Word f-measure per tag for large models.\\n\\nBLEU, COMET, word f-measures statistically significantly higher than no-context ($p < 0.05$) are underlined.\\n\\n| Tag         | BLEU | COMET | Lexical | Pronouns | Verb Tense |\\n|-------------|------|-------|---------|----------|------------|\\n| no-context  |      |       |         |          |            |\\n| context     |      |       |         |          |            |\\n| context-gold|      |       |         |          |            |\\n\\n| Model       | BLEU | COMET | Lexical | Pronouns | Verb Tense |\\n|-------------|------|-------|---------|----------|------------|\\n| DeepL (sent)|      |       |         |          |            |\\n| DeepL (doc) |      |       |         |          |            |\\n\\n### Table 12: Scores for commercial models.\\n\\nDeepL (doc) BLEU, COMET and word f-measures statistically significantly higher than DeepL (sent) are underlined.\"}"}
{"id": "acl-2023-long-36", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nB \u25a1 Did you use or create scientific artifacts?\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC \u25a1 Did you run computational experiments?\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-36", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? \\nSection 5.1 and Appendix F\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? \\nSection 5\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? \\nSection 4 and 5\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants? \\nSection 4.3\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? \\nWe provide a brief description (but not full text instructions) in section 4.3\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? \\nSection 4.3\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? \\nNot applicable. Left blank.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? \\nNot applicable. Left blank.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? \\nSection 4.3\"}"}
