{"id": "emnlp-2023-main-1021", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bai Xuemei entered the room: What do you want from me? We only have two teachers here. Teacher Cao has gone home due to some personal reasons. Drink water.\\n\\nThe leader saw five people lying on the high wall in front of the deck, and the laser was shot from the guns in their hands. Don't shoot. Who are you?\\n\\nMan: He's been preparing for this exam for a long time, but he didn't expect to pass it. (sigh)\\n\\nWoman: Really? What a pity. I don't know why. (sigh)\\n\\nWoman: I met Liu Xiaoru on the street today. (said with a smile)\\n\\nMan: Really? You haven't seen each other for ten years, right? (surprised)\\n\\nWoman: Yeah, but she's still as young and beautiful as she was in college. (nod)\\n\\nMan: Please invite her to sit at home on weekends. (said with a smile)\\n\\nWoman: You see, this dress is very nice, of good quality and not expensive. (said with a smile)\\n\\nMan: Maybe I will check others. (said with a smile)\\n\\nMan: You are sure you don't want this one?\\n\\nWoman: The cloth is great.\\n\\nMan: The cloth is very nice, of good quality and not expensive.\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | No. of Epochs | Learning Rate | Batch Size | MaxLen | TSLen |\\n|------------------------|---------------|---------------|------------|--------|-------|\\n| NM extraction          |               |               |            |        |       |\\n| RoBERTa-wwm-ext-large  | 5             | 3e-5          | 32         | 512    | \u2013     |\\n| MacBERT large          | 5             | 3e-5          | 32         | 512    | \u2013     |\\n| T5                     | 5             | 3e-4          | 64         | 512    | 20    |\\n| BART large             | 5             | 2e-5          | 64         | 512    | 20    |\\n| DialBART large         | 5             | 2e-5          | 64         | 512    | 20    |\\n| NM generation          |               |               |            |        |       |\\n| T5                     | 1             | 3e-4          | 64         | 512    | 8     |\\n| BART large             | 1             | 2e-5          | 64         | 512    | 8     |\\n| DialBART large         | 1             | 2e-5          | 64         | 512    | 8     |\\n\\nTable 18: Hyper-parameters settings for different fine-tuning tasks (LR: learning rate; TSLen: target sequence length).\\n\\nWe implement DIST-1 and DIST-2 \u2014 the number of distinct unigrams and bigrams divided by the total number of generated characters \u2014 following (Li et al., 2016) to evaluate the diversity of the generated text.\\n\\nWe use a public Python library for computing ROUGE scores.\\n\\nTable 19: Descriptions about the Chinese backbone models (E/G: extraction/generation).\\n\\nhttps://github.com/pltrdy/rouge.\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nNonverbal messages (NM) such as speakers' facial expressions and speed of speech are essential for face-to-face communication, and they can be regarded as implicit knowledge as they are usually not included in existing dialogue understanding or generation tasks. This paper introduces the task of extracting NMs in written text and generating NMs for spoken text. Previous studies merely focus on extracting NMs from relatively small-scale well-structured corpora such as movie scripts wherein NMs are enclosed in parentheses by scriptwriters, which greatly decreases the difficulty of extraction. To enable extracting NMs from unstructured corpora, we annotate the first NM extraction dataset for Chinese based on novels and develop three baselines to extract single-span or multi-span NM of a target utterance from its surrounding context. Furthermore, we use the extractors to extract 749K (context, utterance, NM) triples from Chinese novels and investigate whether we can use them to improve NM generation via semi-supervised learning. Experimental results demonstrate that the automatically extracted triples can serve as high-quality augmentation data of clean triples extracted from scripts to generate more relevant, fluent, valid, and factually consistent NMs than the purely supervised generator, and the resulting generator can in turn help Chinese dialogue understanding tasks such as dialogue machine reading comprehension and emotion classification by simply adding the predicted \u201cunspoken\u201d NM to each utterance or narrative in inputs.\\n\\n1 Introduction\\n\\nNonverbal messages (NM), such as facial expressions, body movements, and tones of voice, can complement or modify verbal messages as well as improve the teamwork efficiency (Breazeal et al., 2005) and effectiveness of face-to-face communication (Phutela, 2015). These messages are usually not explicitly mentioned in the transcribed verbal messages as the dialogue participants share most of the NMs via other modalities, and NMs are also seldom included in existing text-based dialogue tasks that mainly focus on verbal messages (Csaky and Recski, 2021). Though human readers can infer missing NMs based on their own knowledge, machines still have difficulty understanding the meanings behind and beyond the words (Zhang et al., 2018) and automatically decide what nonverbal behaviors they should display in interactions (Saudander and Nejat, 2019). We focus on text-based NM extraction and generation, an important step towards reaching the ultimate goal of bridging the human-machine implicit knowledge gap.\\n\\nOne of the most relevant text resources for nonverbal messages is TV and movie scripts. Generally, scripts are written in a standard format: for example, NMs of their corresponding utterances are enclosed in parentheses (e.g., ELIZABETH (ironically) \u201cWith five thousand a year, it would not matter if he had a big pink face.\u201d and MR. DARCY (shakes his head) \u201cYou know how I test it.\u201d), which usually describe what can be seen or heard by the audience beyond the verbal messages. Based on the well-defined screenplay structures, it is relatively easy to use heuristics to extract utterances and their NMs from scripts (Vassiliou, 2006). However, in scripts, only a small percentage (\u2248 10.5% (Section 6.2)) of utterances are followed by NMs, and existing public script corpora are usually small-scale even for resource-rich English (e.g., 1,276 movies (Gorinski and Lapata, 2015) and 917 movies (Gorinski and Lapata, 2018)).\\n\\nIn contrast, novels also contain rich NMs via the words of the writers alongside what their characters speak, and thousands of novels have already been adapted into scripts (mainly by professional scriptwriters). Besides, we observe that the density of NMs in novels is higher than that of scripts (\u2248 67.4% based on the annotated corpus (Section 6.2)).\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tion 3.2)), indicating the potential of leveraging novels for NM extraction. Therefore, we are interested in whether we can use this unstructured resource to alleviate the NM data scarcity problem, which hinders the full utilization of deep neural models. As this direction is still unexplored, we first define the task as extracting one or multiple spans from the surrounding context of the target utterance and annotate NME, the first Nonverbal Message Extraction dataset based on three Chinese novels (Jia et al., 2021) containing 4K (context, utterance, NM) triples. Furthermore, we design three baselines (pattern, extractive, and generative) to extract NMs and evaluate them on NME.\\n\\nAnother question is whether we can leverage unlabeled novel corpora to automatically construct data for improving NM generation. To investigate this question, we first use the trained extractors to extract 749K pseudo-labeled triples from several hundreds of Chinese novels and train generators based on different backbone models to generate a nonverbal message given one target utterance and its context. Experiments show that these triples can serve as high-quality augmentation data of clean triples extracted from well-structured scripts to generate more relevant, fluent, valid, and factually consistent NMs. Furthermore, our semi-supervised generators can in turn help Chinese dialogue and narrative understanding tasks that lack NMs such as the dialogue subset of a machine reading comprehension dataset C3 (Sun et al., 2020) and emotion classification EWECT by simply adding the generated \u201cunspoken\u201d NMs to each utterance or narrative in inputs, showing their usefulness.\\n\\nThe contributions of this paper are as follows.\\n\\n\u2022 We design and annotate the first NM extraction dataset based on unstructured corpora.\\n\u2022 We design several strong nonverbal message extraction and generation baselines upon different backbone models to serve as a foundation for further work.\\n\u2022 We extract large-scale (context, utterance, NM) data from unlabeled unstructured corpora using the NM extractors and demonstrate the usefulness of the data for improving the performance of NM generation.\\n\u2022 Experimental results show that NM generators can in turn help dialogue understanding tasks.\\n\\nWe will release our code, annotated data, guidelines, parsed scripts, and models fine-tuned on the novel data at https://github.com/yudiandoris/nm.\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overview of our framework (NM: nonverbal message. The supervised extractor is trained on NME).\\n\\nTable 1: Example in the annotated nonverbal message extraction dataset NME (DOC: document. TU: target utterance. NM: nonverbal message). We construct unannotated NM data based on a public speaker identification dataset JY (Jia et al., 2021) in which three novels are involved. We simply use the annotated target utterance, its labeled speaker, and ten-sentence context (five sentences before/after the utterance) without any modifications. As \u201cspeaker: utterance\u201d are regarded as two sentences separated by \u201c:\u201d in the JY dataset, the actual number of sentences in the context should be smaller than ten. Each triple (example in Table 1) is annotated by two annotators, and all annotators are Chinese native speakers. On average, each triple costs 0.30 RMB ($0.04) (more discussions in Ethical Considerations). For inter-annotation agreement (IAA) computation, when there are multiple annotated spans, we concatenate them into one span. IAA for span annotation measured by exact span match (Wang et al., 2021) is 83.0% and 92.0% by F1 (Hripcsak and Rothschild, 2005). The authors check and re-annotate the triples with different annotations for the final version.\\n\\n3.2 Data Statistics and Analysis\\n\\nThe writers of novels tend to omit the NM when the utterance or the context is self-explanatory. Besides, triples with relatively uninformative NMs are discarded. Therefore, we only keep 67.4% of the annotated utterances, and each triple corresponds to one or multiple non-empty nonverbal messages (see discussions in Limitations). We report the data statistics in Table 2.\\n\\nTable 2: NME Statistics (\u22c6: exclude target utterance).\\n\\n| Metric Value                          | 3,791 | 341 |\\n|---------------------------------------|-------|-----|\\n| Number of training triples            |       |     |\\n| Number of development triples         |       |     |\\n| Average/max length (in characters)    | \u22c6148 / 490 | |\\n| Average/max length (in characters)    | 4.7 / 33 |     |\\n\\nTo examine the fine-grained types of NMs, we review the literature, analyze the annotated spans, and finally categorize them into thirteen sub-types (Table 3). See detailed definitions for each type and more examples in Appendix A.2. An NM may belong to multiple sub-types. For example, \u201c\u966a\u7b11\u201d (\u201cput up a smiling face in order to please or plac...\u201d\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cate somebody\u201d indicate both a facial expression and an intention, and \u201c\u5747\u60f3\u201d (\u201dboth think\u201d) shows both the number of speakers and the addressee. As shown in Figure 2, the two most frequent types are VOCAL-RELATED (e.g., pitch, volume, and speed) and KINESICS (i.e., body movement and facial expression), which are more expressive than other types such as INTENTION and therefore more likely to support downstream applications in other modalities such as speech and vision.\\n\\nFigure 2: Distribution of nonverbal message types.\\n\\n4 Nonverbal Message Extraction\\n\\nThis section introduces three NM extraction baselines (pattern-based, extractive, and generative) and how to use the extractors and unstructured corpora to construct large-scale pseudo-labeled NM data.\\n\\n4.1 Pattern-Based Method\\n\\nBased on the annotation guideline (Section 3.1), we can remove from considerations speaker names and utterances when we extract NMs. As the writers\u2019 own observations that may contain NMs take place alternating with utterances of characters (Poyatos, 1977), we assume that an NM is very likely to appear in the same paragraph as the given target utterance. We first run a strong (\u223c90% in F1) extractive speaker identification model (Yu et al., 2022) over the paragraph to identify the speaker of the target utterance. As utterance annotations are unavailable in unlabeled novels, we use double quotation marks to segment utterances and regard the first one as the target utterance. Then we remove all utterances and the speaker from the paragraph, separate the remaining context by commas, and use the last span as the NM of the target utterance to reduce noise. For example, given a paragraph \u201cMiejue Shitai shouted \u2018Demon Cult!\u2019\u201d, our pattern-based extractor extracts \u201cshouted\u201d as the NM of the underlined utterance. However, this method will inevitably suffer from relatively low recall (e.g., 63.1% in macro-averaged recall on the dev set of NME). For example, given the context of E6 in Table 3, both \u201cheart was relieved\u201d and \u201cthought\u201d should be regarded as NMs while this method can only extract \u201cthought\u201d.\\n\\n4.2 Extractive Method\\n\\nAs a nonverbal message mention must be a one or multiple spans in the context surrounding the target utterance, we consider an extractive machine reading comprehension (MRC) formulation (Devlin et al., 2019) that originally aims to extract an answer of a given question from a document. We regard the target utterance $u$ as the question and regard the surrounding context of $u$ as well as $u$ as document $d$. The ground truth nonverbal message of $u$ is treated as the answer $a$. We follow previous work to concatenate a special token [CLS], tokens in $u$, a special token [SEP], and tokens in $d$ as the input sequence. Two vectors $p_{\\\\text{start}}$ and $p_{\\\\text{end}}$ are introduced to represent the estimated probabilities of each token in $d$ to be the start or end token of the correct answer span $a$ that appears in $d$, respectively. Let $a_{\\\\text{start}}$ and $a_{\\\\text{end}}$ denote the start offset and end offset of $a$, respectively. We optimize the extractive model with parameters $\\\\theta$ by minimizing $\\\\sum_{t \\\\in V} L(t, \\\\theta)$, where $V$ represents the set of NM extraction instances, and $L$ is defined as:\\n\\n$$L(t, \\\\theta) = -\\\\log p_{\\\\text{start}}(a_{\\\\text{start}} | t) - \\\\log p_{\\\\text{end}}(a_{\\\\text{end}} | t).$$\\n\\n(1)\\n\\nHowever, this classical extractive architecture can only extract a single span from the context, though there are some attempts (e.g., (Segal et al., 2020; Yang et al., 2020)) to extend it to extract multiple spans. For those instances with multiple NMs, we simply use the longest common substring of the context and the concatenation of these NMs as the actual answers for training and the original labels for validation. We introduce multi-span formulation in the following subsection.\\n\\n4.3 Generative Method\\n\\nTo address the single-span limitation of the above extractive method, we regard nonverbal message extraction as a text-to-text task (Raffel et al., 2020): the extractor is fed the surrounding context of the target utterance and is asked to generate the NM of this utterance. For NMs that are a set of non-contiguous spans, we concatenate them using commas to form the ground truth labels.\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Type | Sub-Type | Example | ID\\n--- | --- | --- | ---\\nkinesics | body movement | \u22a2\\n\\nPulling tightly the corner of her quilt with both hands and asked in a shaking voice, \\\"He... What... What happened to him?\\\"\\n\\nFacial expression, immediately \\\"...\\\"\\n\\n\\\"...\\\"\\n\\nInternal states, intention. \\\"...\\\"\\n\\n\\\"...\\\"\\n\\nFeelings/emotions... \\\"...\\\"\\n\\n\\\"...\\\"\\n\\nPause \u2014 \\\"...\\\"\\n\\n\\\"...\\\"\\n\\nVocal-related. \\\"...\\\"\\n\\n\\\"...\\\"\\n\\nOthers. \\\"...\\\"\\n\\n\\\"...\\\"\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"two-stage fine-tuning, which has been widely used in previous semi-supervised studies to reduce the impact of noise (Xie et al., 2020; Sun et al., 2022): we first fine-tune a generator on the combination of the clean and weak-labeled data and then fine-tune the resulting generator on the clean data alone.\\n\\n6 Experiment\\n\\n6.1 Implementations\\n\\nFor the extractive baselines, we consider both encoder-only (RoBERTa-wwm-ext-large and MacBERT, both released by Cui et al. (2021)) and encoder-decoder models (T5 base (Zhao et al., 2019), BART large (Shao et al., 2021), and DialBART large (HIT-TMG, 2022)). Note that DialBART large (HIT-TMG, 2022) is obtained by fine-tuning BART large (Shao et al., 2021) on LUGE dialogue (Section 6.2). The above encoder-decoder models are also used for generative NM extractors and NM generators. We discard the tokens from the bottom of the input if its length exceeds the maximum model sequence length.\\n\\nWe conduct experiments on eight NVIDIA-V100 32GB GPUs. Appendix A.8 introduces the models\u2019 details (Table 19), hyper-parameters (Table 18) for each task, and evaluation metrics. We run each experiment five times with different random seeds.\\n\\n6.2 Datasets\\n\\nScript: we use Chinese scripts to construct clean NM generation data. We collect scripts from a script website4 ONLY available for research and non-commercial use and keep 454 scripts after filtering those with format issues. We introduce more details about how to parse scripts in Appendix A.1 as this is not this work\u2019s main contribution and has been widely explored in the literature. We use triples extracted from non-overlapped scripts as training and dev sets to avoid data leakage. The most recent 50 scripts are used for the dev set.\\n\\nNovel: we collect 521 Chinese novels from Yuewen for weak-labeled NM generation data construction (Section 4.4). Due to copyright restrictions, the novels will not be directly released. We experiment with two weak NM generation data of different sizes: Novel (397K) and Novel L (749K), and Novel is a subset of Novel L.\\n\\nC3D and EWECT6: for dialogue/narrative understanding tasks, we consider a multiple-choice MRC dataset C3D and an emotion classification dataset EWECT (the general-domain version) to investigate the impact of introducing generated NMs into dialogue tasks without NMs. We provide the detailed data statistics in Table 12 (Appendix A.4).\\n\\n6.3 NM Extraction Evaluation\\n\\nSupervised extractors outperform the pattern-based method (Table 4). We find that including the target utterance in the input hurts the performance of generative methods even though the utterance boundary is indicated by [SEP]. Methods such as increasing the training data size (clean or noisy) of NME may help models learn to focus on the writers\u2019 words for identifying NMs. The length distribution of the NMs extracted by our supervised extractors is very similar to that of the clean NMs in Script. Note that the sharp drop in the pattern-based NM distribution is caused by the length constraint we set for weak data construction (Section 4.4). We also find that the generative models underperform in instances with multiple NMs (Appendix A.3).\\n\\n6.4 NM Generation Evaluation\\n\\nFor the majority baseline, we use the most frequent NM (1.03%, \u201c\u7b11\u201d (\u201csmile\u201d), in the training set of the Script as the NMs for all utterances. We notice a model pre-fine-tuned on dialogue generation datasets LUGE dialogue performs better on NM generation (6 vs. 4 in Table 5). For semi-supervised training, we experiment with two backbone models (T5 base and DialBART large) and see consistent gains (10 vs. 3) (15/18 vs. 6) over the purely supervised.\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: The nonverbal message extraction performance on the NME dataset (cat: concatenation; context 1: context before the target utterance. context 2: context after the target utterance. EM: exact match).\\n\\n| Training data/method model type | Model type | F1 | ROUGE-1 | ROUGE-L | DIST-1 | DIST-2 | ID |\\n|--------------------------------|------------|----|---------|---------|--------|--------|----|\\n| majority \u2013 weak                |            | 3.18 | 3.20    | 3.20    | 0.06   | 0.06   | 1  |\\n| CSK (Speer et al., 2017) T5    | indirect   | 1.94 | (0.15)  | 2.02    | (0.15) | 1.99   | (0.16)| 10.05 | (2.17) |\\n| Script train T5 base           | direct     | 7.20 | (1.26)  | 7.73    | (1.35) | 7.62   | (1.33)| 7.47  | (4.07)  |\\n| Script train BART large        | direct     | 7.49 | (0.56)  | 8.05    | (0.74) | 7.92   | (0.73)| 6.65  | (3.52)  |\\n| \u2013 DialBART large               | indirect   | \u22c6   | 2.92    | 3.36    | 3.23   | 10.22  | 43.35 |      |         |\\n| Script train DialBART large    | semi       | \u22c6   | 7.65    | (0.31)  | 8.29   | (0.30)| 8.15  | (0.32)| 10.12 | (0.40)  |\\n| Novel pattern T5 base          | weak       |     | 4.58    | (0.18)  | 4.96   | (0.18)| 4.89  | (0.18)| 5.51  | (0.28)  |\\n| Novel extractive T5 base       | weak       |     | 4.54    | (0.18)  | 4.95   | (0.21)| 4.92  | (0.21)| 14.68 | (0.28)  |\\n| Novel generative T5 base       | weak       |     | 4.83    | (0.23)  | 5.28   | (0.26)| 5.24  | (0.26)| 15.64 | (0.17)  |\\n| Novel pattern \u21a0 Script train T5 base | semi | 7.85   | (0.23)  | 8.59   | (0.23)| 8.46   | (0.25)| 9.93  | (0.35)  |\\n| Novel extractive \u21a0 Script train T5 base | semi | 7.53   | (0.25)  | 8.18   | (0.26)| 8.08   | (0.28)| 10.01 | (0.25)  |\\n| Novel generative \u21a0 Script train T5 base | semi | 7.83   | (0.37)  | 8.44   | (0.37)| 8.33   | (0.35)| 10.37 | (0.40)  |\\n| Novel pattern \u21a0 Script train DialBART large | semi | 7.87   | (0.25)  | 8.50   | (0.28)| 8.40   | (0.27)| 10.15 | (0.62)  |\\n| Novel extractive \u21a0 Script train DialBART large | semi | 7.77   | (0.17)  | 8.44   | (0.21)| 8.34   | (0.17)| 10.44 | (0.46)  |\\n| Novel generative \u21a0 Script train DialBART large | semi | 7.95   | (0.22)  | 8.68   | (0.25)| 8.56   | (0.24)| 10.45 | (0.42)  |\\n| Novel pattern (L) \u21a0 Script train DialBART large | semi | 8.05   | (0.24)  | 8.71   | (0.25)| 8.60   | (0.25)| 10.02 | (0.36)  |\\n| Novel extractive (L) \u21a0 Script train DialBART large | semi | 7.87   | (0.18)  | 8.55   | (0.16)| 8.44   | (0.18)| 10.62 | (0.42)  |\\n| Novel generative (L) \u21a0 Script train DialBART large | semi | 8.15   | (0.28)  | 8.84   | (0.29)| 8.72   | (0.28)| 10.22 | (0.30)  |\\n\\nTable 5: The nonverbal generation average performance and standard deviation on the dev set of the Script (\u21e0: two-stage fine-tuning (Section 5.2). \u22c6: as DialBART large is pre-fine-tuned on LUGE dialogue).\\n\\nFigure 3: Length distribution of nonverbal messages extracted by scripts and our three methods.\\n\\nThe Impact of Context and Speaker ID: to investigate the impact of context on NM generation, remove the context from the training instances. In that case, the extractive baseline is more likely to predict a wrong span boundary by including irrelevant long context in NMs compared with other baselines. This is also commonly seen when this formulation is used for other span extraction tasks (Gao et al., 2019). As shown in Figure 3, more than 5% of the NMs extracted from novels by the extractive baseline contain more than 20 characters, and the length inconsistency of the resulting data may hinder the training of NM generators.\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shopwoman: Are you looking for a wedding ring? [SEP] No. I would like to see this one.\\n\\nShi Hao: Dad, I'm hungry. [SEP] Okay, let's eat! Try this. This is marinated by my wife. It's very crispy, and the spicy taste will burst in all at once,..... Try it!\\n\\n\u4e07\u5b97\u534e: Sink shoulders, drop elbows, and embrace circles. Look what you are practicing. Just like that, how can I participate in the Mid-Autumn Festival Gala? How to perform? [SEP] God, this is so tedious.\\n\\nWan Zonghua: The injury is still not healed, do you want to try my exclusive secret recipe?\\n\\nHuman Evaluation and Error Analysis: we randomly sample 100 instances from the held-out set of Script and randomly shuffle the label and automatically generated NMs for each instance. Given the context, target utterance, and an NM, we ask annotators to rate each NM using the following four binary metrics: (M1) the relevance between the utterance and the NM based on the context, (M2) the fluency of the NM, (M3) the validity of the NM, and (M4) the factual consistency of the NM based on the context and utterance. Table 7 shows some negative examples for each metric. We elaborate on our evaluation guidelines and provide error analysis showing models may need more external knowledge from different resources in Appendix A.5.\\n\\nFor the NM generation human evaluation, the human agreement ($\\\\kappa$) is measured using Cohen's kappa (details in Appendix A.5). For all four metrics, $\\\\kappa = 0.55$ (moderate agreement). When we do not consider the hallucination issue in M4 as ground truth NM label cannot be judged using this metric, $\\\\kappa = 0.64$ (substantial agreement). Similarly to our observations when automatic metrics are used, models trained with automatically extracted data achieve better performance over the purely supervised baseline trained with Script (Table 8).\\n\\nTable 8: Human evaluation (%) on the held-out set of Script (M1: relevance, M2: fluency, M3: validity, M4: consistency, A VG4/3: average of M1\u20134 and M1\u20133).\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"as the clean commonsense and script knowledge on\\nthe two tasks. In Appendix A.7, we show complete\\nexpanded examples for each task in Table 16 and\\nprovide error analysis for future directions.\\n\\n| Source | Dev | Test |\\n|--------|-----|------|\\n| BERT   | 62.3 | 62.1 |\\n| RoBERTa LARGE | 67.3 (3.45) | 66.1 (2.19) |\\n| CSK RoBERTa LARGE | 68.3 (0.72) | 67.0 (0.89) |\\n| Novel L RoBERTa LARGE | 69.4 (0.46) | 67.6 (0.89) |\\n\\n| Source | Dev | Test |\\n|--------|-----|------|\\n| BERT   | 78.7 |       |\\n| RoBERTa LARGE | 79.7 (0.25) | 78.4 (0.34) |\\n| CSK RoBERTa LARGE | 79.4 (0.49) | 78.9 (0.20) |\\n| Novel L RoBERTa LARGE | 80.2 (0.19) | 78.8 (0.24) |\\n\\nTable 9: The accuracy (%) of introducing NMs into C\\n\\n7 Conclusions and Future Work\\nThis work focuses on NM extraction and generation\\nleveraging unlabeled corpora, and experimental re-\\nsults show the effectiveness of our proposed meth-\\nods and the usefulness of the large-scale weakly-\\nlabeled data. Future work includes improving NM\\nextraction via semi-supervised learning, incorporat-\\ning speaker profiles into NM generation, and gener-\\nating both utterances and their NMs (Section A.6).\\n\\nAcknowledgements\\nWe would like to thank the anonymous meta-\\nreviewers/reviewers for their insightful feedback.\\n\\nLimitations\\nScope and Generalizability: though code-switch\\nmay occur in utterances, all the datasets (NME,\\nweak-labeled NM data, and downstream tasks) are\\nmainly written in Chinese. Though our methods es-\\nspecially the supervised ones do not consider many\\nChinese-specific features, it is unclear to what ex-\\ntent the differences in grammar and culture across\\ndifferent languages or even within one language\\nwill impact the generalizability of empirical re-\\nsults and observations based on data in a single\\nlanguage used in this paper. In particular, it has\\nbeen revealed by nonverbal communication litera-\\nture (LaFrance and Mayo, 1978) that considerable\\nculture-related differences are more likely to ex-\\nstist for interpersonal NMs: for example, bowing is\\nused for showing status differences in some cul-\\ntures such as Japan (Morsbach, 1973). In addition,\\nNME is based on an existing novel dataset that in-\\nvolved three novels of similar genres written by a\\nsingle author. Though this may decrease the dif-\\nficulty of NM annotation, the diversity of NMs is\\nrelatively insufficient, which may hurt the robust-\\nness and generalization abilities of the supervised\\nNM extractor trained on NME.\\n\\nSilent NM: it is possible that an utterance is spoken\\nwithout any CLEAR nonverbal messages. This work\\nonly focuses on utterances with non-empty nonver-\\nbal messages when we build clean NM extraction\\ndata (Section 3.1), and we use the pattern-based\\nextractor as the first step of building large-scale\\nweakly-labeled data (Section 4.4), which also helps\\nfilter instances with empty NMs.\\n\\nTo identify silent NMs, besides the pattern-\\nbased method in this work, the discarded instances\\nwith empty NM can be kept to train an extractor\\n(e.g., similar to the formulation and implemen-\\ntation to address the unanswerable questions in\\nmachine reading comprehension tasks (Rajpurkar\\net al., 2018)) or generator (e.g., simply using a pre-\\ndefined answer such as \\\"empty\\\" or \\\"none\\\" as the\\nNM of these instances), though this will inevitably\\nincrease the task difficulty.\\n\\nContext Selection: for our NM generation experi-\\nments except for the ablation studies, we train and\\nevaluate our models on instances with one history\\nutterance or narrative. As the target utterance is\\nmore important for NM generation than dialogue\\nhistory, this does not lead to a negative impact\\non the performance. However, introducing more\\ncontext can facilitate annotators' understanding to\\nbetter score a generated or ground truth NM, and\\nthis is necessary if we aim to generate diverse target\\nutterances as well as their NMs.\\n\\nHuman Evaluation: for each criterion, we simply\\nask annotators to rate 0 or 1 for a nonverbal mes-\\nsage, as NM is shorter (e.g., a verb phrase) than the\\nutterance outputs of dialogue generation tasks that\\nmay allow more fine-grained scales. We admit that\\nthe evaluation guidelines can be further improved.\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ethical Considerations\\n\\nBased on the authors' experience, the price is cheaper than that of similar span selection tasks in earlier years, perhaps due to the impact of the epidemic and supply and demand in China when the annotation job was submitted in 2022. The NM extraction annotation is conducted by annotators from a commercial annotation company (large enterprise) headquartered in Chengdu, Sichuan. A four-day trial annotation is conducted for bidding (seven companies participated), and the authors answered questions posted by the annotators from different companies and updated annotation guidelines accordingly. The Q&A history is maintained and updated in the formal annotation. After the pilot annotation, the quoted price from the selected company is 0.15 \u00d7 2 = 0.30 RMB ($0.04) for each instance. The formal annotation including data acceptance is finished within one week.\\n\\nReferences\\n\\nApoorv Agarwal, Sriramkumar Balasubramanian, Jiehan Zheng, and Sarthak Dash. 2014. Parsing screenplays for extracting social networks from movies. In Proceedings of the CLFL, pages 50\u201358.\\n\\nNalini Ambady and Robert Rosenthal. 1998. Nonverbal communication. Encyclopedia of mental health, 2:775\u2013782.\\n\\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python: analyzing text with the natural language toolkit. O'Reilly Media, Inc.\\n\\nCynthia Breazeal, Cory D Kidd, Andrea Lockerd Thomaz, Guy Hoffman, and Matt Berlin. 2005. Effects of nonverbal communication on efficiency and robustness in human-robot teamwork. In Proceedings of the IROS, pages 708\u2013713.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Proceedings of the NeurIPS, pages 1877\u20131901.\\n\\nMichela Cecot. 2001. Pauses in simultaneous interpretation: A contrastive analysis of professional interpreters' performances. The interpreters' newsletter, 11:63\u201385.\\n\\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. arXiv preprint, cs.CL/2006.14799v2.\\n\\nHerbert H Clark and Thomas B Carlson. 1982. Hearers and speech acts. Language, 58(2):332\u2013373.\\n\\nRichard Csaky and G\u00e1bor Recski. 2021. The Gutenberg dialogue dataset. In Proceedings of the EACL, pages 138\u2013159.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and Ziqing Yang. 2021. Pre-training with whole word masking for chinese bert. TASLP, 29:3504\u20133514.\\n\\nJoseph A DeVito, Susan O'Rourke, and Linda O'Neill. 2000. Human communication. Longman New York.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the NAACL-HLT, pages 4171\u20134186.\\n\\nShuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyouth Chung, and Dilek Hakkani-Tur. 2019. Dialog state tracking: A neural reading comprehension approach. In Proceedings of the SIGDIAL, pages 264\u2013273.\\n\\nDaniel Gatica-Perez. 2009. Automatic nonverbal analysis of social interaction in small groups: A review. Image and vision computing, 27(12):1775\u20131787.\\n\\nPhilip John Gorinski and Mirella Lapata. 2015. Movie script summarization as graph-based scene extraction. In Proceedings of the NAACL-HLT, pages 1066\u20131076.\\n\\nPhilip John Gorinski and Mirella Lapata. 2018. What's this movie about? a joint neural network architecture for movie content analysis. In Proceedings of the NAACL-HLT, pages 1770\u20131781.\\n\\nEun Young Ha, Joseph F. Grafsgaard, Christopher Mitchell, Kristy Elizabeth Boyer, and James C. Lester. 2012. Combining verbal and nonverbal features to overcome the \\\"information gap\\\" in task-oriented dialogue. In Proceedings of the SIGDIAL, pages 247\u2013256.\\n\\nFlorian Hinterleitner, Georgina Neitzel, Sebastian M\u00f6ller, and Christoph Norrenbrock. 2011. An evaluation protocol for the subjective assessment of text-to-speech in audiobook reading tasks. In Proceedings of the Blizzard Challenge.\\n\\nHIT-TMG. 2022. dialogue-bart-large-chinese.\\n\\nGeorge Hripcsak and Adam S Rothschild. 2005. Agreement, the f-measure, and reliability in information retrieval. Journal of the American medical informatics association, 12(3):296\u2013298.\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-1021", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al. 2021. Nero: a biomedical named-entity (recognition) ontology with a large, annotated corpus reveals meaningful associations through text embedding.\\n\\nNPJ systems biology and applications, 7(1):38.\\n\\nYida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong Jiang, Xiaoyan Zhu, and Minlie Huang. 2020. A large-scale Chinese short-text conversation dataset. In Proceedings of the NLPCC.\\n\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. 2020. Self-training with noisy student improves imagenet classification. In Proceedings of the CVPR, pages 10687\u201310698.\\n\\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan. 2020. CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the COLING, pages 4762\u20134772.\\n\\nJunjie Yang, Zhuosheng Zhang, and Hai Zhao. 2020. Multi-span style extraction for generative reading comprehension. arXiv preprint, cs.CL/2009.07382v2.\\n\\nDian Yu, Ben Zhou, and Dong Yu. 2022. End-to-end Chinese speaker identification. In Proceedings of the NAACL-HLT, pages 2274\u20132285.\\n\\nHongming Zhang, Xin Liu, Haojie Pan, Haowen Ke, Jiefu Ou, Tianqing Fang, and Yangqiu Song. 2022. ASER: towards large-scale commonsense knowledge acquisition via higher-order selectional preference over eventualities. Artificial Intelligence, 309:103740.\\n\\nHongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song, and Cane Wing-Ki Leung. 2020. Aser: A large-scale eventuality knowledge graph. In Proceedings of the WWW, pages 201\u2013211.\\n\\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint, cs.CL/1810.12885v1.\\n\\nZhe Zhao, Hui Chen, Jinbin Zhang, Xin Zhao, Tao Liu, Wei Lu, Xi Chen, Haotang Deng, Qi Ju, and Xiaoyong Du. 2019. Uer: An open-source toolkit for pre-training models. Proceedings of the EMNLP-IJCNLP (System Demonstrations), pages 241\u2013246.\\n\\nHao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. 2018. Emotional chatting machine: Emotional conversation generation with internal and external memory. In Proceedings of the AAAI, pages 730\u2013738.\\n\\nHao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang, and Xiaoyan Zhu. 2020. Kdconv: A Chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation. arXiv preprint, cs.CL/2004.04100v1.\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nA.1 Clean NM Generation Data Construction\\n\\nFollowing previous work (Vassiliou, 2006; Wang, 2017; Sun et al., 2022), we extract utterances and their corresponding speakers and NMs from scripts mostly using patterns. A script is made up of multiple scenes. We first segment a script into scenes based on the blank lines and scene headings (e.g., EXT. (exterior spaces) and INT. (interior spaces)). In each scene, the utterance is set under its corresponding character, and the NM (if any exists) appears right after the character and is inside a parenthetical as shown in Table 10. We follow the aforementioned style format to design patterns to extract (utterance, speaker, NM) triples and the context such as previous utterances before the utterance. Some scripts do not strictly follow the screenplay formats, and therefore an NM and its speaker may form a natural language sentence without any parentheticals as hints. We use a unified text-to-structure generation framework UIE (Lu et al., 2022), pretrained on large-scale structured and unstructured corpora. We manually annotate fourteen speaker identification instances (input: text that may contain speaker(s), NM(s), and an utterance; output: speaker of the utterance) and fine-tune UIE with them for few-shot learning. We discard instances with empty speakers and the ones with speakers of relatively confidence values (ranging from 0 to 1) less than 0.5 for quality control.\\n\\nTo have a rough estimation of the quality of the extracted NMs from scripts, the authors manually check the randomly sampled 50 NMs in the dev set of Script, and the extraction accuracy (or exact match) is 100.0%. It is possible to carefully design more patterns or apply supervised methods to improve the recall of script-based NM extraction: for example, extracting those NMs written in the action lines in scripts that describe what the audience or readers are meant to see or hear in the scene, and they are not enclosed in parentheses, though this task itself requires additional annotation, which is beyond the scope of this paper.\\n\\nA.2 Types of Nonverbal Messages\\n\\nKinesics: it contains body movement and facial expression (e.g., \u201cface grew grave\u201d), serving as an important form of nonverbal communication (Key, 1977). Internal States: it contains two sub-types: intention (e.g., \u201ccomfort\u201d, \u201cexhort\u201d, and \u201cpatch up a lie\u201d) and inner feelings/emotions. The internal states are recognized to be associated with other types of nonverbal messages such as facial expressions (Tracy et al., 2015). Pause: this refers to the pause that occurred before the target utterance was spoken (e.g., \u201cstunned for a while\u201d and \u201cwithout hesitation\u201d). Previous studies emphasize the role of pauses as they are considered as oral punctuation that conveys additional information (Cecot, 2001). This kind of information is crucial for speech applications such as audiobook reading (Hinterleitner et al., 2011). Vocal-Related: based on analysis, we notice the majority of NMs are related to the characteristics of voice. We further categorize them into the following sub-types.\\n\\n\u2022 Addressee: we only consider the cases where the addressee \u2014 the person at whom the speech is directed (Clark and Carlson, 1982) \u2014 is the speaker him/herself such as \u201cthought\u201d and \u201ctalked to himself\u201d, we leave exploring of the impacts of the relationship (e.g., trusting) between the speaker and the addressee(s) on nonverbal messages (Larsen and Smith, 1981) to future work.\\n\\n\u2022 Number of speakers: when the target utterance is spoken by multiple speakers. The NMs of each speaker may be constrained by others via social rules consciously or unconsciously (Gatica-Perez, 2009).\\n\\n\u2022 Tone: we categorize an NM into this sub-type (Ambady and Rosenthal, 1998) when there is no evidence in the texts to support that such an NM is observable through fa-\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cial expressions or is one of the inner feelings/emotions, for example, \u201csaid coldly\u201d and \u201csaid in amazement\u201d.\\n\\n- **Speed**: the speed of the speech (e.g., fast and slow).\\n- **Volume**: it refers to the power of a speaker\u2019s voice, for example, \u201cin a soft voice\u201d.\\n- **Pitch**: in speech, it refers to the highness or lowness of a speaker\u2019s voice such as \u201cscream\u201d and \u201cwith a deep voice\u201d. It is regarded as an important aspect of nonverbal communication, just as speed and volume (DeVito et al., 2000).\\n- **Timbre**: it refers to some important traits of the voice such as age, gender, and quality of the voice (e.g., \u201choarse voice\u201d), and these factors can also influence the nonverbal communication between speakers (Siegman, 1987).\\n\\n**Others**: NMs that belong to none of the above categories such as dialect, singing, and languages.\\n\\n**NM Type Annotation**: the authors first go through the data and design the above types and provide examples for each type in the guideline. When the annotators conduct the NM extraction annotation, to simplify the task, they will also select ONE type from the given thirteen candidate types for each selected NM. The inter-rater agreement measured by Cohen\u2019s kappa for NM typing is 0.73 based on the overlapped annotated spans by two annotators.\\n\\n### A.3 Human Annotation of NM Spans and Error Analysis of NM Extraction\\n\\nAs mentioned previously, we only keep those that occur a short time before the utterance is spoken or at the same time when there can exist several NMs in the context to ensure a high relevance between the target utterance and the annotated NMs.\\n\\nFor example, given the preceding sentences:\\n\\nLu Wushuang and Cheng Ying immediately expressed their intentions of coming along with Huang Rong. They came out of Xiangyang, went around the enemy\u2019s camp, and went northwest. Huang Rong thought, \u201cThis time Xiang\u2019er\u2019s intention is to find Yang Guo ... \u201d\\n\\nverb phrases such as \u201cwent around the enemy\u2019s camp\u201d and \u201cwent northwest\u201d are not annotated as NMs of the speaker \u201cHuang Rong\u201d due to the unclear time interval based on the description.\\n\\nThough NMs can be described in the context after the target utterance, we do not label those that are supported to occur after the target utterance. For example, \u201cflew forward\u201d is not regarded as an NM of the underlined target utterance based on the temporal clue \u201cafter he said this\u201d, and \u201csaid\u201d right after the speaker \u201cZhang CuiShan\u201d of the target utterance is also ignored as it belongs to our defined relatively uninformative speech words.\\n\\nBefore Du DaJin could respond, sub-leader Shi cut in, \u201cJust say what you want us to do.\u201d Zhang CuiShan said, \u201cI\u2019m going to break every single bone in your arms ...!\u201d After he said this, he immediately flew forward.\\n\\n**Error Analysis**: to investigate the remaining challenges of NM extraction, we analyze the performance by the NM types (Table 11). There is still plenty of room for improvement to extract accurate NMs that are body movements and addressees. We further manually check the best-performing generative extractor\u2019s predictions that are not exactly the same as the ground truth answers (EM=0).\\n\\nThe common error types are (i) only one NM is extracted when multiple NMs exist (69.4%), (ii) the generated NM is incomplete compared with the single NM label (16.7%): for example, the pre-defined relatively uninformative word \u201c\u9053\u201d (\u201csaid\u201d) in \u201c\u70b9\u70b9\u5934\u9053\u201d (\u201cnodded and said\u201d), and (iii) only the relatively uninformative NM is generated (5.6%). In particular, among the missed NMs, most of them belong to KINESICS (body movement 45.2% and facial expression 12.9%), followed by the feelings/emotions (25.8%) in INTERNAL STATES. For further improvement, it may be useful to increase the diversity of NME training data to involve books written by different authors or use in-context learning based on large language models such as GPT-3 (Brown et al., 2020) to generate weak-labeled multi-NM instances.\\n\\n### A.4 Data Statistics\\n\\n### A.5 Human Evaluation of NM Generation and Error Analysis\\n\\n**Human Evaluation**: we define the following four metrics (M1\u2013M4).\\n\\n- **M1**: the RELEVANCE between the utterance and the NM based on the whole history context.\\n- **M2**: the FLUENCY of the NM. This metric mainly focuses on language expression such as...\\n\\nBased on the annotation guideline introduced in Section 3.1\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Table 11:** The nonverbal message extraction performance by NM types on the dev set of the NME dataset (*)\\n\\n| NM Type       | Script Novel / Novel |\\n|---------------|----------------------|\\n|               | L                    |\\n| human pseudo-labeled | \u22c6                   |\\n| # train instances | 40K 397K / 749K     |\\n| # train sources  | 404 scripts 521 novels |\\n| # dev           | 1,708                |\\n| # dev sources   | 50 scripts           |\\n\\n**Table 12:** Statistics of the nonverbal message generation data (pseudo-labeled: the NMs are extracted automatically by pattern-based, generative, or extractive NM extraction methods) and two dialogue/narrative understanding tasks. Novel is a subset of Novel L.\\n\\n**Table 13:** Inter-rater agreement between different annotation teams for NM generation evaluation.\\n\\n| NM Type       | M1\u2013M3 | M1\u2013M3 |\\n|---------------|-------|-------|\\n| humans a and b| 0.50  | 0.55  |\\n| humans a and c| 0.59  | 0.71  |\\n| humans b and c| 0.56  | 0.66  |\\n| average       | 0.55  | 0.64  |\\n\\n**Error Analysis:** based on the results of the best...\"}"}
{"id": "emnlp-2023-main-1021", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"performing model (the last row in Table 5) on the dev set of Script, it may be helpful to further introduce commonsense knowledge from existing structured knowledge during training or via continual learning from large-scale books explicitly or implicitly. For example, the system generated NM \u201cpoints to a small bottle\u201d is less likely to happen compared with the ground truth \u201cpicks up the cup\u201d (the first instance in Table 14), similar to the weighted edges in eventuality knowledge graph wherein the weight is defined as the frequencies of appearance of a piece of knowledge in the corpora (Zhang et al., 2020, 2022). In addition, including more books of diverse genres may alleviate some biases in script corpora. For example, NM \u201cto a walkie-talkie\u201d (see the full input in the second instance in Table 14) appears more frequently in the training set of Script than Novel L (0.167% vs. 0.001%).\\n\\nA.6 Evaluation of Verbal-Nonverbal Message Generation\\n\\nWe have shown that automatically extracted (context, utterance, NM) triples can benefit NM generation (Table 5). Another interesting question is whether the automatically extracted data can also be helpful when we aim to generate both an utterance and its corresponding NM, which is more challenging than NM generation. We use a similar formulation as that of the NM generation while we change the input to $U = \\\\{u_1, u_2, ..., u_{k-1}\\\\}$ and speaker $s_k$ of utterance $u_k$, and we use the concatenation of $u_k$ and $n = \\\\{n_1, n_2, ..., n_m\\\\}$ as the output. We include the speaker information as there may exist multiple person entities in the previous context, and we are only interested in a certain speaker. We further process the labels by enclosing the nonverbal message with parentheses to generate structured results, motivated by the script formats (Figure 4). As a preliminary study, we experiment with T5 as the backbone model. We observe that, surprisingly, a model trained with weakly-labeled data constructed by any of our extractors can already achieve better performance than the same baseline trained with clean data (Table 15). However, we find that the lack of diversity is the main issue, reflected by the low DIST1/DIST2 scores. It might be useful to introduce more previous utterances or narratives as history context.\\n\\nA.7 Enriched Examples of Downstream Tasks and Error Analysis\\n\\nAs introduced in Section 6.5, we add an NM right after each utterance or narrative in the original text input of a downstream task and keep all others the same. See examples for each task in Table 16. We mainly analyze the wrong instances after the NMs are added to the original task inputs while they can be predicted correctly by the previous baseline. We notice that systems may be distracted by the newly added NMs especially when they are not highly relevant to the question and the conversation is short. For example, the NM \u201csmiles\u201d may distract the system from understanding the unspoken real intention (Table 17), though for human readers it seems that they do not intervene in the original expression.\\n\\nA.8 Backbone Models, Hyper-Parameters, and Evaluation Metrics\\n\\nWe compare the backbone models in Table 19 and list hyper-parameters for three types of tasks in Table 18. We choose the two encoder models due to their superior performance in Chinese natural understanding tasks (Cui et al., 2021). We do not consider Chinese decoder models as publicly available models such as GPT-2 released by Zhao et al. (2019) are usually pre-trained on a relatively small scale corpus (Xu et al., 2020).\\n\\nWe follow extractive machine reading comprehension (span extraction) studies (e.g., (Rajpurkar et al., 2016)) using exact match and macro-averaged F1, which measures the average overlap between the extracted NM and the ground truth NM, both treated as bags of characters. We compute the average of the F1 over all of the NM extraction instances. A similar computation is conducted for the macro-averaged recall mentioned in Section 4.1. We use the NLTK tokenizer (Bird et al., 2009) if English words are also included in texts. We use the public evaluation code released by (Xu et al., 2020) to NM extraction.\\n\\nFollowing previous dialogue response generation studies (e.g., (Celikyilmaz et al., 2020)), we use F1 as well as ROUGE-1 (measuring the overlap...\"}"}
