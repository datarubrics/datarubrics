{"id": "emnlp-2024-main-936", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Medical task-oriented dialogue (TOD) systems are gaining importance in modern healthcare by assisting doctors in patient history-taking, diagnosis suggestions, and treatment recommendations, alleviating doctor burnout and extending the reach of medical services (Valizadeh and Parde, 2022; Kearns et al., 2019; Laranjo et al., 2018). Recently, medical TOD systems have witnessed significant progress, particularly in individual sub-modules such as natural language understanding (NLU) (Zhang et al., 2020), policy learning (POL) (Tchango et al., 2022), and natural language generation (NLG) (Yan et al., 2021). Most of the existing medical TOD datasets contain annotations required for training only one sub-component (Wei et al., 2018; He et al., 2020; Fansi Tchango et al., 2022), and only a few include annotations for all sub-components, thereby enabling the construction of a complete dialogue system (Yan et al., 2021; Chen et al., 2022).\\n\\nTraining NLU, POL, and NLG sub-modules requires dialogues to be annotated with intents, slots, dialogue states, and actions. In existing medical TOD datasets, slots are primarily represented as key-value pairs. However, this simplistic representation often fails to capture the inherent complexity of the medical domain. For example, in Figure 1, the patient expresses two symptoms (pharyngitis and fever) along with its onset (past four days and last two days). Existing annotation schemes would fail to capture the symptom-onset link and only...\"}"}
{"id": "emnlp-2024-main-936", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"represent four independent key-value pairs. To overcome this problem, we define a new slot schema, named Comprehensive Medical Attribute Schema (CMAS), that captures the inherent complexity of the slots in the medical domain. It maintains multiple attributes specific to each slot type for better representation. For example in Figure 1, \u2018onset\u2019 is treated as an attribute of the slot \u2018symptom\u2019, establishing a more accurate patient profile.\\n\\nThis paper presents MediTOD, the first publicly available English medical TOD dataset annotated in the CMAS ontology. MediTOD comprises dialogues from staged doctor-patient interviews in objective structured clinical examination format (Gleeson, 1979; Fareez et al., 2022). By leveraging these high-quality dialogues, privacy concerns are mitigated while providing realistic medical scenarios. Collaborating closely with doctors, we develop a questionnaire-based annotation framework to collect slots and corresponding attributes. The annotations are further canonicalized, where possible, to precise medical concepts in Unified Medical Language System (UMLS). Through the release of MediTOD, we aim to provide a valuable resource for advancing research in medical TOD systems.\\n\\nOur main contributions are as follows.\\n\\n1. We release MediTOD, a dataset of doctor-patient dialogues with 22,503 utterances annotated, in collaboration with doctors, using a questionnaire-based labeling scheme designed for the medical domain.\\n\\n2. To label utterances in MediTOD, we develop an annotation portal based on our questionnaire-based scheme. We release this portal alongside the dataset and invite researchers to contribute further to the dataset, enhancing its richness and diversity.\\n\\n3. We establish baselines in supervised and few-shot settings for NLU, POL, and NLG TOD tasks on MediTOD dataset by evaluating representative models from TOD and bio-medical literature. Our results showcase the challenging nature of the dataset.\\n\\nWe make MediTOD resources publicly available at https://github.com/dair-iitd/MediTOD.\\n\\n2 Related Work\\n\\nTask-Oriented Dialogue (TOD) Systems: General domain TOD systems that assist users in completing tasks such as restaurant table reservation and flight booking often follow a modular design, consisting of three modules \u2013 natural language understanding (NLU), dialogue policy learning (POL), and natural language generation (NLG) (Young et al., 2013; Wen et al., 2017). In recent years, there has been significant progress in the field, majorly due to the availability of publicly accessible datasets with dialogue acts annotations (Budzianowski et al., 2018; Rastogi et al., 2020; Byrne et al., 2019; El Asri et al., 2017).\\n\\nWith pre-trained language models (LMs), recent approaches showcase remarkable performance on all three TOD sub-tasks. Lee et al. (2021), Cao et al., and Bang et al. (2023) achieve state-of-the-art performance for understanding user's requirements (NLU). Wu et al. (2023), Bang et al. (2023), and Sun et al. (2023) showcase similar trends for system action prediction and response generation tasks. In line with this trend, we benchmark pre-trained language models Flan-T5 (Chung et al., 2024), BioGPT (Luo et al., 2022) and PPTOD (Su et al., 2022) on MediTOD dataset.\\n\\nMedical Dialogue Systems: Many such datasets exist; however, only a few (see Table 1) have been annotated. Early works focus on NLU and extract symptom slot-values and their status from a doctor-patient dialogue. CMDD (Lin et al., 2019) and SAT (Du et al., 2019) datasets study this task as sequence labeling, where dialogues are collected from online healthcare forum and clinical setting, respectively. Subsequent datasets, MIE (Zhang et al., 2020), ReMeDi (Yan et al., 2021), DialoAMC (Chen et al., 2022) and Code-Mixed (Dowlagar and Mamidi, 2023), introduce additional slots, such as medical test and surgery and collect novel data for the task.\\n\\nNotably, ReMeDi and Code-Mixed are the only datasets that collect low-level attributes in their labels. However, MediTOD differs from them in several ways. First, while these datasets capture the attributes, they do not link them to appropriate slots. For example, ReMeDi would label the onset in a patient's utterance as (time, onset, past two days) without linking it to the symptom, fever. In contrast, MediTOD uses CMAS to record slots and attributes together through a questionnaire-based annotation framework. Second, MediTOD has canonicalized values for medical labels, such as symptoms and diseases, to ensure meaningful evaluation and to support future research.\\n\\nUnlike medical dialogue systems, summarization involves converting doctor-patient dialogues...\"}"}
{"id": "emnlp-2024-main-936", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets Language Annotations #utterances/ #utterances\\nAll TOD Tasks Comprehensive Canonicalized dialogue\\nCMDD (Lin et al., 2019) zh \u2717 \u2717 \u2713 42.09 87,000\\nMSL (Shi et al., 2020) zh \u2717 \u2717 \u2713 NA 2,652\\nMIE (Zhang et al., 2020) zh \u2717 \u2717 \u2713 16.26 18,212\\nIntRec (Rojowiec et al., 2020) de \u2717 \u2717 \u2713 57.071 2,397\\nReMeDi (Yan et al., 2021) zh \u2713 \u2717 \u2717 16.34 25,446\\nDialoAMC (Chen et al., 2022) zh \u2713 \u2717 \u2713 40.02 1,64,731\\nCode-Mixed (Dowlagar and Mamidi, 2023) te,en \u2713 \u2717 \u2717 9.75 29,294\\nMediTOD (Ours) en \u2713 \u2713 \u2713 95.57 22,503\\n\\nTable 1: Publicly available medical dialogue datasets with annotations. MediTOD is the only English dataset that features both comprehensive (capturing slots and their low-level attributes together) and canonicalized annotations. The language codes \u201cen,\u201d \u201czh,\u201d \u201cde,\u201d and \u201cte\u201d represent English, Chinese, German, and Telugu, respectively.\\n\\n## 3 The MediTOD Dataset\\n\\nTo advance research in medical dialogue systems, datasets which capture canonicalized, comprehensive annotations must be available publicly. However, existing datasets (listed in Table 1) only fulfill a subset of these requirements. Moreover, these datasets are often limited to a single demographic, which restricts their broad applicability.\\n\\nTo address these gaps, we curate MediTOD, an English dataset of doctor-patient dialogues for collecting patient medical histories. First, we form the dialogues in MediTOD by collecting publicly available transcripts of doctor-patient encounters (Fareez et al., 2022). To capture the complexity of the slots, we define a comprehensive medical attribute schema (CMAS) and develop a questionnaire-based labeling framework to annotate dialogues based on CMAS. For each slot type (e.g., symptom, personal medical history), medical professionals vet questions to capture associated attributes (severity, onset, etc.). Utilizing this framework, professional annotators then label utterances by answering the questionnaire corresponding to each slot under the doctor's supervision. Finally, we canonicalize the slots values to standard medical concepts in the UMLS vocabulary.\\n\\n### 3.1 Dialogue Acquisition\\n\\nRecently, Fareez et al. (2022) released a dataset of doctor-patient interviews from five specialties. These interviews involve staged interactions where medical professionals assume the roles of doctor and patient. The doctor systematically gathers healthcare information from the patient following the Objective Structured Clinical Examinations (OSCE) format, covering aspects such as a history of present illness, past medical records, and family health history crucial for diagnosis. Notably, Fareez et al. (2022) make their dataset publicly available for academic use. Further, their dialogues are highly conversational, averaging 95 utterances per dialogue. Unfortunately, the dataset is not annotated with the necessary labels for building a medical TOD system. In response, we form MediTOD by labeling 22,503 utterances from the respiratory and musculoskeletal specialties available in their dataset.\\n\\n### 3.2 The CMAS Format\\n\\nA doctor-patient dialogue consists of complex slot types such as symptoms, patient medical history, and patient's habits. Symptoms have attributes such as onset (see Figure 1), duration, location, frequency, severity, and progression. Capturing the relationship between these attributes is crucial for creating accurate patient profiles and reliable diagnoses. As explained earlier, existing TOD datasets use key-value pairs to represent both slots and attributes, thus missing the links between them.\\n\\nIn response, for MediTOD, we develop a Comprehensive Medical Attribute Schema (CMAS) to capture the inherent nature of slots and a questionnaire-based annotation framework that effectively captures the relationship between slots and their attributes while simplifying the labeling task. We ask doctors to design questions for each attribute in a slot. For example, questions like \\\"Where is the symptom located?\\\" and \\\"When did the symptom appear?\\\" are suggested for the location and onset of symptoms. We provide further details...\"}"}
{"id": "emnlp-2024-main-936", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Using these questions, we develop detailed annotation guidelines for our labeling task that aims to capture intent, slots, and their attributes for each utterance in the dialogues, considering both the utterance itself and the dialogue history. In the next section, we discuss our labeling interface that naturally collects associated attributes.\\n\\nTable 2 lists intents and slot types in CMAS. Table 17 in Appendix C reports the attributes associated with each slot type in the schema.\\n\\n### 3.3 Labeling Interface\\n\\nOur labeling interface displays a doctor-patient dialogue for annotation. For each utterance, annotators select one or more appropriate intents and slot types. Then, the questionnaires for each chosen slot appear for the annotators to answer. On submission, a status box displays the answers for reference. We further enhance usability with editing features, keyboard shortcuts, and a tracking box highlighting the patient's current slot values. Figure 2 shows a snapshot of our labeling interface. We include the user guide with our annotation guidelines in Appendix E.\\n\\nThe labeling interface offers two significant advantages. First, it requires annotators to capture attributes along with each slot annotated. Specifically, annotators need to provide the slot and its value before recording any additional attributes, which ensures attributes are always linked to their slots and do not exist independently. Second, the interface displays the complete questionnaire for a selected slot, reducing the need for annotators to memorize the slot-attribute relationships.\\n\\n### 3.4 Labeling Process and Quality Control\\n\\nWe avail a professional annotation service to hire six annotators with medical sciences or pharmacy backgrounds to label the dialogues under the supervision of a doctor. We train annotators by providing them with our detailed annotation guidelines and an example of an annotated dialogue for their reference. Once familiarized with the task and the labeling interface, we ask the annotators to label a different sample dialogue independently. Based on their responses, we offer feedback and point out any issues that need to be addressed. To ensure\\n\\n---\\n\\n2The doctor has a professional medical degree with two and half years of hospital experience.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"their understanding, we present a third sample dialogue for them to label independently. During their training, annotators are tested twice, covering 200+ utterances across two dialogues.\\n\\nWe then divide all available dialogues into six groups. Each trained annotator independently labels the assigned group. To ensure label quality, we systematically introduce seed dialogues into all the groups so that each seed dialogue gets labeled by a pair of annotators. We use the seed dialogue labels to periodically measure inter-annotator agreement and identify any quality issues. Throughout the process, annotators can raise concerns in real time, resolving any ambiguities. Our labeling process has a strong inter-annotator agreement with $\\\\kappa = 0.94$ for intents and $\\\\kappa = 0.72$ for slot-value pairs, indicating strong label consistency.\\n\\n### 3.5 Post-Processing\\n\\nIn post-processing, we examine our collected labels and identify any utterances that may lack complete labels. For instance, in a few utterances such as *I used over-the-counter medicine at night time to help sleep*, the labels did not indicate the status of whether the patient had taken the medication. For such cases, which amount to 0.7% (165 samples) of all the utterances, we ask annotators to review and rectify the labeling to ensure accuracy and completeness.\\n\\nMedical concepts, such as symptoms and diseases, often exhibit different surface (and layman) forms. For instance, phrases *shortness of breath* and *difficulty breathing* both refer to the medical concept *Dyspnea*. To ensure consistent medical terminology across annotators, we canonicalize the medical terms in our labels. Specifically, we link medical terms to their precise medical concept in UMLS Metathesaurus. UMLS has a large-scale collection of medical vocabularies that facilitate a standardized framework for representing and linking biomedical concepts. Linking to UMLS allows meaningful evaluations and also paves the way for dialogue systems grounded in large-scale online medical databases.\\n\\nWe first divide the slots (and attributes) into two categories - medical and non-medical. Medical ones include symptoms, patient medical history, family medical history, etc. Non-medical ones include duration, frequency, residence, etc.\\n\\n---\\n\\n### Table 3: MediTOD statistics.\\n\\n| Split          | Dialogues | Utterances | Avg #Utterances | Avg #Words |\\n|----------------|-----------|------------|-----------------|------------|\\n| Train          | 175       | 16,852     | 95.29           | 13.46      |\\n| Valid          | 20        | 1,869      | 92.45           | 12.77      |\\n| Test           | 18        | 1,798      | 98.89           | 13.03      |\\n| Out-of-domain Test | 20    | 2,197      | 109.85          | 14.00      |\\n| Total          | 213       | 22,503     | 96.57           | 13.42      |\\n\\nTo canonicalize medical slots/attributes, we use QuickUMLS string matching to generate a set of candidate UMLS concepts. Then, we manually verify these candidates to filter extraneous ones, considering surface forms and the context within the dialogue. We keep the doctor in the loop to review the candidates and pick the final concept, provide corrections or recommendations where necessary, and resolve any ambiguities. Finally, we replace the surface forms with their corresponding canonicalized version to ensure that the medical terminology aligns with professional standards.\\n\\nIn contrast to medical slots/attributes, non-medical ones lack standardized vocabularies of concepts and are thus not canonicalized in our dataset.\\n\\n### 3.6 Dataset Statistics\\n\\nWe treat respiratory and musculoskeletal dialogues in MediTOD separately. We use dialogues from the respiratory specialty for model building and in-domain benchmarking, while dialogues from the musculoskeletal specialty serve as an out-of-domain test set.\\n\\nWe divide the respiratory dialogues into train, validation, and test sets. First, we form a high-quality in-domain test set consisting of the seed dialogues we used for quality control. Each test dialogue is thus labeled by two different annotators. To obtain the final labels, we task a third annotator to resolve the inconsistencies between the two sets of labels. Finally, we randomly split the remaining respiratory dialogues into train and validation sets.\\n\\nThe out-of-domain test set consists of musculoskeletal dialogues. A single annotator labeled all these dialogues. To ensure quality, we conducted periodic checks on the submitted labels and provided feedback to the annotator as needed. Table 3 lists overall statistics for MediTOD dataset. With 22,503 annotated utterances, MediTOD enables meaningful training and evaluation of the machine learning models. Dialogues in MediTOD are highly conversational, with an average of 96 utterances per dialogue.\\n\\n---\\n\\n3. We measure inter-annotator agreement between pairs of annotators and report the average.\\n\\n4: https://github.com/Georgetown-IR-Lab/QuickUMLS\"}"}
{"id": "emnlp-2024-main-936", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Distribution of utterances in MediTOD dataset over different intents and slot values.\\n\\nFor doctors, Inquire accounts for 87.5% of the total utterances, followed by Salutations (5.0%), Chit-chat (4.8%), Diagnose (1.9%), and Other (0.8%). For patients, Inform (92.2%) is the dominant slot, followed by Chit-chat (4.2%) and Salutations (3.6%). Similarly, 56% of the utterances discuss Symptom followed by Patient Medical History (13.5%) and Habits (7.5%).\\n\\nDialogues in MediTOD are highly systematic.\\n\\nTo elucidate that, we divide each dialogue into ten equal segments. For each segment, we find the distribution of the slots across all dialogues. Figure 4 shows the resultant heatmap. During the first half of their conversation, the doctor and the patient primarily discuss symptoms. Then, they transition into other slots, such as patient medical history, habits, medication, etc.\\n\\n4 Experimental Setup\\n\\nMediTOD supports all three subtasks in a TOD system \u2013 natural language understanding (NLU), policy learning (POL), and natural language generation (NLG). Table 11 (appendix) illustrates representative examples from each subtask.\\n\\nNLU involves understanding the information presented in the latest patient utterance. Specifically, given the dialogue so far, an NLU model predicts the intent, active slots, and their attributes of the patient utterance. This updates the dialogue state, i.e., the aggregate information the patient reports up to the current turn in the dialogue.\\n\\nPOL requires predicting the doctor's next action. An action consists of intents, slots, and associated attributes. A POL model inputs the dialogue history and the dialogue state and predicts the action.\\n\\nFinally, NLG involves transforming the doctor's action into natural language. An NLG model predicts the doctor's utterance based on the dialogue history and the action from the POL model.\\n\\n4.1 Evaluation Metrics\\n\\nFor NLU, we compute an F1 score by matching intent, slots, and associated attributes from gold and predicted labels. Before matching, we unroll the gold and predicted NLU labels into sets of the form \\\\(\\\\{(\\\\text{intent}, \\\\text{slot}, \\\\text{value}, \\\\text{attribute}, \\\\text{attribute-value})\\\\}\\\\). For instance, the NLU label in Figure 1 is transformed to \\\\([(\\\\text{inform}, \\\\text{positive symptom}, \\\\text{pharyngitis}), (\\\\text{inform}, \\\\text{positive symptom}, \\\\text{pharyngitis, onset, past four days}), (\\\\text{inform}, \\\\text{positive symptom}, \\\\text{fever}), (\\\\text{inform}, \\\\text{positive symptom}, \\\\text{fever, onset, last two days})]\\\\).\\n\\nIn MediTOD, medical attributes are canonicalized, so we simply use exact match scores for them. However, for non-medical attributes, where different strings can have the same meaning, just string matching is too conservative. E.g., \u20182 days ago\u2019 and \u2018two days before\u2019 convey the same meaning, but will get counted as non-matches. For non-medical attributes, we use ChatGPT (OpenAI, 2024a) to adjudicate semantic equivalence and use that for F1 score computation. Our prompt is in Appendix D.\\n\\nIn experiments, we report medical and non-medical scores separately, in addition to overall scores.\\n\\nFor POL, we could use precision, recall, and F1 scores, similar to NLU; however, in conversation, it is quite possible for the doctor to change the order of questions somewhat. To account for this, we use the Precision@K metric. At a given test turn, we check if the medical attributes predicted from the POL model are present in the gold actions within the next K turns. We report Precision@K for K = 1, 4, 8, and infinity. Finally, for NLG, we use BLEU (Papineni et al., 2002), Rouge (Lin, 2004), and BERTScore (Zhang et al.) to measure the generation quality.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Baselines\\n\\nWe model all three tasks as seq2seq learning and evaluate baselines in the supervised and in-context learning settings. Baselines use a respiratory (in-domain) dataset for development and are evaluated using both an in-domain (respiratory) test set and an out-of-domain (musculoskeletal) test set.\\n\\n**Supervised.** We fine-tune several pre-trained language models on the MediTOD training set for the three tasks. We utilize the PPTOD (base) (Su et al., 2022) and Flan-T5 (base) (Chung et al., 2024) encoder-decoder transformer models, pre-trained on general domain TOD tasks and the Flan suite of tasks, respectively. Additionally, we employ BioGPT (medium) (Luo et al., 2022), a decoder-only transformer model pre-trained on extensive biomedical literature. Finally, we fine-tune Llama3 8B Instruct (AI@Meta, 2024) (referred to as Llama3 henceforth) and OpenBioLLM 8B (Ankit Pal, 2024) models, which serve as representatives of large language models (LLMs). After fine-tuning, we evaluate the models on the MediTOD test set.\\n\\n**In-context Learning.** We prompt several large language models (LLMs) to make predictions on test samples across the three tasks. For each test sample, we select the top five exemplars from the training set whose dialogue histories are semantically closest to the test sample (Liu et al., 2022). To identify the top exemplars, we use the BAAI/bge-large-en-v1.5 model (Xiao et al., 2023) to encode the dialogue history and perform a maximum inner product search. Sample prompts used in our experiments are provided in Appendix F. We evaluate the performance of Llama3 8B/70B Instruct (AI@Meta, 2024), OpenBioLLM 8B/70B (Ankit Pal, 2024), ChatGPT (gpt-3.5-turbo-0125) (OpenAI, 2024a), and GPT-4 (gpt-4-1106-preview) and GPT-4-Turbo (gpt-4-turbo-2024-04-09) (OpenAI, 2024b).\\n\\n4.3 Implementation Details\\n\\nWe adapted a publicly available codebase for the PPTOD model to MediTOD. For training this model, we utilized a learning rate of $1 \\\\times 10^{-3}$ and a batch size of 64. For the Flan-T5 and BioGPT models, we employed a learning rate of $1 \\\\times 10^{-4}$ and a batch size of 16. These models were trained on a single V100 GPU with 32 GB of memory. We train Llama3 with Unsloth to enhance the efficiency of both training and inference. We fine-tuned the Llama3 using LoRA (Hu et al., 2021) and use parameters $r = 32$ and $\\\\alpha = 32$, a learning rate of $1 \\\\times 10^{-4}$, a batch size of 16, and a cosine learning rate scheduler with a warm-up period of 10% for both models. This training was performed on two A100 40GB GPUs, taking approximately five hours to complete. We trained OpenBioLLM models under similar settings. During inference, we employ greedy decoding (temp. $\\\\delta = 0$) for all in-context and supervised baselines.\\n\\n5 Results\\n\\nWe begin by evaluating the models for the NLU, POL, and NLG tasks on the in-domain test set. Next, we assess the top-performing models from the in-domain evaluation on the out-of-domain dataset.\\n\\n### 5.1 Natural Language Understanding\\n\\nTable 4 compares the performance of various baselines on an NLU task. At a high level, Llama3 8B achieves the best overall performance among the supervised models. This superior performance can be attributed to its ability to recognize medical and non-medical slots better than its competitors. While most models demonstrate competitive performance in recognizing non-medical attributes, the Llama3 model has a clear advantage for medical attributes, scoring 0.0249 points higher over the nearest Flan-T5 baseline.\\n\\n| Model               | F1 (Medical) | F1 (Non-Medical) | F1 (Overall) |\\n|---------------------|--------------|------------------|--------------|\\n| PPTOD (base)        | 0.6849       | 0.7268           | 0.5141       |\\n| Flan-T5 (base)      | 0.6887       | 0.7354           | 0.5062       |\\n| BioGPT              | 0.6090       | 0.6612           | 0.4187       |\\n| OpenBioLLM 8B      | 0.6731       | 0.7294           | 0.4791       |\\n| Llama3 8B          | 0.7139       | 0.7603           | 0.5397       |\\n| OpenBioLLM 70B     |              |                  |              |\\n| Llama3 70B         |              |                  |              |\\n| ChatGPT            | 0.5929       | 0.6337           | 0.4425       |\\n| GPT-4              | 0.6351       | 0.6715           | 0.5043       |\\n| GPT-4-Turbo        | 0.6641       | 0.6999           | 0.5329       |\\n\\nTable 4: In-Domain Model Evaluation for the MediTOD NLU Task\"}"}
{"id": "emnlp-2024-main-936", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Model          | F1  | F2  | F3  |\\n|---------------|-----|-----|-----|\\n| Supervised    |     |     |     |\\n| PPTOD (base)  | 0.2099 | 0.2101 | 0.2069 |\\n| Flan-T5 (base) | 0.1999 | 0.2033 | 0.1495 |\\n| BioGPT        | 0.1870 | 0.1853 | 0.2156 |\\n| OpenBioLLM 8B | 0.2085 | 0.2172 | 0.1190 |\\n| Llama3 8B     | 0.2389 | 0.2392 | 0.2329 |\\n| In-context    |     |     |     |\\n| OpenBioLLM 8B | 0.1052 | 0.1086 | 0.0336 |\\n| OpenBioLLM 70B | 0.1214 | 0.1273 | 0.0247 |\\n| Llama3 8B     | 0.1072 | 0.1116 | 0.0160 |\\n| Llama3 70B    | 0.1000 | 0.1042 | 0.0154 |\\n| ChatGPT       | 0.1099 | 0.1124 | 0.0536 |\\n| GPT-4         | 0.0904 | 0.0937 | 0.0167 |\\n| GPT-4-Turbo   | 0.1296 | 0.1346 | 0.0172 |\\n\\nTable 5: In-domain model performance on the MediTOD POL task evaluated using the F1 scores.\\n\\nIn the in-context learning setting, pre-training on biomedical corpus offers a clear advantage, with OpenBioLLM significantly outperforming Llama3 models from the same weight class. Specifically, OpenBioLLM 8B and OpenBioLLM 70B surpass their Llama3 counterparts by 0.0805 and 0.0354 pts, respectively. OpenBioLLM showcases a superior understanding of medical attributes, compared to the general-purpose Llama models. OpenBioLLM 70B exhibits a slight edge of 0.0174 points over ChatGPT. However, GPT-4-Turbo achieves the best in-context performance overall. While GPT-4-Turbo's non-medical F1 is comparable to supervised models, its performance on medical attributes lags behind the supervised Llama3 model by 0.0573 pts. This highlights the potential for further improvements in in-context learning baselines, particularly in the medical domain.\\n\\nTables 14 and 15 display example responses from different baseline models. In table 14, the models struggle with distinguishing between related but distinct medical concepts, such as confusion and mental fatigue. Extracting multiple slots and attributes also presents a challenge for the models. As shown in table 15, the models either fail to recognize all the symptoms from the input or make errors when linking the attributes.\\n\\n5.2 Policy Learning\\n\\nTables 5 and 6 present the performance of various models on the POL task. Unlike NLU, where performance can vary widely, all models demonstrate competitive results in the supervised setting. The Llama3 8B models are the top performers, with Llama3 gaining 0.0069 points over the other baselines. A similar trend is observed with the Precision@K measure, where the Llama3 model maintains a slight advantage over its competitors across different values of K.\\n\\nIn the in-context learning setting, all baseline models show similar results, with the GPT-4-Turbo model achieving the highest score. However, for the Precision@K measure at K=4, ChatGPT surpasses the GPT-4-Turbo model. Interestingly, ChatGPT performs better than GPT-4 in this task. Upon careful study, we found that GPT-4 generates responses that violate the CMAS label structure in 20.75% cases. In contrast, ChatGPT and GPT-4-Turbo make such errors in only 3.31% and 7.07%.\\n\\nNotably, the overall performance of in-context baselines is significantly lower than that of supervised models. This disparity arises because policy learning inherently requires models to plan ahead. Supervised models, which learn policy directly from the data, thus have an advantage.\\n\\nEven though all models behave similarly, the raw scores are not very high, suggesting that more research is needed to improve this component.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Out-of-domain model evaluation on MediTOD NLU task.\\n\\n5.3 Natural Language Generation\\n\\nTable 7 reports NLG results. In the supervised setting, Llama3 8B and OpenBioLLM 8B surpass other baselines, with Llama3 8B achieving the best performance across most of the metrics. In the in-context setting, the OpenBioLLM 70B emerges as a clear winner. Interestingly, few-shot models perform very competitively with supervised models on the BERTScore metric. This suggests that their responses are semantically similar to the gold standard, even if they differ lexically.\\n\\nWe analyze responses from the OpenBioLLM 8B supervised model to identify its shortcomings. The model performs well when asking patients for information, effectively using natural phrases like \u201cOk, and...\u201d to convey understanding. However, it has difficulty converting multiple actions into natural language, particularly towards the end of conversations. It generates repetitive strings when the doctor discusses possible diagnoses and necessary medical tests or provides support. In contrast, ChatGPT and GPT-4 fare well in such cases.\\n\\n5.4 Out-of-domain Evaluation\\n\\nWe tested the performance of leading supervised models (OpenBioLLM 8B, Llama3 8B) and in-context models (ChatGPT, GPT-4-Turbo) on musculoskeletal dialogues in MediTOD. The results for NLU, POL, and NLG tasks are shown in Tables 8, 9, and 10. For NLU and NLG tasks, the models performed worse on out-of-domain data compared to in-domain data. This is because musculoskeletal dialogues use medical terms that are different from those in the respiratory domain. However, for the POL task, models maintained their in-domain performance, even though it was still low. This suggests that more research is needed to close the performance gap for NLU and NLG tasks and to improve POL task performance overall.\\n\\nTable 9: Out-of-domain model evaluation on MediTOD POL task.\\n\\nModel BLEU BLEU ROUGE ROUGE BERT2 4 1 L Score\\n\\nSupervised OpenBioLLM 8B 25.1 13.9 0.486 0.449 0.907\\nLlama3 8B 26.7 13.9 0.469 0.429 0.904\\nIn-context ChatGPT 17.8 7.0 0.340 0.297 0.893\\nGPT-4-Turbo 18.9 7.0 0.342 0.295 0.892\\n\\nTable 10: Out-of-domain model evaluation on MediTOD NLG task.\\n\\n6 Conclusion and Future Works\\n\\nIn this work, we introduced MediTOD, a novel English dataset of doctor-patient dialogues for collecting patient medical history. Unlike existing medical datasets, MediTOD uses a novel schema (CMAS) to capture attributes such as the onset and duration of symptoms relevant for downstream diagnosis. Further, we link values for medical attributes in MediTOD labels to their precise medical concepts within UMLS vocabularies. Finally, we propose new benchmarks for NLU, POL, and NLG tasks in the medical dialogue domain. Our initial experiments with baseline models reveal the challenges inherent in these tasks and underscore the potential for improvement.\\n\\nFurthermore, MediTOD facilitates the exploration of additional research settings such as Knowledge Grounded TOD. Canonicalization in MediTOD allows for seamless integration of UMLS vocabularies and Semantic networks into TOD settings, potentially enhancing performance. MediTOD also opens doors to research in medical dialogue summarization, offering opportunities to distill complex dialogues into concise medical summaries. Our annotation portal can aid in creating large-scale medical dialogue datasets for medical specialties beyond pulmonology and musculoskeletal. This expansion will broaden the applicability and relevance of MediTOD in medical dialogue research. We release the MediTOD resources at https://github.com/dair-iitd/MediTOD.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we introduce the MediTOD dataset, which consists of doctor-patient dialogues aimed at gathering patient medical information. This section scrutinizes our data annotation process, as outlined in section 3, from an ethical perspective.\\n\\nRegarding data sourcing, we thank Fareez et al. (2022) for generously providing their data under the Creative Commons CC0 license. The source dialogues in our dataset are simulated interviews conducted by medical professionals, which portray both doctor and patient roles. It's important to note that no actual patient information is disclosed within these dialogues.\\n\\nWe use a professional annotation service specializing in medical data solutions. The provider has been in business for ten years. The labeling process for our dataset involved employing six annotators under the guidance of a doctor. The annotators have backgrounds in medical sciences or pharmacy, ensuring a high level of expertise. The doctor has a professional medical degree and two and a half years of hospital experience. Before we started the labeling process, we declared that this work was for scientific advancement and, thus, would be released for public consumption. Each annotator is paid 8 USD per hour, which is above the average wage of data annotators in our country.\\n\\nWe utilize the UMLS Metathesaurus to standardize medical slot values in MediTOD, employing the QuickUMLS software. The National Library of Medicine, Department of Health and Human Services (NLM), grants the UMLS vocabulary license free of charge, which we have obtained for our work. We are committed to properly attributing UMLS and meeting their licensing requirements upon the public release of our dataset.\\n\\nWhile releasing MediTOD, we acknowledge its significance in advancing medical dialogue systems. However, it\u2019s crucial to emphasize that this data is intended solely for research purposes. We strongly advise against its use in real-life patient consultations or activities that could potentially endanger patients\u2019 well-being. Finally, through our work, we want to develop systems for assisting doctors in their work and reducing their burnout. However, we do not claim such a system can work independently without any oversight of healthcare providers.\\n\\nWhile MediTOD makes a meaningful contribution to the medical dialogue community through detailed canonical annotations, it\u2019s essential to acknowledge its limitations. Primarily, MediTOD focuses solely on dialogues from the fields of pulmonology and musculoskeletal. This restricted scope might limit its applicability across other medical specialties. Nonetheless, we\u2019re optimistic that our methodology can be adapted to annotate dialogues from different medical fields. Additionally, the dialogues annotated in MediTOD are exclusively in English. This linguistic limitation may restrict access to the non-English speaking portion of the population. However, we recognize the importance of inclusivity and are open to exploring ways to address language barriers to broaden the reach of our work.\\n\\nThis work is supported by IBM AI Horizons Network grant, grants by Google, Verisk, and Microsoft, an IBM SUR award and the Jai Gupta chair fellowship by IIT Delhi. Vishal is supported by a Google Fellowship. We thank the IIT Delhi HPC facility for its computational resources. We are grateful to Microsoft AFMR for supporting this work.\\n\\nAsma Ben Abacha, Wen-wai Yim, Yadan Fan, and Thomas Lin. 2023. An empirical study of clinical note generation from doctor-patient encounters. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2291\u20132302.\\n\\nAI@Meta. 2024. Llama 3 model card.\\n\\nMalaikannan Sankarasubbu Ankit Pal. 2024. Openbiollms: Advancing open-source large language models for healthcare and life sciences. https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B.\\n\\nNamo Bang, Jeehyun Lee, and Myoung-Wan Koo. 2023. Task-optimized adapters for an end-to-end task-oriented dialogue system. In Findings of the Association for Computational Linguistics: ACL 2023, pages 7355\u20137369.\\n\\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gasic. 2018. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 16852.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-936", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in bioinformatics, 23(6):bbac409.\\n\\nOpenAI. 2024a. Gpt-3.5-turbo. Available from https://platform.openai.com/docs/models.\\n\\nOpenAI. 2024b. Gpt-4-1106-preview. Available from https://platform.openai.com/docs/models.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8689\u20138696.\\n\\nRobin Rojowiec, Benjamin Roth, and Maximilian Fink. 2020. Intent recognition in doctor-patient interviews. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 702\u2013709.\\n\\nXiaoming Shi, Haifeng Hu, Wanxiang Che, Zhongqian Sun, Ting Liu, and Junzhou Huang. 2020. Understanding medical conversations with scattered keyword attention and weak supervision from responses. In AAAI Conference on Artificial Intelligence.\\n\\nYixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta, Deng Cai, Yi-An Lai, and Yi Zhang. 2022. Multi-task pre-training for plug-and-play task-oriented dialogue system. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4661\u20134676.\\n\\nHaipeng Sun, Junwei Bao, Youzheng Wu, and Xiaodong He. 2023. Mars: Modeling context & state representations with contrastive learning for end-to-end task-oriented dialog. In Findings of the Association for Computational Linguistics: ACL 2023, pages 11139\u201311160.\\n\\nArs\u00e8ne Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. 2022. Ddxplus: A new dataset for automatic medical diagnosis. In Neural Information Processing Systems.\\n\\nMina Valizadeh and Natalie Parde. 2022. The ai doctor is in: A survey of task-oriented dialogue systems for healthcare applications. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6638\u20136660.\\n\\nJunda Wang, Zonghai Yao, Zhichao Yang, Huixue Zhou, Rumeng Li, Xun Wang, Yucheng Xu, and Hong Yu. 2023. Notechat: A dataset of synthetic doctor-patient conversations conditioned on clinical notes. arXiv preprint arXiv:2310.15959.\\n\\nZhongyu Wei, Qianlong Liu, Baolin Peng, Huaixiao Tou, Ting Chen, Xuanjing Huang, Kam-Fai Wong, and Xiangying Dai. 2018. Task-oriented dialogue system for automatic diagnosis. In Annual Meeting of the Association for Computational Linguistics.\\n\\nTsung-Hsien Wen, David Vandyke, Nikola Mrk\u0161i\u0107, Milica Gasic, Lina M Rojas Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017. A network-based end-to-end trainable task-oriented dialogue system. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 438\u2013449.\\n\\nQingyang Wu, James Gung, Raphael Shu, and Yi Zhang. 2023. Diacttod: Learning generalizable latent dialogue acts for controllable task-oriented dialogue systems. In Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 255\u2013267.\\n\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597.\\n\\nGuojun Yan, Jiahuan Pei, Pengjie Ren, Zhaochun Ren, Xin Xin, Huasheng Liang, M. de Rijke, and Zhumin Chen. 2021. Remedi: Resources for multi-domain, multi-service, medical dialogues. Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval.\\n\\nSteve Young, Milica GASIC, Blaise Thomson, and Jaanson D Williams. 2013. Pomdp-based statistical spoken dialog systems: A review. Proceedings of the IEEE, 101(5):1160\u20131179.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.\\n\\nYuanzhe Zhang, Zhongtao Jiang, Tao Zhang, Shiwan Liu, Jiarun Cao, Kang Liu, Shengping Liu, and Jun Zhao. 2020. Mie: A medical information extractor towards medical dialogues. In Annual Meeting of the Association for Computational Linguistics.\\n\\nA MediTOD Tasks Examples\\nB Additional Results\\n\\nTable 12 and Table 13 present precision, recall and F1 for NLU and POL tasks, respectively.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Doctor: What brings you in here today?\\n\\nPatient: Um, I'm just, I'm here because I've had this cough for the past two weeks and uh, it's just not going away.\\n\\nDoctor: Okay, and um, is it getting worse at all really?\\n\\nPatient: Not really, it's just been the same.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model       | Overall | Medical Attributes | Non-medical Attributes |\\n|-------------|---------|--------------------|------------------------|\\n|             | Precision | Recall | F1   | Precision | Recall | F1   | Precision | Recall | F1   |\\n| **Supervised** |          |        |     |          |        |     |          |        |     |\\n| PPTOD (base) | 0.7205   | 0.6528  | 0.6849 | 0.7513   | 0.7039  | 0.7268 | 0.5827 | 0.4599  | 0.5141 |\\n| Flan-T5 (base) | 0.7049   | 0.6733  | 0.6887 | 0.7468   | 0.7244  | 0.7354 | 0.5347 | 0.4807  | 0.5062 |\\n| BioGPT      | 0.5968   | 0.6217  | 0.6090 |          |        |     |          |        |     |\\n| OpenBioLLM 8B | 0.6774   | 0.6689  | 0.6731 | 0.7488   | 0.7110  | 0.7294 | 0.4514 | 0.5104  | 0.4791 |\\n| Llama3 8B   | 0.7230   | 0.7050  | 0.7139 |          |        |     |          |        |     |\\n| **In-context** |          |        |     |          |        |     |          |        |     |\\n| OpenBioLLM 8B | 0.6654   | 0.5557  | 0.6056 | 0.6826   | 0.6079  | 0.6431 | 0.5735 | 0.3591  | 0.4416 |\\n| OpenBioLLM 70B | 0.5977   | 0.6208  | 0.6090 | 0.6476   | 0.6522  | 0.6499 | 0.4348 | 0.5030  | 0.4664 |\\n| Llama3 8B   | 0.5208   | 0.5296  | 0.5251 | 0.5714   | 0.5512  | 0.5611 | 0.3692 | 0.4481  | 0.4048 |\\n| Llama3 70B  | 0.5497   | 0.5996  | 0.5736 | 0.6109   | 0.6191  | 0.6150 | 0.3812 | 0.5266  | 0.4422 |\\n| ChatGPT     | 0.5843   | 0.6017  | 0.5929 | 0.6273   | 0.6402  | 0.6337 | 0.4290 | 0.4570  | 0.4425 |\\n| GPT-4       | 0.6451   | 0.6254  | 0.6351 | 0.6896   | 0.6543  | 0.6715 | 0.4929 | 0.5163  | 0.5043 |\\n| GPT-4-Turbo | 0.6861   | 0.6434  | 0.6641 | 0.7279   | 0.6740  | 0.6999 | 0.5378 | 0.5282  | 0.5329 |\\n\\nTable 12: Model performance on MediTOD NLU task.\\n\\n| Model       | Overall | Medical Attributes | Non-medical Attributes |\\n|-------------|---------|--------------------|------------------------|\\n|             | Precision | Recall | F1   | Precision | Recall | F1   | Precision | Recall | F1   |\\n| **Supervised** |          |        |     |          |        |     |          |        |     |\\n| PPTOD (base) | 0.2243   | 0.1973  | 0.2099 | 0.2240   | 0.1978  | 0.2101 | 0.2308 | 0.1875  | 0.2069 |\\n| Flan-T5 (base) | 0.1756   | 0.2322  | 0.1999 | 0.1797   | 0.2341  | 0.2033 | 0.1194 | 0.2000  | 0.1495 |\\n| BioGPT      | 0.1831   | 0.1911  | 0.1870 | 0.1816   | 0.1891  | 0.1853 | 0.2069 | 0.2250  | 0.2156 |\\n| OpenBioLLM 8B | 0.2132   | 0.2041  | 0.2085 | 0.2308   | 0.2051  | 0.2172 | 0.0872 | 0.1875  | 0.1190 |\\n| Llama3 8B   | 0.2460   | 0.2322  | 0.2389 | 0.2454   | 0.2333  | 0.2392 | 0.2576 | 0.2125  | 0.2329 |\\n| **In-context** |          |        |     |          |        |     |          |        |     |\\n| OpenBioLLM 8B | 0.1156   | 0.0966  | 0.1052 | 0.1177   | 0.1007  | 0.1086 | 0.0513 | 0.0250  | 0.0336 |\\n| OpenBioLLM 70B | 0.1275   | 0.1158  | 0.1214 | 0.1344   | 0.1210  | 0.1273 | 0.0244 | 0.0250  | 0.0247 |\\n| Llama3 8B   | 0.1164   | 0.0993  | 0.1072 | 0.1199   | 0.1043  | 0.1116 | 0.0222 | 0.0125  | 0.0160 |\\n| Llama3 70B  | 0.1088   | 0.0925  | 0.1000 | 0.1125   | 0.0971  | 0.1042 | 0.0200 | 0.0125  | 0.0154 |\\n| ChatGPT     | 0.1191   | 0.1021  | 0.1099 | 0.1198   | 0.1058  | 0.1124 | 0.0938 | 0.0375  | 0.0536 |\\n| GPT-4       | 0.0948   | 0.0863  | 0.0904 | 0.0970   | 0.0906  | 0.0937 | 0.0250 | 0.0125  | 0.0167 |\\n| GPT-4-Turbo | 0.1384   | 0.1219  | 0.1296 | 0.1416   | 0.1283  | 0.1346 | 0.0278 | 0.0125  | 0.0172 |\\n\\nTable 13: Model performance on MediTOD POL task.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Doctor: OK, um, and so you mentioned that you had a fever as well. Did you check your temperature?\\n\\nPatient: I didn't actually, no I didn't check my temperature, I just feel hot.\\n\\nDoctor: OK, we'll be sure to check that today in clinic. And have you been experiencing any other symptoms?\\n\\nPatient: Um, just like the brain fog that I was speaking of, you know. I just don't remember, I remember things, but it's like I'm doing everything underwater.\\n\\nTable 14: Supervised Llama3 8B and ChatGPT predict confusion as the active symptom instead of mental fatigue. GPT-4 errs in predicting fever from old utterances.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dialogue\\nHistory\\nDoctor: OK, so you're just having difficulties breathing for the past three-four days. Do you have any, uh, do you have a cough?\\nPatient: I did have a, uh, yeah, I've had a cough as well, over these last few days.\\nDoctor: And has it been a dry cough or wet cough?\\nPatient: I've been bringing up, some, like, uh, whitish sputum. It's like, yeah, whitish or clear.\\n\\nTable 15: Supervised Llama3 8B hallucinates positive_symptom_characteristics attribute. Further, its response includes inconsistent characteristics - wet and dry. ChatGPT and GPT-4 do not predict the excess/colored sputum symptom. They incorrectly link attribute color to cough.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"slots = [{\"slot\": \"symptom\",\\n\"description\": \"A symptom relevant to the patient's condition.\\n\\nrelated_attributes\": [\\n{\"name\": \"value\",\\n\"description\": \"The symptom in medical terms.\\n\\nexamples\": [\"coughing\", \"dyspnea\"]},\\n{\"name\": \"onset\",\\n\"description\": \"When did this symptom appear?\\n\\nexamples\": [\"three days ago\", \"one week back\"]},\\n{\"name\": \"initiation\",\\n\"description\": \"How did this symptom appear?\\n\\nexamples\": [\"abruptly\", \"gradually\"]},\\n{\"name\": \"location\",\\n\"description\": \"Where is the symptom located?\\n\\nexamples\": [\"back\", \"neck\"]},\\n{\"name\": \"duration\",\\n\"description\": \"How long does the symptom persist?\\n\\nexamples\": [\"a few minutes\", \"a few hours\"]},\\n{\"name\": \"severity\",\\n\"description\": \"What is the severity of this symptom on a scale of 10?\\n\\nexamples\": [\"4\", \"7\"]},\\n{\"name\": \"progression\",\\n\"description\": \"How is the symptom's progression?\\n\\nexamples\": [\"getting worse\", \"constant\"]},\\n{\"name\": \"frequency\",\\n\"description\": \"Frequency, if applicable, to the symptom.\\n\\nexamples\": [\"3-4 times a day\", \"every hour\"]},\\n{\"name\": \"positive_characteristics\",\\n\"description\": \"A characteristic positively associated with the symptom.\\n\\nexamples\": [\"sharp\", \"burning\"]},\\n{\"name\": \"negative_characteristics\",\\n\"description\": \"A characteristic not associated with the symptom.\\n\\nexamples\": [\"sharp\", \"burning\"]},\\n{\"name\": \"unknown_characteristics\",\\n\"description\": \"A characteristic with unknown relation with the symptom.\\n\\nexamples\": [\"sharp\", \"burning\"]},\\n{\"name\": \"alleviating_factor\",\\n\"description\": \"A condition that alleviates the symptom.\\n\\nexamples\": [\"laying down\", \"sleeping\"]},\\n{\"name\": \"not_alleviating_factor\",\\n\"description\": \"A condition that does not alleviate the symptom.\\n\\nexamples\": [\"laying down\", \"sleeping\"]},\\n{\"name\": \"aggravating_factor\",\\n\"description\": \"A condition that aggravates the symptom.\\n\\nexamples\": [\"laying down\", \"sleeping\"]},\\n{\"name\": \"not_aggravating_factor\",\\n\"description\": \"A condition that does not aggravate the symptom.\\n\\nexamples\": [\"laying down\", \"sleeping\"]},\\n{\"name\": \"not_alleviating_aggravating_factor\",\\n\"description\": \"A condition that neither alleviates nor aggravates the symptom.\\n\\nexamples\": [\"laying down\", \"sleeping\"]},\\n{\"name\": \"unknown_factor\",\\n\"description\": \"A condition with unknown alleviation/aggravation status.\\n\\nexamples\": [\"laying down\", \"sleeping\"]},\\n{\"name\": \"volume\",\\n\"description\": \"Volume, if applicable to the symptom.\\n\\nexamples\": [\"couple of teaspoons\"]},\\n{\"name\": \"color\",\\n\"description\": \"Color, if applicable to the symptom.\\n\\nexamples\": [\"ping\", \"red\"]},\\n{\"name\": \"itching\",\\n\"description\": \"How severe is the itching on a scale of 10?\\n\\nexamples\": [\"4\", \"7\"]},\\n{\"name\": \"lesion_size\",\\n\"description\": \"Is the lesion (or are the lesions) larger than 1cm (Yes/No)?\\n\\nexamples\": [\"Yes\", \"No\"]},\\n{\"name\": \"lesions_peel_off\",\\n\"description\": \"Do the lesions peel off (Yes/No)?\\n\\nexamples\": [\"Yes\", \"No\"]},\\n{\"name\": \"rash_swollen\",\\n\"description\": \"Is the rash swollen (Yes/No)?\\n\\nexamples\": [\"Yes\", \"No\"]},\\n]}],\\n\\n{\"slot\": \"medical_history\",\\n\"description\": \"A medical condition relevant to the patient's medical history.\\n\\nrelated_attributes\": [\\n{\"name\": \"value\",\\n\"description\": \"Name of the medical condition.\\n\\nexamples\": [\"hypertensive disease\", \"malignant neoplasm\"]},\\n{\"name\": \"starting\",\\n\"description\": \"When did the patient start to experience the condition?\\n\\nexamples\": [\"since teenage\", \"ten years ago\"]},\\n{\"name\": \"frequency\",\\n\"description\": \"How frequently does the patient experience the added condition?\\n\\nexamples\": [\"every year\", \"during summer\"]},\\n]}],\\n\\n{\"slot\": \"family_history\",\\n\"description\": \"A medical condition relevant to the patient's family.\\n\\nrelated_attributes\": [\\n{\"name\": \"value\",\\n\"description\": \"Name of the medical condition.\\n\\nexamples\": [\"hypertensive disease\", \"malignant neoplasm\"]},\\n{\"name\": \"relation\",\\n\"description\": \"Relationship with the patient.\\n\\nexamples\": [\"mother\", \"aunt\"]},\\n]}],\\n\\n{\"slot\": \"habit\",\\n\"description\": \"An habitual activity such as smoking, alcoholism, etc.\\n\\nrelated_attributes\": [\\n{\"name\": \"value\",\\n\"description\": \"Name of an activity.\\n\\nexamples\": [\"smoking\", \"marijuana\"]},\\n{\"name\": \"starting\",\\n\"description\": \"When did the patient pick up the activities?\\n\\nexamples\": [\"ten years back\", \"as a child\"]},\\n{\"name\": \"frequency\",\\n\"description\": \"How frequently does the patient engage in the selected activity?\\n\\nexamples\": [\"on weekends\", \"every day\"]},\\n]}],\\n\\n{\"slot\": \"exposure\",\\n\"description\": \"An environmental/chemical factor such as asbestos, pets, etc.\\n\\nrelated_attributes\": [\\n{\"name\": \"value\",\\n\"description\": \"Name of an environmental factor.\\n\\nexamples\": [\"pets\", \"dust\"]},\\n{\"name\": \"where\",\\n\"description\": \"Where was the patient exposed to the selected factor?\\n\\nexamples\": [\"work\", \"home\"]},\\n{\"name\": \"when\",\\n\"description\": \"When was the patient exposed to the selected factor?\\n\\nexamples\": [\"four days ago\"]},\\n]}],\\n\\n{\"slot\": \"medication\",\\n\"description\": \"A medication.\\n\\nrelated_attributes\": [\\n{\"name\": \"value\",\\n\"description\": \"Name of a medication.\\n\\nexamples\": [\"over-the-counter medicine\", \"paracetamol\"]},\\n{\"name\": \"start\",\\n\"description\": \"Since when did the patient start taking the medication?\\n\\nexamples\": [\"few weeks ago\", \"two days back\"]},\\n{\"name\": \"impact\",\\n\"description\": \"Did the medication help the patient (Yes/No/Maybe)?\\n\\nexamples\": [\"Yes\", \"No\", \"Maybe\"]},\\n{\"name\": \"respone_to\",\\n\"description\": \"For which condition/symptom is medication for?\\n\\nexamples\": [\"hypertensive disease\", \"diabetes\"]},\\n{\"name\": \"frequency\",\\n\"description\": \"How frequently does the patient take the medication?\\n\\nexamples\": [\"daily\"]},\\n]}],\\n\\n{\"slot\": \"medical_test\",\\n\"description\": \"A medical test.\\n\\nrelated_attributes\": [\\n{\"name\": \"value\",\\n\"description\": \"Name of a medical test.\\n\\nexamples\": [\"chest X-ray\", \"electrocardiogram\"]},\\n{\"name\": \"when\",\\n\"description\": \"When did the patient had the medical test done?\\n\\nexamples\": [\"yesterday\", \"a week ago\"]},\\n]}],\\n\\n{\"slot\": \"residence\",\\n\"description\": \"Information regarding patient's living conditions.\\n\\nrelated_attributes\": [\\n{\"name\": \"value\",\\n\"description\": \"Place where the patient resides.\\n\\nexamples\": [\"apartment\", \"old building\"]},\\n{\"name\": \"household_size\",\\n\"description\": \"Size of the patient's household.\\n\\nexamples\": [\"2\", \"4\"]},\\n]}],\\n\\n{\"slot\": \"occupation\",\\n\"description\": \"Information regarding the patient's occupation.\\n\\nrelated_attributes\": [\\n{\"name\": \"value\",\\n\"description\": \"Job/occupation of the patient.\\n\\nexamples\": [\"nurse\", \"student\"]},\\n{\"name\": \"exposure\",\\n\"description\": \"Are there any hazards/substances/dangers to which the patient got exposed at work?\\n\\nexamples\": [\"chemical fumes\", \"dust\"]},\\n]}],\\n\\n{\"slot\": \"travel\",\\n\"description\": \"Information regarding the patient's recent travels.\\n\\nrelated_attributes\": [\\n{\"name\": \"destination\",\\n\"description\": \"Where has the patient travelled to?\\n\\nexamples\": [\"canada\", \"united states\"]},\\n{\"name\": \"date\",\\n\"description\": \"When did the patient travel?\\n\\nexamples\": [\"last week\", \"a year ago\"]},\\n]}],\\n\\n{\"slot\": \"basic_information\",\\n\"description\": \"Basic information.\\n\\nrelated_attributes\": []}]}"}
{"id": "emnlp-2024-main-936", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Basic information about the patient.\\n\\nRelated attributes:\\n- **age**: Age of the patient.\\n- **gender**: Gender of the patient.\\n- **name**: Name of the patient.\\n\\nA medical condition.\\n\\nRelated attribute:\\n- **value**: Name of the medical condition.\\n\\nIMPORTANT INSTRUCTIONS:\\n1. Read the given definitions carefully.\\n2. For a given dialogue state and last turn, only some of the intents, slots and related attributes are applicable.\\n3. Related attribute `value` of the slots symptom, medical_history, family_history, habit, exposure, medication and medical_test must be a standard medical concept.\\n4. Make sure that the doctor's action is a continuation of the dialogue. Dialogue state is given as an additional context.\\n\\n**dialogue state**\\n```\\n{\\n  \\\"positive_symptom\\\": [\\n    {\\n      \\\"value\\\": \\\"pharyngitis\\\",\\n      \\\"onset\\\": \\\"past four days\\\"\\n    },\\n    {\\n      \\\"value\\\": \\\"fever\\\",\\n      \\\"onset\\\": \\\"last two days\\\",\\n      \\\"positive_characteristics\\\": [\\n        \\\"during the day\\\"\\n      ]\\n    },\\n    {\\n      \\\"value\\\": \\\"deglutition disorders\\\"\\n    },\\n    {\\n      \\\"value\\\": \\\"erythema\\\",\\n      \\\"location\\\": [\\n        \\\"pharyngeal structure\\\"\\n      ]\\n    },\\n    {\\n      \\\"value\\\": \\\"body substance discharge\\\",\\n      \\\"location\\\": [\\n        \\\"pharyngeal structure\\\"\\n      ],\\n      \\\"color\\\": \\\"whitish\\\"\\n    },\\n    {\\n      \\\"value\\\": \\\"swelling\\\",\\n      \\\"location\\\": [\\n        \\\"left lateral part of neck\\\",\\n        \\\"neck\\\",\\n        \\\"right lateral part of neck\\\"\\n      ]\\n    },\\n    {\\n      \\\"value\\\": \\\"lymphadenopathy\\\",\\n      \\\"location\\\": [\\n        \\\"left lateral part of neck\\\",\\n        \\\"neck\\\",\\n        \\\"right lateral part of neck\\\"\\n      ]\\n    },\\n    {\\n      \\\"value\\\": \\\"chills\\\"\\n    }\\n  ],\\n  \\\"negative_symptom\\\": [\\n    {\\n      \\\"value\\\": \\\"dysphonia\\\"\\n    },\\n    {\\n      \\\"value\\\": \\\"night sweats\\\"\\n    },\\n    {\\n      \\\"value\\\": \\\"headache\\\"\\n    }\\n  ],\\n  \\\"avail_medical_test\\\": [\\n    {\\n      \\\"value\\\": \\\"body temperature measurement\\\",\\n      \\\"when\\\": \\\"last night\\\"\\n    }\\n  ]\\n}\\n```\\n\\n**last turn**\\ndoctor: OK. Uhm, and have you had any headaches?\\npatient: No headaches.\\n\\n**output**\\n```\\n[{\\n  \\\"action\\\": \\\"inquire\\\",\\n  \\\"symptom\\\": [\\n    {\\n      \\\"value\\\": \\\"redness of eye\\\"\\n    }\\n  ]\\n}, {\\n  \\\"action\\\": \\\"inquire\\\",\\n  \\\"symptom\\\": [\\n    {\\n      \\\"value\\\": \\\"body substance discharge\\\",\\n      \\\"checks\\\": [\\n        {\\n          \\\"type\\\": \\\"location\\\",\\n          \\\"values\\\": [\\\"eye\\\"]\\n        }\\n      ]\\n    }\\n  ]\\n}]\\n```\\n\\n**Remaining Exemplars**\\n```\"}"}
{"id": "emnlp-2024-main-936", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Have you had any eye redness or eye discharge?\"}"}
{"id": "emnlp-2024-main-936", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Doctor: OK, that's good. And um travel to anywhere recently? Outside the province?\\nPatient: Uhm, outside of the province, no, not for like the last year at least.\\nDoctor: OK, OK. Um so those were kind of all the questions that I had for you. Did you have any questions for me?\\nPatient: Um yeah, I was just wondering like what you think it might be.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 18, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-936", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 17: MediTOD slots and associated attributes. Attributes marked with * are canonicalised.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dear Annotator,\\n\\nThank you for taking the time to help us with this annotation task. Your efforts are greatly appreciated, and your contribution will play a vital role in scientific progress. By proceeding with the annotations, you agree to the public release of the data collected during the process. This document will guide you through the UI and the ontology for the task.\\n\\nIntroduction to UI\\n\\nYou will annotate the dialogues using the special UI designed for the task. Within each session, you will load a dialogue between a doctor and a patient. In the dialogue, the doctor makes inquiries regarding the patient's symptoms, medical and family history, medication, habits, etc. For each utterance (the doctor's or the patient's) you will be presented with a questionnaire. You must fill out the questionnaire based on the utterance under consideration and the dialogue so far.\\n\\nRule of thumb: Ensure that the labels for utterances are diagnostically informative, enabling a doctor to make a diagnosis without reviewing the conversation.\\n\\nHow to open the UI?\\n\\nThe UI is a simple HTML + JavaScript application. Just open the index.html file from the source folder shared with you. You can use any modern browser you like. However, the tool has been tested extensively on Mozilla Firefox which is recommended.\\n\\nFigure 2 is a screenshot of the UI.\\n\\nThe UI consists of 5 sections as shown in the figure 2.\\n\\n1. Control Box \u2013 allows loading the dialogue JSON file for annotations, saving/loading the annotations JSON file.\\n2. dialogue Box \u2013 displays the utterances from the loaded dialogue file. You can navigate the utterances using mouse scrolls or up-down arrow keys. You can select an utterance for annotation by clicking on it or by pressing enter. It will load the questionnaire.\\n3. Questionnaire \u2013 contains questions which you must answer given the selected utterance and dialogue history so far.\\n4. Status Box \u2013 displays the labels for annotated utterances.\\n5. Tracking Box \u2013 displays keywords from symptoms, medical and family history. Keywords will be helpful for speeding up the labeling as you move along the utterances.\\n\\nHow to load a dialogue file?\\n\\n1. Click on the \\\"Browse. . . \\\" above import dialogue button, in the Control Box.\\n2. Locate and select the JSON file shared with you.\\n3. Click the \\\"Import dialogue\\\" button. dialogue box will now display the imported utterances.\\n\\nHow to add labels for an utterance?\\n\\n1. Select the utterance in the dialogue box. You can use the up-down arrow keys and the Enter key to select the utterance. Use the cross button next to the Submit button to deselect the utterance.\\n2. Questionnaire will now show a form which you must fill.\\n3. Select the appropriate \\\"intent\\\" (defined below) from the drop-down. You can hover over each intent to see the details.\\n4. Select the appropriate \\\"slot type\\\" (defined below) from the drop-down. You can hover over each slot type to see the details. Based on your selection additional questions will be shown.\\n5. You must decide on which questions are relevant for the given utterance and answer them. Answering requires you to choose an option from a drop-down menu or type answers into a text box. You may provide multiple answers in the text box, separating them with commas.\\n6. Click on the submit button to add the labels. Status box will now show the added labels in the JSON format. Make sure you add labels for all the utterances in the dialogue.\\n\\nHow to save the labels?\\n\\n1. Once you finish adding labels for all the utterances, click on the \\\"Save\\\" button in the Control Box.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Provide an appropriate file name. For example, if the dialogue file has the name \\\"ABC,\\\" then you can name the label file \\\"ABC_annotations.\\\"\\n\\nHow can I edit the labels for an utterance?\\n1. Go to the Status Box and find the label that you want to change. Click on the associated \\\"X\\\" button to remove the annotation.\\n2. Re-add the annotation for the utterance as discussed before.\\n\\nCan I view and edit labels from a saved JSON?\\n1. From the Control Box, first load the dialogue as before.\\n2. Click on \\\"Browse\\\" above the Load Annotation button. Select the appropriate JSON labels file.\\n3. Click on the \\\"Load Annotations\\\" button. You will see annotations loaded in the Status Box.\\n\\nTask Ontology\\nIn this section, we describe the overall ontology of the task. In addition, we will also detail out with examples how to label each slot type in the ontology. For each utterance, you must\\n1. Decide an appropriate intent.\\n2. Decide an appropriate slot type.\\n3. Fill out the questionnaire corresponding to the intent-slot type pair.\\n\\nThe UI will automatically display the questionnaire (if any) once you select the intent and slot type.\\n\\nNote:\\nAn utterance can have more than one intent-slot value pair. You must fill out a questionnaire for each pair. The UI allows this by re-selecting the utterance.\\n\\nNote:\\nIn some dialogues, the patient is an infant (or is unable to communicate) and is accompanied by its guardian (like its mother). In such cases patient responses are actually uttered by the guardian. However, you must label the utterance from the perspective of the patient.\\n\\nExample: \\\"Patient: Timmy is my son. He has been running a high fever.\\\" should be labelled appropriately as a patient is having a fever.\\n\\nIntents\\nAn intent represents the underlying purpose or meaning behind a speaker's statement in a dialogue, whether it's the doctor or a patient. The following intent labels are available:\\n\\n1. Inform: When the speaker aims to provide specific information, such as symptoms or medical history. This could be in response to an inquiry or spontaneously offered. Select this intent when \\\"specific information\\\" is required for an informed diagnosis.\\n2. Inquire: When the speaker seeks to gather specific information, such as symptoms or medical history. Choose this intent when \\\"specific information\\\" is necessary for an educated diagnosis.\\n3. Diagnosis: When the doctor is giving a diagnosis of a disease.\\n4. Salutations: When the speaker intends to convey a greeting or farewell message.\\n5. Chit-chat: When the speaker engages in casual conversation. The information in the utterance is unlikely to contribute to an educated diagnosis.\\n6. Nod_prompt: When the speaker is not providing any new information but is showing attention, understanding, or agreement through phrases like 'Okay,' 'Yeah,' and 'uh-huh.' We consider an utterance as nod_prompt when the speaker is either acknowledging something (like a patient when he/she understands a doctor's question) or prompting the listener for additional information (like when the doctor just says okay and patient continues the conversation).\\n7. Other: Any intent not covered by the above categories.\\n\\nEnsure that you consider all possible intents conveyed by the utterance when labeling them.\\n\\nSlots\\nSlots refer to specific pieces of information or variables that are extracted from an utterance in a dialogue.\\n\\nBasic Information\\nSlots in a dialogue capture specific details such as the patient's name, age, and sex. Examples are given in figure 6.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Symptom\\n\\nThe dialogue contains slots with details about a symptom experienced by the patient. These slots encompass the symptom's value (e.g., cough or fever) and additional information like its onset, nature, and more. The UI presents the following questions for symptoms.\\n\\n1. Enter symptoms \u2013 comma separated values\\n2. Is the patient currently suffering from the selected symptom(s)? \u2013 Yes/No\\n3. Where is the symptom(s) located? \u2013 Part of the body affected by the symptom\\n4. When did this symptom(s) appear? \u2013 2 days ago, yesterday, etc\\n5. How did this symptom(s) appear (abruptly, gradually, etc.)? - Onset\\n6. How long does this symptom(s) last (few minutes, few days, etc.)?\\n7. What is the severity of this symptom(s) on scale of 10 (or an indication like can patient perform his/her day to day activity.)?\\n8. What are the characteristics of the symptom(s) (burning pain and dry cough etc.)?\\n9. What factors cause symptom(s) to improve or get worse (alleviating/aggravating factors like symptoms get worse with exercise)?\\n10. How is the symptom(s)'s progression over time (eg. cough has increased over time)?\\n11. Volume if applicable to the symptom (eg. amount of sputum)\\n12. Color if applicable to the symptom (eg. color of sputum)\\n13. Frequency if applicable to the symptom (eg. patient coughs 3-4 times a day)\\n14. Any additional information missing from the above fields\\n\\nExamples are given in figure 7.\\n\\nDermatological Symptom\\n\\nIn this case, the utterance comprises slots with information related to skin, nails, and hair symptoms. It includes the symptom's value (e.g., rash) and additional attributes like color, size, swelling, etc. The UI presents the following questions for dermatological symptoms.\\n\\n1. Does the patient have any lesions, redness or problems on the skin?\\n2. Is the rash swollen?\\n3. Is the lesion (or are the lesions) larger than 1cm?\\n4. Do the lesions peel off?\\n5. How severe is the itching on scale of 10?\\n6. Where is the affected region located?\\n7. What color is the rash?\\n8. Any additional information missing from the above fields.\\n\\nExamples are given in figure 8.\\n\\nDisease\\n\\nThe doctor is diagnosing a disease in the utterance. The UI presents the following questions.\\n\\n1. Enter the disease(s) \u2013 comma separated list of diseases.\\n2. Does the patient have the disease(s)? \u2013 Yes/No/Maybe\\n\\nExamples are given in figure 9.\\n\\nExposure\\n\\nWithin the dialogue, there are slots containing details about situations in which the patient might be exposed to harmful conditions. This includes contact with allergic substances, dust, chemicals, or infected individuals. The UI presents the following questions.\\n\\n1. Select exposure factor(s) \u2013 comma separated values of exposure factors\\n2. Was the patient exposed to the selected exposure factor? \u2013 Yes/No/Maybe\\n3. When was the patient exposed to the above factor?\\n4. Where was the patient exposed to the above factor e.g. at work or at home?\\n5. Any additional information missing from the above fields\\n\\nExamples are given.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Examples are given in figure 10.\\n\\nHabit\\n\\nThe dialogue contains slots with information about the patient\u2019s habits or addictions. A habit refers to an activity the patient regularly engages in, ranging from daily exercise, tea, and coffee to smoking, alcoholism, and marijuana abuse. The UI presents the following questions.\\n\\n1. Select activity/activities \u2013 comma separated list\\n2. Has/had the patient formed a habit/addiction to the selected activities? \u2013 Yes/No/Maybe\\n3. How frequently does the patient engage in the activity?\\n4. When did the patient pick up the activities?\\n5. Enter any additional information missing from above fields.\\n\\nExamples are given in figure 11.\\n\\nNote: Patients tend to get embarrassed with questions like \u201cHow much alcohol do you take?\u201d or \u201cDo you smoke cigarettes\u201d? You must make your own judgment in such cases and decide whether the patient is addicted or not.\\n\\nMedication\\n\\nThe dialogue contains slots with details about medications, either specific ones like Tylenol or general ones like antipsychotic drugs. Additional information may include the purpose of the medication and the duration the patient has been taking it. The doctor may also communicate medication-related information to the patient. The UI presents the following questions.\\n\\n1. Enter medication \u2013 comma separated list\\n2. Medication Status \u2013 currently taking/took in the past/no\\n3. For which condition/symptom is medication for?\\n4. Since when did the patient start taking the medication?\\n5. How frequently does the patient take the medication?\\n6. Did the medication help the patient?\\n7. Any additional information missing from above fields.\\n\\nExamples are given in figure 12.\\n\\nMedical Test\\n\\nSlots in the utterance pertain to a medical test, such as ECG or CAT scan. The doctor might inquire about tests the patient has already undergone or advise the patient to undergo specific tests. The UI presents the following questions.\\n\\n1. Enter medical test\\n2. Does patient have the results for the medical test?\\n3. When did the patient have the medical test done?\\n4. Any additional information missing from above field.\\n\\nExamples are given in figure 13.\\n\\nMedical History\\n\\nThe dialogue includes slots that provide information about the patient\u2019s medical history. This may encompass descriptions of past symptoms, diseases, surgeries, or allergies experienced by the patient. It differs from the \u201cDisease\u201d slot as it describes previous medical conditions rather than ongoing symptoms. The UI presents the following questions.\\n\\n1. Add symptom/disease/surgery relevant to patient\u2019s medical history \u2013 comma separated list\\n2. Status of the above condition\\n   a) Patient still suffers from the condition\\n   b) Patient suffered from the condition in the past\\n   c) Patient did not suffer from the condition in the past\\n   d) Patient is not sure about the status of the condition\\n3. When did the patient start to experience the above condition?\\n4. How frequently does the patient experience the above condition?\\n5. Any additional information missing from the above fields\\n\\nExamples are given in figure 14.\\n\\nFamily History\\n\\nThe utterance contains slots with information about medical conditions prevalent in the patient\u2019s family. This includes diseases like asthma, heart issues, cancer, and others. The UI presents the following questions.\\n\\nExamples are given in figure 10.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Select medical condition(s) \u2013 comma separated list\\n2. Does anyone in the patient's family have the selected medical condition(s)? Yes/No/Maybe\\n3. Relationship with the patient e.g. mother or brother\\n4. Any additional information missing from the above fields\\n\\nExamples are given in figure 15.\\n\\nOccupation\\nWithin the dialogue, there are slots specifying the patient's occupation, such as teacher, trucker, factory worker, etc. The UI presents the following questions.\\n1. Add patient's occupation details like job sector or job title\\n2. Has the patient works/worked at the above occupation?\\n3. Are there any substances/dangers to which the patient is exposed at work?\\n4. Any additional information missing from the above fields.\\n\\nExamples are given in figure 16.\\n\\nResidence\\nSlots in the utterance contain details about the patient's residence, such as urban, rural, suburban, etc. The UI presents the following questions.\\n1. Add details for the patient's residence like urban/rural and apartment/house.\\n2. Status\\n3. Any additional information missing from the above fields.\\n\\nExamples are given in figure 17.\\n\\nTravel\\nThe dialogue includes slots with information about the patient's travel history. This may involve details like the time of travel, locations visited, and frequency of travel. The UI presents the following questions.\\n1. Has the patient travelled recently?\\n2. Where has the patient travelled to?\\n3. When did the patient travel?\\n4. How frequently does the patient travel?\\n5. Enter additional information about travel.\\n\\nExamples are given in figure 18.\\n\\nMedical Discussion\\nThe utterance is part of a chit-chat about a medical topic (e.g., pulmonary embolism). The slot values of this type are unlikely to contribute towards the diagnosis. The UI presents the following questions \u2013 \u201cWhat is the topic of the discussion?\u201d. Examples are given in figure 19.\\n\\nNon-Medical Discussion\\nThe utterance is part of a chit-chat about a non-medical topic (e.g., living conditions). The slot values of this type are unlikely to contribute towards the diagnosis. The UI presents the following questions \u2013 \u201cWhat is the topic of the discussion?\u201d.\\n\\nOther\\nThis slot accounts for any additional details present in the utterance beyond the ones mentioned above. The UI asks for the \u201cother\u201d information.\\n\\nYou must summarize it as succinctly as possible.\\n\\nExamples are given in figure 20.\\n\\nMiscellaneous\\nSpecial Cases\\nFollowing are some special cases (not all) which may frequently appear in the dialogues.\\n1. If the utterance indicates the number of people living with the patient, the slot type is residence and value is household size. For example, \u201cPatient: I live with my parents and my sister\u201d should be annotated as {\"intent\": \"inform\", \"slot\": \"residence\", \"household size\": \"4\"}.\\n2. For a case where the doctor asks \u201chave you experienced these symptoms before?\u201d, the slot type is medical history and value is past experience.\\n3. In case the patient is exposed to secondhand cigarette smoke (smoke from someone else\u2019s cigarette), the slot type is habit and value is secondhand cigarette.\\n4. It is preferred to use key-value format for the Other field in the questionnaire. For example, \u201cDoctor: OK. How has his behaviour been? Patient: He's been very, very fussy.\u201d should be annotated as {\"intent\": \u201cinquire\u201d, \u201cslot\u201d: \u201cother\u201d, \u201cother information\u201d: \u201cbehaviour\u201d}.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. For alcoholism (slot-type habit), the patient might say \u201cI usually drink a glass of wine on the weekends.\u201d You must rely on your medical knowledge to decide whether the patient is alcoholic or not. You may refer CAGE guidelines for alcoholism and annotate the utterance as {\"intent\": \\\"inform\\\", \\\"slot\\\": \\\"habit\\\", \\\"value\\\": \\\"alcoholism\\\", \\\"status\\\": \\\"No\\\", \\\"other\\\": \\\"criterion: CAGE\\\"}. Similarly, for smoking and other substance abuse.\\n\\n6. For the cases where the patient is an infant, the doctor asks questions to the mother like \u201cDid you have any complications during pregnancy?\u201d. Here, slot type is medical history and values can be typed-in. As discussed before, you must answer a questionnaire from the perspective of the patient.\"}"}
{"id": "emnlp-2024-main-936", "page_num": 26, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-936", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "emnlp-2024-main-936", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-936", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-936", "page_num": 30, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "emnlp-2024-main-936", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"F Prompts in In-context Setting\\n\\nF.1 NLU Prompt\\nYou are a professional medical scribe who is an expert in understanding doctor-patient dialogues. The user will show you a dialogue history between a doctor and a patient and the last turn in their dialogue. Your task is to identify the patient's intent, slots, and related attributes (if applicable) from the given dialogue history and the last turn. Definitions for intent, slots, and related attributes are given below as Python dictionaries.\\n\\n```python\\nintents = [\\n    {\\n        \\\"name\\\": \\\"inform\\\",\\n        \\\"description\\\": \\\"The patient is providing information to the doctor.\\\"\\n    },\\n    {\\n        \\\"name\\\": \\\"chit-chat\\\",\\n        \\\"description\\\": \\\"The patient is chit-chatting with the doctor.\\\"\\n    },\\n    {\\n        \\\"name\\\": \\\"nod_prompt_salutations\\\",\\n        \\\"description\\\": \\\"The patient is nodding to the doctor or delivering salutations.\\\"\\n    }\\n]\\n\\nslots = [\\n    {\\n        \\\"slot\\\": \\\"symptom\\\",\\n        \\\"description\\\": \\\"A symptom relevant to the patient\u2019s condition.\\n\\n        \\\"related_attributes\\\": [\\n            {\\n                \\\"name\\\": \\\"value\\\",\\n                \\\"description\\\": \\\"The symptom in medical terms.\\n                \\\"examples\\\": [\\\"coughing\\\", \\\"dyspnea\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"status\\\",\\n                \\\"description\\\": \\\"The status is \u2018positive\u2019 if the patient has the symptom currently or \u2018negative\u2019 if the patient does not have the symptom; otherwise, it is \u2018unknown.\u2019\\n                \\\"examples\\\": [\\\"positive\\\", \\\"negative\\\", \\\"unknown\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"onset\\\",\\n                \\\"description\\\": \\\"When did this symptom appear?\\n                \\\"examples\\\": [\\\"three days ago\\\", \\\"one week back\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"initiation\\\",\\n                \\\"description\\\": \\\"How did this symptom appear?\\n                \\\"examples\\\": [\\\"abruptly\\\", \\\"gradually\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"location\\\",\\n                \\\"description\\\": \\\"Where is the symptom located?\\n                \\\"examples\\\": [\\\"back\\\", \\\"neck\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"duration\\\",\\n                \\\"description\\\": \\\"How long does the symptom persist?\\n                \\\"examples\\\": [\\\"a few minutes\\\", \\\"a few hours\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"severity\\\",\\n                \\\"description\\\": \\\"What is the severity of this symptom on a scale of 10?\\n                \\\"examples\\\": [\\\"4\\\", \\\"7\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"progression\\\",\\n                \\\"description\\\": \\\"How is the symptom\u2019s progression?\\n                \\\"examples\\\": [\\\"getting worse\\\", \\\"constant\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"frequency\\\",\\n                \\\"description\\\": \\\"Frequency, if applicable, to the symptom.\\n                \\\"examples\\\": [\\\"3-4 times a day\\\", \\\"every hour\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"positive_characteristics\\\",\\n                \\\"description\\\": \\\"A characteristic positively associated with the symptom.\\n                \\\"examples\\\": [\\\"sharp\\\", \\\"burning\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"negative_characteristics\\\",\\n                \\\"description\\\": \\\"A characteristic not associated with the symptom.\\n                \\\"examples\\\": [\\\"sharp\\\", \\\"burning\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"unknown_characteristics\\\",\\n                \\\"description\\\": \\\"A characteristic with unknown relation with the symptom.\\n                \\\"examples\\\": [\\\"sharp\\\", \\\"burning\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"alleviating_factor\\\",\\n                \\\"description\\\": \\\"A condition that alleviates the symptom.\\n                \\\"examples\\\": [\\\"laying down\\\", \\\"sleeping\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"not_alleviating_factor\\\",\\n                \\\"description\\\": \\\"A condition that does not alleviate the symptom.\\n                \\\"examples\\\": [\\\"laying down\\\", \\\"sleeping\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"aggravating_factor\\\",\\n                \\\"description\\\": \\\"A condition that aggravates the symptom.\\n                \\\"examples\\\": [\\\"laying down\\\", \\\"sleeping\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"not_aggravating_factor\\\",\\n                \\\"description\\\": \\\"A condition that does not aggravate the symptom.\\n                \\\"examples\\\": [\\\"laying down\\\", \\\"sleeping\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"not_alleviating_aggravating_factor\\\",\\n                \\\"description\\\": \\\"A condition that neither alleviates nor aggravates the symptom.\\n                \\\"examples\\\": [\\\"laying down\\\", \\\"sleeping\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"unknown_factor\\\",\\n                \\\"description\\\": \\\"A condition with unknown alleviation/aggravation status.\\n                \\\"examples\\\": [\\\"laying down\\\", \\\"sleeping\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"volume\\\",\\n                \\\"description\\\": \\\"Volume, if applicable to the symptom.\\n                \\\"examples\\\": [\\\"couple of teaspoons\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"color\\\",\\n                \\\"description\\\": \\\"Color, if applicable to the symptom.\\n                \\\"examples\\\": [\\\"ping\\\", \\\"red\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"itching\\\",\\n                \\\"description\\\": \\\"How severe is the itching on a scale of 10?\\n                \\\"examples\\\": [\\\"4\\\", \\\"7\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"lesion_size\\\",\\n                \\\"description\\\": \\\"Is the lesion (or are the lesions) larger than 1cm (Yes/No)?\\\"\\n            },\\n            {\\n                \\\"name\\\": \\\"lesions_peel_off\\\",\\n                \\\"description\\\": \\\"Do the lesions peel off (Yes/No)?\\\"\\n            },\\n            {\\n                \\\"name\\\": \\\"rash_swollen\\\",\\n                \\\"description\\\": \\\"Is the rash swollen (Yes/No)?\\\"\\n            }\\n        ]\\n    },\\n    {\\n        \\\"slot\\\": \\\"medical_history\\\",\\n        \\\"description\\\": \\\"A medical condition relevant to the patient\u2019s medical history.\\n\\n        \\\"related_attributes\\\": [\\n            {\\n                \\\"name\\\": \\\"value\\\",\\n                \\\"description\\\": \\\"Name of the medical condition.\\n                \\\"examples\\\": [\\\"hypertensive disease\\\", \\\"malignant neoplasm\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"status\\\",\\n                \\\"description\\\": \\\"The status is \u2018positive\u2019 if the patient experienced the medical condition or \u2018negative\u2019 if the patient did not experience the medical condition; otherwise, it is \u2018unknown.\u2019\\n                \\\"examples\\\": [\\\"positive\\\", \\\"negative\\\", \\\"unknown\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"starting\\\",\\n                \\\"description\\\": \\\"When did the patient start to experience the condition?\\n                \\\"examples\\\": [\\\"since teenage\\\", \\\"ten years ago\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"frequency\\\",\\n                \\\"description\\\": \\\"How frequently does the patient experience the added condition?\\n                \\\"examples\\\": [\\\"every year\\\", \\\"during summer\\\"]\\n            }\\n        ]\\n    },\\n    {\\n        \\\"slot\\\": \\\"family_history\\\",\\n        \\\"description\\\": \\\"A medical condition relevant to the patient\u2019s family.\\n\\n        \\\"related_attributes\\\": [\\n            {\\n                \\\"name\\\": \\\"value\\\",\\n                \\\"description\\\": \\\"Name of the medical condition.\\n                \\\"examples\\\": [\\\"hypertensive disease\\\", \\\"malignant neoplasm\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"status\\\",\\n                \\\"description\\\": \\\"The status is \u2018positive\u2019 if someone in the patient\u2019s family suffered from the medical condition or \u2018negative\u2019 if no one in the patient\u2019s family suffered from the medical condition; otherwise, it is \u2018unknown.\u2019\\n                \\\"examples\\\": [\\\"positive\\\", \\\"negative\\\", \\\"unknown\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"relation\\\",\\n                \\\"description\\\": \\\"Relationship with the patient.\\n                \\\"examples\\\": [\\\"mother\\\", \\\"aunt\\\"]\\n            }\\n        ]\\n    },\\n    {\\n        \\\"slot\\\": \\\"habit\\\",\\n        \\\"description\\\": \\\"An habitual activity such as smoking, alcoholism, etc.\\n\\n        \\\"related_attributes\\\": [\\n            {\\n                \\\"name\\\": \\\"value\\\",\\n                \\\"description\\\": \\\"Name of an activity.\\n                \\\"examples\\\": [\\\"smoking\\\", \\\"marijuana\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"status\\\",\\n                \\\"description\\\": \\\"The status is \u2018positive\u2019 if the patient engages in the activity habitually or \u2018negative\u2019 if the patient does not engage in the activity habitually; otherwise, it is \u2018unknown.\u2019\\n                \\\"examples\\\": [\\\"positive\\\", \\\"negative\\\", \\\"unknown\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"starting\\\",\\n                \\\"description\\\": \\\"When did the patient pick up the activities?\\n                \\\"examples\\\": [\\\"ten years back\\\", \\\"as a child\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"frequency\\\",\\n                \\\"description\\\": \\\"How frequently does the patient engage in the selected activity?\\n                \\\"examples\\\": [\\\"on weekends\\\", \\\"every day\\\"]\\n            }\\n        ]\\n    },\\n    {\\n        \\\"slot\\\": \\\"exposure\\\",\\n        \\\"description\\\": \\\"An environmental/chemical factor such as asbestos, pets, etc.\\n\\n        \\\"related_attributes\\\": [\\n            {\\n                \\\"name\\\": \\\"value\\\",\\n                \\\"description\\\": \\\"Name of an environmental factor.\\n                \\\"examples\\\": [\\\"pets\\\", \\\"dust\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"status\\\",\\n                \\\"description\\\": \\\"The status is \u2018positive\u2019 if the patient was exposed to the factor or \u2018negative\u2019 if the patient was not exposed; otherwise, it is \u2018unknown.\u2019\\n                \\\"examples\\\": [\\\"positive\\\", \\\"negative\\\", \\\"unknown\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"where\\\",\\n                \\\"description\\\": \\\"Where was the patient exposed to the selected factor?\\n                \\\"examples\\\": [\\\"work\\\", \\\"home\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"when\\\",\\n                \\\"description\\\": \\\"When was the patient exposed to the selected factor?\\n                \\\"examples\\\": [\\\"four days ago\\\"]\\n            }\\n        ]\\n    },\\n    {\\n        \\\"slot\\\": \\\"medication\\\",\\n        \\\"description\\\": \\\"A medication.\\n\\n        \\\"related_attributes\\\": [\\n            {\\n                \\\"name\\\": \\\"value\\\",\\n                \\\"description\\\": \\\"Name of a medication.\\n                \\\"examples\\\": [\\\"over-the-counter medicine\\\", \\\"paracetamol\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"status\\\",\\n                \\\"description\\\": \\\"The status is \u2018positive\u2019 if the patient took the medicine or \u2018negative\u2019 if the patient did not take the medicine; otherwise, it is unknown.\\n                \\\"examples\\\": [\\\"positive\\\", \\\"negative\\\", \\\"unknown\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"start\\\",\\n                \\\"description\\\": \\\"Since when did the patient start taking the medication?\\n                \\\"examples\\\": [\\\"few weeks ago\\\", \\\"two days back\\\"]\\n            },\\n            {\\n                \\\"name\\\": \\\"impact\\\",\\n                \\\"description\\\": \\\"Did the medication help the patient (Yes/No/Maybe)?\\n                \\\"examples\\\": [\\\"Yes\\\", \\\"No\\\", \\\"Maybe\\\"]\\n            }\\n        ]\\n    }\\n]\\n```\"}"}
{"id": "emnlp-2024-main-936", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IMPORTANT INSTRUCTIONS:\\n1. Read the given definitions carefully.\\n2. For a given dialogue history and last turn, only some of the intents, slots and related attributes are applicable.\\n3. Related attribute 'value' of the the slots symptom, medical_history, family_history, habit, exposure, medication and medical_test must be a standard medical concept.\\n4. Expected output should contain intent, slot and related values from the last turn. dialogue history is given as an additional context.\"}"}
