{"id": "acl-2022-long-341", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nSocial media is a breeding ground for threat narratives and related conspiracy theories. In these, an outside group threatens the integrity of an inside group, leading to the emergence of sharply defined group identities: Insiders \u2013 agents with whom the authors identify and Outsiders \u2013 agents who threaten the insiders. Inferring the members of these groups constitutes a challenging new NLP task: (i) Information is distributed over many poorly-constructed posts; (ii) Threats and threat agents are highly contextual, with the same post potentially having multiple agents assigned to membership in either group; (iii) An agent's identity is often implicit and transitive; and (iv) Phrases used to imply Outsider status often do not follow common negative sentiment patterns. To address these challenges, we define a novel Insider-Outsider classification task. Because we are not aware of any appropriate existing datasets or attendant models, we introduce a labeled dataset (CT5K) and design a model (NP2IO) to address this task. NP2IO leverages pretrained language modeling to classify Insiders and Outsiders. NP2IO is shown to be robust, generalizing to noun phrases not seen during training, and exceeding the performance of non-trivial baseline models by 20%.\\n\\n1 Background and Motivation\\n\\nNarrative models \u2013 often succinctly represented as a network of characters, their roles, their interactions (syuzhet) and associated time-sequencing information (fabula) \u2013 have been a subject of considerable interest in computational linguistics and narrative theory. Stories rest on the generative backbone of narrative frameworks (Bailey, 1999; Beatty, 2016). While the details might vary from one story to another, this variation can be compressed into a limited set of domain-dependent narrative roles and functions (Dundes, 1962).\\n\\nSocial narratives that both directly and indirectly contribute to the construction of individual and group identities are an emergent phenomenon resulting from distributed social discourse. Currently, this phenomenon is most readily apparent on social media platforms, with their large piazzas and niche enclaves. Here, multiple threat-centric narratives emerge and, often, over time are linked together into complex conspiracy theories (Tangherlini et al., 2020). Conspiracy theories, and their constituent threat narratives (legend, rumor, personal experience narrative) share a signature semantic structure: an implicitly accepted Insider group; a diverse group of threatening Outsiders; specific threats from the Outsiders directed at the Insiders; details of how and why Outsiders are threatening; and a set of strategies proposed for the Insiders to counter these threats (Tangherlini, 2018). Indeed, the Insider/Outsider groups are fundamental in most studies of belief narrative, and have been exhaustively studied in social theory and more specifically, in the context of conspiracy theories (Bodner et al., 2020; Barkun, 2013). On social media, these narratives are negotiated one post at a time, expressing only short pieces of the \\\"immanent narrative whole\\\" (Clover, 1986). This gives rise to a new type of computational linguistic problem: Given a large enough corpus of social media text data, can one automatically distill semantically-labeled narratives (potentially several overlapping ones) that underlie the fragmentary conversational threads?\\n\\nRecent work (Shahsavari et al., 2020b; Tangherlini et al., 2020; Shahsavari et al., 2020a; Holur et al., 2021) has shown considerable promise that such scalable automated algorithms can be designed. An automated pipeline of interlocking machine learning modules decomposes the posts into actors, actants and their inter-actant relationships to create narrative networks via aggregation. These network representations are interpretable on inspection, allowing for the easy identification of the various signature semantic structures: Insiders, Outsiders, etc.\"}"}
{"id": "acl-2022-long-341", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: A pair of inferred text segments labeled by NP2IO showing Insider-Outsider context-sensitivity: Colored spans are used to highlight noun phrases that are inferred (red for Outsiders; blue for Insiders). POS tags are shown along with the noun phrases to illustrate an example of syntactic and semantic hints used by NP2IO to generate the inferred labels. Note that, based solely on context, the same agents (\u201ctech\u201d, \u201cvaccines\u201d, \u201cPeople\u201d, \u201cBill Gates\u201d and \u201cthe vaccine\u201d) switch Insider-Outsider label.\\n\\nEven though the training data is highly biased in terms of the identities of the Insiders/Outsiders, the pretrained language model used in our classifier allows NP2IO to learn to infer using the context phrases and not by memorizing the labels.\\n\\nOutsiders, strategies for dealing with Outsiders and their attendant threats and, in the case of conspiracy theories, causal chains of events that support that theory.\\n\\nBy itself, this unsupervised platform does not \u201cunderstand\u201d the different narrative parts. Since the submodules are not trained to look for specific semantic abstractions inherent in conspiracy theories, the platform cannot automatically generate a semantically tagged narrative for downstream NLP tasks. It cannot, for example, generate a list across narratives of the various outside threats and attendant inside strategies being recommended on a social media forum, nor can it address why these threats and strategies are being discussed.\\n\\n2 The Novel Insider vs. Outsider Classification Problem\\n\\nAs a fundamental first step bringing in supervised information to enable automated narrative structure discovery, we introduce the Insider-Outsider classification task: To classify the noun phrases in a post as Insider, Outsider or neither.\\n\\nA working conceptualization of what we consider Insiders and Outsiders is provided in the following insets. As with most NLP tasks, we do not provide formal definitions of and rules to determine these groups. Instead we let a deep learning model learn the representations needed to capture these notions computationally by training on data annotated with human-generated labels.\\n\\nThe partitioning of actors from a post into these different categories is inspired by social categorization, identification and comparison in the well-established Social Identity Theory (SIT) (Tajfel et al., 1979; Tajfel, 1974) and rests on established perspectives from Narrative Theory (Dundes, 1962; Labov and Waletzky, 1967; Nicolaisen, 1987). Following are some of the reasons why this classification task is challenging and why the concepts of Insiders/Outsiders are not sufficiently captured by existing labeled datasets used in Sentiment...\"}"}
{"id": "acl-2022-long-341", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Analysis (SA) (discussed in more detail in Section 3):\\n\\n1. Commonly-held Beliefs and Worldviews:\\nComprehensively incorporating shared values, crucial to the classification of Insider and Outsider, is a task with varied complexity. Some beliefs are easily enumerated: most humans share a perception of a nearly universal set of threats (virus, bomb, cancer, dictatorship) or threatening actions (\u201ckills millions of people\u201d, \u201ctries to mind-control everyone\u201d) or benevolent actions (\u201cdonating to a charitable cause\u201d, \u201ccuring disease\u201d, \u201cfreeing people\u201d). Similarly, humans perceive themselves and their close family units as close, homogeneous groups with shared values, and therefore \u201cI\u201d, \u201cus\u201d, \u201cmy children\u201d and \u201cmy family\u201d are usually Insiders. In contrast, \u201cthey\u201d and \u201cthem\u201d are most often Outsiders.\\n\\nAbstract beliefs pose a greater challenge as the actions that encode them can be varied and subtle. For example, in the post: \u201cThe microchips in vacines track us\u201d, the noun phrase \u201cmicrochips\u201d is in the Outsider category as it violates the Insiders\u2019 right to privacy by \u201ctrack[ing] us\u201d. Thus, greater attention needs to be paid in labeling datasets, highlighting ideas such as the right to freedom, religious beliefs, and notions of equality.\\n\\n2. Contextuality and Transitivity:\\nPeople express their opinions of Insider/Outsider affiliation by adding contextual clues that are embedded in the language of social media posts. For example, a post \u201cWe should build cell phone towers\u201d suggests that \u201ccell phone towers\u201d are helpful to Insiders, whereas a post \u201cWe should build cell phone towers and show people how it fries their brains\u201d suggests, in contrast, that \u201ccell phone towers\u201d are harmful to Insiders and belong, therefore, to the class of Outsider. Insider/Outsider affiliations are also implied in a transitive fashion within a post. For example, consider two posts: (i) \u201cBill Gates is developing a vaccine. Vaccines kill people.\u201d and (ii) \u201cBill Gates is developing a vaccine. Vaccines can eradicate the pandemic.\u201d In the first case, the vaccine\u2019s toxic quality and attendant Outsider status would transfer to Bill Gates, making him an Outsider as well; in the second post, vaccine\u2019s beneficial qualities would transfer to him, now making \u201cBill Gates\u201d an Insider.\\n\\n3. Model Requirement under Biased Data Conditions:\\nDesigning effective classifiers that do not inherit bias from the training data \u2013 especially data in which particular groups or individuals are decried or dehumanized \u2013 is a challenging but necessary task. Because conspiracy theories evolve, building on earlier versions, and result in certain communities and individuals being \u201cothered\u201d, our models must learn the phrases, contexts, and transitivity used to ascribe group membership, here either Insiders or Outsiders and not memorize the communities and/or individuals being targeted. Figure 1 illustrates an example where we probed our model to explore whether such a requirement is indeed satisfied. The first text conforms to the bias in our data, where \u201ctech\u201d, \u201cBill Gates\u201d, and \u201cvaccines\u201d are primarily Outsiders. The second text switches the context by changing the phrases. Our classifier is able to correctly label these same entities, now presented in a different context, as Insiders! We believe that such subtle learning is possible because of the use of pretrained language models. We provide several such examples in Table 3 and Figure 3 and also evaluate our model for Zero-shot learning in Table 1 and Figure 6.\\n\\n3 Our Framework and Related Work\\nRecent NLP efforts have examined the effectiveness of using pretrained Language Models (LM) such as BERT, DistilBERT, RoBERTa, and XLM to address downstream classification tasks through fine-tuning (Sanh et al., 2020; Liu et al., 2019; Lample and Conneau, 2019). Pretraining establishes the contextual dependencies of language prior to addressing a more specialized task, enabling rapid and efficient transfer learning. A crucial benefit of pretraining is that, in comparison to training a model from scratch, fewer labeled samples are necessary. By fine-tuning a pretrained LM, one can subsequently achieve competitive or better performance on an NLP task. As discussed in Section 2, since our model is required to be contextual and transitive, both of which are qualities that rely on the context embedded in language, we utilize a similar architecture.\\n\\nIn recent work involving span-based classification tasks, token-classification heads have proven to be very useful for tasks such as, Parts-of-Speech (POS) Tagging, Named Entity Recognition (NER) and variations of Sentiment Analysis (SA) (Yang et al., 2019; Vlad et al., 2019; Yin et al., 2020). Since the Insider-Outsider classification task is also set up as a noun phrase labeling task, our architecture uses a similar token-classification head on top of the pretrained LM backbone.\"}"}
{"id": "acl-2022-long-341", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Current SA datasets' definitions of positive negative and neutral sentiments can be thought of as a \\\"particularized\\\" form of the Insider-Outsider classification task. For example, among the popular datasets used for SA, Rotten Tomatoes, Yelp reviews (Socher et al., 2013) and others (Dong et al., 2014; Pontiki et al., 2014) implicitly associate a sentiment's origin to the post's author (source) (a single Insider) and its intended target to a movie or restaurant (a single Outsider if the sentiment is negative or an Insider if positive). The post itself generally contains information about the target and particular aspects that the Insider found necessary to highlight.\\n\\nIn more recent SA work, such as Aspect-Based Sentiment Analysis (ABSA) (Gao et al., 2021; Li et al., 2019; Wang et al., 2021; Dai et al., 2021), researchers have developed models to extract sentiments \u2013 positive, negative, neutral \u2013 associated with particular aspects of a target entity. One of the subtasks of ABSA, aspect-level sentiment classification (ALSC), has a form that is particularly close to the Insider-Outsider classification. Interpreted in the context of our task, the author of the post is an Insider although now there can potentially be multiple targets or \\\"aspects\\\" that need to be classified as Insiders and Outsiders. Still, the constructed tasks in ABSA appear to not align well with the goal of Insider-Outsider classification: 1) Datasets are not transitive: Individual posts appear to have only one agent that needs classification, or a set of agents, each with their own separate sets of descriptors; 2) The ALSC data is often at the sentence-level as opposed to post-level, limiting the context-space for inference. Despite these obvious differences, we quantitatively verify our intuitions in Section 7.1, and show that ABSA models do not generalize to our dataset.\\n\\nClosely related to ABSA is Stance Classification (SC) (also known as Stance Detection / Identification), the task of identifying the stance of the text author (in favor of, against or neutral) toward a target (an entity, concept, event, idea, opinion, claim, topic, etc.) (Walker et al., 2012; Zhang et al., 2017; K\u00fc\u00e7\u00fck and Can, 2021). Unlike ABSA, the target in SC does not need to be embedded as a span within the context. For example, a perfect SC model given an input for classification of context: This house would abolish the monarchy. and target: Hereditary succession, would predict the Negative label (Bar-Haim et al., 2017; Du et al., 2017). While SC appears to require a higher level of abstraction and, as a result, a model of higher complexity and better generalization power than those typically used for ABSA, current implementations of SC are limited by the finite set of queried targets; in other words, SC models currently do not generalize to unseen abstract targets. Yet, in real-time social media, potential targets and agents exhibit a continuous process of emergence, combination and dissipation. We seek to classify these shifting targets using the transitive property of language, and would like the language to provide clues about the class of one span relative to another. Ultimately, while SC models are a valuable step in the direction of better semantic understanding, they are ill-suited to our task.\\n\\nParallel to this work in SA, there are complementary efforts in consensus threat detection on social media (Wester et al., 2016; Kandias et al., 2013; Park et al., 2018), a task that broadly attempts to classify longer segments of text \u2013 such as comments on YouTube or tweets on Twitter \u2013 as more general \\\"threats\\\". The nuanced instruction to the labelers of the data is to identify whether the author of the post is an Outsider from the labeler's perspective as an Insider. Once again, we observe that this task aligns with the Insider-Outsider paradigm, but does not exhaust it, and the underlying models cannot accomplish our task. The sets of Insiders and Outsiders comprise a higher-order belief system that cannot be adequately captured with the current working definitions of sentiment nor the currently available datasets. This problem presents a primary motivation for creating a new dataset. For example, the post: \\\"Microchips are telling the government where we are\\\", does not directly feature a form of prototypical sentiment associated with \\\"microchips\\\", \\\"the government\\\" and \\\"we\\\", yet clearly insinuates an invasion on our right to privacy making clear the Insiders (\\\"we\\\") and Outsiders (\\\"microchips\\\", \\\"the government\\\") in the post.\"}"}
{"id": "acl-2022-long-341", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"overreach and the deep state\", \"limits on freedom of choice\\\" and \\\"Satanism\\\". The belief's evolution on social media has already enabled researchers to take the first steps in modeling critical parts of the underlying generative models that drive anti-vaccination conversations on the internet (Tangherlini et al., 2016; Bandari et al., 2017). Moreover, vaccine hesitancy is especially relevant in the context of the ongoing COVID-19 pandemic (Burki, 2020).\\n\\nOn the crawled corpus, we extract the noun-chunks from each post using SpaCy's noun chunk extraction module and dependency parsers (Honnibal and Johnson, 2015). A noun chunk is a subtree of the dependency parse tree, the headword of which is a noun. The result is a set of post-phrase pairs, \\\\((p, n)\\\\), where \\\\(p\\\\) is a post and \\\\(n\\\\) is one of the noun phrases extracted from the post.\\n\\nAmazon Mechanical Turk (AMT) (see Appendix A.2 for labeler instructions) was used to label the post-phrase pairs. For each pair, the labeler was asked, given the context, whether the writer of the post perceives the noun phrase to be an Insider, Outsider or neither (N/A). The labeler then provides a label \\\\(c \\\\in C\\\\), where \\\\(C = \\\\{\\\\text{Insider}, \\\\text{Outsider}, \\\\text{N/A}\\\\}\\\\) (hence \\\\(|C| = 3\\\\)). The triplets of post-phrase pairs along with their labels form the dataset \\\\(D = \\\\{(p_i, n_i, c_i)\\\\}_{i=1}^{|D|}\\\\). Note that a single post can appear in multiple triplets, because multiple different noun phrases can be extracted and labeled from a single post. The overall class distribution and a few conditional class distributions across the labeled samples are provided in Figure 5 in the Appendix B.\\n\\nManual inspection of the labeled samples \\\\(((p, n), c)\\\\) suggests that the quality of the dataset is good (<10% misclassified by random sampling). The now-labeled CT5K dataset (Holur et al., 2022) \\\\(|D| = 5000\\\\) samples is split into training (90%) and 10% testing sets. 10% of the training set is held out for validation. The final training set is 20-fold augmented by BERT-driven multi-token insertion (Ma, 2019).\\n\\n5 Methodology and Pipeline\\n\\nThe Noun-Phrase-to-Insider-Outsider (NP2IO) model adopts a token classification architecture comprising a BERT-like pre-trained backbone and a softmax classifier on top of the backbone. Token-level labels are induced from the span-level labels for the fine-tuning over CT5K, and the span-level labeling of noun phrases is done through majority vote during inference.\\n\\n5.1 Fine-tuning Details\\n\\nAn outline of the fine-tuning pipeline is provided in Figure 2. Given a labeled example \\\\(((p, n), c)\\\\), the model labels each token \\\\(t_i\\\\) in the post \\\\(p = [t_1, \\\\ldots, t_N]\\\\), where \\\\(N\\\\) is the number of tokens in the post. The BERT-like backbone embeds each token \\\\(t_i\\\\) into a contextual representation \\\\(\\\\Phi_i \\\\in \\\\mathbb{R}^d\\\\) (for example, \\\\(d = 768\\\\) for BERT-base or RoBERTa-base). The embedding is then passed to the softmax classification layer \\\\(\\\\pi_i \\\\triangleq \\\\text{Softmax}(W^T \\\\Phi_i + b)\\\\) (1) where \\\\(\\\\pi_i \\\\in \\\\Delta_{|C|}\\\\) is the Insider-Outsider classification prediction probability vector of the \\\\(i\\\\)th token, and \\\\(W \\\\in \\\\mathbb{R}^{d \\\\times |C|}\\\\) and \\\\(b \\\\in \\\\mathbb{R}^{|C|}\\\\) are the parameters of the classifier.\\n\\nThe ground truth class label \\\\(c\\\\) accounts for all occurrences of the noun phrase \\\\(n\\\\) in the post \\\\(p\\\\). We use this span-level label to induce the token-level label and facilitate the computation of the fine-tuning loss.\\n\\nConcretely, consider the spans where the noun phrase \\\\(n\\\\) occurs in the post \\\\(p\\\\): \\\\(S_n = \\\\{s_1, \\\\ldots, s_M\\\\}\\\\), where \\\\(s_j \\\\in S_n\\\\) denotes the span of the \\\\(j\\\\)th occurrence of \\\\(n\\\\), and \\\\(M\\\\) is the number of occurrences of \\\\(n\\\\) in \\\\(p\\\\). Each span is a sequence of one or more tokens. The set of tokens appearing in one of these labeled spans is: \\\\(T_n = \\\\{t \\\\in p | \\\\exists s \\\\in S_n \\\\text{ s.t. } t \\\\in s\\\\}\\\\) (2).\\n\\nWe define the fine-tuning loss \\\\(L\\\\) of the labeled example \\\\(((p, n), c)\\\\) as the cross-entropy (CE) loss computed over \\\\(T_n\\\\) using \\\\(c\\\\) as the label for each token in it, \\\\(L(p, n, c) = \\\\sum_{i: t_i \\\\in T_n} \\\\log(\\\\pi_i^c)\\\\) (3) where \\\\((\\\\pi_i^c)\\\\) denotes the prediction probability for the class \\\\(c \\\\in C\\\\) of the \\\\(i\\\\)th token.\\n\\nThe fine-tuning is done with mini-batch gradient descent for the classification layer and a number of self-attention layers in the backbone. The number of fine-tuned self-attention layers is a hyperparameter. The scope of hyperparameter tuning is provided in Table 4.\"}"}
{"id": "acl-2022-long-341", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.2 Real-time Inference and Accuracy Measurement\\n\\nDuring fine-tuning, we extend the label of a noun phrase to all of its constituent tokens; during inference, conversely, we summarize constituent token labels to classify the noun phrases by a majority vote. For a pair of post and noun-phrase \\\\((p, n)\\\\), assuming the definition of \\\\(\\\\{t_i\\\\}_{i=1}^N, \\\\{\\\\pi_i\\\\}_{i=1}^N\\\\) from the Section 5.1, the Insider-Outsider label prediction \\\\(\\\\hat{c}\\\\) is given by\\n\\n\\\\[\\n\\\\hat{c} = \\\\text{arg max}_k \\\\sum_{i: t_i \\\\in T_n} \\\\left( \\\\text{arg max}_\\\\kappa (\\\\pi_i) \\\\kappa \\\\right)\\n\\\\]\\n\\n(4)\\n\\nNow \\\\(c\\\\) can be compared to \\\\(\\\\hat{c}\\\\) with a number of classification evaluation metrics. Visual display of individual inference results such as those in Figure 1 are supported by displAy (Honnibal and Mon-tani, 2017).\\n\\n6 Baseline Models\\n\\nIn this section, we list baselines that we compare to our model's performance ordered by increasing parameter complexity.\\n\\n- **Random Model (RND):** Given a sample from the testing set \\\\(\\\\{p, n\\\\}\\\\), \\\\(\\\\hat{c}\\\\) is randomly selected with uniform distribution from \\\\(C = \\\\{\\\\text{Insider}, \\\\text{Outsider}, \\\\text{N/A}\\\\}\\\\).\\n\\n- **Deterministic Model (DET - I/O/NA):** For any post-phrase pair \\\\((p, n)\\\\), give a fixed classification prediction: \\\\(\\\\hat{c} = \\\\text{Insider} (DET-I), \\\\hat{c} = \\\\text{Outsider} (DET-O)\\\\) or \\\\(\\\\hat{c} = \\\\text{N/A} (DET-NA)\\\\).\\n\\n- **Na\u00efve Bayes Model (NB / NB-L):** Given a training set, the na\u00efve Bayes classifier estimates the likelihood of each class conditioned on a noun chunk \\\\(P(C, N|c)\\\\), assuming its independence w.r.t. the surrounding context. That is, a noun phrase predicted more frequently in the training-set as an Insider will be predicted as an Insider during the inference, regardless of the context. For noun phrases not encountered during training, the uniform prior distribution over \\\\(C\\\\) is used for the prediction. The noun chunk may be lemmatized (by word) during training and testing to shrink the conditioned event space. We abbreviate the na\u00efve Bayes model without lemmatization as NB, and the one with lemmatization as NB-L.\\n\\n- **GloVe+CBOW+XGBoost (CBOW - 1/2/5):** This baseline takes into account the context of a post but uses global word embeddings, instead of contextual-embeddings. A window length \\\\(w\\\\) is fixed such that for each noun phrase, we extract the \\\\(w\\\\) words before and \\\\(w\\\\) words after the noun phrase, creating a set of context words, \\\\(S_w\\\\). Stopwords are filtered, and the remaining context is used in the classification.\"}"}
{"id": "acl-2022-long-341", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Continuous Bag of Words (CBOW) model (Mikolov et al., 2013) averages the representative GloVe vectors in $S_w$ to create an aggregate contextual vector for the noun phrase. XGBoost (Chen and Guestrin, 2016) is used to classify the aggregated contextual vector. The same model is applied on the test set to generate labels. We consider window lengths of 1, 2 and 5 (CBOW-1, CBOW-2 and CBOW-5 respectively).\\n\\n### 7. Results and Evaluation\\n\\nComparison of NP2IO to baselines is provided in Table 1. The random (RND) and deterministic (DET-I, DET-O, DET-NA) models perform poorly. We present these results to get a better sense of the unbalanced nature of the labels in the CT5K dataset (see Figure 5). The na\u00efve Bayes model (NB) and its lemmatized form (NB-L) outperform the trivial baselines. However, they perform worse than the two contextual models, GloVe+CBOW+XGBoost and NP2IO. This fact validates a crucial property of our dataset: Despite the bias in the gold standard labels for particular noun phrases such as \u201cI\u201d, \u201cthey\u201d and \u201cmicrochip\u201d \u2013 see Figure 5 in Appendix B \u2013 context dependence plays a crucial role in Insider-Outsider classification.\\n\\nFurthermore, NP2IO outperforms GloVe+CBOW+XGBoost (CBOW-1, CBOW-2, CBOW-5) summarily. While both types of models employ context-dependence to classify noun phrases, NP2IO does so more effectively. The fine-tuning loss convergence plot for the optimal performing NP2IO model is presented in Figure 4 in Appendix B and model checkpoints are uploaded in the data repository.\\n\\n#### 7.1 Does CT5K really differ from prior ABSA datasets?\\n\\nGiven the limitations of current ABSA datasets for our task (see Section 2 and Section 3), we computationally show that CT5K is indeed a different dataset, particularly in comparison to other classical ones in Table 2. For this experiment, we train near-state-of-the-art ABSA models with RoBERTa-base backbone (Dai et al., 2021) on three popular ABSA datasets \u2013 Laptop reviews and Restaurant reviews from SemEval 2014 task 4 (Pontiki et al., 2014), and Tweets (Dong et al., 2014). Each trained model is then evaluated on all three datasets as well as the test set of CT5K. The Insider class in CT5K is mapped to the positive sentiment and the Outsider class to the negative sentiment. The F1-macro scores of the models trained and tested among the three ABSA datasets are much higher than the scores when testing on the CT5K dataset. Clearly, models that are successful with typical ABSA datasets do not effectively generalize to CT5K, suggesting that our dataset is different.\\n\\n#### 7.2 Classifying Noun Phrases at Zero-shot\\n\\nA challenge for any model, such as NP2IO, is zero-shot performance, when it encounters noun phrases never tagged during training. Answering this question offers a means for validating the context-dependence requirement, mentioned in Section 2. This evaluation is conducted on a subset of the entire testing set: A sample of the subset $\\\\{p, n\\\\}$ is such that the word-lemmatized, stopword-removed form of $n$ does not exist in the set of word-lemmatized, stopword-removed noun phrases seen during training. We extract 30% of test samples to be in this set. The results are presented in Table 1. As expected, the performance of the na\u00efve Bayes models (NB, NB-L) degrades severely to random. The performance of the contextual models CBOW-1/2/5, and NP2IO stay strong, suggesting effective context sensitivity in inferring the correct labels for these models. A visualization of the zero-shot capabilities of NP2IO on unseen noun phrases is presented in Figure 6 in Appendix B.\\n\\n#### 7.3 Does NP2IO Memorize? An Adversarial Experiment\\n\\nWe construct a set of adversarial samples to evaluate the extent to which NP2IO accurately classifies a noun phrase that has a highly-biased label distribution in CT5K. We consider 3 noun phrases in particular: \u201cmicrochip\u201d, \u201cgovernment\u201d, and \u201cchemical\u201d. Each of these has been largely labeled as Outsiders. The adversarial samples for each phrase, in contrast, are manually aggregated ($5$ seed posts augmented $20$ times each) to suggest that the phrase is an Insider (see Table 5 in Appendix B for the seed posts). We compute the recall of NP2IO in detecting these Insider labels (results in Table 3). NP2IO is moderately robust against adversarial attacks: In other words, highly-skewed distributions of labels for noun phrases in our dataset do not appear to imbue a similar drastic bias into our model.\"}"}
{"id": "acl-2022-long-341", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance of NP2IO versus multiple baselines on the test set:\\n\\nOur model (in bold) performs competitively and outperforms Na\u00efve Bayes, CBOW models across metrics. Furthermore, it retains its performance to classify noun phrases unseen (post-lemmatization and stopword removal) during training. Predictably, the performance of the Na\u00efve Bayes classifier in this zero-shot setting drops drastically to near random.\\n\\nTable 2: F1-macro scores for the ABSA model trained on conventional SA datasets from SemEval 2014 task 4:\\n\\nAll models perform poorly in testing on the CT5K dataset while performing well in testing on ABSA datasets. This suggests that the CT5K dataset is indeed differentiated from the ABSA datasets.\\n\\nTable 3: Adversarial inferencing tasks for the trained NP2IO model:\\n\\nThree noun phrases with very high Outsider status (100%, 80%, 100%, respectively) in the CT5K training set are used to construct posts where their contextual role is beneficial, and hence, should be labeled as Insider (see Section 2). The results show that NP2IO largely learned to use the contextual information for its inference logic, and did not memorize the agent bias in CT5K. We speculate that the exhibited bias towards \u201cChemicals\u201d is due to the large body of text documents that discusses the adverse effects of chemicals, and hence is encoded in the embedding structure of pretrained LM models that NP2IO cannot always overrule; at least yet.\\n\\n8 Concluding Remarks\\n\\nWe presented a challenging Insider-Outsider classification task, a novel framework necessary for addressing burgeoning misinformation and the proliferation of threat narratives on social media. We compiled a labeled CT5K dataset of conspiracy-theoretic posts from multiple social media platforms and presented a competitive NP2IO model that outperforms non-trivial baselines. We have demonstrated that NP2IO is contextual and transitive via its zero-shot performance, adversarial studies and qualitative studies. We have also shown that the CT5K dataset consists of underlying information that is different from existing ABSA datasets.\\n\\nGiven NP2IO\u2019s ability to identify Insiders and Outsiders in a text segment, we can extend the inference engine to an entire set of interrelated samples in order to extract, visualize and interpret the underlying narrative (see Figure 3). This marks a first and significant step in teasing out narratives from fragmentary social media records, with many of its essential semantic parts \u2013 such as, Insider/Outsider \u2013 tagged in an automated fashion. As extensive evaluations of the NP2IO model show, our engine has learned the causal phrases used to designate the labels. We believe an immediate future work can identify such causal phrases, yet another step toward semantic understanding of the parts of a narrative. Broadly, work similar to this promises to expedite the development of models that rely on a computational foundation of structured information, and that are better at explaining causal chains of inference, a particularly important feature in the tackling of misinformation.\"}"}
{"id": "acl-2022-long-341", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An actor-actant subnarrative network constructed from social media posts: Selected posts from anti-vaccination forums such as qresearch on 4chan were decomposed into relationship tuples using a state-of-the-art relationship extraction pipeline from previous work (Tangherlini et al., 2020) and these relationships are overlayed with the inferences from NP2IO. This results in a network where the nodes are the noun phrases and the edges are the verb phrases, with each edge representing an extracted relationship from a post. In this network, a connected component emerged capturing a major sub-theory in vaccine hesitancy. This highlights NP2IO's ability at inferring both the threat-centric orientation of the narrative space as well as the negotiation dynamics in play, thereby providing qualitative insight into how NP2IO may be used in future work to extract large-scale relationship networks that are interpretable. The green boxes highlight the noun phrases that have contradictory membership in the Insider and Outsider classes as their affiliations are deliberated.\\n\\nIndeed, NP2IO's success has answered the question: \\\"Which side are you on?\\\" What remains to be synthesized from language is: \\\"Why?\\\"\\n\\nReferences\\n\\nPaul Bailey. 1999. Searching for storiness: Story-generation from a reader's perspective. In Working notes of the Narrative Intelligence Symposium, pages 157\u2013164.\\n\\nRoja Bandari, Zicong Zhou, Hai Qian, Timothy R. Tangherlini, and Vwani P. Roychowdhury. 2017. A resistant strain: Revealing the online grassroots rise of the antivaccination movement. Computer, 50(11):60\u201367.\\n\\nRoy Bar-Haim, Indrajit Bhattacharya, Francesco Dinuzzo, Amrita Saha, and Noam Slonim. 2017. Stance classification of context-dependent claims. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 251\u2013261, Valencia, Spain. Association for Computational Linguistics.\\n\\nMichael Barkun. 2013. A Culture of Conspiracy: Apocalyptic Visions in Contemporary America. University of California Press.\\n\\nJohn Beatty. 2016. What are narratives good for? Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences, 58:33\u201340.\\n\\nJohn Bodner, Wendy Welch, and Ian Brodie. 2020. COVID-19 conspiracy theories: QAnon, 5G, the New World Order and other viral ideas. McFarland.\\n\\nTalha Burki. 2020. The online anti-vaccine movement in the age of covid-19. The Lancet Digital Health, 2(10):e504\u2013e505.\\n\\nTianqi Chen and Carlos Guestrin. 2016. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pages 785\u2013794, New York, NY, USA. ACM.\\n\\nDavid Chong, Erl Lee, Matthew Fan, Pavan Holur, Shadi Shahsavari, Timothy Tangherlini, and Vwani Roychowdhury. 2021. A real-time platform for contextualized conspiracy theory analysis. In 2021 International Conference on Data Mining Workshops (ICDMW) (forthcoming). IEEE.\\n\\nCarol J Clover. 1986. The long prose form. Arkiv f\u00f6r nordisk filologi, 101:10\u201339.\\n\\nJunqi Dai, Hang Yan, Tianxiang Sun, Pengfei Liu, and Xipeng Qiu. 2021. Does syntax matter? A strong baseline for aspect-based sentiment analysis with roberta. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 1816\u20131829. Association for Computational Linguistics.\\n\\nLi Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent Twitter sentiment classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 49\u201354, Baltimore, Maryland. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-341", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-341", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Timothy R Tangherlini. 2018. Toward a generative model of legend: Pizzas, bridges, vaccines, and witches. Humanities, 7(1):1.\\n\\nTimothy R Tangherlini, Vwani Roychowdhury, Beth Glenn, Catherine M Crespi, Roja Bandari, Akshay Wadia, Misagh Falahi, Ehsan Ebrahimzadeh, and Roshan Bastani. 2016. \u201cmommy blogs\u201d and the vaccination exemption narrative: Results from a machine-learning approach for story aggregation on parenting social media sites. JMIR Public Health Surveill, 2(2):e166.\\n\\nTimothy R Tangherlini, Shadi Shahsavari, Behnam Shahbazi, Ehsan Ebrahimzadeh, and Vwani Roychowdhury. 2020. An automated pipeline for the discovery of conspiracy and conspiracy theory narrative frameworks: Bridgegate, pizzagate and storytelling on the web. PloS one, 15(6):e0233879.\\n\\nMaxim Tkachenko, Mikhail Malyuk, Nikita Shevchenko, Andrey Holmanyuk, and Nikolai Liubimov. 2020-2021. Label Studio: Data labeling software. Open source software available from https://github.com/heartexlabs/label-studio.\\n\\nGeorge-Alexandru Vlad, Mircea-Adrian Tanase, Cristian Onose, and Dumitru-Clementin Cercel. 2019. Sentence-level propaganda detection in news articles with transfer learning and BERT-BiLSTM-capsule model. In Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, pages 148\u2013154, Hong Kong, China. Association for Computational Linguistics.\\n\\nMarilyn Walker, Pranav Anand, Rob Abbott, and Ricky Grant. 2012. Stance Classification using Dialogic Properties of Persuasion. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592\u2013596, Montr\u00e9al, Canada. Association for Computational Linguistics.\\n\\nXinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, and Kewei Tu. 2021. Automated Concatenation of Embeddings for Structured Prediction. In the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021). Association for Computational Linguistics.\\n\\nAksel Wester, Lilja \u00d8vrelid, Erik Velldal, and Hugo Lewi Hammer. 2016. Threat detection in online discussions. In Proceedings of the 7th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 66\u201371.\\n\\nKisu Yang, Dongyub Lee, Taesun Whang, Seolhwa Lee, and Heuiseok Lim. 2019. Emotionx-ku: Bert-max based contextual emotion classifier.\\n\\nDa Yin, Tao Meng, and Kai-Wei Chang. 2020. SentiBERT: A transferable transformer-based architecture for compositional sentiment semantics. In Proceedings of the 58th Conference of the Association for Computational Linguistics, ACL 2020, Seattle, USA.\\n\\nShaodian Zhang, Lin Qiu, Frank Chen, Weinan Zhang, Yong Yu, and No\u00e9mie Elhadad. 2017. We Make Choices We Think are Going to Save Us: Debate and Stance Identification for Online Breast Cancer CAM Discussions. In Proceedings of the 26th International Conference on World Wide Web Companion, WWW '17 Companion, pages 1073\u20131081, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee.\\n\\nAppendices\\n\\nA Data Collection\\n\\nA.1 Automated Crawling of Social Media\\n\\nA daily data collection method (Chong et al., 2021) aggregates heterogeneous data from various social media platforms including Reddit, YouTube, 4chan and 8kun. Our implementation of this pipeline has extracted potentially conspiracy theoretic posts between March 2020 and June 2021. We select a subset of these posts that are relevant to vaccine hesitancy and that include (a) at least one of the words in ['vaccine', 'mrna', 'pfizer', 'moderna', 'j&j', 'johnson', 'chip', 'pharm'] and (b) between 150 to 700 characters. The end-to-end data processing pipeline is uncased.\\n\\nA.2 Instructions to AMT Labelers\\n\\nAmazon Mechanical Turk Labelers were required to be at the Masters\u2019 level (exceeding a trust baseline provided by Amazon), were required to speak English, and were required to be residing in the United States. No personally identifying information was collected. Users were asked to create an account on a LabelStudio (Tkachenko et al., 2020-2021) platform to answer a set of 60-80 questions or 2 hours worth of questions. Each question included (a) A real anonymized social media post with a highlighted sentence, (b) The sentence highlighted in (a) but with the noun phrase of interest highlighted. The question prompt read: Please let us know whether the entity highlighted in bold AS PERCEIVED BY THE WRITER is a good/bad or neutral entity. Labelers were reminded several times via pop-ups and other means that the labels were to be chosen with respect to the author of the post and...\"}"}
{"id": "acl-2022-long-341", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3 Ethics statement about the collected social media data\\n\\nData was collected using verified research Application Programming Interfaces (API) provided by the social media companies for non-commercial study. In order to explore data on fringe platforms such as 4chan and 8kun where standard APIs are not available, the data was scraped using a Selenium-based crawler. All the retrieved samples were ensured to be public: the posts could be accessed by anyone on the internet without requiring explicit consent by the authors. Furthermore, we made sure to avoid using Personal Identifiable Information (PII) such as the user location, time of posting and other metadata: indeed, we hid even the specific social media platform from which a particular post was mined. The extracted text was cleaned by fixing capitalization, filtering special characters, adjusting inter-word spacing and correcting punctuation, all of which further obfuscated the identity of the author of a particular post.\\n\\nB Supplementary Figures and Tables\\n\\nFigure 4: Convergence plot for NP2IO: Shown above is the training and validation CE loss with optimal parameters (in bold) from Table 4. Model checkpoints are in data repository.\\n\\n| Parameters          | Values                  |\\n|---------------------|-------------------------|\\n| Batch Size          | 32, 64, 128             |\\n| Trainable Layers    | 0, 1, 2, 5              |\\n| LR                  | 1E-7, 1E-6, 1E-5, 1E-4  |\\n| Pretrained Backbone  | BERT-base, DistilBERT-base, RoBERTa-base, RoBERTa-large |\\n\\nTable 4: A summary of the parameters considered for finetuning: NP2IO's best-performing (by validation loss) parameters are in bold.\\n\\nFigure 5: Histograms that show the distributions of labels in CT5K: The plot for \\\"All\\\" represents the full 3-category label distribution across all entities, for \\\"I\\\" the bias toward Insiders is evident, \\\"They\\\" are mostly outsiders, and there is no clear consensus label for \\\"Vaccine\\\" and \\\"Herd Immunity\\\". Microchips are always tagged as Outsiders.\"}"}
{"id": "acl-2022-long-341", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NP Seed Posts\\n(augmented to 100 posts per NP)\\nmicrochip\\n\u201cI love microchips.\u201d, \u201cI feel that microchips are great.\u201d, \u201cMicrochips are lovely and extremely useful.\u201d, \u201cI believe microchips are useful in making phones.\u201d, \u201cMicrochips have made me a lot of money.\u201d\\n\\ngovernment\\n\u201cThe government helps keep me safe.\u201d, \u201cThe government does a good job.\u201d, \u201cI think that without the government, we would be worse off.\u201d, \u201cThe government keeps us safe.\u201d, \u201cA government is important to keep our society stable.\u201d\\n\\nchemical\\n\u201cChemicals save us.\u201d, \u201cChemicals can cure cancer.\u201d, \u201cI think chemicals can help elongate our lives.\u201d, \u201cI think chemicals are great and help keep us healthy.\u201d, \u201cChemicals can help remove ringworms.\u201d\\n\\nTable 5: The set of 5 Insider-oriented core posts per noun phrase (in bold) that have a high skew toward Outsider labels in CT5K: Each seed post is augmented 20 times to create a set of 100 adversarial posts per phrase. NP2IO infers the label for the key noun phrase across these samples. The adversarial recall is presented in Table 3.\\n\\nFigure 6: Zero-shot Insider-Outsider Classification Profile: This figure shows the consensus vote for noun phrases that do not occur in the training set. The x-axis represents the consensus-vote-count and the y-axis, the indices of the noun phrases. The consensus vote is computed for each noun phrase \\\\( n \\\\) by passing all the posts that include \\\\( n \\\\) through NP2IO. Each Insider vote is +1 and Outsider vote is \u22121. The consensus-vote-count is also color-coded for better visualization. The zero-shot classification is qualitatively observed to correctly classify popular noun phrases such as \u201creeducation camps\u201d, \u201cdepopulation\u201d as Outsiders and \u201camerican\u201d and \u201cfaith\u201d as Insiders in the subnarrative of the anti-vaccination movement.\"}"}
