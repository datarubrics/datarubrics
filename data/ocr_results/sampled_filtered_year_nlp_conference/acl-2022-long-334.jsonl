{"id": "acl-2022-long-334", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We study the problem of coarse-grained response selection in retrieval-based dialogue systems. The problem is equally important with fine-grained response selection, but is less explored in existing literature. In this paper, we propose a Contextual Fine-to-Coarse (CFC) distilled model for coarse-grained response selection in open-domain conversations. In our CFC model, dense representations of query, candidate contexts and responses is learned based on the multi-tower architecture using contextual matching, and richer knowledge learned from the one-tower architecture (fine-grained) is distilled into the multi-tower architecture (coarse-grained) to enhance the performance of the retriever. To evaluate the performance of the proposed model, we construct two new datasets based on the Reddit comments dump and Twitter corpus. Extensive experimental results on the two datasets show that the proposed method achieves huge improvement over all evaluation metrics compared with traditional baseline methods.\\n\\n1 Introduction\\n\\nGiven utterances of a query, the retrieval-based dialogue (RBD) system aims to search for the most relevant response from a set of historical records of conversations (Higashinaka et al., 2014; Yan et al., 2016; Boussaha et al., 2019). A complete RBD system usually contain two stages: coarse-grained response selection (RS) and fine-grained response selection (Fu et al., 2020). As shown in Figure 1, in coarse-grained RS stage, the retriever identifies a much smaller list of candidates (usually dozens) from large-scale candidate database (up to millions or more), then the ranker in fine-grained RS stage selects the best response from the retrieved candidate list.\\n\\nRecent studies (Whang et al., 2020; Xu et al., 2020, 2021; Whang et al., 2021) pay more attention on fine-grained RS and various complex models are proposed to compute the similarities between the query and candidates for response selection. Although promising improvements have been reported, the performance of fine-grained stage is inevitably limited by the quality of the candidate list constructed. Therefore, a high-quality coarse-grained RS module is crucial, which is less explored in existing literature (Lan et al., 2020).\\n\\nIn this paper, we focus on the task of coarse-grained response selection, i.e., dialogue response retrieval. There are two major challenges. First, different from general text matching tasks such as ad-hoc retrieval (Hui et al., 2018) or question answering (QA) retrieval (Karpukhin et al., 2020), key-words overlapping between context and response in dialogue are potentially rare, such as when a topic transition (Sevegnani et al., 2021) occurs in response. This makes it difficult to directly match the query with candidate responses. Second, compared with fine-grained RS, coarse-grained RS deals with much larger number of candidates. Therefore, it is impractical to apply complex matching model that jointly process query and response for the similarity computation like in fine-grained RS, due to the retrieval latency (traverse millions of candidates on-\"}"}
{"id": "acl-2022-long-334", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instead, the efficient BM25 system (Robertson and Zaragoza, 2009) based on sparse representations is the mainstream algorithm in coarse-grained text matching.\\n\\nTo mitigate the above mentioned two problems, we propose a **Contextual Fine-to-Coarse (CFC)** distilled model for coarse-grained RS. Instead of matching query with response directly, we propose a novel task of query-to-context matching in coarse-grained retrieval, i.e., contextual matching. Given a query, it is matched with candidate contexts to find most similar ones, and the corresponding responses are returned as the retrieved result. In this case, the potential richer keywords in the contexts can be utilized. To take the advantage of complex model and keep the computation cost acceptable, we distillate the knowledge learned from fine-grained RS into coarse-grained RS while maintaining the original architecture.\\n\\nFor the evaluation, there is no existing dataset that can be used to evaluate our model in the setting of contextual matching, because it needs to match context with context during training, while positive pairs of context-context is not naturally available like context-response pairs. Therefore, we construct two datasets based on Reddit comment dump and Twitter corpus. Extensive experimental results show that our proposed model greatly improve the retrieval recall rate and the perplexity and relevance of the retrieved responses on both datasets.\\n\\nThe main contributions of this paper are three-fold: 1) We explore the problem of coarse-grained RS in open domain conversations and propose a Contextual Fine-to-Coarse (CFC) distilled model; 2) We construct two new datasets based on Reddit comment dump and Twitter corpus, as a new benchmark to evaluate coarse-grained RS task; 3) We construct extensive experiments to demonstrate the effectiveness and potential of our proposed model in coarse-grained RS.\\n\\n### 2 Related Work\\n\\n#### Fine-grained Response Selection\\n\\nIn recent years, many works have been proposed to improve the performance of fine-grained selection module in retrieval-based chatbots (Zhang et al., 2018; Zhou et al., 2018; Tao et al., 2019; Whang et al., 2019; Yuan et al., 2019). Owing to the rapid development of pre-trained language models (PLMs) (Radford et al., 2019), recent works (Gu et al., 2020; Whang et al., 2021; Sevegnani et al., 2021) achieve the state-of-the-art (SOTA) results by utilizing PLMs such as BERT (Devlin et al., 2018) to model cross-attention and complex intersection between the context and response.\\n\\n#### Coarse-grained Response Selection\\n\\nOn the other hand, coarse-grained dialogue retrieval is an important but rarely explored field. Limited by efficiency, there are usually two methods for coarse-grained response selection, i.e., the sparse representations based method represented by BM25 (Robertson and Zaragoza, 2009), and the dense representations based method represented by dual-Encoder (Chidambaram et al., 2018; Humeau et al., 2019; Karpukhin et al., 2020; Lan et al., 2020; Lin et al., 2020).\\n\\n### 3 Method\\n\\nIn coarse-grained response selection, there is a fixed candidate database containing a large number of context-response pairs. Formally, given a query, i.e., a new context, the goal is to retrieve Top-K most suitable responses for the query from the candidate database.\\n\\nWe propose a contextual fine-to-coarse distillation framework for the task of coarse-grained RS. First, we formulate the problem as a task of contextual matching, i.e., match query with context instead response; Second, we utilize a multi-tower architecture to deal with the similarity computation of query and candidates in contextual matching; Third, we utilize knowledge distillation to leverage the deep interaction between query and response learned in one-tower architecture.\\n\\n#### 3.1 Contextual Matching\\n\\nAn intuitive idea of coarse-grained RS is to treat all responses as candidate documents and directly use query to retrieve them, while this non-contextual approach results in a quite low retrieval recall rate (Lan et al., 2020). Inspired by recent studies of context-to-context matching in fine-grained RS (Fu et al., 2020), we propose contextual matching in coarse-grained RS, which is to match the query with candidate contexts, and return the responses corresponding to the most similar contexts. We consider three ways of contextual matching.\\n\\n**Query-Context (QC)**\\n\\nIn QC matching, we treat contexts instead of responses as candidate documents. At run-time, we calculate the similarities between query and candidate contexts, and the retrieved results are returned. This method leverages the rich information in contexts, which are often richer than responses.\\n\\n**Context-Query (CQ)**\\n\\nIn CQ matching, we treat contexts as candidate documents and use query to match them. This method is similar to the previous one, but the order of matching is reversed.\\n\\n**Context-Context (CC)**\\n\\nIn CC matching, we treat both contexts and responses as candidate documents and match them with each other. This method is the most complex and requires pairwise similarity computation.\\n\\nThese three methods provide different perspectives on how to leverage contextual information in coarse-grained RS.\\n\\n### 4 Conclusion\\n\\nIn this paper, we have explored the problem of coarse-grained RS in open domain conversations and proposed a Contextual Fine-to-Coarse (CFC) distilled model. We have constructed two new datasets based on Reddit comment dump and Twitter corpus to evaluate our model in the setting of contextual matching. Extensive experimental results show that our proposed model greatly improve the retrieval recall rate and the perplexity and relevance of the retrieved responses on both datasets.\\n\\nThe main contributions of this paper are three-fold: 1) We explore the problem of coarse-grained RS in open domain conversations and propose a Contextual Fine-to-Coarse (CFC) distilled model; 2) We construct two new datasets based on Reddit comment dump and Twitter corpus, as a new benchmark to evaluate coarse-grained RS task; 3) We construct extensive experiments to demonstrate the effectiveness and potential of our proposed model in coarse-grained RS.\"}"}
{"id": "acl-2022-long-334", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Two-tower model based on QS matching\\n\\nThree-tower model based on DQS matching\\n\\nFigure 2: Multi-tower architecture with independent encoders, the hidden representation of the [CLS] token of each sequence is passed through a linear layer followed by a hyperbolic tangent (Tanh) activation function to get the dense representations (embeddings) of the entire sentence.\\n\\nResponses corresponding to the Top-K most similar contexts are returned as the retrieved results. The motivation of using QC matching is similar contexts may also share similar responses.\\n\\nQuery-Session (QS)\\nA session represents the concatenated text of context and corresponding response (Fu et al., 2020), which we think is more informative than context alone. In QS matching, we treat sessions as candidate documents and return the responses in Top-K most similar sessions as the retrieved results.\\n\\nDecoupled Query-Session (DQS)\\nApart from QS matching, we also consider a decoupled way to match query with candidate sessions. In DQS matching, we treat contexts and responses as independent candidate documents. Similarities between query and contexts, query and responses are first calculated independently, then the query-session similarity can be obtained by the weighted sum. QS and DQS matching are actually two different ways to calculate query-session similarity.\\n\\n3.2 Multi-Tower Architecture\\nFor the retriever to search large-scale candidates with low latency, neural-based retrievers are usually designed as (or limited to) multi-tower architecture (Figure 2). In multi-tower models, the query and the candidates are independently mapped to a common vector space by different encoders, where similarity can be calculated. After training, the embeddings of large-scale candidates can be pre-calculated offline, and only the embedding of query needs to be calculated online. In this way, fast sublinear-time approximation methods such as approximate nearest neighbor search (Shrivastava and Li, 2014) can be utilized to search for Top-K vectors that are most similar to the query, which can achieve an acceptable retrieval latency during inference.\\n\\n3.2.1 Two-Tower Model\\nFor QC and QS matching, two-tower architecture is adopted. Taking QS matching as an example (Figure 2(a)), the dense session encoder $E_S(\\\\cdot)$ maps any candidate session to real-valued embedding vectors in a $d$-dimensional space, and an index is built for all the $N$ session vectors for retrieval. At run-time, a different dense query encoder $E_Q(\\\\cdot)$ maps the query to a $d$-dimensional vector, and retrieves $k$ candidate sessions of which vectors are the closest to the query vector. We use the dot product of vectors as the similarity between query and candidate session following (Karpukhin et al., 2020).\\n\\n3.2.2 Three-Tower Model\\nFor DQS matching, dense representations of query, context and response are independently calculated, the architecture is thus designed as three-tower with three encoders, which is query encoder $E_Q(\\\\cdot)$, context encoder $E_C(\\\\cdot)$ and response encoder $E_R(\\\\cdot)$ (Figure 2(b)). Similarly, context and response vectors are calculated and cached offline respectively and two indexes are built for retrieving them. The final similarity of query and session is weighted by the dot product of query-context and query-response. The weighting coefficient $\\\\lambda$ can be adjusted to determine whether it is biased to match the context or match the response.\\n\\n3.2.3 Training Multi-Tower Model\\nWe unify the training of the two-tower and three-tower models by formalizing them into a same metric.\"}"}
{"id": "acl-2022-long-334", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ric learning problem (Kulis et al., 2012). The goal is to learn a matching space where similarities between positive pairs is higher than negative ones, by learning a better embedding function. We use the training of three-tower model (DQS matching) as an example. Formally, we denote the training set as \\\\( D = \\\\{ q_i, \\\\{ k^+ + i, k^- \\\\} \\\\}_{i=1}^{N} \\\\). Each training instance contains a query \\\\( q_i \\\\), a set of positive examples \\\\( k^+ \\\\) and a set of negative examples \\\\( k^- \\\\). Among them, \\\\( k^+ \\\\) contain several positive contexts and several positive responses, similarly, \\\\( k^- \\\\) contain several negative contexts and several negative responses.\\n\\nWe optimize the loss function as the sum of negative log likelihood of all positive pairs simultaneously:\\n\\n\\\\[\\nL(q_i) = -\\\\log P(k' \\\\in \\\\{ k^+ \\\\}) e^{\\\\text{sim}(q_i, k')} P(k' \\\\in \\\\{ k^+, k^- \\\\}) e^{\\\\text{sim}(q_i, k')}\\n\\\\]\\n\\nwhere the similarity function is defined as:\\n\\n\\\\[\\n\\\\text{sim}(q_i, k') = \\\\mathbb{E}_{Q}(q_i) \\\\cdot \\\\mathbb{E}(k')\\n\\\\]\\n\\nThe embedding function \\\\( \\\\mathbb{E}(\\\\cdot) \\\\) in Equation 2 can be \\\\( \\\\mathbb{E}_C(\\\\cdot) \\\\) or \\\\( \\\\mathbb{E}_R(\\\\cdot) \\\\), depending on the type of \\\\( k' \\\\).\\n\\nPositive and negative examples The core issue of training multi-tower models for contextual matching is to find positive pairs of query-context (or query-session). In this paper, we assume that contexts with exactly the same response are positive samples of each other, which is a cautious but reliable strategy. Formally, given a response \\\\( r \\\\), if there are multiple contexts whose response is \\\\( r \\\\), then we can randomly selected one context as the query \\\\( q \\\\), and the other contexts are positive contexts of \\\\( q \\\\), and \\\\( r \\\\) is the positive response of \\\\( q \\\\).\\n\\nNegative samples of contexts and responses can be obtained from in-batch (Karpukhin et al., 2020) or random sampling from database. Similarly, positive query-session is obtained by replacing the context in positive query-context with the whole session.\\n\\n3.3 Distillation from One-Tower Model\\n\\nIn multi-tower architecture, the query and candidates are expressed by their embeddings independently, which may cause the loss of information, and their monotonous way of interaction (inner product) further limits the capability (Lin et al., 2020). Comparing with multi-tower model, one-tower model takes both the query and the candidate as a concatenated input and allow the cross attention between query and candidate in self-attention layer. Despite fewer parameters, one-tower model have been shown to learn a more informative representations than multi-tower model, thus it is preferred in fine-grained RS (Yang and Seo, 2020).\\n\\nTo leverage the richer expressiveness learned by the one-tower model, knowledge from one-tower model is distilled into multi-tower model to enhance the retriever.\\n\\n3.3.1 Training One-Tower Model\\n\\nBefore distillation, we need to train teacher models based on one-tower architecture. Let\u2019s take the training of teacher model for QS matching as an example. A single encoder is trained to distinguish whether the query and the session are relevant (positive), and the form is exactly same as the next sentence prediction (NSP) task in the BERT (Devlin et al., 2018) pre-training. Formally, given a training set \\\\( D = \\\\{ q_i, s_i, l_i \\\\}_{i=1}^{N} \\\\), where \\\\( q_i \\\\) is the query, \\\\( s_i \\\\) is the candidate session and \\\\( l_i \\\\in \\\\{0, 1\\\\} \\\\) denotes whether \\\\( q_i \\\\) and \\\\( s_i \\\\) is a positive pair. To be specific, given a query \\\\( q \\\\) and candidate session \\\\( s \\\\), the encoder obtains the joint representation of the concatenated text of \\\\( q \\\\) and \\\\( s \\\\), and then computes the similarity score through a linear layer, the training objective is binary cross entropy loss.\\n\\nWe summarize the main difference between one-tower and multi-tower as follows: one-tower model is more expressive, but less efficient and cannot handle large-scale candidates. The main reason is that feature-based method of calculating similarity scores rather than inner product limits the capability of offline caching. For new queries, the similarities with all candidates can only be calculated by traversal. The huge latency makes it impossible to use one-tower model in coarse-grained response retrieval. To leverage the expressiveness of one-tower model, we propose fine-to-coarse distillation, which can learn the knowledge of one-tower model while keeping the multi-tower structure unchanged, thereby improving the performance of the retriever.\\n\\n3.3.2 Fine-to-Coarse Distillation\\n\\nTake the two-tower student model (denoted as \\\\( S \\\\)) for QS matching as an example, suppose we have trained the corresponding one-tower teacher model (denoted as \\\\( T \\\\)). For a given query \\\\( q \\\\), suppose there are a list of sessions \\\\( \\\\{ s^+, s^-_1, ..., s^-_n \\\\} \\\\) and the cor-\\n\"}"}
{"id": "acl-2022-long-334", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"responding label $y = \\\\{1, 0, ..., 0\\\\} \\\\in \\\\mathbb{R}^{n+1}$, that is, one positive session and $n$ negative sessions.\\n\\nWe denote the similarity score vector of query-sessions computed by student model $S$ (Equation 2) as $z_S \\\\in \\\\mathbb{R}^{n+1}$, then the objective of Equation 1 is equivalent to maximizing the Kullback\u2013Leibler (KL) divergence (Van Erven and Harremos, 2014) of the two distributions: softmax$(z_S)$ and $y$, where softmax function turns the score vector to probability distribution.\\n\\nThe one-hot label $y$ treats each negative sample equally, while the similarity between query with each negative sample is actually different. To learn more accurate labels, we further use teacher model $T$ to calculate the similarity score vector between $q$ and $S$, denoted as $z_T \\\\in \\\\mathbb{R}^{n+1}$. We then replace the original training objective with minimizing KL divergence of the two distributions softmax$(z_S)$ and softmax$(z_T)$ (Figure 1), where the temperature parameter is applied in softmax function to avoid saturation.\\n\\nThe method of fine-to-coarse distillation is to push the student model (multi-tower) to learn the predicted label of teacher model (one-tower) as a soft target instead of original one-hot label. By fitting the label predicted by the teacher model, the multi-tower model can learn a more accurate similarity score distribution from the one-tower model while keeping the structure unchanged.\\n\\n### 4 Datasets Construction\\n\\nTo evaluate the performance of the proposed model, we construct two new datasets based on the Reddit comments dump (Zhang et al., 2019) and Twitter corpus. We create a training set, a multi-contexts (MC) test set and a candidate database for Reddit and Twitter respectively. For Reddit, we create an additional single-context (SC) test set. The motivation for these settings is explained in \u00a7 5.3. The size of our candidate database is one million in Twitter and ten million in Reddit respectively, which is very challenging for response retrieval. Table 1 shows the detailed statistics. We use exactly the same steps to build dataset for Reddit and Twitter, and similar datasets can also build from other large dialogue corpus in this way.\\n\\n#### MC test set\\n\\nWe first find out a set of responses with multiple contexts from candidate database, denoted as $R$. For each response $r$ in $R$, we randomly select one context $c$ from its all corresponding contexts $C_r$ to construct a context-response (CR) pair, and put the others contexts (denoted as $C_r^-$) back to the database. Our MC test set consists of these CR pairs. Each response in MC test set has multiple contexts, which ensures that there exits other contexts in the database that also correspond to this response, so the retrieval recall rate can be computed to evaluate the MC test set.\\n\\n#### SC test set\\n\\nWe create another test set (SC) for Reddit dataset. Contrary to the MC test set, each response in SC test set has only one context, i.e., there is no context in the database that exactly corresponds to the response. Obviously, the retrieval recall rate is invalid (always zero) on SC test set. We introduce other methods to evaluate SC test set in \u00a7 5.2. The SC test set is a supplement to the MC test set which can evaluate the quality of retrieved responses given those \u201cunique\u201d contexts.\\n\\n#### Candidate database\\n\\nTo adapt to different retrieval methods, the candidate database is designed with 4 fields, namely context, response, session. Our candidate database consists of random context-response pairs except those in the MC and SC test sets. Besides, as mentioned above, those unselected context-response pairs ($C_r^-$) are deliberately merged into the database.\\n\\n#### Train set\\n\\nThe construction of training set is intuitive and similar to test set. It consists of responses and their corresponding multiple contexts. Formally, the training set can be denote as $D = \\\\{r_i, c_{i,1}, ..., c_{i,q}\\\\}_{i=1}^N$, $r_i$ is a response and $\\\\{c_{i,1}, ..., c_{i,q}\\\\}$ are all contexts with response $r_i$, where $q$ depends on $r_i$, and $q \\\\geq 2$.\\n\\nIt is worth noting that there is no overlap between the contexts in the database and the contexts in the training set, which may prevent potential data leakage during training process to overestimate the evaluation metrics. The details of dataset construction are introduced in Appendix A.\"}"}
{"id": "acl-2022-long-334", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5 Experiments\\nWe conduct extensive experiments on the constructed datasets. In this section, we present experimental settings, evaluation metrics, model performance, human evaluation, etc. to demonstrate the effectiveness of the proposed models.\\n\\n5.1 Compared Models\\nFor baselines, we select BM25 (Robertson and Zaragoza, 2009) as sparse representations based method, which is widely used in real scenarios in text matching. Based on BM25 system and the two matching methods (QC and QS matching), two retrievers can be obtained, denoted as BM25-QC and BM25-QS respectively. We choose multi-tower models as dense representations based methods. They are bi-encoder based two-tower models for QC matching and QS matching (denoted as BE-QC and BE-QS), and tri-encoder based three-tower model for DQS matching (denoted as TE-DQS). In addition, to demonstrate the advantages of contextual matching, we also report the results of query-response (QR) matching, two retrievers are build based on BM25 system and two-tower model (denoted as BM-QR and BE-QR).\\n\\nThere are three variants of our proposed CFC models, they are the distilled versions of BE-QC, BE-QS and TE-DQS, which are called CFC-QC, CFC-QS and CFC-DQS respectively. The distillation of each student model needs to train the corresponding teacher model. In particular, the distillation from TE-DQS to CFC-DQS requires two teacher models, because the similarity between both query-context and query-response needs to be calculated.\\n\\nWe summarize the details of compared models and provide training details in Appendix B.\\n\\n5.2 Evaluation Metrics\\nFollowing previous work (Xiong et al., 2020; Karpukhin et al., 2020), Coverage@K is used to evaluate whether Top-K retrieved candidates include the ground-truth response. It is equivalent to recall metric $R_M@K$ that often used in fine-grained RS, where $N$ is the size of candidate database. However, Coverage@K is only suitable for evaluating the MC test set, and it is incapable for evaluating the overall retrieval quality due to the one-to-many relationship between context and response. As a supplement, we propose two automated evaluation metrics based on pre-trained models, i.e., Perplexity@K and Relevance@K. For retrieved Top-K responses, DialogGPT (Zhang et al., 2019) is used to calculate the conditional perplexity of the retrieved response given the query. DialogGPT is a language model pre-trained on 147M multi-turn dialogue from Reddit discussion thread and thus very suitable for evaluating our created Reddit dataset. Perplexity@K is the average perplexity of Top-K retrieved responses. In addition to Perplexity, we also evaluate the correlation between the query and retrieved response. We use DialogRPT (Gao et al., 2020), which is pre-trained on large-scale human feedback data with the human-vs-rand task that predicts how likely the response is corresponding to the given context rather than a random response. Relevance@K is the average predicted correlation degree between query and Top-K retrieved responses. Perplexity@K and Relevance@K are average metrics based on all Top-K retrieved responses, so they can reflect the overall retrieval quality.\\n\\n5.3 Overall Performance\\nWe demonstrate the main results in Table 2 and Table 3 and discuss model performance from multiple perspectives.\\n\\nDense vs. sparse\\nIt can be seen that the performance of dense retrievers far exceed that of the BM25 system, which shows rich semantic information of PLMs and additional training can boost the performance of the retriever. For example, compared with BM25 system, the best undistilled dense retrievers (BE-QS) have a obvious improvement in three metrics. For Coverage@K, the Top-500 recall rate of BE-QS on the MC test set of Reddit and Twitter increase by 12.1% and 17.4% absolute compared with BM25-QS. For Perplexity@K, the Top-20 average perplexity of BE-QS on the MC and SC test sets of Reddit is reduced by 8.1 and 8.5 absolute compared with BM25-QS. For Relevance@K, the Top-20 average relevance of BE-QS on the MC and SC test sets on Reddit increase by 6.3% and 6.5% absolute compared with BM25-QS. Coverage@K measures the retriever\u2019s ability to retrieve gold response, while Perplexity@K and Relevance@K measure the overall retrieval quality. Our results show the consistency of the three metrics, namely, the recall rate and the overall retrieval quality have a positive correlation.\\n\\nMatching method\\nCompared with contextual matching, query-response (QR) matching has a...\"}"}
{"id": "acl-2022-long-334", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Automated evaluation metrics on Reddit test set. For MC and SC test set, we both report Perplexity@1/20 and Relevance@1/20; for SC test set, we additionally report Coverage@1/20/100/500. For Coverage@K and Relevance@K, we report the numerator of its percentage, and the larger the better; for Perplexity@K, the smaller the better.\\n\\nTable 3: Automated evaluation metrics on Twitter test set, we report Coverage@1/20/100/500 on the MC test set.\\n\\nmuch lower retrieval recall rate, which is also verified in (Lan et al., 2020). We think it is because that response is usually a short text of one-sentence and contains insufficient information, and there may be little keywords that overlap with the query. Therefore, it is important to consider contextual matching in the RBD system.\\n\\nCompared to QC matching, QS and DQS matching should be encouraged in practice due to the additional information provided by the response. However, the BM25 system can not make good use of the information of response, as BM25-QS model does not show obvious advantages over BM25-QC on both Reddit and Twitter datasets. In contrast, dense retrieval models can effectively utilize the response. For example, BE-QS outperforms BE-QC greatly by 7.9% absolute in terms of Top-500 response retrieval recall rate in MC test set of Reddit.\\n\\nFor QS and DQS matching, there is little difference in performance. Especially for SC test set on Reddit and MC test set on Twitter, the performance difference is minimal. One potential advantage of DQS is that it can utilize positive query-response pairs, whose number is much larger than positive query-context pairs.\\n\\nDistillation benefit\\nWe further focus on the performance gain from fine-to-coarse distillation. The distilled models achieve obvious improvement in all three metrics. An obvious pattern is that the distilled models get more larger improvement with a smaller \\\\( K \\\\). Take Twitter dataset as example, the Top-500 retrieval recall rate of CFC models increase by \\\\( 1.5 \\\\sim 2.4 \\\\) after distillation, while the Top-1 retrieval recall rate increased by \\\\( 4.6 \\\\sim 6.7 \\\\). On Perplexity@\\\\( K \\\\) and Relevance@\\\\( K \\\\), our CFC models has similar performance. The significant improvement in the retrieval recall rate at small \\\\( K \\\\)'s is especially beneficial to fine-grained response selection, because it opens up more possibility to the ranker to choose good response while seeing fewer candidates. The above results indicate that our student models benefit from learning or inheriting fine-grained knowledge from teacher models. To more clearly demonstrate the performance gains of our model after distillation, we provide the specific values of these gains in Table 8 in Appendix C.\\n\\nDifference between Reddit and Twitter\\nSince DialogGPT and DialogRPT is not pre-trained on Twitter, Perplexity@\\\\( K \\\\) and Relevance@\\\\( K \\\\) are not\"}"}
{"id": "acl-2022-long-334", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Impact of parameter sharing on model performance.\\n\\n| Database Size | Coverage@500 |\\n|---------------|--------------|\\n| 1M            | 13.3         |\\n| 2M            | 11.5         |\\n| 5M            | 9.5          |\\n| 10M           | 8.3          |\\n\\nFigure 3: The Impact of database size on Coverage@500 metric of BM25-QS, BE-QS, CFC-QS.\\n\\nSuitable for evaluating Twitter dataset. Therefore, we do not build SC test set for Twitter. Compared to Twitter, the Reddit dataset we use is much larger with more common multi-turn conversations, and significantly higher retrieval difficulty. The Top-500 retrieval recall rate on Twitter reach 60%, while Reddit only reached about 20%, which indicates that the coarse-grained response retrieval task in open domain conversations still has great challenges.\\n\\n6 Further Analysis\\n\\n6.1 Parameter Sharing\\n\\nSharing parameters in dual-encoder structure is a common practice. As shown in Figure 2, for the encoders in the dotted line, sharing parameters may be beneficial. We try parameter sharing settings on the BE-QC and TE-DQS models, respectively. We add two sets of experiments on the MC test set of Reddit, as shown in Table 4. The results show that whether or not to share parameters has little impact on Coverage@K. Therefore, we can share encoder parameters to reduce model complexity with little loss of performance.\\n\\nOur guess is as follows, the sampling strategy (with replacement) create a certain probability that the query and the context are exactly the same, so the multi-tower model can learn that two identical samples are positive samples for each other, even if the parameters of the encoders are not shared.\\n\\n6.2 Effect of Database Size\\n\\nWe discuss the impact of the size of candidate database on the performance of the model. For different candidate database size (from one million to ten million), we compare the Coverage@500 metric of BM25-QS, BE-QS, and CFC-QS on the MC test set of Reddit (Figure 3). It can be seen that Coverage@500 shows a slow downward trend as the database size increases. Increasing the size of the database will not make the model performance drop rapidly, which shows the effectiveness and robustness of our models.\\n\\n6.3 Human Evaluation\\n\\nTo further evaluate and compare our models, we conduct a human evaluation experiment. We randomly select 1000 queries from the MC and SC test set (500 each) of Reddit dataset, and retrieve the Top-1 response by the BM25-QS, BE-QS and CFC-QS models respectively. Three crowd-sourcing workers are asked to score the responses. For each query, the annotator will strictly rank the retrieved responses of the three models. We report the average rank scores (between 1 and 3, the smaller the better) and the winning rate in pairwise comparison. Each two annotators have a certain number (about 200) of overlapping annotated samples. To evaluate the inter-rater reliability, the Cohen's kappa coefficient (Kraemer, 2014) is adopted.\\n\\nTable 5 and Table 6 report the average ranking score of each model and pairwise comparison between models respectively. The average ranking score of CFC-QS is the highest, and CFC-QS can beat BE-QS and BM25 in most cases (74.7%\u223c81.6%), which indicates CFC-QS occupies a clear advantage in Top-1 retrieval. All Co-\"}"}
{"id": "acl-2022-long-334", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"hen's Kappa coefficients is between 0.6 and 0.7, indicating annotators reach moderate agreement. The results of human evaluation further verify the performance improvement brought by distillation to the model. We select several examples with human evaluation as case study and these results are presented in Appendix D.\\n\\n6.4 Retrieval efficiency\\n\\nWe compare the retrieval latency of BM25-QS and BE-QS on the reddit MC test set, which represent the efficiency of the sparse and dense retriever respectively. We fix the batch size to 32 and retrieve top 100 most similar candidates. With the help of FAISS index, the average retrieval time of each batch by BE-QS is 581.8ms. In contrast, the average retrieval time by BM25 system using file index is 1882.6ms, about three times that of BE-QS. This indicates that the dense retriever also has an advantage in retrieval efficiency.\\n\\nThe relatively inferior of dense retriever is that it needs to compute the embeddings of the candidate database and establish the FAISS index, which is quite time-consuming and it takes about 9 hours for BE-QS to handle 10 million candidates with 8 GPUs, while it only takes about 10 minutes to build a BM25 index.\\n\\nSince distillation does not change the structure of the retriever, it will not affect the retrieval efficiency. The cost of distillation is mainly reflected in the training of the teacher model and the extensive forward calculation in the distillation process.\\n\\n7 Conclusion\\n\\nIn this paper, we propose a Contextual Fine-to-Coarse (CFC) distilled model. In CFC model, we adopt matching on both query-response and query-context. Considering the retrieval latency, we use multi-tower architecture to learn the dense representations of queries, responses and corresponding contexts. To further enhance the performance of the retriever, we distill the knowledge learned by the one-tower architecture (fine-grained) into the multi-tower architecture (coarse-grained). We construct two new datasets based on Reddit comment dump and Twitter corpus, and extensive experimental results demonstrate the effectiveness and potential of our proposed model. In the future work, we will further explore how the enhancement of coarse-grained RS can help fine-grained RS.\\n\\nAcknowledgments\\n\\nThis work is partially supported by Natural Science Foundation of China (No.6217020551, No.61906176), Science and Technology Commission of Shanghai Municipality Grant (No.20dz1200600, 21QA1400600, GWV-1.1, 21511101000) and Zhejiang Lab (No.2019KD0AD01).\\n\\nEthical Statement\\n\\nIn this paper, different ethical restrictions deserve discussion. The datasets we created are derived from large dialogue corpus that publicly available on the Internet, and we strictly followed the platform's policies and rules when obtaining data from web platforms. We did not use any author-specific information in our research. Online large dialogue corpus may includes some bias, such as political bias and social bias, and our model might have inherited some forms of these bias. In order to limit these bias as much as possible, we filter controversial articles and removed data with offensive information when possible.\\n\\nReferences\\n\\nAndrzej Bia\u0142ecki, Robert Muir, Grant Ingersoll, and Lucid Imagination. 2012. Apache lucene 4. In SIGIR 2012 workshop on open source information retrieval, page 17.\\n\\nBasma El Amel Boussaha, Nicolas Hernandez, Christine Jacquin, and Emmanuel Morin. 2019. Deep retrieval-based dialogue systems: a short review. arXiv preprint arXiv:1907.12878.\\n\\nMuthuraman Chidambaram, Yinfei Yang, Daniel Cer, Steve Yuan, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018. Learning cross-lingual sentence representations via a multi-task dual-encoder model. arXiv preprint arXiv:1810.12836.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\\n\\nZhenxin Fu, Shaobo Cui, Mingyue Shang, Feng Ji, Dongyan Zhao, Haiqing Chen, and Rui Yan. 2020. Context-to-session matching: Utilizing whole session for response selection in information-seeking dialogue systems. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1605\u20131613.\"}"}
{"id": "acl-2022-long-334", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-334", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To filter boring and dull content and speed up the retrieval speed, we set a limit for the length of contexts and responses. We limit the context to contain at least 5 words and less than 128 words, and the response contains at least 5 words and less than 64 words. It is specially beneficial to limit the length of the response, since according to our statistics, many short responses such as \u201cFair Enough\u201d and \u201cThanks :D\u201d may have large number (tens of thousands) of different contexts.\\n\\nBesides, we also limit the upper limit of the number of contexts corresponding to the response. The number of contexts of each response in the MC test set is limited to no more than 50, which is to prevent the selected responses from being a meaningless universal response. The detailed construction of the two test sets is described in Algorithm 1.\\n\\nTo construct the training set, we need to find out responses that correspond multiple contexts.\\n\\nAlgorithm 1: Construction of SC & MC test set.\\n\\n1: \\\\( R \\\\) : A set of unique responses.\\n2: \\\\( SC' = \\\\emptyset \\\\)\\n3: \\\\( MC' = \\\\emptyset \\\\)\\n4: for each \\\\( r \\\\in R \\\\) do\\n5: \\\\( C_r = \\\\text{FindAllContexts}(r) \\\\) \\\\( \\\\triangleright \\\\) Find all contexts whose response is \\\\( r \\\\).\\n6: if \\\\( |C_r| > 1 \\\\) then\\n7: \\\\( C^{-r}, c = \\\\text{Split}(C_r) \\\\) \\\\( \\\\triangleright \\\\) Random pick one context \\\\( c \\\\) from \\\\( C_r \\\\), the remaining contexts is denoted as \\\\( C^{-r} \\\\).\\n8: \\\\( MC' = MC' \\\\cup \\\\{c, r\\\\} \\\\)\\n9: else\\n10: \\\\( SC' = SC' \\\\cup \\\\{c \\\\in C_r, r\\\\} \\\\)\\n11: end if\\n12: end for each\\n13: \\\\( MC = \\\\text{RandomSample}(MC') \\\\)\\n14: \\\\( SC = \\\\text{RandomSample}(SC') \\\\)\\n15: return \\\\( SC, MC \\\\)\\n\\nWe use dict to implement it, where the key is the response and the value is the list of corresponding contexts. During the training of the multi-tower model, in each iteration, a batch of keys is randomly sampled from the dict. For each key (i.e., each response) in the batch, two contexts are randomly selected from the corresponding value (i.e., the list of contexts), one of which is used as the query and the other is used as a positive context, and the key is used as a positive response. The other contexts and responses in the batch are all negative instances of the query.\\n\\nB Model Details\\n\\nDue to the different matching methods, the training of different retrievers requires slightly different input. Taking BE-QC as an example, given a query, positive and negative contexts are needed to learn the representation of query and contexts, while in BE-QS, positive and negative sessions are required. Besides, the distillation of each student model requires training corresponding teacher model, and the data of training teacher model is consistent with the student model. We summarize the input, output, and training objectives of student and teacher models in Table 7.\\n\\nTo implement the BM25 method, we use Elasticsearch, which is a powerful search engine based on Lucene library (Bia\u0142ecki et al., 2012). For dense\"}"}
{"id": "acl-2022-long-334", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The input, output and training objectives of tower models in this paper. For each matching method, one or two teacher models need to be trained for knowledge distillation.\\n\\n| Dataset | Distillation Coverage@K | Perplexity@K | Relevance@K |\\n|---------|-------------------------|--------------|-------------|\\n|         | Before | After | Top-1 | Top-20 | Top-100 | Top-500 | Top-1 | Top-20 |\\n| Reddit  |        |       |       |\\n| BE-QC   | 99K    | CFC-QC +1.6 +1.2 +1.0 +0.7 | -5.9 -2.6 +3.6 +2.7 |\\n| BE-QS   | 99K    | CFC-QS +2.6 +1.9 +1.3 +0.9 | -5.3 -3.0 +2.8 +2.7 |\\n| TE-DQS  | 99K    | CFC-DQS +2.3 +1.8 +2.9 +1.3 | -4.9 -2.1 +2.1 +2.1 |\\n| Twitter |        |       |       |\\n| BE-QC   | 99K    | CFC-QC +4.6 +2.9 +2.2 +1.7 | - - - - |\\n| BE-QS   | 99K    | CFC-QS +6.7 +4.8 +3.1 +2.4 | - - - - |\\n| TE-DQS  | 99K    | CFC-DQS +6.7 +4.9 +3.0 +1.5 | - - - - |\\n\\nTable 8: Model performance gain after distillation on the MC test set of Reddit and Twitter dataset.\\n\\n| Number | Query | Method | Response |\\n|--------|-------|--------|----------|\\n| Case 1 | My pc Isn't good enough unfortunately | Gold | How old is your computer ? |\\n|        |       | CFC-QS | what are your PC specs ? |\\n|        |       | BE-QS  | Idk but apps aren't great on ps4 . My roku ultra is much faster for whatever reason . |\\n|        |       | BM25   | I' m on the edge . deals are good , but good enough to reactivate my pc ? |\\n| Case 2 | Can I get Spider Man 2099 | Gold | Good trade , thanks ! |\\n|        |       | CFC-QS | You got it PM sent ! |\\n|        |       | BE-QS  | Sure , 1 by Paypal pls : xxx@hotmail.com |\\n|        |       | BM25   | right now , Spider man 2099 is the best written spider man . |\\n| Case 3 | Gut Knife Scorched FT , worth 19keys | Gold | No thanks . Sorry |\\n|        |       | CFC-QS | I only have 15keys . |\\n|        |       | BE-QS  | Add me on steam ! Nvm I added you . |\\n|        |       | BM25   | Nah only keys , knives are meh to me , all of'em . |\\n| Case 4 | The email is returning failures to deliver | Gold | Should be working now . |\\n|        |       | CFC-QS | THE email ? It's just email ! ! |\\n|        |       | BE-QS  | It asks for your username I think , doesn't it ? Try just enter your username you used to register instead of the email and let me know if that works . |\\n|        |       | BM25   | did you get my email with the pic ? |\\n\\nTable 9: Four retrieved cases on our human evaluation set. We report Top-1 retrieved response of the three models as well as gold response. The Rank column is the ranking of the three responses given by the annotator (the lower the better).\"}"}
{"id": "acl-2022-long-334", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"retrieval methods, FAISS (Johnson et al., 2019) toolkit is used to retrieve candidate vectors. All encoders in our tower models (including one-tower, two-tower and three-tower) are initialized with bert-base-uncased, which includes 12 encoder layers, embedding size of 768 and 12 attention heads. For dense models (BE-QC, BE-QS, TE-DQS), we use the same batch size of 32 for Reddit and Twitter, and we train 30 epochs on Reddit and 10 epochs on Twitter. For all teacher models, we use the same batch size of 16, and we train 40 epochs on Reddit and 20 epochs on Twitter. For the distillation (CFC-QC, CFC-QS, CFC-DQS), we train additional 10 epochs on reddit and 5 epochs on twitter respectively, starting from the early checkpoints (20 epochs in Reddit and 5 epochs in Twitter for fair comparison) of BE-QC, BE-QS, TE-DQS. We use Adam (Kingma and Ba, 2014) optimizer with learning rate of 2e-4 and the warmup steps of 200 to optimize the parameters. We set the knowledge distillation temperature to 3 and the rate of distillation loss to 1.0. All experiments are performed on a server with 4 NVIDIA Tesla V100 32G GPUs.\\n\\nC Distillation Benefit\\nTo more clearly show the performance gains of our model after distillation, we present the specific values of these gains in Table 8. Readers can compare the results in this table when reading the Distillation Benefit part in \u00a7 5.3. Positive Coverage@K and Relevance@K, and negative Perplexity@K all represent the improvement of model performance. After the distillation, the accuracy and correlation between the retrieved responses and the query increase, and the conditional perplexity decreases, indicating the huge benefits of distillation.\\n\\nD Case Study\\nAs sparse representations base method, BM25 system tends to retrieve responses that overlaps with the context. For some complicated cases, BM25 cannot correctly retrieve those seemingly unrelated, but are the best answer in the current context. In second case of Table 9, BM25 selects the response that contains \\\"Spider Man 2099\\\" in the query. But in the context of the forum, \\\"Can I get Spider Man 2099\\\" is actually looking for the e-book files of this comic. Compared to the comments of Spider Man 2099 given by BM25, our model retrieves \\\"You got it PM (private message) sent!\\\" is a harder to find, but more accurate response. The third case is an in-game item trading query. In related forums, \\\"keys\\\" are used as currency. \\\"Knife Scorched FT\\\" and \\\"19keys\\\" in query respectively represent an item to be sold and its expected price. The result of BM25 covers \\\"knife\\\" and \\\"key\\\", but the meaning of the whole sentence does not match the query. On the other hand, our model selected \\\"I only have 15keys\\\", a standard bargaining, perfectly match the query. There are also some examples such as case 4. Our model gives worse results than BM25. In case 4, CFC-QS retrieves a worse result, and the response retrieved by BE-QS is relatively better.\"}"}
