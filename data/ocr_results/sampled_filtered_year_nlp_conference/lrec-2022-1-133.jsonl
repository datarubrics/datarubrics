{"id": "lrec-2022-1-133", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Self-Contained Utterance Description Corpus for Japanese Dialog\\n\\nYuta Hayashibe\\nMegagon Labs, Tokyo, Japan, Recruit Co., Ltd.\\n7-3-5 Ginza Chuo-ku, Tokyo, 104-8227, Japan\\nhayashibe@megagon.ai\\n\\nAbstract\\n\\nOften both an utterance and its context must be read to understand its intent in a dialog. Herein we propose a task, Self-Contained Utterance Description (SCUD), to describe the intent of an utterance in a dialog with multiple simple natural sentences without the context. If a task can be performed concurrently with high accuracy as the conversation continues such as in an accommodation search dialog, the operator can easily suggest candidates to the customer by inputting SCUDs of the customer's utterances to the accommodation search system. SCUDs can also describe the transition of customer requests from the dialog log. We construct a Japanese corpus to train and evaluate automatic SCUD generation. The corpus consists of 210 dialogs containing 10,814 sentences. We conduct an experiment to verify that SCUDs can be automatically generated. Additionally, we investigate the influence of the amount of training data on the automatic generation performance using 8,200 additional examples.\\n\\nKeywords: Dialog corpus, Natural language understanding, Utterance description, Text generation\\n\\n1. Introduction\\n\\nTo develop a task-oriented dialog system that responds appropriately to input utterances, the intent of the utterances must be correctly understood. For example, if a customer says, \\\"Looks good, but expensive?\\\", the omitted phrases and the intent of the question must be recognized. Such an understanding of language has been formulated and studied using two major task frameworks.\\n\\nOne is dialog-act classification or slot filling (Liu and Lane, 2016; Gupta et al., 2019; Shi, 2020). For example, MultiWOZ (Budzianowski et al., 2018) defines act types and slots for task-oriented dialogs in seven domains such as hotels and restaurants. They require predefined labels or slots. This task framework is powerful if the dialog proceeds according to predefined scenarios. However, it lacks flexibility because it can only interpret utterances using predefined types and slots. Therefore, interpreting utterances to solve tasks in an exploratory way in consultative dialogs is difficult using this framework.\\n\\nThe other framework is the generation of summary text. This can handle a wide variety of utterances. Recently, studies have employed this framework in the medical domain (Joshi et al., 2020; Song et al., 2020; Krishna et al., 2021) or the call center domain (Favre et al., 2015). In these studies, the whole dialog is interpreted rather than the intentions of individual utterances. This approach can understand dialog after it is over, but is not suitable to comprehend dialog said in real time. Therefore, we propose a task, Self-Contained Utterance Description (SCUD), to simultaneously describe the intent of an utterance in a dialog with multiple simple natural sentences. By automatically generating such sentences, a dialog system can be connected to an information retrieval system that uses sentences as inputs. For example, in a dialog about finding a place to stay, it is possible to suggest suitable accommodation candidates by accurately understanding the customer's intentions in the sentences. As an example of one such task, this study conducts an experiment involving the automatic generation of SCUDs using the collected dialogs. Additionally, we evaluate the influence of the training data amount on the generation performance.\\n\\n2. SCUD\\n\\nSCUDs allow the intent of an utterance to be understood by simply reading them. SCUDs are sentences that align to each sentence in an utterance. Consider the following example:\\n\\n(1) a. [Operator] There are inns where you can enjoy dinner while looking at the night view.\\n\\nb. [Customer] Sounds good, but expensive?\\n\\n(2) a. I want an inn where I can enjoy dinner while looking at the night view.\\n\\nb. I would like to know whether the inn where I can enjoy dinner while looking at the night view is expensive.\\n\\nFor example, the sentence Utterance (1b) is incomprehensible without Utterance (1a). SCUDs for Utterance (1b) are (2a) and (2b). They are expressions that specify the implied meaning and supplement the omitted phrases. By reading the SCUDs, the intent of the utterance can be understood.\"}"}
{"id": "lrec-2022-1-133", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Date: June 6th\\nReservation: 4 nights starting on July 5th\\nArea: Kyoto Prefecture\\nNumber of people: 2 adults, 2 children.\\n\\nO Thank you very much for visiting our service. Please let us know your preferences regarding accommodations.\\n\\nC I would like to visit Kyoto with my husband and two young children.\\n\\nO Is the destination in Kyoto city? Or is it in another area such as the Tango region?\\n\\nC We would like to have a buffet breakfast. (A)\\n\\nO All right. I will look into plans that include breakfast only. Do you have any other requests?\\n\\nC It would be nice to have a convenience store near the inn.\\n\\nO Okay. I will check for accommodations that have a convenience store nearby. What are the dates of your stay and how many nights will you be staying?\\n\\nC We will be staying for 4 nights starting on July 5th.\\n\\nO I am sure you will have time for more than just sightseeing. Young children will be happy to play in the water in July. How about staying along the Kamo River?\\n\\nC That is very good. (B)\\n\\n(B) We can play in the Kamo River, can't we? Didn't know that.\\n\\n(C) ...\"}"}
{"id": "lrec-2022-1-133", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. We would like to have a buffet breakfast.\\n\\n2. Source (A) We want to eat buffet breakfast.\\n\\n3. Source (B) That is very good.\\n\\n4. Context Young children will be happy to play in the water in July. How about staying along the Kamo River?\\n\\n5. SCUD Accommodations along the Kamo River where we can play in the water are very good.\\n\\n6. Source (C) Didn't know that.\\n\\n7. 3.3. Statistics of Collected Dialogs\\n\\nThe minimum number of utterances per dialog was 11, and the maximum was 35. The average was 19.0. The total number of utterances was 4,006. We also annotated the sentence boundaries and counted the number of sentences per dialog. The minimum number of sentences was 33, the maximum was 78, and the average was 51.0. The total number of sentences was 10,814.\\n\\n8. Table 1 shows an example of a dialog in which the operator proposed \\\"accommodations along the Kamo River where you can play in the water\\\" to the customer. This idea was one that the customer had not initially thought of, and it was appreciated.\\n\\n9. In this paper, we refer to utterances as the chunk that a user enters into Slack at one time. The participants are allowed to keep writing multiple utterances.\\n\\n10. Table 2: Examples of annotated SCUD and alignments. The correspondences are indicated by underlines with the same number. \u22c6 indicates parts that require extrasentential information for the generation.\\n\\n11. 4. Annotation of SCUDs\\n\\n4.1. Annotation Methodology\\n\\nFor each customer's utterance, we annotated SCUDs. We simply call the sentence to be interpreted the \\\"source.\\\" Table 2 shows examples. The annotation exploits the predicate-argument structure because it is a basic representation of a sentence (Fillmore, 1967). The predicate-argument structure indicates the relationship between a verbal expression and its case-labeled arguments such as subjects and objects. It can be regarded as a concise interpretation of the sentence, which is suitable to annotate SCUDs.\\n\\nWe created drafts of the SCUDs using the morphological analyzer JUMAN++ (Tolmachev et al., 2020), the dependency and case structure analyzer KNP (Revision.165d699a) (Kawahara and Kurohashi, 2014), and simple rules which convert predicate-argument structures to natural sentences. We then performed the following manual modifications to use them as SCUDs. First, we fixed grammatical and semantic errors, which were due to analysis errors or conversion errors. Second, we complemented omissions of words or phrases. These included the complement of pro-verb and clauses not performed by KNP. All modifications were performed by one professional annotator.\\n\\n4.2. Analysis of Annotated SCUDs\\n\\nBy annotation, we obtained 3,568 SCUDs for 2,848 sources in customer utterances. Sources with multiple predicates can have more than one SCUD. Out of the 2,848 sources, 2,213 sources (77.7%) had a single SCUD, 561 sources (19.7%) had two SCUDs, 64 sources (2.2%) had three SCUDs, and 10 sources (0.4%) had four or more SCUDs. We manually annotated phrase alignment between sources and SCUDs. The underlines in Table 2 show examples. While (A) can generate a SCUD without referring to anything other than the source, (B) and (C) must refer to other sentences marked with \u22c6.\\n\\nWe counted the farthest distance between the utterance containing the source and the one containing the alignments. Table 3 shows that 93.2% SCUDs can be created by referring to the previous (the distance is one) utterance.\\n\\n5. https://github.com/ku-nlp/jumanpp\\n\\n6. https://github.com/ku-nlp/knp\"}"}
{"id": "lrec-2022-1-133", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Evaluation scores. R\u2081, R\u2082, and R_L indicate ROUGE-1, ROUGE-2, and ROUGE-L, respectively.\\n\\n5. Benchmark\\nWe benchmarked a pre-trained encoder-decoder model, T5, on our corpus to investigate how the state-of-the-art language generation model performs for SCUD generation.\\n\\n5.1. Benchmark Settings\\nText-to-Text Transfer Transformer (T5) (Raffel et al., 2020) performs strongly in various tasks. We used the implementation by HuggingFace and the pre-trained Japanese T5 model.\\n\\nAs shown in Section 4.2, 93.2% SCUDs can be generated to refer to the source and the sentence just before the source. Therefore, we concatenated the source and its preceding sentences as its context with special tokens >> and input to the model. Then, we tagged a sentence in an utterance to generate SCUDs with special tokens <target> and </target>. The generated output was all the SCUDs for the source.\\n\\nOf the 210 dialogs in our corpus, we used 125 for training, 41 for development, and 44 for testing. This generated 1,442 (training), 462 (development), and 581 (testing) examples. We performed the Unicode NFKC normalization for all inputs.\\n\\nIn the training, we set the number of tokens for sources to 128, that of SCUD to 64, the batch size to 40, and the training rate to $10^{-3}$. The number of epochs was 20. In the test, we did not limit the number of tokens for sources.\\n\\nBelow, we refer to the model as T5, and evaluate the results with the ROUGE measure (Lin and Hovy, 2003).\\n\\n5.2. Benchmark Results\\nWhen the source was regarded as the output without processing, the average ROUGE-1, ROUGE-2, and ROUGE-L were 0.565, 0.457, and 0.558, respectively as shown in Table 4. Such low scores indicate that SCUD generation requires considerable rewriting.\\n\\nFor the T5 generation, the average ROUGE-1, ROUGE-2, and ROUGE-L were 0.704, 0.587, and 0.688, respectively. Table 5 shows some examples of SCUD generation. The lines labeled \u201cSCUD (T5)\u201d are the predictions of the trained model. Example #1 is an example where the output is perfect.\\n\\nWe randomly sampled 30 of the cases with Rougel scores below 0.6 and analyzed the types of errors. The most common errors were incorrect extraction from the input and insufficient extraction. Each type had eight errors. Example #2 is an example of an error in which incorrect phrases were taken from the context to complement the customer\u2019s utterance. It is likely that the models are not sufficiently trained in how to complement from the context. Example #3 is an example of an error in which multiple SCUDs should be outputted but only one was generated. As described in Section 4.2, only about 20% of sources had multiple SCUDs. This may be because the training for such sources did not work well. Even the state-of-the-art model could not adequately handle these phenomena.\\n\\nThe next most common error was to produce a SCUD with a significantly different meaning from the correct answer. There were seven of this type of error. Example #4 is an example of this type. The correct answer is \u201cbreakfast is not necessary if the budget is exceeded\u201d, but T5 incorrectly generated \u201cbreakfast is necessary.\u201d\\n\\n5.3. Additional Corpus\\nBased on the results, we created an additional corpus consisting of 8,200 examples. These contained errors identified by the error analysis such as those that require viewing the context to generate SCUDs from an utterance and those that generate multiple SCUDs from a single utterance. We use 6,499 examples for training, 811 for development, and 890 for testing.\\n\\nWe trained another SCUD generation model with the additional corpus. This model is referred to as T5+. The average ROUGE-1, ROUGE-2, and ROUGE-L were 0.824, 0.702, and 0.811 for the dialog corpus test examples, respectively. Table 4 shows the scores.\\n\\nIncreasing the number of training cases significantly improved ROUGE-L from 0.688 to 0.811. Table 5 shows examples of SCUD generation. The lines labeled \u201cSCUD (T5+)\u201d are the predictions of the model. We also investigated the performance of the two models on the additional corpus. The average ROUGE-1, ROUGE-2, and ROUGE-L with T5 were 0.64, 0.48, and 0.63, respectively. In contrast, the average ROUGE-1, ROUGE-2, and ROUGE-L with T5+ were 0.83, 0.73, and 0.83, respectively. The Rouge-L score of T5 was 0.626, which is lower than that of the Dialog corpus, suggesting that the corpus included many difficult cases. However, the performance of T5+ was similar to that of the Dialog corpus performance for the additional corpus.\\n\\nSeveral methods may improve the SCUD generation performance. The most straightforward method is to increase the amount of training data as much as possible. Our experiments confirmed that this is a valid approach to enhance the performance.\"}"}
{"id": "lrec-2022-1-133", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"If you're planning to take the bus, can I help you find a hotel that includes a ticket for the bus that goes around Kyoto?\\n\\nYes, sir. I'll check for inns with elevators. Would you like to have dinner?\\n\\nI didn't know there was such a thing! I'm very happy to hear that. Please look for it.\\n\\nI would like to have a common meal. I prefer to have a large amount. I would be nice if they would serve some local sake.\\n\\nSCUD (Gold) I need to find a hotel that includes a ticket for a bus that goes around Kyoto city.\\n\\nThe more food, the better for me.\\n\\nSCUD (T5) I need to find a hotel that includes a ticket for a bus that goes around Kyoto city. (R1: 0.93, R2: 0.86, RL: 0.93)\\n\\nI want you to serve the best local sake. (R1: 0.32, R2: 0.00, RL: 0.21)\\n\\nSCUD (T5+) I need to find a hotel that includes a ticket for a bus that goes around Kyoto city. (R1: 0.93, R2: 0.86, RL: 0.93)\\n\\nThe more food, the better for me. (R1: 1.00, R2: 1.00, RL: 1.00)\\n\\nSCUD (Gold) I haven't decided on an area yet. I'm thinking Ise-Shima would be good.\\n\\nWe don't mind not having breakfast service if it exceeds our budget.\\n\\nSCUD (T5) I'm thinking Ise-Shima would be good. (R1: 0.37, R2: 0.24, RL: 0.37)\\n\\nIf it is beyond our budget, we would like to have breakfast service. (R1: 0.40, R2: 0.17, RL: 0.40)\\n\\nSCUD (T5+) I haven't decided on an area yet. I'm thinking Ise-Shima would be good. (R1: 0.76, R2: 0.63, RL: 0.76)\\n\\nIf breakfast is beyond our budget, we can do without it. (R1: 0.58, R2: 0.27, RL: 0.50)\\n\\nTable 5: Examples of the SCUD generation for underlined sentences. Pair of \\\"Context\\\" and \\\"Source\\\" is the input and \\\"SCUD\\\" is the output. T5 is the model trained only with dialogs and T5+ is the model trained with dialogs and additional data. R1, R2, and RL indicate ROUGE-1, ROUGE-2, and ROUGE-L, respectively.\"}"}
{"id": "lrec-2022-1-133", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The second is to change the metric used to optimize the training. In this experiment, we directly optimized the evaluation index ROUGE. However, a classifier, which distinguishes between human-made sentences and system outputs, can be created to enhance fluency like Generative Adversarial Networks (Goodfellow et al., 2014). Then the predicted value of the classifier can be used for training. Another possibility is to build a classifier to determine whether the complement of ellipses is sufficient and use that classifier.\\n\\nThe third is to use auxiliary information such as alignment for training. As described in Section 4.2, we manually annotated phrase alignment between sources and SCUDs. If this annotation can be exploited, it may be possible to efficiently learn the information needed to complement the context.\\n\\n6. Related Work\\n\\n6.1. Label Classification and Slot Filling\\n\\nTraditionally the task of capturing utterance intentions is designed as a label classification and slot filling task. This task has been annotated into corpora for training and evaluation, such as DSTC2 Corpus (Henderson et al., 2014), MultiWOZ (Budzianowski et al., 2018), the ICSI Meeting Recorder Dialog Act (MRDA) Corpus (Shriberg et al., 2004), and Action-Based Conversations Dataset (Chen et al., 2021). In these corpora, utterances are understood by classifying them into pre-defined labels and filling in the slots. An advantage of this approach is that it is easy to handle due to the structured nature of the dialogs, which is sufficient when the topic is limited to a specific range. However, designing such a system is difficult for exploratory dialog.\\n\\n6.2. Summarization\\n\\nThe method of understanding dialog by generating natural sentences is well studied in the field of dialog summarization. Summarization describes the main parts of the whole dialog while deleting the minor ones. Each sentence in the utterance is given a description, even if it has nothing to do with the conclusion of the dialog. The corpus created by Fukunaga et al. (2018) is relevant to our study. Their corpus associated users' implicit intents to labels. Specifically, they focused on a scenario of a real estate search and associated utterances with labels. For example, the utterance, \u201cI want to live alone.\u201d is associated with the \u201cone-bedroom\u201d label. In contrast, we aim to understand hidden intents in natural language in the form of SCUDs. The corpus created by (Yamamura and Shimada, 2018) is also relevant to our study. They annotated summarization for transcriptions of verbal dialogs per topic. Our annotation focuses on understanding users' intents rather than summarization.\\n\\n6.3. Ellipsis Resolution\\n\\nTo generate SCUDs, both the target and other sentences must be referenced to generate the omitted expressions, which is ellipsis resolution. This task has been studied as semantic role labeling and predicate-argument structure analysis (PASA) (Gildea and Jurafsky, 2002; Kawahara and Kurohashi, 2004; Iida et al., 2005; Taira et al., 2008). From sentences, they extract relations such as \u201cwho did what to whom\u201d that hold between a predicate and its arguments constituting a semantic unit of a sentence. Although corpora annotated with texts such as newspaper articles (Baker et al., 1998; Palmer et al., 2005; Kawahara et al., 2002; Iida et al., 2007), blogs (Hashimoto et al., 2011), and web texts (Hangyo et al., 2012) are publicly available, most studies have focused on written texts.\\n\\nImamura et al. (2014) constructed a corpus of 285 dialogs and performed PASA on the dialogs. However, these tasks are designed to fill slots for predicates by extracting phrases. They are not designed to phrases based on existing texts. In addition, since almost all PASAs do not generate arguments but extract phrases, they cannot handle cases where complex expressions are omitted.\"}"}
{"id": "lrec-2022-1-133", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gupta, A., Hewitt, J., et al. (2019). Simple, Fast, Accurate Intent Classification and Slot Labeling for Goal-Oriented Dialogue Systems. In SIGdial, pages 46\u201355.\\n\\nIida, R., Inui, K., et al. (2005). Anaphora Resolution by Antecedent Identification Followed by Anaphoricity Determination. TALLIP, 4(4):417\u2013434.\\n\\nImamura, K., Higashinaka, R., et al. (2014). Predicate-argument structure analysis with zero-anaphora resolution for dialogue systems. In COLING, pages 806\u2013815.\\n\\nKawahara, D. and Kurohashi, S. (2004). Zero Pronoun Resolution Based on Automatically Constructed Case Frames and Structural Preference of Antecedents. In IJCNLP, pages 334\u2013341.\\n\\nKawahara, D. and Kurohashi, S. (2014). A Fully-Lexicalized Probabilistic Model for Japanese Syntactic and Case Structure Analysis. Journal of Natural Language Processing, 21(4):799\u2013815.\\n\\nLin, C.-Y. and Hovy, E. (2003). Automatic evaluation of summaries using N-gram co-occurrence statistics. In NAACL, pages 71\u201378.\\n\\nLiu, B. and Lane, I. (2016). Joint Online Spoken Language Understanding and Language Modeling With Recurrent Neural Networks. In SIGdial, pages 22\u201330.\\n\\nRaffel, C., Shazeer, N., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nShi, H. (2020). A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems. In SIGdial, pages 272\u2013277.\\n\\nTaira, H., Fujita, S., et al. (2008). A Japanese Predicate Argument Structure Analysis Using Decision Lists. In EMNLP, pages 523\u2013532.\\n\\nTolmachev, A., Kawahara, D., et al. (2020). Design and Structure of The Juman++ Morphological Analyzer Toolkit. Journal of Natural Language Processing, 27(1):89\u2013132.\\n\\nLanguage Resource References\\n\\nBaker, C. F., Fillmore, C. J., et al. (1998). The Berkeley FrameNet Project. In ACL, pages 86\u201390.\\n\\nChen, D., Chen, H., et al. (2021). Action-based conversations dataset: A corpus for building more in-depth task-oriented dialogue systems. In NAACL, pages 3002\u20133017.\\n\\nFavre, B., Stepanov, E., et al. (2015). Call centre conversation summarization: A pilot task at multiling 2015. In SIGdial, pages 232\u2013236.\\n\\nHangyo, M., Kawahara, D., et al. (2012). Building a Diverse Document Leads Corpus Annotated with Semantic Relations. In PACLIC, pages 535\u2013544.\\n\\nHashimoto, C., Kurohashi, S., et al. (2011). Construction of a Blog Corpus with Syntactic, Anaphoric, and Sentiment Annotations (in Japanese). Journal of Natural Language Processing, 18(2):175\u2013201.\\n\\nHenderson, M., Thomson, B., et al. (2014). The Second Dialog State Tracking Challenge. In SIGdial, pages 263\u2013272.\\n\\nIida, R., Komachi, M., et al. (2007). Annotating a Japanese Text Corpus with Predicate-Argument and Coreference Relations. In Proceedings of the Linguistic Annotation Workshop, pages 132\u2013139.\\n\\nJoshi, A., Katariya, N., et al. (2020). Dr. Summarize: Global summarization of medical dialogue by exploiting local structures. In EMNLP, pages 3755\u20133763.\\n\\nKawahara, D., Kurohashi, S., et al. (2002). Construction of a Japanese Relevance-tagged Corpus. In LREC, pages 2008\u20132013.\\n\\nKrishna, K., Khosla, S., et al. (2021). Generating SOAP notes from doctor-patient conversations using modular summarization techniques. In ACL, pages 4958\u20134972.\\n\\nPalmer, M., Gildea, D., et al. (2005). The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71\u2013106.\\n\\nShriberg, E., Dhillon, R., et al. (2004). The ICSI Meeting Recorder Dialog Act (MRDA) Corpus. In SIGdial, pages 97\u2013100.\\n\\nSong, Y., Tian, Y., et al. (2020). Summarizing medical conversations via identifying important utterances. In COLING, pages 717\u2013729.\\n\\nYamamura, T. and Shimada, K. (2018). Annotation and Analysis of Extractive Summaries for the Kyutech Corpus. In LREC, pages 3216\u20133220.\"}"}
