{"id": "acl-2022-short-41", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References\\n\\nXiang Yue1,\u2217, Xiaoman Pan2, Wenlin Yao2, Dian Yu2, Dong Yu2, and Jianshu Chen2\\n\\n1The Ohio State University\\n2Tencent AI Lab\\n\\nyue.149@osu.edu\\n{xiaomanpan,wenlinyao,yudian,dyu,jianshuchen}@tencent.com\\n\\nAbstract\\n\\nWe consider the problem of pretraining a two-stage open-domain question answering (QA) system (retriever + reader) with strong transfer capabilities. The key challenge is how to construct a large amount of high-quality question-answer-context triplets without task-specific annotations. Specifically, the triplets should align well with downstream tasks by: (i) covering a wide range of domains (for open-domain applications), (ii) linking a question to its semantically relevant context with supporting evidence (for training the retriever), and (iii) identifying the correct answer in the context (for training the reader). Previous pretraining approaches generally fall short of one or more of these requirements. In this work, we automatically construct a large-scale corpus that meets all three criteria by consulting millions of references cited within Wikipedia. The well-aligned pretraining signals benefit both the retriever and the reader significantly. Our pretrained retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our pretrained reader, the entire system improves by up to 4% in exact match.\\n\\n1Introduction\\n\\nOpen-domain question answering (QA) aims to extract the answer to a question from a large set of passages. A simple yet powerful approach adopts a two-stage framework (Chen et al., 2017; Karpukhin et al., 2020), which first employs a retriever to fetch a small subset of relevant passages from large corpora (i.e., retriever) and then feeds them into a reader to extract an answer (text span) from them. Due to its simplicity, a sparse retriever such as TF-IDF/BM25 is generally used together with a trainable reader (Min et al., 2019). However, recent advances show that transformer-based dense retrievers trained on supervised data (Karpukhin et al., 2020) can greatly boost the performance, which better captures the semantic relevance between the question and the correct passages. Such approaches, albeit promising, are restricted by the limited amount of human annotated training data.\\n\\nInspired by the recent progresses of language models pretraining (Devlin et al., 2019; Lee et al., 2019; Guu et al., 2020; Sachan et al., 2021), we would like to address the following central question: can we pretrain a two-stage open-domain QA system (retriever + reader) without task-specific human annotations?\\n\\nUnlike general language models, pretraining such a system that has strong transfer capabilities to downstream open-domain QA tasks is challenging. This is mainly due to the lack of well-aligned pretraining supervision signals. In particular, we need the constructed pretraining dataset (in the form of question-answer-context triplets) to:\\n\\n(i) cover a wide range of domains (for open-domain applications),\\n(ii) link a question to its semantically relevant context with supporting evidence (for training the retriever), and\\n(iii) identify the correct answer in the context (for training the reader).\\n\\nThere have been several recent attempts in addressing these challenges. ORQA (Lee et al., 2019) creates pseudo query-passage pairs by randomly sampling a sentence from a paragraph and treating the sampled sentence as the question while the rest sentences as the context. REALM (Guu et al., 2020) adopts a retrieve-then-predict approach, where the context is dynamically retrieved during training and an encoder (reader) predicts\"}"}
{"id": "acl-2022-short-41", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"The boarding crew freed 14 Iranian and Pakistani fishermen who had been held as hostages for over two months.\"}"}
{"id": "acl-2022-short-41", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of pretraining and finetuning data.\\n\\n| Data Type       | Dataset          | Train  | Dev   | Test  |\\n|-----------------|------------------|--------|-------|-------|\\n| Pretraining     | C-MORE           | 2.96M  | 40K   | -     |\\n| Finetuning      | QA Data          |        |       |       |\\n|                 | NaturalQuestion  | 58,880 | 8,757 | 3,610 |\\n|                 | TriviaQA         | 60,413 | 8,837 | 11,313|\\n|                 | WebQuestion      | 2,474  | 361   | 2,032 |\\n\\nIn our study, we extract around six million statement-reference pairs from Wikipedia. We filter the pairs whose reference documents are not reachable and finally obtain around three million statement-reference pairs (see statistics in Appendix Table 1). The data collection method we proposed is very general and therefore can be easily extended to other domains, e.g., WikiEM (wikem.org) for medical domain or other languages, e.g., Baidu Baike (baike.baidu.com) for Chinese.\\n\\n2.2 QAC Triplets Construction\\n\\nWe now explain how to further convert the statement-reference pairs into question-answer-context pairs. Inspired by previous unsupervised extractive QA work (Lewis et al., 2019), we extract entities as potential answers to construct pseudo question-answer-context pairs where an answer span is extracted from the context given a question to accommodate the extractive QA setting. Specifically, we first adopt an off-the-shelf named entity recognition tool spaCy (Honnibal and Montani, 2017) to identify entities in each query. Next, we filter the entities that do not appear in the evidence based on string matching. If multiple entities are found, we sample one of them as the potential answer to the query. The sampled entity in the query is replaced by an interrogative phrase based on the entity type (e.g., a [DATE] entity will be replaced by phrases such as \\\"when\\\", \\\"what date\\\". In this way, we can construct question-answer-context triplets to train open-domain QA models. See more question reformation rules in Appendix Table 5.\\n\\n3 Experiment\\n\\n3.1 Experimental Setup\\n\\nPretraining Model Architecture. Since conceptually the construed triplets is in the same format as the annotated QA data, they can be used to pretrain any existing neural open-domain QA model. Here, we adopt DPR (Karpukhin et al., 2020), which consists of a dual-encoder as the retriever and a BERT reader, considering its effectiveness and popularity. Specifically, the retriever first retrieves top-k (up to 400 in our experiment) passages, and the reader assigns a passage score to each retrieved passage and extracts an answer with a span score. The span with the highest passage selection score is regarded as the final answer. The reader and retriever can be instantiated with different models and we use BERT-base-uncased for both of them following (Karpukhin et al., 2020).\\n\\nPretraining Data Processing. For our extracted pseudo question-answer-context triplets, sometimes the context (reference document) is too long to fit into a standard BERT (maximum 512 tokens) in the DPR model. Thus, we chunk a long document into \\\\( n \\\\)-word text blocks with a stride of \\\\( m \\\\). Without loss of generality, we use multiple combinations of \\\\( n \\\\) and \\\\( m \\\\):\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\{128, 256, 512\\\\}, \\\\\\\\\\n\\\\{64, 128, 256\\\\}\\n\\\\end{align*}\\n\\\\]\\n\\nThen we calculate relevance scores (using BM25) of the derived blocks with the question and select the most relevant block as the context. Note that the retrieval step is done within the single document (usually less than 20 text blocks). In contrast, the baseline model (Section 3.2) - sparse retriever BM25 - looks up the entire knowledge corpus (20M text blocks). In this way, we can automatically collect the most relevant context that supports the query from a long article.\\n\\nFinetuning QA Datasets. We consider three popular open-domain QA datasets for finetuning: NaturalQuestions (NQ) (Kwiatkowski et al., 2019), TriviaQA (TQA) (Joshi et al., 2017), and WebQuestions (WebQ) (Berant et al., 2013), whose statistics are shown in Table 1. Following the setting of DPR (Karpukhin et al., 2020), we use the Wikipedia as the knowledge source and split Wikipedia articles into 100-word units for retrieval. All the datasets we use are the processed versions from the DPR implementation.\\n\\nOverlap between Pretraining and Finetuning Datasets. Though both C-MORE and downstream QA data are constructed based on Wikipedia, the overlap between them would be very little. C-MORE extracts queries from Wikipedia while the queries of downstream QA data are annotated by human. C-MORE extracts contexts from the external referenced pages (general Web) while the downstream QA data extract contexts from Wikipedia.\\n\\nImplementation Details. For pretraining, we set...\"}"}
{"id": "acl-2022-short-41", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Overall retrieval performance of different models. Results marked with \\\"*\\\" are from DPR (Karpukhin et al., 2020), \\\"-\\\" are from (Sachan et al., 2021) and \\\"-\\\" means it does not apply to the current setting.\\n\\n| Model Architecture | Retriever | Reader | NQ | TQA | WebQ |\\n|--------------------|-----------|--------|----|-----|------|\\n| BM25 - 59.1*       | 66.9*     | 55.0*  |    |     |      |\\n| ORQA (Lee et al., 2019) | 50.67 | -      |    |     |      |\\n| REALM (Guu et al., 2020) | 59.87 | 68.23  |    | 74.9 | 79.4 |\\n| C-MORE Wikipedia 61.9 72.2 62.7 75.8 81.3 78.5 |\\n| DPR-NQ NaturalQuestion - 69.7 69.0 - 79.2 78.8 |\\n| DPR-TQA TriviaQA 69.2 - 71.5 80.3 - 81.0 |\\n| DPR-WebQ WebQ 56.1 66.1 - 70.7 77.6 - |\\n| DPR-supervised Supervised Data 78.4* 79.4* 73.2* 85.4* 85.0* 81.4* |\\n\\nTable 3: End-to-end QA performance based on different retrievers and readers. Note that we only test the effectiveness of C-MORE based on the DPR (Karpukhin et al., 2020) model architecture. ORQA and REALM are listed here as references. The retriever of Row 4 is BM25, which does not involve either pretraining or finetuning.\\n\\n3.2 Retrieval Performance\\n\\nWe consider three settings to demonstrate the usefulness of our pretrained retriever.\\n\\nUnsupervised. We assume no annotated training QA pairs are available. In this setting, we compare our method with existing unsupervised retrievers: a sparse retriever BM25 and two pretrained dense retrievers ORQA and REALM.\\n\\nDomain Adaptation. We consider the condition in which there are QA training pairs in the source domain but no training data in the target domain. The task is to obtain good retrieval performance on the target test set only using source training data. We compare our method with two baselines: one is to directly train a dense retriever on the source domain while the other is to first pretrain a dense retriever on our constructed corpus and then finetune it on the source domain training set.\\n\\nSupervised. In this setting, all the annotated QA training instances are used. Similar to the previous setting, we compare a supervised retriever with and without our C-MORE pretraining.\\n\\nFor all settings, we report the top-k retrieval accuracy ($k \\\\in \\\\{20, 100\\\\}$) on the test set following (Karpukhin et al., 2020). See the overall retrieval performance of different models in each setting in Table 2. We have the following observations.\\n\\nIn the unsupervised setting, compared with the strong sparse retrieval baseline BM25, our pretrained dense retriever shows significant improvement. For example, we obtain around 7% absolute improvement in terms of both Top-20 and Top-100 accuracy on the WebQuestion dataset. Compared with pretrained dense retrievers (i.e., ORQA and REALM), our pretrained model outperforms them by a large margin. This is not surprising as our pretraining data contain better aligned retrieval su-\"}"}
{"id": "acl-2022-short-41", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pervision signals: reference documents often have supporting evidence for the question while their retrieval training signals are relatively indirect. In the domain adaptation and supervised settings, our pretrained dense retriever provides a better finetuning initialization and leads to improvement compared with randomly initialized DPR models. Another surprising result is that our pretrained dense retriever even outperforms some DPR domain adaptation models. For example, on the TriviaQA testing set, our pretrained DPR model achieves 72.2% top-20 and 81.3% top-100 accuracy while the DPR-NQ model obtains 69.7% and 79.2% respectively. This indicates that our pretrained dense retriever can generalize well even without using any annotated QA instances. All the results demonstrate the usefulness and generalization of our pretrained dense retriever for open-domain QA tasks.\\n\\n3.3 End-to-End QA performance\\n\\nWe now examine how our pretrained retriever and reader improve the end-to-end QA performance, measured in exact match (EM). The results are shown in Table 3, from which we make the following observations. (i) Surprisingly, our fully-unsupervised system (pretrained retriever + pretrained reader) shows a certain level of open-domain QA ability (see row #3). For example, on TriviaQA, our fully-unsupervised system can answer around 25% of questions correctly. (ii) Compared to the system with BM25 retriever (row #4), the one with our pretrained dense retriever (line #5) retrieves more relevant passages, leading to better QA performance. (iii) Initializing either the retriever or the reader from our pretrained checkpoint can lead to further improvement (rows #6-#8). For example, on the TriviaQA and WebQuestion datasets, our entire pipeline pretrain leads to about 4% absolute gain in terms of EM. Note that on the WebQuestion dataset, all the DPR models perform worse than REALM, this is because of the limited training data of WebQuestion. The issue can be easily solved by adding Multi datasets for finetuning according to (Karpukhin et al., 2020).\\n\\n3.4 Computational Resource Comparison\\n\\nIn addition to the performance gain, another benefit of C-MORE is its training scalability. We compare the C-MORE pretraining with ORQA and REALM in terms of computational resources they use in Table 4. As can be seen, C-MORE only requires reasonable GPU computational resources, which could be normally conducted on an academic-level computational platform. On the contrary, due to the lack of direct retrieval supervision, ORQA and REALM often needs more computational resources and requires more training steps to converge.\\n\\n4 Conclusion\\n\\nThis paper proposes an effective approach for pretraining open-domain QA systems. Specifically, we automatically construct three million pseudo question-answer-context triplets from Wikipedia that align well with open-domain QA tasks. Extensive experiments show that pretraining a widely-used open-domain QA model (DPR) on our constructed data achieves promising performance gain in both retrieval and QA accuracies. Future work includes exploring the effectiveness of the constructed data on more open-domain QA models (e.g., REALM) and training strategies (e.g., joint optimizing the retriever and reader).\\n\\nAcknowledgements\\n\\nThe authors would thank the anonymous reviewers for their insightful comments and suggestions. The authors would also thank the colleagues in Tencent AI Lab for their internal discussions and feedback.\\n\\nReferences\\n\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533\u20131544.\\n\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\u20131879.\"}"}
{"id": "acl-2022-short-41", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT\u201919, pages 4171\u20134186. Association for Computational Linguistics.\\n\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909.\\n\\nMatthew Honnibal and Ines Montani. 2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. To appear.\\n\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u20131611.\\n\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769\u20136781.\\n\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466.\\n\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096.\\n\\nPatrick Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019. Unsupervised question answering by cloze translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4896\u20134910.\\n\\nZhongli Li, Wenhui Wang, Li Dong, Furu Wei, and Ke Xu. 2020. Harvesting and refining question-answer pairs for unsupervised qa. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6719\u20136728.\\n\\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. A discrete hard em approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2851\u20132864.\\n\\nDevendra Singh Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L. Hamilton, and Bryan Catanzaro. 2021. End-to-end training of neural retrievers for open-domain question answering. In ACL/IJCNLP 2021, pages 6648\u20136662. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-short-41", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| NER Type | Candidate Question Phrases |\\n|----------|---------------------------|\\n| CARDINAL | \u201cwhat\u201d,                   |\\n| DATE     | \u201cwhen\u201d, \u201cwhat time\u201d,      |\\n|          | \u201cwhat date\u201d,              |\\n| EVENT    | \u201cwhat event\u201d, \u201cwhat\u201d,     |\\n|          | \u201cwhich event\u201d,            |\\n| FAC      | \u201cwhere\u201d, \u201cwhat buildings\u201d |\\n| GPE      | \u201cwhere\u201d, \u201cwhat country\u201d   |\\n| LANGUAGE | \u201cwhat language\u201d, \u201cwhich language\u201d |\\n| LAW      | \u201cwhich law\u201d, \u201cwhat law\u201d   |\\n| LOC      | \u201cwhere\u201d, \u201cwhat location\u201d  |\\n|          | \u201cwhich place\u201d, \u201cwhat place\u201d |\\n| MONEY    | \u201chow much money\u201d, \u201chow much\u201d |\\n| NORP     | \u201cwhat\u201d, \u201cwhat groups\u201d, \u201cwhere\u201d |\\n| ORDINAL  | \u201cwhat rank\u201d, \u201cwhat\u201d       |\\n| ORG      | \u201cwhich organization\u201d, \u201cwhat organization\u201d, \u201cwhat\u201d |\\n| PERCENT  | \u201cwhat percent\u201d, \u201cwhat percentage\u201d |\\n| PERSON   | \u201cwho\u201d, \u201cwhich person\u201d     |\\n| PRODUCT  | \u201cwhat\u201d, \u201cwhat product\u201d    |\\n| QUANTITY | \u201chow many\u201d, \u201chow much\u201d    |\\n| TIME     | \u201cwhen\u201d, \u201cwhat time\u201d       |\\n| WORK_OF_ART | \u201cwhat\u201d, \u201cwhat title\u201d    |\"}"}
