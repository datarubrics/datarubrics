{"id": "emnlp-2024-main-679", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do LLMs Overcome Shortcut Learning?\\nAn Evaluation of Shortcut Challenges in Large Language Models\\n\\nYu Yuan1, Lili Zhao1, Kai Zhang1,2, Guangting Zheng2, Qi Liu1\\n\u2020\\n1 State Key Lab of Cognitive Intelligence, University of Science and Technology of China\\n2 School of Computer Science and Technology, University of Science and Technology of China\\n\\n{yyhappier,liliz,zgt}@mail.ustc.edu.cn\\n{kkzhang08,qiliuql}@ustc.edu.cn\\n\\nAbstract\\nLarge Language Models (LLMs) have shown remarkable capabilities in various natural language processing tasks. However, LLMs may rely on dataset biases as shortcuts for prediction, which can significantly impair their robustness and generalization capabilities. This paper presents Shortcut Suite, a comprehensive test suite designed to evaluate the impact of shortcuts on LLMs' performance, incorporating six shortcut types, five evaluation metrics, and four prompting strategies. Our extensive experiments yield several key findings: 1) LLMs demonstrate varying reliance on shortcuts for downstream tasks, significantly impairing their performance. 2) Larger LLMs are more likely to utilize shortcuts under zero-shot and few-shot in-context learning prompts. 3) Chain-of-thought prompting notably reduces shortcut reliance and outperforms other prompting strategies, while few-shot prompts generally underperform compared to zero-shot prompts. 4) LLMs often exhibit overconfidence in their predictions, especially when dealing with datasets that contain shortcuts. 5) LLMs generally have a lower explanation quality in shortcut-laden datasets, with errors falling into three types: distraction, disguised comprehension, and logical fallacy. Our findings offer new insights for evaluating robustness and generalization in LLMs and suggest potential directions for mitigating the reliance on shortcuts. The code is available at https://github.com/yyhappier/ShortcutSuite.git.\\n\\n1 Introduction\\nThe field of Natural Language Processing (NLP) is experiencing rapid advancements, driven by the emergence of Large Language Models (LLMs) such as GPT (OpenAI, 2023; Achiam et al., 2023), Gemini (Team et al., 2023), and LLaMA (Touvron et al., 2023) series. These models have been pivotal in revolutionizing a wide array of tasks by leveraging techniques like In-Context Learning (ICL) (Brown et al., 2020) and Chain-of-Thought (CoT) promptings (Wei et al., 2022; Kojima et al., 2022), demonstrating exceptional capabilities without parameter updates. Despite these advances, the research on the robustness and generalization ability of LLMs across different contexts remains limited. Models with poor robustness and generalization may rely on \u201cshortcut learning,\u201d where they develop decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios (Geirhos et al., 2020). Therefore, evaluating LLMs performance in the face of shortcut information is crucial for understanding their robustness and generalization capabilities.\\n\\nA recent study investigates the reliance of LLMs on shortcuts or spurious correlations within prompts (Tang et al., 2023). However, this research falls short of providing an exhaustive evaluation across a broad spectrum of LLMs and varied prompting contexts, focusing solely on ICL experiments. Furthermore, it only considers relatively...\"}"}
{"id": "emnlp-2024-main-679", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"simple shortcuts such as letters or signs. Consequently, its evaluation lacks comprehensiveness and granularity.\\n\\nTo address this, we introduce Shortcut Suite, an in-depth test suite designed to evaluate the performance of different LLMs across six shortcuts, five metrics, and four prompt settings. Extensive experiments on Shortcut Suite reveal that LLMs tend to capture spurious correlations between source text and particular labels, indicating a prevalence of shortcut learning. For example, as shown in Figure 1, Gemini-Pro resorts to matching subsequences (the professor recommended the bankers) in a Natural Language Inference (NLI) task rather than comprehending the clause structure or delving into the sentence's semantic content. This tendency of LLMs to capture spurious correlations can significantly impair their performance. In this paper, we conduct a comprehensive evaluation of LLMs' behavior concerning shortcut learning from the following perspectives.\\n\\nFirst, to identify the reliance of LLMs on shortcuts in downstream tasks, we collect six datasets containing different shortcuts and analyze the accuracy of LLMs on these datasets. We find a notable performance drop across various shortcuts, especially Constituent and Negation shortcuts, in some cases by more than 40%. Moreover, in the Position dataset, LLMs demonstrate a propensity for shortcut learning behavior by prioritizing the beginning of sentences while neglecting the end, revealing a vulnerability to additional information within sentences. Furthermore, an analysis of the distribution of LLMs' predictions revealed inherent biases, with the LLMs favoring certain labels over others even in a balanced standard dataset.\\n\\nSecond, we perform comprehensive evaluation metrics to assess the impact of shortcuts on LLMs. In addition to accuracy, we introduce three novel metrics to assess the explanatory power of LLMs: Semantic Fidelity Score (SFS), Internal Consistency Score (ICS), and Explanation Quality Score (EQS). Our analyses using these metrics reveal that LLMs' explanations often contain contradictions. Furthermore, we prompt LLMs to report their confidence levels and consistently find that they are overconfident in their predictions.\\n\\nThird, we compare the performance of different LLMs and different prompting strategies in shortcut learning. Closed-source and some open-source LLMs excel on standard datasets but falter on those with shortcuts. Surprisingly, larger LLMs are more prone to utilize shortcuts under zero-shot and few-shot ICL prompts. We find that LLMs are less affected by shortcuts under CoT settings than others. Notably, LLMs often demonstrate inferior performance in few-shot scenarios compared to zero-shot scenarios.\\n\\nFinally, we summarize three error types of LLMs in shortcut learning by checking their CoT responses: distraction, disguised comprehension, and logical fallacy. These errors predispose LLMs to adopt shortcuts, undermining their robustness.\\n\\nRelated Work\\n\\nShortcut Learning in PLMs.\\n\\nShortcuts are decision rules that perform well on Independent and Identically Distributed (IID) test data but fail on Out-Of-Distribution (OOD) tests, revealing a mismatch between intended and learned solutions (Geirhos et al., 2020). Recent studies have shown that Pre-trained Language Models (PLMs) tend to exploit dataset biases as shortcuts to make predictions (Geirhos et al., 2020; Ribeiro et al., 2020), leading to low generalization for OOD samples in various NLP tasks, such as NLI (McCoy et al., 2020), question-answering (Jia and Liang, 2017; Sen and Saffari, 2020), reading comprehension (Lai et al., 2021) and coreference inference (Zhao et al., 2018). For example, NLI models tend to predict the contradiction label if the test samples contain negation words. Several approaches have been proposed to address this problem. He et al. (2019) presented a debiasing algorithm called DRiFt based on residual fitting. Du et al. (2021) proposed a shortcut mitigation framework LTGR to suppress the model from making overconfident predictions for shortcut samples. Zhao et al. (2024) introduced COMI to reduce the models reliance on shortcuts and enhance its ability to extract underlying information integrated with standard Empirical Risk Minimization. Yue et al. (2024) proposed SSR to boost rationalization by discovering and exploiting potential shortcuts.\\n\\nShortcut Learning in LLMs.\\n\\nDu et al. (2023) provided a review of recent developments that address the robustness challenge of LLMs. The most related work was the study investigating the reliance of LLMs on shortcuts within in-context learning (Tang et al., 2023). Our work differs from it in the following ways: First, their experiments were conducted on a limited model scope (GPT2...\"}"}
{"id": "emnlp-2024-main-679", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Framework to Generate Shortcuts\\n\\nGiven a premise $q$, a hypothesis $h$, and a universally true statement $s$ ($s \\\\equiv \\\\top$) that may contain a certain shortcut, the logical relations are preserved upon their conjunction. Specifically, if $q$ and $h$ have the target label $l$, denoted as \\\\{ $(q, h, y)$ | $y = l$ \\\\}, then $q \\\\land s$ maintains the label \\\\{ $(q \\\\land s, h, y)$ | $y = l$ \\\\} since $q \\\\land s \\\\equiv q \\\\land \\\\top \\\\equiv q$.\\n\\nThus, the source text has two mappings for the target label $l$. The model can either use the semantic relationship between the text and label ($x \\\\rightarrow l$) or the injected shortcut ($s \\\\rightarrow l$) for inference.\\n\\n## 4 Shortcut Suite\\n\\nAs NLI is well positioned to serve as a benchmark task for research on NLP and can encapsulate the entire spectrum of the six identified shortcuts, we mainly anchor our framework on it. We also explore other tasks in Appendix C. Building on previous research, we create six datasets with different shortcuts and develop five metrics to investigate LLMs' shortcut learning behavior and understand their robustness generalization capabilities.\\n\\n### 4.1 Dataset Creation\\n\\nWe present six types of shortcuts in Table 1, each with an illustrative definition and an example. Standard.\\n\\nThe Multi-Genre Natural Language Inference (MultiNLI) ($\\\\text{Williams et al.}, 2018$) dataset serves as a benchmark for assessing models on NLI, encompassing ten genres of English. For a focused assessment, we have curated a balanced selection comprising 3000 samples from the development subset of MultiNLI.\\n\\nLexical Overlap & Subsequence & Constituent\\n\\nFor these three sets, we utilize the Heuristic Analysis for NLI Systems (HANS) ($\\\\text{McCoy et al.}, 2020$) dataset for evaluation. HANS is designed to diagnose the use of fallible structural heuristics and is annotated with two labels only (entailment and...\"}"}
{"id": "emnlp-2024-main-679", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"non-entailment). Specifically, we collect 3000 examples for each set from HANS, where the heuristic is lexical overlap, subsequence, and constituent accordingly, with labels and templates equally divided.\\n\\nNegation. We explore the impact of strong negation words like \\\"no\\\", \\\"not\\\", \\\"nothing\\\" and \\\"never\\\" on model predictions. Inspired by (Naik et al., 2018), we append the tautology \u2013 \\\"and false is not true\\\", \\\"and green is not red\\\", \\\"and up is not down\\\", \\\"and no square is a circle\\\", \\\"and nothing comes from nothing\\\", and \\\"and history never change\\\", chosen randomly with equal probability to the end of the hypothesis sentence in the Standard dataset.\\n\\nPosition. To test the influence of the position of label-associated information, we divide the Standard dataset into four equally distributed label and genre groups. In each group, we append phrases like \\\"and true is true\\\", \\\"and red is red\\\" or \\\"and up is up\\\" five times at different positions. This allows us to evaluate whether LLMs rely on irrelevant positional cues when making predictions.\\n\\nStyle. We consider the style of the text as a possible shortcut (Qi et al., 2021) and focus on one prominent style: Bible style. Specifically, we employ a simple but powerful text style transfer model called STRAP (Krishna et al., 2020) and apply it to transfer the premises in the Standard dataset into Bible-style texts.\\n\\n4.2 Metrics\\nWe adopt accuracy to quantify performance on NLI tasks and introduce new metrics to assess the explanatory power of LLMs.\\n\\nSemantic Fidelity Score (SFS) evaluates the extent to which the generated content preserves the essential meaning of the source text. We employ a pre-trained BERT model to create embedding for the input and the output collectively, then compute their cosine similarity. For a prompt $P$ and model output $c$, $SFS$ is given by\\n\\n$$SFS = \\\\text{CosineSimilarity}(\\\\text{BERT}(P), \\\\text{BERT}(c)).$$\\n\\n(2)\\n\\nInternal Consistency Score (ICS) assesses whether there are logical contradictions within the reasoning steps of LLMs or between the reasoning and the answer. To estimate the probability of contradiction $p_{\\\\text{contra}}$, we use an NLI model (Laurer et al., 2024) that categorizes hypothesis-context pairs into classes of entailment, neutral, and contradiction. For a reasoning chain of $N$ steps, $c = (c_1, c_2, \\\\ldots, c_N)$, where the last step is the answer, and $p_{\\\\text{contra}}(c_i, c_j)$ indicates the probability that step $c_i$ contradicts step $c_j$, we define the function $f(c)$ as\\n\\n$$f(c) = \\\\begin{cases} 0, & \\\\text{if } \\\\exists (c_i, c_j), 1 \\\\leq i < j \\\\leq N, \\\\text{ s.t. } p_{\\\\text{contra}}(c_i, c_j) > \\\\frac{1}{3}, \\\\\\\\ 1, & \\\\text{otherwise} \\\\end{cases}.$$\\n\\n(3)\\n\\nThe overall ICS is the mean of all calculated $f(c)$ values for the given explanations.\\n\\nExplanation Quality Score (EQS) integrates the SFS and ICS to reflect the overall quality of LLMs' output and is defined as\\n\\n$$\\\\text{EQS} = w_1 \\\\cdot SFS + w_2 \\\\cdot ICS,$$\\n\\n(4)\\n\\nwhere weights $w_1$ and $w_2$ represent the significance of each score in the overall evaluation. In this work, $w_1$ and $w_2$ are equally set as 0.5.\\n\\nConfidence Score (CFS) is designed to evaluate LLMs' self-assessment capabilities. We follow (Xiong et al., 2023) to prompt LLMs to provide their confidence level, which indicates the degree of certainty they have about their answer and is represented as a percentage.\\n\\n4.3 Evaluated LLMs\\nTo obtain a comprehensive understanding of how LLMs are affected by shortcuts, we conduct experiments on three widely used closed-source LLMs: GPT-3.5-Turbo (OpenAI, 2023), GPT-4 (Achiam et al., 2023) and Gemini-Pro (Team et al., 2023). Regarding open-source LLMs, we select LLaMA2-Chat-series (7B, 13B, 70B) (Touvron et al., 2023), ChatGLM3-6B (Zeng et al., 2022) and Mistral-7B (Jiang et al., 2023) for assessment.\\n\\n4.4 Prompting Strategies\\nOur experiments aim to assess the performance of LLMs in different settings, including zero-shot, few-shot ICL, zero-shot CoT, and few-shot CoT promptings. For zero-shot CoT, we utilize the prompt depicted in Figure 1. To construct few-shot ICL prompts, we enhance the best-performing zero-shot prompt by incorporating three random samples from the remaining examples in MultiNLI. Likewise, we employ a similar sampling approach for few-shot CoT and use GPT-4 to generate analyses for these examples.\"}"}
{"id": "emnlp-2024-main-679", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Accuracy (%) across all datasets under four prompt settings.\\n\\n| Model             | Standard | Lexical Overlap | Subsequence | Constituent | Negation | Position | Style |\\n|-------------------|----------|-----------------|-------------|-------------|----------|----------|-------|\\n| GPT-3.5-Turbo     | 61.7     | 93.3            | 38.7        | 91.3        | 23.3     | 96.7     |       |\\n| GPT-4             | 83.9     | 96.7            | 99.3        | 91.3        | 71.3     | 94.0     | 49.7  |\\n| Gemini-Pro        | 77.9     | 95.3            | 92.9        | 94.0        | 37.0     | 95.8     | 55.3  |\\n| LLaMA2-Chat-7B    | 40.2     | 76.9            | 40.0        | 72.8        | 46.4     | 60.6     | 43.3  |\\n| LLaMA2-Chat-13B   | 59.1     | 97.5            | 48.5        | 87.3        | 12.4     | 92.4     | 12.1  |\\n| LLaMA2-Chat-70B   | 57.8     | 100.0           | 3.6         | 99.8        | 3.1      | 99.6     | 1.6   |\\n| ChatGLM3-6B       | 35.6     | 100.0           | 0.0         | 100.0       | 0.0      | 100.0    | 0.0   |\\n| Mistral-7B        | 63.9     | 84.4            | 84.7        | 73.3        | 57.7     | 72.1     | 48.0  |\\n\\n### 5 Experimental Results\\n\\nWe conduct our experiments based on the Shortcut Suite and observe that LLMs tend to exploit various shortcuts in downstream tasks, resulting in a notable decrease in performance. In this section, we present a comprehensive analysis.\\n\\n#### 5.1 Overall Performance\\n\\n##### 5.1.1 Effect of Different LLMs\\n\\nAs shown in Table 2, closed-source and some open-source LLMs excel on standard datasets, with GPT-4 leading at an accuracy of 85.6%, followed by Gemini-Pro at 77.9%, GPT-3.5-Turbo at 71.7%, LLaMA2-Chat-70B at 70.9% and Mistral-7B at 69.6%. However, this high level of performance does not extend to shortcut datasets. For example, the accuracy of GPT-3.5-Turbo on the Constituent (\u00acE) dataset drops by 52.4% in the few-shot ICL setting. This significant drop suggests that LLMs are easily prone to adopting shortcuts for prediction.\\n\\nAmong open-source LLMs, Mistral-7B performs the best with CoT prompts. It excels on both standard and shortcut datasets, nearly surpassing LLaMA2-Chat-13B in all settings and even exceeding GPT-3.5-Turbo in some scenarios, demonstrating remarkable capabilities in NLI and robustness generalization. On the other hand,\"}"}
{"id": "emnlp-2024-main-679", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Box plots of confidence scores across all datasets under zero-shot CoT prompting (each LLM is denoted by an abbreviation). LLMs generally report confidence scores that significantly exceed their actual accuracy. ChatGLM3-6B is most affected by shortcuts, resulting in the poorest performance. Furthermore, we observe a reverse scaling pattern of LLaMA2-Chat in zero-shot and few-shot ICL scenarios. As the model size increases, it tends to rely more on spurious mapping for NLI tasks, resulting in lower accuracy. However, in the CoT scenario, LLaMA2-Chat-70B outperforms smaller models on most datasets. This indicates that larger models retain improved semantic comprehension and reasoning abilities but require suitable prompts to fully leverage their potential. This phenomenon is also observed in the LLaMA3 series, as illustrated in Appendix C.\\n\\n5.1.2 Effect of Shortcut Types\\nRegarding Lexical Overlap, Subsequence, and Constituent shortcuts, LLMs consistently favor predicting entailment ($E$) and thus struggle with the non-entailment ($\\\\neg E$) class. This indicates that LLMs can easily exploit these spurious correlations with the label $E$, leading to poor performance on $\\\\neg E$ instances. Lexical Overlap appears to be the easiest task for most LLMs across different prompt settings, resulting in high accuracy, while the Constituent shortcut poses the greatest challenge. For instance, in the zero-shot setting, Gemini-Pro experiences a significant 29.0% drop on Constituent, from 76.2% to 47.2%, worse than random guessing at 50%.\\n\\nNegation, Position, and Style shortcuts also prove challenging for most LLMs, as indicated by the notable decrease in accuracy. In the Negation dataset, the accuracy of GPT-4 decreases by 15-35% across the four different prompt settings. In the Style dataset, the accuracy of GPT-4 decreases up to 15.6%. Moreover, the detailed results of the Position shortcut are presented in Table 3. The lowest accuracy rates are predominantly observed when extra phrases are added at the beginning of the sentence, suggesting that the LLMs may rely more heavily on the beginning parts of sentences for cues than the end parts, which could be a potential shortcut for improvement.\\n\\n5.1.3 Effect of Prompting Types\\nMost LLMs demonstrate significant performance gains in all datasets when utilizing the CoT prompt. For example, GPT-4 with a zero-shot CoT prompt on the Constituent ($\\\\neg E$) dataset achieves an accuracy improvement of 14.0% compared to zero-shot, while LLaMA2-Chat-13B shows an improvement of 40.9% under the same conditions. However, the accuracy of GPT-4 and Gemini-Pro also improve under the CoT prompt. For instance, GPT-4 with a zero-shot CoT prompt on the Style dataset achieves an accuracy improvement of 15.6% compared to zero-shot, while Gemini-Pro shows an improvement of 15.6% under the same conditions.\"}"}
{"id": "emnlp-2024-main-679", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pro decreases after applying the CoT prompt on the Standard dataset and Lexical Overlap dataset. This phenomenon reveals that LLMs are prone to utilize shortcuts to predict, and the CoT prompt can promote in-depth inference and reduce the reliance on spurious correlations, thus improving performance. However, for relatively simple datasets, advanced LLMs may already possess sufficient semantic understanding and reasoning capabilities, reducing their dependence on CoT for performance enhancement.\\n\\nAdditionally, it is worth noting that the effectiveness of few-shot prompts is not superior to zero-shot prompting. In several scenarios, the few-shot ICL is less effective than the zero-shot, and the few-shot CoT performs worse than the zero-shot CoT. This discrepancy could be attributed to the LLMs acquiring biases from the in-context examples. Similar phenomena have been reported in (Kim et al., 2023; Tang et al., 2023). We show more experimental results and analysis in Appendix A.\\n\\n5.2 In-depth Analysis\\n\\n5.2.1 Explanation Quality\\n\\nWe evaluate the explanation quality of LLMs in shortcut challenges using Equations 2, 3, and 4, with results presented in Table 4.\\n\\nFor SFS, most LLMs score above 85%, indicating that current models have achieved a relatively high level of semantic fidelity. GPT-3.5-Turbo scores the highest on the Standard dataset with 92.1%, while Mistral-7B scores the lowest at 88.5%. Generally, models demonstrate a slight decline in SFS on shortcut datasets compared to the Standard dataset, indicating a reduced ability to restate inputs effectively in these contexts.\\n\\nRegarding ICS, most LLMs score below 50%, suggesting that more than half of their responses are contradictory. Notably, LLMs exhibit lower ICS scores on shortcut datasets compared to the Standard dataset. For example, LLaMA2-Chat-70B achieves a score of 41.5% on the Standard dataset but only 13.5% on the Negation dataset. These observations suggest that a lack of internal consistency in reasoning is a significant factor contributing to LLMs' reduced performance when dealing with shortcuts.\\n\\nThe overall EQS, which combines SFS and ICS, provides a comprehensive reflection of the overall quality of explanations from LLMs. Typically, models that exhibit higher accuracy also demonstrate greater explanatory capabilities.\\n\\n5.2.2 Confidence Score\\n\\nFigure 2 displays the confidence levels of LLMs, revealing two key findings. First, LLMs tend to be overconfident, with their confidence scores rarely falling below 60% and often significantly exceeding their actual accuracy. Second, the discrepancy between confidence and accuracy is notably greater in datasets containing shortcuts compared to the Standard dataset. This suggests that LLMs not only adopt shortcuts but also exhibit heightened confidence in these spurious mappings without fully understanding the true relationship between the source text and the corresponding label.\\n\\n5.2.3 Prediction Distribution\\n\\nFigure 3 shows the label distribution in each LLM's prediction. Despite a balanced distribution in the ground truth, we can easily observe that in the Standard dataset, GPT-3.5-Turbo, LLaMA2-Chat-7B, and Mistral-7B tend to disproportionately predict neutral over the other two categories. Conversely, LLaMA2-Chat-13B and ChatGLM3-6B show a bias towards entailment. This pattern may stem from multiple factors, including potential overfitting to the NLI task or tasks with a similar categorical structure.\\n\\nFor datasets featuring Lexical Overlap, Subsequence, and Constituent shortcuts, LLMs predominantly predict neutral over the other two categories. This pattern may also stem from potential overfitting to the NLI task or tasks with a similar categorical structure.\"}"}
{"id": "emnlp-2024-main-679", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | SFS (%) | ICS (%) | EQS (%) |\\n|---------------|---------|---------|---------|\\n| GPT-3.5-Turbo | 92.1    | 29.0    | 91.0    |\\n|               | 5.3     | 91.0    | 30.5    |\\n|               | 89.5    | 36.6    | 90.8    |\\n|               | 87.3    | 36.6    | 90.8    |\\n| GPT-4         | 91.1    | 34.7    | 91.1    |\\n|               | 11.3    | 90.8    | 23.3    |\\n|               | 88.7    | 57.0    | 91.8    |\\n| Gemini-Pro    | 89.2    | 43.0    | 88.6    |\\n|               | 29.9    | 87.9    | 30.5    |\\n| LLaMA2-Chat-7B| 88.7    | 20.3    | 90.6    |\\n|               | 4.1     | 90.2    | 24.2    |\\n|               | 88.6    | 19.8    |\\n| LLaMA2-Chat-13B| 90.2   | 41.5    | 91.4    |\\n|               | 11.0    | 91.3    | 26.5    |\\n|               | 88.4    | 13.5    |\\n| LLaMA2-Chat-70B| 90.4   | 33.9    | 90.6    |\\n|               | 6.9     |         |\\n| ChatGLM3-6B  | 90.3    | 22.9    | 87.7    |\\n|               | 9.5     | 88.0    | 22.4    |\\n| Mistral-7B    | 88.5    | 45.5    | 85.1    |\\n|               | 29.4    | 84.2    | 67.7    |\\n\\nTable 4: SFS (%), ICS (%), and EQS (%) across all datasets under zero-shot CoT prompting. The worst score for each LLM is underlined. LLMs typically show the lowest explanation quality in datasets comprising shortcuts.\\n\\n5.2.4 Error Analysis\\n\\nWe identify three types of errors in shortcut learning by analyzing the CoT responses of LLMs. The first issue is distraction, where LLMs are easily distracted by irrelevant information. As shown in Figure 4, they may focus on repetitive tautologies, leading to the neglect of useful information in the original text. Additionally, they often prioritize words at the start of a sentence while neglecting those at the end, as shown in Table 3. This reflects a tendency in LLMs to concentrate on local information while ignoring the comprehensive context.\\n\\nSecond, LLMs suffer from disguised comprehension. Specifically, they struggle to grasp the subtleties of individual words, sentence structures, and complex biblical language styles, shifting one's concept to another. This leads to disguised comprehension where LLMs might inadvertently \\\"borrow\\\" concepts, causing them to rely on shortcuts to make incorrect inferences. The detailed case can be found in Figure 6.\\n\\nThe third issue is logical fallacy. LLMs tend to reduce intricate reasoning to overly simplistic terms, generalizing from specific instances to broader conclusions via the use of shortcuts. This oversimplification in their reasoning process can lead to erroneous results, as illustrated in Figure 7.\\n\\n5.3 Extended Evaluation\\n\\nTo gain further insight into the shortcut challenges in LLMs, we conduct experiments on other NLP tasks. The first is the Sentiment Analysis (SA) task. Specifically, we use the validation set of the Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) as our Standard dataset. We then introduce the Negation shortcut using the method described in Section 4.1 to the Standard dataset.\\n\\nThe second is the Paraphrase Identification (PI) task. We experiment with the Quora Question Pairs (QQP) dataset as Standard dataset and the Paraphrase Adversaries from Word Scrambling (PAWS) (Zhang et al., 2019) dataset to represent\\n\\n1The dataset is available at https://www.kaggle.com/c/quora-question-pairs.\"}"}
{"id": "emnlp-2024-main-679", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Model SA PI\\n\\n| Standard | Negation | Standard Overlap |\\n|----------|----------|------------------|\\n| GPT-3.5-Turbo  | 91.7 | 87.0 | 81.2 | 74.3 |\\n| GPT-4  | 93.0 | 90.2 | 73.7 | 64.2 |\\n| Gemini-Pro | 92.7 | 87.8 | 75.9 | 47.4 |\\n| LLaMA2-Chat-7B | 84.1 | 76.1 | 61.6 | 49.5 |\\n| LLaMA2-Chat-13B | 87.4 | 83.3 | 73.8 | 50.0 |\\n| LLaMA2-Chat-70B | 87.8 | 87.1 | 71.7 | 52.0 |\\n| ChatGLM3-6B | 90.4 | 85.4 | 64.9 | 49.6 |\\n| Mistral-7B | 80.5 | 79.1 | 52.6 | 49.6 |\\n\\nTable 5: Accuracy (%) of the SA and PI tasks under zero-shot prompting. LLMs consistently demonstrate reduced performance on shortcut datasets compared to the Standard, as indicated by the blue highlights.\\n\\nLexical Overlap shortcut. The results, presented in Table 5, demonstrate a consistent decline in performance across both the SA and PI tasks on datasets comprising shortcuts compared to Standard datasets. Furthermore, as shown in Figure 8, there is a noticeable increase in negative predictions on the Negation dataset and an increase in duplicate predictions on the Lexical Overlap dataset. This pattern suggests that LLMs tend to capture spurious correlations between negation words and the negative label, as well as between word overlap and the duplicate label. In conclusion, we find that LLMs are prone to relying on the Negation shortcut in the SA task and the Lexical Overlap shortcut in the PI task, suggesting that shortcut learning is a prevalent phenomenon in LLMs across a wide spectrum of tasks.\\n\\nBesides the LLMs mentioned above, we conduct experiments on the latest LLMs, such as LLaMA3-series, and analyze the results as detailed in Appendix C.\\n\\n6 Conclusion\\n\\nIn this study, we proposed Shortcut Suite, a test suite designed to evaluate the performance of LLMs in shortcut learning across several NLP tasks. Shortcut Suite encompasses six types of shortcuts: Lexical Overlap, Subsequence, Constituent, Negation, Position, and Style, and evaluates performance using five metrics: ACC, SFS, ICS, EQS, and CFS, across four prompt settings: zero-shot, few-shot ICL, zero-shot CoT, and few-shot CoT. Our extensive experiments on diverse LLMs demonstrated that LLMs frequently rely on shortcuts in downstream tasks. We explored the impact of different models, types of shortcuts, and prompting strategies. Our analysis then extended to explanation quality, label distribution, confidence score and error analysis. Our findings offer new perspectives on LLMs' robustness and present new challenges for reducing their shortcut reliance, paving the way for future advancements in this field.\\n\\n7 Limitations\\n\\nIn this paper, we primarily focus on evaluating the effect of shortcut learning in LLMs on the NLI task, with additional exploration into tasks like SA and PI. However, we acknowledge that other NLP tasks, such as question-answering and coreference inference, could offer further insights and should be investigated in future research.\\n\\nWhile this study provides a comprehensive understanding of shortcut learning in LLMs, it does not propose specific methods to mitigate this phenomenon effectively. Nonetheless, we identify shortcut learning behavior in LLMs and categorize potential error types associated with shortcut learning, offering a foundation for future research. Based on our findings, we suggest several potential approaches for addressing shortcut learning in LLMs. One approach is fine-tuning on unbiased datasets, as training models on diverse and representative datasets may help alleviate shortcut learning. Moreover, employing advanced prompting techniques is essential. Our experiments indicate that few-shot prompting is insufficient for mitigating shortcut learning behaviors in LLMs, thus enhancing reasoning capabilities through methods such as CoT prompting may prove effective. Additionally, implementing retrieval augmentation by incorporating relevant external documents can ground LLMs, thereby reducing knowledge gaps and instances of hallucination. We advocate for further research to develop effective strategies aimed at addressing shortcut learning in LLMs.\"}"}
{"id": "emnlp-2024-main-679", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nMengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and Xia Hu. 2023. Shortcut learning of large language models in natural language understanding. Communications of the ACM, 67(1):110\u2013120.\\n\\nMengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, and Xia Hu. 2021. Towards interpreting and mitigating shortcut learning behavior of NLU models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 915\u2013929.\\n\\nRobert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. 2020. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665\u2013673.\\n\\nHe He, Sheng Zha, and Haohan Wang. 2019. Unlearn dataset bias in natural language inference by fitting the residual. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 132\u2013142.\\n\\nRobin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021\u20132031.\\n\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.\\n\\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186.\\n\\nJoongHoon Kim, Sangmin Lee, Seung Hun Han, Saeran Park, Jiyoon Lee, Kiyoon Jeong, and Pilsung Kang. 2023. Which is better? exploring prompting strategy for LLM-based metrics. In Proceedings of the 4th Workshop on Evaluation and Comparison of NLP Systems, pages 164\u2013183.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213.\\n\\nKalpesh Krishna, John Wieting, and Mohit Iyyer. 2020. Reformulating unsupervised style transfer as paraphrase generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 737\u2013762.\\n\\nYuxuan Lai, Chen Zhang, Yansong Feng, Quzhe Huang, and Dongyan Zhao. 2021. Why machine reading comprehension models learn shortcuts? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 989\u20131002.\\n\\nMoritz Laurer, Wouter Van Atteveldt, Andreu Casas, and Kasper Welbers. 2024. Less annotating, more classifying: Addressing the data scarcity issue of supervised machine learning with deep transfer learning and bert-nli. Political Analysis, 32(1):84\u2013100.\\n\\nR Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2020. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019, pages 3428\u20133448.\\n\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11048\u201311064.\\n\\nAakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2340\u20132353.\\n\\nOpenAI. 2023. Introducing chatgpt. OpenAI Blog. Available: https://openai.com/blog/chatgpt.\\n\\nFanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, and Maosong Sun. 2021. Mind the style of text! adversarial and backdoor attacks based on text style transfer. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4569\u20134580.\\n\\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with checklist. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u20134912.\\n\\nPriyanka Sen and Amir Saffari. 2020. What do models learn from question answering datasets? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2429\u20132438.\"}"}
{"id": "emnlp-2024-main-679", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642.\\n\\nRuixiang Tang, Dehan Kong, Longtao Huang, and Hui Xue. 2023. Large language models can be lazy learners: Analyze shortcuts in in-context learning. In Findings of the Association for Computational Linguistics: ACL 2023, pages 4645\u20134657.\\n\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikola Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122.\\n\\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. In The Twelfth International Conference on Learning Representations.\\n\\nLinan Yue, Qi Liu, Yichao Du, Li Wang, Weibo Gao, and Yanqing An. 2024. Towards faithful explanations: Boosting rationalization with shortcuts discovery. In The Twelfth International Conference on Learning Representations.\\n\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations.\\n\\nYuan Zhang, Jason Baldridge, and Luheng He. 2019. Paws: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298\u20131308.\\n\\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2018. Gender bias in coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15\u201320.\\n\\nLili Zhao, Qi Liu, Linan Yue, Wei Chen, Liyi Chen, Ruijun Sun, and Chao Song. 2024. Comi: Correct and mitigate shortcut learning behavior in deep neural networks. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 218\u2013228.\"}"}
{"id": "emnlp-2024-main-679", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix: Label Distribution\\n\\n(a) Negation. (b) Position. (c) Style.\\n\\nFigure 5: Label distribution as percentages (%) for LLMs' prediction under zero-shot prompting (each LLM is denoted by an abbreviation).\\n\\nB Appendix: Error Analysis\\n\\nFigure 6, 7 show the disguised comprehension error example and the logical fallacy error example respectively.\\n\\nINPUT\\n\\nPremise: The doctors that believed the managers introduced the bankers.\\nHypothesis: The managers introduced the bankers.\\n\\nOUTPUT\\n\\nAnalysis:\\n1. The premise is \\\"The doctors that believed the managers introduced the bankers.\\\" This means that some doctors believed that the managers introduced the bankers.\\n2. The hypothesis is \\\"The managers introduced the bankers.\\\" This means that the managers did introduce the bankers.\\n3. If the premise is true, then some doctors believed that the managers introduced the bankers. This means that it is possible that the managers did introduce the bankers. Therefore, the hypothesis is true given the premise.\\n\\nAnswer: neutral\\n\\nINPUT\\n\\nPremise: The judge knew the lawyer thanked the actor.\\nHypothesis: The judge knew the lawyer.\\n\\nOUTPUT\\n\\nAnalysis:\\n1. We can identify the following facts from the premise:\\n   - The judge knew the lawyer thanked the actor.\\n   - We can infer that the judge knew the lawyer.\\n2. The hypothesis is \\\"The judge knew the lawyer.\\\"\\n3. The premise implies that the judge knew the lawyer.\\n4. Therefore, the hypothesis is true given the premise.\\n\\nAnswer: entailment\\n\\nC Appendix: Extended Evaluation of Shortcut Learning Model.\\n\\nIn addition to the LLMs we discussed above, we'd like to extend our investigation to the LLaMA3-series. Notably, LLaMA3 demonstrates superior performance over LLaMA2. Specifically, LLaMA3-8B-Instruct outperforms both LLaMA2-Chat-7B and LLaMA2-Chat-13B on most datasets. Furthermore, LLaMA3-70B-Instruct surpasses GPT-3.5-Turbo and approaches the performance of Gemini-Pro. Despite these advances, we observe a consistent decline in performance on shortcut datasets compared to standard datasets. This trend suggests that LLaMA3-8B, similar to its predecessor, may rely on shortcuts for predictions. Additionally, the reverse scaling pattern persists in shortcut datasets such as Subsequence (\u00acE) and Constituent (\u00acE). These supplementary experiments highlight the propensity of most LLMs to rely on shortcuts across a wide spectrum of tasks, underscoring the need for more robust and generalizable mechanisms.\\n\\nD Appendix: More Discussion on Few-shot Prompting\\n\\nAs discussed above, few-shot ICL is less effective than zero-shot prompting, and few-shot CoT performs worse than zero-shot CoT in several scenarios. This phenomenon may be due to biases introduced by the in-context examples used in few-shot prompting. Similar issues have been reported in other studies. For instance, Kim et al. (2023) observed that demonstrations can introduce biases, leading to reduced performance in language models. Tang et al. (2023) also noted that LLMs might...\"}"}
{"id": "emnlp-2024-main-679", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Accuracy (%) across all datasets of LLaMA3-series.\\n\\n| Prompting     | Standard | Lexical Overlap | Subsequence | Constituent | Negation | Position | Style |\\n|---------------|----------|-----------------|-------------|-------------|----------|----------|-------|\\n|               |          |                 |             |             |          |          |       |\\n| zero-shot     |          |                 |             |             |          |          |       |\\n| LLaMA3-8B-Instruct | 62.2 | 84.3 | 89.2 | 88.3 | | | |\\n| LLaMA3-70B-Instruct | 74.5 | 94.3 | 96.8 | 99.7 | | | |\\n| zero-shot CoT |          |                 |             |             |          |          |       |\\n| LLaMA3-8B-Instruct | 65.3 | 63.5 | 96.1 | 46.9 | 75.7 | 65.3 | 68.6 |\\n| LLaMA3-70B-Instruct | 79.0 | 79.2 | 99.1 | 93.9 | | | |\\n\\nTable 7: Accuracy (%) across all datasets of GPT-3.5-Turbo.\\n\\n| Prompting     | Standard | Lexical Overlap | Subsequence | Constituent | Negation | Position | Style |\\n|---------------|----------|-----------------|-------------|-------------|----------|----------|-------|\\n|               |          |                 |             |             |          |          |       |\\n| few-shot (MNLI) | 61.7 | 93.3 | 38.7 | 91.3 | 23.3 | 96.7 | 9.3 |\\n| few-shot (shortcut) | 61.7 | 86.3 | 90.3 | 81.7 | 56.3 | 82.3 | 35.0 |\\n\\nWe observe that LLMs' performance on shortcut-laden datasets using more similar examples is better than using standard examples, but still worse than zero-shot, indicating that the influence of shortcuts from pre-trained data is more significant than the benefits of in-context examples. LLMs struggle to summarize the important aspects from in-context examples to overcome their inherent biases and are even influenced by the biases from the in-context examples.\"}"}
