{"id": "acl-2023-short-154", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the zero-shot condition, models are simply prompted with the ScoNe-NLG examples. In the few-shot condition, the test is example is proceeded with a fixed set of four demonstrations, separated by double newlines. The examples are as follows:\\n\\nPrompt example\\n\\nGlen is not a fan of learning math. When he sees that his new high school requires that he take a geometry course, he is not pleased.\\n\\nI saw John take his BMW to the store the other day, so when Suzy asked me if John owns a car, I said yes.\\n\\nI've seen John with a dog that isn't very cute, so when Suzy asked me if John owns a pet, I said yes.\\n\\nI recently confirmed that John is not allergic to any shellfish. So it makes sense that when we served shrimp\\n\\nTable 7: In-context learning results for GPT-3 (davinci-002 engine).\"}"}
{"id": "acl-2023-short-154", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\nYes, primarily in the Limitations section.\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\nYes, in the Limitations section.\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper\u2019s main claims?\\nYes, in the abstract and the introduction.\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\nLeft blank.\\n\\nB Did you use or create scientific artifacts?\\nSections 3 and 4.\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\nSection 3.\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\nAppendix A and D.\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\nIn Limitations, and in Appendix A and D, and in supplementary materials.\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\nNot applicable. Left blank.\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\nIn the Introduction and in Limitations section.\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\nSections 3 and 4.\\n\\nC Did you run computational experiments?\\nSections 3 and 4.\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\nNo response.\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-short-154", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nAppendix A.\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nSections 3 and 4.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nNo response.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nNot applicable. Left blank.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nNot applicable. Left blank.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nNot applicable. Left blank.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nNot applicable. Left blank.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nNot applicable. Left blank.\"}"}
{"id": "acl-2023-short-154", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ScoNe: Benchmarking Negation Reasoning in Language Models\\nWith Fine-Tuning and In-Context Learning\\n\\nJingyuan Selena She\\nHaverford College\\njshe@haverford.edu\\n\\nChristopher Potts\\nStanford University\\ncgpotts@stanford.edu\\n\\nSamuel R. Bowman\\nNew York University & Anthropic, PBC\\nbowman@nyu.edu\\n\\nAtticus Geiger\\nStanford University\\natticusg@stanford.edu\\n\\nAbstract\\nA number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scooped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many-shot fine-tuning. For in-context learning, we test InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveals the model can correctly reason about negation, but struggles to do so on prompt-adapted NLI examples outside of its core pretraining regime.\\n\\n1 Introduction\\nNegation is a ubiquitous but complex linguistic phenomenon that poses a significant challenge for NLP systems. A diverse array of benchmarks focused on negation have appeared in recent years, many of which contain families of contrasting examples that provide a local view of the model decision boundary (Gardner et al., 2020). For instance, Cooper et al. (1996), McCoy and Linzen (2018), Wang et al. (2019), Ettinger (2020), Hartmann et al. (2021), and Kassner and Sch\u00fctze (2020) all conduct evaluations with minimal pairs of examples that are identical except for a negative morpheme. These examples reveal whether the presence of negation has a causal impact on model predictions.\\n\\nHowever, negation is not simply present or absent in a sentence. Rather, negation morphemes are semantic operators that take scope in complex ways, as we see in clear contrasts like\\n\\nthe person who was at the talk wasn't happy\\nand\\nthe person who wasn't at the talk was happy.\\n\\nThe recent CondaQA benchmark of Ravichander et al. (2022) includes minimal pairs aimed at determining whether models are sensitive to these differences in scope. With the current paper, we seek to provide an even fuller picture of the complexities of negation and semantic scope. We introduce the English-language Scooped Negation Natural Language Inference Benchmark (ScoNe-NLI). ScoNe-NLI extends the negated portion of the Monotonicity NLI dataset (Geiger et al., 2020) such that each of the 1,202 examples is now a contrast set with six examples in which zero, one, or two negations are present and each negation may or may not have a semantic scope such that the NLI label is impacted by its presence. These six conditions offer a rich picture of how negation affects NLI reasoning, and they allow us to determine whether models are truly able to handle nested negation and scope or whether they have found simplistic solutions.\\n\\nWe evaluate models on ScoNe-NLI using many-shot fine-tuning as well as a wide range of in-context learning strategies. For fine-tuning approaches, we find that RoBERTa and DeBERTa models both solve ScoNe-NLI. For in-context learning, we evaluate the latest InstructGPT model with a variety of prompt strategies. We find that these models perform well on sections of ScoNe-NLI where the negation morphemes can simply be ignored, but they systematically fail in conditions where exactly one negative morpheme has semantic scope such that its presence changes the NLI label. In other words, these models fail to learn in context how negation actually takes scope.\\n\\nTo better understand this result, we introduce a sentence completion test set (ScoNe-NLG) containing...\"}"}
{"id": "acl-2023-short-154", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Two contrast sets from the ScoNe Benchmark\\n\\n(a) A six-example contrast set from ScoNe-NLI.\\n\\nNo negation\\nThe cowboy fell off a horse at the competition\\n\u2290 The cowboy fell off a racehorse at the competition\\n\\nOne Not Scoped\\nThe cowboy did not fear anything, until he fell off a horse at the competition\\n\u2290 The cowboy did not fear anything, until he fell off a racehorse at the competition\\n\\nTwo Not Scoped\\nThe cowboy, who was not very old, was not proud that he fell off a horse at the competition\\n\u2290 The cowboy, who was not very old, was not proud that he fell off a racehorse at the competition\\n\\nTwo Scoped\\nThere is no way that the cowboy did not fall off a horse at the competition\\n\u2290 There is no way that the cowboy did not fall off a racehorse at the competition\\n\\nOne Scoped\\nThe cowboy did not fall off a horse at the competition\\n\u228f The cowboy did not fall off a racehorse at the competition\\n\\nOne Scoped, One not Scoped\\nThe cowboy did not fall off a horse, but the competition was not too important\\n\u228f The cowboy did not fall off a racehorse, but the competition was not too important\\n\\n(b) A three-example contrast set from ScoNe-NLG.\\n\\nA Brief Review of Negation in NLI Benchmarks\\n\\nA diverse array of benchmarks and diagnostic experiments have included negation reasoning in recent years (Nairn et al., 2006; McCoy and Linzen, 2018; Wang et al., 2019; Ettinger, 2020; Hartmann et al., 2021; Kassner and Sch\u00fctze, 2020; Ravichander et al., 2022).\\n\\nHossain et al. (2022) analyze a variety of natural language understanding benchmarks and find that negation is underrepresented, and that when negation is present it often has no impact on the example label. Hossain et al. (2020) address this issue by manually adding negation to the premise-hypothesis pairs in MNLI (Williams et al., 2018), SNLI (Bowman et al., 2015), and RTE (Dagan et al., 2007; Cooper et al., 1996).\\n\\nYanaka et al. (2019a) introduce the crowdsourced MED dataset, which has many NLI examples where negation generates inferences. Monotonicity NLI (MoNLI; Geiger et al. 2020) consists of modified SNLI sentences that have gold labels impacted by lexical entailments in affirmative contexts (PMoNLI) and lexical entailments reversed by a negation (NMoNLI). BERT fine-tuned on SNLI and MNLI fails to generalize to both of these datasets, but succeeds with further fine-tuning on MED/MoNLI. Some automatically generated NLI datasets also include negation reasoning (Geiger et al., 2019; Richardson et al., 2020; Yanaka et al., 2019b, 2021).\"}"}
{"id": "acl-2023-short-154", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: DeBERTa fine-tuning results on ScoNe-NLI.\\n\\n| Dataset                                    | Entailment | Neutral | Entailment | Neutral | Entailment | Neutral |\\n|--------------------------------------------|------------|---------|------------|---------|------------|---------|\\n| MAF-NLI                                    | 82.0       | 86.0    | 81.5       | 91.0    | 5.0        | 5.0     |\\n| MAF-NLI + MoNLI (Geiger et al., 2020)      | 96.2       | 87.5    | 99.5       | 8.9     | 100.0      | 100.0   |\\n| MAF-NLI + MED (Yanaka et al., 2020)        | 84.8       | 83.5    | 82.0       | 58.9    | 99.5       | 97.0    |\\n| MAF-NLI + Neg-NLI (Hossain et al., 2020)   | 91.3       | 88.5    | 83.0       | 70.4    | 37.0       | 29.0    |\\n| MAF-NLI + MoNLI + ScoNe-NLI                | 100.0      | 100.0   | 100.0      | 100.0   | 100.0      | 100.0   |\\n\\nTable 3: Prompts used to adapt a 2-way NLI example (Premise, Hypothesis). Newlines are indicated with \\n.\\n\\nFull prompts with few-shot variants are in Appendix E.\\n\\nThe authors generated ScoNe-NLI by using each example of NMoNLI to create a contrast set of six examples where gold labels are impacted by the scope of zero, one, or two negations, as in Table 1.\\n\\nTo succeed across all sections of ScoNe, models need to attend to the presence of negation as well as the way it scopes semantically. Table 1a shows an actual example of how ScoNe extends MoNLI. We use the train\u2013test split of MoNLI where substituted lexical items are disjoint across training and testing data. Appendix C provides further details.\\n\\nFine-Tuning on ScoNe-NLI\\nWe used publicly available weights on HuggingFace for the DeBERTa-v3-base models already fine-tuned on MNLI, Fever-NLI, and Adversarial-NLI (Laurer et al., 2022; He et al., 2021). Appendix B contains comparable results for the RoBERTa model (Liu et al., 2019). Fine-tuning results are in Table 2.\\n\\nFine-tuning on existing NLI datasets is insufficient for good performance on ScoNe-NLI: DeBERTa-v3-base fine-tuned on existing NLI datasets, even those that focus on negation, systematically fails. Thus, it seems that ScoNe-NLI captures novel aspects of negation reasoning.\\n\\nIn contrast, fine-tuning on MoNLI and ScoNe-NLI training data results in near perfect performance on ScoNe-NLI test data. This shows that DeBERTa can learn negation reasoning and generalize to new lexical items.\\n\\nIn-context Learning on ScoNe-NLI\\nWe evaluated InstructGPT using OpenAI\u2019s API with text-davinci-002 and text-davinci-003 engines and a temperature of 0.0 (Brown et al., 2020). We ask InstructGPT to infer NLI labels given the premise and hypothesis using prompts. All prompts are constructed such that if the response contain \u201cyes\u201d (case-insensitive), then the label entailment is predicted, else the label neutral is predicted. We use six prompts (Table 3). For each prompt, we implemented both zero-shot and few-shot inference experiments. Appendix E provides the full prompts.\\n\\nInstructGPT makes systematic errors similar to a baseline that ignores negation entirely. The best results are for the few-shot reasoning prompt with davinci-003. While its overall accuracy of 82% may initially appear to be a success, further analysis reveals otherwise. InstructGPT succeeds only on the sections of ScoNe-NLI where zero or two negations take scope, namely, no negation (99%), one not scoped (97%), two not scoped (100%).\\n\\nBrown et al.\\n\\nP: Premise\\nQ: Hypothesis\\n\\nYes, No, or Maybe?\\n\\nStructured\\nP: Premise\\nH: Hypothesis\\nL:\\n\\nReasoning: Let\u2019s think logically step by step. The premise basically tells us that...\\n\\nExplain your reasoning in detail, then answer with Yes or No. Your answers should follow this 4-line format:\\n\\nPremise: <a tricky logical statement about the world>.\\n\\nQuestion: <question requiring logical deduction>.\\n\\nReasoning: <an explanation of what you understand about the possible scenarios>\\n\\nAnswer: <Yes or No>.\\n\\nPremise:...\\n\\nQuestion:...\\n\\nReasoning: Let\u2019s think logically step by step. The premise basically tells us that...\"}"}
{"id": "acl-2023-short-154", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: In-context learning results on ScoNe-NLI for InstructGPT (\\\\textit{davinci-003} engine; see Appendix F for corresponding results for \\\\textit{davinci-002}, which are uniformly lower). Zero-shot results are given in the first group of rows, with the best results in that condition underlined. Few-shot results are given in the second group, with the best results for this condition (and overall) in bold. The bottom row specifies a simple, idealized Ignore-Negation baseline that makes predictions as if negations were absent. The baseline shows that the seemingly solid Overall results of these models are driven largely by conditions for which negation can be ignored. Conversely, models are often at or below chance where negation is critical in some way.\\n\\nZero-shot\\n\\n- **Structured**: 0.50\\n- **Brown et al.**: 0.74\\n- **Conditional Q**: 0.79\\n- **Conditional Truth**: 0.98\\n- **Hypothesis Q**: 0.69\\n- **Reasoning**: 0.90\\n- **Ignore-Negation**: 1.00\\n\\nFew-shot\\n\\n- **Structured**: 0.50\\n- **Brown et al.**: 0.86\\n- **Conditional Q**: 0.92\\n- **Conditional Truth**: 0.94\\n- **Hypothesis Q**: 0.98\\n- **Reasoning**: 0.99\\n- **Ignore-Negation**: 1.00\\n\\nTable 5: Results for ScoNe-NLG using \\\\textit{davinci-003}. The three conditions correspond to those of ScoNe and test the essential scope-taking properties of negation. InstructGPT performs much worse on sections where exactly one negation takes scope, namely one scoped (69%), one scoped/one not (48%). An idealized baseline ignoring the presence of negation (last row of Table 4) succeeds and fails on the same sections, indicating a systematic flaw in InstructGPT.\\n\\n4 ScoNe-NLG\\n\\nInstructGPT fails to reason about negation when given NLI examples that must be adapted to natural language generation (NLG) with prompts. We hypothesized that InstructGPT may correctly reason about negation when evaluated on examples hand tailored to its pretraining objective, because there is no need for prompt engineering (Liu et al., 2021; Wei et al., 2022; Kojima et al., 2022).\\n\\nDataset\\n\\nScoNe-NLG is a natural language generation dataset that contains 74 contrasting triplets of examples of half-completed naturalistic narratives that have different coherent completions depending on the presence and scope of a negation.\\n\\nIn-context Learning on ScoNe-NLG\\n\\nWe used InstructGPT to complete the partial sentence inputs with the \\\\textit{text-davinci-003} engine (temperature of 0.0). In the zero-shot setting, the prompt consists of the ScoNe-NLG example. In the few-shot setting, four demonstrations from ScoNe-NLG are given one with no negation, two with scoping negation, and one with non-scoping negation. See Appendix E.13 for the complete prompts.\\n\\nTo evaluate, the authors went through the responses by hand and determined whether the generated text is coherent and compatible with the initial narrative. The authors agreed on these annotations for 216/222 of the zero-shot responses with a Fleiss kappa of 0.84 and 220/222 of the few-shot responses with a Fleiss kappa of 0.91. These agreement rates are so high that we evaluate InstructGPT only for the cases where the annotators agree. Here, InstructGPT is successful but not perfect, achieving 95% and 92% accuracy in the few and zero-shot settings, respectively. We do not observe the systematic failures seen on ScoNe-NLI.\"}"}
{"id": "acl-2023-short-154", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1: Four human-interpretable algorithms for ScoNe-NLI. The first two solve the task perfectly, and the other two implement flawed heuristics that a model might learn to implement. The function \\\\texttt{GET-LEXREL} retrieves the relation between the aligned words in the premise and hypothesis, \\\\texttt{COUNT-SCOPED} counts scoped negations, \\\\texttt{COUNT-NEG} counts negations regardless of scope, and \\\\texttt{GET-FIRST} returns true if the first negation scopes, while \\\\texttt{GET-SECOND} returns true if there is a second negation and it scopes.\\n\\n5 Future Work on Interpretability\\n\\nScoNe is based in naturalistic examples, but it also has a controlled structure that offers valuable opportunities to move beyond simple behavioral testing and more deeply understand how models solve tasks related to lexical entailment and negation. The theory of causal abstraction provides a framework for interpretability (Geiger et al., 2023a), where a neural model can be understood to implement the intermediate variables and internal structure of a program or algorithm (Geiger et al., 2021, 2022; Wu et al., 2022b,a; Huang et al., 2022; Geiger et al., 2023b). In fact, the MoNLI dataset and the technique of interchange interventions (which is the primary technique in causal abstraction analysis) were jointly introduced in Geiger et al. 2020, where interchange interventions were used to investigate whether a BERT model implements a simple, human-interpretable algorithm that can perfectly label MoNLI using a variable representing lexical entailment and a variable representing the presence of negation.\\n\\nWith ScoNe, we can ask even deeper interpretability questions of this form. To encourage future work in this direction, we present a range of algorithmic solutions in Figure 1. Two of these solutions solve ScoNe and could perhaps explain neural models that learn the task perfectly, and two others implement flawed heuristics that could explain neural models with poor task performance.\\n\\nFigure 1a and Figure 1b present two intuitive and correct algorithms that solve ScoNe, but have distinct intermediate variables and internal structure. The first computes two Booleans representing whether each negation scopes, and the second computes a count of how many negations scope.\\n\\nFigure 1d is the flawed heuristic that ignores negation that we discussed in Section 3 as a hypothesis about how models fail at our task. Figure 1d is a second flawed heuristic that counts the number of negations present but ignores scope.\\n\\nUsing the toolkit of causal abstraction, we can assess models not only behaviorally, but also evaluate whether they implement an interpretable algorithm. The results of Geiger et al. (2023b) begin to show how such analyses could be extended to in-context learning with LLMs, as in Section 4.\\n\\n6 Conclusion\\n\\nWe introduced ScoNe, a benchmark for fine-tuning and in-context learning experiments on negation. ScoNe is challenging for NLI models fine-tuned on other datasets, even those designed for negation reasoning, but modest amount of fine-tuning on ScoNe leads to success. For in-context learning, we find that InstructGPT models fail dramatically on ScoNe. However, we also introduce ScoNe-NLG, which uses more narrative-like examples to probe models' capacity to handle negation, and show that InstructGPT is successful with zero-shot and few-shot prompts for this task. These results show that ScoNe supports fine-grained assessments of whether models can reason accurately about natural language negation, and our discussion in Section 5 suggests that ScoNe can be a powerful tool for discovering how models reason semantically.\"}"}
{"id": "acl-2023-short-154", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nWe are releasing ScoNe as a diagnostic tool for conducting controlled scientific experiments. This is our primary intended use, and we advise against uncritical use of ScoNe for real-world applications, as we have not audited the dataset for such purposes.\\n\\nAs a diagnostic tool, ScoNe's primary limitation is its focus on English. Cross-linguistically, we find many strategies for expressing negation. The English-language strategy of using mostly adverbial modifiers for sentential negation is not the only one by any means, and we would expect to see quite different results for languages in which negation is expressed, for example, with verbal suffixes. This highlights the value of potential future efforts extending ScoNe to other languages.\\n\\nBy the same token, we acknowledge that many linguistic phenomena interact with negation even internal to English. ScoNe restricts to negation in the context of lexical entailment, and mostly uses \\\"not\\\" as the negative morpheme. This excludes a wide range of negation morphemes and negation strategies that ultimately need to be brought into the picture.\\n\\nFinally, we note that there may be undesirable biases in ScoNe that could interact with biases in the models. ScoNe is in part derived from SNLI, which is known to contain gaps, social biases, and artifacts (Poliak et al., 2018; McCoy et al., 2019; Belinkov et al., 2019; Gururangan et al., 2018; Tsuchiya, 2018), and ScoNe may inherit some of these.\"}"}
{"id": "acl-2023-short-154", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah Goodman, and Christopher Potts. 2022. Inducing causal structure for interpretable neural networks. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7324\u20137338. PMLR.\\n\\nAtticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. 2023b. Finding alignments between interpretable causal variables and distributed neural representations. Ms., Stanford University.\\n\\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107\u2013112, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nMareike Hartmann, Miryam de Lhoneux, Daniel Hershcovich, Yova Kementchedjhieva, Lukas Nielsen, Chen Qiu, and Anders S\u00f8gaard. 2021. A multilingual benchmark for probing negation-awareness with minimal pairs. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 244\u2013257, Online. Association for Computational Linguistics.\\n\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations.\\n\\nMd Mosharaf Hossain, Dhivya Chinnappa, and Eduardo Blanco. 2022. An analysis of negation in natural language understanding corpora. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 716\u2013723, Dublin, Ireland. Association for Computational Linguistics.\\n\\nMd Mosharaf Hossain, Venelin Kovatchev, Pranoy Dutta, Tiffany Kao, Elizabeth Wei, and Eduardo Blanco. 2020. An analysis of natural language inference benchmarks through the lens of negation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9106\u20139118, Online. Association for Computational Linguistics.\\n\\nJing Huang, Zhengxuan Wu, Kyle Mahowald, and Christopher Potts. 2022. Inducing character-level structure in subword-based language models with Type-level Interchange Intervention Training. Ms., Stanford University and UT Austin.\\n\\nNora Kassner and Hinrich Sch\u00fctze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811\u20137818, Online. Association for Computational Linguistics.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv, abs/2205.11916.\\n\\nMoritz Laurer, Wouter van Atteveldt, Andreu Casas, and Kasper Welbers. 2022. Less annotating, more classifying \u2013 addressing the data scarcity issue of supervised machine learning with deep transfer learning and bert-nli.\\n\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys (CSUR).\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Manordersh Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nR. Thomas McCoy and Tal Linzen. 2018. Non-entailed subsequences as a challenge for natural language inference. CoRR, abs/1811.12112.\\n\\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428\u20133448, Florence, Italy. Association for Computational Linguistics.\\n\\nRowan Nairn, Cleo Condoravdi, and Lauri Karttunen. 2006. Computing relative polarity for textual inference. In Proceedings of the Fifth International Workshop on Inference in Computational Semantics (ICoS-5).\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 8024\u20138035. Curran Associates, Inc.\\n\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830.\"}"}
{"id": "acl-2023-short-154", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abhilasha Ravichander, Matt Gardner, and Ana Marasovi\u0107. 2022. Condaqa: A contrastive reading comprehension dataset for reasoning about negation.\\n\\nKyle Richardson, Hai Hu, Lawrence S. Moss, and Ashish Sabharwal. 2020. Probing natural language inference models through semantic fragments. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 8713\u20138721. AAAI Press.\\n\\nMasatoshi Tsuchiya. 2018. Performance impact caused by hidden bias of training data for recognizing textual entailment. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Glue: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nZhengxuan Wu, Karel D'Oosterlinck, Atticus Geiger, Amir Zur, and Christopher Potts. 2022a. Causal Proxy Models for concept-based model explanations. ArXiv:2209.14279.\\n\\nZhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christopher Potts, and Noah Goodman. 2022b. Causal distillation for language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4288\u20134295, Seattle, United States. Association for Computational Linguistics.\\n\\nHitomi Yanaka, Koji Mineshima, Daisuke Bekki, and Kentaro Inui. 2020. Do neural models learn systematicity of monotonicity inference in natural language? In Annual Meeting of the Association for Computational Linguistics.\\n\\nHitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, and Johan Bos. 2019a. Can neural networks understand monotonicity reasoning? In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 31\u201340, Florence, Italy. Association for Computational Linguistics.\\n\\nHitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, and Johan Bos. 2019b. HELP: A dataset for identifying shortcomings of neural models in monotonicity reasoning. In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), pages 250\u2013255, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nHitomi Yanaka, Koji Mineshima, and Kentaro Inui. 2021. SyGNS: A systematic generalization testbed based on natural language semantics. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 103\u2013119, Online. Association for Computational Linguistics.\"}"}
{"id": "acl-2023-short-154", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Experimental Details\\n\\nA.1 Fine-tuning Protocol\\nFor our fine-tuning experiments, we used a learning rate of $1 \\\\times 10^{-5}$, batch size of 4, gradient accumulation steps of 6 for a total of 10 epochs. We used these default hyperparameters as they were successful in fine-tuning on ScoNe. We implemented these experiments with Pytorch (Paszke et al., 2019) and used the scikit learn package (Pedregosa et al., 2011).\\n\\nA.2 Hugging Face Models\\nWe test RoBERTa and DeBERTa in these experiments. We used the roberta-large model fine-tuned on MNLI with 354 million parameters, 500K steps, and trained on 1,024 V100 GPUs (Liu et al., 2019). DeBERTa-v3-base-mnli-fever-anli model was fine-tuned on MNLI, Fever-NLI, and ANLI.\\n\\nRoBERTa weights link: https://huggingface.co/roberta-large-mnli\\nDeberta weights link: https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\\n\\nA.3 Fine-Tuning Datasets\\nWe further fine-tuned our model on the datasets MoNLI, Negation-NLI, and MED.\\n\\nB RoBERTa Results\\n\\n|                  | No One | Two | Two | One One | Scoped | Scoped | Scoped | Scoped |\\n|------------------|--------|-----|-----|--------|--------|--------|--------|--------|\\n| MAF-NLI          | 96.5   | 97.0| 97.0| 96.5   | 3.0    | 5.0    |\\n| MAF-NLI+ MoNLI (Geiger et al., 2020) | 85.4 | 100.0 | 100.0 | 4.5 | 100.0 | 100.0 |\\n| MAF-NLI+ MED (Yanaka et al., 2020) | 85.1 | 92.0 | 89.5 | 44.6 | 85.5 | 81.5 |\\n| MAF-NLI+ Neg-NLI (Hossain et al., 2020) | 93.1 | 97.5 | 93.0 | 73.2 | 20.5 | 17.5 |\\n| MAF-NLI+ MoNLI + ScoNe-NLI | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |\\n\\nTable 6: RoBERTa fine-tuning results on ScoNe-NLI. MAF-NLI stands for on MNLI, ANLI, and Fever-NLI.\\n\\nC ScoNe Dataset Details\\nFor some examples, we modified the lexical items replaced. Consider the NMoNLI sentence pair 'a man is not tossing anything'-'a man is not tossing socks' (entailment), and non-scoping counterpart 'a man not here is tossing something'-'a man not here is tossing socks' (neutral). Here, 'anything' must be replaced by 'something'. The positive and negative examples in MoNLI do not come in minimal pairs, so the examples in ScoNe-NLI with no negation are not from PMoNLI.\\n\\n1 released under the MIT license\\n2 released under the MIT license\\n3 released under the MIT license\\n4 released under the MIT license\\n5 released under the Creative Commons Attribution-ShareAlike License (version 3.0)\\n6 released under the Attribution-NonCommercial 4.0 International license\\n7 released under the Creative Commons Attribution Share Alike 4.0 International license\\n8 released under the MIT license\\n9 released under the Creative Commons Attribution Share Alike 4.0 International license\"}"}
{"id": "acl-2023-short-154", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The experimental runs reported in the paper were conducted on January 11, 2023. We used InstructGPT models with 1.3 billion parameters and 6 billion parameter. The exact cost of constructing the InstructGPT models is not public, but the pre-training protocol involves (1) fine-tuning a GPT3 model on an instruction following dataset, (2) fine-tuning a GPT3 model to rank different answers to the instruction following dataset, and (3) using reinforcement learning to combine these two models. We use a temperature parameter of 0.0 for all experiments. If the response contains \\\"yes\\\" (case-insensitive), then we infer the label entailment, else we infer neutral. Across experiments, the only thing that varies is the nature of the prompt function.\\n\\nE. In-Context Learning Prompts\\n\\nWe have indicated all actual newlines with \\\\n. The newlines in the formatting are just to make them intuitive to read.\\n\\nE.1 Conditional Question Prompt\\n\\nPrompt example\\n\\nIs it true that if we didn't eat pizza, then we didn't eat food?\\n\\nE.2 Few-Shot Conditional Question Prompt\\n\\nPrompt example\\n\\nQ1: Is it true that if a not so tall person reading a paper is not currently sitting inside a building, then a not so tall person reading a paper is not currently sitting inside a club?\\n\\nA1: Yes\\n\\nQ2: Is it true that if the man does not own a dog and does not own a cat, then the man does not own a retriever and does not own a cat?\\n\\nA2: Yes\\n\\nQ3: Is it true that if a not so tall person reading a paper is not currently sitting inside a cabin, then a not so tall person reading a paper is not currently sitting inside a building?\\n\\nA3: Maybe\\n\\nQ4: Is it true that if a not so tall person reading a paper is not currently sitting inside a casino, then a not so tall person reading a paper is not currently sitting inside a building?\\n\\nA4: Maybe\\n\\nQ: Is it true that if we didn't eat pizza, then we didn't eat food?\\n\\nA: Yes\\n\\nE.3 Hypothesis Question Prompt\\n\\nPrompt example\\n\\nAssume that we didn't eat pizza. Is it then definitely true that we didn't eat food? Answer Yes or No.\"}"}
{"id": "acl-2023-short-154", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.4 Few-Shot Hypothesis Question Prompt\\n\\n**Prompt example**\\n\\nQ1: Assume that a not so tall person reading a paper is not currently sitting inside a building. Is it then definitely true that a not so tall person reading a paper is not currently sitting inside a casino? Answer Yes or No.\\n\\nA1: Yes\\n\\nQ2: Assume that the girl will not get a stuffed dog as a gift, but not because she failed the exam. Is it then definitely true that the girl will not get a stuffed pinscher as a gift, but not because she failed the exam? Answer Yes or No.\\n\\nA2: Yes\\n\\nQ3: Assume that the girl will not get a stuffed shetland as a gift, but not because she failed the exam. Is it then definitely true that the girl will not get a stuffed dog as a gift, but not because she failed the exam? Answer Yes or No.\\n\\nA3: No\\n\\nQ4: Assume that a not so tall person reading a paper is not currently sitting inside a monastery. Is it then definitely true that a not so tall person reading a paper is not currently sitting inside a building? Answer Yes or No.\\n\\nA4: No\\n\\nQ: Assume that we didn't eat pizza. Is it then definitely true that we didn't eat food? Answer Yes or No.\\n\\nA: \\n\\nE.5 Conditional Truth Evaluation Prompt\\n\\n**Prompt example**\\n\\nIf we didn't eat pizza, then we didn't eat food. Is this true?\"}"}
{"id": "acl-2023-short-154", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.6 Few-Shot Conditional Truth Evaluation Prompt\\n\\nPrompt example\\n\\nC1: If the man does not own a dog and does not own a cat, then the man does not own a shetland and does not own a cat. Is this true?\\n\\nA1: Yes\\n\\nC2: If a not so tall person reading a paper is not currently sitting inside a building, then a not so tall person reading a paper is not currently sitting inside a house. Is this true?\\n\\nA2: Yes\\n\\nC3: If the man does not own a collie and does not own a cat, then the man does not own a dog and does not own a cat. Is this true?\\n\\nA3: Maybe\\n\\nC4: If the man does not own a corgi and does not own a cat, then the man does not own a dog and does not own a cat. Is this true?\\n\\nA4: Maybe\\n\\nC: If we didn't eat pizza, then we didn't eat food. Is this true?\\n\\nA:\\n\\nE.7 Brown Et Al Style Prompt\\n\\nPrompt example\\n\\nC: We didn't eat pizza\\n\\nQ: We didn't eat food. Yes, No, or Maybe?\\n\\nE.8 Few-Shot Brown Et Al Style Prompt\\n\\nPrompt example\\n\\nC1: The man, who's eyes are not open, is not steering a car.\\n\\nQ1: The man, who's eyes are not open, is not steering a sedan. Yes, No, or Maybe?\\n\\nA2: Yes\\n\\nC2: A dog not on the playground did not catch any ball.\\n\\nQ2: A dog not on the playground did not catch any volleyball. Yes, No, or Maybe?\\n\\nA3: Yes\\n\\nC3: the man does not own a collie and does not own a cat.\\n\\nQ3: the man does not own a dog and does not own a cat. Yes, No, or Maybe?\\n\\nA4: Maybe\\n\\nC4: A not so tall person reading a paper is not currently sitting inside a inn.\\n\\nQ4: A not so tall person reading a paper is not currently sitting inside a building. Yes, No, or Maybe?\\n\\nA5: Maybe\\n\\nC: We didn't eat pizza\\n\\nQ: We didn't eat food. Yes, No, or Maybe?\\n\\nA:\"}"}
{"id": "acl-2023-short-154", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.9 Structured Prompt\\n\\nPrompt example\\n\\nP: We didn't eat pizza\\n\\nH: We didn't eat food\\n\\nL:\\n\\nE.10 Few-Shot Structured Prompt\\n\\nPrompt example\\n\\nP1: The players who did not score did not have a ball.\\n\\nH1: The players who did not score did not have a baseball.\\n\\nL1: entailment\\n\\nP2: the man does not own a dog and does not own a cat.\\n\\nH2: the man does not own a poodle and does not own a cat.\\n\\nL2: entailment\\n\\nP3: the man does not own a terrier and does not own a cat.\\n\\nH3: the man does not own a dog and does not own a cat.\\n\\nL3: neutral\\n\\nP4: the man does not own a husky and does not own a cat.\\n\\nH4: the man does not own a dog and does not own a cat.\\n\\nL4: neutral\\n\\nE.11 Reasoning Prompt\\n\\nPrompt example\\n\\nLogical and commonsense reasoning exam.\\n\\nExplain your reasoning in detail, then answer with Yes or No. Your answers should follow this 4-line format:\\n\\nPremise: <a tricky logical statement about the world>.\\n\\nQuestion: <question requiring logical deduction>.\\n\\nReasoning: <an explanation of what you understand about the possible scenarios>.\\n\\nAnswer: <Yes or No>.\\n\\nPremise: we didn't eat pizza\\n\\nQuestion: Can we logically conclude for sure that we didn't eat food?\\n\\nReasoning: Let's think logically step by step. The premise basically tells us that\"}"}
{"id": "acl-2023-short-154", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.12 Few-shot Reasoning Prompt\\n\\nFor this prompt, we insert two demonstrations right before the test example. These are of the correct type for the test example, and they exemplify each of the two labels. The demonstrations are from a fixed set of examples, which we include here:\\n\\nE.12.1 No Negation\\n\\nPrompt example\\n\\nHere are some examples of the kind of reasoning you should do:\\n\\nPremise: The students ate pizza\\nQuestion: Can we logically conclude for sure that the students ate food?\\nReasoning: Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students ate pizza entails that the students ate food.\\nAnswer: Yes\\n\\nPremise: The students ate food\\nQuestion: Can we logically conclude for sure that the students ate pizza?\\nReasoning: Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students ate food does not allow us to conclude that the students ate pizza. They might have eaten something else.\\nAnswer: No\\n\\nE.12.2 One Scoped\\n\\nPrompt example\\n\\nHere are some examples of the kind of reasoning you should do:\\n\\nPremise: The students didn't eat any pizza\\nQuestion: Can we logically conclude for sure that the students didn't eat any food?\\nReasoning: Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students didn't eat any pizza does not allow us to conclude that the students didn't eat any food. They might have eaten something else.\\nAnswer: No\\n\\nPremise: The students didn't eat any food\\nQuestion: Can we logically conclude for sure that the students didn't eat any pizza?\\nReasoning: Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students didn't eat any food entails that the students didn't eat any pizza.\\nAnswer: Yes\"}"}
{"id": "acl-2023-short-154", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here are some examples of the kind of reasoning you should do:\\n\\nPremise: The students who weren't in class ate pizza\\nQuestion: Can we logically conclude for sure that the students who weren't in class ate food?\\nReasoning: Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students who weren't in class ate pizza entails that the students who weren't in class ate food.\\nAnswer: Yes\\n\\nPremise: The students who weren't in class ate food\\nQuestion: Can we logically conclude for sure that the students who weren't in class ate pizza?\\nReasoning: Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students who weren't in class ate food does not allow us to conclude that the students who weren't in class ate pizza. They might have eaten something else.\\nAnswer: No\\n\\nPremise: The students who weren't in class didn't eat any pizza\\nQuestion: Can we logically conclude for sure that the students who weren't in class didn't eat any food?\\nReasoning: Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students who weren't in class didn't eat any pizza does not allow us to conclude that the students who weren't in class didn't eat any food. They might have eaten something else.\\nAnswer: No\\n\\nPremise: The students who weren't in class didn't eat any food\\nQuestion: Can we logically conclude for sure that the students who weren't in class didn't eat any pizza?\\nReasoning: Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students who weren't in class didn't eat any food entails that the students who weren't in class didn't eat any pizza.\\nAnswer: Yes\"}"}
{"id": "acl-2023-short-154", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here are some examples of the kind of reasoning you should do:\\n\\n**Premise:** The students who weren't in class ate pizza that wasn't hot\\n\\n**Question:** Can we logically conclude for sure that the students who weren't in class ate food that wasn't hot?\\n\\n**Reasoning:** Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students who weren't in class ate pizza that wasn't hot entails that the students who weren't in class ate food that wasn't hot.\\n\\n**Answer:** Yes\\n\\n**Premise:** The students who weren't in class ate food that wasn't hot\\n\\n**Question:** Can we logically conclude for sure that the students who weren't in class ate pizza that wasn't hot?\\n\\n**Reasoning:** Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that the students who weren't in class ate food that wasn't hot does not allow us to conclude that the students who weren't in class ate pizza that wasn't hot. They might have eaten something else.\\n\\n**Answer:** No\\n\\n**Premise:** It is not the case that the students didn't eat any pizza\\n\\n**Question:** Can we logically conclude for sure that it is not the case that the students didn't eat any food?\\n\\n**Reasoning:** Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that it is not the case that the students didn't eat any pizza entails that it is not the case that the students didn't eat any food.\\n\\n**Answer:** Yes\\n\\n**Premise:** It is not the case that the students didn't eat any food\\n\\n**Question:** Can we logically conclude for sure that it is not the case that the students didn't eat any pizza? Reasoning: Let's think logically step by step. The premise basically tells us that pizza is a type of food. Therefore, the premise that it is not the case that the students didn't eat any food does not allow us to conclude that it is not the case that the students didn't eat any pizza. They might have eaten something else.\\n\\n**Answer:** No\"}"}
