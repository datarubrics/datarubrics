{"id": "acl-2023-long-73", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"nuwa-xl: diffusion over diffusion for extremely long video generation\\n\\nshengming yin\\\\(^1\\\\)\u2217, chenfei wu\\\\(^2\\\\)\u2217, huan yang\\\\(^2\\\\), jianfeng wang\\\\(^3\\\\), xiaodong wang\\\\(^2\\\\), minheng ni\\\\(^2\\\\), zhengyuan yang\\\\(^3\\\\), linjie li\\\\(^3\\\\), shuguang liu\\\\(^2\\\\), fan yang\\\\(^2\\\\), jianlong fu\\\\(^2\\\\), gong ming\\\\(^2\\\\), lijuan wang\\\\(^3\\\\), zicheng liu\\\\(^3\\\\), houqiang li\\\\(^1\\\\), nan duan\\\\(^2\\\\)\u2020\\n\\nuniversity of science and technology of china\\nmicrosoft research asia\\nmicrosoft azure ai\\n{sheyin@mail., lihq@USTC.EDU.CN, {chewu, huan.yang, jianfw, v-xiaodwang, t-mni, zhengyang, lindsey.li, shuguanl, fanyang, jianf, migon, lijuanw, zliu, nanduan}@microsoft.com\\n\\nabstract\\n\\nin this paper, we propose nuwa-xl, a novel diffusion over diffusion architecture for extremely long video generation. most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. instead, our approach adopts a \\\"coarse-to-fine\\\" process, in which the video can be generated in parallel at the same granularity. a global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. this simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap and makes it possible to generate all segments in parallel. to evaluate our model, we build flintstoneshd dataset, a new benchmark for long video generation. experiments show that our model not only generates high-quality long videos with both global and local coherence, but also decreases the average inference time from 7.55 min to 26 s (by 94.26%) at the same hardware setting when generating 1024 frames. the homepage link is https://msra-nuwa.azurewebsites.net/\\n\\n1 introduction\\n\\nrecently, visual synthesis has attracted a great deal of interest in the field of generative models. existing works have demonstrated the ability to generate high-quality images (ramesh et al., 2021; saharia et al., 2022; rombach et al., 2022) and short videos (e.g., 4 seconds (wu et al., 2022b), 5 seconds (singer et al., 2022), 5.3 seconds (ho et al., 2022a)). however, videos in real applications are often much longer than 5 seconds. a film typically lasts more than 90 minutes. a cartoon is usually 30 minutes long. even for \\\"short\\\" video applications like tiktok, the recommended video length is 21 to 34 seconds. longer video generation is becoming increasingly important as the demand for engaging visual content continues to grow.\\n\\nhowever, scaling to generate long videos has a significant challenge as it requires a large amount of computation resources. to overcome this challenge, most current approaches use the \\\"autoregressive over x\\\" architecture, where \\\"x\\\" denotes any generative models capable of generating short video clips, including autoregressive models like phenaki (villegas et al., 2022), tats (ge et al., 2022), nuwa-infinity (wu et al., 2022a); diffusion models like mcvd (voleti et al., 2022), fdm (harvey et al., 2022), lvdm (he et al., 2022). the main idea behind these approaches is to train the model on short video clips and then use it to generate long videos by a sliding window during inference. \\\"autoregressive over x\\\" architecture not only greatly reduces the computational burden, but also relaxes the data requirements for long videos, as only short videos are needed for training.\\n\\nunfortunately, the \\\"autoregressive over x\\\" architecture, while being a resource-sufficient solution to generate long videos, also introduces new challenges: 1) firstly, training on short videos but forcing it to infer long videos leads to an enormous training-inference gap. it can result in unrealistic shot change and long-term incoherence in generated long videos, since the model has no opportunity to learn such patterns from long videos. for example, phenaki (villegas et al., 2022) and tats (ge et al., 2022) are trained on less than 16 frames, while generating as many as 1024 frames when applied to long video generation. 2) secondly, due to the dependency limitation of the sliding window, the inference process can not be done in parallel and thus takes a much longer time. for example, tats (ge et al., 2022) takes 7.5 minutes to generate 1024 frames, while phenaki (villegas et al., 2022) takes 4.1 minutes.\"}"}
{"id": "acl-2023-long-73", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overview of NUWA-XL for extremely long video generation in a \\\"coarse-to-fine\\\" process. A global\\ndiffusion model first generates $L_k$ keyframes which form a \\\"coarse\\\" storyline of the video, a series of local diffusion\\nmodels are then applied to the adjacent frames, treated as the first and the last frames, to iteratively complete the\\nmiddle frames resulting $O(L_m)$ \\\"fine\\\" frames in total.\\n\\nTo address the above issues, we propose NUWA-XL, a \\\"Diffusion over Diffusion\\\" architecture to\\ngenerate long videos in a \\\"coarse-to-fine\\\" process, as shown in Fig. 1. In detail, a global dif-\\nfusion model first generates $L_k$ keyframes based on $L$ prompts which forms a \\\"coarse\\\" storyline\\nof the video. The first local diffusion model is then applied to $L_k$ prompts and the adjacent\\nkeyframes, treated as the first and the last frames, to complete the middle $L_2$ frames resulting in\\n$O(L_2) \\\\approx L_m^2$ \\\"fine\\\" frames in total.\\n\\nBy iteratively applying the local diffusion to fill in\\nthe middle frames, the length of the video will in-\\ncrease exponentially, leading to an extremely long\\nvideo. For example, NUWA-XL with $m$ depth and $L$ local diffusion length is capable of generating a\\nlong video with the size of $O(L_m^2)$. The advantages\\nof such a \\\"coarse-to-fine\\\" scheme are three-fold:\\n1) Firstly, such a hierarchical architecture enables\\nthe model to train directly on long videos and thus\\neliminating the training-inference gap; 2) Secondly,\\nit naturally supports parallel inference and thereby\\ncan significantly speed up long video generation;\\n3) Thirdly, as the length of the video can be ex-\\ntended exponentially w.r.t. the depth $m$, our model\\ncan be easily extended to longer videos. Our key\\ncontributions are listed in the following:\\n\\n- We propose NUWA-XL, a \\\"Diffusion over\\nDiffusion\\\" architecture by viewing long video\\ngeneration as a novel \\\"coarse-to-fine\\\" process.\\n- To the best of our knowledge, NUWA-XL is\\nthe first model directly trained on long videos\\n(3376 frames), which closes the training-\\ninference gap in long video generation.\\n- NUWA-XL enables parallel inference, which\\nsignificantly speeds up long video generation.\\nConcretely, NUWA-XL speeds up inference\\nby 94.26% when generating 1024 frames.\\n- We build FlintstonesHD, a new dataset to val-\\nidate the effectiveness of our model and pro-\\nvide a benchmark for long video generation.\\n\\n2 Related Work\\nImage and Short Video Generation\\nImage Gen-\\neration has made many progresses, auto-regressive\\nmethods (Ramesh et al., 2021; Ding et al., 2021;\\nYu et al., 2022; Ding et al., 2022) leverage VQV AE\\nto tokenize the images into discrete tokens and use\\nTransformers (Vaswani et al., 2017) to model the\\ndependency between tokens. DDPM (Ho et al.,\\n2020) presents high-quality image synthesis results.\\nLDM (Rombach et al., 2022) performs a diffusion\\nprocess on latent space, showing significant effi-\\nciency and quality improvements.\\n\\nSimilar advances have been witnessed in video\\ngeneration, (Vondrick et al., 2016; Saito et al.,\"}"}
{"id": "acl-2023-long-73", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pan et al. (2017); Li et al. (2018); Tulyakov et al. (2018) extend GAN to video generation. Syncdraw (Mittal et al., 2017) uses a recurrent VAE to automatically generate videos. GODIVA (Wu et al., 2021) proposes a three-dimensional sparse attention to map text tokens to video tokens. VideoGPT (Yan et al., 2021) adapts Transformer-based image generation models to video generation with minimal modifications. NUWA (Wu et al., 2022b) with 3D Nearby Attention extends GODIVA (Wu et al., 2021) to various generation tasks in a unified representation. Cogvideo (Hong et al., 2022) leverages a frozen T2I model (Ding et al., 2022) by adding additional temporal attention modules. More recently, diffusion methods (Ho et al., 2022b; Singer et al., 2022; Ho et al., 2022a) have also been applied to video generation. Among them, VDM (Ho et al., 2022b) replaces the typical 2D U-Net for modeling images with a 3D U-Net. Make-a-video (Singer et al., 2022) successfully extends a diffusion-based T2I model to T2V without text-video pairs. Imagen Video (Ho et al., 2022a) leverages a cascade of video diffusion models to text-conditional video generation.\\n\\nDifferent from these works, which concentrate on short video generation, we aim to address the challenges associated with long video generation.\\n\\nLong Video Generation\\n\\nTo address the high computational demand in long video generation, most existing works leverage the \\\"Autoregressive over X\\\" architecture, where \\\"X\\\" denotes any generative models capable of generating short video clips. With \\\"X\\\" being an autoregressive model, NUWA-Infinity (Wu et al., 2022a) introduces autoregressive over autoregressive model, with a local autoregressive to generate patches and a global autoregressive to model the consistency between different patches. TATS (Ge et al., 2022) presents a time-agnostic VQGAN and time-sensitive transformer model, trained only on clips with tens of frames but can infer thousands of frames using a sliding window mechanism. Phenaki (Villegas et al., 2022) with C-ViViT as encoder and MaskGiT (Chang et al., 2022) as backbone generates variable-length videos conditioned on a sequence of open domain text prompts. With \\\"X\\\" being diffusion models, MCVD (Voleti et al., 2022) trains the model to solve multiple video generation tasks by randomly and independently masking all the past or future frames. FDM (Harvey et al., 2022) presents a DDPMs-based framework that produces long-duration video completions in a variety of realistic environments.\\n\\nDifferent from existing \\\"Autoregressive over X\\\" models trained on short clips, we propose NUWA-XL, a Diffusion over Diffusion model directly trained on long videos to eliminate the training-inference gap. Besides, NUWA-XL enables parallel inference to speed up long video generation.\\n\\n3 Method\\n\\n3.1 Temporal KLV AE (T-KLV AE)\\n\\nTraining and sampling diffusion models directly on pixels are computationally costly, KLV AE (Rombach et al., 2022) compresses an original image into a low-dimensional latent representation where the diffusion process can be performed to alleviate this issue. To leverage external knowledge from the pre-trained image KLV AE and transfer it to videos, we propose Temporal KLV AE (T-KLV AE) by adding external temporal convolution and attention layers while keeping the original spatial modules intact.\\n\\nGiven a batch of video \\\\( v \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times C \\\\times H \\\\times W} \\\\) with \\\\( b \\\\) batch size, \\\\( L \\\\) frames, \\\\( C \\\\) channels, \\\\( H \\\\) height, \\\\( W \\\\) width, we first view it as \\\\( L \\\\) independent images and encode them with the pre-trained KLV AE spatial convolution. To further model temporal information, we add a temporal convolution after each spatial convolution. To keep the original pretrained knowledge intact, the temporal convolution is initialized as an identity function which guarantees the output to be exactly the same as the original KLV AE. Concretely, the convolution weight \\\\( W_{\\\\text{conv}}^{1d} \\\\in \\\\mathbb{R}^{c_{\\\\text{out}} \\\\times c_{\\\\text{in}} \\\\times k} \\\\) is first set to zero where \\\\( c_{\\\\text{out}} \\\\) denotes the out channel, \\\\( c_{\\\\text{in}} \\\\) denotes the in channel and is equal to \\\\( c_{\\\\text{out}} \\\\), \\\\( k \\\\) denotes the temporal kernel size. Then, for each output channel \\\\( i \\\\), the middle of the kernel size \\\\( (k - 1) / 2 \\\\) of the corresponding input channel \\\\( i \\\\) is set to 1:\\n\\n\\\\[\\nW_{\\\\text{conv}}^{1d}[i, i, (k - 1) / 2] = 1\\n\\\\]\\n\\nSimilarly, we add a temporal attention after the original spatial attention, and initialize the weights \\\\( W_{\\\\text{att}} \\\\_\\\\text{out} \\\\_\\\\text{in} \\\\) in the out projection layer into zero:\\n\\n\\\\[\\nW_{\\\\text{att}} \\\\_\\\\text{out} \\\\_\\\\text{in} = 0\\n\\\\]\\n\\nFor the T-KLV AE decoder \\\\( D \\\\), we use the same initialization strategy. The training objective of T-KLV AE is the same as the image KLV AE. Finally, we get a latent code \\\\( x_0 \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c \\\\times h \\\\times w} \\\\), a compact representation of the original video \\\\( v \\\\).\"}"}
{"id": "acl-2023-long-73", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Mask Temporal Diffusion (MTD)\\n\\nIn this section, we introduce Mask Temporal Diffusion (MTD) as a basic diffusion model for our proposed Diffusion over Diffusion architecture. For global diffusion, only L prompts are used as inputs which form a \u201ccoarse\u201d storyline of the video, however, for the local diffusion, the inputs consist of not only L prompts but also the first and last frames. Our proposed MTD which can accept input conditions with or without first and last frames, supports both global diffusion and local diffusion. In the following, we first introduce the overall pipeline of MTD and then dive into an UpBlock as an example to introduce how we fuse different input conditions.\\n\\nInput L prompts, we first encode them by a CLIP Text Encoder to get the prompt embedding \\\\( p \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times l_p \\\\times d_p} \\\\) where \\\\( b \\\\) is batch size, \\\\( l_p \\\\) is the number of tokens, \\\\( d_p \\\\) is the prompt embedding dimension. The randomly sampled diffusion timestep \\\\( t \\\\sim U(1, T) \\\\) is embedded to timestep embedding \\\\( t \\\\in \\\\mathbb{R}^{c} \\\\). The video \\\\( v_0 \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times C \\\\times H \\\\times W} \\\\) with \\\\( L \\\\) frames is encoded by T-KLVAE to get a representation \\\\( x_0 \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c \\\\times h \\\\times w} \\\\). According to the predefined diffusion process:\\n\\n\\\\[\\nq(x_t | x_{t-1}) = \\\\mathcal{N}(x_t; \\\\sqrt{\\\\alpha_t} x_{t-1}, (1-\\\\alpha_t) I) \\\\tag{3}\\n\\\\]\\n\\n\\\\( x_0 \\\\) is corrupted by:\\n\\n\\\\[\\nx_t = \\\\sqrt{\\\\bar{\\\\alpha}_t} x_0 + \\\\sqrt{(1-\\\\bar{\\\\alpha}_t)} \\\\epsilon \\\\epsilon \\\\sim \\\\mathcal{N}(0, I) \\\\tag{4}\\n\\\\]\\n\\nwhere \\\\( \\\\epsilon \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c \\\\times h \\\\times w} \\\\) is noise, \\\\( x_t \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c \\\\times h \\\\times w} \\\\) is the \\\\( t \\\\)-th intermediate state in diffusion process, \\\\( \\\\alpha_t, \\\\bar{\\\\alpha}_t \\\\) is hyperparameters in diffusion model.\\n\\nFor the global diffusion model, the visual conditions \\\\( v_c_0 \\\\) are all-zero. However, for the local diffusion models, \\\\( v_c_0 \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times C \\\\times H \\\\times W} \\\\) are obtained by masking the middle \\\\( L-2 \\\\) frames in \\\\( v_0 \\\\). \\\\( v_c_0 \\\\) is also encoded by T-KLVAE to get a representation \\\\( x_c_0 \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c \\\\times h \\\\times w} \\\\). Finally, the \\\\( x_t, p, t, x_c_0 \\\\) are fed into a Mask 3D-UNet \\\\( \\\\epsilon_\\\\theta(\\\\cdot) \\\\).\\n\\nThen, the model is trained to minimize the distance between the output of the Mask 3D-UNet \\\\( \\\\epsilon_\\\\theta(x_t, p, t, x_c_0) \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c \\\\times h \\\\times w} \\\\) and \\\\( \\\\epsilon \\\\).\\n\\n\\\\[\\nL_\\\\theta = ||\\\\epsilon - \\\\epsilon_\\\\theta(x_t, p, t, x_c_0)||^2_2 \\\\tag{5}\\n\\\\]\"}"}
{"id": "acl-2023-long-73", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Visualization of the last UpBlock in Mask 3D-UNet with purple lines standing for diffusion process, red for prompts, pink for timestep, green for visual condition.\\n\\nwhile the $x_c^0$ is downsampled to the corresponding resolution with a cascade of convolution layers and fed to the corresponding DownBlock and UpBlock.\\n\\nTo better understand how Mask 3D-UNet works, we dive into the last UpBlock and show the details in Fig. 3. The UpBlock takes hidden states $h$, skip connection $s$, timestep embedding $t$, visual condition $x_c^0$ and prompts embedding $p$ as inputs and output hidden state $h_{out}$. It is noteworthy that for global diffusion, $x_c^0$ does not contain valid information as there are no frames provided as conditions, however, for local diffusion, $x_c^0$ contains encoded information from the first and last frames.\\n\\nThe input skip connection $s \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c_{skip} \\\\times h \\\\times w}$ is first concatenated to the input hidden state $h_{in} \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c_{in} \\\\times h \\\\times w}$.\\n\\n$$h := [s; h_{in}]$$ (6)\\n\\nwhere the hidden state $h \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times (c_{skip} + c_{in}) \\\\times h \\\\times w}$ is then convoluted to target number of channels.\\n\\n$$h := \\\\text{SpatialConv}(h)$$ (8)\\n\\n$$h := \\\\text{TemporalConv}(h)$$ (9)\\n\\nThen, $h$ is conditioned on $x_c^0 \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c \\\\times h \\\\times w}$ and $x_m^0 \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times 1 \\\\times h \\\\times w}$ where $x_m^0$ is a binary mask to indicate which frames are treated as conditions. They are first transferred to scale $w_c$, $w_m$ and shift $b_c$, $b_m$ via zero-initialized convolution layers and then injected to $h$ via linear projection.\\n\\n$$h := w_c \\\\cdot h + b_c + h$$ (10)\\n\\n$$h := w_m \\\\cdot h + b_m + h$$ (11)\\n\\nAfter that, a stack of Spatial Self-Attention (SA), Prompt Cross-Attention (PA), and Temporal Self-Attention (TA) are applied to $h$.\\n\\nFor the Spatial Self-Attention (SA), the hidden state $h \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c \\\\times h \\\\times w}$ is reshaped to $h \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times h \\\\times w \\\\times c}$ with length dimension $L$ treated as batch-size.\\n\\n$$Q_{SA} = hW_{SA} Q; K_{SA} = hW_{SA} K; V_{SA} = hW_{SA} V$$ (12)\\n\\n$$\\\\tilde{Q}_{SA} = \\\\text{Selfattn}(Q_{SA}, K_{SA}, V_{SA})$$ (13)\\n\\nwhere $W_{SA} Q, W_{SA} K, W_{SA} V \\\\in \\\\mathbb{R}^{c \\\\times d_{in}}$ are parameters to be learned.\\n\\nFor the Prompt Cross-Attention (PA), the prompt embedding $p \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times l_p \\\\times d_p}$ is reshaped to $p \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times l_p \\\\times d_p}$ with length dimension $L$ treated as batch-size.\\n\\n$$Q_{PA} = hW_{PA} Q; K_{PA} = pW_{PA} K; V_{PA} = pW_{PA} V$$ (14)\\n\\n$$\\\\tilde{Q}_{PA} = \\\\text{Crossattn}(Q_{PA}, K_{PA}, V_{PA})$$ (15)\\n\\nwhere $Q_{PA} \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times h \\\\times d_{in}}, K_{PA} \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times l_p \\\\times d_{in}}, V_{PA} \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times l_p \\\\times d_{in}}$ are query, key and value, respectively. $W_{PA} Q \\\\in \\\\mathbb{R}^{c \\\\times d_{in}}, W_{PA} K \\\\in \\\\mathbb{R}^{d_p \\\\times d_{in}}, W_{PA} V \\\\in \\\\mathbb{R}^{d_p \\\\times d_{in}}$ are parameters to be learned.\\n\\nThe Temporal Self-Attention (TA) is exactly the same as Spatial Self-Attention (SA) except that spatial axis $hw$ is treated as batch-size and temporal length $L$ is treated as sequence length.\\n\\nFinally, the hidden state $h$ is upsampled to target resolution $h_{out} \\\\in \\\\mathbb{R}^{b \\\\times L \\\\times c \\\\times h_{out} \\\\times w_{out}}$ via spatial convolution. Similarly, other blocks in Mask 3D-UNet leverage the same structure to deal with the corresponding inputs.\"}"}
{"id": "acl-2023-long-73", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3 Diffusion over Diffusion Architecture\\n\\nIn the following, we first introduce the inference process of MTD, then we illustrate how to generate a long video via Diffusion over Diffusion Architecture in a novel \u201ccoarse-to-fine\u201d process.\\n\\nIn inference phase, given the prompts $p$ and visual condition $v_{c0}$, $x_0$ is sampled from a pure noise $x_T$ by MTD. Concretely, for each timestep $t = T, T-1, \\\\ldots, 1$, the intermediate state $x_t$ in diffusion process is updated by\\n\\n$$x_{t-1} = \\\\sqrt{\\\\alpha_t} (x_t - \\\\alpha_t \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} \\\\epsilon_{\\\\theta}(x_t, p, t, x_c0)) + (1 - \\\\bar{\\\\alpha}_{t-1}) \\\\beta_t$$\\n\\nwhere $\\\\epsilon \\\\sim N(0, I)$, $p$ and $t$ are embedded prompts and timestep, $x_c0$ is encoded $v_{c0}$, $\\\\alpha_t$, $\\\\bar{\\\\alpha}_t$, $\\\\beta_t$ are hyperparameters in MTD.\\n\\nFinally, the sampled latent code $x_0$ will be decoded to video pixels $v_0$ by T-KLV AE. For simplicity, the iterative generation process of MTD is noted as $v_0 = \\\\text{Diffusion}(p, v_{c0})$ (17).\\n\\nWhen generating long videos, given the $L$ prompts $p_1$ with large intervals, the $L$ keyframes are first generated through a global diffusion model.\\n\\n$v_{01} = \\\\text{GlobalDiffusion}(p_1, v_{c01})$ (18)\\n\\nwhere $v_{c01}$ is all-zero as there are no frames provided as visual conditions. The temporally sparse keyframes $v_{01}$ form the \u201ccoarse\u201d storyline of the video.\\n\\nThen, the adjacent keyframes in $v_{01}$ are treated as the first and the last frames in visual condition $v_{c02}$. The middle $L-2$ frames are generated by feeding $p_2$, $v_{c02}$ into the first local diffusion model where $p_2$ are $L$ prompts with smaller time intervals.\\n\\n$v_{02} = \\\\text{LocalDiffusion}(p_2, v_{c02})$ (19)\\n\\nSimilarly, $v_{c03}$ is obtained from adjacent frames in $v_{02}$, $p_3$ are $L$ prompts with even smaller time intervals. The $p_3$ and $v_{c03}$ are fed into the second local diffusion model.\\n\\n$v_{03} = \\\\text{LocalDiffusion}(p_3, v_{c03})$ (20)\\n\\nCompared to frames in $v_{01}$, the frames in $v_{02}$ and $v_{03}$ are increasingly \u201cfine\u201d with stronger consistency and more details. By iteratively applying the local diffusion to complete the middle frames, our model with $m$ depth is capable of generating extremely long video with the length of $O(Lm)$. Meanwhile, such a hierarchical architecture enables us to directly train on temporally sparsely sampled frames in long videos (3376 frames) to eliminate the training-inference gap. After sampling the $L$ keyframes by global diffusion, the local diffusions can be performed in parallel to accelerate the inference speed.\\n\\n4 Experiments\\n\\n4.1 FlintstonesHD Dataset\\n\\nExisting annotated video datasets have greatly promoted the development of video generation. However, the current video datasets still pose a great challenge to long video generation. First, the length of these videos is relatively short, and there is an enormous distribution gap between short videos and long videos such as shot change and long-term dependency. Second, the relatively low resolution limits the quality of the generated video. Third, most of the annotations are coarse descriptions of the content of the video clips, and it is difficult to illustrate the details of the movement.\\n\\nTo address the above issues, we build FlintstonesHD dataset, a densely annotated long video dataset, providing a benchmark for long video generation. We first obtain the original Flintstones cartoon which contains 166 episodes with an average of 38000 frames of $1440 \\\\times 1080$ resolution. To support long video generation based on the story and capture the details of the movement, we leverage the image captioning model GIT2 (Wang et al., 2022) to generate dense captions for each frame in the dataset first and manually filter some errors in the generated results.\\n\\n4.2 Metrics\\n\\nAvg-FID Frechet Inception Distance (FID) (Heusel et al., 2017), a metric used to evaluate image generation, is introduced to calculate the average quality of generated frames. Block-FVD Frechet Video Distance (FVD) (Unterthiner et al., 2018) is widely used to evaluate the quality of the generated video. In this paper, we propose Block FVD for long video generation, which splits a long video into several short clips to calculate the average FVD of all clips. For simplicity, we name it B-FVD-X where X denotes the length of the short clips.\\n\\n3.3 Diffusion over Diffusion Architecture\\n\\nIn the following, we first introduce the inference process of MTD, then we illustrate how to generate a long video via Diffusion over Diffusion Architecture in a novel \u201ccoarse-to-fine\u201d process. In inference phase, given the $L$ prompts $p$ and visual condition $v_{c0}$, $x_0$ is sampled from a pure noise $x_T$ by MTD. Concretely, for each timestep $t = T, T-1, \\\\ldots, 1$, the intermediate state $x_t$ in diffusion process is updated by\\n\\n$$x_{t-1} = \\\\sqrt{\\\\alpha_t} (x_t - \\\\alpha_t \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} \\\\epsilon_{\\\\theta}(x_t, p, t, x_c0)) + (1 - \\\\bar{\\\\alpha}_{t-1}) \\\\beta_t$$\\n\\nwhere $\\\\epsilon \\\\sim N(0, I)$, $p$ and $t$ are embedded prompts and timestep, $x_c0$ is encoded $v_{c0}$, $\\\\alpha_t$, $\\\\bar{\\\\alpha}_t$, $\\\\beta_t$ are hyperparameters in MTD.\\n\\nFinally, the sampled latent code $x_0$ will be decoded to video pixels $v_0$ by T-KLV AE. For simplicity, the iterative generation process of MTD is noted as $v_0 = \\\\text{Diffusion}(p, v_{c0})$ (17).\\n\\nWhen generating long videos, given the $L$ prompts $p_1$ with large intervals, the $L$ keyframes are first generated through a global diffusion model.\\n\\n$v_{01} = \\\\text{GlobalDiffusion}(p_1, v_{c01})$ (18)\\n\\nwhere $v_{c01}$ is all-zero as there are no frames provided as visual conditions. The temporally sparse keyframes $v_{01}$ form the \u201ccoarse\u201d storyline of the video.\\n\\nThen, the adjacent keyframes in $v_{01}$ are treated as the first and the last frames in visual condition $v_{c02}$. The middle $L-2$ frames are generated by feeding $p_2$, $v_{c02}$ into the first local diffusion model where $p_2$ are $L$ prompts with smaller time intervals.\\n\\n$v_{02} = \\\\text{LocalDiffusion}(p_2, v_{c02})$ (19)\\n\\nSimilarly, $v_{c03}$ is obtained from adjacent frames in $v_{02}$, $p_3$ are $L$ prompts with even smaller time intervals. The $p_3$ and $v_{c03}$ are fed into the second local diffusion model.\\n\\n$v_{03} = \\\\text{LocalDiffusion}(p_3, v_{c03})$ (20)\\n\\nCompared to frames in $v_{01}$, the frames in $v_{02}$ and $v_{03}$ are increasingly \u201cfine\u201d with stronger consistency and more details. By iteratively applying the local diffusion to complete the middle frames, our model with $m$ depth is capable of generating extremely long video with the length of $O(Lm)$. Meanwhile, such a hierarchical architecture enables us to directly train on temporally sparsely sampled frames in long videos (3376 frames) to eliminate the training-inference gap. After sampling the $L$ keyframes by global diffusion, the local diffusions can be performed in parallel to accelerate the inference speed.\"}"}
{"id": "acl-2023-long-73", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method          | Temporal Layers | FID 128f | B-FVD-16 128f | Time 128f | FID 256f | B-FVD-16 256f | Time 256f | FID 1024f | B-FVD-16 1024f | Time 1024f |\\n|-----------------|----------------|----------|---------------|-----------|----------|---------------|-----------|-----------|----------------|------------|\\n| Phenaki         | AR over AR     | \u2193 40.14  | 544.72        | 4s        | \u2193 43.13  | 573.55        | 65s       | \u2193 48.56   | 622.06        | 259s       |\\n| FDM*            | AR over Diff   | \u2193 34.47  | 532.94        | 7s        | \u2193 38.28  | 561.75        | 114s      | \u2193 43.24   | 618.42        | 453s       |\\n| NUWA-XL         | AR over Diff   | \u2193 35.95  | 520.19        | 7s        | \u2193 35.68  | 542.26        | 17s       | \u2193 35.79   | 572.86        | 26s        |\\n| NUWA-XL         | Diff over Diff | \u2193 32.66  | 580.21        | 15s       | \u2193 32.05  | 609.32        |           |           |                |            |\\n| NUWA-XL         | Diff over Diff |          |               |           |          |               |           |           |                |            |\\n| NUWA-XL         | Diff over Diff |          |               |           |          |               |           |           |                |            |\\n| NUWA-XL         | Diff over Diff |          |               |           |          |               |           |           |                |            |\\n| NUWA-XL         | Diff over Diff |          |               |           |          |               |           |           |                |            |\\n\\nTable 1: Quantitative comparison with the state-of-the-art models for long video generation on FlintstonesHD dataset. 128 and 256 denote the resolutions of the generated videos. *Note that the original FDM model does not support text input. For a fair comparison, we implement an FDM with text input.\\n\\n### 4.3 Quantitative Results\\n\\n#### 4.3.1 Comparison with the state-of-the-arts\\n\\nWe compare NUWA-XL on FlintstonesHD with the state-of-the-art models in Table 1. Here, we report FID, B-FVD-16, and inference time. For \\\"Autoregressive over X (AR over X)\\\" architecture, due to error accumulation, the average quality of generated frames (Avg-FID) declines as the video length increases. However, for NUWA-XL, where the frames are not generated sequentially, the quality does not decline with video length. Meanwhile, compared to \\\"AR over X\\\" which is trained only on short videos, NUWA-XL is capable of generating higher quality long videos. As the video length grows, the quality of generated segments (B-FVD-16) of NUWA-XL declines more slowly as NUWA-XL has learned the patterns of long videos.\\n\\nBesides, because of parallelization, NUWA-XL significantly improves the inference speed by 85.09% when generating 256 frames and by 94.26% when generating 1024 frames.\\n\\n#### 4.3.2 Ablation study\\n\\n**KLV AE**\\n\\nTable 2a shows the comparison of different KLV AE settings. KLV AE means treating the video as independent images and reconstructing them independently. T-KLV AE-R means the introduced temporal layers are randomly initialized. Compared to KLV AE, we find the newly introduced temporal layers can significantly increase the ability of video reconstruction. Compared to T-KLV AE-R, the slightly better FID and FVD in T-KLV AE illustrate the effectiveness of identity initialization.\\n\\n**MTD**\\n\\nTable 2b shows the comparison of different MTD settings. MTD w/o MS means the model is trained without motion synchronization. MTD w/o S means the model is trained without self-supervision. MTD means the model is trained with motion synchronization and self-supervision. Compared to MTD w/o MS and MTD w/o S, NUWA-XL is capable of generating higher quality long videos. As the video length grows, the quality of generated segments (B-FVD-16) of NUWA-XL declines more slowly as NUWA-XL has learned the patterns of long videos.\\n\\n**NUWA-XL depth**\\n\\nTable 2c shows the comparison of different NUWA-XL depth. NUWA-XL-D1 means the model is trained with 1 layer of diffusion, NUWA-XL-D2 means the model is trained with 2 layers of diffusion, and NUWA-XL-D3 means the model is trained with 3 layers of diffusion. Compared to NUWA-XL-D1 and NUWA-XL-D2, NUWA-XL-D3 is capable of generating higher quality long videos. As the video length grows, the quality of generated segments (B-FVD-16) of NUWA-XL-D3 declines more slowly as NUWA-XL-D3 has learned the patterns of long videos.\\n\\n**Local diffusion length**\\n\\nTable 2d shows the comparison of different local diffusion length. NUWA-XL-L8 means the model is trained with 8 layers of diffusion, NUWA-XL-L16 means the model is trained with 16 layers of diffusion, and NUWA-XL-L32 means the model is trained with 32 layers of diffusion. Compared to NUWA-XL-L8 and NUWA-XL-L16, NUWA-XL-L32 is capable of generating higher quality long videos. As the video length grows, the quality of generated segments (B-FVD-16) of NUWA-XL-L32 declines more slowly as NUWA-XL-L32 has learned the patterns of long videos.\\n\\n**OOM**\\n\\nOOM stands for Out Of Memory.\"}"}
{"id": "acl-2023-long-73", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Qualitative comparison between AR over Diffusion and Diffusion over Diffusion for long video generation on FlintstonesHD. The Arabic number in the lower right corner indicates the frame number with yellow standing for keyframes with large intervals and green for small intervals. Compared to AR over Diffusion, NUWA-XL generates long videos with long-term coherence (see the cloth in frame 22 and 1688) and realistic shot change (frame 17-20).\\n\\nMTD Tab. 2b shows the comparison of different global/local diffusion settings. MI (Multi-scale Injection) means whether visual conditions are injected to multi-scale DownBlocks and UpBlocks in Mask 3D-UNet or only injected to the Downblock and UpBlock with the highest scale. SI (Symmetry Injection) means whether the visual condition is injected into both DownBlocks and UpBlocks or it is only injected into UpBlocks. Comparing MTD w/o MS and MTD w/o S, multi-scale injection is significant for long video generation. Compared to MTD w/o S, the slightly better FID and FVD in MTD show the effectiveness of symmetry injection.\\n\\nDepth of Diffusion over Diffusion Tab. 2c shows the comparison of B-FVD-16 of different NUWA-XL depth $m$ with local diffusion length $L$ fixed to 16. When generating 16 frames, NUWA-XL with different depths achieves comparable results. However, as the depth increases, NUWA-XL can produce videos that are increasingly longer while still maintaining relatively high quality.\\n\\nLength in Diffusion over Diffusion Tab. 2d shows the comparison of B-FVD-16 of diffusion local length $L$ with NUWA-XL depth $m$ fixed to 3. In comparison, when generating videos with the same length, as the local diffusion length increases, NUWA-XL can generate higher-quality videos.\\n\\n4.4 Qualitative results\\nFig. 4 provides a qualitative comparison between AR over Diffusion and Diffusion over Diffusion for long video generation on FlintstonesHD. As introduced in Sec. 1, when generating long videos, \\\"Autoregressive over X\\\" architecture trained only on short videos will lead to long-term incoherence (between frame 22 and frame 1688) and unrealistic shot change (from frame 17 to frame 20) since the model has no opportunity to learn the distribution of long videos. However, by training directly on long videos, NUWA-XL successfully models the distribution of long videos and generates long videos with long-term coherence and realistic shot change.\\n\\n5 Conclusion\\nWe propose NUWA-XL, a \\\"Diffusion over Diffusion\\\" architecture by viewing long video generation as a novel \\\"coarse-to-fine\\\" process. To the best of our knowledge, NUWA-XL is the first model directly trained on long videos (3376 frames), closing the training-inference gap in long video generation. Additionally, NUWA-XL allows for parallel inference, greatly increasing the speed of long video generation by 94.26% when generating 1024 frames. We further build FlintstonesHD, a new dataset to validate the effectiveness of our model and provide a benchmark for long video generation.\"}"}
{"id": "acl-2023-long-73", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\nAlthough our proposed NUWA-XL improves the quality of long video generation and accelerates the inference speed, there are still several limitations:\\n\\nFirst, due to the unavailability of open-domain long videos (such as movies, and TV shows), we only validate the effectiveness of NUWA-XL on public available cartoon Flintstones. We are actively building an open-domain long video dataset and have achieved some phased results, we plan to extend NUWA-XL to open-domain in future work.\\n\\nSecond, direct training on long videos reduces the training-inference gap but poses a great challenge to data. Third, although NUWA-XL can accelerate the inference speed, this part of the gain requires reasonable GPU resources to support parallel inference.\\n\\nEthics Statement\\nThis research is done in alignment with Microsoft\u2019s responsible AI principles.\\n\\nAcknowledgements\\nWe'd like to thank Yu Liu, Jieyu Xiao, and Scarlett Li for the discussion of the potential cartoon scenarios. We'd also like to thank Yang Ou and Bella Guo for the design of the homepage. We'd also like to thank Yan Xia, Ting Song, and Tiantian Xue for the implementation of the homepage.\\n\\nReferences\\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. 2022. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315\u201311325.\\n\\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, and Hongxia Yang. 2021. Cogview: Mastering text-to-image generation via transformers. In Advances in Neural Information Processing Systems, volume 34, pages 19822\u201319835.\\n\\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. 2022. CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers.\\n\\nSongwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. 2022. Long video generation with time-agnostic vqgan and time-sensitive transformer.\\n\\nWilliam Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. 2022. Flexible Diffusion Modeling of Long Videos.\\n\\nYingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. 2022. Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths.\\n\\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, volume 30.\\n\\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, and David J. Fleet. 2022a.Imagen video: High ~video generation with diffusion models.\\n\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851.\\n\\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. 2022b. Video diffusion models.\\n\\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. 2022. CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers.\\n\\nYitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. 2018. Video generation from text. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.\\n\\nGaurav Mittal, Tanya Marwah, and Vineeth N. Balasubramanian. 2017. Sync-draw: Automatic video generation using deep recurrent attentive architectures. In Proceedings of the 25th ACM International Conference on Multimedia, pages 1096\u20131104.\\n\\nYingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. 2017. To create what you tell: Generating videos from captions. In Proceedings of the 25th ACM International Conference on Multimedia, pages 1789\u20131798.\\n\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. In Proceedings of the 38th International Conference on Machine Learning, pages 8821\u20138831. PMLR.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. pages 10684\u201310695.\"}"}
{"id": "acl-2023-long-73", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and Rapha Gontijo Lopes. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.\\n\\nMasaki Saito, Eiichi Matsumoto, and Shunta Saito. 2017. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE International Conference on Computer Vision, pages 2830\u20132839.\\n\\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. 2022. Make-A-Video: Text-to-Video Generation without Text-Video Data.\\n\\nSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. 2018. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1526\u20131535.\\n\\nThomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. 2018. Towards accurate generative models of video: A new metric & challenges.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.\\n\\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. 2022. Phenaki: Variable length video generation from open domain textual description.\\n\\nVikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. 2022. Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation.\\n\\nCarl von Brandis, Hamed Pirsiavash, and Antonio Torralba. 2016. Generating Videos with Scene Dynamics.\\n\\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. 2022. GIT: A Generative Image-to-text Transformer for Vision and Language.\\n\\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. 2021. GODIVA: Generating Open-Domain Videos from Natural Descriptions.\\n\\nChenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan Duan. 2022a. NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis.\\n\\nChenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. 2022b. NUWA: Visual Synthesis Pre-training for Neural Visual World Creation. In Proceedings of the European Conference on Computer Vision (ECCV).\\n\\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. 2021. VideoGPT: Video Generation using VQ-VAE and Transformers.\\n\\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, and Burcu Karagol Ayan. 2022. Scaling Autoregressive Models for Content-Rich Text-to-Image Generation.\"}"}
{"id": "acl-2023-long-73", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\n\u25a1 B Did you use or create scientific artifacts?\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\n\u25a1 C Did you run computational experiments?\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-73", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\nNo response.\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\nNo response.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\nNo response.\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\nLeft blank.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\nNo response.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\nNo response.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\nNo response.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\nNo response.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\nNo response.\"}"}
