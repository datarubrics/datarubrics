{"id": "lrec-2024-main-1177", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Query-driven Relevant Paragraph Extraction from Legal Judgments\\n\\nSantosh T.Y.S, Elvin Quero Hernandez, Matthias Grabmair\\nSchool of Computation, Information, and Technology; Technical University of Munich, Germany\\n{santosh.tokala, elvin.quero, matthias.grabmair}@tum.de\\n\\nAbstract\\nLegal professionals often grapple with navigating lengthy legal judgements to pinpoint information that directly address their queries. This paper focus on this task of extracting relevant paragraphs from legal judgements based on the query. We construct a specialized dataset for this task from the European Court of Human Rights (ECtHR) using the case law guides. We assess the performance of current retrieval models in a zero-shot way and also establish fine-tuning benchmarks using various models. The results highlight the significant gap between fine-tuned and zero-shot performance, emphasizing the challenge of handling distribution shift in the legal domain. We notice that the legal pre-training handles distribution shift on the corpus side but still struggles on query side distribution shift, with unseen legal queries. We also explore various Parameter Efficient Fine-Tuning (PEFT) methods to evaluate their practicality within the context of information retrieval, shedding light on the effectiveness of different PEFT methods across diverse configurations with pre-training and model architectures influencing the choice of PEFT method.\\n\\nKeywords: Relevant Paragraph Identification, Parameter Efficient Retrieval, Legal Retrieval\\n\\n1. Introduction\\nLegal professionals including lawyers, judges and paralegals, often need to sift through voluminous legal judgments that encompass crucial insights for case law interpretations and judicial reasoning. These judgments, often lengthy, contain nuanced paragraphs holding the key to understanding legal principles, precedents and arguments. Finding relevant case law accounts for roughly 15 hours per week for a lawyer (Lastres, 2015) or nearly 30% of their annual working hours (Poje, 2014). Recent advances in NLP offer new possibilities to bridge this gap by providing summaries of these documents (e.g., Bhattacharya et al. 2019; Shukla et al. 2022 inter alia). Nonetheless, practitioners still face challenges in navigating these texts to uncover specific paragraphs that address their queries. The current manual approach is labor-intensive and susceptible to overlooking essential details. Automating this process of identifying paragraphs relevant to the query streamlines legal research, allowing them to access relevant information efficiently.\\n\\nFinding relevant paragraphs to a query is a challenging task unlike traditional adhoc information retrieval. Firstly, the legal domain is characterized by a vast and intricate vocabulary, interwoven with domain-specific jargon that can vary across different legal jurisdictions. This linguistic complexity demands an in-depth understanding of nuanced legal concepts, posing a substantial challenge for automated systems. The variation in legal writing style further compounds the challenge. Judgments may employ different degrees of formalism and offer varying levels of explicitness. These nuances can lead to difficulties in discerning context and accurately identifying relevant paragraphs that address specific queries. Another key challenge stems from the evolving nature of the legal case law. New legal doctrines, precedents and interpretations continually emerge, leading to an ever-evolving array of legal concepts and principles. This dynamism necessitates a flexible and adaptive approach to comprehend new queries and determine relevance.\\n\\nTo investigate the ability of current retrieval models to identify relevant paragraphs, a high-quality labeled dataset is imperative. However, creating such datasets is resource-intensive, often necessitating the involvement of legal experts to produce queries and relevance labels. In this study, we employ distant supervision to construct a dataset tailored for the task of query-driven relevant paragraph extraction from legal judgments by the European Court of Human Rights (ECtHR) which addresses grievances by individuals against states for alleged violations of rights outlined in the European Convention of Human Rights. Our approach capitalizes on the case-law guides available through the ECtHR\u2019s Knowledge Sharing platform.\\n\\nWe pose the case-law guide\u2019s section headers as queries, mirroring the legal concepts professionals utilize when searching within ECtHR judgments. We gather relevance signals by identifying the pinpointed citations to the paragraphs in the judgments within these guides under each section. Further, we meticulously design various splits to assess the generalizability of systems towards new queries (legal concepts), adapting to the evolution of law.\"}"}
{"id": "lrec-2024-main-1177", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As a second contribution, we assess the performance of current retrieval models in a zero-shot manner using our dataset and further establish fine-tuning benchmarks employing diverse retrieval techniques encompassing dense bi-encoder and cross-encoder architectures. Our experiments reveal the drastic gap between fine-tuned and the zero-shot performance. Furthermore, we investigate into the efficacy of fine-tuning a general pre-trained model that was fine-tuned using other retrieval datasets (such as BERT fine-tuned on MSMARCO), comparing it against a legally pre-trained model (such as LegalBERT) that remains untouched by other retrieval datasets except ours. This investigation revealed that legal pre-training helps to handle distribution shift of the corpus, but still lacks in handling the distribution shift towards unseen queries.\\n\\nWhile complete fine-tuning has shown better performance, the trend towards larger models with billions or trillions of trainable parameters makes this fine-tuning process resource-intensive and costly. This spurred the exploration of Parameter Efficient Fine-Tuning (PEFT) strategies which update only a small number of extra parameters while keeping the original pre-trained model parameters frozen. In our study, we delve into this emerging area by evaluating representative methods of PEFT, namely Adapter (Houlsby et al., 2019), prefix-tuning (Li and Liang, 2021) and LoRA (Hu et al., 2021), within the context of our paragraph retrieval dataset. This investigation contributes to the ongoing discourse regarding the practicality of adopting PEFT in the realm of Information Retrieval (Pal et al., 2023; Tam et al., 2022; Ma et al., 2022; Jung et al., 2022).\\n\\nOur experiments demonstrate that PEFT methods achieve comparable performance to full fine-tuning on both seen and unseen queries, with the choice of the best PEFT method contingent on configuration such as general vs. legal pre-training and bi-vs. cross-encoder settings.\\n\\n2. Related Work\\n\\nLegal IR\\n\\nRetrieving essential legal information is integral to the workflow of lawyers, encompassing tasks such as searching for legislation (adhoc search or by providing a factual description to identify the relevant statutes (Wang et al., 2018; Paul et al., 2022)), similar prior cases (Rabelo et al., 2022; Mandal et al., 2017), civil codes (Kim et al., 2016, 2014), litigation documents such as technology-assisted-review (Cormack et al., 2010), patents (Piroi et al., 2013) and within law firm\u2019s internal support system (Moens, 2001). Our work focuses specifically on legal case retrieval. Most of the existing legal case law retrieval works primarily aim to retrieve entire cases (Sansone and Sperl\u00ed, 2022) based on different query granularities, including whole cases (Rabelo et al., 2022; Ma et al., 2021; Mandal et al., 2017) or specific legal queries (Locke et al., 2017; Locke and Zuccon, 2018; Koniaris et al., 2016). In contrast, our approach involves retrieving relevant paragraphs at a finer granularity, providing practitioners with a more targeted means of identifying essential information. At the paragraph granularity level, the legal case entailment task in COLIEE involves identifying a paragraph from existing cases that matches the decision of a new case (Rabelo et al., 2022), but it employs the entire case as the query, in contrast to the short queries used in our work. This paragraph-level retrieval functionality is integral to building legal Question Answering (Khazaeli et al., 2021; Verma et al., 2020) and Query-focused summarization systems.\\n\\nTasks on ECtHR Corpora\\n\\nPrevious works involving ECtHR corpus has dealt with judgement prediction (Aletras et al., 2016; Chalkidis et al., 2019, 2021; Santosh et al., 2022, 2023; Tyss et al., 2023; Xu et al., 2023b), argument mining (Mochales and Moens, 2008; Habernal et al., 2023; Poudyal et al., 2019, 2020), vulnerability detection (Xu et al., 2023a), event extraction (Filtz et al., 2020; Navas-Loro and Rodriguez-Doncel, 2022). In this work, we capitalize on the case law guides maintained by registry of ECtHR to derive a query-driven relevant paragraph extraction dataset. We offer this dataset to the research community to facilitate advancements in area of AI-enabled tools for legal practitioners.\\n\\nParameter Efficient Retrieval\\n\\nWith sizes of pre-trained language models soaring up (Brown et al., 2020), full-parameter fine-tuning has become more challenging, this has created an interest in PEFT methods such as prompt tuning (Li and Liang, 2021; Lester et al., 2021; Liu et al., 2022), adapters (Houlsby et al., 2019; Pfeiffer et al., 2021; Mahabadi et al., 2021), additive methods (Hu et al., 2021; Guo et al., 2021; Zhang et al., 2020) and hybrid methods (Mao et al., 2022; Chen et al., 2022). Specifically in IR, Ma et al. 2022 conducted a comprehensive study of several PEFT methods for both the retrieval and re-ranking stages. Jung et al. 2022 has explored prefix-tuning and LoRA on bi-encoder models. Tam et al. 2022 examined the effect of these methods on in-domain, cross-domain and cross-topic retrieval. Pal et al. 2023 studied the effect of adapters on sparse retrieval models contrary to dense models. We contribute to this ongoing discourse using both bi- and cross-encoders using our paragraph retrieval dataset on legal judgements.\"}"}
{"id": "lrec-2024-main-1177", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Task & Dataset\\n\\nOur task of query-driven relevant paragraph extraction from legal judgements is defined as follows: Given a query $Q$ and a judgement document $J$ composed of $n$ paragraphs $P_J = \\\\{p_1, p_2, \\\\ldots, p_n\\\\}$, the objective is to identify the subset of paragraphs $P_J^+ \\\\subseteq P_J$ which are relevant to the query.\\n\\n3.1. Dataset Creation\\n\\nJudgements Collection\\n\\nWe acquire ECtHR judgements collection as an HTML data dump from HUDOC, the publicly available database of the ECtHR, along with their associated metadata. We retain only the English documents based on their metadata (Document Type: 'HEJUD'). The parsing of judgment into paragraphs posed challenges due to inconsistent HTML structure, the presence of sub-paragraph numbers within each paragraph and the occurrence of spurious paragraph numbers resulting from verbatim text copied from other documents to cross-reference those paragraphs.\\n\\nTo address these issues, we devised a range of hand-crafted heuristics to segment the judgment documents into paragraphs. Each paragraph is uniquely identified by its paragraph number at the beginning, facilitating cross-referencing.\\n\\nQueries and Paragraph Relevance Collection\\n\\nWe curate our query-paragraph relevance dataset using case-law guides accessible on ECtHR Knowledge Sharing Platform. This platform, maintained by the court's registry, analyzes case law development for each convention article (e.g., Article 4 - Prohibition of slavery and forced labor) and transversal themes (e.g., Data Protection, Rights of LGBTI persons). It comprises 28 article and 8 theme-related case law guides, updated weekly, making them up-to-date with evolving case law with every new judgement and our proposed task can in turn assist registry in achieving this goal of updating these guides regularly.\\n\\nObtaining queries\\n\\nThe case law guides provide the details of the key concepts involved under each article/theme and discuss them in detail by providing references to the relevant judgements. The legal concepts involved under each article/theme are structured in a hierarchical fashion, with sub-concepts enumerated. A representative index structure of a case law guide is illustrated in Figure 1. For instance, this is a hierarchical path of sections within the theme guide of Rights of LGBTI persons \u2192 Freedom of expression and association \u2192 Imposed silence and legal bans concerning homosexuality. We can extract this hierarchical structure by parsing the PDF case law guides' structural information. To construct each query, we combine these multiple concepts along the path (from the article or metatitle to the leaf node in the PDF structure) by using a delimiter. This approach generates queries that mirror lists of legal concepts, akin to those sought after by legal practitioners when searching in ECtHR judgments. These queries/legal concepts could be used to index legal analytics databases that inform litigation strategies.\\n\\nFigure 1: Query construction process from case law guide. The above table of contents is obtained from 'Rights of LGBTI persons' guide.\\n\\nFigure 2: Illustration of pin-pointed paragraph relevance in case law guides.\\n\\nObtaining relevant paragraphs in Judgements\\n\\nThese case-law guides provide in-depth discussions of each legal concept, offering pin-pointed paragraph references to the judgements from the ECtHR. An example of a legal concept description from a case-law guide is depicted in Fig. 2, demonstrating how relevant paragraphs are referenced under each query. We gather all paragraph references in a specific judgement under each legal concept.\"}"}
{"id": "lrec-2024-main-1177", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cept and mark all of them relevant corresponding to the given query in that judgement. However, it\u2019s worth mentioning that all judgements are not exhaustively covered in the case-law guide unless they contribute to the expansion or contraction of existing case law. Taking this into account, we pair queries with specific judgements referenced within them, subsequently extracting relevant paragraphs from these judgements. This contrasts with using all the paragraphs from all the judgements as the candidate set for identifying relevance. While our proposed methodology could theoretically be applied to all judgements across the corpus, we opt to restrict each query to the judgements specifically referenced under it. This deliberate limitation aims to ensure a high-quality evaluation setup, controlling false negatives.\\n\\nWe filter out those query-judgement pairs in which reference to judgement is missing paragraph-level reference. Finally, we map back judgements in query-judgement pairs to our judgements collection, removing the ones which we could not map back as some may refer to non-English documents which have not considered in our collection.\\n\\n3.2. Data Splits & Analysis\\nWe eventually end up with 4109 query-judgement pairs with 708 unique queries. The number of paragraphs in judgement range from 21 to 942 with a mean of 102.78 (Fig. 3a). The percentage of relevant paragraphs in each query-judgement pair range from 0.10% to 15% to the total number of paragraphs in that judgement with a mean around 1.95%, depicted in Fig. 3b. The queries and paragraph have a mean length of 36 and 135 tokens, illustrated in Figures 3c and 3d respectively.\\n\\nWe partition the article/theme case law guides into two distinct splits: one exclusively designated for testing with 403 query-judgment pairs (111 unique queries) derived from these case law guides, referred to as \u2018Unseen article/themes\u2019. This creates a rigorous unseen evaluation scenario, assessing the model\u2019s performance on unfamiliar legal concepts from themes and articles that were not encountered during training. Queries originating from the other split are further divided into two subsets, resulting in \u2018Seen article/theme, Unseen Query\u2019 with 694 pairs (120 unique queries) and \u2018Seen article/theme, Seen Query\u2019 with 3012 pairs (477 unique queries). The former, reserved for testing, exposes the model to previously encountered themes/articles, but with new queries. The latter group is further divided into training (2230 pairs), validation (302 pairs), and test (480 pairs) sets. The test set within the \u2018Seen article/theme, Seen Query\u2019 category assesses the model\u2019s comprehension of familiar legal concepts on new judgments in the test set.\\n\\n4. Retrieval Models\\nWe benchmark our task of identifying relevant paragraphs from a legal judgement given a query using the following models. We compute relevance score for each paragraph in a given judgement with respect to the query and obtain the top-k most relevant paragraphs with the highest scores.\\n\\n**BM25** (Robertson et al., 1995) is a bag-of-words approach that estimates paragraph relevance to a query by considering the presence of query terms in the paragraph.\\n\\n**Bi-encoders** employ separate encoders to encode queries and paragraphs into low-dimensional representations independently, leveraging neural architectures to capture semantic relationship and the final relevance score is computed using dot-product between the representations of query and paragraph obtained from encoder as:\\n\\n$$rel(q, p) = E_q(q) \\\\cdot E_p(p)$$\\n\\nwhere $E_q$ and $E_p$ represent query and paragraph encoder respectively. The training objective is to learn representations such that relevant pairs of query and paragraphs will have higher similarity than the irrelevant ones. To reduce the training cost given there are lot of irrelevant paragraphs, negative sampling has been employed. Let $\\\\{<q_i, p^+_i, p^-_{i,1}, \\\\ldots, p^-_{i,n}>\\\\}_m$ be the training data that consists of $m$ instances with each instance consisting of one query $q_i$ and one relevant passage $p^+_i$, along with $n$ irrelevant (negative) passages $p^-_{i,j}$. Note these negative paragraphs for a query are sampled from the same document as positive. We optimize negative log likelihood loss function as:\\n\\n$$L = -\\\\log (\\\\frac{\\\\exp(rel(q_i, p^+_i))}{\\\\exp(rel(q, p^+_i))} + \\\\sum_{j=1}^{n} \\\\exp(rel(q, p^-_{i,j})))$$\\n\\nFollowing Karpukhin et al. 2020, we consider negatives chosen from the irrelevant paragraphs randomly and the top paragraphs returned by BM25 which are not relevant to the query. We refer this approach as Dense Passage Retrieval (DPR).\\n\\nRecently, Xiong et al. 2020 proposed Approximate nearest neighbor Negative Contrastive Learning (ANCE) mechanism for denser retrieval. Instead of random or static BM25 negatives, ANCE constructs negatives using the being-optimized dense retrieval model. This helps to align the distribution of negative samples based on the models\u2019 training dynamics. While the model undergoes updates with each iteration, it would be expensive to update the negatives for every batch based on the updated model. Hence we asynchronously refresh the negatives at every checkpoint to reduce the computational cost to construct them.\\n\\nThese above methods follow a single-vector paradigm where each query and each paragraph\"}"}
{"id": "lrec-2024-main-1177", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Data Analysis is encoded into a single high-dimensional vector which is used to calculate relevance using a dot product. Khattab and Zaharia 2020 proposed a late interaction method named contextualized late interaction over BERT (ColBERT) where queries and documents are encoded at a finer granularity into multi-vector representations and relevance is estimated using interactions between these two sets of vectors. ColBERT produces an embedding for every token in the query and the paragraph and computes relevance as the sum of maximum similarities between each query vector and all vectors in the document as $rel(q, p) = \\\\sum_{i=1}^{N} \\\\max_{j=1}^{M} Q_i.D_j$ where $Q$ is a query encoding matrix corresponding to $N$ token vectors and $D$ denotes the paragraph encoding matrix corresponding to $M$ token vectors.\\n\\nCross-encoders concatenate both of them before being provided to the model instead of encoding query and paragraph separately. The relevance score is directly computed by feed-forward network using the combined representation of the both (Yates et al., 2021) as $rel(q, p) = f(E_{\\\\phi}(q, p))$ where $E_{\\\\phi}$ represents a pre-trained model such as BERT and $f$ denotes a feed-forward network which takes [CLS] representation as input to compute relevance score and is trained end-to-end with binary cross entropy loss. This allows for deeper interaction between the query and paragraph but this effectiveness comes with a cost on efficiency as it now involves whole pass through the model for each query-paragraph pair, instead of being able to pre-compute all the paragraph representations and use the model once to obtain query representation to calculate the relevance score as in bi-encoders.\\n\\n5. Zero-shot & Fine-tune Experiments\\n\\nInitially, we investigate the performance of retrieval models in a zero-shot evaluation scenario, where models trained on the MS MARCO paragraph ranking dataset (Bajaj et al., 2016) - a large-scale adhoc retrieval dataset derived from the Bing search log containing 8.8 million passages and around 800K queries for training, are directly evaluated on our legal judgement paragraph ranking dataset. We examine the following models: (i) DPR (ii) ANCE (iii) ColBERT (iv) Cross encoder. We also evaluate a legal-domain-specific encoder model, Legal-BERT (Chalkidis et al., 2020) which is pre-trained on diverse English legal texts encompassing legislative content, court cases, and contracts using cosine similarity between obtained [CLS] embeddings as relevance score. Notably, LegalBERT has been exposed to case law from ECtHR.\\n\\nSubsequently, we fine-tune these models on the training split of our legal judgment paragraph extraction dataset. We create two variants of each model, with distinct initializations: (i) model already fine-tuned on MSMARCO (models used in the zero-shot evaluation) and (ii) LegalBERT.\\n\\nImplementation Details\\n\\nFor DPR, we use mix of negatives from BM25 and random in ratio of 4:1 and train with total of 5 negatives per query-positive pair. For ANCE, we use same number of negatives derived from model. While for COLBERT and cross encoders, we use seven negatives samples for every positive query, where 4 are sampled randomly and 3 are from BM25 negatives. We sweep over learning rates $\\\\{1e^{-5}, 3e^{-5}, 5e^{-5}, 1e^{-4}, 3e^{-4}\\\\}$ and the model is trained end-to-end for 5 epochs with Adam optimizer (Kingma and Ba, 2014) and we select the best model based on the performance on the validation set.\\n\\nMetrics\\n\\nWe evaluate the performance using Recall@k% (R@K%). Recall@k% measures the proportion of relevant paragraphs in the top-k% of the total paragraphs in the judgement and we report mean across all instances. We report for $k = \\\\{2, 5, 10\\\\}$. We use the k as percentage instead of absolute value to account for varying number of\"}"}
{"id": "lrec-2024-main-1177", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"paragraphs across different judgements. Higher recall scores indicate better performance.\\n\\n5.1. Results\\n\\nWe report the results of both the zero-shot and the fine-tuning experiments in Table 1.\\n\\nZero-shot: We observe neural models demonstrate better performance across all the splits compared to BM25, bridging the lexical gap issue. ANCE displays slightly better performance than DPR demonstrating effectiveness of its dynamic negative sampling. COLBERT demonstrates superior performance across all variants, with a larger margin. This can be owed to its multi-vector representations at the granularity of each token and its training with distillation loss from re-ranker models. We notice cross encoder are comparable to other dense models except to COLBERT, due to its ability to act better in re-ranking stage rather than retrieval stage. The performance order of these models is consistent with the out-of-domain zero-shot results on BEIR leaderboard (Thakur et al., 2021). Surprisingly, LegalBERT performs comparably similar or less than BM25 and significantly below retrieval fine-tuned general models, contrary to what one might expect that legal pre-training would mitigate distribution shift on the corpus side to capture relevance. This points out general masked language model objective can not effectively translate to capture relevance in retrieval settings and calls for investigation of pre-training objectives suitable for retrieval such as inverse cloze task (Lee et al., 2019), masking salient spans (Singh et al., 2021) to handle phrase level query matching and contrastive based pre-training (Izacard et al., 2022).\\n\\nZero-shot vs Fine-tune: All the fine-tuning models (both MSMARCO and LegalBERT initialized ones) substantially improve over zero-shot variants in all the three splits. This difference highlights the need for future research to improve the generalization ability of current IR models to domains without any relevance label by handling distribution shifts from both the query and corpus side.\\n\\nFine-tune: Despite COLBERT demonstrating a better zero-shot performance, cross encoders performed better with fine-tuning due to their deep interactions through concatenations, but that comes at a cost of efficiency to compute joint representation. Among bi-encoders, COLBERT perform well compared to ANCE followed by DPR due to its late interaction using multiple vector representations. The difference between them gets closer with fine-tuning on the 'Seen Article, Seen Query' split, adapting the model to those specific queries. Across the other splits, we notice fine-tuning in general brings improvement over the zero-shot. However, the difference of improvement decreases with 'Seen Article, Unseen Query' setting which further decreases with 'Unseen article' setting. This highlights the need of effective strategies for domain adaptation with minimal labeled domain data without getting overfitted to those specific seen queries and handle distribution shift on query side.\\n\\nMSMARCO vs Legal\\n\\nAcross all the four models, we observe LegalBERT initialization outperforms MSMARCO variant, despite the opposite trend in zero-shot performance. This is more noticeable in unseensplits, where the legal pre-training helps the model in grasping context from the under specified queries compared to general pre-trained model with exposure to general factual-based QA instances. To unveil this capability of LegalBERT in zero-shot setup, it is crucial to design a pre-training objectives closely related to the retrieval task, as discussed before, to address the task shift.\\n\\nThis meticulous design of three different splits, coupled with these results highlight that this dataset can serve as a testbed to study how to adapt these IR models to the distribution shifts between the source training task (such as MS MARCO) and the target tasks (such as ours) in zero-shot setup and also with minimal labeled data with some specific queries.\\n\\n6. Parameter-Efficient Retrieval\\n\\nPEFT aims to tune only a small portion of parameters rather than the full parameters as in traditional fine-tuning. PEFT approaches fall into three primary categories: Parameter Composition, Input Composition, and Function Composition (Ruder et al., 2022). Given a neural network $f_{\\\\theta}: X \\\\rightarrow Y$, it is decomposed into a sequence of functions $f_{\\\\theta} = f_{\\\\theta_1} \\\\circ f_{\\\\theta_2} \\\\circ \\\\ldots \\\\circ f_{\\\\theta_l}$, where $\\\\theta_1, \\\\theta_2, \\\\ldots, \\\\theta_l$ represent parameters which are held constant in PEFT and a module with parameters $\\\\phi$ is introduced, which are updated during training to modify the $i^{th}$ sub-function as follows: Parameter composition involves interpolating models' parameter with new parameters as $f'_i(x) = f_{\\\\theta_i}(x) \\\\circ \\\\phi(x)$. Input Composition augments a model's input with a learnable parameter vector as $f'_i(x) = f_{\\\\theta_i}([x, \\\\phi])$. Function composition augments a model's functions with new task-specific functions as $f'_i(x) = f_{\\\\theta_i} \\\\circ f_{\\\\phi_i}(x)$.\\n\\nWe pick one representative method from each category and study their performance on our retrieval task.\"}"}
{"id": "lrec-2024-main-1177", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Results of various systems on our Query-driven Paragraph retrieval task. For zero-shot settings, all these splits are unseen, as they are not fine-tuned on any task related data.\\n\\nAdapters (Houlsby et al., 2019) fall under the category of function composition where we inject two small modules between the self-attention sub-layer and the feed forward sub-layer inside each layer of transformer sequentially. The adapter module consists of a down-projection, an up-projection and a nonlinear function between them with a residual connection across each module.\\n\\nAdapter \\\\((h) = h + W_{up} \\\\psi(W_{down}h) \\\\quad (2)\\\\)\\n\\nwhere \\\\(W_{down} \\\\in \\\\mathbb{R}^{D_{hidden} \\\\times D_{mid}}\\\\) and \\\\(W_{up} \\\\in \\\\mathbb{R}^{D_{mid} \\\\times D_{hidden}}\\\\), \\\\(D_{mid}\\\\) denote the bottleneck dimension and \\\\(\\\\psi\\\\) is a nonlinear RELU activation function.\\n\\nPrefix-Tuning (Li and Liang, 2021) falls under the category of input-composition where we prepend a fixed number of trainable vectors to the input of multi-head attention in each Transformer layer, which the original tokens can attend to as if they were virtual tokens. Specifically two prefix matrices \\\\(P_K \\\\in \\\\mathbb{R}^{L \\\\times D_{hidden}}\\\\) and \\\\(P_V \\\\in \\\\mathbb{R}^{L \\\\times D_{hidden}}\\\\) are prepended to \\\\(K\\\\) and \\\\(V\\\\) where \\\\(L\\\\) denotes prefix length.\\n\\n\\\\(h = \\\\text{Attention}(Q, [P_k, k], [P_v, v]) \\\\quad (3)\\\\)\\n\\nLoRA (Hu et al., 2021) Low-Rank Adaptation falls under the category of parameter-composition, introduces trainable low-rank matrices and combines them with the original matrices in the multi-head attention. Specifically, it learns two low-rank matrices \\\\(W_{down} \\\\in \\\\mathbb{R}^{D_{hidden} \\\\times D_{mid}}\\\\) and \\\\(W_{up} \\\\in \\\\mathbb{R}^{D_{mid} \\\\times D_{hidden}}\\\\) for each of the query and value projections along with their original matrix \\\\(W_Q \\\\in \\\\mathbb{R}^{D_{hidden} \\\\times D_{hidden}}\\\\) and \\\\(W_V \\\\in \\\\mathbb{R}^{D_{hidden} \\\\times D_{hidden}}\\\\). Taking \\\\(W_Q\\\\) as example:\\n\\n\\\\(Q = (W_{Q}^{\\\\top} + \\\\alpha W_{up} W_{down})h \\\\quad (4)\\\\)\\n\\nwhere \\\\(\\\\alpha\\\\) is a tunable hyper-parameter. Once after the training is complete, we can sum up these additional LoRA weights to the original weights, thus making the inference overhead to zero.\\n\\n7. PEFT Experiments\\n\\nWe investigate the effect of PEFT by applying each method separately on bi-encoder and cross-encoder, using MSMARCO and LegalBERT initializations. Among bi-encoders, we choose COLBERT due to its better performance in full fine-training. We report Recall@k\\\\% for \\\\(k = \\\\{2, 5, 10\\\\}\\\\) in Table 2.\\n\\nImplementation Details\\n\\nWe use the AdapterHub library\\\\(^{13}\\\\) for implementing PEFT methods. For Prefix-tuning, we use prefix lengths of 10, 15 and 30. For Bottleneck adapters, we used reduction factors of 8, 16, and 32. In case of LoRA, we use configuration of rank and alpha in \\\\{8, 16\\\\}. We sweep over learning rates \\\\{1e^{-5}, 3e^{-5}, 5e^{-5}, 1e^{-4}, 3e^{-4}\\\\} select the best model based on the performance on the validation set. We train the model for 15 epochs with Adam optimizer (Kingma and Ba, 2014).\\n\\n7.1. Results\\n\\nCrossEncoder (MSMARCO): We observe all the PEFT methods under perform than full fine-tuning across all the splits. Among them, LORA underperforms consistently across all the splits, while prefix tuning is better among them. However, Adapter takes the lead in 'unseen article' split and this can be attributed to better generalization capability derived through adding new functional composition rather than additional input tokens in case of prefix tuning.\"}"}
{"id": "lrec-2024-main-1177", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method         | Full Fine-Tuning | 2% | 5% | 10% |\\n|----------------|------------------|----|----|-----|\\n| Cross Encoder  |                  | 0.25| 0.45| 0.63|\\n| Adapter        |                  | 0.28| 0.47| 0.67|\\n| Pre. Tun.      |                  | 0.30| 0.51| 0.68|\\n| LORA           |                  | 0.26| 0.46| 0.63|\\n| COLBERT (MSMARCO) |                | 0.25| 0.46| 0.64|\\n| Adapter        |                  | 0.24| 0.43| 0.62|\\n| Pre. Tun.      |                  | 0.21| 0.40| 0.59|\\n| LORA           |                  | 0.24| 0.43| 0.62|\\n| COLBERT (Legal) |                  | 0.25| 0.48| 0.67|\\n| Adapter        |                  | 0.24| 0.48| 0.68|\\n| Pre. Tun.      |                  | 0.21| 0.40| 0.57|\\n| LORA           |                  | 0.24| 0.46| 0.63|\\n\\nTable 2: Comparison between full fine-tuning and various parameter-efficient tuning methods.\\n\\nCrossEncoder (Legal): We observe all the PEFT methods comparable to each other across all the splits, which can be attributed to domain-specific legal knowledge from base model. On the 'seen query' split, they even surpass the full fine-tuning, demonstrating that with fine-tuning $\\\\sim 1\\\\%$ of the original model parameters, they can achieve comparable performance to the full fine-tuning baseline, making them to adopt easily in low-compute settings. However, these methods fall back on generalizability, compared to full-tuning, opening up potential directions to tackle in future, how to augment these PEFT methods to handle these distribution shifts to perform effectively on unseen settings.\\n\\nCOLBERT (MSMARCO): PEFT methods underperform compared to full fine-tuning. Among them, Prefix Tuning turns out to be lowest performer and rest of them are comparable to each other, across all the splits consistently. This can be attributed to the short queries in our case. COLBERT (bi-encoder) models encode queries and paragraphs separately, and for shorter queries, they struggle to extract meaningful contextual information using BERT alone. Prefix Tuning, in particular, fails to enhance this contextual information just by adding additional parameters in the input compared to others which can handle the representation embeddings through function or parameter composition.\\n\\nCOLBERT (Legal): We observe similar to the COLBERT (MSMARCO), prefix tuning underperforming compared to rest of the methods.\\n\\nCross encoder vs COLBERT: While prefix tuning turned out to be a better PEFT method in cross encoder setting (especially in MSMARCO), it turned out to be lowest in bi-encoder, COLBERT, encouraging further studies to develop model agnostic PEFT methods and analyze the interplay between architecture and the PEFT method.\\n\\nMSMARCO vs Legal: Overall, legal pre-training helped to account for distribution shift for corpus, demonstrating better results. This coupled with cross-encoder deep interactions, demonstrated parameter efficiency when fine-tuning. Moreover, legal oriented models witness only a small decline with sparse fine-tuning from full fine-tuning in comparison to MSMARCO variants.\\n\\nOverall, we empirically demonstrate that PEFT methods can achieve comparable performance to full-parameter fine-tuning not only in seen query setting but also in challenging unseen settings and motivate further work to bridge the existing gap between them, making them more adaptable in low data and compute resource settings.\\n\\n8. Conclusion\\n\\nWe present an empirical study focused on the task of extracting relevant paragraphs from legal judgments based on the query. We rigorously curate a dataset for this task from ECtHR jurisdiction, leveraging the case-law guides produced by the court's registry. We assess the current retrieval models on this task in a zero-shot way to emphasize the need of retrieval specific pre-training objectives.\"}"}
{"id": "lrec-2024-main-1177", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We further fine-tune several models encompassing bi- and cross-encoders for this task. We evaluate the generalizability of different fine-tuning models when faced with unseen concepts or queries to illustrate how legal pre-training can effectively address distribution shifts on the corpus side but still faces challenges in adapting to shift on the query side. In addition, we demonstrate the efficacy of different PEFT methods on these retrieval methods shedding light on their intricate effects concerning legal pre-training, bi-encoder, and cross-encoder models. Our findings reveal that there is no one-size-fits-all PEFT method that performs well across all settings. We hope that both our dataset and the fine-tuned models will be useful to the research community working in the space of legal information retrieval.\\n\\n9. Limitations\\n\\nIn this study, we treat each paragraph as an independent unit during the training of neural models. However, it's important to note that paragraphs are not entirely independent; they often constitute small excerpts from longer documents, and their content may not always provide a comprehensive estimate of their relevance. To be more precise, some paragraphs draw context not just from other paragraphs within the same document but also from other documents, as evident through citations and cross-references to other judgments or texts within each paragraph. In the future, harnessing this inter-paragraph and cross-document contextual signal could lead to a more enriched understanding of each paragraph's relevance.\\n\\nFurthermore, this practice of segmenting documents into smaller chunks is common in retrieval tasks, where documents are broken down into shorter lengths for indexing in retrieval systems. This may lead to losing some of their context in the process. It's worth noting that this chunking effect could be more pronounced in our task compared to more fact-focused general retrieval tasks. As a result, future research should explore methods to effectively capture contextual information from the sequential nature of paragraphs within documents and develop discourse-aware representations.\\n\\nA specific challenge with respect to PEFT methods are that they converge slower and relatively more sensitive to hyper-parameters such as learning rate than full fine-tuning. We do observe same characteristics during our work and have to bypass the problem by training for more epochs and experimenting with different hyper-parameters. It is thus imperative to analyse them more theoretically and design more robust and stable training strategies for PEFT methods in the future.\\n\\nAdditionally, while our findings of the relevant paragraph retrieval experiments are specific to the ECtHR domain and datasets, comparable experiments in other domains will see variation based on the nature of the legal concepts and the legal documents. Nevertheless, all derivation of insights from legal case data comes with jurisdiction-related limitations.\\n\\n10. Ethics Statement\\n\\nWith the release of our data as a public resource, we do not foresee any ethical concerns in a repurposed and bundled release of this dataset as both the judgements data and the caselaw guides are already available through the HUDOC and ECHR KS platform respectively and it complies with the ECtHR data policy. These decisions, although not anonymized, include the real names of individuals involved. However, our work does not engage with the data in a way that we consider harmful beyond this availability. We believe that this work can foster further research in this data scarce legal NLP field, to build assistive technology for legal professionals. We are conscious by employing pre-trained language models, we inherit the biases they may have acquired from their training corpus and need to be further scrutinized of any biases that may arise and is crucial to ensure that the systems developed are fair.\\n\\n11. Bibliographical References\\n\\nNikolaos Aletras, Dimitrios Tsarapatsanis, Daniel Preo\u0163iuc-Pietro, and Vasileios Lampos. 2016. Predicting judicial decisions of the european court of human rights: A natural language processing perspective. PeerJ computer science, 2:e93.\\n\\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, TriNguyen, et al. 2016. Msmarco: Ahumangen-erated machine reading comprehension dataset. arXiv e-prints, pages arXiv\u20131611.\\n\\nPaheli Bhattacharya, Kaustubh Hiware, Subham Rajgaria, Nilay Pochhi, Kripabandhu Ghosh, and Saptarshi Ghosh. 2019. A comparative study\"}"}
{"id": "lrec-2024-main-1177", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ivan Habernal, Daniel Faber, Nicola Recchia, Sebastian Bretthauer, Iryna Gurevych, Indra Spiecker genannt D\u00f6hmann, and Christoph Burghard. 2023. Mining legal arguments in court decisions. *Artificial Intelligence and Law*, pages 1\u201338.\"}"}
{"id": "lrec-2024-main-1177", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mi-Young Kim, Ying Xu, Yao Lu, and Randy Goebel. 2016. Legal question answering using paraphrasing and entailment analysis. In Tenth International Workshop on Juris-informatics (JURISIN).\\n\\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\\n\\nMarios Koniaris, Ioannis Anagnostopoulos, and Yannis Vassiliou. 2016. Multi-dimension diversification in legal information retrieval. In Web Information Systems Engineering\u2013WISE 2016: 17th International Conference, Shanghai, China, November 8-10, 2016, Proceedings, Part I, pages 174\u2013189. Springer.\\n\\nSteven A. Lastres. 2015. Rebooting legal research in a digital age.\\n\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096.\\n\\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059.\\n\\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597.\\n\\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 61\u201368.\\n\\nDaniel Locke and Guido Zuccon. 2018. A test collection for evaluating legal case law search. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 1261\u20131264.\\n\\nDaniel Locke, Guido Zuccon, and Harrisen Scells. 2017. Automatic query generation from legal texts for case law retrieval. In Information Retrieval Technology: 13th Asia Information Retrieval Societies Conference, AIRS 2017, Jeju Island, South Korea, November 22-24, 2017, Proceedings, pages 181\u2013193. Springer.\\n\\nXinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, and Xueqi Cheng. 2022. Scattered or connected? an optimized parameter-efficient tuning approach for information retrieval. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pages 1471\u20131480.\\n\\nYixiao Ma, Yunqiu Shao, Yueyue Wu, Yiqun Liu, Ruizhe Zhang, Min Zhang, and Shaoping Ma. 2021. Lecard: a legal case retrieval dataset for Chinese law system. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2342\u20132348.\\n\\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. 2021. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 565\u2013576.\\n\\nArpan Mandal, Kripabandhu Ghosh, Arnab Bhabatcharya, Arindam Pal, and Saptarshi Ghosh. 2017. Overview of the fire 2017 irled track: Information retrieval from legal documents. In FIRE (Working Notes), pages 63\u201368.\\n\\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. 2022. Unipelt: A unified framework for parameter-efficient language model tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6253\u20136264.\\n\\nRaquel Mochales and Marie-Francine Moens. 2008. Study on the structure of argumentation in case law. In Proceedings of the 2008 conference on legal knowledge and information systems, pages 11\u201320.\\n\\nMarie-Francine Moens. 2001. Innovative techniques for legal text retrieval. Artificial Intelligence and Law, 9:29\u201357.\\n\\nMar\u0131a Navas-Loro and V\u0131ctor Rodriguez-Doncel. 2022. Whenthefact: Extracting events from European legal decisions. In Legal Knowledge and Information Systems, pages 219\u2013224. IOS Press.\\n\\nVaishali Pal, Carlos Lassance, Herv\u00e9 D\u00e9jean, and St\u00e9phane Clinchant. 2023. Parameter-efficient sparse retrievers and rerankers using adapters. In European Conference on Information Retrieval, pages 16\u201331. Springer.\"}"}
{"id": "lrec-2024-main-1177", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shounak Paul, Pawan Goyal, and Saptarshi Ghosh. 2022. Lesicin: A heterogeneous graph-based approach for automatic legal statute identification from Indian legal documents. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 11139\u201311146.\\n\\nJonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. 2021. Adapterfusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 487\u2013503.\\n\\nFlorina Piroi, Mihai Lupu, and Allan Hanbury. 2013. Overview of clef-ip2013 lab: Information retrieval in the patent domain. In Information Access Evaluation. Multilinguality, Multimodality, and Visualization: 4th International Conference of the CLEF Initiative, CLEF 2013, Valencia, Spain, September 23-26, 2013. Proceedings 4, pages 232\u2013249. Springer.\\n\\nJoshua Poje. 2014. Legal research. American Bar Association Techreport, 2014.\\n\\nPrakash Poudyal, Teresa Gon\u00e7alves, and Paulo Quaresma. 2019. Using clustering techniques to identify arguments in legal documents. ASAIL@ICAIL, 2385.\\n\\nPrakash Poudyal, Jarom\u00edr \u0160avelka, Aagje Ieven, Marie Francine Moens, Teresa Goncalves, and Paulo Quaresma. 2020. Echr: Legal corpus for argument mining. In Proceedings of the 7th Workshop on Argument Mining, pages 67\u201375.\\n\\nJuliano Rabelo, Randy Goebel, Mi-Young Kim, Yoshinobu Kano, Masaharu Yoshioka, and Ken Satoh. 2022. Overview and discussion of the competition on legal information extraction/entailment (coliee) 2021. The Review of Socionetwork Strategies, 16(1):111\u2013133.\\n\\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at trec-3. Nist Special Publication Sp, 109:109.\\n\\nSebastian Ruder, Jonas Pfeiffer, and Ivan Vuli\u0107. 2022. Modular and parameter-efficient fine-tuning for nlp models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 23\u201329.\\n\\nCarlo Sansone and Giancarlo Sperl\u00ed. 2022. Legal information retrieval systems: State-of-the-art and open issues. Information Systems, 106:101967.\\n\\nTyss Santosh, Oana Ichim, and Matthias Grabmair. 2023. Zero-shot transfer of article-aware legal outcome classification for European Court of Human Rights cases. In Findings of the Association for Computational Linguistics: EACL 2023, pages 593\u2013605.\\n\\nTyss Santosh, Shanshan Xu, Oana Ichim, and Matthias Grabmair. 2022. Deconfounding legal judgment prediction for European Court of Human Rights cases towards better alignment with experts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1120\u20131138.\\n\\nAbhay Shukla, Paheli Bhattacharya, Soham Poddar, Rajdeep Mukherjee, Kripabandhu Ghosh, Pawan Goyal, and Saptarshi Ghosh. 2022. Legal case document summarization: Extractive and abstractive methods and their evaluation. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, pages 1048\u20131064.\\n\\nDevendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. 2021. End-to-end training of multi-document reader and retriever for open-domain question answering. Advances in Neural Information Processing Systems, 34:25968\u201325981.\\n\\nWeng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Jiahua Liu, Maodi Hu, and Jie Tang. 2022. Parameter-efficient prompt tuning makes generalized and calibrated neural text retrievers. arXiv preprint arXiv:2207.07087.\\n\\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\\n\\nSantosh Tyss, Marcel Perez San Blas, Phillip Kemper, and Matthias Grabmair. 2023. Leveraging task dependency and contrastive learning for case outcome classification on European Court of Human Rights cases. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1103\u20131103.\\n\\nAayushi Verma, Jorge Morato, Arti Jain, and Anuja Arora. 2020. Relevant subsection retrieval for law domain question answer system. Data Visualization and Knowledge Engineering: Spotting Data Points with Artificial Intelligence, pages 299\u2013319.\"}"}
{"id": "lrec-2024-main-1177", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pengfei Wang, Ze Yang, Shuzi Niu, Yongfeng Zhang, Lei Zhang, and ShaoZhang Niu. 2018. Modeling dynamic pairwise attention for crime classification over legal articles. In the 41st international ACM SIGIR conference on research & development in information retrieval, pages 485\u2013494.\\n\\nLeeXiong, ChenyanXiong, YeLi, Kwok-Fung Tang, Jialin Liu, Paul N Bennett, Junaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In International Conference on Learning Representations.\\n\\nShanshan Xu, Leon Staufer, Santosh T.y.s.s, Oana Ichim, Corina Heri, and Matthias Grabmair. 2023a. VECHR: A dataset for explainable and robust classification of vulnerability type in the European court of human rights. In Proceedingsof the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11738\u201311752, Singapore. Association for Computational Linguistics.\\n\\nShanshan Xu, Santosh T.y.s.s, Oana Ichim, Isabella Risini, Barbara Plank, and Matthias Grabmair. 2023b. From dissonance to insights: Dissecting disagreements in rationale construction for case outcome classification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9558\u20139576, Singapore. Association for Computational Linguistics.\\n\\nAndrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained transformers for text ranking: Bert and beyond. In Proceedings of the 14th ACM International Conference on web search and data mining, pages 1154\u20131156.\\n\\nJeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. 2020. Side-tuning: a baseline for network adaptation via additive side networks. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pages 698\u2013714. Springer.\"}"}
