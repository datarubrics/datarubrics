{"id": "emnlp-2023-main-1005", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Coherence-Enhanced Machine-Generated Text Detection Under Low Resource With Contrastive Learning\\n\\nXiaoming Liu1,\u2020, Zhaohan Zhang1,2,\u2020, Yichen Wang1,\u2020, Hang Pu1, Yu Lan1, Chao Shen1\\n\\n1Faculty of Electronic and Information Engineering, Xi'an Jiaotong University\\nNo.28, Xianning West Road, Xi'an, China\\n2Queen Mary University of London, London, UK\\n\\n{xm.liu,ylan2020,chaoshen}@xjtu.edu.cn\\n{zzh1103,yichen.wang,hpu2022}@stu.xjtu.edu.cn\\n\\n\u2020Equal contribution,\\n\u2217Corresponding author\\n\\nAbstract\\nMachine-Generated Text (MGT) detection, a task that discriminates MGT from Human-Written Text (HWT), plays a crucial role in preventing misuse of text generative models, which excel in mimicking human writing style recently. The latest proposed detectors usually take coarse text sequences as input and fine-tune pre-trained models with standard cross-entropy loss. However, these methods fail to consider the linguistic structure of texts. Moreover, they lack the ability to handle the low-resource problem, which could often happen in practice considering the enormous amount of textual data online. In this paper, we present a coherence-based contrastive learning model named COCO to detect the possible MGT under the low-resource scenario. To exploit the linguistic feature, we encode coherence information in the form of graph into the text representation. To tackle the challenges of low data resources, we employ a contrastive learning framework and propose an improved contrastive loss for preventing performance degradation brought by simple samples. The experiment results on two public datasets and two self-constructed datasets prove our approach outperforms the state-of-the-art methods significantly. Also, we surprisingly find that MGTs originated from up-to-date language models could be easier to detect than these from previous models, in our experiments. And we propose some preliminary explanations for this counter-intuitive phenomena. All the codes and datasets are open-sourced.\\n\\n1 Introduction\\nThriving progress in the field of text generative models (TGMs) (Yang et al., 2019; Kenton and Toutanova, 2019; Liu et al., 2019; Keskar et al., 2019; Lewis et al., 2020; Brown et al., 2020; Gao et al., 2021a; Madotto et al., 2021; Ouyang et al., 2022; Touvron et al., 2023; Anil et al., 2023), e.g., ChatGPT2 and GPT-4 (OpenAI, 2023), enables everyone to produce MGTs massively and rapidly. However, the accessibility to high-quality TGMs is prone to cause misuses, such as fake news generation (Zellers et al., 2019; Yanagi et al., 2020; Huang et al., 2022), product review forging (Adelani et al., 2020), and spamming (Tan et al., 2012), etc. MGTs are hard to distinguish by an untrained human for their human-like writing style (Ippolito et al., 2020) and the excessive amount (Grinberg et al., 2019), which calls for the study of reliable automatic MGT detectors.\\n\\nPrevious works on MGTs detection mainly concentrate on sequence feature representation and classification (Gehrmann et al., 2019; Solaiman 2https://chat.openai.com, 2020). More recently, some researchers proposed to incorporate graph structures into TGMs, which can be more expressive in capturing the dependency of texts. However, most of the existing methods fail to consider the linguistic structure of texts. Moreover, they lack the ability to handle the low-resource problem, which could often happen in practice considering the enormous amount of textual data online. In this paper, we present a coherence-based contrastive learning model named COCO to detect the possible MGT under the low-resource scenario. To exploit the linguistic feature, we encode coherence information in the form of graph into the text representation. To tackle the challenges of low data resources, we employ a contrastive learning framework and propose an improved contrastive loss for preventing performance degradation brought by simple samples. The experiment results on two public datasets and two self-constructed datasets prove our approach outperforms the state-of-the-art methods significantly. Also, we surprisingly find that MGTs originated from up-to-date language models could be easier to detect than these from previous models, in our experiments. And we propose some preliminary explanations for this counter-intuitive phenomena. All the codes and datasets are open-sourced.\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al., 2019; Zellers et al., 2019; He et al., 2023; Mitchell et al., 2023). Recent studies have shown the good performance of automated detectors in a fine-tuning fashion (Solaiman et al., 2019; Mireshghallah et al., 2023). Although these fine-tuning-based detectors have demonstrated their effectiveness, they still suffer from two issues that limit their conversion to practical use: (1) Existing detectors treat input documents as flat sequences of tokens and use neural encoders or statistical features (e.g., TF-IDF, perplexity) to represent text as the dense vector for classification. These fine-tuning-based methods rely much on the token-level distribution difference of texts in each class, which ignores high-level linguistic representation of text structure. (2) Compared with the enormous number of online texts, the annotated dataset for training MGT detectors is rather low-resource. Constrained by the amount of available annotated data, traditional detectors sustain frustrating accuracy and even collapse during the test stage.\\n\\nThe defect in the coherence of LMs in generating long text has been revealed by previous works. Malkin et al. (2022) mentions that long-range semantic coherence remains challenging in language generation. Sun et al. (2020) also provides examples of incoherent MGTs. As shown in Fig. 1, MGTs and HWTs exhibit differences in terms of coherence traced by entity consistency. Accordingly, we propose that coherence could be an entry point for MGT detection via the perspective of high-level linguistic structure representation, where MGTs could be less interactive than HWTs. Specifically, we propose an entity coherence graph to model the sentence-level structure of texts based on the thoughts of Centering Theory (Grosz and Sidner, 1986), which evaluates text coherence by entity consistency. The entity coherence graph treats entities as nodes and builds edges between entities in the same sentences and the same entities among different sentences to reveal the text structure. Instead of treating text as a flat sequence, coherence modeling helps to introduce distinguishable linguistic features at the input stage and provides explainable differences between MGTs and HWTs.\\n\\nTo alleviate the low-resource problem in the second issue, inspired by the resurgence of contrastive learning (He et al., 2020; Chen et al., 2020), we utilize the proper design of sample pair and contrastive process to learn fine-grained instance-level features under low resource. However, it has been proven that the easiest negative samples are unnecessary and insufficient for model training in contrastive learning (Cai et al., 2020). To circumvent the performance degradation brought by the easy samples, we propose a novel contrastive loss with the capability to reweight the effect of negative samples by difficulty score to help the model concentrate more on hard samples and ignore the easy samples. Extensive experiments on multiple datasets (GROVER, GPT-2, GPT-3.5) demonstrate the effectiveness and robustness of our proposed method. Surprisingly, we find that the GPT-3.5 datasets are easier for all the detectors compared with datasets of smaller and older models (GPT-2 and GROVER) under our setting. We take a small step to exploring why the GPT-3.5 dataset is overly simple by probing statistical cues, including perspective from token spans and individual tokens.\\n\\nIn summary, our contributions are summarized as follows:\\n\\n\u2022 Coherence Graph Construction: We model the text coherence with entity consistency and sentence interaction while statistically proving its distinctiveness in MGT detection, and we further introduce the linguistic feature at the input stage.\\n\\n\u2022 Improved Contrastive Loss: We propose a novel contrastive loss in which hard negative samples are paid more attention to improve the detection accuracy of challenging samples.\\n\\n\u2022 Outstanding Performance: We achieve state-of-the-art performance on four MGT datasets in both low-resource and high-resource settings. Experimental results verify the effectiveness and robustness of our model.\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of COCO. Input document is parsed to construct a coherence graph (3.1), the text and graph are utilized by a supervised contrastive learning framework (3.2), in which coherence encoding module is designed to encode and aggregate to generate coherence-enhanced representation (3.2.3). After that, we employ a MoCo-based contrastive learning architecture in which key encodings are stored in a dynamic memory bank (3.2.4) with improved contrastive loss to make final prediction (3.2.5).\\n\\noutput of TGMs as negative samples to demonstrate the generalization ability. Deep learning models incorporating stylometry and external knowledge are also feasible for improving the performance of MGT detectors (Uchendu et al., 2019; Zhong et al., 2020). Our method differs from the previous work by analyzing and modeling text coherence as a distinguishable feature and emphasizing performance improvement under low-resource scenarios.\\n\\nCoherence Modeling. For generative models, coherence is the critical requirement and vital target (Hovy, 1988). Previous works mainly discuss two types of coherence, local coherence (Mellish et al., 1998; Althaus et al., 2004) and global coherence (Mann and Thompson, 1987). Local coherence focus on sentence-to-sentence transitions (Lapata, 2003), while global coherence tries to capture comprehensive structure (Karamanis and Manurung, 2002). Our method strives to represent both local and global coherence with inner- and inter-sentence relations between entity nodes.\\n\\nContrastive Learning. Contrastive learning in NLP demonstrates superb performance in learning token-level embeddings (Su et al., 2022) and sentence-level embeddings (Gao et al., 2021b) for natural language understanding. With an in-depth study of the mechanism of contrastive learning, the hardness of samples is proved to be crucial in the training stage. Cai et al. (2020) define the dot product between the queries and the negatives in normalized embedding space as hardness and figured out the easiest 95% negatives are insufficient and unnecessary. Song et al. (2022) propose a difficulty measure function based on the distance between classes and apply curriculum learning to the sampling stage. Differently, our method pays more attention to hard negative samples for improving the detection accuracy of challenging samples.\\n\\n3 Methodology\\n\\nThe workflow of COCO mainly contains coherence graph construction and supervised contrastive learning discriminator. Fig. 2 illustrates its overall architecture. The pseudocode of the training process is shown in Algorithm 1.\\n\\n3.1 Coherence Graph Construction\\n\\nIn this part, we illustrate how to construct coherence graph to dig out the coherence structure of the text by modeling sentence interaction. According to Centering Theory (Grosz and Sidner, 1986), the coherence of texts could be modeled by sentence interaction around center entities. To better reflect text structure and avoid semantic overlap, we propose to construct an undirected graph with entities as nodes. Specifically, we first implement the ELMo-based NER model...\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: Illustration of CEM. It encodes and fuses the coherence graph and text sequence to generate coherence-enhanced representation of document.\\n\\nTagLM (Peters et al., 2017) with the help of the NER toolkit AllenNLP to extract the entities from document. A relation $<$ inter $>$ is constructed between the same entities in different sentences and nodes within the same sentences are connected by relation $<$ inner $>$ for their natural structure relevance. Formally, the mathematical form of the coherence graph's adjacent matrix is defined as follows:\\n\\n\\\\[\\nA_{ij} = \\\\begin{cases} \\n1_{\\\\text{rel}} & \\\\text{if } v_i,a \\\\neq v_j,b, \\\\ a = b \\\\\\\\\\n1_{\\\\text{rel}} & \\\\text{if } v_i,a = v_j,b, \\\\ a \\\\neq b \\\\\\\\\\n0_{\\\\text{rel}} & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\nwhere $v_i,a$ represents the $i$-th entity in sentence $a$, which is regarded as node in coherence graph. We verify how MGT and HWT separate through static analysis on coherence graph in Appendix I.\\n\\n3.2 Supervised Contrastive Learning\\n\\n3.2.1 Model Overview\\n\\nThe training process is illustrated in Fig. 2. Each entry in the dataset is documented with its coherence graph. The entries in the training set are sampled randomly into keys and queries. Two coherence encoder modules (CEM) $f_k$ and $f_q$, are initialized the same to generate coherence-enhanced representation $D_k$ and $D_q$ for key and query. A dynamic memory bank with the size of all training data is initialized to store all key representation and their annotations for providing enough contrastive pairs in low-resource scenarios. In every training step, the newly encoded key graphs update the memory bank following the First In First Out (FIFO) rule to keep it updated and the training process consistent.\\n\\nA novel loss composed of improved contrastive loss and cross-entropy loss ensures the model's ability to achieve instance-level intra-class compactness and inter-class separability while maintaining class-level distinguishability. A linear discriminator takes query representations as input and generates prediction results.\\n\\n3.2.2 Positive/Negative Pair Definition\\n\\nIn the supervised setting, where we have access to label information, we define two samples with the same label as positive pairs and those with different labels as negative pairs for incorporating label information into the training process.\\n\\n3.2.3 Encoder Design\\n\\nIn this part, we introduce the structure of graph neural network structure, an innovative coherence encoder module (CEM), which is utilized to integrate coherence information into a semantic representation of text by propagating and aggregating information from different granularity. The workflow is illustrated in Fig. 3.\\n\\nNode Representation Initialization. We initialize the representation of entity nodes with the powerful pre-trained model RoBERTa for its superior ability to encode contextual information into text representation. Given an entity $e$ with a span of $n$ tokens, we utilize RoBERTa to map input document $x$ to embeddings $h(x)$. The contextual representation of $e$ is calculated as follows:\\n\\n\\\\[\\nZ_v = \\\\frac{1}{n} \\\\sum_{i=0}^{n} h(x)_e^i, \\\\ (1)\\n\\\\]\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $e_i$ is the absolute position where the $i$-th token in $e$ lies in the whole document.\\n\\nRelation-aware GCN. Based on the vanilla Graph Convolutional Networks (Welling and Kipf, 2016), we propose a novel method to assign different weight $W_r$ for inter and inner relation $r$ with Relation-aware GCN. Relation-aware GCN convolute edges of each kind of relation in the coherence graph separately. The final representation is the sum of GCN outputs from all relations. We use two-layer GCN in the model because more layers will cause an overfitting problem under low resources. We define the relation set as $R$, and the calculation formula is as follows:\\n\\n$$H(i+1) = \\\\sum_{r \\\\in R} \\\\hat{A} \\\\text{ReLU}((\\\\hat{A}H(i)W(i)r)W(i+1)r),$$ (2)\\n\\nwhere $H(i) \\\\in \\\\mathbb{R}^{N \\\\times d}$ is node representation in $i$-th layer. $\\\\hat{A} = \\\\tilde{D}^{-1/2} \\\\tilde{A} \\\\tilde{D}^{-1/2}$, $A$ is the adjacency matrix of the coherence graph, $\\\\hat{A}$ is the normalized Laplacian matrix of $\\\\tilde{A}$, $W_r$ is the relation transformation matrix for relation $r$.\\n\\nSentence Representation. Afterward, we aggregate updated node representation from the last layer of Relation-aware GCN into sentence-level representation to prepare for concatenation with sequence representation from RoBERTa. The aggregation follows the below rule:\\n\\n$$Z_s(i) = \\\\frac{1}{M_i} \\\\sum_{j} \\\\sigma(W_sH(i,j) + b_s),$$ (3)\\n\\nwhere $M_i$ represents the number of entities in $i$-th sentence, $H(i,j)$ represents the embedding of $j$-th entity in $i$-th sentence, $W_s$ is weight matrix and $b_s$ is bias. All the sentence representations within the same document are concatenated as sentence matrix $Z_s$.\\n\\nDocument Representation with Attention LSTM. We design a self-attention mechanism for discovering the sentence-level coherence between one sentence and other sentences, and apply LSTM with the objective to track the coherence in continuous sentences and take the last hidden state of LSTM for aggregated document representation containing comprehensive coherence information. The calculation is described as follows:\\n\\n$$Z_c = \\\\text{LSTM}(\\\\text{softmax}(\\\\gamma \\\\text{norm}(K) \\\\text{norm}(Q)^T \\\\sqrt{d}Z) V),$$ (4)\\n\\nwhere $K, Q, V$ are linear transformations of $Z_s$ with matrix $W_k, W_q, W_v$, $d$ is the dimension of representation $Z_s$, and $\\\\gamma$ is a hypergamma-parameter for scaling.\\n\\nFinally, we concatenate $Z_c$ and the sequence representation $h([CLS])$ from the RoBERTa's last layer to generate document coherence-enhanced representation $D$.\\n\\n3.2.4 Dynamic Memory Bank. The dynamic memory bank is created to store as much as key encoding $D_k$ to form adequate positive and negative pairs within a batch. The dynamic memory bank is maintained as a queue so that the newly encoded keys can replace the outdated ones, which keeps the consistency between the key encoding and the current training step.\\n\\n3.2.5 Loss Function. Following the definition of positive pairs and negative pairs above, traditional supervised contrastive loss (Gunel et al., 2021) treats all positive pairs and negative pairs equally. However, with a recognition that not all negatives are created equal (Cai et al., 2020), our goal is to emphasize the informative samples to help the model differentiate difficult samples. Thus, we propose an improved contrastive loss that dynamically adjusts the weight of negative pair similarity according to the hardness of negative samples. To be specific, the hard negative samples should be assigned a larger weight to stimulate the model to pull the same classes together and push different classes away. The improved contrastive loss is defined as:\\n\\n$$L_{ICL} = \\\\frac{1}{M} \\\\sum_{j=1}^{M} \\\\log \\\\frac{S_{ij}}{\\\\sum_{p \\\\in P(i)} S_{ip} + \\\\sum_{n \\\\in N(i)} S_{in}},$$ (5)\\n\\nwhere $P(i)$ is the positive set in which data has the same label with $q_i$ and $N(i)$ is the negative set in which data has a different label from $q_i$.\\n\\nApart from instance-level learning mechanism, a linear classifier combined with cross-entropy loss $L_{CE}$ is employed to provide the model with class-level separation ability. $L_{CE}$ is calculated by\\n\\n$$L_{CE} = \\\\frac{1}{N} \\\\sum_{i=1}^{N} -[y_i \\\\log(p_i) + (1\u2212y_i) \\\\log(1\u2212p_i)],$$ (6)\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $p_i$ is the prediction probability distribution of the $i$-th sample. The final loss $L_{total}$ is a weighted average of $L_{ICL}$ and $L_{CE}$ as:\\n\\n$$L_{total} = \\\\alpha L_{ICL} + (1 - \\\\alpha) L_{CE},$$\\n\\n(7)\\n\\nwhere the hyperparameter $\\\\alpha$ adjusts the relative balance between instance compactness and class separability.\\n\\n3.2.6 Momentum Update\\n\\nThe parameters of query encoder $f_q$ and the classifier can be updated by gradient back-propagated from $L_{total}$. We denote the parameters of $f_q$ as $\\\\theta_q$, the parameters of $f_k$ as $\\\\theta_k$, the key encoder $f_k$'s parameters are updated by the momentum update mechanism:\\n\\n$$\\\\theta_k \\\\leftarrow \\\\beta \\\\theta_k + (1 - \\\\beta) \\\\theta_q,$$\\n\\n(8)\\n\\nwhere the hyperparameter $\\\\beta$ is the momentum coefficient.\\n\\nAlgorithm 1\\n\\nAlgorithm of CCO\\n\\nInput:\\n\\n- Input $X$, consisting of documents $D$ and corresponding coherence graph $G$\\n- Hyper-parameters such as the size of dynamic memory bank $M$ and batch size $S$\\n- Labels $Y$\\n\\nOutput:\\n\\n- A learned model CCO, consisting of key encoder $f_k$ with parameters $\\\\theta_k$, query encoder $f_q$ with parameters $\\\\theta_q$, classifier $f_c$ with parameters $\\\\theta_c$\\n\\n1: Initialize $\\\\theta_k = \\\\theta_q$, $\\\\theta_c$\\n2: Initialize dynamic memory bank with $f_k(x_1, x_2, ..., x_M)$, where $x_i$ is randomly sampled from $X$.\\n3: Freeze $\\\\theta_k$\\n4: $\\\\text{epoch} \\\\leftarrow 0$\\n5: while $\\\\text{epoch} \\\\leq \\\\text{epoch}_{\\\\text{max}}$ do\\n6: $n \\\\leftarrow 0$\\n7: while $n \\\\leq n_{\\\\text{max}}$ do\\n8: Randomly select batch $b_k, b_q$\\n9: $D_q = f_q(b_q)$, $D_k = f_k(b_k)$\\n10: $\\\\hat{p} = f_c(D_q)$\\n11: Calculate $L_{ICL}$ with equation 5, calculate $L_{CE}$ with equation 6, calculate $L_{total}$ with equation 7\\n12: Backward on $L_{total}$ and update $\\\\theta_q, \\\\theta_c$ based on AdamW gradient descent with an adjustable learning rate\\n13: Momentum update $\\\\theta_k$ with equation 8\\n14: Update dynamic memory bank queue with enqueue(queue, $D_k$), dequeue(queue)\\n15: $k \\\\leftarrow k + 1$\\n16: end while\\n17: if Early stopping then\\n18: break\\n19: else\\n20: $\\\\text{epoch} \\\\leftarrow \\\\text{epoch} + 1$\\n21: end if\\n22: end while\\n23: return A trained model CCO\\n\\n4 Experiments\\n\\n4.1 Datasets\\n\\nWe evaluate our model on the following datasets:\\n\\n- **GROVER Dataset** (Zellers et al., 2019) is a News-style dataset in which HWTs are collected from RealNews, a large corpus of news from Common Crawl, and MGTs are generated by Grover-Mega (1.5B), a transformer-based news generator.\\n\\n- **GPT-2 Dataset** is a Webtext-style dataset provided by OpenAI with HWTs adopted from WebText and MGTs produced by GPT-2 XLM-1542M.\\n\\n- **GPT-3.5 Dataset** is a News-style open-source dataset constructed by us based on the text-davinci-003 model (175B) of OpenAI, which is one of the most capable GPT-3.5 models so far and can generate longer texts (maximum 4,097 tokens). The GPT-3.5 model refers to various latest newspapers (Dec. 2022 - Feb. 2023) whose full texts act as the HWTs part, and the model generates by imitation. We design two subsets: mixed- and unmixed-provenances, whose details are explained in Appendix B. The brand-new datasets ensure no existing models have been pre-trained on the corpus, which accounts for the fairness of comparison.\\n\\nThe statistics of datasets are summarized in Appendix A. We randomly sample 500 examples as training data for low-resource settings. As for the full dataset setting, we utilize all training data. The implementation details are in Appendix D.\\n\\n4.2 Comparison Models\\n\\nWe compare CCO to state-of-the-art detection methods to reveal the effectiveness. We mainly divide comparison methods into two categories, model-based and metric-based methods. The metrics-based methods detect based on specific statistical text-evaluation metrics and logistic regression while the model-based methods learn features via fine-tuning a model.\\n\\nThe model-based baselines are as follows: GPT-2 (Radford et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019) are powerful transformers-based models fine-tuned on the binary classification task, implementing GPT-2 small (124M), RoBERTa-base (110M) and XLNet-base (110M).\\n\\nCE+SCL (Gunel et al., 2021), a state-of-the-art supervised contrastive learning method in various downstream task. We train the detector with Cross-Entropy loss (CE) and supervised contrastive loss (SCL) calculated within a mini-batch.\\n\\nDualCL (Chen et al., 2022), a contrastive learning method.\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset               | GROVER | GPT-2 |\\n|----------------------|--------|-------|\\n| **Size**             |        |       |\\n| Limited Dataset (500 examples) |       |       |\\n| Full Dataset         |        |       |\\n| **Metric**           | ACC    | F1    |\\n| GPT2                 | 0.5747 | \u00b1 0.0217 | 0.4394 | \u00b1 0.0346 |\\n| XLNet                | 0.5660 | \u00b1 0.0265 | 0.4707 | \u00b1 0.0402 |\\n| RoBERTa              | 0.6621 | \u00b1 0.0133 | 0.5895 | \u00b1 0.0231 |\\n| DualCL               | 0.5835 | \u00b1 0.0857 | 0.4628 | \u00b1 0.1076 |\\n| CE+SCL               | 0.6870 | \u00b1 0.0142 | 0.5961 | \u00b1 0.0197 |\\n| GLTR                 | 0.3370 |        | 0.4935 |        |\\n| DetectGPT            | 0.5910 |        | 0.4258 |        |\\n| **Dataset**          |        |       |\\n| GPT-3.5 Unmixed      |        |       |\\n| GPT-3.5 Mixed        |        |       |\\n| **Size**             |        |       |\\n| Limited Dataset (500 examples) |       |       |\\n| Full Dataset         |        |       |\\n| **Metric**           | ACC    | F1    |\\n| GPT2                 | 0.9023 | \u00b1 0.0095 | 0.8920 | \u00b1 0.0073 |\\n| XLNet                | 0.9107 | \u00b1 0.0068 | 0.9037 | \u00b1 0.0064 |\\n| RoBERTa              | 0.9670 | \u00b1 0.0084 | 0.9681 | \u00b1 0.0077 |\\n| CE+SCL               | 0.9823 | \u00b1 0.0053 | 0.9703 | \u00b1 0.0070 |\\n| GLTR                 | 0.9255 | \u00b1 0.0070 | 0.9350 | \u00b1 0.0077 |\\n| DetectGPT            | 0.9220 | \u00b1 0.0106 | 0.9245 | \u00b1 0.0089 |\\n| **4.3 Performance Comparison** |\\n\\nAs shown in Table 1, COCO surpasses the state-of-the-art methods in MGT detection task by at least 1.23% and 1.64%, 1.75% and 2.83% on the GROVER, GPT-2 limited datasets in terms of Accuracy and F1-Score, respectively. And COCO achieves comparable performance with the most capable detectors in the complete dataset setting. The result indicates the utility of contrastive learning and the rationality of coherence representation.\\n\\nMoreover, it should be noticed that compared with metric-based methods, model-based methods usually tend to achieve better results. This can be explained because metric-based methods can only concern and regress on a few features, which are over-compressed and under-represented for the detection task. Also, metric-based methods mainly use the pre-trained model for token probability instead of fine-tuning the whole model. And with more training samples involved, the performance of model-based methods improves drastically, while metric-based methods do not benefit much from more training examples. It reveals that logistic regression is not strong enough to take in many texts with diverse semantics. Meanwhile, COCO outperforms CE+SCL and DualCL regardless of the size of the training set, which suggests the success of improved contrastive loss to solve the performance degradation problem brought by simple negative samples.\\n\\nWe also find GROVER Dataset is the hardest to detect. It is because the GROVER generator is trained in an adversarial heuristic with the objective of deceiving the verifier, which endows the generator with a deceptive nature. To our surprise, the GPT-3.5 dataset is overly simple for all detectors. The result is also in accord with conclusions in recent works (Mireshghallah et al., 2023; Chen et al., 2023). We conduct extensive experiments on...\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"different self-constructed and published GPT-3.5 datasets generated by a series of prompts, validating this thundering conclusion. The experiment details and results are in Appendix C. We also implement experiments and discussions to explore further explanations in Sec. 4.5.2.\\n\\nNotably, a more comprehensive comparison experiment with 8 datasets (Pu et al., 2023) and 12 methods is presented in Appendix E, which substantiates the advantage of COCO.\\n\\n4.4 Ablation Study\\nTo illustrate the necessity of components of COCO, we conduct ablation experiments on 1,000-example GROVER dataset. The ablation models' structure is as follows:\\n\\n| Model                  | ACC  | F1   |\\n|------------------------|------|------|\\n| COO (Plain)            | 0.7697 | 0.6428 |\\n| COO (Sentence Nodes)   | 0.7733 | 0.6379 |\\n| COO (Coherence)        | 0.7777 | 0.6463 |\\n| COO (Coherence+LSTM)   | 0.7787 | 0.6471 |\\n| COO (Coherence+LSTM+SCL)| 0.7827 | 0.6609 |\\n\\nTable 2: Results of the ablation study on 1,000-example GROVER dataset.\\n\\nCOCO (Plain) removes graph information and encodes only by RoBERTa parts. The model removes contrastive learning and only uses CE loss. COO (Sentence Nodes) treats sentences (instead of entities) as nodes and establishes edges between sentences that share the same entities. Node representation is initialized by RoBERTa embedding and mean-pooling operation. Document representation is obtained by one CEM discarding sentence representation and attention LSTM part in Sec. 3.2.3. Document representation is calculated by mean-pooling operation on sentence node representations. A linear classification head with cross-entropy loss is used for detection. COO (Coherence) incorporates the coherence graph into the representation of document and deploys the sentence representation of Sec. 3.2.3. The rest are the same with COO (Sentence Nodes). COO (Coherence+LSTM) uses attention LSTM for document-level aggregation, and the rest is the same as COO (Coherence). COO (Coherence+LSTM+SCL) utilizes the contrastive learning framework, but the loss function is traditional supervised contrastive loss (SCL) instead of the improved contrastive loss.\\n\\nAs shown in Table 2, coherence information and the contrastive learning framework greatly contribute to the development of model performance, especially in F1-Score. Replacing entity nodes in the coherence graph with sentences impairs the detector, which could be caused by semantic overlap between graph representation and text sequence representation. The attention LSTM also plays an important role in preserving the coherence information during sentence aggregation. Lastly, the results show the advantage of improved contrastive loss over standard supervised contrastive loss.\\n\\nFurthermore, we also conduct ablation studies on other scenarios, including GPT-2, GPT-3.5-Unmixed, and GPT-3.5-Mixed datasets. More detailed results are discussed in the Appendix G, which clearly stands for the performance gain of COO components. Moreover, the helpfulness of contrastive learning is verified to be orthogonal to the helpfulness of coherence information.\\n\\n4.5 Discussion\\n4.5.1 Model Robustness to Perturbation\\nTo validate the robustness of COO to various perturbations, we train COO on the GROVER dataset in the low-resource setting and perturb the test set with four different operations: Delete (randomly delete tokens in each entry), Repeat (randomly select tokens and repeat them twice in the text), Insert (add random tokens from the vocabulary of the pre-trained model into random positions in the text), Replace (randomly replace tokens with randomly selected tokens from the vocabulary). The perturbation scale is set to 15%. The experiment result is shown in Table 3.\\n\\n| Model | Metric | Acc  | F1   |\\n|-------|--------|------|------|\\n|        | Original | 0.6635 | 0.5901 |\\n| Delete |        | 0.5736 (-0.0899) | 0.5545 (-0.0356) |\\n|        | Repeat | 0.6320 (-0.0315) | 0.5743 (-0.0158) |\\n|        | Insert | 0.6325 (-0.0310) | 0.4881 (-0.1020) |\\n|        | Replace | 0.5554 (-0.1081) | 0.4814 (-0.1087) |\\n|        | Average | 0.5984 (-0.0651) | 0.5246 (-0.0655) |\\n\\nTable 3: Model robustness to different perturbations.\\n\\nDespite the structural complexity, COO keeps outperforming the baseline during perturbations. COO's performance fluctuations are as minor as the baseline. And COO maintains 4.53% better in\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: N-gram Coverage in GPT-3.5 Mixed Dataset.\\n\\n|   | MGT | HWT |\\n|---|-----|-----|\\n| \u03b31 | 0.6659 | 0.6377 |\\n| \u03b32 | 0.4250 | 0.3630 |\\n| \u03b33 | 0.2883 | 0.2076 |\\n| \u03b34 | 0.2019 | 0.1372 |\\n| \u03b35 | 0.1425 | 0.0935 |\\n\\nTable 5: Individual tokens with top-3 productivity.\\n\\n|   | Token Productivity Coverage |\\n|---|-----------------------------|\\n|   | according | 0.6923 | 0.3126 |\\n|   | where | 0.6842 | 0.1998 |\\n|   | they | 0.6316 | 0.3837 |\\n\\n4.5.2 Statistic Cues for Detectable Feature in GPT-3.5\\n\\nTo further investigate the rationale behind the easy-to-detect nature of GPT-3.5 generated texts, we utilize Transformers-Interpret\\\\(^6\\\\), a tool for evaluating feature attribution in predictions based on Integrated Gradients (Sundararajan et al., 2017), for discovering the supporters and opponents (tokens) in the decision-making stage.\\n\\nWe probe the statistical cues of the GPT-3.5 mixed dataset from two perspectives: spans of tokens and individual tokens.\\n\\nWe define spans of tokens coverage \\\\(\\\\gamma^n\\\\) as \\\\(n\\\\)-gram supporters for true positives \\\\(P^n\\\\), i.e., \\\\(n\\\\) consecutive tokens all contribute positively to the correct prediction, over all \\\\(n\\\\)-gram tokens in true positives \\\\(A^n\\\\), which could be formulated as\\n\\n\\\\[\\n\\\\gamma^n = \\\\frac{P^n}{A^n}. \\n\\\\]\\n\\nMoreover, we apply productivity \\\\(\\\\pi_k\\\\) and coverage \\\\(\\\\epsilon_k\\\\) of statistic cue \\\\(k\\\\) (Niven and Kao, 2019) on the GPT-3.5 mixed dataset to find out if there are individual tokens acting as common and strong signals contribute to model predictions. Formally, productivity \\\\(\\\\pi_k\\\\) is defined as:\\n\\n\\\\[\\n\\\\alpha_k = \\\\sum_{i=1}^{n} \\\\left[ \\\\exists j, k \\\\in T(i) \\\\land j \\\\not\\\\in T(i) \\\\right], \\n\\\\]\\n\\n\\\\[\\n\\\\pi_k = \\\\frac{\\\\alpha_k}{\\\\sum_{i=1}^{n} \\\\left[ \\\\exists j, k \\\\in T(i) \\\\land j \\\\not\\\\in T(i) \\\\land y_i = j \\\\right]} \\\\alpha_k. \\n\\\\]\\n\\nHere, \\\\(T(i)\\\\) is the set of tokens for text \\\\(i\\\\) with label \\\\(j\\\\). And the coverage \\\\(\\\\epsilon_k\\\\) is the portion that all applicable cues over the total number of data points.\\n\\nWe fine-tune the RoBERTa-base model with a classification head on the GPT-3.5 mixed dataset\\\\(^6\\\\) and quantify how tokens in GPT-3.5 mixed test data affect the model predictions with the criteria mentioned above. The results are shown in Table 4 and Table 5. It could be noticed that although \\\\(\\\\gamma^1\\\\) for MGT and HWT is about the same, the gap widens from \\\\(\\\\gamma^2\\\\) to \\\\(\\\\gamma^5\\\\), indicating that more consecutive spans of tokens act as an indicator for MGT than HWT. Table 5 shows that \\\"according\\\", \\\"where\\\", and \\\"they\\\" are top-3 strongest tokens for detection. However, we could not reach any valid conclusions from their semantics. Meanwhile, these tokens only cover a small portion of the total number of data points (less than 0.4), leading to the weak strength of the signal they provide. Therefore, we come up with a hypothesis that the easy-to-detect nature of GPT-3.5 does not originate from specific token but from certain language patterns (could be demonstrated by a span of tokens). The reason might be that advanced LLMs fit extremely well to the corpus so that it generates more general expressions, which could be much easier to be expected by fine-tuned detectors. A case study for token importance illustration is shown in Appendix H.2.\\n\\nFurther, we discuss more topics in the Appendix, e.g., the effect of hyper-parameters (F), case study (H), static geometric analysis on coherence graph (I), and exploration on imbalanced data (J).\\n\\n5 Conclusion\\n\\nIn this paper, we propose COCO, a coherence-enhanced contrastive learning model for MGT detection. We construct a novel coherence graph from the document and implement a MoCo-based contrastive learning framework to improve model performance in low-resource settings. An innovative encoder composed of relation-aware GCN and attention LSTM is designed to learn the coherence representation from the coherence graph, which is further incorporated with the sequence representation of the document. To alleviate the effect of unnecessary easy samples, we propose an improved contrastive learning loss to force the model to pay more attention to hard negative samples.\\n\\nCOCO outperforms all detection tasks generated by GROVER, GPT-2, and GPT-3.5, respectively, in both low-resource and high-resource settings. We also find the outputs from the advanced GPT-3.5 are more detectable and explore the rationale behind the phenomena through the perspective of spans of tokens and individual tokens.\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nIn this work, we step forward to better distinguishing MGTs under the low-resource setting. However, several limitations still exist for the broader applications of this detector. Firstly, MGTs are easier to generate and collect than HWTs, which may cause an imbalanced label distribution in the dataset. And CO literally corrupts in extremely imbalanced data distribution condition, as shown in J. Future work could build upon the contrastive learning method of CO with innovation on sampling strategy for harsh low-resource and imbalanced data settings. Secondly, our method artificially generates a coherence graph for every entry, which is not efficient for larger datasets. What's more, short text, codes, and mathematical proofs, which are hard to generate coherence graphs, are also limitedly detected by CoCo. More distinctive and easy-to-calculate features are worth exploring for generating distinguishable representations for texts with efficiency while better understanding the essence of TGMs. Thirdly, with instruct-based generation and human-in-loop fine-tuning models prevailing, the strategy and defect of TGMs change slightly but constantly. The entity relation with the same semantic granularity and concretization in this paper would not be enough to detect the high-quality content by TGMs in the future. More generative and adaptive detection models should be considered.\\n\\nEthical Considerations\\n\\nWe provide insight into the potential weakness of TGMs and publish the GPT-3.5 news datasets. We understand that the discovery of our work can be viciously used to confront detectors. And we understand that malicious users can copy the contents of our GPT-3.5 news dataset to disguise real news and publish them. However, with the purpose of calling for attention to detecting and controlling possible misuse of TGMs, we believe our work will inspire the advancement of the stronger detector of MGTs and prevent all potential negative uses of language models.\\n\\nOur work complies with the sharing & publication policy of OpenAI and all data we collect is in the public domain and licensed for research purposes.\\n\\nAcknowledgements\\n\\nWe thank all the reviewers, the area chair, Kevin Yang (UC Berkeley), and Prof. Pietro Li\u00f2 (Univ. of Cambridge) for their helpful feedback, which aided us in greatly improving the paper. This work is supported by National Natural Science Foundation of China (62272371, 62103323, U21B2018), Initial Postdocs Supporting Program (BX20190275, BX20200270), China Postdoctoral Science Foundation (2019M663723, 2021M692565), Fundamental Research Funds for the Central Universities under grant (xhj032021013), and Shaanxi Province Key Industry Innovation Program (2021ZDLGY01-02).\\n\\nReferences\\n\\nDavid Ifeoluwa Adelani, Haotian Mai, Fuming Fang, Huy H Nguyen, Junichi Yamagishi, and Isao Echizen. 2020. Generating sentiment-preserving fake online reviews using neural language models and their human-and machine-based detection. In International Conference on Advanced Information Networking and Applications, pages 1341\u20131354. Springer.\\n\\nErnst Althaus, Nikiforos Karamanis, and Alexander Koller. 2004. Computing locally coherent discourses. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 399\u2013406.\\n\\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnsson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.\\n\\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc'Aurelio Ranzato, and Arthur Szlam. 2019. Real or fake? learning to discriminate machine from human generated text. arXiv preprint arXiv:1906.03351.\\n\\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373.\\n\\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745.\\n\\nRoi Blanco and Christina Lioma. 2011. Graph-based term weighting for information retrieval. Information Retrieval, 15:54\u201392.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind 16176.\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nTiffany Tianhui Cai, Jonathan Frankle, David J Schwab, and Ari S Morcos. 2020. Are all negatives created equal in contrastive instance discrimination? arXiv preprint arXiv:2010.06682.\\n\\nQianben Chen, Richong Zhang, Yaowei Zheng, and Yongyi Mao. 2022. Dual contrastive learning: Text classification via label-aware data augmentation. arXiv preprint arXiv:2201.08702.\\n\\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. 2020. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297.\\n\\nYutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, and Bhiksha Ramakrishnan. 2023. Gpt-sentinel: Distinguishing human and chatgpt generated content. arXiv preprint arXiv:2305.07969.\\n\\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021a. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816\u20133830.\\n\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910.\\n\\nSebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. 2019. Gltr: Statistical detection and visualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 111\u2013116.\\n\\nNir Grinberg, Kenneth Joseph, Lisa Friedland, Briony Swire-Thompson, and David Lazer. 2019. Fake news on twitter during the 2016 us presidential election. Science, 363(6425):374\u2013378.\\n\\nBarbara J Grosz and Candace L Sidner. 1986. Attention, intentions, and the structure of discourse. Computational linguistics, 12(3):175\u2013204.\\n\\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Veselin Stoyanov. 2021. Supervised contrastive learning for pre-trained language model fine-tuning. In International Conference on Learning Representations.\\n\\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738.\\n\\nXinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. 2023. Mgtbench: Benchmarking machine-generated text detection. arXiv preprint arXiv:2303.14822.\\n\\nXiaochen Hou, Peng Qi, Guangtao Wang, Rex Ying, Jing Huang, Xiaodong He, and Bowen Zhou. 2021. Graph ensemble learning over multiple dependency trees for aspect-level sentiment classification. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2884\u20132894.\\n\\nEduard H Hovy. 1988. Planning coherent multisentential text. In Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 163\u2013169.\\n\\nBinxuan Huang and Kathleen M Carley. 2019. Syntax-aware aspect level sentiment classification with graph attention networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5469\u20135477.\\n\\nKung-Hsiang Huang, Preslav Nakov, Yejin Choi, and Heng Ji. 2022. Faking fake news for real fake news detection: Propaganda-loaded training data generation. ArXiv, abs/2203.05386.\\n\\nDaphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1808\u20131822.\\n\\nNikiforos Karamanis and Hisar Maruli Manurung. 2002. Stochastic text structuring using the principle of continuity. In Proceedings of the International Natural Language Generation Conference, pages 81\u201388.\\n\\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186.\\n\\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858.\\n\\nMirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In ACL, volume 3, pages 545\u2013552. Citeseer.\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-1005", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Improving language generation with sentence coherence objective. \\n\\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In International conference on machine learning, pages 3319\u20133328. PMLR.\\n\\nEnhua Tan, Lei Guo, Songqing Chen, Xiaodong Zhang, and Yihong Zhao. 2012. Spammer behavior analysis and detection in user generated content on social networks. In 2012 IEEE 32nd International Conference on Distributed Computing Systems, pages 305\u2013314. IEEE.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\\n\\nPeter D Turney. 2002. Learning to extract keyphrases from text. arXiv preprint cs/0212013.\\n\\nAdaku Uchendu, Jeffrey Cao, Qiaozhi Wang, Bo Luo, and Dongwon Lee. 2019. Characterizing man-made vs. machine-made chatbot dialogs. In TTO.\\n\\nAdaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. 2020. Authorship attribution for neural text generation. In Conf. on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nAdaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, and Dongwon Lee. 2021. Turingbench: A benchmark environment for turing test in the age of neural text generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2001\u20132016.\\n\\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.\\n\\nFeng Wang and Huaping Liu. 2021. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2495\u20132504.\\n\\nMax Welling and Thomas N Kipf. 2016. Semi-supervised classification with graph convolutional networks. In J. International Conference on Learning Representations (ICLR 2017).\\n\\nYuta Yanagi, Ryohei Orihara, Yuichi Sei, Yasuyuki Tahara, and Akihiko Ohsuga. 2020. Fake news detection with generated comments for news articles. In 2020 IEEE 24th International Conference on Intelligent Engineering Systems (INES), pages 85\u201390. IEEE.\\n\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32.\\n\\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph convolutional networks for text classification. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7370\u20137377.\\n\\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. Advances in neural information processing systems, 32.\\n\\nWanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020. Neural deepfake detection with factual structure of text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2461\u20132470.\\n\\n### Table 6: Basic statistics of datasets.\\n\\n| Dataset Class | Train | Valid | Test |\\n|---------------|-------|-------|------|\\n| GROVER HWT   | 5,000 | 2,000 | 8,000 |\\n| MGT           | 5,000 | 1,000 | 4,000 |\\n| GPT-2 HWT    | 25,000| 5,000 | 5,000 |\\n| MGT           | 25,000| 5,000 | 5,000 |\\n| GPT-3.5 Unmixed | 3,032 | 1,000 | 1,000 |\\n| MGT           | 3,032 | 1,000 | 1,000 |\\n| GPT-3.5 Mixed  | 3,454 | 1,000 | 1,000 |\\n| MGT           | 3,454 | 1,000 | 1,000 |\\n\\n**GPT-3.5 Dataset for CO**\\n\\nCO is our latest dataset for the MGT detection task. There are two subsets in the self-made dataset for easy analysis of the impact of provenance and writing styles: unmixed- and mixed provinces. We use the text-davinci-003 model of OpenAI to generate MGT examples. The maximum length of HWTs is 1,024 tokens, and the target generation length is set as 1,024 tokens. Here is an example of the MGT data.\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DOHA, Qatar. The president of world soccer's governing body on Saturday sought to blunt mounting concerns about the World Cup in Qatar with a strident defense of both the host country's reputation and FIFA's authority over its showpiece championship. ...... Citing statistics, history and even childhood to bolster his case, he at one point likened his own experience as a redheaded child of immigrants to Switzerland to the assimilation problems of gays in the Middle East, and defended the laws, customs and honor of the host country.\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generate a news passage.\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with PSG until early January after World Cup success\\n\\nThe news is written by Matias Grez from CNN in 2022-12-28 00:00:00.\\nTitle: Lionel Messi isn't expected to be back with"}
{"id": "emnlp-2023-main-1005", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset         | Metric | GPT2          | RoBERTa       | COC          |\\n|-----------------|--------|---------------|---------------|--------------|\\n|                 | ACC(val/test) | 0.9914/0.9916 | 0.9946/0.9950 | 0.9955/0.9950 |\\n|                 | F1(val/test)  | 0.9916/0.9918 | 0.9950/0.9952 | 0.9942/0.9945 |\\n|                 | ACC(val/test) | 0.9890/0.9893 | 0.9935/0.9941 | 0.9938/0.9941 |\\n|                 | F1(val/test)  | 0.9885/0.9889 | 0.9933/0.9937 | 0.9936/0.9940 |\\n|                 | ACC(val/test) | 0.9925/0.9928 | 0.9946/0.9943 | 0.9942/0.9943 |\\n|                 | F1(val/test)  | 0.9923/0.9924 | 0.9942/0.9943 | 0.9942/0.9943 |\\n|                 | ACC(val/test) | 0.9884/0.5422* | 0.9962/0.6406* | 0.9966*       |\\n|                 | F1(val/test)  | 0.9880/0.6335* | 0.9960/0.7273* | 0.9970*       |\\n\\nTable 9: Experiment of different detectors on different GPT-3.5 Dataset. *: The great performance difference between validation set and test set on GPT-3.5 (TB) are because the test set randomly sample 50% of the words of each article in the dataset (Uchendu et al., 2021). We do not test COC on GPT-3.5 (TB) for the reason that such operation greatly influences the coherence in texts. We provide an example of this in Table 10.\\n\\nRecent changes to key international indexes have resulted in the unprecedented exclusion of Russian stocks at a \u201czero\u201d price, causing further losses in Moscow\u2019s already-dismal stock exchange. This exclusion has made Russia no longer an option for investors, prompting a shift to other emerging markets.\\n\\nThe dramatic shift was made in early March, when FTSE Russell and MSCI announced the removal of Russian stocks from their indexes due to the country\u2019s escalating economic and geopolitical problems. Shortly after, the Moscow Exchange suspended trading, sending ripples through the market.\\n\\nThe possible default on Russian debt has Western investors further reconsidering their investments in Russia...\\n\\nTable 10: A comparison example between texts in test set of GPT-3.5 (TB) and GPT-3.5 (OP). The GPT-3.5 (TB) text shows great disorder while GPT-3.5 (OP) text is neat.\\n\\nD Implementation Details\\n\\nThis part mentions the implementation details and hyper-parameter settings of all the methods in the experiment. To imitate the situation of low data-resources, we randomly sample 500 entries from the datasets as limited dataset (positive:negative=1:1), which will test models together with the complete datasets. And we conduct experiments on 10 different seeds and report the average test accuracy, F1-Score, and standard deviation only for model-based methods because metric-based methods would not be affected by random seeds.\\n\\nWe use RoBERTa base model to initialize the embedding of our representation and optimize the model using AdamW (Loshchilov and Hutter, 2018) optimizer with a 0.01 weight decay. We set the initial learning rate to $10^{-5}$ and the batch size to 8 for all datasets based on experiences.\\n\\nWe utilize packages, namely transformers, pytorch, and allennlp to implement COC. And the GPT-3.5 datasets and ChatGPT case is generated by OpenAI API and websites. We spend $300 for API costs, including development and final generation costs. We train and do experiments on 8 NVIDIA A100 GPUs on 2 Ubuntu-based servers. The total budget for training 20 epochs, dev, and testing on the GROVER dataset is 2.5 hours. On GPT-2 dataset is 12 hours, and on GPT-3.5 dataset is 1.5 hours. We will publish our code and dataset recently.\\n\\nE More Comparison Experiments\\n\\nProvisioning empirical evidence to claim effectiveness is a relatively broad topic, and in Table 1 we have shown COC outperforms on 4 datasets (8 settings) compared with 6 models, including Roberta and CE+SCL, the SOTA of the model-based methods, and DetectGPT, the SOTA of the metric-based methods. Moreover, our model is outperforming on very wide scenarios. Due to the limitation of pages, we do not post all the results in the main text, so we would love to share with you a more comprehensive result here.\\n\\nDataset. Following Pu et al. (2023), we use Real-News dataset (Raffel et al., 2020) as human-written texts, and the machine-generators are the most representative models nowadays, namely GPT-2 (medium and xl) (Radford et al., 2019), GPT-3 (text-davinci-003) (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), GPT-Neo (2.7B) (Black et al., 2022), GPT-J (Wang and Komatsuzaki, 2021), and\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset Generator | GPT-Neo lg | GPT-J | LLaMA 7B | LLaMA 13B |\\n|-------------------|-----------|-------|---------|-----------|\\n| Type Method       | ACC AUROC | ACC AUROC | ACC AUROC | ACC AUROC |\\n| Probability       | GLTR      | GLTR  | GLTR    | GLTR      |\\n| Metric-based      | GLTR 0.7840 | GLTR 0.7240 | GLTR 0.7580 | GLTR 0.7240 |\\n|                  | Rank 0.6680 | Rank 0.6660 | Rank 0.6420 | Rank 0.6420 |\\n|                  | LogRank 0.8080 | LogRank 0.7580 | LogRank 0.7480 | LogRank 0.7480 |\\n| Perturbed         | DetectGPT-10d | DetectGPT-10d | DetectGPT-10d | DetectGPT-10d |\\n| Metric-based      | DetectGPT-10d 0.8620 | DetectGPT-10d 0.6900 | DetectGPT-10d 0.7560 | DetectGPT-10d 0.7560 |\\n|                  | DetectGPT-10z 0.8480 | DetectGPT-10z 0.6860 | DetectGPT-10z 0.7560 | DetectGPT-10z 0.7560 |\\n| Off-the-shelf     | OpenAI-detector 0.8460 | OpenAI-detector 0.7620 | OpenAI-detector 0.7480 | OpenAI-detector 0.7480 |\\n| Model-based       | ChatGPT-detector 0.4760 | ChatGPT-detector 0.8400 | ChatGPT-detector 0.8400 | ChatGPT-detector 0.8400 |\\n| Fine-tuned        | OpenAI-GPT 0.8050 | OpenAI-GPT 0.7480 | OpenAI-GPT 0.8050 | OpenAI-GPT 0.8050 |\\n|                   | BERT-base 0.8480 | BERT-base 0.7390 | BERT-base 0.8480 | BERT-base 0.8480 |\\n|                   | GPT-2 0.6680 | GPT-2 0.6680 | GPT-2 0.9920 | GPT-2 0.9920 |\\n|                   | RoBERTa-base 0.8940 | RoBERTa-base 0.9270 | RoBERTa-base 0.9840 | RoBERTa-base 0.9840 |\\n|                   | Electra-base 0.8710 | Electra-base 0.7880 | Electra-base 0.8880 | Electra-base 0.8880 |\\n|                   | CoCo 0.9067 | CoCo 0.9462 | CoCo 0.9936 | CoCo 0.9936 |\\n\\nTable 11: Comprehensive experimental results on wide scenarios. The same as the limited setting in Sec. 4.1, which uses 500 examples for these models to fine-tune.\\n\\nComparison Models. More detailed, current detection methods can be categorized into four types: probability metric-based, perturbed metric-based, off-the-shelf model-based, and fine-tuned model-based. Our model is in the fine-tuned model-based category.\\n\\n- **Probability metric-based methods**: GLTR (Gehrmann et al., 2019), i.e. using token log-likelihood; Rank (Solaiman et al., 2019) and LogRank (Ippolito et al., 2020), i.e. using the rank/log-rank of token likelihood.\\n\\n- **Perturbed metric-based methods**: DetectGPT (Mitchell et al., 2023), in the nomenclature of Table 11, the number '10' means the number of perturbation samples. The letter 'd' means not normalized on distribution, while 'z' means normalized.\\n\\n- **Off-the-shelf model-based model**: OpenAI-detector (Solaiman et al., 2019), built by OpenAI mainly for GPT-2 detection based on the RoBERTa model; ChatGPT-detector (Guo et al., 2023), made based on SimpleAI based on the HC3 dataset.\\n\\n- **Fine-tuned model-based methods**: All the models we use have the same level of size, i.e., around 110M parameters, including OpenAI-GPT, Bert-base-uncased, GPT-2, RoBERTa-base, Google Electra-base discriminator, and COCO.\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GPT-2s and GPT-Neo datasets. And ChatGPT-detector, in reverse, excels on GPT-3, GPT-4, Llamas, GPT-J, and GPT-Neo.\\n\\n- Probability metric-based methods rely on the likelihood from the generation model, which is mainly designed for white-box machine-generated detection. For white-box models like GPT-2, GPT-Neo, and GPT-J, their performance is relatively good. But when applied to totally black-box models, these methods could easily fail. DetectGPT, the perturbed metric-based method, shares the same limitation with a similar mechanism.\\n\\n- Among all the fine-tuned model-based methods, RoBERTa-base shows the best performance average on all datasets compared to other base models. Thus, it supports our claim that recognizing RoBERTa as SOTA for this category, and further built CL methods and CO based on RoBERTa.\\n\\n**F. Effect of Hyper-Parameters**\\n\\n**F.1 Contrastive Learning Parameters**\\n\\nWe evaluate the influence of contrastive learning hyper-parameters $\\\\alpha$ and $\\\\tau$ with experiments on different combinations of them. The result is shown in Fig. 4. Considering the discovering that smaller $\\\\tau$ leads to better hard negative mining ability (Wang and Liu, 2021), we select $\\\\alpha$ from \\\\{0.1, 0.2, ..., 0.9\\\\} and $\\\\tau$ from \\\\{0.1, 0.2, 0.3\\\\}. We find that the extreme $\\\\alpha$ value causes the performance degradation and the best hyper-parameter combination is $\\\\alpha, \\\\tau = 0.6, 0.2$. Our analysis is that large $\\\\alpha$ forces the model to concentrate on the instance-level contrast and small $\\\\alpha$ lets class separation objective take control. Both will reduce the generalization performance of the detector on test set.\\n\\n**Figure 4: Effect of parameters $\\\\alpha$ and $\\\\tau$ on model performance.**\\n\\n**F.2 Graph Parameters**\\n\\nWe further investigate the effect of max node number and max sentence number on model performance. The result is shown in Fig. 5. We select max node number from \\\\{60, 90, 120, 150\\\\} and max sentence number from \\\\{30, 45, 60, 75\\\\}. The detector performs best when max node number is 90 and max sentence number is 45. The experiment results prove that the large node and sentence number are not necessary for the improvement of detection accuracy. We infer that even though setting large node and sentence number includes more entity information, excessive nodes bring noise to the model and impair the distinguishability of coherence feature.\\n\\n**Figure 5: Performance of CO with different graph parameters.**\\n\\n**G. Ablation Study**\\n\\nIn Sec. 4.4, we mainly show the performance gain on the GROVER dataset. To further verify the effectiveness of CO across other scenarios. We also do the ablation study on 500-example GPT-2, GPT-3.5-Unmixed, and GPT-3.5-Mixed datasets. The result is shown in Table 12.\\n\\nHere, we add a new ablated setting, CO (ICL), which applies the improved contrastive learning we proposed but does not include any part of the coherence graph representation model (i.e., Coherence and LSTM).\\n\\nBy comparing CO (Coherence) with CO (Plain), we can evaluate the effectiveness of the coherence model. It shows an average improvement of 1.14% accuracy and 1.54% F1 on the plain version. Furthermore, if we add attention LSTM for concatenation, it can achieve 1.26% accuracy enhancement.\\n\\nMoreover, by comparing CO (ICL) and CO, we further show the effectiveness of the coherence model based on the ICL model. There's a gap of 0.86% accuracy and 0.61% F1 between with Coherence model and w/o it. The result shows the effectiveness of the coherence model component doesn't heavily overlap with the effectiveness of the ICL method component. In conclusion, both\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset      | GPT-2   | GPT-3.5 Unmixed | GPT-3.5 Mixed | Avg. Increase |\\n|-------------|---------|-----------------|--------------|--------------|\\n| Metric      |         |                 |              |              |\\n| ACC         | 0.8223  | 0.9670          | 0.9565       |              |\\n| F1          | 0.7978  | 0.9681          | 0.9583       |              |\\n| COCO (Plain)|         |                 |              |              |\\n| ACC         | 0.8325  | 0.9778          | 0.9698       | +0.0114      |\\n| F1          | 0.8217  | 0.9785          | 0.9704       | +0.0154      |\\n| COCO (Coherence) |         |                 |              |              |\\n| ACC         | 0.8356  | 0.9778          | 0.9703       | +0.0126      |\\n| F1          | 0.8274  | 0.9787          | 0.9710       | +0.0176      |\\n| COCO (Coherence + LSTM) |         |                 |              |              |\\n| ACC         | 0.8417  | 0.9798          | 0.9646       | +0.0134      |\\n| F1          | 0.8319  | 0.9779          | 0.9654       | +0.0170      |\\n| COCO (ICL)  |         |                 |              |              |\\n| ACC         | 0.8530  | 0.9889          | 0.9701       | +0.0220      |\\n| F1          | 0.8410  | 0.9791          | 0.9735       | +0.0231      |\\n\\nTable 12: Results of ablation study on 500-example GPT-2, GPT-3.5-Unmixed, and GPT-3.5-Mixed datasets.\\n\\nH Case Study\\nH.1 Coherence Graph Difference\\nIn this subsection, we conduct a case study with HWT and MGT produced by sensational ChatGPT with the same metadata. As illustrated in Fig. 6, we parse two news as coherence graphs. And we observe that although ChatGPT expresses fluently, it is not coherent from the perspective of coherence graph. Hence, COCO utilizes the distinctive coherence feature and makes correct predictions. However, RoBERTa fails to discriminate the MGT without noticing the coherence difference. This reflects even the most popular and advanced language model could suffer from weak coherence and be detected by COCO.\\n\\nFigure 6: An illustration for case study of our method.\\n\\nH.2 Token Importance in GPT-3.5 Detection\\nAs shown in Fig. 7, we take segments from two text pairs consisting of HWT and its corresponding MGT in GPT-3.5 mixed and GROVER dataset. It could be noticed that consecutive spans in text generated by GPT-3.5 tend to contribute more to the model decision. However, in HWTs, model pays more attention to individual tokens. Following this observation, we infer that with the improvement of model scale, LLMs fit extremely well to the corpus so that it generates more general expressions compared with HWTs, which follows certain patterns (always demonstrated by a span of tokens) that could be expected by fine-tuned models. Thus, barely all the methods show nearly perfect performance on GPT-3.5 dataset.\\n\\nAs for GROVER dataset, more tokens contribute negatively to the model prediction, even if the prediction is correct. This reflects the deceptive nature of GROVER and explains the reason why it is the hardest dataset in our experiment to some extent.\\n\\nI Static Geometric Analysis on Coherence Graph\\nWe have witnessed performance enhancement by applying the graph-based coherence model to the detection model, but how does the coherence graph help detection? In this subsection, we apply static geometric features analysis to coherence graph we construct to evaluate the distinguishable difference between HWTs and MGTs with explanation. In the following discussion, we take the dataset of GROVER into the analysis. Some basic metrics of data and the corresponding graph are shown in Table 13.\\n\\n| Metric         | HWT | MGT |\\n|----------------|-----|-----|\\n| Sample Num.    | 4994| 4991|\\n| Avg. Num. of Token | 463.2 | 456.0 |\\n| Avg. Num. of Vertex | 43.60 | 32.37 |\\n| Avg. Num. of Edge | 107.4 | 65.44 |\\n\\nTable 13: Basic metrics of texts and corresponding graphs.\\n\\nThough HWTs and MGTs have approximately the same number of tokens in every text, coherence...\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Visualization of token attributions. The first text pair is sampled from GPT-3.5 mixed dataset and the second text pair is from GROVER dataset. The tokens in green represent contributing positively to the predicted label, while those in red contribute negatively. Label \u201c0\u201d represents HWT, and Label \u201c1\u201d represents MGT.\\n\\nTable 14: Average of degree (whole dataset).\\n\\n| Metric         | Avg. Degree |\\n|----------------|-------------|\\n| HWT            | 2.980       |\\n| MGT            | 2.591       |\\n\\nThe coherence graph for HWTs has a larger scale than MGTs\u2019 with 34.7% more vertexes and 64.1% more edges, which shows that HWTs have more complex semantic relation structures than MGTs.\\n\\nI.1 Degree Distribution\\n\\nSemantically, degree of coherence graph measures the co-occurrence and TF-IDF feature of keywords. Moreover, degree distribution shows global coherence because high-degree nodes devote to the main topic and low-degree nodes are the extension.\\n\\nAs shown in Table 14, the degree of the graph representation of HWTs is 2.980, which is 15.0% larger than MGTs (2.591), which shows disparities of MGTs to form coherent interaction between sentences. Fig. 8 measures the distribution of each graph\u2019s average nodes\u2019 degree, showing that the distribution of HWTs has a longer tail than MGTs.\\n\\nFurthermore, we analyze the distinguishability of degree features when impacted by other factors. One most considerable influence is the style and genre of different provenance. We chose around 60 articles from The Sun and Boston. Then we use GROVER to mimic their style to generate similar topic news. Fig. 9 shows the degree distribution of HWTs and MGTs of both provenances.\\n\\nWe use the Jensen\u2013Shannon divergence to evaluate the similarity of the degree distribution. The JS-divergence of MGTs mimicking The Sun and Boston is 0.029, while the JS-divergence of MGTs and HWTs in Boston is 0.050, in The Sun is 0.061. The apparent gap shows that degree distribution can robustly detect MGTs and HWTs when impacted by provenance differences.\\n\\nI.2 Aggregation\\n\\nAggregation is a shared metric for complex networks and linguistics, depicting how closely the whole is organized around its core. We propose two metrics to evaluate the aggregation of graph-based text representation in our coherence model, the size of the largest connected subgraph and the clustering coefficient.\\n\\nIn our representation, not all sentences have entities related to others. Hence the graph is an unconnected one. The average number of nodes in\\n\\n11https://www.thesun.co.uk/\\n12https://www.boston.com/\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"subgraphs of MGTs is 4.49 and of HWTs is 4.84. We propose that the size of the largest connected subgraph shows the contents which are closely organized around the topic. Moreover, the size of graphs may be an unfair factor, so we use the portion of nodes in the largest connected subgraph to reflect its size. The average portion in HWTs is 0.6725 and in MGTs is 0.6458. Fig. 10 shows the distribution of the portion of graphs, and HWTs distribute more high-portion ones than MGTs.\\n\\nThe clustering coefficient represents how nodes tend to cluster. For the entities of texts, clustering evaluates how the author narrates around the central theme. The larger the clustering coefficient is, the tighter the semantic structure is. The average cluster coefficient of the graphs of HWTs is 0.2213 and of MGTs is 0.1983, HWTs is 11.6% better than MGTs. Fig. 11 shows the distribution.\\n\\nI.3 Core & Degeneracy\\n\\nThe degeneracy of a graph is a measure of how sparse it is, and the \\\\( k \\\\)-core is the subgraph corresponding to its significance in the graph. We propose that, in our graph representation, the degeneracy process of graphs equals summarizing texts semantically. The maximum of core-number shows the complexity of hierarchical structure in texts. Furthermore, the distribution of the core-number reflects the overall sparse and is a graph-perspective N-gram module. Based on experiments, the average core-number of HWTs is 5.772 while MGTs with 4.458. HWTs are 29.5% ahead. Fig. 12 is the distribution of the core-number.\\n\\nI.4 Entropy\\n\\nEntropy is a scientific concept to measure a state of disorder, randomness, or uncertainty. The well-known Shannon entropy is the core of the information theory, measuring the self-information content. For the graph data, network structure entropy defined as the following can examine the information amount of the graph structure.\\n\\n\\\\[\\n\\\\text{Entropy} = - \\\\sum_{i=1}^{N} I_i \\\\ln I_i = - \\\\sum_{i=1}^{N} \\\\frac{k_i}{\\\\sum_{j=1}^{N} k_j} \\\\ln \\\\left( \\\\frac{k_i}{\\\\sum_{j=1}^{N} k_j} \\\\right),\\n\\\\]\\n\\n(10)\"}"}
{"id": "emnlp-2023-main-1005", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $I_i$ is the information content represented by the degree distribution, $N$ is the number of nodes, and $k_i$ is the degree of the $i$-th node.\\n\\nGlobal coherence, from our perspective, equals refining more information inside the semantic structure of the whole text, which matches to structure entropy of our graph representation. From our experiments, the structure entropy of HWTs (2.263) is 6.80% larger than MGTs (2.119), which means HWTs obtain more structured information because their semantic information is globally organized.\\n\\nWe show the network structure entropy distribution in Fig. 13.\\n\\nExploration on Imbalanced Data\\n\\nImbalanced distribution in data is another crucial limitation in the task of MGTs detection, which is similar to the low resource limitation. It is imaginable that, with the development of generation technology, MGTs will overwhelmingly dominate low-quality articles since they are easier and faster to generate than human writing. The detection model will face training resources with MGTs as the main part and HWTs as the small part. We test the current models in the imbalanced limitation and find the dramatic decline in accuracy when the ratio of HWTs is less than 30%, as shown in the Fig. 14.\\n\\nThe test is based on the 10% GROVER dataset.\\n\\nFigure 14: Model comparison results on DL dataset with 9 different human-generated text portions. All models show poor performance at low HWTs ratios. With a percentage of HWTs of 0.1 (only 100 HWTs in the training set in this case), most of the models have an accuracy below 50%, which performance is close to random and reflects intolerance for extreme cases. Besides, we find that a high proportion of HWTs also cause a decrease in F1 score to some extent.\\n\\nRelated Work: Graph-based Text Representation\\n\\nGraph-of-Words (GoW) Model (Turney, 2002; Mihalcea and Tarau, 2004) is a type graph representation method in which each document is represented by a graph, whose nodes correspond to terms and edges capture co-occurrence relationships between terms. Using GoW, keywords can be extracted by retaining the document graph (Turney, 2002). Thus, graph representation is sensible to apply in tasks like information retrieval (Blanco and Lioma, 2011), categorization (Malliaros and Skianis, 2015) and sentiment classification tasks (Huang and Carley, 2019; Hou et al., 2021).\\n\\nMost models enhance classification or detection performance by combining graph representation with neural networks. Text-GCN (Yao et al., 2019) first builds a single large graph for the whole corpus, followed by Tensor-GCN (Liu et al., 2020) with tensor representation. Also, the relation between words varies, and should be treated as different edges. COO matches keywords PLM embedding to nodes and sentence representation, considers dealing inner- and inter-sentence relation differently in GCN, and merges the structure graph and flat sequence representation to predict accurately.\"}"}
