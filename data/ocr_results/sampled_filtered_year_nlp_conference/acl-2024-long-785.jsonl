{"id": "acl-2024-long-785", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CausalGym: Benchmarking causal interpretability methods on linguistic tasks\\n\\nAryaman Arora Dan Jurafsky Christopher Potts Stanford University {aryamana,jurafsky,cgpotts}@stanford.edu\\n\\nAbstract\\n\\nLanguage models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behaviour. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M\u20136.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler\u2013gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.\\n\\nhttps://github.com/aryamanarora/causalgym\\n\\n1 Introduction\\n\\nLanguage models have found increasing use as tools for psycholinguistic investigation\u2014to model word surprisal (Smith and Levy, 2013; Goodkind and Bicknell, 2018; Wilcox et al., 2023a; Shain et al., 2024, inter alia), graded grammaticality judgements (Hu et al., 2024), and, broadly, human language processing (Futrell et al., 2019; Warstadt and Bowman, 2022; Wilcox et al., 2023b). To benchmark the linguistic competence of LMs, computational psycholinguists have created targeted syntactic evaluation benchmarks, which feature minimally-different pairs of sentences differing in grammaticality; success is measured by whether\\n\\n1. Minimal pairs\\n   b The man near him \u2192 is (yb)\\n   s The men near him \u2192 is are (ys)\\n\\n2. Intervention\\n   The men near him The man near him\\n   ev f*(b, s)\\n\\n3. Evaluation\\n   is are \u2192 is are\\n   Causal effect\\n\\nFigure 1: The CausalGym pipeline: (1) take an input minimal pair (b, s) exhibiting a linguistic alternation that affects next-token predictions (yb, ys); (2) intervene on the base forward pass using a pre-defined intervention function that operates on aligned representations from both inputs; (3) check how this intervention affected the next-token prediction probabilities. In aggregate, such interventions assess the causal role of the intervened representation on the model's behaviour.\"}"}
{"id": "acl-2024-long-785", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Methods for finding such features, and even modifying activations in feature subspaces to causally influence model behaviour, have proliferated, including probing (Ettinger et al., 2016; Adi et al., 2017), distributed alignment search (DAS; Geiger et al., 2023b), and difference-in-means (Marks and Tegmark, 2023).\\n\\nPsycholinguistics and interpretability have complementary needs: thus far, psycholinguists have evaluated LMs on extensive benchmarks but neglected understanding their internal mechanisms, whereas interpretability methods have only been evaluated on one-off datasets and sorely need more diverse benchmarks. Thus, we introduce CausalGym (Figure 1). We adapt linguistic tasks from SyntaxGym (Gauthier et al., 2020) to benchmark interpretability methods on their ability to find linear features in LMs that, when subject to intervention, causally influence linguistic behaviours. We study the pythia family of models (Biderman et al., 2023), finding that DAS is the most efficacious method. However, our investigation corroborates earlier findings that DAS is powerful enough to make the model produce arbitrary input\u2013output mappings (Wu et al., 2023). To address this, we adapt the notion of control tasks from the probing literature (Hewitt and Liang, 2019), finding that adjusting for performance on the arbitrary mapping task reduces the gap between DAS and other methods.\\n\\nWe further investigate how LMs learn two difficult linguistic behaviours during training: filler\u2013gap extraction and negative polarity item licensing. We find that the causal mechanisms require multi-step movement of information, and that they emerge in discrete stages (not gradually) early in training.\\n\\n2 Related work\\n\\nTargeted syntactic evaluation.\\n\\nBenchmarks adhering to this paradigm include SyntaxGym (Gauthier et al., 2020; Hu et al., 2020), BLiMP (Warstadt et al., 2020), and several earlier works (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019). We use the SyntaxGym evaluation sets over BLiMP even though the latter has many more examples, because we require minimal pairs that are grammatical sentences alternating along a specific feature (e.g. number). Such pairs can be constructed templateically using SyntaxGym's format.\\n\\nInterventional interpretability.\\n\\nInterventions are the workhorse of causal inference (Pearl, 2009), and have thus been adopted by recent work in interpretability for establishing the causal role of neural network components in implementing certain behaviours (Vig et al., 2020; Geiger et al., 2021, 2022, 2023a; Meng et al., 2022; Chan et al., 2022; Goldowsky-Dill et al., 2023), particularly linguistic ones like coreference and gender bias (Lasri et al., 2022; Wang et al., 2023; Hanna et al., 2023; Chintam et al., 2023; Yamakoshi et al., 2023; Hao and Linzen, 2023; Chen et al., 2023; Amini et al., 2023; Guerner et al., 2023). The approach loosely falls under the nascent field of mechanistic interpretability, which seeks to find interpretable mechanisms inside neural networks (Olah, 2022).\\n\\nWe illustrate the interventional paradigm in Figure 1; given a base input $b$ and source input $s$, all interventional approaches take a model-internal component $f$ and replace its output with that of $f^*(b, s)$, which modifies the representation of $b$ using that of $s$. The core idea of intervention is adopted directly from the do-operator used in causal inference; we test the intervention's effect on model output to establish a causal relationship.\\n\\n3 Benchmark\\n\\nTo create CausalGym, we converted the core test suites in SyntaxGym (Gauthier et al., 2020) into templates for generating large numbers of span-aligned minimal pairs, a process we describe below along with our evaluation setup.\\n\\n3.1 Premise\\n\\nEach test suite in SyntaxGym focuses on a single linguistic feature, constructing English-language minimal pairs that minimally adjust that feature to change expectations about how a sentence should continue. A test suite contains several items which share identical settings for irrelevant features, and each item has some conditions which vary only the important feature. All items adhere to the same templatic structure, sharing the same ordering and set of regions (syntactic units). To measure whether models match human expectations, SyntaxGym evaluates the model's surprisal at specific regions between differing conditions. For example, the Subject-Verb Number Agreement (with prepositional phrase) task constructs items consisting of 4 conditions, which set all possible combinations of the number feature on subjects.\"}"}
{"id": "acl-2024-long-785", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SyntaxGym\\n#1\\n#2\\n#3\\n#4\\nintro\\nThe\\nThe\\nThe\\nThe\\nnp_subj\\naut hor\\naut hor\\naut hors\\naut hors\\nprep\\nnear\\nnear\\nnear\\nnear\\nthe\\nthe\\nthe\\nthe\\nprep_np\\nsenat ors\\nsenat or\\nsenat or\\nsenat or\\nmatrix_verb\\nis\\nare\\nare\\nare\\ncontinuation\\ngood\\ngood\\ngood\\ngood\\n\u2192\\n\u2192\\n\u2192\\n\u2192\\nCausalGym\\nintro\\nThe\\nThe\\nThe\\nThe\\nnp_subj\\naut hor\\naut hor\\naut hors\\narchitect\\narchitects\\nprep\\nbehind\\nbehind\\nbehind\\nbehind\\nnear\\nfront of\\nfront of\\nfront of\\nfront of\\nthe\\nthe\\nthe\\nthe\\nprep_np\\nsenators\\nsenator\\nsenator\\nsenator\\nlabel\\nis\\nwere\\nwere\\nwere\\nwas\\nT ypes\\nsingular\\nplural\\nSt ep 1: T ypes\\nSt ep 2: Sample labels\\narchitect\\n\u2192 was\\naut hors\\n\u2192 are\\nSt ep 3: Ot her r egions\\nThe\\nnear\\nthe\\npilots\\nR esult\\nThe\\narchitect\\nnear\\nthe\\npilots\\n\u2192 was\\nThe\\naut hors\\nnear\\nthe\\npilots\\n\u2192 are\\n3.2 Templatising SyntaxGym\\nOur goal is to study how LMs implement mechanisms for converting feature alternations in the input into corresponding alternations in the output\u2014e.g., how does an LM keep track of the number feature on the subject when it needs to output an agreeing verb? In adapting SyntaxGym for this purpose, we must address two issues: (1) to study model mechanisms, we only want grammatical pairs of sentences; and (2) SyntaxGym test suites contain <50 items, while we need many more for training supervised interpretability methods and creating non-overlapping test sets. Thus, we select the two grammatical conditions from each item and simplify the behaviour of interest into an explicit input\u2013output mapping. For example, we recast Subject-Verb Number Agreement (with prepositional phrase) into counterfactual pairs that elicit singular or plural verbs based on the number feature of the subject, and hold everything else (including the distractor) constant:\\n\\n(6) a. The author near the senators \u21d2 is\\n\\nb. The authors near the senators \u21d2 are\\n\\nTo be able to generate many examples for training, we use the aligned regions as slots in a template that we can mix-and-match between items to combinatorially generate pairs, illustrated in Figure 2. We manually removed options (potential choices to fill slots) that would have resulted in questionably grammatical sentences.\\n\\nFor generation using our format, each template has a set of types $T$ which govern the input label variable and the expected next-token prediction label. To generate a counterfactual pair, we first sample two types $t_1, t_2 \\\\sim T$ such that $t_1 \\\\neq t_2$. Then, for the label variable and label, we sample an option of that type $t_1$ (for the first sentence) or $t_2$ (for the second). Finally, for the non-label variable regions, we sample one option and set both sentences to that. In Figure 2, we show the generation process in the bottom panel; types for the label variable and label options are colour-coded.\"}"}
{"id": "acl-2024-long-785", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Agreement | Licensing | Garden path effects | Gross syntactic state | Long-distance Parameters |\\n|-----------|-----------|---------------------|-----------------------|-------------------------|\\n| 0.4       | 0.6       | 0.8                 | 1                     | 3                       |\\n\\nFigure 3: Accuracy of pythia-family models on the CausalGym tasks, grouped by type, with scale. The dashed line is random-chance accuracy (50%).\\n\\n3.3 Tasks\\nCausalGym contains 29 tasks, of which one is novel (agr_gender) and 28 were templatised from SyntaxGym. Of the 33 test suites in the original release of SyntaxGym, we only used tasks from which we could generate paired grammatical sentences (leading us to discard the 2 center embedding tasks), and merged the 6 gendered reflexive licensing tasks into 3 non-gendered ones.\\n\\nWe report task accuracy vs. model scale in Figure 3 to give a sense of how well the models perform the linguistic behaviours we study. Examples of pairs generated for each task are provided in appendix A.\\n\\n3.4 Evaluation\\nAn evaluation sample consists of a base input $b$, source input $s$, ground-truth base label $y_b$, and ground-truth source label $y_s$. For example, the components of (6) are\\n\\n(7) The author near the senators\\n\\n(8) The authors near the senators\\n\\nA successful intervention will take the original LM running on input $b$ and make it predict $y_s$ as the next token. We measure the strength of an intervention by its log odds-ratio.\\n\\nFirst, we select a component $f$, which can be any part of a neural network that outputs a representation, inside the model $p$. When the model is run on input $b$, this component produces a representation we denote $f(b)$. We perform an intervention which replaces the output of $f$ with an output of $f^*$ as in \u00a72. To produce a representation, $f^*$ may modify the base representation with reference to the source representation, and so its output is $f^*(b, s)$.\\n\\nThe intervention results in an intervened language model which we denote informally as $p^{f \u2190 f^*}$. In the framework of causal abstraction (Geiger et al., 2021), if this intervention successfully makes the model behave as if its input was $s$, then the representation at $f$ is causally aligned with the high-level linguistic feature alternating in $b$ and $s$.\\n\\nWe now operationalise a measure of causal effect. Taking the original model $p$, the intervened model $p^{f \u2190 f^*}$, and the evaluation sample, we define the log odds-ratio as:\\n\\n\\\\[\\n\\\\text{Odds}(p, p^{f \u2190 f^*}, \\\\langle b, s, y_b, y_s \\\\rangle) = \\\\log \\\\left( \\\\frac{p(y_b | b)}{p(y_s | b)} \\\\cdot \\\\frac{p^{f \u2190 f^*}(y_s | b, s)}{p^{f \u2190 f^*}(y_b | b, s)} \\\\right)\\n\\\\]\\n\\nwhere a greater log odds-ratio indicates a larger causal effect at that intervention site, and a log odds-ratio of 0 indicates no causal effect. Given an evaluation set $E$, the average log odds-ratio is\\n\\n\\\\[\\n\\\\text{AvgOdds}(p, p^{f \u2190 f^*}, E) = \\\\frac{1}{|E|} \\\\sum_{e \\\\in E} \\\\text{Odds}(p, p^{f \u2190 f^*}, e)\\n\\\\]\\n\\n4 Methods\\nWe briefly describe our choice of $f^*$ and the feature-finding methods that we benchmark in this paper.\\n\\n4.1 Preliminaries\\nIn this paper, we only benchmark interventions along a single feature direction, i.e. one-dimensional distributed interchange intervention (1D DII; Geiger et al., 2023b). DII is an interchange intervention that operates on a non-basis-aligned subspace of the activation space. Formally,\"}"}
{"id": "acl-2024-long-785", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given a feature vector \\\\( a \\\\in \\\\mathbb{R}^n \\\\) and \\\\( f \\\\), 1D DII defines \\\\( f^* \\\\) as:\\n\\n\\\\[\\nf^*(b,s) = f(b) + (f(s)a^\\\\top - f(b)a^\\\\top)a\\n\\\\]\\n\\n(11)\\n\\nAs noted above, when our intervention replaces \\\\( f \\\\) with \\\\( f^* a \\\\), we denote the new model as \\\\( p_f \\\\leftarrow f^* a \\\\).\\n\\nWe fix \\\\( f \\\\) to operate on token-level representations; since \\\\( b \\\\) and \\\\( s \\\\) may have different lengths due to tokenisation, we align representations at the last token of each template region.\\n\\nIn principle, we allow future work to consider other forms of \\\\( f^* \\\\), but 1D DII has two useful properties. Given the linear representation hypothesis and that CausalGym exclusively studies binary linguistic features, 1D DII ought to be sufficiently expressive for controlling model behaviour. Furthermore, probes trained on binary classification tasks operate on a one-dimensional subspace of the representation, and thus we can directly use the weight vector of a probe as the parameter \\\\( a \\\\) in eq. (11)\u2014Tigges et al. (2023) used a similar setup to causally evaluate probes.\\n\\nWe study seven methods, of which four are supervised: distributed alignment search (DAS), linear probing, difference-in-means, and LDA. The other three are unsupervised: PCA, \\\\( k \\\\)-means, and (as a baseline) sampling a random vector. All of these methods provide us a feature direction \\\\( a \\\\) that we use as a constant in eq. (11). For probing and unsupervised methods, we use implementations from scikit-learn (Pedregosa et al., 2011). To train distributed alignment search and run 1D DII, we use the pyvene library (Wu et al., 2024). Further training details are in appendix B.\\n\\nWe formally describe each method below.\\n\\n### 4.2 Definitions\\n\\n**DAS.**\\n\\nGiven a training set \\\\( T \\\\), we learn the intervention direction, potentially distributed across many neurons, that maximises the output probability of the counterfactual label. Formally, we first randomly initialise \\\\( a_{\\\\text{das}} \\\\) and intervene on the model \\\\( p_f \\\\) with it to get \\\\( p_f \\\\leftarrow f^* a_{\\\\text{das}} \\\\). We freeze the model weights and optimise \\\\( a_{\\\\text{das}} \\\\) such that we minimise the cross-entropy loss with the target output \\\\( y_b \\\\):\\n\\n\\\\[\\n\\\\min_{a_{\\\\text{das}}} \\\\left\\\\{ -\\\\sum_{(b,s) \\\\in T} \\\\log p_f \\\\leftarrow f^* a_{\\\\text{das}}(y_s | b,s) \\\\right\\\\}\\n\\\\]\\n\\n(12)\\n\\nThe learned DAS parameters \\\\( a_{\\\\text{das}} \\\\) then define a function \\\\( f^* a_{\\\\text{das}} \\\\) using (11).\\n\\n**Linear probe.**\\n\\nLinear probing classifiers have been the dominant feature-finding method for neural representations of language (Belinkov, 2022). A probe outputs a distribution over classes given a representation \\\\( x \\\\in \\\\mathbb{R}^n \\\\):\\n\\n\\\\[\\nq_{\\\\theta}(y | x) = \\\\text{softmax}(a_{\\\\text{probe}} \\\\cdot f(x) + b)\\n\\\\]\\n\\n(13)\\n\\nWe learn the parameters \\\\( \\\\theta \\\\) of the probe over the base training set examples (so, maximising \\\\( q_{\\\\theta}(y_b | b) \\\\)) using the SAGA solver (Defazio et al., 2014) as implemented in scikit-learn, and the parameters \\\\( a_{\\\\text{probe}} \\\\) define the intervention function \\\\( f^* a_{\\\\text{probe}} \\\\).\\n\\n**Diff-in-means.**\\n\\nThe difference in per-class mean activations has been surprisingly effective for controlling representations (Marks and Tegmark, 2023; Li et al., 2023) and erasing linear features (Belrose et al., 2023; Belrose, 2023). To implement this approach, we take the base input\u2013output pairs \\\\( \\\\langle b,y_b \\\\rangle \\\\) from the training set \\\\( T \\\\), where \\\\( y_b \\\\in \\\\{ y_1, y_2 \\\\} \\\\), and group them by the identity of their labels. Thus, we have \\\\( X_1 = \\\\{ b \\\\in T : y_b = y_1 \\\\} \\\\) and \\\\( X_2 = \\\\{ b \\\\in T : y_b = y_2 \\\\} \\\\). The diff-in-means method is then defined as follows:\\n\\n\\\\[\\na_{\\\\text{mean}} = \\\\frac{1}{|X_1|} \\\\sum_{x \\\\in X_1} f(x) - \\\\frac{1}{|X_2|} \\\\sum_{x \\\\in X_2} f(x)\\n\\\\]\\n\\n(14)\\n\\n\\\\[\\n= \\\\mu_1 - \\\\mu_2\\n\\\\]\\n\\n(15)\\n\\nand as usual \\\\( a_{\\\\text{mean}} \\\\) defines the function \\\\( f^* a_{\\\\text{mean}} \\\\).\\n\\n**Linear discriminant analysis.**\\n\\nLDA assumes that each class is distributed according to a Gaussian and all classes share the same covariance matrix \\\\( \\\\Sigma \\\\).\\n\\nGiven the per-class means \\\\( \\\\mu_1 \\\\) and \\\\( \\\\mu_2 \\\\),\\n\\n\\\\[\\na_{\\\\text{lda}} = \\\\Sigma^{-1}(\\\\mu_1 - \\\\mu_2)\\n\\\\]\\n\\n(16)\\n\\n**Principal component analysis (PCA).**\\n\\nWe intervene along the first principal component, which is a vector \\\\( a_{\\\\text{pca}} \\\\) that maximises the variance in mean-centered activations (denoted \\\\( \\\\tilde{f}(x) \\\\)):\\n\\n\\\\[\\n\\\\max_{a_{\\\\text{pca}}} \\\\left\\\\{ \\\\sum_{x \\\\in X_1 \\\\cup X_2} (\\\\tilde{f}(x) \\\\cdot a_{\\\\text{pca}})^2 \\\\right\\\\}\\n\\\\]\\n\\n(17)\\n\\nPCA was previously used to debias gendered word embeddings by Bolukbasi et al. (2016).\\n\\n**\\\\( k \\\\)-means.**\\n\\nWe use 2-means and learn a clustering of activations into two sets \\\\( S_1, S_2 \\\\) that minimises the variance of the activations relative to their class centroids \\\\( \\\\mu_1, \\\\mu_2 \\\\). Our feature direction is\\n\\n\\\\[\\na_{\\\\text{kmeans}} = \\\\mu_1 - \\\\mu_2\\n\\\\]\\n\\n(18)\"}"}
{"id": "acl-2024-long-785", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Overall odds-ratio (\u00a75.1) and selectivity (\u00a75.2) of each feature-finding method averaged over all tasks in CausalGym. We also report average task accuracy, which increases with scale. For models larger than pythia-70m, we report the better of two probes trained with different hyperparameters (appendix C).\\n\\n5 Experiments\\n\\nWe perform all experiments on the pythia model series (Biderman et al., 2023), which includes 10 models ranging from 14 million to 12 billion parameters, all trained on the same data in the same order. This model series allows us to study how feature representations change with scale and training data size in a controlled manner\u2014all models were trained on the same data in the same order, and checkpoints are provided.\\n\\n5.1 Measuring causal efficacy\\n\\nThe Transformer (Vaswani et al., 2017) is organised around the residual stream (Elhage et al., 2021), which each attention and MLP layer reads from and additively writes to. The residual stream is an information bottleneck; information from the input must be present at some token in every layer's residual stream in order to reach the next layer and ultimately affect the output.\\n\\nTherefore, given a feature present in the input and influencing the model output, we should be able to find a causally-efficacious subspace encoding that feature in at least one token position in every layer. If the feature is binary (such as the ones we study in CausalGym) and processed by a single mechanism in the model, then 1D DII should be sufficient for this. Conversely, if 1D DII using a given method fails to produce causal effect, either the method is poor, or the feature is processed by multiple mechanisms in the model or not represented linearly at all.\\n\\nThus, for each task in CausalGym, we take the function of interest $f$ to be the state of the residual stream after the operation of a Transformer layer $l \\\\in L$ at the last token of a particular region $r \\\\in R$.\\n\\nFor notational convenience, we denote this function as $f(l, r)$. We learn 1D DII using each method $m$ for every such function. We use a trainset $T$ of 400 examples for each benchmark task, and evaluate on a non-overlapping set $E$ of 100 examples. Each such experiment results in an intervened model that we denote $p_f(l, r) \u2190 f^\u2217a_m$. To compute the overall log odds-ratio for a feature-finding method on a particular model on a single task, we take the maximum of the average odds-ratio (\u00a73.4) over regions at a specific layer, and then average over all layers:\\n\\n$$\\\\text{OverallOdds}(p, m, E) = \\\\frac{1}{|L|} \\\\sum_{l \\\\in L} \\\\left( \\\\max_{r \\\\in R} \\\\left( \\\\text{AvgOdds}(p, p_f(l, r) \u2190 f^\u2217a_m, E) \\\\right) \\\\right)$$\\n\\nThis metric rewards a method for finding a highly causally-efficacious region in every layer.\\n\\n5.2 Controlling for expressivity\\n\\nDAS is the only method with a causal training objective. Other methods do not optimise for, or even have access to, downstream model behaviour. Wu et al. (2023) found that a variant of DAS achieves substantial causal effect even on a randomly-initialised model or with irrelevant next-token labels, both settings where no causal mechanism should exist. How much of the causal effect found by DAS is due to its expressivity? Research on probing has faced a similar concern: to what extent is a probe's accuracy due to its expressivity rather than any aspect of the representation being studied? Hewitt and Liang (2019) propose comparing to accuracy on a control task that requires memorising an input-to-label mapping.\\n\\nWe adapt this notion to CausalGym, introducing control tasks where the next-token labels $y_b, y_s$ are mapped to the arbitrary tokens '_dog' and '_give'.\\n\\nFurther training details are given in appendix B, and we report hyperparameter tuning experiments on a dev set in appendix C.\"}"}
{"id": "acl-2024-long-785", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The athlete that loved the ministers has landed.\\n\\n| Checkpoint | Task | Accuracy |\\n|------------|------|----------|\\n| 0          |      |          |\\n| 5          |      |          |\\n| 10         |      |          |\\n| 15         |      |          |\\n\\nFigure 4: Odds-ratio for checkpoints of pythia-1b on the task npi_any_subj_relc, plotted at every layer and template region. The y-axis is labelled with an example pair of sentences. The plot titles are labelled with the checkpoint and task accuracy. Darker regions indicate a token in a specific layer where causal effect was high.\\n\\nMy friend reported that the uncle forged the painting with the help of.\\n\\nFigure 5: Odds-ratio for each layer and region using DAS and probing on pythia-1b, on two tasks. While preserving the class partitioning.\\n\\n2 For example, on the gender-agreement task agr_gender, we replace the label '_he' with '_dog' and '_she' with '_give'. We define selectivity for each method by taking the difference between odds-ratios on the original task and the control task for each f, and then compute the overall odds-ratio as in eq. (19).\\n\\n5.3 Results We summarise the results for each method in Table 1 by reporting overall odds-ratio and selectivity averaged over all tasks for each model. For a breakdown, see appendices E.1 and E.2.\\n\\nWe find that DAS consistently finds the most causally-efficacious features. The second-best method is probing, followed by difference-in-means. The unsupervised methods PCA and k-means are considerably worse. Despite supervision, LDA barely outperforms random features.\\n\\n2 The input-to-label mapping in CausalGym tasks is dependent on the input token types, so we cannot exactly replicate Hewitt and Liang. The setup we instead use is from Wu et al. (2023).\\n\\nHowever, DAS is not considerably more selective or (at larger scales) even less selective than probing or diff-in-means; it can perform well on arbitrary input\u2013output mappings. This suggests that its access to the model outputs during training is responsible for much of its advantage.\\n\\n6 Case studies In this section, we use CausalGym to study how LMs learn negative polarity item (NPI) licensing and wh-extraction from prepositional phrases over the course of training using checkpoints of pythia-1b. We first describe the tasks.\\n\\nnpi_any_subj-relc. NPIs are lexemes that can only occur in negative-polarity sentential contexts. In this task, we specifically check whether the NPI any is correctly licensed by a negated subject, giving minimal pairs like\\n\\n(20) No athlete that loved the ministers has landed\\n\\n\u21d2 any\\n\\n(21) The athlete that loved the ministers has landed\\n\\n\u21d2 some\\n\\nIn (21), where there is no negation at the sentence level, it would be ungrammatical to continue the sentence with the NPI any.\\n\\nfiller_gap_subj. Filler\u2013gap dependencies in English occur when interrogatives are extracted out of and placed in front of a clause. The position from which they are extracted must remain empty. The task filler_gap_subj requires an LM to apply this rule when extracting from a distant prepositional phrase, e.g.\\n\\n(22) My friend reported that the uncle forged the painting with the help of \u21d2him\"}"}
{"id": "acl-2024-long-785", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"My friend reported who the uncle forged the painting with the help of \u21d2.\\n\\nIn (23), it would be ungrammatical for the preposition to have an explicit object since who was extracted from that position, leaving behind a gap.\\n\\nWe use the experimental setup of \u00a75.1 and plot the average odds-ratio for each region and layer on the final checkpoint of pythia-1b in Figure 5. For both tasks, we find that the input feature crosses over several different positions before arriving at the output position. For example, in the NPI mechanism (Figure 5b), the negation feature is moved to the complementiser that in the early layer, into the auxiliary verb at middle layers, and the main verb in later layers, where its presence is used to predict the NPI any.\\n\\nThe filler\u2013gap mechanism is similarly complex.\\n\\n6.1 Training dynamics\\n\\nTo study how the mechanisms emerge over the course of training, we run the exact same experiments on earlier checkpoints of pythia-1b. In Figure 4, the effect first emerges at the NPI (all but last layer) and the main verb (step 1000), then abruptly the auxiliary becomes important at middle layers and the NPI effect is pushed down to early layers (step 2000), and finally another intermediate locations is added at that (step 3000). The effect is also distributed across multiple regions in the intermediate layers.\\n\\nThis behaviour takes longer to learn than NPI licensing (Figure 6). The mechanism emerges in two stages: at step 2000, it includes the filler position (that/who), the first determiner the, and the final token. After step 10K, the main verb is added to the mechanism.\\n\\nDiscussion.\\n\\nFor both tasks, the model initially learns to move information directly from the alternating token to the output position. Later in training, intermediate steps are added in the middle layers. DAS finds a greater causal effect across the board, but both methods largely agree on which regions are the most causally efficacious at each layer. Notably, DAS finds causal effect at all timesteps, even when the model has just been initialised; this corroborates Wu et al.\u2019s (2023) findings.\\n\\n7 Conclusion\\n\\nWe introduced CausalGym, a multi-task benchmark of linguistic behaviours for measuring the causal efficacy of interpretability methods. We showed the impressive performance of distributed alignment search, but also adapted a notion of control tasks to causal evaluation to enable fairer comparison of methods. Finally, we studied how causal effect propagates in training on two linguistic tasks: NPI licensing and filler\u2013gap dependency tracking.\\n\\nIn recent years, much effort has been devoted towards developing causally-grounded methods for understanding neural networks. A probe achieving high classification accuracy provides no guarantee that the model actually distinguishes those classes in downstream computations; evaluating probe directions for causal effect is an intuitive test for whether they reflect features that the model uses downstream. Overall, while methods may come and go, we believe the causal evaluation paradigm will continue to be useful for the field.\\n\\nA major motivation for releasing CausalGym is to encourage computational psycholinguists to move beyond studying the input\u2013output behaviours of LMs. Our case studies in \u00a76 are a basic example of the analysis that new methods permit. Ultimately, understanding how LMs learn linguistic behaviours may offer insights into fundamental properties of language (cf. Kallini et al., 2024; Wilcox et al., 2023b).\\n\\nWe hope that CausalGym will encourage comprehensive evaluation of new interpretability methods and spur adoption of the interventional paradigm in computational psycholinguistics.\\n\\nLimitations\\n\\nWhile CausalGym includes a range of linguistic tasks, there are many non-linguistic behaviours on which we may want to use interpretability methods, and so we encourage future research on a greater variety of tasks. In addition, CausalGym includes only English data, and comparable experiments with other languages might yield substantially different results, thereby providing us with a much fuller picture of the causal mechanisms that LMs learn to use. Furthermore, results may differ on other models, since models in the pythia series were trained on the same data in a fixed order; different training data may result in different mechanisms. Finally, justified by the nature of our tasks, we only benchmark methods that operate on one-dimensional linear subspaces; multi-dimensional linear methods as well as non-linear ones await being benchmarked.\"}"}
{"id": "acl-2024-long-785", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"My friend reported that the uncle forged the painting with the help of...\\n\\nFigure 6: Odds-ratio for checkpoints of pythia-1b on the task filler_gap_subj, plotted at every layer and template region.\\n\\nEthics statement\\nInterpretability is a rapidly-advancing field, and our benchmark results render us optimistic about our ability to someday understand the mechanisms inside complex neural networks. However, successful interpretability methods could be used to justify deployment of language models in high-risk settings (e.g. to autonomously make decisions about human beings) or even manipulate models to produce harmful outputs. Understanding a model does not mean that it is safe to use in every situation, and we caution model deployers and users against uncritical trust in models even if they are found to be interpretable.\\n\\nAcknowledgements\\nWe thank Atticus Geiger, Jing Huang, Harshit Joshi, Jordan Juravsky, Julie Kallini, Chenglei Si, Tristan Thrush, and Zhengxuan Wu for helpful discussion about the project and their comments on the manuscript.\\n\\nReferences\\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2017. Fine-grained analysis of sentence embeddings using auxiliary prediction tasks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France.\\n\\nAfra Amini, Tiago Pimentel, Clara Meister, and Ryan Cotterell. 2023. Naturalistic causal probing for morpho-syntax. Transactions of the Association for Computational Linguistics, 11:384\u2013403.\\n\\nYonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics, 48(1):207\u2013219.\\n\\nNora Belrose. 2023. Diff-in-means concept editing is worst-case optimal. EleutherAI Blog.\\n\\nNora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. 2023. LEACE: Perfect linear concept erasure in closed form. arXiv:2306.03819.\\n\\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Halathan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, ICML 2023, volume 202 of Proceedings of Machine Learning Research, pages 2397\u20132430, Honolulu, Hawaii, USA. PMLR.\\n\\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.\\n\\nLawrence Chan, Adri\u00e0 Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. 2022. Causal scrubbing: A method for rigorously testing interpretability hypotheses. In Alignment Forum.\\n\\nAngelica Chen, Ravid Schwartz-Ziv, Kyunghyun Cho, Matthew L. Leavitt, and Naomi Saphra. 2023. Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs. arXiv:2309.07311.\\n\\nAbhijith Chintam, Rahel Beloch, Willem Zuidema, Michael Hanna, and Oskar van der Wal. 2023. Identifying and adapting transformer-components responsible for gender bias in an English language model. In Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 379\u2013394, Singapore. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-785", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-785", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jennifer Hu, Kyle Mahowald, Gary Lupyan, Anna Ivanova, and Roger Levy. 2024. Language models align with human judgments on key grammatical constructions. arXiv:2402.01676.\\n\\nJulie Kallini, Isabel Papadimitriou, Richard Futrell, Kyle Mahowald, and Christopher Potts. 2024. Mission: Impossible language models. arXiv:2401.06416.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA.\\n\\nKarim Lasri, Tiago Pimentel, Alessandro Lenci, Thierry Poibeau, and Ryan Cotterell. 2022. Probing for the usage of grammatical number. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8818\u20138831, Dublin, Ireland. Association for Computational Linguistics.\\n\\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inference-time intervention: Eliciting truthful answers from a language model. In Advances in Neural Information Processing Systems, volume 36.\\n\\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitve dependencies. Transactions of the Association for Computational Linguistics, 4:521\u2013535.\\n\\nSamuel Marks and Max Tegmark. 2023. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. arXiv:2310.06824.\\n\\nRebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192\u20131202, Brussels, Belgium. Association for Computational Linguistics.\\n\\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems, volume 35, pages 17359\u201317372. Curran Associates, Inc.\\n\\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 746\u2013751, Atlanta, Georgia. Association for Computational Linguistics.\\n\\nNeel Nanda, Andrew Lee, and Martin Wattenberg. 2023. Emergent linear representations in world models of self-supervised sequence models. In Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 16\u201330, Singapore. Association for Computational Linguistics.\\n\\nChris Olah. 2022. Mechanistic interpretability, variables, and the importance of interpretable bases. Transformer Circuits Thread.\\n\\nKiho Park, Yo Joong Choe, and Victor Veitch. 2023. The linear representation hypothesis and the geometry of large language models. arXiv:2311.03658.\\n\\nJudea Pearl. 2009. Causality: Models, Reasoning, and Inference, 2nd edition. Cambridge University Press.\\n\\nFabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research, 12:2825\u20132830.\\n\\nCory Shain, Clara Meister, Tiago Pimentel, Ryan Cotterell, and Roger Levy. 2024. Large-scale evidence for logarithmic effects of word predictability on reading time. Proceedings of the National Academy of Sciences. To appear.\\n\\nNathaniel J. Smith and Roger Levy. 2013. The effect of word predictability on reading time is logarithmic. Cognition, 128(3):302\u2013319.\\n\\nCurt Tigges, Oskar John Hollinsworth, Atticus Geiger, and Neel Nanda. 2023. Linear representations of sentiment in large language models. arXiv:2310.15154.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30, pages 5998\u20136008. Curran Associates, Inc.\\n\\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. In Advances in Neural Information Processing Systems, volume 33, pages 12388\u201312401. Curran Associates, Inc.\\n\\nKevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2023. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda.\\n\\nAlex Warstadt and Samuel R. Bowman. 2022. What artificial neural networks can tell us about human language acquisition. Algebraic Structures in Natural Language, pages 17\u201360.\"}"}
{"id": "acl-2024-long-785", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bowman. 2020. BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Association for Computational Linguistics, 8:377\u2013392.\\n\\nEthan Wilcox, Clara Meister, Ryan Cotterell, and Tiago Pimentel. 2023a. Language model quality correlates with psychometric predictive power in multiple languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7503\u20137511, Singapore. Association for Computational Linguistics.\\n\\nEthan Gotlieb Wilcox, Richard Futrell, and Roger Levy. 2023b. Using computational models to test syntactic learnability. Linguistic Inquiry, pages 1\u201344.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nZhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Noah D. Goodman, Christopher D. Manning, and Christopher Potts. 2024. pyvene: A library for understanding and improving PyTorch models via interventions. Under review.\\n\\nZhengxuan Wu, Atticus Geiger, Christopher Potts, and Noah D. Goodman. 2023. Interpretability at scale: Identifying causal mechanisms in Alpaca. In Advances in Neural Information Processing Systems, volume 36.\\n\\nTakateru Yamakoshi, James McClelland, Adele Goldberg, and Robert Hawkins. 2023. Causal interventions expose implicit situation models for commonsense language understanding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13265\u201313293, Toronto, Canada. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-785", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Tasks\\n\\nTask Example\\n\\nAgreement (4)\\n\\nagr_gender [John/Jane] walked because [he/she]\\n\\nagr_sv_num_subj-relc The [guard/guards] that hated the manager [is/are]\\n\\nagr_sv_num_obj-relc The [guard/guards] that the customers hated [is/are]\\n\\nagr_sv_num_pp The [guard/guards] behind the managers [is/are]\\n\\nLicensing (7)\\n\\nagr_refl_num_subj-relc The [farmer/farmers] that loved the actors embarrassed [himself/themselves]\\n\\nagr_refl_num_obj-relc The [farmer/farmers] that the actors loved embarrassed [himself/themselves]\\n\\nagr_refl_num_pp The [farmer/farmers] behind the actors embarrassed [himself/themselves]\\n\\nnpi_any_subj-relc [No/The] consultant that has helped the taxi driver has shown [any/some]\\n\\nnpi_any_obj-relc [No/The] consultant that the taxi driver has helped has shown [any/some]\\n\\nnpi_ever_subj-relc [No/The] consultant that has helped the taxi driver has [ever/never]\\n\\nnpi_ever_obj-relc [No/The] consultant that the taxi driver has helped has [ever/never]\\n\\nGarden path effects (6)\\n\\ngarden_mvrr The infant [who was/\u2205] brought the sandwich from the kitchen [by/].\\n\\ngarden_mvrr_mod The infant [who was/\u2205] brought the sandwich from the kitchen with a new microwave [by/].\\n\\ngarden_npz_obj While the students dressed [\u2205,] the comedian [was/for]\\n\\ngarden_npz_obj_mod While the students dressed [\u2205,] the comedian who told bad jokes [was/for]\\n\\ngarden_npz_v-trans As the criminal [slept/shot] the woman [was/for]\\n\\ngarden_npz_v-trans_mod As the criminal [slept/shot] the woman who told bad jokes [was/for]\\n\\nGross syntactic state (4)\\n\\ngss_subord [While the/The] lawyers lost the plans [they/].\\n\\ngss_subord_subj-relc [While the/The] lawyers who wore white lab jackets studied the book that described several advances in cancer therapy [\u2205,].\\n\\ngss_subord_obj-relc [While the/The] lawyers who the spy had contacted repeatedly studied the book that colleagues had written on cancer therapy [\u2205,].\\n\\ngss_subord_pp [While the/The] lawyers in a long white lab jacket studied the book about several recent advances in cancer therapy [\u2205,].\\n\\nLong-distance dependencies (8)\\n\\ncleft What the young man [did/ate] was [make/for]\\n\\ncleft_mod What the young man [did/ate] after the ingredients had been bought from the store was [make/for]\\n\\nfiller_gap_embed_3 I know [that/what] the mother said the friend remarked the park attendant reported your friend sent [him/].\\n\\nfiller_gap_embed_4 I know [that/what] the mother said the friend remarked the park attendant reported the cop thinks your friend sent [him/].\\n\\nfiller_gap_hierarchy The fact that the brother said [that/who] the friend trusted [the/was]\\n\\nfiller_gap_obj I know [that/what] the uncle grabbed [him/].\\n\\nfiller_gap_pp I know [that/what] the uncle grabbed food in front of [him/].\\n\\nfiller_gap_subj I know [that/who] the uncle grabbed food in front of [him/].\\n\\nB Training and evaluation details\\n\\nWe load models using the HuggingFace transformers (Wolf et al., 2020) library. Up to size 410m we load weights in float32 precision, 1b in bfloat16 precision, and larger models in float16 precision.\\n\\nOur training set starts with 200 examples sampled according to the scheme in \u00a73.2. We then double the size of the set (400) by swapping the base and source inputs/labels and adding these to the training set; including both directions of the intervention makes the comparison fairer between DAS and the other non-paired methods, and also ensures a perfect balance between labels.\\n\\nThe evaluation set consists of 50 examples sampled the same way (effectively 100), except we resample in case we encounter a sentence already present in the training set. Thus, there is no overlap with the training set. We evaluate all metrics (odds-ratio and probe classification accuracy) on this set.\\n\\nWe train DAS for one epoch with a batch size of 4, resulting in 100 backpropagation steps. We use the Adam optimiser (Kingma and Ba, 2015) and a linear learning rate schedule, with the first 10% of training being a warmup from 0 to the learning rate, followed by the learning rate linearly decaying to 0 for the rest of training. The scheduling and optimiser is identical to Wu et al. (2023). We use a learning...\"}"}
{"id": "acl-2024-long-785", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The rate of 5 \\\\cdot 10^{-3}, which is higher than previous work (usually 10^{-3}) due to the small training set size; see appendix C for hyperparameter tuning experiments which justify this choice.\\n\\nTo run our experiments, we used a cluster of NVIDIA A100 (40 GB) and NVIDIA RTX 6000 Ada Generation GPUs. The total runtime for the benchmarking experiments in \u00a75 was \\\\sim 400 hours, and for the case studies in \u00a76 it was \\\\sim 25 hours.\\n\\n**C Hyperparameter tuning**\\n\\nTo ensure fair comparison, we tuned hyperparameters for DAS, probes, and PCA on a dev set, sampled the same way as the eval set (non-overlapping with train set) but with a different random seed. We train on all tasks in CausalGym and report the average odds-ratio following the same evaluation setup as in \u00a75.1. We studied only the three smallest models (pythia-14m, 31m, 70m) due to the large number of experiments needed. Specifically, we tune the learning rate for DAS, the type of regularisation and whether or not to include a bias term in the logit for probes, and averaging of the first \\\\( c \\\\) components for PCA. We report the overall log odds-ratio for various hyperparameter settings in Table 2. These experiments were run on a NVIDIA RTX 6000 Ada Generation. The total runtime was \\\\sim 25 hours.\\n\\nFor probing (Table 2a), we found that including a bias term and using only \\\\( L_2 \\\\) regularisation with the saga solver delivers the best performance. However, the setting of the weight coefficient \\\\( \\\\lambda \\\\) on the regularisation term in the loss depends on the model. The main architectural difference between these three models is the hidden dimension size, so we suspect that the optimal choice for \\\\( \\\\lambda \\\\) depends on that. Roughly extrapolating the observed trend, in our main experiments we check \\\\( \\\\lambda = \\\\{10^4, 10^5\\\\} \\\\) for pythia-160m and 410m, \\\\( \\\\lambda = \\\\{10^5, 10^6\\\\} \\\\) for pythia-1b, 1.4b, and 2.8b, and \\\\( \\\\lambda = \\\\{10^6, 10^7\\\\} \\\\) for pythia-6.9b. As for why \\\\( L_2 \\\\) regularisation increases causal efficacy, we note that Hewitt and Liang (2019) found that it also increases probe selectivity\u2014we leave this as an open question for future work.\\n\\nFor PCA (Table 2b), we found that averaging the first \\\\( c \\\\) components did not improve performance over just using the first component; thus, we used just the first PCA component in our main-text experiments.\\n\\nFor DAS (Table 2c), we found that using the learning rate suggested by Wu et al. (2023), 10^{-3}, understated performance and a higher learning rate did not result in any apparent training instability. However, our experimental setup is quite different (smaller training set, no learned boundary, greater variety of model scales). We did not find any consistent differences or trends with model scale between learning rates of 5 \\\\cdot 10^{-3} and 10^{-2}, so we used the former for all experiments.\\n\\n**D Data and licensing**\\n\\nWe use the original test suites from SyntaxGym which were described in Hu et al. (2020). These were released under the MIT License, and our data release will also use the MIT license for compatibility.\\n\\n---\\n\\n3 Cf. Tigges et al. (2023), who did not include a bias term in their causal evaluation of probing.\"}"}
{"id": "acl-2024-long-785", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Model Probe\\n\\n| Component | \u03bb       | pythia-14m | pythia-31m | pythia-70m |\\n|-----------|---------|------------|------------|------------|\\n|           |         | 100 101 102 103 104 | 100 101 102 103 104 | 100 101 102 103 104 |\\n| No reg., no int. | 0.80 (d= 128) | 0.85 | 0.38 0.21 0.00 | 0.41 0.22 0.00 |\\n| L1, no int. | 0.93 0.55 0.08 | 1.07 1.15 1.08 | 1.09 1.18 1.15 1.07 1.05 |\\n| L1 + L2, no int. | 0.93 0.55 0.08 | 1.45 0.99 0.14 | 1.42 0.93 0.14 |\\n\\n### pythia-14m\\n\\n| LR Step | odds-ratio |\\n|---------|------------|\\n| 0       | 10^{-3} 0.06 0.37 1.01 1.48 1.63 |\\n| 25      | 5\u00b710^{-3} 0.04 2.53 3.58 3.82 3.91 |\\n| 50      | 10^{-2} 0.04 3.17 3.72 3.95 4.02 |\\n| 75      | 10^{-3} 0.02 2.25 4.69 5.42 5.57 |\\n| 99      | 10^{-3} 0.02 7.21 7.48 7.54 7.55 |\\n| 10       | 10^{-2} 0.03 6.92 7.37 7.66 7.75 |\\n\\n### pythia-31m\\n\\n| LR Step | odds-ratio |\\n|---------|------------|\\n| 0       | 10^{-3} 0.04 1.09 2.83 3.64 3.83 |\\n| 25      | 5\u00b710^{-3} 0.04 5.19 5.78 6.00 6.04 |\\n| 50      | 10^{-2} 0.03 5.05 5.44 5.77 5.87 |\\n| 75      | 10^{-3} 0.02 2.25 4.69 5.42 5.57 |\\n| 99      | 10^{-3} 0.02 7.21 7.48 7.54 7.55 |\\n| 10       | 10^{-2} 0.03 6.92 7.37 7.66 7.75 |\\n\\n### pythia-70m\\n\\n| LR Step | odds-ratio |\\n|---------|------------|\\n| 0       | 10^{-3} 0.02 2.25 4.69 5.42 5.57 |\\n| 25      | 5\u00b710^{-3} 0.02 7.21 7.48 7.54 7.55 |\\n| 50      | 10^{-2} 0.03 6.92 7.37 7.66 7.75 |\\n| 75      | 10^{-3} 0.02 2.25 4.69 5.42 5.57 |\\n| 99      | 10^{-3} 0.02 7.21 7.48 7.54 7.55 |\\n| 10       | 10^{-2} 0.03 6.92 7.37 7.66 7.75 |\\n\\n### Overall odds-ratio across variants of PCA, averaging the first \\\\( c \\\\) components.\\n\\n### Overall odds-ratio across various hyperparameter settings for probes.\\n\\n### E. Detailed odds-ratio results\\n\\nIn these comprehensive results, we include an additional method: vanilla interchange intervention. Instead of as in eq. (11), vanilla intervention defines \\\\( f^* \\\\) as\\n\\n\\\\[\\nf^*_{\\\\text{vanilla}}(b, s) = f(s)\\n\\\\]\\n\\ni.e. it entirely replaces the activation with that of the source input. This is equivalent to \\\\( n \\\\)-dimensional DII where \\\\( f(s) \\\\in \\\\mathbb{R}^n \\\\), and is a significantly more expressive intervention than any methods we tested. We include it as a non-learned baseline.\\n\\n### E. Per-layer\\n\\n| Layer | odds-ratio |\\n|-------|------------|\\n| das   | kmeans     | lda | mean | pca | probe | random | vanilla |\\n|       |     |     |     |     |     |        |        |\\n\\n### Figure 7: Average odds-ratio per layer and model across all tasks in CausalGym.\"}"}
{"id": "acl-2024-long-785", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Rows in gray indicate tasks where the model achieves <60% accuracy.\\n\\n| Task                  | Task Acc. | Feature-finding methods | Vanilla | DAS Probe Mean | PCA k-means | LDA | Rand. agr_gender |\\n|-----------------------|-----------|--------------------------|---------|----------------|-------------|-----|------------------|\\n| agr_gender            | 0.58      |                          | 0.32    | 0.94           | 0.49        | 0.35 | 0.36             |\\n| agr_sv_num_subj-relc  | 0.61      |                          | 2.50    | 1.74           | 1.48        | 0.14 | 0.08             |\\n| agr_sv_num_obj-relc   | 0.79      |                          | 2.03    | 2.02           | 2.05        | 0.30 | 0.32             |\\n| agr_sv_num_pp         | 0.77      |                          | 3.47    | 3.15           | 2.85        | 0.36 | 0.14             |\\n| agr_refl_num_subj-relc| 0.78      |                          | 2.39    | 2.18           | 1.79        | 0.17 | 0.13             |\\n| agr_refl_num_obj-relc | 0.72      |                          | 1.79    | 1.46           | 1.18        | 0.12 | 0.10             |\\n| agr_refl_num_pp       | 0.83      |                          | 2.56    | 2.14           | 1.57        | 0.20 | 0.14             |\\n| npi_any_subj-relc     | 0.56      |                          | 5.62    | 0.64           | 0.67        | 0.41 | 0.41             |\\n| npi_any_obj-relc      | 0.57      |                          | 5.27    | 0.54           | 0.56        | 0.37 | 0.36             |\\n| npi_ever_subj-relc    | 0.38      |                          | 5.50    | 0.10           | 0.10        | 0.20 | 0.19             |\\n| npi_ever_obj-relc     | 0.41      |                          | 5.07    | 0.14           | 0.14        | 0.25 | 0.25             |\\n| garden_mvrr           | 0.63      |                          | 4.72    | 1.62           | 1.71        | 0.86 | 1.49             |\\n| garden_mvrr_mod       | 0.50      |                          | 3.73    | 1.01           | 1.12        | 0.99 | 1.05             |\\n| garden_npz_obj        | 0.83      |                          | 5.93    | 0.56           | 1.04        | 1.04 | 1.04             |\\n| garden_npz_obj_mod    | 0.66      |                          | 7.55    | 0.21           | 0.20        | 0.23 | 0.23             |\\n| garden_npz_v-trans    | 0.46      |                          | 2.32    | 0.49           | 0.45        | 0.05 | 0.06             |\\n| garden_npz_v-trans_mod| 0.50      |                          | 0.64    | 0.08           | 0.05        | 0.06 | 0.02             |\\n| gss_subord            | 0.72      |                          | 4.38    | 3.53           | 2.37        | 1.92 | 2.01             |\\n| gss_subord_subj-relc  | 0.69      |                          | 4.70    | 0.99           | 0.93        | 0.93 | 0.10             |\\n| gss_subord_obj-relc   | 0.68      |                          | 5.10    | 1.33           | 1.27        | 1.25 | 1.27             |\\n| gss_subord_pp         | 0.84      |                          | 6.80    | 1.07           | 0.96        | 0.93 | 0.96             |\\n| cleft                 | 0.50      |                          | 7.89    | 2.30           | 1.73        | 0.45 | 0.52             |\\n| cleft_mod             | 0.50      |                          | 1.74    | 0.06           | 0.06        | 0.07 | 0.02             |\\n| filler_gap_embed_3    | 0.55      |                          | 3.54    | 0.46           | 0.50        | 0.21 | 0.21             |\\n| filler_gap_embed_4    | 0.52      |                          | 3.23    | 0.32           | 0.30        | 0.12 | 0.12             |\\n| filler_gap_hierarchy  | 0.50      |                          | 3.79    | 1.22           | 1.23        | 0.61 | 0.61             |\\n| filler_gap_obj        | 0.80      |                          | 5.72    | 2.54           | 2.46        | 1.24 | 1.28             |\\n| filler_gap_pp         | 0.54      |                          | 3.85    | 0.70           | 0.65        | 0.33 | 0.31             |\\n| filler_gap_subj       | 0.49      |                          | 2.15    | 0.17           | 0.13        | 0.03 | 0.02             |\\n\\nAverage 0.62\\n\\nTable 3: pythia-14m\\nTable 4: pythia-14m (selectivity)\"}"}
{"id": "acl-2024-long-785", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task          | Task Acc. | Feature-finding methods | Vanilla | DAS | Probe | Mean | PCA | \\\\(k\\\\)-means | LDA | Rand. |\\n|---------------|-----------|-------------------------|---------|-----|-------|------|-----|-------------|-----|-------|\\n|              |           |                         |         |     |       |      |     |             |     |       |\\n| agr_gender   | 0.85      |                         | 2.50    | 1.74| 0.52  | 0.13 | 0.01| 2.04        |     |       |\\n| agr_sv_num_subj-relc | 0.85    |                         | 4.56    | 3.88| 2.84  | 0.23 | 0.20| 3.93        |     |       |\\n| agr_sv_num_obj-relc | 0.94    |                         | 4.44    | 3.95| 3.42  | 0.26 | 0.12| 4.02        |     |       |\\n| agr_sv_num_pp | 0.77      |                         | 3.62    | 3.28| 2.37  | 0.32 | 0.18| 3.31        |     |       |\\n| agr_refl_num_subj-relc | 0.87    |                         | 3.84    | 3.50| 2.54  | 0.17 | 0.13| 3.68        |     |       |\\n| agr_refl_num_obj-relc | 0.88    |                         | 3.66    | 3.47| 2.57  | 0.20 | 0.17| 3.68        |     |       |\\n| agr_refl_num_pp | 0.87      |                         | 4.25    | 3.44| 1.82  | 0.20 | 0.14| 3.85        |     |       |\\n| npi_any_subj-relc | 0.84     |                         | 5.16    | 2.04| 2.01  | 1.05 | 1.05| 2.09        |     |       |\\n| npi_any_obj-relc | 0.86      |                         | 5.72    | 2.10| 2.08  | 1.07 | 1.08| 2.12        |     |       |\\n| npi_ever_subj-relc | 0.84      |                         | 6.09    | 2.22| 2.18  | 1.49 | 1.57| 2.15        |     |       |\\n| npi_ever_obj-relc | 0.90      |                         | 6.39    | 2.34| 2.28  | 1.52 | 1.57| 2.34        |     |       |\\n| garden_mvrr  | 0.53      |                         | 5.89    | 1.79| 1.52  | 1.01 | 1.07| 1.77        |     |       |\\n| garden_mvrr_mod | 0.50      |                         | 7.85    | 1.61| 1.03  | 0.91 | 1.02| 1.64        |     |       |\\n| garden_npz_obj | 0.85      |                         | 9.93    | 2.18| 1.71  | 1.43 | 1.39| 3.29        |     |       |\\n| garden_npz_obj_mod | 0.69      |                         | 9.14    | 2.53| 1.50  | 1.41 | 1.47| 2.41        |     |       |\\n| garden_npz_v-trans | 0.62      |                         | 3.53    | 1.01| 0.79  | 0.07 | 0.08| 1.14        |     |       |\\n| garden_npz_v-trans_mod | 0.51      |                         | 0.70    | 0.04| 0.04  | 0.02 | 0.02| 0.07        |     |       |\\n| gss_subord   | 0.72      |                         | 7.41    | 3.05| 2.84  | 2.76 | 2.81| 4.48        |     |       |\\n| gss_subord_subj-relc | 0.89      |                         | 9.49    | 2.08| 1.52  | 1.42 | 1.41| 2.86        |     |       |\\n| gss_subord_obj-relc | 0.93      |                         | 10.08   | 2.05| 1.63  | 1.55 | 1.60| 2.84        |     |       |\\n| gss_subord_pp | 0.88      |                         | 8.83    | 2.07| 1.61  | 1.54 | 1.59| 3.24        |     |       |\\n| cleft        | 0.63      |                         | 12.54   | 4.30| 3.76  | 0.89 | 1.35| 4.12        |     |       |\\n| cleft_mod    | 0.50      |                         | 3.88    | 0.20| 0.07  | 0.01 | 0.02| 0.05        |     |       |\\n| filler_gap_embed_3 | 0.56   |                         | 3.04    | 0.57| 0.55  | 0.27 | 0.26| 0.59        |     |       |\\n| filler_gap_embed_4 | 0.52    |                         | 2.55    | 0.24| 0.23  | 0.13 | 0.13| 0.27        |     |       |\\n| filler_gap_hierarchy | 0.54    |                         | 6.16    | 2.36| 2.36  | 0.92 | 0.86| 2.38        |     |       |\\n| filler_gap_obj | 0.78      |                         | 8.87    | 4.17| 4.17  | 2.19 | 2.39| 4.14        |     |       |\\n| filler_gap_pp | 0.65      |                         | 4.42    | 1.19| 1.17  | 0.45 | 0.43| 1.13        |     |       |\\n| filler_gap_subj | 0.67      |                         | 4.16    | 1.03| 1.02  | 0.40 | 0.40| 1.03        |     |       |\\n| Average      | 0.74      |                         | 5.82    | 2.22| 1.80  | 0.83 | 0.85| 2.44        |     |       |\\n\\nTable 5: pythia-31m\\n\\nTable 6: pythia-31m (selectivity)\"}"}
{"id": "acl-2024-long-785", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task                  | Feature-finding methods | Vanilla | DAS | Probe | Mean | PCA |\\n|-----------------------|-------------------------|---------|-----|-------|------|-----|\\n|                       | Task Acc.               |         |     |       |      |     |\\n|                       |                         |         |     |       |      |     |\\n| agr_gender            |                         | 0.95    | 3.29| 2.95  | 1.09 | 0.74|\\n|                       |                         |         |     |       |      |     |\\n| agr_sv_num_subj-relc  |                         | 0.97    | 4.77| 4.11  | 3.66 | 0.53|\\n|                       |                         |         |     |       |      |     |\\n| agr_sv_num_obj-relc   |                         | 0.86    | 3.60| 3.38  | 3.36 | 0.49|\\n|                       |                         |         |     |       |      |     |\\n| agr_sv_num_pp         |                         | 1.00    | 5.83| 4.66  | 4.20 | 0.47|\\n|                       |                         |         |     |       |      |     |\\n| agr_refl_num_subj-relc|                         | 0.93    | 5.35| 3.80  | 2.88 | 0.32|\\n|                       |                         |         |     |       |      |     |\\n| agr_refl_num_obj-relc |                         | 0.90    | 3.97| 2.94  | 2.16 | 0.29|\\n|                       |                         |         |     |       |      |     |\\n| agr_refl_num_pp       |                         | 0.89    | 5.13| 3.64  | 2.50 | 0.28|\\n|                       |                         |         |     |       |      |     |\\n| npi_any_subj-relc     |                         | 0.73    | 6.65| 1.77  | 1.83 | 0.97|\\n|                       |                         |         |     |       |      |     |\\n| npi_any_obj-relc      |                         | 0.78    | 7.03| 2.01  | 2.04 | 1.11|\\n|                       |                         |         |     |       |      |     |\\n| npi_ever_subj-relc    |                         | 0.68    | 6.69| 2.58  | 2.70 | 2.21|\\n|                       |                         |         |     |       |      |     |\\n| npi_ever_obj-relc     |                         | 0.84    | 8.18| 3.28  | 3.39 | 3.06|\\n|                       |                         |         |     |       |      |     |\\n| garden_mvrr           |                         | 0.73    | 10.69| 5.16  | 3.19 | 3.13|\\n|                       |                         |         |     |       |      |     |\\n| garden_mvrr_mod       |                         | 0.63    | 11.47| 2.83  | 1.70 | 1.68|\\n|                       |                         |         |     |       |      |     |\\n| garden_npz_obj        |                         | 0.96    | 12.71| 2.90  | 1.63 | 1.63|\\n|                       |                         |         |     |       |      |     |\\n| garden_npz_obj_mod    |                         | 0.91    | 12.97| 1.23  | 0.62 | 0.58|\\n|                       |                         |         |     |       |      |     |\\n| garden_npz_v-trans    |                         | 0.80    | 5.60 | 2.48  | 1.38 | 0.29|\\n|                       |                         |         |     |       |      |     |\\n| garden_npz_v-trans_mod|                         | 0.61    | 2.25 | 0.52  | 0.42 | 0.11|\\n|                       |                         |         |     |       |      |     |\\n| gss_subord            |                         | 0.87    | 15.67| 3.67  | 2.84 | 2.83|\\n|                       |                         |         |     |       |      |     |\\n| gss_subord_subj-relc  |                         | 0.68    | 12.00| 2.93  | 2.12 | 2.10|\\n|                       |                         |         |     |       |      |     |\\n| gss_subord_obj-relc   |                         | 0.77    | 9.03 | 3.07  | 2.31 | 2.30|\\n|                       |                         |         |     |       |      |     |\\n| gss_subord_pp         |                         | 0.88    | 10.79| 2.49  | 2.00 | 1.99|\\n|                       |                         |         |     |       |      |     |\\n| cleft                 |                         | 0.71    | 14.55| 4.24  | 2.13 | 0.55|\\n|                       |                         |         |     |       |      |     |\\n| cleft_mod             |                         | 0.50    | 5.57 | 0.27  | 0.23 | 0.18|\\n|                       |                         |         |     |       |      |     |\\n| filler_gap_embed_3    |                         | 0.50    | 4.48 | 0.48  | 0.46 | 0.20|\\n|                       |                         |         |     |       |      |     |\\n| filler_gap_embed_4    |                         | 0.51    | 4.05 | 0.39  | 0.39 | 0.16|\\n|                       |                         |         |     |       |      |     |\\n| filler_gap_hierarchy  |                         | 0.55    | 7.06 | 2.85  | 2.88 | 1.36|\\n|                       |                         |         |     |       |      |     |\\n| filler_gap_obj        |                         | 0.86    | 9.54 | 4.02  | 3.91 | 2.56|\\n|                       |                         |         |     |       |      |     |\\n| filler_gap_pp         |                         | 0.59    | 5.49 | 1.66  | 1.65 | 0.70|\\n|                       |                         |         |     |       |      |     |\\n| filler_gap_subj       |                         | 0.64    | 5.98 | 1.93  | 1.93 | 0.77|\\n\\nAverage 0.77 2.87 2.86 2.15 1.05 1.09 0.16 0.05 2.77\\n\\nTable 8: pythia-70m (selectivity)\"}"}
{"id": "acl-2024-long-785", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task | Task Acc. | Feature-finding methods | Vanilla | DAS | Probe 0 | Probe 1 | Mean | PCA | k-means | LDA | Rand. |\\n|------|-----------|-------------------------|---------|-----|---------|---------|------|-----|---------|-----|-------|\\n| agr_gender | 0.99 | 5.64 | 3.89 | 2.87 | 1.52 | 0.96 | 0.96 | 0.06 | 0.03 | 4.35 |\\n| agr_sv_num_subj-relc | 0.96 | 4.79 | 4.10 | 3.20 | 2.52 | 0.19 | 0.22 | 0.26 | 0.01 | 4.20 |\\n| agr_sv_num_obj-relc | 0.95 | 4.52 | 4.49 | 4.01 | 3.22 | 0.24 | 0.26 | 0.36 | 0.00 | 4.19 |\\n| agr_sv_num_pp | 0.98 | 5.02 | 4.42 | 3.48 | 2.91 | 0.23 | 0.20 | 0.25 | 0.01 | 4.49 |\\n| agr_refl_num_subj-relc | 0.92 | 4.56 | 3.93 | 2.85 | 1.99 | 0.11 | 0.13 | 0.28 | 0.01 | 3.91 |\\n| agr_refl_num_obj-relc | 0.94 | 4.65 | 3.74 | 2.71 | 1.97 | 0.19 | 0.17 | 0.38 | 0.00 | 3.83 |\\n| agr_refl_num_pp | 0.91 | 3.49 | 3.23 | 2.07 | 1.52 | 0.10 | 0.07 | 0.17 | 0.01 | 3.58 |\\n| npi_any_subj-relc | 0.86 | 8.09 | 2.57 | 2.58 | 2.57 | 1.34 | 1.36 | 0.05 | 0.01 | 2.74 |\\n| npi_any_obj-relc | 0.98 | 9.28 | 3.82 | 3.82 | 3.83 | 1.85 | 1.89 | 0.10 | 0.01 | 3.87 |\\n| npi_ever_subj-relc | 0.82 | 8.59 | 3.88 | 3.90 | 3.92 | 3.81 | 3.98 | 0.09 | 0.01 | 3.69 |\\n| npi_ever_obj-relc | 1.00 | 10.14 | 5.74 | 5.72 | 5.71 | 5.53 | 5.69 | 0.18 | 0.01 | 5.72 |\\n| garden_mvrr | 0.87 | 12.14 | 6.10 | 3.84 | 2.90 | 2.85 | 2.90 | 0.13 | 0.05 | 3.71 |\\n| garden_mvrr_mod | 0.57 | 10.04 | 3.66 | 2.06 | 1.57 | 1.55 | 1.57 | 0.14 | 0.05 | 2.86 |\\n| garden_npz_obj | 0.88 | 12.51 | 2.42 | 1.92 | 1.76 | 1.75 | 1.76 | 0.07 | 0.09 | 3.03 |\\n| garden_npz_obj_mod | 0.89 | 14.14 | 1.56 | 1.31 | 1.30 | 1.30 | 1.30 | 0.15 | 0.03 | 2.51 |\\n| garden_npz_v-trans | 0.72 | 4.59 | 2.63 | 2.34 | 1.48 | 0.18 | 0.18 | 0.03 | 0.01 | 2.46 |\\n| garden_npz_v-trans_mod | 0.66 | 2.23 | 0.97 | 0.69 | 0.53 | 0.12 | 0.13 | 0.02 | 0.01 | 1.21 |\\n| gss_subord | 0.75 | 17.03 | 4.10 | 3.19 | 2.64 | 2.63 | 2.64 | 0.36 | 0.05 | 3.32 |\\n| gss_subord_subj-relc | 0.81 | 8.82 | 1.50 | 1.38 | 1.19 | 1.17 | 1.19 | 0.07 | 0.04 | 2.01 |\\n| gss_subord_obj-relc | 0.87 | 8.66 | 2.20 | 1.99 | 1.82 | 1.81 | 1.82 | 0.08 | 0.07 | 2.50 |\\n| gss_subord_pp | 0.86 | 8.86 | 1.62 | 1.57 | 1.38 | 1.37 | 1.38 | 0.06 | 0.05 | 2.25 |\\n| cleft | 1.00 | 14.41 | 6.08 | 3.89 | 2.44 | 0.42 | 0.43 | 0.03 | 0.00 | 6.99 |\\n| cleft_mod | 0.54 | 6.93 | 0.63 | 0.38 | 0.28 | 0.11 | 0.12 | 0.02 | 0.01 | 0.81 |\\n| filler_gap_embed_3 | 0.50 | 4.72 | 0.12 | 0.13 | 0.13 | 0.09 | 0.09 | 0.01 | 0.00 | 0.19 |\\n| filler_gap_embed_4 | 0.50 | 4.33 | 0.03 | 0.03 | 0.03 | 0.03 | 0.03 | 0.02 | 0.01 | 0.02 |\\n| filler_gap_hierarchy | 0.69 | 6.75 | 3.12 | 3.12 | 3.11 | 1.41 | 1.40 | 0.05 | 0.01 | 3.17 |\\n| filler_gap_obj | 0.87 | 10.14 | 4.09 | 4.07 | 4.07 | 3.05 | 3.24 | 0.09 | 0.01 | 4.09 |\\n| filler_gap_pp | 0.73 | 7.11 | 3.04 | 3.02 | 3.02 | 1.08 | 1.10 | 0.04 | 0.01 | 2.95 |\\n| filler_gap_subj | 0.77 | 7.70 | 3.23 | 3.22 | 3.21 | 1.20 | 1.18 | 0.04 | 0.01 | 3.20 |\\n| Average | 0.82 | 7.93 | 3.13 | 2.60 | 2.23 | 1.26 | 1.29 | 0.12 | 0.02 | 3.17 |\\n\\nTable 9: pythia-160m\\n\\nTable 10: pythia-160m (selectivity)\"}"}
{"id": "acl-2024-long-785", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Feature-finding methods | Vanilla | Mean | PCA | k-means | LDA | Rand. |\\n|-------------------------|---------|------|-----|---------|-----|-------|\\n| agr_gender              | 1.00    | 3.86 | 2.99| 2.80    | 1.45| 1.03  |\\n| agr_sv_num_subj-relc    | 0.97    | 4.01 | 4.32| 4.19    | 3.72| 0.29  |\\n| agr_sv_num_obj-relc     | 0.99    | 5.61 | 5.01| 5.46    | 4.52| 0.32  |\\n| agr_sv_num_pp           | 0.97    | 5.63 | 4.96| 4.83    | 4.55| 0.38  |\\n| agr_refl_num_subj-relc  | 0.92    | 3.65 | 3.86| 3.77    | 1.85| 0.15  |\\n| agr_refl_num_obj-relc   | 0.96    | 4.43 | 4.03| 4.04    | 1.90| 0.31  |\\n| agr_refl_num_pp         | 0.89    | 3.90 | 3.73| 3.20    | 1.92| 0.16  |\\n| npi_any_subj-relc       | 0.95    | 8.76 | 4.01| 4.00    | 3.99| 2.28  |\\n| npi_any_obj-relc        | 0.96    | 8.59 | 4.08| 4.07    | 4.06| 2.47  |\\n| npi_ever_subj-relc      | 0.99    | 12.12| 6.94| 6.90    | 6.68| 6.91  |\\n| npi_ever_obj-relc       | 1.00    | 12.15| 7.12| 7.07    | 7.06| 7.06  |\\n| garden_mvrr             | 0.89    | 19.74| 3.62| 5.04    | 4.47| 4.46  |\\n| garden_mvrr_mod         | 0.61    | 17.40| 1.85| 2.43    | 3.22| 3.21  |\\n| garden_npz_obj          | 0.90    | 19.03| 3.33| 3.72    | 2.99| 2.98  |\\n| garden_npz_obj_mod      | 0.85    | 20.07| 1.79| 1.96    | 1.95| 1.95  |\\n| garden_npz_v-trans      | 0.81    | 5.43 | 2.87| 3.22    | 1.53| 0.18  |\\n| garden_npz_v-trans_mod  | 0.67    | 2.55 | 1.17| 1.17    | 0.62| 0.10  |\\n| gss_subord              | 0.82    | 22.47| 3.42| 3.18    | 4.35| 4.35  |\\n| gss_subord_subj-relc    | 0.85    | 14.07| 2.17| 2.47    | 2.43| 2.43  |\\n| gss_subord_obj-relc     | 0.94    | 13.50| 1.81| 1.81    | 2.37| 2.37  |\\n| gss_subord_pp           | 0.93    | 13.24| 1.86| 2.21    | 2.52| 2.52  |\\n| cleft                   | 0.95    | 14.46| 5.53| 4.84    | 1.78| 0.86  |\\n| cleft_mod               | 0.67    | 11.27| 3.37| 2.93    | 1.52| 1.25  |\\n| filler_gap_embed_3      | 0.52    | 3.98 | 0.96| 0.98    | 0.98| 0.37  |\\n| filler_gap_embed_4      | 0.50    | 3.11 | 0.31| 0.34    | 0.34| 0.17  |\\n| filler_gap_hierarchy    | 0.87    | 9.71 | 4.97| 4.96    | 4.95| 2.94  |\\n| filler_gap_obj          | 0.82    | 11.28| 3.97| 3.97    | 3.97| 3.54  |\\n| filler_gap_pp           | 0.88    | 10.22| 5.07| 5.03    | 5.02| 2.77  |\\n| filler_gap_subj         | 0.89    | 11.84| 6.53| 6.44    | 6.41| 4.87  |\\n| Average                 | 0.86    | 10.24| 3.64| 3.69    | 3.22| 2.15  |\\n\\nTable 11: pythia-410m; Probe 0 has $\\\\lambda = 10^4$, Probe 1 has $\\\\lambda = 10^5$.  \\n\\nTable 12: pythia-410m (selectivity)\"}"}
{"id": "acl-2024-long-785", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Feature-finding methods | Vanilla | DAS | Probe 0 | Probe 1 | Mean | PCA | k-means | LDA | Rand. |\\n|------------------------|---------|-----|---------|---------|------|-----|---------|-----|-------|\\n| agr_gender             | 1.00    | 4.72| 2.43    | 2.27    | 1.32 | 0.90| 0.90    | 0.01| 0.05  |\\n| agr_sv_num_subj-relc   | 1.00    | 6.62| 5.23    | 4.98    | 4.24 | 0.36| 0.39    | 1.40| 0.01  |\\n| agr_sv_num_obj-relc    | 0.94    | 5.30| 4.75    | 4.62    | 3.72 | 0.38| 0.41    | 1.39| 0.02  |\\n| agr_sv_num_pp          | 0.94    | 5.81| 4.78    | 4.45    | 3.86 | 0.36| 0.40    | 0.82| 0.03  |\\n| agr_refl_num_subj-relc | 0.85    | 4.13| 3.57    | 2.76    | 1.78 | 0.17| 0.21    | 0.48| 0.01  |\\n| agr_refl_num_obj-relc  | 1.00    | 5.89| 4.71    | 4.09    | 2.60 | 0.39| 0.42    | 0.47| 0.00  |\\n| agr_refl_num_pp        | 0.82    | 3.62| 3.00    | 2.18    | 1.43 | 0.23| 0.26    | 1.39| 0.02  |\\n| npi_any_subj-relc      | 0.97    | 10.51| 4.33 | 4.33    | 4.33 | 2.80| 2.93    | 0.20| 0.00  |\\n| npi_any_obj-relc       | 0.99    | 10.70| 4.33 | 4.33    | 4.33 | 2.85| 2.94    | 0.34| 0.01  |\\n| npi_ever_subj-relc     | 0.99    | 14.09| 7.14 | 7.11    | 7.09 | 6.67| 7.10    | 0.28| 0.01  |\\n| npi_ever_obj-relc      | 1.00    | 13.85| 6.87 | 6.84    | 6.84 | 6.62| 6.84    | 0.41| 0.00  |\\n| garden_mvrr            | 0.91    | 19.24| 4.82  | 5.23    | 4.67 | 4.67| 4.67    | 0.19| 0.12  |\\n| garden_mvrr_mod        | 0.63    | 17.51| 2.86  | 2.88    | 3.37 | 3.38| 3.37    | 0.10| 0.03  |\\n| garden_npz_obj         | 0.89    | 21.13| 3.33  | 3.67    | 2.52 | 2.52| 2.52    | 0.35| 0.09  |\\n| garden_npz_obj_mod     | 0.84    | 22.09| 2.08  | 2.33    | 1.82 | 1.81| 1.82    | 0.17| 0.06  |\\n| garden_npz_v-trans     | 0.73    | 5.43 | 2.58  | 2.48    | 1.40 | 0.22| 0.23    | 0.07| 0.01  |\\n| garden_npz_v-trans_mod | 0.72    | 2.65 | 0.87  | 0.80    | 0.62 | 0.07| 0.07    | 0.03| 0.01  |\\n| gss_subord             | 0.82    | 20.00| 2.87  | 3.48    | 4.84 | 4.85| 4.84    | 0.32| 0.08  |\\n| gss_subord_subj-relc   | 0.87    | 11.91| 2.00  | 2.39    | 2.57 | 2.57| 2.57    | 0.10| 0.05  |\\n| gss_subord_obj-relc    | 0.92    | 13.31| 2.21  | 2.38    | 2.73 | 2.73| 2.73    | 0.18| 0.08  |\\n| gss_subord_pp          | 0.94    | 11.44| 2.19  | 2.42    | 2.67 | 2.67| 2.67    | 0.17| 0.03  |\\n| cleft                  | 0.97    | 15.56| 5.59  | 3.76    | 1.62 | 0.25| 0.43    | 0.16| 0.01  |\\n| cleft_mod              | 0.81    | 11.99| 3.47  | 2.33    | 1.51 | 1.15| 1.18    | 0.02| 0.03  |\\n| filler_gap_embed_3     | 0.62    | 6.40 | 1.08  | 1.09    | 1.09 | 0.41| 0.41    | 0.02| 0.01  |\\n| filler_gap_embed_4     | 0.54    | 5.79 | 0.57  | 0.59    | 0.59 | 0.23| 0.23    | 0.01| 0.00  |\\n| filler_gap_hierarchy   | 0.83    | 8.69 | 3.90  | 3.90    | 3.90 | 1.86| 1.94    | 0.10| 0.00  |\\n| filler_gap_obj         | 0.76    | 10.69| 3.61  | 3.63    | 3.63 | 3.33| 3.50    | 0.22| 0.00  |\\n| filler_gap_pp          | 0.85    | 10.52| 4.69  | 4.67    | 4.67 | 1.78| 1.80    | 0.02| 0.00  |\\n| filler_gap_subj        | 0.89    | 11.82| 6.23  | 6.18    | 6.17 | 3.76| 3.87    | 0.03| 0.00  |\\n| Average                | 0.86    | 10.74| 3.66  | 3.52    | 3.17 | 2.07| 2.13    | 0.29| 0.03  |\\n\\nTable 13: pythia-1b; Probe 0 has $\\\\lambda = 10^5$, Probe 1 has $\\\\lambda = 10^6$. Table 14: pythia-1b (selectivity)\"}"}
{"id": "acl-2024-long-785", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Feature-finding methods | Vanilla | DAS | Probe 0 | Probe 1 | Mean | PCA | k-means | LDA | Rand. |\\n|-------------------------|---------|-----|---------|---------|------|-----|---------|-----|-------|\\n| agr_gender              | 1.00    | 3.62| 2.58    | 2.00    | 1.11 | 0.58| 0.58    | 0.00| 3.24  |\\n| agr_sv_num_subj-relc    | 0.98    | 4.82| 4.24    | 4.07    | 3.89 | 0.48| 0.44    | 2.43| 4.32  |\\n| agr_sv_num_obj-relc     | 0.97    | 5.11| 4.89    | 4.46    | 3.97 | 0.49| 0.61    | 2.37| 4.44  |\\n| agr_sv_num_pp           | 0.99    | 5.75| 4.94    | 4.77    | 4.58 | 0.54| 0.37    | 0.71| 5.11  |\\n| agr_refl_num_subj-relc  | 0.94    | 3.44| 3.27    | 2.19    | 1.83 | 0.17| 0.38    | 0.92| 3.31  |\\n| agr_refl_num_obj-relc   | 0.99    | 4.02| 3.96    | 2.71    | 1.98 | 0.29| 0.39    | 0.91| 3.85  |\\n| agr_refl_num_pp         | 0.96    | 3.87| 3.07    | 2.15    | 1.91 | 0.22| 0.25    | 0.40| 3.67  |\\n| npi_any_subj-relc       | 0.96    | 9.16| 4.16    | 4.15    | 4.14 | 2.08| 2.17    | 0.20| 4.35  |\\n| npi_any_obj-relc        | 0.96    | 8.85| 4.19    | 4.18    | 4.17 | 2.20| 2.27    | 0.28| 4.43  |\\n| npi_ever_subj-relc      | 1.00    | 12.95|7.08| 7.06| 7.06| 6.85| 7.06| 0.57| 6.92|\\n| npi_ever_obj-relc       | 1.00    | 12.99|6.90| 6.85| 6.84| 6.56| 6.84| 0.49| 6.89|\\n| garden_mvrr             | 0.85    | 18.23|3.18| 3.59| 3.76| 3.76| 3.76| 0.22| 4.86|\\n| garden_mvrr_mod         | 0.61    | 15.37|1.38| 1.41| 2.45| 2.45| 2.45| 0.04| 4.09|\\n| garden_npz_obj          | 0.98    | 17.52|3.27| 3.19| 2.33| 2.33| 2.33| 0.10| 4.44|\\n| garden_npz_obj_mod      | 0.87    | 17.76|2.17| 1.73| 1.56| 1.56| 1.56| 0.12| 3.14|\\n| garden_npz_v-trans      | 0.78    | 5.48| 2.97    | 2.50    | 1.54 | 0.31| 0.31    | 0.03| 3.32|\\n| garden_npz_v-trans_mod  | 0.67    | 2.25| 0.93    | 0.73    | 0.65 | 0.15| 0.15    | 0.03| 1.59|\\n| gss_subord              | 0.83    | 19.10|2.86| 2.53| 3.88| 3.88| 3.88| 0.03| 4.90|\\n| gss_subord_subj-relc    | 0.90    | 9.04| 1.52    | 1.97    | 2.21 | 2.21| 2.21    | 0.11| 3.39|\\n| gss_subord_obj-relc     | 0.98    | 11.14|1.52| 1.77| 2.42| 2.42| 2.42    | 0.07| 3.39|\\n| gss_subord_pp           | 0.93    | 9.89| 1.89    | 2.24    | 2.45 | 2.45| 2.45    | 0.05| 3.79|\\n| cleft                   | 1.00    | 14.65|5.25| 3.85| 1.54 | 0.25| 0.28    | 0.13| 5.86|\\n| cleft_mod               | 0.80    | 11.72|3.72| 2.62| 1.64| 1.20| 1.22    | 0.01| 4.34|\\n| filler_gap_embed_3      | 0.55    | 5.14| 1.16    | 1.19    | 1.19 | 0.30| 0.29    | 0.03| 1.21|\\n| filler_gap_embed_4      | 0.53    | 4.35| 0.38    | 0.40    | 0.39 | 0.16| 0.14    | 0.01| 0.43|\\n| filler_gap_hierarchy    | 0.94    | 9.25| 5.01    | 5.00    | 4.98 | 2.96| 3.07    | 0.13| 5.27|\\n| filler_gap_obj          | 0.76    | 10.51|3.62| 3.64| 3.64| 3.57| 3.66    | 0.23| 3.69|\\n| filler_gap_pp           | 0.86    | 9.86| 4.71    | 4.69    | 4.68 | 1.72| 2.06    | 0.02| 4.74|\\n| filler_gap_subj         | 0.90    | 11.93|6.20| 6.10| 6.08| 4.60| 4.96    | 0.03| 6.14|\\n| Average                 | 0.88    | 9.58| 3.48    | 3.23    | 3.06 | 1.96| 2.02    | 0.37| 4.11|\\n\\nTable 15: pythia-1.4b; Probe 0 has $\\\\lambda = 10^5$, Probe 1 has $\\\\lambda = 10^6$. \\n\\nTable 16: pythia-1.4b (selectivity)\"}"}
{"id": "acl-2024-long-785", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Feature-finding methods | Vanilla | DAS | Probe 0 | Probe 1 | Mean | PCA | k-means | LDA | Rand. |\\n|-------------------------|---------|-----|---------|---------|------|-----|---------|-----|-------|\\n| **agr_gender**          | 1.00    | 4.99| 2.53    | 1.89    | 1.08 | 0.38| 0.38    | 0.01| 3.54  |\\n| **agr_sv_num_subj-relc** | 0.97    | 5.87| 5.15    | 4.99    | 4.19 | 0.34| 0.35    | 2.21| 5.30  |\\n| **agr_sv_num_obj-relc** | 0.97    | 6.14| 5.68    | 5.88    | 5.06 | 0.39| 0.38    | 2.28| 5.65  |\\n| **agr_sv_num_pp**       | 0.97    | 6.21| 5.69    | 5.38    | 4.64 | 0.34| 0.33    | 0.27| 5.92  |\\n| **agr_refl_num_subj-relc** | 0.97 | 4.90| 3.48    | 2.89    | 2.06 | 0.16| 0.16    | 0.94| 3.94  |\\n| **agr_refl_num_obj-relc** | 0.97 | 5.98| 4.15    | 3.32    | 1.95 | 0.31| 0.34    | 0.76| 4.66  |\\n| **agr_refl_num_pp**     | 0.94    | 4.95| 3.36    | 2.49    | 1.85 | 0.21| 0.21    | 0.52| 4.12  |\\n| **npi_any_subj-relc**   | 0.94    | 9.39| 3.83    | 3.80    | 3.79 | 1.84| 1.89    | 0.26| 4.02  |\\n| **npi_any_obj-relc**    | 0.96    | 9.23| 3.61    | 3.58    | 3.57 | 1.88| 1.92    | 0.30| 3.84  |\\n| **npi_ever_subj-relc**  | 1.00    | 13.55| 7.05   | 7.02    | 7.01 | 6.87| 7.01    | 0.36| 6.93  |\\n| **npi_ever_obj-relc**   | 1.00    | 13.84| 7.18   | 7.12    | 7.10 | 6.90| 7.10    | 0.44| 7.34  |\\n| **garden_mvrr**         | 0.82    | 11.90| 3.99   | 4.47    | 3.21 | 3.20| 3.21    | 0.03| 4.60  |\\n| **garden_mvrr_mod**     | 0.58    | 10.37| 2.08   | 2.37    | 2.03 | 2.03| 2.03    | 0.01| 3.80  |\\n| **garden_npz_obj**      | 0.93    | 11.57| 1.94   | 2.46    | 2.15 | 2.14| 2.15    | 0.03| 3.94  |\\n| **garden_npz_obj_mod**  | 0.82    | 11.22| 1.70   | 1.82    | 1.27 | 1.27| 1.27    | 0.05| 2.98  |\\n| **garden_npz_v-trans**  | 0.81    | 5.87 | 2.98   | 2.57    | 1.74 | 0.30| 0.33    | 0.03| 3.57  |\\n| **garden_npz_v-trans_mod** | 0.75 | 3.15 | 1.34   | 1.18    | 0.87 | 0.13| 0.13    | 0.04| 2.11  |\\n| **gss_subord**          | 0.86    | 12.98| 3.29   | 3.97    | 3.13 | 3.12| 3.13    | 0.01| 4.49  |\\n| **gss_subord_subj-relc** | 0.89    | 7.25 | 1.72   | 2.13    | 2.08 | 2.08| 2.08    | 0.02| 3.35  |\\n| **gss_subord_obj-relc** | 0.97    | 7.62 | 2.01   | 2.33    | 2.50 | 2.50| 2.50    | 0.07| 3.63  |\\n| **gss_subord_pp**       | 0.93    | 8.10 | 1.81   | 2.39    | 2.36 | 2.36| 2.36    | 0.02| 3.85  |\\n| **cleft**               | 1.00    | 14.74| 5.50   | 4.52    | 2.60 | 0.23| 0.27    | 0.06| 6.30  |\\n| **cleft_mod**           | 0.86    | 12.06| 3.77   | 3.37    | 2.44 | 1.37| 1.37    | 0.01| 4.52  |\\n| **filler_gap_embed_3**  | 0.57    | 5.89 | 1.85   | 1.89    | 1.88 | 0.43| 0.45    | 0.03| 1.88  |\\n| **filler_gap_embed_4**  | 0.54    | 4.73 | 0.89   | 0.94    | 0.94 | 0.31| 0.28    | 0.03| 1.00  |\\n| **filler_gap_hierarchy**| 0.94    | 9.44 | 4.35   | 4.33    | 4.32 | 2.93| 3.20    | 0.16| 4.78  |\\n| **filler_gap_obj**      | 0.79    | 11.23| 3.98   | 4.04    | 4.03 | 3.87| 4.02    | 0.08| 4.20  |\\n| **filler_gap_pp**       | 0.88    | 11.09| 5.46   | 5.39    | 5.37 | 2.37| 3.03    | 0.02| 5.28  |\\n| **filler_gap_subj**     | 0.94    | 13.24| 7.55   | 7.37    | 7.30 | 5.73| 6.03    | 0.03| 7.42  |\\n| **Average**             | 0.88    | 8.88 | 3.72   | 3.65    | 3.19 | 1.93| 2.00    | 0.31| 4.38  |\"}"}
{"id": "acl-2024-long-785", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Feature-finding methods | Task | Task Acc. | Feature-finding methods | Vanilla | DAS | Probe 0 | Probe 1 | Mean | PCA | k-means | LDA | Rand. |\\n|-------------------------|------|----------|------------------------|---------|-----|---------|---------|------|-----|---------|-----|-------|\\n| agr_gender              |      |          |                        | 0.99    | 4.18| 3.58    | 2.36    | 1.37 | 0.56 | 0.56    | 0.05 | 0.01  |\\n| agr_sv_num_subj-relc    |      |          |                        | 0.99    | 5.23| 4.42    | 3.86    | 3.56 | 0.32 | 0.30    | 2.22 | 0.01  |\\n| agr_sv_num_obj-relc     |      |          |                        | 0.99    | 5.82| 6.16    | 5.23    | 4.31 | 0.31 | 0.27    | 2.10 | 0.01  |\\n| agr_sv_num_pp           |      |          |                        | 0.99    | 4.93| 4.16    | 3.66    | 3.44 | 0.32 | 0.28    | 0.04 | 0.01  |\\n|agr_refl_num_subj-relc   |      |          |                        | 0.94    | 4.00| 3.52    | 2.41    | 2.24 | 0.14 | 0.13    | 0.07 | 0.01  |\\n| agr_refl_num_obj-relc   |      |          |                        | 1.00    | 5.06| 4.48    | 2.86    | 2.23 | 0.25 | 0.27    | 0.09 | 0.01  |\\n| agr_refl_num_pp         |      |          |                        | 0.92    | 4.03| 3.03    | 2.05    | 2.00 | 0.17 | 0.17    | 0.08 | 0.00  |\\n| npi_any_subj-relc       |      |          |                        | 0.96    | 10.46| 3.73    | 3.74    | 3.75 | 1.68 | 1.72    | 0.28 | 0.00  |\\n| npi_any_obj-relc        |      |          |                        | 0.99    | 11.13| 3.85    | 3.85    | 3.85 | 1.79 | 1.85    | 0.31 | 0.00  |\\n| npi_ever_subj-relc      |      |          |                        | 0.97    | 14.01| 6.09    | 6.10    | 6.10 | 5.99 | 6.10    | 0.64 | 0.00  |\\n| npi_ever_obj-relc       |      |          |                        | 0.99    | 15.05| 6.75    | 6.75    | 6.75 | 6.55 | 6.75    | 0.50 | 0.01  |\\n| garden_mvrr             |      |          |                        | 0.81    | 16.98| 2.86    | 2.97    | 3.23 | 3.22 | 3.23    | 0.19 | 0.03  |\\n| garden_mvrr_mod         |      |          |                        | 0.57    | 16.65| 1.37    | 1.10    | 2.18 | 2.18 | 2.18    | 0.01 | 0.03  |\\n| garden_npz_obj          |      |          |                        | 0.98    | 16.11| 2.52    | 2.73    | 1.62 | 1.62 | 1.62    | 0.15 | 0.04  |\\n| garden_npz_obj_mod      |      |          |                        | 0.85    | 17.55| 1.54    | 1.61    | 0.95 | 0.95 | 0.95    | 0.23 | 0.02  |\\n| garden_npz_v-trans      |      |          |                        | 0.81    | 5.37 | 2.55    | 1.94    | 1.56 | 0.27 | 0.27    | 0.05 | 0.01  |\\n| garden_npz_v-trans_mod  |      |          |                        | 0.71    | 2.58 | 0.98    | 0.75    | 0.54 | 0.08 | 0.08    | 0.03 | 0.00  |\\n| gss_subord              |      |          |                        | 0.87    | 18.25| 2.52    | 2.16    | 3.50 | 3.49 | 3.50    | 0.04 | 0.05  |\\n| gss_subord_subj-relc    |      |          |                        | 0.87    | 8.93 | 1.66    | 1.78    | 2.14 | 2.14 | 2.14    | 0.13 | 0.01  |\\n| gss_subord_obj-relc     |      |          |                        | 0.99    | 9.08 | 1.93    | 2.02    | 2.50 | 2.50 | 2.50    | 0.18 | 0.02  |\\n| gss_subord_pp           |      |          |                        | 0.89    | 9.65 | 1.89    | 2.07    | 2.41 | 2.41 | 2.41    | 0.17 | 0.02  |\\n| cleft                   |      |          |                        | 1.00    | 14.71| 4.53    | 3.22    | 1.43 | 0.06 | 0.04    | 0.03 | 0.00  |\\n| cleft_mod               |      |          |                        | 0.96    | 13.13| 4.41    | 3.27    | 2.12 | 1.46 | 1.49    | 0.01 | 0.01  |\\n| filler_gap_embed_3      |      |          |                        | 0.59    | 5.83 | 1.08    | 1.10    | 1.09 | 0.31 | 0.31    | 0.04 | 0.00  |\\n| filler_gap_embed_4      |      |          |                        | 0.52    | 4.44 | 0.32    | 0.33    | 0.33 | 0.13 | 0.12    | 0.01 | 0.00  |\\n| filler_gap_hierarchy    |      |          |                        | 0.90    | 9.95 | 4.34    | 4.35    | 4.33 | 2.83 | 3.51    | 0.22 | 0.00  |\\n| filler_gap_obj          |      |          |                        | 0.77    | 11.15| 3.38    | 3.41    | 3.41 | 3.19 | 3.38    | 0.02 | 0.01  |\\n| filler_gap_pp           |      |          |                        | 0.92    | 11.24| 5.04    | 5.04    | 5.02 | 2.61 | 2.88    | 0.04 | 0.01  |\\n| filler_gap_subj         |      |          |                        | 0.95    | 12.97| 6.57    | 6.52    | 6.47 | 4.92 | 5.18    | 0.03 | 0.01  |\\n| Average                 |      |          |                        | 0.89    | 9.95 | 3.42    | 3.08    | 2.91 | 1.81 | 1.87    | 0.27 | 0.01  |\\n\\nTable 20: pythia-6.9b (selectivity)\"}"}
{"id": "acl-2024-long-785", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| Method       | Odds Ratio |\\n|--------------|------------|\\n| pythia-14m   | 0.58       |\\n| pythia-31m   | 0.85       |\\n| pythia-70m   | 0.95       |\\n| pythia-160m  | 0.99       |\\n| pythia-410m  | 1.00       |\\n| pythia-1b    | 1.00       |\\n| pythia-1.4b  | 1.00       |\\n| pythia-2.8b  | 1.00       |\\n| pythia-6.9b  | 0.99       |\\n\\n**Figure 8:**\\n\\nThe / No pilot that discussed no ministers has known...\\n\\n**Figure 9:**\\n\\nThe / No pilot that discussed no ministers has known...\\n\\n**Figure 10:**\\n\\nThe / No pilot that discussed no ministers has known...\"}"}
{"id": "acl-2024-long-785", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"While the woman yelled / applauded the coach:\\n\\n- pythia-14m: 0.80\\n- pythia-31m: 0.78\\n- pythia-70m: 0.86\\n- pythia-160m: 0.87\\n- pythia-410m: 0.82\\n- pythia-1b: 0.76\\n- pythia-1.4b: 0.76\\n- pythia-2.8b: 0.79\\n- pythia-6.9b: 0.77\\n- pythia-1.4b: 0.78\\n- pythia-2.8b: 0.79\\n- pythia-6.9b: 0.77\\n\\nShe told me that / what the new friend repaired:\\n\\n- probe\\n- mean\\n- pca\\n- kmeans\\n- lda\\n- random\\n- vanilla\\n\\n**Figure 10:** garden_npz_v-trans\\n\\n**Figure 11:** filler_gap_obj\\n\\n48421014663\"}"}
