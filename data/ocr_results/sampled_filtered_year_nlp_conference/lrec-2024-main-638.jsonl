{"id": "lrec-2024-main-638", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FFSTC: Fongbe to French Speech Translation Corpus\\n\\nD. Fortun\u00e9 Kponou, Fr\u00e9jus A. A. Laleye, Eug\u00e8ne C. Ezin\\n\\n1 Institut de Math\u00e9matiques et de Sciences Physiques, Dangbo, B\u00e9nin\\n2 OPSCIDIA, Paris, France\\n\\n{fortune.kponou,eugene.ezin}@imsp-uac.org, frejus.laleye@opscidia.com\\n\\nAbstract\\n\\nIn this paper, we introduce the Fongbe to French Speech Translation Corpus (FFSTC). This corpus encompasses approximately 31 hours of collected Fongbe language content, featuring both French transcriptions and corresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset compiled through various collection methods and the efforts of dedicated individuals. Furthermore, we conduct baseline experiments using Fairseq's transformer_s and conformer models to evaluate data quality and validity. Our results indicate a score BLEU of 8.96 for the transformer_s model and 8.14 for the conformer model, establishing a baseline for the FFSTC corpus.\\n\\nKeywords: Speech translation corpus, spoken language translation, low-resource language, Fongbe-French, Fongbe\\n\\n1. Introduction\\n\\nIn the era of global communication and rapid technological advancement, the development of efficient translation systems holds immense importance. These communications systems have the potential to break language barriers, facilitating meaningful knowledge exchange, and increasing the cross-cultural connections among individuals from diverse linguistic backgrounds (Doherty, 2016). They play a pivotal role in enhancing activities such as tourism, contributing to economic growth (Lenba and Ennebati, 2022). Translation systems help people to access educational resources and information in their native languages, promoting literacy and knowledge sharing. While text-based translation systems have revolutionized communication for many languages, they fall short for tonal languages like Fongbe, which are more spoken than written. This creates a critical need for speech-to-text translation tailored to tonal languages such as Fongbe. Fongbe is the most spoken dialect of Benin, by more than 50% of Benin's population, including 8 million speakers. Fongbe is also spoken in Nigeria and Togo (Laleye et al., 2016).\\n\\nSpeech translation involves the transformation of spoken language into text in another language. It is worth noting that speech translation has predominantly focused on languages with abundant linguistic resources, including English, French, Chinese, and Spanish (Qietal., 2022; Jia et al., 2022; Iranzo-S\u00e1nchez et al., 2020). The traditional speech translation approach called cascade method, involve two separate modules, the first module for automatic speech recognition (ASR) and the second for text translation (Etchegoyhen et al., 2022). This approach require two separate corpora to train individually each modules that are subsequently coupled. The advent of sequence-to-sequence architectures (Sutskever et al., 2014) has reshaped this landscape, enabling the creation of a single corpus containing audio recordings and target texts. This development is particularly relevant for low-resource languages like Fongbe, which are in the category of spoken language and have limited written resources. So it is challenging to get written text in Fongbe and its corresponding in French, than getting a recording of voice in Fongbe with transcription in French.\\n\\nIn response to these challenges and opportunities, we present the Fongbe-French Speech Translation Corpus (FFSTC), a unique resource comprising 31 hours of spoken Fongbe paired with French text. Our objective is to advance voice translation technologies for less commonly studied languages like Fongbe.\\n\\nThis paper makes two key contributions. The first is the introduction of the Fongbe-French Speech Translation Corpus (FFSTC), the first public dataset of its kind. This rich dataset, contain 31 hours of speech data with diverse speakers and topics. The second is the establishment of a baseline performance by evaluating a transformer_s and conformer model of Fairseq toolkit (Ott et al., 2019) on the FFSTC, achieving a baseline BLEU of 8.96 and 8.14 respectively. This benchmark provides a reference point for future research using the FFSTC and helps identify areas for improvement.\\n\\nThis paper is structured as follows. Section 2 provides an overview of existing related studies. Section 3 delves into the methodology used to create the FFSTC dataset, detailing the data collection and processing procedures. Section 4 then explores the structure and statistics of the corpus, offering...\"}"}
{"id": "lrec-2024-main-638", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"insights into its composition and characteristics. Section 5 presents the experiments conducted to establish a baseline performance for a transformer and conformer model on the FFSTC, using BLEU metrics to assess its effectiveness. Finally, Section 6 concludes the paper by summarizing the key contributions and outlining potential future directions.\\n\\n2. Related work\\nLarge-scale speech translation datasets play a crucial role in advancing research in speech translation. However, the availability of such datasets is limited, especially for low-resource languages. To achieve meaningful breakthroughs in speech translation, it is crucial to acknowledge that, even though the research in this domain is relatively recent, substantial efforts must be focused on the construction of parallel datasets. Recent available datasets encompass a variety of language pairs, including Chinese-Mongolian (Qietal., 2022), Chinese-English (Zhang et al., 2021), translations from 21 languages into English (Jia et al., 2022), and translations from English into 15 languages (Wang et al., 2020c). The introduced corpora, such as GigaST (Ye et al., 2022), LibriVox (Beilharz et al., 2020), LibriSpeech-FR (Kocabiyikoglu et al., 2018), mintzai-ST (Etchegoyhen et al., 2021), Multilingual TEDx Corpus (Salesky et al., 2021), and Europarl-ST (Iranzo-S\u00e1nchez et al., 2020), contribute to filling the gap of limited resources available for training end-to-end speech translation models. These datasets enable researchers to develop more robust and effective systems for various well-resourced languages. It is therefore undeniable that speech translation research has been extensively conducted for major languages like English, Japanese, and Spanish. However, there is a lack of research in speech translation for under-resourced languages. Some efforts have nevertheless been made for the creation of resources for certain low-resource languages. Woldeyohannis et al. (2018), have created an Amharic speech corpus by preparing 7,43 hours of read-speech from 8,112 sentences. Additionally, they have developed a parallel Amharic-English corpus of 19,972 sentences with tourism as the application domain. Another study focuses on constructing a large corpus for speech translation from Khmer (Cambodian) to English and French (Soky et al., 2021). The corpus includes approximately 155 hours of speech and 1.7 million words of text from the Extraordinary Chambers in the Courts of Cambodia (ECCC). Moreover, another project aims to provide datasets for Tamasheq, a developing language spoken in Mali and Niger (Boito et al., 2022). The datasets consist of radio recordings from Studio Kalangou in Niger and Studio Tamani in Mali. They include a large amount of unlabeled audio data in five languages (French, Fulfulde, Hausa, Tamasheq, and Zarma) and a smaller 17 hour parallel corpus of audio recordings with translations in French. The quantity of data collected for the Ahmaric and Tamasheq languages, compared to other well-resourced languages, confirms the difficulty in establishing solid dataset for a system of speech translation for low-resourced languages. For these languages, the lack of available speech and text corpora remains a significant challenge for speech translation.\\n\\n3. Methodology\\nIn this section, we delve into the intricate process of creating the corpus, shedding light on the meticulous steps taken to acquire and compile the translations. Our methodology not only outlines the data collection procedure but also discusses the quality control measures implemented to ensure the reliability of the dataset.\\n\\n3.1. Clips creation process\\nThe data within this corpus originates from three distinct sources, each contributing significantly to its composition. The first source is the ALFFA corpus (Laleye et al., 2016), from which we extracted sentences in the Fongbe language and corresponding audio recording. These sentences were subsequently translated into French by experienced linguists.\\n\\nThe second source is the FFR1.1 corpus (Dossou and Emezue, 2020), which serves as a corpus for machine translation. It comprises sentences in French, accompanied by their written translations in the Fongbe language. In this case, we provided the Fongbe sentences to linguists who read and recorded them through a dedicated web platform.\\n\\nFinally, the third source involves an assembly of sentences gathered from various books, including \\\"Kondo le requin\\\" (Pliya, 1981) and \\\"Un pi\u00e8ge sans fin\\\" (Olympe, 1960). We also excerpts from texts found on news websites. These sentences were thoughtfully selected, condensed, and made accessible via a web platform. Given the complexity of this task, we encouraged participants to collaborate within teams to ensure the coherence and quality of the collection process.\\n\\nThe participants teams provided Fongbe translations for the given sentences in French. They committed to a voice recording process using laptops or smartphones through our web platform's interface. This involved audibly translating the sentences displayed in French on the screen. Following this, recorded submissions underwent a rigorous validation process by designated validators. These validators used a straightforward voting system to assess and determine the quality of each translation.\"}"}
{"id": "lrec-2024-main-638", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Clips validation process\\nIn the case of the first source, French transcripts derived from Fongbe transcriptions undergo a meticulous peer-review process by a second validator. This stringent review ensures the translation's accuracy and conformity.\\n\\nFor the second source, the validation process adopts a straightforward binary approach, utilizing a yes-or-no voting system. Audio recordings that align precisely with the provided Fongbe transcripts are retained, while those that do not are omitted.\\n\\nThe third source follows a selective inclusion process. Only clips endorsed as valid by the system are integrated into the corpus. Evaluation criteria encompass multiple facets, including translation completeness, audio recording clarity, and overall translation quality. Our platform features a robust voting system that facilitates the filtration of clips created by participants based on validators' assessments. The voting system employs a scale ranging from zero to five. Recordings that receive a score below two are classified as invalid and are consequently excluded from the dataset. Conversely, recordings scoring three or higher are regarded as valid contributions and are included in the dataset for further analysis. Subsequently, all recordings undergo a comprehensive review by a dedicated team of validators, who make the final determination of approval or rejection.\\n\\n4. Corpus structure and statistics\\nIn the previous section of the report, we outlined the origins of our dataset, which emanate from three distinct sources. The ASR corpus ALFFA (Laleye et al., 2016) translation, a recording selection of sentences extracted from the FFR1.1 corpus (Dossou and Emezue, 2020) with recorded readings, and a collaborative effort within a team for translating French sentences.\\n\\nCombining the first two sources, we accumulated a total of 10 hours of data, while the latter made a significant contribution of 21 hours. The team work was done by a group of volunteers who were very committed. There were 8 women and 12 men in the group, all between 20 and 40 years old, making a total of 20 people.\\n\\nFollowing the corpus collection, we split it into training (Train), development (Dev), and test (Test) sets, as outlined in Table 1. The corpus, in its entirety, comprises 16,447 sentences, which were presented randomly to the participants. Additionally, multiple participants within each team were allowed to record simultaneously using the same credentials.\\n\\nFigure 1 illustrates the distribution of vocabulary and word counts across the Train, Dev, and Test partitions of our collected dataset for the Fongbe speech translation task. Figure 1 highlights the substantial vocabulary diversity and word richness in our dataset, with Train exhibiting the highest vocabulary count at approximately 7,500 unique words. This variation in vocabulary and word counts across splits underscores the dataset's suitability for training and evaluating speech translation models with varying language complexities.\\n\\nTable 1: Corpus statistics.\\n\\n| Split | Sentences | Audio (hours) |\\n|-------|-----------|---------------|\\n| Train | 11636     | 20            |\\n| Dev   | 2329      | 7             |\\n| Test  | 2482      | 4             |\\n\\n5. Fongbe-French End-to-End Speech Translation Baseline\\nIn this work, we use the Fairseq toolkit to establish a baseline for Fongbe-French End-to-End speech translation. We run the baseline experiments using a state-of-the-art approach, implementing the transformer and conformer architecture within the Fairseq framework to build a speech translation model.\\n\\n5.1. Experiments\\nWe ran the experiments on the collected dataset with the downsampling of the initially recorded audio.\"}"}
{"id": "lrec-2024-main-638", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"dio clips from 44 kHz to 16 kHz during the prepro-\\ncessing step. Following this, to maintain uniformity\\nand streamline the training process, we subjected\\nthe extracted features to a normalization procedure,\\naligning them to a mean of 0 and a standard devi-\\nation of 1. As part of our data quality control mea-\\nsures, we implemented a filtering step to exclude\\nsamples that exceeded 3,000 frames, thus ensur-\\ning the quality and manageability of the dataset.\\n\\nWe used for 80-dimensional mel-filterbank features\\nand built character-level vocabulary using Sentencepiece (Kudo and Richardson, 2018). We em-\\nployed a label-smoothed cross-entropy loss func-\\ntion with a label-smoothing factor set to 0.1. Opti-\\nmizing the models utilized the Adam optimizer with\\na learning rate of $1 \\\\times 10^{-3}$, coupled with a learning rate\\nscheduler of the inverse square root type, featuring\\nwarm-up updates and gradient clipping. The model\\nis trained for 500 epochs using the Adam optimizer\\n(Kingma and Ba, 2014).\\n\\n5.2. Results and discussion\\n\\nWe utilized a beam size of 5 and averaged the last\\n10 checkpoints when evaluating the baseline model\\ntrained on the test set. Our evaluation metric was\\ncase-insensitive tokenized BLEU, implemented us-\\nusing the sacreBLEU tool kit (Post, 2018) following the\\nmethodology proposed by (Papineni et al., 2002a).\\n\\nTable 2: Baseline for Fongbe-French End-to-end\\nspeech translation BLEU-4 results.\\n\\n| Architecture | enc-type | FFSTC (test) |\\n|--------------|----------|--------------|\\n| transformer_s | - | 8.96 (18.4/10.9/10.7/10.7) |\\n| conformer abs | 8.14 (16.3/8.8/8.5/8.4) |\\n\\nThe BLEU score achieved on the test set of the\\ncollected dataset using the transformer_s and con-\\nformer architecture, is reported in Table 2. The\\nreported score is dissected into precision scores\\nfor various n-gram lengths. For the transformer_s\\nthere is 28.7% for unigrams (1-grams), 4.6% for\\nbigrams (2-grams), 1.8% for trigrams (3-grams),\\nand 0.6% for 4-grams. For the conformer there is\\n16.3% for 1-grams, 8.8% for 2-grams, 8.5% for 3-grams\\nand 8.4% for 4-grams. We set the positional encod-\\ning for the conformer to absolute. This evaluation\\nprovides valuable insights into the quality of our\\ncollected dataset. Despite the relatively low BLEU\\nscore of 8.96 for the transformer_s and 8.14 for the\\nconformer, which signifies room for improvement,\\nit is worth noting that this score holds promise for\\nthe development of an effective Fongbe-French\\nend-to-end speech translation model. While there\\nis certainly room for enhancement, particularly in\\nterms of n-gram precision, this result serves as\\na foundation upon which further refinements and\\noptimizations can be built.\\n\\nIn future research endeavors, we anticipate that\\nthe integration of pre-training models and advanced\\ntechniques, such as transfer learning, could sig-\\nnificantly enhance the performance of the base-\\nline model. Leveraging the rich resources of pre-\\ntraining models in French and further optimization\\nstrategies promises to elevate the accuracy and\\nfluency of Fongbe-French speech translation, thus\\nbridging linguistic and cultural divides more effec-\\ntively. This work lays the groundwork for future stud-\\nies aimed at harnessing the full potential of state-\\nof-the-art models and data-driven approaches to\\nadvance the field of speech translation for under-\\nrepresented languages like Fongbe.\\n\\n6. Conclusion\\n\\nIn this paper, we introduced FFSTC, the first speech\\ntranslation corpus for Fongbe language. This\\ndata set has been carefully curated, including a wide\\nvariety of language details and difficulties for tonal\\nlanguage. The corpus is valuable for researchers\\nand professionals who are working on speech trans-\\nlation and natural language processing. It stands\\nas the first of its kind for the West African languages,\\ncontributing not only to the advancement of Fongbe-\\nFrench translation but also to the broader goal of\\npreserving and promoting linguistic diversity. Our\\nbaseline experiments, conducted on this novel re-\\nsource, demonstrate promising potential for the\\ndevelopment of robust speech translation systems.\\n\\nBy providing a dataset focused on a tonal language,\\nthe purpose is to explore new methodologies, and\\npotentially develop breakthrough technologies that\\ncould benefit not only Fongbe speakers but speak-\\ners of other tonal and underrepresented languages.\\n\\nWe envision that this dataset will play a pivotal role\\nin bridging linguistic and cultural divides, facilitating\\nenhanced communication and mutual understand-\\ning across languages and communities in the West\\nAfrican context. Its availability paves the way for ex-\\nciting future research endeavors and applications\\nin the field.\\n\\n7. Acknowledgements\\n\\nWe would like to thank the Partnership for Skills\\nin Applied Sciences, Engineering, and Technology\\n(PASET) through the Regional Scholarship and In-\\nnovation Fund (RSIF) for the support for this re-\\nsearch.\\n\\nReferences\\n\\nAlfred V. Aho and Jeffrey D. Ullman. 1972. The\\nTheory of Parsing, Translation and Compiling,\\nvolume 1. Prentice-Hall, Englewood Cliffs, NJ.\\n\\nMohamed Anwar, Bowen Shi, Vedanuj Goswami,\\nWei-Ning Hsu, Juan Pino, and Changhan Wang.\"}"}
{"id": "lrec-2024-main-638", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2023. Muavic: A multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation. arXiv preprint arXiv:2303.00628.\\n\\nJeong-Uk Bang, Joon-Gyu Maeng, Jun Park, Seung Yun, and Sang-Hun Kim. 2023. English\u2013Korean speech translation corpus (enkost-c): Construction procedure and evaluation results. ETRI Journal, 45(1):18\u201327.\\n\\nBenjamin Beilharz, Xin Sun, Sariya Karimova, and Stefan Riezler. 2020. LibriVoxDeEn: A corpus for German-to-English speech translation and German speech recognition. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 3590\u20133594, Marseille, France. European Language Resources Association.\\n\\nMarcely Zanon Boito, Fethi Bougares, Florentin Barbier, Souhir Gahbiche, Lo\u00efc Barrault, Mickael Rouvier, and Yannick Est\u00e9ve. 2022. Speech resources in the tamasheq language. Language Resources and Evaluation Conference (LREC).\\n\\nWon Ik Cho, Seok Min Kim, Hyunchang Cho, and Nam Soo Kim. 2021. Kosp2e: Korean speech to English translation corpus. arXiv preprint arXiv:2107.02875.\\n\\nB.B. Dadi\u00e9. 1955. Le pagne noir: contes africains. Pr\u00e9sence africaine: Ed. Africaines. Pr\u00e9sence africaine.\\n\\nMattia A Di Gangi, Roldano Cattoni, Luisa Benittivogli, Matteo Negri, and Marco Turchi. 2019. Must-c: a multilingual speech translation corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2012\u20132017. Association for Computational Linguistics.\\n\\nStephen Doherty. 2016. The impact of translation technologies on the process and product of translation. International Journal of Communication, 10:969.\\n\\nBonaventure FP Dossou and Chris C Emezue. 2020. Ffrv1.1: Fon-French neural machine translation. arXiv preprint arXiv:2006.09217.\\n\\nThierry Etchegoyhen, Haritz Arzelus, Harritxu Gete Ugarte, Aitor Alvarez, Ander Gonz\u00e1lez-Docasal, and Edson Benites Fernandez. 2022. Cascade or direct speech translation? A case study. Applied Sciences, 12(3).\\n\\nThierry Etchegoyhen, Haritz Arzelus, Harritxu Gete Ugate, Aitor Alvarez, Ander Gonz\u00e1lez-Docasal, and Edson Benites Fernandez. 2021. mintzai-ST: Corpus and Baselines for Basque-Spanish Speech Translation. In Proc. IberSPEECH 2021, pages 190\u2013194.\\n\\nChristian Federmann and William D. Lewis. 2016. Microsoft speech language translation (MSLT) corpus: The IWSLT 2016 release for English, French and German. In Proceedings of the 13th International Conference on Spoken Language Translation, Seattle, Washington D.C. International Workshop on Spoken Language Translation.\\n\\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. 2020. Conformer: Convolution-augmented transformer for speech recognition. arXiv preprint arXiv:2005.08100.\\n\\nJavier Iranzo-S\u00e1nchez, Joan Albert Silvestre-Cerd\u00e0, Javier Jorge, Nahuel Rosell\u00f3, Adri\u00e0 Gim\u00e9nez, Albert Sanchis, Jorge Civera, and Alfons Juan. 2020. Europarl-st: A multilingual corpus for speech translation of parliamentary debates. In ICASSP2020-2020IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8229\u20138233.\\n\\nYe Jia, Michelle Tadmor Ramanovich, Quan Wang, and Heiga Zen. 2022. CVSS corpus and massively multilingual speech-to-speech translation. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 6691\u20136703, Marseille, France. European Language Resources Association.\\n\\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\\n\\nAli Can Kocabiyikoglu, Laurent Besacier, and Olivier Kraif. 2018. Augmenting LibriSpeech with French translations: A multimodal corpus for direct speech translation evaluation. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nTaku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\\n\\nFr\u00e9jus A. A. Laleye, Laurent Besacier, Eug\u00e8ne C. Ezin, and Cina Motamed. 2016. First automatic fonfie continuous speech recognition system:\"}"}
{"id": "lrec-2024-main-638", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-638", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang, and Jun Cao. 2022. Gigast: A 10,000-hour pseudo speech translation corpus. arXiv preprint arXiv:2204.03939.\\n\\nRuiqing Zhang, Xiyang Wang, Chuanqiang Zhang, Zhongjun He, Hua Wu, Zhi Li, Haifeng Wang, Ying Chen, and Qinfei Li. 2021. Bstc: A large-scale Chinese-English speech translation dataset.\"}"}
