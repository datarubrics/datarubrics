{"id": "emnlp-2022-main-29", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"methods fail to identify gaps in narrative coherence and are not suited for evaluating long summaries.\\n\\nOur SNAC dataset and annotation framework releases a large-scale dataset of fine-grained coherence annotations and establishes a protocol for eliciting such annotations from crowdworkers. This provides a foundation for future research efforts in this area.\\n\\nHas the dataset been used already?\\n\\nAt the time of submission, the dataset has only been used in the current paper for analysis of generation errors made by current state-of-the-art summarization models and for training automatic coherence detection models.\\n\\nWho funded the dataset?\\n\\nWe withhold this information to maintain anonymity but will include it upon publication.\\n\\nE.2 Dataset Composition\\n\\nWhat are the instances?\\n\\nEach instance in this dataset is a model generated summary from either the book or the movie domain. All summaries are in the English language.\\n\\nHow many instances are there?\\n\\nOur dataset contains annotations for 160 generated summaries (including both expert and crowd annotations).\\n\\nWhat data does each instance consist of?\\n\\nEach instance contains multiple span-level highlights corresponding to coherence errors, each of which is tagged with a specific error category.\\n\\nDoes the data rely on external sources?\\n\\nYes. For the book datasets, we annotate summaries from the publicly available model outputs released by Wu et al. (2021). For movies, we generate summaries using the Summ\u02c6N model (Zhang et al., 2022) on the publicly available TRIPOD dataset (Papalampidi et al., 2020).\\n\\nAre there recommended data splits or evaluation measures?\\n\\nWe will include the recommended training, development, and test splits for our annotations with the dataset release. The statistics for the data splits are outlined in Section 5.\\n\\nE.2.1 Data Collection Process\\n\\nWho was involved in the collection process and what were their roles?\\n\\nFor expert annotations, 3 authors of the paper with experience in engaging with model-generated text annotated 10 book summaries. To recruit crowd annotators, we launched a qualification task on Mechanical Turk. After this qualification, 11 workers were asked to annotate 150 summaries.\\n\\nHow was the dataset collected?\\n\\nGiven a generated summary, annotators were asked to select span highlights that correspond with coherence errors and categorize the type of that error. We provided all annotators with detailed instructions describing the task interface, error type definitions as well as the overall workflow.\\n\\nOver what time frame was the data collected?\\n\\nThe dataset was collected over the months of March and April 2022.\\n\\nDoes the dataset contain all possible instances?\\n\\nNo, we only annotate narrative summaries from two summarization models on two domains (movies and books). Moreover, our dataset only contains English language summaries.\\n\\nIf the dataset is a sample, then what is the population?\\n\\nThe dataset is a subset of generated summaries produced by state-of-the-art summarization models on narratives like books or movie screenplays.\\n\\nE.3 Data Preprocessing\\n\\nWhat preprocessing/cleaning was done?\\n\\nWe fix sentence and word boundaries for highlighted spans from crowd annotations.\\n\\nWas the raw data saved in addition to the cleaned data?\\n\\nYes.\\n\\nDoes this dataset collection/preprocessing procedure achieve the initial motivation?\\n\\nYes. This dataset serves as a large-scale collection of annotated coherence errors and provides the first characterization of such errors in long narrative summaries.\\n\\nE.4 Dataset Distribution\\n\\nHow is the dataset distributed?\\n\\nOur dataset is publicly released at this link: https://github.com/tagoyal/snac.\\n\\nWhen was it released?\\n\\nThe dataset was released in October, 2022.\\n\\nWhat license (if any) is it distributed under?\\n\\nThe dataset is released under the CC BY-SA 4.0 license.\\n\\nhttps://creativecommons.org/licenses/by-sa/4.0/legalcode\"}"}
{"id": "emnlp-2022-main-29", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Who is supporting and maintaining the dataset?\\n\\nThis dataset is maintained by authors of this paper.\\n\\nE.5 Legal and Ethical Considerations\\n\\nWere workers told what the dataset would be used for and did they consent?\\n\\nCrowdworkers were aware that their responses were being collected as part of a research study on analyzing coherence errors in narrative text. The Amazon Mechanical Turk Participation Agreement permits the use of their annotated responses for this work. We do not release any personal information, e.g. worker IDs, of the crowdworkers.\\n\\nIf it relates to people, could this dataset expose people to harm or legal action?\\n\\nNo.\\n\\nIf it relates to people, does it unfairly advantage or disadvantage a particular social group?\\n\\nNo.\"}"}
{"id": "emnlp-2022-main-29", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: Screenshot of the task interface for SNAC annotations\\n\\nFigure 17: Screenshot of the first page of the tutorial provided to crowd annotators\"}"}
{"id": "emnlp-2022-main-29", "page_num": 20, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-29", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Introduction\\n\\nAs pre-trained models for news summarization (Lewis et al., 2020; Zhang et al., 2020; Brown et al., 2020) have improved drastically, researchers have begun tackling increasingly challenging settings, particularly long document summarization and generation of longer summaries (Krysci\u0144ski et al., 2021; Huang et al., 2021; Zhang et al., 2022; Wu et al., 2021). Summaries in these settings differ considerably from the newswire summaries of past research efforts (Nallapati et al., 2016; Narayan et al., 2018): models now need to extract salient information from different parts of a significantly longer document, and na\u00efvely combining these in a much longer output is less likely to yield a summary with coherent discourse structure.\\n\\nThis shift in the scope of the summarization task calls for a reexamination of the summarization evaluation framework. Even for short newswire summaries, Fabbri et al. (2021) showed that automated metrics are inadequate, and consequently, reporting results from a human evaluation study has become the standard practice. However, human evaluation is rarely done for longer summaries possibly due to the associated labor costs of reading and evaluating long text. It is also unclear whether A/B testing or Likert-scale based annotation frameworks transfer to long summary settings. Establishing human evaluation protocols is critical for comparing different modeling approaches and measuring progress.\\n\\nRecently, Wu et al. (2021) proposed a strong book summarization model but showed that although generated summaries covered important information from the books, they read like a list of events stapled together without any coherent narrative structure (see Figure 1). We found similar\"}"}
{"id": "emnlp-2022-main-29", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"John Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career. In London, he becomes infatuated with Madame de Pastourelles, a beautiful and intelligent artist.\\n\\nIn Paris, he impresses Lord Findon with his work. One afternoon, Fenwick goes to lunch with Madam de Pastourelles, a beautiful artist, and Eugenie, a wealthy benefactor. She promises to help him help customers.\"}"}
{"id": "emnlp-2022-main-29", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Summary-level agreement, measured by Krippendorff\u2019s $\\\\alpha$.\\n\\n\u2217\u2217 Expert agreement after one round of annotations; this aligns with the crowd setting.\\n\\nLimitations of Current Human Evaluation\\n\\nSummary-level Likert scale annotations are the most commonly used setup for collecting coherence in single-document news summarization research (Fabbri et al., 2021). Here, we run an analogous study for our longer narrative summaries. We ask 3 Mechanical Turk workers with prior experience in annotation for NLP tasks, specifically discourse analysis and text simplification, to rate the overall coherence of 100 generated summaries on a 5-point scale. Table 1 reports the observed agreement, measured by Krippendorff\u2019s $\\\\alpha$. Compared to newswire summaries collected under a similar setup (Fabbri et al., 2021), annotations for longer narratives have a much lower agreement. This shows the difficulty in obtaining a consensus on coherence for a 500+ word summary through a single value on a 5-point scale.\\n\\nIn Appendix B, we further show that automatic metrics like ROUGE and BERTScore (Zhang et al., 2019) that are primarily used for evaluating long document summarization fail to penalize coherence errors in summaries. Better tools for both automatic and human evaluation are needed.\\n\\n3 SNAC Annotation Methodology\\n\\nWe design our methodology to: 1) simplify the summary-level annotation task into smaller sub-tasks, and 2) provide a structured framework that allows annotators to specify the type of coherence error, instead of evaluating coherence holistically.\\n\\n3.1 Task Workflow and Notation\\n\\nWe decompose the summary-level task into smaller segment-level tasks: at each step, annotators evaluate a subpart of the summary, which is usually 2-4 sentences long. Let $S_0, S_1, ..., S_N$ denote these summary segments. While evaluating segment $S_i$, coherence judgments are made with respect to both the context $S_0, S_1, ..., S_{i-1}$ and text within $S_i$.\\n\\nTo annotate a single error in $S_i$, annotators select the error span $t_j \\\\in S_i$ and the coherence error type $e_j$ (error taxonomy outlined in Section 3.2) to construct the error triple $a_j = (S_i, t_j, e_j)$. This process is repeated until all errors in segment $S_i$ have been added, after which they proceed to the next segment $S_{i+1}$ for annotation. At the end of the annotation, workers produce the full set of annotations $A = \\\\{a_j \\\\forall j\\\\}$ across all the text segments. The outcome of this is shown in Figure 2.\\n\\nFor book summaries, i.e. BOOK-175B and BOOK-6B, our segments come from boundaries present in the generated summaries. These are an average of 2.7 sentences. For MOVIE-B, we segment summaries into chunks of 3 sentences.\\n\\n3.2 Error Taxonomy\\n\\nReinhart (1980) states three conditions for coherence: connectedness (cohesion), consistency, and relevance. Our error taxonomy is guided by these conditions while covering the broad range of coherence errors produced by current models.\\n\\nWe divide errors into two categories: a) Coherence errors: these measure whether the summary is well-structured and events in the summary make narrative sense, and b) Language errors: these measure other aspects of the quality of generated text, such as grammar. While these do not come under the ambit of coherence errors, we found it useful to provide these additional error types for crowd workers to anchor other \u201cbadness\u201d in text to.\\n\\n3.2.1 Coherence Errors\\n\\nNew character without introduction ($\\\\text{CharE}$)\\n\\nThese refer to scenarios where a new person is introduced in the narrative without providing any background about the person, or their relation with other characters in the story. This violates condition 1 of coherence, i.e. connectedness. Note that well-known people, e.g. Barack Obama, do not need an introduction.\\n\\nMissing reference to an event or object ($\\\\text{RefE}$)\\n\\nThese refer to scenarios where an event or object is mentioned for the first time, but the phrasing strongly implies that it must have been introduced previously or that some context is missing to fully understand it. E.g., in Figure 2, the phrasing of her husband\u2019s suicide gives the strong impression that the reader is already aware of this event.\\n\\nAbrupt scene transition ($\\\\text{SceneE}$)\\n\\nThese occur where there is a sudden shift in the narrative and are\"}"}
{"id": "emnlp-2022-main-29", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Miss Manette receives a letter from the bank informing her that information about her father's small property has been discovered. She wants to travel to France to identify him and restore him to life.\\n\\nMr. Lorry explains that her father has been found under another name and is being held in a house in Paris.\\n\\nIn court, Mr. Darnay is accused of treason. However, he is acquitted after his patriot friend, Roger Cly, testifies against him.\\n\\nMr. Lorry visits the Doctor's house on a Sunday afternoon as he often does. Miss Pross, the housekeeper, worries that many people will come to the house to look for Ladybird.\\n\\nSuddenly, the Doctor starts to feel ill and says they should go inside.\\n\\nCharles Darnay, the Marquis' nephew, returns to France to pursue the sacred object that took him away. He tells the Marquis that he renounces his French property as it is full of misery.\\n\\nCharles has been in love with Lucie Manette for a long time but has never told her about his feelings.\\n\\nStryver tells Lorry that he intends to marry Lucie for pragmatic reasons.\"}"}
{"id": "emnlp-2022-main-29", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: The left graph shows the fraction of times a specific error type is detected for each individual dataset: CharE, RefE, and SceneE errors constitute the majority of coherence errors. The right graph shows the average fraction of tokens belonging to each error-type.\\n\\nNLP tasks. It included detailed instructions explaining the task workflow, interface, and error schema. Each worker was asked to annotate 2 book summaries; these summaries were chosen from the set of expert annotations. Workers were paid $12 for attempting this qualification.\\n\\nWe evaluated each worker's annotations against experts and sent individual feedback. Among coherence errors, we observed that workers generally tended to disagree on RefE; each worker had a different calibration of which events or objects require more context to improve overall narrative understanding. Another common source of disagreement between workers and experts were SceneE errors.\\n\\nTo help align their understanding with experts, we provided workers with a complete set of expert annotations for a whole summary for reference. We recruited 11 workers after the qualification to annotate 150 generated summaries. Each summary was annotated by 3 different annotators. Workers were paid an average of $12/hr.\\n\\n4.3 SN A C Dataset\\n\\nOur resulting dataset consists of ~9.6k span-level annotations for coherence judgments, across 160 summaries. Dataset statistics for the entire collected dataset, including both expert and crowd annotations, are shown in Table 2.\\n\\nA summary-wide expert annotation is SN A C is shown in Figure 3. Noticeably, CharE spans constitute the majority of errors; this observation is consistent throughout all datasets. Annotators tend to show higher recall and agreement over this category. SceneE and RefE are the next two major error categories. The annotations also illustrate the two reasons for SceneE: 1) there is a sudden change in setting and characters, e.g. Mr Lorry visits the... and 2) the previous scene is abruptly cut off, e.g. In court, Mr. Darnay..., where Ms. Mannette's story is unfinished.\\n\\nWe observed that worker annotations are high precision but low recall (CharE errors are an exception; workers have both high precision and recall for this category). This means that error spans identified by each worker tended to be actual errors, even when they were not detected by other annotators. Therefore, we combine annotations of all 3 annotators to construct the full SN A C dataset.\\n\\nError Distributions\\n\\nFigure 4 shows the fraction of unique errors of each error type annotated across all datasets. As seen in Figure 3 annotations, the majority of the coherence errors are due to CharE, RefE or SceneE. The bottom graph of Figure 4 shows the number of error tokens annotated (instead of numbers of errors) for each error type. We see that annotators mark a larger fraction of tokens in the BOOK-6B dataset as erroneous compared to BOOK-175B. The main difference comes from the difference in SceneE (annotators are instructed to select entire sentences) and GramE. As expected, for smaller summarization models, i.e. GPT-3 6B and BART, a larger fraction of errors and error tokens are associated with language errors compared to GPT-3 175B. In fact, we noticed that workers were more likely to skip coherence error annotations, e.g. RefE, when these co-occur with GramE for these models, particularly on BOOK-6B.\\n\\nHuman annotators focus on language errors while assessing coherence holistically. To understand which aspects of a summary contribute to the summary-level coherence rating provided by crowd workers, we compute the correlation between the number of errors of each type with the overall coherence score (Likert rating on a scale of 1-5, described previously in Section 2).\"}"}
{"id": "emnlp-2022-main-29", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3 outlines our results. First, it shows that the total number of errors is correlated with the overall coherence score, but annotators tend to weight language errors higher than coherence-specific errors. Surprisingly, we see negligible correlation with SceneE errors although these are a prominent distinguisher between generated and human-written summaries. Amongst other error types, both RefE errors and GramE errors show relatively higher correlation. Although not directly evaluating coherence, Clark et al. (2021) report similar observations where annotators tend to focus on grammar errors while judging text quality.\\n\\nNarrative Summarization \u2260 Open-Ended Generation\\nIn story completion, models are not required to cover all salient information from a document and only condition on past generated text; generated open-ended summaries rarely diverge off-topic. Examples of GPT-3 generated stories in Figure 8 (Appendix A) show that these generate almost no CharE, RefE or SceneE errors that form the majority in SNaC, and instead mainly exhibit repetition. Therefore, research efforts that introduce fine-grained taxonomies for this task, e.g. Scarecrow (Dou et al., 2022), are directly applicable to summarization which needs to be independently studied.\\n\\n4.4 Inter-Annotator Agreement\\nWe first compute inter-annotator agreements at the sentence- and segment-levels. This allows for an apples-to-apples comparison with Fabbri et al. (2021) as the average length of news summaries is roughly equal to our segment length. We convert their 5-point Likert ratings into binary labels using that each annotator\u2019s aggregated segment-level errors are correlated with their own summary-level judgment; here, agreement between annotators is not relevant.\\n\\nTable 4: Segment and sentence-level agreement, measured by Krippendorff\u2019s $\\\\alpha$ for SNAC. Our dataset reports higher inter-annotator agreement compared to newswire summaries adapted to a similar setting.\\n\\nTable 5: Token-level agreement for errors in the coherence sub-category. For RefE and InconE, we also report agreement (in parentheses) after normalizing span boundaries for overlapping errors.\\n\\nWe compare Krippendorff\u2019s $\\\\alpha$ for SNAC and news in Table 4: SNAC reports high inter-annotator agreement at both the sentence- and segment-level. Notably, this segment level agreement is better than that of crowdworkers in the news domain.\\n\\nSpan-level analysis\\nNext, we evaluate category-specific agreement between annotators at the span level. We report two metrics: 1) Krippendorff\u2019s $\\\\alpha$ and 2) two-agree %; borrowed from Dou et al. (2022), this reports the percentage of tokens labeled as erroneous by at least one annotator that were also labelled by one or more additional annotators. For RefE and InconE, we noticed that small differences in span boundaries caused a significant drop in agreement, therefore, for these we also report metrics after normalizing span boundaries of overlapping spans to their union.\\n\\nTable 5 outlines the agreement: for both expert and crowdworkers, we see high agreement for CharE and fair agreement for SceneE. On the other hand, lower agreement is observed for RefE; this aligns with our observation that individual annotators may have low recall. Different annotators fundamentally have different notions of what extra information is critical for understanding the text.\\n\\nSimilar overall results at the token-level are reported by Dou et al. (2022) for their error taxonomy: their error categories Commonsense and Encyclopedic report the lowest metrics, the two-agree\"}"}
{"id": "emnlp-2022-main-29", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gabriel Oak leases a sheep farm and becomes infatuated with Bathsheba, a beautiful young woman. He asks her aunt for her hand in marriage, but she turns him down because she doesn\u2019t love him. Gabriel\u2019s reputation as a shepherd makes it difficult for him to find work, so he plays his flute to earn money. Bathsheba dismisses the bailiff for stealing and decides to manage the farm on her own.\\n\\nFigure 5: RefE errors identified by only one annotator. % is as low as 20 and 12 respectively for 10 annotators. Note that we only have 3 annotators, so we expect our two-agree numbers to be much lower.\\n\\nDetecting Coherence Errors\\n\\nSetup\\n\\nWe aim to see whether models can automatically detect our coherence errors. We formulate all models as sequence classifiers: given a context $c$ and a sentence $s$, the goal is to classify whether $s$ contains coherence errors. Similar to Section 4.3, we project span-level errors to a sentence-level gold coherence label $y^\\\\ast \\\\in \\\\{0, 1\\\\}$.\\n\\nLet $E = \\\\{(e^\\\\ast j, t^\\\\ast j)\\\\}$ denote the set of error types and corresponding spans in $s$.\\n\\nWe split SN A C into train (4.2k), dev (230) and test (1.8k) examples and evaluate on the test set.\\n\\nMetrics\\n\\nFirst, we consider a sentence-level binary classification version of this task: can models correctly predict if a sentence contains coherence errors? In this case, our models take the form $P(y|c,s)$ where $y \\\\in \\\\{0, 1\\\\}$. We report precision, recall and F1. Note that the sentence-level $y^{\\\\text{pred}}$ judgment can be due to any of the error types.\\n\\nWe omit comparison with Krippendorff\u2019s \u03b1 reported in Dou et al. (2022) as they report observed agreement without normalizing by expected agreement. We re-compute their interannotator agreement on their dataset with normalization for a randomly selected subset of 3 annotators (comparable to our setting). This gives an average of 0.14 Krippendorff\u2019s \u03b1 across all categories, with the bottom 5 categories reporting an average of 0.05 \u03b1.\\n\\nWe next evaluate fine-grained prediction: can models identify the specific coherence error type and pinpoint the error span? In this case, our models predict $P(y|c,s)$, where $y$ is a bundle consisting of $y$ and a set of error tuples $\\\\{(e^{\\\\text{pred}} j, t^{\\\\text{pred}} j)\\\\}$ if $y = 0$. We report the precision, recall and F1 performance at correctly identifying the error type, i.e. $e^{\\\\text{pred}} j = e^\\\\ast j \\\\forall e^\\\\ast j$. We also report ov. computed as the fraction of times the predicted error span overlaps with the correct error span.\\n\\n5.1 Models for Comparison\\n\\nWe compare performances of three types of models: (1) unsupervised (UNSUP). (2) Models trained on synthetic data targeting coherent errors (SYN). We follow prior work (Joty et al., 2018; Shen et al., 2021) and generate synthetic training data by introducing artificial coherence errors in reference text, specifically on the BookSum dataset (Krysci\u0144ski et al., 2021). We ensure zero overlap between this synthetic train set and the evaluation test set. (3) Models fine-tuned on the SN A C data (FT).\"}"}
{"id": "emnlp-2022-main-29", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mr. Bingley meets the Bennet family at Netherfield Park. Jane, the eldest Bennett girl is attracted to him.\\n\\nDarcy starts to notice Elizabeth and asks her to marry him.\\n\\nFanny is also displeased by the closeness between Edward, her brother, and Elinor, the elder Dashwood daughter.\\n\\nThe Dashwood family is introduced. Mr. Dashwood's wife is left with little when he dies and the estate goes to his son, John Dashwood. John and his wife Fanny have a lot of money. Yet they refuse to help.\\n\\nFigure 6: Training data generation, and T5 inputs and outputs for the SYN and FT w/ span models. The FT w/o span model only generates yes/no and not the specific category or span.\\n\\nWe derive non-coherent examples by setting \\\\( s = s_j \\\\) and removing sentence \\\\( s_i \\\\) from the context, i.e. \\\\( c = s_1 s_2 \\\\ldots s_i - 1 s_i + 1 \\\\ldots s_j - 1 \\\\) (shown in Figure 6). Conversely, for positive coherent training data, we retain the original context from the gold summaries, i.e. \\\\( c = s_1 s_2 \\\\ldots s_i \\\\ldots s_j - 1 \\\\).\\n\\nWe fine-tune T5-Large (Raffel et al., 2020) for binary classification \\\\( P(y|c,s) \\\\) on these \\\\((y,c,s)\\\\) triples; training data sizes and intrinsic performance are reported in Appendix C.\\n\\nThis method is designed to target SceneE errors and closely resembles the sentence insertion method from prior work (Shen et al., 2021). Given context \\\\( c = s_1 s_2 \\\\ldots s_i \\\\), we obtain negative coherence examples by replacing the next sentence with another randomly sampled sentence from the remainder of the same summary, i.e. \\\\( s = s_j \\\\), where \\\\( j > i + 1 \\\\). Positive examples are created by retaining the original summary completion, i.e. \\\\( s = s_i + 1 \\\\). Figure 6 illustrates this.\\n\\nWe fine-tune T5-Large to model \\\\( P(y|c,s) \\\\).\\n\\nWe consider two versions: 1) w/o span: trained to generate yes/no reflecting the coherence of sentence \\\\( s \\\\), and 2) w/ span: trained to additionally predict the error category (e.g. CharE) and the corresponding error spans. Note that \\\\( s \\\\) can have errors belonging to multiple error categories, the model is trained to generate these in sequence. Figure 6 illustrates this. For SceneE, we omit span prediction as these are designed to incorporate the whole sentence.\\n\\nFigure 7: Performance of the different models on the \\\\( \\\\text{SNAC} \\\\) test set. Models trained on \\\\( \\\\text{SNAC} \\\\) outperform those trained on synthetically generated datasets.\\n\\n| Error       | FT w/ span | Human | P R F1 ov. |\\n|-------------|------------|-------|------------|\\n| CharE       | 0.79 (.86) | 0.88  | 0.71       | 0.79 | 0.98  |\\n| SceneE      | 0.35 (.58) | 0.58  | 0.36       | 0.44 | 1.0   |\\n| RefE        | 0.19 (.44) | 0.31  | 0.17       | 0.22 | 0.92  |\\n| InconE      | 0.25 (.25) | 0.29  | 0.16       | 0.20 | 0.97  |\\n\\nTable 6: Comparison between FT w/ span model and humans. Humans have higher precision while trained models report better recall across the top 3 error types.\\n\\nSimilar to SYN, we fine-tune T5-Large on these datasets.\\n\\n5.2 Results\\n\\nSentence-level binary classification\\n\\nFigure 7 shows an ROC curve of different models; the dotted black line indicates random chance. It shows that the entity-grid approach performs poorly compared to all neural approaches. Next, all trained models outperform the LM perplexity model; language models aggregating token-level probabilities cannot detect coherence errors. Finally, models trained on \\\\( \\\\text{SNAC} \\\\) outperform synthetic datasets which are the primary source of training data in prior coherence work. This shows that human annotations are needed to train strong coherence classifiers.\\n\\nFine-grained prediction\\n\\nOnly our FT w/ span model is trained to predict both the error category and the corresponding spans. Therefore, we compare its performance against human annotators. For an apples-to-apples comparison, we re-construct our test set by aggregating annotations of two randomly chosen annotators. This unfairly penalizes FT w/ span by introducing a mismatch between its train and test conditions, especially precision. Therefore, we also report precision scores on the original test set in brackets. Full set of results on the original test set are in Appendix C.\"}"}
{"id": "emnlp-2022-main-29", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6 outlines the results. As observed during qualitative evaluation, the held-out human annotations are high precision and low recall. On the other hand, FT w/ span is trained on the aggregated annotations from three annotators and reports higher recall than humans. Consequently, its F1 scores are comparable to human performance except for InconE. We attribute this to the limited number of training examples of this category.\\n\\nSimilar to previous analysis, we observe that models and humans report the best performance at detecting CharE. Interestingly, the trained model can identify both SceneE and RefE with higher recall compared to human annotators. For these top three error types, trained models are successful at localizing error to specific spans, reporting high overlap scores.\\n\\n6 Discussion\\nOur analysis of current narrative summarization models reveals that these do not generate coherent narratives; in fact, each generated summary contains ~30 coherence errors of varying degrees of severity. Moreover, both automatic and human approaches for coherence evaluation fail to reliably measure coherence. SNAC addresses this gap. However, we stop short of providing a prepackaged metric: which errors are more severe is application-dependent and subjective, and overall error counts cannot be compared. We encourage future work to focus on fine-grained error annotations, like those we present here, instead of sentence- or document-level annotations that do not provide actionable insights. We also recommend fine-grained error modeling for future coherence systems as well. While previous modeling has targeted document- or sentence-level coherence, our models trained on SNAC data can detect span-level coherence errors, particularly CharE errors with high accuracy. This automatic error localization opens up future avenues of post-hoc error correction systems built on top of coherence models.\\n\\n7 Related Work\\nCoherence frameworks\\nInspired by Centering Theory (Grosz et al., 1995), Barzilay and Lapata (2005, 2008) proposed entity-grid models to measure coherence through transitions of entity roles. This was further extended to incorporate non-head entities (Elsner and Charniak, 2011), discourse roles (Lin et al., 2011), and other improvements (Feng and Hirst, 2012; Feng et al., 2014), including neural variations (Guinaudeau and Strube, 2013; Nguyen and Joty, 2017; Joty et al., 2018) to better model text coherence. However, these models have been evaluated primarily on document-level essay scoring tasks (Mesgar and Strube, 2018) or artificial sentence-ordering tasks (Shen et al., 2021), and not on model-generated coherence errors.\\n\\nSummarization Evaluation\\nAutomatic metrics such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), and others have been used to evaluate summarization, but Fabbri et al. (2021) showed that these correlate poorly with summary quality. Human evaluation is widely considered the gold standard for generation tasks, however, recent work (Karpinska et al., 2021; Clark et al., 2021) demonstrated that humans are not reliable for evaluating strong models like GPT-3.\\n\\n8 Conclusion\\nWe introduce SNAC, a narrative coherence evaluation framework for long summaries. We develop an error taxonomy grounded in coherence errors made by current models and annotate data to provide the first characterization of such errors in narrative summaries. We also make our annotation tool publicly available to support future research efforts.\\n\\n9 Limitations\\nAlthough we view this work as an important step towards better understanding and evaluation of coherence in summaries, we acknowledge there is much more to do here. In this work, we only collect annotations and analyze coherence errors in summaries of English language books and movie screenplays. Our proposed taxonomy may not cover errors made by text summarization models for other languages and our trained models and analysis are English-specific. Moreover, some of these books summarized were written decades ago and may reflect the societial biases of those times, which could conceivably bias our trained error detection models. In this work, we use the text from the model generated summaries as is and do not perform any filtering. Finally, our work studies generated summaries for long narrative text. While we believe that our taxonomy is generalizable to other types of narrative text, we do not investigate whether it covers...\"}"}
{"id": "emnlp-2022-main-29", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"other domains involving summarization of long documents, such as government report summarization (Huang et al., 2021) or meeting summarization (Zhong et al., 2021).\\n\\nAcknowledgments\\nThanks to Eunsol Choi and Michael Strube for providing feedback on this work, as well as our Mechanical Turk annotators for conducting the annotation. This project was partially supported by Good Systems, a UT Austin Grand Challenge to develop responsible AI technologies, a grant from the UT Austin Office of the Vice President for Research through the \u201cCreating Connections for National Security Research Grants\u201d program, a grant from Open Philanthropy, NSF grants IIS-2107524, IIS-2145479, and gifts from Salesforce, Amazon, and Adobe.\\n\\nReferences\\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65\u201372.\\n\\nRegina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 141\u2013148.\\n\\nRegina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1\u201334.\\n\\nManik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347\u20139359.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:1877\u20131901.\\n\\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022. SummScreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics.\\n\\nElizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A Smith. 2021. All that's 'human'is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282\u20137296.\\n\\nYao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A Smith, and Yejin Choi. 2022. Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics.\\n\\nMicha Elsner and Eugene Charniak. 2011. Extending the entity grid with entity-specific features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 125\u2013129.\\n\\nAlexander Richard Fabbri, Wojciech Krysci\u0144ski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating Summarization Evaluation. Transactions of the Association for Computational Linguistics, 9:391\u2013409.\\n\\nVanessa Wei Feng and Graeme Hirst. 2012. Extending the entity-based coherence model with multiple ranks. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 315\u2013324.\\n\\nVanessa Wei Feng, Ziheng Lin, and Graeme Hirst. 2014. The impact of deep hierarchical discourse structures in the evaluation of text coherence. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 940\u2013949.\\n\\nBarbara J Grosz, Scott Weinstein, and Aravind K Joshi. 1995. Centering: a framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203\u2013225.\\n\\nCamille Guinaudeau and Michael Strube. 2013. Graph-based local coherence modeling. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 93\u2013103.\\n\\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient Attentions for Long Document Summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419\u20131436.\\n\\nShafiq Joty, Muhammad Tasnim Mohiuddin, and Dat Tien Nguyen. 2018. Coherence modeling of asynchronous conversations: A neural entity grid approach. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 558\u2013568.\"}"}
{"id": "emnlp-2022-main-29", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marzena Karpinska, Nader Akoury, and Mohit Iyyer.\\n2021. The perils of using mechanical turk to evaluate open-ended text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1265\u20131285.\\n\\nWojciech Krysci\u0144ski, Nitish Shirish Keskar, Bryan Mc Cann, Caiming Xiong, and Richard Socher. 2019. Neural text summarization: A critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 540\u2013551.\\n\\nWojciech Krysci\u0144ski, Nazneen Rajani, Divyansh Agrawal, Caiming Xiong, and Dragomir Radev. 2021. BookSum: A collection of datasets for long-form narrative summarization. arXiv preprint arXiv:2105.08209.\\n\\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-to-fine inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 687\u2013692.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381.\\n\\nZiheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 997\u20131006.\\n\\nZiming Mao, Chen Henry Wu, Ansong Ni, Yusen Zhang, Rui Zhang, Tao Yu, Budhaditya Deb, Chenguang Zhu, Ahmed H Awadallah, and Dragomir Radev. 2021. DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization. arXiv preprint arXiv:2110.08168.\\n\\nMohsen Mesgar and Michael Strube. 2018. A neural local coherence model for text quality assessment. In Proceedings of the 2018 conference on empirical methods in natural language processing, pages 4328\u20134339.\\n\\nHan Cheol Moon, Muhammad Tasnim Mohiuddin, Shafiq Joty, and Chi Xu. 2019. A unified neural coherence model. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2262\u20132272.\\n\\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos, Caglar Gulcehre, and Bing Xiang. 2016. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 280\u2013290.\\n\\nShashi Narayan, Shay B Cohen, and Mirella Lapata. 2018. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807.\\n\\nDat Tien Nguyen and Shafiq Joty. 2017. A neural local coherence model. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1320\u20131330.\\n\\nBo Pang, Erik Nijkamp, Wojciech Krysci\u0144ski, Silvio Savarese, Yingbo Zhou, and Caiming Xiong. 2022. Long Document Summarization with Top-down and Bottom-up Inference. arXiv preprint arXiv:2203.07586.\\n\\nPinelopi Papalampidi, Frank Keller, Lea Frermann, and Mirella Lapata. 2020. Screenplay Summarization Using Latent Narrative Structure. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1920\u20131933.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nTanya Reinhart. 1980. Conditions for text coherence. Poetics today, 1(4):161\u2013180.\\n\\nAili Shen, Meladel Mistica, Bahar Salehi, Hang Li, Timothy Baldwin, and Jianzhong Qi. 2021. Evaluating document coherence modeling. Transactions of the Association for Computational Linguistics, 9:621\u2013640.\\n\\nJeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. 2021. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862.\"}"}
{"id": "emnlp-2022-main-29", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Section 4.3, we noted that narrative summarization exhibits substantially different errors than open-ended text generation tasks like story generation or story completion, hence the need for our new taxonomy. We show examples of generated stories using the GPT-3 DaVinci in Figure 8. We prompt the GPT-3 text-davinci-002 model with the first few sentences of three generated summaries and ask for a 500-word completion. The coherence errors contained in these model outputs are very different from those in our narrative summarization setting. In particular, the stories hardly introduce any new characters (only Mr. Greene is introduced in the third example), and when they do, these are properly contextualized with the narrative. Furthermore, these models rarely generate RefE and generate no SceneE type of errors. In fact, repetition errors, shown in blue, dominate these narratives. Therefore, error taxonomies devised for these tasks, e.g. SCARECROW (Dou et al., 2022), are not useful for summarization settings which needs to be independently studied.\\n\\nB Limitations of Automatic Metrics\\n\\nLong document summarization research (Chen et al., 2022; Huang et al., 2021; Krysci\u00b4nski et al., 2021; Mao et al., 2021; Pang et al., 2022) has primarily relied on ROUGE scores to evaluate summaries. But do these capture narrative coherence? We test this for long narrative summaries, using the BOOK-175B dataset as a case study. Specifically, we test whether ROUGE or BERTScore (Zhang et al., 2019) can differentiate between actual generated summaries and their corrupted versions with artificially injected coherence errors. We introduce 3 types of coherence errors to generated summaries:\\n\\n1. Random shuffling using a random permutation of all sentences in a BOOK-175B summary. This does not change the overall length of the generated summary.\\n2. Repetition of a randomly selected subset of sentences. We randomly sample 50% of the sentences to repeat, all other sentences only occur once.\\n3. Retaining only named entities in the summary and top generated bigrams. We first extract the top 200 bigrams from the generated summaries in BOOK-175B, which include frequent bigrams like of the, that he, then he, in the, etc. For each test set summary, we construct a corrupted summary by concatenating all named entities in the summary (appending each named entity as many times as it occurs in the original summary) and the top bigrams extracted from the testset-wide summaries.\\n\\nFor an upper bound, we also report metrics for a different human-written summary for the same input book sampled from the BookSum dataset. Automatic metrics fail to penalize coherence errors. Table 7 shows that both shuffling and repetition do not hurt ROUGE or BERTScore, despite introducing critical coherence errors in generated summaries. The +NE & bigram setting does lead to a significant drop in BERTScore as these summaries are no longer fully-formed sentences. However, even this trivial baseline reports ROUGE scores on par with the original BOOK-175B summaries, showing that ROUGE is easy to \u201cgame\u201d for this task. Finally, we see that human-written summaries, i.e., gold coherent summaries, only report 2 points of improvement in R2 and BERTScore over artificially incoherent baselines. This clearly shows that these metrics are inadequate to measure...\"}"}
{"id": "emnlp-2022-main-29", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"John Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career. In London, he impresses several wealthy art collectors with his work. One of them, Mr. Greene, purchases one of his paintings for a large sum of money. With the money from the sale, Fenwick is able to pay back Mr. Morrison and also has enough left over to buy materials for more paintings. He continues to sell his paintings and gains more recognition in the art world. John Fenwick was grateful when Mr. Morrison agreed to loan him the money to move to London so he could pursue his art career. He was determined to make a success of himself and worked hard to impress the wealthy art collectors in the city. His efforts paid off when one of his paintings was purchased by Mr. Greene for a large sum of money. Fenwick was able to pay back the loan and also had enough money left over to buy materials for more paintings. His talent and hard work continued to be rewarded with more sales and recognition in the art world. He was on his way to becoming one of the most successful artists in London. One day, however, he was visited by Mr. Morrison, who had come to collect on the loan. Fenwick was surprised and disappointed to learn that the loan had never been forgiven and that he now owed a considerable amount of money, with interest. He was forced to sell more paintings to pay off the debt and was left with little money to live on. His dream of being a successful artist was over. Fenwick was heartbroken but he didn't give up. He continued to paint and eventually regained his place in the art world. He never forgot what Mr. Morrison did to him and he was determined to make sure that no one else would ever be treated so unfairly. Fenwick became a vocal advocate for artists' rights and worked to ensure that they were fairly compensated for their work. Thanks to his efforts, artists today are able to earn a living from their art and pursue their dreams. John Fenwick's story is one of talent, hard work, and determination. Despite being dealt a difficult hand, he was able to overcome adversity and make a lasting impact on the art world. When Fenwick was first starting out, he accepted a loan from Mr. Morrison to help him move to London so he could pursue his art career. He quickly made a name for himself in the art world and started impressing wealthy art collectors with his work. One of these collectors, Mr. Greene, ended up purchasing one of Fenwick's paintings for a large sum of money.\"}"}
{"id": "emnlp-2022-main-29", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: ROUGE and BERTScore for BOOK-175B and several artificially corrupted versions. Results show that automatic metrics fail to penalize coherence errors.\\n\\n| Method             | #train | #dev | F1  | Acc. |\\n|--------------------|--------|------|-----|------|\\n| Coref-based        | 6.0k   | 920  | .78 | .77  |\\n| Next-Sent          | 3.8k   | 880  | .71 | .74  |\\n\\nTable 8: Dataset sizes and intrinsic performance of T5-Large models trained on synthetic datasets.\\n\\nC Detecting Coherence Errors: Details and Additional Results\\n\\nC.1 Models trained on synthetic data (SYN)\\n\\nTable 8 shows the training data and development data size, as well as the intrinsic performance of these synthetic dataset-based coherence models on this development set. We construct both our datasets with an equal number of positive and negative coherence examples. The results show that T5 learns to model the synthetic task with reasonable accuracy. We do not expect the models to perform perfectly, as the synthetic data may have false positives (examples constructed to exhibit errors that are actually coherent).\\n\\nC.2 Implementation Details\\n\\nTable 9 shows the hyperparameters used for fine-tuning the T5-Large models on both synthetic training datasets and SNAC.\\n\\n| Computing Infrastructure |\\n|--------------------------|\\n| 32GB NVIDIA V100 GPU    |\\n| Max Input Seq Length     | 1024 |\\n| Max Output Seq Length    | 80 (for FT w/ span) |\\n| Optimizer               | Adam |\\n| Optimizer Params         | $\\\\beta = (0.9, 0.999)$, $\\\\epsilon = 10^{-8}$ |\\n| Learning Rate Decay      | Linear |\\n| Learning rate            | $1e^{-4}$ |\\n| Batch size               | 8 |\\n| Epochs                   | 5 |\\n\\nTable 9: Hyperparameters used for fine-tuning T5-Large on synthetic and SNAC train sets.\\n\\nC.3 Additional Results\\n\\nSentence-level binary classification\\n\\nIn Section 5, we reported sentence-level binary classification results for all models. However, the sentence-level $y_{pred}$ judgment in that setting can be due to any of the 4 error types or their combination and binary classification metrics do not tell us which of these error types are easier to detect.\\n\\nTo answer this, we compute the error-wise recall under the binary setting. We assume $e_{pred} = 0$ if $y_{pred} = 0$ for all error types $e_{j}$; that is, a prediction of a binary error counts as detecting an error of any type in that sentence. This overestimates the recall performance and can be viewed as an upper bound; a model that can only detect CharE may report non-zero recall for other errors if these co-occur with CharE.\\n\\nFor fair comparison between different models, we report category-wise recall for all models at the same precision level $P = 0.7$. Table 10 outlines our results. Both synthetic models report higher recall for the error category they were designed for. E.g., the coref-based method can detect CharE errors better than other error types. However, our FT models significantly outperform both synthetic approaches across all error types at thresholds with high precision performance. In particular, we observe high recall scores for CharE and SceneE.\\n\\nFine-grained prediction\\n\\nIn Table 6, we compared human and model (FT w/ spans) performance on a modified test set created by combining annotations from 2 crowdworkers. This unfairly penalized the trained models, which may have slightly higher recall due to being trained on annotations from 3 crowdworkers. In Table 11, we report results on the original test set that combines annotations from all 3 annotators.\"}"}
{"id": "emnlp-2022-main-29", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The definitions of error types and illustrative examples provided to the crowdworkers during training are outlined here.\\n\\nD.1 CharE\\nWe call these New Person not Introduced in the task interface. We provide the illustrative example shown in Figure 9 along with the following definition:\\n\\n\u201cThese refer to coherence errors where a new person is introduced into the narrative WITHOUT providing any background about the person, or their relation with other characters in the story. Note, however, that famous or well-known people do not need to be explicitly introduced.\\n\\nContext:\\nJohn Fenwick, an aspiring artist, accepts a loan to move to London to pursue his art career.\\n\\nCurrent Segment:\\nIn London, he impresses Lord Findon with his work.\\n\\nReasoning:\\nHere, a new person Lord Findon is introduced without explicitly stating who he is, or his connection to the other previous characters. On the other hand, if the sentence read \u201cLord Findon, a wealthy benefactor\u201d, then this would not be a coherence error.\\n\\nContext:\\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career.\\n\\nFigure 9: Illustration of CharE errors provided to crowdworkers during training.\\n\\nD.2 RefE\\nWe call these Missing Information about an Event/Object in the task interface. We provide the illustrative example shown in Figure 10 along with the following definition:\\n\\n\u201cThese refer to coherence errors where an event or object is mentioned for the first time, but the phrasing strongly implies some context is missing to understand this event/object and that it must have been introduced previously.\\n\\nContext:\\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career.\\n\\nCurrent Segment:\\nHe burns Mr. Morrison\u2019s letter and goes to visit galleries in London.\\n\\nReasoning:\\nHere, the object \u2018Mr. Morrison\u2019s letter\u2019 is not previously introduced, but is referred to familiarly. Therefore, it is marked incoherent.\\n\\nContext:\\nHe writes a letter to Mrs. Morrison expressing sympathy for her husband\u2019s suicide.\\n\\nFigure 10: Illustration of RefE errors provided to crowdworkers during training.\\n\\nD.3 SceneE\\nThese are called Abrupt Transition from the Previous Scene in the task interface. We provide the illustrative example shown in Figure 11 along with the following definition:\\n\\n\u201cThese refer to coherence errors where there is a sudden shift in the setting or the narrative in the story. These often happen in two scenarios:\\n1. There is an abrupt change in the people/characters being discussed and/or an abrupt change in the surroundings/event.\\n2. Scenarios where the previous scene\u2019s phrasing strongly implies that more information/events are forthcoming, but the previous scene gets abruptly cut off and a completely new scene starts.\\n\\nPlease choose full sentences as spans for this error type.\\n\\nContext:\\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career.\\n\\nHe becomes infatuated with Madame de Pastourelles, a beautiful and intelligent artist.\\n\\nReasoning:\\nHere, the scene suddenly shifts from the previous one (talking about Fenwick\u2019s infatuation), to a different scene where a character is threatened by a tramp. In this case, this entire next segment span should be selected, and annotated as \u2018Abrupt Scene Transition\u2019 Error.\\n\\nCurrent Segment:\\nFenwick\u2019s wife becomes frightened when a tramp threatens to kill her and her child.\\n\\nFigure 11: Illustration of SceneE errors provided to crowdworkers during training.\\n\\nD.4 InconE\\nFigure 12 shows an example of Inconsistent error shown to annotators.\\n\\n| Error | P | R | F1 | ov. |\\n|-------|---|---|----|-----|\\n| CharE | .86 | .74 | .80 | .99 |\\n| SceneE | .58 | .49 | .53 | 1.0 |\\n| RefE | .45 | .25 | .32 | .87 |\\n| InconE | .25 | .01 | .02 | 0.0 |\\n\\nTable 11: Performance of the T5-Large model fine-tuned on the SNAC dataset at predicting the correct error type in each summary sentence. We also report the percentage of times the predicted span overlaps with the error span in the gold data.\"}"}
{"id": "emnlp-2022-main-29", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"These refer to text spans that contradict previous content (either in the context or the next segment box itself.)\\n\\nNote: You will also be asked to highlight the \u2018previous\u2019 span that is contradictory to the selected span. Highlighting this previous span (from either the context or the next segment box itself) will populate the relevant input box automatically.\\n\\nContext:\\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career.\\n\\nStep 1: Highlight the span in the Next Segment box that is inconsistent with earlier text.\\nStep 2: Highlight the earlier span that is being contradicted. This will automatically populate the relevant text box.\\n\\nCurrent Segment:\\nHe moves to Paris to set up his workshop.\\n\\nContext:\\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career. This will automatically populate the relevant text box.\\n\\nCurrent Segment:\\nHe moves to Paris to set up his workshop.\\n\\nFigure 12: Illustration of InconE errors provided to crowdworkers during training.\\n\\nD.5 CorefE\\n\\nFigure 13 shows an example of Unclear Coreference provided to annotators.\\n\\n\u201cThese refer to errors where it is unclear who/what a pronoun or refers to.\\\"\\n\\nCurrent Segment:\\nKendall and Greenlee go to Aiden\u2019s house the next evening. She rings the doorbell.\\n\\n\u2018She\u2019 could be referring to either Kendall or Greenlee. This coreference is unclear.\\n\\nFigure 13: Illustration of CorefE errors provided to crowdworkers during training.\\n\\nD.6 RepE\\n\\nFigure 14 shows an example of Repetition errors.\\n\\n\u201cThese refer to spans where content is repeated. Note: For these, you will also be asked to highlight the \u2018previous\u2019 span that contains the same text/content as the selected span. Highlighting this previous span (from either the context or the next segment box itself) will populate the relevant input box automatically.\\n\\nStep 1: Highlight the span in the Next Segment box that is repeated\\nStep 2: Highlight the earlier span that is being repeated. This will automatically populate the relevant text box.\\n\\nCurrent Segment:\\nFenwick is an aspiring artist who searches for work in London.\\n\\nContext:\\nJohn Fenwick, an aspiring artist, accepts a loan from Mr. Morrison to move to London to pursue his art career.\\n\\nStep 1: Highlight the span in the Next Segment box that is repeated\\nStep 2: Highlight the earlier span that is being repeated. This will automatically populate the relevant text box.\\n\\nFigure 14: Illustration of RepE errors provided to crowdworkers during training.\\n\\nD.7 GramE\\n\\nThese are called Ungrammatical/Nonsensical in the interface.\\n\\n\u201cThese refer to text spans that have grammar errors. Also included in this category are cases...\u201d\\n\\nD.8 Task Interface\\n\\nIn Section 3.1, we described the annotation work for the SNAC framework. Figure 15 visually illustrates this overall workflow for annotating errors in segment $S_i$ with respect to the context, i.e. $S_0, S_1, ..., S_{i-1}$.\\n\\nwhere there are obvious commonsense errors or the text does not make any sense at all.\\n\\nD.8 Task Interface\\n\\nIn Section 3.1, we described the annotation work for the SNAC framework. Figure 15 visually illustrates this overall workflow for annotating errors in segment $S_i$ with respect to the context, i.e. $S_0, S_1, ..., S_{i-1}$.\\n\\nwhere there are obvious commonsense errors or the text does not make any sense at all.\\n\\nFigure 15: Workflow for annotating coherence errors in segment $S_i$ with respect to the context, i.e. $S_0, S_1, ..., S_{i-1}$.\\n\\nWe also include screenshots of our task instructions. Figure 17 explains the basic task to the annotators. Figure 18 shows the detailed task workflow and the steps to annotate errors in a text segment.\\n\\nFigure 19 shows an example annotation with multiple coherence errors for reference.\\n\\nE Datasheet for SNAC\\n\\nE.1 Motivation for Dataset Creation\\n\\nWhy was the dataset created?\\n\\nDespite recent interest in long document summarization research and generation of long narrative summaries (Kry\u015bcis\u0144ki et al., 2021; Zhang et al., 2022; Mao et al., 2021; Wu et al., 2021), we lack evaluation frameworks to compare these approaches and measure progress. Current automatic and human evaluation...\"}"}
