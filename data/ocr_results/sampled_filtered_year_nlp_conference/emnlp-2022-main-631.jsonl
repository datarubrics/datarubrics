{"id": "emnlp-2022-main-631", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In honor of Veterans Day, we salute all of the men and women who have served in America's armed forces.\\n\\nIn honor of Veterans Day, we salute all of the men and women who have served in America's armed forces.\\n\\nWhat is the scope for a food technologist in India?\\n\\nWhat is the scope of food technology in India?\\n\\nA bear is taking a walk through the forest.\\n\\nA bear is taking a walk in the woods.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-631", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Instruction of our crowdsourcing annotation on the Figure Eight platform for creating MULTI PIT CROWD.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: An example question of our crowdsourcing annotation on the Figure Eight platform for creating MULTIPITCROWD.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A and B is a paraphrase pair if:\\nCase 1: A and B are completely equivalent (mean the same thing, though differ in expression):\\nA: Chad from World of Jenks is so adorable.\\nB: Chad from World of Jenks is the absolute cutest!\\nExplanation: Two sentences convey the same meaning (liking Chad) using different expressions.\\nCase 2: B keeps the main meaning of A, but deletes some minor details from A:\\nA: Sweden's first female PM Magdalena Andersson, resigns on day one!\\nB: Swedish PM Magdalena Andersson resigns hours after taking job.\\nExplanation: The main content of A is about Magdalena Andersson resigning on day one, so deleting \\\"first female\\\" is fine and considered as simplification.\\nCase 3: B keeps the main meaning of A, and add new information based on commonsense or world knowledge:\\nA: Facebook announces it will be changing its name to Meta.\\nB: Facebook relaunches itself as 'Meta' in a clear bid to dominate the metaverse.\\nExplanation: The new added \\\"to dominate the metaverse\\\" is world knowledge as many people know it. We consider B as a paraphrase of A.\\n\\nA and B is a non-paraphrase pair if:\\nCase 1: B adds new information that requires fact-checking:\\nA: 100% of the 140,000 U.S. jobs lost in December were held by women.\\nB: In fact women lost 111% of the jobs in December because men gained 16,000 jobs.\\nExplanation: Even though both sentences are talking about the same thing, but B introduces new information that is not commonsense or world knowledge.\\nCase 2: A and B share some details but focus on different things:\\nA: Apple unveils new Macbook Airs and a Mac Pro.\\nB: I was pumped for the new macbook air.\\nExplanation: Two sentences are talking about different things: \\\"Apple unveils\\\" vs \\\"I was pumped\\\".\\nCase 3: A and B are on different topics:\\nA: Rhode Island Senate approves marriage equality by vote of 26-12\\nB: So glad to hear that the Kings are staying in Sac.\\nExplanation: Both sentences are completely irrelevant.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-631", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To rate Diversity, you just answer the following question: Is sentence 2 different from sentence 1?\\n\\nHere is each score (1 to 5) represents:\\n\\n5 - Uses more than 1 score 4 and 3 changes.\\n\\n4 - Uses one of the following types of change 1 time:\\n  - change of sentence structure\\n  - simplifying\\n  - adding new phrase or meaningful word\\n  - rearranging word order\\n  - using idiomatic expressions\\n  - change of part of speech\\n  - expanding a word in detail\\n  - synonym replacement phrase-wise (e.g. \\\"10 years\\\" <-> \\\"a decade\\\", \\\"hotel employee\\\" <-> \\\"bell boy\\\")\\n  - Or uses synonym replacement word-wise more than 2 times.\\n\\nNote: mark 5 if sentence 1 contains less than 6 words.\\n\\n3 - Uses synonym replacement word-wise 1 or 2 times.\\n\\n2 - Very simple grammatical changes such as:\\n  - determiners changes (remove or add \\\"the\\\", \\\"the\\\" <-> \\\"a\\\", \\\"a\\\" <-> \\\"one\\\", \\\"that\\\" <-> \\\"it\\\", \\\"his\\\" <-> \\\"this\\\", \\\"some\\\" <-> \\\"any\\\", ...)\\n  - contraction changes (\\\"n't\\\" <-> \\\"not\\\", \\\"'re\\\" <-> \\\"are\\\", \\\"will\\\" <-> \\\"'ll\\\", \\\"let's\\\" <-> \\\"let us\\\", ...)\\n  - singular and plural switching (\\\"a\\\" <-> \\\"some\\\", \\\"are\\\" <-> \\\"is\\\", add \\\"es/s\\\", ...)\\n  - tense changes (\\\"is\\\" <-> \\\"was\\\", \\\"did\\\" <-> \\\"have done\\\", \\\"is doing\\\" <-> \\\"do\\\", \\\"will\\\" <-> \\\"would\\\", ...)\\n  - number and text switching (\\\"7\\\" <-> \\\"seven\\\", \\\"five\\\" <-> \\\"5\\\", ...)\\n  - preposition changes (remove or add \\\"on\\\", \\\"at\\\" <-> \\\"on\\\", \\\"upon\\\" <-> \\\"on\\\", \\\"of\\\" <-> \\\"for\\\", ...)\\n  - adding or removing conjunction word or meaningless word (\\\"... that ...\\\" <-> ... ...\\\", \\\"And...\\\" <-> \\\"...\\\", \\\"just\\\", ...)\\n  - other cases (\\\"to\\\" <-> \\\"will\\\")\\n\\nNote: Multiple 2 changes is still a 2.\\n\\n1 - Copies sentence 1 completely.\\n\\nNote: we ignore lettercase and punctuation issue.\\n\\nExamples\\n\\nsentence 1: How beautiful it is .\\nsentence 2: Oh my god, this is so beautiful.\\n\\nDiversity score: 5\\n\\nNote: cases like \\\"is going to\\\" <-> \\\"will\\\", \\\"wanna\\\" <-> \\\"want to\\\", \\\"gonna\\\" <-> \\\"go to\\\" are word-wise synonym replacement as well.\\n\\nFigure 14: Instruction for rating diversity aspect in our human evaluation.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nThis paper addresses the quality issues in existing Twitter-based paraphrase datasets, and discusses the necessity of using two separate definitions of paraphrase for identification and generation tasks. We present a new Multi-Topic Paraphrase in Twitter (MUltiPI) corpus that consists of a total of 130k sentence pairs with crowdsourcing (MUltiPI-CROWD) and expert (MUltiPI-EXPERT) annotations using two different paraphrase definitions for paraphrase identification, in addition to a multi-reference test set (MUltiPI-NMR) and a large automatically constructed training set (MUltiPI-AUTO) for paraphrase generation. With improved data annotation quality and task-specific paraphrase definition, the best pre-trained language model fine-tuned on our dataset achieves the state-of-the-art performance of 84.2 F1 for automatic paraphrase identification. Furthermore, our empirical results also demonstrate that the paraphrase generation models trained on MUltiPI-AUTO generate more diverse and high-quality paraphrases compared to their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and ParaNMT.\\n\\n1 Introduction\\nParaphrases are alternative expressions that convey a similar meaning (Bhagat and Hovy, 2013). Studying paraphrase facilitates research in both natural language understanding and generation. For instance, identifying paraphrases on social media is important for tracking the spread of misinformation (Bakshy et al., 2011) and capturing emerging events (Vosoughi and Roy, 2016). On the other hand, paraphrase generation improves the linguistic diversity in conventional agents (Li et al., 2016) and machine translation (Thompson and Post, 2020). It has also been successfully applied in data argumentation to improve information extraction (Zhang et al., 2015; Ferguson et al., 2018) and question answering systems (Gan and Ng, 2019).\\n\\nFigure 1: Two sets of paraphrases in MUltiPI, discussing a trending topic or a news article, respectively.\\n\\nMany researchers have been leveraging Twitter data to study paraphrase given its lexical and style diversity as well as coverage of up-to-date events. However, existing Twitter-based paraphrase datasets, namely PIT-2015 (Xu et al., 2015) and Twitter-URL (Lan et al., 2017), suffer from quality issues such as topic unbalance and annotation noise, which limit the performance of the models trained using them. Moreover, past efforts on creating paraphrase corpora only consider one paraphrase criteria without taking into account the fact that the desired \u201cstrictness\u201d of semantic equivalence in paraphrases varies from task to task (Bhagat and Hovy, 2013; Liu and Soh, 2022). For example, for the purpose of tracking unfolding events, \u201cA tsunami hit Haiti.\u201d and \u201c303 people died because of the tsunami in Haiti\u201d are sufficiently close to be considered as paraphrases; whereas for paraphrase generation, the extra information \u201c303 people dead\u201d in the latter sentence may lead models to learn to...\"}"}
{"id": "emnlp-2022-main-631", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our Multi-Topic Paraphrase in Twitter (MULTIPITCROWD) Dataset\\n\\n| Topic   | #Train | #Dev | #Test | Sent/Tweet Len | %Paraphrase | #Trends/URLs | #Uniq Sent | %Multi-Ref |\\n|---------|--------|------|-------|----------------|-------------|-------------|------------|------------|\\n| Sports  | 25,255 | 3,157| 3,157 | 10.24 / 13.79  | 40.52%      | 1,201       | 34,786     | 17.89%     |\\n| Entertainment | 11,547 | 1,443| 1,444 | 10.44 / 13.80  | 62.33%      | 610         | 15,784     | 18.11%     |\\n| Event   | 8,624  | 1,078| 1,079 | 10.86 / 15.32  | 82.83%      | 359         | 11,746     | 17.75%     |\\n| Others  | 17,751 | 2,219| 2,219 | 10.41 / 14.56  | 67.16%      | 817         | 24,286     | 18.33%     |\\n| Science/Tech | 7,384  | 923  | 923   | 10.94 / 19.17  | 46.13%      | 1,032       | 10,327     | 17.74%     |\\n| Health  | 9,123  | 1,140| 1,141 | 11.29 / 21.68  | 46.78%      | 1,298       | 12,772     | 17.86%     |\\n| Politics | 7,981  | 998  | 998   | 10.95 / 18.48  | 56.56%      | 1,063       | 10,999     | 17.68%     |\\n| Finance | 4,552  | 569  | 569   | 11.19 / 23.08  | 18.96%      | 554         | 5,907      | 20.13%     |\\n| Total   | 92,217 | 11,527|11,530 |10.62 / 16.10   | 53.73%      | 6,934       | 124,438    | 18.65%     |\\n\\nOur MULTIPITEXPERT Dataset\\n\\n| Topic | #Train | #Dev | #Test | Sent/Tweet Len | %Paraphrase | #Trends/URLs | #Uniq Sent | %Multi-Ref |\\n|-------|--------|------|-------|----------------|-------------|-------------|------------|------------|\\n| Trends/URLs | 4,458  | 555  | 557   | 12.08 / 17.02  | 53.11%      | 200         | 5,743      | 100%       |\\n\\nExisting Twitter Paraphrase Datasets\\n\\n| Dataset                  | #Train | #Dev | #Test | Sent/Tweet Len | %Paraphrase | #Trends/URLs | #Uniq Sent | %Multi-Ref |\\n|--------------------------|--------|------|-------|----------------|-------------|-------------|------------|------------|\\n| PIT-2015 (Xu et al.)     | 13,063 | 4,727| 972   | 11.9 / \u2013       | 30.60%      | 420         | 19,297     | 24.67%     |\\n| Twitter URL (Lan et al.) | 42,200 | \u2013    | 9,324 | \u2013 / 14.8       | 22.77%      | 5,187       | 48,906     | 23.91%     |\\n\\nTable 1: Statistics of MULTIPITCROWD and MULTIPITEXPERT datasets. The sentence/tweet lengths are calculated based on the number of tokens per unique sentence/tweet. %Multi-Ref denotes the percentage of source sentences with more than one paraphrase. Compared with prior work, our MULTIPITCROWD dataset has a significantly larger size, a higher portion of paraphrases, and a more balanced topic distribution.\\n\\nIn this paper, we present an effective data collection and annotation method to address these issues. We curate the Multi-Topic Paraphrase in Twitter (MULTIPIT) corpus, which includes MULTIPITCROWD, a large crowdsourced set of 125K sentence pairs that is useful for tracking information on Twitter, and MULTIPITEXPERT, an expert annotated set of 5.5K sentence pairs using a stricter definition that is more suitable for acquiring paraphrases for generation purpose. Compared to PIT-2015 and Twitter-URL, our corpus contains more than twice as much data with more balanced topic distribution and better annotation quality. Two sets of examples from MULTIPIT are shown in Figure 1.\\n\\nWe extensively evaluate several state-of-the-art neural language models on our datasets to demonstrate the importance of having task-specific paraphrase definition. Our best model achieves 84.2 F1 for automatic paraphrase identification. In addition, we construct a continually growing paraphrase dataset, MULTIPITAUTO, by applying the automatic identification model to unlabelled Twitter data. Empirical results and analysis show that generation models fine-tuned on MULTIPITAUTO generate more diverse and high-quality paraphrases compared to models trained on other corpora, such as MSCOCO (Lin et al., 2014), ParaNMT (Wieting and Gimpel, 2018), and Quora.\\n\\nWe hope our MULTIPIT corpus will facilitate future innovation in paraphrase research.\\n\\n2 Multi-Topic PIT Corpus\\n\\nIn this section, we present our data collection and annotation methodology for creating MULTIPITCROWD and MULTIPITEXPERT datasets. The data statistics is detailed in Table 1.\\n\\n2.1 Collection of Tweets\\n\\nTo gather paraphrases about a diverse set of topics as illustrated in Figure 1, we first group tweets that contain the same trending topic (year 2014\u20132015) or the same URL (year 2017\u20132019) retrieved through Twitter public APIs over a long time period. Specifically, for the URL-based method, we extract the URLs embedded in the tweets that are posted by 15 news agency accounts (e.g., NYTScience, CNNPolitics, and ForbesTech). To get cleaner paraphrases, we split the tweets into sentences, eliminating the extra noises caused by multi-sentence tweets. More details of the improvements we made to address the data preprocessing issues in prior work are described in Appendix B.\\n\\n2.2 Topic Classification and Balancing\\n\\nTo avoid a single type of topics dominating the entire dataset as in prior work (Xu et al., 2015; Lan et al., 2017), we manually categorize the topics for each group of tweets and balance their distribution. For trending topics, we ask three in-house annotators to classify them into 4 different categories: sports, entertainment, event, and others.\\n\\n3 https://www.twitter.com/explore/tabs/trending\\n4 https://developer.twitter.com/en/docs/twitter-api\"}"}
{"id": "emnlp-2022-main-631", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Topic breakdown on 100 randomly sampled sentence pairs from MultiPIT, PIT-2015 and Twitter-URL. Our MultiPIT corpus has a more balanced topic distribution.\\n\\nWe include the tweets grouped by URLs that belong to the science/tech, health, politics, and finance categories.\\n\\n2.3 Candidate Selection\\n\\nThe PIT-2015 (Xu et al., 2015) and Twitter-URL (Lan et al., 2017) corpora contain only 23% and 31% sentence pairs that are paraphrases, respectively. To increase the portion of paraphrases and improve the annotation efficiency, we introduce an additional step to filter out the tweet groups that contain either too much noise or too few paraphrases, and adaptively select sentence pairs for annotation (\u00a72.4). For each of the trend-based groups, we first select the top 2 sentences using a simple ranking algorithm (Xu et al., 2015) based on the averaged probability of words. We pair each of these two sentences with 10 other sentences that are randomly sampled from the top 20 in each group. Among these 20 sentence pairs, if the annotators found $n \\\\in [4, 6]$ or $[7, 9]$ or $[10, 12]$ or $[13, 20]$ pairs as paraphrases, then we further deploy 20, 30, 40, or 50 sentence pairs for annotation, respectively. We pair one of the top 5 ranked sentences with 10 sentences randomly selected from those ranked between top 6 and top 50. Since the URL-based groups generally contain fewer sentences, we select the top 11 sentences and ask annotators to choose one as the seed sentence that can be paired with the rest 10 sentences to produce at least 3 paraphrase pairs. If such a seed sentence exists, we pair it with the rest 10 sentences and deploy them for annotation. Otherwise, we skip the entire group.\\n\\n2.4 Crowd Annotation for Paraphrase Identification\\n\\nWe then annotate the selected sentence pairs using the crowdsourcing platform Figure-Eight to construct MultiPIT CROWD. We design a 1-vs-1 annotation schema, where we present one sentence pair to workers at a time and ask them to annotate whether it is a paraphrase pair or not. A screenshot of the annotation interface is provided in Appendix A.1. We collect 6 judgments for every sentence pair and pay $0.2 per annotation ($7 per hour). For creating MultiPIT CROWD, with the purpose of identifying similar sentences and tracking information spreading on Twitter in mind, we consider two sentences as paraphrases even if one contains some new information that does not appear in the other sentence (see Figure 3 for examples). As a side note, because these sentences are grouped under the same trend or URL, the new information is always relevant and based on the context, otherwise, we will consider them non-paraphrases.\\n\\nQuality Control. In every five sentence pairs, we embed one hidden test sentence pair that are pre-labeled by one of the authors, and constantly monitor the workers' performance. Whenever annotators make a mistake on the test pair, they will be alerted and provided with an explanation. Workers can continue in the task if they achieve $>85\\\\%$ accuracy on the test pairs and $>0.2$ Cohen's (Cohen, 1960) kappa when compared with the major vote of other workers. All workers are in the U.S.\\n\\nInter-Annotator Agreement. The average Cohen's kappa is 0.75 for URL-sourced sentence pairs.\\n\\n6https://www.appen.com/\"}"}
{"id": "emnlp-2022-main-631", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Two different paraphrase definitions used for creating M\\\\textsc{ulti-pit-crowd} and M\\\\textsc{ulti-pit-expert}, with examples. The difference between the two criteria is whether considering Sentence2 that contains new information that requires fact-checking as a paraphrase of Sentence1.\\n\\nWe also sample 400 sampled sentence pairs and hire two experienced in-house annotators to label them. Assuming the in-house annotation is gold, the F\\\\textsubscript{1} of crowdworkers' majority vote is 89.1.\\n\\nAccessing Topic Diversity.\\n\\nWe manually examine 100 sentence pairs randomly sampled from M\\\\textsc{ulti-pit-crowd}, PIT-2015 (Xu et al., 2015) and Twitter-URL (Lan et al., 2017). Figure 2 shows the results of the manual inspection. M\\\\textsc{ulti-pit-crowd} has a much more balanced topic distribution, compared to prior work where 58% of sentences in PIT-2015 are about sports and 63% of sentences in Twitter-URL are politics-related. This improvement can be attributed to the long time period (\u00a72.1) and topic classification step (\u00a72.2) in our data collection process. In contrast, PIT-2015 was collected within only 10 days (04/24/2013 \u2013 05/03/2013) that was overwhelmed by a popular sports event \u2013 the 2013 NFL draft (04/25 - 04/27), and Twitter-URL was collected during the 3 months of the 2016 US presidential election.\\n\\n2.5 Expert Annotation for Paraphrase Generation\\n\\nText generation models are prone to memorize training data and generate unfaithful hallucinations (Maynez et al., 2020; Carlini et al., 2021). Including paraphrase pairs that contain extra information other than world or commonsense knowledge in the training data only worsens the problem, as shown in Table 15 in Appendix F. For the purpose of paraphrase generation, we further create M\\\\textsc{ulti-pit-expert} with expert annotations, using a stricter paraphrase definition than the one used in M\\\\textsc{ulti-pit-crowd}. The different paraphrase criteria used for creating these two datasets and their corresponding examples are illustrated in Figure 3.\\n\\nData Selection.\\n\\nTo create a high-quality corpus that focuses on differentiating strict paraphrases from the more loosely defined ones, we first use our best paraphrase identifier (\u00a73) fine-tuned on M\\\\textsc{ulti-pit-crowd} to filter the sentence pairs and then have experienced in-house annotators to further annotate them. Specifically, we gather sentence pairs that are identified as paraphrases by the automatic classifier from 9,762 trending topic groups (from Oct-Dec 2021) and 181,254 URL groups (from Jan 2020-Jun 2021). To improve the diversity of our dataset, instead of presenting these pairs directly to the experts for annotation, we cluster the sentences by considering the paraphrase relationship transitive, i.e., if sentence pairs (s\\\\textsubscript{1},s\\\\textsubscript{2}) and (s\\\\textsubscript{2},s\\\\textsubscript{3}) are both identified as paraphrases, then (s\\\\textsubscript{1},s\\\\textsubscript{2},s\\\\textsubscript{3}) is a cluster. For each trend or URL, we show two seed sentences paired with up to 30 sentences in the largest cluster for the experts to annotate. In total, we have 5,570 sentence pairs annotated for M\\\\textsc{ulti-pit-expert}, in which 100 sentences sourced by trend and 100 ones sourced by URL have at least 8 corresponding paraphrases. We use these 200 sets to form M\\\\textsc{ulti-pit-nmr}, the first multi-reference test set for paraphrase generation evaluation (\u00a74).\\n\\nExpert Annotation.\\n\\nWe ask two experienced annotators with linguistic backgrounds and rich annotation experience to annotate each sentence pair as paraphrases or not. Annotators thoroughly discuss\"}"}
{"id": "emnlp-2022-main-631", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Results on the test sets of MULTI PIT CROWD and MULTI PIT EXPERT. Models are fine-tuned on the corresponding training set. DeBERTaV3 large performs the best on both datasets. LR: learning rate.\\n\\n| Model       | #Params | LR    | Precision | Recall | F1     | Accuracy |\\n|-------------|---------|-------|-----------|--------|--------|----------|\\n| ESIM        | 17M     | 4e-4  | 89.55     | 70.15  | 78.67  | 82.15    |\\n| Infersent   | 47M     | 1e-3  | 87.03     | 87.57  | 87.29  | 86.47    |\\n| T5 base     | 220M    | 1e-4  | 89.21     | 93.76  | 91.43  | 90.67    |\\n| T5 large    | 770M    | 1e-4  | 90.36     | 93.58  | 91.94  | 91.29    |\\n| BERT base   | 109M    | 3e-5  | 88.59     | 91.24  | 89.90  | 89.12    |\\n| BERT large  | 335M    | 2e-5  | 88.73     | 93.17  | 90.90  | 90.10    |\\n| RoBERTa large | 355M   | 2e-5  | 90.81     | 92.70  | 91.74  | 91.14    |\\n| BERTweet large | 355M  | 2e-5  | 89.72     | 93.95  | 91.79  | 91.08    |\\n| ALBERTV2 xxlarge | 235M | 1e-5  | 90.36     | 92.96  | 91.64  | 91.00    |\\n| DeBERTaV3 large | 400M | 5e-6  | 90.46     | 93.59  | 92.00  | 91.36    |\\n\\nTable 3: Results of different methods on the test set of MULTI PIT EXPERT. MC: MULTI PIT CROWD, ME: MULTI PIT EXPERT. We use DeBERTaV3 large in the experiments.\\n\\n| Method | Data | P. R. | F1  | Acc. |\\n|--------|------|-------|-----|------|\\n| Fine-tuning MC | 61.81 | 88.58 | 72.82 | 69.84 |\\n| Fine-tuning ME | 82.56 | 83.86 | 83.20 | 84.56 |\\n| Fine-tuning MC + ME | 62.99 | 87.80 | 73.36 | 70.92 |\\n| + Filtering MC + ME | 77.24 | 88.19 | 82.35 | 82.76 |\\n| + Flipping MC + ME | 83.40 | 85.04 | 84.21 | 85.46 |\\n\\n3.2 Results\\n\\nTable 2 presents results for the models fine-tuned on each dataset. DeBERTaV3 large achieves the best results with 92 F1 on MULTI PIT CROWD and 83.2 F1 on MULTI PIT EXPERT. Transformer-based models consistently outperform BiLSTM-based models, especially on MULTI PIT EXPERT.\\n\\nBeyond Fine-tuning.\\n\\nAs MULTI PIT CROWD is a large-scale dataset annotated with a loose paraphrase definition, we test whether leveraging these \u201cnoisy\u201d data improves model performance on MULTI PIT EXPERT. To reduce the noise that comes from the difference in definitions, we first adjust the labeling threshold for MULTI PIT CROWD from 3 to 4. Then we consider two noisy training techniques adopted in prior work (Xie et al., 2020; Zhang and Sabuncu, 2018), namely filtering and flipping. Specifically, we fine-tune a teacher model on MULTI PIT EXPERT and use it to go through MULTI PIT CROWD as follows: for each sentence pair \\\\( p \\\\), if its label is \\\\( i \\\\) (0 for non-paraphrase, 1 for paraphrase) and \\\\( P_{\\\\text{teacher}}(y = i | p) \\\\leq \\\\lambda \\\\), we filter out \\\\( p \\\\) or flip its label to \\\\( 1 - i \\\\) (i.e. 0 \u2192 1).\\n\\nWe perform a small grid search on \\\\( \\\\lambda \\\\) over \\\\{0.05, 0.15, 0.25, 0.35, 0.45\\\\}, and find 0.35 works well for the filtering method and 0.25 for the flipping method.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Performance at Various Data Size\\n\\nFigure 4: Test set performance of model fine-tuned on varying amounts of data in MULTIEXPERT. The experimental results are shown in Table 3. Compared to fine-tuning on MULTIEXPERT, adding the original MULTIPEWD to the training data results in a 9.8 and 19.5 points drop in F1 and precision, respectively, demonstrating the necessity of task-specific paraphrase definition. Among all methods, the flipping approach achieves the best F1 of 84.2. We thus use it to create MULTIAUTO (\u00a74).\\n\\n3.3 Impact of Data Size\\n\\nFigure 4 shows test set performance of DeBERTaV3 large fine-tuned on different amounts of data in MULTIEXPERT. As there are 156 trend/URL groups in the train set, we truncate the data by group. With more training data, the model achieves better F1 and accuracy but in a slower fashion compared to the early stage. This finding suggests that annotating more data can further improve the model's performance.\\n\\n4 Paraphrase Generation\\n\\nParaphrase generation is a task that rewrites the input sentence while preserving its semantic meaning. Since new data is generated on Twitter every day, we introduce MULTIAUTO, an automated continual growing dataset for paraphrase generation. We show that the model fine-tuned on MULTIAUTO generates more diverse and high-quality paraphrases than other paraphrase datasets.\\n\\n4.1 Comparison with Existing Datasets\\n\\nMSCOCO (Lin et al., 2014), and ParaNMT (Wieting and Gimpel, 2018), and Quora are three widely used datasets in paraphrase generation research (Zhou and Bhat, 2021). The Quora dataset contains over 400K question pairs, including 144K pairs labeled as duplicated (i.e., paraphrase), which are split into 134K/5K/5K as train/dev/test sets, respectively. MSCOCO consists of over 120K images, each of which has five captions. Following Chen et al. (2020), for each image, we randomly pick a caption and pair it with each of the other four captions, resulting in about 490K paraphrase pairs. We split them into train/dev/test sets with 330K/80K/80K pairs, respectively. ParaNMT is a dataset with more than 50 million paraphrase pairs that are automatically generated through back-translation. Since back-translation may introduce noise, we use the manually labeled dev and test sets from Chen et al. (2019), which contain 499 and 871 instances, respectively.\\n\\nWe use the best performing model in Section 3 to extract paraphrase pairs from recent Twitter data (trending topics in Oct-Dec 2021 and URLs in Jan 2020-Jun 2021). We call these automated identified paraphrase pairs MULTIAUTO, which contains 302,307 pairs. One of the authors manually annotates 215 paraphrase pairs and uses them as the dev set. We use the multi-reference MULTINMR test set (\u00a72.5) for evaluation. As the test set and MULTIAUTO come from the same time period, we filter out sentence pairs in MULTIAUTO that share similar trends or URLs with the pairs from the test set. This leaves us with 290,395 pairs as the training set.\\n\\nFollowing Chen et al. (2019), we remove paraphrase pairs with high BLEU scores in each training set to ensure there is enough variation between paraphrases, leaving about 137K pairs for MULTIAUTO, 47K for Quora, 275K for MSCOCO, and 443K for ParaNMT. Table 14 in Appendix F shows BLEU filtering improves model performance for all datasets. Detailed dataset statistics are provided in Appendix E.\\n\\n4.2 Evaluation Metrics\\n\\nWe consider four automated metrics that are commonly used in previous work (Li et al., 2019; Niu et al., 2021) for paraphrase generation: BLEU (Papineni et al., 2002), Self-BLEU (Liu et al., 2021), BERT-Score (Zhang et al., 2020), and BERT-iBLEU (Niu et al., 2021). Self-BLEU is BLEU\"}"}
{"id": "emnlp-2022-main-631", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Test set results of different transformer models fine-tuned on MULTIPITUTO, except GPT-3, where in-context learning is used. BL: BLEU, S-B: Self-BLEU, B-S: BERT-Score, B-iB: BERT-iBLEU. LR: learning rate. Bold: the best. The Self-BLEU of human reference is calculated by taking the min/avg/max score of the 8 references for each input sentence first, and then averaging across all scores.\\n\\n4.3 Generation Models\\n\\nWe consider two autoregressive language models, GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020), and two encoder-decoder language models, BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). For GPT-3, we try both zero-shot and few-shot (4 examples) setups using in-context learning without any fine-tuning. For other models, we fine-tune seven configurations of them on MULTIPITUTO. Table 4 shows the test set results of each model and the diversity of human references measured by Self-BLEU. Among all models, the few-shot setting of GPT-3 achieves the highest BERT-iBLEU score, and the zero-shot setting achieves the second-best number with only 1 point behind, which is not surprising given its size. Compared to GPT-3 generations, human references are much more diverse with a decrease of 24.5 in Self-BLEU under the best case and 13.5 under the average case, indicating that there is still a big gap between large language models and humans. For supervised small-scale models, T5large outperforms others with the best Self-BLEU and BERT-iBLEU scores. Although BARTlarge gets the highest BLEU score, our experiments in Appendix F show BERT-iBLEU has the best correlation with human evaluation. We thus use T5large in all the rest experiments.\\n\\nFor all models except GPT-3, we use beam search with beam size = 4. Please refer to Appendix C for details on the training setup and hyperparameter tuning. GPT-3 prompting and hyperparameter setup are provided in Appendix D. Generation examples are displayed in Figure 16 in Appendix G.\\n\\nImpact of Data Size.\\n\\nFigure 5 shows test set performance of T5large fine-tuned on different amount of data in MULTIPITUTO from 1K to 137K. With more training data, the model generates more diverse and high-quality paraphrases as Self-BLEU decreases (improves) and BERT-iBLEU increases. This suggests that the paraphrase generation models will benefit from the continually growing size of our MULTIPITUTO corpus.\\n\\n4.4 Cross-Dataset Generalization\\n\\nBuilding a paraphrase generation model that generalizes to new data is always an ambitious goal. To better understand the generalizability of each dataset, we fine-tune T5large on MULTIPITUTO, Quora, MSCOCO, and ParaNMT separately and evaluate their performance across datasets. For fair comparisons, we use the same architecture, T5large, in this experiment. Appendix G displays examples generated by these models on each dataset.\\n\\nTable 5 presents automatic evaluation of test set performance across all four datasets. As MULTIPITUTO and ParaNMT consist of sentences in different styles, models trained on them have better generalizability, achieving the best cross-domain...\"}"}
{"id": "emnlp-2022-main-631", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Automatic evaluation of models fine-tuned on four datasets. Here, BL: BLEU, S-B: Self-BLEU, B-S: BERT-Score, B-iB: BERT-iBLEU.\\n\\n| Model       | Fluency | Semantic Similarity | Diversity |\\n|-------------|---------|---------------------|-----------|\\n| MultiPIT-Auto | 41.14   | 85.86               | 77.79     |\\n| ParaNMT     | 33.34   | 77.79               | 26.28     |\\n| Quora       | 77.79   | 67.31               | 19.69     |\\n| MSCOCO      | 26.28   | 46.98               | 91.73     |\\n\\nTable 6: Human evaluation results on generations by model fine-tuned on MultiPIT-Auto or ParaNMT.\\n\\nModel Fluency Semantic Similarity Diversity\\nMultiPIT-Auto 4.98 4.67 3.59\\nParaNMT 4.95 4.64 3.40\\n\\n13 The input is 4 \u00d7 50 sentences from each test set.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Paraphrase types with examples and statistics observed in the generations by models fine-tuned on MULTI AUTO (MAUTO) or ParaNMT. Statistics are based on manual inspection of generations by each model on 200 sampled sentences. The shown generation example for each type is by model with the higher value (bold).\"}"}
{"id": "emnlp-2022-main-631", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nEytan Bakshy, Jake M Hofman, Winter A Mason, and Duncan J Watts. 2011. Everyone's an influencer: quantifying influence on twitter. In Proceedings of the fourth ACM international conference on Web search and data mining.\\n\\nRahul Bhagat and Eduard Hovy. 2013. What is a paraphrase? Computational Linguistics, pages 463\u2013472.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems.\\n\\nNicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models. In USENIX Security Symposium.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). Association for Computational Linguistics.\\n\\nTuhin Chakrabarty, Debanjan Ghosh, Adam Poliak, and Smaranda Muresan. 2021. Figurative language in recognizing textual entailment. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.\\n\\nMingda Chen, Qingming Tang, Sam Wiseman, and Kevin Gimpel. 2019. Controllable paraphrase generation with a syntactic exemplar. In Proceedings of the Association for Computational Linguistics.\\n\\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference. In Proceedings of the Association for Computational Linguistics.\\n\\nWenqing Chen, Jidong Tian, Liqiang Xiao, Hao He, and Yaohui Jin. 2020. A semantically consistent and syntactically variational encoder-decoder framework for paraphrase generation. In Proceedings of International Conference on Computational Linguistics.\\n\\nJacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement.\\n\\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of Empirical Methods in Natural Language Processing.\\n\\nDipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\\n\\nWilliam B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).\\n\\nQingxiu Dong, Xiaojun Wan, and Yue Cao. 2021. Parasci: A large scientific paraphrase dataset for longer paraphrase generation. In Proceedings of the European Chapter of the Association for Computational Linguistics.\\n\\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In Proceedings of the Association for Computational Linguistics.\\n\\nJames Ferguson, Colin Lockard, Daniel Weld, and Hannaneh Hajishirzi. 2018. Semi-supervised event extraction with paraphrase clusters. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\\n\\nWee Chung Gan and Hwee Tou Ng. 2019. Improving the robustness of question answering systems to question paraphrasing. In Proceedings of the Association for Computational Linguistics.\\n\\nJuri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. Ppdb: The paraphrase database. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\\n\\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. ArXiv.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-631", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations.\\n\\nQizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. 2020. Self-training with noisy student improves imagenet classification. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nWei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).\\n\\nCongle Zhang, Stephen Soderland, and Daniel S. Weld. 2015. Exploiting parallel news streams for unsupervised event extraction. Transactions of the Association for Computational Linguistics.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. Proceedings of International Conference on Learning Representation.\\n\\nZhilu Zhang and Mert Rory Sabuncu. 2018. Generalized cross entropy loss for training deep neural networks with noisy labels. In Proceedings of Advances in Neural Information Processing Systems.\\n\\nJianing Zhou and Suma Bhat. 2021. Paraphrase generation: A survey of the state of the art. In Proceedings of Empirical Methods in Natural Language Processing.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Annotation Interface\\n\\nA.1 Crowdsourcing\\n\\nFigure 9 and Figure 10 display screenshots of the instruction and an example question of our crowdsourcing annotation for MULTICROWD.\\n\\nA.2 Expert\\n\\nFigure 11 displays a screenshot of the instruction of our expert annotation for MULTIPITEXPERT.\\n\\nB Data Pre-processing\\n\\nBoth PIT-2015 (Xu et al., 2015) and Twitter URL (Lan et al., 2017) datasets share similar pre-processing steps that introduced tokenization and sentence splitting errors. Moreover, PIT-2015 contains some spam patterns, such as \\\"Follow Me PLEASE\\\". We improved the quality of our dataset by fixing the pre-processing methods and removing spam patterns. More importantly, we split tweets into sentences to get cleaner paraphrases (see Table 8 for an example), without added noises from extra sentences in the tweet. We improve the sentence splitting script by Xu et al. (2015) and tokenization script by O'Connor et al. (2010) used in prior work with a number of errors fixed:\\n\\n1. Emojis and most symbols are cleaned while punctuation are kept;\\n2. Extremely short sentences (<5 tokens) are filtered out while remaining sentences are deduplicated by comparing lowercased strings w/o any punctuation.\\n\\nC Implementation Details\\n\\nWe use HuggingFace Transformers (Wolf et al., 2020) version of all pre-trained models. We use Python 3.8, PyTorch 1.9.0, and Transformers 4.12.0. For all experiments, we use 4 \u00d7 48GB NVIDIA A40 GPUs.\\n\\nParaphrase Identification. Hyperparameters for fine-tuning models in paraphrase identification experiments are given in Table 9.\\n\\nFor T5 model, we consider learning rates \u2208 {1e-4, 3e-4, 1e-5, 3e-5}. For DeBERTaV3 model, we consider learning rates \u2208 {1e-5, 3e-5, 5e-6, 8e-6} following He et al. (2021). We fine-tune for 5 epochs and eval every 500 steps (every epoch if total training steps is less than 1500) on the dev set. The only hyperparameter we tune is the learning rate and use $F_1$ on the dev set for model selection.\\n\\nFor Infersent and ESIM models, we use their original implementation initialized with GloVe embedding (Pennington et al., 2014), and also only tune the learning rate based on the dev set.\\n\\nParaphrase Generation. Hyperparameters for fine-tuning models in paraphrase generation experiments are given in Table 10.\\n\\nWe use perplexity on the dev set for model selection.\\n\\nAs ParaNMT contains only lowercase letters, we lowercase the input and references for generation and evaluation of the model fine-tuned on ParaNMT and lowercase the other models' generations while evaluating on ParaNMT.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D GPT-3 Setup\\nD.1 Hyperparameters\\nWe use the text-davinci-002 GPT-3 model for paraphrase generation. To generate paraphrase, we use the following hyperparameters: temperature=1, max tokens=100, top-p=0.9, best of=1, frequency penalty=0.5, presence penalty=0.5, based on Chakrabarty et al. (2021).\\n\\nD.2 Prompts\\nZero-shot setting:\\nYour task is to generate a diverse paraphrase for a given sentence.\\nSentence: {sentence}\\nParaphrase:\\n\\nFew-shot setting:\\nYou will be presented with examples of some input sentences and their paraphrases. Your task is to generate a diverse paraphrase for a given sentence.\\nSentence: Mike Bloomberg is sending $18 million from his defunct presidential campaign to the DNC.\\nParaphrase: Mike Bloomberg is transferring $18M from his campaign to DNC, stretching campaign finance law.\\nSentence: Google Assistant on Android can read web pages to you.\\nParaphrase: Google Assist lets your Android devices read entire web pages aloud.\\nSentence: Charlie Patino scored a goal on his debut!\\nParaphrase: Charlie Patino's debut and he capped it off with a goal.\\nSentence: Khem Birch is the difference maker for the Raptors this game.\\nParaphrase: Khem Birch may be the MVP tonight for the Raptors.\\n\\nTable 11: Statistics of datasets for paraphrase generation.\\nWe calculate sentence length based on the number of tokens per unique sentence. As ParaNMT is too large, we sample 500K for the calculation of sentence length and BLEU. W/o BF denotes without BLEU filtering.\\n\\n|                | MULTI | AUTO | Quora | MSCOCO | ParaNMT |\\n|----------------|-------|------|-------|--------|---------|\\n| Genre          | Twitter | Question | Description | Novels, Laws |        |\\n| Sentence Length| 11.34 | 9.66 | 10.49 | 11.33 |         |\\n| Sentence BLEU  | 24.48 | 26.37 | 9.30  | 24.85  |         |\\n| Train/dev/test split | | | | | |\\n| #Train w/o BF  | 290,395 | 134,378 | 331,330 | 50M    |         |\\n| #Train         | 136,645 | 47,393 | 275,583 | 443,512 |         |\\n| #Dev           | 215     | 5,255 | 20,186 | 499    |         |\\n| #Test          | 200     | 5,255 | 20,187 | 781    |         |\\n| #Test Refs     | 8       | 1.34 | 4     | 1      |         |\\n\\nTable 12: Spearman correlations with human evaluation on 100 generations on MULTI\\nNMR (50 by model trained on MULTI AUTO and 50 by model trained on ParaNMT). Here, \u2217\u2217\u2217: p < 0.0001, \u2217\u2217: p < 0.001, \u2217: p < 0.01. Overall is the summation score of all three aspects.\\n\\n|                | Fluency     | Semantic Diversity | Overall Similarity |\\n|----------------|-------------|--------------------|--------------------|\\n| BLEU           | 0.212       | -0.233             | -0.091             |\\n| Self-BLEU      | \u2193 0.068     | 0.412              | -0.655             |\\n| BERT-Score     | 0.062       | -0.722             | -0.507             |\\n| BERT-iBLEU     | -0.166      | 0.370              | 0.381              |\\n\\nTable 13: Spearman correlations with human evaluation on all 400 generations. Here, \u2217\u2217\u2217: p < 0.0001, \u2217\u2217: p < 0.001, \u2217: p < 0.01.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Label distribution of 1200 ratings on 400 generations by models fine-tuned on MULPIT AUTO and ParaNMT.\\n\\nFigure 8: MULPIT AUTO dev set performance on various BLEU filtering thresholds.\\n\\nBLEU Filtering. We evaluate different BLEU thresholds on the dev set of MULPIT AUTO as shown in Figure 8. The model achieves the best performance at the threshold of 14, which is used across our experiments.\\n\\nNext, we compare model performance on all four datasets with and without BLEU filtering. Results are presented in Table 14. Applying BLEU filtering improves model performance with higher BERT-iBLEU on all datasets.\\n\\nTable 14: In-domain test set results of fine-tuning model on data with or without BLEU filtering. w/o BF denotes without BLEU filtering.\\n\\nImpact of Definition. We investigate how different paraphrase definitions affect generation performance. As shown in Table 15, model fine-tuned on MULPIT AUTO outperforms fine-tuning on the loosely defined data such as MULPIT CROWD.\\n\\nData Size BL S-B \u2193 B-S B-iB\\n\\nMULPIT CROWD 26,091 36.15 32.09 85.53 74.19\\nMULPIT-AUTO-CROWD 326,517 45.55 37.90 85.80 74.12\\nMULPIT-AUTO 136,645 41.14 33.34 85.86 77.79\\n\\nTable 15: Test set results of models fine-tuned on data constructed with different paraphrase definitions. MULPIT CROWD contains its paraphrase pairs. MULPIT-AUTO-CROWD is the automatically identified paraphrase pairs by the identifier fine-tuned on MULPIT CROWD.\\n\\nG Examples\\n\\nGeneration Examples. Table 16 presents generation examples by GPT-3 and fine-tuned T5 large on MULPIT NMR.\\n\\nTable 17 presents generation examples by T5 large fine-tuned on MULPIT AUTO, Quora, MSCOCO, and ParaNMT.\\n\\nMulti-Reference Examples. Table 18 displays three examples from the MULPIT NMR test set.\\n\\nH Human Evaluation Details\\n\\nWe display our human evaluation instruction for each aspect (fluency, semantic similarity, diversity) in Figure 12,13,14.\"}"}
{"id": "emnlp-2022-main-631", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Today is the winter solstice, marking the beginning of winter in the Northern Hemisphere.\\n\\nCory Booker and Elizabeth Warren have tested positive for COVID-19.\\n\\nMalcolm Subban with the save of the year.\\n\\nFormer Oklahoma quarterback Spencer Rattler is transferring to the University of South Carolina.\\n\\nThe Vancouver Canucks have let go of head coach Travis Green and replaced him with Bruce Boudreau.\\n\\nWhat a fitting tribute to Eddie Kidd on Top Gear.\\n\\nHow to become immune to Secondhand Stress\\n\\nTwitter is testing a new 24-hour Stories feature called Fleets.\\n\\n| Source sentence | Generated sentences |\\n|----------------|---------------------|\\n| Today is the winter solstice, marking the beginning of winter in the Northern Hemisphere. | T5 large \u2192 Today marks the beginning of winter in the Northern Hemisphere. ; GPT-3 zero-shot \u2192 Today is the first day of winter in the Northern Hemisphere. ; GPT-3 few-shot \u2192 Today is the winter solstice, which means winter has arrived in the Northern Hemisphere. |\\n| Elizabeth Warren and Cory Booker test positive for COVID-19 | T5 large \u2192 Cory Booker and Elizabeth Warren have tested positive for COVID-19. ; GPT-3 zero-shot \u2192 In a breakthrough, Elizabeth Warren and Cory Booker test positive for COVID. ; GPT-3 few-shot \u2192 Elizabeth Warren and Cory Booker have both contracted COVID-19 in what are being called breakthrough cases. |\\n| that malcolm subban save is f*cking incredible | T5 large \u2192 Malcolm Subban with the save of the year. ; GPT-3 zero-shot \u2192 That save by Malcolm Subban was insane! ; GPT-3 few-shot \u2192 That Malcolm Subban save was absolutely incredible. |\\n| that malcolm subban save is f*cking incredible | T5 large \u2192 Former Oklahoma quarterback Spencer Rattler is transferring to the University of South Carolina. ; GPT-3 zero-shot \u2192 Former Oklahoma quarterback Spencer Rattler is transferring to the University of South Carolina. ; GPT-3 few-shot \u2192 former OU quarterback Spencer Rattler is transferring to the University of South Carolina. |\\n| The Vancouver Canucks have fired head coach Travis Green and hired Bruce Boudreau as a replacement. | T5 large \u2192 Canucks fire Travis Green, hire Bruce Boudreau as new head coach ; GPT-3 zero-shot \u2192 The Vancouver Canucks have replaced head coach Travis Green with Bruce Boudreau. ; GPT-3 few-shot \u2192 The Vancouver Canucks have let go of head coach Travis Green and replaced him with Bruce Boudreau. |\\n| The Vancouver Canucks have fired head coach Travis Green and hired Bruce Boudreau as a replacement. | T5 large \u2192 Twitter is testing a new 24-hour stories feature called Fleets. ; GPT-3 zero-shot \u2192 Twitter is testing out a new 24-hour Stories feature called Fleets. ; GPT-3 few-shot \u2192 Twitter is testing a new 24-hour Stories feature called Fleets. |\\n| Twitter is trialling its own 24-hour Stories feature called Fleets | T5 large \u2192 Twitter is testing a new 24-hour stories feature called Fleets. ; GPT-3 zero-shot \u2192 Twitter is testing out a new 24-hour Stories feature called Fleets. ; GPT-3 few-shot \u2192 Twitter is testing a new 24-hour Stories feature called Fleets. |\\n\\nTable 16: Paraphrase generation examples by GPT-3 and fine-tuned T5 large on MUlti PInT NMR.\"}"}
