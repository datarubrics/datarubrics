{"id": "emnlp-2022-main-93", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation\\n\\nYu Zhao1, Jianguo Wei1, Zhichao Lin2, Yueheng Sun1, Meishan Zhang3, Min Zhang3\\n\\n1 College of Intelligence and Computing, Tianjin University\\n2 School of New Media and Communication, Tianjin University\\n3 Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen)\\n\\n{zhaoyucs,jianguo,chaosmyth,yhs}@tju.edu.cn, {zhangmeishan,zhangmin2021}@hit.edu.cn\\n\\nAbstract\\n\\nImage-to-text tasks, such as open-ended image captioning and controllable image description, have received extensive attention for decades. Here, we further advance this line of work by presenting Visual Spatial Description (VSD), a new perspective for image-to-text toward spatial semantics. Given an image and two objects inside it, VSD aims to produce one description focusing on the spatial perspective between the two objects. Accordingly, we manually annotate a dataset to facilitate the investigation of the newly-introduced task and build several benchmark encoder-decoder models by using VL-BART and VL-T5 as backbones. In addition, we investigate pipeline and joint end-to-end architectures for incorporating visual spatial relationship classification (VSRC) information into our model. Finally, we conduct experiments on our benchmark dataset to evaluate all our models. Results show that our models are impressive, providing accurate and human-like spatial-oriented text descriptions. Meanwhile, VSRC has great potential for VSD, and the joint end-to-end architecture is the better choice for their integration. We make the dataset and codes public for research purposes.\\n\\n1 Introduction\\n\\nText generation from images is a widely-adopted means for deep understanding of cross-modal data that has received increasing interest of both computer vision (CV) and natural language processing (NLP) communities (He and Deng, 2017). Image-to-text tasks generate natural language texts to assist in understanding the scene meaning of a specific image, which might be beneficial for a variety of applications such as image retrieval (Diao et al., 2021; Ahmed et al., 2021), perception assistance (Xu et al., 2018; Shashirangana et al., 2021), pedestrian detection (Hasan et al., 2021), and medical system (Miura et al., 2021).\\n\\nOur Task: VSD\\n\\nA man is walking behind a red car from right to left.\\nA red car is parked to the left of a pole.\\n\\nFigure 1: A comparison of three example image-to-text generation tasks and the proposed VSD in this work.\\n\\nImage-to-text tasks take on various forms when serving different purposes. Figure 1 illustrates a comparison of three example tasks. First, the generic open-ended image captioning aims to provide a summarised description that describes an input image and reflects the overall understanding of the image (Lindh et al., 2020; Vinyals et al., 2015; Ji et al., 2020). Furthermore, the verb-specific semantic roles (VSR) guided captioning (Chen et al., 2021) and visual question answering (VQA) (Antol et al., 2015) are two examples of controllable image description, which produce human-like and stylized descriptions under specified conditions based on a thorough comprehension of the input image (Chen et al., 2021; Fei et al., 2021b; Mathews et al., 2018; Cornia et al., 2019; Lindh et al., 2020; Pont-Tuset et al., 2020; Deng et al., 2020; Zhong et al., 2020; Kim et al., 2019; Chen et al., 2020a; Fei et al., 2022; Jhamtani and Berg-Kirkpatrick, 2018).\\n\\nThe VSR-guided captioning produces a description focusing on a verb with specified semantic roles, and the VQA generates a reasoning answer based on a given question.\"}"}
{"id": "emnlp-2022-main-93", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tics is a fundamental aspect of both language and image interpretation in relation to human cognition (Zlatev, 2007), and it has shown great value in spatial-based applications such as automatic navigation, personal assistance, and unmanned manipulation (Irshad et al., 2021; Raychaudhuri et al., 2021; Zeng et al., 2018). Here, we introduce a new task, Visual Spatial Description (VSD), which generates text pieces to describe the spatial semantics in the image. The task takes an image with two specified objects in it as inputs and outputs one sentence that describes the detailed spatial relation of the objects. We manually annotate a dataset for inquiry to benchmark this task.\\n\\nVSD is a typical vision-language generation problem that can be addressed by multi-modal encoder-decoder modeling. Multi-modal models allow both visual and linguistic inputs and encode them to a joint representation that can learn information from both modal inputs. Moreover, recent studies show that vision-language pretraining can bring remarkable achievements in most image-to-text tasks (Lu et al., 2019; Sun et al., 2019; Tan and Bansal, 2019; Zhou et al., 2020; Li et al., 2019; Hu and Singh, 2021; Li et al., 2021; Xiao et al., 2022). Here, we follow these tasks and adopt VL-BART and VL-T5 (Cho et al., 2021) as backbones, which exhibit state-of-the-art performance in vision-language generation.\\n\\nIn particular, a closely-related task, visual spatial relationship classification (VSRC), which outputs the spatial relationship between two objects inside an image, might be beneficial for our proposed VSD. The predefined discrete spatial relations such as \u201cnext to\u201d and \u201cbehind\u201d, in VSRC should be able to effectively guide the VSD generation. To this end, we first make a thorough comparison of the connections between VSD and VSRC, which can be regarded as shallow and deep analyses of spatial semantics, respectively, and further investigate the VSRC-enhanced VSD models, performing visual spatial understanding from shallow to deep. Specifically, we present two straightforward architectures to integrate VSRC into VSD, one being the pipeline strategy and the other being the end-to-end joint strategy, respectively.\\n\\nFinally, we conduct experiments on our constructed dataset to evaluate all proposed models. First, we examine the two start-up models for VSD only with VL-BART and VL-T5. The results show that the two models are comparable in terms of performance, and both models can provide highly accurate and fluent human-like outputs of spatial understanding. Second, we verify the effectiveness of VSRC for VSD and find that: (1) VSRC has great potentials for VSD because gold-standard VSRC can lead to striking improvements on VSD; (2) VSD can be benefited from automatic VSRC, and the end-to-end joint framework is slightly better. We further perform several analyses to intensively understand VSD and the proposed models.\\n\\n2 Related Work\\n\\nImage-to-text has been intensively investigated with the support of neural networks in the past years (He and Deng, 2017). The encoder-decoder architecture is an often considered framework, where the encoder extracts visual features from the image and the decoder generates text for specific tasks. Early works employ a convolutional neural network (CNN) as the visual encoder and a recurrent neural network (RNN) as the text decoder (Vinyals et al., 2015; Rennie et al., 2017). Recently, the Transformer neural network (Vaswani et al., 2017), which is impressively powerful in feature representation learning on both vision and language, has gained increasing interest. The Transformer-based encoder-decoder models have been adopted in a wide range of image-to-text tasks (Cornia et al., 2020; Herdade et al., 2019; Fei et al., 2021a). These models coupled with visual-language pretraining have achieved the top performance for these tasks (Lu et al., 2019; Sun et al., 2019; Tan and Bansal, 2019; Zhou et al., 2020; Li et al., 2019; Hu and Singh, 2021; Li et al., 2021). In this work, we exploit the Transformer-based architecture and two pretrained visual-language models: VL-BART and VL-T5 (Cho et al., 2021), reaching several strong benchmark models for our task.\\n\\nImage-to-text can be varied depending on the objective of the visual description. Image captioning is the most well-studied task, which aims to summarize a given image or to describe a particular region in it (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015). Several subsequent studies have attempted to produce captions with specified patterns and styles (Cornia et al., 2019; Kim et al., 2019; Deng et al., 2020; Zhong et al., 2020; Zheng et al., 2019). For example, VQA and visual reasoning can be regarded as such attempts, which are conditioned by a specific question directed at the input image (Antol et al., 2015; Agrawal et al., 2018; Hudson et al., 2016; Zellers et al., 2017; Lin et al., 2017; Kale et al., 2017; Zhong et al., 2018; Jiang et al., 2018; Yang et al., 2019; Hao et al., 2019; Lim and Yoo, 2019; Wang et al., 2019; Zhou et al., 2019; Liao et al., 2019; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al., 2020; Liu et al.,"}
{"id": "emnlp-2022-main-93", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and Manning, 2019; Johnson et al., 2017). The VSR-guided image captioning (Chen et al., 2021) is the most close to our work, which generates a sentence for a particular event in the image with well-specified semantic roles. Here we focus on spatial semantics instead, generating a description based on the spatial relationship.\\n\\nSpatial semantics is an important topic in both language and visual analysis. Kordjamshidi et al. (2011) propose an preliminary study on text-based spatial role labeling. Later, spatial element extraction and relation extraction from texts are investigated by (Nichols and Botros, 2015). Pustejovsky et al. (2015) present a fine-grained spatial semantic analysis in texts with rich spatial roles. Based on the image input, Yang et al. (2019) propose VSRC and benchmark it with a manually-crafted dataset. The VSRC is actually a shallow task for visual spatial analysis based on a closed relationship set and by using a simple classification schema. Following, Chiou et al. (2021) build a much stronger model on the dataset. Many studies have exploited spatial semantics to assist other image understanding tasks (Kim et al., 2021; Wu et al., 2021; Collell et al., 2021; Xiao et al., 2021; Pierrard et al., 2021). In addition, learning spatial representations from multiple modalities also receives particular attention (Collell and Moens, 2018; Dan et al., 2020). In this work, we extend image-to-text and propose VSD, which aims for the spatial understanding of the image.\\n\\n3 Visual Spatial Description\\n\\n3.1 Task Description\\n\\nFormally, we define the task of VSD as follows: given an image $I$ and an object pair $\\\\langle O_1, O_2 \\\\rangle$ inside $I$, the VSD aims to output a word sequence $S = \\\\{w_1, ..., w_n\\\\}$ to describe the spatial semantics between $O_1$ and $O_2$. The provided $O_1$ and $O_2$ include both the object tags and their bounding boxes. In Figure 1, we would receive \u201cA man is walking behind a red car from right to left.\u201d for $\\\\langle$man, car$\\\\rangle$ and \u201cA red car is parked to the left of a pole.\u201d for $\\\\langle$car, pole$\\\\rangle$ based on the same input image. The generated sentences of VSD must encode the spatial semantics between the given two objects, which differs from conventional image-to-text generation.\\n\\n3.2 Compared with VSRC\\n\\nNoticeably, VSRC is another representative task of visual spatial understanding that decides the spatial relation of two objects in an image. The relation is chosen from a closed set which is manually predefined. We can regard VSRC as a shallow analysis task for spatial semantics understanding, while the VSD task can offer a deeper spatial analysis by using the much more flexible output. In particular, compared with VSRC, VSD has three major advantages. First, VSD can offer richer semantics which could be necessary for spatial understanding. Meanwhile, VSRC only outputs a spatial relation from a closed set in general. VSD can raise other semantic roles to deepen the spatial understanding beyond the relations, such as predicates and object attributes. Second, the spatial relations might be overlapped. For example, the two relationships, \u201cbehind\u201d and \u201cto the right of\u201d might be both correct for VSRC given the \u201cman\u201d and \u201ccar\u201d in Figure 1. The newly proposed task VSD can more accurately describe the multiple spatial semantics. Third, from the viewpoint of downstream tasks, especially the systems that require automatic content-based image indexing or visual dialogue, VSD is more straightforward and adequate to support them.\\n\\n3.3 Data Collection\\n\\nWe build an initial dataset To benchmark the VSD task. The constructed dataset is extended from a VSRC dataset to facilitate the investigation between VSD and VSRC. Thus, our final corpus includes both VSRC and VSD annotations. Our VSRC dataset is sourced from two existing datasets: SpatialSense (Yang et al., 2019) and VisualGenome (Krishna et al., 2017). SpatialSense is a dataset initially constructed for VSRC with nine well-defined spatial relations, namely, \u201con\u201d, \u201cin\u201d, \u201cnext to\u201d, \u201cunder\u201d, \u201cabove\u201d, \u201cbehind\u201d, \u201cin front of\u201d, \u201cto the left of\u201d, and \u201cto the right of\u201d. The only disadvantage of SpatialSense is its relatively-small scale. Consequently, we enlarge the corpus with the help of VisualGenome a widely adopted dataset for scene graph generation with annotations in the form of $\\\\langle$subject, predicate, object$\\\\rangle$. We add the triplets in VisualGenome, whose predicates can be easily aligned with the nine spatial relations in SpatialSense. Accordingly, we can obtain a larger dataset of VSRC. We develop a simple visualization tool to facilitate the VSD annotation. The system randomly as-\\n\\n2 The alignment is achieved by a map, which will be released along with the dataset.\"}"}
{"id": "emnlp-2022-main-93", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The data annotation flow.\\n\\nSigns the instances to the annotators. Each instance contains one image and two objects inside it. The annotators are asked to write text descriptions for the given instance. We also set up another interface for experts to check the correctness of all annotated sentences and to ensure the quality of these written descriptions. In the description checking step, the given instances include the image inputs, paired objects, and the written descriptions by the first step. The annotator mainly checks whether the description is valid. The annotation flow is shown in Figure 2.\\n\\nAll annotators we recruited are college students who are native English speakers. During the preparation, we train the annotators with a guideline and perform two pre-annotation tests from easy to difficult. In the first test, the annotators are asked to participate in the checking interface, where several well-written descriptions are prepared in advance by experts and various pseudo-ill-conditioned descriptions by intentional word substitutions. Thereafter, we start the second test to let annotators write the real spatial descriptions. The annotators are allowed for official annotations only when both tests are passed. All annotators are properly paid under the open market competition.\\n\\nSpecifically, we have three basic principles mainly in our data annotation guideline as follows:\\n\\n- The sentence must correctly describe the spatial semantics of the given object pair.\\n- The descriptions can help us correctly locate the exacted objects in the image.\\n- The length of each text description should be limited to no more than 40 tokens.\\n\\nThe annotation submissions with excess invalid annotations (more than 4/100) according the above principles would be returned to the annotators to rework until it reaches the standard. Figure 2 shows some examples of invalid annotations. There might be several exceptions, such as, spelling mistakes or mismatches between the image and object tag inputs. In these cases, annotators should skip and report these instances, leaving them for further discussions by experts. In the expert-checking step, the remaining invalid and controversial annotations would be discussed and then finalized.\\n\\nFinally, we annotate a total of 29K images with 143K descriptions, wherein 6,591 images are from the SpatialSense with 9,744 descriptions, and the remaining images and descriptions are sourced from VisualGenome. Furthermore, we randomly split the whole annotated VSD dataset by a ratio of 7:1:2 as training / validation / testing sections. The statistics of the dataset are shown in Table 1.\"}"}
{"id": "emnlp-2022-main-93", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A house is behind of a tree on the left.\\n\\nSpatial\\nClassifier\\nBehind\\nEncoder\\n\\nA house is behind of a tree on the left.\\n\\nSpatial\\nClassifier\\nBehind\\nEncoder\\n\\nA house is behind of a tree on the left.\\n\\nSpatial\\nClassifier\\nBehind\\nEncoder\\n\\nFigure 3: Overview of our pipeline and end-to-end models with VSRC, where FC denotes fully-connected network.\\n\\na joint feature representation, and the text decoder generates the output sentential words incrementally with the joint representation.\\n\\nFormally, the VSD input includes: (1) one image $I$ and (2) the inside object pair $\u27e8O_1, O_2\u27e9$.\\n\\nFirst, we obtain a sequence of visual features by:\\n\\n$$F_V = \\\\text{VisionExtractor}(I)$$\\n\\nThen a fully-connected (FC) linear transformation layer is used to align the vectorial dimensions between vision and language, leading to $E_V$.\\n\\nSecond, a textual embedding layer is used to represent $O_1$ and $O_2$ by their respective textual tags (i.e., $T_O_1$ and $T_O_2$):\\n\\n$$E_T = \\\\text{TextEmbed}(\u27e8T_O_1, T_O_2\u27e9)$$\\n\\nThereafter that, a Transformer is exploited to produce the final encoder output by the following expression:\\n\\n$$H = \\\\text{Transformer}(\u27e8E_T, E_V\u27e9)$$\\n\\nWe generate a sequence of words incrementally for the decoder, wherein one word is predicted each step based on the previous context:\\n\\n$$o_j(y|y_{i<j}) = \\\\text{FC}(\\\\text{Transformer}(y_{i<j}, H))$$\\n\\nwhere $y_{i<j}$ denotes the previously generated tokens and $H$ represents the encoder outputs. The decoder is also dominated by Transformer. Thereafter, an FC layer is used to score all candidate words.\\n\\nWe exploit the cross-entropy as objective loss to train the model, following the majority of sentence generation models (Lewis et al., 2020; Raffel et al., 2020). During the decoding, we can apply the beam search algorithm to obtain better results.\\n\\n4.2 VL-BART and VL-T5\\n\\nVL-BART is a well-pretrained model that can be directly applied to our VSD model with an initializing-then-fine-tuning mode. VL-BART is a standard Transformer-based model similar to our VSD model with a bidirectional joint V&L encoder and an autoregressive text decoder, which is extended from BART (Lewis et al., 2020) by importing an extra visual embedding module for the joint encoding. Before pretraining, VL-BART is partially initialized with BART on the shared parameters, which is trained on the text-only corpus by corrupting documents and optimizing the model by a reconstruction loss.\\n\\nVL-T5 is similar to VL-BART on model architecture but differs in that it extends from T5 (Raffel et al., 2020). The T5 model uses relative position embeddings on text representation and is trained on a very different text-only corpus with a span-based reconstruction process.\"}"}
{"id": "emnlp-2022-main-93", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our VSD task aims to control image-to-text generation by the aspect of spatial semantics. If we know the explicit spatial relation by VSRC in advance for the given two objects, then the description generation could be more instructional. In this section, we introduce two architectures of integrating VSRC into the above-mentioned VSD models.\\n\\n### 5.1 Pipeline\\n\\nThe pipeline architecture includes two stages. In the first stage, VSRC is executed to extract spatial relations between the two given objects of the VSD input. In the second stage, our VSD model adds the spatial relation as one additional textual input to enhance the encoder. We illustrate the architecture in the left portion of Figure 3.\\n\\nOur VSRC model takes the same input as VSD, an image and two objects inside it. Accordingly, our encoder can be highly similar to that of the VSD model: VisionEmbed and TextEmbed, followed by the Transformer as mentioned in Equations 2 and 3. Here, we make a slight modification to adapt the VSRC task. Specifically, a special [MASK] token is added inside the TextEmbed module:\\n\\n$$E_T = \\\\text{TextEmbed}([T_O^1, T_O^2, \\\\text{MASK}])$$\\n\\nwhere the updated TextEmbed has been illustrated in Figure 3 by the input depiction of the VSRC. We only use one vector $h_{\\\\text{MASK}}$ from the sequential encoder output $H$ for relation classification, which is exactly corresponding to the position of the special [MASK] token.\\n\\nBefore the final-step classification, we follow (Chiou et al., 2021) to add the bounding box coordinates of the two objects for geometric information. Each bounding box is converted into a 4-dimensional vector, thus we have $c_{O^1}$ and $c_{O^2}$ for the two objects, respectively. Then, we use the following fully-connected (FC) networks sequentially to reach a bounding box representation:\\n\\n$$\\\\tilde{h}_{\\\\text{coord}} = \\\\text{FC}(c_{O^1}) + \\\\text{FC}(c_{O^2}),$$\\n$$h_{\\\\text{coord}} = \\\\text{FC}(\\\\text{FC}(\\\\tilde{h}_{\\\\text{coord}})),$$\\n\\nwhere $h_{\\\\text{coord}}$ is the desired bounding box representation. Finally, we concatenate $h_{\\\\text{coord}}$ and $h_{\\\\text{MASK}}$ to score candidate spatial relations:\\n\\n$$o_{\\\\text{VSRC}} = \\\\text{MLP}_{\\\\text{VSRC}}([h_{\\\\text{MASK}}, h_{\\\\text{coord}}]),$$\\n\\nwhere $\\\\text{MLP}_{\\\\text{VSRC}}$ is the classifier for VSRC. In this way, the VSRC task is accomplished. The middle part of Figure 3 shows the detailed network operation of the classification.\\n\\nOur VSD task receives three types of inputs from the VSRC output, with additional spatial relation as one supplement compared with the original VSD. Considering the textual property of the spatial relation, we add this information to the textual embedding of the original VSD encoder:\\n\\n$$E_T = \\\\text{TextEmbed}([T_O^1, T_O^2, r_{O^1,O^2}])$$\\n\\nwhere $r_{O^1,O^2}$ is the textual expression of the spatial relation between the given objects $O^1$ and $O^2$.\\n\\nThis distinction is the only difference between the VSRC-enhanced and the original VSD models, and the other parts remain the same.\\n\\n### 5.2 End to End\\n\\nThe end-to-end model for joint VSRC and VSD is not only more elegant in form, allowing their full natural interactions, but also can avoid the error propagation problem where the VSRC errors may result in further degraded VSD performance. We adopt multi-task learning (MTL) to achieve the end-to-end goal with a single model. Figure 3 shows the detailed structure by the right part. The joint encoder is directly borrowed from the individual VSRC model, resulting in the encoder output $H$. Thus, the input of the joint model is the same as the original VSD model and the VSRC model. Then, we execute the decoders of VSRC and VSD, achieving the goal of joint learning.\\n\\nDuring the training, given the VSRC input (also the joint input) and the VSRC and VSD outputs, we optimize the end-to-end model by the joint loss, which is a weighted addition of the VSRC and VSD losses. During the inference, we have two strategies for our VSD task. First, we can use the end-to-end joint model to simultaneously obtain the VSRC and VSD results under the MTL architecture (Figure 3 End2End without the red dashed line). Second, we can execute the end-to-end model by two rounds, where the first round outputs the VSRC result, and the second round uses the VSRC result to substitute the [MASK] part of the joint encoder, and then executes the VSD part only. The second strategy is similar to the pipeline architecture, but only a single model is involved.\"}"}
{"id": "emnlp-2022-main-93", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The main results of our proposed models on the VSD test dataset, where we implement three types of baseline models (i.e., VL-BART, VL-T5 and OSCAR), and the ones equipped with VSRC supporting.\\n\\n6 Experiments\\n6.1 Setup\\nImplementation Details\\nWe initialize our encoder-decoder backbone with two pretrained models VL-BART and VL-T5, and follow (Anderson et al., 2018) to obtain visual region features from Faster R-CNN. We use the two-round strategy as default for the decoding of the end-to-end models with VSRC. We present more model details and hyperparameters in Appendix A.\\n\\nEvaluation\\nWe report five standard evaluation metrics of the text generation for the VSD task, including BLEU-4 (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). In this work, we use BLEU-4 and SPICE as the primary metrics to evaluate our models, where the former can measure the syntactic quality of the generated descriptions, and the latter emphasizes the consistency with the input scene graphs. Although CIDEr has been widely-adopted for image-to-text generation as the major metric, it might be unsuitable for our VSD task because it can lower the importance of frequently occurring words closely related to spatial relations by the IDF values. We conduct each experiment by five times and report the average number.\\n\\n6.2 Main Results\\nTable 2 shows the main results on the test dataset. The model results based on VL-BART and VL-T5 are reported in two different regions. The first row of each region shows the performance of our original models. The base VL-BART and VL-T5 models can achieve impressive performance as a whole, and the two models are generally comparable. The rows with \u201c+VSRC-\u2217\u201d stand for the results of our VSD models with the support of spatial relation. Meanwhile, the \u201cVL-T5-\u2217\u201d models demonstrate better performance under this setting. To show the potential of VSRC for VSD, we first examine the oracle performance with gold-standard spatial relations as input. The results are highly exciting, as shown by \u201c+VSRC-golden\u201d with the gray numbers. We can obtain very large improvements over all evaluation metrics based on both VL-BART and VL-T5. The observation indicates that spatial relation is very useful to our VSD task. However, using gold-standard spatial relations in real scenarios is impractical. Thus, it is deserved to investigate the benefits of spatial relations outputted from a VSRC model.\\n\\nSpatial relations from a VSRC model can be incorporated in two ways, as shown by \u201c+VSRC-pipeline\u201d and \u201c+VSRC-end2end\u201d in Table 2. The two types of models show significant performance decreases compared with that of \u201c+VSRC-golden\u201d. Nonetheless, these models can still lead to positive gains on the VSD task by comparing their performance with the basic models without spatial relation information. In addition, our end-to-end joint models (i.e., \u201c+VSRC-end2end\u201d) outperform their corresponding pipelines. If the gain on VSRC is larger, then the increase on VSD is also more significant, indicating that the VSRC performance is the key.\\n\\nFurthermore, we compare our VL-BART and VL-T5 models with another representative image-\"}"}
{"id": "emnlp-2022-main-93", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Fine-grained results of the VL-T5+VSRC-end2end model in terms of spatial relations.\\n\\nThe major difference between our models and OSCAR+ is that OSCAR+ exploits VL-BERT as the backbone, which only contains an encoder. The spatial relation can effectively improve the OSCAR+ model as well. Notice that VL-BERT excels at understanding tasks because of its discriminative pretraining benefiting based on sole encoder learning, so we can find that Oscar+VSRC-end2end can achieve the best VSRC accuracy. Overall, the OSCAR+ models still obtain lower VSD performance than our suggested VL-BART and VL-T5 models, demonstrating the advantage of the encoder-decoder pretraining on the VSD task.\\n\\n6.3 Discussion\\n\\nFine-grained Performance by Spatial Relations\\n\\nThe performance differences among various spatial relations are interesting. Several particular relations may be more difficult to comprehend within the images. Figure 4 shows the BLEU-4 results across different spatial relations by the VL-T5+VSRC-end2end model, where the VSRC precisions are also shown for comparison. Overall, one approximative correction exists between the VSD and VSRC performance by fine-grained evaluation. Additionally, spatial relations such as \\\"to the left of\\\" and \\\"to the right of\\\" show significantly lower performance than the others. The two possible reasons are as follows: (1) These relations (e.g., including ambiguities by compounds) are visually not as clear as the others, such as \\\"on\\\", \\\"under\\\", and \\\"in\\\". (2) The distribution of spatial relations is unbalanced. Although we have paid particular attention to this issue while building our dataset, this problem is still challenging to handle due to the natural characteristic of spatial semantics.\\n\\nPipeline v.s. End-to-End\\n\\nTo further understand the disparity between the pipeline and the end-to-end models, we divide the model outputs by the Pipeline and End2End modes. For instance, if the VSRC output is correct, then we regard the instance as positive; otherwise, it is negative. Figure 5 shows the BLEU-4 and SPICE results by the VL-T5+VSRC-end2end model. The end-to-end model outperforms the pipeline model on the positive samples while doing the opposite on negative samples. The obversion is reasonable because the end-to-end model tends to trust its VSRC output by its overall positive influence, thus resulting in downgraded performance when the VSRC outputs are incorrect. According to the finding, we can see that the VSRC accuracy is vital for the final VSD performance.\\n\\nDecoding in End-to-end: One Round or Two\\n\\nAs mentioned in Section 5.2, we have two strategies for the decoding of the end-to-end models. The two-round strategy is selected by default. Here, we compare the two decoding strategies based on the VL-T5+VSRC-end2end model. Figure 6 shows the results, where the pipeline results are also shown for reference. The two-round decoding is highly critical for the end2end model, without which the model can even be inferior to the pipeline one. The possible reason might be that the simple one-round decoding is unable to leverage this advantage even though our MTL architecture for the end2end learning can effectively learn the interactions between the two tasks.\\n\\nHuman Evaluation\\n\\nWe perform a human evaluation on the VSRC correctness into two categories and then evaluate the VSD results on them separately. Concretely, if the VSRC output is correct, then we regard the instance as positive; otherwise, it is negative. Figure 5 shows the BLEU-4 and SPICE results by the VL-T5+VSRC-end2end model. The end-to-end model outperforms the pipeline model on the positive samples while doing the opposite on negative samples. The obversion is reasonable because the end-to-end model tends to trust its VSRC output by its overall positive influence, thus resulting in downgraded performance when the VSRC outputs are incorrect. According to the finding, we can see that the VSRC accuracy is vital for the final VSD performance.\"}"}
{"id": "emnlp-2022-main-93", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results of human evaluation.\\n\\n| Model                        | Spatial Fluency Location | Avg  |\\n|------------------------------|--------------------------|------|\\n| VL-T5(Base)                  | 93.2 93.8 96.1           | 94.4 |\\n| +VSRC-pipeline               | 93.9 94.0 96.3           | 94.7 |\\n| +VSRC-end2end               | 94.9 95.2 96.6           | 95.6 |\\n| +VSRC-golden                | 99.3 95.0 96.6 97.0     |\\n\\nTable 3 shows the accumulation scores of over the 100 instances with one decimal place retained. Noticeably, the Spatial Correctness is different from the VSRC accuracy in Table 2, where the former is for human judgement of VSD descriptions and the latter is for a nine-way classification. Generally, the VSRC accuracy can only evaluate one of multiple reasonable spatial relations of the given two objects while the human evaluation is more tolerant and reasonable. The tendency in performance is consistent with the automatic evaluation, where VSRC can help VSD because it can offer more spatial information, and the end-to-end model is better in utilizing automatic VSRC. The model with golden VSRC achieves a very high score of 99.3, which is reasonable due to the golden VSRC information of inputs.\\n\\n7 Conclusion\\n\\nIn this work, we introduced a novel image-to-text generation task, namely VSD, aiming to generate text descriptions containing spatial semantics of two objects in an image, and constructed a dataset to benchmark this task. We adopted the models with Transformer-based encoder-decoder architectures (i.e., VL-BART and VL-T5) for our task to obtain the baseline results. Moreover, we proposed to integrate VSRC into our models by pipeline and end-to-end architectures, enhancing VSD with the support of spatial relations. The experimental results show that the VSRC-enhanced approach achieves significant progress over our initial models. Moreover, the end-to-end models outperform the pipeline ones due to joint learning.\\n\\nLimitations\\n\\nThis work has two major limitations. The first limitation lies in our dataset. Our annotated dataset is built on SpatialSence and VG-Relation, aiming to study the relationship between VSRC and VSD. Under this setting, the variety of the spatial relations is limited to only nine. In addition, we only annotate one sentence for each instance, which limits the diversity of the description styles. We plan to continuously improve our dataset with more spatial relations and descriptions as the future work to improve this condition. The second limitation is that our base models only focus on single spatial relations in this work, ignoring the compound relations such as \u201cleft\u201d and \u201cbehind\u201d concurrently occurring. To solve this issue, we need to explore more methods to model multiple relations to generate descriptions with richer semantics. We also leave this aspect to future in-depth studies.\\n\\nEthical Considerations\\n\\nWe construct a new large-scale image-to-text generation dataset with crowd annotations. All the images of our dataset are sourced from two existing public datasets, SpatialSense and VisualGenome, which are open-access. All the annotators were voluntary participants and can quit at any time. They were informed of the study\u2019s goals before giving their express consent. All annotators were properly paid by their actual efforts and there is no information related to annotator privacy in the dataset.\\n\\nAcknowledgement\\n\\nThis work is supported by grants from the National Natural Science Foundation of China (No. 62176180).\"}"}
{"id": "emnlp-2022-main-93", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018. Don't just assume; look and answer: Overcoming priors for visual question answering. In CVPR, pages 4971\u20134980.\\n\\nKhawaja Tehseen Ahmed, Sumaira Aslam, Humaira Afzal, Sajid Iqbal, Arif Mehmood, and Gyu Sang Choi. 2021. Symmetric image contents analysis and retrieval using decimation, pattern analysis, orientation, and features fusion. IEEE Access, 9:57215\u201357242.\\n\\nPeter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. SPICE: semantic propositional image caption evaluation. In ECCV, volume 9909 of Lecture Notes in Computer Science, pages 382\u2013398.\\n\\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, pages 6077\u20136086.\\n\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: visual question answering. In ICCV, pages 2425\u20132433.\\n\\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of ACL, pages 65\u201372.\\n\\nLong Chen, Zhihong Jiang, Jun Xiao, and Wei Liu. 2021. Human-like controllable image captioning with verb-specific semantic roles. In CVPR, pages 16846\u201316856.\\n\\nShizhe Chen, Qin Jin, Peng Wang, and Qi Wu. 2020a. Say as you wish: Fine-grained control of image caption generation with abstract scene graphs. In CVPR, pages 9959\u20139968.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020b. UNITER: universal image-text representation learning. In ECCV, volume 12375 of Lecture Notes in Computer Science, pages 104\u2013120.\\n\\nMeng-Jiun Chiou, Roger Zimmermann, and Jiashi Feng. 2021. Visual relationship detection with visual-linguistic knowledge from multimodal representations. IEEE Access, 9:50441\u201350451.\\n\\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021. Unifying vision-and-language tasks via text generation. In Proceedings of ICML, volume 139 of Proceedings of Machine Learning Research, pages 1931\u20131942.\\n\\nGuillem Collell, Thierry Deruyttere, and Marie-Francine Moens. 2021. Probing spatial clues: Canonical spatial templates for object relationship understanding. IEEE Access, 9:134298\u2013134318.\\n\\nGuillem Collell and Marie-Francine Moens. 2018. Learning representations specialized in spatial knowledge: Leveraging language and vision. Trans. Assoc. Comput. Linguistics, 6:133\u2013144.\\n\\nMarcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2019. Show, control and tell: A framework for generating controllable and grounded captions. In CVPR, pages 8307\u20138316.\\n\\nMarcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2020. Meshed-memory transformer for image captioning. In CVPR, pages 10575\u201310584.\\n\\nSoham Dan, Hangfeng He, and Dan Roth. 2020. Understanding spatial relations through multiple modalities. In LREC, pages 2368\u20132372.\\n\\nChaorui Deng, Ning Ding, Mingkui Tan, and Qi Wu. 2020. Length-controllable image captioning. In ECCV, volume 12358 of Lecture Notes in Computer Science, pages 712\u2013729.\\n\\nHaiwen Diao, Ying Zhang, Lin Ma, and Huchuan Lu. 2021. Similarity reasoning and filtration for image-text matching. In AAAI, pages 1218\u20131226.\\n\\nHao Fei, Yafeng Ren, Shengqiong Wu, Bobo Li, and Donghong Ji. 2021a. Latent target-opinion as prior for document-level sentiment classification: A variational approach from fine-grained perspective. In WWW, pages 553\u2013564.\\n\\nHao Fei, Shengqiong Wu, Yafeng Ren, Fei Li, and Donghong Ji. 2021b. Better combine them together! integrating syntactic constituency and dependency representations for semantic role labeling. In Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, pages 549\u2013559.\\n\\nHao Fei, Shengqiong Wu, Yafeng Ren, and Meishan Zhang. 2022. Matching structure for dual learning. In ICML, pages 6373\u20136391.\\n\\nIrtiza Hasan, Shengcai Liao, Jinpeng Li, Saad Ullah Akram, and Ling Shao. 2021. Generalizable pedestrian detection: The elephant in the room. In CVPR, pages 11328\u201311337.\\n\\nXiaodong He and Li Deng. 2017. Deep learning for image-to-text generation: A technical overview. IEEE Signal Process. Mag., 34(6):109\u2013116.\\n\\nSimao Herdade, Armin Kappeler, Kofi Boakye, and Joao Soares. 2019. Image captioning: Transforming objects into words. In NeurIPS, pages 11135\u201311145.\\n\\nRonghang Hu and Amanpreet Singh. 2021. Transformer is all you need: Multimodal multitask learning with a unified transformer. CoRR, abs/2102.10772.\\n\\nDrew A. Hudson and Christopher D. Manning. 2019. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, pages 6700\u20136709.\"}"}
{"id": "emnlp-2022-main-93", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Muhammad Zubair Irshad, Niluthpol Chowdhury, Mithun, Zachary Seymour, Han-Pang Chiu, Supun Samarasekera, and Rakesh Kumar. 2021. SASRA: semantically-aware spatio-temporal reasoning agent for vision-and-language navigation in continuous environments. CoRR, abs/2108.11945.\\n\\nHarsh Jhamtani and Taylor Berg-Kirkpatrick. 2018. Learning to describe differences between pairs of similar images. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 4024\u20134034. Association for Computational Linguistics.\\n\\nWei Ji, Xi Li, Lina Wei, Fei Wu, and Yueting Zhuang. 2020. Context-aware graph label propagation network for saliency detection. IEEE Transactions on Image Processing, 29:8177\u20138186.\\n\\nJustin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. 2017. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 1988\u20131997.\\n\\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In CVPR, pages 3128\u20133137.\\n\\nDong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, and In So Kweon. 2019. Dense relational captioning: Triple-stream networks for relationship-based captioning. In CVPR, pages 6271\u20136280.\\n\\nGeonuk Kim, Honggyu Jung, and Seong-Whan Lee. 2021. Spatial reasoning for few-shot object detection. Pattern Recognit., 120:108118.\\n\\nParisa Kordjamshidi, Martijn van Otterlo, and Marie-Francine Moens. 2011. Spatial role labeling: Towards extraction of spatial relations from natural language. ACM Trans. Speech Lang. Process., 8(3):4:1\u20134:36.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123(1):32\u201373.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, pages 7871\u20137880.\"}"}
{"id": "emnlp-2022-main-93", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sonia Raychaudhuri, Saim Wani, Shivansh Patel, Unnat Jain, and Angel X. Chang. 2021. Language-aligned waypoint (LAW) supervision for vision-and-language navigation in continuous environments. CoRR, abs/2109.15207.\\n\\nShaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: towards real-time object detection with region proposal networks. In NeurIPS, pages 91\u201399.\\n\\nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In CVPR, pages 1179\u20131195.\\n\\nJithmi Shashirangana, Heshan Padmasiri, Dulani Medeniya, and Charith Perera. 2021. Automated license plate recognition: A survey on methods and techniques. IEEE Access, 9:11203\u201311225.\\n\\nChen Sun, Austin Myers, Carl von Ondrick, Kevin Murphy, and Cordelia Schmid. 2019. Videobert: A joint model for video and language representation learning. In ICCV, pages 7463\u20137472.\\n\\nHao Tan and Mohit Bansal. 2019. LXMERT: learning cross-modality encoder representations from transformers. In EMNLP-IJCNLP, pages 5099\u20135110.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS, pages 5998\u20136008.\\n\\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In CVPR, pages 4566\u20134575.\\n\\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In CVPR, pages 3156\u20133164.\\n\\nXinxiao Wu, Ruiqi Wang, Jingyi Hou, Hanxi Lin, and Jiebo Luo. 2021. Spatial-temporal relation reasoning for action prediction in videos. Int. J. Comput. Vis., 129(5):1484\u20131505.\\n\\nJunbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua. 2022. Video as conditional graph hierarchy for multi-granular question answering. In AAAI, pages 2804\u20132812.\\n\\nShaoning Xiao, Long Chen, Songyang Zhang, Wei Ji, Jian Shao, Lu Ye, and Jun Xiao. 2021. Boundary proposal network for two-stage natural language video localization. In AAAI, pages 2986\u20132994.\\n\\nZhenbo Xu, Wei Yang, Ajin Meng, Nanxue Lu, Huan Huang, Changchun Ying, and Liusheng Huang. 2018. Towards end-to-end license plate detection and recognition: A large dataset and baseline. In ECCV, volume 11217 of Lecture Notes in Computer Science, pages 261\u2013277.\\n\\nKaiyu Yang, Olga Russakovsky, and Jia Deng. 2019. Spatialsense: An adversarially crowdsourced benchmark for spatial relation recognition. In ICCV, pages 2051\u20132060.\\n\\nZhen Zeng, Zheming Zhou, Zhiqiang Sui, and Odest Chadwicke Jenkins. 2018. Semantic robot programming for goal-directed manipulation in cluttered scenes. In ICRA, pages 7462\u20137469.\\n\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. 2021. Vinvl: Revisiting visual representations in vision-language models. In CVPR, pages 5579\u20135588.\\n\\nYue Zheng, Yali Li, and Shengjin Wang. 2019. Intention oriented image captions with guiding objects. In CVPR, pages 8395\u20138404.\\n\\nYiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, and Yin Li. 2020. Comprehensive image captioning via scene graph decomposition. In ECCV, volume 12359 of Lecture Notes in Computer Science, pages 211\u2013229.\\n\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. 2020. Unified vision-language pre-training for image captioning and VQA. In AAAI, pages 13041\u201313049.\\n\\nJordan Zlatev. 2007. Spatial semantics. The Oxford handbook of cognitive linguistics, pages 318\u2013350.\"}"}
{"id": "emnlp-2022-main-93", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Detailed Experiment Settings\\n\\nWe adopt the default settings of VL-BART and VL-T5 backbones (Cho et al., 2021). In VSRC, the dimension size of the bounding box coordinate features ($c_O^1$ and $c_O^2$ in Equation (6)) is 64 and the dimension of the fully connected layers is set to 1024. For hyper-parameters, we detail them in Appendix A. We train our models using the AdamW optimizer (Loshchilov and Hutter, 2017), setting the initial learning rate to $5 \\\\times 10^{-4}$ and weight decay to 0.01. We apply the gradient clipping mechanism by a maximum value of 5.0 to avoid gradient explosion. The training batch size is 16 and the max epoch number is 40.\\n\\nB Case Study\\n\\nWe show five case studies in Figure 7 to extensively understand the model outputs. In the first case, all four models (human is the golden answer) are able to output good descriptions because the relation in the image is simple and easy to understand. In the second case, the VL-T5 (base) model is unable to provide a correct answer, while the other models are all correct due to the benefit from VSRC. In the third case, the VL-T5+VSRC-end2end and VL-T5+VSRC-golden models output acceptable results, while the other two models fail. The reason might be that the two models can identify the correct or more important spatial relation between the two objects. In the fourth case, we can only obtain a correct description by VL-T5+VSRC-golden because the spatial relation is very difficult to recognize by automatic VSRC. Finally, our VSRC-enhanced VSD model fails in the fifth case even with the golden spatial relation. The reason might be the extreme complexity of this particular input image.\"}"}
