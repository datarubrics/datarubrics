{"id": "lrec-2024-main-730", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How to Understand \\\"Support\\\"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding\\n\\nJiamin Luo \u2217, Jianing Zhao \u2217, Jingjing Wang \u2020, Guodong Zhou\\nSchool of Computer Science and Technology, Soochow University, China\\nNo.1, Shizi Street, Suzhou City, Jiangsu Province, China\\n{20204027003, jnzhao1106}@stu.suda.edu.cn, {djingwang, gdzhou}@suda.edu.cn\\n\\nAbstract\\nWeakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting finding that IECI outperforms the advanced multimodal LLMs by a large margin on this implicit-enhanced dataset, which may facilitate more research to evaluate the multimodal LLMs in this direction.\\n\\nKeywords: Weakly-supervised Phrase Grounding, Implicit Phrase-Region Matching, Causal Inference\\n\\n1. Introduction\\nPhrase Grounding (PG) (Wang et al., 2019), a fundamental task in the field of multimodal learning, aims to find all the regions within an image that correspond to various phrases present in a given sentence. This correspondence serves as a fundamental foundation for numerous vision-language tasks, including image captioning (Feng et al., 2019), vision question answering (Mun et al., 2018), and visual dialog (Guo et al., 2020). However, PG heavily relies on expensive annotations of linking phrases to the corresponding image regions, which is labor-intensive and time-consuming. Thus, existing studies on PG mainly seek to address this task in the Weakly-supervised Phrase Grounding (WPG) (Chen et al., 2018) setting which merely leverages coarse-grained sentence-image pairs during training while subsequently evaluates the performance on fine-grained phrase-region pairs, achieving substantial advancements.\\n\\nDespite this, these studies fail to deeply explore the semantic nature of phrases that some phrases often exhibit the implicit and intricate semantics, rendering it arduous for models to establish correct connections with image regions. Take Figure 1 as an example, the phrase \\\"support\\\" necessitates the integration of the commonsense knowledge to precisely find its corresponding image region, i.e., red boxes. In this study, we refer to such correspondence as one type of implicit phrase-region matching relations, which could be defined as phrases in the sentence that are not specific and explicit nominal phrases of the objects in the image, and the grasp of such implicit relations serves as a valuable evaluation of the model capability in understanding the deep multimodal semantics. Additionally, the weakly-supervised setting of WPG could bring the supervised noise problem (Xiao et al., 2017) which may lead to more difficulties in capturing such implicit information, making it urgent to address the implicit relations problem in WPG. In this study, we contend that capturing such relations at least faces two main challenges.\\n\\nIn this paper, this commonsense-involved implicit relation is named as \\\"commonsense understanding\\\". Besides, we also propose another three implicit relations as shown in Figure 3.\"}"}
{"id": "lrec-2024-main-730", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For one thing, we argue that modeling the implicit relations is challenging. Still take the implicit relation sample in Figure 1 as the example, the implicit phrase \\\"support\\\" actually only corresponds to a special small-scale red-box region (i.e., the people's arms with the held flags or signs), which is easily confounded by the other common regions and thus difficult to be predicted. Fortunately, in the literature, a few recent studies on other tasks (e.g., Wang et al., 2020b) have also encountered with the similar confounding bias problem and suggested to leverage the causation-based approach for mitigating this confounding bias. Inspired by this, we believe that a well-behaved approach to WPG should take advantage of the causation-based approach (e.g., causal inference (Pearl and Mackenzie, 2018)) to model the implicit relations.\\n\\nFor another, we argue that highlighting the implicit relations beyond the explicit is rather challenging. As exemplified in Figure 1, it is obvious that the occurrence ratio of implicit relations (i.e., \\\"support\\\") is significantly lower (about 1:9, see Section 4.1) compared to explicit relations (i.e., \\\"some people\\\", \\\"rainbow flags\\\", \\\"a woman\\\", \\\"a sign\\\"), which could mislead the model to prefer capturing the explicit phrase-region relations instead of the implicit. Therefore, we believe that a better-behaved causation-based approach to WPG should further consider this imbalance for better aligning the implicit phrase-region pairs.\\n\\nTo tackle the aforementioned challenges, this paper proposes a causation-based approach namely Implicit-Enhanced Causal Inference (IECI) for WPG. Specifically, this approach first leverages the intervention technique (Pearl and Mackenzie, 2018) and proposes an implicit-aware deconfounded attention (IDA) block to model the implicit relations for mitigating the confounding bias. Furthermore, this approach leverages the counterfactual technique (Pearl and Mackenzie, 2018) and proposes an implicit-aware counterfactual inference (ICI) block to highlight the implicit relations beyond the explicit for better aligning the implicit phrase-region pairs. Particularly, a high-quality implicit-enhanced dataset is annotated for benchmarking our IECI approach. The main contributions of our work are summarized as follows:\\n\\n- We are the first to address the implicit relations problem in WPG, and annotate a high-quality implicit-enhanced dataset to evaluate the ability of models in understanding deep multimodal semantics.\\n- We propose a new IECI approach, which integrates both the intervention and counterfactual techniques for addressing the implicit challenges inside WPG. Detailed evaluations on our implicit-enhanced dataset demonstrate the superiority of our IECI approach over the state-of-the-art baselines.\\n- We observe an interesting finding that our IECI approach exhibits significant advantages compared to the advanced multimodal LLMs on the annotated implicit-enhanced dataset, which may further facilitate the evaluation of multimodal LLMs in this direction.\\n\\n2. Related Work\\n\\n2.1. Phrase Grounding\\n\\nPhrase Grounding (PG) and Referring Expression Comprehension (REC) (Kazemzadeh et al., 2014) are two prevalent tasks within the field of Visual Grounding (VG) (Rohrbach et al., 2016). While both tasks involve establishing relations between sentence phrases and image regions, PG focuses on predicting regions for all phrases within a sentence-image pair, whereas REC pertains to identifying a single region in the image corresponding to the given sentence. PG can be broadly categorized into two forms, i.e., one-stage models (Yang et al., 2022; Deng et al., 2021) and two-stage models (Chen et al., 2021; Li et al., 2020). Recognizing the expensive and difficult annotation of phrase-region, recent studies have predominantly shifted toward WPG. Early studies (Chen et al., 2018; Zhao et al., 2018) primarily focus on directly learning the phrase-region relations. Besides, some studies (Liu et al., 2019a, b, 2021) acknowledge the importance of context cues, which exploit linguistic contexts to enforce cross-modal consistency. Moreover, Datta et al. (2019) introduce a ranking-loss to minimize the distances between associated sentence-image and maximize the distance between irrelevant pairs, inspiring numerous studies (Gupta et al., 2020; Wang et al., 2021a, 2020a; Chen et al., 2022) to employ contrastive learning techniques to predict phrase-region matching in the WPG task.\\n\\nIn summary, all the above studies always ignore the implicit phrase-region matching relations problem, which however holds significant potential for evaluating the ability of models in understanding the deep multimodal semantics.\\n\\n2.2. Causal Inference\\n\\nIn recent years, causal inference has sparked significant interest across a range of areas, including scene graph generation (Tang et al., 2020b), semantic segmentation (Zhang et al., 2020a), vision-language tasks (Chen et al., 2020a), etc. Pearl and Mackenzie (2018) have defined three levels of causality, encompassing intervention and counterfactual that are frequently employed to mitigate confounding bias and achieve unbiased estimations. For example, Wang et al. (2020b) employ...\"}"}
{"id": "lrec-2024-main-730", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The overall framework of our proposed Implicit-Enhanced Causal Inference (IECI) approach. Wherein (a) and (b) are causal graphs for modeling the implicit relations (see Section 3.2), while (c) and (d) are those for highlighting the implicit relations beyond the explicit (see Section 3.3).\\n\\nthe causal intervention to deal with spurious correlation within datasets for visual common sense learning, and Zhanget al. (2020b) alleviate the spurious correlations between vision and language in vision-linguistic pre-training. For the strategy of intervention adjustment, Yang et al. (2021) employ front-door adjustment to realize the causal intervention, while Wang et al. (2021b) use back-door adjustment to self-annotate the confounder in an unsupervised way. Huang et al. (2022) propose a confounder-agnostic approach to remove the confounding bias between language and location. In addition, Tang et al. (2020a) and Niu et al. (2021) employ counterfactual inference to alleviate long-tailed categories bias in image classification and language bias in VQA, respectively.\\n\\nDifferent from all above studies, our study is a pioneering effort in integrating both intervention and counterfactual techniques for multimodal tasks, where we leverage intervention to effectively mitigate the bias of confounding, and employ counterfactual to highlight the implicit matching relations.\\n\\n3. Implicit-Enhanced Causal Inference Approach\\n\\nIn this section, we formulate the WPG task as follows. Given a collection of \\\\( T \\\\) sentence-image pairs \\\\((S, V)\\\\), where \\\\( S = [S_1, ..., S_T] \\\\) and \\\\( V = [V_1, ..., V_T] \\\\). Each sentence \\\\( S_i \\\\) consists of multiple phrases \\\\( S_i = [s_1i, ..., s_li, ..., s_ni] \\\\), while each image \\\\( V_i \\\\) comprises a set of regions \\\\( V_i = [v_1i, ..., v_ki, ..., v_mi] \\\\), where \\\\( n \\\\) and \\\\( m \\\\) represent the number of phrases and regions, respectively. The goal is to predict the region \\\\( v_ki \\\\) from the set of \\\\( m \\\\) regions in image \\\\( V_i \\\\) that correspond to the given phrase \\\\( s_li \\\\) in sentence \\\\( S_i \\\\). However, under the WPG setting, we only have access to coarse-grained sentence-image pair \\\\((S_i, V_i)\\\\) for training, whereas fine-grained phrase-region pair \\\\((s_li, v_ki)\\\\) is available during inference.\\n\\nIn this paper, we propose an Implicit-Enhanced Causal Inference (IECI) approach to model the implicit relations. Figure 2 shows the overall architecture of the proposed IECI approach, consisting of three major components: 1) Encoding Block, 2) Implicit-aware Deconfounded Attention (IDA) Block, 3) Implicit-aware Counterfactual Inference (ICI) Block. Prior to delving into the intricacies of the core components within IECI, we provide an overview of the encoding block.\\n\\n3.1. Encoding Block\\n\\nGiven \\\\( T \\\\) pairs of sentence and image, following the setting by Gupta et al. (2020), BERT (Devlin et al., 2019) and Faster R-CNN (Ren et al., 2015) are adopted to encode phrases and regions. Phrase Encoder. BERT-base model released by Devlin et al. (2019) is adopted as the phrase encoder, which is a lightweight language encoding model. Specifically, the phrases are first extracted by following Plummer et al. (2015). Then, BERT is utilized to encode the sentence \\\\( S_i \\\\), and finally the word vectors of all words in each phrase \\\\( s_li \\\\) are averaged as the phrase encoding.\\n\\nRegion Encoder. Faster R-CNN is adopted as the region encoder. Specifically, Faster R-CNN first utilizes the convolutional network (i.e., ResNet (He et al., 2016)) to compute a feature map of each image \\\\( V_i \\\\). On this basis, a region proposal network is used to generate the encoding of the region \\\\( v_ki \\\\) along with the corresponding bounding box.\"}"}
{"id": "lrec-2024-main-730", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this study, we take advantage of the intervention technique (Pearl and Mackenzie, 2018) and propose an Implicit-aware Deconfounded Attention (IDA) block to model the implicit relations for mitigating the confounding bias inside WPG. Specifically, we address two crucial questions: 1) how to mitigate the confounding bias through the front-door adjustment strategy (Pearl and Mackenzie, 2018); 2) how to implement the front-door adjustment strategy in the WPG task. We will provide comprehensive answers to these two questions in the subsequent section, formulated as follows.\\n\\nDeconfounded Causal Graph is leveraged to answer the question 1). As illustrated in Figure 2(a), we formulate the causation among sentence-image pairs $X$, multimodal knowledge $M$, phrase-region locations $L$, and confounding factors $C$. $X \\\\rightarrow M \\\\rightarrow L$ denotes the desired causal effect from sentence-image pairs $X$ to phrase-region locations $L$, where multimodal knowledge $M$ acts as a mediator. $X \\\\leftarrow C \\\\rightarrow L$ denotes the causal effect from the invisible confounding factors $C$ to sentence-image pairs $X$ and phrase-region locations $L$.\\n\\nWe leverage do-operator (Pearl and Mackenzie, 2018) to mitigate the confounding bias $(X, C)$ present in the path $M \\\\rightarrow L$. As shown in Figure 2(b), we block the back-door path $M \\\\leftarrow X \\\\leftarrow C \\\\rightarrow L$ under the condition of $X$. Then, we leverage the front-door adjustment strategy to analyze the causal effect of $X \\\\rightarrow L$, denoted as follows:\\n\\n$$P(L = l | do(X = x)) = X_m P(m | x) X_x P(x) [P(l | x, m)] (1)$$\\n\\nImplicit-aware Attention is leveraged to answer the question 2). On the basis of the front-door adjustment strategy in Eq.(1), we consider implementing it through the utilization of attention mechanisms. Considering the expensive computation of network forward propagation for all samples, we introduce the Normalized Weighted Geometric Mean (NWGM) (Srivastava et al., 2014; Xu et al., 2015) approximation. Therefore, we can sample $X$, $M$ and complete $P(L | do(X))$ by feeding them into the network, and then leverage NWGM approximation to achieve the goal of Eq.(1), denoted as follows:\\n\\n$$P(L | do(X)) \\\\approx \\\\text{softmax}[g(\\\\hat{X}, \\\\hat{M})] (2)$$\\n\\nwhere $g(\\\\cdot)$ is a network employed to parameterize the predictive distribution $P(l | x, m)$, which is followed by a softmax layer. Besides, $\\\\hat{M} = P_m P(M = m | h(X))$ and $\\\\hat{X} = P_x P(X = x | f(X))$ represent the estimations of self-sampling and cross-sampling, respectively. The variables $m$, $x$ correspond to the embedding vectors of $m$, $x$. The query embedding functions $h(\\\\cdot)$ and $f(\\\\cdot)$ are utilized to transform the input $X$ into two distinct query sets, which can be parameterized as networks. Consequently, we leverage attention mechanisms to estimate the self-sampling $\\\\hat{M}$ and cross-sampling $\\\\hat{X}$ as shown in Figure 2:\\n\\n$$\\\\hat{M} = \\\\frac{V_M \\\\text{softmax}(Q_M^\\\\top K_M)}{V_M} \\\\frac{V_C \\\\text{softmax}(Q_C^\\\\top K_C)}{V_C} (3)$$\\n\\n$$\\\\hat{X} = V_C \\\\text{softmax}(Q_M^\\\\top K_C) (4)$$\\n\\nwhere Eq.(3) and Eq.(4) denote as the self-sampling attention and cross-sampling attention. Particularly, the upper formula of Eq.(3) calculates the self-sampling attention for multimodal knowledge $M$, while the lower formula of Eq.(3) calculates the self-sampling attention for confounding factors $C$. In the implementation, $Q_M$ and $Q_C$ are derived from $h(X)$ and $f(X)$. $K_M$ and $V_M$ are obtained from the current input sample, while $K_C$ and $V_C$ come from other samples in the training set and serve as global dictionary compressed from the whole training dataset. Specifically, we initialize this dictionary by using K-means clustering (Hartigan and Wong, 1979) on all the embeddings of samples in the training set, such as region features.\\n\\n3.3. Implicit-aware Counterfactual Inference Block\\n\\nIn this study, we take advantage of the counterfactual technique (Pearl and Mackenzie, 2018) and propose an Implicit-aware Counterfactual Inference (ICI) block to highlight the implicit relations beyond the explicit, thereby addressing the imbalance problem between them. Specifically, we treat the explicit relations as the direct effect in the counterfactual technique, and then reduce such direct effect to achieve the goal of reducing the importance of the explicit relations while highlighting the implicit relations. Therefore, there are also two questions to be answered: 1) how to analyze the direct effect of the explicit relations; 2) how to reduce such direct effect to improve the alignment of implicit phrase-region pairs. Next, we will answer the two questions, formulated as follows.\\n\\nCounterfactual Causal Graph is leveraged to answer the question 1). As illustrated in Figure 2(c), we formulate the causation between explicit and implicit relations through the path $X \\\\rightarrow E \\\\rightarrow L$, which denotes the causal effect from the imbalanced explicit relations $E$ to phrase-region locations $L$. On this basis, we leverage counterfactual to analyze the causal effects. Following the counterfactual notations in Pearl and Mackenzie (2018) and Niu et al. (2021), we denote $L_{x,e} = L(X = x, E = e)$ as the total effect (TE). We block the path $X \\\\rightarrow E$ to obtain the explicit direct effect (EDE) on $L$ as shown in\"}"}
{"id": "lrec-2024-main-730", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Four main types of the implicit phrase-region matching relations together with their corresponding ratios within the implicit phrase-region pairs.\\n\\nFigure 2 (d), denoted as $L_x^*, e = L(X = x^*, E = e)$, which represents the value of $L$ when we set $x$ to $x^*$. Note that only in the counterfactual world, $X$ can be simultaneously set to different values $x$ and $x^*$. To reduce the EDE from TE, we aim to derive the explicit indirect effect (EIE), denoted as follows:\\n\\n$$EIE = TE - EDE = L_{x^*, e} - L_{x, e}$$\\n\\nImplicit-aware Inference is leveraged to answer the question 2). Upon obtaining the output representations $o_i$ from IDA block, we utilize ICI to reduce the direct effect of explicit relations in Eq.(5). In this context, the representations $o_i$ can be seen as value $x$ in ICI. For the value of $x^*$, we assume that the model will randomly guess with equal probability, denoted as follows:\\n\\n$$x = o_i, x^* = r$$\\n\\nwhere $r$ denotes a learnable parameter. Finally, we can compute the EIE representation (i.e., similarity matrix $A_{n,m}$) in Eq.(5), which are then employed to calculate similarities for the WPG task.\\n\\n3.4. Weakly-supervised Optimization\\n\\nDuring the training stage, our access is limited to coarse-grained sentence-image pairs $(S, V)$, from which we derive the ground-truth label $y$ based on the matching relations of each sentence-image pair. Therefore, following Chen et al. (2018), we convert the phrase-region similarity matrix $A_{n,m}$ to sentence-image similarity matrix $A_{T,T}$ for weakly-supervised optimization. Specifically, we first compute the similarity value $\\\\text{sim}_i$ for the sentence-image pair $(S_i, V_i)$, i.e.,\\n\\n$$\\\\text{sim}_i = \\\\frac{1}{n} \\\\sum_{n} A_{n,m}.$$\\n\\nOn this basis, we then calculate the similarity values between the current sentence $S_i$ and all images $V$. Finally, we obtain the sentence-image similarity matrix $A_{T,T}$ encompassing all sentence-image pairs. After the argmax operation, we obtain the predicted label $\\\\hat{y}$. To train our IECI approach end-to-end, we leverage the cross-entropy loss function, denoted as follows:\\n\\n$$L_{wpg} = - \\\\sum_{i=1}^{T} y_i \\\\log \\\\hat{y}_i$$\\n\\nBesides, we incorporate a learnable parameter $r$ in Eq.(6), which controls the sharpness of the distribution of $L(x^*, e)$. If $r$ is inappropriate, EIE would be guided by TE or EDE. To avoid this, we leverage a Kullback-Leibler divergence to update $r$ via back propagation:\\n\\n$$L_{kl} = \\\\sum_{x} p(y|x, e) \\\\log \\\\frac{p(y|x, e)}{p(y|x^*, e)}$$\\n\\nConsequently, our optimization objective comprises both $L_{wpg}$ and $L_{kl}$, denoted as $L_{total} = L_{wpg} + \\\\alpha L_{kl}$, where $\\\\alpha$ serves as a hyper-parameter responsible for balancing the loss between the WPG task and ICI block.\\n\\n4. Experimental Settings\\n\\n4.1. Data Annotation and Settings\\n\\nTo evaluate the effectiveness of our IECI approach in WPG, we construct an implicit-enhanced dataset based on the Flickr30K-Entities (Plummer et al., 2015) dataset. Specifically, for the annotation of the implicit-enhanced dataset, we first summarize four main types of implicit relations through preliminary annotation, and estimate their ratios by analyzing 100 randomly-selected annotated samples within Flickr30K-Entities. Specifically, the four types of implicit relations (as illustrated in Figure 3) are introduced as follows:\\n\\n- **CU** denotes the model needs extra knowledge to understand commonsense, e.g., \\\"support\\\" needs to understand the commonsense of holding a flag to precisely predict the region.\\n- **CCU** denotes the model needs the context of sentences to understand the meaning of...\"}"}
{"id": "lrec-2024-main-730", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of several state-of-the-art baselines and our approaches on both the Flickr30K and COCO datasets (training sets).\\n\\nImplicit, Explicit and Full represent the evaluation results on our annotated implicit and explicit datasets, the original Flickr30K-Entities dataset, respectively. The result with symbol \u266f is retrieved from Chen et al. (2018); those with \u2020 are from Gupta et al. (2020); this with \u266e is from Wang et al. (2021a) and this with \u00a7 is from Liu et al. (2021). The symbol - denotes the results are not reported by these papers.\\n\\nphrases, e.g., \\\"another person\\\" can hardly correspond to the exact region without context.\\n\\n\u2022 SRU represents the position relation between two target objects in the sentence, e.g., the region of \\\"next to another man\\\" contains the position relation between two men.\\n\\n\u2022 NU represents a phrase that may correspond to multiple regions, e.g., \\\"three of them\\\" corresponds to three regions in the image.\\n\\nFor annotation, we assign two annotators to tag each phrase-region pair and the Kappa consistency check value of the annotation is 0.85. When two annotators cannot reach an agreement, an expert will make the final decision, ensuring the quality of data annotations. Furthermore, we annotate 2K coarse-grained sentence-image samples, including 15K fine-grained phrase-region pairs in the original Flickr30K-Entities dataset. After the manual annotation, we finally obtain 1.4K implicit phrase-region pairs. The ratio between the implicit and explicit phrase-region pairs is about 1:9, which shows an extreme imbalance between them, thereby encouraging us to address the challenge of highlighting the implicit relations beyond the explicit.\\n\\nFor training WPG, following Gupta et al. (2020), we leverage the training sets from Flickr30K and COCO datasets. For inference, we maintain the same setting of validation and test splits as the original Flickr30K-Entities (Full) dataset, and obtain the Implicit, Explicit datasets based on these splits.\\n\\n4.2. Baselines\\n\\nWe choose several state-of-the-art baselines for WPG to compare performance with our IECI approach, described as follows.\\n\\n\u2022 KAC-Net (Chen et al., 2018) explores the consistency in visual and language as complementary external knowledge.\\n\\n\u2022 ARN (Liu et al., 2019a) builds the correspondence between image region and query in an adaptive manner.\\n\\n\u2022 KPRN (Liu et al., 2019b) models the relation between target and contextual entities.\\n\\n\u2022 InfoGround (Gupta et al., 2020) leverages contrastive learning to maximize a lower bound between region features and contextualized word representations.\\n\\n\u2022 ALBEF (Li et al., 2021) introduces a contrastive loss to align the image and text representations before fusing them into cross-modal attention.\\n\\n\u2022 CL&KD (Wang et al., 2021a) focuses on distilling knowledge from a generic object detector under the framework of contrastive learning.\\n\\n\u2022 ReIR (Li et al., 2021) incorporates coarse-to-fine object refinement and entity relation modeling into a two-stage deep network.\\n\\nMoreover, an advanced multimodal pre-training model BLIP (Li et al., 2022) is leveraged to compare with IECI. Here, it should be noted that BLIP cannot directly fine-tune the WPG task, since BLIP is pre-trained with the coarse-grained sentence-image...\"}"}
{"id": "lrec-2024-main-730", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"pairs. Thus, we treat the phrase-region pairs as the coarse-grained pairs to fine-tune the WPG task.\\n\\n4.3. Implementation Details and Metrics\\n\\nIn our experiments, we leverage open-source codes to obtain experimental results of all the baselines on both Implicit and Explicit datasets, and re-implement the results of several baselines by their open-source codes on Full datasets. The hyper-parameters of these baselines reported by their public papers maintain the same setting, and the others are tuned according to the validation set. Specifically, for phrase encoder, the parameters of BERT are following Devlin et al. (2019). For region encoder, we utilize a pre-trained Faster R-CNN on the Visual Genome (Krishna et al., 2017) dataset to generate a maximum number of 100 object regions. For IECI, we employ the Adam optimizer with an initial learning rate of 1e-5 for training. The regularization weight of parameters is 1e-4. The hyper-parameter $\\\\alpha$ is set to be 0.1 and the batch size is set to be 64. The layers of the self-sampling attention and cross-sampling attention in IDA are both set to be 6. The overall training parameters of our approach are 0.16B. All the baselines are reproduced using PyTorch on a machine equipped with an NVIDIA GeForce RTX 3090, an Intel(R) Xeon(R) E5-2650 v4 CPU (2.20 GHz), CUDA version 11.7, and the PyTorch 1.7.1 library with Python 3.6.13, running on Ubuntu 20.04.1 LTS. To facilitate the corresponding research in this direction, all codes alongside our implicit-enhanced dataset are released.\\n\\nBesides, the WPG performance is evaluated using Recall@$k$, $k \\\\in (1, 5)$, which measures the fraction of phrases for which the ground-truth bounding box exhibits an IoU $\\\\geq 0.5$ with any of the top-$k$ predicted boxes. Moreover, $t$-test is used to evaluate the significance of the performance difference by following Chen et al. (2020b).\\n\\n5. Results and Discussions\\n\\n5.1. Experimental Results\\n\\nTable 1 shows the performance comparison of different approaches. From this table, we can see that:\\n\\n1) The performances of all approaches on the Implicit dataset are consistently lower than the Explicit dataset. This indicates that modeling the implicit relations is more challenging than the explicit, and encourages us to consider such implicit relations problem in WPG. Besides, we also observe that the performance of both Implicit and Explicit datasets is improved, and we analyze the reason that there may be some overlap between the implicit and explicit relations in the current dataset, leading to performance improvements in explicit relations when employing causal inference.\\n\\n2) The performance improvements of our IECI approach on the Implicit dataset are larger than those on the Explicit dataset. This indicates that IECI can effectively address the challenge of highlighting the implicit relations beyond the explicit. Furthermore, our IECI approach consistently performs better than all other approaches. Impressively, compared to the best-performing ReIR approach, our IECI approach achieves the average R@1 improvements of 2.83%, 2.29% and 2.34% on all three Implicit, Explicit and Full datasets, respectively. Significance test shows that all these improvements are significant ($p$-value < 0.01). These demonstrate that IECI can better highlight the implicit relations and meantime maintain the performance of the explicit. Particularly, we observe that the performance of BLIP is quite worse than IECI. This is reasonable since BLIP is pre-trained on the coarse-grained sentence-images pairs, thus is not suitable for aligning the fine-grained phrase-region pairs.\\n\\n3) Our IECI approach trained on both the Flickr30K and COCO datasets still outperforms all other approaches. This justifies the robustness of IECI, and again encourages us to consider the important implicit relations in the WPG task.\\n\\n5.2. Contributions of Causal Inference\\n\\nTo further investigate the influence of key causal components within our IECI approach, we conduct a series of ablation studies as shown in Table 1. From this table, we can see that:\\n\\n1) w/o IDA exhibits inferior performance compared to IECI, with an average decrease in R@1 by 2.98% ($p$-value < 0.01) and 1.35% ($p$-value < 0.05) on the Implicit and Explicit datasets, respectively. This further justifies that our proposed IDA block can effectively model the implicit relations, encouraging us to leverage the intervention technique to mitigate the confounding bias.\\n\\n2) w/o ICI also shows inferior performance compared to IECI, with an average R@1 decrease\"}"}
{"id": "lrec-2024-main-730", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of 2.56% (**p**-value < 0.01) and 0.76% (**p**-value < 0.05) on the Implicit and Explicit datasets, respectively. This further justifies that ICI block can effectively highlight the implicit relations beyond the explicit, encouraging us to leverage the counterfactual technique to address the imbalance problem.\\n\\n3) **w/o Both** yields a significant average decrease in R@1 by 4.70% (**p**-value < 0.01) on the Implicit dataset, which excludes both IDA and ICI blocks. This again justifies the effectiveness of our IECI approach in modeling and highlighting the implicit phrase-region matching relations.\\n\\n5.3. Comparison with Multimodal LLMs\\n\\nRecently, the emergence of large language models (LLMs) has demonstrated their remarkable abilities across various fields and tasks. Since GPT-4 is closed-sourced and currently cannot evaluate the multimodal tasks, we choose two advanced open-sourced multimodal LLMs, i.e., MiniGPT4-13B (Zhu et al., 2023) and LLaVA-13B (Liu et al., 2023), to verify the effectiveness of IECI. Specifically, we first randomly select 30 sentence-image examples with the implicit and explicit relations for comparisons on the WPG task. Then, we select two well-studied evaluation methods for LLMs as follows:\\n\\n1) **Zero-Shot (ZS).** Following the ZS setting for evaluating LLMs proposed by Bang et al. (2023), given a prompt including the task instructions, sentence-image pair and the bounding box list of each phrase, we utilize multimodal LLMs to \\\"generate the bounding boxes from the list that can reflect the given phrase\\\".\\n\\n2) **In-Context Learning (ICL).** Following the ICL setting for evaluating LLMs proposed by Li et al. (2023b), building upon the ZS, we introduce one additional demonstration example \\\"phrase, boxes;...;phrase, boxes\\\" as the prompt. As shown in Figure 4, we observe that IECI outperforms the multimodal LLMs by a large margin, indicating that the multimodal LLMs still face challenges in understanding the deep multimodal semantics. Moreover, we find that the performance of ZS is better than ICL, since the image encoders BLIP-2 and CLIP within MiniGPT4-13B and LLaVA-13B lack the ability of ICL as reported in Li et al. (2023a) and Radford et al. (2021).\\n\\n5.4. Visualization Study\\n\\nWe conduct a visualization analysis of IECI in our implicit-enhanced dataset. As illustrated in Figure 5, we can see that:\\n\\n1) The prediction of the region corresponding to the phrase \\\"support\\\" (red box in Figure 5 (a)) is challenging, which requires contextual and commonsense knowledge. This further indicates the difficulty in precisely aligning such implicit phrase-region pairs.\\n\\n2) We compare the performance of the best-performing baseline ReIR and IECI, as shown in Figure 5 (b) and (c). For the phrase \\\"support\\\", ReIR fails to predict the corresponding region, while IECI successfully identifies the correct region (yellow box). This again justifies the effectiveness of IECI in precisely identifying such implicit phrase-region matching.\\n\\n5.5. What Would We Do Next?\\n\\nOur proposed IECI approach has demonstrated promising results in comprehensive experiments, showcasing its effectiveness in addressing the challenges of modeling the implicit relations and highlighting them beyond the explicit. However, we believe that there are still three potential directions to precisely predict implicit phrase-region matching relations. Despite this is not the focus of this paper, we believe that these three directions can not only further improve the performance of the WPG task, but also facilitate related research in this direction, as described below.\\n\\nFirstly, for **Multimodal Representation**, our IECI approach has limitations in effectively representing underlying features. However, the existing multimodal pre-training models (e.g., CLIP (Radford et al., 2021), BLIP (Li et al., 2022)) are mostly pre-trained on coarse-grained sentence-image pairs, making them lack the ability of fine-grained multimodal semantics understanding, as reported in Section 5.3. In our future work, we would like to incorporate the multimodal LLMs (e.g., MiniGPT4 (Zhu et al., 2023), LLaVA (Liu et al., 2023)) to enhance the multimodal representation abilities of our approach for the WPG task.\\n\\nSecondly, for **Knowledge Injection**, we observe that the ratio of commonsense understanding is 34.5% as illustrated in Figure 3, which further inspires us to consider integrating the external knowledge (e.g., multimodal knowledge graph (Zhu et al., 2022)) to assist in capturing the implicit relations.\\n\\nFinally, for **Evaluation of Each Implicit Relation**, due to the imbalanced proportion of implicit relations in the whole dataset, the sample scale of...\"}"}
{"id": "lrec-2024-main-730", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"each implicit relation is relatively small (see Section 4.1), making it difficult to comprehensively evaluate the effectiveness of our IECI approach on each implicit relation. In our future work, we would like to leverage the multimodal LLMs to automatically annotate (e.g., the automatic annotation approach proposed by Pei et al. (2023)) different types of implicit relations, which may promote the further research in this direction.\\n\\n6. Conclusion\\n\\nIn this paper, we propose an Implicit-Enhanced Causal Inference (IECI) approach to address the implicit phrase-region matching relations problem inside WPG. The key idea of IECI is to utilize the causal inference (i.e., intervention and counterfactual techniques) to effectively model the implicit relations and highlight them beyond the explicit. To comprehensively evaluate IECI, we construct a specialized implicit-enhanced dataset. Detailed experimental results on this dataset demonstrate the superior performance of IECI over several state-of-the-art baselines.\\n\\nIn our future work, we would like to introduce more information (e.g., multimodal knowledge graph (Zhu et al., 2022)) to assist in aligning implicit phrase-region pairs. Moreover, we would like to transfer IECI to other tasks which also have the implicit relation problems, such as Referring Expression Comprehension and video grounding.\\n\\nEthics Statement\\n\\nData Disclaimer. We construct a high-quality implicit-enhanced dataset based on Flickr30K-Entities, a dataset which is widely used by other academics and is typically accessible to the public. Therefore, our proposed dataset does not involve any sensitive information that may harm others.\\n\\nHuman Annotation. When recruiting annotators for this study, we claim that all potential annotators are free to choose whether they want to participate, and they can withdraw from the study anytime without any negative repercussions. Thus, the establishment of our dataset is compliant with ethics.\\n\\nAcknowledgments\\n\\nWe thank our anonymous reviewers for their helpful comments. This work was supported by three NSFC grants, i.e., No.62006166, No.62076175 and No.62076176. This work was also supported by a Project Funded by the Priority Academic Program Development of Jiangsu Higher Education Institutions (PAPD).\\n\\nReferences\\n\\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. CoRR, abs/2302.04023.\\n\\nKan Chen, Jiyang Gao, and Ram Nevatia. 2018. Knowledge aided consistency for weakly supervised phrase grounding. In Proceedings of CVPR 2018, pages 4042\u20134050.\\n\\nKeqin Chen, Richong Zhang, Samuel Mensah, and Yongyi Mao. 2022. Contrastive learning with expectation-maximization for weakly supervised phrase grounding. In Proceedings of EMNLP 2022, pages 8549\u20138559.\\n\\nLong Chen, Wenbo Ma, Jun Xiao, Hanwang Zhang, and Shih-Fu Chang. 2021. Ref-nms: Breaking proposal bottlenecks in two-stage referring expression grounding. In Proceedings of AAAI 2021, pages 1036\u20131044.\\n\\nLong Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang. 2020a. Counterfactual samples synthesizing for robust visual question answering. In Proceedings of CVPR 2020, pages 10797\u201310806.\\n\\nXiao Chen, Changlong Sun, Jingjing Wang, Shoushan Li, Luo Si, Min Zhang, and Guodong Zhou. 2020b. Aspect sentiment classification with document-level sentiment preference modeling. In Proceedings of ACL 2020, pages 3667\u20133677.\\n\\nSamyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, and Ajay Divakaran. 2019. Align2ground: Weakly supervised phrase grounding guided by image-captionalignment. In Proceedings of ICCV 2019, pages 2601\u20132610.\\n\\nJiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. 2021. Transvg: End-to-end visual grounding with transformers. In Proceedings of ICCV 2021, pages 1749\u20131759.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL 2019, pages 4171\u20134186.\\n\\nYang Feng, Lin Ma, Wei Liu, and Jiebo Luo. 2019. Unsupervised image captioning. In Proceedings of CVPR 2019, pages 4125\u20134134.\"}"}
{"id": "lrec-2024-main-730", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-730", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: towards real-time object detection with region proposal networks. In Proceedings of NeurIPS 2015, pages 91\u201399.\\n\\nAnna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. 2016. Grounding of textual phrases in images by reconstruction. In Proceedings of ECCV 2016, pages 817\u2013834.\\n\\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1):1929\u20131958.\\n\\nKaihua Tang, Jianqiang Huang, and Hanwang Zhang. 2020a. Long-tailed classification by keeping the good and removing the bad momentum causal effect. In Proceedings of NeurIPS 2020.\\n\\nKaihua Tang, Yulei Niu, Jianqiang Huang, Jiaxin Shi, and Hanwang Zhang. 2020b. Unbiased scene graph generation from biased training. In Proceedings of CVPR 2020, pages 3713\u20133722.\\n\\nLiwei Wang, Jing Huang, Yin Li, Kun Xu, Zhengyuan Yang, and Dong Yu. 2021a. Improving weakly supervised visual grounding by contrastive knowledge distillation. In Proceedings of CVPR 2021, pages 14090\u201314100.\\n\\nLiwei Wang, Yin Li, Jing Huang, and Svetlana Lazebnik. 2019. Learning two-branch neural networks for image-text matching tasks. IEEE Trans. Pattern Anal. Mach. Intell., 41(2):394\u2013407.\\n\\nQinxin Wang, Hao Tan, Sheng Shen, Michael W. Mahoney, and Zhewei Yao. 2020a. MAF: multimodal alignment framework for weakly-supervised phrase grounding. In Proceedings of EMNLP 2020, pages 2030\u20132038.\\n\\nTan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. 2020b. Visual commonsense R-CNN. In Proceedings of CVPR 2020, pages 10757\u201310767.\\n\\nTan Wang, Chang Zhou, Qianru Sun, and Hanwang Zhang. 2021b. Causal attention for unbiased visual recognition. In Proceedings of ICCV 2021, pages 3091\u20133100.\\n\\nFanyi Xiao, Leonid Sigal, and Yong Jae Lee. 2017. Weakly-supervised visual grounding of phrases with linguistic structures. In Proceedings of CVPR 2017, pages 5253\u20135262.\\n\\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of ICML 2015, pages 2048\u20132057.\\n\\nLi Yang, Yan Xu, Chunfeng Yuan, Wei Liu, Bing Li, and Weiming Hu. 2022. Improving visual grounding with visual-linguistic verification and iterative reasoning. In Proceedings of CVPR 2022, pages 9489\u20139498.\\n\\nXu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai. 2021. Causal attention for vision-language tasks. In Proceedings of CVPR 2021, pages 9847\u20139857.\\n\\nDong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua, and Qianru Sun. 2020a. Causal intervention for weakly-supervised semantic segmentation. In Proceedings of NeurIPS 2020.\\n\\nShengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, and Fei Wu. 2020b. Devlbert: Learning deconfounded visio-linguistic representations. In Proceedings of ACM MM 2020, pages 4373\u20134382.\\n\\nFang Zhao, Jianshu Li, Jian Zhao, and Jiashi Feng. 2018. Weakly supervised phrase localization with multi-scale anchored transformer network. In Proceedings of CVPR 2018, pages 5696\u20135705.\\n\\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. CoRR, abs/2304.10592.\\n\\nXiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang, Penglei Sun, Xuwu Wang, Yanghua Xiao, and Nicholas Jing Yuan. 2022. Multi-modal knowledge graph construction and application: A survey. CoRR, abs/2202.05786.\"}"}
