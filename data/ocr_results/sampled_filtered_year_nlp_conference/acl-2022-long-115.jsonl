{"id": "acl-2022-long-115", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems\\n\\nBosheng Ding\u22171,2 Junjie Hu3 Lidong Bing1 Sharifah Mahani Aljunied1 Shafiq Joty2 Luo Si1 Chunyan Miao2\\n\\n1 DAMO Academy, Alibaba Group 2 Nanyang Technological University, Singapore 3 University of Wisconsin-Madison\\n\\n{bosheng.ding, l.bing, mahani.aljunied, luo.si}@alibaba-inc.com junjie.hu@wisc.edu, {srjoty, ascymiao}@ntu.edu.sg\\n\\nAbstract\\n\\nOver the last few years, there has been a move towards data curation for multilingual task-oriented dialogue (ToD) systems that can serve people speaking different languages. However, existing multilingual ToD datasets either have a limited coverage of languages due to the high cost of data curation, or ignore the fact that dialogue entities barely exist in countries speaking these languages. To tackle these limitations, we introduce a novel data curation method that generates GlobalWoZ\u2014a large-scale multilingual ToD dataset globalized from an English ToD dataset for three unexplored use cases of multilingual ToD systems. Our method is based on translating dialogue templates and filling them with local entities in the target-language countries. Besides, we extend the coverage of target languages to 20 languages. We will release our dataset and a set of strong baselines to encourage research on multilingual ToD systems for real use cases.\\n\\n1 Introduction\\n\\nOne of the fundamental objectives in pursuit of artificial intelligence is to enable machines with the ability to intelligently communicate with human in natural languages, with one of the widely-heralded applications being the task-oriented dialogue (ToD) systems (Gupta et al., 2006; Bohus and Rudnicky, 2009). Recently, ToD systems have been successfully deployed to assist users with accomplishing certain domain-specific tasks such as hotel booking, alarm setting or weather query (Eric et al., 2017; Wu et al., 2019; Lin et al., 2020; Zhang et al., 2020), thanks to the joint advent of neural networks and availability of domain-specific data. However, most existing ToD systems are predominately built for English, limiting their service for all of the world\u2019s citizens. The reason of this limitation lies in the stark lack of high-quality multilingual ToD datasets due to the high expense and challenges of human annotation (Razumovskaia et al., 2021). One solution to this is annotating conversations in other languages from scratch, e.g., CrossWoZ (Zhu et al., 2020) and BiToD (Lin et al., 2021). However, these methods involve expensive human efforts for dialogue collection in the other languages, resulting in a limited language/domain coverage. The other major line of work focused on translating an existing English ToD dataset into target languages by professional human translators (Upadhyay et al., 2018; Schuster et al., 2019; van der Goot et al., 2021; Li et al., 2021). Despite the increasing language coverage, these methods simply translated English named entities (e.g., location, restaurant name) into the target languages, while ignored the fact that these entities barely exist in countries speaking these languages. This hinders a trained ToD system from supporting the real use cases where a user looks for local entities in a target-language country. For example in Figure 1, a user may look for the British Museum when traveling to London (A.), while look for the Oriental Pearl Tower when traveling to Shanghai (B.). In addition, prior studies (Cheng and Butler, 1989; Kim, 2006) have shown that code-switching phenomena frequently occurs in a dialogue when a speaker cannot express an entity immediately and has to alternate between two languages to convey information more accurately. Such phenomena could be ubiquitous during the cross-lingual and cross-country task-oriented conversations. One of the reasons for code-switching is that there are no exact translations for many local entities in the other languages. Even though we have the translations, they are rarely used by local people. For example in Figure 1 (C.), after obtaining the recommendation from a ToD system, a Chinese speaker traveling to London would rather use the English\"}"}
{"id": "acl-2022-long-115", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"London\\nA. Use Case: E&E\\nI'm looking for an attraction to visit in London.:\\n\\n\ud83e\udd16: I will recommend the British Museum.\\n\\nShanghai\\nB. Use Case: F&F\\n\u6211 \u60f3 \u5728 \u4e0a \u6d77 \u627e \u4e2a \u5730 \u65b9 \u73a9\u3002\\n\\n\ud83e\udd16: \u6211 \u63a8 \u8350 \u4e1c \u65b9 \u660e \u73e0\u3002\\n\\nC. Use Case: F&E\\n\u6211 \u60f3 \u5728 \u4f26 \u6566 \u627e \u4e2a \u5730 \u65b9 \u73a9\u3002\\n\\n\ud83e\udd16: \u6211 \u63a8 \u8350 The British Museum\u3002\\n\\nD. Use Case: E&F\\n\\nFigure 1: Examples of four use cases for multilingual ToD systems: A. Use Case E&E: A English speaker travels to a country of English. B. Use Case F&F: A foreign language speaker travels to a country of the foreign language. C. Use Case F&E: A foreign language speaker travels to a country of English. D. Use Case E&F: A English speaker travels to a country of a foreign language.\\n\\nentity \u201cBritish Museum\u201d than its Chinese translation to search online or ask local people. To verify this code-switching phenomena, we have also conducted a case study (\u00a76.1) which shows that searching the information about translated entities online yields a much higher failure rate than searching them in their original languages. Motivated by these observations, we define three unexplored use cases of multilingual ToD where a foreign-language speaker uses ToD in the foreign-language country (F&F) or an English country (F&E), and an English speaker uses ToD in a foreign-language country (E&F). These use cases are different from the traditional E&E use case where an English speaker uses ToD in an English-speaking country.\\n\\nTo bridge the aforementioned gap between existing data curation methods and the real use cases, we propose a novel data curation method that globalizes an existing multi-domain ToD dataset beyond English for the three unexplored use cases. Specifically, building on top of MultiWoZ (Budzianowski et al., 2018) \u2014 an English ToD dataset for dialogue state tracking (DST), we create GlobalWoZ, a new multilingual ToD dataset in three new target-languages via machine translation and crawled on-tologies in the target-language countries.\\n\\nOur method only requires minor human efforts to post-edit a few hundred machine-translated dialogue templates in the target languages for evaluation. Besides, as cross-lingual transfer via pre-trained multilingual models (Devlin et al., 2019; Conneau et al., 2020; Liu et al., 2020; Xue et al., 2021) has proven effective in many cross-lingual tasks, we further investigate another question: How do these multilingual models trained on the English ToD dataset transfer knowledge to our globalized dataset?\\n\\nTo answer this question, we prepare a few baselines by evaluating popular ToD systems on our created test datasets in a zero-shot cross-lingual transfer setting as well as a few-shot setting.\\n\\nOur contributions include the following:\\n\\n\u2022 To the best of our knowledge, we provide the first step towards analyzing three unexplored use cases for multilingual ToD systems.\\n\u2022 We propose a cost-effective method that creates a new multilingual ToD dataset from an existing English dataset. Our dataset consists of high-quality test sets which are first translated by machines and then post-edited by professional translators in three target languages (Chinese, Spanish and Indonesian). We also leverage machine translation to extend the language coverage of test data to another 17 target languages.\\n\u2022 Our experiments show that current multilingual systems and translate-train methods fail in zero-shot cross-lingual transfer on the dialogue state tracking task. To tackle this problem, we propose several data augmentation methods to train strong baseline models in both zero-shot and few-shot cross-lingual transfer settings.\"}"}
{"id": "acl-2022-long-115", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to globalize an existing English ToD dataset for the three aforementioned use cases, we propose an approach consisting of four steps as shown in Figure 2: (1) we first extract dialogue templates from the English ToD dataset by replacing English-specific entities with a set of general-purpose placeholders (\u00a72.1); (2) we then translate the templates to a target language for both training and test data, with one key distinction that we only post-edit the test data by professional translators to ensure the data quality for evaluation (\u00a72.2); (3) next, we collect ontologies (Kiefer et al., 2021) containing the definitions of dialogue acts, local entities and their attributes in the target-language countries (\u00a72.3); (4) finally, we tailor the translated templates by automatically substituting the placeholders with entities in the extracted ontologies to construct data for the three use cases (\u00a72.4).\\n\\n2.1 Automatic Template Creation\\nWe start with MultiWoZ 2.2 (Zang et al., 2020) \u2013 a high-quality multi-domain English ToD dataset with more accurate human annotations compared to its predecessors MultiWoZ 2.0 (Budzianowski et al., 2018) and MultiWoz 2.1 (Eric et al., 2020). For the sake of reducing human efforts for collecting ToD context in the target languages, we re-use the ToD context written by human in MultiWoZ as the dialogue templates. Specifically as shown in Figure 2, we replace the English entities in MultiWoz by a set of general-purpose placeholders such as \\\\[\\\\text{attraction-name0}\\\\] and \\\\[\\\\text{attraction-postcode1}\\\\], where each placeholder contains the entity's domain, attribute and ID. To do so, we first build a dictionary with entity-placeholder pairs by parsing the annotations of all dialogues. For example, from a dialogue text \u2014\\n\\n\u201cI recommend Whale of a time and the postcode is cb238el. \u201d\\n\\n\u2014 we obtain two entity-placeholder pairs from its human annotations, i.e., \\\\((\\\\text{Whale of a time}, \\\\text{attraction-name0})\\\\) and \\\\((\\\\text{cb238el}, \\\\text{attraction-postcode1})\\\\). Next, we identify entities in the dialogue by their word index from the human annotations, replace them with their placeholders in the dictionary, and finally obtain dialogue templates with placeholders. Notably, we skip the entities with their attributes of \\\\[\\\\text{choice}\\\\] and \\\\[\\\\text{ref}\\\\] that represent the number of choices and booking reference number, as these attributes could be used globally.\\n\\n2.2 Labeled Sequence Translation\\nFollowing Liu et al. (2021) that translates sentences with placeholders, we use a machine translation system to translate dialogue templates with our designed placeholders. As we observe, a placeholder containing an entity domain, attribute and ID (e.g., \\\\[\\\\text{attraction-name0}\\\\]) is useful to provide contextually meaningful information to the translation system, thus usually resulting in a high-quality translation with the placeholder unchanged. This also enables us to easily locate the placeholders in the translation output and replace them with new entities in the target language.\\n\\nTo build a high-quality test set for evaluation, we further hire professional translators to post-edit a few hundred machine-translated templates, which produces natural and coherent sentences in the target languages.\\n\\nWith the goal of selecting representative test templates for post-editing, we first calculate the frequency of all the 4-gram combinations in the MultiWoZ data, and then score each dialogue in the test set by the sum of the frequency of all the 4-gram combinations in the dialogue divided by the dialogue's word length. We use this scoring function to estimate the representiveness of a dialogue in the original dataset. Finally, we select the top 500 high-scoring dialogues in the test set for post-editing.\\n\\nWe also use the same procedure to create a small high-quality training set for few-shot cross-lingual transfer setting.\\n\\n2.3 Collection of Local Ontology\\nMeanwhile, we crawl the attribute information of local entities in three cities from public websites (e.g., tripadvisor.com, booking.com) to create three ontologies for the three corresponding target languages respectively. As shown in Table 8 in Appendix E, we select Barcelona for Spanish (an Indo-European language), Shanghai for Mandarin (a Sino-Tibetan language) and Jakarta for Indonesian (an Austronesian language), which cover a set of typologically different language families. Given a translated dialogue template, we can easily sample a random set of entities for a domain of interest from a crawled ontology and assign the entities to the template's placeholders to obtain a\\n\\n3 We use Google Translate (https://cloud.google.com/translate), an off-the-shelf MT system.\\n\\n4 Appendix B has an example of label sequence translation.\\n\\n5 Appendix C shows the bleu scores between MT test data and MTPE test data.\\n\\n6 Appendix D shows the English test data distribution.\"}"}
{"id": "acl-2022-long-115", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I recommend Whale of a time and the post code is cb238el.\\n\\nDomain: attraction\\nName: Whale of a time\\nPostcode: cb238el\\n\\n\u6211\u63a8\u8350 Whale of a time\uff0c\u90ae\u653f\u7f16\u7801\u662f cb238el\u3002\\n\\nDomain: attraction\\nName: Whale of a time\\nPostcode: cb238el\\n\\n\u6211\u63a8\u8350 [attraction-name0]\uff0c\u90ae\u653f\u7f16\u7801\u662f [attraction-postcode1]\u3002\\n\\nDomain: attraction\\nName: [attraction-name0]\\nPostcode: [attraction-postcode1]\\n\\nFigure 2: Illustration of our proposed pipeline: 1. Automatic Template Creation 2. Labeled Sequence Translation 3. Localized Ontologies Collection 4. Automatic Template Filling\\n\\nNew dialogue in the target language. Repeating this procedure on each dialogue template, we can easily build a high-quality labeled dataset in the target language. Table 9 in Appendix F shows the statistics of our collected entities in the target languages compared with the English data. The number of our collected entities are either larger than or equal to those in the English data except for the \\\"train\\\" domain; we collected the information about only 100 \\\"trains\\\" for each language due to the complexity in collecting relevant information.\\n\\n2.4 Template Filling for Three Use Cases\\n\\nAfter the above steps, we assign entities in a target language to the translated templates in the same target language for the F&F case, while assigning target-language entities to the English (source-language) templates for the F&E case. As for the E&F case, we keep the original English context by skipping the translation step and replace the placeholders with local entities in the target language (see Figure 2 for examples).\\n\\nTo sum up, our proposed method has three key properties: (1) our method is cost-effective as we only require a limited amount of post-editing efforts for a test set when compared to the expensive crowd-sourced efforts from the other studies; (2) we can easily sample entities from an ontology to create large-scale machine-translated data as a way of data augmentation for training; (3) our method is flexible to update entities in a ToD system whenever an update of ontology is available, e.g., extension of new entities. We refer the readers to Table 10 for the data statistics of GlobalWoZ and Figure 9 for dialogue examples in the appendix.\"}"}
{"id": "acl-2022-long-115", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We prepare a base model for GlobalWoZ in the zero-shot and few-shot cross-lingual transfer settings. We select Transformer-DST (Zeng and Nie, 2020) as our base model as it is one of the state-of-the-art models on both MultiWoZ 2.0 and MultiWoZ 2.1. In our paper, we replace its BERT encoder with an mBERT encoder (Devlin et al., 2019) for our base model and propose a series of training methods for GlobalWoZ. As detailed below, we propose several data augmentation baselines that create different training and validation data for training a base model. Note that all the proposed baselines are model agnostic and the base model can be easily substituted with other popular models (Heck et al., 2020; Lin et al., 2020). For each baseline, we first train a base model on its training data for 20 epochs and use its validation set to select the best model during training. Finally we evaluate the best model of each baseline on the same test set from GlobalWoZ. We will release GlobalWoZ and our pre-trained models to encourage faster adaptation to future research. We refer the readers to Table 11 and Table 12 in Appendix I while reading the subsequent methods for a better understanding.\\n\\n4.1 Pure Zero-Shot (E&E)\\nWe train a base model on the gold standard English data (E&E) and directly apply the learned model to the test data of the three use cases in GlobalWoZ. With this method, we simulate the condition of having labeled data only in the source language for training, and evaluate how the model transfers knowledge from English to the three use cases. We use Zero-Shot (E&E) to denote this method.\\n\\n4.2 Translate-Train\\nWe use our data curation method (\u00a72) to translate the templates by an MT system but replace the placeholders in the translated templates with machine-translated entities to create a set of pseudo-labeled training data. Next, we train a base model on the translated training data without local entities, and evaluate the model on the three use cases. We denote this method as Translate-Train.\\n\\n4.3 Single-Use-Case Training\\nBy skipping the human post-editing step in our data curation method (\u00a72), we leverage a machine translation system to automatically create a large set of pseudo-labeled training data with local entities for the three use cases. In the F&F case, we translate the English templates by the MT system and replace the placeholders in the translated templates with foreign-language entities to create a training dataset. In the F&E case, we replace the placeholders in the translated templates with the original English entities to create a code-switched training dataset. In the E&F case, we use the original English templates and replace the placeholders in the English templates with foreign-language entities to create a code-switch training dataset. With this data augmentation method, we can train a base model on each pseudo-labeled training dataset created for each use case. We denote this method as SUC (Single-Use-Case).\\n\\n4.4 Bi-/Multi-lingual Bi-Use-Case Training\\nWe investigate the performance of combining the existing English data and the pseudo-labeled training data created for one of the three use cases (i.e., F&F, F&E, E&F), one at a time, to do bi-use-case training. In the bilingual training, we only combine the gold English data (E&E) with the pseudo-labeled training data in one target language in one use case for joint training. We denote this method as BBUC (Bilingual Bi-Use-Case). In the multilingual training, we combine gold English data (E&E) and pseudo-labeled training data in all languages in one use case for joint training. We denote this method as MBUC (Multilingual Bi-Use-Case).\\n\\n4.5 Multilingual Multi-Use-Case Training\\nWe also propose to combine the existing English data (E&E) and all the pseudo-labeled training data in all target languages for all the use cases (F&F, F&E, E&F). We then train a single model on this combined multilingual training dataset and evaluate the model on test data in all target languages for all three use cases. We denote this method as MMUC (Multilingual Multi-Use-Case).\\n\\n5 Experiment Results\\nIn this section, we show the results of all methods in the zero-shot (\u00a75.1) and few-shot (\u00a75.2) settings.\"}"}
{"id": "acl-2022-long-115", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.1 Zero-shot Cross-lingual Transfer\\n\\n5.1.1 Use Case F&F, F&E and E&F\\n\\nTable 1 reports the joint goal accuracy of all proposed methods on the three different sets of test data in the F&F, F&E, and E&F use cases. Both Zero-Shot (E&E) and Translate-Train struggle, achieving average accuracy of less than 10 in all use cases. Despite its poor performance, Zero-Shot (E&E) works much better in F&E than F&F, while its results in F&F and E&F are comparable, indicating that a zero-shot model trained in E&E can transfer knowledge about local English entities more effectively than knowledge about English context in downstream use cases. Besides, we also find that Zero-Shot (E&E) performs better on the Spanish or Indonesian context than the Chinese context in F&E. One possible reason is that English is closer to the other Latin-script languages (Spanish and Indonesian) than Chinese.\\n\\nOur proposed data augmentation methods (SUC, BBUC, MBUC) perform much better than non-adapted methods (Zero-Shot (E&E) and Translate-Train) that do not leverage any local entities for training. In particular, it is worth noting that even though Translate-Train and SUC both do training on foreign-language entities in F&F and E&F, there is a huge gap between these two methods, since Translate-Train has only access to the machine-translated entities rather than the real local entities used by SUC. This huge performance gap not only shows that Translate-Train is not an effective method in practical use cases but also proves that having access to local entities is a key to building a multilingual ToD system for practical usage.\\n\\nComparing our data augmentation methods SUC and BBUC, we find that the base model can benefit from training on additional English data (E&E), especially yielding a clear improvement of up to 5.58 average accuracy points in F&E. Moreover, when we increase the number of languages in the bi-use-case data augmentations (i.e., MBUC), we observe an improvement of around 1 average accuracy points in all three use cases w.r.t. BBUC. These observations encourage a potential future direction that explores better data augmentation methods to create high-quality pseudo-training data.\\n\\n5.1.2 One Model for All\\n\\nNotice that we can train a single model by MMUC for all use cases rather than training separate models, one for each use case. In Figure 3, we compare MMUC and MBUC (rows) on the test data in the four use cases (columns). Although MMUC may not achieve the best results in each use case, it achieves the best average result over the four use cases, indicating the potential of using one model to simultaneously handle all the four use cases.\\n\\n5.2 Few-shot Cross-lingual Transfer\\n\\nIn few-shot experiments, we use the same scoring function based on frequency of all 4-gram combinations (\u00a72.2) to select 100 additional dialogues from train set for human-post editing, and create high-quality training data for each of the three use cases. To avoid overfitting on this small few-shot dataset, we combine the few-shot data with the existing English data for training a base model (Few-Shot+Zero-Shot (E&E)). Next, we also investigate a model trained with additional synthetic data created by our proposed SUC. In Figure 4, we find that our proposed SUC without additional few-shot...\"}"}
{"id": "acl-2022-long-115", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Few-shot cross-lingual average joint accuracy on DST over three target languages in three use cases.\\n\\nData has already outperformed the model trained with few-shot data and English data (Few-shot + Zero Shot (E&E)), indicating that the model benefit more from a large amount of pseudo-labeled data than a small set of human-labeled data. If we combine the data created by SUC with the few-shot data or with both few-shot and English data to train the model, we observe improvements over SUC, especially with a clear gain of 8.06 accuracy points in F&E. We refer the readers to Table 14 in the appendix for detailed scores in all target languages.\\n\\n6 Discussion\\n\\n6.1 Motivation for Code-Switched Use Cases\\n\\nOne key research question is to validate whether code-switched use cases with local entities (i.e., F&E, E&F) are practically more useful for information seeking. To answer this question, we compare the failure rate of using local entities and machine-translated entities in information search, which is a proxy to the efficiency of using these two types of entities in conversations. We first randomly select 100 entities (33 attractions, 33 hotels and 34 restaurants) of Cambridge, Shanghai, Barcelona and Jakarta. We translate the English entities into Mandarin, Spanish and Indonesian and the foreign-language entities into English via Google Translate. We then manually search the translated entities on Google to check whether we can find the right information of the original entities. Notice that the failure of the above verification partially come from the translation error made by Google Translate, or the search failure due to the fact that this entity does not have a bilingual version at all. In Table 2, we observe a high failure rate of around 60% for almost all translated directions (except Zh \u2192 En).\\n\\nTable 2: The search and translation results of 100 translated entities on Google. En \u2192 Zh refers to the translation of English entities to Mandarin and Zh \u2192 En refers to the translation of Mandarin entities to English.\\n\\n| Failure Case (MTed Entities) | Failure Rate (MTed Entities) | Failure Rate (Original Entities) |\\n|------------------------------|------------------------------|----------------------------------|\\n|                             |                             |                                 |\\n| Zh \u2192 En                     | 65%                         | 3%                               |\\n| En \u2192 Zh                     | 58%                         | 3%                               |\\n| En \u2192 Es                     | 64%                         | 3%                               |\\n| Es \u2192 En                     | 37%                         | 0%                               |\\n| En \u2192 Id                     | 70%                         | 0%                               |\\n| Id \u2192 En                     | 69%                         | 0%                               |\\n| Avg                          |                             | 3%                               |\\n\\nFigure 5: Joint accuracy of Translate-Train for DST on the F&F Test vs Translate-Test data.\\n\\nDue to translation and search failures, significantly exceeding the low failure rate of searching original entities online. Besides, even if we can find the right information of the translated entities, local people may not recognize or use the translated entities for communication, thus this results in inefficient communication with local people.\\n\\n6.2 Overestimate of Translate-Train\\n\\nIn previous translation-based work, a multilingual ToD system is usually built based on the translation of English training data (Translate-Train), and is evaluated on translated test data without any local entities (Translate-Test). To verify whether this procedure is reliable to build a multilingual ToD system, we also create a test dataset with translated entities instead of local entities in the target languages. As shown in Figure 5, we find the Translate-Train model performs well on the test data with translated entities, but performs badly on the test data with real local entities. To the best of our knowledge, we provide the first analysis to identify this performance gap between the translated test data and data with real local entities in a more realistic use case. Our work sheds light on the development of a globalized multilingual ToD system in practical use cases. We can tackle\\n\\nPlease refer to Appendix L for concrete examples where Translate-Train fails in predicting real local entities.\"}"}
{"id": "acl-2022-long-115", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the challenge of localization issues by exploring\\nnew data augmentation method. Alternatively we\\ncan also explore new methods from the model level\\nby building modular network to update the entities\\nor perform transfer learning to adapt to new case\\nwithout retraining.\\n\\n6.3 Local Context vs. Local Entities\\nWe compare the impact of training a model on data\\nwith either local contexts or local entities when\\nthe model is evaluated on monolingual test data\\nin F&F and E&E. Specifically, when the train set\\nhas access to local context only, all the entities in\\nthe train set are replaced by entities in non-target\\nlanguages. Similarly, when the train set has access\\nto local entities only, the contexts in the train set\\nare replaced by context in the non-target languages.\\n\\nTable 3 shows that both local contexts and local\\nentities are essential to building ToD systems in the\\ntarget language. A further analysis in Table 15 and\\nTable 16 in the appendix shows that training with\\nlocal entities is more important if the entities and\\ncontexts are written in the same type of language\\n(e.g. Latin script).\\n\\n| Train Set           | E&E (en) | F&F (zh) | F&F (es) | F&F (id) | avg  |\\n|---------------------|---------|---------|---------|---------|------|\\n| Local Context Only  | 5.46    | 1.77    | 2.37    | 2.40    | 3.20 |\\n| Local Entities Only | 6.39    | 0.36    | 2.41    | 2.75    | 3.05 |\\n| Local Context & Entities | 52.78 | 36.97   | 24.66   | 25.26   | 38.13 |\\n\\nTable 3: Comparison of training with local context\\nor/and local entities on the joint accuracy for\\nDST in E&E (en) and F&F (zh, es, id).\\n\\n6.4 Scaling up to 20 Languages\\nWith our proposed data curation method, it is pos-\\nsible to extend the dataset to cover more languages\\nwithout spending extra costs if we skip the human\\npost-editing step. Before doing so, one key ques-\\ntion is whether the evaluation on the translated data\\nwithout human post-editing is reliable as a proxy\\nof the model performance. Thus, we conduct the\\nexperiments by evaluating the model performance\\nof all baselines (\u00a74) on two sets of test data built\\nwith local entities: (1) MT test data where trans-\\nlated template is created by machine translation\\nonly (\u00a72.2); (2) MTPE test data where translated\\ntemplate is first translated by machines and post-\\nedited later by professional translators. As shown\\nin Table 4, the overall reported results on MT test\\ndata are higher than those reported on MTPE test\\ndata, which is expected because the distribution of\\nthe MT test data is more similar to the MT training\\ndata. Although there are some differences on indi-\\nvidual languages, the conclusions derived from the\\nevaluations on the MT test data remain the same\\nas those derived from the evaluation on the MTPE\\ntest data. We also calculate the Spearman rank\\ncorrelation coefficient between the average results\\nreported on MTPE test data and MT test data in Ta-\\nble 4, which shows a statistically high correlation\\nbetween the system performance on the MT test\\ndata and MTPE test data. Therefore, we show\\nthat the MT test data can be used as a proxy to esti-\\nmate the model performance on the real test data\\nfor more languages. Thus we build MT test data for\\nanother 17 languages that are supported by Google\\nTranslate, Trip Advisor and Booking.com at the\\nsame time, as stated in Table 8 and Table 9 in the\\nappendix. Table 5 shows the results of Zero-Shot\\n(E&E) and SUC on the test data of F&F, F&E and\\nE&F in 20 languages. The results show that the\\nmodel has the best performance in the F&E use\\ncase compared with the other two use cases, which\\nis consistent with our findings in Table 1.\\n\\nTable 17 in the appendix shows detailed scores.\"}"}
{"id": "acl-2022-long-115", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"scale annotation corpora. These corpora cover a wide range of topics from a single domain (e.g., ATIS (Hemphill et al., 1990), DSTC 2 (Henderson et al., 2014), Frames (El Asri et al., 2017), KVRET (Eric et al., 2017), WoZ 2.0 (Wen et al., 2017), M2M (Schatzmann et al., 2007)) to multiple domains (e.g., MultiWoZ (Budzianowski et al., 2018), SGD (Rastogi et al., 2020)). Most notably among these collections, MultiWoZ is a large-scale multi-domain dataset that focuses on transitions between different domains or scenarios in real conversations (Budzianowski et al., 2018). Due to the high cost of collecting task-oriented dialogues, only a few monolingual or bilingual non-English ToD datasets are available (Zhu et al., 2020; Quan et al., 2020; Lin et al., 2021). While there is an increasing interest in data curation for multilingual ToD systems, a vast majority of existing multilingual ToD datasets do not consider the real use cases when using a ToD system to search for local entities in a country.\\n\\nWe fill this gap in this paper to provide the first analysis on three previously unexplored use cases.\\n\\n8 Conclusions\\n\\nIn this paper, we provide an analysis on three unexplored use cases for multilingual task-oriented dialogue systems. We propose a new data curation method that leverages a machine translation system and local entities in target languages to create a new multilingual TOD dataset, GlobalWoZ. We propose a series of strong baseline methods and conduct extensive experiments on GlobalWoZ to encourage research for multilingual ToD systems. Besides, we extend the coverage of languages on multilingual ToD to 20 languages, marking the one step further towards building a globalized multilingual ToD system for all of the world's citizen.\\n\\n9 Ethical Review\\n\\nIn this section, we would like to address the ethical concerns. All the professional translators in this project have been properly compensated. For Chinese and Spanish, we have followed the standard procurement requirements and engaged three translation companies for quality and price comparison. A small sample of the data had been given to them for MTPE and we then compared their translation results. Following that, we selected the company that produced the best sample translation, and submitted the full translation orders according to the agreed price quotations. For Indonesian, three translation companies were also requested to provide sample MTPE, but our quality check found the quality of these samples to be unsatisfactory. So, no company was engaged, and our in-house Indonesian linguistic resources were used instead. These Indonesian linguists were assigned to work on this project during normal working hours and given proper compensation complying with the local labor laws.\\n\\nAcknowledgements\\n\\nThis research is partly supported by the Alibaba-NTU Singapore Joint Research Institute, Nanyang Technological University. All the costs for machine translation post-editing are funded by DAMO Academy, Alibaba Group. We would like to thank the help from our Alibaba colleagues, Haiyun Peng, Zifan Xu and Ruidan He, and our NTU-NLP team member, Chengwei Qin in this work as well.\\n\\nReferences\\n\\nDan Bohus and Alexander I. Rudnicky. 2009. The ravenclaw dialog management framework: Architecture and systems. Computer Speech & Language, 23:332\u2013361.\\n\\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ra\u2019madan, and Milica Gasi\u0107. 2018. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5016\u20135026, Brussels, Belgium. Association for Computational Linguistics.\\n\\nLi-Rong Cheng and Katharine Butler. 1989. Code-switching: a natural phenomenon vs language \u2018deficiency\u2019. World Englishes, 8(3):293\u2013309.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 8440\u20138451, Online. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), Volume 1 (Long and Short\")}"}
{"id": "acl-2022-long-115", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Layla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, and Kaheer Suleman. 2017. Frames: a corpus for adding memory to goal-oriented dialogue systems. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pages 207\u2013219, Saarbr\u00fccken, Germany. Association for Computational Linguistics.\\n\\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, and Dilek Hakkani-T\u00fcr. 2020. Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. In Proceedings of The 12th Language Resources and Evaluation Conference (LREC), pages 422\u2013428, Marseille, France. European Language Resources Association.\\n\\nMihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. 2017. Key-value retrieval networks for task-oriented dialogue. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL), pages 37\u201349, Saarbr\u00fccken, Germany. Association for Computational Linguistics.\\n\\nNarendra Kumar Gupta, G\u00f6khan T\u00fcr, Dilek Z. Hakkani-T\u00fcr, Srinivas Bangalore, Giuseppe Riccardi, and Mazin Gilbert. 2006. The at&t spoken language understanding system. IEEE Transactions on Audio, Speech, and Language Processing, 14:213\u2013222.\\n\\nMichael Heck, Carel van Niekerk, Nurul Lubis, Christian Geishauser, Hsien-Chin Lin, Marco Moresi, and Milica Gasic. 2020. TripPy: A triple copy strategy for value independent neural dialog state tracking. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 35\u201344, 1st virtual meeting. Association for Computational Linguistics.\\n\\nCharles T Hemphill, John J Godfrey, and George R Doddington. 1990. The atis spoken language systems pilot corpus. In Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, Pennsylvania, June 24\u201327, 1990.\\n\\nMatthew Henderson, Blaise Thomson, and Jason D Williams. 2014. The second dialog state tracking challenge. In Proceedings of the 15th annual meeting of the special interest group on discourse and dialogue (SIGDIAL), pages 263\u2013272.\\n\\nBernd Kiefer, Anna Welker, and Christophe Biwer. 2021. V onda: A framework for ontology-based dialogue management. In Increasing Naturalness and Flexibility in Spoken Dialogue Interaction, pages 93\u2013105. Springer.\\n\\nEunhee Kim. 2006. Reasons and motivations for code-mixing and code-switching. Issues in EFL, 4(1):43\u201361.\\n\\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume (EACL), pages 2950\u20132962, Online. Association for Computational Linguistics.\\n\\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata, and Pascale Fung. 2020. MinTL: Minimalist transfer learning for task-oriented dialogue systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3391\u20133405, Online. Association for Computational Linguistics.\\n\\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata, Peng Xu, Feijun Jiang, Yuxiang Hu, Chen Shi, and Pascale Fung. 2021. BiToD: A bilingual multi-domain dataset for task-oriented dialogue modeling. arXiv preprint arXiv:2106.02787.\\n\\nLinlin Liu, Bosheng Ding, Lidong Bing, Shafiq Joty, Luo Si, and Chunyan Miao. 2021. MulDA: A multilingual data augmentation framework for low-resource cross-lingual NER. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP), Volume 1: Long Papers, pages 5834\u20135846, Online. Association for Computational Linguistics.\\n\\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Translations of the Association for Computational Linguistics (TACL), 8:726\u2013742.\\n\\nJun Quan, Shian Zhang, Qian Cao, Zizhong Li, and Deyi Xiong. 2020. RiSAWOZ: A large-scale multi-domain Wizard-of-Oz dataset with rich semantic annotations for task-oriented dialogue modeling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 930\u2013940, Online. Association for Computational Linguistics.\\n\\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 34, pages 8689\u20138696.\\n\\nEvgeniia Razumovskaia, Goran Glava\u0161, Olga Majewska, Anna Korhonen, and Ivan Vu\u010di\u0107. 2021. Crossing the conversational chasm: A primer on multilingual task-oriented dialogue systems. arXiv preprint arXiv:2104.08570.\"}"}
{"id": "acl-2022-long-115", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-115", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA Comparison of Four Use Cases\\n\\n| Use Case Source ToD | Speaker Country |\\n|---------------------|-----------------|\\n| F&F                 | Foreign Lang. Foreign Lang. |\\n| F&E                 | Foreign Lang. English |\\n| E&F                 | English Foreign Lang. |\\n| E&E                 | English English |\\n\\nTable 6: Four use cases of multilingual ToD systems: A foreign language or English speaker travels to a country of a foreign language or English.\\n\\nC BLEU Score of MT versus MTPE Test Template\\n\\n| Languages | Zh | Es | Id | Avg |\\n|-----------|----|----|----|-----|\\n| BLEU Score| 55.61 | 49.33 | 48.97 | 51.30 |\\n\\nTable 7: BLEU Scores of MT Test Template using MTPE Test Template as reference.\"}"}
{"id": "acl-2022-long-115", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Gold English Test Set Distribution by Domains. We follow this distribution to select the top 500 high-scoring dialogues in the test set for post-editing.\\n\\nTable 8: Statistics about languages in the cross-lingual benchmark. The selected 21 languages (including English) belong to 8 language families and 1 isolate, with Indo-European (IE) having the most members. We categorize the languages with more than 1 million, more than 400 thousand but less than 1 million, less than 400 thousand Wikipedia articles as high resource languages, middle resource languages and low resource languages. For each language, we select one city for each language to collect localized ontology.\"}"}
{"id": "acl-2022-long-115", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### F Statistics of Entities in the Collected Ontology\\n\\n| Language | rest | hotel | attr | train | taxi |\\n|----------|------|-------|------|-------|------|\\n| en       | 110  | 33    | 79   | 2828  | 222  |\\n| zh       | 3000 | 496   | 1000 | 100   | 4496 |\\n| es       | 3000 | 426   | 1000 | 100   | 4426 |\\n| id       | 3000 | 999   | 792  | 100   | 4791 |\\n| ar       | 2989 | 680   | 1000 | 100   | 4669 |\\n| da       | 2343 | 165   | 1000 | 100   | 3508 |\\n| de       | 2988 | 659   | 1000 | 100   | 4647 |\\n| el       | 2600 | 1000  | 1000 | 100   | 4600 |\\n| fr       | 3000 | 1000  | 1000 | 100   | 5000 |\\n| he       | 1558 | 258   | 1000 | 100   | 2258 |\\n| it       | 3000 | 800   | 1000 | 100   | 2800 |\\n| ja       | 2967 | 864   | 1000 | 100   | 4831 |\\n| ko       | 2990 | 532   | 1000 | 100   | 4522 |\\n| nl       | 2990 | 537   | 1000 | 100   | 4527 |\\n| no       | 1293 | 95    | 757  | 100   | 2145 |\\n| pt       | 2993 | 951   | 1000 | 100   | 4944 |\\n| ru       | 2985 | 531   | 1000 | 100   | 4516 |\\n| sv       | 3000 | 214   | 891  | 100   | 4105 |\\n| th       | 2995 | 1000  | 1000 | 100   | 4995 |\\n| tr       | 2986 | 533   | 1000 | 100   | 4519 |\\n| vi       | 2991 | 773   | 1000 | 100   | 4764 |\\n\\nTable 9: Statistics of entities in the collected ontology in different languages. We count the number of entities in the database of each domain. Noticed that in the Taxi database of MultiWoZ, it only list down the taxi colors, taxi types and taxi phones. The taxi destination and departure refer to the entities in the restaurant, hotel and attraction domains. Thus, we use the sum of the number of entities in Restaurant, Hotel and Attraction domains as a proxy of the total number of entities in taxi domain. Besides, we follow MultiWoZ to collect one hospital and one police station for each city.\\n\\n### G Statistics of GlobalWoZ\\n\\n| Use Case | F&F | F&E | E&F |\\n|----------|-----|-----|-----|\\n| Languages | Train & Dev Method | Test Method |\\n| zh       | 9438 MT 1000 | MTPE 9438 |\\n| es       | 9438 MT 1000 | MTPE 9438 |\\n| id       | 9438 MT 1000 | MTPE 9438 |\\n| ar       | 9438 MT 1000 | MTPE 9438 |\\n| da       | 9438 MT 1000 | MTPE 9438 |\\n| de       | 9438 MT 1000 | MTPE 9438 |\\n| el       | 9438 MT 1000 | MTPE 9438 |\\n| fr       | 9438 MT 1000 | MTPE 9438 |\\n| he       | 9438 MT 1000 | MTPE 9438 |\\n| it       | 9438 MT 1000 | MTPE 9438 |\\n| ja       | 9438 MT 1000 | MTPE 9438 |\\n| ko       | 9438 MT 1000 | MTPE 9438 |\\n| nl       | 9438 MT 1000 | MTPE 9438 |\\n| no       | 9438 MT 1000 | MTPE 9438 |\\n| pt       | 9438 MT 1000 | MTPE 9438 |\\n| ru       | 9438 MT 1000 | MTPE 9438 |\\n| sv       | 9438 MT 1000 | MTPE 9438 |\\n| th       | 9438 MT 1000 | MTPE 9438 |\\n| tr       | 9438 MT 1000 | MTPE 9438 |\\n| vi       | 9438 MT 1000 | MTPE 9438 |\\n\\nTable 10: Statistics of created dataset, GlobalWoZ for each use case in each target language. For E&F, as the context is the original English data, we consider it is created by human. For test data of zh, es and id, we replace the entities twice to bootstrap the test data to 1000 dialogues. We are currently preparing the post editing of the other 500 dialogues in test data. Meanwhile, we are leveraging machine translation to prepare the train data for the 17 languages and will release it with baselines in the next version soon.\"}"}
{"id": "acl-2022-long-115", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hi can you help me find a very nice Asian Cuisine restaurant near the center of Shanghai?\\n\\nPlease specify your price range.\\n\\nIt's cheap.\\n\\nShou'er Menxianhui supplies Asian Cuisine in the center.\\n\\nCan I have the address, phone number, and postcode?\\n\\nCertainly. The address is 396 Guilin Road, and the postal code is 200000. Their phone number is +86 21 5497 6668. Is there anything else?\\n\\nHello, can you help me find a very delicious Asian Cuisine restaurant near the center of Shanghai?\\n\\nPlease tell me about your preferred price range.\\n\\nAs long as it's cheap.\\n\\nShou'er Menxianhui serves Asian Cuisine in the center.\\n\\nWould it be convenient for you to tell me its address, phone number and postcode?\\n\\nCertainly. The address is 396 Guilin Road, and the postal code is 200000. Their phone number is +86 21 5497 6668. Is there anything else that can help you?\\n\\nHi can you help me find a very nice Italian restaurant near the centre of Cambridge?\\n\\nPlease specify your price range.\\n\\nIt doesn't matter.\\n\\nZizzi Cambridge serves Italian in the centre.\\n\\nCan I have the address, phone number, and postcode?\\n\\nCertainly. The address is 47-53 regent street with the postcode of cb21ab. Their phone number is 01223365599. is there anything else?\"}"}
{"id": "acl-2022-long-115", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I Summary of Proposed Baselines\\n\\n| Methods      | En Context | En Entities | Local Context | Local Entities | Translated Entities |\\n|--------------|------------|-------------|---------------|----------------|---------------------|\\n| Zero-Shot (E&E) |           |             |               |                |                     |\\n| Translate-Train |           |             |               |                |                     |\\n| SUC (F&F)    |           |             |               |                |                     |\\n| SUC (F&E)    |           |             |               |                |                     |\\n| SUC (E&F)    |           |             |               |                |                     |\\n\\nTable 11: Accessibility of different types of context and entities for each method.\\n\\n| Methods      | E&E | F&F | F&E | E&F |\\n|--------------|-----|-----|-----|-----|\\n| Zero-Shot (E&E) |     |     |     |     |\\n| Translate-Train |     |     |     |     |\\n| SUC (F&F)    |     |     |     |     |\\n| SUC (F&E)    |     |     |     |     |\\n| SUC (E&F)    |     |     |     |     |\\n| BBUC (E&E + F&F) |     |     |     |     |\\n| BBUC (E&E + F&E) |     |     |     |     |\\n| BBUC (E&E + E&F) |     |     |     |     |\\n| MBUC (E&E + F&F) |     |     |     |     |\\n| MBUC (E&E + F&E) |     |     |     |     |\\n| MBUC (E&E + E&F) |     |     |     |     |\\n| MMUC (E&E + F&F + F&E + E&F) |   |   |   |   |\\n\\nTable 12: Accessibility of data in each use case for each method. Noticed that Translate-Train doesn\u2019t have access to the data of the four use cases. Translate-Train has access to a set of pseudo-labeled training data created by replacing the placeholders in the translated template with machine-translated entities instead of local entities.\\n\\nWe also compare the performance of all methods on the original E&E test data. As Zero-Shot (E&E) is trained on monolingual English training data, it gets a high accuracy of 52.78 on the English test data. In contrast, Translate-Train and SUC (F&F) perform poorly on the English test data, because both of them have no access to any English data. Comparing to SUC (F&F), SUC (F&E) and SUC (E&F) achieve higher accuracy scores as they either have access to English context or English entities. When we perform bilingual and multilingual joint training (i.e., BBUC and MBUC), the base model has a performance increase except MBUC (E&E + E&F). This shows that bilingual and multilingual joint training may be used to improve the performance on source language. Further research can be done in this line.\"}"}
{"id": "acl-2022-long-115", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 13: Joint accuracy on DST in three target languages on the English test data.\\n\\n| Use Case | Zh | Es | Id | Avg |\\n|----------|----|----|----|-----|\\n| F2F      | 1.22 | 1.38 | 1.26 | 1.28 |\\n| F2E      | 6.92 | 11.34 | 9.09 | 9.12 |\\n| E2F      | 1.69 | 1.81 | 1.82 | 1.77 |\\n\\n### Table 14: A breakdown of few-shot cross-lingual average joint accuracy on DST over three target languages in three use cases.\\n\\n| Use Case | Zh | Es | Id | Avg |\\n|----------|----|----|----|-----|\\n| F2F      | 15.93 | 7.13 | 12.09 | 11.72 |\\n| F2E      | 39.88 | 39.38 | 43.26 | 40.84 |\\n| E2F      | 20.61 | 14.17 | 18.55 | 17.78 |\\n\\n| Use Case | Zh | Es | Id | Avg |\\n|----------|----|----|----|-----|\\n| F2F      | 36.97 | 24.66 | 25.26 | 28.96 |\\n| F2E      | 56.28 | 41.94 | 47.93 | 48.71 |\\n| E2F      | 38.56 | 28.00 | 43.82 | 36.79 |\\n\\n| Use Case | Zh | Es | Id | Avg |\\n|----------|----|----|----|-----|\\n| F2F      | 37.81 | 25.15 | 39.51 | 34.16 |\\n| F2E      | 58.39 | 53.03 | 54.02 | 55.15 |\\n| E2F      | 38.75 | 27.66 | 44.23 | 36.88 |\\n\\n| Use Case | Zh | Es | Id | Avg |\\n|----------|----|----|----|-----|\\n| F2F      | 37.52 | 26.44 | 40.15 | 34.70 |\\n| F2E      | 59.21 | 54.93 | 56.17 | 56.77 |\\n| E2F      | 39.51 | 27.84 | 45.48 | 37.61 |\"}"}
{"id": "acl-2022-long-115", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Concrete Examples where Translate-Train Performs Badly on the Test Data with Real Local Entities.\\n\\nThrough investigation, we found that the Translate-Train method usually performed badly in two main scenarios. Figure 9 is the illustrations of the two scenarios. Scenario 1 is when the Translate-Train can predict values that are close to the meaning of the ground truth values but suffer from the problems of translationese. For example, model trained with Translate-Train may predict \\\"\u7f8e\u98df\u9152\u5427\\\" (gastropub), which is a direct translation of gastropub and not commonly used in Chinese instead of \\\"\u9152\u5427\u9910\\\" (bar).\\n\\nScenario 2 is when Translate-Train needs to predict the name of real localized entities which Translate-Train doesn't have access to. For example, trained with Translate-Train may predict \\\"\u5188\u7ef4\u5c14\u9152\u5e97\\\" (Gonville Hotel) which is a direct translation of Gonville Hotel, instead of \\\"\u6c49\u5ead\u9152\u5e97\\\" (Hanting Hotel) which is unseen in Translate-Train training data.\\n\\n**Prediction:**\\n- **restaurant-area:** \u4e2d\u5fc3 (center)\\n- **restaurant-food:** \u7f8e\u98df\u9152\u5427 (gastropub)\\n- **restaurant-pricerange:** \u7f13\u548c (mild)\\n\\n**Ground Truth:**\\n- **restaurant-area:** \u5e02\u4e2d\u5fc3 (city center)\\n- **restaurant-food:** \u9152\u5427\u9910 (bar)\\n- **restaurant-pricerange:** \u9002\u4e2d\u7684 (moderate)\\n\\n**Prediction:**\\n- **hotel-name:** \u5188\u7ef4\u5c14\u9152\u5e97 (Gonville Hotel)\\n\\n**Ground Truth:**\\n- **hotel-name:** \u6c49\u5ead\u9152\u5e97 (Hanting Hotel)\\n\\n---\\n\\n**Breakdown of the Results of Local Context vs Local Entities by Languages**\\n\\n| Language | Context vs Entities | Zh | Es | Id | Avg |\\n|----------|---------------------|----|----|----|-----|\\n| **En**   | Context             | 5.37 | 5.33 | 5.67 | 5.46 |\\n|          | Entities            | 3.49 | 7.78 | 7.90 | 6.39 |\\n| **Zh**   | Context             | 1.74 | 1.77 | 1.80 | 1.77 |\\n|          | Entities            | 0.27 | 0.73 | 0.10 | 0.36 |\\n| **Es**   | Context             | 1.73 | 2.01 | 3.37 | 2.37 |\\n|          | Entities            | 3.92 | 0.44 | 2.86 | 2.41 |\\n| **Id**   | Context             | 2.07 | 2.18 | 2.94 | 2.40 |\\n|          | Entities            | 3.92 | 0.84 | 3.48 | 2.75 |\\n\\n**Table 15:** A breakdown of comparison of the impact of local context and local entities on joint accuracy for DST in each language. The cases where context and entities are in different script types are highlighted in lavender color.\"}"}
{"id": "acl-2022-long-115", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: Comparison of the impact of script type on Local Context Only vs Local Entities Only. It shows that training with local entities is more important if the entities and contexts are written in the same type of language script (e.g. Latin script), otherwise training with local contexts is more important.\\n\\nTable 17: Spearman rank correlation coefficient between the results on MTPE test data and MT test data for each language.\\n\\nTable 18: Results of Zero-Shot (E&E) on test data of F&F, F&E and E&F in 20 languages. Test data of F&F and F&E in the three languages highlight in pink color are built with MTPE data and the rest are built with MT data.\\n\\nWe observe that there are a few languages like Thai and Vietnamese have low results than other languages. Through investigation, we found that it was caused by failing to predict the tone marks in most of cases. For example, the model may predict \\\"nha khach\\\" for hotel type while \\\" nh\u00e0 kh\u00e1ch\\\" is the ground truth. We may explore options for post-processing or other models to improve the performance on these languages upon the release of the data.\"}"}
