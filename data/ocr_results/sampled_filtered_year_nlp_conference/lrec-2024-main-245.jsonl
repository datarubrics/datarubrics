{"id": "lrec-2024-main-245", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 15: Distribution of Labels (Prose).\\n\\nFrequency distributions of the labels in gold genre labels, annotations of genres, annotations of topic-10, and annotations of topic-100 (log scale). For the annotations, the number is divided by three to get an average distribution. The mapping of topic-10 and topic-100 labels can be found in Appendix F. The tag \u201cNo\u201d in the topic annotations means \u201cNo topic.\u201d\\n\\nGuidelines\\n\\nGoal/Task\\n\\nIn this annotation project, we are interested in knowing what the topic and genre is of a sentence and whether we humans can identify these. For Topics, we make use of the Dewey Decimal Classification (DDC) system. For genres, we make use of the genres provided in the Georgetown University Multilayer Corpus (GUM) corpus. The goal is to put the sentence/paragraph at hand into the most probable class (determined by you).\\n\\nGenre has a one-layer annotation scheme, while Topic has a two-layer annotation scheme, which we will refer to as L1 and L2. We want to annotate for all three. There is an option for \u201cNot Sure\u201d (abbreviated to \u201cNS\u201d). This is when you feel that the label for the sentence is not present in the options. In addition, feel free to add any notes for clarification (e.g., clarify your choice or something else).\\n\\nPreliminaries\\n\\nBelow we give an introduction to the topics and genre labels of this annotation project. It takes around 15-20 minutes to read. Note that you don\u2019t have to remember the label numbers. This introduction is to make you aware of the definition of the classes. All the labels are present in the annotation spreadsheet.\\n\\nIntroduction Genres\\n\\nWe make use of the text types (genres) in the GUM corpus. These genres do not have a specific number like the topics above. Therefore we simply enumerate them. The genres are the following:\\n\\n\u2022 Academic\\n\u2022 Bio\\n\u2022 Conversation\\n\u2022 Fiction\\n\u2022 Interview\\n\u2022 News\\n\u2022 Speech\\n\u2022 Textbook\\n\u2022 Vlog\\n\u2022 Voyage\\n\u2022 Whow\\n\\nBrief explanation of the genre classes\\n\\n\u2022 Academic (writing) is nonfiction writing adhering to academic standards and disciplines. It includes research reports, monographs, and undergraduate versions. It uses a formal style, references other academic work, and employs consistent rhetorical techniques to define scope, situate in research, and make new contributions.\"}"}
{"id": "lrec-2024-main-245", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A biography is a detailed description of a person's life. It involves more than just basic facts like education, work, relationships, and death; it portrays a person's experience of these life events. Unlike a profile or curriculum vitae (r\u00e9sum\u00e9), a biography presents a subject's life story, highlighting various aspects of their life, including intimate details of experience, and may include an analysis of the subject's personality. Biographical works are usually non-fiction, but fiction can also be used to portray a person's life. One in-depth form of biographical coverage is called legacy writing. Works in diverse media, from literature to film, form the genre known as biography. An authorized biography is written with the permission, cooperation, and at times, participation of a subject or a subject's heirs. An autobiography is written by the person themselves, sometimes with the assistance of a collaborator or ghostwriter.\\n\\nConversation: naturally occurring spoken interaction. Represents a wide variety of people of different regional origins, ages, occupations, genders, and ethnic and social backgrounds. The predominant form of language use represented is face-to-face conversation, but also documents many other ways that people use language in their everyday lives: telephone conversations, card games, food preparation, on-the-job talk, classroom lectures, sermons, story-telling, town hall meetings, tour-guide spiels, and more. Fiction refers to creative works, particularly narrative works, that depict imaginary individuals, events, or places. These portrayals deviate from history, fact, or plausibility. In our data, fiction pertains to written narratives like novels, novellas, and short stories.\\n\\nAn interview is a structured conversation where one person asks questions and another person answers them. It can be a one-on-one conversation between an interviewer and an interviewee. The information shared during the interview can be used or shared with others.\\n\\nNews is information about current events, shared through various media like word of mouth, printing, broadcasting, electronic communication, and witness testimonies. It covers topics such as war, government, politics, education, health, environment, economy, business, fashion, entertainment, sports, and unusual events. Government announcements and technological advancements have accelerated news dissemination and influenced its content.\\n\\nA (political) speech is a public address given by a political figure or a candidate for public office, usually with the aim of persuading or mobilizing an audience to support their ideas, policies, or campaigns. Political speeches are an essential tool for politicians to communicate their vision, articulate their positions, and connect with voters or constituents.\\n\\nA textbook is a book containing a comprehensive compilation of content in a branch of study with the intention of explaining it. Textbooks are produced to meet the needs of educators, usually at educational institutions. Schoolbooks are textbooks and other books used in schools. Today, many textbooks are published in both print and digital formats.\\n\\nA vlog, also known as a video blog or video log, is a form of blog for which the medium is video. The dataset contains transcripts of the speech occurring in the video.\\n\\nA travel/voyage guide is a wiki providing information for visitors or tourists about a particular place. It typically includes details about attractions, lodging, dining, transportation, and activities. It may also contain maps, historical facts, and cultural insights. Guide wikis cater to various travel preferences, such as adventure, relaxation, budget, or specific interests like LGBTQ+ travel or dietary needs.\\n\\nA Wikihow how-to (whow) guide is an instructional document that offers step-by-step guidance on accomplishing a specific task or reaching a particular goal. It aims to assist individuals in learning and comprehending the process involved in successfully completing the task. These guides are typically written in a clear and concise manner, simplifying complex processes into manageable steps. They often include detailed explanations, diagrams, illustrations, or examples to enhance understanding. How-to guides cover various topics, such as technical tasks, practical skills, creative endeavors, troubleshooting, and more.\\n\\nIntroduction\\n\\nTopics\\n\\nThe DDC system is a widely used library classification system developed by Melvil Dewey in the late 19th century. The DDC is based on the principle of dividing knowledge (in our case sentences) into ten main classes, each identified by a three-digit number; we only focus on the first two:\\n\\n1. The ten main classes in the Dewey Decimal Classification system are as follows:\"}"}
{"id": "lrec-2024-main-245", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"These higher level classes belong to L1 in the annotation spreadsheet, and we added the NO-TOPIC label (see description below).\\n\\n2. Each main class is further divided into subclasses using additional digits (10s). For example, in the 500s (natural sciences and mathematics), you'll find 510 for mathematics, 520 for astronomy, 530 for physics, and so on. The system allows for more specific classification of books and materials based on their subject matter.\\n\\nSee the following page: https://www.oclc.org/content/dam/oclc/dewey/ddc23-summaries.pdf\\n\\nThis page separates the ten classes above into more finer-grained classes. There is not an explanation for each of them, but usually the name of the label encapsulates the subclass already. Note that the subclasses overwrite the main classes (so you can't pick 400 and 510, then you'd have to change 510 to 500).\\n\\nThese subclasses belong to L2 in the annotation spreadsheet. Note that for each fine-grained class we deem the main number/code (e.g., 100, 200, 300) in L2 as the No-topic/Other category. The \u201cOther\u201d class can only be chosen in the fine-grained label classes (L2). Choosing this means that you believe that the current sentence belongs to a specific class. But the label is not present.\\n\\nThe Dewey Decimal Classification system is used in many libraries around the world to organize their collections and make it easier for users to locate resources. It provides a systematic way of arranging materials and enables efficient browsing and retrieval of information based on subject areas.\\n\\nBrief explanation of the topic classes (L1)\\n\u2022 000 Computer science, information & general works is the most general class and is used for works not limited to any one specific discipline, e.g., encyclopedias, newspapers, general periodicals. This class is also used for certain specialized disciplines that deal with knowledge and information, e.g., computer science, library and information science, journalism. Each of the other main classes (100-900) comprises a major discipline or group of related disciplines. Note that in our experiments, we do not consider this a miscellaneous category, we have \u201cNo-topic\u201d for this.\\n\u2022 100 Philosophy & psychology covers philosophy, parapsychology and occultism, and psychology.\\n\u2022 200 Religion is devoted to religion.\\n\u2022 300 Social sciences covers the social sciences. Class 300 includes sociology, anthropology, statistics, political science, economics, law, public administration, social problems and services, education, commerce, communications, transportation, and customs.\\n\u2022 400 Language comprises language, linguistics, and specific languages. Literature, which is arranged by language, is found in 800.\\n\u2022 500 Science is devoted to the natural sciences and mathematics.\\n\u2022 600 Technology is technology.\\n\u2022 700 Arts & recreation covers the arts: art in general, fine and decorative arts, music, and the performing arts. Recreation, including sports and games, is also classed in 700.\\n\u2022 800 Literature covers literature, and includes rhetoric, prose, poetry, drama, etc. Folk literature is classed with customs in 300.\\n\u2022 900 History & geography is devoted primarily to history and geography. A history of a specific subject is classed with the subject.\\n\u2022 No topic: For cases where the topic can not be determined, or even guessed. For example for utterances that contain no natural language or do not have enough context.\"}"}
{"id": "lrec-2024-main-245", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yes, apart from that the colours should match, the first number of the class to which the sentence belongs should also match. For example, a sentence that belongs to Arts (700), is restricted to anything in the 700 class, e.g., a painting (750).\\n\\n- If a sentence has a clear topic in general, but the L2 category does not match, how do we annotate? The fine-grained (L2) topics have the priority, and since they have to match you adjust the main topic accordingly.\\n\\n- Does my choice of Topic depend on the Genre or vice versa? No, by default, annotating for genre and topic should be a separate task and should not influence each other.\\n\\n- How do we distinguish between something that is in the No-topic (or Others) class and NS (\u201cnot sure\u201d)? Use the \u201cothers\u201d category when you believe the current instance to belong to a class which is not in the listed ones. Mark your choice with \u201cNS\u201d when you have a guess, but you are not confident about it (e.g., because the instance is very short, or you are not familiar with the genre/topic).\\n\\n- If you are able to find L1, but none of the labels fit for the sentence in L2, you should choose \u201cOther\u201d (e.g., 000, 100, 200, etc.) in the same colour (class) of L2. The \u201cOther\u201d class can only be chosen in the fine-grained label classes (L2). Choosing this means that you believe that the current sentence belongs to a specific class. But the label is not present. Otherwise, mark your best guess with \u201cNS.\u201d\\n\\n- Is it better to label a sentence as \u201cNO-TOPIC\u201d if there is not a clear label associated with it or are we encouraged to take a guess? You are encouraged to take a guess. However, for cases where you have no preference for any of the labels (i.e., a wild guess), label it as NO-TOPIC.\\n\\n- There is already another \u201cOther\u201d class in Religion/Language (e.g., 290 Other religion). Good catch, imagine this situation. Let\u2019s say the sentence is talking about Buddhism. This falls under 290, because we\u2019re talking about another religion. However, if the sentence is \u201cvaguely\u201d talking about religion and doesn\u2019t fit within any of the labels, then choose 200 (Other).\\n\\n- Where do ads/exam questions fit? In whichever of the genres you would expect to come across advertisements/exam questions. However, note that the data is scraped from the main information channel of source (i.e., advertisements next to a news text or before a vlog are not included).\\n\\n- Can we use external resources? External resources are allowed, but do not look up the literal sentence.\\n\\n- How to pick topics (L1/L2) for fiction (genre)? Note that the genre and topic tasks should be seen as distinct tasks. So, the genre fiction should not automatically lead to a literature topic label (unless the fiction work is about literature).\\n\\n- Some utterances seem to be taken from the same text; do we have to give them the same label, or take the contexts into account? No, each utterance should be judged independently.\\n\\nNote for L3:\\n- For each L2, there is a finer-grained class namely L3. These numbers go in the thousands. Now, try to pick the most likely thousands\u2019 topic:\\n  - You will have to refer to the PDF (L3-1000.pdf) for the right classes.\\n  - Please write the class number in the spreadsheet cell. There is no dropdown menu.\\n\\n- The \u201cno-topic\u201d option still exists. Use \u201cNT\u201d;\\n- You should pick the fine-grained L3 topic that best fits the utterance. This time you don\u2019t have to match the L1-L2 categories, but we ask you to NOT update your previous L1-L2 annotations, and just annotate L3 independently.\\n\\nG. Annotation Tool\\nWe used Google Spreadheets for annotation. The setup is shown in Figure 16.\"}"}
{"id": "lrec-2024-main-245", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: Example of annotation in Google Spreadsheets. NS = Not Sure\"}"}
{"id": "lrec-2024-main-245", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Can Humans Identify Domains?\\n\\nMaria Barrett\\nMax M\u00fcller-Eberstein\\nElisa Bassignana\\nAmalie Brogaard Pauli\\nMike Zhang\\nRob van der Goot\\n\\nIT University of Copenhagen,\\nAarhus University,\\nAalborg University,\\nPioneer Centre for AI\\n\\nAbstract\\n\\nTextual domain is a crucial property within the Natural Language Processing (NLP) community due to its effects on downstream model performance. The concept itself is, however, loosely defined and, in practice, refers to any non-typological property, such as genre, topic, medium or style of a document. We investigate the core notion of domains via human proficiency in identifying related intrinsic textual properties, specifically the concepts of genre (communicative purpose) and topic (subject matter). We publish our annotations in TGeGUM: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017) with single sentence and larger context (i.e., prose) annotations for one of 11 genres (source type), and its topic/subtopic as per the Dewey Decimal library classification system (Dewey, 1979), consisting of 10/100 hierarchical topics of increased granularity. Each instance is annotated by three annotators, for a total of 32.7k annotations, allowing us to examine the level of human disagreement and the relative difficulty of each annotation task. With a Fleiss' kappa of at most 0.53 on the sentence level and 0.66 at the prose level, it is evident that despite the ubiquity of domains in NLP, there is little human consensus on how to define them. By training classifiers to perform the same task, we find that this uncertainty also extends to NLP models.\\n\\nKeywords: domain, genre, topic, multi-annotation\\n\\n1. Introduction\\n\\nThe concept of \u201cdomain\u201d is ubiquitous in Natural Language Processing (NLP), as differences between \u201csublanguages\u201d have strong effects on model transferability (Kittredge and Grisham, 1986). This issue of domain divergence has prompted comprehensive surveys on how to best adapt language models (LMs) trained on one or more source domains to more specific targets (Ramponi and Plank, 2020; Ramesh Kashyap et al., 2021; Saunders, 2022), and remains an open issue, even with LMs of increasing size (Ling et al., 2023; Singhal et al., 2023; Wu et al., 2023). Despite its importance, what constitutes a domain remains loosely defined, typically referring to any non-typological property that degrades model transferability. In practice, textual properties with the largest domain effects relate to a document\u2019s genre/medium/style (McClosky, 2010; Plank, 2011; M\u00fcller-Eberstein et al., 2021b), topic (Lee, 2001; Karouzos et al., 2021), or mixtures thereof (Aharoni and Goldberg, 2020).\\n\\nMore broadly, domains can be viewed as a high-dimensional space with variation across the aforementioned properties, plus factors such as author personality, age, or gender (Plank, 2011, 2016).\\n\\nWe attempt to gain a better understanding of the foundational concept of domain, by taking a step back from modeling this phenomenon, and instead investigating whether humans themselves can distinguish between different instantiations of domain-related properties of textual data. In linguistics literature, these properties are separated into register, style and genre (Biber, 1988; Biber and Conrad, 2009, 2019), of which we choose to focus on genre, as it distinguishes itself from register and style by remaining consistent across complete texts. In ad-\"}"}
{"id": "lrec-2024-main-245", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"dition, we examine the orthogonal factor of topic, i.e., the subject matter of a text, which can be expressed independently of genre (Kessler et al., 1997; Lee and Myaeng, 2002; Stein and Zu Eissen, 2006; Webber, 2009). We operationalize these two factors analogously to van der Wees et al. (2015) as genre stemming from different source types with distinct communicative styles, and topic being the principal subject matter of a given text. More formally, our main research question is: To what extent can humans detect genres and topics from text alone, and how does this align with machines? We investigate the human proficiency in detecting these intrinsic properties by turning our attention to the Georgetown University Multi-layer Corpus (GUM; Zeldes, 2017), a large-scale multi-layer corpus consisting of texts from 11 different source types (henceforth genre). These act as gold annotations against which we compare the manual genre labels provided by 12 human annotators for the entirety of the corpus (Figure 1). In addition, the annotators supply a new annotation layer regarding the texts' subject matter (henceforth topic). As no gold labels are available for topic, they are annotated according to Dewey Decimal Classification (DDC; Dewey, 1979), a library classification system that allows new books to be added to a collection based on the subject matter. The DDC consists of 10 topics, 100 fine-grained topics, and 1,000 even finer-grained topics, of which we investigate the former two in detail and provide a preliminary study on the latter. To understand the importance of context, we have annotators label genre and topic at both the sentence and prose level (defined as sequences of five sentences), and compare annotator agreement. Due to the subjective uncertainty associated with these types of characteristics, we gather three annotations per instance, measure their agreement, and release them in their unaggregated form as multi-annotations for future research. Finally, we investigate the ability of machines to identify the same characteristics by training multiple ablations of genre and topic classifiers. Concretely, these experiments examine the difficulty of discerning each property, whether metadata or human notions of genre are more easily recoverable, as well as which level of context is most appropriate for the different ways in which the genre and topic label distributions can be represented. Overall, this work is the first to explore the discernability of domain by both humans and machines. In Section 5, we further discuss the implications of our findings, both with respect to domain-sensitive downstream applications, as well as for the NLP community's more general definition of domain. Our contributions thus include:\\n\\n1. **GUM** (Topic-Genre GUM), a multi-layer extension of GUM, covering 9.1k sentences triple-annotated for a diverse set of 11 genres and 10/100 topics (Section 3).\\n\\n2. An in-depth exploratory data analysis of the human annotations concerning annotator disagreement, uncertainty, and overall trends for domain characteristics across different context sizes (Section 4).\\n\\n3. A case study on the capability of NLP models to discern the human notions of genre and topic, as well as an analysis of which factors affect classification performance (Section 5).\\n\\n### Related Work\\n\\nDomains Initially coined as \u201csublanguages\u201d (Kittredge and Lehrberger, 1982; Kittredge and Grisham, 1986), domains have long been a topic of study in traditional linguistics and NLP (Lee, 2002; Lee and Myaeng, 2002; Stein and Zu Eissen, 2006; Eisenstein et al., 2014; van der Wees et al., 2015; Plank, 2016). Some of the early work mentioning domains as textual categories include Sekine (1997); Ratnaparkhi (1999), which categorize texts into, e.g., \u201cgeneral fiction\u201d, \u201cromance & love\u201d, and \u201cpress:reportage\u201d. However, as also mentioned by Lee (2002); Lee and Myaeng (2002); Plank (2011); van der Wees et al. (2015), the concept of domain is under-defined. Plank (2011) considers domains as a multi-dimensional space, spanning all kinds of variability between texts, such as genre, topic, style, medium, etc. In this work, we follow a definition of domains similar to van der Wees et al. (2015), focusing on two of the largest dimensions of variability: i.e., genres (the communicative purpose and style) as well as topics (the subject matter). The former is closely tied to the source of a text, such as academic papers versus fiction books, while the latter may include subjects such as sports, politics, and philosophy, which can occur in multiple genres.\\n\\n### Automatic Domain Detection\\n\\nIn NLP, automatic domain detection is essential for ensuring robust downstream performance, as it degrades with increasing levels of domain shift (Ramponi and Plank, 2020). Since this issue occurs independently of the application, domain classification has been explored in many contexts. Generally, the problem is either phrased in terms of a binary task, i.e., whether a target text matches the domain of the training data or not (e.g., Tan et al., 2019; Pokharel and Agrawal, 2023), or a multi-label classification task, in which the exact domain is to be...\"}"}
{"id": "lrec-2024-main-245", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"determined (e.g., M\u00fcller-Eberstein et al., 2021a). Here, we use the latter approach as it requires a more formalized operationalization of domain.\\n\\nAt a broader level, genre is frequently used as a proxy for domain, as it has lower internal variability than many more specific dimensions, including topic (Kessler et al., 1997; Webber, 2009). Its automatic detection has been leveraged for selecting training data for transfer learning across a broad range of applications, such as classification (Ruder and Plank, 2017; van der Goot et al., 2021a; Gururangan et al., 2020) and generative tasks (Aharoni and Goldberg, 2020). Beyond English, genre has further been shown to provide a cross-lingually consistent signal for enabling more robust transfer in syntactic parsing (M\u00fcller-Eberstein et al., 2021a).\\n\\nTopics provide a more granular differentiation between texts, also with close ties to domain. Automatically detecting topics has more immediate practical implications, as knowledge of the subject matter is critical for many downstream information extraction systems (Liu et al., 2021; Bassignana and Plank, 2022) and more datasets with topic annotations are available (Sandhaus, 2008; Maas et al., 2011; Wang and Manning, 2012; Zhang et al., 2015); however, these works typically contain source data from only a single corpus. Going beyond prior work with limited sets of post-hoc topic labels for single-genre corpora, we build on the general-purpose DDC system (Dewey, 1979) for libraries and apply its hierarchical set of 10/100 topics to a corpus containing data from 11 genres. By building on the existing annotations of the GUM dataset (Zeldes, 2017), we further enable research not only ascertaining to domain classification for its own sake, but also with applications to other downstream NLP tasks.\\n\\nMulti-annotations\\n\\nGiven the subjective nature of domains and their associated properties of genre and topic, each text in our dataset is annotated multiple times and retains individual labels without aggregating them. This approach of multi-annotations (Plank, 2022) avoids obscuring human uncertainty in the annotation process and has benefits both for tasks with high variability, such as ours, as well as tasks for which a ground truth is typically assumed. E.g., Plank et al. (2014) map part-of-speech (POS) tags from Gimpel et al. (2011) to the universal 12-tag set by Petrov et al. (2012), retaining five crowdsourced POS labels per token.\\n\\nFor Relation Classification (RC), Dumitrache et al. (2018) obtained annotations for 975 sentences for medical RC, where each sentence is annotated by at least 15 annotators on average. For Natural Language Inference (NLI), Nie et al. (2020) released ChaosNLI: A dataset with 4,645 examples and 100 annotations per example for some existing data points in the development set of SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and Abductive NLI (Bhagavatula et al., 2020). For a more in-depth overview of multi-annotation datasets, we refer to Uma et al. (2021).\\n\\n3. The Dataset\\n\\n3.1. Source Data\\n\\nThe source dataset on top of which we build our domain-related annotations is the GUM corpus which in turn incorporates data from a wide variety of sources. We use the portion of the GUM corpus released as part of the Universal Dependencies project (UD; Nivre et al., 2017), i.e., excluding Reddit. Since a text's source is closely tied to its communicative purpose, we consider GUM's data source metadata field of each instance as the gold genre label. For the topic, no equivalent gold label is discernible from the metadata.\\n\\nThe entire dataset is annotated both at sentence and prose level to investigate the importance of context for genre and topic annotation. For this purpose, we follow the gold sentence segmentation provided by GUM. We opted for these blocks instead of paragraphs, as the latter are not natural dividers for all text types and can have a high variety of conventions and functions across genres. To avoid the same annotator observing the same sentence individually as well as in prose, we shuffle the dataset such that annotations of a sentence with and without context are distributed across different annotators, while maintaining coverage of the full dataset.\\n\\n3.2. Annotation Procedure\\n\\nSince there are no official descriptions of the genres in GUM, our annotation guidelines refer to the descriptions from the homepages of the websites of the source or the corresponding abstracts from Wikipedia. For topic annotation, we follow the Dewey Decimal library classification system (Dewey, 1979) consisting of 10/100/1,000 hierarchical topics of increased granularity. We consider the 10 high-level and the 100 mid-level classes for the coarse- and fine-grained topic annotations. We constrain our guidelines such that topic-100 should always be a sub-type of topic-10. For example, if topic-100 is \u201c520 Astronomy\u201d, then topic-10 should be \u201c500 Science\u201d. When none of the topic-100 labels fit the fine-grained topic of the instance, the annotators were allowed to leave the more specific topic blank, i.e., annotating topic-100 with the same label as topic-10. In addition, we include the no-topic label for when it is not possible\"}"}
{"id": "lrec-2024-main-245", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Frequency distributions of the labels in gold genre labels, annotations of genres, annotations of topic-10, and annotations of topic-100 (log scale) on sentence level. For the human annotations, the number is divided by three in order to align with the (unique) gold label. The mapping of topic-10 and topic-100 labels can be found in Appendix F. The tag \u201cNo\u201d in the topic annotations refers to no-topic.\\n\\nWe completed an initial annotation round of 20 instances with all annotators and authors of this paper to evaluate the guidelines and annotation setup. None of this data is included in the final dataset. We continued with groups of three annotators annotating different subsets of the data. After an introductory meeting, further unclarities were discussed asynchronously throughout the process. Annotators were asked to pose their questions in general terms and to not use direct examples as to not bias the other annotators on specific instances. We did not conduct inter-annotator studies over the course of annotation and only had minor guideline revisions during the annotation process since we are mostly interested in human intuitions of genre and topic, and there are no gold labels for the topic task.\\n\\nAnnotators could indicate whether they were unsure about the annotation of a specific instance, and were also asked to provide notes/comments, if applicable. The annotation rate started at approximately 80\u2013150 instances per hour. To ensure a similar amount of effort across annotators, we asked them to aim for approximately 150 instances per hour (also considering that annotation speed increases over time).\\n\\nIn total, we hired 12 annotators, who were paid 34,21 EUR per hour (before tax) for a total of 32 hours per person over a period of 4 weeks. The mean age was 27 (\u00b12), and their highest completed education was equally split between a bachelor's and a master's degree. All rated their English skills as either C2/proficient or native. Seven instances Annotations Sentence Prose Sentence Prose\\n\\n|     | Train | Dev | Test | Total |\\n|-----|-------|-----|------|-------|\\n|     | 6,911 | 1,117 | 1,096 | 9,124 |\\n|     | 1,358 | 217  | 221  | 1,796 |\\n|     | 20,733 | 3,351 | 3,288 | 27,372 |\\n|     | 4,074 | 651  | 663  | 5,388 |\\n\\nTable 1: Dataset Statistics: Note that each instance has three associated annotations. 3.3.\\n\\nDataset Statistics\\n\\nTable 1 shows the final dataset statistics of TGeGUM. The dataset includes around 9.1K sentences, and 1.8K prose, each of them annotated by three individual annotators for genre, coarse-grained topic, and fine-grained topic. Comparing gold and annotated genre labels, we observe a skew towards conversation and textbook. We hypothesize that this is due to the small amount of context an annotator receives. For example, the sentence \u201cIs that all that\u2019s left?\u201d with the gold genre label fiction is annotated by all annotators as conversation. Another example is the\"}"}
{"id": "lrec-2024-main-245", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Agreement scores across annotators, and accuracy of majority vote among annotators compared to gold genre labels.\\n\\n| Sentence | Prose |\\n|----------|-------|\\n| Some of the greatest poetry has been born out of failure and the depths of adversity in the human experience. | textbook |\\n\\nAll annotators annotated this example as textbook.\\n\\nFor topic, we note that despite skewness, almost all 100 topics are used. The 300 Social sciences including, e.g., Political science and Education, stand out as being the most prevalent topics. The most frequent label, however, is no-topic, indicating that it is challenging to identify a specific topic given only one sentence and that individual sentences can be associated with different topics, depending on the surrounding context.\\n\\nThe genre distribution at the prose level (Appendix D) reveals a more accurate distribution for conversation-like utterances; however, the general skew towards textbook remains. Concerning topic, the main contrast to the sentence-level distributions is the reduction of the no-topic label, confirming that more context is crucial for this task.\\n\\n4. Exploratory Data Analysis\\n\\nIn addition to the previous aggregated overview, we are interested in exploring whether domain characteristics are recoverable by humans in a consistent manner. While we can compare human annotations to the original gold labels for genre, no equivalent is available for topic. Therefore, we place more emphasis on inter-annotator agreement, in the form of Fleiss' Kappa (Fleiss, 1971), to measure intuitive alignment and ease of identification.\\n\\nTable 2 and Figure 3 shows this agreement across the different genres, topics and levels of available context.\\n\\n4.1. Human Genre Detection\\n\\nAccuracy and Agreement\\n\\nConsidering that annotation guidelines were phrased to avoid any intentional alignment to an existing ground truth (i.e., annotators were unaware of the existence of gold genre labels), an accuracy of 67.68% at the sentence level shows that genre is recoverable to a far higher degree than by random chance or by a majority baseline. This further increases to 81.11% given more context at the prose level and is also reflected in the increase from moderate academic, biography, conversation, fiction, interview, news, speech, textbook, vlog, voyage, who, academic, bio, conversation, fiction, interview, news, speech, textbook, vlog, voyage, who.\\n\\nSentence Prose\\n\\nFigure 3: Confusion matrix with all annotated pairs of labels for Genre and Topic-10 (across all annotators) in our training data: The darker the color, the higher the number of annotations for that label pair. The diagonal can be seen as agreement, whereas off-diagonal is a proxy for disagreement.\\n\\nFleiss' Kappa (0.53) to substantial agreement (0.66).\\n\\nThe additional context appears to help differentiate genres that have more similarities to each other. This phenomenon is especially pronounced for spoken-language data, such as conversation, interview and vlog, which differ with respect to genre-specific conventions such as who the speech is directed towards (i.e., bi-directional, interviewee, video viewer), or how formal the register is. Both properties are more easily discernible across multiple turns.\\n\\nNonetheless, even given more context, high amounts of confusion remain between certain genres such as non-fiction texts of the type academic, biography, and textbook. These are intuitively similar to each other and may require even more context to distinguish. Generally, genres appear to lie on a more continuous spectrum that is difficult to discretize in conceptually similar cases.\\n\\nHuman Uncertainty\\n\\nIn case of uncertainty, annotators were encouraged to select a \u201cbest guess\u201d label and to indicate uncertainty by ticking a check-box. In addition to overall uncertainty, we also hypothesize that sentence length affects accuracy due to the amount of information available. To evaluate these two effects for genre detection, we measure the Pearson correlation between human accuracy concerning the gold label, with 1) sentence length, 2) the number of uncertainty flags (Table 3). As expected, longer sentences are annotated correctly more often.\\n\\nFigure 4 further highlights how spoken-language genres have a strong\"}"}
{"id": "lrec-2024-main-245", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Frequency of sentence lengths, measured by the number of characters, per gold genre. Skew towards shorter sentences, and for which annotators have the lowest agreements. Additionally, sentences marked as \u201cunsure\u201d align with gold labels less often, showing that annotators appear to have well-calibrated judgments of their own uncertainty, even for this relatively difficult task.\\n\\n4.2. Human Topic Detection Agreement\\n\\nIn the absence of gold labels, inter-annotator agreement allows us to estimate the difficulty of discerning broader vs. granular topics. For the 10 broader topics, Table 2 shows a moderate agreement of 0.52 for both the sentence and prose levels. As expected with an order of magnitude more labels, Topic-100 sees a drop in agreement to 0.42 and an additional drop to 0.38 at the prose level. While this may seem counter-intuitive due to topic's higher specificity compared to genre, Figure 3 sheds some light on this peculiarity: In contrast to genre, topic has a no-topic label (Section 3.2), which, in turn, is used frequently by all annotators at the sentence level, due to the absence of any subject matter in many shorter utterances\u2014especially in speech. Given the additional context, topic becomes more apparent, and agreement spreads toward more topics along the diagonal. As such, sentence-level agreement mainly hinges on no-topic, while prose-level annotations agree more with respect to actual topics. This is less apparent for 10-topic kappa, for which this effect cancels out, but is more prevalent with 100 topics, where the shift away from no-topic at the prose level comes with a much wider spread of topics, thereby reducing overall agreement, despite having a higher level of true topic annotations.\\n\\nOverall, topics which were most consistently identified include social sciences, arts & recreation, technology, science and history & geography. On the other hand, literature was least consistently annotated and most frequently confused with the aforementioned topics, potentially due to its broader scope compared to the others.\\n\\n1,000 Topics\\n\\nAfter completing the full set of genre and topic-10/100 annotations with three annotators per instance, the remaining time of the annotators was spent on a preliminary study to label the most fine-grained categories of DDC. With 1,000 labels, this task is substantially more difficult. We obtained a total of 904 sentences and 172 prose sequences with three annotations each. Measuring inter-annotator agreement at this level of granularity, we find a Fleiss' Kappa of 0.32 for sentences and 0.26 for prose. Although substantially lower than for coarser topic granularities as well as genre, this score still indicates above-random agreement among annotators. Similarly to the previous topic results, prose-level context allows humans to detect more actual topics than no-topic, leading to lower overall agreement but a broader coverage of actual topics.\\n\\nIn general, despite the importance of topic to downstream applications (i.e., topic classification as a task in itself), there is no clear human consensus regarding discrete topic classification. Similarly to genre, topic appears to be a concept for which human intuition shares some agreement at a broader level, but is also spread along a continuum\u2014especially as granularity increases.\\n\\n5. Modeling Domain\\n\\nFollowing our examination of human notions of genre and topic, we investigate automatic methods' ability to model the same properties. Ablating across different setups for representing the multiple annotations per instance (Section 5.1), we train models to classify genre and topic at different levels of granularity (Section 5.2) and evaluate their ability to learn the underlying distribution (Section 5.3). While pre-neural work typically performed document-level classification (Webber, 2009; Petrenz and Webber, 2011), contemporary...\"}"}
{"id": "lrec-2024-main-245", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"trends have shifted towards the sentence-level (Aharoni and Goldberg, 2020; M\u00fcller-Eberstein et al., 2021b). Leveraging our multi-level annotations, we investigate genre and topic classification at both the sentence and prose-level, mirroring our human annotation setup.\\n\\n5.1. Setup\\n\\nMost work on modeling multiple annotators is based on tasks consisting of only two or three labels, e.g., hate speech detection, or RTE (Uma et al., 2021). An exception is Kennedy et al. (2020), who use multiple classification heads to predict a score for a variety of aspects of hate speech, which are then used to predict a final floating point score for hate speech detection. Other related work predicts multiple task labels simultaneously (e.g., Demszky et al., 2020; Kiesel et al., 2023; Piskorski et al., 2023), however these are typically discrete and do not model annotator certainty. We propose a variety of methods to model the distribution of the annotations (overview in Figure 5):\\n\\n- **Majority**: Discretizes the labels using a majority vote, and uses a single classification head to predict it. For the distribution similarity metric (see below), we assign a score of 1.0 to the chosen label.\\n\\n- **PerLabel-Regression**: Converts the human annotations to scores per label and then predicts these as a regression task. Each label has its own decoder head, trained using an MSE loss, and mapped to the [0;1] range afterwards.\\n\\n- **PerLabel-Classification**: Converts the human annotations into score bins and predicts them as four possible labels: \\\"0.0\\\", \\\"0.33\\\", \\\"0.66\\\", \\\"1.0\\\".\\n\\n- **PerAnnotator**: One decoder head modeling each annotator, that predicts their annotation as a discrete label. Afterwards, the three predictions are converted to a distribution.\\n\\nWe evaluate these models using the standard accuracy over each singular predicted label (i.e., highest score or majority). In addition, we conduct a finer-grained evaluation that takes the multi-annotations into account. For this purpose, we propose a similarity metric for comparing the predicted and annotated label distribution per instance. Let $n$ be the number of label types, and $X$ and $Y$ are label distributions that sum to 1, with a score for each label. Then, the distributional similarity per instance can be computed as:\\n\\n$$\\\\text{distr\\\\_sim} = 1 - \\\\sum_{n=0}^{n} |X_n - Y_n|^2.$$\\n\\nWe implement all our model variants in the MaChAmp (van der Goot et al., 2021b) toolkit v0.4 using default parameters. MaChAmp is a toolkit focused on multi-task learning for NLP, and allowed us to implement all varieties of the tasks described earlier. Each way of phrasing the task is implemented on top of a single language model for fair comparison. From an initial evaluation of the bert-large-cased (Devlin et al., 2019), luke-large-lite (Yamada et al., 2020), deberta-v3-large (He et al., 2021), xlm-roberta-large (Conneau et al., 2020) LMs on the gold genre labels, we identify that DeBERTa has the highest accuracy; hence we use it in the following experiments.\\n\\n5.2. Classification Results\\n\\nWe examine which notion of domain is more learnable and distinguishable for a model; genre or topic? Since genre has associated ground truth labels, we additionally examine whether the human annotators' perception of genre or the ground truth genre is easier to learn.\\n\\nWe establish a majority vote based on the human annotations; in case of a tie, the first element in the annotation list is chosen as the label, both for sentences and prose. This happens in $\\\\sim 10\\\\%$ of cases for genre and topic-10 (sentence and prose), and $\\\\sim 20\\\\%$ cases for topic-100.\\n\\nTable 4 shows accuracy and macro-F1 scores of the annotators' majority vote evaluated against\"}"}
{"id": "lrec-2024-main-245", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Performance of annotators' majority vote compared with the gold genre (development set).\\n\\nThe gold genre. As noted previously, more context (prose level) helps disambiguate the genre.\\n\\nTo evaluate how well a model can align with the human intuition of genres and topics, we fine-tune an LM on the majority labels of the annotators. We compare the performance on the gold genre labels (the only task for which we have gold labels) and compare the accuracy and macro-F1 scores (Table 5). We notice the following:\\n\\n1) Unsurprisingly, DeBERTa fine-tuned on the gold genre labels (gold_genre) is better aligned with the ground truth genre than the human majority vote, i.e., 73.20 (Table 5) versus 67.68 (Table 4) accuracy at the sentence level (note that other LMs performed worse).\\n\\n2) In contrast, the fine-tuned DeBERTa model has higher accuracy when trained and tested on the human majority vote (maj_genre) than when using gold genre labels (gold_genre), i.e., 75.88 versus 73.20, although macro-F1 is lower. This indicates that less common genre labels are easier to learn from gold labels, while more frequent genres are easier to learn based on human intuitions.\\n\\n3) Despite topic-10 having fewer classes than genre, the notion of topic appears to be more difficult for a model to learn (lower F1).\\n\\n4) The skew of the fine-grained topics (maj_topic-100) and the difficulty of the long tail become apparent in the large divergence across the accuracy and macro-F1 score.\\n\\n5) In contrast to the sentence level, our fine-tuned DeBERTa model generalizes better to the gold genre labels (gold_genre) than the human majority vote (maj_genre). At this level of context, the majority vote topic is also harder for a model to learn than the majority vote genre.\\n\\n5.3. Distributional Results\\n\\nIn Figure 6, we report the results of the models trained on all instances (sentences and prose) with DeBERTaV3-large. The main trends show that the model performs better on the genre task. Unsurprisingly, for topics, the granularity of the labels impacts performance.\\n\\nTraining on sentences and prose separately leads to similar trends (Appendix B).\"}"}
{"id": "lrec-2024-main-245", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"procedure designed to capture human variability instead of forcing alignment (Section 3).\\n\\nOur exploratory analysis (Section 4) shows that despite the subjective nature of this task, as reflected in a Fleiss' Kappa of 0.53\u20130.66, humans can identify certain domain characteristics consistently from one sentence alone. Nonetheless, genres with a high similarity benefit substantially from added context. This is even more crucial for identifying topics, where we observe a shift from annotators not being able to discern any topic at all to being able to reach an above-random agreement, even when presented with 100 or 1,000 topics.\\n\\nFinally, our experiments of modeling these domain characteristics automatically (Section 5) show that genre is easier to model than topic. For both the agreements between human annotators, and the performance from the automatic model, we see that context is crucial for the genre classification task, but not for topic classification, where adding context even leads to decrease in scores if the label space is large.\\n\\nOverall, this work highlights that despite the importance of \\\"domain\\\", there is little consensus regarding its definition, both in the NLP community as well as in our human annotations. Taking a closer look at what intuition predicted, further reveals that genres and topics are difficult to discretize completely, and that a continuous space of domain variability may be more suited for characterizing these phenomena.\\n\\n7. Ethics Statement\\n\\nOur approach to modeling human label variation is intrinsically linked to the larger issue of human social bias. As highlighted by Plank (2022), significant social implications are tied to the study of label variation. In the context of our research, it is essential to acknowledge that variations in labeling might stem from societal biases and disparities. To address this, we recognize the necessity of addressing bias mitigation techniques as we aim to create more equitable and just models. However, we also contend that our focus on modeling generic subjects, such as genre and topic, may carry less severe implications compared to more subjective tasks like hate speech detection (Akhtar et al., 2021; Davani et al., 2022). The differences in annotations within our work may primarily relate to two categories: \\\"Missing Information\\\" and \\\"Ambiguity\\\" (Sandri et al., 2023).\\n\\nAnother ethical facet we must address is the potential biases present in the classification system we use. In particular, the Dewey Decimal Classification System, which is the de-facto standard for libraries worldwide, has been found to exhibit prejudice (Gooding-Call, 2021). For example, the classification of information related to religion, specifically within class 200, demonstrates a clear skew, with a majority of subjects (six out of ten) reserved for Christianity-related topics. The remaining four slots are designated for other dominant religions, with an other section meant to encompass all other belief systems. This reveals an inherent bias toward Christianity, which can affect the accessibility of non-dominant religions and belief systems.\\n\\nThere are alternatives to knowledge organization systems like the Dewey Decimal Classification, as suggested by Franzen (2022), to promote a more inclusive and equitable information landscape.\\n\\n8. Acknowledgments\\n\\nMany thanks to our annotators: Nina Sand Hostrup, Leonie Brockhaus, Birk Staanum, Constantin Bogdan Craciun, Sofie Bengaard Pedersen, Yiping Duan, Axel Sorensen, Henriette Granh\u00f8j Dam, Trine Naja Eriksen, Cathrine Damgaard, and the other two anonymous annotators. Maria Barrett is supported by a research grant (34437) from VILLUM FONDEN. Mike Zhang is supported by the Independent Research Fund Denmark (DFF) grant 9131-00019B. Elisa Bassignana and Max M\u00fcller-Eberstein are supported by the Independent Research Fund Denmark (DFF) Sapere Aude grant 9063-00077B. Amalie Pauli is supported by the Danish Data Science Academy, which is funded by the Novo Nordisk Foundation (NNF21SA0069429) and VILLUM FONDEN (40516).\\n\\n9. Bibliographical References\\n\\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised domain clusters in pretrained language models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7747\u20137763, Online. Association for Computational Linguistics.\\n\\nSohail Akhtar, Valerio Basile, and Viviana Patti. 2021. Whose opinions matter? perspective-aware models to identify opinions of hate speech victims in abusive language detection. ArXiv preprint, abs/2106.15896.\\n\\nElisa Bassignana and Barbara Plank. 2022. CrossRE: A cross-domain dataset for relation extraction. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3592\u20133604, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\"}"}
{"id": "lrec-2024-main-245", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-245", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chris J Kennedy, Geoff Bacon, Alexander Sahn, and Claudia von Vacano. 2020. Constructing interval variables via faceted Rasch measurement and multitask deep learning: a hate speech application. ArXiv preprint, abs/2009.10277.\\n\\nBrett Kessler, Geoffrey Nunberg, and Hinrich Schutze. 1997. Automatic detection of text genre. In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 32\u201338, Madrid, Spain. Association for Computational Linguistics.\\n\\nJohannes Kiesel, Milad Alshomary, Nailia Mirzakhmedova, Maximilian Heinrich, Nicolas Handke, Henning Wachsmuth, and Benno Stein. 2023. SemEval-2023 task 4: ValueEval: Identification of human values behind arguments. In Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 2287\u20132303, Toronto, Canada. Association for Computational Linguistics.\\n\\nRichard Kittredge and Ralph Grisham. 1986. Analyzing Language in Restricted Domains: Sublanguage Description and Processing. Lawrence Erlbaum Associates.\\n\\nRichard Kittredge and John Lehrberger. 1982. Sublanguage: Studies of language in restricted semantic domains. Walter de Gruyter.\\n\\nDavid Lee. 2002. Genres, registers, text types, domains and styles: clarifying the concepts and navigating a path through the BNC jungle. In Teaching and Learning by Doing Corpus Analysis, pages 245\u2013292. Brill.\\n\\nDavid YW Lee. 2001. Genres, registers, text types, domain, and styles: Clarifying the concepts and navigating a path through the BNC jungle. Language Learning & Technology, 5(3):37\u201372.\\n\\nYong-Bae Lee and Sung Hyon Myaeng. 2002. Text genre classification with genre-revealing and subject-revealing features. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 145\u2013150.\\n\\nChen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Jian Pei, and Liang Zhao. 2023. Domain specialization as the key to make large language models disruptive: A comprehensive survey. ArXiv.\\n\\nZihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung. 2021. Crossner: Evaluating cross-domain named entity recognition. Proceedings of the AAAI Conference on Artificial Intelligence, 35(15):13452\u201313460.\\n\\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA. Association for Computational Linguistics.\\n\\nDavid McClosky. 2010. Any domain parsing: automatic domain adaptation for natural language parsing. Ph.D. thesis, Brown University.\\n\\nMax M\u00fcller-Eberstein, Rob van der Goot, and Barbara Plank. 2021a. Genre as weak supervision for cross-lingual dependency parsing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4786\u20134802, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nMax M\u00fcller-Eberstein, Rob van der Goot, and Barbara Plank. 2021b. How universal is genre in Universal Dependencies? In Proceedings of the 20th International Workshop on Treebanks and Linguistic Theories (TREET, SyntaxFest 2021), pages 69\u201385, Sofia, Bulgaria. Association for Computational Linguistics.\\n\\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What can we learn from collective human opinions on natural language inference data? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9131\u20139143, Online. Association for Computational Linguistics.\\n\\nJoakim Nivre, \u017deljko Agi\u0107, Lars Ahrenberg, Lene Antonsen, Maria Jesus Aranzabe, Masayuki Asahara, Luma Ateyah, Mohammed Attia, Aitziber Atutxa, Elena Badmaeva, Miguel BallesTEROS, Esha Banerjee, Sebastian Bank, John Bauer, Kepa Bengoetxea, Riyaz Ahmad Bhat, Eckhard Bick, Cristina Bosco, Gosse Bouma, Sam Bowman, Aljoscha Burchardt, Marie Candito, Gauthier Caron, G\u00fcl\u015fen Cebiro\u011flu Eryi\u011fit, Giuseppe G. A. Celano, Savas Cetin, Fabr\u00edcio Chalub, Jinho Choi, Yongseok Cho, Silvie Cinkov\u00e1, \u00c7a\u011fr\u0131 \u00c7\u00f6ltekin, Miriam Connor, Marie-Catherine de Marneffe, Valeria de Paiva, Arantza Diaz de Ilarraza, Kaja Dobrovoljc, Timothy Dozat, Kira Droganova, Marhaba Eli, Ali\"}"}
{"id": "lrec-2024-main-245", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Elkahky, Toma\u017e Erjavec, Rich\u00e1rd Farkas, H\u00e9ctor Fernandez Alcalde, Jennifer Foster, Claudia Freitas, Katar\u00edna Gajdo\u0161ov\u00e1, Daniel Galbraith, Marcos Garcia, Filip Ginter, Iakes Goe\u00f1aga, Koldo Gojenola, Memduh G\u00f6k\u0131rmak, Yoav Goldberg, Xavier G\u00f3mez Guinovart, Berta Gonz\u00e1les Saavedra, Matias Grioni, Normunds Gr\u016bz\u012btis, Bruno Guillaume, Nizar Habash, Jan Haji\u010d, Jan Haji\u010d jr., Linh H\u00e0 M\u1ef9, Kim Harris, Dag Haug, Barbora Hladk\u00e1, Jaroslava Hlav\u00e1\u010dov\u00e1, Petter Hohle, Radu Ion, Elena Irimia, Anders Johannsen, Fredrik J\u00f8rgensen, H\u00fcner Ka\u015f\u0131kara, Hiroshi Kanayama, Jenna Kanerva, Tolga Kayadelen, V\u00e1clava Kettnerov\u00e1, Jesse Kirchner, Natalia Kotsyba, Simon Krek, Sookyoung Kwak, Veronika Laippala, Lorenzo Lambertino, Tatiana Lando, Phuong L\u00ea H\u1ed3ng, Alessandro Lenci, Saran Lertpradit, Herman Leung, Cheuk Ying Li, Josie Li, Nikola Ljube\u0161i\u0107, Olga Loginova, Olga Lyashevskaya, Teresa Lynn, Vivien Macketanz, Aibek Makazhanov, Michael Mandl, Christopher Manning, Ruli Manurung, C\u0103t\u0103lina M\u0103r\u0103nduc, David Mare\u010dek, Katrin Marheinecke, H\u00e9ctor Mart\u00ednez Alonso, Andr\u00e9 Martins, Jan Ma\u0161ek, Yuji Matsumoto, Ryan McDonald, Gustavo Mendon\u00e7a, Anna Missil\u00e4, Verginica Mititelu, Yusuke Miyao, Simonetta Montemagni, Amir More, Laura Moreno Romero, Shunsuke Mori, Bohdan Moskalevskyi, Kadri Muischnek, Nina Mustafina, Kaili M\u00fc\u00fcrisep, Pinkey Nainwani, Anna Nedoluzhko, Luong Nguy\u1ec5n Th\u1ecb, Huy\u1ec1n Nguy\u1ec5n Th\u1ecb Minh, Vitaly Nikolaev, Rat\u00edtima Nitisaroj, Hanna Nurmi, Stina Ojala, Petya Osenova, Lilja \u00d8vrelid, Elena Pascual, Marco Passarotti, Cenel-Augusto Perez, Guy Perrier, Slav Petrov, Jussi Piitulainen, Emily Pitler, Barbara Plank, Martin Popel, Lauma Pretkalni\u0146a, Prokopis Prokopidis, Tiina Poulakainen, Sampo Pyysalo, Alexandre Rade maker, Livy Real, Siva Reddy, Georg Rehm, Larissa Rinaldi, Laura Rituma, Rudolf Rosa, Davide Rovati, Shadi Saleh, Manuela Sanguinetti, Baiba Saul\u012bte, Yanin Sawanakunanon, Sebastian Schuster, Djam\u00e9 Seddah, Wolfgang Seeker, Mojgan Seraji, Lena Shakurova, Mo Shen, Atsuko Shimada, Muh Shohibussirri, Natalia Silveira, Maria Simi, Radu Simionescu, Katalin Simk\u00f3, M\u00e1ria \u0160imkov\u00e1, Kiril Simov, Aaron Smith, Antonio Stella, Jana Strnadov\u00e1, Alane Suhr, Umut Sulubacak, Zsolt Sz\u00e1nt\u00f3, Dima Taji, Takaaki Tanaka, Tround T rosterud, Anna Trukhina, Reut T sarfaty, Francis Tyers, Sumire Uematsu, Zde\u0148ka Ure\u0161ov\u00e1, Larraitz Uria, Hans Uszkoreit, Gertjan van Noord, Viktor Varga, Veronika Vincze, Jonathan North Washington, Zhuoran Yu, Zden\u011bk \u017dabokrtsk\u00fd, Daniel Zeman, and Hanzhi Zhu. 2017.\\n\\nUniversal dependencies 2.0 \u2013 CoNLL 2017 shared task development and test data. LINDA T/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL), Faculty of Mathematics and Physics, Charles University.\\n\\nPhilipp Petrenz and Bonnie Webber. 2011. Squibs: Stable classification of text genres. Computational Linguistics, 37(2):385\u2013393.\\n\\nSlav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A universal part-of-speech tagset. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 2089\u20132096, Istanbul, Turkey. European Language Resources Association (ELRA).\\n\\nJakub Piskorski, Nicolas Stefanovitch, Giovanni Da San Martino, and Preslav Nakov. 2023. SemEval-2023 task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. In Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 2343\u20132361, Toronto, Canada. Association for Computational Linguistics.\\n\\nBarbara Plank. 2011. Domain adaptation for parsing. Ph.D. thesis, University of Groningen.\\n\\nBarbara Plank. 2016. What to do about non-standard (or non-canonical) language in nlp. In KONVENS 2016, Ruhr-University Bochum.\\n\\nBarbara Plank. 2022. The \u201cproblem\u201d of human label variation: On ground truth in data, modeling and evaluation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10671\u201310682, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nBarbara Plank, Dirk Hovy, and Anders S\u00f8gaard. 2014. Linguistically debatable or just plain wrong? In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 507\u2013511, Baltimore, Maryland. Association for Computational Linguistics.\\n\\nRhitabrat Pokharel and Ameeta Agrawal. 2023. Estimating semantic similarity between in-domain and out-of-domain samples. In Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 409\u2013416, Toronto, Canada. Association for Computational Linguistics.\\n\\nAbhinav Ramesh Kashyap, Devamanyu Hazarika, Min-Yen Kan, and Roger Zimmermann. 2021. Domain divergences: A survey and empirical...\"}"}
{"id": "lrec-2024-main-245", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-245", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bonnie Webber. 2009. Genre distinctions for discourse in the Penn TreeBank. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 674\u2013682, Suntec, Singapore. Association for Computational Linguistics.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. BloombergGPT: A large language model for finance. ArXiv.\\n\\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. LUKE: Deep contextualized entity representations with entity-aware self-attention. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6442\u20136454, Online. Association for Computational Linguistics.\\n\\nAmir Zeldes. 2017. The gum corpus: creating multilayer resources in the classroom. Language Resources and Evaluation, 51(3):581\u2013612.\\n\\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649\u2013657.\"}"}
{"id": "lrec-2024-main-245", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A.\\n\\nConfusion Matrices Genre\\n\\nIn Figure 7\u2013Figure 9 we plot the confusion matrices of our DeBERTa model trained on the gold genre labels. The conversation genre shows to be the most difficult label; it is commonly confused with fiction, interview and vlog; which also overlap in length (Section 4).\\n\\nFigure 7: Confusion matrix on the sentence level, numbers are summed over all five random seeds.\\n\\nFigure 8: Confusion matrix on the prose level, numbers are summed over all five random seeds.\\n\\nFigure 9: Confusion matrix on all data, numbers are summed over all five random seeds.\\n\\nB. Sentence and Prose Results\\n\\nIn Figure 10 we show the results of our proposed models trained and evaluated only on the sentence level data. Figure 11 has the same evaluation on the prose level data.\\n\\nC. Visualization of Embeddings\\n\\nWe encode sentences using Sentence-BERT (Reimers and Gurevych, 2019), apply a PCA-downprojection, and color each sentence according to gold genres, our majority-vote genre annotations, as well as majority-vote topic-10 annotations. The results are shown in Figures 12\u201314.\"}"}
{"id": "lrec-2024-main-245", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Results of our proposed models on the prose level data.\\n\\nFigure 12: PCA plot of sentence embeddings with the gold genres.\\n\\nD. Prose-level Statistics\\n\\nLabel statistics on the prose level are shown in Figure 15. While general trends, such as the majority genres and topics remain the same as on the sentence level, additional context spreads annotations more evenly, and allows for disambiguations such as for spoken data genres. This is also reflected in the higher alignment between gold and annotated genre labels\u2014both in terms of number, but also in terms of accuracy (Table 2). For topic, we further observe almost an order of magnitude fewer no-topic annotations, which are consequently distributed across the spectrum of actual topics.\\n\\nFigure 13: PCA plot of sentence embeddings with our annotation for genres, majority vote is used for each instance.\\n\\nFigure 14: PCA plot of sentence embeddings with our annotation for coarse topics, majority vote is used for each instance.\\n\\nE. Annotator Comments\\n\\nAnnotators were provided with a free-form field to provide optional comments regarding each annotation. Of the final dataset, 3.9% of annotations have an annotator comment attached, with a median length of 38 characters. They primarily contain explanations of annotations which were marked with high annotator uncertainty.\"}"}
