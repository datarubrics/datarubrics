{"id": "acl-2024-long-730", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A\\\\textsc{rch}C\\\\textsc{ode}: Incorporating Software Requirements in Code Generation with Large Language Models\\n\\nHojae Han\u2660\u02db Jaejin Kim\u2660\u02db Jaeseok Yoo \u2746 Youngwon Lee \u2746 Seung-won Hwang \u2746 Seoul National University, SNU-LG AI Research Center\\n\\\\{stovecat,jaejin.kim,jaeseok2.yoo,ludaya,seungwonh\\\\}@snu.ac.kr\\n\\nAbstract\\nThis paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e., achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce A\\\\textsc{rch}C\\\\textsc{ode}, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. A\\\\textsc{rch}C\\\\textsc{ode} generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmarks show that A\\\\textsc{rch}C\\\\textsc{ode} enhances to satisfy functional requirements, significantly improving Pass@$k$ scores. Furthermore, we introduce HumanEval-NFR, the first evaluation of LLMs' non-functional requirements in code generation, demonstrating A\\\\textsc{rch}C\\\\textsc{ode}'s superiority over baseline methods. The implementation of A\\\\textsc{rch}C\\\\textsc{ode} and the HumanEval-NFR benchmark are both publicly accessible.\\n\\n1 Introduction\\nRecent advancements in large language models (LLMs) have significantly improved code generation capabilities (Chen et al., 2021; Li et al., 2022; OpenAI, 2023). Although the primary goal for LLMs in this domain is to generate functionally correct code based on textual descriptions (Hendrycks et al., 2021; Austin et al., 2021; Chen et al., 2021; Li et al., 2022), real-world software development encompasses more than just functionality. Despite the critical role of software requirements, considering these criteria has not been studied actively in previous code generation works, merely...\"}"}
{"id": "acl-2024-long-730", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"```\\nimport sys\\nimport math\\n\\ndef is_prime(n):\\n    if not isinstance(n, int):\\n        sys.stderr.write('Invalid input: n must be an integer.'\\n        return None\\n    if n < 2:\\n        return False\\n    for i in range(2, int(math.sqrt(n)) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\ndef x_or_y(n, x, y):\\n    # Validate inputs\\n    if not isinstance(x, (int, float)) or not isinstance(y, (int, float)):\\n        sys.stderr.write('Invalid input: x and y must be numeric values.'\\n        return None\\n    if is_prime(n):\\n        return x\\n    else:\\n        return y\\n```\\n\\n- **Functional Requirements**\\n- **Input/Output Conditions**\\n  - The function takes three arguments: an integer `n`, and two values `x` and `y`.\\n  - It returns a single value.\\n- **Expected Behavior**\\n  - If `n` is a prime number, the function should return the value of `x`.\\n  - If `n` is not a prime number, the function should return the value of `y`.\\n- **Edge Cases**\\n  - Handle negative values of `n`, returning the value of `y`.\\n  - Handle when `n` equals to 0, returning the value of `y`.\\n\\n- **Non-Functional Requirements**\\n  - **Time Performance**\\n    - Time complexity: \\\\( O(\\\\sqrt{n}) \\\\) for the `is_prime` helper function, where \\\\( n \\\\) is the value of `n`.\\n    - Ensure efficiency even for extremely large inputs, providing results within 5 seconds.\\n  - **Robustness**\\n    - If non-integer values are provided for `n`, print an error message to `stderr` and return None.\\n    - If `x` or `y` is not a numeric value, print an error message to `stderr` and return None.\\n  - **Maintainability**\\n    - Target Cyclomatic Complexity: \\\\( \\\\leq 5 \\\\).\\n\\n```\\nassert x_or_y(13, 77, 2) == 77, 'Failed to return the value of x for a prime number.'\\nassert x_or_y(24, 8, 9) == 9, 'Failed to return the value of y for a non-prime number.'\\nassert x_or_y(-7, 77, -5) == -5, 'Failed to handle a negative input number.'\\nassert x_or_y(0, 77, 0) == 0, 'Failed to handle a zero input number.'\\nassert x_or_y(2**31-1, 34, 0) == 34, 'Failed to handle large input size.'\\nassert not x_or_y('invalid', 34, 0), 'Failed to handle a non-integer input number.'\\nassert ComplexityVisitor.from_code('''def x_or_y ...''').total_complexity <= 5, 'Failed to have a Cyclomatic Complexity less than or equal to 5 by Radon.'\\n```\\n\\nFigure 2: An illustrative example of code and test case generation. Existing approaches derive code and test cases directly from problem descriptions, often missing key requirements. A RCODE, in contrast, reformulates (underlined) and extrapolates (not underlined) requirements from these descriptions, then generates code and test cases to meet them comprehensively. Best viewed in color.\"}"}
{"id": "acl-2024-long-730", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that ARCH CODE notably outperforms existing techniques in terms of the satisfaction of FRs\u2014surpassing GPT-4's Pass@1 score on both benchmarks and achieving new state-of-the-art on CodeContests. Moreover, we introduce HumanEval-NFR based on HumanEval, the first benchmark to evaluate NFRs alongside FRs, to confirm that ARCH CODE is also effective in pursuing NFRs.\\n\\nOur main contributions are as follows:\\n\\n\u2022 We propose ARCH CODE, a novel framework that leverages ICL to incorporate software requirements in code generation.\\n\\n\u2022 ARCH CODE with GPT-3.5-Turbo surpasses GPT-4's Pass@1 scores on both HumanEval and CodeContests by 4.81%p and 10.45%p, while requiring 50\u02c6smaller number of test cases to be generated compared to existing methods.\\n\\n\u2022 We introduce HumanEval-NFR, the first code generation benchmark for NFR evaluation to confirm the effectiveness of ARCH CODE for NFR satisfaction.\\n\\n---\\n\\nTable 1: ARCH CODE is a novel code and test case generation framework that pursues the satisfaction of both FRs and NFRs. In NFRs column, \u25b3 denotes that only one or two NFRs were addressed in those works, whereas ARCH CODE addresses four different NFR categories, marked as \u2713.\"}"}
{"id": "acl-2024-long-730", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"assert element([3, 2, 5], 1) == 0.2\\nassert not element([], 0)\\nassert element(list(range(10**3)), 1)\\nassert not element(\\\"invalid\\\", 1)\\nassert not element([1, 2], \\\"invalid\\\")\\nassert not element([3, 2], 6)\\nassert ComplexityVisitor.from_code...\\n# Reliability satisfied if no runtime error..\\nassert element([9, 3, 8], 2) == 0.4\\nassert not element([5, -5], 3)\\nassert element([778, 3, 293, 4022], 1)\\nassert not element(\\\"[43, 2]\\\", 1)\\nassert not element([4, 9], 4)\\nassert ComplexityVisitor.from_code...\\n\\nassert complexity:\\nnode.\\n\\nassert\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n\\nassert complexity:\\nnode.\\n"}
{"id": "acl-2024-long-730", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"valid input type is a list, or considering '0' for non-negative integer inputs.\\n\\nTarget NFRs\\n\\nARCH CODE considers NFRs that are both pivotal in real-world applications and feasible for assessment either through code execution or using existing metrics.\\n\\n- **Time Performance**: Pertains to time-centric aspects like algorithmic time complexity or stipulated timeout conditions.\\n- **Robustness**: Ensures that code is resilient to invalid inputs (McConnell, 2004). For instance, a function designed for integer addition must prevent unforeseen or undesirable outcomes from the '+' operation, like mistakenly returning the concatenated string when given two strings.\\n- **Maintainability**: Considers factors that contribute to the ease of maintenance, such as reducing code complexity via code modularization (Magel et al., 1982), measured by cyclomatic complexity (McCabe, 1976).\\n- **Reliability**: Ensures that the code can handle errors gracefully, without causing system failures, thereby increasing the mean time between failures (McConnell, 2004).\\n\\n3.2 Requirements-aware Generation\\n\\nUpon obtaining software requirements \\\\( \\\\hat{r} \\\\), ARCH CODE conditions \\\\( \\\\hat{r} \\\\) with the given description \\\\( p \\\\) to generate code samples and test cases.\\n\\nSpecifically, ARCH CODE generates code \\\\( \\\\hat{c} \\\\) and test cases \\\\( \\\\hat{t} \\\\) in a parallel manner:\\n\\n\\\\[\\n\\\\hat{c} = f(p_1, r_1, c_1) \\\\ldots (r_p, \\\\hat{r}_{sq}),\\n\\\\hat{t} = h(p_1, r_1, t_1) \\\\ldots (r_p, \\\\hat{r}_{sq}),\\n\\\\]\\n\\nwhere \\\\( c_i \\\\) and \\\\( t_i \\\\) are the code and the list of test cases of the \\\\( i \\\\)-th example, and each \\\\( t_j \\\\) in \\\\( t_i \\\\) is a test case corresponding to \\\\( r_j \\\\) in \\\\( \\\\hat{r} \\\\).\\n\\n4 Experiments\\n\\nWe evaluate ARCH CODE's effectiveness using three benchmarks, categorized into two types: 1) A novel benchmark for assessing both FR and NFRs satisfaction; 2) Two public benchmarks aimed at FR evaluation, facilitating comparison of ARCH CODE with existing baselines. For the former, we introduce HumanEval-NFR for comprehensive NFR assessment, overcoming the conventional focus on FR alone. For the latter, we explore two code modalities: 1) function-level and 2) competition-level code generation.\\n\\n4.1 Experimental Setup\\n\\nWe evaluate the effectiveness of ARCH CODE on code generation with LLMs. Throughout the experiments, we used GPT-3.5-Turbo-16k (OpenAI, 2022) as the backbone LLMs for generating code, software requirements, test cases, etc. More details can be found in Appendix A.\\n\\nEvaluation Metrics\\n\\nWe mainly consider the widely used Pass@k:\\n\\n\\\\[\\nE = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\mathrm{Prob}(\\\\text{Problem}_i \\\\text{ is passed given } k \\\\text{ chances to sample } c \\\\text{ correct code snippets among } n \\\\text{ samples})\\n\\\\]\\n\\n(Chen et al., 2021) metric for evaluation, which is the unbiased estimator of the probability that the code generation system would have passed a problem if it were given \\\\( k \\\\) chances to sample \\\\( c \\\\) correct code snippets among \\\\( n \\\\) samples. Adhering to Chen et al. (2023), when applying code filtering, we denote the existence of passed code among the \\\\( k \\\\) filtered samples.\\n\\nWhile this metric is sometimes referred to as \\\\( n@k \\\\)\u2014the pass ratio of filtered \\\\( n \\\\) samples from \\\\( k \\\\)\u2014we avoid this notation.\"}"}
{"id": "acl-2024-long-730", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2 presents that ARCHCODE outperforms all baseline methods across various NFR categories except for maintainability. Our conjecture is that, as which NFR categories to prioritize is uninformed in this experiment, ARCHCODE\u2019s consideration of all NFRs could potentially impede maintainability due to the influence of other categories. We study the informed case of optimizing specific categories in Section 5.3. Across all approaches, satisfying the robustness category appears to be more difficult compared to other NFR categories, for which we provide further discussion in Appendix G.\"}"}
{"id": "acl-2024-long-730", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3 HumanEval and CodeContests: Public Benchmarks for FR Evaluation\\n\\nWe additionally report results on two popular code generation benchmarks targeting functional correctness. HumanEval (Chen et al., 2021) is a hand-crafted test benchmark with 164 programming problems along with public and hidden test cases. CodeContests (Li et al., 2022) consists of 13k/113/165 instances of train/valid/test data collected from multiple code competition websites. While HumanEval tests the model's capability to implement rather simpler functions without errors, the competitive programming oriented nature of CodeContests often requires more complex forms of reasoning such as algorithmic reasoning. Each of these addresses different aspects of industrial software: the former is related to solving each of the simpler tasks composing larger and complex projects while the latter focuses on the logical and algorithmic perspective of software development.\\n\\nIn Table 3, A\\n\\\\textsc{RCHCODE}\\nconsistently outperforms the baseline methods. Specifically, on both benchmarks, A\\n\\\\textsc{RCHCODE}\\nleveraging GPT-3.5-Turbo, exceeds GPT-4's performance by a substantial margin of 4.81\\\\%p and 10.45\\\\%p in terms of Pass@1. In comparison with WizardCoder34B\u2014a baseline that partially incorporates NFR considerations during the finetuning phase\u2014A\\n\\\\textsc{RCHCODE}\\n, which covers NFRs more comprehensively, achieves significantly higher performance. In CodeContests, while our custom GPT-3.5-Turbo + CoT prompting baseline is outdone by the state-of-the-art CoT methods \\\\textsc{RAINSTORM} and \\\\textsc{ALGO}, the application of A\\n\\\\textsc{RCHCODE}\\noutperforms both approaches, setting new state-of-the-art of Pass@1.\\n\\nWe also compare A\\n\\\\textsc{RCHCODE}\\nwith MPSC, a very recent baseline. Notably, A\\n\\\\textsc{RCHCODE}\\nsurpasses MPSC in all Pass@k metrics on HumanEval and Pass@1 on CodeContests, while A\\n\\\\textsc{RCHCODE}\\nis much more cost-efficient. We provide further discussion on computational costs in Section 5.1.\\n\\n5 Analysis and Discussion\\n5.1 Efficiency and Effectiveness of Requirement-aware Test Case Generation\\n\\nEfficiency\\n\\nIn code filtering, a crucial step involves minimizing the number of generated test cases to reduce computational and time costs for code execution. As shown in Figure 4, existing approaches such as MPSC and \\\\textsc{CODET} requires to generate hundreds of test cases for performance. \\\\textsc{A\\n\\\\textsc{RCHCODE}} (this work)\\nMPSC\\n\\\\textsc{CODET}\\nHumanEval Pass@1 (\u2191)\\n# gen. TCs (\u2193)\\n\\n| Code Gen. Method       | k = 1 | k = 2 | k = 5 |\\n|------------------------|-------|-------|-------|\\n| None                   | 6.73  | 9.79  | 14.63 |\\n| \\\\textsc{CODET}         | 11.09 | 13.59 | 17.18 |\\n| \\\\textsc{CODET} (w/o clustering) | 13.16 | 14.14 | 16.48 |\\n| \\\\textsc{A\\n\\\\textsc{RCHCODE}} | 16.52 | 16.67 | 17.37 |\\n\\nTest Case Generation Results on CodeContests, while the code generation method is fixed to \\\\textsc{A\\n\\\\textsc{RCHCODE}}. GPT-3.5-Turbo is used as the backbone model. MPSC is omitted as it is publicly unavailable. In contrast, \\\\textsc{A\\n\\\\textsc{RCHCODE}} targeting diverse requirement categories shows the best performance while significantly improving the efficiency by generating 50x smaller number of test cases.\\n\\nEffectiveness\\n\\nTables 4, 5, and 6 compare two test case generation methods, the naive way (\\\\textsc{CODET}) and \\\\textsc{A\\n\\\\textsc{RCHCODE}}. With the same code generation and filtering strategy applied, the latter generally outperforms the former with large margins, demonstrating the effectiveness of leveraging generated requirements to optimize test case generation. Meanwhile, \\\\textsc{A\\n\\\\textsc{RCHCODE}} yielded comparable results to \\\\textsc{CODET} without the use of clustering on HumanEval. We conjecture that for simpler benchmarks like HumanEval, \\\\textsc{CODET}'s approach of generating 'general' test cases suffices. While...\"}"}
{"id": "acl-2024-long-730", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Code filtering results with different test case generation methods on HumanEval-NFR (All). GPT-3.5-Turbo is used as the backbone model. MPSC is omitted as it is publicly unavailable.\\n\\n| Test Case | Code Generation Method |\\n|-----------|------------------------|\\n|           | GPT-3.5-Turbo          |\\n|           | GPT-3.5-Turbo + CoT     |\\n|           | ARCH CODE              |\\n| Generation Method | \\\\(k=1\\\\) | \\\\(2\\\\) | \\\\(5\\\\) | \\\\(k=1\\\\) | \\\\(2\\\\) | \\\\(5\\\\) |\\n| None      | 75.06 81.83 87.95      |\\n| CODE      | 79.92 87.63 91.00      |\\n| CODE T (w/o clustering) | 86.40 88.21 90.66 |\\n| ARCHCODE  | 86.36 88.62 90.48 |\\n\\nTable 6: Code filtering results with different test case generation methods on HumanEval, while the code generation method is fixed to ARCHCODE. GPT-3.5-Turbo is used as the backbone model. MPSC is omitted as it is publicly unavailable.\\n\\nCODE T focuses on general test cases which are likely to have limited coverage, ARCHCODE distinctly promotes a diverse set of test cases targeting various requirement (sub)types.\\n\\n5.2 Conditioning Code Generation on Test Cases\\n\\nIn contrast to our approach of generating code and test cases in parallel and then applying subsequent postprocess mechanisms such as filtering, one can also consider conditioning the code generation on test cases, taking inspiration from the Test-Driven Development (TDD; Beck, 2022) methodology. Table 7 shows results consistent with those reported in Chen et al. (2023), indicating marginal improvement in performance is observed when conditioning code generation on the ground-truth and generated test cases, while incorporating software requirements through ARCHCODE effectively boosts the score, even without code filtering. This suggests the overhead from introducing new sequential dependency in the generation process might not be worth the additional costs incurred.\\n\\n6 One may also consider the opposite direction of generating the test cases conditioned on the generated code, which we do not visit in this paper.\\n\\nTable 7: Results on HumanEval with generating code conditioned additionally on test cases. While incurring sequential dependency and increased latency, TDD-like conditioning brings marginal improvement, as opposed to our method being effective even without filtering.\\n\\nFigure 5: Pass@1 score of ARCHCODE for each requirement category in HumanEval-NFR. Using dedicated test cases for filtering consistently outperforms blindly using all test cases. Best viewed in color.\\n\\n5.3 Preference over Requirements\\n\\nAs mentioned before, ARCHCODE can be informed of any user preferences over the software requirements at code filtering time\u2014after several code candidates have been generated and awaiting to be ranked. Figure 5 presents the Pass@1 scores\u2014after several code candidates have been generated and awaiting to be ranked.\"}"}
{"id": "acl-2024-long-730", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for each NFR category in the HumanEval-NFR benchmark, with different code filtering strategies applied. Using targeted test cases for reranking yields higher pass rates for that specific software requirement than using all test cases does.\\n\\nAnother approach to incorporate user preference over the requirements is to consider a subset of requirements when generating code or test cases, or to put emphasis on a subset of requirements by editing the prompt while presenting all requirements to the model. We present detailed analyses for such scenarios in Appendix F.\\n\\n5.4 ARCH CODE under Diverse Settings\\n\\nHere we provide empirical results suggesting that ARCH CODE generalizes well to other models, datasets, etc. than those considered in the main experiments.\\n\\nOpen-source LLMs\\n\\nFirst, we showcase ARCH CODE combined with a relatively smaller model, namely WizardCoder 7B. Table 8 indicates that applying ARCH CODE with the said backbone model leads to a notable 15.67\\\\% improvement in Pass@1 on HumanEval, while incorporating in-context learning directly into WizardCoder 7B itself has negative impacts. Note that this observation is consistent with prior findings such as that in Yuan et al. (2023), that instruction tuning might compromise in-context learning capabilities of LLMs; WizardCoder 7B is an instruction-tuned model based on CODELAMA 7B.\\n\\nMeanwhile, in practical settings, diverse LLMs offer complementary benefits in terms of cost-performance trade-off, and thus mixing two models has been a conventional approach to explore cost-performance design space (Sun et al., 2023; Wang et al., 2023). ARCH CODE MIX shown in Tables 8 and 9 similarly capitalizes on this space by directing most of the generation calls to affordable LLMs, while selectively delegating the part requiring the most of the reasoning capabilities to stronger ones.\\n\\nOther Programming Languages\\n\\nWe also extend the evaluation of ARCH CODE to the task of Java code generation, using the MultiPL-E (Casasano et al., 2022) benchmark and the backbone model SantaCoder 1B (Allal et al., 2023). To address the rather limited capacity of a smaller model, we further applied sparse fine-tuning (Ansell et al. (2022); SFT) on a public Java train set. We provide more details in Appendix A.1. The results in Table 9 demonstrate the effectiveness of the proposed method in generating Java code, supporting that our method is generally applicable to programming languages other than Python.\\n\\n6 Conclusion\\n\\nWe proposed ARCH CODE, a framework incorporating software requirements from textual descriptions for LLM-based code generation. This systematic approach not only identifies these requirements but also harnesses them to guide the code generation process. The verification of code snippets with the generated test cases tailored to each requirement provides a robust validation layer for the alignment with detected requirements. On HumanEval and CodeContests, ARCH CODE with GPT-3.5-Turbo exceeded GPT-4's performance by 4.81\\\\% and 10.45\\\\% of Pass@1. ARCH CODE requires 50x less generated test cases compared to MPSC and CODET, while outperforming them. In addition, we introduced a new benchmark named HumanEval-NFR for evaluating how well LLMs can pursue non-functional requirements in code generation task. Further analysis shows the pertinence of parallel generation of code and test case, and the efficiency and the effectiveness of ARCH CODE's requirement-aware test case generation.\"}"}
{"id": "acl-2024-long-730", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgment\\nThis work was supported by LG AI Research, and partly supported by Electronics and Telecommunications Research Institute (ETRI) grant funded by ICT R&D program of MSIT/IITP (2022-0-00995, Automated reliable source code generation from natural language descriptions).\\n\\nLimitations\\nARCHCODE leverages in-context learning as a tool to integrate both functional and non-functional requirements in the processes of code and test case generation. We did not studied prompt engineering and devising more sophisticated in-context examples which is beyond the scope of this work.\\n\\nARCHCODE encompassed three functional and four non-functional requirements, aligning with the established taxonomy within software engineering literature (Glinz, 2007). However, the potential for future work lies in addressing more complex and varied requirements involving larger pieces of code, as well as accommodating changes in software requirements over time.\\n\\nLastly, as ARCHCODE relies on generated requirements to guide subsequent code and test case generation process, although qualitative analysis suggests its impact could be limited in practice, additional measures to mitigate cascading errors via human intervention or self-correction by LLMs, etc. (Shinn et al., 2023; Wang et al., 2023; Yao et al., 2023; Chen et al., 2024) can be necessitated.\\n\\nEthical and Social Implications\\nARCHCODE leverages LLMs to automatically generate software requirements, code, and test cases, thereby enhancing productivity and reducing manual labor for developers. However, to maximize these advantages while addressing potential risks, such as the creation of code with safety or security vulnerabilities as discussed in Chen et al. (2021), careful consideration is essential. Strategies to mitigate these risks include establishing default requirements for desired outcomes, delineating the permissible scope of generated code, and ensuring that the code remains within its authorized boundaries.\\n\\nReferences\\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988.\\n\\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vuli\u0107. 2022. Composable sparse fine-tuning for cross-lingual transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1778\u20131796, Dublin, Ireland. Association for Computational Linguistics.\\n\\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. CoRR, abs/2108.07732.\\n\\nLen Bass, Paul Clements, and Rick Kazman. 2003. Software architecture in practice. Addison-Wesley Professional.\\n\\nKent Beck. 2022. Test driven development: By example. Addison-Wesley Professional.\\n\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\\n\\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. Multipl-e: A scalable and extensible approach to benchmarking neural code generation. arXiv preprint arXiv:2208.08227.\\n\\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2023. Codet: Code generation with generated tests. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond\u00e9 de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummins, Matthias Plappert, Fotios Chantzis, Eliza-\"}"}
{"id": "acl-2024-long-730", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-730", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. In International Conference on Learning Representations.\\n\\nOpenAI. 2022. Chatgpt: Optimizing language models for dialogue. openai.\\n\\nOpenAI. 2023. Gpt-4 technical report.\\n\\nDewayne E Perry and Alexander L Wolf. 1992. Foundations for the study of software architecture. ACM SIGSOFT Software engineering notes, 17(4):40\u201352.\\n\\nHenry Gordon Rice. 1953. Classes of recursively enumerable sets and their decision problems. Transactions of the American Mathematical Society, 74(2):358\u2013366.\\n\\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org.\\n\\nFreda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. 2022a. Natural language to code translation with execution. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3533\u20133546, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nZhengxiang Shi, Yue Feng, and Aldo Lipani. 2022b. Learning to execute actions or ask clarification questions. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 2060\u20132070, Seattle, United States. Association for Computational Linguistics.\\n\\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems.\\n\\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT good at search? investigating large language models as re-ranking agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14918\u201314937, Singapore. Association for Computational Linguistics.\\n\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man- dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291.\\n\\nArthur Henry Watson, Dolores R Wallace, and Thomas J McCabe. 1996. Structured testing: A testing methodology using the cyclomatic complexity metric, volume 500. US Department of Commerce, Technology Administration, National Institute of . . . .\\n\\nChunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. 2023. Universal fuzzing via large language models. arXiv preprint arXiv:2308.04748.\\n\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Thirty-seventh Conference on Neural Information Processing Systems.\\n\\nZhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, and Yiling Lou. 2023. Evaluating instruction-tuned large language models on code comprehension and generation. arXiv preprint arXiv:2308.01240.\\n\\nKexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, and Lei Li. 2023a. ALGO: Synthesizing algorithmic programs with generated oracle verifiers. In Thirty-seventh Conference on Neural Information Processing Systems.\\n\\nTianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, and Sida Wang. 2023b. Coder reviewer reranking for code generation. In International Conference on Machine Learning, pages 41832\u201341846. PMLR.\\n\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023c. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations.\"}"}
{"id": "acl-2024-long-730", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Implementation Details\\n\\nWe used gpt-3.5-turbo-16k (OpenAI, 2022) as the backbone LLM for most of the experiments, with ICL and nucleus sampling (Holtzman et al., 2020) with $p = 0.95$ and temperature $T = 0.8$ following Chen et al. (2021); Nijkamp et al. (2023); Chen et al. (2023). We used different in-context examples for each benchmark: a single HumanEval-style (problem description, code) pair from Li et al. (2023) for HumanEval-NFR, eight pairs from the training set of the MBPP (Austin et al., 2021) benchmark for HumanEval (Chen et al., 2021). For CodeContests (Li et al., 2022) we used a single pair from the train set.\\n\\nTo apply CoT prompting (Kojima et al., 2022; Shao et al., 2023; Zhang et al., 2023c), as the state-of-the-art methods BRAINSTORM (Li et al., 2023) and ALGO (Zhang et al., 2023a) are publicly unavailable, we generated the reasoning chains of code outline by using self-planning (Jiang et al., 2023). However, directly using the reasoning chains provided by Self-planning can result in data contamination on HumanEval because these chains are based on the test examples. Thus, rather using them directly, we utilized them to generate the reasoning chains for the aforementioned in-context examples, then used the generated reasoning chains for ICL.\\n\\nARCHCODE uses three reasoning chains when generating code: the initial program outline (the same reasoning chains as in GPT-3.5-Turbo + CoT), requirements described in Subsection 3.1 and Appendix D, and the final program outline\u2014the revised version of the initial program outline, modified to meet the requirements.\\n\\nWe generated $n = 10$ code samples for every problem in the benchmarks. To enhance the diversity of the generated code, we employed nucleus sampling to produce $n$ initial program outlines induced from Self-planning. The rest of the reasoning chains were concurrently generated using greedy sampling, culminating in a total of $n$ final code outputs.\\n\\nOur implementation is largely based on the LangChain library.\\n\\nRegarding the execution and evaluation of the generated code, we modified some code from the CodeEval repository which is available on Huggingface.\\n\\nTable 10: Experimental results using WizardCoder 7B w/o ICL for code generation on CodeContests. 'ARCHCODE MIX' indicates that code filtering is applied with test cases generated by ARCHCODE using GPT-3.5-Turbo.\\n\\nRegarding the alignment of sub-requirements to the corresponding test cases for code filtering, we generate all test cases for all sub-requirements in one iteration to minimize LLM calls, as shown in Tables 32 and 34. This approach leverages formatted in-context examples from Tables 24 and 26. Subsequently, test cases are parsed and categorized according to corresponding sub-requirement types, followed by a test run with generated code snippets for code filtering.\\n\\nA.1 Open-sourced Backbone Models and Java Language\\n\\nOpen-source LLMs We utilized huggingface\u2019s text-generation-inference to parallelize WizardCoder 7B on two NVIDIA RTX A6000 48GBs for inference purposes exclusively. It took approximately one hour to experiment with one method on the entire HumanEval benchmark. Consistent to the results on HumanEval shown in Table 8, Table 10 also shows that ARCHCODE significantly contributes to Pass@$k$ scores on CodeContests.\\n\\nOther Programming Languages\\n\\nFor sparse fine-tuning, we followed Ansell et al. (2022) to train 3% of the SantaCoder 1B (Allal et al., 2023) parameters with the batch size of 8 (1*grad_accum of 8), the learning rate of 2e-5, the L1 regularization of 0, and the max training epochs of 3, using a single NVIDIA RTX A6000 48GB for 2 hours. For the training set, we utilized MegaCodeTraining, a public dataset set, while using java related data only.\\n\\nB HumanEval-NFR Construction\\n\\nThe HumanEval-NFR benchmark, an extension of HumanEval (Chen et al., 2021), evaluates both FRs.\"}"}
{"id": "acl-2024-long-730", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: The average number of ground truth test cases (GT TCs) per problem on HumanEval-NFR. Note that reliability is confirmed by checking whether other test cases completed gracefully (without errors), regardless of whether the output was correct. Regarding the maintainability, one test case was sufficient as we defined it as whether the generated code exhibits the specified level of Cyclomatic Complexity or not.\\n\\nWriting new ground truth test cases involved a two-step process. First, we generated candidate test cases based on the existing HumanEval problems using ARCHCODE based on GPT-3.5-Turbo. Second, we revised those test cases both in automatic or manual manner to ensure the quality of the test suite, based on the following protocols.\\n\\nB.1 Quality Control for FR Test Cases\\nFor candidate test cases evaluating FRs, we executed the ground truth code from the original HumanEval benchmark against each test case tailored to functional requirements. Those that the ground truth code does not pass were discarded.\\n\\nB.2 Quality Control for NFR Test Cases\\nFor candidate test cases verifying NFRs, three authors manually validated the quality of generated test cases. During validation, the authors adhered to the following principles:\\n\\n- Misclassified test cases should be rectified, and any duplicates should be eliminated.\\n- Test cases should compatible to the original ground truth code. If any discrepancy is found in the code, or if a test case is deemed impractical or overly complex, adjustments should be made to ensure it aligns with the original problem description.\\n\\nIn addition, the authors consider guidelines specific to each NFR category:\\n\\nTime Performance\\nAs Rice\u2019s Theorem (Rice, 1953) states, all non-trivial properties of Turing-recognizable languages are undecidable, which in essence means that there could be no \u2018time-complexity checkers.\u2019 Therefore, HumanEval-NFR follows conventional strategies used in competitive programming contests, such as Codeforces, where code is executed with relatively larger inputs to ensure that inefficient implementations cannot complete within the specified timeout. Specifically, we set the timeout as 5 seconds for all problems.\\n\\nRobustness\\nTest cases for this category verify whether the implementation gracefully handles diverse types of invalid inputs, such as a string passed as an argument where an integer is expected. For technical reasons, we expect the code to return values like `None`, an empty list, or `False`\u2014all of which are logically evaluated as False in the Python language\u2014rather than forcing it to raise exceptions or using any other means to indicate it has detected an abnormal input.\\n\\nMaintainability\\nTo validate maintainability, we consider code complexity, which affects the ease of understanding and updating the code (Magel et al., 1982). Specifically, HumanEval-NFR computes the Cyclomatic Complexity (CC; McCabe, 1976) of code, which evaluates code complexity by counting for the depth of nested indented blocks, then checks whether the observed CC score is lower than the threshold. The threshold is set to 5 if the ground truth code from the original HumanEval benchmark has a CC value below 5; if the CC value exceeds 5, we set the threshold as 10 (Watson et al., 1996).\\n\\nReliability\\nRather than generating dedicated test cases, HumanEval-NFR assesses code reliability by executing all the ground truth test cases for the problem and checks if any runtime errors are raised, without verifying if the outputs are correct. This approach aligns with the category\u2019s focus on minimizing system failures and extending the mean-time-to-failure.\"}"}
{"id": "acl-2024-long-730", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Experimental results of requirements instruction prompting on HumanEval-NFR. Boldfaced and underlined values indicate the 1st and 2nd largest scores, respectively. '+ NFR Instruction' means that the further prompt engineered instruction for NFR consideration shown in Table 13 is applied. \u2018Filtering\u2019 denotes an ablated version of ARCHCODE, without code filtering.\\n\\n# Non-functional Requirements\\n\\n## Performance: Pertains to time-centric aspects such as algorithmic time complexity or stipulated timeout conditions.\\n\\n## Robustness: Ensures that code is resilient to invalid inputs.\\n\\n## Maintainability: Considers factors that contribute to the ease of maintenance.\\n\\n## Reliability: Ensures that code can handle errors gracefully without causing system failures over an extended period.\\n\\nWrite a code for the problem.\\n\\nTable 13: Engineered prompt which further specified the details of each NFR, used in Table 12.\\n\\nC Gains from Prompt Engineering\\n\\nIn this study, we did not focus on devising sophisticated prompts, as our main contribution does not rely heavily on using prompt-engineered instructions. Therefore, we can expect even more performance gains when the prompt is further engineered as in Table 13, as we intentionally kept prompt simple in our main experiments.\\n\\nTable 12 shows that ARCHCODE is scalable to requirement instruction prompts, showing the best performance on HumanEval-NFR (All) when both are applied. Unlike CoT and NFR Instruction that improve Robustness and Reliability only, ARCHCODE contributes to all NFR types. Notably, time performance and maintainability are enhanced solely by ARCHCODE\u2019s code filtering, highlighting the unique contribution over prompt engineering.\\n\\nD Correctness of Generated Requirements\\n\\nFormat\\n\\nAs presented in Figure 6, we organized the structure of software requirements into two parts: problem-agnostic and problem-specific. The former describes general guidelines throughout problems related to reliability, performance, and maintainability. The latter includes more specific instructions depending on the problem description, including all three subtypes of functional requirements, the target time complexity for time performance, the invalid conditions for robustness, and the target Cyclomatic Complexity for maintainability.\\n\\nValidation\\n\\nTo confirm the correctness of the requirements generated by ARCHCODE with GPT-3.5-Turbo, we randomly selected three problems, one for each of the following categories: (1) all, (2) some or (3) none of the generated code samples passed the tests. We manually verified validity of each set of requirements for each case, of which the results are summarized in Table 17. Surprisingly, all the generated requirements were correct, regardless of the corresponding generated code\u2019s correctness.\\n\\nE Analysis of Generated Test Cases by ARCHCODE\\n\\nTable 14 shows the average number of generated test cases by ARCHCODE for each requirement category. Table 15 reports the accuracy of generated test cases by ARCHCODE for each requirement category.\"}"}
{"id": "acl-2024-long-730", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem Description\\nGenerated Requirements\\nCheck if in given list of numbers, are any two numbers closer to each other than given threshold.\\n\\n# Problem Agnostic Requirements\\n- Ensure error-free execution, avoiding type errors, index errors, arithmetic operation errors (e.g., division by zero, overflow), and so on.\\n- Demonstrate efficiency in the algorithmic approach to problem-solving.\\n- Code should be clear, well-commented, and easy to understand and maintain.\\n\\n# Functional Requirements\\n## Input\\n- **Output Conditions**\\nThe function takes a list of floating-point numbers `numbers` and a threshold value `threshold` as input.\\n- It returns a boolean value.\\n\\n## Expected Behavior\\n- The function should return True if there are any two numbers in the `numbers` list that are closer to each other than the given `threshold`.\\n- Otherwise, it should return False.\\n\\n## Edge Cases\\n- Handle an empty `numbers` list, returning False.\\n- Handle the case where no two numbers in `numbers` are closer to each other than the threshold, returning False.\\n- Handle the case where all numbers in `numbers` are equal, returning False.\\n\\n# Non-functional Requirements\\n## Performance\\n- Time complexity: O(n^2) where n is the length of the `numbers` list.\\n- Space complexity: O(1).\\n- Ensure efficiency even for extremely large inputs, providing results within 5 seconds.\\n\\n## Specific Quality Requirements\\n### Robustness\\n- If non-list `numbers` input or a non-float `threshold` input is provided, print an error message to `stderr` and return None.\\n- If non-float elements in the `numbers` list or a negative `threshold` is provided, print an error message to `stderr` and return None.\\n\\n### Maintainability\\n- Target Cyclomatic Complexity: \u2264 10.\\n\\nFigure 6: A real-life example of generated requirements from HumanEval-NFR/0 by A RCH CODE. Best viewed in color.\"}"}
{"id": "acl-2024-long-730", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: NFR preference control of A\\n\\n| Preference Control by Instruction | All | Time Perf. | All - Time Perf. |\\n|----------------------------------|-----|------------|-----------------|\\n| Time Perf.                      | 15.79 | 26.50 | 68.96 |\\n| Maintainability                  | 52.38 | 44.15 | 86.10 |\\n| Reliability                      | 65.56 | 44.15 | 95.18 |\\n| All                              | 25.30 | 40.22 | 45.55 |\\n| All - Time Perf.                 | 45.55 | 63.40 | 82.44 |\\n\\n| Preference Control by Plug-and-Play | All - Time Perf. | All - Time Perf. |\\n|-------------------------------------|-----------------|-----------------|\\n| Time Perf.                          | 17.50 | 28.74 | 66.69 |\\n| Maintainability                     | 52.01 | 45.82 | 68.13 |\\n| Reliability                         | 66.69 | 44.15 | 70.39 |\\n| All                                 | 27.68 | 43.63 | 49.82 |\\n| All - Time Perf.                    | 49.82 | 60.22 | 68.13 |\\n\\nBoldfaced and underlined values indicate the 1st and 2nd largest scores, respectively. All means all NFRs\u2014time performance, robustness, maintainability, and reliability\u2014are targeted, and All - Time Perf. means all NFRs except for time performance are targeted. In the 'Preference Control by Instruction' setting, all NFRs are included in the prompt, and an additional instruction to prioritize specific NFR(s) is appended. In the 'Plug-and-Play' setting, only targeted NFRs are included in the few-shot examples, while no preference among them is assumed.\\n\\nPreference Control by Plug-and-Play: We only present the preferred NFRs in prompts, without an explicit description of the preference. All included NFRs are considered equally important, with no prioritization among them.\\n\\nTable 16 shows that the plug-and-play approach inflicts a larger impact on Pass@k on HumanEval-NFR (All), compared to the instruction-based method. Notably, the plug-and-play approach considering all but time performance showed the best Pass@k scores, which we attribute to the trade-off between time performance and the rest of the NFRs: focusing on the other requirements is relatively free of negative interference that would hurt the performance. In the categories of time performance and robustness, both instruction and plug-and-play preference settings showed improvements when each of them was targeted. For the maintainability category, the best results were observed when only time performance was preferred in the plug-and-play approach. This is likely due to the omission of code lines for handling exceptions related to invalid inputs, as the robustness category was not considered. In the case of reliability, which is assessed by the error-free execution of code across all test cases (without dedicated test cases), performance improvement was observed irrespective of preference in the instruction-based approach. As demonstrated in Table 12, this suggests that prompt engineering can reduce error ratios; we leave exploration towards this direction to future work.\\n\\nG Varying Difficulty Levels of NFRs in HumanEval-NFR Orthogonal to A\\n\\nOrthogonal to A's contribution towards satisfying every requirement category, we observe a general trend of relatively low performance in the robustness category compared to others in HumanEval-NFR, as shown in Table 2 and Figure 5. One conjecture is that the difficulty lies among the NFRs as inferred from the original HumanEval benchmark. As the ground truth code snippets are generally not complex, handling large input size (time performance), managing small Cyclomatic Complexity (maintainability), and avoiding run-time error while running other test cases (reliability) might be easier than correctly handling every possible invalid input (robustness).\"}"}
{"id": "acl-2024-long-730", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Requirement Set | Correct Passes | Correct Fails |\\n|-----------------|---------------|--------------|\\n| HumanEval-NFR/51 | 9             | 0            |\\n|                 | 0 Correct failed: invalid literal for int() with base 2: '' |\\n| HumanEval-NFR/11 | 9             | 0            |\\n|                 | 0 Correct failed: invalid literal for int() with base 2: '' |\\n\\nTable 17: Validation of generated requirements by a generated code on HumanEval-NFR. Note that each generated code snippet is uniquely tailored to correspond with a distinct set of requirements, ensuring that no code snippet shares the same set of requirements.\"}"}
{"id": "acl-2024-long-730", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Description: remove_vowels is a function that takes a string and returns a string without vowels.\\n\\nGeneral:\\n- The function takes a string 'text' as input. Correct: The function takes a string with or without vowels as input.\\n- It returns a string. Correct: The function should return a string without vowels.\\n- The function should remove all vowels from the input 'text' and return the modified string. Correct: The function should return a string without vowels from the input string.\\n\\nEdge:\\n- Handle an empty 'text' string, returning an empty string. Correct: Input can be an empty string. In this case, the function should return an empty string.\\n- Handle the case where 'text' contains only vowels, returning an empty string. Correct: Input can contain only vowels. In this case, all vowels should be removed and the function should return an empty string.\\n- Handle the case where 'text' contains no vowels, returning the original string. Correct: Input can contain no vowels. In this case, there are nothing to be removed and the function should return the original string.\\n- Handle both lowercase and uppercase vowels. Correct: Input can be a string in lowercase and uppercase. Both of them should be handled.\\n\\nPerformance:\\n- Time complexity: $O(n)$ where $n$ is the length of the 'text' string. Correct: To traverse the string once and check whether each character is a vowel or not, and then add non-vowel characters to the result string, it requires $O(n)$ time, where $n$ is the length of the string.\\n- Space complexity: $O(n)$ where $n$ is the length of the 'text' string. Correct: Creating a new string to store the non-vowel characters requires additional space. If the length of the input string is $n$, the length of the resulting string can be at most $n$, leading to an $O(n)$ space complexity.\\n\\nRobustness:\\n- Ensure efficiency even for extremely large inputs, providing results within 5 seconds. Correct: The inputs should be handled in predetermined testing time.\\n\\nMaintainability:\\n- Target Cyclomatic Complexity: $\\\\leq 5$. Correct: 5 indicates low risk, where the code mainly consists of simple blocks.\"}"}
{"id": "acl-2024-long-730", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HumanEval-NFR/11\\n\\nDescription: Input are two strings a and b consisting only of 1s and 0s. Perform binary XOR on these inputs and return result also as a string.\\n\\nGeneral The function takes two strings 'a' and 'b' as input.\\nCorrect The function should take two strings consisting only of 1s and 0s.\\n\\nGeneral It returns a string. Correct The function should return string having result of a binary XOR operation on these inputs.\\n\\nGeneral The function should perform a binary XOR operation on the input strings 'a' and 'b' and return the result as a string.\\nCorrect The function takes two binary strings and return a result of a binary XOR operation.\\n\\nEdge Handle empty strings as input, returning an empty string.\\nCorrect The function should handle empty strings and return empty string.\\n\\nEdge Handle strings with different lengths, returning None.\\nCorrect When the two inputs have different length, a binary XOR operation is impossible.\\n\\nEdge Handle strings with characters other than '0' and '1', returning None.\\nCorrect When the two inputs are not valid binary string, a binary XOR operation is impossible.\\n\\nPerformance Time complexity: \\\\(O(n)\\\\) where \\\\(n\\\\) is the length of the longer input string.\\nCorrect The time complexity of performing binary XOR on two strings is proportional to the length of the strings.\\n\\nPerformance Space complexity: \\\\(O(n)\\\\) where \\\\(n\\\\) is the length of the longer input string.\\nCorrect If a new string is created to store the result, its length would be equal to the length of the input strings.\\n\\nPerformance Ensure efficiency even for extremely large inputs, providing results within 5 seconds.\\nCorrect The inputs should be handled in predetermined testing time.\\n\\nRobustness If non-string inputs are provided, print an error message to 'stderr' and return None.\\nCorrect Invalid inputs should be handled and return None.\\n\\nRobustness If the input strings contain characters other than '0' and '1', print an error message to 'stderr' and return None.\\nCorrect Invalid inputs should be handled and return None.\\n\\nMaintainability Target Cyclomatic Complexity: \\\\(\\\\leq 10\\\\). Correct 10 indicates low risk, where the code consists of well-structured and stable blocks.\"}"}
{"id": "acl-2024-long-730", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sub-requirement Generated Requirement Human Explanation Validation\\n\\nHumanEval-NFR/0\\n\\nDescription: Check if in given list of numbers, are any two numbers closer to each other than given threshold.\\n\\nGeneral\\nThe function takes a list of floating-point numbers 'numbers' and a threshold value 'threshold' as input.\\n\\nCorrect\\nThe function takes two inputs. The first is a list of floating-point numbers and the second is a threshold value.\\n\\nGeneral\\nIt returns a boolean value. Correct\\nThe function should return True if there are any two numbers in the 'numbers' list that are closer to each other than the given 'threshold'. Otherwise, it should return False.\\n\\nGeneral\\nIt should return 'True' if there are any two numbers in the 'numbers' list that are closer to each other than the given 'threshold'. Otherwise, it should return 'False'.\\n\\nCorrect\\nThe function checks if there are any two numbers closer to each other than given threshold in given list of numbers.\\n\\nEdge\\nHandle an empty 'numbers' list, returning 'False'.\\n\\nCorrect\\nThe function should return False if there are not any numbers in the 'numbers' list.\\n\\nEdge\\nHandle the case where no two numbers in 'numbers' are closer to each other than the 'threshold', returning 'False'.\\n\\nCorrect\\nThe function should return True if there are any two numbers in the 'numbers' list that are closer to each other than the given 'threshold'. Otherwise, it should return False.\\n\\nEdge\\nHandle the case where all numbers in 'numbers' are closer to each other than the 'threshold', returning 'True'.\\n\\nCorrect\\nThe function should return True if there are any two numbers in the 'numbers' list that are closer to each other than the given 'threshold'. Otherwise, it should return False.\\n\\nPerformance\\nTime complexity: $O(n^2)$ where $n$ is the length of the 'numbers' list.\\n\\nCorrect\\nAll elements of the given list need to be iterated through, and the difference between each pair of numbers needs to be calculated. This process is proportional to the size of the input list. Therefore, if we denote the length of the input list as $n$, the time complexity is $O(n^2)$.\\n\\nPerformance\\nSpace complexity: $O(1)$.\\n\\nCorrect\\nIf the problem is solved without using additional memory, the space complexity is $O(1)$.\\n\\nPerformance\\nEnsure efficiency even for extremely large inputs, providing results within 5 seconds.\\n\\nCorrect\\nThe inputs should be handled in predetermined testing time.\\n\\nRobustness\\nIf non-list 'numbers' input or a non-float 'threshold' input is provided, print an error message to 'stderr' and return None.\\n\\nCorrect\\nInvalid inputs should be handled and return None.\\n\\nRobustness\\nIf non-float elements in the 'numbers' list or a negative 'threshold' is provided, print an error message to 'stderr' and return None.\\n\\nCorrect\\nInvalid inputs should be handled and return None.\\n\\nMaintainability\\nTarget Cyclomatic Complexity: \u2264 10. Correct\\n10 indicates low risk, where the code consists of well-structured and stable blocks.\"}"}
{"id": "acl-2024-long-730", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"0 assert remove_vowels('') == '', 'Failed to handle an empty input string.'  \\nGeneral 10/10 Correct\\n1 assert remove_vowels(\"abcdef\\nghijklm\") == 'bcdf\\nghjklm', 'Failed to remove all vowels from the input string.'  \\nGeneral 10/10 Correct\\n2 assert remove_vowels('abcdef') == 'bcdf', 'Failed to remove vowels from the input string.'  \\nGeneral 10/10 Correct\\n3 assert remove_vowels('aaaaa') == '', 'Failed to handle case where the input string contains only vowels.'  \\nEdge 10/10 Correct\\n4 assert remove_vowels('aaBAA') == 'B', 'Failed to remove only lowercase vowels from the input string.'  \\nEdge 10/10 Correct\\n5 assert remove_vowels('zbcd') == 'zbcd', 'Failed to handle case where the input string contains no vowels.'  \\nEdge 10/10 Correct\\n6 assert remove_vowels('a' * 10**6) == '', 'Failed to handle large input size.'  \\nPerformance 10/10 Correct\\n7 assert remove_vowels('z' * 10**6) == 'z' * 10**6, 'Failed to handle large input size.'  \\nPerformance 10/10 Correct\\n8 assert remove_vowels(123) == None, 'Failed to handle case where the input is not a string.'  \\nRobustness 10/10 Correct\\n9 assert result.total_complexity <= 5, 'Failed to have a Cyclomatic Complexity less than or equal to 5 by Radon.'  \\nMaintainability 10/10 Correct\\n\\n0 assert starts_one_ends(1) == 9, 'Failed to count the numbers of 1-digit positive integers that start or end with 1.'  \\nGeneral 0/10 Incorrect\\n1 assert starts_one_ends(2) == 19, 'Failed to count the numbers of 2-digit positive integers that start or end with 1.'  \\nGeneral 0/10 Correct\\n2 assert starts_one_ends(3) == 271, 'Failed to count the numbers of 3-digit positive integers that start or end with 1.'  \\nGeneral 0/10 Incorrect\\n3 assert starts_one_ends(0) == 0, 'Failed to handle the case where n is 0.'  \\nEdge 0/10 Incorrect\\n4 assert starts_one_ends(-5) == 0, 'Failed to handle the case where n is negative.'  \\nEdge 0/10 Incorrect\\n5 assert starts_one_ends(10**6) == 900000, 'Failed to handle large input size.'  \\nPerformance 0/10 Incorrect\\n6 assert starts_one_ends('invalid') == None, 'Failed to handle case where the input n is not an integer.'  \\nRobustness 10/10 Correct\\n7 assert result.total_complexity <= 5, 'Failed to have a Cyclomatic Complexity less than or equal to 5 by Radon.'  \\nMaintainability 2/10 Correct\\n\\n0 assert make_a_pile(3) == [3, 5, 7], 'Failed to create the pile correctly.'  \\nGeneral 5/10 Correct\\n1 assert make_a_pile(5) == [5, 7, 9, 11, 13], 'Failed to create the pile correctly.'  \\nGeneral 5/10 Correct\\n2 assert make_a_pile(0) == [], 'Failed to handle the case where n is 0.'  \\nEdge 5/10 Correct\\n3 assert make_a_pile(-5) == None, 'Failed to handle the case where n is negative.'  \\nEdge 5/10 Correct\\n4 assert make_a_pile(10**6) == list(range(10**6, 10**6 + 2 * 10**6, 2)), 'Failed to handle large input size.'  \\nPerformance 0/10 Correct\\n5 assert make_a_pile('invalid') == None, 'Failed to handle the case where the input n is not an integer.'  \\nRobustness 5/10 Correct\\n6 assert make_a_pile(0.5) == None, 'Failed to handle the case where the input n is not a positive integer.'  \\nRobustness 5/10 Correct\\n7 assert result.total_complexity <= 10, 'Failed to have a Cyclomatic Complexity less than or equal to 10 by Radon.'  \\nMaintainability 10/10 Correct\"}"}
{"id": "acl-2024-long-730", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Problem Agnostic Requirements\\n\\n- Ensure error-free execution, avoiding type errors, arithmetic operation errors (e.g., division by zero, overflow), and so on.\\n- Demonstrate efficiency in the algorithmic approach to problem-solving.\\n- Code should be clear, well-commented, and easy to understand and maintain.\\n\\n# Functional Requirements\\n\\n## Input-output Conditions\\n\\n- The function takes an integer 'decimal' as input.\\n- It returns a string.\\n\\n## Expected Behavior\\n\\n- The function should convert the given decimal number to binary and return a string representation of the binary number.\\n- The resulting string should have the characters 'db' at the beginning and end.\\n- The string should have the format \\\"db\\\" + binary representation + \\\"db\\\".\\n- Each character in the binary representation should be '0' or '1'.\\n\\n## Edge Cases\\n\\n- Handle negative decimal numbers, returning the binary representation with a '-' sign.\\n- Handle the case where 'decimal' is 0, returning \\\"db0db\\\".\\n- Handle decimal numbers equal to 0, returning the string \\\"db0db\\\".\\n- Handle the case where 'decimal' is negative, returning None.\\n- Handle the case where 'decimal' is not an integer, returning None.\\n\\n# Non-functional Requirements\\n\\n## Specific Quality Requirements\\n\\n## Performance\\n\\n- Time complexity: O(log n) where n is the value of the 'decimal' input.\\n- Space complexity: O(log n) where n is the value of the 'decimal' input.\\n- Ensure efficiency even for extremely large inputs, providing results within 5 seconds.\\n\\n### Robustness\\n\\n- If a non-integer 'decimal' input is provided, print an error message to 'stderr' and return None.\\n- If a negative 'decimal' input is provided, print an error message to 'stderr' and return None.\\n\\n### Maintainability\\n\\n- Target Cyclomatic Complexity \u2264 5.\"}"}
{"id": "acl-2024-long-730", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Preferred NFR(s)\\nTime Perf. All - Time Perf.\\nimport sys import sys\\nimport math import math\\nimport re import re\\nimport numpy import numpy\\nimport numpy as np import numpy as np\\nfrom typing import *\\n\\ndef decimal_to_binary(decimal):\\n    binary = bin(decimal)[2:] # Convert decimal to binary and remove '0b' prefix\\n    # Input Validation\\n    return 'db' + binary + 'db' # Add 'db' prefix and suffix to the binary representation\\n    if not isinstance(decimal, int):\\n        sys.stderr.write('Invalid input: decimal must be an integer.')\\n        return None\\n    # Initialize variables\\n    binary = ''\\n    # Handle special case for 0\\n    if decimal == 0:\\n        return 'db0db'\\n    # Convert decimal to binary\\n    while decimal > 0:\\n        binary += str(decimal % 2)\\n        decimal //= 2\\n    # Reverse the binary string\\n    binary = binary[::-1]\\n    # Add prefix and suffix\\n    binary = 'db' + binary + 'db'\\n    return binary\\n\\nTable 23: Real life examples of generated code snippets for HumanEval-NFR/79 by ARCH CODE with Plug-and-Play NFR Preference Control. The conditioned requirements are shown in Table 22.\"}"}
{"id": "acl-2024-long-730", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem Description Example\\nfrom typing import List\\ndef longest_subarray_with_sum_limit(nums: List[int], target: int) -> List[int]:\\n    \"\"\"Find the longest subarray of the given array 'nums' such that the sum of its elements is less than or equal to the given 'target'.\\n    >>> longest_subarray_with_sum_limit([3, 1, 5, 2, 1, 9, 7, 4], 10)\\n    [3, 1, 5]\\n    >>> longest_subarray_with_sum_limit([1, 2, 3, 4, 5], 3)\\n    [1, 2]\\n    \"\"\"\\n\\nRequirement Generation Prompt\\nWrite requirements for the problem.\\n\\nRequirement Generation Example\\n# Problem Agnostic Requirements\\n- Ensure error-free execution, avoiding type errors, index errors, arithmetic operation errors (e.g., division by zero, overflow), and so on.\\n- Demonstrate efficiency in the algorithmic approach to problem-solving.\\n- Code should be clear, well-commented, and easy to understand and maintain.\\n\\n# Functional Requirements\\n## Input-output Conditions\\n- The function takes a list of integers 'nums' and a target integer 'target' as input.\\n- It returns a list of integers.\\n\\n## Expected Behavior\\n- The function should return the longest subarray of 'nums' whose sum is less than or equal to 'target'.\\n\\n## Edge Cases\\n- Handle an empty 'nums' list, returning an empty list.\\n- Handle the case where no subarray in 'nums' satisfies the condition, returning an empty list.\\n- Handle multiple subarrays with the same maximum length, returning any of them.\\n\\n# Non-functional Requirements\\n## Performance\\n- Time complexity: O(n) where n is the length of the 'nums' list.\\n- Space complexity: O(1).\\n- Ensure efficiency even for extremely large inputs, providing results within 5 seconds.\\n\\n## Specific Quality Requirements\\n### Robustness\\n- If non-list 'nums' input or a non-integer 'target' input is provided, print an error message to 'stderr' and return None.\\n- If non-integer elements in the 'nums' list or a negative 'target' is provided, print an error message to 'stderr' and return None.\\n\\n### Reliability\\n- Avoid index errors while accessing 'nums' elements.\\n\\n### Maintainability\\n- Target Cyclomatic Complexity \u2264 10.\\n\\nTable 24: The in-context example for requirement generation by ARCH CODE. The problem description example is from Shinn et al. (2023).\"}"}
{"id": "acl-2024-long-730", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from typing import List, Optional\\nimport sys\\ndef longest_subarray_with_sum_limit(nums: Optional[List[int]], target: Optional[int]) -> List[int]:\\n    # Validate inputs\\n    if not isinstance(nums, list) or not all(isinstance(num, int) for num in nums):\\n        sys.stderr.write('Invalid input: nums must be a list of integers.')\\n        return\\n    if not isinstance(target, int):\\n        sys.stderr.write('Invalid input: target must be an integer.')\\n        return\\n    n = len(nums)\\n    left, right = 0, 0\\n    max_length = 0\\n    current_sum = 0\\n    result = []\\n    # Handle empty list\\n    if n == 0:\\n        return []\\n    # Initial window expansion\\n    while right < n and current_sum + nums[right] <= target:\\n        current_sum += nums[right]\\n        right += 1\\n    # Update result if a valid window was found during initial expansion\\n    if right > 0:\\n        result = nums[left:right]\\n        max_length = right\\n    # Main loop: Slide the window through the array\\n    while right < n:\\n        current_sum += nums[right]\\n        # Shrink window from the left if current_sum exceeds target\\n        while current_sum > target:\\n            current_sum -= nums[left]\\n            left += 1\\n        # Update result if a longer valid window is found\\n        if right - left + 1 > max_length:\\n            max_length = right - left + 1\\n            result = nums[left:right+1]\\n        right += 1\\n    return result\\n\\nTable 25: The in-context example for code generation by ARCH. The skipped contents\u2014Problem Description Example and Requirement Generation Example\u2014are available in Table 24.\"}"}
{"id": "acl-2024-long-730", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem Description Example\\n\\n```\\nfrom typing import List ...\\n```\\n\\nRequirement Generation Prompt\\nWrite requirements for the problem.\\n\\nRequirement Generation Example\\n\\n```\\n# Problem Agnostic Requirements ...\\n```\\n\\nTest Case Generation Prompt\\nWrite test cases for the problem.\\n\\nTest Case Generation Example\\n\\n```\\n# Test Cases Regarding Functional Requirements\\n\\n## General Cases\\n\\n### The longest subarray with sum less than or equal to 10 is \\\\[1, 5, 2, 1\\\\]\\nassert longest_subarray_with_sum_limit([3, 1, 5, 2, 1, 9, 7, 4], 10) == [1, 5, 2, 1], 'Failed to find the longest subarray.'\\n\\n### None of the subarrays have a sum less than or equal to 3\\n\\n### The function should return an empty list\\nassert longest_subarray_with_sum_limit([2, 3, 4, 5, 6], 1) == [], 'Failed to handle case where no subarray satisfies the condition.'\\n\\n## Edge Cases\\n\\n### The input list is empty, so the function should return an empty list\\nassert longest_subarray_with_sum_limit([], 5) == [], 'Failed to handle an empty input list.'\\n\\n### The longest subarray with sum less than or equal to 10 is \\\\[5, 5\\\\]\\nassert longest_subarray_with_sum_limit([5, 5, 5, 5], 10) == [5, 5], 'Failed to find the longest subarray when all elements have the same value.'\\n\\n### The entire nums array is a valid subarray with sum less than or equal to 15\\nassert longest_subarray_with_sum_limit([1, 2, 3, 4, 5], 15) == [1, 2, 3, 4, 5], 'Failed to handle case where the entire array is a valid subarray.'\\n\\n# Test Cases Regarding Non-functional Requirements\\n\\n## Performance Requirements\\n\\n### The nums list contains 10^6 elements with increasing values from 1 to 10^6\\nassert longest_subarray_with_sum_limit(list(range(1, 10**6 + 1)), 10**5) == list(range(1, 446 + 1)), 'Failed to handle large input size.'\\n\\n### The nums list contains 10^6 elements with increasing values from 1 to 10^6\\n### The longest subarray with sum less than or equal to 2 is \\\\[1\\\\]\\nassert longest_subarray_with_sum_limit(list(range(1, 10**6 + 1)), 2) == [1], 'Failed to handle case where the subarray length is 1.'\\n\\n### The nums list contains 10^6 elements with increasing values from 1 to 10^6\\n### The longest subarray with sum less than or equal to 5 is \\\\[1, 2\\\\]\\nassert longest_subarray_with_sum_limit(list(range(1, 10**6 + 1)), 5) == [1, 2], 'Failed to handle case where the subarray length is 2.'\\n\\n### The nums list contains 10^6 elements, all of which are 10^6\\n### The sum of all the elements in nums is 10^12, which is larger than the target\\n### Therefore, there is no subarray that satisfies the condition and the function should return an empty list\\nassert longest_subarray_with_sum_limit([10**8] * 10**6, 10**7) == [], 'Failed to handle case where no subarray satisfies the condition.'\\n\\n### The nums list contains 10^6 elements, all of which are 10^6\\n### The target is the sum of all the elements in nums, so the entire array is a valid subarray\\nassert longest_subarray_with_sum_limit(list(range(1, 10**5 + 1)), 10**10) == list(range(1, 10**5 + 1)), 'Failed to handle case where the entire array is a valid subarray.'\\n\\n## Specific Quality Requirements\\n\\n### Robustness\\n\\n#### The sum of the elements in nums is larger than the target,\\n#### so the function should return None\\nassert longest_subarray_with_sum_limit([10**20, 10**20], 10**19) == None, 'Failed to handle case where the sum of the elements in nums is larger than the target.'\\n\\n#### The nums input is not a list of integers, so the function should return None\\nassert longest_subarray_with_sum_limit('invalid', 10) == None, 'Failed to handle case where the input nums is not a list.'\\n\\n#### The target input is not an integer, so the function should return None\\nassert longest_subarray_with_sum_limit([1, 2, 3], 'invalid') == None, 'Failed to handle case where the input target is not an integer.'\\n\\n#### The nums list contains elements that are not integers, so the function should return None\\nassert longest_subarray_with_sum_limit([1, 2, 'invalid', 4], 5) == None, 'Failed to handle case where the input nums contains non-integer elements.'\\n\\n#### The target is a negative number, so the function should return None\\nassert longest_subarray_with_sum_limit([1, 2, 3, 4, 5], -10) == None, 'Failed to handle case where the input target is negative.'\\n\\n### Reliability\\n\\n#### Satisfied if no errors occur across all test cases\\n\\n### Maintainability\\n\\n#### Calculate Cyclomatic Complexity using Radon\\n#### Check if the Cyclomatic Complexity is less than or equal to 10\\nfrom radon.visitors import ComplexityVisitor\\nresult = ComplexityVisitor.from_code('''\n${Generated Code}\n''')\\nassert result.total_complexity <= 10, 'Failed to have a Cyclomatic Complexity less than or equal to 10 by Radon.'\"}"}
{"id": "acl-2024-long-730", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In-Context Learning Prompt Templates\\n\\n{Problem Description Example}\\nWrite requirements for the problem.\\n\\n{Requirement Generation Example}\\n\\n{Test Problem Description}\\nWrite requirements for the problem.\\n\\nTable 27: The in-context learning prompt template for requirement generation by ARCHCODE.\\n\\n{Problem Description Example}\\nWrite requirements for the problem.\\n\\n{Requirement Generation Example}\\nWrite the code for the problem.\\n\\nTable 28: The in-context learning prompt template for code generation by ARCHCODE.\\n\\n{Test Problem Description}\\nWrite requirements for the problem.\\n\\n{Generated Requirements by ARCHCODE}\\nWrite the code for the problem.\\n\\nTable 29: The in-context learning prompt template for test case generation by ARCHCODE.\\n\\n{Problem Description Example}\\nWrite the code for the problem.\\n\\n{Code Generation Example}\\n\\n{Test Problem Description}\\nWrite the code for the problem.\\n\\nTable 30: The in-context learning prompt template for code generation by GPT-3.5-Turbo baseline.\\n\\n{Problem Description Example}\\nWrite the plan for the problem.\\n\\n{Chain-of-Thought Example}\\nWrite the code for the problem.\\n\\nTable 31: The in-context learning prompt template for code generation by GPT-3.5-Turbo + CoT baseline.\"}"}
{"id": "acl-2024-long-730", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Problem Agnostic Requirements\\n- Ensure error-free execution, avoiding type errors, index errors, arithmetic operation errors (e.g., division by zero, overflow), and so on.\\n- Demonstrate efficiency in the algorithmic approach to problem-solving.\\n- Code should be clear, well-commented, and easy to understand and maintain.\\n\\n# Functional Requirements\\n## Input-output Conditions\\n- The function takes two integers 'x' and 'base' as input.\\n- It returns a string.\\n\\n## Expected Behavior\\n- The function should convert the number 'x' to the given 'base' and return the string representation of the converted number.\\n\\n## Edge Cases\\n- Handle the case where 'x' is 0, returning '0'.\\n- Handle the case where 'base' is 1, returning a string of '1' repeated 'x' times.\\n- Handle the case where 'base' is less than 2 or greater than 9, returning None.\\n- Handle the case where 'x' is negative, returning None.\\n\\n# Non-functional Requirements\\n## Performance\\n- Time complexity: O(log(x)) where x is the value of 'x'.\\n- Space complexity: O(log(x)) to store the result string.\\n- Ensure efficiency even for extremely large inputs, providing results within 5 seconds.\\n\\n## Specific Quality Requirements\\n### Robustness\\n- If a non-integer 'x' or 'base' input is provided, print an error message to 'stderr' and return None.\\n- If a negative 'base' is provided, print an error message to 'stderr' and return None.\\n\\n### Maintainability\\n- Target Cyclomatic Complexity: \\\\( \\\\leq 10 \\\\).\\n\\nTable 32: A real-life example of generated requirements for HumanEval-NFR/44 by ARCH CODE.\\n\\nTable 33: A real-life example of generated code for HumanEval-NFR/44 by ARCH CODE.\"}"}
{"id": "acl-2024-long-730", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Test Cases Regarding Functional Requirements\\n\\n## General Cases\\n\\n### The number 8 in base 3 is represented as '22'\\n\\n```python\\nassert change_base(8, 3) == '22', 'Failed to convert number to the desired base.'\\n```\\n\\n### The number 8 in base 2 is represented as '1000'\\n\\n```python\\nassert change_base(8, 2) == '1000', 'Failed to convert number to the desired base.'\\n```\\n\\n### The number 7 in base 2 is represented as '111'\\n\\n```python\\nassert change_base(7, 2) == '111', 'Failed to convert number to the desired base.'\\n```\\n\\n## Edge Cases\\n\\n### The input number is 0, so the function should return '0' in any base\\n\\n```python\\nassert change_base(0, 3) == '0', 'Failed to handle case where the input number is 0.'\\n```\\n\\n### The input base is 1, which is not a valid base, so the function should return None\\n\\n```python\\nassert change_base(8, 1) is None, 'Failed to handle case where the base is not a valid base.'\\n```\\n\\n### The input number is negative, so the function should return None\\n\\n```python\\nassert change_base(-8, 2) is None, 'Failed to handle case where the input number is negative.'\\n```\\n\\n### The input base is greater than 9, which is not a valid base, so the function should return None\\n\\n```python\\nassert change_base(8, 10) is None, 'Failed to handle case where the base is not a valid base.'\\n```\\n\\n# Test Cases Regarding Non-functional Requirements\\n\\n## Performance Requirements\\n\\n### The input number is \\\\(10^6\\\\) and the base is 2\\n\\n```python\\nassert change_base(10**6, 2) == '11110100001001000000', 'Failed to handle large input size.'\\n```\\n\\n### The input number is \\\\(10^6\\\\) and the base is 9\\n\\n```python\\nassert change_base(10**6, 9) == '1783661', 'Failed to handle large input size.'\\n```\\n\\n### The input number is \\\\(10^{18}\\\\) and the base is 2\\n\\n```python\\nassert change_base(10**18, 2) == '110111100000101101101011001110100111011001000000000000000000', 'Failed to handle large input size.'\\n```\\n\\n## Specific Quality Requirements\\n\\n### Robustness\\n\\n#### The input number is not an integer, so the function should return None\\n\\n```python\\nassert change_base('invalid', 2) is None, 'Failed to handle case where the input number is not an integer.'\\n```\\n\\n#### The input base is not an integer, so the function should return None\\n\\n```python\\nassert change_base(8, 'invalid') is None, 'Failed to handle case where the input base is not an integer.'\\n```\\n\\n### Reliability\\n\\n#### Satisfied if no errors occur across all test cases\\n\\n### Maintainability\\n\\n#### Calculate Cyclomatic Complexity using Radon\\n\\n```python\\nimport inspect\\nfrom radon.visitors import ComplexityVisitor\\nresult = ComplexityVisitor.from_code('Generated Code')\\nassert result.total_complexity <= 10, 'Failed to have a Cyclomatic Complexity less than or equal to 10 by Radon.'\\n```\\n\\nTable 34: A real-life example of generated test cases for HumanEval-NFR/44 by ARCH CODE.\\n\\n${Generated Code}$ denotes the string text of the code that is to be checked.\"}"}
{"id": "acl-2024-long-730", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Problem Agnostic Requirements\\n- Ensure execution is error-free, mitigating type errors, index errors, and arithmetic operation errors (e.g., division by zero, overflow) among others.\\n- Showcase efficiency in the algorithmic approach to problem-solving.\\n- Ensure code is clear, well-commented, and both easy to understand and maintain.\\n\\n## Functional Requirements\\n### Input-output Conditions\\n#### Inputs\\n- Initial input values: \\\\( n \\\\) and \\\\( m \\\\)\\n- Must be positioned in the first line of input\\n- Adhere to the format: \\\"tnum\\\"\\n- Integer \\\\( n \\\\) range: \\\\((0 \\\\leq n \\\\leq 2000)\\\\)\\n- Integer \\\\( m \\\\) range: \\\\((0 \\\\leq m \\\\leq 2000)\\\\)\\n- Subsequent input values: grid denoting the positions of telephone poles\\n- Each input line format: \\\"tai,1 utai,2 utai,m`\\n- Integer \\\\( a_{i,j} \\\\) range: \\\\((0 \\\\leq a_{i,j} \\\\leq 1)\\\\)\\n- Each line represents a row in the grid\\n- There is at least one telephone pole in the given grid.\\n\\n#### Outputs\\n- Output must be a single integer denoting the value of \\\\( \\\\sum_{x=0}^{n-1} \\\\sum_{y=0}^{m-1} S_{px, y} \\\\)\\n\\n### Expected Behavior\\n- The city is represented as a plane.\\n- The plane is represented by a grid of size \\\\( p \\\\times q \\\\).\\n- Each point \\\\( px, y \\\\) on the plane has a square of the Euclidean distance between the nearest pole and \\\\( px, y \\\\) denoted as \\\\( S_{px, y} \\\\).\\n- \\\\( S_{px, y} \\\\) is defined as \\\\( px^2 \\\\cdot y^1 + px^2 \\\\cdot y^2 \\\\), where \\\\( px^1, y^1 \\\\) is the nearest pole and \\\\( px^2, y^2 \\\\) is \\\\( px, y \\\\).\\n- The code must calculate the sum of all \\\\( S_{px, y} \\\\) for each \\\\( 0 \\\\leq x \\\\leq n \\\\) and \\\\( 0 \\\\leq y \\\\leq m \\\\).\\n- The code must output the value of \\\\( \\\\sum_{x=0}^{n-1} \\\\sum_{y=0}^{m-1} S_{px, y} \\\\) as a single integer.\\n\\n### Edge Cases\\nEnsure the code functions accurately, even if:\\n- Inputs reach the condition boundaries\\n- \\\\( n \\\\) or \\\\( m \\\\) is 0\\n- \\\\( n \\\\) or \\\\( m \\\\) is the maximum value\\n- There is only one telephone pole in the grid\\n\\n## Non-functional Requirements\\n### Performance\\n- Test time limit: 2 seconds\\n- Test memory limit: 256 megabytes\\n\\n### Specific Quality Requirements\\n#### Robustness\\nEnsure the code operates correctly even if:\\n- Input contains redundant spaces or newlines\\n- Enforce the code to print an appropriate error message if:\\n  - Input format is incorrect\\n  - Input data type is invalid\\n  - Input is out of the specified range\\n  - The grid does not contain at least one telephone pole.\\n\\n#### Reliability\\n- Mitigate type errors during input and output stream management.\\n- Prevent index errors while managing grid values.\\n\\n#### Maintainability\\n- Aim for a Cyclomatic Complexity of \\\\( \\\\leq 10 \\\\).\"}"}
{"id": "acl-2024-long-730", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"import sys\\nimport math\\ndef read_line():\\n    input_str = \\\"\\\"\\n    while not input_str:\\n        input_str = input().strip()\\n    return input_str\\n\\nline = read_line()\\nn, m = map(int, line.split())\\nif not (0 <= n < 2000 and 0 <= m < 2000):\\n    sys.stderr.write(\\\"error at the boundary of n or m\\\")\\n    exit()\\n\\ngrid = [[0] * (m + 1) for _ in range(n + 1)]\\nfor i in range(n + 1):\\n    line = read_line()\\n    if len(line) != m + 1:\\n        sys.stderr.write(\\\"error at the boundary of the grid\\\")\\n        exit()\\n    for j in range(m + 1):\\n        if not (0 <= int(line[j]) <= 1):\\n            sys.stderr.write(\\\"error at the boundary of the grid\\\")\\n            exit()\\n        grid[i][j] = int(line[j])\\n\\nresult = 0\\nfor i in range(n + 1):\\n    for j in range(m + 1):\\n        if grid[i][j] == 1:\\n            continue\\n        minDistance = math.inf\\n        for x in range(n + 1):\\n            for y in range(m + 1):\\n                if grid[x][y] == 1:\\n                    distance = (x - i) ** 2 + (y - j) ** 2\\n                    minDistance = min(minDistance, distance)\\n        result += minDistance\\n\\nprint(result)\\n\\nTable 36: A real-life example of generated code for CodeContests/2 (1575_M. Managing Telephone Poles) by ARCH CODE.\"}"}
{"id": "acl-2024-long-730", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Test Cases Regarding Functional Requirements\\n\\n## General Cases\\nERROR_MESSAGE = \\\"general case error\\\"\\nINPUT = \\\"2 2\\n101\\n000\\n000\\\"\\nOUTPUT = \\\"18\\\"\\n\\nERROR_MESSAGE = \\\"general case error\\\"\\nINPUT = \\\"5 4\\n10010\\n00000\\n01000\\n00001\\n00100\\n00010\\\"\\nOUTPUT = \\\"36\\\"\\n\\n## Edge Cases\\nERROR_MESSAGE = \\\"error at the lower boundary of n\\\"\\nINPUT = \\\"0 2\\n00\\n00\\n00\\\"\\nOUTPUT = \\\"0\\\"\\n\\nERROR_MESSAGE = \\\"error at the upper boundary of n\\\"\\nINPUT = \\\"1999 2\\n0\\\" * 2000 + \\\"\\n0\\\" * 2000\\nOUTPUT = \\\"0\\\"\\n\\nERROR_MESSAGE = \\\"error at the lower boundary of m\\\"\\nINPUT = \\\"2 0\\n0\\n0\\n0\\\"\\nOUTPUT = \\\"0\\\"\\n\\nERROR_MESSAGE = \\\"error at the upper boundary of m\\\"\\nINPUT = \\\"2 1999\\n0\\\"\\nOUTPUT = \\\"0\\\"\\n\\nERROR_MESSAGE = \\\"error when a_{i, j} is not in the correct format\\\"\\nINPUT = \\\"2 2\\n101\\n000\\n00\\\"\\nSTD_ERROR = \\\"a_{i, j} is not in the correct format\\\"\\n\\nERROR_MESSAGE = \\\"error when a_{i, j} is not in the correct range\\\"\\nINPUT = \\\"2 2\\n101\\n000\\n002\\\"\\nSTD_ERROR = \\\"a_{i, j} is not in the correct range\\\"\\n\\nERROR_MESSAGE = \\\"error when there is no telephone pole in the given grid\\\"\\nINPUT = \\\"2 2\\n000\\n000\\n000\\\"\\nSTD_ERROR = \\\"there is no telephone pole in the given grid\\\"\\n\\n# Test Cases Regarding Non-functional Requirements\\n\\n## Performance Requirements\\nERROR_MESSAGE = \\\"execution failed while running exceptionally large input\\\"\\nINPUT = \\\"1999 1999\\n1\\\" * 2000 + \\\"\\n1\\\" * 2000\\nOUTPUT = \\\"0\\\"\\n\\n## Specific Quality Requirements\\n\\n### Robustness\\nERROR_MESSAGE = \\\"error when redundant spaces or newlines are added to the input\\\"\\nINPUT = \\\"2 2\\n101\\n000\\n\\n\\n\\\"\\nOUTPUT = \\\"18\\\"\\n\\nERROR_MESSAGE = \\\"error when the input is not in the correct format\\\"\\nINPUT = \\\"2 2\\n101\\n000\\n00\\\"\\nSTD_ERROR = \\\"the input is not in the correct format\\\"\\n\\nERROR_MESSAGE = \\\"error when the data type of the input is not correct\\\"\\nINPUT = \\\"2 2\\n101\\n000\\n00a\\\"\\nSTD_ERROR = \\\"the data type of the input is not correct\\\"\\n\\nERROR_MESSAGE = \\\"error when the input is not in the correct range\\\"\\nINPUT = \\\"2 2\\n101\\n000\\n003\\\"\\nSTD_ERROR = \\\"the input is not in the correct range\\\"\\n\\nERROR_MESSAGE = \\\"error when there is no telephone pole in the given grid\\\"\\nINPUT = \\\"2 2\\n000\\n000\\n000\\\"\\nSTD_ERROR = \\\"there is no telephone pole in the given grid\\\"\\n\\n## Reliability\\nSatisfied if no errors occur across all test cases\\n\\n## Maintainability\\nERROR_MESSAGE = \\\"error when cyclomatic complexity is more than the limit\\\"\\nCOMPLEXITY_LIMIT = 10\\n\\n${Generated Code} denotes the string text of the code that is to be checked.\"}"}
