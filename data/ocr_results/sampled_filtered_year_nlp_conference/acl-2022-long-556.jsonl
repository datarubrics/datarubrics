{"id": "acl-2022-long-556", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fantastically Ordered Prompts and Where to Find Them:\\nOvercoming Few-Shot Prompt Order Sensitivity\\n\\nYao Lu\u2020 Max Bartolo\u2020 Alastair Moore\u2021 Sebastian Riedel\u2020 Pontus Stenetorp\u2020\\n\\n\u2020University College London\\n\u2021Mishcon de Reya LLP\\n\\n{yao.lu,m.bartolo,s.riedel,p.stenetorp}@cs.ucl.ac.uk\\nalastair.moore@mishcon.com\\n\\nAbstract\\nWhen primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \u201cfantastic\u201d and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.\\n\\n1 Introduction\\nLarge pretrained language models (PLMs, Devlin et al., 2019; Peters et al., 2018; Raffel et al., 2020; Liu et al., 2019; Yang et al., 2019; Radford et al., 2019) have shown remarkable performance when conditioned with an appropriate textual context (Petroni et al., 2019, 2020; Jiang et al., 2020; Shin et al., 2020; Davison et al., 2019). For example, when conditioned on a long document and a \u201cTL;DR:\u201d token, they can generate a summary of said document, and when provided a partial question (\u201cThe theory of relativity was developed by __\u201d), they can generate the correct answer. Perhaps most strikingly, when primed with a context consisting of very few training examples, they produce text classification results that can match those of fully supervised models. This type of few shot setting is commonly referred to as \u201cIn-context Learning\u201d (Brown et al., 2020).\\n\\nA core component of in-context learning is the text-based prompt that serves as the context. Composing a prompt requires: (i) text linearisation using a template; and (ii) training sample concatenation (See Table 1 for an example). It has been established that the structure of the template has a large impact on performance (Shin et al., 2020; Gao et al., 2020; Schick and Sch\u00fctze, 2020; Jiang et al., 2020). However, to the best of our knowledge, no work has studied the effect of the sample ordering on In-context Learning performance.\\n\\nPerhaps counter-intuitively, we find that the right sample order can make as much of a difference as...\"}"}
{"id": "acl-2022-long-556", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Procedures for prompt construction.\\n\\nthe right template. As can be seen in Figure 1, some permutations have comparable performance (over 85% accuracy) to supervised training for sentiment classification, while others perform close to random (around 50%). This order sensitivity is universal across models, and although increasing the model size somewhat addresses it, the problem is still present for some text classification tasks (Subj in Figure 1) for models with billions of parameters.\\n\\nIn our analysis, we find no common denominator between performant sample orders and that they are not transferable across different model sizes and tasks. In a fully-supervised setting, we could rely on a development set to select among sample orders. However, this is not desirable in a few-shot setting where the size of the development set is very limited, even unavailable (Perez et al., 2021). Instead, we use the generative nature of language models to construct an unlabelled artificial development set and refer to it as a probing set. As the probing set is unlabelled, we use the predicted label distribution statistics and propose entropy-based metrics to measure the quality of candidate prompts. Experimental results show that we can achieve on average 13% relative improvement across eleven different established text classification tasks across all different sizes (four orders of magnitude) of PLMs.\\n\\nTo summarise, our contributions are as follows:\\n\\n1. We study order sensitivity for In-context Learning, which we show is crucial for the success of pretrained language models for few-shot learning.\\n2. We propose a simple, generation-based probing method to identify performant prompts without requiring additional data.\\n3. Our probing method is universally applicable and effective across different sizes of pretrained language models and for different types of datasets \u2013 achieving on average a\\n\\nFigure 2: Training sample permutations for the In-context Learning setting. The concatenation of training samples as well as test data transforms the classification task into a sequence generation task.\\n\\n2 Order Sensitivity and Prompt Design\\nIn this section, we study the relationship between permutation performance and various factors. For the ease of visualisation, we use a fixed random subset of four samples with a balanced label distribution from the SST-2 dataset and consider all 24 possible sample order permutations. This setup is illustrated in Figure 2. We also test five randomly-selected sets of examples and summarised variance statistics in the experiment section (Section 5).\\n\\nAlthough beneficial, increasing model size does not guarantee low variance. We evaluate the order permutations for four different sizes of GPT-2 (0.1B\u20131.5B) and GPT-3 (2.7B\u2013175B). As we can observe in Figure 1, models can obtain remarkable few-shot performance. We see that the GPT2-XL (1.5B) model can even surpass 90% accuracy given just four samples. This result is comparable to those of supervised models trained on more than 60,000 samples. However, the performance variation of different permutations remain a big issue, especially for \u201csmaller\u201d models.\\n\\nThe same model can exhibit nearly perfect behaviour given one sample order, but then fall back to be on par with a random baseline for another. While increasing the model size (by a few order of magnitudes) can sometimes alleviate the issue, it still cannot resolve it entirely (especially if we consider tasks other than SST-2). In contrast, different initialisations of supervised fine-tuning approaches typically result in less than 1% standard deviation for their test set performance (Gao et al., 2020).\\n\\nWe can also refer these models as GPT2-base, GPT2-medium, GPT2-Large, and GPT2-XL.\\n\\n2 The smallest model in our experiment is the same size as BERT-base.\"}"}
{"id": "acl-2022-long-556", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Order sensitivity using different numbers of training samples.\\n\\nAdding training samples does not significantly reduce variance. To further explore the order sensitivity of few-shot prompts, we increase the number of training samples and then sample a subset of at most 24 different orderings. We use the GPT2 family models for this experiment. In Figure 3, we can observe that increasing the number of training samples leads to increases in performance. However, a high level of variance remains, even with a large number of samples and can even increase. Based on this, we draw the conclusion that order sensitivity is likely to be a fundamental issue of In-context Learning regardless of the number of training samples.\\n\\nPerformant prompts are not transferable across models. We find that a specific permutation's performance may drop from 88.7% to 51.6% by changing the underlying model from GPT2-XL (1.5B) to GPT2-Large (0.8B). This suggests that a particular permutation working well for one model does not imply that it will provide good results for another model. To validate this hypothesis, we use all possible order permutations of the four samples as prompts \u2013 24 in total. We then perform prediction conditioned on each of these prompts for different models and calculate the pairwise Spearman's rank correlation coefficient between the scores. These results are shown in Figure 4. If there is a common pattern for performant prompts, we should then be able to observe high correlation across models. However, the behaviour of permutations is seemingly random even across different sizes of the same model. For example, the 175B and 2.7B model only has a correlation of 0.05, this means a good permutation for the 2.7B model is in no way guaranteed that it will also yield good performance for the 175B model.\\n\\nPerformant label orderings are not consistent across models. In addition to training example ordering, we also explore label ordering for training prompts. We use all patterns of the above-mentioned full permutations \u2013 six different label patterns. We then compute the pairwise Spearman correlation across different models as described in the previous paragraph. As shown in Figure 5, the behaviour of label orderings is once again seemingly random across different sizes of the same model. It is thus not possible to identify a label ordering that works consistently across models.\"}"}
{"id": "acl-2022-long-556", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Degenerate behaviour of bad prompts\\n\\nWe perform error analysis across performant and non-performant prompts and observe that the majority of failing prompts suffer from highly unbalanced predicted label distributions (Figure 6, left). An intuitive way to address this would be by calibrating the output distribution, along the lines of Zhao et al. (2021). However, we find that although calibration leads to much higher performance, the variance remains high (Figure 6, right).\\n\\n3 Methodology\\n\\nThe previous section demonstrates that prompt order can have a substantial effect on performance, with some orderings of the same prompts for the same model providing random performance, and other \u201cbetter\u201d orderings providing performance competitive with supervised approaches. This suggests that there could be various ways of selecting prompt orders to achieve better performance, but the challenge is to do so automatically and without the need for additional labels (e.g., a development set).\\n\\nHence, in this section, we explore the question of: \u201cHow can we automatically generate a \u2018prob- ing set\u2019 to find performant prompt orderings\u201d? We approach this by: (i) for a randomly-selected set of training samples, we use every possible ordering permutation of this set as candidates; (ii) constructing a probing set by querying the language model using all candidate prompts as context; and (iii) use this probing set to identify the best ordering by ranking them using a probing metric.\\n\\n3.1 Sampling from the Language Model to Construct a Probing Set\\n\\nWe propose a simple methodology to automatically construct a \u201cprobing set\u201d, by directly sampling from the language model itself. This approach makes it possible to generate probing sets automatically, without access to any additional data. Concretely, given a set of training samples $S = \\\\{(x_i, y_i)\\\\}_{i=1}^n$, where $x_i$ and $y_i$ denote the sentence and label of the $i$th training sample. We then define a transformation $T$, mapping each sample into natural language space, such that $t_i = T(x_i, y_i)$. $t_i$ is therefore a text sequence of the $i$th training sample using the template defined by $T$. In this work, we use a simple transformation function $T$ such that $T(x_i, y_i) = \\\\text{input: } x_i \\\\text{ type: } y_i$.\\n\\nThis transforms each sample into a standard format sentence, which linearises each element in the set into natural language space defined as $S' = \\\\{t_i\\\\}_{i=1}^n$.\\n\\nWe then define a full permutation function group of $n$ training samples, $F = \\\\{f_m\\\\}_{m=1}^n$, where each function $f_m$ takes $S'$ as input and outputs $c_m$: the concatenation of a unique permutation. In our case, sampling four training samples at random gives up to 24 possible ordering permutations of the transformed samples.\\n\\nFor each prompt candidate $c_m$, we then sample from the language model to obtain the probing sequence $g_m \\\\sim P(\\\\cdot|c_m; \\\\theta)$, where $\\\\theta$ denotes the parameters of the pretrained language model. We stop decoding from the language model upon generating the special end-of-sentence token defined by a template, or reach the generation length limit.\\n\\nOur probing set construction method is illustrated in Figure 7, where the objective is to generate a probing set that shares a similar distribution to the training samples.\\n\\nWe run this sampling process for all possible prompt ordering permutations and extract probing samples from them ($T^{-1}(g)$). Then gather extracted samples together to form the probing set $D = T^{-1}(g_1) \\\\oplus \\\\ldots \\\\oplus T^{-1}(g_n)$.\\n\\nAlthough the probing set contains predicted label for each sentence, there is no guarantee on the validity of these labels. Therefore, we discard them from the probing set as we are only interested in sampling probes from the language model corresponding to the input distribution.\\n\\n3.2 Probing Metrics\\n\\nOnce we have constructed a probing set for a given set of samples, we can now use that probing set to identify the best possible prompt ordering for that particular sample set. Here, we explore two\"}"}
{"id": "acl-2022-long-556", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"methods for selecting the best ordering: Global Entropy (GlobalE), and Local Entropy (LocalE).\\n\\nGlobal Entropy (GlobalE)\\nThe motivation behind GlobalE is to identify prompts of specific sample orderings that avoid the issue of extremely unbalanced predictions (as we have previously established it as key problem for non-performant prompts). We compute the predicted label $\\\\hat{y}_i$ for data point $(x'_i, y'_i)$ under context $c_m$ as follows:\\n\\n$$\\\\hat{y}_{i,m} = \\\\arg\\\\max_{v \\\\in V} P(v|c_m \\\\oplus T(x'_i); \\\\theta)$$ (1)\\n\\nFor each label $v \\\\in V$ (where $V$ denotes the target label set), we compute the label probability over the probing set as:\\n\\n$$p_{v,m} = \\\\frac{\\\\sum_{i \\\\in D} \\\\{\\\\hat{y}_{i,m} = v\\\\}}{|D|}$$ (2)\\n\\nWe then use the predicted category label entropy as the GlobalE score for $c_m$ as follows:\\n\\n$$\\\\text{GlobalE}_m = \\\\sum_{v \\\\in V} -p_{v,m} \\\\log p_{v,m}$$ (3)\\n\\nLocal Entropy (LocalE)\\nThe motivation behind LocalE is that if a model is overly confident for all probing inputs, then it is likely that the model is not behaving as desired. At the very least, it is poorly calibrated, which could also be an indication of a poor capability to appropriately differentiate between classes. Similar to the GlobalE computation, we calculate the prediction probability of a data point $(x'_i, y'_i)$ over the target labels $v \\\\in V$ under context $c_m$, as follows:\\n\\n$$p_{v,i,m} = P(x'_i, y'_i) \\\\sim D(v|c_m \\\\oplus T(x'_i); \\\\theta), v \\\\in V$$ (4)\\n\\nWe then calculate the average prediction entropy per data point as the LocalE score:\\n\\n$$\\\\text{LocalE}_m = \\\\frac{\\\\sum_{i} \\\\sum_{v \\\\in V} -p_{v,i,m} \\\\log p_{v,i,m}}{|D|}$$ (5)\\n\\nAs we now have a way to score each prompt ordering, based on its effect against the probing set, we can rank each prompt ordering by performance as measured by GlobalE or LocalE respectively.\\n\\n4 Experimental Setup\\nWe use four different sizes of GPT-2 (Radford et al., 2019) (with 0.1B, 0.3B, 0.8B, and 1.5B parameters) and two sizes of GPT-3 (Brown et al., 2020) (with 2.7B, and 175B parameters). Due to limited context window size (up to 1024 word-pieces for the GPT-2 series of models), we use a 4-shot setting for all datasets except AGNews and DBPedia. Our experiments are based on the open-source checkpoints of GPT-2 models and access to the OpenAI GPT-3 API.\\n\\nFor probing set generation, we restrict the maximum generation length to 128. We also use sampling with a temperature, $t$, of 2, and we also make use of block $n$-gram repetitions (Paulus et al., 2018) to encourage diverse generation. We use 24 different permutations for each set of randomly selected training samples and use 5 different sets (except for GPT-3 with 175B parameters, where we only do two sets with 12 different permutation due to the high monetary cost) for each experiment, giving a total of 120 runs. We report the mean and standard deviation of the corresponding evaluation metric over 5 different sets.\\n\\nFor performant prompt selection, we rank candidate prompts using the LocalE and GlobalE probabilities.\"}"}
{"id": "acl-2022-long-556", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We find that GlobalE achieves, on average, a 9.6% relative improvement over the upper-bound of performance by selecting the top \\\\( k \\\\) prompts. We also provide results for local and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance.\\n\\nWe report experimental results in Table 2 and observe an average 9.6% relative improvement over the upper-bound of performance by selecting the top \\\\( k \\\\) prompts. We also provide results for local and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance.\\n\\nSimilar to previous work (Gao et al., 2020; Zhao et al., 2021; Zhou et al., 2022), we also select performant prompts to evaluate performance on various datasets and demonstrate both better performance and lower variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and lower variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and lower variance.\\n\\n| Model       | SST-2  | SST-5  | DBPedia | MR   | CR   | MPQA  | Subj  | TREC  | AGNews | RTE   | CB   | Major       |\\n|-------------|--------|--------|---------|------|------|-------|-------|-------|--------|-------|------|-------------|\\n| GPT-2 0.1B  | 58     | 43.2   | 48.0    | 85.4 | 95.0 | 89.4  | 87.8  | 97.0  | 97.4   | 94.7  | 80.9 | 88.0        |\\n| GPT-2 0.3B  | 58     | 43.2   | 48.0    | 85.4 | 95.0 | 89.4  | 87.8  | 97.0  | 97.4   | 94.7  | 80.9 | 88.0        |\\n| GPT-2 0.8B  | 58     | 43.2   | 48.0    | 85.4 | 95.0 | 89.4  | 87.8  | 97.0  | 97.4   | 94.7  | 80.9 | 88.0        |\\n| GPT-3 175B  | 58.3   | 48.0   | 51.8    | 93.9 | 93.9 | 93.9  | 93.9  | 93.9  | 93.9   | 93.9  | 93.9 | 93.9        |\\n\\nWe find that GlobalE achieves, on average, a 9.6% relative improvement over the upper-bound of performance by selecting the top \\\\( k \\\\) prompts. We also provide results for local and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance.\\n\\nWe report experimental results in Table 2 and observe an average 9.6% relative improvement over the upper-bound of performance by selecting the top \\\\( k \\\\) prompts. We also provide results for local and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for local prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance.\"}"}
{"id": "acl-2022-long-556", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ranking using Entropy-based probing is robust\\n\\nIn Figure 8, we visualise the average performance when varying $K$ for the top $K$ prompt selection. $K = 24$ corresponds to using all sampled prompt orders, which is equivalent to the baseline model performance in Table 2. We can observe that the slope of curves are negative for all datasets, suggesting that our method can rank performant prompts effectively. Though $K = 1$ can provide good performance for most cases, in our experiments, we use $K = 4$ as preliminary experiments indicated that it yielded stable performance across datasets.\\n\\nFigure 8: Average performance of different Top $K$ permutation selection on GPT2-Large (0.8B)\\n\\nEntropy-based probing is effective across templates\\n\\nWe evaluate Entropy-based probing for four different templates similar to Gao et al. (2020) and Zhao et al. (2021) (Table 4) for the SST-2 dataset. Experimental results in Table 3 indicate that Entropy-based probing is valid for different templates. We also observe that the randomness across different templates is similar to Section 2. These findings suggest that Entropy-based probing is not sensitive to specific templates, as it consistently provides improvements for all cases.\\n\\nPerformant permutation selection is a safe option for In-context Learning\\n\\nWe find that for models that suffer from high prompt variance, our prompt selection process can show large improvements \u2013 up to 30% relative improvement. Furthermore, for tasks with low initial prompt performance variance, our method does not negatively impact performance. Our prompt selection provides marginal improvement at worse and on average a 13% relative improvement in the most cases.\\n\\nSentence-pair tasks remain challenging for smaller-sized models even with performant permutation selection\\n\\nFor the CB and RTE datasets, the performance of GPT-2 models is not significantly different from that of a random baseline. Despite this, we find that our method for identifying performant prompts can still provide minimal performance gains, although these are still within the levels of a random guess or majority vote. One reason for this could be that, for these particular sizes of models on these tasks, no good prompt exists. As such, optimising the prompt is not particularly effective in this setting. This is further supported by the observation that prompt selection can considerably improve performance on both CB and RTE at larger model sizes (particularly so for the GPT-3 175B parameter model). In fact, we find that prompt selection using GlobalE improves performance by 4.9% for GPT-3 175B on CB. This indicates that our method is widely applicable to all model sizes, and across all tasks, as long as they already possess some existing classification ability that can be improved through prompt design.\\n\\nEntropy-based probing outperforms using subsets of the training data for tuning\\n\\nIf one was not to rely on generation, an alternative approach to prompt selection could be to split the (limited) training data to form a validation set. To compare...\"}"}
{"id": "acl-2022-long-556", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Comparing our method with splitting the training set into train and development for SST-2.\\n\\nAgainst this approach, we split the 4-shot training samples (same setting as in Table 2) in half. We then select the top four performing prompts using validation set performance. As can be seen in Table 5, this approach consistently outperforms the baseline. However, both Entropy-based probing methods consistently provide better performance across all model sizes.\\n\\n6 Related Work\\n\\nUnified Interface Design for NLP\\n\\nMost previous work focuses on shared-parameters models, pretrain on some tasks, then fine-tune for different tasks, e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), etc. Eventually, leading to multiple task-specific models. There has for some time been attempts to design a unified interface for NLP tasks (Kumar et al., 2016; Raffel et al., 2020). In parallel with these works, GPT-2 (Radford et al., 2019) shows that appending trigger tokens (e.g. \u201cTL;DR\u201d) at the end of language model input can cause language models to behave like summarization models. The zero-shot capability of language models shows the potential to unify NLP tasks into a language modelling framework where fine-tuning is not necessary to achieve good performance. Furthermore, GPT-3 (Brown et al., 2020) shows that task-agnostic, few-shot performance can be improved by scaling up language models. It can sometimes even become competitive with prior state-of-the-art fine-tuning approaches.\\n\\nPrompt Design for PLMs\\n\\nThe core challenge of prompt design is to convert training data (if it exists) into a text sequence. Most work on prompt design focuses on how to make prompts more compatible with language models. Petroni et al. (2019) uses human effort to design natural language sentences and then performs token prediction given the input context. However, hand-crafted templates require significant human effort and are likely to end up with sub-optimal performance. Recent work has explored automatic template construction: Schick and Sch\u00fctze (2020) uses cloze-style tasks to construct templates, Gao et al. (2020) uses an external language model to generate templates, and Shin et al. (2020) uses gradient-guided search to find templates that maximize performance. Jiang et al. (2020) uses a mining-based method to create multiple diverse templates automatically.\\n\\nOrder Sensitivity of Prompt Design\\n\\nGao et al. (2020) demonstrated that finetuning-based approaches are not as order sensitive as In-context Learning. Making use of a standard-size training set, Liu et al. (2021) used nearest neighbour search to retrieve the most relevant training samples for a specific test sample. They were successful in retrieving relevant samples and concluded that after retrieving them the order in which they are provided in the prompt has little to no effect on performance. While our study is fundamentally different from theirs in that we do not make use of a standard-size training set, we do come to the opposite conclusion. All previous work on prompt design focuses on the textual quality of the prompt and, to the best of our knowledge, none has studied order sensitivity in detail.\\n\\nTrue Few-shot Learning\\n\\nPerez et al. (2021) evaluated few-shot capability of LMs when a held-out validation set is not available. Experimental result suggested that previous work overestimate the few-shot ability of LMs in this (true few-shot learning) setting. Our work instead uses the generative nature of language models to construct a probing set without relying on held-out examples. We show that our probing method is better than relying on held out examples (Figure 5) and thus enables true few-shot learning.\\n\\n7 Conclusion\\n\\nWe have shown that few-shot prompts suffer from order sensitivity, in that for the same prompt the order in which samples are provided can make the difference between state-of-the-art and random performance. In our analysis of the problem, we established that it is present across tasks, model sizes, prompt templates, samples, and number of training samples. To alleviate this problem, we introduced a novel probing method that exploits the generative nature of language models to construct an artificial development set. We were able to identify performing permutations using entropy-based statistics over this set, leading to an on average 13% improvement across eleven text classification tasks.\"}"}
{"id": "acl-2022-long-556", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.\\n\\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177\u2013190. Springer.\\n\\nJoe Davison, Joshua Feldman, and Alexander M Rush. 2019. Commonsense knowledge mining from pre-trained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173\u20131178.\\n\\nMarie-Catherine De Marneffe, Mandy Simons, and Juidith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In Proceedings of Sinn und Bedeutung, pages 107\u2013124.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186.\\n\\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.\\n\\nMinqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177.\\n\\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438.\\n\\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural language processing. In International conference on machine learning, pages 1378\u20131387. PMLR.\\n\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nBo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271\u2013278.\\n\\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL'05), pages 115\u2013124.\\n\\nRomain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive summarization. In International Conference on Learning Representations.\\n\\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. arXiv preprint arXiv:2105.11447.\\n\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u20132237.\\n\\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2020. How context affects language models' factual predictions. In Automated Knowledge Base Construction.\\n\\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, Hong Kong, China. Association for Computational Linguistics.\\n\\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. In OpenAI Blog.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1\u201367.\\n\\nTimo Schick and Hinrich Sch\u00fctze. 2020. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.\\n\\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with\"}"}
{"id": "acl-2022-long-556", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642.\\n\\nEllen M Voorhees and Dawn M Tice. 2000. Building a question answering test collection. In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 200\u2013207.\\n\\nJanyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2):165\u2013210.\\n\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.\\n\\nXiang Zhang, Junbo Zhao, and Yann Lecun. 2015. Character-level convolutional networks for text classification. Advances in Neural Information Processing Systems, 2015:649\u2013657.\\n\\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.\"}"}
{"id": "acl-2022-long-556", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset      | Prompt                                                                                           | Label Mapping                           |\\n|--------------|--------------------------------------------------------------------------------------------------|-----------------------------------------|\\n| SST-2        | Review: contains no wit, only labored gags                                                      | Sentiment: negative                     |\\n| SST-5        | Review: apparently reassembled from the cutting-room floor of any given daytime soap.           | Sentiment: terrible                     |\\n| MR           | Review: lame sweet home leaves no southern stereotype unturned                                   | Sentiment: negative                     |\\n| CR           | Review: bluetooth does not work on this phone                                                    | Sentiment: negative                     |\\n| MPQA         | Review: dangerous situation                                                                     | Sentiment: negative                     |\\n| Subj         | Input: too slow, too boring, and occasionally annoying                                           | Type: subjective                        |\\n| TREC         | Question: When did the neanderthal man live?                                                    | Type: number                            |\\n| AGNews       | Input: Wall St. Bears Claw Back Into the Black (Reuters).                                        | Type: business world/sports/business/technology |\\n| DBPedia      | Input: CMC Aviation is a charter airline based in Nairobi, Kenya                                 | Type: company                          |\\n\\n**CB**\\n\\n| Premise      | It was a complex language. Not written down but handed down.                                    | Hypothesis: the language was peeled down | Prediction: true |\\n|--------------|--------------------------------------------------------------------------------------------------|-----------------------------------------|-----------------|\\n\\n**RTE**\\n\\n| Premise      | No Weapons of Mass Destruction Found in Iraq Yet.                                               | Hypothesis: Weapons of Mass Destruction Found in Iraq. | Prediction: False |\\n|--------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------|-------------------|\\n\\nTable 6: Prompt template and label mapping for different tasks.\"}"}
{"id": "acl-2022-long-556", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Examples of transformation notations.\\n\\nTable 8: Statistics of evaluation datasets, average length is calculated based on GPT-2 sentence-piece length. For sentence-pair tasks, we report each sentence's average length separately.\"}"}
{"id": "acl-2022-long-556", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"not sure where to even begin\\n\\nthe only real film on our watch lists\\n\\nno one will care because it is just one story\\n\\nSST-5\\n\\nnot a bad documentary, but the story feels tacked on.\\n\\none that i have never liked and was always too long to understand and not enjoyable in parts.\\n\\nThis movie is the opposite of what it pretentious title implies.\\n\\nGweno Mott's book: Gweno is a New Yorker cartoonist published by Little, Brown, 1995/2002/2013.\\n\\nL. Ego Equestrians is North America's first dedicated equine show in Las Vegas.\\n\\nGraphed is a graph visualization package from the GraphViz project.\\n\\na solid first film for the debut helmer.\\n\\nA good deal more of the material in his previous films can be found here but this film does not come across...\\n\\nIt feels more real\\n\\nAnd at some point, maybe it was about...\\n\\nIt works just the same, i just prefer my iPhone 6.\\n\\nthe battery last so long for me it feels like ive already had my phone a year.\\n\\nworks great with both phones\\n\\nthis is really going nowhere\\n\\nwhy does it look so angry??\\n\\nExcellent book and will get a good reputation\\n\\nthis will become apparent as it gets older.\\n\\nhow about something more subtle to show this girl's love?\\n\\na perfect summary of an episode where the entire series is one massive meta romp, with...\\n\\nWhales can hold 4 gallons. Whaler can also be written as: What whale is named Whalerel?\\n\\nTo a certain degree, how do human eyes perceive colour?\\n\\nFrom where does our moon orbit, in Earth's Solar System?\\n\\nGoogle buys for $11bn: A-Z and thesaurus online, music search; photo service and TV site...\\n\\nSaudi-born billionaire takes $5 Billion Hit With Bankrupt. Saudi millionaire Sultan Al-Amoudi said...\\n\\nChina's 'Sesame' takes over for South Korea in world TV race as US TV loses market dominance.\\n\\nPremise: The Tuareg are a nomadic people who live in the Sahara desert.\\n\\nHypothesis: Tuareg are nomadic people who lived in the Sahara desert before the arrival of the Arabs.\\n\\nPremise: In the early 1940s, the United States and the Soviet Union were at war with Germany.\\n\\nHypothesis: Germany was at war with the United States and Russia.\\n\\nPremise: Water is a precious commodity.\\n\\nHypothesis: Water is not a precious commodity.\\n\\nPremise: In the back corner of Melissa's classroom her father walked through the door and walked across the front. ...\\n\\nHypothesis: his curiosity was directed towards some, something other than Melissa\\n\\nPremise: Maggie took Gloria out for a drive to the nearby city limits of Fort Myers on Tuesday...\\n\\nHypothesis: he couldn't bear looking down his nose at all the other houses\\n\\nPremise: There was one in Dallas. When it came out in New Jersey. And there were, ...\\n\\nHypothesis: I would never see that movie\"}"}
