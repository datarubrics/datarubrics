{"id": "emnlp-2022-main-518", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extending Phrase Grounding with Pronouns in Visual Dialogues\\n\\nPanzhong Lu1, Xin Zhang1, Meishan Zhang2\u2217, Min Zhang2\\n\\n1School of New Media and Communication, Tianjin University\\n2Institute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen)\\n{panzhong171,hsinz}@tju.edu.cn, {zhangmeishan,zhangmin2021}@hit.edu.cn\\n\\nAbstract\\nConventional phrase grounding aims to localize noun phrases mentioned in a given caption to their corresponding image regions, which has achieved great success recently. Apparently, sole noun phrase grounding is not enough for cross-modal visual language understanding. Here we extend the task by considering pronouns as well. First, we construct a dataset of phrase grounding with both noun phrases and pronouns to image regions. Based on the dataset, we test the performance of phrase grounding by using a state-of-the-art literature model of this line. Then, we enhance the baseline grounding model with coreference information which should help our task potentially, modeling the coreference structures with graph convolutional networks. Experiments on our dataset, interestingly, show that pronouns are easier to ground than noun phrases, where the possible reason might be that these pronouns are much less ambiguous. Additionally, our final model with coreference information can significantly boost the grounding performance of both noun phrases and pronouns.\\n\\n1 Introduction\\n\\nGrounded language learning has been prevailing for decades in many fields (Chandu et al., 2021), generally aiming to learn the real-world meaning of textual units (e.g., words or phrases) by jointly leveraging the perception data (e.g., images or videos). Bisk et al. (2020) advocate that we cannot overlook the physical world that language describes when doing language understanding research from a novel perspective. In particular, with the stimulation of modeling techniques and multi-modal data collection paradigms, the task has made excellent progress in the downstream tasks, which involves multi-modal question answering (Agrawal et al., 2017; Chang et al., 2022), video-text alignment (Yang et al., 2021) and robot navigation (Romani et al., 2020; Gu et al., 2022).\\n\\nTypically, as one branch of grounded language learning, phrase grounding, first proposed by Plummer et al. (2015), also plays a key role in visual language understanding. Its goal is to ground the phrases in a given caption to the corresponding image regions. Recently, many researchers have attempted varied approaches to explore this task. Mu et al. (2021) propose a novel graph learning framework for phrase grounding to distinguish the diversity of context among phrases and image regions. Wang et al. (2020) develop a multimodal alignment framework to utilize the caption-image datasets under weak supervision. Kamath et al. (2021) advance phrase grounding with their end-to-end modulated pre-trained network named MDETR. Overall, the natural language processing (NLP) and computer vision (CV) communities have seen huge achievements in the task of phrase grounding.\\n\\nIn spite of its apparent success, there remains a worth-thinking weakness. Almost all previous works mainly focus on the noun phrases/words, which can derive their meanings by the expressive forms to some extent. There is little work that takes account into pronouns. As shown in Fig. 1, with an image described by a caption, two people are discussing what they can see. Here, we annotate the same object with the same color, respectively. And obviously, the same object mentioned in the text naturally forms a coreference chain.\\n\\n![Figure 1: An example of grounding noun phrases and pronouns referred in the caption and dialogue (partly) to the associated image regions.](image)\\n\\nWith an image described by a caption, two people are discussing what they can see. Here, we annotate the same object with the same color, respectively. And obviously, the same object mentioned in the text naturally forms a coreference chain.\"}"}
{"id": "emnlp-2022-main-518", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1, pronouns definitely have underlying effects on the performance of visual grounding, which should be carefully examined (Yu et al., 2019). As a result, here we shift our eyes from the common (almost noun) phrase grounding with the extension of pronouns for the first time.\\n\\nIn this paper, we present the first work for investigating phrase grounding that includes pronouns, and explore how coreference chains can have an effect on the performance of our task. We annotate an initial dataset based on visual dialogue (Das et al., 2017), as shown in Figure 1. For the model, we can directly apply MDETR (Kamath et al., 2021), which is an end-to-end modulated detector. However, the model does offer much information to understand pronouns. Thus, we enhance the vanilla model with coreference information from the dialogue end, where a graph neural network is adopted to encode the graph-style coreference knowledge.\\n\\nFinally, we conduct experiments on our constructed dataset to benchmark the extended phrase grounding task. According to the results, we find that interestingly, pronouns are easier to ground by MDETR than phrases. The underlying reason might be that the pronouns are always more important during dialogue, leading to less ambiguity in speech communication. In addition, our final model can be significantly enhanced by adding the gold graph-style coreference knowledge; however, the model fails to obtain any positive gain when the coreference information is sourced from a state-of-the-art machine learning model. We conduct several in-depth analyses for comprehensive understanding of our task as well as the model.\\n\\nIn summary, our contributions are as follows:\\n\\n\u2022 We extend the task of phrase grounding by taking account of pronouns, and correspondingly establish a new dataset manually, named VD-Ref, which is the first dataset with ground-truth mappings from both noun phrases and pronouns to image regions.\\n\\n\u2022 We benchmark the extended phrase grounding task by a state-of-the-art model, and also investigate our task with the coreference knowledge of the text, which should benefit our task straightforwardly.\\n\\n\u2022 We observe several unexpected results by our empirical verification, and to understand these results, in-depth analyses are offered to illustrate them, which might be useful for the future investigation of phrase grounding.\\n\\n2.1 Task Description\\n\\nThe phrase grounding task's general purpose is to map multiple noun phrases to the image regions, however, in this paper, we take the challenge a step further by grounding various noun phrases and pronouns from the given dialogue to the appropriate regions of an image. Take Figure 1 for example, with all the expressions mentioned in the dialogue, like the coreference chain that includes \\\"Two kids\\\" and \\\"they\\\", the task needs to predict the corresponding regions of the object \\\"kids\\\" using bounding boxes in image.\\n\\nFormally, we define the task as follows: given an image $I$ and the corresponding ground-truth dialogue $D$, we denote $M=\\\\{N, P\\\\}$ as all the language expressions, typically, $N$ is the noun phrases and $P$ is the pronouns, the prime objective of the task is to predict a bounding box (or bounding boxes) $B$ for each expression.\\n\\n2.2 Data Collection\\n\\nWith the aim to build a high-quality dataset that includes sufficient pronouns, we adopt the large-scaled VisDialog dataset (Das et al., 2017) which contains 120k images from the COCO (Lin et al., 2014), where each image is associated with a dialogue around to the image. We randomly choose a set of 10k complete sets from the VisDialog dataset, and use the StanfordCoreNLP (Manning et al., 2014) tool to tokenize the sentences, making it proper for the succeeding human annotation.\\n\\n2.3 Annotation Process\\n\\nThe whole annotation workflow is divided into three stages as follows: (i) developing a convenient online tool for the user annotation; (ii) setting up a standard annotation guideline according to our task purpose; (iii) Recruiting sufficient expert users to annotate the dataset and ensuring each instance.\"}"}
{"id": "emnlp-2022-main-518", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The proportion of noun phrases and pronouns in different number coreference chains.\\n\\nwith three annotations. Firstly, we adopt the label-studio platform (Tkachenko et al., 2020-2022) as the basis to design a user-friendly interface targeted to our task, where the concreted interface is shown in Appendix A. Then, we let three people with the visual grounding research experience previously as our experts. They annotate 100 data-pairs together as examples, and establish an annotation guideline based on their consensus after several discussions. Next, we recruit a number of college students who are expertised at English skills to annotate our dataset. Before starting our task, the students are asked to read the guideline of the annotation process carefully and attempt to annotate some test sets of data, during this period, we examine these students and choose 20 of them to do the following annotation task. In the annotation of each datapoint, the prepared data is split into micro-tasks so that each one consists of 500 dialogues. We assign three workers to each micro-task, and their identities are remained hidden from each other. After all annotation tasks are finished, we let our experts check the results and make corrections of the unconsistent annotations as well.\\n\\nFinally, we establish the VD-Ref dataset, which is annotated manually with the noun phrases and pronouns that naturally form the coreference chains as well as the relevant bounding boxes in images.\\n\\n2.4 Statistics of the VD-Ref Dataset\\n\\nTotally, we collect 74,687 entity mentions and 23,980 objects from 8,857 VisDialog datasets, where the mentions include 48,798 noun phrases and 25,889 pronouns, on average, a dialogue consists of 5.51 noun phrases and 2.92 pronouns. On the contrary, the existing datasets for phrase grounding hardly consider the pronouns. The ReferItGame dataset (Kazemzadeh et al., 2014) only involves in the noun and noun phrases, while the Flickr30k Entities dataset does not label the corresponding bounding boxes in images, although it annotate the pronouns in captions.\\n\\nAlternatively, because of the diversity of our dataset, the number of coreference chains varies. As Figure 2 shows, the pie charts display the distinctive distributions of noun phrases and pronouns in the VD-Ref dataset. It is clear that whether for the noun phrases or the pronouns, the dialogues that have no more than three coreference chains account for the major proportion, up to 70%, accordingly, the dialogues that have more than three coreference chains constitute the rest proportion. Moreover, as the mentions of the coreference chains and bounding boxes come in pairs, we can define the coreference chain into four types:\\n\\n- one mention vs. one box: This type contains only one mention and one corresponding box, indicating that the chain exclude pronoun.\\n- one mention vs. boxes: As the referred object is separated into several regions, more than one box is needed to annotate.\\n- mentions vs. one box: In this coreference chain, all noun phrases and pronouns refer to the same single box on the image.\\n- mentions vs. boxes: This type contains several mentions that have noun phrases and pronouns and associated multiple boxes.\\n\\nFinally, the train, validation and test sets contain 6,199 (70.00%), 1,063 (12.00%) and 1,595 (18.00%) image-dialogue pairs, respectively. We report other statistics in Table 1 as well.\\n\\n3 Method\\n\\nRecent works (Kamath et al., 2021; Li et al., 2022) bring the successful vision-language transformer architecture and the pre-train-then-finetune paradigm to the phrase grounding task, achieving state-of-the-art performance. To explore our constructed dataset, we adopt the representative MDETR (Kamath et al., 2021) model. Meanwhile, we propose to enhance the textual representations with the natural coreference chains in texts by Relational Graph Convolutional Networks (R-GCN) (Schlichtkrull et al., 2018). Below, we briefly describe how MDETR learns and grounds, and then present our suggested coreference graph encoding.\\n\\n3.1 Grounding Model\\n\\nAs depicted in Figure 3, for a given image-text pair, MDETR first use an image-encoder (Tan and Le, 2019, EfficientNet) to extract visual features. Then, the features are projected to the image-text shared embedding space by a conv layer, flattened to a sequence features. Then, the features are projected to the image-text shared embedding space by a conv layer, flattened to a sequence features.\"}"}
{"id": "emnlp-2022-main-518", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: MDETR Model (right) and our suggested coreference graph encoding (left, dashed). Here, we use R-GCN to encode the coreference graph into the RoBERTa representation and directly feed the output into the linear layer.\\n\\nSimiliarly, a text-encoder (Liu et al., 2019, Roberta) and a linear layer are used to extract and project textual features, respectively. Next, we concatenate vectors of two modalities into one sequence, encoding it by a transformer (Vaswani et al., 2017) encoder. We set N queries and apply a transformer decoder to cross-attend the encoded sequence.\\n\\nFinally, for each one of N hidden states from the decoder, two feedforward networks (FFN) separately predict the object box and a distribution over all token positions that correspond to the object, which is named soft token prediction. Figure 3 shows an example that a query predicts the box of the cat and a distribution where tokens refer to this cat are with highest values.\\n\\nTraining. MDETR uses the bipartite matching (Carion et al., 2020; Tan et al., 2021) to find the best match between the predicted boxes and the gold-standard objects then computes the box losses (L1 & GIoU). The soft token prediction is supervised by a soft-cross-entropy between the predicted distribution and a uniform distribution, where tokens referred to the matched gold box have equal probabilities and sum to 1. In addition, the matching cost consists of this grounding loss and the box L1 & GIoU losses. The final loss for the MDETR training is the weighted sum of the above losses and an extra contrastive alignment loss.\\n\\nInference. For each referring expression, we rank all N proposed boxes by scores of the max over scores assigned to the tokens in this expression, and output the top 10 boxes for the evaluation.\\n\\nThis loss is able to align the query hidden state from the decoder and its matched referring tokens, please refer to the \u00a72.2.2 of Kamath et al.'s (2021) paper for more details.\\n\\n3.2 Coreference Graph Encoding\\n\\nBy carefully examining the MDETR model in our extended task, we find that it actually predicts the coreferenced expressions for each detected object to some extent. We guess that explicitly injecting the text coreference information into the representations could boost the model performance to some extent. Thus we propose to encode a simple coreference graph via R-GCN.\\n\\nGraph Construction. Following the previous graph-based NLP studies (Sahu et al., 2019; Wu et al., 2021; Hu et al., 2020, 2021), we construct our coreference graph in two steps. For the node building, we first initiate the word nodes by the input text embeddings. To represent the multi-word mention in text, we generate a virtual span node and setup the embedding by the mean embedding of all words in it.\\n\\nBased on the above two node types (i.e., word & span), we build the coreference graph with the following six edge types:\\n\\n- self-loop: include the information of itself.\\n- next-word: to keep the sequential information, we link a word to its next word.\\n- last-word: likewise link a word to the last.\\n- span-word: we link a span node to its words for the graph message passing.\\n- word-span: likewise link a word to its span.\\n- coref: we use this undirected edge to connect words or spans referred to the same object.\\n\\nR-GCN Encoding. We compute node representations on this edge-labeled graph by the R-GCN (Schlichtkrull et al., 2018). Formally, we denote...\"}"}
{"id": "emnlp-2022-main-518", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Test results. \u2020 means the result is statistically significant compared with MDETR.\\n\\n| Model            | F1   | Recall@1 | Recall@5 | Recall@10 | Overall Pronoun Phrase |\\n|------------------|------|----------|----------|-----------|------------------------|\\n| MDETR -         | 43.35 | 50.15    | 39.94    | 57.18     | 67.35                  |\\n| Overall Pronoun Phrase |\\n| MDETR + NeuralCoref | 37.6 | 42.04    | 49.39    | 38.36     | 53.72                  |\\n| Overall Pronoun Phrase |\\n| MDETR + C2f-SpanBERT | 66.0 | 42.36    | 49.45    | 38.87     | 54.80                  |\\n| Overall Pronoun Phrase |\\n| MDETR + Gold    | 100  | 47.54    | 58.79    | 41.91     | 59.30                  |\\n| Overall Pronoun Phrase |\\n| MDETR -         | 51.98 | 60.86    | 47.59    | 62.86     | 71.98                  |\\n| Overall Pronoun Phrase |\\n| MDETR + NeuralCoref | 51.04 | 62.14    | 45.60    | 62.01     | 72.45                  |\\n| Overall Pronoun Phrase |\\n| MDETR + C2f-SpanBERT | 66.0 | 51.96    | 62.45    | 46.71     | 62.44                  |\\n\\nIn the end, we re-construct the sequence from all word node representations of the last layer. It is worth to note that this coreference graph encoding is general and could be applied to any grounding models. In our experiments, the output of the R-GCN is directly fed to the Linear layer.\\n\\n4 Experiment\\n\\n4.1 Settings\\n\\nImplementation. We use the pretrained MDETR with Roberta-base and EfficientNet-B3. We employ 2-layer R-GCN to encode the Roberta representations. We use the AdamW (Loshchilov and Hutter, 2019) to update model parameters with lr $10^{-5}$, weight decay $10^{-4}$, and batch size 16. The lr of the re-initiated MDETR soft token prediction head and R-GCN module is set to $10^{-4}$. The 2-norms of gradients are clipped to a maximum of 0.1 to avoid the gradient explosion problem. All experiments are implemented with AllenNLP (Gardner et al., 2018) and conducted on a RTX 3090 GPU.\\n\\n4 Coreference.\\n\\nWe consider three ways to obtain coreference chains for our graph-encoding:\\n\\n- **Gold**: the gold-standard coreference chains annotated in our dataset.\\n- **NeuralCoref**: the off-the-shelf coreference resolution toolbox based-on SpaCy from HuggingFace (2019), we load the \u201cen-core-web-md\u201d model for SpaCy.\\n- **C2f-SpanBERT**: the widely used span-based coarse-to-fine model (Lee et al., 2018) with a pretrained SpanBERT-large-cased (Joshi et al., 2020).\\n\\nWe train it with the gold coreferences and perform an 5-fold cross-validation to get predictions of the whole dataset. In our main results, we train the MDETR + NeuralCoref or C2f-SpanBERT with only pseudo coreferences, which would fit the real scenario. We will investigate the recent state-of-the-art works of text coreference resolution (Wu et al., 2020) and update the results in the future version paper.\\n\\nEvaluation. Following previous studies, we compute the Recall@k to measure whether the model is able to give the \u201ccorrect\u201d box in top k predictions, where a box is treated as \u201ccorrect\u201d if the Intersection-over-Union (IoU) between it and a ground-truth box is above a threshold of 0.5. For each text mention, we consider $k \\\\in \\\\{1, 5, 10\\\\}$. We conduct experiments on both Any-Box and Merged-Box protocol, where the former decides a proposed box is correct to a mention when it has an IoU $> 0.5$ with any of the gold boxes of this mention, while the latter merges all ground-truth boxes of a mention into one smallest enclosing box. We use the best-performing model on the devset to evaluate the performance of the testset. We run\"}"}
{"id": "emnlp-2022-main-518", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Test recall@1 of MDETR with NeuralCoref or C2f-SpanBERT, by the mention prediction types, where Miss means a mention is not extracted by coref models, Part (resp. Correct) denote a mention is extracted with the incorrect (resp. correct) coreference cluster. We also provide the number of each type, i.e., the #Mention row.\\n\\nFigure 4: The assessment of our whole dataset on the maximum number of pronouns (in one coreference chain) for every dialogue (left) and one example (right).\\n\\n4.2 Main Results\\nTable 2 summarizes the main results of phrase grounding experiments on our VD-Ref dataset. We group the models into two settings, Any-Box and Merged-Box protocols, and report the performance of grounding pronouns and phrases in terms of Recall@k (k = 1, 5, 10). In details, we have the following intriguing findings:\\n\\nAmong all the models, we find that pronouns are easier to ground than phrases, no matter the protocol setting. The possible reason is that as an essential part of the sentence in dialogue, pronouns are straightforward and appear more frequently, containing richer details in context, thus promoting the performance to be grounded.\\n\\nBesides, after comparing the results of the MDETR with gold (MDETR + Gold) and without (MDETR), we see that adding the gold graph-style coreference knowledge can also considerably improve the model's performance. This empirically supports the value of introducing coreference knowledge. Noticeably, the Recall@10 is generally utilized to evaluate the best recall performance, and at this point, MDETR would reach its limit on this task, making it hard to be improved to some extent, while the addition of the gold graph-style coreference increase that by more than 1%, which further proof the significance of coreference knowledge.\\n\\nHowever, we still observe that performance declines when we apply machine learning models (e.g., NeuralCoref and C2f-SpanBERT) to obtain the coreference chains for our graph structure representations. One possible reason is that these models do not do so well in dialogues, making investigating the more thorough sense worthwhile.\\n\\n4.3 Analysis\\nPronouns outperform phrases. To dig into the in-deep reasons for this performance, we count the maximum number of pronouns (in one coreference chain) for every dialogue, and select one annotated dialogue as an example (see Figure 4). Here, we find that pronouns frequently occur in dialogues, and the maximum number of pronouns larger than three accounts for 44% in our dataset, indicating the importance of pronouns in dialogues. Besides, take Figure 4 (right) for an example. Four pronouns refer to \u201cwoman\u201d in the dialogue. The reason behind this is that expressing pronouns are more concise to refer to the specific object, reducing ambiguity in communication.\\n\\nDetailed Comparison of Non-Gold Methods. To find reasons for the unexpected failure of MDETR with pseudo coreferences from NeuralCoref and C2f-SpanBERT, we split testset mentions by the coreference cluster prediction of each of them is failed/partially correct/correct. Detailed R@1 values in the three types are in Table 3.\\n\\nWhen the prediction fails (Miss), model performance...\"}"}
{"id": "emnlp-2022-main-518", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Recall@1 Overall\\nMDETR + NeuralCoref + C2f-SpanBERT + Gold\\n(a) A NY -B OX (b) M ERGED -B OX\\n\\nFigure 5: The test Recall@1 (overall) scores grouped by #cluster, which act as the number of visual concepts and represent the difficulty of a data point.\\n\\nPerformances are significantly lower than the average, which hurts the overall performances much since these mentions took considerable portions. Surprisingly, the partially correct scores are above the average, which means that even with the defective coreference knowledge, models could precisely ground to a certain extent. Improving the coref model recall could be an effective way to promote grounding performance of suggested method.\\n\\nUnderstanding Complex Scenarios. Generally, models would perform worse in a complex scene than in a simple one. We design analysis to validate it in practice to evaluate model abilities in complex scenarios. We measure the difficulty of a scenario (data point) by the number of coreference clusters, which represents the number of visual concepts that need identifying, grouping testset into different parts. As shown in Figure 5, performances of all methods decline as the clusters increase. The Gold offers notable improvements in the simple data (#cluster $\\\\leq 3$). All methods perform poorly in complex scenarios, which would be one major limitation of phrase grounding models currently.\\n\\nGrounding Single/Multi-Object Mentions. As discussed by Kamath et al. (2021), the Any-Box and Merged-Box protocols are used to handle that the recall@$k$ implies each mention referring to single object. Here we divide mentions into two types, single and multi reference (e.g., the multi reference \\\"two kids\\\" referred two boxes in Figure 1), and compare the performances. In Figure 6, indeed the multiple reference are much challenging, showing shocking gaps to the single. That is, except for the challenges in complex scenarios (instance-level), the model ability on multi-object mentions (prediction-level) also need to be upgraded. Besides, the performance of pronouns is consistently better than that of noun phrases as expected.\\n\\nTable 4: Results of different grounding objective, where for the pronoun (resp. noun phrase) task, the noun phrase (resp. pronoun) is not trained and evaluated. Sides, the performance of pronouns is consistently better than that of noun phrases as expected.\"}"}
{"id": "emnlp-2022-main-518", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Ablation study of graph encoding.\\n\\n4.4 Ablation Study\\nTo verify the effectiveness of our designed coreference graph, we conduct ablation experiments in the gold coreference setting.\\n\\nCoreference Edge. We first drop coref edges to show the importance of text coreference knowledge. As presented in Table 5, obviously, the model performance decreases dramatically in both protocols. However, the graph with virtual spans provides mild improvements, which we investigate at the last.\\n\\nVirtual Span Node. The virtual spans are used to represent the multi-word text mentions. Here we remove them and the span-word & word-span edges, then densely connect every word with each other in one coreference cluster by the coref edge. As shown, the model performance is degraded to a certain extent. Thus, the virtual span scheme is not only conceptually advantageous but also performs better. In addition, we can directly use the span node features when applying to other span-based models, like Liu and Hockenmaier (2019).\\n\\nOnly Word Node and Relation. In the end, we remove both coref and virtual span, keeping only next-word, last-word, and self-loop edges. In Table 5, we can see that this model is only comparable to the baseline. First, this corroborates, without virtual spans, the coreference is still effective (the above paragraph). Moreover, virtual span nodes alone act as the span indicator could improve the model as well (the first paragraph).\\n\\n5 Related work\\n\\nVisual Grounding. General visual grounding, also known as referring expression comprehension (Deng et al., 2021; Qiao et al., 2021), is akin to phrase grounding to some extent, since they all aim to study the mapping from the expressions to the specific image regions. The main difference between them is that the visual grounding particularly focuses on one single expression, while the phrase grounding is more general and can be applied to multiple expressions.\\n\\nPhrase Grounding. A wealth of prior work (Yu et al., 2020; Dogan et al., 2019; Wang et al., 2020; Kamath et al., 2021) on phrase grounding has achieved promising results. Typically, Bajaj et al. (2019) present an end-to-end framework with a separate graph neural network to explore phrase grounding, and Liu et al. (2020) enhance this task by proposing a language-guided graph representation to capture the global context of grounding entities and their relations. In this work, we first propose that grounding pronouns is indispensable, then follow the foundation of using graph structures to our task, positing that the extra coreference knowledge in texts are positive and useful.\\n\\nVisual Coreference Resolution. It is true that our proposed task has some similarities with the visual coreference resolution task. Yu et al. (2019) formalizes visual-aware pronoun coreference resolution (PCR), builds a dataset for PCR in visual-supported dialogues, and then presents a PCR model with image features. In other words, it solves the pronoun coreference at the text side with the help of visual information. In contrast, our task tackles coreference across the text and image, and in addition, we are also concerned about noun phrases, not only the pronouns. Additionally, Kottur et al. (2018) indeed presents visual coreference resolution (VCR) very similar to ours, with only a difference in the coreference direction (image-to-text v.s. ours text-to-image). As it targets visual question answering, the work does not build a dataset for VCR nor evaluate it. Moreover, it handles VCR at the sentence level for each question in the visual dialogue. In our work, we focus on VCR directly, with a released benchmark dataset, initial models as well as benchmark results.\\n\\nRelated Datasets. The usual visual grounding datasets (Yu et al., 2016), RefCOCO, RefCOCO+ and RefCOCOg, only include one simple expression without pronouns. There are several benchmark datasets (Lin et al., 2014; Krishna et al., 2017) for phrase grounding, and the most well-known is\"}"}
{"id": "emnlp-2022-main-518", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Flickr30k Entities dataset (Plummer et al., 2015).\\n\\nNevertheless, since these datasets are among the first to build the relations between the noun phrases mentioned in a sentence and the specific localization of a corresponding image, they may ignore the pronouns, which can also be grounded and assistant to visual language understanding.\\n\\n6 Conclusion\\n\\nIn this work, we proposed to extend phrase grounding task with pronouns, additionally, we established our dataset, VD-Ref, the first dataset which contains ground-truth mappings from both noun phrases and pronouns to image regions. Furthermore, we took the state-of-the-art model MDETR as our baseline and introduced extra coreference knowledge with graph neural networks. Experiments on our dataset showed the exciting phenomenon that pronouns are more accessible grounded than phrases and demonstrated the significance of coreference knowledge in visual language understanding. To this end, we conducted in-depth analyses of our results. In the future, we would expand more sophisticated dataset, and do more richer experiments on our dataset.\\n\\nOur dataset and baseline code are available at https://github.com/izhx/Phrase-Grounding-with-Pronoun.\\n\\nLimitations\\n\\nIn this work, we collect our dataset and extend phrase grounding with pronouns by a series of explored experiments. Admittedly, due to the uneven distribution of raw data and complex annotation process, the main limitation is that our dataset only considers the visual phrases and pronouns, while lacking the annotations on non-visual textual expressions, and giving no insight into the scenery regions as well, which could restrict the research on more sophisticated conditions with varied coreference chains. Future work should be undertaken to expand a more complicated dataset and do more abundant experiments with coreference chains.\\n\\nEthical Statement\\n\\nWe build the dataset VD-Ref to go on our researches, aiming to extend the phrase grounding task with pronouns, and study the performance where the coreference chains impact on. In the data annotation process, we adhere to a certain code of conduct on ethical consideration. When recruiting annotators for our task, we claim that all the potential annotators are free to choose whether they want to participate, and they can withdraw from the study anytime without any negative repercussions. Additionally, the whole annotation tasks are anonymized, totally agnostic to any private information of annotators. Furthermore, the annotation results and dataset do not involve any sensitive information that may harm others. Overall, the establishment of our dataset is compliant with ethics.\\n\\nAcknowledgments\\n\\nWe thank all reviewers for their hard work. This research is supported by grants from the National Natural Science Foundation of China (No. 62176180).\\n\\nReferences\\n\\nAishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra. 2017. VQA: visual question answering - www.visualqa.org. Int. J. Comput. Vis., 123(1):4\u201331.\\n\\nMohit Bajaj, Lanjun Wang, and Leonid Sigal. 2019. G3raphground: Graph-based language grounding. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 4280\u20134289. IEEE.\\n\\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020. Experience grounds language. In Proc. of the EMNLP, pages 8718\u20138735, Online. Association for Computational Linguistics.\\n\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, volume 12346 of Lecture Notes in Computer Science, pages 213\u2013229. Springer.\\n\\nKhyathi Raghavi Chandu, Yonatan Bisk, and Alan W Black. 2021. Grounding 'grounding' in NLP. In Findings of the ACL-IJCNLP, pages 4283\u20134305, Online. Association for Computational Linguistics.\\n\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop and multimodal qa. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16495\u201316504.\"}"}
{"id": "emnlp-2022-main-518", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-518", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yongfei Liu, Bo Wan, Xiaodan Zhu, and Xuming He. 2020. Learning cross-modal context graph for visual grounding. In AAAI, New York, NY, USA, February 7-12, 2020, pages 11645\u201311652. AAAI Press.\\n\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\\n\\nChristopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proc. of the ACL: System Demonstrations, pages 55\u201360, Baltimore, Maryland. Association for Computational Linguistics.\\n\\nZongshen Mu, Siliang Tang, Jie Tan, Qiang Yu, and Yueting Zhuang. 2021. Disentangled motif-aware graph learning for phrase grounding. In AAAI, Virtual Event, February 2-9, 2021, pages 13587\u201313594. AAAI Press.\\n\\nBryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 2641\u20132649. IEEE Computer Society.\\n\\nYanyuan Qiao, Chaorui Deng, and Qi Wu. 2021. Referencing expression comprehension: A survey of methods and datasets. IEEE Trans. Multim., 23:4426\u20134440.\\n\\nHomero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz, and Jianfeng Gao. 2020. RMM: A recursive mental model for dialogue navigation. In Findings of the EMNLP, pages 1732\u20131745, Online. Association for Computational Linguistics.\\n\\nSunil Kumar Sahu, Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2019. Inter-sentence relation extraction with document-level graph convolutional neural network. In Proc. of the ACL, pages 4309\u20134316, Florence, Italy. Association for Computational Linguistics.\\n\\nMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In The Semantic Web - 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings, volume 10843 of Lecture Notes in Computer Science, pages 593\u2013607. Springer.\\n\\nMingxing Tan and Quoc V. Le. 2019. Efficientnet: Rethinking model scaling for convolutional neural networks. In Proc. of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 6105\u20136114. PMLR.\\n\\nZeqi Tan, Yongliang Shen, Shuai Zhang, Weiming Lu, and Yueting Zhuang. 2021. A sequence-to-set network for nested named entity recognition. In Proc. of the IJCAI, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 3936\u20133942. ijcai.org.\\n\\nMaxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov. 2020-2022. Label Studio: Data labeling software. Open source software available from https://github.com/heartexlabs/label-studio.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998\u20136008.\\n\\nQinxin Wang, Hao Tan, Sheng Shen, Michael Mahoney, and Zhewei Yao. 2020. MAF: Multimodal alignment framework for weakly-supervised phrase grounding. In Proc. of the EMNLP, pages 2030\u20132038, Online. Association for Computational Linguistics.\\n\\nLingfei Wu, Yu Chen, Kai Shen, Xiaojie Guo, Hanning Gao, Shucheng Li, Jian Pei, and Bo Long. 2021. Graph neural networks for natural language processing: A survey. CoRR, abs/2106.06090.\\n\\nWei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Jiwei Li. 2020. CorefQA: Coreference resolution as query-based span prediction. In Proc. of the ACL, pages 6953\u20136963, Online. Association for Computational Linguistics.\\n\\nJianwei Yang, Yonatan Bisk, and Jianfeng Gao. 2021. Taco: Token-aware cascade contrastive learning for video-text alignment. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 11542\u201311552. IEEE.\\n\\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. 2016. Modeling context in referring expressions. In Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II, volume 9906 of Lecture Notes in Computer Science, pages 69\u201385. Springer.\\n\\nTianyu Yu, Tianrui Hui, Zhihao Yu, Yue Liao, Sansi Yu, Faxi Zhang, and Si Liu. 2020. Cross-modal omni interaction modeling for phrase grounding. In MM '20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020, pages 1725\u20131734. ACM.\\n\\nXintong Yu, Hongming Zhang, Yangqiu Song, Yan Song, and Changshui Zhang. 2019. What you see is what you get: Visual pronoun coreference resolution in dialogues. In Proc. of the EMNLP-IJCNLP, pages 5123\u20135132, Hong Kong, China. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-518", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We designed the annotation interface. As illustrated in Figure 7, the left panel is the image section, while the right panel is the text section. They both have several boxes with distinct colors, which are used to annotate image regions and textual expressions. Moreover, the interface provides seven colors to choose from since the number of objects in the dialogue does not exceed 7 as a precondition. Notably, there is one option, \u201cAre you sure to delete the data?\u201d, for the annotators and reviewers to remove vague and ambiguous datasets, where the dialogue contains too much irrelevant content or the image is incomplete, making it challenging to be recognized.\"}"}
