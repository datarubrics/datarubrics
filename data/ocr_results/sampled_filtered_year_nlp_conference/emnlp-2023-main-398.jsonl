{"id": "emnlp-2023-main-398", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: Different demonstrations on ASQA.\\n\\n| #demo | Fluency Correct. Citation (MAUVE) (EM Rec.) | Recall | Precision |\\n|-------|--------------------------------------------|--------|-----------|\\n| 0     | ChatGPT                                    | 66.6   | 40.4      |\\n| 1     | FiD + POST + CITE                          | 75.8   | 28.4      |\\n| 2     | FiD + POST + CITE                          | 75.8   | 28.4      |\\n\\nTable 17: Comparison of Fusion-in-Decoder with ChatGPT on ASQA. Both models use top-5 GTR passages.\\n\\nG.5 More Human Evaluation\\n\\nWe evaluate the accuracy of our automatic metrics by treating the human annotations as gold labels. For citation recall, ALCE achieves an accuracy of 85.1%; for citation precision, ALCE has an accuracy of 77.6%. Regarding detecting insufficient citations, ALCE has a recall of 82.3% and a precision of 84.2%; regarding detecting \u201cirrelevant\u201d citations, ALCE has a recall of 75.6% and a precision of 66.1%\u2014ALCE is effective in detecting \u201cirrelevant\u201d citations, but due to the limitation of the NLI model (cannot detect \u201cpartial support\u201d), it has a relatively high false positive rate.\\n\\nG.6 Main Results\\n\\nWe show full results of our experiments along with the standard deviation in Tables 19, 20, and 21. We repeat all experiments with three different random seeds. However, for ChatGPT RERANK, we use only one seeded run since each run repeats the generation step four times, and more experiments would incur significant costs.\\n\\nH Prompts\\n\\nWe show detailed prompts used in our paper in Tables 23, 24, 25, 26, 27, 28, and 29.\\n\\nI Examples\\n\\nIn Tables 30 and 31 we show some examples of questions and model generated outputs.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 19: ASQA full results.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model         | Rec.-5 | Prec. | Rec. | Prec. | Num Pred. |\\n|--------------|--------|-------|------|-------|-----------|\\n| ChatGPT-16K  | 20.8   | 2.2   | 20.8 | 0.2   | 20.5      |\\n| ANILLA (5-psg)| 21.8 | 0.7 | 18.4 | 0.1 | 15.1      |\\n| Vicuna-13B   | 15.0   | 0.2   | 14.0 | 0.1   | 9.0       |\\n| LLaMA-2-70B-Chat V | 14.1 | 0.2 | 13.4 | 0.1 | 8.0       |\\n| Oasst-33B V  | 15.1   | 0.2   | 14.2 | 0.1   | 8.0       |\\n| LLaMA-33B S  | 15.0   | 0.2   | 14.2 | 0.1   | 8.0       |\\n| LLaMA-13B S  | 15.0   | 0.2   | 14.2 | 0.1   | 8.0       |\\n| Vicuna-13B S | 15.0   | 0.2   | 14.2 | 0.1   | 8.0       |\\n| LLaMA-2-13B-Chat V | 14.1 | 0.2 | 13.4 | 0.1 | 8.0       |\\n| Oasst-33B S  | 15.1   | 0.2   | 14.2 | 0.1   | 8.0       |\\n\\nTable 20: QAMPARI full results.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model         | Fluency Correct. Citation | MAUVE | (Claim) | Rec. Prec. | ROUGE-L | Length |\\n|--------------|---------------------------|-------|---------|-----------|----------|--------|\\n| ChatGPT-16K  |                           |       |         |           |          |        |\\n| VANILLA (5-psg) |                           | 57.2  | 1.6     | 12.0     | 0.6      | 51.1   |\\n| LLaMA-2-7B-Chat V |                       | 41.5  | 4.8     | 12.8     | 1.0      | 38.3   |\\n| LLaMA-2-13B-Chat V |                       | 38.6  | 4.8     | 12.8     | 1.0      | 37.9   |\\n| LLaMA-33B S  |                           |       |         |           |          |        |\\n| Oasst-33B V  |                           |       |         |           |          |        |\\n| Vicuna-13B S |                           |       |         |           |          |        |\\n| LLaMA-13B S  |                           |       |         |           |          |        |\\n| LLaMA-13B O  |                           |       |         |           |          |        |\\n| LLaMA-13B V  |                           |       |         |           |          |        |\\n| Oasst-33B O  |                           |       |         |           |          |        |\\n| LLaMA-33B V  |                           |       |         |           |          |        |\\n| LLaMA-13B S  |                           |       |         |           |          |        |\\n| LLaMA-13B O  |                           |       |         |           |          |        |\\n| LLaMA-33B V  |                           |       |         |           |          |        |\\n| LLaMA-13B S  |                           |       |         |           |          |        |\\n| LLaMA-13B O  |                           |       |         |           |          |        |\\n| LLaMA-33B V  |                           |       |         |           |          |        |\\n| LLaMA-13B S  |                           |       |         |           |          |        |\\n| LLaMA-13B O  |                           |       |         |           |          |        |\\n| LLaMA-33B V  |                           |       |         |           |          |        |\\n\\nTable 21: ELI5 full results.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Read the original question and passage, and generate 3 additional claims that are supported by the passage and answer the question.\\n\\nOriginal question: What's the difference between Shia vs. Sunni Islam?\\n\\nPassage: The main difference between Shia and Sunni Muslim is related to ideological heritage and issues of leadership. This difference is first formed after the death of the Prophet Muhammad in 632 A.D. The ideological practice of the Sunni branch strictly follows Prophet Muhammad and his teachings, while the Shia branch follows Prophet Muhammad's son-in-law Ali. Nowadays, Sunni and Shia are the major branches of Islam.\\n\\nClaim 1: The major branches of Islam are Sunni and Shia.\\nClaim 2: Prophet Muhammad died in 632 A.D.\\nClaim 3: The ideological practice of the Sunni branch strictly follows Prophet Muhammad and his teachings.\\n\\nOriginal question: What causes Bi-polar disorder?\\n\\nPassage: Bipolar disorder is an emotional disorder that causes extreme mood swings between excitement and depression. The spectrum of mood swing may span from days to months. We are still not certain of the exact factors that cause such disorder, but genetics is considered a major factor.\\n\\nClaim 1: One symptom of Bi-polar disorder is extreme mood swings between excitement and depression.\\nClaim 2: Genetics could be one of the major factors that causes Bi-polar disorder.\\nClaim 3: The mood swing from Bi-polar disorder can last days to months.\\n\\nOriginal question: How do we hear differences in sound besides volume and pitch?\\n\\nPassage: Pitch refers to the frequency of soundwave, and volume refers to the amplitude of the soundwave. Besides volume and pitch, we can also tell the difference between sounds based on the tone of sound. For example, we can differentiate the sound of different instruments based on the tone of the sounds.\\n\\nClaim 1: Volume of sound is the amplitude of the soundwave.\\nClaim 2: Pitch is the frequency of soundwave.\\nClaim 3: We can use the tone of the sounds to differentiate the sound of different instruments.\\n\\nOriginal question: How are we able to discern whether a sound is coming from in front of us or behind us?\\n\\nPassage: There are multiple explanations for why we can localize sounds. One explanation is that sounds travelling to the corresponding side of one's ear will be slightly louder. Another explanation is that there is a slight difference in the hitting time to one's left and right ear based on the sound's direction. However, these explanation means that when a sound is exactly in front of someone or exactly behind someone, he or she can not tell the difference.\\n\\nClaim 1: We can localize sounds by recognizing that the sound travelling to the corresponding side of one's ear will be slightly louder.\\nClaim 2: We can also localize sounds by recognizing the difference in hitting time to one's left and right ear based on the sound's direction.\\nClaim 3: We cannot tell the difference between a sound that is exactly in front of us or exactly behind us.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 25: Prompts for S UMM.\\n\\nGiven the following passage and the question \"{QUESTION}\", extract a useful span from the passage that can answer the question. Resolve all the coreference issues to make the extracted span understandable standalone. If the passage is not helpful for answering the question, return \\\"irrelevant\\\".\\n\\nTitle: {TITLE}\\nText: {TEXT}\\nExtracted span: Table 26: Prompts for S NIPPET.\\n\\nInstruction: Write an accurate, engaging, and concise answer for the given question using only the provided search results and cite them properly. Use an unbiased and journalistic tone. Always cite for any factual claim.\\n\\nYou are provided summaries/snippets of the search results. You can use \\\"Check: Document [1][2]\\\" to check the corresponding full documents (you should only check relevant documents and you can at most check 3 documents at a time) and use \\\"Output:\\\" to output a sentence in the answer. In the answer, cite properly by using [1][2][3]. Cite at least one document and at most three documents in each sentence. If multiple documents support the sentence, only cite a minimum sufficient subset of the documents. Use \\\"End\\\" to end the generation.\\n\\n<Retrieve for question \\\"...\\\">\\n<Get summaries/snippets for the passages and delete those that are \\\"irrelevant\\\">\\n\\nDocument [1](Title: ...) {SUMMARY OR SNIPPET}\\n...\\n\\nQuestion: When did US break away from England?\\nCheck: Document [1][2]\\n\\nDocument [1] {FULL TEXT}\\nDocument [2] {FULL TEXT}\\n\\nOutput: The United States...\\n[1]\\n...\\n[2]\\n\\nEnd.\\n\\nTable 27: An example for I NTERACT.\\n\\nInstruction: Write an accurate, engaging, and concise answer for the given question using only the provided search results and cite them properly. Use an unbiased and journalistic tone.\\n\\nYou can use \\\"Search: key words\\\" to check the most relevant document's full text and use \\\"Output:\\\" to output a sentence in the answer. In the answer, cite properly by using [1][2][3]. Cite at least one document and at most three documents in each sentence. If multiple documents support the sentence, only cite a minimum sufficient subset of the documents. Use \\\"End\\\" to end the generation.\\n\\nTable 28: Instruction for I NLINE S EARCH.\\n\\nInstruction: Write an accurate, engaging, and concise answer for the given question. Use an unbiased and journalistic tone.\\n\\nTable 29: Instruction for CLOSED B OOK.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Salmonella is most commonly caused by eating undercooked or raw foods like eggs or meat. If you do contract salmonella, you could start to feel bad within a couple of hours after eating contaminated food, and sometimes it could take a day or two. Common symptoms are nausea and vomiting, loose stools (sometimes bloody), flu like symptoms, and stomach cramps.\\n\\nEating raw flour is also a risk for food poisoning. The egg in cookie dough is usually pasteurized, so it's not likely that the egg would make you sick, but other ingredients in the dough could potentially harbor pathogens. However, prepackaged cookie dough like Cookie Dough Bites is safe to eat because the dough is made with pasteurized egg products and heat-treated flour..\\n\\n1. Cookie Dough Bites are safe to eat since they do not contain eggs.\\n2. Salmonella is a bacteria found in eggs.\\n3. Eating raw cookie dough carries the risk of salmonella infection.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How are firms like Snapchat, Uber, and Xiaomi valued so highly while still not making a profit? Do venture capitalists not expect some form of repayment within a number of years?\\n\\nVenture capitalists invest in a number of highly scalable high-risk technology ventures hoping to make a multi-fold return on their investment in a short amount of time. Highly valued companies like Snapchat, Uber, and Xiaomi are able to grow so fast and command high valuations because they have the potential to generate significant revenue in the future. Venture capitalists expect a return on their investment within a number of years, but they also invest in the long-term potential of the company. It is important to note that every start-up must eventually turn a profit, but some start-ups like Uber have not yet figured out how to do that.\\n\\nGround truth claims:\\n1. Firms like Snapchat and Uber need to establish their brand and amass users before introducing ads.\\n2. Introducing ads too early can deter potential users.\\n3. Uber is reinvesting a lot of money to make their service better.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Enabling Large Language Models to Generate Text with Citations\\n\\nTianyu Gao Howard Yen Jiatong Yu Danqi Chen\\nDepartment of Computer Science & Princeton Language and Intelligence\\nPrinceton University\\n{tianyug,hyen,jiatongy,danqic}@cs.princeton.edu\\n\\nAbstract\\nLarge language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs\u2019 Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions\u2014fluency, correctness, and citation quality\u2014and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement\u2014For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.\\n\\n1 Introduction\\nLarge language models (LLMs; Brown et al., 2020; OpenAI, 2023) have gained increasing popularity as a tool for information seeking. While they generate engaging and coherent responses, their outputs are prone to hallucination and often contain factually incorrect information (Ji et al., 2023). This makes it harder for users to trust and verify LLM-generated outputs without any supporting evidence. In this work, we study a new generation paradigm for LLMs, in which we require LLMs to provide citations to one or a few text passages for any statement they generate (Figure 1). Incorporating citations brings several benefits: (1) users can easily verify LLMs\u2019 claims with the provided citations; (2) LLMs can generate text that faithfully follows cited passages, which has the promise to improve correctness and alleviate hallucination. Multiple commercial systems have adopted this paradigm: Bing Chat and perplexity.ai respond to user questions in natural language with references to Web pages. Nakano et al. (2021); Menick et al. (2022) share a similar motivation, but they mainly experiment with commercial search engines and closed-source models, making their results difficult to evaluate. Retrieval-augmented LMs (Borgeaud et al., 2022; Izacard et al., 2022) incorporate retrieved passages during both training and inference, but do not guarantee faithfulness to retrieved passages or explicitly provide citations. Additionally, previous studies mostly rely on human evaluation (Nakano et al., 2021; Menick et al., 2022; Liu et al., 2023), which is expensive and difficult to reproduce. We argue that the absence of automated evaluation hinders the advances of such systems.\\n\\nhttps://www.bing.com/new\\nhttps://www.perplexity.ai\"}"}
{"id": "emnlp-2023-main-398", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present ALCE, the first reproducible benchmark for automatically evaluating LLMs' generations with citations. ALCE assumes a natural-language question and a retrieval corpus, and requires building end-to-end systems to retrieve relevant passages from the corpus, generate a response to the question, and cite corresponding supporting passages. We compile three datasets that cover different types of questions and corpora\u2014ASQA (Stelmakh et al., 2022), QAMPARI (Rubin et al., 2022), and ELI5 (Fan et al., 2019)\u2014as shown in Table 1. Different from previous benchmarks (Lee et al., 2019; Bohnet et al., 2022), ALCE evaluates long-text generation, focusing on automatically evaluating citation quality, and allows citing multiple passages for individual statements.\\n\\nWe design automatic evaluation methods in three dimensions: fluency, correctness, and citation quality. Specifically, we use MAUVE (Pillutla et al., 2021) to measure fluency, propose tailored correctness metrics for each dataset, and adopt a natural language inference (NLI) model (Honovich et al., 2022) to measure citation quality. We showcase how the three dimensions together contribute to a robust evaluation, preventing systems from exploiting shortcuts. Additionally, we conduct human evaluation and demonstrate a strong correlation with our automatic metrics.\\n\\nWe experiment on multiple systems with state-of-the-art LLMs and retrievers and also propose novel prompting strategies to synthesize retrieved text into text generation. Although all systems are capable of providing fluent and coherent responses, there remains substantial room for improvement in terms of correctness and citation quality: For example, on the ELI5 dataset, around 50% generations of our ChatGPT and GPT-4 baselines are not fully supported by the cited passages. Additionally, we find that (1) a closed-book model (generating answers without accessing any retrieved documents) with post-hoc citing achieves good correctness but much worse citation quality; (2) although interactive retrieval approaches (Yao et al., 2023; Schick et al., 2023) offer more flexibility in when/what to retrieve, they do not improve the performance on this challenging benchmark; (3) summarizing the retrieved passages in a shorter text improves correctness but not citation quality; (4) reranking multiple generations boosts citation quality measured by human evaluation; (5) incorporating more retrieved passages in context does not help ChatGPT but improves GPT-4 performance.\\n\\nOur extensive analyses highlight three major challenges of building LLMs to generate text with citations: (1) the retrieval quality is crucial to the final performance and has substantial room for improvement; (2) LLMs' limited context window restricts the number of passages they can incorporate; (3) current LLMs struggle to synthesize multiple documents in context without being distracted by irrelevant ones, although better instruction tuning brings significant improvement. These challenges pose promising research directions for developing better systems integrating retrieval and LLMs.\\n\\n2 Task Setup and Datasets\\nOur task is formalized as follows: Given a query $q$ and a corpus of text passages $D$, the system is required to return an output $S$, which consists of $n$ statements $s_1, \\\\ldots, s_n$, and each statement $s_i$ cites a list of passages $C_i = \\\\{c_{i,1}, c_{i,2}, \\\\ldots\\\\}$, where $c_{i,j} \\\\in D$. In this work, we segment LLMs' output into statements by sentence boundaries. While LLMs may include sentences that do not require a citation, such as \\\"I'm happy to help\\\", we observe that almost all sentences that LLMs output provide citations.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"valuable information and require citations, similar to findings in Liu et al. (2023). In this work, citations are enclosed by box brackets such as [1][2].\\n\\nWe divide the corpus $D$ into 100-word passages following previous works on open-domain question answering (Karpukhin et al., 2020; Petroni et al., 2021; Piktus et al., 2021), in contrast to commercial systems like Bing Chat, which cite entire Web pages. We take 100-word passages because it is easier for humans to verify, and allows for more retrieved passages to fit in LLMs' limited context.\\n\\nWe choose QA datasets so that (1) they contain factual questions, in which references are important; (2) questions require long-text answers that cover multiple aspects; (3) answering the questions requires synthesizing multiple sources. We select three datasets (Table 1) and introduce them below. See \u00a7B for additional statistics.\\n\\n**ASQA** (Stelmakh et al., 2022) is a long-form factoid dataset. As shown in Figure 1, each question is an ambiguous question from AmbigQA (Min et al., 2020) that requires multiple short answers to cover different aspects, and the dataset provides a long-form answer that covers all short answers. Since most questions can be answered by Wikipedia, we use the 2018-12-20 Wikipedia snapshot as $D$.\\n\\n**QAMPARI** (Rubin et al., 2022) is a factoid QA dataset constructed from Wikipedia, where the answer is a list of entities that are drawn from different passages. Same as ASQA, we use the 2018-12-20 Wikipedia as the corpus.\\n\\n**ELI5** (Fan et al., 2019) is a long-form QA dataset built on the Reddit forum \u201cExplain Like I\u2019m Five.\u201d Most ELI5 questions are how/why/what questions that require long answers and multiple passages as evidence. Due to the diverse topics discussed in the questions, we use Sphere (Piktus et al., 2021)\u2014a filtered version of Common Crawl\u2014 as the corpus. The ELI5 dataset is widely used in related work due to its challenging nature (Nakano et al., 2021; Menick et al., 2022; Liu et al., 2023).\\n\\nWe randomly select 1,000 examples from the development set of each dataset for ALCE. Our benchmark primarily assesses the citation capabilities of existing LLMs and does not provide training data, as there are no available examples that provide supervision for citations in these datasets.\\n\\n### 3 Automatic Evaluation\\n\\nOur benchmark measures the following three dimensions of system responses:\\n\\n- **Fluency**: whether the model's generated text is fluent and coherent.\\n- **Correctness**: whether the answer is accurate and covers all aspects of interest.\\n- **Citation quality**: whether the answer is well supported by the cited passages and no irrelevant passages are cited.\\n\\nIn the following, we present automatic metrics for each dimension and discuss why the combination of the three metrics provides a robust evaluation.\\n\\n#### 3.1 Fluency\\n\\nWe use MAUVE (Pillutla et al., 2021) to evaluate the fluency of the output (\u00a7C). We deploy MAUVE for ASQA and ELI5 and omit it for QAMPARI, as QAMPARI only requires a list of short answers as the response and LLMs consistently adhere to the format in our experiments. As MAUVE is sensitive to output length and text style, and most LLMs are capable of producing fluent text, we mainly employ it as a sanity check as long as the MAUVE scores are high enough.\\n\\n#### 3.2 Correctness\\n\\nOur objective is to measure the informativeness and utility of the generation to the question. Liu et al. (2023) propose to directly evaluate perceived utility by humans, a process difficult to automate. Therefore, we use correctness\u2014whether the response is accurate compared to a ground truth answer\u2014as a proxy. Evaluating the correctness of long-form generation is a challenging task (Krishna et al., 2021), and we describe our strategy for each dataset below. Figure 2 illustrates the metrics and we include additional implementation details in \u00a7C.\\n\\nFor **ASQA**, we follow Stelmakh et al. (2022) and calculate the recall of correct short answers by checking whether the short answers (provided by the dataset) are exact substrings of the generation (exact match recall; EM recall).\\n\\nFor **QAMPARI**, we follow Rubin et al. (2022) and calculate the precision and recall of the model prediction, by checking the exact match to the gold answer list. We add one additional adjustment: considering that users often want to know only a few example answers of the question, our evaluation considers recall to be 100% if the prediction includes at least 5 correct answers (recall-5).\"}"}
{"id": "emnlp-2023-main-398", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When did the US break away from England?\\n\\nQuestion\\n\\nShort answers (from the dataset)\\n\\n- July 2, 1776\\n- September 3, 1783\\n- July 4, 1776\\n\\n... it declared independence on July 3, 1776 ... The Treaty of Paris was signed on September 3, 1783 ...\\n\\nModel output\\n\\nHow do student loans affect getting a mortgage?\\n\\nQuestion\\n\\nThey do not appear in credit history and do not affect debt to income ratio. Thus they do not affect getting a mortgage.\\n\\nModel output\\n\\nClaims (generated by text-davinci-003 based on gold answers)\\n\\n- Student loan does not appear in credit history.\\n- Student loan can affect the debt to income ratio.\\n- Debt to income ratio affects mortgage applications.\\n\\nString exact match\\n\\nRecall = 33.3%\\n\\nPrecision = 50%\\n\\nFigure 2: Evaluation of correctness (details in \u00a73.2).\\n\\nUnlike ASQA and QAMPARI, the ELI5 dataset does not provide short entity answers. Fan et al. (2019) use ROUGE for evaluation, which does not reflect the correctness well (Krishna et al., 2021; \u00a7A). Inspired by works in summarization evaluation (Zhang and Bansal, 2021; Kamoi et al., 2023; Wang et al., 2020), we use InstructGPT (text-davinci-003; Ouyang et al., 2022) to generate three \u201csub-claims\u201d. Then we use TRUE (Honovich et al., 2022), a T5-11B (Raffel et al., 2020) model fine-tuned on a collection of natural language inference (NLI) datasets, to check whether the model output entails the sub-claims (claim recall). TRUE targets factual correctness and has been used by previous works in similar context (Bohnet et al., 2022; Gao et al., 2023). We demonstrate that claim recall provides a more accurate measure of correctness than existing metrics (more details in \u00a7A).\\n\\n3.3 Citation Quality\\n\\nWe evaluate citation qualities using two metrics: (1) citation recall, which determines if the output is entirely supported by cited passages, and (2) citation precision, which identifies any irrelevant citations. Although we prioritize citation recall as it entails a well-supported and truthful answer, enhancing precision is crucial for better user satisfaction, reducing the need for human review of extraneous citations.\\n\\nWe use an NLI model to verify whether a statement is supported by its citations. Figure 3 provides an illustrated example. We use the NLI model TRUE (Honovich et al., 2022) again to automatically examine whether the cited passages entail the model generation. We conduct human evaluation (\u00a76) to demonstrate strong human correlation of our metric.\\n\\nCitation recall.\\n\\nWe calculate the citation recall of each statement (0 or 1) and average over all statements in the model response. For each statement $s_i$, its citation recall is 1 if and only if there is at least one citation ($C_i \\\\neq \\\\emptyset$) and $\\\\phi(\\\\text{concat}(C_i), s_i) = 1$, where $\\\\phi(\\\\text{premise}, \\\\text{hypothesis})$ is the NLI model that outputs 1 if the premise entails the hypothesis, and 0 otherwise; $\\\\text{concat}(C_i)$ concatenates all passages in $C_i$ together (details in \u00a7C). The NLI evaluation is in accordance with the attributable to identified sources (AIS) framework (Rashkin et al., 2023): $\\\\phi(\\\\text{concat}(C_i), s_i) = 1$ implies that $s_i$ is true based solely on $\\\\text{concat}(C_i)$.\\n\\nCitation precision.\\n\\nOur citation precision evaluation detects citations that are irrelevant, but it does not require citing a minimal set. We follow this design because human writing often cites redundant sources to enhance credibility; human readers may also appreciate multiple citations, especially when it pertains to critical claims such as medical advice.\\n\\nWe calculate the citation precision for each citation (0 or 1) and average over all citations in the...\"}"}
{"id": "emnlp-2023-main-398", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When did the US break away from England?\\n\\nAnswer: The United States took the first step towards gaining independence from England. The Treaty of Paris was later signed, which formalized the end of the Revolutionary War and the establishment of the United States as an independent nation.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When did US break away from England?\\nSearch: Declaration of Independence\\nSearch: Treaty of Paris\\nEnd.\\n\\nTable 3: An example of INLINE SEARCH.\\n\\n| Output | 11 |\\n|--------|----|\\n| by using GTR; the \\\"Output\\\" and \\\"End\\\" actions are the same as INTERACT. For each \\\"Search\\\" action, we display the best retrieved passage in the context. The passage is removed after one action to save context space. Table 3 shows an example.\\n\\nCLOSED BOOK. We also add a simple closed-book baseline, where the model is only prompted with the instruction and the question, without any retrieved passages provided. Consequently, this variant does not cite any evidences.\\n\\n4.3 Post-editing\\n\\nIn this section we discuss two strategies for refining the output to further improve its quality.\\n\\nRERANK. We randomly sample \\\\( n = 4 \\\\) responses for each question, and select the best response using the automatic citation recall score. We expect RERANK to improve the citation quality.\\n\\nPOSTCITE. For each statement, we find the best matching passage among the top-100 retrieved passages using GTR and cite it. We combine this with CLOSED BOOK in our experiments.\\n\\n5 Experiments\\n\\nWe describe experiment details in \u00a7C. We use ChatGPT (gpt-3.5-turbo-0301) with a 4K context window for most main experiments and ablations. We also report results with ChatGPT-16K (gpt-3.5-turbo-16k-0613; 8K context window). For open-source models, we test LLaMA (Touvron et al., 2023) and its instruction-tuned versions, including Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), and\\n\\nWe do not search over the entire corpus because {query} may leave out certain context in the question and searching among the already-retrieved passages gives better results.\\n\\n| Fluency Correct. Citation (MAUVE) | Rec. | Prec. |\\n|-----------------------------------|------|-------|\\n| ChatGPT VANILLA (5-psg)           | 66.6 | 40.4  |\\n| w/ RERANK                         | 77.0 | 84.8  |\\n| SUMM (10-psg)                     | 70.0 | 68.9  |\\n| w/ INTERACT                       | 69.0 | 66.5  |\\n| SNIPPET (10-psg)                  | 69.8 | 65.3  |\\n| INLINE SEARCH                     | 58.7 | 58.3  |\\n| CLOSED BOOK                       | 52.7 | 26.7  |\\n\\nGPT-4 (VANILLA prompting)\\n\\nGPT-4 (5-psg) 67.1 41.3 68.5 75.6\\nGPT-4 (20-psg) 64.9 44.4 73.0 76.5\\n\\nLLaMA (VANILLA prompting)\\n\\nLLaMA-13B (3-psg) 68.4 26.9 10.6 15.4\\nVicuna-13B (3-psg) 82.6 31.9 51.1 50.1\\nChat-13B (5-psg) 72.4 35.2 38.4 39.4\\nChat-70B (5-psg) 88.3 41.5 62.9 61.3\\n\\nTable 4: Experiments on ASQA. For CLOSED BOOK, we use POSTCITE to get citations.\\n\\nk-psg: putting top-k passages from the retrieval results into the context. Chat-13B and Chat-70B refer to LLaMA-2-Chat. Oasst (K\u00f6pf et al., 2023). They all have a 2K context window. We use short instructions for LLaMA (Table 24) to save context budget. Additionally, we test LLaMA-2-Chat, which were also trained to follow instructions (Touvron et al., 2023b). These models have a context window of 4K tokens, which allows for 5 passages per question.\\n\\n5.1 Main Results\\n\\nWe present the main results on three datasets in Table 4, 5, and 6 respectively (full results in \u00a7G.6).\\n\\nWe first note that all models achieve good fluency scores (except some models on ELI5 mainly due to their longer generations). We summarize the main takeaways from the experiments below.\\n\\nVANILLA achieves strong performance. Despite its simplicity, VANILLA (putting retrieved passages in context) achieves close-to-the-best performance among all prompting strategies.\\n\\nUsing summaries or snippets improves correctness. We see a universal trend that SUMM or SNIPPET improves correctness, though on ASQA and ELI5, such an improvement comes at a cost of citation quality due to the lossy compression. Combining INTERACT with SUMM/SNIPPET does not bring improvement, and we hypothesize that checking the full passages offers limited benefit and current LLMs are not proficient in an interactive usage.\\n\\nRetrieving text on the fly does not improve performance. All datasets show that VANILLA outperforms INLINE SEARCH on citation quality (and\"}"}
{"id": "emnlp-2023-main-398", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Experiments on QAMPARI. \u201cRec.-5\u201d: we set the recall to be 100% if the prediction includes at least 5 correct answers.\\n\\nOn correctness for ASQA and ELI5). By manually examining the examples, we find that it is challenging to ask detailed questions without seeing any passages. To improve INLINE SEARCH, one may need to provide more context about the questions in advance or encourage the model to call retrievers with more detailed and diverse queries.\\n\\nRERANK boosts citation quality. We observe that RERANK leads to consistent improvement in citation quality (on ASQA and ELI5). As the automatic scores may be biased in RERANK, we also conduct human evaluation (\u00a76) and verify its effectiveness.\\n\\nCLOSED BOOK delivers strong correctness but poor citation quality. CLOSED BOOK outperforms VANILLA in correctness on ELI5 and QAMPARI, and has only a 2% gap on ASQA. However, CLOSED BOOK cannot provide any citation; when combined with POST CITE, the citation quality remains inadequate. For instance, citation recall of CLOSED BOOK + POST CITE is lower than VANILLA by 47% on ASQA.\\n\\nTo understand why CLOSED BOOK achieves better correctness and why POST CITE cannot deliver satisfying citation quality, we manually examine model outputs and find that: (1) open-book models are easily distracted by irrelevant passages and generate responses with lower correctness, a phenomenon also observed by Shi et al. (2023); (2) CLOSED BOOK often generates texts that are correct but not similar to any retrieved passages, making it difficult to match a citation post-hoc.\\n\\nTable 6: Experiments on ELI5. We use claim recall for the correctness evaluation. Chat-13B and Chat-70B refer to LLaMA-2-Chat.\\n\\nGPT-4 brings limited improvement but is better at using long context. We evaluate GPT-4 with VANILLA and different numbers of passages (more results in \u00a7G.6). GPT-4 brings consistent (but limited) improvement on correctness, but often at a cost of citation quality. GPT-4 can also incorporate more passages due to its longer context window, which boosts both correctness and citation quality. On the contrary, including more passages with ChatGPT-16K does not improve the results (Table 7), suggesting that processing more passages is non-trivial and GPT-4 is better at synthesizing information from its long context than ChatGPT.\\n\\n5.2 Comparison of Different LLMs\\n\\nTable 7 compares different LLMs on ASQA using VANILLA (more results in \u00a7G.6). Notably, instruction-tuned models (Vicuna-13B and LLaMA-2-Chat) outperform the original LLaMA models in correctness and considerably enhance the citation quality. We observe that while the original LLaMA models are able to copy facts from the context, they struggle with accurately citing the sources or simply do not cite. Notably, the best open-source model, LLaMA-2-70B-Chat, achieves comparable correctness score as the OpenAI models, but still lags behind in citation quality.\\n\\n5.3 Retrieval Analysis\\n\\nThe retrieval results play a crucial role to the correctness and the citation quality. Figure 4 presents the retrieval recall@k with different datasets and...\"}"}
{"id": "emnlp-2023-main-398", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Retrieval recall@k on ASQA (EM recall), QAMPARI (recall-5), and ELI5 (claim recall). Retrieval recall serves as an upper bound for model performance, and we compare them with two models' correctness results in the figure (dashed lines): \u201cVanilla (5-psg)\u201d is ChatGPT with top-5 passages in context; \u201cOracle\u201d is the same model except that it uses 5 gold passages (\u00a7G.1), whose recall matches Recall@100 on all three datasets.\\n\\nTable 7: Comparison of different LLMs on ASQA (GTR+Vanilla). LLaMA-13B and Vicuna-13B have a context limit of 2,048 tokens, and thus can only use a short version of instructions and at most top-3 passages. Chat-13B and Chat-70B refer to LLaMA-2-Chat. As the number of passages increases, retrieval recall steadily improves. Additionally, Figure 4 shows the correctness performance of two models: (1) ChatGPT Vanilla with top-5 passages (our primary baseline); (2) an oracle version of the same model employing 5 gold passages (\u00a7G.1; the 5 gold passages match the retrieval recall@100). Notably, both models' correctness lags behind the corresponding retrieval recall (except for ELI5 top-5). The discrepancy suggests that despite the presence of accurate answers in context, LLMs struggle to utilize them in their outputs.\\n\\nWe compare the impact of different retrievers and different numbers of passages to LLMs. Figure 4 (right) shows that GTR outperforms DPR in both correctness and citation quality, emphasizing the importance of deploying better retrievers. Contrary to the retrieval recall trend in Figure 4, more passages in context do not yield substantial improvement for ChatGPT. Specifically, correctness plateaus at top-1 passage and citation quality plateaus at top-3. GPT-4 (Table 7) exhibits an increasing trend with more passages, but the improvement is not proportional to the retrieval performance. This indicates the limited ability of LLMs in utilizing multiple passages within context.\\n\\n5.4 Other Ablations\\nWe provide additional ablations in \u00a7G. In summary, we find that (1) using comprehensive instructions enhances the citation quality of instruction-tuned models (\u00a7G.2); (2) including at least one demonstration improves the performance (\u00a7G.3); (3) fine-tuned models (FiD; Izacard and Grave, 2021) with POCITE lag behind LLMs in both correctness and citation quality and fail to generalize (\u00a7G.4).\\n\\n6 Human Evaluation\\nTo verify that our automatic evaluation correlates with human judgement, we conduct human evaluation on selected models and request workers to judge model generations on three dimensions similar to Liu et al. (2023)\u2014(1) utility: a 1-to-5 score indicating whether the generation helps answer the question; (2) citation recall: the annotator is given a sentence and all passages that the sentence cited, and is asked to judge whether the passages fully support the sentence; (3) citation precision: given a sentence and one of its citations, the annotator is asked to judge whether the citation \u201cfully supports\u201d, \u201cpartially supports\u201d, or \u201cdoes not support\u201d the sentence. Each citation gets a precision score 1 if the output sentence has a citation recall of 1 and this citation at least \u201cpartially supports\u201d it. See Appendix F for more details.\\n\\nModel outputs score high utility. The utility scores do not differ significantly between models, ranging 3.7-3.9 for ASQA and 3.5-3.6 for ELI5. Upon inspection, all tested models are mostly able...\"}"}
{"id": "emnlp-2023-main-398", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Human citation quality evaluation vs. ALCE citation quality evaluation on ASQA.\\n\\n|                  | Human scores | ALCE scores |\\n|------------------|--------------|-------------|\\n|                  | Rec. Prec.   | Rec. Prec.  |\\n| ChatGPT          |              |             |\\n| ANILLA           | 74.7 76.6    | 75.3 74.4   |\\n| w/ RERANK        | 79.3 81.9    | 83.9 80.8   |\\n| Vicuna-13B       |              |             |\\n| ANILLA           | 51.6 51.5    | 50.3 50.1   |\\n\\nTable 9: Human citation quality evaluation vs. ALCE citation quality evaluation on ELI5.\\n\\n|                  | Human scores | ALCE scores |\\n|------------------|--------------|-------------|\\n|                  | Rec. Prec.   | Rec.Prec.   |\\n| ChatGPT          |              |             |\\n| ANILLA           | 50.8 52.4    | 52.8 50.4   |\\n| w/ RERANK        | 59.7 60.6    | 63.0 60.6   |\\n| Vicuna-13B       |              |             |\\n| ANILLA           | 13.4 19.2    | 13.6 18.1   |\\n\\nOur automatic evaluation of citation quality strongly correlates with human judgements. As shown in Table 8 (ASQA) and Table 9 (ELI5), the relative rankings induced by human and our automatic metrics are consistent. The absolute citation scores from human and ALCE are very close except for RERANK (which uses the automated citation recall for reranking). This suggests that an improvement on ALCE citation metrics translates to improvement on human preferences. Furthermore, the Cohen's kappa coefficient between human and ALCE suggests substantial agreement for citation recall (0.698) and moderate agreement for citation precision (0.525). We also show in \u00a7G.5 that our automatic evaluation achieves high accuracy when treating human annotations as gold labels (85.1% for citation recall and 77.6% for citation precision).\\n\\n7 Related Work\\n\\nEvaluating citations. Generating text with citations is closely related to attribution. Rashkin et al. (2023) define the \u201cattributable to identified sources\u201d (AIS) score to measure how faithful a generated text is to its sources. Bohnet et al. (2022) apply AIS scores on a single-document short-answer QA dataset. Honovich et al. (2022); Yue et al. (2023) study automatic evaluations for the AIS score. A concurrent work (Liu et al., 2023) conduct human evaluation on commercial generative search engines to examine their citation qualities.\\n\\nScientific citation text generation (Funkquist et al., 2022) is a related task to ALCE where the model is provided the papers-to-cite and context and is required to recover the citing text. It is different from ALCE as all citations are provided and the model only needs to perform the summarization.\\n\\nRetrieval-augmented LMs. Many studies have explored augmenting LMs with externally retrieved information. Guu et al. (2020); Borgeaud et al. (2022); Izacard et al. (2022) pre-train language models with retrieved passages, while Khandelwal et al. (2020); Zhong et al. (2022) augment LLMs' output by interpolating it with a $k$NN module; though none of them explicitly provide citations to the retrieved sources. Other works prompt or fine-tune LLMs to \u201cretrieve on-the-fly\u201d (Parisi et al., 2022; Schick et al., 2023; Shuster et al., 2022; Jiang et al., 2023; Yao et al., 2023; Press et al., 2022), which offers flexibility of when and what to search. Gao et al. (2023); He et al. (2022) propose to first generate text without accessing external documents and then retrieve relevant documents and revise the generation to be consistent.\\n\\nAmong previous explorations, Nakano et al. (2021); Menick et al. (2022) are the closest to our setting, where LLMs are trained to answer questions while providing citations. However, they do not explore retrieval strategies and simply use commercial search engines, which are not reproducible, and their models and training data are closed-source. To the best of our knowledge, we are the first to implement end-to-end systems that retrieve, synthesize, and cite documents with LLMs.\\n\\n8 Conclusion\\n\\nWe propose ALCE, the first automatic benchmark for evaluating LLM generations with citations. We deploy automatic metrics to measure fluency, correctness, and citation quality, and verify their efficacy via human evaluation. We explore a variety of strategies for incorporating citations in LLMs and demonstrate that current systems have considerable room for improvement on ALCE.\\n\\nOur experiments highlight a number of promising research directions, including (1) enhancing retrieval and refining retrieval integrations in LLMs, (2) developing long-context LLMs, and (3) advancing LLMs' ability to synthesize multiple sources. What's even more intriguing is that these research proposals extend beyond the ALCE setup (for example, long-context LLMs have numerous exciting applications), and ALCE can serve as a valuable testbed for their development.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nOur evaluation still has room for improvement: (1) MAUVE is found to be sensitive to output length and may provide unstable results; (2) for the ELI5's correctness evaluation, the automatically generated claims may not cover all possible answers due to the open-ended nature of the questions; (3) our citation quality evaluation is limited by the accuracy of the NLI model; for citation precision, the NLI model cannot detect the case of \\\"partially support\\\" and thus leads to a lower citation precision score than the human evaluation.\\n\\nAlthough we believe our curated datasets closely resemble the distribution of real-world user questions, we acknowledge that they do not cover more challenging scenarios, such as multi-hop reasoning, math reasoning, and code completion.\\n\\nIn our experiments, we focus on prompting LLMs without updating their model weights. Training a model directly to incorporate citations remains challenging due to the lack of supervised data. However, we observe that certain human-instruction datasets contain examples similar to our task setup. We leave the exploration of training LLMs to generate citations for future work.\\n\\nAcknowledgments\\n\\nWe appreciate the helpful feedback from the members of the Princeton NLP group. We thank Alexander Wettig, Nelson Liu, Tianyi Zhang, Yu Meng, Sadhika Malladi, Yangsibo Huang, Zhiyuan Zeng, and Dan Friedman for the valuable discussion. We thank Surge AI (especially Anna Folinsky and Edison Chen) for their support with the human evaluation. Tianyu Gao is supported by an IBM PhD Fellowship. This research is supported by an NSF CAREER award (IIS-2239290), a Sloan Research Fellowship, and Microsoft Azure credits through the \\\"Accelerate Foundation Models Academic Research\\\" Initiative.\\n\\nReferences\\n\\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob Eisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. 2022. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037.\\n\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Milligan, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning (ICML), volume 162, pages 2206\u20132240.\\n\\nSamuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In Empirical Methods in Natural Language Processing (EMNLP).\\n\\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS).\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality.\\n\\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Association for Computational Linguistics (ACL), pages 3558\u20133567.\\n\\nMartin Funkquist, Ilia Kuznetsov, Yufang Hou, and Iryna Gurevych. 2022. CiteBench: A benchmark for Scientific Citation Text Generation. arXiv preprint arXiv:2212.09577.\\n\\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. 2023. RARR: Researching and revising what language models say, using language models. In Association for Computational Linguistics (ACL).\\n\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-augmented language model pre-training. In International Conference on Machine Learning (ICML).\\n\\nHangfeng He, Hongming Zhang, and Dan Roth. 2022. Rethinking with retrieval: Faithful large language model inference. arXiv preprint arXiv:2301.00303.\\n\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text generation. In International Conference on Learning Representations (ICLR).\\n\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. TRUE: Re-evaluating factual\"}"}
{"id": "emnlp-2023-main-398", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-398", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Riedel. 2021. KILT: a benchmark for knowledge-intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523\u20132544, Online. Association for Computational Linguistics.\\n\\nAleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Dmytro Okhonko, Samuel Broscheit, Gautier Izacard, Patrick Lewis, Barlas O\u011fuz, Edouard Grave, Wen-tau Yih, et al. 2021. The Web Is Your Oyster\u2013Knowledge-Intensive NLP against a Very Large Web Corpus. arXiv preprint arXiv:2112.09924.\\n\\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. MAUVE: Measuring the gap between neural text and human text using divergence frontiers. In Advances in Neural Information Processing Systems.\\n\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text Transformer. The Journal of Machine Learning Research (JMLR), 21(140).\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP), pages 2383\u20132392.\\n\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023. Measuring Attribution in Natural Language Generation Models. Computational Linguistics, pages 1\u201364.\\n\\nSamuel Joseph Amouyal Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, and Jonathan Berant. 2022. QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. arXiv preprint arXiv:2205.12665.\\n\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.\\n\\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 624\u2013643.\\n\\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning (ICML).\\n\\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. 2022. Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 373\u2013393.\\n\\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid questions meet long-form answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8273\u20138288, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model.\\n\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 809\u2013819.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135020, Online. Association for Computational Linguistics.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).\\n\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR).\\n\\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. 2023. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311.\\n\\nShiyue Zhang and Mohit Bansal. 2021. Finding a balanced degree of automation for summary evaluation. In Empirical Methods in Natural Language Processing (EMNLP), pages 6617\u20136632.\\n\\nYuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 1298\u20131308.\\n\\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training language models with memory augmentation. In Empirical Methods in Natural Language Processing (EMNLP), pages 5657\u20135673.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We elect not to use ROUGE-L as our main correctness metrics since it does not account for the different ways of expressing the same answer and it can be easily gamed (Krishna et al., 2021). We further illustrate this issue in Table 10. A system can easily achieve high ROUGE-L score by retrieving and returning the top passage from a BM25 index. However, the claims evaluation metric does not reward this approach since the output often lacks different aspects of the answers.\\n\\n| ROUGE-L Claim recall | ChatGPT V | ANILLA | 20.6 12.0 |\\n|----------------------|-----------|--------|----------|\\n| ChatGPT O RACLE      | 21.2      | 21.3   |\\n| LLaMa-13B V ANILLA   | 16.2      | 3.9    |\\n| Top-1 passage        | 19.1      | 3.0    |\\n\\nTable 10: Comparison between ROUGE-L and claim recall scores on ELI5.\\n\\nInstead, we leverage the original answers to generate sub-claims and use them to serve as an estimate of the different aspects of the answers that we expect the model to cover. This approach is inspired by works in summarization evaluation and claim verification (Zhang and Bansal, 2021; Kamoi et al., 2023; Wang et al., 2020).\\n\\nSpecifically, we use text-davinci-003 to generate the sub-claims. We first manually annotate three question and answer pairs from the original ELI5 training set with 3 sub-claims each. Then, we prompt text-davinci-003 with these pairs as demonstrations. The full prompt with an example is shown in Table 22.\\n\\nInstructGPT generates coherent and faithful sub-claims. To ensure that the generated sub-claims are of good quality, we manually inspect a random sample of 40 answers and their generated sub-claims (totaling to 120 sub-claims). For each sub-claim, we assign a score of 1 if it is relevant to the question and faithful to the facts presented in the ground truth, and 0 otherwise. We found that 112 out of the 120 (93.33%) sub-claims received a score of 1, meaning that our generated sub-claims are of high quality and faithful to the ground truth. Furthermore, the average number of words in the generated sub-claims is 14 words, and they are typically just one sentence long. This is aligned with the intent behind the metric\u2014to capture short factual claims made by the original answer.\\n\\nNLI model accurately predicts the entailment of sub-claims. We further analyze our sub-claim evaluation metrics by checking the error rate of the final prediction of the NLI model. To this end, we first manually annotate the entailment scores between 40 outputs and their sub-claims (in total of 120 pairs; these are the same questions from the previous analysis). We then use the NLI model to obtain the entailment scores for the output and sub-claims. Using the human annotations as the ground truth label, we found that the NLI model achieved an accuracy of 80.0%.\\n\\nB Dataset Statistics\\nFor ASQA, human answers have an average length of 65 words. For QAMPARI, each question has on average 13 answers. For ELI5, human answers have an average length of 131 words.\\n\\nC Implementation Details\\nNLI model. We use the version of TRUE model from https://huggingface.co/google/t5_xxl_true_nli_mixture, which is trained on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), Fever (Thorne et al., 2018), SciTail (Khot et al., 2018), PAWS (Zhang et al., 2019), and VitaminC (Schuster et al., 2021). This model uses the following prompt: \\\"premise: {PREMISE} hypothesis: {}\\\" and outputs \\\"1\\\" if the premise entails the hypothesis. We format each passage (when used as premise) by the format of \\\"Title: {TITLE}\\n{TEXT}\\\" and concatenate all passages with \\\"\\n\\\" as a separator.\\n\\nMAUVE. When running MAUVE, we concatenate the question and the model output (or human answer) by space. We truncate both the references and the model generations to 100 words, as we found MAUVE results are unstable beyond this length for ELI5 (this is due to that ELI5 has a lot of extremely long human answers).\\n\\nExact match for ASQA and QAMPARI. Both ASQA and QAMPARI provide aliases for their short answers. We normalize the response and the short answers similarly to Rajpurkar et al. (2016) and report the score with the best-matching aliases. For ASQA, Stelmakh et al. (2022) also propose a QA-based evaluation which we found to be not as stable, and thus we do not report it in our paper.\\n\\nOutput truncation. Before evaluation, we truncate.\"}"}
{"id": "emnlp-2023-main-398", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cates model output by new lines, as non-instruction-tuned models may generate more content after new lines that are irrelevant.\\n\\n**INTERACT.** Empirically, we found that models tend to execute too many consecutive \\\"check\\\" actions, so we force the model to always \\\"output\\\" after each \\\"check\\\". We limit the maximum number of passages to check as 3 to avoid exceeding the length limit. The full passages are removed from the context after one action to save context space. Table 27 provides an example for INTERACT.\\n\\n**Main experiments.** For all experiments except ChatGPT RERANK, we run each model three times with different seeds and each time we sample two demonstrations from a pool of four. We report the averaged scores for all experiments in the main paper and we report the standard deviations in Appendix G.6.\\n\\n**Decoding methods.** Based on preliminary experiments we choose the following decoding methods: For ChatGPT and GPT-4, we use sampling with temperature 0.5; for all open-source models, we use Nucleus sampling (Holtzman et al., 2020) and set top_p = 0.95.\\n\\n**DALCE Catches Shortcut Cases**\\n\\n| Fluency Correct | Citation Recall Discussion |\\n|-----------------|---------------------------|\\n| (MAUVE) (EM Rec.) | Rec. Prec. |\\n| ChatGPT 66.6 40.4 73.6 63.0 | Top-1 passage 20.8 35.1 99.4 99.4 |\\n| First 2 sents 67.2 18.9 98.7 98.7 | |\\n\\n**Table 11: ASQA cheating cases.** \\\"ChatGPT\\\": the ChatGPT VANILLA model with GTR-retrieved top-5 passages. \\\"Top-1 passage\\\": use the top-1 retrieved passage as the response. \\\"First 2 sents\\\": use the first 2 sentences of the top-1 retrieved passage.\\n\\n**F.1 Utility**\\n\\nTo check if the model output is useful to downstream users, we measure the utility of the response $S$. We first show the query $q$ and model response $S$ to the worker and ask them to rate their agreement with the statement \\\"The response is a helpful and informative answer to the query\\\" on a Likert scale of 1-5, corresponding to Strongly Disagree, Disagree, Neutral, Agree, and Strongly Agree.\\n\\n**F.2 Citation Recall**\\n\\nThe annotators are shown the question $q$, the statement $s_i$, and all of its citations $C_i$, and they rate if the joint set of citations fully support the statement (recall=1) or if they do not support all the claims (recall=0). We calculate the overall recall score for the generation by taking an average of all the statements' recall scores.\\n\\n**F.3 Citation Precision**\\n\\nWe show the question $q$ and a pair of a statement $s_i$ and one of its citation $c_{i,j} \\\\in C_i$ to the annotator. We ask the annotator if the citation fully supports,\"}"}
{"id": "emnlp-2023-main-398", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 12: Retrieval results for ASQA (EM recall).\\n\\nTable 13: Retrieval results for QAMPARI (recall-5).\\n\\npartially supports, or does not support the factual claims in $s_i$. Citation $c_{i,j}$ has a citation precision of 1 if $s_i$ has a recall of 1, and $c_{i,j}$ fully or partially supports $s_i$. Finally, we take an average of precision scores of all citations in the statement $S$ to obtain the citation precision score.\\n\\nG More Experiments\\n\\nG.1 Retrieval Analysis\\n\\nOracle. Since the original datasets do not contain gold passages at the same granularity level as our setting (100-word passages), we approximate gold passages by running the following algorithm on the top-100 retrieved passages. We first calculate the recall score for each passage. Then, we sort the passages using their recall score and take the top 5 passages as our initial oracle set. Finally, we iterate through all passages that were not initially in the oracle set and try to replace the passages in the oracle set in a greedy fashion: we calculate the change in the recall score of the oracle set for every possible replacement and proceed with the replacement that results in the largest recall improvement. The set of 5 oracle passages were able to match the recall scores of the top-100 retrieved passages.\\n\\nDetailed retrieval results. We show detailed retrieval results in Tables 12, 13, and 14.\\n\\nG.2 Effect of Instructions\\n\\nTable 15 shows results of using a full instruction (Table 23) and a short version of the instruction (Table 24). We see that the full version induces stronger correctness and citation recall, while the two instructions lead to similar citation precision.\\n\\nG.3 Effect of Demonstrations\\n\\nTable 16 shows results on effect of different numbers of demonstrations. We see that numbers of demonstrations do not affect ChatGPT's correctness but using at least one demonstration ensures high citation recall. For the original LLaMA model, Table 16 shows the trend that more demonstrations lead to better performance.\\n\\nG.4 Fine-tuned Models\\n\\nTo better understand the differences between fine-tuned models and prompted large language models, we train state-of-the-art question answering model, Fusion-in-Decoder (FiD; Izacard and Grave (2021)), and evaluate it in conjunction with POSTCITE. Due to the lack of training data with citation annotation, we first train a T5-base FiD model for 5 epochs on the ASQA training set with a batch size of 64 and a learning rate of 1e-4. During evaluation, we use POSTCITE to add citations to the output. We also use $k = 5$ passages during both training and evaluation of the FiD model.\\n\\nThen, we evaluate this model on both ASQA (in-domain) and ELI5 (out-of-domain), and the results can be found in Tables 17 and 18. Note that this is not a direct comparison, as ALCE assumes only evaluation data available and uses only few-shot data for prompting. As the results show, the FiD baseline still significantly lags behind prompting ChatGPT in both correctness and citation quality (even though it is trained on 4000+ examples). When tested on another dataset (ELI5), FiD performs even worse, showing that it is challenging to solve the problem by fine-tuning a small pre-trained model.\"}"}
