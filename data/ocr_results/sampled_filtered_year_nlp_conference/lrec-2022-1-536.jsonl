{"id": "lrec-2022-1-536", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Construction and Evaluation of the LEAFTOP Dataset of Automatically Extracted Nouns in 1480 Languages\\n\\nGreg Baker, Diego Molla-Aliod\\nMacquarie University\\n4 Research Park Drive\\ngregory.baker2@hdr.mq.edu.au, diego.molla-aliod@mq.edu.au\\n\\nAbstract\\nThe LEAFTOP (language extracted automatically from thousands of passages) dataset consists of nouns that appear in multiple places in the four gospels of the New Testament. We use a naive approach \u2014 probabilistic inference \u2014 to identify likely translations in 1480 other languages. We evaluate this process and find that it provides lexiconaries with accuracy from 42% (Korafe) to 99% (Runyankole), averaging 72% correct across evaluated languages. The process translates up to 161 distinct lemmas from Koine Greek (average 159). We identify nouns which appear to be easy and hard to translate, language families where this technique works, and future possible improvements and extensions. The claims to novelty are: the use of a Koine Greek New Testament as the source language; using a fully-annotated manually-created grammatically parse of the source text; a custom scraper for texts in the target languages; a new metric for language similarity; a novel strategy for evaluation on low-resource languages.\\n\\nKeywords: leaftop, bible, corpus, Koine Greek, lexicon, low-resource languages\\n\\n1. Introduction\\nThis paper discusses a large new dataset designed to be useful for tasks involving part-of-speech identification, grammar morphology and language similarity measures which was created by extracting vocabulary from Bible translations. It establishes a baseline of accuracy for target language noun extraction using sensible naive techniques at scale, for the languages spoken by the majority of the world's population.\\n\\nThe resulting LEAFTOP dataset is both wide (1480 languages) and deep (160 nouns). For inflected languages it contains forms for number (singular vs plural) for all nouns; for some nouns it can also supply gender and case variations.\\n\\nThe Ethnologue (Eberhard et al., 2021) reports the existence of 7,139 living languages. There are surprisingly few data sets which cover a large proportion of the world's population that also have sufficient depth for machine learning techniques. There are only 82 nouns listed in the extended Swadesh 207 list; the ASJP database (Wichmann and Brown, 2020) covers only the Swadesh 100 and is therefore even smaller with only 46 nouns. This is a vocabulary that is too small for many machine learning techniques. Panlex (Kamholz et al., 2014) has sufficient depth and structure for many tasks,\"}"}
{"id": "lrec-2022-1-536", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The LEAFTOP extraction approach works surprisingly well for languages in the Niger-Congo family and for some Austronesian languages, and less well for Trans-New Guinea languages. But unfortunately has surprising gaps for the grammar morphology task we were pursuing.\\n\\nBible translations exist for the native languages of 80% of the world's population (Wycliffe, 2020). As of the end of December 2020, the Bible has been fully translated into 717 languages. A further 1,582 languages have a New Testament translation. As discussed in section 4, the four gospels alone are sufficient to generate singular and plural forms for 160 nouns in most (but not all) of these 2,299 languages.\\n\\n2. Extensions of past work\\n\\nCompared to past work in lemma extraction and massively parallel language corpora, a number of small incremental changes have been made:\\n\\n- More successful scraping by writing site-specific parsers.\\n- An algorithm (algorithm 2) for identifying whether a language uses word markers and whether it has an alphabet.\\n- Automated identification of whether the translation attempted to be literal or used paraphrasing extensively.\\n- Used Koine Greek as the source language and leveraging existing manually-created part-of-speech annotations.\\n\\nMcCarthy et al. (2020) presented their work collating the Bible in 1600 languages at LREC in 2020, which re-used the CMU Wilderness Corpus (Black, 2019). They encountered limitations with this \u2014 verse alignment was challenging because the verse numbers are in-line in the text. Our improvement over this approach was to scrape from Bible gateways where the verse information is encoded in the metadata of the HTML.\\n\\n1 For example, as of the time of writing, Panlex correctly offers \u201cmfuasi\u201d as a translation for the English word \u201cdisciple\u201d, but without a direct English-to-Swahili translation, it offers \u201cchama\u201d (which is incorrect) as a two-step translation for \u201cdisciples\u201d.\\n\\n2 Prior to the 9th Circuit\u2019s decision on hiQ Labs, Inc. v. LinkedIn Corp in April 2022 scraping of this nature was of dubious legality.\\n\\nFigure 3: Identifying the written structure of the language. Korean is included as an alphabetic language with word break markers by algorithm 2 but is \u2014 as would be expected \u2014 an outlier. All other languages in the dataset are handled correctly.\\n\\nIn addition, where McCarthy et al. could only handle languages that have word-marker boundaries, the LEAFTOP dataset is able to distinguish between languages that have word marker boundaries, languages that don\u2019t have word marker boundaries that are nonetheless alphabetic (e.g. Thai, Khmer), and languages that don\u2019t have word marker boundaries that are non-alphabetic (e.g. Chinese characters, which LEAFTOP calls \u201cuni-token\u201d extractions) using the method in algorithm 2 (discussed in section 3.3. For alphabetic languages without word markers, the LEAFTOP dataset defaults to using quad-tokens, which is incorrect, and serves as a placeholder for future improvements.\\n\\nChristodouloupoulos and Steedman (2015) assembled a corpus of translations into 100 different languages, preferring the oldest common translation where multiple translations exist. Their hope was that this would not be too archaic in language and also be the most literal translation. This led to them choosing the King James Version (or Authorized Version) for English which sadly fails on both criteria. Our reservations on this choice led us to want to use statistical methods to identify the translations which are likely to be literal, using the consistency of the translations of each source lemma. If two translations into the same language...\\n\\n3 Which, uniquely among documents from the 17th century is not in the public domain. It is protected (still) by royal charter, but is allowed to be used for research purposes.\\n\\n4 Even at the time of publishing, \u201cyou\u201d was displacing \u201cthee\u201d in spoken English for example. The many mistranslations of the KJV are well known, see (Tsoraklidis, 2001) for a small sample.\"}"}
{"id": "lrec-2022-1-536", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nVocabulary extraction algorithm\\n\\n1. Let $L$ be the set of Greek lemmas that appear twice or more in the Gospels maintaining the same case, number and gender.\\n\\n2. Let $B$ be the Bible versions in the target language.\\n\\n3. For all $l \\\\in L$ do:\\n   - For all $b \\\\in B$ do:\\n     - $C \\\\leftarrow \\\\{\\\\text{verses present in translation } b\\\\}$\\n     - $V \\\\leftarrow C \\\\cap \\\\{\\\\text{verses where lemma } l\\\\}$\\n     - $U \\\\leftarrow C \\\\cap \\\\{\\\\text{verses where lemma } l\\\\}$\\n     - For $k \\\\leftarrow \\\\text{unigram, unitoken, quadtoken}$ do:\\n       - $T \\\\leftarrow \\\\emptyset$\\n       - For $v \\\\leftarrow V$ do:\\n         - $s \\\\leftarrow \\\\text{TOKENIZE VERSE}(b, v, k)$\\n         - $T \\\\leftarrow T \\\\cup s$\\n       - End for\\n     - $m \\\\leftarrow \\\\infty$\\n     - $n \\\\left\\\\{}$\\n       - For $t \\\\leftarrow T$ do:\\n         - $Q \\\\leftarrow \\\\{\\\\text{verses where } t\\\\}$\\n         - $R \\\\leftarrow \\\\{\\\\text{verses where } t\\\\}$\\n         - $X \\\\leftarrow |V \\\\cap Q|$\\n         - $Y \\\\leftarrow |V \\\\cap R|$\\n         - $Z \\\\leftarrow |U \\\\cap Q|$\\n         - $W \\\\leftarrow |U \\\\cap R|$\\n         - $p \\\\leftarrow \\\\text{BINOMTEST}(X,Y,Z,W,\\\\text{greater})$\\n         - If $p = m$ then:\\n           - $n \\\\left\\\\{ \\\\cup \\\\{t\\\\}$\\n         - Else if $p \\\\leq m$ then:\\n           - $n \\\\left\\\\{ \\\\{t\\\\}$\\n         - End if\\n       - End for\\n   - End for\\n   - End for\\n   - If $|n| = 1$ then:\\n     - $\\\\text{Trans}(b, l, k) \\\\leftarrow t$\\n   - End if\\nEnd for\"}"}
{"id": "lrec-2022-1-536", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Scatterplot of mean and standard deviation (across all languages) of the confidence of the vocabulary extraction for each Koine Greek lemma (with English translation)\\n\\nhave of the 4 Gospels of the New Testament, but the grammatical annotations of the Codex Sinaiticus (and the Codex Sinaiticus itself) are the most accessible to researchers \u2014 they are available on github (Porter et al., 2018). Our review of the annotations found few mistakes: \u03c3\u03c5\u03ba\u1fc6 (fig tree) is marked as neuter in Mark 11:13 instead of feminine; \u1f00\u03bb\u03ac\u03b2\u03b1\u03c3\u03c4\u03c1\u03bf\u03bd (jar for alabaster) is marked as feminine in Mark 14:3 instead of neuter. On no occasions was a noun marked as any other part of speech, nor any other part of speech marked as a noun. This is far from a perfect evaluation, but if these are the only problems, then the accuracy of these annotations is above 99.9%. The OpenText.org annotations also include Louw-Nida domains (Louw and Nida, 1998), which could be used in future projects for establishing p-adic word embeddings.\\n\\nThe XML sources were imported and all nouns whose annotated lemma did not begin with a capital letter (979 distinct lemmas) were identified and grouped based on gender (masculine, feminine, neuter), number and case (nominative, accusative, dative and genitive). Koine Greek (by the time of the New Testament) had lost the dual as a number, and only had singular and plural. Nouns that appeared only once in a given gender, number and case were dropped (leaving 567 lemmas, in 1185 forms).\\n\\nWe made the decision to limit the extraction to pairs of nouns that appear in both singular and plural forms, of which there are only 188. They appear in 666 different forms. On the one hand, this does allow for interesting grammar morphology tasks and substantially reduced the computation time required, but on the other hand, it is an arbitrary constraint that could be dropped in a future version of the dataset.\\n\\nIn practice, Algorithm 1 was never able to extract more than 161 lemmas. On average, it extracted 159.2 terms (standard deviation = 4.87).\\n\\n3.2. Scraping\\n\\nThere are 3370 verses in the Gospels that contain nouns; only 2724 of them contain one of the 188 lemmas, so a small optimisation is not to fetch verses that will not be useful. This also helped establish that the purpose of the scraping is for research and not to create a complete copy. The scraper was written to use Selenium (Software Freedom Conservancy, 2021) to control a web browser to fetch the data; allowing for the inefficiency of this, and long delays to avoid triggering CAPTCHAs, the process of scraping the 6,008,134 verses from www.bible.com took several weeks.\\n\\nThe scraper rejected 217 of the 2,356 Bible versions because of some fundamental problem \u2014 a required book of the Bible being not present being the most common. Disputed verses (such as the passage from John 7:51 \u2013 8:11, which was merged into the gospels later and therefore not present in all Bible translations) were captured as empty strings. This does not appear to reduce the accuracy of the lemma identification of the 10 nouns that appear in that passage that are also found in the repeated nouns list, since even the least common lemma \u03b4\u03ac\u03ba\u03c4\u03c5\u03bb\u03bf\u03c2 (finger) appears another 8 times in the Gospels.\\n\\nManual corrections were made for the Armenian Catholic Bible (which has many out-by-one misnumbered verses), and also for the Vulgate where some verses are coalesced.\\n\\nEach Bible scrape collected the language code for the translation, which is a near match to ISO639-3, except that languages can have variants based on geography or orthography. For example, there are separate translations for por and por_pt (Portuguese in different countries); and shu and shu_rom (Chadian Arabic in...\"}"}
{"id": "lrec-2022-1-536", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proportion Correct = 0.572\\n\\nln (Median LeafTop Confidence). P-value < 0.0001, $R^2 = 0.732$\\n\\nFigure 5: Relationship between confidence and accuracy with forced linear regression\\n\\nTraditional script or Roman script). These are stored as separate languages within LEAFTOP. Each ISO639-3 code (and variant) is connected to a language name and geography using a Wikidata extract.\\n\\n3.3. Vocabulary Extraction\\n\\nThe vocabulary extraction took 26,000 CPU hours, which was parallel-processed on a 64-cpu ARM processor in Amazon Web Services.11\\n\\nThe vocabulary extraction algorithm is given in Algorithm 1. Note that it inefficiently calculates results for all tokenisation methods \u2014 unigrams (single words), unitokens (single unicode points) and quadgrams. On completion, algorithm 2 was run to identify which tokenisation method is the most appropriate for the language, and other results are discarded.\\n\\nIn essence Algorithm 1 runs a one-sided binomial test asking whether a word or token in the target language appears improbably often in the same verses as a Koine Greek lemma; the word or token that is found most improbably often (having the lowest $p$-value from the binomial test) is declared to be the best translation. If there is a tie for least-likely-to-be-a-chance result, no word is chosen.\\n\\n3.3.1. Example\\n\\nConsider how \u03bc\u03bd\u03b7\u03bc\u03b5\u1fd6\u03bf\u03bd (tomb, grave) is translated in the World English Bible. \u03bc\u03bd\u03b7\u03bc\u03b5\u1fd6\u03bf\u03bd appears in 35 distinct verses in the gospels in various forms; it appears in the nominative singular in only two verses: John 19:41 and John 19:42.12\\n\\nThe scraping process successfully captured 2862 verses, but missed two of these 35 \u03bc\u03bd\u03b7\u03bc\u03b5\u1fd6\u03bf\u03bd verses.\\n\\n11 Ac6g.16xlarge spot instance.\\n12 [41] Now in the place where he was crucified there was a garden. In the garden was a new tomb in which no man had ever yet been laid. [42] Then because of the Jews' Preparation Day (for the tomb was near at hand) they laid Jesus there. \u2014 World English Bible\\n\\nSetting aside the other 31 verses because they have the lemma \u03bc\u03bd\u03b7\u03bc\u03b5\u1fd6\u03bf\u03bd in some other case or number leaves 2831 verses from which we can calculate a baseline probability-of-appearance for an English unigram.\\n\\nFocussing on the two verses with the nominative singular of \u03bc\u03bd\u03b7\u03bc\u03b5\u1fd6\u03bf\u03bd, the World English Bible translation shows 52 unigrams (including punctuation) of which 38 are distinct: unigrams such as \\\"tomb\\\", \\\"laid\\\", \\\"garden\\\" and \\\"the\\\".\\n\\nThe unigram \\\"tomb\\\" appears 6 times in the 2831 verses, giving it a baseline probability of $2 \\\\times 10^{-3}$; \\\"garden\\\" appears 4 times, giving it a baseline probability of $1 \\\\times 10^{-3}$; \\\"laid\\\" appears 29 times (baseline probability $1 \\\\times 10^{-2}$). At the other extreme, the unigram \\\"the\\\" appears 1,916 times, for a baseline probability of $0.68$.\\n\\nIt is unsurprising when \\\"the\\\" appears in a verse, and very surprising when \\\"garden\\\" does.\\n\\nThe unigram \\\"tomb\\\" appears in both John 19:41 and John 19:42, as do the unigrams \\\"the\\\" and \\\"laid\\\", but \\\"garden\\\" only appears in the first of these verses. We can then perform a one-sided binomial test for each unigram. For \\\"tomb\\\", $B(2, 2, 2 \\\\times 10^{-3}) = 4 \\\\times 10^{-6}$; \\\"laid\\\" $B(2, 1, 1.4 \\\\times 10^{-3}) = 1.04 \\\\times 10^{-4}$; \\\"garden\\\" $B(1, 2, 1.4 \\\\times 10^{-3}) = 2.8 \\\\times 10^{-3}$; \\\"the\\\" $B(2, 0, 0.68) = 0.46$.\\n\\nFrom this we can conclude that a valid translation for \u03bc\u03bd\u03b7\u03bc\u03b5\u1fd6\u03bf\u03bd into English is \\\"tomb\\\".\\n\\n3.3.2. Short-comings and failures\\n\\nAn obvious problem with this approach is that languages that inflect nouns with a case system that is substantially different to Koine Greek's \u2014 such as Arabic \u2014 are handled quite poorly, since there will be many distinct forms in the target language \\\"competing\\\" to be the best translation for each Koine Greek lemma.\\n\\nA more subtle problem arises when more than one lemma translates into the same word in a target language (such as \\\"fish\\\" in English being a translation for \u1f30\u03c7\u03b8\u03cd\u03c2 and \u1f40\u03c8\u03ac\u03c1\u03b9\u03bf\u03bd), since this alters the baseline appearance probability. The binomial test is very sensitive to changes in this baseline, and where there are mistakes in the LEAFTOP dataset, this is often the root cause.\\n\\n3.3.3. Confidence score\\n\\nThese p-values from the binomial tests can be remarkably small \u2013 the median p-value across all languages for translating \u03b8\u03b5\u03cc\u03c2 (God) is $4.46 \\\\times 10^{-17}$, so it is more convenient to work in terms of the negative base-10 log of the p-value.\\n\\nThe ratio of this negative log p-value of the best word to the second best word is recorded in the LEAFTOP database as the confidence score. In the \u03bc\u03bd\u03b7\u03bc\u03b5\u1fd6\u03bf\u03bd example, the next nearest alternative to \\\"tomb\\\" is \\\"laid\\\"; the ratio between their log p-values is $1.3$.\\n\\nAs discussed in section 4.2, the confidence score is a useful (but not sufficient) predictor of whether the vocabulary is correct.\"}"}
{"id": "lrec-2022-1-536", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Top 10 lemmas most likely to be extracted correctly\\n\\n| Lemma  | English | correct% | Rank |\\n|--------|---------|----------|------|\\n| \u03c0\u03c1\u03bf\u03c6\u03ae\u03c4\u03b7\u03c2 | prophet  | 98.4     | 161  |\\n| \u1f55\u03b4\u03c9\u03c1    | water    | 95.9     | 160  |\\n| \u1f14\u03c4\u03bf\u03c2    | year     | 95.5     | 159  |\\n| \u1f04\u03bd\u03b5\u03bc\u03bf\u03c2  | wind     | 94.9     | 157  |\\n| \u03c0\u03c1\u03cc\u03b2\u03b1\u03c4\u03bf\u03bd | sheep    | 94.9     | 157  |\\n| \u03c0\u03b1\u03c1\u03b1\u03b2\u03bf\u03bb\u03ae | parable  | 92.2     | 156  |\\n| \u03c7\u03b5\u03af\u03c1    | hand     | 91.4     | 155  |\\n| \u1f04\u03c1\u03c4\u03bf\u03c2   | bread    | 91.3     | 154  |\\n| \u03b8\u03b5\u03cc\u03c2    | God      | 91.3     | 153  |\\n| \u1f31\u03bc\u03ac\u03c4\u03b9\u03bf\u03bd | coat     | 90.3     | 152  |\\n\\nFigure 4 shows the mean and standard deviations of the confidence scores for each lemma. Words like \u03b8\u03b5\u03cc\u03c2 (God), \u03bc\u03b1\u03b8\u03b7\u03c4\u03ae\u03c2 (disciple) and \u03c7\u03b5\u03af\u03c1 (hand) are usually easy to identify in most target languages, which is unsurprising as they are very commonly used in the gospels and are unlikely to be paraphrased. These have very high confidence scores.\\n\\nConversely, words like \u03bb\u03af\u03c4\u03c1\u03b1 (a unit of measure), \u03c3\u03c4\u03b1\u03c6\u03c5\u03bb\u03ae (grapes) and \u03bb\u03cd\u03ba\u03bf\u03c2 (wolf) are usually the hardest to identify, suggesting that translators were either unable to be consistent in the way that they translate these terms or that these terms regularly appear as part of a repeated multi-term phrase.\\n\\n\u1f40\u03b4\u03bf\u03cd\u03c2 (tooth) is either easy to extract if the translators translated Matthew 5:38 very literally or nearly impossible to extract otherwise. Similarly the confidence in extracting \u03bb\u03cd\u03c7\u03bd\u03bf\u03c2 (lamp) is heavily influenced by the translators' choices in Luke 12:35.\\n\\n245 language codes have more than one translation available. For these languages, a consensus-by-vote for each lemma is taken based on the results from the Bible versions in that language. The confidence scores are multiplied and stored as cumulative_confidence in the LEAFTOP extracts for each language. Where there is a tie for the best word, nothing is chosen. For languages without a second translation, a pseudo-consensus (the answer derived from the sole translation) is used.\\n\\n4. Results\\n\\nThe LEAFTOP dataset has 625,351 distinct words in 1502 different languages. 22 of those languages are variant forms of some other language (e.g. zho_tw, urd_dv). Taking each language and considering each Koine Greek lemma as a concept, there are 239,156 distinct records.\\n\\nYou have heard that it was said, \\\"An eye for an eye, and a tooth for a tooth.\\\"\\n\u2014 World English Bible\\n\\nLet your waist be dressed and your lamps burning.\\n\u2014 World English Bible\\n\\nAn additive model is also being investigated.\"}"}
{"id": "lrec-2022-1-536", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2. Approaches for improving vocabulary accuracy\\nLinguists working with the LEAFTOP data may be willing to trade off a smaller vocabulary for higher accuracy. As shown by the trend line in Figure 5, confidence doesn\u2019t fully explain accuracy. Even when the regression is against the rank of the proportion correct, the $R^2$ is still only 0.83; so even a non-linear monotonic relationship is not fully explanatory. But in general, higher confidence scores are predictive of the higher accuracy. By setting a minimum confidence cut-off threshold, it is possible to have a smaller data set that has higher accuracy. Figure 6 shows the trade-offs that are possible. Considering Figure 5 again, words like wind, disease and finger are disproportionately likely to be extracted correctly, and conversely, religious terms such as prophet, God and parable are much harder to extract correctly given how often these words appear in the Gospels \u2014 it is common for the LEAFTOP algorithm to mistake the word for God (for example) in an usual case or number.\\n\\nThis offers an alternative approach, which is to filter out by vocabulary. Table 2 lists the lemmas that are the most likely to be correct, and Table 3 lists the lemmas that are most likely to be incorrect.\\n\\nFinally, it is rare for the singular and plural of a word to differ substantially, but there are many lemmas in the LEAFTOP database where the extracted singulars and plurals differ. \u03c7\u03c1\u03cc\u03bd\u03bf\u03c2 (time) is translated into German as Zeit in the singular (which is correct), and as l\u00e4ngere (\u201clonger\u201d) in the plural. An obvious filter that could be implemented for alphabetic languages is to count the number of letter sequences in common and find a threshold below which it is unlikely to be a correct translation; a Levenshtein distance (or equivalent) could also be used. A more sophisticated filter could be created by building a machine learning model to predict the plural from the singular and to discard lemmas where there is a mismatch. We are working on this latter approach.\\n\\n5. Exploratory Tools\\nThe last component of the LEAFTOP database is the explorer. This is an interactive set of web pages for showing the relationships between different languages. The idea behind it is that if two languages are related, then the challenges faced by translators should have been similar \u2014 words and concepts that do not map nicely from Koine Greek to one language should also be hard to map into a related language. Likewise, concepts that have straightforward mappings, should also be straightforward in a related language.\\n\\nThis is of course a vast simplification of a much more complex system and the goal is to explore the limitations of this toy model. This relatedness of two languages is quantified in the LEAFTOP dataset by the Spearman correlation statistic between the confidence scores of the two languages \u2014 for each lemma the confidence score from language 1 is the $x$ value and the confidence score for language 2 is the $y$ axis. The approximately-160 data points can then be correlated.\\n\\nTo visualise these correlation statistics and make an interesting interactive demonstration of this data, the first author created a D3.js (Bostok, 2018) force simulation model. Each language is modelled as a ball connected to other languages by a spring. The strength of the spring is proportional to the correlation. To simplify\"}"}
{"id": "lrec-2022-1-536", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the user interface, only the languages with the closest and strongest springs are shown to the user. Sample outputs are shown in Figure 7.\\n\\nThe results are simultaneously disappointing and exciting. Many languages are connected to each other correctly. Unfortunately, the languages spoken by European missionaries and translators correlate very strongly with many target languages in completely different language families \u2014 e.g. the confidence scores of Spanish and Hiligaynon are highly correlated across lemmas. Possible causes for this could be an implicit bias by translators; or could be related to the introduction of new vocabulary from the missionary's native language substituting for vocabulary that didn't exist previously; or it could simply be random noise.\\n\\n6. Conclusion\\n\\nThe LEAFTOP dataset is an extremely large collection of nouns across many different languages, with a measured accuracy across a variety of language families. It has been used for building multilingual pluralization models and for language exploration. There are straightforward extensions that could be done to improve its accuracy and coverage.\\n\\nThe source code for creating the dataset is https://github.com/solresol/thousand-language-morphology, and the final outputs (the dataset itself) are in https://github.com/solresol/leaftop.\\n\\n7. Acknowledgements\\n\\nThe authors would like to thank Daniel Everett and Mat\u00edas Guzm\u00e1n Naranjo for their support, and acknowledge the contributions of the freelance translators (some of whom have requested anonymity) who have checked the translations: Benazir Bhagad, Eleanor M. Mendoza, Maureen Y. Ong, Wewalage Roshan Chanaka Perera, Rim Sayed, Ferdaous J., Eric Ojecty, Farah Taymour, Owembabazi Don, Auma Sharot, Okotii Ruby, Okullo Joel, Bwambale Hamza, Frencelin Laurice, Sidime Amadou, Chimankpa Stanley, Moussa Keita, Paul Malanou, Uhtman Alake and especially Bradley Gewa for his tireless work finding an Australian Aboriginal language translator and all the translators for languages in New Guinea.\\n\\n8. Bibliographical References\\n\\nAgi\u0107, \u017d., Hovy, D., and S\u00f8gaard, A. (2015). If all you have is a bit of the Bible: Learning POS taggers for truly low-resource languages. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 268\u2013272, Beijing, China, July. Association for Computational Linguistics.\\n\\nBlack, A. W. (2019). Cmu wilderness multilingual speech dataset. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5971\u20135975.\\n\\nBostok, M. (2018). D3.js. https://d3js.org.\\n\\nChristodouloupoulos, C. and Steedman, M. (2015). A massively parallel corpus: the bible in 100 languages. Language resources and evaluation, 49(2):375\u2013395.\\n\\nEberhard, D., Simons, G., and Fennig, C. (2021). Ethnologue: Languages of the world.\\n\\nKamholz, D., Pool, J., and Colowick, S. M. (2014). Panlex: Building a resource for panlingual lexical translation. Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014).\\n\\nLand, C. D. and Pang, F. G. H., (2017). The Past, Present, and Future of the OpenText.org Annotated Greek Corpus, chapter 2, pages 69\u2014105. Brill.\\n\\nLouw, J. P. and Nida, E. A. (1998). Greek-English lexicon of the New Testament: Based on semantic domains. United Bible Societies, New York, NY, USA.\\n\\nMcCarthy, A., Vylomova, K., Wu, S., Malaviya, C., Wolf-Sonkin, L., Nicolai, G., Kirov, C., Silfverberg, M., Mielke, S., Heinz, J., Cotterell, R., and Hulden, M. (2019). The sigmorphon 2019 shared task: Morphological analysis in context and cross-lingual transfer for inflection. pages 229\u2013244, 01.\\n\\nMcCarthy, A. D., Wicks, R., Lewis, D., Mueller, A., Wu, W., Adams, O., Nicolai, G., Post, M., and Yarowsky, D. (2020). The Johns Hopkins University Bible corpus: 1600+ tongues for typological exploration. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 2884\u20132892, Marseille, France, May. European Language Resources Association.\\n\\nMetzger, B. M. (1991). Manuscripts of the Greek Bible: An Introduction to Palaeography. Oxford University Press.\\n\\nNicolai, G. and Yarowsky, D. (2019). Learning morphosyntactic analyzers from the Bible via iterative annotation projection across 26 languages. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1765\u20131774, Florence, Italy, July. Association for Computational Linguistics.\\n\\nPorter, S. E., O'Donnell, M. B., and Reed, J. T. (2018). Software Freedom Conservancy. (2021). Selenium. https://selenium.dev/.\\n\\nTsoraklidis, C. (2001). Mistranslations and Misinterpretations of Biblical Verses in the English and Modern Greek Versions of the Holy Scriptures. Citeseer.\\n\\nWichmann, S\u00f8ren, E. W. H. and Brown, C. H. (2020). Asjp database (version 19). https://doi.org/10.5281/zenodo.3843469.\\n\\nWycliffe. (2020). https://www.wycliffe.org.uk/about/our-impact/.\"}"}
