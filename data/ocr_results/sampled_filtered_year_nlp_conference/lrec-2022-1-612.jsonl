{"id": "lrec-2022-1-612", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MASALA: Modelling and Analysing the Semantics of Adpositions in Linguistic Annotation of Hindi\\n\\nAryaman Arora, Nitin Venkateswaran, Nathan Schneider\\n\\nGeorgetown University\\nWashington, D.C., USA\\n\\nAbstract\\n\\nWe present a completed, publicly available corpus of annotated semantic relations of adpositions and case markers in Hindi. We used the multilingual SNACS annotation scheme, which has been applied to a variety of typologically diverse languages. Building on past work examining linguistic problems in SNACS annotation, we use language models to attempt automatic labelling of SNACS supersenses in Hindi and achieve results competitive with past work on English. We look towards upstream applications in semantic role labelling and extension to related languages such as Gujarati.\\n\\nKeywords: SNACS, semantic parsing, Hindi\\n\\n1. Introduction\\n\\nCase markers express semantic roles, describing the relationship between the arguments they apply to and the action of a verb. Adpositions (prepositions, postpositions, and circumpositions) further express a range of semantic relations, including space, time, possession, properties, and comparison.\\n\\nLanguages have different strategies for encoding these kinds of semantic relations. Hindi\u2013Urdu uses a case-marking system along with a large postposition inventory (Kachru, 2006; Koul, 2008). Idiosyncratic bundling of case and adpositional relations poses problems in many natural language processing tasks for Hindi, such as machine translation (Ratnam et al. 2018, Jha 2017, Ramanathan et al. 2009, Rao et al. 1998) and semantic role labelling (Pal and Sharma 2019, Gupta 2019). Many models for these tasks rely on human-annotated corpora for training data, such as the one created for the Hindi\u2013Urdu PropBank (Bhatt et al., 2009), and in Kumar et al. (2019). The study of adposition and case semantics in corpora is also useful from a comparative/typological linguistic perspective, in comparing and categorizing the encoding of such relations across languages.\\n\\nTo that end, we release a completed Hindi corpus annotated for adposition and case semantic labels using the SNACS formalism (Schneider et al., 2018a, 2020). We approach the problem of automatic tagging of these labels using a variety of language models and explore what these models learn. Drawing on parallel SNACS corpora in English, German, Mandarin, and Korean, we compare strategies for encoding semantic roles across languages.\\n\\n1 Hindi and Urdu are two registers written in two different scripts of a single language (usually called 'Hindi\u2013Urdu' or 'Hindustani') with a largely identical grammar. While our corpus is in Hindi in the Devanagari script, the linguistic portions of our work (e.g. annotation guidelines) are applicable to Urdu as well.\\n\\n2. Background\\n\\nHindi is a language of India, of the Indo-Aryan branch of the Indo-European family, and one of the best-resourced South Asian languages for research in natural language processing and computational linguistics (Joshi et al., 2020). Hindi has a small number of core case markers as well as a large class of adpositions for signalling semantic relations. We will discuss the linguistic features of case and adposition in Hindi below, related work from linguistics in this area, and introduce the SNACS schema.\\n\\n2.1. Case and adposition in Hindi\\n\\nHindi is generally described as having three layers of case/adposition: the three basic morphological cases (example 1a), a small class of case markers/clitics that indicate core arguments to verbs (example 1b), and a larger class of postpositions governed by the genitive k\u00afa or ablative se (example 1c) (Kachru, 2006).\\n\\n(1) a. bacc\u0113 'children', bacc\u014dm 'children. OBL', baccoom 'children. VOC'\\n\\nb. usne 'she. ERG', usko 'she. ACC/DAT'\\n\\nc. uske_liye 'for her', uske_nazd\u00af\u0131k 'near her', uske_ 'under her' [code-switching]\\n\\nMasica (1993) grouped these three \u201clayers\u201d on the basis of historical development. Diachronically, morphological cases are the remnants of the Indo-European case system (via Sanskrit) that largely encode syntactic information, the case markers are Middle Indo-Aryan developments from spatial adverbs (e.g. Sanskrit upari \u2018above\u2019 > Hindi par \u2018LOC-on\u2019) that encode fundamental semantic roles on complements, and postpositions are more recent developments that even include borrowings from Persian, Arabic, and English and which indicate more concrete, e.g. spatial, relations between nominals.\\n\\nThe case markers most commonly mark relations between verbs and their arguments and adjuncts, followed by relations between nominals. Case markers in Hindi\"}"}
{"id": "lrec-2022-1-612", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are highly multi-functional even when using coarse de-\\nscriptors from linguistic typology; e.g. se is described\\nas indicating the ablative, instrumental, comitative, or\\ncomparative cases depending on context, respectively\\nexemplified in (2).\\n\\n(2) a. yah \u00afa \u02d9m se j\u00afa\u00afo 'Go away\\nfrom\\nhere.'\\nb. cammac se k\\n\u00afan\u00afa 'eating\\nwith\\na spoon'\\nc. us se mil\u00afu \u02d9mg\u00afa 'I will meet\\nwith\\nhim.'\\nd. das se kam 'less\\nthan\\nten'\\n\\nThat is not to say that se is three different case mark-\\ners; the semantic role of a se-marked argument is just\\nlicensed by the predicate or other governor of the ar-\\ngument. Understanding how and in what context such\\nmarkers indicate what semantic relations is an inter-\\nesting problem. Thus far, there is no semantically-\\nannotated corpus of case and postposition semantics\\nin Hindi, which motivated our annotation of this corpus.\\n\\n2.2. Related work\\n\\nThere is a great deal of work on case and adpositions\\nin Hindi. In syntax, some research topics are syn-\\ntactic differences between morphological case, case\\nmarkers, and adpositions (Spencer, 2005), the issue\\nof differential case marking in the ergative and dative\u2013\\naccusative (Bhatt and Anagnostopoulou, 1996; de Hoop\\nand Narasimhan, 2005; de Hoop and Narasimhan, 2009;\\nMontrul et al., 2015; Montaut, 2018), word order (Mo-\\nhanan, 1994), and agreement (Montrul et al., 2012).\\n\\nOn the other hand, there has been less research on\\nthe semantics of case and adpositions in Hindi. The\\nmapping of case-marked arguments to lexical-semantic\\nroles has been done in various computational projects\\n(Begum et al., 2008; Vaidya et al., 2011). Paul et al.\\n(2010) is an investigation of paraphrasing nominal com-\\npound relations with case in Hindi and English.\\n\\n2.3. SNACS\\n\\nThe Semantic Network of Adposition and Case Super-\\nsenses (SNACS; Schneider et al., 2018a, 2020) is a mul-\\ntilingual annotation scheme with 50 supersenses that\\ncharacterize the use of adpositions and case markers\\nat a coarse level of granularity. This scheme is akin\\nto linguistic models of argument structure such as se-\\nmantic roles and theta roles (including traditional cat-\\negories such as A\\nGENT\\nand T\\nHEME\\n), but expanded to\\ninclude roles for adpositional relations, such as W\\nHOLE\\nfor whole\u2013part, S\\nOCIAL\\nR\\nEL\\nfor interpersonal relations,\\netc.\\n\\nA useful feature of SNACS is the construal system\\n(Hwang et al., 2017), which allows an annotator to give\\none label for the morphosyntactic role or inherent lexical\\nmeaning (function) and another label for the predicate-\\nlicensed semantic relation (scene role) of a token. This\\nis expressed as S\\nCENE\\nR\\nOLE\\n\u219d\\nFUNCTION\\nif they differ.\\n\\nExamples of SNACS annotation for Hindi are given\\nbelow.\\n\\nTable 1: Cumulative statistics of the Hindi corpus.\\n\\n(3) vah 3\\nSG\\nghar\\nhome\\nke_p\u00afas\\nL\\nOCUS\\nnear\\nhai\\nCOP\\nIND.3\\nSG\\n'He is near the house.'\\n\\n(4) mai \u02d9m 1\\nSG\\nus\\n3\\nSG\\nko\\nT\\nHEME\\nACC\\nkh\u00afa-t\u00afa\\neat-IPFV.\\n\\n(5) mai \u02d9m 1\\nSG\\nne\\nE\\nXPERIENCER\\n\u219d\\nA\\nGENT\\nERG\\nnad\u00af\u0131\\nriver\\nke_p\u00afar\\nL\\nOCUS\\n\u219d\\nP\\nATH\\nacross\\nek\\none\\nbacc\u00afa\\nchild.\\n\\n'Ve saw a child across the river.'\\n\\nSNACS, thus far, has been used to annotate the En-\\nglish STREUSLE corpus (Schneider and Smith, 2015),\\nThe Little Prince\\nin English and translations of it into Ko-\\nrean (Hwang et al., 2020), Mandarin (Peng et al., 2020),\\nand German (Prange and Schneider, 2021). There has\\nalso been annotation of L2 English (Kranzlein et al.,\\n2020). This effort has been accompanied by the release\\nof guidelines for annotator training, including for En-\\nglish (Schneider et al., 2020) and Hindi\u2013Urdu (Arora\\net al., 2021a). Some earlier works also discussed lin-\\nguistic issues in Hindi annotation (Arora et al., 2021b).\\n\\nThere is also an online interface for exploring\\nSNACS corpora and interactive annotation guidelines:\\nhttp://www.xposition.org/\\n(Gessler et al., 2022).\\n\\n3. Corpus and annotation\\n\\nThe corpus was the entirety of Nanh\u00afa R \u00afajkum\u00afar, the\\nHindi translation of the The Little Prince\\nby Antoine\\nde Saint-Exup\u00e9ry.\\n\\nWe used the SNACS annotation\\nscheme, of which a brief overview is given in \u00a72.3. An-\\nnotation was done by two Hindi speakers: A (the first\\nauthor, who is a native speaker) and B (the second au-\\nthor, who is highly proficient) during June 2020\u2013January\\n2021, and annotation guidelines were developed simulta-\\nneously (Arora et al., 2021a). Table 1 contains statistics\\nabout the final corpus, which was released in CoNLL-\\nU-Lex format with Universal Dependencies annotations\\ngenerated with Stanza (Qi et al., 2020).\\n\\nThe corpus is available at\\nhttps://github.com/\\naryamanarora/carmls-hi\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n2\\n\\n2\\n"}
{"id": "lrec-2022-1-612", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There were two phases of annotation. In the first, A annotated the whole corpus (including all case markers and adpositions) and developed basic guidelines. In the second, B annotated chapter-by-chapter and A and B adjudicated disagreements concurrently. B also annotated focus markers, which were not included as targets in the first phase. A final pass was then conducted over the whole corpus to reconcile any remaining annotation disagreements.\\n\\n3.1. Annotation targets\\n\\nFollowing Masica's (1993) analysis of Indo-Aryan languages, we annotated the Layer II and III function markers in Hindi. These include all of the simple case markers and all of the adpositions.\\n\\nWe also decided to annotate the suffix v\u00afal\u00afa when used in an adjectival sense (e.g. chot. \u00afa-v\u00afal\u00afa kamr\u00afa 'the room that is small'), the comparison terms jais\u00afa and jaise, the extent and similarity particle s\u00afa (chot. \u00afa-s\u00afa kamr\u00afa 'small-ish room'), and the emphatic particles bh\u00af\u0131, h\u00af\u0131, to (Koul, 2008, 137\u2013156). All of these modify the preceding token and mediate a semantic relation between their object and the object's governor, just as conventionally-designated postpositions do.\\n\\nThe directly-declined Layer I cases of nominative, oblique, and vocative were not annotated due to the much greater annotation load that would involve and how much greater the breadth of the annotations would be relative to other SNACS-annotated languages. This means verbal arguments without case clitics were not annotated. However, future work (especially with application to semantic role labelling) would benefit from such annotations, and similar work has been done on SNACS annotation of non-adpositionally-marked subjects and objects in English (Shalev et al., 2019).\\n\\n3.2. Linguistic issues\\n\\nSeveral linguistic features of Hindi\u2013Urdu adposition and case semantics posed difficulties in annotating. Some are examined below. The annotation process itself relied on grammatical analyses of Hindi such as Koul (2008), dictionaries (McGregor, 1993; Dasa, 1965\u20131975), and native speaker judgements.\\n\\nFunctions for case markers\\n\\nCase markers encode little lexical content relative to adpositions. Table 2 shows the dominance of case markers in every category; given their versatility, delineating their prototypical functions is difficult. For example, a comparative in Hindi\u2013Urdu is expressed with the ablative case marker se\u2014should the function be S_HERE (as expected for the ablative case) or the narrower C_REFERENCE in this sense? This is an unresolved question; in labelling, we chose narrower functions when their use seemed to be a relation that is not completely supplied by the predicate.\\n\\n3 ne (ergative), ko (dative-accusative), se (instrumental-ablative-comitative), k\u00afa/ke/k\u00af\u0131 (genitive), m\u0113m (locative-IN), tak (allative), par (locative-ON). Declined forms of the pronouns (including the reflexive apn\u00afa) were also included.\\n\\nIn other cases, with highly polysemous markers such as se, it is difficult to pick a single function corresponding to an obvious grammatical case. For example, the verb p\u00afuchn\u00afa 'to ask' takes an argument, marked with se, indicating the person being asked. This instance of se could be construed as the ablative case (reflecting the return of a response from the person asked) or the comitative case (indicating a co-participant in communication, exactly as for verbs such as kahn\u00afa 'to say').\\n\\nTo resolve this issue we looked to typological evidence, in keeping with SNACS's multilingual aims: the closely-related language Punjabi, which has separate ablative (to \u02d9m) and comitative (n\u00afal) markers, uses the ablative in this construction, so we labelled the function S_HERE.\\n\\nNon-nominative/ergative subjects\\n\\nThe AGENT is prototypically expressed with the ergative case marker ne or the unmarked nominative. To express modality, Hindi\u2013Urdu, like other Indo-Aryan languages, employs various aspectual light verbs along with differential subject marking (de Hoop and Narasimhan, 2005). One example is the dative subject indicating obligation:\\n\\n(7) a. mai \u02d9m-ne likh\u00afa write.\\n   PRF 1SG-ERG\\n   'I:O_ORIGINATOR \u219d AGENT wrote it.'\\n\\nb. mujh-kho likhn\u00afa write.\\n   INF 1SG.OBL-DAT\\n   par.\u00afa fall.\\n   PRF\\n   'I:O_ORIGINATOR \u219d ? had to write it.'\\n\\nIn these, the subject's scene role is O_ORIGINATOR as it is a producer of writing. In example 7b, an expression of obligation, the subject is not only compelled to act by some outer force (fitting a THEME) but is also performing the action unaided (AGENT). SNACS currently cannot resolve the conflict between these two equally valid functions; we currently label example 7b as O_ORIGINATOR \u219d RECIPIENT in keeping with the morphosyntax of the dative subject. The issue is a broader problem of dealing with force dynamics in semantic role labelling, and may require new labels.\\n\\nOther unconventional subjects are less problematic. South Asian languages near-universally have dative subjects EXPERIENCERS (Verma and Mohanan, 1990).\\n\\nFor these, the prototypical RECIPIENT subject is fitting. The passive subject also has the unambiguous function of AGENT, just as the English passive by.\\n\\nCausative constructions\\n\\nIndo-Aryan languages, through suffixation, derive indirect and direct causative verbs from intransitive verbs. Indirect causatives take an argument in the instrumental case that is an impelled agent, grammatically distinguished from a true INSTRUMENT:\"}"}
{"id": "lrec-2022-1-612", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Breakdown of label counts along various dimensions, divided between case markers and adpositions. Each of the 8 tables is independent. (E.g., the topmost 'Scene role' table shows that 8.8% of annotated targets in the corpus are case markers with the scene role EXPERIENCER.)\\n\\n(8) us-ne 3 SG.ERG cabh\u012b = se key. OBL = INS darv\u0101z\u0101 door. NOM khol\u0101 open. PRF 'She opened the door [with a key].'\\n\\n(9) us-ne 3 SG.ERG m\u0101lik = se owner. OBL = INS darv\u0101z\u0101 door. NOM khulv\u0101y\u0101 open. IND. CAUS. PRF 'She made [the landlord]: open the door.'\\n\\nMuch like an obligated agent, the impelled agent takes part in two events, exhibiting properties of both AGENT and THEME. Furthermore, an impelled agent can control INSTRUMENTs of its own, and there cannot be two participants in the scene with the same semantic role (Begum and Sharma, 2010). For SNACS, Shalev et al. (2019) mentioned similar issues in English. This construction was rare in our corpus, but we find the best solution for this is a new label for animate and ambiguously volitional counterparts to INSTRUMENT in the SNACS hierarchy, much like the distinction between inanimate CAUSER and animate AGENT.\\n\\nEmphatic particles Following work on SNACS for Korean, which created a new label FOCUS for \u201cpost-positions that indicate the focus of a sentence (FOC), contributing information such as contrastiveness, like-lihood, or value judgements\u201d (Hwang et al., 2020), we found that the Hindi emphatic particles h\u012b \u2018only\u2019, bh\u012b \u2018also, too\u2019, to (contrastive), and some uses of tak \u2018even\u2019 function as focus postpositions and thus merited annotation.\\n\\n3.3. Corpus analysis\\n\\nAnnotator agreement 2,368 targets (79.7% of the total) were annotated independently by both annotators.\\n\\nTable 3: Raw agreement on targets with at least 20 doubly-annotated instances in the corpus, sorted by agreement on the construal. Case markers are in bold.\\n\\nIn the first round of annotation by annotator A, the focus markers and a small number of case markers were not annotated. Cohen's \u03ba between both annotators for double-annotated targets was 0.78 on scene roles, 0.85 on functions, and 0.73 on construals (role \u2193 function), all of which are very high even compared to previous work on SNACS. It is not surprising that functions, which are inherent to the target type and less dependent on semantics, are easier to annotate than scene roles.\\n\\nTable 3 shows raw agreements on high-frequency targets, sorted by agreement on the construal label. Among the case markers, ne (ERG) and ko (ACC/DAT) are the easiest to annotate, which is unsurprising given that their usage is very consistent syntactically (subjects and objects/indirect objects, respectively). The\"}"}
{"id": "lrec-2022-1-612", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Estimated entropy of targets with at least 20 instances in the corpus. Case markers are in bold.\\n\\nLow agreement on tak (ALL, \u201cuntil, up to\u201d) was due to uncertainty over whether it indicates the endpoint of movement (GOAL) or the length of the distance covered to the endpoint (EXTENT); after adjudication, we standardised on the latter. The adposition ke pas \u201cnear\u201d had a similar problem, where we disagreed on whether POSSESSOR was an inherent syntactic function of it or a semantic extension of its spatial use.\\n\\nMarker and tag distributions\\n\\nCounts of targets and labels are presented in Table 2, which shows that case markers generally indicate core arguments of verbs (e.g. AGENT as in subjects of verbs) and basic spatial relations (FOCUS, SOURCE), focus markers have discourse uses, and adpositions indicate non-core adjuncts (e.g. PURPOSE \u2018in order to\u2019).\\n\\nSince Hindi has case markers, annotated targets were dominated by a few types with very large semantic breadth. We can operationalise a measure of semantic range using the entropy of the distribution of scene role labels, which are a coarse representation of semantics, for each case marker. Given a distribution x (the scene roles) with classes K, Shannon entropy (in bits) is defined as:\\n\\n\\\\[ H(x) = -\\\\sum_{k=1}^{K} p(x_k) \\\\log_2 p(x_k) \\\\] (1)\\n\\nWe further adjust for the sample size and distribution using the entropy estimator due to Chao and Shen (2003), which is suited for linguistic distributions (Arora et al., 2022). In Table 4, we report entropy of scene role for adpositions and case markers with at least 20 occurrences in the corpus. Case markers, as expected, occupy the top 5 places. However, tak (\u201cuntil, up to\u201d) and ergative-case ne are much less semantically diverse than the other case markers. Some of the more frequent adpositions are also very semantically diverse, but most are not and form a long tail.\\n\\n4. Automatic tagging\\n\\nGiven the recent abundance of language models (both multi- and monolingual) for Hindi, we were interested in how well SNACS labels could be automatically tagged. To that end, we trained a neural sequence tagger on the task of adposition and case marker segmentation and tagging of scene role and function. This is a sub-instance of the lexical semantic recognition (LSR) task first proposed in Liu et al. (2021), who approached it with models similar to those used for named entity recognition (NER). Our tagger feeds the output of a contextual language model through a biLSTM then to a CRF which emits the final tagging. We loaded language models through HuggingFace (Wolf et al., 2020) and implemented our models with PyTorch (Paszke et al., 2019) and AllenNLP (Gardner et al., 2018).\\n\\n4.1. Data preparation\\n\\nIn preparation for training a classifier, we converted the SNACS labels to the BIO scheme for sequence tagging. We only marked the label to be predicted on the B-tag of each sequence. In the case of the B-tagged-word being segmented into subwords by the language model being used, we labelled all non-initial subwords as I.\\n\\nFor example, using multilingual BERT the phrase uske pas \u2018behind them (sg.)\u2019 is tokenised into subwords and tagged for scene role labels as:\\n\\n_ us ke _ pas\\nB-Locus I I I\\n\\nThe sentences in the dataset are randomly split 80/10/10 between train/dev/test. Training occurs on the train set with period checks against the development set to measure convergence. Scores are reported on the test set.\"}"}
{"id": "lrec-2022-1-612", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2. Model\\n\\nThe language models we tested are IndicBERT (Kakwani et al., 2020), the original multilingual BERT (Devlin et al., 2019), MuRIL (Khanuja et al., 2021), XLM-RoBERTa (Conneau et al., 2020), and some of the models from the Indic-Transformers library (Jain et al., 2020).\\n\\nThe outputs from the language models are inputted to a 2-layer biLSTM with dropout of 0.3. Its output goes to a CRF, and the highest probability tags are outputted through Viterbi decoding. The number of epochs trained \\\\{30, 60\\\\}, the learning rate \\\\{0.0001, 0.0002, 0.0005, 0.001\\\\}, and LSTM layer size \\\\{64, 128, 256, 512\\\\} are manually tuned hyperparameters. We did experiment with other architectures (e.g. RNNs, Transformers instead of the LSTM) but this was the best architecture we found.\\n\\n4.3. Results\\n\\nWe report F1 scores on tagging in Table 5. The best model is the Indic-Transformers BERT, with a distilled version (that is more efficient) coming in a close second. It is surprising that multilingual language models perform much worse than monolingual ones; IndicBERT, for example, is barely better than the baseline. These results are also competitive with F1 scores on English SNACS tagging, which bodes well for future work on multilingual SNACS given the complexity of the Hindi case marker system.\\n\\nOne issue that was prevalent across models was tokenisation errors involving the Devanagari script. The Indic-Transformers models dropped the vowel markers for $u, \\\\bar{u}, e, ai$, the bindu (nasalisation marker), and the halant (vowel-killer) while tokenising. Somehow, they still were the highest-performing models; it is likely that with a fixed tokeniser and retraining they could have been even better.\\n\\nNevertheless, these are promising results, especially considering that the complex Hindi case system requires knowledge of verb frame semantics to accurately tag with SNACS.\\n\\n5. Conclusion\\n\\nWe released an annotated corpus for Hindi of semantic relations encoded by case markers and adpositions, using the multilingual SNACS schema. We presented analysis of the distribution of labels and annotator agreement, explored linguistic issues encountered in annotation that pose problems for SNACS, and ran experiments on automatic sequence tagging for SNACS in Hindi with language models and biLSTM-CRF. We show that this is a feasible computational task and hope that this guides further work on SNACS for other languages, especially for those related to Hindi.\\n\\nFuture work on SNACS could consider multilingual comparisons, building upon work on aligning Korean and English annotations (Hwang et al., 2020) and multilingual tagging as explored in this paper. Particularly, there is ongoing work on SNACS annotation of Gujarati, a language closely related to Hindi; multilingual tagging of the two would be an interesting next task. Leveraging SNACS annotations for upstream tasks is also underexplored, despite a growing interest in the semantic relations encoded in prepositions which have otherwise been understudied in NLP (Elazar et al., 2021). We hope that this corpus will also be useful for future study on semantics-reliant tasks in Hindi.\\n\\nAcknowledgements\\n\\nWe thank members of the CARMLS group (particularly Maitrey Mehta, Jena Hwang, and Vivek Srikumar) for stimulating discussions on case and adposition semantics, students in NERT, and the three anonymous reviewers for their helpful feedback.\\n\\n6. Bibliographical References\\n\\nArora, Aryaman, Meister, Clara, and Cotterell, Ryan (2022). Estimating the entropy of linguistic distributions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. doi:10.48550/ARXIV.2204.01469.\\n\\nArora, Aryaman, Venkateswaran, Nitin, and Schneider, Nathan (2021a). Hindi-Urdu Adposition and Case Supersenses v1.0. arXiv:2103.01399 [cs.CL].\\n\\nArora, Aryaman, Venkateswaran, Nitin, and Schneider, Nathan (2021b). SNACS annotation of case markers and adpositions in Hindi. In Proceedings of the Society for Computation in Linguistics 2021, pages 454\u2013458. Association for Computational Linguistics, Online.\\n\\nBegum, Rafiya, Husain, Samar, Bai, Lakshmi, and Sharma, Dipti Misra (2008). Developing verb frames for Hindi. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08). European Language Resources Association (ELRA), Marrakech, Morocco.\\n\\nBegum, Rafiya and Sharma, Dipti Misra (2010). A preliminary work on Hindi causatives. In Proceedings of the Eighth Workshop on Asian Language Resources, pages 120\u2013128. Coling 2010 Organizing Committee, Beijing, China.\\n\\nBhatt, Rajesh and Anagnostopoulou, Elena (1996). Object shift and specificity: Evidence from ko-phrases in Hindi. Papers from the main session of CLS, 32:11\u201322.\\n\\nBhatt, Rajesh, Narasimhan, Bhuvana, Palmer, Martha, Rambow, Owen, Sharma, Dipti, and Xia, Fei (2009). A multi-representational and multi-layered treebank for Hindi/Urdu. In Proceedings of the Third Linguistic Annotation Workshop (LAW III), pages 186\u2013189. Association for Computational Linguistics, Suntec, Singapore.\"}"}
{"id": "lrec-2022-1-612", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chao, Anne and Shen, Tsung-Jen (2003). Nonparametric estimation of Shannon's index of diversity when there are unseen species in sample. Environmental and Ecological Statistics, 10(4):429\u2013443.\\n\\nConneau, Alexis, Khandelwal, Kartikay, Goyal, Naman, Chaudhary, Vishrav, Wenzek, Guillaume, Guzm\u00e1n, Francisco, Grave, Edouard, Ott, Myle, Zettlemoyer, Luke, and Stoyanov, Veselin (2020). Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451. Association for Computational Linguistics, Online. doi:10.18653/v1/2020.acl-main.747.\\n\\nDasa, Syamasundara (1965\u20131975). Hindi \u015babda\u015bagara. Nagari Pracarini Sabha.\\n\\nde Hoop, Helen and Narasimhan, Bhuvana (2005). Differential case-marking in Hindi. In Amberber, Mengistu and de Hoop, Helen, editors, Competition and Variation in Natural Languages, Perspectives on Cognitive Science, pages 321\u2013345. Elsevier, Oxford. doi:https://doi.org/10.1016/B978-008044651-6/50015-X.\\n\\nde Hoop, Helen and Narasimhan, Bhuvana (2009). Ergative case-marking in Hindi. In Differential subject marking, pages 63\u201378. Springer.\\n\\nDevlin, Jacob, Chang, Ming-Wei, Lee, Kenton, and Toutanova, Kristina (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186. Association for Computational Linguistics, Minneapolis, Minnesota. doi:10.18653/v1/N19-1423.\\n\\nElazar, Yanai, Basmov, Victoria, Goldberg, Yoav, and Tsarfaty, Reut (2021). Text-based NP enrichment. arXiv:2109.12085 [cs].\\n\\nGardner, Matt, Grus, Joel, Neumann, Mark, Tafjord, Oyvind, Dasigi, Pradeep, Liu, Nelson F., Peters, Matthew, Schmitz, Michael, and Zettlemoyer, Luke (2018). AllenNLP: A deep semantic natural language processing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 1\u20136. Association for Computational Linguistics, Melbourne, Australia. doi:10.18653/v1/W18-2501.\\n\\nGessler, Luke, Blodgett, Austin, Ledford, Joseph, and Schneider, Nathan (2022). Xposition: An online multilingual database of adpositional semantics. In Proc. of LREC. Marseille, France.\\n\\nGupta, Aishwary (2019). Semantic Role Labeling for Indian languages. Ph.D. thesis, International Institute of Information Technology Hyderabad.\\n\\nHwang, Jena D., Bhatia, Archna, Han, Na-Rae, O'Gorman, Tim, Srikumar, Vivek, and Schneider, Nathan (2017). Double trouble: The problem of construal in semantic annotation of adpositions. In Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 178\u2013188. Association for Computational Linguistics, Vancouver, Canada. doi:10.18653/v1/S17-1022.\\n\\nHwang, Jena D., Choe, Hanwool, Han, Na-Rae, and Schneider, Nathan (2020). K-SNACS: Annotating Korean adposition semantics. In Proceedings of the Second International Workshop on Designing Meaning Representations, pages 53\u201366. Association for Computational Linguistics, Barcelona Spain (online).\\n\\nJain, Kushal, Deshpande, Adwait, Shridhar, Kumar, Laumann, Felix, and Dash, Ayushman (2020). Indic-transformers: An analysis of transformer language models for Indian languages. In ML-RSA @ NeurIPS 2020.\\n\\nJha, Sanjay Kumar (2017). Translation of English Prepositions into Hindi Postpositions. International Journal of Innovations in TESOL and Applied Linguistics, 3(4).\\n\\nJoshi, Pratik, Santy, Sebastin, Budhiraja, Amar, Bali, Kalika, and Choudhury, Monojit (2020). The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282\u20136293. Association for Computational Linguistics, Online. doi:10.18653/v1/2020.acl-main.560.\\n\\nKachru, Yamuna (2006). Hindi. Number 12 in London Oriental and African Language Library. John Benjamins Publishing.\\n\\nKakwani, Divyanshu, Kunchukuttan, Anoop, Golla, Satish, N.C., Gokul, Bhattacharyya, Avik, Khapra, Mitesh M., and Kumar, Pratyush (2020). IndicNLP-Suite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4948\u20134961. Association for Computational Linguistics, Online. doi:10.18653/v1/2020.findings-emnlp.445.\\n\\nKhanuja, Simran, Bansal, Diksha, Mehtani, Sarvesh, Khosla, Savya, Dey, Atreyee, Gopalan, Balaji, Margam, Dilip Kumar, Aggarwal, Pooja, Nagipogu, Rajiv Teja, Dave, Shachi, et al. (2021). Muril: Multilingual representations for Indian languages. arXiv preprint arXiv:2103.10730.\\n\\nKoul, Omkar N. (2008). Modern Hindi Grammar. Dunwoody Press.\"}"}
{"id": "lrec-2022-1-612", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kranzlein, Michael, Manning, Emma, Peng, Siyao, Wein, Shira, Arora, Aryaman, and Schneider, Nathan (2020). PASTRIE: A corpus of prepositions annotated with supersense tags in Reddit international English. In Proceedings of the 14th Linguistic Annotation Workshop, pages 105\u2013116. Association for Computational Linguistics, Barcelona, Spain.\\n\\nKumar, Ritesh, Lahiri, Bornini, and Ojha, Atul Kr. (2019). Cross-linguistic semantic tagset for case relationships. In Proceedings of TyP-NLP: The First Workshop on Typology for Polyglot NLP.\\n\\nLiu, Nelson F., Hershcovich, Daniel, Kranzlein, Michael, and Schneider, Nathan (2021). Lexical semantic recognition. In Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021), pages 49\u201356. Association for Computational Linguistics, Online. doi:10.18653/v1/2021.mwe-1.6.\\n\\nMasica, Colin P. (1993). The Indo-Aryan Languages. Cambridge University Press.\\n\\nMcGregor, R. S. (1993). The Oxford Hindi-English dictionary. Oxford University Press.\\n\\nMohanan, Tara (1994). Case OCP: A constraint on word order in Hindi. Theoretical perspectives on word order in South Asian languages, 185:216.\\n\\nMontaut, Annie (2018). The rise of differential object marking in Hindi and related languages. Studies in Diversity Linguistics, (19).\\n\\nMontrul, Silvina, Bhatt, Rakesh, and Girju, Roxana (2015). Differential object marking in Spanish, Hindi, and Romanian as heritage languages. Language, pages 564\u2013610.\\n\\nMontrul, Silvina A, Bhatt, Rakesh M, and Bhatia, Archna (2012). Erosion of case and agreement in Hindi heritage speakers. Linguistic Approaches to Bilingualism, 2(2):141\u2013176.\\n\\nPal, Riya and Sharma, Dipti (2019). A dataset for semantic role labelling of Hindi-English code-mixed tweets. In Proceedings of the 13th Linguistic Annotation Workshop, pages 178\u2013188. Association for Computational Linguistics, Florence, Italy. doi: 10.18653/v1/W19-4020.\\n\\nPaszke, Adam, Gross, Sam, Massa, Francisco, Lerer, Adam, Bradbury, James, Chanan, Gregory, Killeen, Trevor, Lin, Zeming, Gimelshein, Natalia, Antiga, Luca, Desmaison, Alban, Kopf, Andreas, Yang, Edward, DeVito, Zachary, Raison, Martin, Tejani, Alykhan, Chilamkurthy, Sasank, Steiner, Benoit, Fang, Lu, Bai, Junjie, and Chintala, Soumith (2019). Pytorch: An imperative style, high-performance deep learning library. In Wallach, H., Larochelle, H., Beygelzimer, A., d\u2019Alch\u00e9-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.\\n\\nPaul, Soma, Mathur, Prashant, and Kishore, Sushant (2010). Syntactic construct: An aid for translating English nominal compound into Hindi. In Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 32\u201338. Association for Computational Linguistics, Los Angeles, California.\\n\\nPeng, Siyao, Liu, Yang, Zhu, Yilun, Blodgett, Austin, Zhao, Yushi, and Schneider, Nathan (2020). A corpus of adpositional supersenses for Mandarin Chinese. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 5986\u20135994. European Language Resources Association, Marseille, France.\\n\\nPrange, Jakob and Schneider, Nathan (2021). Draw mir a sheep: A supersense-based analysis of German case and adposition semantics. K\u00fcnstliche Intelligenz, 35(2).\\n\\nQi, Peng, Zhang, Yuhao, Zhang, Yuhui, Bolton, Jason, and Manning, Christopher D. (2020). Stanza: A python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 101\u2013108. Association for Computational Linguistics, Online. doi:10.18653/v1/2020.acl-demos.14.\\n\\nRamanathan, Ananthakrishnan, Choudhary, Hansraj, Ghosh, Avishek, and Bhattacharyya, Pushpak (2009). Case markers and morphology: Addressing the crux of the fluency problem in English-Hindi SMT. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 800\u2013808. Association for Computational Linguistics, Suntec, Singapore.\\n\\nRao, D., Bhattacharya, P., and Mamidi, Radhika (1998). Natural language generation for English to Hindi human-aided machine translation. Proceedings of the International Conference on Knowledge Based Computer Systems.\\n\\nRatnam, D. Jyothi, Kumar, M. Anand, Premjith, B., Soman, K. P., and Rajendran, S. (2018). Sense disambiguation of English simple prepositions in the context of English\u2013Hindi machine translation system. In Margret Anouncia, S. and Wiil, Uffe Kock, editors, Knowledge Computing and Its Applications: Knowledge Manipulation and Processing Techniques, volume 1, pages 245\u2013268. Springer, Singapore.\\n\\nSchneider, Nathan, Hwang, Jena D., Bhatia, Archna, Srikumar, Vivek, Han, Na-Rae, O\u2019Gorman, Tim, Moeller, Sarah R., Abend, Omri, Shalev, Adi, Blodgett, Austin, and Prange, Jakob (2020). Adposition\"}"}
{"id": "lrec-2022-1-612", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Schneider, Nathan, Hwang, Jena D., Srikumar, Vivek, Prange, Jakob, Blodgett, Austin, Moeller, Sarah R., Stern, Aviram, Bitan, Adi, and Abend, Omri (2018a). Comprehensive supersense disambiguation of English prepositions and possessives. In Proc. of ACL, pages 185\u2013196. Melbourne, Australia.\\n\\nSchneider, Nathan, Hwang, Jena D., Srikumar, Vivek, Prange, Jakob, Blodgett, Austin, Moeller, Sarah R., Stern, Aviram, Bitan, Adi, and Abend, Omri (2018b). Comprehensive supersense disambiguation of English prepositions and possessives. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 185\u2013196. Association for Computational Linguistics, Melbourne, Australia. doi:10.18653/v1/P18-1018.\\n\\nSchneider, Nathan and Smith, Noah A. (2015). A corpus and model integrating multiword expressions and supersenses. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1537\u20131547. Association for Computational Linguistics, Denver, Colorado. doi:10.3115/v1/N15-1177.\\n\\nShalev, Adi, Hwang, Jena D., Schneider, Nathan, Srikumar, Vivek, Abend, Omri, and Rappoport, Ari (2019). Preparing SNACS for subjects and objects. In Proceedings of the First International Workshop on Designing Meaning Representations, pages 141\u2013147. Association for Computational Linguistics, Florence, Italy. doi:10.18653/v1/W19-3316.\\n\\nSpencer, Andrew (2005). Case in Hindi. In Proceedings of the LFG05 Conference, pages 429\u2013446. CSLI Publications Stanford, CA.\\n\\nVaidya, Ashwini, Choi, Jinho, Palmer, Martha, and Narasimhan, Bhuvana (2011). Analysis of the Hindi Proposition Bank using dependency structure. In Proceedings of the 5th Linguistic Annotation Workshop, pages 21\u201329. Association for Computational Linguistics, Portland, Oregon, USA.\\n\\nVerma, Mahendra K. and Mohanan, Karuvananur Puthanveettil (1990). Experiencer subjects in South Asian languages. Center for the Study of Language (CSLI).\\n\\nWolf, Thomas, Debut, Lysandre, Sanh, Victor, Chaudron, Julien, Delangue, Clement, Moi, Anthony, Cistac, Pierric, Rault, Tim, Louf, Remi, Funtowicz, Morgan, Davison, Joe, Shleifer, Sam, von Platen, Patrick, Ma, Clara, Jernite, Yacine, Plu, Julien, Xu, Canwen, Le Scao, Teven, Gugger, Sylvain, Drame, Mariama, Lhoest, Quentin, and Rush, Alexander (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345. Association for Computational Linguistics, Online. doi:10.18653/v1/2020.emnlp-demos.6.\"}"}
