{"id": "acl-2024-long-787", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| Training Steps | Layer 1 | Layer 3 | Layer 6 | Layer 9 | Layer 12 |\\n|---------------|---------|---------|---------|---------|----------|\\n| 500           | 55      | 55      | 55      | 55      | 55       |\\n| 1000          | 60      | 60      | 60      | 60      | 60       |\\n| 1500          | 65      | 65      | 65      | 65      | 65       |\\n| 2000          | 70      | 70      | 70      | 70      | 70       |\\n| 2500          | 75      | 75      | 75      | 75      | 75       |\\n| 3000          | 77.5    | 77.5    | 77.5    | 77.5    | 77.5     |\\n\\n**Figure 9:** Constituency probe accuracy for *Reverse* and *Hop* models over training steps. Span representations were extracted by averaging the last four hidden layers of GPT-2. Error bars indicate 95% confidence intervals across 5 training runs initialized with different random seeds and evaluated on different test samples.\\n\\n**Figure 10:** Constituency probe accuracy for *Reverse* and *Hop* models using span representations extracted from different GPT-2 layers (1, 3, 6, 9, 12) over training steps. Error bars indicate 95% confidence intervals across 5 training runs initialized with different random seeds and evaluated on different test samples.\"}"}
{"id": "acl-2024-long-787", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 11: Test perplexities for each Deterministic Shuffle model ($s = 21$ left, $s = 57$ middle, $s = 84$ right) on the Nondeterministic Shuffle test sample and all other Deterministic Shuffle test samples. Perplexities were taken on a sample of 10K test sentences from each shuffled test set. Error bars indicate 95% confidence intervals across 5 training runs initialized with different random seeds and evaluated on different test samples.\"}"}
{"id": "acl-2024-long-787", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Layer | Position |\\n|-------|----------|\\n|       | 2.6\u00b13.0  |\\n|       | 2.6\u00b13.0  |\\n|       | 20.2\u00b121.0|\\n|       | 2.6\u00b13.0  |\\n|       | 2.7\u00b13.2  |\\n|       | 19.5\u00b120.4|\\n|       | 2.6\u00b13.0  |\\n|       | 2.7\u00b13.3  |\\n|       | 18.7\u00b119.6|\\n|       | 2.6\u00b13.0  |\\n|       | 2.9\u00b13.4  |\\n|       | 17.6\u00b118.5|\\n|       | 2.6\u00b13.0  |\\n|       | 3.0\u00b13.6  |\\n|       | 17.1\u00b117.9|\\n|       | 2.6\u00b13.0  |\\n|       | 3.3\u00b14.2  |\\n|       | 16.8\u00b117.6|\\n|       | 2.6\u00b13.0  |\\n|       | 3.5\u00b14.4  |\\n|       | 16.1\u00b117.1|\\n|       | 2.6\u00b13.0  |\\n|       | 3.5\u00b14.4  |\\n|       | 15.9\u00b117.1|\\n|       | 2.6\u00b13.0  |\\n|       | 3.3\u00b14.1  |\\n|       | 16.2\u00b117.0|\\n|       | 2.6\u00b13.0  |\\n|       | 3.2\u00b14.0  |\\n|       | 17.3\u00b118.4|\\n|       | 2.6\u00b13.0  |\\n|       | 2.3\u00b13.3  |\\n|       | 21.1\u00b121.8|\\n|       | 1.4\u00b11.0  |\\n|       | 1.4\u00b11.0  |\\n|       | 91.6\u00b14.2 |\\n|       | 1.4\u00b11.0  |\\n|       | 1.4\u00b11.0  |\\n|       | 91.4\u00b14.3 |\\n|       | 1.4\u00b11.0  |\\n|       | 1.6\u00b10.9  |\\n|       | 91.0\u00b14.6 |\\n|       | 1.4\u00b11.0  |\\n|       | 1.6\u00b10.9  |\\n|       | 90.1\u00b15.4 |\\n|       | 1.4\u00b11.0  |\\n|       | 1.7\u00b10.9  |\\n|       | 89.6\u00b16.0 |\\n|       | 1.4\u00b11.0  |\\n|       | 1.9\u00b11.1  |\\n|       | 89.3\u00b16.0 |\\n|       | 1.4\u00b11.0  |\\n|       | 2.2\u00b11.8  |\\n|       | 89.0\u00b16.3 |\\n|       | 1.4\u00b11.0  |\\n|       | 2.3\u00b11.9  |\\n|       | 88.8\u00b16.3 |\\n|       | 1.4\u00b11.0  |\\n|       | 3.0\u00b11.9  |\\n|       | 86.3\u00b17.2 |\\n|       | 1.4\u00b11.0  |\\n|       | 3.3\u00b12.0  |\\n|       | 84.7\u00b17.2 |\\n|       | 1.4\u00b11.0  |\\n|       | 4.9\u00b13.7  |\\n|       | 80.3\u00b111.1|\\n|       | 1.4\u00b11.0  |\\n|       | 16.4\u00b116.6|\\n|       | 62.0\u00b115.5|\\n|       | 1.1\u00b12.0  |\\n|       | 1.1\u00b12.0  |\\n|       | 95.6\u00b12.8 |\\n|       | 1.1\u00b12.0  |\\n|       | 1.1\u00b12.0  |\\n|       | 95.4\u00b12.9 |\\n|       | 1.1\u00b12.0  |\\n|       | 1.2\u00b12.2  |\\n|       | 95.1\u00b13.1 |\\n|       | 1.1\u00b12.0  |\\n|       | 1.5\u00b12.5  |\\n|       | 94.8\u00b13.5 |\\n|       | 1.1\u00b12.0  |\\n|       | 1.7\u00b12.6  |\\n|       | 94.4\u00b13.6 |\\n|       | 1.1\u00b12.0  |\\n|       | 1.7\u00b12.6  |\\n|       | 94.2\u00b13.8 |\\n|       | 1.1\u00b12.0  |\\n|       | 2.1\u00b12.8  |\\n|       | 93.7\u00b13.8 |\\n|       | 1.1\u00b12.0  |\\n|       | 2.2\u00b12.8  |\\n|       | 93.3\u00b13.6 |\\n|       | 1.1\u00b12.0  |\\n|       | 5.1\u00b15.6  |\\n|       | 85.2\u00b113.5|\\n|       | 1.1\u00b12.0  |\\n|       | 10.5\u00b18.9 |\\n|       | 74.6\u00b117.4|\\n|       | 1.1\u00b12.0  |\\n|       | 21.5\u00b120.5|\\n|       | 62.9\u00b119.8|\\n|       | 1.1\u00b12.0  |\\n|       | 45.4\u00b126.4|\\n|       | 34.6\u00b132.7|\\n|       | 0.5\u00b10.2  |\\n|       | 0.5\u00b10.2  |\\n|       | 96.4\u00b12.0 |\\n|       | 0.5\u00b10.2  |\\n|       | 0.6\u00b10.4  |\\n|       | 96.4\u00b11.8 |\\n|       | 0.5\u00b10.2  |\\n|       | 0.7\u00b10.4  |\\n|       | 96.1\u00b12.0 |\\n|       | 0.5\u00b10.2  |\\n|       | 0.8\u00b10.4  |\\n|       | 96.0\u00b12.0 |\\n|       | 0.5\u00b10.2  |\\n|       | 0.9\u00b10.5  |\\n|       | 95.6\u00b12.3 |\\n|       | 0.5\u00b10.2  |\\n|       | 0.9\u00b10.5  |\\n|       | 95.3\u00b12.2 |\\n|       | 0.5\u00b10.2  |\\n|       | 1.3\u00b10.9  |\\n|       | 94.9\u00b12.0 |\\n|       | 0.5\u00b10.2  |\\n|       | 1.6\u00b11.1  |\\n|       | 94.5\u00b12.2 |\\n|       | 0.5\u00b10.2  |\\n|       | 6.0\u00b19.4  |\\n|       | 86.7\u00b113.6|\\n|       | 0.5\u00b10.2  |\\n|       | 20.7\u00b112.4|\\n|       | 69.5\u00b111.8|\\n|       | 0.5\u00b10.2  |\\n|       | 37.3\u00b119.7|\\n|       | 48.2\u00b116.2|\\n|       | 0.5\u00b10.2  |\\n|       | 60.9\u00b112.5|\\n|       | 22.0\u00b18.1 |\\n\\nFigure 12: Subject\u2013verb agreement interchange intervention accuracies (IIA) for NHOOP, with confidence intervals across models trained on 5 different random seeds. Vertical axes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention. td, ts, and tv represent the tokens for the determiner, subject, and verb, respectively.\"}"}
{"id": "acl-2024-long-787", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Step       | 300 Training Steps | 600 Training Steps | 900 Training Steps | 1200 Training Steps | 1500 Training Steps |\\n|------------|--------------------|--------------------|--------------------|--------------------|--------------------|\\n| Token      |                   |                    |                    |                    |                    |\\n| 11.6\u00b17.2   | 10.9\u00b18.1          | 35.5\u00b19.0           | 9.9\u00b111.1           | 10.9\u00b16.2           | 43.8\u00b19.3           |\\n| 84.4\u00b13.7   | 86.5\u00b13.9          | 10.0\u00b16.2           |                    |                    |                    |\\n| 87.4\u00b13.2   | 10.6\u00b16.3          |                     |                    |                    |                    |\\n| 66.6\u00b118.0  | 78.8\u00b111.1         | 48.7\u00b111.3          | 21.7\u00b113.2          | 48.0\u00b125.6          | 39.2\u00b124.9          |\\n| 81.0\u00b113.6  | 79.2\u00b111.9         | 77.6\u00b111.6          | 80.1\u00b113.8          | 66.4\u00b119.1          | 51.3\u00b112.1          |\\n| 67.0\u00b118.6  | 49.5\u00b111.6         | 81.0\u00b113.3          |                    |                    |                    |\\n| 77.8\u00b111.4  | 78.2\u00b111.5         | 67.0\u00b119.0          |                    |                    |                    |\\n| 80.5\u00b113.2  |                    |                     |                    |                    |                    |\\n| 80.9\u00b113.6  |                    |                     |                    |                    |                    |\\n| 80.1\u00b113.8  |                    |                     |                    |                    |                    |\\n| 79.2\u00b111.9  |                    |                     |                    |                    |                    |\\n| 78.2\u00b111.5  |                    |                     |                    |                    |                    |\\n| 77.8\u00b113.4  |                    |                     |                    |                    |                    |\\n| 77.6\u00b111.6  |                    |                     |                    |                    |                    |\\n| 77.8\u00b111.4  |                    |                     |                    |                    |                    |\\n| 77.6\u00b111.6  |                    |                     |                    |                    |                    |\\n| 77.5\u00b111.2  |                    |                     |                    |                    |                    |\\n| 77.4\u00b111.1  |                    |                     |                    |                    |                    |\\n| 77.3\u00b111.0  |                    |                     |                    |                    |                    |\\n| 77.2\u00b110.9  |                    |                     |                    |                    |                    |\\n| 77.1\u00b110.8  |                    |                     |                    |                    |                    |\\n| 77.0\u00b110.7  |                    |                     |                    |                    |                    |\\n| 76.9\u00b110.6  |                    |                     |                    |                    |                    |\\n| 76.8\u00b110.5  |                    |                     |                    |                    |                    |\\n| 76.7\u00b110.4  |                    |                     |                    |                    |                    |\\n| 76.6\u00b110.3  |                    |                     |                    |                    |                    |\\n| 76.5\u00b110.2  |                    |                     |                    |                    |                    |\\n| 76.4\u00b110.1  |                    |                     |                    |                    |                    |\\n| 76.3\u00b110.0  |                    |                     |                    |                    |                    |\\n| 76.2\u00b19.9   |                    |                     |                    |                    |                    |\\n| 76.1\u00b19.8   |                    |                     |                    |                    |                    |\\n| 76.0\u00b19.7   |                    |                     |                    |                    |                    |\\n| 75.9\u00b19.6   |                    |                     |                    |                    |                    |\\n| 75.8\u00b19.5   |                    |                     |                    |                    |                    |\\n| 75.7\u00b19.4   |                    |                     |                    |                    |                    |\\n| 75.6\u00b19.3   |                    |                     |                    |                    |                    |\\n| 75.5\u00b19.2   |                    |                     |                    |                    |                    |\\n| 75.4\u00b19.1   |                    |                     |                    |                    |                    |\\n| 75.3\u00b19.0   |                    |                     |                    |                    |                    |\\n| 75.2\u00b18.9   |                    |                     |                    |                    |                    |\\n| 75.1\u00b18.8   |                    |                     |                    |                    |                    |\\n| 75.0\u00b18.7   |                    |                     |                    |                    |                    |\\n| 74.9\u00b18.6   |                    |                     |                    |                    |                    |\\n| 74.8\u00b18.5   |                    |                     |                    |                    |                    |\\n| 74.7\u00b18.4   |                    |                     |                    |                    |                    |\\n| 74.6\u00b18.3   |                    |                     |                    |                    |                    |\\n| 74.5\u00b18.2   |                    |                     |                    |                    |                    |\\n| 74.4\u00b18.1   |                    |                     |                    |                    |                    |\\n| 74.3\u00b18.0   |                    |                     |                    |                    |                    |\\n| 74.2\u00b17.9   |                    |                     |                    |                    |                    |\\n| 74.1\u00b17.8   |                    |                     |                    |                    |                    |\\n| 74.0\u00b17.7   |                    |                     |                    |                    |                    |\\n| 73.9\u00b17.6   |                    |                     |                    |                    |                    |\\n| 73.8\u00b17.5   |                    |                     |                    |                    |                    |\\n| 73.7\u00b17.4   |                    |                     |                    |                    |                    |\\n| 73.6\u00b17.3   |                    |                     |                    |                    |                    |\\n| 73.5\u00b17.2   |                    |                     |                    |                    |                    |\\n| 73.4\u00b17.1   |                    |                     |                    |                    |                    |\\n| 73.3\u00b17.0   |                    |                     |                    |                    |                    |\\n| 73.2\u00b16.9   |                    |                     |                    |                    |                    |\\n| 73.1\u00b16.8   |                    |                     |                    |                    |                    |\\n| 73.0\u00b16.7   |                    |                     |                    |                    |                    |\\n| 72.9\u00b16.6   |                    |                     |                    |                    |                    |\\n| 72.8\u00b16.5   |                    |                     |                    |                    |                    |\\n| 72.7\u00b16.4   |                    |                     |                    |                    |                    |\\n| 72.6\u00b16.3   |                    |                     |                    |                    |                    |\\n| 72.5\u00b16.2   |                    |                     |                    |                    |                    |\\n| 72.4\u00b16.1   |                    |                     |                    |                    |                    |\\n| 72.3\u00b16.0   |                    |                     |                    |                    |                    |\\n| 72.2\u00b15.9   |                    |                     |                    |                    |                    |\\n| 72.1\u00b15.8   |                    |                     |                    |                    |                    |\\n| 72.0\u00b15.7   |                    |                     |                    |                    |                    |\\n| 71.9\u00b15.6   |                    |                     |                    |                    |                    |\\n| 71.8\u00b15.5   |                    |                     |                    |                    |                    |\\n| 71.7\u00b15.4   |                    |                     |                    |                    |                    |\\n| 71.6\u00b15.3   |                    |                     |                    |                    |                    |\\n| 71.5\u00b15.2   |                    |                     |                    |                    |                    |\\n| 71.4\u00b15.1   |                    |                     |                    |                    |                    |\\n| 71.3\u00b15.0   |                    |                     |                    |                    |                    |\\n| 71.2\u00b14.9   |                    |                     |                    |                    |                    |\\n| 71.1\u00b14.8   |                    |                     |                    |                    |                    |\\n| 71.0\u00b14.7   |                    |                     |                    |                    |                    |\\n| 70.9\u00b14.6   |                    |                     |                    |                    |                    |\\n| 70.8\u00b14.5   |                    |                     |                    |                    |                    |\\n| 70.7\u00b14.4   |                    |                     |                    |                    |                    |\\n| 70.6\u00b14.3   |                    |                     |                    |                    |                    |\\n| 70.5\u00b14.2   |                    |                     |                    |                    |                    |\\n| 70.4\u00b14.1   |                    |                     |                    |                    |                    |\\n| 70.3\u00b14.0   |                    |                     |                    |                    |                    |\\n| 70.2\u00b13.9   |                    |                     |                    |                    |                    |\\n| 70.1\u00b13.8   |                    |                     |                    |                    |                    |\\n| 70.0\u00b13.7   |                    |                     |                    |                    |                    |\\n| 69.9\u00b13.6   |                    |                     |                    |                    |                    |\\n| 69.8\u00b13.5   |                    |                     |                    |                    |                    |\\n| 69.7\u00b13.4   |                    |                     |                    |                    |                    |\\n| 69.6\u00b13.3   |                    |                     |                    |                    |                    |\\n| 69.5\u00b13.2   |                    |                     |                    |                    |                    |\\n| 69.4\u00b13.1   |                    |                     |                    |                    |                    |\\n| 69.3\u00b13.0   |                    |                     |                    |                    |                    |\\n| 69.2\u00b12.9   |                    |                     |                    |                    |                    |\\n| 69.1\u00b12.8   |                    |                     |                    |                    |                    |\\n| 69.0\u00b12.7   |                    |                     |                    |                    |                    |\\n| 68.9\u00b12.6   |                    |                     |                    |                    |                    |\\n| 68.8\u00b12.5   |                    |                     |                    |                    |                    |\\n| 68.7\u00b12.4   |                    |                     |                    |                    |                    |\\n| 68.6\u00b12.3   |                    |                     |                    |                    |                    |\\n| 68.5\u00b12.2   |                    |                     |                    |                    |                    |\\n| 68.4\u00b12.1   |                    |                     |                    |                    |                    |\\n| 68.3\u00b12.0   |                    |                     |                    |                    |                    |\\n| 68.2\u00b11.9   |                    |                     |                    |                    |                    |\\n| 68.1\u00b11.8   |                    |                     |                    |                    |                    |\\n| 68.0\u00b11.7   |                    |                     |                    |                    |                    |\\n| 67.9\u00b11.6   |                    |                     |                    |                    |                    |\\n| 67.8\u00b11.5   |                    |                     |                    |                    |                    |\\n| 67.7\u00b11.4   |                    |                     |                    |                    |                    |\\n| 67.6\u00b11.3   |                    |                     |                    |                    |                    |\\n| 67.5\u00b11.2   |                    |                     |                    |                    |                    |\\n| 67.4\u00b11.1   |                    |                     |                    |                    |                    |\\n| 67.3\u00b11.0   |                    |                     |                    |                    |                    |\\n| 67.2\u00b10.9   |                    |                     |                    |                    |                    |\\n| 67.1\u00b10.8   |                    |                     |                    |                    |                    |\\n| 67.0\u00b10.7   |                    |                     |                    |                    |                    |\\n| 66.9\u00b10.6   |                    |                     |                    |                    |                    |\\n| 66.8\u00b10.5   |                    |                     |                    |                    |                    |\\n| 66.7\u00b10.4   |                    |                     |                    |                    |                    |\\n| 66.6\u00b10.3   |                    |                     |                    |                    |                    |\\n| 66.5\u00b10.2   |                    |                     |                    |                    |                    |\\n| 66.4\u00b10.1   |                    |                     |                    |                    |                    |\\n| 66.3\u00b10.0   |                    |                     |                    |                    |                    |\\n\\nThe tokens for the determiner, subject, and verb. t1 . . . t4 represent the four tokens/words between the verb.\"}"}
{"id": "acl-2024-long-787", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| t1 | t2 | t3 | t4 |\\n|----|----|----|----|\\n| 5.8\u00b14.8 | 5.9\u00b14.8 | 5.8\u00b14.9 | 7.2\u00b14.3 |\\n| 5.8\u00b14.8 | 6.0\u00b14.8 | 5.4\u00b14.2 | 5.6\u00b14.6 |\\n| 5.8\u00b14.8 | 5.8\u00b14.5 | 5.1\u00b13.7 | 5.2\u00b14.0 |\\n\\nFigure 14: Subject\u2013verb agreement interchange intervention accuracies (IIA) for W\\n\\n- (a) 300 Training Steps.\\n- (b) 600 Training Steps.\\n- (c) 900 Training Steps.\\n- (d) 1200 Training Steps.\\n- (e) 1500 Training Steps.\\n\\nThe tokens for the determiner, subject, and verb. t1 . . . t4 represent the four tokens/words between the verb.\"}"}
{"id": "acl-2024-long-787", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Table 1: Subject-verb agreement interchange intervention accuracies (IIA) for the NOHOP model trained without positional encodings, with confidence intervals across models trained on 5 different random seeds.\\n\\n| Steps     | td | ts | tv   | Accuracy | CI       |\\n|-----------|----|----|------|----------|----------|\\n| 300       | 2.3\u00b14.5 | 2.3\u00b14.5 | 25.1\u00b130.6 | 2.3\u00b14.5 | 2.7\u00b15.3 | 23.2\u00b128.6 | 2.3\u00b14.5 | 3.6\u00b16.4 | 20.4\u00b125.3 | ... |\\n| 600       | 1.7\u00b11.8 | 1.7\u00b11.8 | 58.3\u00b137.7 | 1.7\u00b11.8 | 1.6\u00b11.5 | 58.3\u00b137.6 | 1.7\u00b11.8 | 1.7\u00b11.5 | 56.4\u00b139.9 | ... |\\n| 900       | 1.0\u00b10.7 | 1.0\u00b10.7 | 95.3\u00b16.4 | 1.0\u00b10.7 | 1.1\u00b10.9 | 95.3\u00b16.2 | 1.0\u00b10.7 | 1.2\u00b10.8 | 95.3\u00b15.8 | ... |\\n| 1200      | 0.6\u00b10.5 | 0.6\u00b10.5 | 98.5\u00b11.0 | 0.6\u00b10.5 | 0.7\u00b10.7 | 98.4\u00b11.0 | 0.6\u00b10.5 | 0.8\u00b10.6 | 98.3\u00b10.8 | ... |\\n| 1500      | 0.4\u00b10.3 | 0.4\u00b10.3 | 98.9\u00b10.4 | 0.4\u00b10.3 | 0.3\u00b10.3 | 98.9\u00b10.4 | 0.4\u00b10.3 | 0.3\u00b10.3 | 98.9\u00b10.4 | ... |\\n\\n**Figure 15**: Subject-verb agreement interchange intervention accuracies (IIA) for the NOHOP model trained without positional encodings, with confidence intervals across models trained on 5 different random seeds. Vertical axes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention. td, ts, and tv represent the tokens for the determiner, subject, and verb, respectively.\"}"}
{"id": "acl-2024-long-787", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"... the tokens for the determiner, subject, and verb. \\\\( t_1 \\\\ldots t_4 \\\\) represent the four tokens/words between the verb.\\n\\n(b) 600 Training Steps.\\n(f) 3000 Training Steps.\\n(d) 1200 Training Steps.\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement interchange intervention accuracies (IIA) for the T\\n\\n\\\\[ \\\\text{Td Ts Tv T1 T2 T3 T4} \\\\]\\n\\n... t\\n\\naxes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention.\\n\\nFigure 16: Subject\u2013verb agreement"}
{"id": "acl-2024-long-787", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| t1  | t2  | t3  | t4  | t5  | t6  | t7  | t8  | t9  | t10 | t11 | t12 | t13 | t14 | t15 | t16 | t17 | t18 | t19 | t20 | t21 | t22 | t23 | t24 | t25 | t26 | t27 | t28 | t29 | t30 | t31 | t32 | t33 | t34 | t35 | t36 | t37 | t38 | t39 | t40 | t41 | t42 | t43 | t44 | t45 | t46 | t47 | t48 | t49 | t50 | t51 | t52 | t53 | t54 | t55 | t56 | t57 | t58 | t59 | t60 | t61 | t62 | t63 | t64 | t65 | t66 | t67 | t68 | t69 | t70 | t71 | t72 | t73 | t74 | t75 | t76 | t77 | t78 | t79 | t80 | t81 | t82 | t83 | t84 | t85 | t86 | t87 | t88 | t89 | t90 | t91 | t92 | t93 | t94 | t95 | t96 | t97 | t98 | t99 | t100 | t101 | t102 | t103 | t104 | t105 | t106 | t107 | t108 | t109 | t110 | t111 | t112 | t113 | t114 | t115 | t116 | t117 | t118 | t119 | t120 | t121 | t122 | t123 | t124 | t125 | t126 | t127 | t128 | t129 | t130 | t131 | t132 | t133 | t134 | t135 | t136 | t137 | t138 | t139 | t140 | t141 | t142 | t143 | t144 | t145 | t146 | t147 | t148 | t149 | t150 | t151 | t152 | t153 | t154 | t155 | t156 | t157 | t158 | t159 | t160 | t161 | t162 | t163 | t164 | t165 | t166 | t167 | t168 | t169 | t170 | t171 | t172 | t173 | t174 | t175 | t176 | t177 | t178 | t179 | t180 | t181 | t182 | t183 | t184 | t185 | t186 | t187 | t188 | t189 | t190 | t191 | t192 | t193 | t194 | t195 | t196 | t197 | t198 | t199 | t200 | t201 | t202 | t203 | t204 | t205 | t206 | t207 | t208 | t209 | t210 | t211 | t212 | t213 | t214 | t215 | t216 | t217 | t218 | t219 | t220 | t221 | t222 | t223 | t224 | t225 | t226 | t227 | t228 | t229 | t230 | t231 | t232 | t233 | t234 | t235 | t236 | t237 | t238 | t239 | t240 | t241 | t242 | t243 | t244 | t245 | t246 | t247 | t248 | t249 | t250 | t251 | t252 | t253 | t254 | t255 | t256 | t257 | t258 | t259 | t260 | t261 | t262 | t263 | t264 | t265 | t266 | t267 | t268 | t269 | t270 | t271 | t272 | t273 | t274 | t275 | t276 | t277 | t278 | t279 | t280 | t281 | t282 | t283 | t284 | t285 | t286 | t287 | t288 | t289 | t290 | t291 | t292 | t293 | t294 | t295 | t296 | t297 | t298 | t299 | t300 | t301 | t302 | t303 | t304 | t305 | t306 | t307 | t308 | t309 | t310 | t311 | t312 | t313 | t314 | t315 | t316 | t317 | t318 | t319 | t320 | t321 | t322 | t323 | t324 | t325 | t326 | t327 | t328 | t329 | t330 | t331 | t332 | t333 | t334 | t335 | t336 | t337 | t338 | t339 | t340 | t341 | t342 | t343 | t344 | t345 | t346 | t347 | t348 | t349 | t350 | t351 | t352 | t353 | t354 | t355 | t356 | t357 | t358 | t359 | t360 | t361 | t362 | t363 | t364 | t365 | t366 | t367 | t368 | t369 | t370 | t371 | t372 | t373 | t374 | t375 | t376 | t377 | t378 | t379 | t380 | t381 | t382 | t383 | t384 | t385 | t386 | t387 | t388 | t389 | t390 | t391 | t392 | t393 | t394 | t395 | t396 | t397 | t398 | t399 | t400 | t401 | t402 | t403 | t404 | t405 | t406 | t407 | t408 | t409 | t410 | t411 | t412 | t413 | t414 | t415 | t416 | t417 | t418 | t419 | t420 | t421 | t422 | t423 | t424 | t425 | t426 | t427 | t428 | t429 | t430 | t431 | t432 | t433 | t434 | t435 | t436 | t437 | t438 | t439 | t440 | t441 | t442 | t443 | t444 | t445 | t446 | t447 | t448 | t449 | t450 | t451 | t452 | t453 | t454 | t455 | t456 | t457 | t458 | t459 | t460 | t461 | t462 | t463 | t464 | t465 | t466 | t467 | t468 | t469 | t470 | t471 | t472 | t473 | t474 | t475 | t476 | t477 | t478 | t479 | t480 | t481 | t482 | t483 | t484 | t485 | t486 | t487 | t488 | t489 | t490 | t491 | t492 | t493 | t494 | t495 | t496 | t497 | t498 | t499 | t500 | t501 | t502 | t503 | t504 | t505 | t506 | t507 | t508 | t509 | t510 | t511 | t512 | t513 | t514 | t515 | t516 | t517 | t518 | t519 | t520 | t521 | t522 | t523 | t524 | t525 | t526 | t527 | t528 | t529 | t530 | t531 | t532 | t533 | t534 | t535 | t536 | t537 | t538 | t539 | t540 | t541 | t542 | t543 | t544 | t545 | t546 | t547 | t548 | t549 | t550 | t551 | t552 | t553 | t554 | t555 | t556 | t557 | t558 | t559 | t560 | t561 | t562 | t563 | t564 | t565 | t566 | t567 | t568 | t569 | t570 | t571 | t572 | t573 | t574 | t575 | t576 | t577 | t578 | t579 | t580 | t581 | t582 | t583 | t584 | t585 | t586 | t587 | t588 | t589 | t590 | t591 | t592 | t593 | t594 | t595 | t596 | t597 | t598 | t599 | t600 | t601 | t602 | t603 | t604 | t605 | t606 | t607 | t608 | t609 | t610 | t611 | t612 | t613 | t614 | t615 | t616 | t617 | t618 | t619 | t620 | t621 | t622 | t623 | t624 | t625 | t626 | t627 | t628 | t629 | t630 | t631 | t632 | t633 | t634 | t635 | t636 | t637 | t638 | t639 | t640 | t641 | t642 | t643 | t644 | t645 | t646 | t647 | t648 | t649 | t650 | t651 | t652 | t653 | t654 | t655 | t656 | t657 | t658 | t659 | t660 | t661 | t662 | t663 | t664 | t665 | t666 | t667 | t668 | t669 | t670 | t671 | t672 | t673 | t674 | t675 | t676 | t677 | t678 | t679 | t680 | t681 | t682 | t683 | t684 | t685 | t686 | t687 | t688 | t689 | t690 | t691 | t692 | t693 | t694 | t695 | t696 | t697 | t698 | t699 | t700 | t701 | t702 | t703 | t704 | t705 | t706 | t707 | t708 | t709 | t710 | t711 | t712 | t713 | t714 | t715 | t716 | t717 | t718 | t719 | t720 | t721 | t722 | t723 | t724 | t725 | t726 | t727 | t728 | t729 | t730 | t731 | t732 | t733 | t734 | t735 | t736 | t737 | t738 | t739 | t740 | t741 | t742 | t743 | t744 | t745 | t746 | t747 | t748 | t749 | t750 | t751 | t752 | t753 | t754 | t755 | t756 | t757 | t758 | t759 | t760 | t761 | t762 | t763 | t764 | t765 | t766 | t767 | t768 | t769 | t770 | t771 | t772 | t773 | t774 | t775 | t776 | t777 | t778 | t779 | t780 | t781 | t782 | t783 | t784 | t785 | t786 | t787 | t788 | t789 | t790 | t791 | t792 | t793 | t794 | t795 | t796 | t797 | t798 | t799 | t800 | t801 | t802 | t803 | t804 | t805 | t806 | t807 | t808 | t809 | t810 | t811 | t812 | t813 | t814 | t815 | t816 | t817 | t818 | t819 | t820 | t821 | t822 | t823 | t824 | t825 | t826 | t827 | t828 | t829 | t830 | t831 | t832 | t83"}
{"id": "acl-2024-long-787", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nChomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.\\n\\n1 Introduction\\n\\nChomsky (2023), Chomsky et al. (2023), Moro et al. (2023), and Bolhuis et al. (2024) make very broad claims to the effect that large language models (LLMs) are equally capable of learning possible and impossible human languages. For these authors, it follows from this claim that LLMs cannot teach us anything about language, and so the claim (if true) would have significant consequences for linguistic methodology and potentially also for the viability of LLMs as the basis for robust language capabilities.\\n\\nThese authors state this claim in absolute terms. For example, Chomsky et al. (2023) flatly assert that LLMs \u201care incapable of distinguishing the possible from the impossible,\u201d Chomsky (2023) says this property \u201ccan\u2019t be modified,\u201d and Moro et al. (2023) write that \u201cthe distinction between possible versus impossible languages cannot be formulated by definition for LLM.\u201d Bolhuis et al. (2024) go so far as to claim that \u201cLLMs can produce \u2018impossible\u2019 languages [\u2026] just as well as (if not better than) natural language output.\u201d One might expect such strong claims to be supported by extensive formal analysis and/or experimental evidence. However, as far as we are aware, this is not the case. The sole experimental paper cited by the above authors is Mitchell and Bowers 2020\u2014an important and...\"}"}
{"id": "acl-2024-long-787", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"inspiring paper but not one that can resolve these questions on its own. In addition, linguists themselves do not even have an agreed upon notion of what defines the possible or the impossible languages, to say nothing of having formal results with respect to LLMs.\\n\\nHere we provide extensive new experimental evidence to inform the claim that LLMs are equally capable of learning possible and impossible languages in the human sense. Arguably, the central challenge for such work is the fact that there is no agreed-upon way of distinguishing these two groups. We do not feel positioned ourselves to assert such a definition, so we instead offer some examples of impossible languages on a continuum of intuitive complexity (Figure 1). Some of these examples seem intuitively impossible, such as random sentence-level shuffling of English words. Others operationalize less obvious but common claims in the linguistics literature about rules that are impossible, like those that depend on counting words.\\n\\nAll of our examples are, we take it, uncontroversial instances of impossible languages. Thus, our experiments can inform the core hypotheses as follows: if LLMs learn these languages as well as they learn natural languages, then the claims of Chomsky and others are supported (for the specific class of LLMs tested). Conversely, if LLMs do not learn these languages as well as the possible ones, it would call into question those assertions. In that case, proponents of those claims ought to provide examples of impossible languages that they find more informative, which we can then evaluate using our approach to further advance the discussion.\\n\\nOur experiments use GPT-2 small models (Radford et al., 2018, 2019), and our base training corpus is the BabyLM dataset (Warstadt et al., 2023), which we modify in various ways to implement our impossible languages. What we find is that these models indeed struggle to learn impossible languages, shown through three core experiments:\\n\\n- **Experiment 1**: We train GPT-2 models on our set of defined possible and impossible languages, measuring their learning efficiency through test set perplexities. We find that models trained on possible languages learn more efficiently, evident from lower perplexities achieved in fewer training steps.\\n\\n- **Experiment 2**: We more closely examine a set of languages that exhibit count-based verb marking rules, using surprisal comparisons to target the relevant patterns. We find that GPT-2s trained on possible languages are more surprised by ungrammatical constructions, indicating that models disprefer agreement rules involving counting.\\n\\n- **Experiment 3**: We dive deeper into the internal mechanisms that models may develop to learn such count-based grammar rules using causal abstraction analysis. We find that models develop natural, modular solutions to unnatural grammatical patterns.\\n\\nOverall, our experimental results strongly challenge the claims of Chomsky and others given above, and we believe they pave the way for even deeper discussions of LLMs as models of language learning. At the same time, we recognize that models and humans exhibit fundamental differences, but the extent to which models favor or disfavor natural languages can be influenced by specific architectural decisions (as demonstrated by our findings on tokenization and positional encodings). We hope this paper initiates a new line of work that explores how different model architectures can distinguish between the possible and impossible languages.\\n\\n---\\n\\n1 The code for this paper is available at https://github.com/jkallini/mission-impossible-language-models.\"}"}
{"id": "acl-2024-long-787", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"languages? Moro et al. (2023) claim that the class of impossible languages would use the \\\"opposite\\\" type of rules: those based on the linear order of words. Musso et al. (2003) provide a few concrete examples that involve counting word positions to mark features like negation and agreement, and we include languages with similar rules in our set of tested impossible languages.\\n\\nIt is important to also distinguish what is impossible from what is merely typologically marked, such as the word order patterns listed in Greenberg's (1963) language universals. Previous work has shown that such word order universals can arise through a language's optimization of communication efficiency, achieved by balancing complexity and ambiguity (Hahn et al., 2020; Futrell and Hahn, 2022). While our current exploration does not encompass attested languages, various impossible languages can similarly differ in their information-theoretic complexity, informing the patterns that lie at the boundary between possible and impossible.\\n\\n2.2 Training Language Models with Unnatural Word Orders\\n\\nThe only work cited by Chomsky that investigates neural language models' ability to learn impossible languages is Mitchell and Bowers 2020, which finds that recurrent neural networks (RNNs; Elman, 1990) trained on various unnatural language constructs, such as reversed sentences and randomized vocabularies, achieve high accuracy on a subject\u2013verb number agreement task. Other work turns to more recent Transformer-based language models (Vaswani et al., 2017), observing their sensitivity to word order and phrase structure (Alleman et al., 2021; Galke et al., 2023) as well as their surprising ability to learn from syntactic information alone (Huang et al., 2023). Studies by Sinha et al. (2021) and Abdou et al. (2022) debate the impact of tokenization, pretraining adjustments, and positional encodings in recovering word order information from shuffled languages. Further investigations into BERT's (Devlin et al., 2019) reliance on word order for grammatical role classification suggest that lexical cues alone may not always be sufficient for good performance (Papadimitriou et al., 2022; see also Hessel and Schofield, 2021; Pham et al., 2021).\\n\\n2.3 Language Models and Formal Languages\\n\\nA related line of research examines the abilities of neural language models to express formal languages, as defined by the Chomsky hierarchy (Chomsky, 1956, 1959). Human language is considered to be slightly more expressive than context-free languages due to certain syntactic phenomena that interleave constituents (Shieber, 1985; Joshi, 1985). Previous work has shown that RNNs or related models can represent variants of counter and D_{YCK} languages, which are context-free (Weiss et al., 2018; Merrill, 2019; Merrill et al., 2020; Hewitt et al., 2020).\\n\\nSimilar work on Transformer architectures has shown that, while they are theoretically Turing-complete provided arbitrary precision and decoder steps (P\u00e9rez et al., 2021), they cannot empirically model many regular and non-regular languages (Hahn, 2020; Ebrahimi et al., 2020; Deletang et al., 2023).\\n\\nThe inability of Transformer-based language models to learn more complex languages in the Chomsky hierarchy seems surprising, given their impressive performance on natural language. This could be interpreted as evidence that theoretically weak computational models are sufficient for expressing human language. Alternatively, Transformer-based models can be augmented to have inductive biases for nested, hierarchical structures through architecture changes, like the addition of a stack component (Hao et al., 2018; Murty et al., 2023), or data-centered approaches, like structural pretraining (Papadimitriou and Jurafsky, 2023).\\n\\n3 Impossible Languages\\n\\nCore to our experiments are the set of impossible languages we synthesize. In constructing these artificial counterfactual languages, we consider their information-theoretic attributes relevant to machine learning, such as entropy rate, as well as their formal linguistic characteristics, such as adherence to hierarchical grammatical structures. We believe that our choice of languages broadly spans the impossibility continuum hypothesized in Figure 1.\\n\\nConcretely, we specify impossible languages by defining perturbation functions of English sentences. These perturbation functions map English input sentences to sequences of tokens. We categorize our languages into three classes: *S_{HUFFLE}, *R_{EVERSE}, and *H_{OP}, defined in the next subsections. Each class has one control language that represents unaltered English, or a pattern that is very similar to English. Table 1 provides examples\\n\\n2Though counter and D_{YCK} languages are context-free, some of the variants in the cited work are regular.\"}"}
{"id": "acl-2024-long-787", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Class       | Language                          | Example 1                                               | Example 2                                               |\\n|-------------|-----------------------------------|---------------------------------------------------------|---------------------------------------------------------|\\n| *SHUFFLE    | NOSHUFFLE                         | He cleans his very messy bookshelf.                     | ... like English does.                                   |\\n|             | NONDETERMINISTIC SHUFFLE          | *HUFFLE*                                               | *HUFFLE*                                               |\\n|             | DETERMINISTIC SHUFFLE (s)         | *HUFFLE*                                               | *HUFFLE*                                               |\\n|             | LOCAL SHUFFLE (w)                 | *HUFFLE*                                               | *HUFFLE*                                               |\\n|             | EVEN ODD SHUFFLE                  | *HUFFLE*                                               | *HUFFLE*                                               |\\n\\nThe random shuffling function that generates the NONDETERMINISTIC SHUFFLE language is irreversible, resulting in sentences that are purely bags of words\u2014any structural information in the original linguistic signal is irretrievable. While the DETERMINISTIC SHUFFLE languages are created using a reversible perturbation function, this function operates in an entirely non-linguistic manner; words are ordered based solely on the random seed and sentence length, without considerations for linguistic features or information locality\u2014the property that, when parts of text predict each other, they are often close together (Futrell, 2019; Mansfield and Kemp, 2023). This method is arguably even less humanly feasible than NONDETERMINISTIC SHUFFLE, as it relies on an arbitrarily complex yet consistent rule to determine word order.\"}"}
{"id": "acl-2024-long-787", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 *R EVERSE Languages.\\n\\nThe *R EVERSE impossible languages involve reversals of all or part of input sentences.\\n\\n1. N O R EVERSE: The input sentence is tokenized, and a special marker token \\\\textit{R} is inserted at a random position in the token list. Like N O S HUFFLE, this language is most similar to English. We use it for comparison with other *R EVERSE languages.\\n\\n2. P ARTIAL R EVERSE: The input sentence is tokenized, a special marker token \\\\textit{R} is inserted at a random position in the list of tokens, and the following tokens are reversed.\\n\\n3. F ULL R EVERSE: The input sentence is tokenized, a special marker token \\\\textit{R} is inserted at a random position in the token list, and all tokens are reversed. The P ARTIAL R EVERSE language is inspired by the experiments of Mitchell and Bowers (2020) on partially reversed English data, though our experiments are not a direct replication, since we use a different model architecture and dataset. F ULL - R EVERSE may seem like a plausible language syntactically, but higher-level linguistic concepts like anaphora would be highly disrupted. The \\\\textit{R} tokens are placed at the same positions across the data in all *R EVERSE languages to control for the entropy introduced by their random placement.\\n\\n3.3 *H OP Languages.\\n\\nThe *H OP languages perturb verb inflection with counting rules.\\n\\n1. N O H OP: All 3rd-person present tense verbs in the input sentence are lemmatized, and the sentence is tokenized. For each 3rd-person present tense verb, a special marker representing the verb's number and tense is placed right after the lemmatized verb. Singular verbs are marked with a special token \\\\textit{S}, and plural verbs are marked with \\\\textit{P}. Like the other control languages, N O H OP has a pattern that is most similar to English.\\n\\n2. TOKEN H OP: Identical transformation to N O - H OP, but the special number/tense markers are placed 4 tokens after the verb.\\n\\n3. W ORD H OP: Identical transformation to N O - H OP and TOKEN H OP, but the special number/tense markers are placed 4 words after the verb, skipping punctuation.\\n\\nThese languages specifically investigate GPT-2's ability to learn grammar rules that involve counting the positions of words or tokens.\\n\\n4 Experiments\\n\\nWe run several experiments to assess GPT-2's learning of our impossible languages. Our first experiment (Section 4.2) uses perplexities as a general evaluation to compare how well each impossible language model has learned its own perturbed language and see whether this reflects the hypothesized impossibility continuum. In our second and third experiments, we conduct a closer examination of the *H OP languages. Given that their count-based verb marking rules appear to be the least clearly implausible among our proposed languages, we focus on examining these rules specifically through targeted assessments using surprisal theory (Section 4.3). Finally, we dive deeper into the mechanisms each *H OP model uses to predict their respective verb marking rules using causal abstraction analysis (Section 4.4). For all evaluations, we run tests on several model checkpoints to observe the learning process over intervals of training steps.\\n\\n4.1 Implementation Details\\n\\nFor each impossible language, we apply its perturbation function to each sentence of the BabyLM dataset (Warstadt et al., 2023) to create a transformed dataset. Appendix A provides details on preprocessing and formatting, and describes the language-specific filtering needed to achieve the criteria that define each language.\\n\\nWe train standard GPT-2 small models (Radford et al., 2018, 2019) on each impossible language. To produce confidence intervals for our experiments, we train 5 sets of models for each language using different random seeds, which affect the model parameter initialization and dataset shuffling during training. Training and model hyperparameter choices are detailed in Appendix B. The primary set of GPT-2 models we train have absolute positional encodings. We also train a set of GPT-2 small models with relative positional encodings. We also conduct a constituency probing experiment to test effects on GPT-2's implicit understanding of syntax, with minimal observed differences among models (see Appendix D).\"}"}
{"id": "acl-2024-long-787", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Experiment 1: Language Models Reflect the Impossibility Continuum\\n\\nWe train GPT-2 models on all of the languages described in Table 1, and evaluate each model's perplexities on a test set over the course of training. Test perplexities provide a general metric for the extent to which a model has learned a language.\\n\\nSetup. We sample 10K sentences from the BabyLM test set and perturb this sample for each impossible language. For a given impossible language model, we report the geometric mean of the individual sentence perplexities in the corresponding test sample.\\n\\nHypothesis. Models trained on possible languages will achieve lower average perplexities more quickly (as measured in training steps) than those trained on impossible languages.\\n\\nResults. Our results are in Figure 2. There are clear distinctions between model perplexities after about 500 training steps. First considering the *Shuffle models, the NoDeterministicShuffle model has the highest perplexities, followed by the three DeterministicShuffle models, indicating that GPT-2 is better at learning shuffling patterns when they are deterministic, invertible functions.\\n\\nThe prevalence of certain sentence lengths in the corpus could also limit the variety of sentence shuffles in the DeterministicShuffle languages, potentially resulting in similarly functioning words frequently occupying the same token positions, thus increasing their predictability. Following the sentence-level shuffles, the next models in the order of decreasing perplexity are the three LocalShuffle models, with smaller window sizes having lower perplexities. LocalShuffle\\\\((w = 3)\\\\) and EvenOddShuffle have perplexities closest to the NoShuffle model (which represents unaltered English), but NoShuffle consistently has the lowest perplexities throughout the training process.\\n\\nCompared to the *Shuffle models, the experimental *Reverse models have perplexities that are much closer to the NoReverse model, and PartialReverse is slightly better than FullReverse. For the *Hop languages, their respective control model again has the lowest perplexities, although differences among the models are quite minimal. This warrants our deep-dive into the particular verb marking patterns for this set of models.\\n\\n4.3 Experiment 2: Language Models Disprefer Counting Rules\\n\\nIn Experiment 1, we show that impossible languages are harder for GPT-2 to learn. However, perplexity is a coarse-grained metric of language learning, and the question remains: do language models disprefer counting rules?\\n\\nThis result is also supported by separate evaluations of each DeterministicShuffle model on test data from other shuffles (see Appendix E). Each model has lower perplexities on its own deterministic shuffle.\"}"}
{"id": "acl-2024-long-787", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Surprisal tests for each *HOP model over training steps. Error bars indicate 95% confidence intervals across 5 training runs initialized with different random seeds and evaluated on different test samples.\\n\\nModels learn natural grammatical structures better than impossible grammars? The structure of the *HOP languages invites a finer-grained evaluation of their verb marking rules. We use surprisals to measure how well each *HOP model can predict the placement of its verb marker tokens, $S$ and $P$. The surprisal $S(w_i)$ of a word $w_i$ is the negative log probability of $w_i$ given the context words $w_1, \\\\ldots, w_{i-1}$ that precede it:\\n\\n$$S(w_i) = - \\\\log_2 p(w_i | w_1, \\\\ldots, w_{i-1}).$$\\n\\nSurprisals have been used as acceptability judgments from neural language models to probe for their processing of syntactic information (Wilcox et al., 2018; Futrell et al., 2019; Hu et al., 2020; Wilcox et al., 2023) and have been shown to correlate with human sentence processing difficulty (Hale, 2001; Levy, 2008).\\n\\nSetup. To test the *HOP models' sensitivity to marker placement, we conduct two tests on a sample of 10K sentences extracted from the BabyLM dataset containing the verb marker tokens ($S$ or $P$). As an example, consider the following pair of sentences for the NOHOP language shown in (1).\\n\\n(1) a. He clean $S$ his very messy books he left.\\n\\nb.* He clean __ his very messy books he left.\\n\\nSentence (1-a) is an example in the NOHOP language, and (1-b) is an ungrammatical counterfactual in which the marker token does not appear.\\n\\nIn the first test, we compare the average surprisals of the marker tokens across the three *HOP languages, using grammatical examples like (1-a). In the case of (1-a), the marker is singular, and its surprisal $S(S)$ is defined as:\\n\\n$$S(S) = - \\\\log_2 p(S | He clean).$$\\n\\nWe average this surprisal value for instances of $S$ or $P$ in the test sample.\\n\\nIn the second test, we construct minimal pairs from the example sentences in which the marker token appears and does not appear, and then compare the surprisal of the marker token to the surprisal of the token that follows it, both conditioned on the same context. In example (1-b), the surprisal of the following token $S(his)$ is defined as:\\n\\n$$S(his) = - \\\\log_2 p(his | He clean).$$\\n\\nWe expect $S(his) - S(S)$ to be a large positive value. We average such surprisal differences over instances of the marker tokens in the test sample and similarly define marker surprisals and minimal pair configurations for the other *HOP languages.\\n\\nHypothesis. For the first surprisal test, our hypothesis is that the mean surprisal of the marker tokens across test examples will be smaller for the control language than for the impossible languages. For the second test, our hypothesis is that the mean surprisal difference across all test pairs will be larger for possible languages than for impossible ones.\\n\\nResults. Our results are presented in Figure 3. The NOHOP model, which has the verb marking pattern most similar to English, consistently has the lowest mean marker surprisal across training steps in test 1 (Figure 3a). The NOHOP model also has the highest mean surprisal difference across training steps in test 2 (Figure 3b).\"}"}
{"id": "acl-2024-long-787", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"steps in test 2 (Figure 3b). Both of these results indicate that GPT-2 has learned to expect the marker tokens when they follow a more natural grammatical pattern and was very surprised when they did not appear at the correct positions.\\n\\nGPT-2 learns to expect marker tokens at the right locations in the other HOP models, just not as well as the control. TOKEN HOP tends to have a lower marker surprisal and a higher mean surprisal difference compared to WORD HOP across training steps, indicating that GPT-2 is better at learning the verb marking rule when the units being counted are tokens instead of words.\\n\\n4.4 Experiment 3: Language Models Develop Natural Solutions to Unnatural Patterns\\n\\nExperiment 2 demonstrates that, while GPT-2 favors natural grammar rules, it is also capable of acquiring count-based grammar rules like those seen in the verb marking patterns of our HOP languages. But what sorts of internal mechanisms does it implement to learn such grammar rules, and how do these mechanisms compare to the more natural control? To address this, we conduct a final experiment using causal abstraction analysis, which offers an interpretability framework for identifying and examining causal mechanisms within neural models (Geiger et al., 2020, 2021; Wu et al., 2022, 2023a,b; Geiger et al., 2023). We employ the interchange intervention technique on our HOP models.\\n\\nSetup.\\n\\nWe use interchange interventions to identify representations in our HOP models that have causal effects on their output behaviors on a subject\u2013verb agreement task. In our experimental setup, $b$ is a sentence prefix with a singular subject and $s$ is an identical prefix with the plural form of the subject. These prefixes include all tokens up to but not including the markers ($S$ and $P$). We interchange the GPT-2 block outputs from processing $b$ with GPT-2 block outputs from processing $s$ and observe whether the probability of plural marker $P$ is higher than the probability of singular marker $S$.\\n\\nThe man be The men be\\n\\nThe man be The men be\\n\\nFigure 4: An interchange intervention on the NOHOP model with base input $b = \\\\text{The man be}$ and source input $s = \\\\text{The men be}$. The intervention is performed at the second layer and second token position, causing a change in prediction from $S$ to $P$ after the intervention. This is shown more concretely in Figure 4.\\n\\nWe run such interventions at each GPT-2 layer and token position to see which parts of the model cause a change in the marker prediction. We run all of these interventions over several test examples and report the interchange intervention accuracy (IIA), a metric that represents the subject\u2013verb agreement accuracy if the counterfactual (i.e. plural) were the ground truth. The test examples for each HOP model are extracted from their respective versions of the BabyLM test set, and minimally-different counterfactual examples are created by changing the singular subjects to plural subjects. To ensure that interventions on different examples are analogous, we use regular expressions to locate examples that follow the same structure (i.e. subjects and verbs at the same positions).\\n\\nResults.\\n\\nOur results are presented in Figure 5. The IIA graphs demonstrate how information about the marker tokens flows through the models. We can see that, in all three HOP models, IIA is high at the token position of the subject up until about layer 3; then there is a transition to the position of the last token in the prefix, preceding the location where the marker should be predicted. All models develop the same modular solution to the task by tracking agreement through the representations at the relevant positions, but the NOHOP model obtains nearly 100% IIA earlier during training, at about 1,500 training steps, supporting the previous surprisal results.\"}"}
{"id": "acl-2024-long-787", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Subject\u2013verb agreement interchange intervention accuracies (IIA) for each *HOP model over training steps. Vertical axes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention. td, ts, and tv represent the tokens for the determiner, subject, and verb, respectively. t1...t4 represent the four tokens/words between the verb and its marker for TOKENHOP and WORDHOP. IIA values are averaged over results from 5 models initialized on different random seeds. See Appendix F for confidence intervals.\\n\\n5 Discussion and Conclusion\\nContra claims by Chomsky and others that LLMs cannot possibly inform our understanding of human language, we argue there is great value in treating LLMs as a comparative system for human language and in understanding what systems like LLMs can and cannot learn. Prior explorations of neural language models have already been fruitful for understanding the generalization of syntactic principles from data (Wilcox et al., 2018; Marvin and Linzen, 2018; Futrell et al., 2019; Prasad et al., 2019; Hu et al., 2020). Our paper complements this line of work. We have shown that GPT-2 models do not master our set of synthetic impossible languages as well as natural ones, challenging the unfounded assertions stated previously.\\n\\nEven in the absence of a clear definition of what constitutes a possible or impossible language, we believe that our investigations advance this debate regarding LLMs. The lack of a definition does not hinder inquiry into this topic; in fact, it beckons further explorations of the boundary between the possible and impossible languages, as shown in our hypothesized continuum in Figure 1. We believe that the *HOP languages we propose closely approach this boundary.\\n\\nAt the same time, conclusions about LLMs\u2019 linguistic competence and preferences for natural languages should be informed by an understanding of the ways that models fundamentally differ from humans. For instance, we saw that models can perform operations that involve counting tokens because LLMs rely on tokens as basic units. While humans are sensitive to morpheme boundaries and word boundaries, it is unlikely humans rely on atomic tokens in the way that LLMs do. This does not mean that LLMs can fundamentally tell us nothing about human language. Rather, as we did here, it is valuable to consider and control for this difference before making generalizations.\\n\\nSince at least the 1950s, a major line of linguistic inquiry has focused on what aspects of syntactic structure can be learned just from data, without domain-specific innate priors (e.g. a Universal Grammar). LLMs lack strong in-built linguistic priors, yet they can learn complex syntactic structures. While many LLMs are trained with vastly more data than children see, there is increasing evidence that even systems trained on smaller amounts of data can learn interesting linguistic information (Warstadt et al., 2023). The current paper raises further questions along similar lines. Since we do find that real languages are more learnable by GPT-2, this leads us to wonder what inductive bias of GPT language models matches natural language. We believe that this inductive bias is related to information locality, the tendency for statistical correlations in text to be short range. Information locality arises in GPTs due to their autoregressive training objective and has been argued to arise in humans due to the incremental nature of real-time language processing (Futrell, 2019; Hahn et al., 2021).\\n\\nSince LLMs have been shown to learn the complex structures of human language and have a preference for learning such structures over unnatural counterfactuals, it follows that they are clearly relevant to investigations and claims about the necessary innate priors for language learning. Arguments that they are \u201cby design, unlimited in what they can \u2018learn\u2019\u201d and \u201cincapable of distinguishing the possible from the impossible\u201d (Chomsky et al., 2023) do not offer convincing evidence otherwise.\"}"}
{"id": "acl-2024-long-787", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments\\nThe authors would like to thank Aryaman Arora, Christiane Fellbaum, Roger Levy, Tristan Thrush, and Diyi Yang for helpful comments on the project. We would also like to thank the members of the Stanford NLP Group, the MIT Computational Psychology Lab, and the anonymous reviewers for useful discussions. Julie Kallini is supported by a National Science Foundation Graduate Research Fellowship under grant number DGE-2146755.\\n\\nLimitations\\nDue to resource constraints, we exclusively use the GPT-2 architecture to train models on our various synthetic impossible languages. Each of our experiments involves training a GPT-2 model from scratch on a different language dataset, and for every such language, we train multiple GPT-2 models to establish confidence intervals for our evaluation metrics. Applying this approach to several different model architectures would be quite resource-intensive, so we opted to choose a single architecture in this paper. Future work could apply our methodology to models trained with different architectures or training objectives.\\n\\nOur impossible languages are derived by manipulating an English dataset. While we do not conduct experiments that use other natural languages as a starting point, our experimental choices (i.e., the synthetic languages we design) are informed by linguistic diversity and typology, distinguishing our impossible languages from those that are rare but attested. However, future work might involve deriving impossible languages from base languages other than English and include more morphological manipulations.\\n\\nEthics Statement\\nWhile this work makes the case for language models as useful tools for cognitive science and linguistics research, these models learn and generate language through processes that are fundamentally different from those employed by humans. Making direct claims about human language learning based on the results of this paper could pose potential risks and harms. This research merely aims to explore the learnability of different languages (specifically, those languages that cannot be acquired by humans and are not representative of any known human language) through the lens of neural models.\\n\\nReferences\\nMostafa Abdou, Vinit Ravishankar, Artur Kulmizev, and Anders S\u00f8gaard. 2022. Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6907\u20136919, Dublin, Ireland. Association for Computational Linguistics.\\n\\nMatteo Alleman, Jonathan Mamou, Miguel A Del Rio, Hanlin Tang, Yoon Kim, and SueYeon Chung. 2021. Syntactic perturbations reveal representational correlates of hierarchical phrase structure in pretrained language models. In Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 263\u2013276, Online. Association for Computational Linguistics.\\n\\nJohan J. Bolhuis, Stephen Crain, Sandiway Fong, and Andrea Moro. 2024. Three reasons why AI doesn't model human language. Nature, 627(8004):489\u2013489.\\n\\nNoam Chomsky. 1956. Three models for the description of language. IRE Transactions on Information Theory, 2(3):113\u2013124.\\n\\nNoam Chomsky. 1957. Syntactic Structures. De Gruyter Mouton, Berlin, Boston.\\n\\nNoam Chomsky. 1959. On certain formal properties of grammars. Information and Control, 2(2):137\u2013167.\\n\\nNoam Chomsky. 1965. Aspects of the Theory of Syntax. The MIT Press.\\n\\nNoam Chomsky. 2002. On Nature and Language. Cambridge University Press.\\n\\nNoam Chomsky. 2023. Conversations with Tyler: Noam Chomsky. Conversations with Tyler Podcast.\\n\\nNoam Chomsky, Ian Roberts, and Jeffrey Watumull. 2023. Noam Chomsky: The false promise of ChatGPT. The New York Times.\\n\\nBernard Comrie. 1989. Language universals and linguistic typology: Syntax and morphology. University of Chicago press.\\n\\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. 2023. Neural networks and the Chomsky hierarchy. In The Eleventh International Conference on Learning Representations.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-787", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-787", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-787", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela. 2021. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2888\u20132913, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, and Ellie Pavlick. 2019. What do you learn from context? Probing for sentence structure in contextualized word representations. In International Conference on Learning Representations.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\\n\\nAlex Warstadt, Leshem Choshen, Aaron Mueller, Adina Williams, Ethan Wilcox, and Chengxu Zhuang. 2023. Call for papers \u2013 the BabyLM challenge: Sample-efficient pretraining on a developmentally plausible corpus.\\n\\nGail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of finite precision RNNs for language recognition. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 740\u2013745, Melbourne, Australia. Association for Computational Linguistics.\\n\\nEthan Wilcox, Roger Levy, Takashi Morita, and Richard Futrell. 2018. What do RNN language models learn about filler\u2013gap dependencies? In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 211\u2013221, Brussels, Belgium. Association for Computational Linguistics.\\n\\nEthan Gotlieb Wilcox, Richard Futrell, and Roger Levy. 2023. Using computational models to test syntactic learnability. Linguistic Inquiry, pages 1\u201344.\\n\\nZhengxuan Wu, Karel D'Oosterlinck, Atticus Geiger, Amir Zur, and Christopher Potts. 2023a. Causal proxy models for concept-based model explanations. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 37313\u201337334. PMLR.\\n\\nZhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah Goodman. 2023b. Interpretability at scale: Identifying causal mechanisms in Alpaca. In Advances in Neural Information Processing Systems, volume 36, pages 78205\u201378226. Curran Associates, Inc.\\n\\nZhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu, Thomas Icard, Christopher Potts, and Noah Goodman. 2022. Causal distillation for language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4288\u20134295, Seattle, United States. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-787", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Supplementary Materials\\n\\nA Dataset Filters\\n\\nThe BabyLM dataset (Warstadt et al., 2023) is an English-language dataset of about 100 million words intended to approximate the amount of linguistic data available to an English-speaking child. To create a dataset for an impossible language, we first pre-process the BabyLM dataset using Stanza (Qi et al., 2020). We perform sentence segmentation on each dataset file and then extract part-of-speech (POS) and morphological feature tags for all the sentences, which are required for the *HOP transformations. We transform each tagged sentence in the original BabyLM dataset using the impossible language's rule-based perturbation function, as described in Section 3. Depending on the class of the impossible language and the specific features of the input sentence, perturbed sentences may be included or excluded from the final dataset used for model training (see below for details on this filtering). Since we apply these filters, the language classes have datasets of slightly different sizes. The *SHUFFLE and *RVERSE languages have training sets of about 9.69 million sentences, and the *HOP languages have training sets of about 8.43 million sentences.\\n\\n*SHUFFLE Filters\\n\\nFor the *SHUFFLE languages, we filter sentences from the BabyLM dataset such that the set of token sequence lengths seen in the validation and test sets are also seen in the training set. This ensures that any shuffles for the DETERMINISTIC SHUFFLE perturbation (which are determined by the token sequence length) in the test set have also occurred at least once in the training set. We apply these filters for all *SHUFFLE languages such that their datasets are comprised of the same subset of original sentences.\\n\\n*RVERSE Filters\\n\\nFor the *RVERSE languages, we do not apply any sentence filtering, so their models are trained on the entire BabyLM dataset.\\n\\n*HOP Filters\\n\\nFor the *HOP languages, we filter out sentences from the BabyLM dataset that would not allow the special markers to fully complete 4 hops in the TOKEN or WORD perturbations, i.e. sentences in which a 3rd-person present tense verb is too close to the end of the sentence. We again filter out these sentences from all perturbations, so TOKEN, WORD, and NO are comprised of the same subset of original sentences from the BabyLM dataset.\\n\\nB GPT-2 Training Details and Hyperparameters\\n\\nWe train GPT-2 small models with a standard training regime (Radford et al., 2018, 2019) using the library of Karamcheti et al. (2021). We mostly use the default GPT-2 small hyperparameters to train our models (context length of 1024, batch size of 512, etc.). We only change the total number of training steps and the number of warm-up steps. We train with a learning rate that linearly warms up from 0 to 6e-4 over 300 steps. While 10% of steps for warm-up is typical for LLM training, we acknowledge that the best warm-up may be different when using a small pretraining dataset, so we also tried 1,000 warm-up steps and 4,000 warm-up steps. (4,000 steps is the GPT-2 default. Since we only train for 3,000 steps, this effectively means we have a learning-rate that linearly warms up from 0 to 4.5e-4.) Using a different warm-up did not change the ranking of impossible language model perplexities.\\n\\nWe train the models for 3,000 training steps, which equates to about 11.03 epochs for the *SHUFFLE languages, 10.05 epochs for the *RVERSE languages, and 12.04 epochs for the *HOP languages. The vocabulary set also varies based on the language. The *SHUFFLE languages use the standard GPT-2 vocabulary containing 50,257 tokens; the *RVERSE languages add one special token R, for a vocabulary size of 50,258; and the *HOP languages add two special tokens S and P for verb inflection, for a vocabulary size of 50,259. We train on NVIDIA RTX 3090 (24GB) GPUs and NVIDIA RTX A6000 (48GB) GPUs. The runtime for each pretraining experiment was \u223c24 hours (for one language and one random seed), for a total experiment runtime of \u223c1800 hours.\\n\\nC Results for Models without Positional Encodings\\n\\nHere, we present results for each of our experiments using GPT-2 models we trained without positional encodings. All other aspects of the experiments are the same, including the impossible language datasets and training hyperparameters. We again train 5 sets of models initialized using different random seeds. Figure 6 presents the perplexity results; Figure 7 presents the surprisal results; and...\"}"}
{"id": "acl-2024-long-787", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8 presents the causal intervention results. D Constituency Probing Evaluation\\n\\nWe also test how perturbations might influence latent linguistic properties in sentences that are seemingly unaffected by the perturbations. For this, we develop a constituency probing experiment to examine whether the contextual representations generated by different models are effective in classifying a sequence of tokens with an appropriate constituent label, similar to the edge probing experiments of Tenney et al. 2019. For example, if the input sentence is \u201cI enjoy strawberry ice cream\u201d and the span of tokens in question represents the constituent \u201cstrawberry ice cream,\u201d the span should be labeled as a noun phrase (NP).\\n\\nSetup. We conduct these experiments for *R EVERSE and *H OP languages, since these languages have constituents in contiguous token sequences. For N ORVERSE and P ARTIAL R EVERSE, we take a sample of unaltered BabyLM test sentences and omit the reversal token $R$. For F ULL R EVERSE, we use the same sample sentences, but reverse the tokens. For the *H OP languages, we use a sample of BabyLM test sentences that are unaffected by the perturbation, which are sentences that do not contain 3rd-person present tense verbs. To extract constituents for testing, we parse the sample sentences using Stanza\u2019s BERT-based constituency parser. We include noun phrases (NP), verb phrases (VP), adjective phrases (ADJP), adverb phrases (ADVP), and prepositional phrases (PP), and we stratify the samples so that there are equal numbers of example constituents for each phrasal category. We obtain a total of 10K examples for probe training and testing for each language class, where an example is comprised of a tokenized sentence, indices of the constituent span, and the constituent label.\\n\\nOur probes are L2-regularized logistic regression classifiers trained on the span representations of the tokens corresponding to constituents in the examples. To obtain span representations for training the probes, we mean-pool the representations of the tokens within the span. We try extracting representations from GPT-2 by averaging the last four hidden layers of the model or using different layers individually. We train each probe for a maximum of 10 iterations and hold out 20% of constituent examples for testing.\\n\\nHypothesis. Constituency probes will achieve higher accuracy for possible languages than impossible ones, in virtue of the fact that the impossible languages are defined by some rules that do not respect constituency boundaries.\\n\\nResults. The results of the probing experiment using the average of the last four GPT-2 layers are presented in Figure 9. Across *R EVERSE and *H OP models trained with positional encodings, there are not any clear trends indicating that certain models have better representations of constituents than others, as differences among probe accuracies are minimal and unstable across training steps. However, looking closely at the *R EVERSE models without positional encodings, we can see that P ARTIAL R EVERSE has significantly lower probe accuracy than the other models up until 2K training steps. We found similar results when using different layers for span representations, as shown in Figure 10. These results might indicate that the *H OP perturbations were too weak to fundamentally affect the models\u2019 representations of latent linguistic structure, but quite unnatural reversal rule of the P ARTIAL R EVERSE language disturbed constituency boundaries in a way that could not be recovered by GPT-2 models without positional encodings.\\n\\nE Additional D ETERMINISTIC S HUFFLE Results\\n\\nIn addition to perplexities of each impossible language model on its own test data, we also obtain perplexities for each D ETERMINISTIC S HUFFLE model on the N ONDETERMINISTIC S HUFFLE test sample and all other D ETERMINISTIC S HUFFLE test samples. This measures whether these models have learned to distinguish their own shuffles from other shuffles. We found that this was indeed the case, as shown in the results in Figure 11.\\n\\nF Confidence Intervals for Interchange Intervention Accuracies\\n\\nWe present the same results of our causal abstraction experiments from Section 4.4, but include confidence intervals for results across models initialized on different random seeds. Figure 12 presents the results for N O; Figure 13 presents the results for T OKEN H OP; and Figure 14 presents the results for W ORD H OP. Figures 15, 16, and 17 show the same plots for each *H OP model trained without positional encodings, respectively.\"}"}
{"id": "acl-2024-long-787", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Perplexities\\n\\n- **No Shuffle**\\n  - Geometric Mean Perplexity: 500 (s = 21)\\n  - Geometric Mean Perplexity: 1000 (s = 57)\\n  - Geometric Mean Perplexity: 1500 (s = 84)\\n\\n- **Local Shuffle**\\n  - Geometric Mean Perplexity: 500 (w = 3)\\n  - Geometric Mean Perplexity: 1000 (w = 5)\\n  - Geometric Mean Perplexity: 1500 (w = 10)\\n\\n- **Even Odd Shuffle**\\n\\n- **Reverse**\\n  - Geometric Mean Perplexity: 500 (s = 21)\\n  - Geometric Mean Perplexity: 1000 (s = 57)\\n  - Geometric Mean Perplexity: 1500 (s = 84)\\n\\n- **Hop**\\n  - Geometric Mean Perplexity: 500 (s = 21)\\n  - Geometric Mean Perplexity: 1000 (s = 57)\\n  - Geometric Mean Perplexity: 1500 (s = 84)\\n\\n### Training Steps\\n\\n| Training Steps | Geometric Mean Perplexity | Local Shuffle | Even Odd Shuffle | Reverse | Hop |\\n|----------------|---------------------------|---------------|-----------------|---------|-----|\\n| 500            | 500                       | 500           | 500             | 500     | 500 |\\n| 1000           | 1000                      | 1000          | 1000            | 1000    | 1000|\\n| 1500           | 1500                      | 1500          | 1500            | 1500    | 1500|\\n| 2000           | 2000                      | 2000          | 2000            | 2000    | 2000|\\n| 2500           | 2500                      | 2500          | 2500            | 2500    | 2500|\\n| 3000           | 3000                      | 3000          | 3000            | 3000    | 3000|\\n\\n### Marker Surprisal\\n\\n- **No Hop**\\n  - Mean surprisal of the verb marker token (S or P) for each *Hop* model.\\n\\n- **Token Hop**\\n  - Mean surprisal difference between the verb marker token (S or P) and the following token for each *Hop* model.\\n\\n### Surprisal Difference\\n\\n- **No Hop**\\n  - Mean surprisal difference between the verb marker token (S or P) and the following token for each *Hop* model.\\n\\n### Subject\u2013Verb Agreement Interventions\\n\\n- **IIA**\\n  - Subject\u2013verb agreement interchange intervention accuracies (IIA) for each *Hop* model trained without positional encodings.\\n\\n**Figure 6:** Perplexities on a sample of 10K test sentences for each impossible language model trained without positional encodings. Error bars indicate 95% confidence intervals across 5 training runs initialized with different random seeds and evaluated on different test samples.\\n\\n**Figure 7:** Surprisal tests for each *Hop* model over training steps (trained without positional encodings). Error bars indicate 95% confidence intervals across 5 training runs initialized with different random seeds and evaluated on different test samples.\\n\\n**Figure 8:** Subject\u2013verb agreement interchange intervention accuracies (IIA) for each *Hop* model trained without positional encodings. Vertical axes denote the GPT-2 layer of the intervention, and horizontal axes denote the token position of the intervention. t1...t4 represent the tokens for the determiner, subject, and verb, respectively. IIA values are averaged over results from 5 models initialized on different random seeds. See Figures 15, 16, and 17 for confidence intervals.\"}"}
