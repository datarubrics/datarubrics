{"id": "acl-2022-long-328", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"WIKI Diverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types\\n\\nXuwu Wang1, Junfeng Tian2, Min Gui3\u2217, Zhixu Li1, Rui Wang4, Ming Yan2, Lihan Chen1, Yanghua Xiao1,5\\n\\n1School of Computer Science, Fudan University, China\\n2Alibaba Group, China\\n3Shopee, Singapore\\n4Vipshop (China) Co., Ltd., China\\n5Fudan-Aishu Cognitive Intelligence Joint Research Center, China\\n\\n{xwwang18,zhixuli,shawyh}@fudan.edu.cn, {tjf141457, ym119608}@alibaba-inc.com, min.gui@shopee.com, mars198356@hotmail.com, lhc825@gmail.com\\n\\nAbstract\\nMultimodal Entity Linking (MEL) which aims at linking mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia), is an essential task for many multimodal applications. Although much attention has been paid to MEL, the shortcomings of existing MEL datasets including limited contextual topics and entity types, simplified mention ambiguity, and restricted availability, have caused great obstacles to the research and application of MEL. In this paper, we present WIKI Diverse, a high-quality human-annotated MEL dataset with diversified contextual topics and entity types from Wikinews, which uses Wikipedia as the corresponding knowledge base. A well-tailored annotation procedure is adopted to ensure the quality of the dataset. Based on WIKI Diverse, a sequence of well-designed MEL models with intra-modality and inter-modality attentions are implemented, which utilize the visual information of images more adequately than existing MEL models do. Extensive experimental analyses are conducted to investigate the contributions of different modalities in terms of MEL, facilitating the future research on this task. The dataset and baseline models are available at https://github.com/wangxw5/wikiDiverse.\\n\\n1 Introduction\\nEntity linking (EL) has attracted increasing attention in the natural language processing community, which aims at linking ambiguous mentions to the referent unambiguous entities in a given knowledge base (KB) (Shen et al., 2014). It has been applied to a lot of downstream tasks such as information extraction (Yaghoobzadeh et al., 2016), question answering (Yih et al., 2015) and semantic search (Blanco et al., 2015).\\n\\nAs named entities (i.e., mentions) with multimodal contexts such as texts and images are ubiquitous in daily life, recent studies (Moon et al., 2018; Adjali et al., 2020a) turn their focus towards improving the performance of EL models through utilizing visual information, i.e., Multimodal Entity Linking (MEL). Several MEL examples are depicted in Figure 1, where the images could effectively help the disambiguation for entity mentions of different types. Due to its importance to many multimodal understanding tasks including VQA, multimodal retrieval, and the construction of multimodal KBs, much effort has been dedicated to the research of MEL. Moon et al. (2018) first addressed the MEL task under the zero-shot setting. Adjali et al. (2020a) designed a model to combine the visual information with text information.\\n\\nIn this paper, we focus on mentions coming from text spans and leave the visual mentions (i.e. objects from the images) for the future work.\"}"}
{"id": "acl-2022-long-328", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Overview of EL and MEL datasets.\\n\\n| Task         | Dataset       | Source | KB    | Modality | Topic                      | Ent. Types                      | Manual | Open | Lang | Size   |\\n|--------------|---------------|--------|-------|----------|----------------------------|---------------------------------|--------|------|------|--------|\\n|              | AIDA          | News   | Wikipedia | Tm \u2192 Te | Multiple                   | Multiple                        |        |      | en   | 1K docs |\\n|              | MSNBC        | News   | Wikipedia | Tm \u2192 Te | Multiple                   | Multiple                        |        |      | en   | 20 docs |\\n|              | AQUA          | News   | Wikipedia | Tm \u2192 Te | Multiple                   | Multiple                        |        |      | en   | 50 docs |\\n|              | ACE2004       | News   | Wikipedia | Tm \u2192 Te | Multiple                   | Multiple                        |        |      | en   | 57 docs |\\n|              | CWEB          | Web    | Wikipedia | Tm \u2192 Te | Multiple                   | Multiple                        |        |      | en   | 320 docs |\\n|              | WIKI          | Wiki   | Wikipedia | Tm \u2192 Te | Multiple                   | Multiple                        |        |      | en   | 320 docs |\\n|              | Zeshel        | Wiki   | Wikia  | Tm \u2192 Te | Multiple                   | Multiple                        |        |      | en   | -      |\\n|              | Snap          | Social Media | Freebase  | Tm, Vm \u2192 Te | Multiple                   | Multiple                        |        |      | en   | 12K captions |\\n|              | Twitter       | Social Media | Twitter users | Tm, Vm \u2192 Te, Vm | Multiple, PER, ORG | Multiple                        |        |      | en   | 4M tweets |\\n|              | Movie         | Movie Reviews | Wikipedia | Tm, Vm \u2192 Te, Vm | Movie, PER | Multiple                        |        |      | en   | 1K reviews |\\n|              | Weibo         | Social Media | Baidu Baike | Tm, Vm \u2192 Te, Vm | Multiple, PER | Multiple                        |        |      | cn   | 25K posts |\\n|              | MEL WikiDiverse | News | Wikipedia | Tm \u2192 Te, Vm \u2192 Te, Vm \u2192 Te | Multiple, Multiple | Multiple                        |        |      | en   | 8K captions |\\n\\nDiagram 2: (a) compares the topic distribution of different domains. The statistics of social media are observed on sampled Twitter (Adjali et al., 2020a). The statistics of news domain are observed on WikiDiverse. The statistics of Movie domain are observed on movie reviews sampled from IMDb. (b) compares the ambiguity distribution of different domains, where ten types of ambiguity are observed on our dataset, including different types of objects with the same name (Diff), persons with the same name (Per), Alias, metonymy (Metm), inferring (Infer), abbreviation (Abbr), surname or first name (SurFirst), acronym (Acrm), reference (Refer) and others.\\n\\nAlthough much attention has been paid to MEL, the existing MEL datasets as listed in the middle rows of Table 1 have deficiencies in the following aspects, which hinder the further advancement of research and application for MEL.\\n\\n- **Limited Contextual Topics.** As shown in Figure 2(a), the existing MEL datasets are mainly collected from social media or movie reviews, where there are only 5 topics in the social media domain and 1 topic in the movie review domain. But as we observed in the news domain, there are more than 10 topics including other popular topics like disaster and education. The lack of topics would limit the generalization ability of the MEL model.\\n\\n- **Limited Entity Types.** Entities in the existing MEL datasets mainly belong to the types of \\\"person (PER)\\\" and \\\"organization (ORG)\\\". This restricts the application of the MEL models over other entity types such as locations, events, etc., which are also ubiquitous in common application scenarios.\\n\\n- **Simplified Mention Ambiguity.** Some datasets such as Twitter (Adjali et al., 2020a) create artificial ambiguous mentions by replacing the original entity names with the surnames of persons or acronyms of organizations. Besides, limited entity types also lead to the limited mention ambiguity that only occurs with PER and ORG. According to our statistics of different domains as depicted in Figure 2(b), there are overall ten kinds of mention ambiguities in news domain.\"}"}
{"id": "acl-2022-long-328", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"such as Wikinews, while existing datasets collected from social media or movie reviews only cover a small scope of ambiguity.\\n\\n\u2022 Restricted Availability. Most of the existing MEL datasets are not publicly available. To enable more detailed research of MEL, we propose a manually-annotated MEL dataset named WIKI-Diverse with multiple topics and multiple entity types. It consists of 8K image-caption pairs collected from WikiNews and is based on the KB of Wikipedia with ~16M entities in total. Both the mentions and entities are characterized by multimodal contexts. We design a well-tailored annotation procedure to ensure the quality of WIKI-Diverse and analyze the dataset from multiple perspectives (Section 4). Based on WIKI-Diverse, we propose a sequence of MEL models with intra-modality and inter-modality attentions, which utilize the visual information of images more adequately than the existing MEL models (Section 5). Furthermore, extensive empirical experiments are conducted to analyze the contributions of different modalities for the MEL task and visual clues provided by the visual contexts (Section 6). In summary, the contributions of our work are as follows:\\n\\n\u2022 We present a new manually annotated high-quality MEL dataset that covers diversified topics and entity types.\\n\u2022 Multiple well-designed MEL models with intra-modal attention and inter-modal attention are given which could utilize the visual information of images more adequately than the previous MEL models.\\n\u2022 Extensive empirical results quantitatively show the role of textual and visual modalities for MEL, and detailed analyses point out promising directions for the future research.\\n\\n2 Related Work\\n\\nTextual EL\\n\\nThere is vast prior research on textual entity linking. Multiple datasets have been proposed over the years including the manually-annotated high-quality datasets like AIDA (Hoffart et al., 2011), automatically-annotated large-scale datasets like CWEB (Guo and Barbosa, 2018) and zero-shot datasets like Zeshel (Logeswaran et al., 2019). To evaluate the EL models\u2019 performance, it is usual to train on the AIDA-train dataset, and test on the datasets of AIDA-test, MSNBC (Cucerzan, 2007), AQUAINT (Milne and Witten, 2008), etc. However, as mentioned in (Cao et al., 2021), many methods have achieved high and similar results within recent three years. One possible explanation is that it may simply be near the ceiling of what can be achieved for these datasets, and it is difficult to conduct further research based on them.\\n\\nMultimodal EL\\n\\nIn recent years, the growing trend towards multimodality requires to extend the research of EL from monomodality to multimodality. Moon et al. (2018) first address the MEL task and build a zero-shot framework, which extracts textual, visual and lexical information for EL in social media posts. However, its proposed dataset is unavailable due to GDPR rules. Adjali et al. (2020a,b) propose a framework of automatically building the MEL dataset from Twitter. The dataset has limited entity types and ambiguity of mentions, thus it is not challenging enough. Zhang et al. (2021) study on a Chinese MEL dataset collected from the Chinese social media platform Weibo, which mainly focuses on the person entities. Gan et al. (2021) release a MEL dataset collected from movie reviews and propose to disambiguate both visual and textual mentions. This dataset mainly focuses on characters and persons of the movie domain. Peng (2021) propose three MEL datasets, which are built from Weibo, Wikipedia, and Richpedia information and use CNDBpedia, Wikidata and Richpedia as the corresponding KBs. However, using Wikipedia as the target dataset may lead to the data leakage problem as many language models are pretrained on it.\\n\\nOur MEL dataset is also related to other named entity-related multimodal datasets, including entity-aware image caption datasets (Biten et al., 2019; Tran et al., 2020; Liu et al., 2021), multimodal NER datasets (Zhang et al., 2018; Lu et al., 2018), etc. However, the entities in these datasets are not linked to a unified KB. So our research of MEL can enhance the understanding of named entities, thereby enhancing the research in these areas.\\n\\n3 Problem Formulation\\n\\nMultimodal entity linking is defined as mapping a mention with multimodal contexts to its referent entity in a pre-defined multimodal KB. Since the boundary and granularity of mentions may be con-\"}"}
{"id": "acl-2022-long-328", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"troversial, the mention span is usually pre-specified. Here we assume each mention has a corresponding entity in the KB, which is the in-KB evaluation problem. Formally, let $E$ represent the entity set of the KB, which usually contains millions of entities. Each mention $m$ or entity $e_i \\\\in E$ is characterized by the corresponding visual context $V_m$, $V_{e_i}$ and textual context $T_m$, $T_{e_i}$. Here $T_m$ and $T_{e_i}$ represent the textual spans around $m$ and $e_i$ respectively. $V_m$ is the image correlated with $m$ and $V_{e_i}$ is the image of $e_i$ in the KB. In real life, entities in KBs may contain more than one image. To simplify it, we select the first image of $e_i$ as $V_{e_i}$ and leave MEL with multiple images per entity as the future work. So the referent entity of mention $m$ is predicted through:\\n\\n$$e^* (m) = \\\\arg \\\\max_{e_i \\\\in E} \\\\Psi (m (T_m, V_m), e_i (T_{e_i}, V_{e_i})).$$\\n\\nwhere $\\\\Psi (\\\\cdot)$ represents the similarity score between the mention and entity.\\n\\n4 Dataset Construction\\n\\nIn this section, we present the dataset construction procedure. Many factors including annotation quality, coverage of topics, diversity of entity types, coverage of ambiguity are taken into consideration to ensure the research value of WIKI Diverse.\\n\\n4.1 Data Collection\\n\\nData Source Selection\\n\\n1) For the source of image-text pairs, considering news articles are widely-studied in traditional EL (Hoffart et al., 2011; Cucerzan, 2007) and usually cover a wide range of topics and entity types, we decide to use news articles. Wikinews and BBC are two popular sources of news articles. So we compared them from two aspects. As shown in Table 2, Wikinews has advantages in terms of alignment degree between image-text pairs and MEL difficulty. So we select the image-caption pairs of Wikinews to build the corpus. 2) For the source of KB, we use the commonly-used Wikipedia (Hoffart et al., 2011; Ratinov et al., 2011; Guo and Barbosa, 2018). We also provide the annotation of the corresponding Wikidata entity for flexible studies.\\n\\nData Acquisition\\n\\n1) For the image-caption pairs, we collect all the English news from the year 2007 to 2020 from Wikinews with multiple topics including sports, politics, entertainment, disaster, technology, crime, economy, education, health and weather. The data cover most of the common topics in the real world. Finally, we obtain a raw corpus with 14k image-caption pairs. 2) For the KB, we use the Wikipedia dump of January 01, 2021. The entity set consists of all the entities in the main namespace with the size of ~16M.\\n\\nData Cleaning\\n\\nFor the image-caption pairs, we remove the cases that 1) contain pornographic, profane, and violent content; 2) the text is shorter than 3 words. Finally, we get a corpus with 8K image-caption pairs.\\n\\n4.2 Annotation\\n\\nAnnotation Design\\n\\nThe primary goal of WIKI Diverse is to link mentions with multimodal contexts to the corresponding Wikipedia entity. Therefore, given an image-text pair, annotators need to 1) detect mentions from the text (Mention Detection, MD) and 2) label each detected mention with the corresponding entity in the form of a Wikipedia URL (Entity Linking, EL). For mentions that do not have corresponding entities in Wikipedia, they are labeled with \\\"NIL\\\". Seven common entity types (i.e., Person, Organization, Location, Country, Event, Works, Misc) are required to be annotated. To avoid subjective errors, we design detailed annotation guidelines with multiple samples to avoid the controversy of mention boundary, mention granularity, entity URL, etc. Details can be found in the Appendix. We also hold regular communications to discuss some emerging annotations problems.\\n\\nAnnotation Procedure\\n\\nThe annotators include 13 annotators and 2 experienced experts. All annotators have linguistic knowledge and are instructed with detailed annotation principles. Each image-caption pair is independently annotated by two annotators. Then an experienced expert goes over\"}"}
{"id": "acl-2022-long-328", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The former Birka Princess (MS Sea Diamond) in 2005. MS Sea Diamond was a cruise ship operated by Louis Hellenic Cruise Lines\u2026\\n\\nFigure 3: An example from WikiDiverse. GT denotes the ground truth entity. The red text and blue text indicate the annotated entity type and Wikipedia entity respectively.\\n\\nTable 3: Statistics of WikiDiverse.\\n\\n|          | Train | Dev  | Test | Total |\\n|----------|-------|------|------|-------|\\n| # pairs  | 6377  | 796  | 796  | 7969  |\\n| # mentions per pair | 2.04  | 2.03 | 1.87 | 2.02  |\\n| # words per pair     | 10.07 | 10.28| 9.92 | 10.08 |\\n\\n4.3 Analysis of WikiDiverse\\n\\nSize and Distribution of WikiDiverse\\n\\nWe divide WikiDiverse into training set, validation set, and test set with the ratio of 8:1:1. The statistics of WikiDiverse are shown in Table 3. The collected Wikipedia KB has ~16M entities in total (i.e. |E| \u2248 16M). Besides, we report the entity type distribution in Figure 4(a) and report the topic distribution in Figure 2(a).\\n\\nDifficulty Measure\\n\\nFirstly, we compare surface form similarity of mentions and ground-truth entities. 51.31% of the mentions have different surface forms compared with ground-truth entities. Specifically, 16.05% of the mentions are totally different from the ground-truth entities. The large difference of the surface form brings challenges for MEL.\\n\\nSecondly, we report the #candidate entities for each mention in Figure 4(b). Intuitively, the more entities a mention may refer to, the more ambiguous the mention is, and the more difficult the EL/MEL is. Specifically, we generate a m\u2192e hash list based on the (m, e) co-occurrence statistics from Wikipedia (See Section 5.1 for details).\\n\\nThirdly, we randomly sample 200 image-caption pairs from WikiDiverse to evaluate the diversity of ambiguity. As shown in Figure 2(b), WikiDiverse covers a wide range of ambiguity.\\n\\n5 Methods\\n\\nIt is challenging to directly predict the entity from a large-scale KB because it consumes large amounts of time and space resources. Therefore, following previous work (Yamada et al., 2016; Ganea and Hofmann, 2017; Cao et al., 2021), we split MEL into two steps: 1) candidate retrieval (CR) is first used to guarantee the recall and obtain a candidate entity set consisting of the TopK entities that are most similar to the mention; 2) entity disambiguation (ED) is then conducted to guarantee the precision and predict the entity with the highest matching score.\\n\\n5.1 Candidate Retrieval\\n\\nExisting methods (Yamada et al., 2016; Ganea and Hofmann, 2017; Le and Titov, 2018) mainly utilize two types of clues to generate the candidate entity set $E_m$: (I) the $m\u2192e$ hash list recording prior probabilities from mentions to entities: $P(e|m)$. (II) the similarity between the contexts of mention $m$ and entity $e$.\\n\\nFollowing these works, we implement a series of baselines as follows: (I) $P(e|m)$ (Ganea and Hofmann, 2017): $P(e|m)$ is calculated based on 1) mention entity hyperlink count statistics from Wikipedia; 2) Wikipedia redirect pages; 3) Wikipedia disambiguation pages. (II) Baselines of textual modality: we retrieve the TopK candidate entities with the most similar textual context of the\"}"}
{"id": "acl-2022-long-328", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Lions versus the Packers.\\n\\nFigure 5: Framework of the introduced baselines.\\n\\nmention based on BM25 (Robertson and Zaragoza, 2009), pretrained embeddings of words and entities obtained from (Yamada et al., 2020) (denoted as WikiVec) and BLINK (Wu et al., 2020). (III)\\n\\nBaseline of visual modality: we retrieve the TopK candidate entities with the most similar visual contexts of the mention based on CLIP (Radford et al., 2021).\\n\\n5.2 Contrastive Entity Disambiguation\\n\\nThe interaction between multimodal contexts of mentions and entities is complicated. It may bring noises to the model without careful handling. So we also introduce several baselines to explore the fusion of multimodal information.\\n\\nThe key component of ED is to design the function $\\\\Psi(m; e_i)$ that quantifies the matching score between the mention $m$ and every entity $e_i \\\\in E_m$.\\n\\nAs shown in Figure 5, the backbone of $\\\\Psi(m; e_i)$ includes different multimodal encoders of $m$ and $e_i$ respectively, followed by dot-production to evaluate the matching degree between them. Specially, a multi-layer perceptron (MLP) is then used to combine the $P(e_i|m)$. Formally, $e^*_{m}$ is predicted through:\\n\\n$$m = \\\\text{Encoder}_m(T_m, V_m); e_i = \\\\text{Encoder}_e(T_e, V_e)$$\\n\\n$$e^* = \\\\arg \\\\max_{e_i \\\\in E_m} \\\\text{MLP}(m \\\\odot e_i, P(e_i|m))$$ (1)\\n\\nSo the multimodal encoders of mentions and entities are the most significant parts of MEL. They use the same structure but training with different parameters.\\n\\nMultimodal Encoder\\n\\nFirstly, we get the textual context's embeddings. For the mention's textual context $T_m = \\\\{w_1, \\\\ldots, w_{L_1}\\\\}$, we directly embed it with the word embedding layer of BERT (Devlin et al., 2019). While for $e_i$, we embed it as the pre-trained embeddings from Yamada et al. (2020), which have compressed the semantics of $e_i$'s entire contexts from Wikipedia.\\n\\n$$\\\\{\\\\hat{w}_1, \\\\ldots, \\\\hat{w}_{L_1}\\\\} = \\\\text{BERT}_{\\\\text{EMB}}(T_m)$$ (2)\\n\\nSecondly, we get the visual context embeddings. Instead of the widely used region-based visual features, we adopt grid features following (Huang et al., 2020), which has the advantage of end-to-end. Specifically, the visual features are represented with the grid features from:\\n\\n$$\\\\{\\\\hat{v}_1, \\\\ldots, \\\\hat{v}_{L_2}\\\\} = \\\\text{Flat} (\\\\text{ResNet}(V))$$ (3)\\n\\nwhere $\\\\text{Flat}(\\\\cdot)$ represents flatting the feature along the spatial dimension and $L_2$ indicates the number of grid features.\\n\\nFinally, taking the embeddings of the two modalities as inputs, we capture the interaction between them. We adopt several backbones to fuse multiple modalities. 1) UNITER (Chen et al., 2020): the two modalities are concatenated and then fed into self-attention transformers to fuse them together. 2) UNITER*: we apply separate self-attention transformers to the two modalities before UNITER for better feature extraction of each modality. 3) LXMERT (Tan and Bansal, 2019): the two modalities are fed into separate self-attention transformers at first and then interact with cross-modal attention. The design of intra-modal and inter-modal attention helps better alignment and interaction of multiple modalities.\\n\\nAfter multiple layers of the fusion operation:\\n\\n$$\\\\text{Fuse}(\\\\{\\\\hat{w}_1, \\\\ldots, \\\\hat{w}_{L_1}\\\\}, \\\\{\\\\hat{v}_1, \\\\ldots, \\\\hat{v}_{L_2}\\\\})$$\\n\\nthe hidden states of the mention's tokens $\\\\{h_i, \\\\ldots, h_j\\\\}$ are obtained. Then we concatenate the hidden states of the first and the last tokens and feed them into a MLP to get the mention's embeddings:\\n\\n$$\\\\text{MLP}([h_i||h_j])$$\\n\\nContrastive Loss\\n\\nWe introduce contrastive learning (Karpukhin et al., 2020; Gao et al., 2021) to learn a more robust representation of both mentions and entities. It is widely acknowledged that selecting negative examples could be decisive for learning a good model. To this end, we utilize both hard negatives and in-batch negatives to improve our model's ability to distinguish between gold entities and hard/general negatives. Let $e_{i,j}$ represent the $j$th candidate entity of the $i$th mention in a batch and let $P_i$ denote the index of $m_i$'s gold\"}"}
{"id": "acl-2022-long-328", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Performance of candidate retrieval. R@K represents recall of the TopK retrieved entities. The modality of P, T, V represent the $P(e|m)$, textual context and visual context respectively. T+V and P+T+V represent the ensemble of different sub-methods, the T of which is BLINK. Results with * are generated using grid search over the Dev. dataset to find the best combination of different sub-methods.\\n\\nAs shown in Table 4: 1) Our model achieves 93.14% of R@100, which indicates most related entities can be recalled from the large 16M KB. For retrieval, each mention takes about 12ms of computation. 2) As for ensemble of different modalities, T + V achieves better results than V and T, which verifies that the information of different modalities are complementary.\\n\\n6.1 Candidate Retrieval Results\\n\\n6.2 Entity Disambiguation Results\\n\\nFollowing previous work, we report micro F1, precision, recall in Table 5. According to the experimental results, we can see that: First, the proposed multimodal methods outperform all the methods with a single modality, which benefit from multi-modal contexts. Besides, contrastive learning can even improve the performance. We reckon that contrastive learning improves the ability to distinguish entities. Second, the textual baselines perform better than the visual ones, which indicates the textual context still plays a dominant role in MEL. Third, the methods using transformers to model the interaction between modalities perform better than those with simple interaction (Moon et al., 2018; Adjali et al., 2020a), which verifies the importance of fusing different modalities.\"}"}
{"id": "acl-2022-long-328", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The former Birka Princess in 2005, now called M/S Sea Diamond.\\n\\nExtra Match of the Women 50m freestyle.\\n\\nVials of the COVID-19 vaccine with labels in English.\\n\\nThe men's 400 m T53.\\n\\nFigure 6: Examples of the \u2018Visual Clues\u2019.\\n\\n6.3 Multimodal Analysis\\n\\nWe also conduct some experiments on the ED tasks as following. Is the multiple modalities complementary? We draw a Venn diagram of different modalities in Figure 8. The circle of Method \\\\( i \\\\) is calculated through \\\\( \\\\# \\\\text{Hit}_i \\\\) | Dataset and the interaction of two circles are calculated through \\\\( \\\\#(\\\\text{Hit}_i \\\\cap \\\\text{Hit}_j) \\\\) | Dataset. One can see that the textual modality is dominant, while the visual modality provides complementary information. Specially, the multimodal method predicts more new entities of 16.86%, which verifies the importance of fusing two modalities.\\n\\nIs it better to have multimodal contexts of both mentions and entities? We conduct an ablation study and report the results in Table 6. We can see that the model with multimodal contexts of both mentions and entities achieves the best result. So linking multimodal mentions to multimodal entities is better than linking multimodal mentions to monomodal entities as done in (Moon et al., 2018).\\n\\nWhat visual clues are provided by the visual contexts? We randomly select 800 image-caption pairs from the test dataset, and then ask annotators to label each mention with the types of visual clues. The visual clues include 4 types: 1) Object: the image contains the entity object. 2) Scene: the image reveals the scene that the entity belongs to (e.g. a basketball player of the \u2018basketball game\u2019 scene). 3) Property: the image contains some properties of the entity (e.g. an American flag reveals the property of a person\u2019s nationality). 4) Others: other important contexts. Note that the four types of clues can be crossed and a sample could have no clues. Examples of the visual clues can be found in Figure 6. We find that visual context is helpful for 60.54% mentions and 81.56% image-caption pairs. We report the contribution of different types of visual clues in Table 7. One can see that: 1) For property clues and object clues, the T+V is 11.20% and 8.48% higher than T. So the multimodal model benefits a lot from the information of objects and properties in the images. 2) For scene clues, the T+V is slightly worse than T, which shows implicit visual clues are not used well and indicates the direction of future research.\\n\\n6.4 Case Study\\n\\nWe present several examples where multimodal contexts influence MEL in Figure 7. Example (a) and (b) verify the helpfulness of the multimodal context. From the error cases, we can see that the\"}"}
{"id": "acl-2022-long-328", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A field goal by Mason Crosby helped the Packer...\\n\\nField_goal_percentage(basketball)\\n\\nBart writing HDTV is worth every cent in the chalkboard gag.\\n\\nMontcada is just a few kilometres away from Barcelona.\\n\\nThe Rose Garden immediately before the speech.\\n\\nFigure 7: Case study. Successful predictions and failed predictions for the underlined mention are shown.\\n\\n11.67% 37.12% 16.86% 6.71%\\n\\n15.37% 35.47% 13.63% 7.19%\\n\\n13.63% 7.19% 15.37% 35.47%\\n\\n4.37% 1.57%\\n\\nLXMERT (T+V) BERT* (T) ResNet-50 (V)\\n\\nVisual Clues Proportion\\n\\nObject 45.40% 64.47 72.95\\n\\nScene 18.96% 60.62 60.33\\n\\nProperty 26.22% 55.25 66.45\\n\\nOthers 14.80% 60.00 64.00\\n\\nTable 7: Model performance under different visual clues. T+V denotes the multimodal model LXMERT, and T represents the textual model BERT.\\n\\n7 Conclusion and Future Work\\n\\nWe propose WIKI Diverse, a manually-annotated Wikipedia-based MEL dataset collected from Wikinews. To overcome the weaknesses of existing datasets, WIKI Diverse covers a wide range of topics, entity types and ambiguity. We implement a series of baselines and carry out multiple experiments over the dataset. According to the experimental results, WIKI Diverse is a challenging dataset worth further exploration. Besides multimodal entity linking, WIKI Diverse can also be applied to evaluate the pre-trained language model, multimodal named entity typing/recognition, multimodal topic classification, etc. In the future, we plan to 1) utilize more than one images of each entity 2) adopt finer-grained multimodal interaction models for this task and 3) transfer the model to more general scenarios such as EL in articles.\\n\\nAcknowledgement\\n\\nWe thank all the reviewers for their valuable suggestions. This research was supported by the National Key Research and Development Project (No. 2020AAA0109302), National Natural Science Foundation of China (No. 62072323), Shanghai Science and Technology Innovation Action Plan (No. 19511120400), Shanghai Municipal Science and Technology Major Project (No. 2021SHZDZX0103) and Alibaba Research Intern Program.\"}"}
{"id": "acl-2022-long-328", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ethical Considerations\\n\\nWe collected publicly available Wikinews image-caption pairs without storing any personal data. During data cleaning, we remove the cases that contain pornographic, profane, and violent content. We annotate the data using the crowdsourcing platform of Alibaba. To ensure that the crowd workers were fairly compensated, we paid them at an hourly rate of 15 USD per hour, which is a fair and reasonable rate of pay for crowdsourcing.\\n\\nReferences\\n\\nOmar Adjali, Romaric Besan\u00e7on, Olivier Ferret, Herv\u00e9 Le Borgne, and Brigitte Grau. 2020a. Building a multimodal entity linking dataset from tweets. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4285\u20134292, Marseille, France. European Language Resources Association.\\n\\nOmar Adjali, Romaric Besan\u00e7on, Olivier Ferret, Herv\u00e9 Le Borgne, and Brigitte Grau. 2020b. Multimodal entity linking for tweets. In Advances in Information Retrieval, pages 463\u2013478, Cham. Springer International Publishing.\\n\\nAli Furkan Biten, Lluis Gomez, Mar\u00e7al Rusinol, and Dimosthenis Karatzas. 2019. Good news, everyone! context driven entity-aware captioning for news images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12466\u201312475.\\n\\nRoi Blanco, Giuseppe Ottaviano, and Edgar Meij. 2015. Fast and space-efficient entity linking for queries. In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, pages 179\u2013188. ACM.\\n\\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021. Autoregressive entity retrieval. In International Conference on Learning Representations.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In ECCV.\\n\\nSilviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 708\u2013716, Prague, Czech Republic. Association for Computational Linguistics.\\n\\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nNing Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Hai-Tao Zheng, and Zhiyuan Liu. 2021. Few-nerd: A few-shot named entity recognition dataset. In ACL-IJCNLP.\\n\\nJingru Gan, Jinchang Luo, Haiwei Wang, Shuhui Wang, Wei He, and Qingming Huang. 2021. Multimodal entity linking: a new dataset and a baseline. Multimedia.\\n\\nOctavian-Eugen Ganea and Thomas Hofmann. 2017. Deep joint entity disambiguation with local neural attention. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2619\u20132629, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821.\\n\\nZhaochen Guo and Denilson Barbosa. 2018. Robust named entity disambiguation with random walks. Semantic Web, 9(4):459\u2013479.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778.\\n\\nJohannes Hoffart, Mohamed Amir Yosef, Ilaria Bordin, Hagen F\u00fcrstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 782\u2013792, Edinburgh, Scotland, UK. Association for Computational Linguistics.\\n\\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849.\\n\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769\u20136781, Online. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-328", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Phong Le and Ivan Titov. 2018. Improving entity linking by modeling latent relations between mentions.\\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1595\u20131604, Melbourne, Australia. Association for Computational Linguistics.\\n\\nFuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. 2021. Visual news: Benchmark and challenges in news image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6761\u20136771.\\n\\nLajanugen Logeswaran, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Jacob Devlin, and Honglak Lee. 2019. Zero-shot entity linking by reading entity descriptions. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3449\u20133460, Florence, Italy. Association for Computational Linguistics.\\n\\nDi Lu, Leonardo Neves, Vitor Carvalho, Ning Zhang, and Heng Ji. 2018. Visual attention model for name tagging in multimodal social media. In Proceedings of ACL, pages 1990\u20131999, Melbourne, Australia.\\n\\nDavid Milne and Ian H Witten. 2008. Learning to link with Wikipedia. In Proceedings of the 17th ACM conference on Information and knowledge management, pages 509\u2013518.\\n\\nSeungwhan Moon, Leonardo Neves, and Vitor Carvalho. 2018. Multimodal named entity disambiguation for noisy social media posts. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2000\u20132008, Melbourne, Australia. Association for Computational Linguistics.\\n\\nWang Peng. 2021. Multimodal entity linking datasets benchmark.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020.\\n\\nLev Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and global algorithms for disambiguation to Wikipedia. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1375\u20131384, Portland, Oregon, USA. Association for Computational Linguistics.\\n\\nStephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Now Publishers Inc.\\n\\nWei Shen, Jianyong Wang, and Jiawei Han. 2014. Entity linking with a knowledge base: Issues, techniques, and solutions. IEEE Transactions on Knowledge and Data Engineering, 27(2):443\u2013460.\\n\\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100\u20135111, Hong Kong, China. Association for Computational Linguistics.\\n\\nAlasdair Tran, Alexander Mathews, and Lexing Xie. 2020. Transform and tell: Entity-aware news image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13035\u201313045.\\n\\nLedell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. 2020. Scalable zero-shot entity linking with dense entity retrieval. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6397\u20136407, Online. Association for Computational Linguistics.\\n\\nYadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch\u00fctze. 2016. Noise mitigation for Du neural entity typing and relation extraction. arXiv preprint arXiv:1612.07495.\\n\\nIkuya Yamada, Akari Asai, Jin Sakuma, Hiroyuki Shindo, Hideaki Takeda, Yoshiyasu Takefuji, and Yuji Matsumoto. 2020. Wikipedia2Vec: An efficient toolkit for learning and visualizing the embeddings of words and entities from Wikipedia. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 23\u201330. Association for Computational Linguistics.\\n\\nIkuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2016. Joint learning of the embedding of words and entities for named entity disambiguation. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 250\u2013259, Berlin, Germany. Association for Computational Linguistics.\\n\\nScott Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao. 2015. Semantic parsing via staged query graph generation: Question answering with knowledge base. Proceedings of the Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP.\\n\\nLi Zhang, Zhixu Li, and Qiang Yang. 2021. Attention-based multimodal entity linking with high-quality images. In International Conference on Database Systems for Advanced Applications, pages 533\u2013548. Springer.\\n\\nQi Zhang, Jinlan Fu, Xiaoyu Liu, and Xuanjing Huang. 2018. Adaptive co-attention network for named entity recognition in tweets. In Proceedings of AAAI.\"}"}
{"id": "acl-2022-long-328", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Annotation Details\\n\\nA.1 Annotation Guidelines\\nTo avoid subjective errors, we designed detailed annotation guidelines with multiple samples to avoid the controversy of mention boundary, mention granularity and Wikipedia URL. The entire annotation guideline is summarized as follows.\\n\\n1. Only label mentions with the entities that can be inferred from the image-caption pairs instead of the entities that can only be inferred from the entire news.\\n\\n2. Label mentions that do not have corresponding entities in Wikipedia with \u2018NIL\u2019.\\n\\n3. Mention types include persons, organizations, locations, events, works, currency and others.\\n\\n4. We assume that mentions are non-recursive and non-overlapping. So if a mention is embedded in another mention, only the top-level mention is annotated.\\n\\n5. The mention boundary is detected with the smallest granularity while avoiding overlapping boundaries. An example is labeling \u201cFrench President Nicolas Sarkozy\u201d with both \u201cFrench\u201d and \u201cNicolas Sarkozy\u201d instead of \u201cFrench President Nicolas Sarkozy\u201d.\\n\\n6. The title before mention is also part of the mention span.\\n\\n7. Metonymy is needed. Metonymy is a figure of speech that replaces the name of a thing with the name of something else with which it is closely associated. An example is using England to represent the England national football team.\\n\\nA.2 Details of Image Data Cleaning\\nDuring the data cleaning, we have also done some processing on the images:\\n\\n\u2022 To prevent image processing tools from being unable to process certain types of images, we normalize the images with less popular formats (e.g. .svg, .tif, .gif) into the images with popular formats (i.e., .png, .jpg);\\n\\n\u2022 As some \u201cimages\u201d are videos actually, we manually select a certain frame of the videos.\\n\\n4\\n\\nOnly 14 instances contain videos, which account for a small proportion. We have tagged them in the dataset for users to decide whether to use them.\\n\\nB Other Details of Experimental Settings\\nB.1 Details about the Baselines\\nIn the ED step, we also compare with the following baselines:\\n\\n\u2022 Baselines of Textual Modality: 1) REL (Le and Titov, 2018): it is a robust EL baseline that incorporates latent relation variables into the EL model for better understanding of the text. 2) BERT (Devlin et al., 2019): it is a widely acknowledged pre-trained language model. 3) BLINK (Wu et al., 2020): it applies cross-attention to the mention and entities for MEL.\\n\\n\u2022 Baselines of Visual Modality: 1) ResNet-50 (He et al., 2016): it is a widely acknowledged model with residual learning framework trained on the ImageNet (Deng et al., 2009). 2) CLIP (Radford et al., 2021): it is a model trained to predict the matching degree of texts and images, which also achieves competitive performance on visual tasks.\\n\\n\u2022 Multimodal Baselines: 1) MMEL18 (Moon et al., 2018) uses modality attention to fuse features from different modalities. 2) MMEL20 (Adjali et al., 2020b) uses Sent2vec, BM25 and Inception-V3 to extract features of different modalities, then integrates different modalities together with the concatenation operation followed by MLP.\\n\\nB.2 Implementations Details\\nWe train all the models on the same device for 20 iterations with the early stopping mechanism. The learning rate is set as 1e-3. The batch size is set as 12. The model of UNITER consists of 12 layers of transformers. The UNITER* consists of 8 layers of textual transformers and visual transformers respectively, followed by the concatenation operation and 4 layers of transformers. The LXMERT consists of 8 layers of textual transformers and visual transformers respectively, followed by 4 layers of cross-modality attention mechanism.\\n\\nC Supplementary Experimental Results\\nC.1 Detailed Main Results\\nTo verify the robustness of our method, we report the model performance in Table 8.\"}"}
{"id": "acl-2022-long-328", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Comparison with baselines with results averaged over 5 runs. Models with \u2020 are enhanced with contrastive learning. All the models use the same candidate entity set retrieved through $P(e|m) + \\\\text{BLINK} + \\\\text{CLIP}$ with $K = 10$.\\n\\nTable 9: Comparison of mention's pooling strategies.\\n\\nC.2 Comparison of Mention's Pooling Strategies\\n\\nAfter multiple layers of multimodal fusion, we get the hidden states of the mention's token sequence. Then a pooling operation is needed to get the representation of the entire mention. Here we compare three pooling methods: 1) the first token of the mention sequence (denoted as First); 2) the concatenation of the first and last of the mention sequence (denoted as First&Last); 3) the average of the entire mention sequence (denoted as Average). According to the result in Table 9, First&Last has achieved the best performance, which is thus selected in the final version of our model.\\n\\nC.3 Analysis of the Contrastive Loss\\n\\nTo evaluate the influence of the contrastive loss, we also performed a detailed analysis. Specifically, we conducted experiments with different numbers of # hard negatives and # in-batch negatives. For expression convenience, we use $K$ and $B$ to represent their numbers. To prevent other factors from affecting the results, we do not change the batch size or the candidate entity number, but only change the number of negative instances.\\n\\nDo hard negatives or in-batch negatives have a greater impact on the results? By comparing Figure 9(a) and Figure 9(b), we can find out that even without in-batch negatives, the model still achieves relatively good results. However, the decrease of hard negative leads to a sharp drop in model performance. Therefore, the hard negatives influence the model performance more.\\n\\nEmpirical analysis of the number of negative samples. We can see that no matter hard negatives or in-batch negatives, the more negative examples are introduced, the effect will be improved. Therefore, under the premise of sufficient GPU memory, negative examples should be increased as much as possible, especially for the number of hard negatives.\"}"}
