{"id": "emnlp-2024-main-382", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mirror stories are stories that reflect the reader's identity, culture, and experiences, serving to engage, validate, and empower individuals (Bishop, 1990). Such books are crucial in educational settings, fostering a sense of belonging and self-understanding through diverse narratives (Fleming et al., 2016), while also improving engagement and comprehension (Walkington and Bernacki, 2014; Heineke et al., 2022). Beyond education, personalized narratives have shown potential in various fields, including health communication and marketing, where they enhance patient understanding and adherence, and strengthen emotional connections between brands and consumers (Galitsky, 2024; Babatunde et al., 2024).\\n\\nDespite the profound need for these personalized narratives, there is a noticeable underrepresentation of non-white minority groups in literature (CCBC, 2021) relative to their population size, detailed in Appendix Figure 6. The gap in cultural representation highlights the need for more inclusive narratives that reflect diverse reader identities, enhance empathy, and promote cultural awareness (Hoytt et al., 2022). Diversity in literature can lead to improved innovation and a broader consideration of ideas, ultimately enriching the reading experience for all (Phillips, 2014).\\n\\nAdvancements in natural language processing, particularly through the development of LLMs like GPT-4, PaLM, and LLaMA have introduced the potential to address these gaps on a large scale (OpenAI, 2023; Chowdhery et al., 2022; Touvron et al., 2023). LLMs excel in generating human-like text and adapting content to various contextual needs (Brown et al., 2020; Zhao et al., 2023).\\n\\nRecent studies have investigated LLMs' capabilities in expressing personality within generated content (Li et al., 2024; Jiang et al., 2024) and developing methods to induce and edit personality expressions in LLM outputs (Jiang et al., 2023; Li et al., 2024; Mao et al., 2024). New benchmarks have also been released to assess personality traits in LLM outputs (Jiang et al., 2023; Wang et al., 2024). However, there remains a gap in research concerning whether LLMs can generate content that incorporates identity traits and faithfully mirrors the diverse identities of a global readership.\\n\\nOur study addresses this gap by exploring the potential of LLMs to create mirror stories\u2014narratives that genuinely reflect and resonate with the identities of individual readers. We present a framework...\"}"}
{"id": "emnlp-2024-main-382", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Generation and evaluation process for human-written generic, LLM-generated generic, and LLM-generated personalized narratives that evaluates the effectiveness of LLM-generated mirror stories in comparison to traditional narratives, assessing their impact on engagement, satisfaction, and the perception of personal relevance (see Figure 1). Our contributions are three-fold:\\n\\n1. We release MIRRORSTORIES, a corpus of 1,500 personalized short stories generated by integrating elements such as name, gender, age, ethnicity, reader interest, and moral of the story. We demonstrate that LLMs can effectively incorporate identity elements into narratives, with human evaluators identifying them in the stories with high accuracy.\\n\\n2. Through a comprehensive evaluation involving 26 diverse human judges, personalized LLM-generated stories consistently outperform both generic human-written and LLM-generated stories across all engagement metrics, with a significantly higher average rating.\\n\\n3. We present analyses that assess text diversity, coherence, and moral comprehension across each story type, and examine biases exhibited by LLMs when evaluating personalized narratives. We also explore the potential of integrating images and incorporate MIRRORSTORIES into an interactive web application where users can browse and generate stories.\\n\\n2 MIRRORSTORIES\\n\\n2.1 Overview\\n\\nMIRRORSTORIES is a corpus designed to evaluate the ability of LLMs to generate both generic and personalized short narratives based on predefined morals and identity elements. Each dataset instance consists of a moral (e.g., \u201cKindness is never wasted\u201d) guiding the narrative\u2019s tone and a set of identity elements (name, age, gender, ethnicity, and personal interest) to personalize the story. Specifically, the dataset includes a human-written and an LLM-generated generic story, both of which do not incorporate specific identity elements, and an LLM-generated personalized story that includes these elements to enhance relevance and engagement. Appendix A.5 provides a detailed example of the dataset structure.\\n\\n2.2 Dataset Collection\\n\\nHuman-written Stories & Morals\\n\\nMIRRORSTORIES incorporates human-written stories derived from Aesop\u2019s fables (Wier et al., 1890), a well-known collection famous for its clear narrative structure and explicit moral conclusions. The morals serve as guides for generating both generic and personalized stories. The complete list of morals is provided in Appendix A.5 Table 7.\\n\\nIdentities\\n\\nIdentity traits such as name, age, gender, ethnic background, and interests are included to personalize the narratives. We drew from 123 unique ethnic backgrounds, 124 diverse interests, and 28 distinct morals. The complete set of identities is provided in Appendix A.5 Table 7.\\n\\nGeneric & Personalized LLM-Generated Stories\\n\\nGeneric stories are generated focusing solely\\n\\nScraped from https://read.gov/aesop/001.html\"}"}
{"id": "emnlp-2024-main-382", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Illustration demonstrating the personalization validation and impact processes on the moral, while personalized stories additionally integrate the specified identity elements. For the specific prompts, refer to Figure 1. GPT-4 (ver. 0613), Claude-3 Sonnet, and Gemini 1.5 Flash (Reid et al., 2024) were used, each responsible for generating one-third of the narratives.\\n\\nMIRROR STORIES comprises 1,500 narratives with an almost even split between male and female characters. The dataset spans a broad age range from 10 to 60 years. Detailed illustrations of the distributions are in Appendix A.1 Figures 9 and 10.\\n\\n3 Experiments\\n\\nWe conducted two experiments to assess the effectiveness of personalization in LLM-generated stories:\\n\\n- **Personalization Validation**, which validates the integration of identity elements within the narratives,\\n- **Personalization Impact**, which assesses the impact of these narratives on user engagement, comprehension, satisfaction, and personalness.\\n\\n**Prompts**\\n\\nIn both experiments, personalized prompts incorporating identity elements were used to generate personalized stories. For Personalization Validation, these elements were specifically asked not to be stated explicitly, to test their seamless integration into the narrative. In the Personalization Impact experiment, personal elements were aligned with those of 26 human evaluators, ensuring that each story was tailored to evaluators. Figure 1 and 2 provide detailed structures of the prompts for both generation and evaluation.\\n\\n**Human Evaluation**\\n\\nIn both experiments, the same 26 human evaluators\u2014all students from diverse disciplines\u2014were tasked with evaluating the narratives (for demographic details, see Appendix Figure 8). For the Personalization Validation, they answered a structured questionnaire for a random sample of 30 stories to identify the personalized elements. In the Personalization Impact test, each evaluator reviewed a human-written, generic LLM-generated, and personalized LLM-generated story, with the personalized LLM-generated story specifically tailored to reflect their personal identity. They provided feedback on all three story types, rating them on satisfaction, quality, engagement, and personalness. The detailed questionnaire is provided in Appendix A.2.\\n\\n**Models**\\n\\nGPT-4 (ver. 0613, temperature 0.4) was used as an evaluator in both experiments. Initially, it assessed the integration of personalized elements. Later, it was used to evaluate the stories for satisfaction, quality, engagement, and personalness, with a sample of the evaluation process and prompts provided in Appendix Figure 7. GPT-4 was chosen for its increasing adoption as an evaluator across domains (Gilardi et al., 2023; Tarkka et al., 2024; Malik et al., 2024), with potential advantages such as scalability, cost-efficiency, and consistency.\\n\\n4 Results\\n\\nAre MIRROR STORIES personalized?\\n\\nThe effectiveness of personalization in MIRROR STORIES is evident from the high accuracy rates in identifying identity elements by both human and LLM evaluators. As shown in Figure 3, human evaluators were particularly adept at identifying gender and ethnicity with accuracies at 100% and 94%, respectively. Similarly, GPT-4 showed robust performance, matching or exceeding human accuracy in all categories, which confirms the high level of personalization achieved in the narratives.\\n\\nPersonalized LLM-generated stories also effectively incorporate both the provided moral and the reader\u2019s interests, with a stronger emphasis on the moral. To demonstrate this, we used BERTopic (Grootendorst, 2022) for topic modeling to identify the top five terms for each story. We then...\"}"}
{"id": "emnlp-2024-main-382", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Accuracy of human and LLM evaluators in identifying identity elements in the story\\n\\n| Story Type                  | Correctly Identified Morals |\\n|-----------------------------|----------------------------|\\n| Generic Human-Written      | 23/24                      |\\n| Generic LLM-Generated      | 23/23                      |\\n| Personalized LLM-Generated | 25/25                      |\\n\\nTable 1: Number of correctly identified morals for each story type, excluding 'N/A' responses\\n\\n4 Calculation of cosine similarity using Word2Vec embeddings (Mikolov et al., 2013) between these top terms and the provided interest and moral. The average cosine similarity was 0.12 for the provided interest and 0.27 for the moral, demonstrating a balance between incorporating the reader's interest and maintaining the intended moral.\\n\\n5 A detailed sample of the top terms identified for each story is provided in Appendix Table 4.\\n\\nAre MIRRORS STORIES preferred?\\n\\nFigure 4 shows that personalized LLM-generated stories in MIRRORS are consistently rated higher across all metrics compared to both generic LLM-generated and human-written narratives. This preference is pronounced in evaluations by both humans and GPT-4, with personalized narratives outperforming generic versions, particularly in terms of personalness and engagement where the ratings significantly diverge.\\n\\nHow does personalization affect moral comprehension?\\n\\nWe analyzed the impact of personalization on moral comprehension in stories. Evaluators were asked to identify the main message of each story, or provide 'N/A' if they could not. We manually assessed the evaluators' responses to the intended morals. Excluding 'N/A' responses, the correctly identified morals are detailed in Table 1. The results indicate that differences in moral identification across story types are not statistically significant, demonstrating that adding personalization did not negatively affect the model's ability to convey the intended moral. A sample of evaluator responses is shown in Appendix Figure 3.\\n\\nWhat is the impact of personalization on textual diversity?\\n\\nWe analyzed how personalization elements impact textual diversity using the Shannon Diversity Index (SDI). Table 2 shows that personalized stories achieve the highest SDI among all story types. Including a single personalization element, such as the 'interest' element, also increases SDI compared to generic and human-written stories with the same moral. Additionally, we observed that increasing GPT-4's temperature negatively affects the diversity and coherence of generic LLM-generated stories. At a temperature of 1.2, the stories showed increased diversity but began to lose coherence. Further increasing the temperature to 1.5 resulted in nonsensical outputs.\"}"}
{"id": "emnlp-2024-main-382", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Are there biases in LLM evaluations of personalized stories?\\n\\nWe found several preferential biases in GPT-4\u2019s evaluation results. Figure 5 shows an instance of gender-based bias, with stories featuring non-binary characters receiving the highest personalness ratings, while those with male characters rated lower in quality and engagement. Ethnic background also influences evaluations, with Norwegian and Japanese characters rated higher across all metrics (Appendix Figure 14). We also observed inter-model preferential biases across the three models used for generating personalized stories, with Claude-3 consistently receiving higher ratings compared to GPT-4 and Gemini-1.5. An overview of all bias results is provided in Appendix A.3.1.\\n\\nExtended Analyses\\n\\nQualitative comparison of human and LLM evaluations\\n\\nWe examine cases where human and LLM evaluators either contradict or agree on the scores assigned to stories, providing insights into the differences in evaluations and preferences for various types of stories. Examples of these cases, highlighting instances of both agreement and disagreement between human and LLM evaluators, are presented in Appendix Figure 15.\\n\\nImage generation for personalized stories\\n\\nWe explored the potential of incorporating images into stories to enhance engagement and representation. The image generation and evaluation processes are detailed in Appendix Table 18. Notably, human evaluators show a high accuracy in identifying personalized elements in the images generated by DALL\u00b7E 2 (Ramesh et al., 2022), with gender and interest being recognized with 100% and 95% accuracy, respectively (Appendix Figure 17).\\n\\nCorrelation between human and LLM evaluators\\n\\nCorrelation analysis revealed a low to moderate alignment between human evaluators and GPT-4 in story evaluation metrics. GPT-4 aligned more closely with human evaluators on quality across all story types (correlations 0.22-0.47), but showed the weakest correlation in assessing personalness, particularly for personalized stories (as low as 0.08). This suggests that while GPT-4 is increasingly used for various evaluation tasks, its effectiveness in assessing subjective aspects of creative tasks is limited. A detailed analysis of these correlations and temperature variations is presented in Appendix A.4.2, Table 5 and Figure 16.\\n\\nRelated Work\\n\\nOur study builds on research on the effectiveness of personalized narratives in engaging readers and improving learning outcomes (Zhang et al., 2024; Pennebaker and Graybeal, 2001; Hirsh and Peterson, 2009). We extend this work by examining how LLMs can generate personalized narratives to increase reader engagement and satisfaction. While promising, the accuracy of personal traits in generated content remains challenging, with studies showing mixed results (Jiang et al., 2024; Bhandari and Brennan, 2023). Concurrently, LLM exploration in narrative generation has focused on improving coherence and depth (Andreas, 2022; Shen and Elhoseiny, 2023; El-Refai et al., 2024; G\u00f3mez-Rodr\u00edguez and Williams, 2023). To assess these advancements, recent evaluative techniques for narrative systems emphasize user interactions and alignment metrics between visual content and narratives (El-Refai et al., 2024; Ning et al., 2023).\\n\\nConclusion\\n\\nOur study demonstrates the potential of LLMs in generating personalized narratives that effectively incorporate diverse identity elements and enhance reader engagement compared to generic stories. MIRROR STORIES consists of 1,500 personalized stories that consistently outperform generic ones on key metrics. By making MIRROR STORIES publicly available and integrating it into an interactive web application, we aim to encourage further research on personalized narrative generation, contributing to more engaging and inclusive content. Future work could explore out-group perceptions of these narratives, broadening our understanding of personalization\u2019s impact across diverse audiences.\"}"}
{"id": "emnlp-2024-main-382", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nStory Constraints: To maintain consistency and feasibility within the scope of our study, we imposed certain constraints on the stories generated, such as limiting the length to 250-300 words and focusing on a specific set of morals. While these constraints allowed for a controlled comparison between personalized and generic stories, they may not fully capture the potential of LLMs in generating longer, more complex narratives or exploring a wider range of themes and morals. Future research could investigate the impact of personalization on stories of varying lengths and themes to gain a more comprehensive understanding of how these factors influence reader engagement and satisfaction.\\n\\nDemographic Diversity: While our study aimed to include a diverse range of identities and backgrounds, the demographic diversity of our human evaluators was by no means the perfect sample of global readership. The majority of our evaluators were university students, which may not be representative of the broader population. Future research should include a more diverse pool of evaluators across age, education, and cultural backgrounds to ensure the generalizability of the findings and to capture a wider range of perspectives on personalized storytelling.\\n\\nScope of Personalization: Our study primarily examined personalization factors like age, gender, interests, and ethnic background. However, aspects such as personality traits, emotional resonance, and narrative preferences were not extensively investigated but could notably enhance engagement and narrative impact. For example, aligning story elements with reader emotional responses or tailoring narratives to specific preferences like mystery, romance, or adventure could significantly boost satisfaction and engagement.\\n\\nSubjectivity of Evaluation: Another limitation of our study is the inherent subjectivity involved in evaluating the impact of personalized stories. Despite our attempts to standardize evaluation criteria and maintain consistency among evaluators, individual preferences, biases, and interpretations can still significantly influence the outcomes. This subjectivity can lead to variability in how different evaluators perceive and rate the same narrative elements.\\n\\nModel Selection and Variety: Our study utilized GPT-4, Claude3, Gemini-1.5, and DALL\u00b7E 2 for generating and evaluating narratives and images. This limited selection may affect the generalizability of our findings, as different models might produce or assess stories differently based on their training data and algorithms. Expanding future research to include a variety of models, including open-source ones, could provide a more comprehensive understanding of how different language models handle personalization in storytelling and evaluate narrative elements.\\n\\nEthical Considerations: We followed strict ethical standards throughout our research to ensure validity and fairness. Consent and transparency were central to our approach, with all participants fully informed and providing explicit consent. We also ensured compliance with intellectual property rights by using Aesop's fables, which are in the public domain.\\n\\nData Privacy and Security: Ensuring the privacy and security of participants' personal information was a top priority. We collected and used personal details such as age, gender, interests, and ethnic background to generate personalized stories. Robust data protection measures were implemented, including secure storage, anonymization, and restricted access to sensitive information. Participants were informed about how their data would be used, stored, and protected.\\n\\nPotential Misuse and Unintended Consequences: While personalized storytelling has the potential to enhance engagement and representation, we carefully considered the potential for misuse or unintended consequences. To mitigate risks such as the manipulation of individuals' emotions or the reinforcement of stereotypes, we implemented safeguards against harmful content and regularly audited the generated stories for potential biases or inappropriate themes.\\n\\nInclusivity and Representation: When generating personalized stories, we strived to ensure that the stories were inclusive and representative of diverse identities and experiences. This included considering factors such as race, ethnicity, gender identity, sexual orientation, disability, and socioeconomic status. We aimed to create stories that were respectful, authentic, and empowering for all individuals, avoiding stereotypes and promoting positive representation.\\n\\nAccountability and integrity were paramount in our approach to ensure the validity and fairness of our research.\"}"}
{"id": "emnlp-2024-main-382", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"reporting our results, including limitations and implications. Additionally, every narrative generated by LLMs underwent a thorough review to maintain quality and appropriateness, enhancing the reliability of our findings and participant well-being.\\n\\nAcknowledgements\\n\\nThis work was supported by the Natural Sciences and Engineering Research Council of Canada and by the New Frontiers in Research Fund.\\n\\nReferences\\n\\nJacob Andreas. 2022. Language models as agent models.\\n\\nSodiq Babatunde, Opeyemi Odejide, Tolulope Edunjobi, and Damilola Ogundipe. 2024. The role of AI in marketing personalization: A theoretical exploration of consumer engagement strategies. International Journal of Management & Entrepreneurship Research, 6:936\u2013949.\\n\\nPrabin Bhandari and Hannah Marie Brennan. 2023. Trustworthiness of children stories generated by large language models. arXiv preprint arXiv:2308.00073.\\n\\nRudine Sims Bishop. 1990. Mirrors, windows, and sliding glass doors. Perspectives: Choosing and using books for the classroom, 6(3).\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nCCBC. 2021. Books by and/or about black, indigenous, and people of color (all years). Data retrieved from the Cooperative Children's Book Center.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\\n\\nKarim El-Refai, Zeeshan Patel, and Jonathan Pei. 2024. Swag: Storytelling with action guidance. arXiv preprint arXiv:2402.03483.\\n\\nJane Fleming, Susan Catapano, Candace M Thompson, and Sandy Ruvalcaba Carrillo. 2016. More mirrors in the classroom: Using urban children's literature to increase literacy. Rowman & Littlefield.\\n\\nBoris A. Galitsky. 2024. LLM-based personalized recommendations in health. Preprints.\\n\\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. 2023. ChatGPT outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30).\\n\\nCarlos G\u00f3mez-Rodr\u00edguez and Paul Williams. 2023. A confederacy of models: A comprehensive evaluation of LLMs on creative writing. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14504\u201314528, A Coru\u00f1a, Spain and Sunshine Coast, Australia. Association for Computational Linguistics.\\n\\nMaarten Grootendorst. 2022. Bertopic: Neural topic modeling with a class-based tf-idf procedure. arXiv preprint arXiv:2203.05794.\\n\\nAmy J. Heineke, Aimee Papola-Ellis, and Joseph Elliott. 2022. Using texts as mirrors: The power of readers seeing themselves. The Reading Teacher, 76(3):277\u2013284.\\n\\nJacob Hirsh and Jordan Peterson. 2009. Personality and language use in self-narratives. Journal of Research in Personality, 43:524\u2013527.\\n\\nKarima Hoytt, Sherrica Hunt, and Margaret A Lovett. 2022. Impact of cultural responsiveness on student achievement in secondary schools. Alabama Journal of Educational Leadership, 9:1\u201312.\\n\\nD Huyck and SP Dahlen. 2019. Diversity in children\u2019s books 2018. sarahpark.com blog. created in consultation with Edith Campbell, Molly Beth Griffin, KT Horning, Debbie Reese, Ebony Elizabeth Thomas, and Madeline Tyner, with statistics compiled by the Cooperative Children\u2019s Book Center, School of Education, University of Wisconsin-Madison.\\n\\nGuangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, and Yixin Zhu. 2023. Evaluating and inducing personality in pre-trained language models.\\n\\nHang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Deb Roy, and Jad Kabbara. 2024. Personallm: Investigating the ability of large language models to express personality traits.\\n\\nTianlong Li, Shihan Dou, Changze Lv, Wenhao Liu, Jianhan Xu, Muling Wu, Zixuan Ling, Xiaoqing Zheng, and Xuanjing Huang. 2024. Tailoring personality traits in large language models via unsupervisedly-built personalized lexicons.\\n\\nUsman Malik, Simon Bernard, Alexandre Pauchet, Cl\u00e9ment Chatelain, Romain Picot-Clemente, and J\u00e9r\u00f4me Cortinovis. 2024. Pseudo-labeling with large language models for multi-label emotion classification of French tweets. IEEE Access, 12:15902\u201315916.\\n\\nShengyu Mao, Xiaohan Wang, Mengru Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Ningyu Zhang. 2024. Editing personality for large language models.\"}"}
{"id": "emnlp-2024-main-382", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.\\n\\nMunan Ning, Yujia Xie, Dongdong Chen, Zeyin Song, Lu Yuan, Yonghong Tian, and Qixiang Ye. 2023. Album storytelling with iterative story-aware captioning and large language models. arXiv preprint arXiv:2305.12943.\\n\\nOpenAI. 2023. Gpt-4 technical report.\\n\\nJames W. Pennebaker and Anna Graybeal. 2001. Patterns of natural language use: Disclosure, personality, and social integration. Current Directions in Psychological Science, 10(3):90\u201393.\\n\\nKatherine Phillips. 2014. How diversity works. Scientific American, 311:42\u20137.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents.\\n\\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530.\\n\\nXiaoqian Shen and Mohamed Elhoseiny. 2023. Storygpt-v: Large language models as consistent story visualizers. arXiv preprint arXiv:2402.03483.\\n\\nOtto Tarkka, Jaakko Koljonen, Markus Korhonen, Jusso Laine, Kristian Martiskainen, Kimmo Elo, and Veronika Laippala. 2024. Automated emotion annotation of Finnish parliamentary speeches using GPT-4. In Proceedings of the IV Workshop on Creating, Analysing, and Increasing Accessibility of Parliamentary Corpora (ParlaCLARIN) @ LREC-COLING 2024, pages 70\u201376, Torino, Italia. ELRA and ICCL.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\\n\\nCandace Walkington and Matthew Bernacki. 2014. Motivating Students by \u201cPersonalizing\u201d Learning around Individual Interests: A Consideration of Theory, Design, and Implementation Issues, volume 18, chapter 4. Preprints.\\n\\nXintao Wang, Yunze Xiao, Jen tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. 2024. Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 1\u201323, New York, NY, USA. ACM.\\n\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models.\"}"}
{"id": "emnlp-2024-main-382", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nFigure 6: Illustration of diversity representation in children's books based on 2018 publishing statistics. Data derived from the Cooperative Children's Book Center, University of Wisconsin-Madison. (Illustration by David Huyck, in consultation with Sarah Park Dahlen) (Huyck and Dahlen, 2019).\\n\\nGPT-4 as an Evaluator\\n\\nSample GPT-4 INPUT:\\n\\nYou are ANDRII, a MALE character, aged 25, of RUSSIAN ethnicity, with an interest in WRESTLING. As an evaluator, evaluate the story based on the following metrics:\\n\\n**METRICS:**\\n\\n**SATISFACTION:** How would you rate your overall satisfaction with the story?\\n\\n1 (BAD): The story did not meet any expectations, had significant flaws in plot, character development, or other key areas, and provided little to no enjoyment or value.\\n\\n5 (GOOD): The story exceeded all expectations, was exceptional in most if not all aspects.\\n\\n**QUALITY:** How would you rate the quality of the story?\\n\\n**STORY:**\\n\\nIn the heart of Moscow lived Andrii, a 25-year-old whose heart and soul were intertwined with the art of wrestling. Every morning, before the first ray of sunlight kissed the domes of Saint Basil's Cathedral...\\n\\nSample GPT-4 OUTPUT:\\n\\nI would rate my overall satisfaction with the story a 5. The unexpected turn in the plot, well-developed character, and the consistent twists kept me engaged, and the story exceeded all my expectations.\\n\\nI would rate the quality of content of the story a 4. The intertwining of fiction and reality, the character...\\n\\nFigure 7: Sample input and output of GPT-4 evaluating a personalized story\"}"}
{"id": "emnlp-2024-main-382", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Annotator #1\\n\\n**Generic Human-Written**\\nTo preserve and never quit. The tortoise never stopped during the race but the hare did and ended up losing because of it.\\n\\n**Actual Moral of the Story**\\n\\n**Generic LLM-Generated**\\nPerseverance. Keep your eye on the prize, don\u2019t worry about what others are doing.\\n\\n**Personalized LLM-Generated**\\nPerseverance. Not everyone\u2019s goals in life will be the same and will take time. Keep taking steps towards your goal and you will be rewarded.\\n\\nAnnotator #2\\n\\n**Generic Human-Written**\\nDespite one\u2019s status someone can still help another.\\n\\n**Actual Moral of the Story**\\n\\n**Generic LLM-Generated**\\n\\nIn order to truly help somebody, one should do their best to accommodate their needs and avoid demanding some kind of compensation for their troubles.\\n\\n**Personalized LLM-Generated**\\nThat kindness has a way of repaying itself one day.\\n\\nTable 3: Sample responses from two annotators on the main message or moral of each story type, compared with the actual intended moral of the stories\\n\\n| A.1 Annotators and Dataset Diversity |\\n|-------------------------------------|\\n| 10-20 | 20-25 | 25-30 | 30+ |\\n| 0 | 5 | 10 | 15 |\\n\\n| Age Distribution |\\n|------------------|\\n| Asian | White | Black | Mix |\\n| 0 | 5 | 10 | 15 |\\n\\n| Race Distribution |\\n|-------------------|\\n| Male | Female | Non-binary |\\n| 48.3% | 47.7% | 4.0% |\\n\\n| Gender Distribution |\\n|---------------------|\\n| Male | Female | Non-binary |\\n| 50% | 50% | 0% |\"}"}
{"id": "emnlp-2024-main-382", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Questionnaire for Annotators\\n\\nFigure 11: Questionnaire used to assess story satisfaction, quality, engagement, and personalness.\"}"}
{"id": "emnlp-2024-main-382", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.3 Personalization Example\\n\\nPersonalization Elements\\n\\nPersonalized Story\\n\\nTop 5 Term\\n\\nAveline, 19, Non-binary, Reading, French\\n\\nAveline was a confluence of distinctive characteristics; their name a symbol of French roots and their gender identity, non-binary, a testament to their unfettered self-expression. At 19, they were a sagacious soul, finding immense joy in...\\n\\nFarida, 23, Female, Carpentry, Uzbek\\n\\nIn the heart of the bustling Uzbek city, Tashkent, Farida, a young, passionate woman, was determined to carve out a unique reputation for herself. At 23, she was an anomaly in her city; she was not working a typical job like...\\n\\nRami, 21, Male, Trekking, Syrian\\n\\nRami, a 21-year-old Syrian youth, was known in his community for two things \u2014 his irresistible passion for trekking and his firm belief in honesty. Dark-haired, with...\\n\\nTable 4: This table presents the personalization elements for three individuals, their personalized stories, and the top 5 terms identified by BERTopic for each story, along with the corresponding relevance scores.\\n\\nA.3.1 Preferential Bias Analysis\\n\\nFigure 12: Average evaluation ratings by GPT-4 across models\\n\\nFigure 13: Average evaluation ratings by GPT-4 across gender\"}"}
{"id": "emnlp-2024-main-382", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: Average evaluation ratings by GPT-4 across ethnic background.\\n\\nA.4 Extended Analysis\\n\\nA.4.1 Qualitative Analysis\\n\\nFigure 15: Qualitative comparison of ratings for three types of stories by both human evaluators and GPT-4, including both conflicting and consistent ratings.\"}"}
{"id": "emnlp-2024-main-382", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.4.2 Correlation Analysis\\n\\nStory Type\\n\\n|                           | Satisfaction | Quality | Engagement | Personalness |\\n|---------------------------|--------------|---------|------------|--------------|\\n| Personalized LLM-generated| 0.24         | 0.47    | 0.34       | 0.08         |\\n| Generic LLM-generated     | 0.19         | 0.22    | 0.20       | 0.12         |\\n| Generic Human-written     | 0.12         | 0.26    | 0.21       | 0.19         |\\n\\nTable 5: Spearman's rank correlation coefficients between human evaluators and GPT-4 for story evaluation metrics, where values closer to 1 indicate a stronger positive correlation.\\n\\nFigure 16: Spearman's rank correlation coefficient between human evaluators and GPT-4 at various temperatures for personalized LLM-generated stories.\\n\\nA.4.3 Image generation for personalized stories\\n\\n| Age | Gender | Interest | Ethnicity |\\n|-----|--------|----------|-----------|\\n| 40% |        |          |           |\\n| 50% |        |          |           |\\n| 60% |        |          |           |\\n| 70% |        |          |           |\\n| 80% |        |          |           |\\n| 90% |        |          |           |\\n| 100%|        |          |           |\\n\\nFigure 17: Accuracy of human and LLM evaluators in identifying personalization elements in the image.\"}"}
{"id": "emnlp-2024-main-382", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.5 Dataset Structure, Categories and Values\\n\\n| Name     | Age | Gender | Interest | Ethnicity | Moral                  |\\n|----------|-----|--------|----------|-----------|------------------------|\\n| LLM-generated Personalized Story | 23  | Male   | Boxing   | Indian    | Learn from your mistakes |\\n| LLM-generated Generic Story     | 29  | Female | History  | Ukrainian | Strive for excellence   |\\n| Human-written Generic Story     |     |        | Volunteering | Italian | Take responsibility for your actions |\\n\\nBehzad, a sturdy 23-year-old Indian man, was far more attached to his boxing gloves than his engineering textbooks. Son of a mechanic, his life was perched comfortably on the rusty edges of Mumbai, lit up by the lambent moon by night and the intruding sun by day. Unlike his peers, Behzad wasn't fascinated by the jingling call of corporate ladders; instead, he...\\n\\nKarina's eyes danced with curiosity as she traced her fingers over the ancient tome. The worn leather binding and yellowed pages spoke of centuries past, each word a whisper from a forgotten era. For her, the study of history was more than a mere pursuit of knowledge; it was a gateway to understanding...\\n\\nDario's eyes sparkled with determination as they stepped into the volunteer center. At 35 years old, they had already made a name for themselves in the local community for their unwavering commitment to making a difference. Born to Italian parents who instilled a deep sense of responsibility and compassion...\\n\\nTable 6: Dataset structure of MIRROR STORIES. The dataset includes personal attributes (Name, Age, Gender, Interest, Ethnicity), a moral, and three types of stories: LLM-Generated Personalized Story, LLM-Generated Generic Story, and Human-Written Generic Story.\"}"}
{"id": "emnlp-2024-main-382", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category       | Values                                                                 |\\n|----------------|------------------------------------------------------------------------|\\n| Age            | 10 ... 60.                                                             |\\n| Gender         | Male, Female, Non-binary.                                              |\\n| Ethnicity      | Albanian, Arab, Arab-Amazigh, Armenian, Australian, Austrian, Akan, ... |\\n| Interest       | Acting, Archery, Arts, Astronomy, Badminton, Bagpiping, Baking, Ballet, ... |\\n| Moral          | Maintain humility, Learn from your mistakes, Be optimistic, Show empathy, ... |\\n\\nTable 7: Breakdown of the different categories and values included in the MIRROR STORIES dataset. It covers a diverse range of ages, genders, ethnicities, interests, and moral values.\"}"}
