{"id": "emnlp-2022-main-751", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 4: Prompts for FETA-DailyDialog tasks\\n\\n| Task                        | Prompt                       |\\n|-----------------------------|------------------------------|\\n| Emotion Recognition (Emory) | emotion:                     |\\n| Reading Comprehension       | question: out of <entities>  |\\n| Character Identification     | out of <options>, <mention>  |\\n| Question Answering          | question: answer:            |\\n| Personality Detection       | <entity> is <characteristic> |\\n| Relation Extraction         | <head> has the following relations with <tail> |\\n| Emotion Recognition (MELD)  | emotion:                     |\\n\\n### Table 5: Prompts for FETA-Friends tasks\\n\\n| Task                        | Prompt                       |\\n|-----------------------------|------------------------------|\\n\\n### Figure 9: Score $\\\\Delta$ by source task type\\n\\nThe number of samples for an individual task are fixed, but source/target ratios vary depending on which task pair is used.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model   | P/F | M     | M/F    |\\n|---------|-----|-------|--------|\\n| BERT    | 0.46| 1.86  | 2.58   |\\n| GPT-2   | 1.14| 1.30  | 3.43   |\\n| T5      | -1.08| 1.54  | 3.00   |\\n\\nTable 6: Results from the multi-source experiment, where we use the top-3 source tasks in a multi-source task transfer setting. We include individual scores from all 3 top-3 source tasks and include their average score as a comparison. Multi-source experiments that improve over the top-3 average are underlined.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nTask transfer, transferring knowledge contained in related tasks, holds the promise of reducing the quantity of labeled data required to fine-tune language models. Dialogue understanding encompasses many diverse tasks, yet task transfer has not been thoroughly studied in conversational AI. This work explores conversational task transfer by introducing FETA: a benchmark for few-sample task transfer in open-domain dialogue. FETA contains two underlying sets of conversations upon which there are 10 and 7 tasks annotated, enabling the study of intra-dataset task transfer; task transfer without domain adaptation. We utilize three popular language models and three learning algorithms to analyze the transferability between 132 source-target task pairs and create a baseline for future work. We run experiments in the single- and multi-source settings and report valuable findings, e.g., most performance trends are model-specific, and span extraction and multiple-choice tasks benefit the most from task transfer. In addition to task transfer, FETA can be a valuable resource for future research into the efficiency and generalizability of pre-training datasets and model architectures, as well as for learning settings such as continual and multitask learning.\\n\\n1 Introduction\\n\\nImproving sample efficiency through transfer learning has been a long-standing challenge in the machine learning and natural language processing communities (Pratt et al., 1991; Ando and Zhang, 2005). Dialogue data requires multiple cohesive turns with consistent speaker personalities (Urbanek et al., 2019; Huang et al., 2020), creating a challenge for data collection and motivating the development of techniques that improve sample efficiency in conversational AI (Lin et al., 2020).\\n\\nFurthermore, dialogue understanding tasks require a shared knowledge of semantics, pragmatics, human behavior, and commonsense, making dialogue an area of study that can benefit greatly from a deeper understanding of transfer learning.\\n\\nTwo essential transfer learning settings, namely domain adaptation and task transfer, have been studied on language tasks (Ruder et al., 2019). While domain adaptation has been studied in task-oriented dialogue (Mehri et al., 2020), task transfer has been studied with less rigor in conversational AI. Prior studies of task transfer in dialogue consider only 2-4 tasks, focus on multitask learning, and do not compare learning algorithms (Hosseini-Asl et al., 2020; Peng et al., 2021b).\\n\\nPrior studies have focused on cross-dataset task transfer, gathering tasks annotated on disjoint datasets (Vu et al., 2020; Ye et al., 2021), but this can lead to improvements in domain adaptation being confounded as improvements in task transfer. A precise study of task transfer should be on a single dataset.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gle data source in an intra-dataset transfer setting, as in Zamir et al. (2018). Additionally, previous studies focus on learning algorithms and use only a single language model architecture (Pruksachatkun et al., 2020; Lourie et al., 2021; Aribandi et al., 2022), which may lead to a narrow understanding. To the best of our knowledge, this is the first rigorous study on task transfer in dialogue and the most extensive intra-dataset task transfer study in NLP.\\n\\nIn this work, we create FETA, a benchmark for few-sample task transfer for language understanding in open-domain dialogue with 17 total tasks. FETA datasets cover a variety of properties (dyadic vs. multi-party, anonymized vs. recurring speaker, varying dialogue lengths) and task types (utterance-level classification, dialogue-level classification, span extraction, multiple-choice), and maintain a wide variety of data quantities.\\n\\nWe study task transfer on FETA by comparing three task transfer algorithms and three commonly used language models in single-source and multi-source settings. Figure 1 illustrates some results in the single-source setting. For example, we find that Dialogue Reasoning Span Extraction benefits from nearly all source tasks. On the other hand, Adversarial Response Selection and Emotion Recognition improve the performance of many target tasks when utilized as a source task.\\n\\nIn this study, we find that:\\n(i) Trends are largely model-dependent, a finding that previous works have not discussed.\\n(ii) Out of all task types, span extraction tasks gain the most as a target, especially with few samples.\\n(iii) Adding source tasks does not uniformly improve over a single source task, motivating a better understanding of the complex relationship between source and target tasks.\\n\\nFETA provides a resource for various future studies, e.g., on the generalizability of model architectures, and pre-training datasets that enable efficient transfer. In addition to task transfer, FETA can also facilitate the study of continual and multitask learning.\\n\\nIn summary, our main contributions are:\\n\u2022 We create the first large-scale benchmark for task transfer in dialogue, with 132 source-target task pairs.\\n\u2022 Extensive experimentation on FETA in both the single-source and multi-source settings, and an in-depth analysis comparing models, learning algorithms, sample sizes, and task types, finding new and non-intuitive results.\\n\u2022 A readily extensible transfer learning framework that allows for rapid experimentation and an online leaderboard to encourage deeper research into task transfer.\\n\\n2 Related Work\\n\\nTransfer Learning in NLP\\nPrior works on transfer learning in NLP have studied a wide variety of topics, including domain adaptation (Ben-David et al., 2010), multitask learning (Collobert and Weston, 2008; Bingel and S\u00f8gaard, 2017), and learning representations of words (Brown et al., 1992; Mikolov et al., 2013; Peters et al., 2017, 2018). More recently, DialoGLUE (Mehri et al., 2020) and RADDLE (Peng et al., 2021a) study domain adaptation for language understanding tasks in task-oriented dialogue. Shuster et al. (2020) focuses on multitasking in dialogue response generation across multiple datasets. Similar to this work, Pruksachatkun et al. (2020) study task transfer, although they study cross-dataset task transfer in general NLP tasks. Lourie et al. (2021) also study task transfer, but they focus on the T5 model and a suite of commonsenseQA datasets.\\n\\nTask Transfer in Dialogue\\nTask transfer has been applied in Task-Oriented Dialogue (TOD) settings but never rigorously studied. For example, Hosseini-Asl et al. (2020) and Lin et al. (2020) develop multitask models to perform 2-4 TOD tasks but do not aim to analyze the efficiency of models or learning algorithms for task transfer.\\n\\nIntra-dataset Task Transfer\\nIntra-dataset transfer has been studied in computer vision applications (Zamir et al., 2018; Pal and Balasubramanian, 2019), but to our best knowledge it has never been studied in NLP.\\n\\n3 FETA\\n\\nIn this section, we briefly define intra-dataset transfer, the problem setting of FETA. Then, we introduce FETA, our benchmark for few-sample task transfer in open-domain dialogue. Finally, we define the metrics we use to evaluate models and learning algorithms on FETA.\\n\\n3.1 Problem Definitions\\n\\nLet a dataset be composed of the instance set, $X$, and $n$ task-specific label sets $Y_1, Y_2, \\\\ldots, Y_n$. In...\"}"}
{"id": "emnlp-2022-main-751", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task Name                      | Train | Dev  | Test | Metrics        |\\n|-------------------------------|-------|------|------|----------------|\\n| DailyDialog Emotion Recognition | 102978 | 7230 | 1269 | 15885          |\\n| Dialogue Act Classification   | 102978 | 7230 | 1269 | 15885          |\\n| Topic Classification         | 13118  | 958  | 161  | 1919           |\\n| Causal Emotion Span Extraction | 36324 | 2141 | 169  | 9133           |\\n| Causal Emotion Entailment    | 36324 | 2141 | 169  | 9133           |\\n| Dialogue-Level NLI           | 5817  | 569  | 52   | 1302           |\\n| Dialogue Reasoning Span Extraction | 1098 | 123  | 13   | 244            |\\n| Dialogue Reasoning Multiple Choice | 2165 | 224  | 26   | 496            |\\n| Commonsense Relation Extraction | 4009 | 350  | 38   | 851            |\\n| Adversarial Response Selection | 57145 | 3400 | 895  | 10750          |\\n| Emotion Recognition (EmoryNLP) | 12606 | 844  | 207  | 1912           |\\n| Reading Comprehension        | 13865 | 912  | 181  | 2284           |\\n| Character Identification     | 50247 | 3593 | 638  | 7803           |\\n| Question Answering           | 12257 | 819  | 191  | 1937           |\\n| Personality Detection        | 711   | 54   | 15   | 110            |\\n| Relation Extraction          | 7636  | 519  | 121  | 1188           |\\n| Emotion Recognition (MELD)   | 9140  | 616  | 148  | 1247           |\\n\\n**Table 1:** Overview of FETA tasks. Task types are abbreviated as follows: Utt Cls for utterance-level classification, Dial Cls for dialogue-level classification, Span Ex for span extraction, and Mult Ch for multiple choice. Metrics are abbreviated as follows: M-F1 for macro-F1, m-F1 for micro-F1, T-F1 for token-F1, W-F1 for weighted-F1, EM for exact match and Acc for accuracy.\\n\\nFETA, each instance \\\\( x \\\\in X \\\\) is a dialogue.\\n\\n**Definition 1** (Domain and Task).\\n\\n\\\\[ D = \\\\{ X, P(X) \\\\} \\\\]\\n\\nA domain consists of a feature space \\\\( X \\\\) and a marginal probability distribution \\\\( P(X) \\\\). The marginal probabilities are over the instance set \\\\( X = \\\\{ x_1, x_2, \\\\ldots, x_n \\\\} \\\\in X \\\\).\\n\\nA task \\\\( T = \\\\{ Y, f(X) \\\\} \\\\) is composed of a label space \\\\( Y \\\\) and a predictive function, \\\\( f: X \\\\rightarrow Y \\\\).\\n\\n**Definition 2** (Learning Algorithm).\\n\\nA learning algorithm, \\\\( A \\\\), is a protocol that determines the method by which the instance set \\\\( X \\\\) and task-specific label sets \\\\( Y_1, Y_2, \\\\ldots, Y_n \\\\) will be used to train a predictive function, \\\\( f \\\\).\\n\\n**Definition 3** (Task Transfer).\\n\\nGiven a source task \\\\( T_S = \\\\{ Y_S, f_S(X_S) \\\\} \\\\) and target task \\\\( T_T = \\\\{ Y_T, f_T(X_T) \\\\} \\\\), task transfer is the use of a learning algorithm, \\\\( A \\\\), to improve the learning of \\\\( f_T \\\\) by using the knowledge in \\\\( T_S \\\\).\\n\\nIn cross-dataset task transfer, when \\\\( X_S \\\\neq X_T \\\\), we also have \\\\( P(X_S) \\\\neq P(X_T) \\\\) and \\\\( D_S \\\\neq D_T \\\\); domain shift.\\n\\nIn intra-dataset task transfer, when \\\\( X_S = X_T \\\\), there is no domain shift. This enables the study of the learning algorithm's performance on task transfer, isolated from domain adaptation.\\n\\nWe refer the reader to Pan and Yang (2010) and Zhuang et al. (2021) for expanded discussions on transfer learning definitions.\\n\\n**3.2 FETA Datasets**\\n\\nIn this section, we describe the two dialogue sources we use, DailyDialog (Li et al., 2017) and Friends (Chen and Choi, 2016), and the tasks annotated on each source.\\n\\nWe select these datasets because they complement each other in desirable ways. DailyDialog contains 2-speaker dialogues where speakers are anonymized and averages 88 words per dialogue. In contrast, Friends consists of multiparty dialogues (3.6 speakers mean, 15 max) with recurring characters and averages 283 words per dialogue. These differences lead to each set of dialogue instances having different task annotations, giving FETA a wider variety of tasks. For example, DailyDialog tasks include understanding the causes of emotions and commonsense reasoning, while tasks annotated on Friends revolve more around recognizing them.\\n\\nDue to the challenge and cost of collecting and annotating data, many real-world applications of NLP techniques are limited by data quantities. For this reason, we focus on the few-sample setting, defined in FETA as 10% of the original instance set. Out of 10%, 5%, and 1%,\\n\\n10% was empirically determined to be the smallest percentage that retains labels from all label sets in both the train and development partitions. Given the recent attention focused on NLP applications in low-resource settings (Brown et al., 2020; Bansal et al., 2020; Mukherjee et al., 2021; Ye et al., 2021), we expect research done in such a low-data setting will lead to insights useful for many researchers and practitioners.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To create FETA versions of each dataset, we first partition the dialogues into 70/15/15% splits for training, validation, and test sets. After splitting, we randomly down-sample the train and development dialogues to 10% of the original quantities. Thus, FETA splits use 7/1.5/15% of the original dialogues. Not every dialogue is annotated for all tasks, allowing some tasks to have more samples than others. Crucially, the data splits are the same for all tasks, preventing data leakage. Table 1 shows an overview of the tasks, samples, and metrics used for each dataset.\\n\\nFETA-DailyDialog\\nLi et al. (2017) present the DailyDialog dataset, with chit-chat conversations covering 10 various topics including relationships, politics, and work. Many works add annotations on top of these dialogues and FETA utilizes 10 of them. Figure 2 provides an overview of the tasks:\\n\\n- emotion recognition\\n- dialogue act classification\\n- topic classification (from DailyDialog (Li et al., 2017))\\n- causal emotion span extraction\\n- causal emotion entailment (from RECCON (Poria et al., 2021))\\n- dialogue-level natural language inference\\n- dialogue reasoning span extraction\\n- dialogue reasoning multiple choice\\n- commonsense relation extraction (from CIDER (Ghosal et al., 2021))\\n- adversarial response selection (from DailyDialog++ (Sai et al., 2020)).\\n\\nFor further details of these tasks, we refer the reader to Appendix A and their original papers.\\n\\nFETA-Friends\\nThe Friends dialogues come from transcripts of 10 seasons of the TV show by the same name (Chen and Choi, 2016). In addition to dialogue, the transcripts contain situational information such as behaviors and non-verbal information like scene information. In total, FETA has 7 task annotations on top of the Friends scripts. As illustrated in Figure 2, the incorporated tasks include:\\n\\n- Emory emotion recognition (from (Zahiri and Choi, 2018))\\n- reading comprehension (from (Ma et al., 2018))\\n- character identification (from (Chen and Choi, 2016; Zhou and Choi, 2018))\\n- question answering (from (Yang and Choi, 2019))\\n- personality detection (from (Jiang et al., 2020))\\n- relation extraction (from DialoGRE (Yu et al., 2020a)) and\\n- MELD emotion recognition (from MELD (Poria et al., 2019)).\\n\\nThere are two emotion recognition label sets (Emory and MELD), but they have only 22% overlap in instance sets and have different label spaces. For further details of these tasks, we refer the reader to Appendix A and their original papers.\\n\\n3.3 Evaluation Metrics\\nTo define the metrics, we consider 4 variables: source tasks $s$, target task $t$, model $f$, and learning algorithm $A$, and we abuse notation slightly to allow for $f_A(s, t)$ to represent a model trained on the source and target tasks using the given learning algorithm $A$. There are two emotion recognition label sets (Emory and MELD), but they have only 22% overlap in instance sets and have different label spaces.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm. In FETA, we evaluate the performance of a model and learning algorithm with multiple metrics: average and top-1 raw scores, as well as average and top-1 score $\\\\Delta$.\\n\\n### Average and Top-1 Scores\\n\\nFirst, we consider the two raw scores: average score and top-1 score. These metrics aim to answer the following questions:\\n\\n- How well do a model and algorithm perform across all task pairs?\\n- How well do a model and algorithm perform supposing that we knew the best source task a priori.\\n\\nWe calculate an average score across all source-target task pairs to understand how each model and algorithm performs in the aggregate. Formally, let the score for a single task be computed as:\\n\\n$$\\\\text{score}(s, t, f, A) = \\\\frac{1}{|M_t|} \\\\sum_{i=1}^{M_t} (f_A(s, t))$$\\n\\nwhere $M_t$ is the set of metrics associated with task $t$, found in Table 1, and $M_{t,i}(f)$ is the $i$th calculated metric of model $f$ on task $t$.\\n\\nAll metrics range from 0 to 100. Then, we calculate the average score as:\\n\\n$$\\\\text{Average Score}(f, A) = \\\\sum_{t \\\\in T} \\\\sum_{s \\\\neq t \\\\in T} \\\\text{score}(s, t, f, A) \\\\times (|T| - 1)$$\\n\\nwhere $T$ is the set of tasks.\\n\\nAdditionally, we calculate top-1 score to understand how models and algorithms perform if the best source task is known ahead of time. This score is calculated as the maximum score over source tasks averaged over target tasks. The top-1 score does not consider scores less than the baseline, which is a model trained directly on the target task. Denote the baseline algorithm by $A_B$ and the baseline score as $\\\\text{score}(s, t, f, A_B)$.\\n\\nFormally, the top-1 score is calculated as:\\n\\n$$\\\\text{Top-1}(f, A) = \\\\sum_{t \\\\in T} \\\\max_{s \\\\neq t \\\\in T} (\\\\text{score}(s, t, f, A_B), \\\\text{score}(s, t, f, A))$$\\n\\n### Average and Top-1 $\\\\Delta$s\\n\\nIn addition to raw scores, we also calculate score differences to measure how much a source task benefits a target task. The average $\\\\Delta$ describes how much benefit the model saw in the aggregate over all source tasks, while the top-1 $\\\\Delta$ considers only the best source. Score $\\\\Delta$s are calculated with respect to the baseline score as:\\n\\n$$\\\\Delta(s, t, f, A) = \\\\text{score}(s, t, f, A) - \\\\text{score}(s, t, f, A_B)$$\\n\\nand the average $\\\\Delta$ is calculated as:\\n\\n$$\\\\text{Average } \\\\Delta(f, A) = \\\\sum_{t \\\\in T} \\\\sum_{s \\\\neq t \\\\in T} \\\\Delta(s, t, f, A) \\\\times (|T| - 1)$$\\n\\nAdditionally, we calculate the top-1 $\\\\Delta$ as the maximum positive score difference over source tasks averaged over target tasks:\\n\\n$$\\\\text{Top-1 } \\\\Delta(f, A) = \\\\sum_{t \\\\in T} \\\\max_{s \\\\neq t \\\\in T} (0, \\\\Delta(s, t, f, A))$$\\n\\n### 4 Task Transfer Algorithms\\n\\nIn this work, we consider three commonly used task transfer methods: Pre-train/Fine-tune, Multitask, Multitask/Fine-tune. We apply these methods with cross-entropy loss to further optimize pretrained language models on FETA.\\n\\n**Pre-train/Fine-tune**\\n\\nCommonly used in NLP today, the pre-train/fine-tune algorithm consists of two stages of training (Pratt et al., 1991). First, the model is trained on the source task $T_S$, optimizing Eq 1, followed by a separate stage of training on the target task $T_T$, optimizing Eq 2:\\n\\n$$L_S = -\\\\mathbb{E}_{(x, y_s) \\\\sim \\\\{X, Y_S\\\\}} \\\\left[ \\\\log p(y_s \\\\mid x) \\\\right] \\\\quad (1)$$\\n\\n$$L_T = -\\\\mathbb{E}_{(x, y_t) \\\\sim \\\\{X, Y_T\\\\}} \\\\left[ \\\\log p(y_t \\\\mid x) \\\\right] \\\\quad (2)$$\\n\\n**Multitask**\\n\\nIn this algorithm, there is only a single stage of multitask training (Caruana, 1994). Formally, the training is conducted on both the source and target task by optimizing Eq 3:\\n\\n$$L_{S,T} = -\\\\mathbb{E}_{(x, y_s, y_t) \\\\sim \\\\{X, Y_S, Y_T\\\\}} \\\\left[ \\\\log p(y_s \\\\mid x) + \\\\log p(y_t \\\\mid x) \\\\right] \\\\quad (3)$$\\n\\n**Multitask/Fine-tune**\\n\\nThis algorithm combines the previous algorithms in two stages. In the first stage, the source and target task are optimized jointly, as in Eq 3. Then, the second stage trains using only the target task, as in Eq 2.\\n\\nEven though model selection in multitasking is generally done w.r.t. multiple source and target tasks (Caruana, 1994), we modify the setting to validate a model on a single target task at a time. This allows hyperparameter search and early stopping to be controlled by the desired target task.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Average and Top-1 Source task transfer scores. Average scores and $\\\\Delta$s aggregate scores over all source tasks, compared with Top-1 scores and $\\\\Delta$s which are calculated with scores from the highest performing source task. $\\\\Delta$s are the difference from the baseline score without task transfer. Highest values for each model are underlined, highest values across all models are bolded.\\n\\n5 Experiment Setup\\nTo study task transfer on FETA, we run extensive experimentation. We utilize three task transfer algorithms: pre-train/fine-tune, multitask, and multitask/fine-tune, as described in Section 4. To draw broad conclusions about the performance of each learning algorithm, we utilize pretrained language models with three different architectures: encoder-only (BERT) (Devlin et al., 2019), decoder-only (GPT-2) (Radford et al., 2019), and encoder-decoder (T5) (Raffel et al., 2020). Implementation details, including hyperparameters and prompts, can be found in Appendix B.\\n\\nA complete experiment for a single target task, $T$, is as follows: First, we directly fine-tune on $T$ to get the baseline score. Then, for each source task, $S$, we take the model pre-trained on $S$ and fine-tune on $T$. Next, we jointly train on $S$ and $T$ together. Finally, we fine-tune the jointly trained model on $T$.\\n\\nFETA datasets have 10 and 7 tasks, giving $90 + 42 = 132$ unique source-target task pairs. Our experiments include three learning algorithms, three models, and we run each experiment with 5 random seeds. In total, we run $132 \\\\times 3 \\\\times 3 \\\\times 5 = 5940$ transfer experiments, and $17 \\\\times 3 \\\\times 5 = 255$ baseline experiments leading to 6195 trained models.\\n\\nIn addition to the single-source setting described above, we also consider a subset of tasks to study in the multi-source setting, where multiple tasks are simultaneously used as source tasks to transfer to a single target task (6.2). For our experiments, we select two target tasks from each dataset that benefit the most from task transfer, and we use the three source tasks that transferred best onto those targets.\\n\\n6 Results and Analysis\\n6.1 Single-Source Setting\\nTable 2 shows the results for all three models and algorithms, and we use this table to understand general trends. Figure 3 shows the relative improvement of a source task for each target task, demonstrating trends across tasks.\\n\\n Aggregate Performance\\nWe find that, on average, Friends tasks get scores between 7-8 points less than DailyDialog, likely due to the greater number of speakers and utterance length of Friends. We find that GPT-2 lags behind the raw scores of BERT and T5 by $\\\\sim 10$ points. This is expected as autoregressive decoder models are not designed with classification in mind. We find that the largest average $\\\\Delta$ is 1.4, leaving room for improvement in task transfer on FETA.\\n\\n Furthermore, we are interested in knowing: how much we would gain by using the best source task vs. a random source task. We calculate the differences between average $\\\\Delta$ and top-1 $\\\\Delta$ and find the mean difference to be $\\\\sim 1.6$ and the largest difference to be $\\\\sim 3.5$, motivating a further understanding of which source tasks transfer best to target tasks.\\n\\n Performance Across Learning Algorithms\\nWe average scores across both datasets and find that pre-train/fine-tune gets an average score of 42.85, multitask 42.84, and multitask/fine-tune 44.07. Table 2 shows that multitask/fine-tune achieves the best average score for all models and datasets, and indeed its average score is a 2.8% improvement over the other algorithms. However, aggregate scores obscure some interesting nuances.\\n\\n Do Trends Vary Across Models?\\nPrevious studies on task transfer have focused on a single model...\"}"}
{"id": "emnlp-2022-main-751", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Relative improvement of transfer over fine-tuned baselines. Rows are source tasks and columns are target tasks. Diagonal cells are baseline scores. Looking at an individual column can demonstrate best source tasks for that target. Looking at rows can determine which source task works well across multiple targets. (Pruksachatkun et al., 2020; Lourie et al., 2021; Aribandi et al., 2022), but we find that trends vary depending on the model. For example, we find results similar to Lourie et al. (2021), namely, that fine-tuning on the target task always benefits the T5 model. However, we discover that this does not hold for BERT and GPT-2, which achieve better scores from multitasking than pre-train/fine-tune. Furthermore, Figure 3 shows that trends on individual tasks also vary depending on the model. For example, T5 positively transferred knowledge to question answering with all learning algorithms and from most source tasks, while GPT-2 had a negative transfer from all algorithms and sources. For nearly all dimensions of analysis (e.g., sample sizes, learning algorithm), we find different trends between models. We strongly suggest that future research be performed on multiple models before attempting to draw broad conclusions on transfer learning.\\n\\nMultitask/Fine-tune As Regularization\\n\\nWe find that T5\u2019s top-1 score and $\\\\Delta$ on DailyDialog are highest for pre-train/fine-tune, but the average score and $\\\\Delta$ are highest for multitask/fine-tune. To understand why this occurred, we find the bottom-1 scores for T5 on DailyDialog: 46.78, 46.69, and 48.26 for pre-train/fine-tune, multitask, and multitask/fine-tune algorithms, confirming that multitask/fine-tune does achieve the best worst-case performance. Moreover, we find that for all datasets and models, multitask/fine-tune does achieve the best worst-case performance. In fact, for GPT-2 on Friends, utilizing the bottom-1 source tasks still lead to a 0.74% improvement over the baseline.\\n\\nDo All Task Types Benefit Equally?\\n\\nWe find that span extraction tasks gain the most as target tasks, shown in Figure 4 to benefit at all source-to-target sample ratios. Multiple choice tasks also stand to gain from task transfer, but we find that only occurs at a 10:1 ratio of source-target samples. This gain is likely due to the high-level language understanding required by both tasks. Additionally, we find that utterance-level classification tasks decrease in score $\\\\Delta$ at increasing source-to-target sample ratios. This is possibly due to models overfitting to specific tasks and a catastrophic forgetting of general skills learned during their large-scale pre-training.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Score $\\\\Delta$ by target task type. Lines show the average score $\\\\Delta$ when the target task is of the specified task type, computed as a best-fit linear interpolation of the data with a 95% confidence interval. The number of samples for an individual task are fixed, but source/target ratios vary depending on which task pair is used.\\n\\nHow Do Sample Sizes Affect Transfer?\\n\\nFigure 5 shows that, interestingly, GPT-2 and T5 have opposite trends in relation to sample size. We find that $\\\\Delta$s for GPT-2 increase with high target samples and decrease with high source samples. This suggests that GPT-2 may be overfitting to the source task and performs better with resource-rich target tasks. We find that T5 $\\\\Delta$s decrease as target-task samples increase, suggesting that T5 is more sample efficient than both GPT-2 and BERT.\\n\\n6.2 Multi-Source Setting\\n\\nFor multi-source transfer we select the two target tasks from each dataset with the best score differences from the single-source setting, shown in Figures 7 and 8 in the Appendix. We find those four tasks to be Dialogue Reasoning Span Extraction (DRSE), Dialogue-Level NLI (DNLI), Character Identification (CI), and Question Answering (QA). For each of these target tasks, we select the top-3 best source tasks, shown in Table 6 of the Appendix. Learning in this setting is similar to single-source, except we now simultaneously optimize the loss for multiple source tasks. Table 3 shows the multi-source results compared with the average score of the top-3 source tasks from the single-source setting. Full results, including score $\\\\Delta$s from the single-source baselines, average top-3 score $\\\\Delta$s, and multi-source score $\\\\Delta$s are in Table 6 of the Appendix.\\n\\nDoes Multi-source Improve Over Single-source?\\n\\nWe expect that by utilizing the top-3 source tasks from the single-source setting, the multi-source setting will improve performance for all models and algorithms, but find results to the contrary. We find that 6/9 multi-source algorithms outperform their average top-3 single-source counterparts in DRSE, 6/9 for DNLI, 3/9 for CI, and only 2/9 for QA, showing that naively combining source tasks is not always beneficial. The impressive result for DRSE follows our original intuition, given that there is an almost unanimous benefit from all source tasks, shown in Figure 3. Similarly, we find that multi-source performance on CI also correlates with the performance of individual source tasks. We find that in the single-source setting GPT-2 is the only model that improves with any source task, and indeed GPT-2 sees benefits from multi-source training on all algorithms.\\n\\nWhich Models Benefit From Multi-Source?\\n\\nTable 6 shows that GPT-2 improves in 8/12 experiments.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Target DRSE DNLI CI QA\\nBERT\\nP/F -1.18 +1.37 -2.11 -0.99\\nM +2.77 +1.57 -0.54 -1.14\\nM/F +1.61 +2.28 -0.34 -0.55\\nGPT-2\\nP/F +0.40 +0.16 +4.25 -3.90\\nM +0.78 +0.98 +1.28 -2.46\\nM/F +0.73 -0.09 +0.00 -0.95\\nT5\\nP/F +0.60 +1.95 -0.79 +0.48\\nM -1.08 -0.96 -1.49 +0.08\\nM/F -1.22 -1.20 -0.24 -0.22\\n\\nTable 3: Multi-source score \u2206s from the average score of the top-3 source tasks. Full results, including score \u2206s from the fine-tuned baseline are in Table 6.\\n\\nments over its average top-3 single-source counterparts, but BERT only 5/12 and T5 in only 4/12 experiments. It is counter-intuitive that T5 should perform the worst as we expect that it has a higher capacity for learning due to twice the model size. On the other hand, the additional parameters may be causing T5 to overfit on training data in the few-sample setting.\\n\\n7 Conclusion\\nWe introduce FETA, a comprehensive benchmark for evaluating language models and task transfer learning algorithms in open-domain dialogue with few samples. Through extensive experimentation, we find new and non-intuitive insights on the mechanisms of transfer learning. In particular, we find that most trends are model-specific, and we strongly encourage researchers to consider multiple model architectures before attempting to draw broad conclusions on transfer learning. It is our hope that FETA enables further research not only in task transfer, but also in other learning settings, and in the generalizability and efficiency of model architectures and pre-training datasets.\\n\\nLimitations\\nA concern regarding any work that includes large-scale experiments with large language models is the energy consumption and environmental impact, the current work included. While there is a cost to running these experiments, the goal of this work is to improve sample efficiency in the future and we hope that the benefits in future energy saved will outweigh the up-front costs of discovering efficient methods.\\n\\nAnother concern of a large-scale benchmark is that of accessibility. A benchmark requiring too many resources will limit those who can reasonably compete. For this reason and others, in addition to our large-scale benchmark we also include a smaller multi-source setting which requires only 4 experiments to be run for a single model and algorithm, rather than 132 in the single-source setting. We believe this smaller setting will maintain the ability to extract high-quality insights on task transfer, yet allow for increased community access and reduce the carbon footprint of this benchmark.\\n\\nWhile we do control for domain adaptation in our experiments on task transfer, there are some aspects that we cannot control. For example, each model has done language model pre-training with a different corpus. BERT was trained on English Wikipedia and BookCorpus (Zhu et al., 2015), GPT-2 was trained on a WebText (Radford et al., 2019), and T5 was trained on C4 (Raffel et al., 2020). This difference likely affects model performance on the dialogue tasks in FETA.\\n\\nAdditionally, we cannot exhaustively test every language model, but still try to provide enough variety in order to draw broad conclusions on task transfer. For example, we don't run any experiments on language models pre-trained in the dialogue domain or language models larger than base-sized. We expect that both of these changes would improve raw performance on FETA. More importantly though, it is unclear whether either of these changes would lead to improved task-transfer performance (average and top-1 \u2206s) and we leave this exploration for future work.\\n\\nFurthermore, we cannot exhaustively test all learning algorithms. For example, Wang et al. (2020) propose a transfer learning method that minimizes negative task interference via meta-learning for multilingual models, Albalak et al. (2022) propose a policy-guided algorithm for task transfer in low-data settings, and Yu et al. (2020b) propose an optimization algorithm that mitigates gradient interference for reinforcement learning agents. Finally, we stress the importance of intra-dataset task transfer in this work. However, this limits the number of pre-annotated tasks that are available, and there are certainly some tasks which we were not able to accommodate in FETA.\\n\\nAcknowledgements\\nThe authors would like to thank William Cohen and Tania Bedrax-Weiss for their valuable insights and constructive feedback about this work. This material is based on work that is partially funded by an unrestricted gift from Google. This work...\"}"}
{"id": "emnlp-2022-main-751", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"was supported by the National Science Foundation award #2048122. The views expressed are those of the author and do not reflect the official policy or position of the US government. Finally, we thank the Robert N. Noyce Trust for their generous gift to the University of California via the Noyce Initiative.\\n\\nReferences\\n\\nAlon Albalak, Varun Embar, Yi-Lin Tuan, Lise Getoor, and William Yang Wang. 2022. D-REX: Dialogue relation extraction with explanations. In Proceedings of the 4th Workshop on NLP for Conversational AI, pages 34\u201346, Dublin, Ireland. Association for Computational Linguistics.\\n\\nRie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(61):1817\u20131853.\\n\\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. 2022. Ext5: Towards extreme multi-task scaling for transfer learning. In International Conference on Learning Representations.\\n\\nTrapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, and Andrew McCallum. 2020. Self-supervised meta-learning for few-shot natural language classification tasks. In EMNLP.\\n\\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Vaughan. 2010. A theory of learning from different domains. Machine Learning, 79:151\u2013175.\\n\\nJoachim Bingel and Anders S\u00f8gaard. 2017. Identifying beneficial task relations for multi-task learning in deep neural networks. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 164\u2013169, Valencia, Spain. Association for Computational Linguistics.\\n\\nPeter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467\u2013480.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nRich Caruana. 1994. Learning many related tasks at the same time with backpropagation. In Advances in Neural Information Processing Systems, volume 7. MIT Press.\\n\\nYu-Hsin Chen and Jinho D. Choi. 2016. Character identification on multiparty conversation: Identifying mentions of characters in TV shows. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 90\u2013100, Los Angeles. Association for Computational Linguistics.\\n\\nRonan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, ICML '08, page 160\u2013167, New York, NY, USA. Association for Computing Machinery.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nDeepanway Ghosal, Pengfei Hong, Siqi Shen, Navonil Majumder, Rada Mihalcea, and Soujanya Poria. 2021. CIDER: Commonsense inference for dialogue explanation and reasoning. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 301\u2013313, Singapore and Online. Association for Computational Linguistics.\\n\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue. In Advances in Neural Information Processing Systems, volume 33, pages 20179\u201320191. Curran Associates, Inc.\\n\\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020. Challenges in building intelligent open-domain dialog systems. ACM Transactions on Information Systems (TOIS), 38:1 \u2013 32.\\n\\nHang Jiang, Xianzhe Zhang, and Jinho D Choi. 2020. Automatic text-based personality recognition on monologues and multiparty dialogues using attentional networks and contextual embeddings (student abstract). In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13821\u201313822.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, 10945.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. DailyDialog: A manually labeled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 986\u2013995, Taipei, Taiwan. Asian Federation of Natural Language Processing.\\n\\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata, and Pascale Fung. 2020. MinTL: Minimalist transfer learning for task-oriented dialogue systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3391\u20133405, Online. Association for Computational Linguistics.\\n\\nNicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. AAAI.\\n\\nKaixin Ma, Tomasz Jurczyk, and Jinho D. Choi. 2018. Challenging reading comprehension on daily conversation: Passage completion on multiparty dialog. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2039\u20132048, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nS. Mehri, M. Eric, and D. Hakkani-Tur. 2020. Dialoglue: A natural language understanding benchmark for task-oriented dialogue. ArXiv, abs/2009.13570.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.\\n\\nSubhabrata (Subho) Mukherjee, Xiaodong Liu, Guoqing Zheng, Saghar Hosseini, Hao Cheng, Greg Yang, Chris Meek, Ahmed H. Awadallah, and Jianfeng Gao. 2021. Clues: Few-shot learning evaluation in natural language understanding. In NeurIPS 2021.\\n\\nArghya Pal and Vineeth N Balasubramanian. 2019. Zero-shot task transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2189\u20132198.\\n\\nSinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22:1345\u20131359.\\n\\nBaolin Peng, Chengkun Li, Zhu Zhang, Chenguang Zhu, Jinchao Li, and Jianfeng Gao. 2021a. Raddle: An evaluation benchmark and analysis platform for robust task-oriented dialog systems. ArXiv, abs/2012.14666.\\n\\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-deh, Lars Liden, and Jianfeng Gao. 2021b. Soloist: Building Task Bots at Scale with Transfer Learning and Machine Teaching. Transactions of the Association for Computational Linguistics, 9:807\u2013824.\\n\\nMatthew E. Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1756\u20131765, Vancouver, Canada. Association for Computational Linguistics.\\n\\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2019. Meld: A multimodal multi-party dataset for emotion recognition in conversations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 527\u2013536.\\n\\nSoujanya Poria, Navonil Majumder, Devamanyu Hazarika, Deepanway Ghosal, Rishabh Bhardwaj, Samson Yu Bai Jian, Pengfei Hong, Romila Ghosh, Abhinaba Roy, Niyati Chhaya, Alexander Gelbukh, and Rada Mihalcea. 2021. Recognizing emotion cause in conversations. Cognitive Computation.\\n\\nLorien Y. Pratt, Jack Mostow, and Candace A. Kamm. 1991. Direct transfer of learned information among neural networks. In Proceedings of the Ninth National Conference on Artificial Intelligence - Volume 2, AAAI'91, page 584\u2013589. AAAI Press.\\n\\nYada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5231\u20135247, Online. Association for Computational Linguistics.\\n\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, and Thomas Wolf. 2019. Transfer learning in natural language processing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials, pages 15\u201318, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nAnanya B. Sai, Akash Kumar Mohankumar, Siddhartha Arora, and Mitesh M. Khapra. 2020. Improving Dialog Evaluation with a Multi-reference Adversarial Dataset and Large Scale Pretraining. Transactions of the Association for Computational Linguistics, 8:810\u2013827.\\n\\nKurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Y-Lan Boureau, and Jason Weston. 2020. The dialogue dodecathlon: Open-domain knowledge and image grounded conversational agents. In ACL.\\n\\nJack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rockt\u00e4schel, Douwe Kiela, Arthur Szlam, and Jason Weston. 2019. Learning to speak and act in a fantasy text adventure game. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 673\u2013683, Hong Kong, China. Association for Computational Linguistics.\\n\\nTu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, and Mohit Iyyer. 2020. Exploring and predicting transferability across nlp tasks. In EMNLP.\\n\\nZirui Wang, Zachary C. Lipton, and Yulia Tsvetkov. 2020. On negative interference in multilingual models: Findings and a meta-learning treatment. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4438\u20134450, Online. Association for Computational Linguistics.\\n\\nGloria Wilcox. 1982. The feeling wheel. Transactional Analysis Journal, 12:4:274\u2013276.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierre Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nZhengzhe Yang and Jinho D. Choi. 2019. FriendsQA: Open-domain question answering on TV show transcripts. In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 188\u2013197, Stockholm, Sweden. Association for Computational Linguistics.\\n\\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. Crossfit: A few-shot learning challenge for cross-task generalization in nlp. In EMNLP.\\n\\nDian Yu, Kai Sun, Claire Cardie, and Dong Yu. 2020a. Dialogue-based relation extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\\n\\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020b. Gradient surgery for multi-task learning. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA. Curran Associates Inc.\\n\\nSayyed M Zahiri and Jinho D Choi. 2018. Emotion detection on tv show transcripts with sequence-based convolutional neural networks. In Workshops at the thirty-second aaai conference on artificial intelligence.\\n\\nAmir R. Zamir, Alexander Sax, William Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese. 2018. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nEthan Zhou and Jinho D. Choi. 2018. They exist! introducing plural mentions to coreference resolution and entity linking. In Proceedings of the 27th International Conference on Computational Linguistics, pages 24\u201334, Santa Fe, New Mexico, USA. Association for Computational Linguistics.\\n\\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV).\\n\\nFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. 2021. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109:43\u201376.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Dataset Details\\n\\nA.1 DailyDialog\\nDailyDialog\\nAlong with the dialogues, Li et al. (2017) provide annotations for emotion recognition, dialogue act classification, and topic classification.\\n\\nRECCON\\nPoria et al. (2021) introduce the task of recognizing emotion causes in conversation and provide annotations for two subtasks: causal emotion span extraction and causal emotion entailment. Recognizing the cause behind emotions is an important aspect of developing conversational agents that can respond appropriately and these tasks test that ability. Both tasks assume that the emotion of an utterance is already known and require a model to identify the evidence or cause of the given emotion. In causal emotion span extraction, the model is given input as \\\"The target utterance is \\\\text{<U_t>}. The evidence utterance is \\\\text{<U_e>}. What is the causal span from evidence in the context that is relevant to the target utterance's emotion <E_t>?\\\". On the other hand, if the conversation history up to utterance \\\\text{U_t} is \\\\text{H(U_t)}, then the task of causal emotion entailment is to classify the triple \\\\text{(U_t, U_e, H(U_t))} as entailment or not entailment. In this case, entailment means that the emotion expressed in the target utterance, \\\\text{U_t}, is caused by the evidence utterance, \\\\text{U_e}.\\n\\nCIDER\\nGhosal et al. (2021) provide annotations for four tasks designed to explore commonsense inference and reasoning in dialogue: dialogue-level natural language inference (DNLI), dialogue reasoning span extraction, dialogue reasoning multiple choice, and commonsense relation extraction. These tasks are created by annotating knowledge triplets on 31 relations that are either explicitly stated in the dialogue or that require commonsense reasoning using contextual information. In DNLI, the task is to determine whether a triplet is true or false given the dialogue. Given a knowledge triplet as \\\\text{<head, relation, tail>}, the span extraction task is formulated as identifying the tail when given the head, relation, and dialogue for context. The multiple choice task is motivated by the SWAG commonsense inference task (Zellers et al., 2018), given a head, relation, and conversation as context, the goal is to predict the tail of the relation from 4 possible choices. Finally, commonsense relation extraction is formulated as usual relation extraction tasks; given the head, tail, and conversation as context, the goal is to predict the correct relation out of 31 options.\\n\\nDailyDialog++\\nSai et al. (2020) present the DailyDialog++ dataset, where they aim to improve evaluation of response generation. They do so by collecting five relevant responses and five adversarially crafted irrelevant responses for each dialogue in their dataset, and we recycle their data for a new task called adversarial response selection. Adversarial response selection is formulated as a multiple choice selection between a correct response, a randomly selected negative response, and an adversarial negative response.\\n\\nA.2 Friends\\nEmoryNLP\\nChen and Choi (2016) and Zhou and Choi (2018) provide annotations for character identification, a subtask of entity linking, where entity mentions in an utterance need to be matched to their correct entity. For this task there are seven possible entities: the six main characters and an \\\"other\\\" entity. Zahiri and Choi (2018) provide annotations on emotion recognition, with the 7 fine-grained emotions from the Feeling Wheel (Wilcox, 1982). Ma et al. (2018) present annotations for a subtask of reading comprehension, called passage completion. In passage completion, given a dialogue and factual statement about the dialogue where character mentions are removed, the task is to fill in the blanks with the correct character from the dialogue. This task is similar to a multiple choice task because entity choices are presented to\"}"}
{"id": "emnlp-2022-main-751", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the model, but because there are varying number of options in each dialogue, it is formulated as a span extraction that is evaluated based on accuracy.\\n\\nYang and Choi (2019) introduce annotations for question answering. The answers to question-answer pairs can either be a speaker name or exist as a span within the dialogue, and multiple spans may be correct.\\n\\nJiang et al. (2020) present the personality detection task by annotating speakers with five traits: agreeableness, conscientiousness, extraversion, openness, and neuroticism. The goal of the task is to correctly identify whether a given character from a dialogue either has or does not have each of the five traits.\\n\\nDialogRE Yu et al. (2020a) introduce a relation extraction dataset annotated with 36 different relations. Their dataset anonymizes speakers which allows for an entity linking relation called \\\"per:alternative_name\\\". However, our version of the Friends dataset is named and so we remove this relation from our data. This task is similar to the relation extraction from DailyDialog, however the relations in DailyDialog are commonsense relations, and the relations in Friends are focused on information about entities.\\n\\nMELD Poria et al. (2019) provide additional annotations for emotion recognition, with only 22.2% dialogue overlap with Zahiri and Choi (2018)'s dialogues. Additionally, while both use 7 total emotions, Poria et al. (2019) use 2 different emotions from Zahiri and Choi (2018).\\n\\nB Implementation Details\\nFor our experiments, we use the pretrained model implementations from the HuggingFace Transformers library (Wolf et al., 2020), where the bert-base-uncased model has 110M parameters, GPT-2 has 124M parameters, and T5-base has 223M parameters. We use the Adam optimizer (Kingma and Ba, 2015) with a batch size of 60 and run a learning rate sweep across \\\\{3 \\\\times 10^{-6}, 1 \\\\times 10^{-5}, 3 \\\\times 10^{-5}, 1 \\\\times 10^{-4}\\\\} during the pre-training phase, finding that \\\\(3 \\\\times 10^{-5}\\\\) worked well across all models. In all experiments we utilize validation-based best model selection, and train models for 30 epochs on DailyDialog tasks and 20 epochs on Friends tasks.\\n\\nC Expanded Single-Source Results\"}"}
{"id": "emnlp-2022-main-751", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: Aggregate task transfer performance on DailyDialog.\"}"}
{"id": "emnlp-2022-main-751", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Aggregate task transfer performance on Friends.\"}"}
