{"id": "acl-2024-long-835", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CHECK WHY: Causal Fact Verification via Argument Structure\\n\\nJiasheng Si1,4\u2020, Yibo Zhao2,3\u2020, Yingjie Zhu2,3, Haiyang Zhu2,3, Wenpeng Lu1,4, Deyu Zhou2,3*\\n\\n1 Key Laboratory of Computing Power Network and Information Security, Ministry of Education, Shandong Computer Science Center, Qilu University of Technology (Shandong Academy of Sciences), China\\n2 School of Computer Science and Engineering, Southeast University, China\\n3 Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China\\n4 Shandong Provincial Key Laboratory of Computer Networks, Shandong Fundamental Research Center for Computer Science, China\\n\\n{jiashengsi, lwp}@qlu.edu.cn, {yibozhao, yj_zhu, haiyangzhu, d.zhou}@seu.edu.cn\\n\\nAbstract\\n\\nWith the growing complexity of fact verification tasks, the concern with \\\"thoughtful\\\" reasoning capabilities is increasing. However, recent fact verification benchmarks mainly focus on checking a narrow scope of semantic factoids within claims and lack an explicit logical reasoning process. In this paper, we introduce CHECK WHY, a challenging dataset tailored to a novel causal fact verification task: checking the truthfulness of the causal relation within claims through rigorous reasoning steps. CHECK WHY consists of over 19K \\\"why\\\" claim-evidence-argument structure triplets with supports, refutes, and not enough info labels. Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment. Through extensive experiments on state-of-the-art models, we validate the importance of incorporating the argument structure for causal fact verification. Moreover, the automated and human evaluation of argument structure generation reveals the difficulty in producing satisfying argument structure by fine-tuned models or Chain-of-Thought prompted LLMs, leaving considerable room for future improvements.\\n\\n1 Introduction\\n\\nFact verification is a crucial debunking task that entails verifying the truthfulness of claims by cross-referencing them with reliable evidence drawn from established resources (Guo et al., 2022b), which prevents the proliferation of erroneous information online and fosters public trust (Lewandowsky et al., 2020; Glockner et al., 2022). However, with the multi-step reasoning capability in fact verification models remaining uncertain (Schuster et al., 2019; Pan et al., 2023; Zhang et al., 2024), existing research reflects the deficiency in in-depth understanding of the explicit reasoning mechanisms when performing inference on multi-hop evidence. This prompts us to develop a strong benchmark that incorporates the interpretable \\\"thought\\\" process to assess the logical reasoning capabilities of models.\\n\\nCurrently, substantial progress has been made on common fact verification benchmarks, e.g., HOVER (Jiang et al., 2020), FEVEROUS (Aly et al., 2021), and SCITAB (Lu et al., 2023). Nevertheless, existing resources have inherent limitations. Classic datasets primarily focus on verifying the semantic factoids of \\\"who\\\", \\\"what\\\", \\\"when\\\", and \\\"where\\\" within the claim (Rani et al., 2023). For example, verifying the claim \\\"John Lennon was born before the astronaut who drank the first coffee in space.\\\" can be decomposed into verifying factoids such as \\\"who, and when the astronaut drank the first coffee in space.\\\" and \\\"when the man was born.\\\" However, these semantic factoids can be answered individually by straightforward factoids-matching between the claim and distinct independent evidence (Jiang et al., 2020; Pan et al., 2023), e.g., word overlapping or proof matching (Krishna et al., 2022), thereby limiting its significant potential to summarize the correlational evidence with \\\"thought\\\" steps. Heretofore, noticeably absent in prior datasets are \\\"why\\\" claims: containing causal relations that need to be verified. These claims prompt for not simple factoid matching, but an explicit logical reasoning path for verification (Ho et al., 2023).\\n\\nMore specifically, Figure 1 presents a \\\"why\\\" claim featuring a cause-effect pair where \\\"military crises\\\" causes the \\\"decrement of the purity of denarius silver.\\\" Verifying such causal relations is quite challenging, which necessitates complex logical reasoning and context information beyond...\"}"}
{"id": "acl-2024-long-835", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Claim: The military crisis confronting the empire forced Marcus Aurelius to reduce the purity of the denarius silver.\\n\\nLabel: SUPPORTS\\n\\nImplicit Reasoning: By reducing the purity of its silver, the empire could devalue the denarius, thereby increasing the amount of currency available for military spending.\\n\\nSuppressing financial crises requires an appropriate increase in the amount of currency.\\n\\nIn the Roman Empire, the right to mint money was safeguarded by Roman Law.\\n\\nThe empire decided to increase the amount of currency.\\n\\nTrade disruptions and financial shortages often accompany military crises.\\n\\nThe military crises likely demanded more resources, which required additional government spending.\\n\\nA military crisis is likely to lead to a financial crisis.\\n\\nProponent: The military crisis confronted the empire.\\n\\nCAUSE: Marcus Aurelius reduced the purity of the denarius silver.\\n\\nEFFECT: Supports\\n\\nFigure 1: An entry from CHECK WHY: a \\\"why\\\" claim with its corresponding cause and effect, and an argument structure representing the reasoning process from cause to effect. Notably, the cause-effect pair is used solely during the annotation process and not included in the argument structure, implying that it is implicitly inferred from the claim, rather than being provided explicitly.\\n\\nThe factoids within the claim (Jin et al., 2023, 2024; Romanou et al., 2023). For instance, to support this causal relations (i.e., military crises $\\\\rightarrow$ decrease purity of silver), it is essential to incorporate the extra intermediate reasoning steps to bridge the connection between the cause-effect pair, thus forming a logical reasoning path:\\n\\n1. military crises $\\\\rightarrow$ financial crisis\\n2. increase amount of currency $\\\\rightarrow$ decrease purity of silver.\\n\\nThe rationale behind 1 is that military crises often coincide with extra factors such as trade disruption or more government spending, leading to the financial crisis. The reason supporting 2 is that increase amount of currency is a demanding response to suppress the financial crisis. Furthermore, the rights to mint money that is safeguarded by Roman Law ensures the behavior of 3 is established. The above inference reveals a coherent reasoning process, which involves aligning the construction of a \\\"thoughtful\\\" logical structure among correlational evidence with causal relation verification.\\n\\nIn this paper, we introduce CHECK WHY, a challenging dataset built around a novel causal fact verification task: assessing whether the causal relation within the claim is valid by explicit logical structure. This dataset consists of 19,596 claim-evidence-argument structure triplets derived from the WIKIWHY dataset (Ho et al., 2023). The uniqueness of CHECK WHY is that each entry contains a \\\"why\\\" claim with causal relations and an argument structure formed by correlated evidence: the latter is inspired by the theory literature on argument structure (Grimshaw, 1990; Freeman, 2011), which depicts how the different statements fit together as wholes to allegedly lend support to the claim. Moreover, inspired by prior research (Glockner et al., 2021), we assume that the label of a causal relation within the claim depends on the provided argument structures, rather than the semantics itself. Thus, each claim is labeled as supports, refutes, or not enough info based on different argument structures. In addition, to prevent the bias in human cognition, we employ a human-model collaboration annotation approach to generate claims, evidence, and corresponding argument structures. Compared to existing datasets, CHECK WHY covers a variety of topics and argument structures, which may prove valuable for performing causal reasoning across various scenarios.\\n\\nBased on the experiments on four tasks we propose and the human evaluation in our CHECK WHY, our experiments reveal the significance of incorporating the argument structure for causal fact verification. Meanwhile, our experiments in argument structure generation also validate the difficulty in producing satisfying argument structures for causal claims. Our key contributions are summarized as follows: (I) We propose verifying the \\\"why\\\" claims with causal relations through reasoning on argument structure as a novel causal fact verification formulation. (II) We construct CHECK WHY by introducing a human-model collaboration annotation approach, drawing inspiration from the theory research on argument structure. (III) We conduct thorough experiments on state-of-the-art models with four tasks, including fine-tuned models and LLMs, which investigates various settings and points out the potential for improvement.\"}"}
{"id": "acl-2024-long-835", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"logical theory (Thomas, 1973; Toulmin, 2003; Walton et al., 2008; Walton, 2013). Thus, we begin by outlining the standard argumentation structure and then present a concise overview of our argument structure.\\n\\n2.1 Standard Argumentation Structure\\nThe standard argumentation structure (Thomas, 1973) is the scheme for structurally representing argument macrostructure: concerning what are the structural patterns in which the elements that constitute an argument may combine to form the overall argument. It consists of four basic structures.\\n\\n- **Serial Argument** ($P \\\\rightarrow C$): an argument structure has one premise $P$ to give a reason to support the conclusion $C$.\\n\\n- **Convergent Argument** ($P_1 \\\\lor P_2 \\\\rightarrow C$): an argument structure has more than one premise, with each one function separately ($P_1 \\\\lor P_2$) as a reason to support the conclusion $C$.\\n\\n- **Linked Argument** ($P_1 \\\\land P_2 \\\\rightarrow C$): an argument structure has more than one premise, and the premises function together ($P_1 \\\\land P_2$) to give a reason to support the conclusion $C$.\\n\\n- **Divergent Argument** ($P \\\\rightarrow C_1 \\\\lor C_2$): an argument structure has more than one separate conclusion ($C_1 \\\\lor C_2$) that can be supported by the same premise $P$.\\n\\n2.2 Our Argument Structure\\nThe argument structure in CHECKWHY follows a tree-like framework, with the claim serving as the root node and evidence branching out as child nodes. These nodes are connected by directed edges that symbolize logical relations. Specifically, following the standard argumentation structure (Thomas, 1973), we blend various basic structures into a unified argument tree. This is achieved by referring to the semantics of each piece of evidence and the logical relations between different pieces of evidence. In this structure, the serial argument leads from one child node to one parent node, while the divergent argument deduces multiple parent nodes from one child node. Furthermore, due to the challenging to discern the subtle distinction between convergent and linked argument, we merge these structural types into a new combined argument. This argument tree formalizes the reasoning process that begins with foundational evidence and progresses toward claim establishment.\\n\\nA tricky issue here is that the argument structure inherently supports in the claim based on its definition, whereas the refute instances are indispensable for causal fact verification. To tackle this, drawing inspiration from the warrant and rebuttal concepts in Toulmin's structure (Toulmin, 2003), wherein the warrant offers facts or rules to back up the claim, and the rebuttal indicates conditions that negate the claim. We apply diverse argument structures to the same claim to collect both supports and refutes instances. In specific, offering the basic argument structure (i.e., warrant) to acquire support labels, and providing another structure (i.e., rebuttal) that upholds the opposition of causal relation in the claim to obtain refutes labels.\\n\\n3 The CHECKWHY Dataset\\nWe adopt a human-model collaboration annotation approach to construct our CHECKWHY, as shown in Figure 2, which including data preparation (\u00a7 3.1), generation with LLMs (\u00a7 3.2), and manual validation (\u00a7 3.3).\\n\\n3.1 Data Preparation\\nWe utilize the WIKIWHY dataset (Ho et al., 2023), a publicly available cause-and-effect QA resource, as our data source. Each QA pair in this dataset was annotated with effect (question), cause (answer), and associated explanations, making it well-suited for our purposes. From the entire collections of cause-effect pairs in WIKIWHY, we initially filter out instances that contain incomplete syntactic structure in either the cause or the effect, thus remaining cause-effect pairs with valid causal relations. This is realized by justifying the absence of a predicate component in the sentence using StanfordNLP Parser tool (Qi et al., 2020). To ensure quality, we include an option in the subsequent manual validation process to mark a claim as \\\"Discard - not a valid causal relation, incomplete, or redundant\\\". During this stage, we extract 7,403 cause-effect pairs from a total of 9,406 instances within the WIKIWHY dataset.\\n\\n3.2 Generation with LLMs\\nThe rich knowledge and generative capabilities of large language models (LLMs) raise widespread attention on their use in aiding the construction of dataset (Si et al., 2023; Chen et al., 2023). As such, we adopt GPT-4 (Achiam et al., 2023) for our data generation through in-context learning (Ouyang et al., 2022). Notably, for each cause-effect pair,\"}"}
{"id": "acl-2024-long-835", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Step 1\\nData Preparation\\nStep 2 Generation with LLMs\\nStep 3 Manual Validation\\n  Editing\\nStep 4 Manual Validation\\n  Filtering\\n\\nCHECK-WHY-Claim\\nGeneration\\n- SUP Evidence\\nGeneration\\n- Argument Structure\\nGeneration\\n\\nInstructions: ...\\nExamples: ...\\nInput: ...\\n\\nInstructions: ...\\nExamples: ...\\nInput: ...\\n\\nInstructions: ...\\nExamples: ...\\nInput: ...\\n\\nLLMs\\n- SUP Evidence\\n- REF Evidence\\n- NEI Evidence\\n\\nLLMs\\n- Effect\\n- Counterfactual Effect\\n\\nLLMs\\n- Cause\\n- REF Evidence\\n\\nLLMs\\n- SUP Evidence\\n- REF Evidence\\n- Counterfactual Effect\\n\\nCCheck-WHY-Dataset\\nStanfordNLP Parser\\n\\nStep 1 Data Preparation\\nStep 2 Generation with LLMs\\nStep 3 Manual Validation\\n  Editing\\nStep 4 Manual Validation\\n  Filtering\\n\\nCHECK-WHY-Claim\\nGeneration\\n- SUP Evidence\\nGeneration\\n- Argument Structure\\nGeneration\\n\\nInstructions: ...\\nExamples: ...\\nInput: ...\\n\\nInstructions: ...\\nExamples: ...\\nInput: ...\\n\\nInstructions: ...\\nExamples: ...\\nInput: ...\\n\\nLLMs\\n- SUP Evidence\\n- REF Evidence\\n- NEI Evidence\\n\\nLLMs\\n- Effect\\n- Counterfactual Effect\\n\\nLLMs\\n- Cause\\n- REF Evidence\\n\\nLLMs\\n- SUP Evidence\\n- REF Evidence\\n- Counterfactual Effect\\n\\nClaim\\nSUP/REF Evidence\\nAnnotator\\nSUP/REF Argument Structure\\n\\nClaim\\nSUP/REF Evidence\\nAnnotator\\nSUP/REF Argument Structure\\n\\nNEI Evidence\\nClaim\\n&\\n\\nWIKI-WHY-Dataset\\nStanfordNLP Parser\\n\\nC\\nHECK\\nW\\nHY\\n\\nFigure 2: The human-model collaboration annotation process of C\\nHECK\\nW\\nHY, which contains three steps: (I) data preparation; (II) generation with LLMs (including the generation of claim, evidence, and argument structure); (III) manual validation.\\n\\nOur C\\nHECK\\nW\\nHY applies three argument structures to each claim to acquire the SUP, REF, and NEI instances with supports, refutes, and not enough info labels, which ensures a dataset with balanced distribution of veracity labels. All the prompts used in our annotation process are presented in Appendix E.\\n\\n\u2022 Support Evidence Generation\\nTo obtain valid SUP instances, textual evidence that support the claim is generated via GPT-4. In specific, by taking the cause-effect pair and associated explanation within WIKI-WHY as input, the GPT-4 is prompted with the designed ReAct-like (Yao et al., 2023) examples, enabling LLMs to logically deduce the effect from the cause through a step-by-step thought process. Finally, the reasoning process generated by LLMs is considered as support evidence.\\n\\n\u2022 Refute Evidence Generation\\nDue to the significantly broader generation scope compared to the SUP instances, building REF instances is a challenging task (Zhu et al., 2023; Tan et al., 2023). To produce valid evidence that refutes the claim, following Zhu et al. (2023), we apply the generation of evidence that refutes the counterfactual effect as an intermediate step. In specific, starting with the original cause, we prompt GPT-4 to generate the counterfactual effect by crafting an effect with an opposite label. Then, we prompt GPT-4 with ReAct-like examples to reason from cause to the counterfactual effect, resulting in the generation of evidence that refutes the original effect.\\n\\n\u2022 NEI Evidence Generation\\nFollowing Aly et al. (2021), we incorporate instances labeled as not enough info into our dataset. To this end, we either randomly eliminate evidence from the corresponding SUP/REF instances or merge evidence from these two types to generate NEI evidence.\\n\\n\u2022 Argument Structure Generation\\nTo describe the logical reasoning process for each instance, we require GPT-4 to generate succinct argument structures for both the SUP and REF instances. Inspired by Wang et al. (2023a), we prompt GPT-4 with Python-style code examples that have strict format restrictions. These examples help establish a logical reasoning path for each instance, thus deepening the structural understanding of arguments.\\n\\nIn specific, by referring to the argument structure outlined in \u00a7 2.2, we introduce the FACT class, which symbolizes an evidence node within the argument structure, and three distinct functions.\"}"}
{"id": "acl-2024-long-835", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The one_to_one_linking function (corresponding to series argument) represents the deduction from one piece of evidence to another. (II) The one_to_multiple_linking function (corresponding to divergent argument) illustrates the deduction from one piece of evidence to multiple pieces. (III) The multiple_to_one_linking function (corresponding to combined argument) represents the reasoning from several pieces of evidence to support a single one.\\n\\n\u2022 **Claim Generation**\\n\\nBy taking the cause-effect pairs extracted from WIKI as input, the GPT-4 is prompted with crafted examples to generate diverse causal claims linguistically without accessing external information.\\n\\nNotably, to better simulate the real-world fact verification scenarios and boost the distinguishing capability of models to the misleading evidence, we employ BM25 (Robertson and Zaragoza, 2009) to retrieve and extract 300 pieces of evidence from the entire dataset that show a higher degree of lexical overlap with the claim. Then, we use BLEURT (Sellam et al., 2020) to select 8-12 pieces of evidence with close semantic similarity to serve as distractor evidence for SUP, REF, and NEI.\\n\\n3.3 Manual Validation\\n\\nThe uncontrollable generation of LLMs might result in unfaithful or invalid instances. Thus, we subsequently incorporate the human validation procedure for two purposes: (I) to revise and edit the content of generated instances by LLMs; (II) to critically review and filter out the low-quality instances. The details are shown in Appendix C.\\n\\n**Manual Editing**\\n\\nWe ask a group of annotators with NLP backgrounds to edit and refine the generated instances.\\n\\n\u2022 **Claim and Evidence**\\n\\nAnnotators are required to review the claims and assess whether they accurately contain all details from the provided cause-effect pair and express clear causality. Then, annotators are instructed to refine the generated evidence by evaluating whether all evidence aligns with the label and is presented in concise language (e.g., without repetition or redundancy).\\n\\n\u2022 **Argument Structure**\\n\\nThe annotators are required to review and correct errors within argument structures by double-checking the validity of each reasoning step and removing unnecessary non-code content, such as comments. In this context, we encourage annotators to apply their judgment and expertise throughout the process.\\n\\n**Manual Filtering**\\n\\nAnother group of annotators is required to filter out the low-quality edited instances to further improve the quality of the dataset. Given the instructions and demonstration of the annotated argument structures, annotators need to decide whether to retain or discard instances based on the following three criteria. Finally, 6,757 and 6,638 instances pass the filter for SUP and REF, respectively.\\n\\n\u2022 **Faithfulness:** The truthfulness of the claim can be verified by the evidence and is consistent with the given label.\\n\\n\u2022 **Fluency:** The claim and associated evidence are written fluently and without grammatical errors.\\n\\n\u2022 **Validity:** The argument structure is logically coherent and correctly depicts the reasoning process from cause to effect.\\n\\n**Quality Control**\\n\\nWe apply strict quality control measures by following the principles outlined in the Dataset Statement (Bender and Friedman, 2018), ensuring high-quality annotation. Specifically, for each instance, two annotators are required to perform the manual editing and filtering, and resolve any disagreements identified by a third annotator. We employ majority voting to determine the final retained instances, reaching an inter-rater agreement at Fleiss\u2019s $\\\\kappa = 0.75$ (Fleiss et al., 1981). The details are listed in Appendix C.\\n\\n4 Dataset Analysis\\n\\n4.1 Dataset Statistics\\n\\nTable 1 presents the statistics of our CHECK dataset. This dataset comprises 6,532 sets of instances, with each set including a SUP instance, a REF instance, and an NEI instance, resulting in 19,596 instances in total. In addition, due to the lack of argument structure for NEI\u2014the SUP and REF instances contain three elements {claim, evidence, argument structure}, while the NEI instance is limited to the claim and evidence\u2014we build a CHECK2 dataset by excluding NEI instances to assess the reasoning ability through argument structures within SUP/REF instances. Compared to WIKI, our CHECK dataset includes more extended and detailed reasoning steps, with an average of 13.7 pieces of evidence and 4.08 reasoning steps. The histograms of evidence length and reasoning step counts are detailed in Appendix B.\"}"}
{"id": "acl-2024-long-835", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Summary statistics of CHECKWHY. Here, CHECKWHY is the dataset containing \\\\{SUP, REF, NEI\\\\} instances, while CHECKWHY2 is composed of \\\\{SUP, REF\\\\} instances. Avg. # denotes the number on average, and the Evidence denotes the combination of the valid annotated evidence and the extracted distractor evidence.\\n\\n### 4.2 Argument Structure Analysis\\n\\nInspired by Dalvi et al. (2021), to understand the intricacies of reasoning present within CHECKWHY, we analyzed the reasoning steps extracted from 50 randomly selected instances with diverse argument structures. We identified 5 prevalent high-level categories of reasoning, as shown in Table 2.\\n\\n- **Event causality** (36%) refers to the relations where one event directly leads to another event.\\n- **Inference from Properties** (22%) involves concluding by referring to the properties present within one input.\\n- **Rule-based Inference** (17%) is about deducing new information or making decisions grounded in a set of predefined rules or conditions.\\n- **Inductive reasoning** (14%) requires a model to make generalized conclusions based on specific observations or evidence, deriving general principles or patterns from particular examples or observations. Moreover, with 10% of instances require applying sequential reasoning. As a whole, this analysis reveals diverse forms of reasoning steps that are essential for inferring the argument structure in CHECKWHY.\\n\\nIn addition, we summarize 5 types of prevalent complex argument structures, as shown in Figure 5, which also shows the complexity of our argument structure through a macro perspective.\\n\\n### 5 Experiment\\n\\n#### Task Notation\\n\\nThe instance within CHECKWHY is denoted by the quadruple $(C,E/LE,S,Y)$, where $C$ denotes the claim, $E = \\\\{e_1,e_2,...,e_n\\\\}$ denotes the evidence, $LE$ refers to the initial leaf evidence in an argument structure. Notably, we ensure that $LE$ includes the valid leaf evidence and all the distractor evidence, which requires the denoising capability of models. In addition, $S$ denotes the argument structure, and $Y_3 \\\\in \\\\{REF,SUP,NEI\\\\}$ or $Y_2 \\\\in \\\\{REF,SUP\\\\}$ for CHECKWHY2. Based on our CHECKWHY, we define four tasks of increasing difficulty, with the aim of (i) predicting whether the evidence supports or refutes the claim, or presents not enough information; (ii) generating valid argument structures outlining the reasoning process for causal fact verification. The following describes four tasks.\\n\\n**Task 1:** Input = $(C,LE)$, Output = $Y_2 | Y_3$. We follow the traditional fact verification formulation to evaluate the inference capability of models on the CHECKWHY dataset, i.e., verifying the claim based on foundational leaf evidence. In this vein, we conduct the experiments on three types of baselines: (I) Discriminative models: BERT (Devlin et al., 2019), Transformer-XH (Zhao et al., 2020), and UniXcoder (Guo et al., 2022a), (II) Generative models: CodeT5 (Wang et al., 2021) and CodeT5+ (Wang et al., 2023b), (III) LLMs: ChatGPT and GPT-4 (Achiam et al., 2023) with Chain-of-Thought prompts (Wei et al., 2022).\\n\\n**Task 2:** Input = $(C,LE,S)$, Output = $Y_2$. Task 2 is designed to assess whether models have a better understanding when incorporating the structural information within the argument structure during the verification. In this vein, we adopt the same set of baselines as in Task 1.\\n\\n**Task 3:** Input = $(C,E)$, Output = $(Y_2,S)$. To investigate the logical reasoning ability of existing models when performing verification, we experiment that involves the simultaneous verification of the claim and the generation of the argument structure. Here, we conduct experiments on CodeT5, CodeT5+, GPT-4 and ChatGPT with Chain-of-Thought prompts.\\n\\n**Task 4:** Input = $(C,LE)$, Output = $(Y_2,S)$. Task 4 investigates whether the model can independently construct an entire argument structure from bottom to top based on leaf evidence and its reasoning ability, which is the most challenging experimental setup in this paper. The model must generate the verification label and a complete argument structure based on the input claim and leaf evidence nodes. We use the same baseline models as in Task 3 for this task.\\n\\n3 https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\"}"}
{"id": "acl-2024-long-835", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inference Type Prop. Example\\n\\nEvent causality 36%\\n\\nE1 Heavy rain and flooding have caused significant damage to the area.\\nE2 Road closures and traffic disruptions are affecting the entire city.\\nE1 \u2192 E2\\n\\nInference from Properties 22%\\n\\nE1 If the patient has a fever and cough, diagnose them with a respiratory infection.\\nE2 David has a fever and cough.\\nE3 he is likely suffering from a respiratory infection.\\nE1, E2 \u2192 E3\\n\\nRule-based Inference 17%\\n\\nE1 All humans are mortal.\\nE2 Socrates is a human.\\nE3 Therefore, Socrates is mortal.\\nE1, E2 \u2192 E3\\n\\nInductive reasoning 14%\\n\\nE1 Every morning for the past week, you've woken up to find the grass wet.\\nE2 Based on these observations, you hypothesize that it rains during the night.\\nE1 \u2192 E2\\n\\nSequential reasoning 10%\\n\\nE1 A heavy rainstorm has occurred in the capital of Italy.\\nE2 The capital of Italy is Rome.\\nE3 Rome has experienced a heavy rainstorm.\\nE1, E2 \u2192 E3\\n\\nTable 2: The prevalence of 5 reasoning steps required for reasoning on argument structure, sampled from 50 random instances in the training set. Here, En denotes input evidence, and the \\\"\u2192\\\" symbolizes the \\\"reasoning\\\".\\n\\n| Model                  | Task1 (Y3) | Task1 (Y2) | Task2       | Acc. F1 | Acc. F1 | Acc. F1 |\\n|------------------------|------------|------------|-------------|---------|---------|---------|\\n| Discriminative Models  |            |            |             |         |         |         |\\n| BERT (FT)              | 73.3       | 72.9       | 76.9        |         |         | 88.0    |\\n| Transformer-XH (FT)    | 63.2       | 62.2       | 78.9        |         |         | 90.4    |\\n| UniXcoder (FT)         | 70.1       | 69.5       | 77.2        |         |         | 83.4    |\\n| Generative Models      |            |            |             |         |         |         |\\n| CodeT5 (FT)            | 72.3       | 72.0       | 71.6        |         |         | 78.0    |\\n| CodeT5+ (FT)           | 72.8       | 72.5       | 65.6        |         |         | 79.3    |\\n| Large Language Models  |            |            |             |         |         |         |\\n| ChatGPT (CoT)          | 37.8       | 31.1       | 60.6        |         |         | 60.7    |\\n| GPT-4 (CoT)            | 49.6       | 41.0       | 62.5        |         |         | 71.0    |\\n\\nTable 3: The performance of baselines on Task 1 and Task 2, where FT denotes that the model is fine-tuned on CHECKW HY training set. The CoT denotes the Chain-of-Thought prompts with few examples. The best results are marked in bold and the results with further improvement after the incorporation of argument structures are underlined.\\n\\n5.1 Evaluation Metrics\\n\\nAutomatic Evaluation Metrics\\n\\nDue to the uniqueness of argument structure within CHECKW HY, traditional evaluation metrics in text generation may not be suitable in our setting. Thus, we propose two new evaluation metrics focusing on argument structure generation.\\n\\n- **Structure Similarity**\\n  Inspired by Saha et al. (2021), we introduce an automated evaluation metric named Structure Similarity. The metric treats the argument structure as a collection of edges, with each edge considered as a sentence, and a matching method is employed to determine the optimal alignment between the edges in the predicted structure and those in the gold argument structure. In our experiments, the BERTScore based on DeBERTa-large (He et al., 2021) is used as the scoring function to measure how closely the predicted edges match the gold edges. The detailed algorithm can be found in Saha et al. (2021).\\n\\n- **Exact Match Similarity**\\n  Similar to structure similarity, exact match similarity employs a stricter matching strategy. Here, structures are also regarded as sequences of edges, but a match is considered successful only when the predicted edge exactly matches the gold edge.\\n\\nHuman Evaluation Criteria\\n\\nTo improve the reliability of the evaluation on argument structure, we introduce extra human evaluations beyond the automated evaluation metrics. Empirically, we randomly select 50 instances and ask three graduate students with NLP background to assess the generated argument structure according to the following criteria with binary score.\\n\\n- **Validity**: Is the generated argument structure logically coherent?\\n- **Win/Tie/Lose**: Comparing the generated argument structure against the provided reference. Mark Win if you prefer the generated structure, Tie if you have no preference, and Lose if you prefer the reference structure.\\n\\n5.2 Results and Discussion\\n\\nMain Results\\n\\nThe overall results of baselines on our CHECKWHY dataset are reported in Table 3.\"}"}
{"id": "acl-2024-long-835", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task | Generative Models | Large Language Models |\\n|------|-------------------|-----------------------|\\n|      | CodeT5 (FT)       | ChatGPT (CoT)         |\\n|      | CodeT5+ (FT)      | GPT-4 (CoT)           |\\n| Task 3 | 87.4 48.4 10.0 27.5 62.5 52.5 83.7 83.5 | 69.5 26.4 0.0 17.5 82.5 37.5 63.4 58.5 |\\n|       | 88.8 53.4 7.5 40.0 52.5 62.5 86.6 86.6 | 77.7 34.2 21.6 40.5 37.8 62.2 72.5 70.9 |\\n| Task 4 | 72.8 1.1 5.5 30.3 64.2 55.2 67.9 68.9 | 60.9 1.8 4.2 28.5 67.3 52.9 53.9 44.7 |\\n|       | 75.7 2.5 6.1 33.9 60.0 59.5 72.6 72.6 | 66.7 1.4 6.2 33.2 60.6 57.8 57.0 47.8 |\\n\\nTable 4: The performance of baselines on Task 3 and Task 4, where SS denotes structure similarity and ES denotes exact match similarity. The best results are marked in bold.\\n\\nIn general, for Task 1 and Task 2, generative models show notably poorer performance compared to the discriminative models, and LLMs encounter substantial challenges in causal verification. Despite the powerful capability of GPT-4, which shows an improvement over ChatGPT for verification, it still lags behind the top-performing fine-tuned models. In addition, through the comparison of $Y_3$ and $Y_2$ on Task 1, we observe the consistent improvement for both discriminative models and LLMs, which indicates the disruptive impact brought by the ambiguous evidence within NEI instance, particularly on LLMs. An interesting observation here is that there is a slight decrease in the performance of generative models, which may be attributed to the disparities between the datasets utilized during the pretraining and fine-tuning phases. For Task 3 and Task 4, compared to the generative models, a similar phenomenon observed is that LLMs struggle to achieve satisfactory results in the generation of argument structure and show notably poorer performance on causal verification. This may be due to the conflict raised by the internal world knowledge and the evidence we generated, e.g., there is evidence that contradicts the real-world situation. In addition, the difference can be observed in the human evaluation, where LLMs present a slightly higher performance compared to the generative models. This may be attributed to the fact that LLMs generate more fluency and human-readable text with the powerful generative capability. Overall, the results of four tasks validate the difficulty when facing the setting of our CHECKWITY.\\n\\nThe Effectiveness of Argument Structures\\n\\nArgument structures prove to be particularly advantageous, especially in specific experimental setups. In Task 1, where structures are absent, models such as the CodeT5 series and LLMs, relying solely on textual information, may struggle to achieve high accuracy. Furthermore, when comparing Task 1 and Task 2, a consistent improvement is noted across all baselines if structures are directly provided. For example, GPT-4 achieves approximately a 10% increase in accuracy in Task 2 compared to Task 1. This effect shows the most prominent performance on Transformer-XH, a GNN method conducting reasoning over the evidence graph. This underscores the significance of structural information within the argument structure for causal fact verification.\\n\\nThe Construction of Argument Structures\\n\\nIn Task 3 and Task 4, we investigate whether models can generate argument structures. Structure similarity may be significantly higher than exact match similarity because structure similarity considers the semantic meaning within nodes, whereas exact match similarity only checks whether characters are identical. CodeT5+ exhibits particularly noticeable performance, achieving the highest exact match similarity and structure similarity, as well as the highest F1 score and accuracy. Task 4 investigates whether models can construct an entire structure and complete inference given only the leaf nodes. All models perform poorly on Task 4, which may be attributed to the vast generation space in creating an entire structure. This indicates that when...\"}"}
{"id": "acl-2024-long-835", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"all evidence nodes are provided, the model only\\nneeds to connect these nodes, instead of inferring\\nthe complete reasoning path. However, when only\\nleaf nodes are provided, the task becomes much\\nmore challenging.\\n\\nHuman Evaluation\\nOur human evaluation ex-\\ndetailed in Table 4, reveal that there is\\nsignificant room for improvement across different\\naspects. None of the baseline models, including\\nCodeT5, CodeT5+, and GPT-4, exhibit notably\\nsuperior performance. Specifically, our strongest\\nbaseline, CodeT5+, generates valid argument struc-\\ntures only 62.5% of the time in Task 3, whereas in\\nTask 4 it generates up to 60.0% of argument struc-\\ntures that are worse than the gold references. These\\nresults from the strongest LLMs leave ample room\\nfor improvement and serve as motivation for future\\nwork on these tasks.\\n\\n6 Related work\\n6.1 Fact Verification Datasets\\nThe fact verification task has recently garnered\\nsignificant attention within the NLP community.\\nResearchers prompt the development of fact veri-\\nification by introducing various resources. Well-\\nownen datasets such as FEVER (Thorne et al.,\\n2018), PolitiFact (Garg and Sharma, 2020), and\\nCREAK (Onoe et al., 2021) regard the fact verifi-\\ncation task as a form of natural language inference,\\naiming to predict whether the evidence supports or\\ncontradicts a claim. Subsequently, some datasets\\nhave been proposed to address fact-checking for\\ncomplex claims necessitating multi-step reasoning\\n(Aly et al., 2021; Jiang et al., 2020), which typ-\\nically present multiple pieces of evidence linked\\nto the claim through entity matching. Addition-\\nally, to improve the interpretability of the verac-\\nity predictions, Alhindi et al. (2018) extends the\\nLIAR dataset by incorporating summaries from\\nfact-checking articles. Furthermore, Kotonya and\\nToni (2020) introduces the first dataset explicitly\\ncontaining gold explanations, composed of fact-\\nchecking articles and other news items. Rani et al.\\n(2023) leverages interrogative questions to enhance\\ninterpretability by exploring the semantics factoids\\nwithin the claim\\n\\nDespite the advancement, existing datasets are\\nprimarily designed for verifying the semantic fac-\\ntoids with limited types within claims. Our work\\nfor the first time focuses on \\\"why\\\" claims: verify-\\ning the causal relation within the claim rather than\\nbasic semantic factoids. In addition, inspired by the\\nliterature on logical theory, we construct argument\\nstructures to explicitly represent the reasoning pro-\\ncess, enhancing the causal reasoning ability and\\ninterpretability of the model.\\n\\n6.2 Causal Reasoning\\nWith the increasing attention on causality (Willig\\net al., 2023; Zhang et al., 2023), several formula-\\ntions of causality-related skills for NLP are pro-\\nposed, which can be summarized into (1) causality\\nas knowledge (Sap et al., 2019), encompasses the\\nrepresentation, understanding, and utilization of\\ncausal relationships embedded within textual data,\\n(2) causality as language comprehension (Stede,\\n2008; Cao et al., 2022; Yu et al., 2019), originates\\nfrom traditional linguistic studies on causal connec-\\ntives and the usage of causal language, extending\\nto more recent efforts in causal relation extraction,\\nand (3) causality as reasoning (Jin et al., 2023),\\ninvolves identifying factors leading to outcomes\\nand understanding underlying mechanisms. Our\\nwork is the first attempt to introduce causality into\\nthe fact verification task inspired by the causal rea-\\nsoning in argumentation (Habernal et al., 2018;\\nHeinisch et al., 2022), in which causal knowledge,\\ncausal language comprehension, and causal rea-\\nsoning are significant. Exploring whether models\\ncan grasp the causal relationship between evidence\\nand claim in the verification process will further\\npromote the development of the task.\\n\\n7 Conclusion\\nIn this paper, we introduce CHECK-WHY, a chal-\\nlenging dataset and benchmark built around a novel\\ncausal fact verification task, annotated by a human-\\nmodel collaborative approach. By drawing inspi-\\nration from the logical theory, we incorporate the\\nargument structure to represent the explicit logi-\\ncal reasoning process when assessing the causal\\nrelation within the \\\"why\\\" claim, which may prove\\nvaluable for developing multi-step reasoning skills\\nacross various scenarios. Extensive experiments on\\nvarious state-of-the-art models validate the impor-\\ntance of incorporating argument structures and the\\ndifficulty of generating them. With the baselines\\nachieving limited results, we believe that CHECK-\\nWHY is a challenging yet attractive benchmark for\\nthe development of fact verification systems.\"}"}
{"id": "acl-2024-long-835", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\n\u2022 Within the CHECK WHY framework, the label assigned to each claim is based on distinct argument structures. Therefore, the assigned label may not always correspond to real-world circumstances.\\n\\n\u2022 The CHECK WHY dataset is created by LLMs, which could face difficulties in retrieving evidence in open-domain scenarios compared to previous datasets. This includes selecting relevant evidence in a broad and unrestricted information space.\\n\\n\u2022 The initial evidence in the argument structure, as provided by LLMs, may not meet everyone's expectations or align with their understanding. This could affect the universal acceptability and perceived validity of the evidence.\\n\\nAcknowledgement\\n\\nThe authors would like to thank the anonymous reviewers for their insightful comments. This work is funded by the National Natural Science Foundation of China (Grant No.62176053, No.62376130), Shandong Provincial Natural Science Foundation (Grant No.ZR2022MF243), Program of New Twenty Policies for Universities of Jinan (Grant No.202333008), and supported by the Big Data Computing Center of Southeast University.\\n\\nReferences\\n\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nTariq Alhindi, Savvas Petridis, and Smaranda Muresan. 2018. Where is your evidence: Improving fact-checking by justification modeling. In Proceedings of the First Workshop on Fact Extraction and VERifi cation (FEVER), pages 85\u201390.\\n\\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. The fact extraction and VERification over unstructured and structured information (FEVEROUS) shared task. In Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER).\\n\\nEmily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604.\\n\\nAngela Cao, Gregor Williamson, and Jinho D. Choi. 2022. A cognitive approach to annotating causal constructions in a cross-genre corpus. In Proceedings of the Linguistic Annotation Workshop (LAW-XVI), pages 151\u2013159.\\n\\nZeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. 2023. DISCO: Distilling counterfactuals with large language models. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 5514\u20135528.\\n\\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining answers with entailment trees. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 7358\u20137370.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, pages 4171\u20134186.\\n\\nJoseph L Fleiss, Bruce Levin, Myunghee Cho Paik, et al. 1981. The measurement of interrater agreement. Statistical methods for rates and proportions, 2(212\u2013236):22\u201323.\\n\\nJames B Freeman. 2011. Argument Structure:: Representation and Theory. Springer Science & Business Media.\\n\\nSonal Garg and Dilip Kumar Sharma. 2020. New politifact: A dataset for counterfeit news. In International Conference System Modeling and Advancement in Research Trends (SMART), pages 17\u201322.\\n\\nMax Glockner, Yufang Hou, and Iryna Gurevych. 2022. Missing counter-evidence renders NLP fact-checking unrealistic for misinformation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 5916\u20135936.\\n\\nMax Glockner, Ieva Staliunaite, James Thorne, Gisela Vallejo, Andreas Vlachos, and Iryna Gurevych. 2021. Ambifc: Fact-checking ambiguous claims with evidence. Transactions of the Association for Computational Linguistics, 12:1\u201318.\\n\\nJane Grimshaw. 1990. Argument structure. the MIT Press.\\n\\nDaya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. 2022a. UniXcoder: Unified cross-modal pre-training for code representation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 7212\u20137225.\\n\\nZhijiang Guo, Michael Sejr Schlichtkrull, and Andreas Vlachos. 2022b. A survey on automated fact-checking. Trans. Assoc. Comput. Linguistics, pages 178\u2013206.\"}"}
{"id": "acl-2024-long-835", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-835", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atomic: an atlas of machine commonsense for if-then reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence Conference, pages 3027\u20133035. Tal Schuster, Darsh J. Shah, Yun Jie Serene Yeo, Daniel Filizzola, Enrico Santus, and Regina Barzilay. 2019.\\n\\nTowards debiasing fact verification models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 3417\u20133423. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\\n\\nBLEURT: Learning robust metrics for text generation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892. Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng, Hal Daum\u00e9 III, and Jordan Boyd-Graber. 2023.\\n\\nLarge language models help humans verify truthfulness\u2014except when they are convincingly wrong. arXiv preprint arXiv:2310.12558. Manfred Stede. 2008.\\n\\nConnective-based local coherence analysis: A lexicon for recognizing causal relationships. In Semantics in Text Processing. STEP Conference Proceedings, pages 221\u2013237. Neset Tan, Trung Nguyen, Josh Bensemann, Alex Peng, Qiming Bao, Yang Chen, Mark Gahegan, and Michael Witbrock. 2023.\\n\\nMulti2Claim: Generating scientific claims from multi-choice questions for scientific fact-checking. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics, pages 2652\u20132664. Stephen Naylor Thomas. 1973.\\n\\nPractical Reasoning in Natural Language. Prentice-Hall. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018.\\n\\nFEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, pages 809\u2013819. Stephen E Toulmin. 2003.\\n\\nThe uses of argument. Cambridge University Press. Douglas Walton. 2013.\\n\\nArgumentation schemes for presumptive reasoning. Routledge. Douglas Walton, Christopher Reed, and Fabrizio Macagno. 2008.\\n\\nArgumentation schemes. Cambridge University Press. Xingyao Wang, Sha Li, and Heng Ji. 2023a.\\n\\nCode4Struct: Code generation for few-shot event structure prediction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 3640\u20133663. Yue Wang, Hung Le, Akhilesh Gotmare, Nghi Bui, Junnan Li, and Steven Hoi. 2023b.\\n\\nCodeT5+: Open code large language models for code understanding and generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1069\u20131088. Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. 2021.\\n\\nCodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 8696\u20138708. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.\\n\\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837. Moritz Willig, Matej Zecevi\u0107, Devendra Singh Dhami, and Kristian Kersting. 2023.\\n\\nProbing for correlations of causal facts: Large language models and causality. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023.\\n\\nReact: Synergizing reasoning and acting in language models. In Proceedings of the International Conference on Learning Representations. Bei Yu, Yingya Li, and Jun Wang. 2019.\\n\\nDetecting causal language use in science findings. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 4664\u20134674. Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jennings, Chao Ma, Tom Minka, Nick Pawlowski, et al. 2023.\\n\\nUnderstanding causality with large language models: Feasibility and opportunities. arXiv preprint arXiv:2304.05524. Congzhi Zhang, Linhai Zhang, and Deyu Zhou. 2024.\\n\\nCausal walk: Debiasing multi-hop fact verification with front-door adjustment. In Proceedings of the AAAI Conference on Artificial Intelligence. Chen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul N. Bennett, and Saurabh Tiwary. 2020.\\n\\nTransformer-xh: Multi-evidence reasoning with extra-hop attention. In Proceedings of the International Conference on Learning Representations. Yingjie Zhu, Jiasheng Si, Yibo Zhao, Haiyang Zhu, Deyu Zhou, and Yulan He. 2023.\"}"}
{"id": "acl-2024-long-835", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Experiment Details\\n\\nIn the fine-tuning process for Discriminative Models, we choose BERT-base-uncased, Transformer-XH (using BERT-base-uncased as the backbone), and UniXcoder-base. The batch size is set to 8, and we utilize the AdamW optimizer with a learning rate of 5e-6.\\n\\nWe select CodeT5-base and CodeT5+ (0.7B) for fine-tuning generation models, with a batch size of 4 and a learning rate of 1e-5. Additionally, we specify a maximum input length of 512 and a maximum generation length of 400.\\n\\nRegarding Large Language Models (LLMs), our choices are gpt-4-1106-preview and ChatGPT-turbo, with a temperature setting of 0.1.\\n\\nB Ablation Experiment\\n\\nNumber of Irrelevant Evidence\\n\\nWe perform experiments on Task 4 with different average amounts of irrelevant evidence. The findings are summarized in Table 5, suggesting that task difficulty increases slightly as the number of irrelevant evidence increases.\\n\\nNumber of Reasoning Steps\\n\\nWe group the results of Task 4 according to the number of reasoning steps in the gold argument structures. The outcomes are outlined in Table 6, demonstrating a significant decrease in scores as the number of steps increases.\\n\\nFurther Evaluation\\n\\nWe refer to the evaluation metrics from ENTAILMENT BANK and make slight modifications to further evaluate the argument structures generated in Task 3 and Task 4. The specific results are shown in the Table 8.\\n\\n- Leaf Nodes (F1, AllCorrect): Does the predicted argument structure use the correct leaf evidence? We compute an F1 score by comparing predicted leaf evidence to golden leaf evidence. The \u201cAllCorrect\u201d score is 1 if all nodes are identified correctly (F1=1.0), 0 otherwise.\\n\\n- Steps (F1, AllCorrect): Are the individual reasoning steps structurally correct? As each intermediate node represents (the conclusion of) a single step, the step is considered structurally correct (score 1) if it perfectly matches the gold, 0 otherwise. We then measure F1 comparing all steps in the two trees. Then AllCorrect=1 if F1=1.0, 0 otherwise.\\n\\n- Intermediates (F1, AllCorrect): Are the intermediate conclusions correct? For comparing gold and generated conclusions. F1 is computed using the number of aligned, correct intermediates wrt. the number of gold/predicted intermediates. AllCorrect=1 if F1=1, otherwise 0.\\n\\nBased on the experimental results, the performance of the fine-tuned model significantly surpasses that of large language models (LLMs). From the Leaves metric, it is evident that the model can identify most of the evidence related to verification. However, the last two metrics indicate that the model encounters difficulties in linking these pieces of evidence to form an argumentative structure.\\n\\nPipeline Setup\\n\\nWe also design an experiment in pipeline mode, using both CodeT5 and CodeT5+ as baseline models. In the first stage, models output argument structures, and in the second stage, they produce the final classification results. The experimental results shown in Table 7 demonstrate that CodeT5+ generates better argument structures, and achieves better classification performance.\\n\\nFigure 3: Histogram of evidence number in the\\nCHECK WHY\\nFigure 4: Histogram of argument steps in the\\nCHECK WHY\"}"}
{"id": "acl-2024-long-835", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: The performance of CodeT5 on Task 4 involving different numbers of irrelevant evidence.\\n\\n| Number of Steps | Number of instances | Acc  | F1   | SS   | ES  |\\n|-----------------|---------------------|------|------|------|-----|\\n| 1               | 40                  | 80   | 80   | 78.6 | 8.6 |\\n| 2               | 273                 | 74.3 | 72   | 77.4 | 2.6 |\\n| 3               | 734                 | 68.9 | 66.4 | 73.3 | 0.7 |\\n| 4               | 464                 | 65.7 | 65.4 | 70.4 | 0.1 |\\n| 5               | 77                  | 64.9 | 59.6 | 64.6 | 0.3 |\\n| \u22656              | 12                  | 58.3 | 49.6 | 62.7 | 0 |\\n\\nTable 6: Results on Task 4 with the varying number of argument steps in the gold structure.\\n\\n| Number of Steps | Number of instances | Acc  | F1   | SS   | ES  |\\n|-----------------|---------------------|------|------|------|-----|\\n| 1               | 40                  | 80   | 80   | 78.6 | 8.6 |\\n| 2               | 273                 | 74.3 | 72   | 77.4 | 2.6 |\\n| 3               | 734                 | 68.9 | 66.4 | 73.3 | 0.7 |\\n| 4               | 464                 | 65.7 | 65.4 | 70.4 | 0.1 |\\n| 5               | 77                  | 64.9 | 59.6 | 64.6 | 0.3 |\\n| \u22656              | 12                  | 58.3 | 49.6 | 62.7 | 0 |\\n\\nC Detailed Description of the Annotation Process\\n\\nSource of Annotator\\n\\nTo ensure the quality of our complex annotation process, we do not adopt CrowdSource platforms, even though more CrowdWorkers can be employed. We hire 20 university students with experience in NLP, especially those majoring in logical reasoning or argument mining. Before the formal annotation, we draft detailed guidelines, set up the annotation platform, and conduct three rounds of thorough training and one round of testing. In this way, 13 students pass the exam and are retained. Furthermore, we split these students into two groups, where 1 PhD student and 8 postgraduate students perform the Editing, and 1 PhD student and 3 postgraduate students perform the Filtering. Moreover, we apply a strict quality control procedure during our annotation process. In addition, authors conduct casual inspections during the annotation process and rate each annotator, with annotators who receive low scores undergoing additional training.\\n\\nAnnotation Cost\\n\\nThe annotating time for each sample ranges from 8 to 30 minutes, averaging 15 minutes per sample. After annotation, we follow local labor laws, and each annotator is paid $3.05 per hour.\\n\\nDue to the complex argument structure within each instance, the analysis of argument structure type and evaluation of prediction is quite time-consuming and labor-sensitive, especially facing complex structures. The average time for analyzing argument structure type is more than 20 minutes per instance, and the average time of evaluation on prediction ranges from 15 minutes to 20 minutes per instance, depending on different baseline models.\\n\\nQuality Control\\n\\nWe implemented several quality control measures to minimize the possibility of annotators cutting corners and to ensure the quality of the data:\\n\\n- **Editing**\\n  Given each instance, two annotators are required to perform manual editing of the Claim and Evidence and to resolve any errors identified by the third annotator. The final result is chosen by voting from all three annotators. Due to the complexity of the argument structure and its highly time-consuming nature, we don't apply this procedure to the editing of the argument structure.\\n\\n- **Filtering**\\n  We apply a strict quality control procedure by filtering each instance with three anonymous annotators, wherein two annotators are required to filter the instance, and the third annotator (the PhD student) serves as the supervisor. The decision of each instance is made by all three annotators. Moreover, the data transfer is completed by our annotation platform. According to our statistics, the ratio of \u201call-pass\u201d is acceptable. A display of an example modified by humans is shown in Table 10.\\n\\n- **Automatic Check**\\n  We have implemented a grammar check mechanism (e.g. the argument graph must be a connected graph, etc.) to verify the legality of the annotated argument structures. Before annotators can submit their results, they must pass this check.\\n\\n- **Human Check**\\n  The authors conduct a random casual inspection of the edited data and rate each annotator. Moreover, since we have separated the annotators into two groups at different stages if the annotators in Filtering find low-quality edited instances, the annotators in Editing will receive a low rating.\\n\\nD Annotation Interface\\n\\nFigure 8 to Figure 11 illustrate the interfaces utilized for claim annotation, evidence annotation, argument map annotation, and the final review stage.\"}"}
{"id": "acl-2024-long-835", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: The results evaluated using the metrics from the ENTAILS BANK (Dalvi et al., 2021).\\n\\n| Task 3 | Generative Models | Large Language Models |\\n|--------|-------------------|-----------------------|\\n|        | CodeT5 (FT)       | CodeT5+ (FT)          |\\n|        | 85.6 60.8 20.7 3.4 | 36.1 4.4              |\\n|        | 84.4              |                       |\\n| Task 4 | Generative Models | Large Language Models |\\n|        | CodeT5 (FT)       | CodeT5+ (FT)          |\\n|        | 81.1              |                       |\\n|        | 77.8              |                       |\\n|        | 79.7              |                       |\\n\\nTable 9: Comparison of CHECKWHY with previous Fact Verification datasets\\n\\n| Dataset | Reasoning Structure | Complicated | Multi-hop | Causal Verification |\\n|---------|---------------------|-------------|-----------|---------------------|\\n| FEVEROUS | enc-37 | /enc-34 | /enc-37 | |\\n| FEVER  | enc-37 | /enc-37 | /enc-37 | |\\n| CHECKW | enc-37 | /enc-34 | /enc-37 | |\\n| HY     | enc-34 | /enc-34 | /enc-34 | |\\n\\nThese figures include comprehensive instructions, as well as contextual information for cause-effect pairs, to aid annotators in their understanding. Additionally, examples for each annotation stage are provided to enhance annotator proficiency.\\n\\nTable 11 to Table 14\\n\\nTable 15 to Table 18\\n\\nThe prompts utilized during the data collection stage are depicted in Table 11 to Table 14. Meanwhile, the prompts for experiments can be found in Table 15 to Table 18.\"}"}
{"id": "acl-2024-long-835", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Claim: The existence of an ancient lake thousands of years ago has led to the flat geography of South Jordan today.\\n\\nGPT-4: SUPPORT Evidence\\nG1: Ancient lakes, when dried up, leave behind sediment that is evenly spread out, resulting in flat land.\\nG2: Geological evidence suggests that South Jordan was once covered by an ancient lake that left sediment deposits.\\nG3: The sediment from the ancient lake in South Jordan has, over thousands of years, compacted and created a flat landscape.\\nG4: No significant geological events have occurred to disrupt the flatness of the ancient lake bed in South Jordan.\\nG5: The ancient lake's flat bed is the primary reason for South Jordan's flatness, as no other events have altered the landscape.\\n\\nGPT4: Argument Structure\\nG2 \u2192 (G1, G3) \u2192 G5\\n(G4, G5) \u2192 Claim\\n\\nHuman: SUPPORT Evidence\\nE1: Ancient lakes, when dried up, leave behind sediment that is evenly spread out, resulting in flat land.\\nE2: Geological evidence suggests that South Jordan was once covered by an ancient lake that left sediment deposits.\\nE3: In the South Jordan area, dried-up lakes leave behind sediment that is evenly distributed.\\nE4: Sediments from the ancient lake in South Jordan have been compacted over thousands of years, compacted and created a flat landscape.\\nE5: No significant geological events have occurred to disrupt the flatness of the ancient lake bed in South Jordan.\\nE6: The compacted sediment may eventually form a flat landscape.\\n\\nHuman: Argument Structure\\n(E1, E2) \u2192 E3 \u2192 E4 \u2192 E6\\n(E5, E6) \u2192 Claim\\n\\nTable 10: A comparison example between texts written by GPT-4 and human modifications.\"}"}
{"id": "acl-2024-long-835", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "acl-2024-long-835", "page_num": 18, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "acl-2024-long-835", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I am an excellent linguist. Your task is to generate a refined claim by synthesizing the content of the {Cause} and the {Effect}. Be sure to use creative and diverse ways to generate the {Claim}. You will be provided with {Cause} is the reason why the {Effect} is established; {Effect} is the consequence of {Cause}. Note: No matter whether this pair of {Cause} and {effect} is correct or not, you must synthesize a concise and clean {Claim} with causality contained. Do not induce any extra information in your generation except what we provided.\\n\\nExample:\\n\\n{Cause}: Powerranger's film market is in the doldrums.\\n{Effect}: It's not possible that the Power Rangers series will get a sequel.\\n{Claim}: The lackluster performance of the Power Rangers film in the market has plunged its sequel prospects into uncertainty.\\n\\nTable 11: Prompt for Claim Generation\"}"}
{"id": "acl-2024-long-835", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I am an excellent logician. Your task is to generate persuasive factual evidence by expanding the anchor explanation to explain why a cause can lead to the effect in a step-by-step manner. The anchor explanation provides the basic explanation you should refer to and generate around. Solving this task with multiple steps, each step includes interleaving Thought, Action, Inference. {Thought} is the analysis of the previous step by combining the information of cause effect pair, the generated entailment Inference in previous steps, and the anchor explanation; {Thought} is also to infer which Action should be taken next to reach the effect from the current step; {Inference} is the result of the current step when taking the Action; {Action} is the solution you can operate based on the Thought in current step, and consists of 3 types:\\n\\n1. {Further Reasoning}: infer directly using the content of the previous Inference, cause, and the anchor explanation.\\n2. {Add new condition}: provide new factual condition to support the cause effect pair. Do this step only when you need context information that has not been provided in the cause effect pair and the anchor explanation, to make the inference logically and reasonably.\\n3. {Finish}: summary the final results based on the Inference.\\n\\n(Don't simply list facts, but reason in a detailed and step-by-step manner.)\\n\\nExample:\\n\\ncause: Virtue of compassion for all living things in Buddhism.\\neffect: Qisong argued that Buddhist ethics were superior to Confucian ethics.\\nanchor explanation: Confucian ethics do not dictate compassion for all living things.\\n\\nBuilding support reasoning process:\\n\\nThought1: By referring to the anchor explanation and the Cause, the difference between Buddhism and Confucianism towards compassion for all living things leads to the Qisong arguing that Buddhist ethics are superior to Confucian ethics. To support the cause-effect pair, I need to understand in detail what would make Qisong argue Buddhism against Confucian ethics.\\n\\nAction1: Further Reasoning...\\n\\nThought5: Based on the above reasoning, I can conclude that the final effect is established: Qisong argued that Buddhist ethics were superior to Confucian ethics.\\n\\nAction5: Finish\\n\\nSummary: Based on the Inference1, Inference2, Inference3, and Inference4, the cause-effect pair is supported.\\n\\n(Every Inference has a maximum of 25 words)\\n\\n(The reasoning process should be completed in multiple steps. The information in each step should be as concise as possible, and no redundant information should be included between each step.)\\n\\n...\"}"}
{"id": "acl-2024-long-835", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I am an excellent logician and fact checker.\\n\\nMy task is to verify whether the claim is {SUPPORTS}, {REFUTES} or {NOT ENOUGH INFO}, based on {Evidence}.\\n\\n(No need to focus on whether the given evidence is correct.\\n\\n(No need to return anything else superfluous)\\n\\nExample:\\n\\n# Input:\\nClaim: Joanna Briscoe attributes the novel 'The Great Lover's evocative \u201csense of time and place\u201d to its adept treatment of distinct elements such as Fabianism and class politics in that British era.\\n\\n# Evidence:\\n...\\n\\nDetermine whether {Claim} is supported or refuted:\\n\\n# Output:\\n{SUPPORTS}\"}"}
{"id": "acl-2024-long-835", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"class FACT:\\n    \"\"\"\\n    self.fact is the factual sentence.\\n    self.children is the child node that is to be linked.\\n    self.parents is the parent node that links the children node.\\n    \"\"\\\"\\n\\ndef __init__(self, fact: str):\\n    self.fact = fact\\n    self.children = []\\n    self.parents = []\\n\\ndef multiple_to_one_linking(p_fact_list: List[FACT], c_fact: FACT):\\n    \"\"\"\\n    Implement the logic to link multiple parent nodes to a single child node.\\n    p_fact_list contains the list of parent nodes.\\n    c_fact is the child node.\\n    \"\"\\\"\\n    assert len(p_fact_list) > 1\\n    for p_fact in p_fact_list:\\n        p_fact.children.append(c_fact)\\n        c_fact.parents.append(p_fact)\\n\\ndef one_to_one_linking(p_fact: FACT, c_fact: FACT):\\n    \"\"\"\\n    Implement the logic to link a single child node to a single parent node.\\n    p_fact is the parent node.\\n    c_fact is the child node.\\n    \"\"\\\"\\n    p_fact.children.append(c_fact)\\n    c_fact.parents.append(p_fact)\\n\\ndef one_to_multiple_linking(p_fact: FACT, c_fact_list: List[FACT]):\\n    \"\"\"\\n    Implement the logic to link a single parent to multiple children.\\n    p_fact is the parent node.\\n    c_fact_list contains the list of children nodes.\\n    \"\"\\\"\\n    assert len(c_fact_list) > 1\\n    for c_fact in c_fact_list:\\n        c_fact.parents.append(p_fact)\\n        p_fact.children.append(c_fact)\"}"}
{"id": "acl-2024-long-835", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I am an excellent logician and programmer.\\n\\nThe input is an instance of {CAUSE}, an instance of {EFFECT} and an explanation consist of multiple {FACT} instances.\\n\\nYour task is to complete a {build_tree_proof()} function by reasoning the relationship among the {CAUSE} instance, {EFFECT} instance, and multiple {FACT} instances, and linking these instances to form a Proof Structure logically and formally.\\n\\nBuilding the Proof Structure by calling {multiple_to_one_linking}, {one_to_one_linking}, and {one_to_multiple_linking}, which aims to describe the reasoning process that why the {CAUSE} leads to the {EFFECT} by using the intermediate {FACT} in a step-by-step manner.\\n\\nOnly when you are sure that the parent node can infer the child node, you can connect them.\\n\\n{multiple_to_one_linking} is to implement the logic to link multiple parents to a single child.\\n\\n{one_to_one_linking} is to implement the logic to link a single child to a single parent.\\n\\n{one_to_multiple_linking} is to implement the logic to link a single parent to multiple children.\\n\\nConsider the following aspects of your generation, this is very important to me:\\n\\n1. The first line of {build_tree_proof()} does not need to start with {CAUSE}, but must end with {EFFECT}.\\n\\n2. {CAUSE} node and {EFFECT} node must be used, ensuring {EFFECT} only be used ones in the Proof Structure.\\n\\n3. If a parent node can infer multiple children nodes, be sure to use {one_to_multiple_linking}, instead of {one_to_one_linking}.\\n\\n---\\n\\n**Figure 13: Argument Structure Generation Task Prompt Example**\\n\\n**<FACT CLASS PROMPT>**\\n\\n**<TASK PROMPT>**\\n\\nExample:\\n\\n# Input:\\nClaim = FACT(\\\"...\\\")\\n\\n# Evidence:\\nSentence1 = FACT(\\\"...\\\")\\n...\\n\\n# Determine whether {Claim} is supported or refuted, complete this function:\\ndef build_tree_proof():\\n\\n# Output:\\n\\\"'python\\none_to_one_linking(Sentence8, Sentence15)\\none_to_one_linking(Sentence15, Sentence16)\\none_to_one_linking(Sentence16, Sentence17)\\none_to_one_linking(Sentence17, Sentence18)\\none_to_one_linking(Sentence18, Claim)\\nprint(\\\"SUPPORTS\\\")\\n\\\"'\\n\\n**Table 17: Prompt for Task3**\"}"}
{"id": "acl-2024-long-835", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Example:\\n# Input:\\nClaim = FACT(\\\"...\\\")\\n# Evidence:\\nSentence1 = FACT(\\\"...\\\")\\n...\\n# Determine whether {Claim} is supported or refuted, complete this function:\\n\\ndef build_tree_proof():\\n    \\n    Output:\\n    \\\"'python\\n    Sentence15 = FACT(\\\"...\\\")\\n    one_to_one_linking(Sentence8, Sentence15)\\n    Sentence16 = FACT(\\\"...\\\")\\n    one_to_one_linking(Sentence15, Sentence16)\\n    Sentence17 = FACT(\\\"...\\\")\\n    one_to_one_linking(Sentence16, Sentence17)\\n    Sentence18 = FACT(\\\"...\\\")\\n    one_to_one_linking(Sentence17, Sentence18)\\n    one_to_one_linking(Sentence18, Claim)\\n    print(\\\"SUPPORTS\\\")\\n    \\\"'\\n\\nTable 18: Prompt for Task4\"}"}
