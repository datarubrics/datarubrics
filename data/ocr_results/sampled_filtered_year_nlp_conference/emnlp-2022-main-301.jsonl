{"id": "emnlp-2022-main-301", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Less is More: Summary of Long Instructions is Better for Program Synthesis\\n\\nKirby Kuznia\u2217 Swaroop Mishra\u2217 Mihir Parmar Chitta Baral\\n\\nArizona State University\\n\\nAbstract\\nDespite the success of large pre-trained language models (LMs) such as Codex, they show below-par performance on the larger and more complicated programming related questions. We show that LMs benefit from the summarized version of complicated questions. Our findings show that superfluous information often present in problem description such as human characters, background stories, and names (which are included to help humans in understanding a task) does not help models in understanding a task. To this extent, we create a meta-dataset from the frequently used APPS dataset and the newly created CodeContests dataset for the program synthesis task. Our meta-dataset consists of human and synthesized summaries of the long and complicated programming questions. Experimental results on Codex show that our proposed approach outperforms baseline by 8.13% on the APPS dataset and 11.88% on the CodeContests dataset on average in terms of strict accuracy. Our analysis shows that summaries significantly improve performance for introductory (9.86%) and interview (11.48%) programming questions. However, it shows improvement by a small margin (\u223c2%) for competitive programming questions, implying scope for future research in this direction.\\n\\n1 Introduction\\nRecently, large pre-trained LMs have been proven pivotal in programming-related tasks (Wang et al., 2021; Chen et al., 2021; Hendrycks et al., 2021; Lu et al., 2021; Papineni et al., 2002). Program synthesis aims to generate a code given the natural language description of a problem. Programming requirements in these problems vary in terms of complexity from a 3-5 line simple function to multiple functions that use advanced data structures. However, LMs such as Codex show below-par performance on the long and complicated programming questions. We observe that the natural language description of the program becomes long and complicated when there is superfluous information (see section 2.1.1). The goal of adding this information to the description is to make it more understandable to humans. However, we find that this information confuses the model in understanding a task. We propose that removing the excess information and providing the model with the exact specifications of the problem can improve the performance of the LMs.\\n\\nTo remove excess information, we summarize the descriptions of the program in such a way that it does not lose important specifications. We use the APPS dataset (Hendrycks et al., 2021) and CodeContests dataset (Li et al., 2022) which are a collection of coding problems from different online sources and create a meta-dataset consisting of human and synthesized summaries.\\n\\nWe perform all experiments using the GPT-based Codex model (Chen et al., 2021) on the proposed meta-dataset and show that the summarized version of complicated questions improves strict accuracy by 8.13% on the APPS dataset and 11.85% on CodeContests. From our analysis, we can see significant improvement for introductory (9.86%) and interview (11.48%) related programming questions. However, it shows improvement by a small margin (\u223c2%) for competitive programming questions. Considering that automatic evaluation of a program does not reward for partial correctness, we perform qualitative evaluation on our meta-dataset and find that original questions often confuse models in understanding the underlying problem, as models latch on to some spurious words in the text (e.g. the word 'list' in question makes the model...\"}"}
{"id": "emnlp-2022-main-301", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"design a list even though the underlying problem is on graphs). We further analyze model performance on different types of summaries (i.e., basic, expert, and synthetic) and provide instruction-design principles that can help future research on prompting in program synthesis.\\n\\n2 Method\\n\\n2.1 Dataset\\nWe use the APPS (Hendrycks et al., 2021) and CodeContests (Li et al., 2022) datasets to create summaries. We crowd-sourced the creation of human summaries. The result was 373 human summaries for APPS and 80 summaries for CodeContests along with 8663 synthetic summaries using both datasets. Table 1 shows the statistics of the generated summaries.\\n\\n| Data Source | Difficulty | # of Problems |\\n|-------------|------------|---------------|\\n| Human       | Introductory | 145           |\\n|             | Interview   | 123           |\\n|             | Competition | 105           |\\n|             | CodeContests| 80            |\\n|             | Total       | 453           |\\n| Studio21    | Introductory | 1588          |\\n|             | Interview   | 4551          |\\n|             | Competition | 1286          |\\n|             | CodeContests| 80            |\\n|             | Total       | 7505          |\\n| GPT-3       | Introductory | 194           |\\n|             | Interview   | 267           |\\n|             | Competition | 244           |\\n|             | CodeContests| 80            |\\n|             | Total       | 785           |\\n\\nTable 1: Statistics of the proposed meta-dataset.\\n\\n2.1.1 Human Generated Summaries\\nFor the APPS and CodeContests human-generated summaries, the crowd worker reads and understands the original questions, then creates summaries in two steps. First, we create a basic summary of the given problem and remove any information that is repeated and any hypothetical information without concrete instructions. For example, if the problem constructs a fake company or situation, we replace the fake situation with direct instructions. Full example is included in Appendix C. Second, we create an expert summary of the problem. To create this, we further summarize the first summary. This expert summary includes the absolute minimum information for an expert to understand the problem. We would not expect a novice to understand these prompts. An example of expert summaries is given in Appendix C.3.\\n\\n2.1.2 Synthetic Summaries\\nWe have generated synthetic summaries of program descriptions using jumbo (178B), large (7.5B) Studio21 model (Lieber et al., 2021), GPT-3 Davinci model (175B) (Brown et al., 2020) and PEGASUS model (Zhang et al., 2019). To generate a summary, we provide these models with a few examples in the in-context learning setup (Brown et al., 2020) from the human-generated summaries. For the few-shot examples, we use expert-level summaries.\\n\\nStudio21\\nWe use five examples with the large model, and three examples with the jumbo model. For both models, we use a temperature of 0.3, and topP of 1. For the format of our prompt, we use De-Jargonizer template with a change to their header as shown in Appendix D. We create a total of 7,505 synthetic summaries using these models.\\n\\nGPT-3\\nWe use three examples for GPT-3 model. We empirically set temperature to 0.05, topP to 1, frequency penalty to 0.01, presence penalty to 0.05. To generate prompts, we followed their tl;dr template as shown in Appendix D. We create 785 synthetic summaries using this model.\\n\\nPEGASUS\\nWe use the PEGASUS model (Zhang et al., 2019) to create program summaries for the same set of problems that were summarized by humans. We choose this model because it was trained specifically for abstractive summarization.\\n\\n2.2 Model\\nWe use OpenAI Codex to build baselines and the proposed approach.\\n\\nBaseline\\nTo create a baseline, we have used original program descriptions given in the datasets as prompts for the Codex model.\\n\\n6 Examples are included in Appendix D\\n7 https://studio.ai21.com/\\n8 https://beta.openai.com/playground/p/default-tldr-summary?model=text-davinci-001\"}"}
{"id": "emnlp-2022-main-301", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Difficulty | AP | EWPR | BWPR |\\n|------------|----|------|------|\\n| Introductory | 42.96 | 50.00 | 44.53 |\\n| Interview | 37.70 | 51.82 | 44.26 |\\n| Competition | 4.76 | 5.71 | 4.76 |\\n| Weighted Average | 30.47 | 35.64 | 30.31 |\\n| CodeContests | 12.50 | 25.00 | 13.33 |\\n\\n**Table 2**: Results of baseline and proposed model in terms of Strict Accuracy (SAcc). The first block is from the APPS dataset. The last block is from the CodeContests dataset. AP: All Problems, EWPR: Either Worst Problem Removal, BWPR: Both Worst Problem Removal (see explanation in section 3). All results are in %.\\n\\n**Table 3**: Results when taking the best summary for each problem. The EWPR baseline is different from Table 2 because a different set of problems have been removed.\\n\\n**Proposed Approach**\\nWe have used summaries of original program descriptions given in the datasets as prompts for the Codex model.\\n\\n**3 Experimental Setup**\\nAll the experiments are performed using the davinci\u2212codex (Chen et al., 2021) model provided through OpenAI. At inference time, we use a modified version of the evaluation code provided by Hendrycks et al. (2021). This evaluation code has four different outputs for each test case: (1) \u22122: the code has a syntax error and can not run, (2) \u22121: the code is syntactically correct but has a run time error, (3) 0: the code runs without any errors but fails the test case, and (4) 1: the code runs without any error and passes the test case. Similar to Chen et al. (2021), we implement a timeout for the code at inference time. If a test case takes more than 4 seconds to run then we throw an exception and count that test case as a \u22121.\\n\\n**Experiments**\\nTo show effectiveness of the proposed approach, we have performed three different experiments using human generated summaries:\\n\\n1. All problems from basic and expert summaries are used at inference time. We term this experiment All Problems (AP).\\n2. We eliminate problems that perform worse for either basic or expert summaries. We term this experiment Either Worst Problem Removal (EWPR).\\n3. We eliminate problems that perform worse for both basic and expert summaries. We term this experiment Both Worst Problem Removal (BWPR).\\n\\n**Motivation behind EWPR and BWPR**\\nIf a summary caused every test case to perform worse then it's likely the crowd worker produced a faulty summary. To mitigate the effect of outliers in the dataset, we use the EWPR method to remove such problems. Another hypothesis is that every problem benefits from some level of summarization (i.e., basic or expert). To measure this, we use the BWPR method. From Table 6 results, we identify that only 1 problem had both summaries (basic and expert) perform worse.\\n\\n**Metric**\\nIn (Austin et al., 2021a), they show that the BLEU metric (Papineni et al., 2002) does not correlate well with synthesis performance. Thus, we use Strict Accuracy (SAcc) as our evaluation metric for all experiments (see Appendix E).\\n\\n**4 Results and Analysis**\\n\\n**4.1 Human Generated Summaries**\\nFrom Table 2, we can observe that both the summary-based models show on average superior performance compared to baseline. In particular, when calculating results for every problem,\"}"}
{"id": "emnlp-2022-main-301", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Results of baseline and proposed approach (All results are in %). Summaries generated by GPT-3, Studio21, and PEGASUS used for inference from APPS.\\n\\n| Model     | Difficulty | AP  | EWPR |\\n|-----------|------------|-----|------|\\n| Baseline  | Introductory | 41.75 | 38.66 | 41.11 | 41.67 |\\n| Baseline  | Interview  | 20.30 | 18.80 | 18.18 | 20.66 |\\n| Baseline  | Competition | 2.87  | 3.28  | 2.73  | 3.64  |\\n| Proposed | Introductory | 39.53 | 31.63 | 39.04 | 36.36 |\\n| Proposed | Interview  | 12.28 | 11.00 | 10.57 | 12.37 |\\n| Proposed | Competition | 1.67  | 1.21  | 1.38  | 1.38  |\\n| Weighted Average | Introductory | 20.17 | 18.89 | 19.14 | 20.55 |\\n| Weighted Average | Interview  | 11.53 | 9.66  | 10.61 | 10.98 |\\n\\nTable 5: Results of baseline and proposed approach (All results are in %). 80 summaries generated by GPT-3 and Studio21 used for inference from CodeContests.\\n\\nBasic and expert summary-based models outperform baseline by 4.34% and 5.15% on average for APPS dataset, respectively. Further analysis shows that the expert summary-based model shows improved performance by $\\\\sim 1\\\\%$ compared to the basic summary-based model.\\n\\nOn the CodeContests dataset (Li et al., 2022), we show an average improvement of 11.88% in terms of SAcc. For this dataset, we did not separate the problems by difficulty. This is because the problems come from different sources and have different scales of difficulty. Thus, we did not report the SAcc when weighted by difficulty in Table 2.\\n\\nOur analysis shows that many problems where the basic summary would fail, however, the expert summary would succeed and vice-versa. Thus, we choose the best summary for each problem after evaluating both summaries and then calculate the results for the best summaries. Table 3 shows results when taking the best summary for each problem for APPS dataset. We observe a 9.86%, 11.48%, and 1.91% increase on SAcc for introductory, interview, and competition level problems, respectively.\\n\\n4.2 Synthetic Summaries\\n\\nTable 4 and 5 show the results for baseline, synthetic summaries generated by GPT-3, Studio21 and PEGASUS in terms of SAcc for two experiments. For the AP experiment, we can observe that the performance of the baseline outperforms synthetic summary-based models. However, the proposed model shows an average similar performance compared to the baseline for the EWPR experiment. Moreover, Appendix I shows the results for top 500 and top 1000 summaries from GPT-3 and Studio21, respectively.\\n\\n4.3 Analysis\\n\\nWhy does eliminating the worst problems help? From Tables 2, we can observe that EWPR and BWPR have improved performance compared to AP for both human and synthetically generated summaries. By analyzing the summarized worst problems, we notice a difference in the summarization style which shows that these summaries are outliers and do not match the distribution of the other summaries. This can cause a problem in synthesizing a good program since the model loses important information. Hence, we believe that eliminating the worst problems improves model performance.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: (Top plot) Mean frequency of POS for problems where programs were generated by both the original and summarized prompts pass all test cases, and (Bottom plot) mean frequency of POS for problems where the summary passed all test cases and the original did not. The blue bar represents the mean of the entire dataset. The plot shows that a higher number of nouns degrades model performance.\\n\\nIs there any possible bias in the meta-dataset? Recent studies show that bias propagates in human-annotated datasets (Geva et al., 2019; Parmar et al., 2022a). Given that our summaries are also human-generated, there will be some bias in the dataset. Some details that are critical to one person can be trivial to others. In the context of generating expert summaries, assumptions about expert knowledge can vary. This bias causes drift in the dataset and hinders the model\u2019s performance. Similar to Mishra et al. (2021), we can provide a template for what is expected from the summary generator to reduce bias.\\n\\nWhy is competition accuracy low? We believe that these problems require multi-hop reasoning, even after summarization, which is still a challenge for language models.\\n\\nImpact of POS on Accuracy\\nIn the top plot of Figure 1, we observe that the frequency of nouns and proper nouns for problems that passed all test cases is lower than the entire dataset. In the bottom plot, we observe that the frequency for nouns and proper nouns is higher for the original question (which had < 100% accuracy on the test cases) and lower for the summary (which had 100% accuracy on the test cases). Thus, we can see that the number of nouns degrades performance. We also see in the bottom chart that overuse of punctuation can be detrimental to performance. From the results in Figure 1, we see results of nouns affecting performance along with excessive punctuation. Additional detailed analysis is presented in Appendix B.\\n\\nConclusion\\nThis paper introduces a summarization-based approach for efficient program synthesis. Experimental results show that the proposed approach improves the performance of the Codex model by on average $\\\\sim 8\\\\%$ across various levels of programming questions provided by the APPS and $\\\\sim 11\\\\%$ on the CodeContests. Further, this paper proposes a meta-dataset consisting of $\\\\sim 450$ human-generated basic and expert-level summaries as well as $\\\\sim 8k$ synthetically generated summaries by GPT-3 and Studio21; this can be helpful for future research on writing better instructions for the program synthesis. We show that program synthesis models benefit from concise prompts, hence, we believe that less number of high-quality instances are better than more low-quality data instances.\\n\\nFuture Extensions\\nThe decomposition of prompts has been shown to improve accuracy (Mishra et al., 2022; Patel et al., 2022); splitting up the summarization task into smaller tasks can potentially result in higher accuracy for the Codex model in future. Additionally, the PEGASUS model could be used in conjunction with other models to perform the detailed algorithm outlined in Appendix N.\\n\\nLimitations\\nOur summary-based approach shows improved performance on program synthesis models, however, it shows competitive performance on synthetic summaries. We believe that the generation of high-quality summaries can improve performance, hence, designing efficient prompts to improve synthetic summaries can be the scope of further research. Furthermore, human-generated summaries...\"}"}
{"id": "emnlp-2022-main-301", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"show competitive performance on competition-level problems. These problems require reasoning with multiple logical leaps and knowledge of advanced algorithms and data structures. Hence, exploring new techniques for summarization can be a future research direction. In addition, this work only analyzes the codex model, hence, exploring the effect of summarization on other program synthesis models can be interesting.\\n\\nReferences\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021a. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021b. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.\\n\\nMatej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. 2016. Deepcoder: Learning to write programs. arXiv preprint arXiv:1611.01989.\\n\\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.\\n\\nJacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. 2017. Robustfill: Neural program learning under noisy i/o. In International conference on machine learning, pages 990\u2013998. PMLR.\\n\\nRuifang Ge and Raymond Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 9\u201316.\\n\\nMor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1161\u20131166, Hong Kong, China. Association for Computational Linguistics.\\n\\nSumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. 2017. Program synthesis. Foundations and Trends\u00ae in Programming Languages, 4(1-2):1\u2013119.\\n\\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. 2021. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938.\\n\\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814.\\n\\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs.\\n\\nShuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664.\\n\\nMan Luo, Sharad Saxena, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. Biotabqa: Instruction learning for biomedical table question answering. arXiv preprint arXiv:2207.02419.\\n\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022. Reframing instructional prompts to GPTk\u2019s language. In Findings of the Association for Computational Linguistics: ACL 2022, pages 589\u2013612, Dublin, Ireland. Association for Computational Linguistics.\\n\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generalization via natural language crowdsourcing instructions. arXiv preprint arXiv:2104.08773.\\n\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nMihir Parmar, Swaroop Mishra, Mor Geva, and Chitta Baral. 2022a. Don't blame the annotator: Bias already starts in the annotation instructions. arXiv preprint arXiv:2205.00415.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral. 2022b. In-BoXBART: Get instructions into biomedical multi-task learning. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 112\u2013128, Seattle, United States. Association for Computational Linguistics.\\n\\nPruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. Is a question decomposition unit all we need? EMNLP 2022, Abu Dhabi.\\n\\nRavsehaj Singh Puri, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. How many data samples is an additional instruction worth? arXiv preprint arXiv:2203.09161.\\n\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaf\ufb01n, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.\\n\\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859.\\n\\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.\\n\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2019. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Related Work\\n\\nIn the past, there are several methods including semantic parsing (Ge and Mooney, 2005), deductive approaches, enumerative and stochastic search, and constraint solving which have gained attention for program synthesis (Gulwani et al., 2017). With the advent of machine/deep learning, Balog et al. (2016) introduced a neural network based model for solving programming competition-style problems. Devlin et al. (2017) used sequence-to-sequence approach to do program synthesis. Furthermore, Hendrycks et al. (2021) introduced the APPS dataset for testing the accuracy of large LMs on program synthesis. Hendrycks et al. (2021) leveraged the GPT-Neo model (Black et al., 2021) which they fine-tune for this task using APPS dataset. CodeT5 model (Wang et al., 2021) utilizes many different training objectives. Recently, Austin et al. (2021b) explore limitations of large language models and propose two new benchmarks, MBPP and MathQA-Python. The Codex model (Chen et al., 2021) is an advanced code generation model that powers GitHub's Copilot. The state of the art model for program synthesis was introduced by Deepmind called AlphaCode (Li et al., 2022). They released their dataset CodeContests, which was used to fine-tune and test their model, and was used in this paper. Our approach suggesting smaller instructions compliments other approaches in improving model performance in instruction paradigm (Mishra et al., 2021; Wei et al., 2022; Parmar et al., 2022b; Nye et al., 2021; Puri et al., 2022; Luo et al., 2022; Wei et al., 2021; Sanh et al., 2021).\\n\\nB Additional Analysis\\n\\nDifficulty of CodeContests\\n\\nThe accuracies for CodeContests is notably lower than the APPS dataset since this dataset is more challenging, e.g. the number and complexity of programming operations is relatively higher than APPS. From the baseline results in Table 2, we can observe that problems in CodeContests are harder than interview but easier than competition.\\n\\nImpact of Entities on Accuracy\\n\\nIn Figure 2, we can observe that the total number of entities $num_{entities}$ is higher for problems that performed worse. Here, we can see that the original problems (which failed test cases) had a higher mean than the dataset and the summaries (which passed all test cases) had a lower number of entities.\\n\\nC Example of removing fake information\\n\\nTo see the code produced by the model for this example, refer to Appendix J. There are more examples of superfluous information confusing the model in Appendix O and of made up information confusing the model in Appendix P.\\n\\nC.1 Original Prompt\\n\\nCodefortia is a small island country located somewhere in the West Pacific. It consists of $n$ settlements connected by $m$ bidirectional gravel roads. Curiously enough, the beliefs of the inhabitants require the time needed to pass each road to be equal either to $a$ or $b$ seconds. It's guaranteed that one can go between any pair of settlements by following a sequence of roads.\\n\\nCodefortia was recently struck by the financial crisis. Therefore, the king decided to abandon some of the roads so that: it will be possible to travel between each pair of cities using the remaining roads only, the sum of times required to pass each remaining road will be minimum possible (in other words, remaining roads must form minimum spanning tree, using the time to pass the road as its weight), among all the plans minimizing the sum of times above, the time required to travel between the king's\"}"}
{"id": "emnlp-2022-main-301", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The king, however, forgot where the parliament house was. For each settlement \\\\( p = 1, 2, \\\\ldots, n \\\\), can you tell what is the minimum time required to travel between the king's residence and the parliament house (located in settlement \\\\( p \\\\)) after some roads are abandoned?\\n\\n\u2014\u2013Input\u2014\u2013\\n\\nThe first line of the input contains four integers \\\\( n \\\\), \\\\( m \\\\), \\\\( a \\\\) and \\\\( b \\\\) (\\\\( 2 \\\\leq n \\\\leq 70 \\\\), \\\\( n - 1 \\\\leq m \\\\leq 200 \\\\), \\\\( 1 \\\\leq a < b \\\\leq 10^7 \\\\)) \u2014 the number of settlements and gravel roads in Codefortia, and two possible travel times. Each of the following lines contains three integers \\\\( u, v, c \\\\) (\\\\( 1 \\\\leq u, v \\\\leq n \\\\), \\\\( u \\\\neq v \\\\), \\\\( c \\\\in \\\\{a, b\\\\} \\\\)) denoting a single gravel road between the settlements \\\\( u \\\\) and \\\\( v \\\\), which requires \\\\( c \\\\) minutes to travel.\\n\\nYou can assume that the road network is connected and has no loops or multiedges.\\n\\n\u2014\u2013Output\u2014\u2013\\n\\nOutput a single line containing \\\\( n \\\\) integers. The \\\\( p \\\\)-th of them should denote the minimum possible time required to travel from \\\\( 1 \\\\) to \\\\( p \\\\) after the selected roads are abandoned. Note that for each \\\\( p \\\\) you can abandon a different set of roads.\\n\\n\u2014\u2013Examples\u2014\u2013\\n\\nInput\\n5 5 20 25\\n1 2 25\\n2 3 25\\n3 4 20\\n4 5 20\\n5 1 20\\nOutput\\n0 25 60 40 20\\nInput\\n6 7 13 22\\n1 2 13\\n2 3 13\\n1 4 22\\n3 4 13\\n4 5 13\\n5 6 13\\n6 1 13\\nOutput\\n0 13 26 39 26 13\\n\\n\u2014\u2013Note\u2014\u2013\\n\\nThe minimum possible sum of times required to pass each road in the first example is 85 \u2014 exactly one of the roads with passing time 25 must be abandoned. Note that after one of these roads is abandoned, it's now impossible to travel between settlements 1 and 3 in time 50.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, we can assume that an expert would already know what a minimum spanning tree is. Thus, we can remove this detailed description of an MST.\\n\\nYou are given a connected graph of \\\\( n \\\\) nodes and \\\\( m \\\\) bidirectional edges. For each node \\\\( p = 1, 2, \\\\ldots, n \\\\), you need to find a minimum spanning tree. Then output the minimum cost required to travel between node 1 and node \\\\( p \\\\).\\n\\n**\u2014\u2013Input\u2014\u2013**\\n\\nThe first line of the input contains four integers \\\\( n, m, a, b \\\\) (\\\\( 2 \\\\leq n \\\\leq 70, n - 1 \\\\leq m \\\\leq 200, 1 \\\\leq a < b \\\\leq 10^7 \\\\)) \u2014 the number of nodes and edges in the graph, and two possible travel times. Each of the following lines contains three integers \\\\( u, v, c \\\\) (\\\\( 1 \\\\leq u, v \\\\leq n, u \\\\neq v, c \\\\in \\\\{a, b\\\\} \\\\)) denoting an edge between the nodes \\\\( u \\\\) and \\\\( v \\\\), which has cost \\\\( c \\\\).\\n\\nYou can assume that the graph is connected and has no loops or multiedges.\\n\\n**\u2014\u2013Output\u2014\u2013**\\n\\nOutput a single line containing \\\\( n \\\\) integers. The \\\\( p \\\\)-th of them should denote the minimum possible cost required to travel from 1 to \\\\( p \\\\) after the selected edges are abandoned. Note that for each \\\\( p \\\\) you can abandon a different set of edges.\\n\\n**\u2014\u2013Examples\u2014\u2013**\\n\\n**Input**\\n\\n```\\n5 5 20 25\\n1 2 25\\n2 3 25\\n3 4 20\\n4 5 20\\n5 1 20\\n```\\n\\n**Output**\\n\\n```\\n0 25 60 40 20\\n```\\n\\n**Input**\\n\\n```\\n6 7 13 22\\n1 2 13\\n2 3 13\\n1 4 22\\n3 4 13\\n4 5 13\\n5 6 13\\n6 1 13\\n```\\n\\n**Output**\\n\\n```\\n0 13 26 39 26 13\\n```\"}"}
{"id": "emnlp-2022-main-301", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: These are the numbers of problems in each split of the dataset. For GPT and Studio21 we did not look at problems that were worse or same for both experiments because there was insignificant overlap between the two experiments.\\n\\nF Codex Configuration\\nWe did a small test with 75 summaries to find our hyper-parameters for Codex. We set temperature to 0, topP to 1, frequency penalty to 0, and presence penalty to 0. We did not provide few-shot examples to Codex since we want to see if summarization only could improve the performance of the Codex model.\\n\\nG Worst Problems and Statistics\\nUsing the test case labels as defined in section 3 we defined a test case as getting worse if its label (result) was lower. Then we defined a problem as worse if every test case had a lower label. Our methodology behind this was, if we removed problems that had a worse accuracy, then it would be a non-trivial result that accuracy improved. Also, if we removed problems with worse accuracy, then a problem that originally had all 0 labels (all False test cases) would score the same if the summary had all \u22121 labels (runtime error) or a \u22122 (syntax error). So, we removed problems which every test case performed worse, to see if removing these outliers would improve results. You can see the overall breakdown of each split in table 6.\\n\\nH Average length of Problems and Solutions\\nTable 7 represents the statistics for average length of problems and solutions for original and summarized prompts.\\n\\nI Abbreviated Synthetic Results\\nIn table 8, we show the results for our synthetic summaries when taking the top 500 and 1000 summaries for GPT3 and StudioAI21, respectively. In our initial experiment, this was the amount of problems we tested for each model. However, in our final experiment we changed our configurations and generated more problems. For a comparison, we took the top performing summaries and and reported those results.\\n\\nJ Generated Code\\nIn figure 3 is the code that was generated for the example mentioned in Appendix C and C.3. Given that the Codex model was prompted with the def code(): the model did not generate that function definition or the call to that function. That was added in afterwards, but everything inside that function was generated by Codex. The originally generated code (far left) fails with a \u22121 because it did not take in the input correctly. It added in another line p = int(input()), which most likely refers to the p mentioned in the original text. The expert summary generated code (middle) fails every test case. The basic summary generated code (right) passed 16/19 (84%) test cases and was the only code to pass at least 1 test case.\\n\\nK StudioAI21 Generated Code\\nBelow is an example of a competition problem where StudioAI21 summarized the prompt too much but Codex was still able to produce viable code. Here is the original prompt:\\n\\nCengiz recently learned Fibonacci numbers and now he is studying different algorithms to find them. After getting bored of reading them, he came with his own new type of numbers that he named XORinacci numbers. He defined them as follows:\\n\\n\\\\[ f(0) = a; \\\\]\\n\\\\[ f(1) = b; \\\\]\\n\\\\[ f(n) = f(n-1) \\\\oplus f(n-2) \\\\] when \\\\( n > 1 \\\\), where \\\\( \\\\oplus \\\\) denotes the bitwise XOR operation.\\n\\nYou are given three integers \\\\( a, b, \\\\) and \\\\( n \\\\), calculate \\\\( f(n) \\\\).\\n\\n\u2014\u2013Input\u2014\u2013\\nThe input contains one or more independent test cases.\\nThe first line of input contains a single integer \\\\( T \\\\) (1 \u2264 \\\\( T \\\\) \u2264 10^3), the number of test cases.\\nEach of the \\\\( T \\\\) following lines contains three space-separated integers \\\\( a, b, \\\\) and \\\\( n \\\\) (0 \u2264 \\\\( a, b, n \\\\) \u2264 10^9) respectively.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The average length of the original/summarized prompt and generated code. The average length of the code solutions is the average len of the solutions provided by the creators of the APPS dataset. A problem could have one or multiple solutions. The length is reported in characters.\\n\\nFigure 3: On the far left is the code generated by the original prompt. The middle is the code generated by the expert summary. The right is the code generated by the basic summary.\\n\\n| Model     | Difficulty | AP | EW | PR |\\n|-----------|------------|----|----|----|\\n| Baseline  |            |    |    |    |\\n| GPT-3     | Introductory | 41.97 | 38.86 |    |\\n|           | Interview   | 25.27 | 27.47 |    |\\n|           | Competition | 4.80  | 6.40  |    |\\n|           | Weighted Average | 26.60 | 26.60 |    |\\n| StudioAI21| Introductory | 39.91 | 31.92 |    |\\n|           | Interview   | 15.97 | 14.50 |    |\\n|           | Competition | 2.57  | 2.57  |    |\\n|           | Weighted Average | 16.90 | 14.50 |    |\\n\\nTable 8: Results when taking the top 500 GPT problems and top 1000 Studio problems\\n\\n\u2014\u2013Output\u2014\u2013\\nFor each test case, output $f(n)$.\\n\\n\u2014\u2013Example\u2014\u2013\\nInput\\n3\\n3 4 2\\n4 5 0\\n325 265 1231232\\nOutput\\n7\\n4\\n\\n\u2014\u2013Note\u2014\u2013\\nIn the first example, $f(2) = f(0) \\\\oplus f(1) = 3 \\\\oplus 4 = 7$.\\n\\nHere is the summary that StudioAI21 generated:\\n\\nYou are given three integers $a$, $b$, and $n$. Calculate $f(n)$.\\n\\n\u2014\u2013Input\u2014\u2013\\nThe input contains one or more independent test cases.\\nThe first line of input contains a single integer $T$ ($1 \\\\leq T \\\\leq 10^3$), the number of test cases.\\nEach of the $T$ following lines contains three space-separated integers $a$, $b$, and $n$ ($0 \\\\leq a, b, n \\\\leq 10^9$) respectively.\\n\\n\u2014\u2013Output\u2014\u2013\\nFor each test case, output $f(n)$. \\n\\n\u2014\u2013Example\u2014\u2013\\nInput\\n3\\n3 4 2\\n4 5 0\\n325 265 1231232\\nOutput\\n7\\n4\\n4543\"}"}
{"id": "emnlp-2022-main-301", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Note\\n\\nIn the first example, $f(2) = f(0) \\\\oplus f(1) = 3 \\\\oplus 4 = 7$.\\n\\nBecause any input/output examples provided by the prompt are appended to the summary, Codex was able to figure out the pattern in the problem and generate code that was almost correct. In figure 4, the solution (left) used the pattern in the problem and simplify by taking $n \\\\mod 3$. The Studio21 summary code (right) recognizes this pattern but erroneously does not take the modulus of the number. The original code (center) also makes the same mistake by not taking the modulus, but also brute forces the answer. This shows that the model did not recognize the pattern in this problem because of the superfluous details. Even though Studio21 might have summarized too much, the model was still able to make an improvement and understand the pattern in the problem more.\\n\\nHere is an example of a summary made by StudioAI21 where the qualitative aspect of the code but it still failed. Here is the original prompt:\\n\\nGiven is a tree $G$ with $N$ vertices. The vertices are numbered 1 through $N$, and the $i$-th edge connects Vertex $a_i$ and Vertex $b_i$. Consider painting the edges in $G$ with some number of colors. We want to paint them so that, for each vertex, the colors of the edges incident to that vertex are all different. Among the colorings satisfying the condition above, construct one that uses the minimum number of colors.\\n\\n--- Constraints ---\\n- $2 \\\\leq N \\\\leq 10^5$\\n- $1 \\\\leq a_i < b_i \\\\leq N$\\n- All values in input are integers.\\n- The given graph is a tree.\\n\\n--- Input ---\\nInput is given from Standard Input in the following format:\\n$N$\\n$a_1 b_1$\\n$a_2 b_2$\\n...\\n$a_{N-1} b_{N-1}$\\n\\n--- Output ---\\nPrint $N$ lines.\\nThe first line should contain $K$, the number of colors used.\\nThe $(i+1)$-th line $(1 \\\\leq i \\\\leq N-1)$ should contain $c_i$, the integer representing the color of the $i$-th edge, where $1 \\\\leq c_i \\\\leq K$ must hold.\\nIf there are multiple colorings with the minimum number of colors that satisfy the condition, printing any of them will be accepted.\\n\\n--- Sample Input ---\\n3\\n1 2\\n2 3\\n\\n--- Sample Output ---\\n2\\n1\\n2\\n\\nIn 5 the left is the original solution which fails with a $-2$ because the runtime of the algorithm is exponential. Note that it tries to create a list of all possible edge colorings which is $O(2^N)$. The right is the code produced when using the StudioAI21 summary. You can see that this code is much closer.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: The left is the code generated using the original prompt. The right is the code generated when using the StudioAI21 generated summary. It tries to print the sum of a boolean (near the end before the last for loop). Which fails in Python because a bool is not iterable.\\n\\nHere is the original prompt:\\n\\nPolycarpus has a sequence, consisting of $n$ non-negative integers: $a_1, a_2, ..., a_n$.\\n\\nLet's define function $f(l, r)$ (l, r are integer, $1 \\\\leq l \\\\leq r \\\\leq n$) for sequence $a$ as an operation of bitwise OR of all the sequence elements with indexes from $l$ to $r$. Formally: $f(l, r) = a_l | a_{l+1} | ...$\\n\\nPolycarpus took a piece of paper and wrote out the values of function $f(l, r)$ for all $l, r$ (l, r are integer, $1 \\\\leq l \\\\leq r \\\\leq n$). Now he wants to know, how many distinct values he's got in the end. Help Polycarpus, count the number of distinct values of function $f(l, r)$ for the given sequence $a$.\\n\\nExpression $x|y$ means applying the operation of bitwise OR to numbers $x$ and $y$. This operation exists in all modern programming languages, for example, in language C++ and Java it is marked as \u201c|\\\", in Pascal \u2014 as \\\"or\\\".\\n\\n\u2014\u2013Input\u2014\u2013\\nThe first line contains integer $n$ ($1 \\\\leq n \\\\leq 10^5$) \u2014 the number of elements of sequence $a$. The second line contains $n$ space-separated integers $a_1, a_2, ..., a_n$ ($0 \\\\leq a_i \\\\leq 10^6$) \u2014 the elements of sequence $a$.\\n\\n\u2014\u2013Output\u2014\u2013\\nPrint a single integer \u2014 the number of distinct values of function $f(l, r)$ for the given sequence $a$.\\n\\nPlease, do not use the lld specifier to read or write 64-bit integers in C++. It is preferred to use cin, cout streams or the I64d specifier.\\n\\n\u2014\u2013Examples\u2014\u2013\\nInput\\n3\\n1 2 0\\nOutput\\n4\\nInput\\n10\\n1 2 3 4 5 6 1 2 9 10\\nOutput\\n11\\n\\n\u2014\u2013Note\u2014\u2013\\nIn the first test case Polycarpus will have 6 numbers written on the paper: $f(1, 1) = 1$, $f(1, 2) = 3$, $f(1, 3) = 3$, $f(2, 2) = 2$, $f(2, 3) = 2$, $f(3, 3) = 0$. There are exactly 4 distinct numbers among them: 0, 1, 2, 3.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the first test case Polycarpus will have 6 numbers written on the paper: f(1, 1) = 1, f(1, 2) = 3, f(1, 3) = 3, f(2, 2) = 2, f(2, 3) = 2, f(3, 3) = 0. There are exactly 4 distinct numbers among them: 0, 1, 2, 3.\\n\\nIn the left is the original solution which gets 77% accuracy. The right is the summary code which gets 100% accuracy.\\n\\nHere are two summaries where GPT perfectly summarized the prompt and gave a concise description of what the task was. In both cases the original prompt did not have 100% accuracy but the summarized prompt did have 100% accuracy.\\n\\nHere is the original prompt for the first question:\\n\\nBo\u017eo is a strange little boy. Every day he tires his friends with strange questions. Today's question is: how many integers in the interval \\\\([A, B]\\\\) are there such that the sum of their digits is \\\\(S\\\\), and which is the smallest such number?\\n\\nWrite a program that answers Bo\u017eo's question so that he can get some sleep.\\n\\n\u2014\u2013Input\u2014\u2013\\nThe input contains three integers \\\\(A\\\\), \\\\(B\\\\) and \\\\(S\\\\) (\\\\(1 \\\\leq A \\\\leq B < 10^{15}\\\\), \\\\(1 \\\\leq S \\\\leq 135\\\\)).\\n\\n\u2014\u2013Output\u2014\u2013\\nThe first line should contain the number of integers in the interval with the digit sum equal to \\\\(S\\\\).\\nThe second line should contain the smallest such integer.\\nThe input data will guarantee that the first number is at least 1.\\n\\n\u2014\u2013Examples\u2014\u2013\\nSample Input 1:\\n1 9 5\\nSample Output 1:\\n1\\n5\\n\\nSample Input 2:\\n1 100 10\\nSample Output 2:\\n9\\n19\\n\\nHere is the summary that GPT Generated:\\n\\nWrite a program that calculates the smallest integer in the given interval whose sum of digits is equal to the given sum.\\n\\n\u2014\u2013Input\u2014\u2013\\nThe input contains three integers \\\\(A\\\\), \\\\(B\\\\) and \\\\(S\\\\) (\\\\(1 \\\\leq A \\\\leq B < 10^{15}\\\\), \\\\(1 \\\\leq S \\\\leq 135\\\\)).\\n\\n\u2014\u2013Output\u2014\u2013\\nThe first line should contain the number of integers in the interval with the digit sum equal to \\\\(S\\\\).\\nThe second line should contain the smallest such integer.\\nThe input data will guarantee that the first number is at least 1.\\n\\n\u2014\u2013Examples\u2014\u2013\\nSample Input 1:\\n1 9 5\\nSample Output 1:\\n1\\n5\"}"}
{"id": "emnlp-2022-main-301", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In 7 you can see the original code on the left and the summary code on the right. There is a subtle difference but it's that difference that improved the problem from 33% accuracy to 100%.\\n\\nProfessor GukiZ makes a new robot. The robot are in the point with coordinates $(x_1, y_1)$ and should go to the point $(x_2, y_2)$. In a single step the robot can change any of its coordinates (maybe both of them) by one (decrease or increase). So the robot can move in one of the 8 directions. Find the minimal number of steps the robot should make to get the finish position.\\n\\n\u2014\u2013Input\u2014\u2013\\nThe first line contains two integers $x_1$, $y_1$ ($-10^9 \\\\leq x_1, y_1 \\\\leq 10^9$) \u2014 the start position of the robot.\\nThe second line contains two integers $x_2$, $y_2$ ($-10^9 \\\\leq x_2, y_2 \\\\leq 10^9$) \u2014 the finish position of the robot.\\n\\n\u2014\u2013Output\u2014\u2013\\nPrint the only integer $d$ \u2014 the minimal number of steps to get the finish position.\\n\\n\u2014\u2013Examples\u2014\u2013\\nInput\\n0 0\\n4 5\\nOutput\\n5\\nInput\\n3 4\\n6 1\\nOutput\\n3\\n\\n\u2014\u2013Note\u2014\u2013\\nIn the first example robot should increase both of its coordinates by one four times, so it will be in position (4, 4). After that robot should simply increase its y coordinate and get the finish position.\\n\\nIn the second example robot should simultaneously increase x coordinate and decrease y coordinate by one three times.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: The left is the code generated using the original prompt. The right is the code generated when using the GPT3 generated summary.\\n\\n\u2014\u2013Note\u2014\u2013\\nIn the first example robot should increase both of its coordinates by one four times, so it will be in position (4, 4). After that robot should simply increase its y coordinate and get the finish position.\\n\\nIn the second example robot should simultaneously increase x coordinate and decrease y coordinate by one three times.\\n\\nIn 8 you can see the original code on the left and the summary code on the right. There is a subtle difference but it's that difference that improved the problem from 20% accuracy to 100%.\\n\\nN Human Generated Instructions\\nThe section below was given to each crowd worker as instructions to follow when creating the regular and expert summaries.\\n\\nN.1 Summarization\\nCreate a file called summary.txt this will contain your summary of the prompt. It's recommended that you copy the question.txt file into the summary.txt file then starting from the top of the prompt follow the steps and remove words/lines as necessary.\\n\\nThese are the rough steps for making a summary. Following these steps will create the most consistency in our dataset. However, you should summarize as you see fit. First, read through the prompt and understand what it's asking, then follow these steps to help create a summary.\\n\\n1. Directly state what is given in the problem.\\n   \u2022 Most problems start by setting the scene, to help humans understand.\\n   \u2022 Start the problems by explicitly telling the model what the input is.\\n\\n2. Remove any notes given in the prompt.\\n   \u2022 They are usually reemphasizing points, which is redundant and not needed in the summary.\\n   \u2022 This includes the Notes section at the bottom of the file.\\n   \u2022 If there is pertinent information given from a note, include it in the prompt without describing it as a note.\\n\\n3. Remove any text in parenthesis.\\n   \u2022 Most of the text in parenthesis is repeating the information that precede them.\\n   \u2022 If the text in parenthesis provides more context or information, then remove the preceding text.\\n   \u2022 Keep any parenthesis if it is describing constraints, such as the minimum and maximum values for the input etc...\\n\\n4. Remove any made up people, places, things, etc...\\n   \u2022 These abstractions are made to help humans understand but confuse the model.\\n   \u2022 The prompts often mention things like Codefortia or Polycarp, try to replace these with the word you.\\n   \u2022 Any text visualizing what the problem is asking, should be removed.\\n\\n5. If the Input or Output section reference an abstraction they should be changed.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Overall, these sections are fine. However, if they mentioned something you removed in the previous steps, they should be changed to reflect that.\\n\\nIf these sections repeat themselves remove any redundancies.\\n\\nIn most cases these sections will be left alone.\\n\\nN.2 Expert Summary\\n\\nCreate a file called expert.txt this will contain an expert summary of the prompt. It's recommended that you copy the summary.txt file into the expert.txt file then starting from the top of the prompt remove words/lines as necessary. You should aim for the expert prompt to be 2\u22124 lines.\\n\\nImagine you are describing the prompt to a senior software engineer. What else could you trim out? The difference between the original and expert summary, is the original summary may include something obvious, whereas the expert solution should be the absolute bare minimum. To create summary.txt you want to remove superfluous details from the original prompt. To create expert.txt you want to remove details that an expert would find obvious, from the summary.\\n\\nFor example, in problem 2000 (which is competitive difficulty) the summary mentions \u2018It will be possible to travel between each pair of nodes . . . , and the sum of times . . . will be the minimum possible\u2019. This process is describing a minimum spanning tree so you can just say \u2018Find a minimum spanning tree\u2019.\\n\\nAlso, if the prompt included an example and subsequent explanation, that should remain in the summary but should be removed from the expert summary. An expert already understands the problem and does not need any extra explanation. You should still keep the \u2212 Examples \u2212 section.\\n\\nTakeaways\\n\\n\u2022 Removing made up people, places, and things from the prompt improved the quality of code generated.\\n\\n\u2022 The optimal summarization depends on the difficulty of the problem.\\n\\n\u2022 Synthetically generate summaries were close to maintaining accuracy.\\n\\n\u2022 With more rigorous instructions, human summaries could be made with less noise which would further improve synthetic summary generation.\\n\\nO Superfluous Information Confusing the Model\\n\\nHere is an example of an interview level string problem where the original prompt got 0% and both human generated summaries got 100% accuracy.\\n\\nThe question wants you to write code that will return the number of unique character in the given string.\\n\\nO.1 Original Prompt\\n\\nYou have initially a string of N characters, denoted by A1,A2...AN. You have to print the size of the largest subsequence of string A such that all the characters in that subsequence are distinct.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A subsequence of string A is a sequence that can be derived from A by deleting some elements and without changing the order of the remaining elements.\\n\\n\u2014\u2013Input\u2014\u2013 First line contains T, number of testcases. Each testcase consists of a single string in one line. Each character of the string will be a small alphabet (ie. 'a' to 'z').\\n\\n\u2014\u2013Output\u2014\u2013 For each testcase, print the required answer in one line.\\n\\n\u2014\u2013Constraints\u2014\u2013\\n- 1 \u2264 T \u2264 10\\n- Subtask 1 (20 points): 1 \u2264 N \u2264 10\\n- Subtask 2 (80 points): 1 \u2264 N \u2264 10^5\\n\\n\u2014\u2013Example\u2014\u2013\\nInput:\\n2\\nabc\\naba\\n\\nOutput: 3\\n2\\n\\n\u2014\u2013Explanation\u2014\u2013 For first testcase, the whole string is a subsequence which has all distinct characters. In second testcase, we can delete last or first 'a' to get the required subsequence.\"}"}
{"id": "emnlp-2022-main-301", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Write a code to print the average of the multiplication of a given number \\\\( N \\\\) with \\\\( N-1 \\\\) integer.\\n\\n\u2014\u2013Input:\u2014\u2013\\n- First-line will contain \\\\( T \\\\), the number of test cases.\\n- Then the test cases follow. - Each test case contains a single line of input, \\\\( N \\\\).\\n\\n\u2014\u2013Output:\u2014\u2013\\nFor each test case, output in a single line answer as displayed on the screen.\\n\\n\u2014\u2013Constraints\u2014\u2013\\n- \\\\( 1 \\\\leq T \\\\leq 10^6 \\\\)\\n- \\\\( 1 \\\\leq N \\\\leq 10^6 \\\\)\\n\\n\u2014\u2013Sample Input:\u2014\u2013\\n1\\n7\\n\u2014\u2013Sample Output:\u2014\u2013\\n21\"}"}
{"id": "emnlp-2022-main-301", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: The left is the code generated by the expert summary. The right is the code generated by the original prompt.\"}"}
