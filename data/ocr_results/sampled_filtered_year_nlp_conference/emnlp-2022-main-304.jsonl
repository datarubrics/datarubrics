{"id": "emnlp-2022-main-304", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ExPUNations: Augmenting Puns with Keywords and Explanations\\nJiao Sun1\u2217\u2020 Anjali Narayan-Chen2\u2020 Shereen Oraby2 Alessandra Cervone2 Tagyoung Chung2 Jing Huang2 Yang Liu2 Nanyun Peng2,3\\n1University of Southern California\\n2Amazon Alexa AI\\n3University of California, Los Angeles\\njiaosun@usc.edu\\n{naraanja,orabys,cervon,tagyoung,jhuangz,yangliud}@amazon.com\\nvioletpeng@cs.ucla.edu\\n\\nAbstract\\nThe tasks of humor understanding and generation are challenging and subjective even for humans, requiring commonsense and real-world knowledge to master. Puns, in particular, add the challenge of fusing that knowledge with the ability to interpret lexical-semantic ambiguity. In this paper, we present the ExPUNations (ExPUN) dataset, in which we augment an existing dataset of puns with detailed crowdsourced annotations of keywords denoting the most distinctive words that make the text funny, pun explanations describing why the text is funny, and fine-grained funniness ratings. This is the first humor dataset with such extensive and fine-grained annotations specifically for puns. Based on these annotations, we propose two tasks: explanation generation to aid with pun classification and keyword-conditioned pun generation, to challenge the current state-of-the-art natural language understanding and generation models' ability to understand and generate humor. We showcase that the annotated keywords we collect are helpful for generating better novel humorous texts in human evaluation, and that our natural language explanations can be leveraged to improve both the accuracy and robustness of humor classifiers.\\n\\n1 Introduction\\nHumor serves multiple purposes and provides numerous benefits, such as relieving anxiety, avoiding painful feelings and facilitating learning (Buxman, 2008). As a specific example of humor, the creative uses of puns, wordplay and ambiguity are important ways to come up with jokes (Chiaro, 2006). Pun understanding and generation are particularly challenging tasks because they require extensive commonsense and world knowledge to compose and understand, even for humans. Despite growing interest in the area, there are limited amounts of data available in the domain of humor understanding and generation.\\n\\nExisting humor datasets are usually only annotated with binary labels indicating whether each sentence is a joke, pun, or punchline (Hasan et al., 2019; Weller and Seppi, 2019; Castro et al., 2018; Mittal et al., 2021). This is insufficient to benchmark models' ability to understand and generate novel humorous text, since hardly anything meaningful can be learned from such a sparse supervision signal and coarse-grained annotation.\\n\\nTo facilitate research on humor understanding and generation, we present the ExPUNations (ExPUN) dataset, in which we augment an existing dataset of puns from SemEval 2017 Task 7 (Miller et al., 2017) with detailed crowdsourced annotations of fine-grained funniness ratings on a Likert scale of one to five, along with keywords denoting the most distinctive words that make the text funny and natural language explanations describing why the text is funny (Table 1). In addition, we collect annotations indicating whether a person understands the sentence, thinks it is a pun, and finds\\n\\n| Text | Keywords | Natural Language Explanation |\\n|------|----------|-----------------------------|\\n| When artists dream in color it's a pigment of their imagination. | artists, dream, color, pigment, imagination. | Pigments are non-soluble materials often used in painting, and pigment sounds like figment, which is something that is not real but someone believes it is. |\\n| The man found something to catch fish, which was a net gain. | catch fish, net gain. | This is a play on words. A \u201cnet gain\u201d means an increase in revenue but here \u201cnet\u201d refers to how a net is used to catch fish. |\\n\\nTable 1: Two examples of annotated Keywords (KWD) and Natural Language Explanations (NLEx) for puns in our dataset. The highlighted texts are annotated keywords that contribute to making the text funny.\\n\\n...addition, we collect annotations indicating whether a person understands the sentence, thinks it is a pun, and finds...\"}"}
{"id": "emnlp-2022-main-304", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Be True to your teeth, or they will be false to you.\\n\\nDrinking too much of a certain potent potable may require a leave of absinthe.\\n\\nUnderstandable\\n\\nIs a joke?\\n\\nFunniness (1-5)\\n\\nNatural Language Explanation (NLEx)\\n\\nNLEx1: Talking about being true as in being real or they will be fake/false teeth.\\n\\nNLEx2: False teeth are something people who lose their teeth may have, and being true to your teeth may be a way of saying take care of them otherwise you\u2019ll lose them.\\n\\nNLEx1: It\u2019s a pun that replaces the word absence with absinthe, which is notoriously strong alcohol.\\n\\nNLEx2: This is a play on words. Absinthe here represents the liquor by the same name but is meant to replace the similar-sounding \u201cabsence\u201d. Too much absinthe will make you ill.\\n\\nJoke keywords (KWD)\\n\\nKWD1: [\u201ctrue\u201d, \u201cteeth\u201d, \u201cfalse\u201d]\\n\\nKWD2: [\u201cbe true\u201d, \u201cteeth\u201d, \u201cfalse to you\u201d]\\n\\nKWD1: [\u201cdrinking\u201d, \u201cleave of absinthe\u201d]\\n\\nKWD2: [\u201cdrinking too much\u201d, \u201cleave of absinthe\u201d]\\n\\n2 ExPUN Dataset\\n\\nIn this section, we describe our data annotation procedure, including details of the annotation fields and our assessment of the annotation quality.\\n\\n1 Resources will be available at: https://github.com/amazon-research/expunations\\n\\n2 https://alt.qcri.org/semeval2017/task7/. The data is released under CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/legalcode).\\n\\n2.1 Data Preparation\\n\\nThe original SemEval 2017 Task 7 dataset (Miller et al., 2017) contains puns that are either homographic (exploiting polysemy) or heterographic (exploiting phonological similarity to another word). The dataset also contains examples of non-pun text. We sample 1,999 text samples from SemEval 2017 Task 7 as the basis for our humor annotation.\\n\\n2.2 Dataset Annotation\\n\\nThe annotated fields (AF) come in the order of:\\n\\nAF1 [understandability]: whether the annotator understands the text or not, regardless of whether they perceive it as funny.\\n\\nAF2 [offensiveness]: whether the annotator finds the text offensive or inappropriate.\\n\\nAF3 [joke]: whether the annotator thinks the text is intended to be a joke.\\n\\nAF4 [funniness]: rate the funniness on a Likert scale of 1-5, where 1 means very not funny and 5 means very funny.\\n\\nAF5 [explanation]: explain in concise natural language about why this joke is funny. More specifically, if external or commonsense knowledge is required to understand the joke and/or its humor, the annotator should include the relevant knowledge in the explanation. If the joke is a pun or play on words, they must provide an explanation of how the play on words works.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"pick out (as few as possible) keyword phrases from the joke that are related to the punchline/the reason the joke is funny. We emphasize that phrases should be sparse and mainly limited to content words, can be multiple words long, and the keywords should be copied verbatim from the joke.\\n\\nIf an annotator rates the instance as not understandable, they will skip the rest of the annotation (AF2\u2013AF6). In addition, if an annotator rates an example as not a joke, they can skip the rest of the annotation (AF4\u2013AF6). Table 2 shows two examples in our dataset. The first example has two annotators who think the text is a joke, and therefore it has two explanations. In the second instance, all annotators unanimously agree it is a joke. Here, we sample two explanations from the original five. For both instances, we use underline to highlight the external commonsense knowledge in the explanation. If the joke is a play on words, the explanation also shows how the play on words works (e.g., the second joke). We show the full annotation guidelines, including calibrating examples, in Appendix A.\\n\\nWe crowdsourced 5 annotations per sample using a professional team of 10 dedicated full-time annotators within our organization. Before starting the task, we held a kick-off meeting with the team to explain the annotation guidelines in detail. We then conducted 3 pilot rounds for calibration and iteratively met with annotators, including more details and examples to address annotator questions. Finally, we conducted 7 rounds of annotation, each with between 100-300 puns per round grouped into minibatches of 50 examples. Each sample in a minibatch was annotated by consistent subteams of 5 annotators. After receiving a completed batch of annotations, we manually examined their quality and provided feedback on any quality issues, redoing batches as necessary.\\n\\n2.3 Dataset Statistics and Quality Control\\nWe report overall dataset statistics in Table 3. For AF1\u2013AF3, we count the number of samples labeled positive by majority vote. For AF4, we compute the average of all funniness scores, excluding blank annotations, and find that while annotators recognized most samples as jokes, they did not find them to be particularly funny. For AF5 and AF6, we report lexical statistics of our explanations and keyword annotations and provide deeper analysis of these key annotation fields in Section 2.4.\\n\\nWe report inter-annotator agreement for all annotation fields in Table 4. For fields AF1\u2013AF4, we compute agreement using (1) the average of Cohen\u2019s kappa scores of each annotator against the majority vote, and (2) the average Spearman correlation between each pair of annotators. We find that annotators show moderate agreement when deciding if the given text is a joke (AF3), but lower agreement on the task of understanding the text (AF1) as well as the much more subjective task of rating how funny a joke is (AF4). We also find weak average Spearman correlation between each pair of annotations for the subjective categories of offensiveness (AF2), whether the text is a joke (AF3) and joke funniness (AF4).\\n\\nFor the free text fields in AF5 and AF6, we compute averaged BLEU-4 (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) scores in a pairwise fashion. We treat each annotator\u2019s explanation (for AF5) or list of keyword phrases joined into a string (for AF6) as candidate text, with the remaining annotators\u2019 annotations as a set of references. We find high similarity between joke keyword annotations, suggesting that annotators identify similar spans of keyword phrases, and a lower degree of similarity between pun explanations.\\n\\n2.4 Dataset Analysis\\nExplanations. As seen in Figures 1a and 1b, on average, samples are annotated with multiple explanations, and the explanations are lengthy, spanning multiple sentences, and lexically diverse (14,748 words).\"}"}
{"id": "emnlp-2022-main-304", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Agreement stats for annotated fields in the ExPUN dataset. We report averaged Cohen\u2019s \\\\(\\\\kappa\\\\) and Spearman\u2019s \\\\(\\\\rho\\\\) for numeric ratings (\\\\(AF_1 - AF_4\\\\)), and averaged BLEU-4 and METEOR for text fields (\\\\(AF_5 - AF_6\\\\)).\\n\\ntoken vocabulary size, with 210,580 tokens overall). Figure 3 in Appendix B shows the distribution of the top 50 most frequent content-words in our explanations. The frequent use of usually and often indicate the explanation of commonsense knowledge, e.g., thunder and lightning are usually present in a weather storm or \u201cpain\u201d means physical discomfort often felt by a hospital patient. The most frequent words, means and word, indicate that annotators frequently provide word sense information as part of their explanations, while sounds frequently appears in explanations of heterographic puns. Each of these most frequent words comprise less than 2.8% of all tokens in the explanations, illustrating the rich diversity of our corpus.\\n\\nKeywords.\\n\\nAs seen in Figures 1c and 1d, on average, keyword phrases in ExPUN, which are derived from the original puns, are short and sparse (5,497 token vocabulary size, with 27,820 tokens overall). This follows from our guidelines to annotate keywords concisely, focusing mainly on content words that are essential to understanding the joke. Table 5 shows two examples of pun keyword annotations in our dataset that showcase different annotation styles among annotators. For instance, one annotator may tend to select wordy keyword phrases that introduce unnecessary tokens, while another may omit salient keywords that other annotators mention. Aggregating these annotations among annotators to construct a single ground truth set of keyword phrases is therefore challenging because of differing annotation styles. The problem of merging keywords is further complicated because the keywords from different annotators are often not aligned well, as different annotators may annotate varying numbers of keyword phrases and different spans. Taking these considerations into account, we propose a keyword aggregation algorithm to address these issues and construct a single set of aggregated keywords per sample.\\n\\nKeywords Aggregation.\\n\\nAlgorithm 1 in Appendix C describes our keyword aggregation method. The algorithm aims to generate a comprehensive list of concise keywords for each sample. First, we compute a reliability score for each annotation, defined as the average of \\\\((\\\\# \\\\text{ keyword phrases} - \\\\# \\\\text{average tokens in each keyword phrase})\\\\). The higher the score, the more comprehensive and concise the keywords from an annotator should be. We choose the annotator with the highest score to be the anchor. We note, however, that keyword annotations are not always error-free; e.g., in the first example of Table 5, w\\\\(_4\\\\) has an incorrect word (fancy chairs instead of royal chairs). Therefore, for each keyword phrase, we compute the fuzzy matching score between the anchor\u2019s annotation with the rest of annotators\u2019 annotations. For each annotator, we keep the keyword phrase that has the highest fuzzy matching score with the anchor annotator\u2019s, with a minimum threshold score of 60. This process produces a filtered keyword list where each of the remaining keyword phrases look similar to the anchor\u2019s. Then, we compute the average fuzzy matching score between the anchor\u2019s keyword phrase and each element in the filtered keyword list. We then choose the annotator with this average score to be the final set of aggregated keywords.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Royal chairs are rarely throne out. She didn't marry the gardener. Too rough around the hedges.\\n\\nTable 5: Keyword annotations from different workers. A shows aggregated keywords from our algorithm.\\n\\n3 Experiments\\n\\nWith the collected annotations, we propose two new tasks, pun explanation and keyword conditioned pun generation, to showcase novel tasks that our dataset uniquely enables and push the frontiers of NLU and NLG for humor. Note that the rich annotations in ExPUN can also enable many other interesting tasks such as pun keywords extraction, fine-grained funniness prediction, and others. However, we prioritize NLG tasks as they are relatively under-explored compared to NLU tasks. In this section, we benchmark current state-of-the-art models\u2019 performance on the proposed tasks.\\n\\n3.1 Pun Explanation\\n\\nThe task of pun explanation takes a pun sentence as input and outputs a natural language explanation of why the pun is funny. This requires extensive understanding of background and commonsense knowledge. We hypothesize that existing NLP models would struggle to generate high-quality explanations for puns. On the other hand, high-quality explanations can improve humor understanding, and thus help tasks such as humor classification.\\n\\nFormally, given text $T$, our target is to generate an explanation $E_T$ why $T$ is funny. Additionally, we use the explanations to support the task of pun classification, where, given $T$ (and optionally an explanation $E_T$), we output whether $T$ is a joke.\\n\\nData Preparation.\\n\\nFor each data sample, we use the longest human-written explanation from ExPUN ($AF^5$), substituting in the pun text if no explanations exist. For pun classification, we assign output labels using the majority vote of $AF^3$ (is a joke). For both tasks, we split our dataset into 1,699/100/200 for train/dev/test. Dev and test contain an equal distribution jokes to non-jokes, while training contains 1,299 jokes and 400 non-jokes.\\n\\nEvaluation Metrics.\\n\\nWe do not report lexical overlap metrics as our primary evaluation metric for generated explanations because these are not suited for measuring plausibility (Camburu et al., 2018; Kayser et al., 2021; Clinciu et al., 2021) or faithfulness of explanations (Jacovi and Goldberg, 2020). Rather, we follow prior work and use the \\\"simulatability score\\\" metric from Wiegreffe et al. (2021) to measure explanation quality from the lens of usability of the explanation. It reflects the utility of explanations by measuring the improvement in task performance when explanations are provided as additional input vs. when they are not: $acc(IE \\\\rightarrow O) - acc(I \\\\rightarrow O)$, where $I$ denotes the input text, $E$ is the explanation and $O$ is the classification of whether $I$ is a joke. We evaluate how useful explanations can be by measuring the performance increase of $acc(IE \\\\rightarrow O)$ as we increase the ratio of samples with explanations in the training data, and report $acc(I \\\\rightarrow O)$ as a constant baseline that uses no explanations.\\n\\nModels.\\n\\nWe use the following model variations:\\n\\n1. No explanations. As a baseline, we finetune BERT-base (Devlin et al., 2019), RoBERTa-base (Liu et al., 2019) and DeBERTa-base (He et al., 2021) to classify whether the given text is a joke without any explanations in the input.\\n\\n2. Gold explanations. To find the upper bound of how useful explanations can be, we augment the input to the above baseline models with gold human-annotated explanations in both training and testing. The majority of non-punny examples (identified as unfunny by majority vote and thus labeled as unfunny) contain at least one explanation from an annotator who marked it as funny. In these cases, only 168 samples have no annotated explanations.\\n\\nFurther experimental details in Appendix D.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The impact of using human-written (2a) and model-generated explanations (2b and 2c) vs. no explanations on pun classification accuracy. All reported numbers are computed with three-seed average. For each data point, we train a model on the full dataset, but only provide explanations for a given percentage, as shown on the x-axis.\\n\\nWe use any provided explanations as E, both in training and in testing with gold explanations. Otherwise, to construct training examples that have no annotated explanations, or where explanations are held out, we try two variants: (1) representing the missing explanation as an empty string (\\\"w/ gold expl.\\\"), or (2) randomly sampling a negative explanation from another annotated example to use as input (\\\"w/ gold + sampled neg.\\\").\\n\\nGenerated explanations. Following previous work on explanation generation (Wiegreffe et al., 2021), we first finetune a T5 (Raffel et al., 2020) model to generate pun explanations given pun sentences as input. For text that contains no annotated explanations, we use the pun sentence itself as the output explanation. We then use gold human-annotated explanations to train and T5-generated explanations to test the explanation-augmented classification models.\\n\\nELV (Zhou et al., 2020a). ELV is a probabilistic framework for text classification where natural language explanations are treated as latent variables. Two modules, an explanation generation module and an explanation-augmented prediction module are jointly trained using a variational EM framework. As another baseline, we train an ELV model for pun classification using the ExPUN dataset.\\n\\nResults. We show our results on the pun classification task in Figure 2. Baseline performance of the no explanations models are shown using constant dotted lines. Figure 2a shows the upper bound of performance improvement when models are provided with gold explanations, indicating that human-written explanations are useful for this task, and that including more gold explanations in training data generally helps. In particular, adding randomly-sampled negative explanations (\\\"w/ gold + sampled neg.\\\") further improves the classification accuracy, showing the utility of our collected explanations in improving model performance. However, Figure 2b shows that using generated explanations at test time does not help to improve classification accuracy. Using the more carefully-designed ELV framework to jointly train the generation and classification modules shows improvement in classification accuracy (Figure 2c); however, qualitative analysis of the ELV explanations showed that many generated outputs are not fluent natural language, suggesting that performance improvements may stem more from modeling improvements as opposed to explanations. Given the huge improvements we see when incorporating gold explanations during test, we note explanations are clearly highly valuable if the quality of generated explanations can be improved.\\n\\nTable 6 shows examples of T5-generated explanations for given puns. Qualitative analysis shows that generated explanations often identify the relevant pun word, and can include somewhat accurate word sense information for one sense of the pun. However, the model usually fails to explain the alternate word sense and its relation, which is crucial to understanding the wordplay. The model especially fails to explain phonological similarity in heterographic puns; e.g., in the first three examples, explanations fail to mention alternate words carry, whet and humor. For both pun types, our model can devolve into repetitively copying words from the input. Our results exhibit the challenge of generating good pun explanations and that high-quality explanations are useful for understanding humor.\\n\\n3.2 Keyword-Conditioned Pun Generation\\n\\nThe task of keyword-conditioned pun generation takes human-annotated pun keywords as input and...\\n\"}"}
{"id": "emnlp-2022-main-304", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"My name is Cary. I'm a porter.\\nThe joke is a pun on the word \\\"cary\\\". A porter is someone who transports goods.\\n\\nFishers often wet their appetite.\\nThis is a play on words. The word \\\"wet\\\" means to wet your appetite, which is a characteristic of fish.\\n\\nA gossip is someone with a great sense of rumor.\\nThis is a play on words. The word \\\"rumor\\\" sounds like \\\"rumor\\\". A gossip is someone who has a great sense of rumor.\\n\\nOil executives are always using crude language.\\nThe joke is a pun on the word \\\"crude\\\". Crude language is used to describe crude oil, which is a type of petroleum product.\\n\\nPlease mix me a martini, said Tom, dryly.\\nThis is a play on words. The word \\\"dryly\\\" means dryly, but \\\"dryly\\\" sounds like \\\"dryly\\\".\\n\\nTable 6: Pun explanations generated by the T5 model.\\nWe use underline to indicate the pun word in the input. Produces novel puns as output. This benchmarks models' capability to draw connections among words to generate novel fluent, sensible, and humorous texts. This is a challenging task with many downstream applications, such as context-situated humor generation, a task that involves generating humorous text in a given situation or context. In this case, input keywords can come from conversational context (e.g., chatbot dialogues) or narrative context (e.g., creative short stories).\\n\\nMore formally, we take as input keywords $K$, the pun word $p_w$ and alternate pun word $a_w$, and produce novel and fluent puns that incorporate the keywords. Optionally, we also include pun word sense annotations $S_{p_w}$ and $S_{a_w}$ from the original SemEval 2017 Task 7 annotations.\\n\\nData Preparation.\\nFor this task, we limit our data to samples that contain both (1) annotated human keywords $K$ from ExPUN ($A^F_6$), and (2) pun word sense annotations $S_{p_w}$ and $S_{a_w}$ from SemEval 2017 Task 7. There are 1,482 such samples that have both annotations, from which we reserve 100 as test data and use the rest for model training. To construct input human-annotated keywords for this task, we aggregate keywords for each sample using the method described in Section 2.4. Additionally, we evaluate the effect of finetuning on automatically-extracted keywords instead of human-annotated keywords by automatically extracting keywords for each sample by running the RAKE (Rose et al., 2010) algorithm on the pun text.\\n\\nEvaluation Metrics.\\nWe use both automatic metrics and human evaluation to evaluate the quality of generated puns. For automatic evaluation, we calculate word incorporation rate for both pun words and keywords, which measure the model's ability to incorporate all input keywords. Additionally, we run human evaluation using Amazon Mechanical Turk, in which we asked Turkers to label whether or not a given generated pun was successful.\\n\\nModels.\\nWe use the following models: AmbiPun (Mittal et al., 2022). We use the current state-of-the-art homographic pun generation model, AmbiPun, with no further finetuning. We follow the AmbiPun prompt format: \\\"generate sentence: $K, p_w, a_w$\\\".\\n\\nFine tuned T5 (T5FT). We finetune T5-base on ExPUN using input prompt \\\"generate a pun that is situated in $K$, using the word $p_w$, $p_w$ means $S_{p_w}$, $a_w$ means $S_{a_w}$.\\\" The output is the pun itself.\\n\\nFine tuned T5 with pretraining (T5PT+FT). To increase the model's ability to incorporate keywords, we pretrain T5 on non-pun text. For a given pun word, we first extract 200 sentences that contain the pun word from BookCorpus (Zhu et al., 2015), then use RAKE to automatically extract keywords for each sentence. We construct examples where inputs are automatically extracted keywords, and outputs are sentences from BookCorpus including pun words. We pretrain a T5 model with this data before finetuning it on ExPUN.\\n\\nResults.\\nTable 7 shows results of our pun generation models. While the AmbiPun baseline achieves superior word incorporation performance, our T5PT+FT model finetuned using ExPUN keywords generates successful puns at a higher rate, showing the value of training on our dataset. Furthermore, while pun word incorporation is improved, the success rate increases even further.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Key-Word Incorp. % Success\\n\\nTable 7: Automatic (Word Incorporation Rate) and human evaluation (Success %) of puns generated by models finetuned using automatically-extracted (RAKE) and human-annotated (ExPUN) keywords (with AmbiPun baseline (Mittal et al., 2022)).\\n\\nPT stands for Pre-Training and FT stands for Fine-Tuning. Both T5PT+FT models finetuned with RAKE-based keywords or ExPUN-based keywords use RAKE-based keywords during pretraining.\\n\\nOur results show pun generation is a very challenging task, and that careful selection of pun keywords and a deeper understanding of humor in wordplay is essential for generating puns successfully.\\n\\nTable 8 shows examples of generated puns from our ExPUN-T5PT+FT model. The model is able to generate both homographic and heterographic puns somewhat coherently using one of the pun word senses. However, while some puns are successful, Rows 3 and 6 show some ways our model can struggle to generate the respective pun types: it does not always incorporate the alternate word sense in a clever or meaningful way, and can stitch copied input keywords together into incoherent sentences.\\n\\nOur keyword-conditioned pun generation task encourages models to focus more on the linguistic structures via pun keywords as we observe that human-extracted keywords usually reflect the ambiguity and distinctiveness principles as discussed in Kao et al. (2016). The keyword-conditioned pun generation setup can also facilitate more engaging pun generation scenarios such as context-situated pun generation (Sun et al., 2022).\\n\\n4 Related Work\\n\\nIn this work, we contribute annotations for a humor dataset as well as two humor-related generation tasks. The work is broadly related to pun generation, pun detection, explanation generation, and humor generation. We briefly summarize works in these directions.\\n\\nPun generation.\\n\\nMany of the previous works on pun generation have focused on phonological or syntactic patterns rather than semantic patterns (Miller and Gurevych, 2015; Hong and Ong, 2009; Petrovi\u0107 and Matthews, 2013; Valitutti et al., 2013), thus lacking flexibility. He et al. (2019) make use of local-global surprisal principle to generate homophonic puns and Yu et al. (2020) uses constrained lexical rewriting for the same task. Hashimoto et al. (2018) use a retrieve and edit approach to generate homographic puns and Yu et al. (2018); Luo et al. (2019) propose complex neural model architectures such as constrained language model and GAN. Mittal et al. (2022) generate homographic puns given a polyseme and try to incorporate the multiple senses of the polyseme. Tian et al. (2022) proposed a unified framework to generate both homographic and homophonic puns leveraging humor principles.\\n\\nOur keyword-conditioned pun generation task encourages models to focus more on the linguistic structures via pun keywords as we observe that human-extracted keywords usually reflect the ambiguity and distinctiveness principles as discussed in Kao et al. (2016). The keyword-conditioned pun generation setup can also facilitate more engaging pun generation scenarios such as context-situated pun generation (Sun et al., 2022).\\n\\nHumor generation.\\n\\nWith the recent advent of diverse datasets (Hasan et al., 2019; Mittal et al., 2021; Yang et al., 2021), it has become easier to detect and generate humor. While large pre-trained models have become fairly successful at detection, humor generation still remains an unsolved problem. Therefore, humor generation is usually studied in a specific setting. Petrovi\u0107 and Matthews (2017) proposed a method to generate humor using a neural network architecture. However, humor generation remains an active area of research with ongoing efforts to develop more effective methods.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(2013) generates jokes of the type 'I like my X like\\nI like my Y, Z'. Garimella et al. (2020) develop a\\nmodel to fill blanks in a Mad Libs format to gener-\\nate humorous sentences and Yang et al. (2020) edit\\nheadlines to make them funny. More research is\\nrequired to generate humorous sentences that are\\nnot constrained by their semantic structure.\\n\\nNatural language explanation generation.\\nCollecting and utilizing natural language explanations\\nto help various NLP tasks is an emerging topic.\\nThe earliest work by Ling et al. (2017) collected\\nnatural language justifications, called rationales, to\\nhelp solve math problems. However, their setup\\nis limited to solving math problems given how\\ntheir rationales and models were structured. Jansen\\net al. (2018) compose a dataset of explanation\\ngraphs for elementary science questions to support\\nmulti-hop inference. Like Ling et al. (2017), they\\nemphasize the explanation structures. Several\\nworks have introduced large-scale datasets of natu-\\nral language explanations for the natural language\\ninference (NLI) (Camburu et al., 2018; Kumar and\\nTalukdar, 2020), commonsense reasoning (Rajani\\net al., 2019), and hate speech detection (Mathew\\net al., 2021) tasks. However, there are no existing\\ndatasets or models that focus on explaining humor,\\nwhich is a challenging task that involves common-\\nsense and world knowledge.\\n\\nPun detection.\\nBeing able to detect puns can be\\nan essential step to generating them. SemEval 2017\\nTask 7 (Miller et al., 2017) introduced the chal-\\nlenge of pun detection, location detection and sense\\ninterpretation for homographic and heterographic\\npuns. They also released a dataset which has be-\\ncome the backbone of our and several other related\\nworks. Diao et al. (2019) make use of gated atten-\\ntion networks to detection heterographic puns. Zou\\nand Lu (2019) introduce a tagging scheme to jointly\\ndetect and locate puns, and apply this approach to\\nboth heterographic and homographic puns. Zhou\\net al. (2020b) jointly model contextual and phono-\\nlogical features into a self-attentive embedding in\\ntheir approach for pun detection and location tasks.\\n\\n5 Conclusion\\nIn this paper, we contribute a dataset of extensive,\\nhigh-quality annotations of humor explanation, key-\\nwords, and fine-grained funniness ratings. This is\\nthe first humor dataset with such extensive and fine-\\ngrained annotations. Based on the annotations, we\\npropose two tasks: pun explanation and keyword-\\nconditioned pun generation, to challenge state-of-\\nthe-art natural language understanding and genera-\\ntion models' ability to understand and generate hu-\\nmorous text. We benchmark several strong models'\\nperformances on the two proposed tasks to validate\\nthe practical usage of the proposed annotations, and\\nshow that our human-annotated explanations and\\nkeywords are beneficial in understanding and gen-\\nerating humor. Future directions include a deeper\\nanalysis of how to characterize pun explanation\\nmore objectively within our annotation scheme, as\\nwell as further exploration of better models for both\\nthe pun explanation and pun generation tasks.\\n\\nAcknowledgements\\nThe authors would like to thank Scott Benson and\\nthe rest of the Alexa Data Services Rapid Machine\\nLearning Prototyping (RAMP) team for all of their\\nhelp with preparing and performing the annotation\\ntask. We also thank anonymous reviewers for their\\nconstructive feedback and suggestions that helped\\nimprove the paper.\\n\\nLimitations\\nThis work focuses on understanding and generation\\nof puns, a single and very specific form of humor-\\nous language. We hope that our annotation schema\\nand methods can be used in the future to extend to\\nother forms of humor, e.g., joke generation. Ad-\\nditionally, we acknowledge that humor is a highly\\nsubjective area, i.e., what might be perceived as\\nhumorous may differ greatly from one person to\\nanother depending on their unique backgrounds\\nand experiences. We hope this work can be used\\nas an initial framework to begin characterizing hu-\\nmor through human-written explanations, such that\\nit can be used more broadly to give insight into\\nwhat contributes to humorous content for different\\nindividuals and groups.\\n\\nFinally, since we use pretrained language models\\nfor our generation tasks, we note that this makes\\nour models susceptible to generating biased or sen-\\nsitive content. While we do not explicitly address\\nconcerns around bias/sensitive content within our\\nframework to date, we aim to incorporate these con-\\nsiderations into pun generation as we develop new\\nmodels, including methods to filter our inputs and\\ngenerated data for toxicity and biased references\\nthat may be deemed offensive.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We hereby acknowledge that all of the co-authors of this work are aware of the provided ACL Code of Ethics and honor the code of conduct. The text in the dataset (puns and non-pun text) is from the SemEval 2017 Task 7 dataset (Miller et al., 2017) including jokes, aphorisms, and other short texts sourced from professional humorists and online collections. No user data from commercial voice assistant systems is used. We collect the human annotation of pun keywords, explanations, and other meta-fields via full-time employees (with all employee-entitled fringe benefits) who are hired by the co-authors' organization for the purposes of data annotation and are not co-authors of the paper. We ensure that all the personal information of the workers involved (e.g., usernames, emails, urls, demographic information, etc.) is discarded in our dataset. Overall, we ensure our pay per task is above the annotator's local minimum wage (approximately $15 USD / Hour).\\n\\nReferences\\n\\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65\u201372, Ann Arbor, Michigan. Association for Computational Linguistics.\\n\\nKaryn Buxman. 2008. Humor in the OR: A stitch in time? AORN journal, 88(1):67\u201377.\\n\\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 9539\u20139549. Curran Associates, Inc.\\n\\nSantiago Castro, Luis Chiruzzo, Aiala Ros\u00e1, Diego Garat, and Guillermo Moncecchi. 2018. A crowd-annotated Spanish corpus for humor analysis. In Proceedings of the Sixth International Workshop on Natural Language Processing for Social Media, pages 7\u201311, Melbourne, Australia. Association for Computational Linguistics.\\n\\nDelia Chiaro. 2006. The language of jokes: Analyzing verbal play. Routledge.\\n\\nMiruna-Adriana Clinciu, Arash Eshghi, and Helen Hastie. 2021. A study of automatic metrics for the evaluation of natural language explanations. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2376\u20132387, Online. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nYufeng Diao, Hongfei Lin, Liang Yang, Xiaochao Fan, Di Wu, Dongyu Zhang, and Kan Xu. 2019. Het-erographic pun recognition via pronunciation and spelling understanding gated attention network. In The World Wide Web Conference, page 363\u2013371.\\n\\nAparna Garimella, Carmen Banea, Nabil Hossain, and Rada Mihalcea. 2020. \\\"judge me by my size (noun), do you?\\\" YodaLib: A demographic-aware humor generation framework. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2814\u20132825, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nMd Kamrul Hasan, Wasifur Rahman, AmirAli Bagher Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe Morency, and Mohammed (Ehsan) Hoque. 2019. UR-FUNNY: A multimodal language dataset for understanding humor. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2046\u20132056, Hong Kong, China. Association for Computational Linguistics.\\n\\nTatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S. Liang. 2018. A retrieve-and-edit framework for predicting structured outputs. In NeurIPS, pages 10073\u201310083.\\n\\nHe He, Nanyun Peng, and Percy Liang. 2019. Pun generation with surprise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1734\u20131744, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. DeBERTa: Decoding-enhanced BERT with disentangled attention. In International Conference on Learning Representations.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4198\u20134205, Online. Association for Computational Linguistics.\\n\\nPeter Jansen, Elizabeth Wainwright, Steven Mar-morstein, and Clayton Morrison. 2018. WorldTree: A corpus of explanation graphs for elementary science questions supporting multi-hop inference. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nJustine T Kao, Roger Levy, and Noah D Goodman. 2016. A computational model of linguistic humor in puns. Cognitive science, 40(5):1270\u20131285.\\n\\nMaxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz. 2021. E-ViL: A dataset and benchmark for natural language explanations in vision-language tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1244\u20131254.\\n\\nSawan Kumar and Partha Talukdar. 2020. NILE: Natural language inference with faithful natural language explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8730\u20138742, Online. Association for Computational Linguistics.\\n\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-som. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158\u2013167, Vancouver, Canada. Association for Computational Linguistics.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\\n\\nFuli Luo, Shunyao Li, Pengcheng Yang, Lei Li, Baobao Chang, Zhifang Sui, and Xu Sun. 2019. Pun-GAN: Generative adversarial network for pun generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3388\u20133393, Hong Kong, China. Association for Computational Linguistics.\\n\\nBinny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021. HateXplain: A benchmark dataset for explainable hate speech detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14867\u201314875.\\n\\nTristan Miller and Iryna Gurevych. 2015. Automatic disambiguation of English puns. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 719\u2013729, Beijing, China. Association for Computational Linguistics.\\n\\nTristan Miller, Christian Hempelmann, and Iryna Gurevych. 2017. SemEval-2017 task 7: Detection and interpretation of English puns. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 58\u201368, Vancouver, Canada. Association for Computational Linguistics.\\n\\nAnirudh Mittal, Pranav Jeevan P, Prerak Gandhi, Diptesh Kanojia, and Pushpak Bhattacharyya. 2021. \\\"so you think you're funny?\\\": Rating the humour quotient in standup comedy. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10073\u201310079, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nAnirudh Mittal, Yufei Tian, and Nanyun Peng. 2022. AmbiPun: Generating humorous puns with ambiguous context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1053\u20131062, Seattle, United States. Association for Computational Linguistics.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\\n\\nSa\u0161a Petrovi\u0107 and David Matthews. 2013. Unsupervised joke generation from big data. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 228\u2013232, Sofia, Bulgaria. Association for Computational Linguistics.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather inne Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-304", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A ExPUN Dataset Annotation Details\\nA.1 Annotation Guidelines\\nBelow, we include the annotation guidelines we used to collect the ExPUN dataset. All pun texts in the provided examples are from the original SemEval 2017 Task 7 dataset (Miller et al., 2017).\\n\\n1. Guidelines\\n\\nYou will be provided a CSV file of short texts, one short text to be annotated per row. Each row contains the text content as well as columns for each of the requested annotations. For each row, read the text carefully, and provide the following annotations:\\n\\n1. Mark whether you understood the text with 0/1 (0 didn't understand, 1 understood the text).\\n   - If you don't understand the meaning of the text (regardless of whether or not it should be perceived as funny), rate the sample as 0 (didn't understand).\\n   - For this assessment, you can use a quick Google search to look up any vocabulary/terms you don't immediately understand. However, if the amount of research it would take to understand the text goes beyond a quick (<1 min) search, rate the sample as 0 (didn't understand).\\n   - Example text that was marked \u201cdon't understand\u201d (0): A doctor\u2019s mistakes go six feet under; a dentist\u2019s cover an acre.\\n   - If you rate this sample as 0 (didn't understand), skip the rest of the annotation for this sample.\\n\\n2. Mark whether you find the text offensive or inappropriate with 0/1 (0 not offensive, 1 offensive), meaning the text is racist or is biased against marginalized groups, or is generally offensive. If you rate this sample as 1 (is offensive), you may optionally skip the rest of the annotation for this sample.\\n\\n3. Mark whether you think the text is intended to be a joke with 0/1 (0 not a joke, 1 is a joke).\\n   - Text should be labeled as 1 (is a joke) even if it intends to be humorous, but falls flat or is a lame/bad joke.\\n   - Example text labeled 0 (not a joke): All that glistens is not gold.\\n   - Example text labeled 1 (is a joke): These are my parents, said Einstein relatively.\\n   - Why is this a joke? Though subtle, the text is a pun on the word \u201crelatively\u201d that associates Einstein with his relatives (parents) and his theory of relativity.\\n   - If you rate this sample as 0 (not a joke), skip the rest of the annotation for this sample.\\n\\n4. Rate funniness on a Likert scale of 1-5 (1 very not funny, 5 very funny).\\n   - Score of 1: A very not funny joke consists of a joke that is not funny at all, or tries to be funny but does not achieve the intended effect.\\n   - Example of Funniness 1 (not funny): These are my parents, said Einstein relatively.\\n   - Score of 3: An average joke consists of a joke that that is average and may elicit some chuckles (or groans) from you or others.\\n   - Example of Funniness 3 (average funniness): When they told him that his drum couldn't be fixed, it didn't resonate very well.\\n   - Score of 5: A very funny joke consists of a good joke that you find humorous and potentially would want to share/tell to others.\\n   - Example of Funniness 5 (very funny): Yesterday I accidentally swallowed some food coloring. The doctor says I'm OK, but I feel like I've dyed a little inside.\\n\\n5. Explain in concise natural language about why this joke is funny. If external or commonsense knowledge is required to understand the joke and/or its humor, please include the relevant knowledge in your explanation. If the joke is a pun or play on words, you must provide an explanation of how the play on words works.\\n   - Example joke: What do you use to cut a Roman Emperor's hair? Caesars.\\n   - Bad explanation: The joke is a play on words about Caesar and scissors.\\n   - Good explanation: The joke is a play on words about Caesar and scissors.\\n\\nThe data is released under CC BY-NC 4.0 license (https://creativecommons.org/licenses/by-nc/4.0/legalcode).\"}"}
{"id": "emnlp-2022-main-304", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Caesar was a Roman Emperor, and \u201cCaesars\u201d sounds like \u201cscissors\u201d, which is something you use to cut hair.\\n\\nExample joke:\\nThere was a kidnapping at school yesterday. Don\u2019t worry, though\u2014he woke up!\\n\\nBad explanation: The joke is a play on words about kidnapping \u2192 kid napping.\\n\\nGood explanation: The joke is a play on words. The word \u201ckidnapping\u201d implies that a kid was taken hostage at school, but \u201che woke up\u201d suggests that it was actually just a kid taking a nap instead.\\n\\n6. Pick out (as few as possible) keyword phrases from the joke that are related to the punchline/the reason of the joke being funny (written as a pipe-separated (|) list of phrases with spaces).\\n\\n\u2022 Phrases can be multiple words long.\\n\u2022 The keyword phrases should be copied verbatim from the joke (no need to reword them).\\n\u2022 Keep keyword phrases sparse and mainly limited to content words. The keyword phrases should not span the entire joke.\\n\u2022 As a general guideline, the words in keyword phrases should make up <50% of the words in the full joke (though this may be difficult to achieve for shorter jokes).\\n\\n\u2022 Example joke: I used to hate maths but then I realised decimals have a point.\\n\\nBad keywords (too dense):\\nI used to hate maths but | decimals have a point\\n\\nGood keywords:\\nmaths | decimals | point\\n\\nWe note that this is a highly subjective task, since different people perceive humor differently! We encourage you to do your best to determine how to annotate each item as consistently as possible.\\n\\nExample annotations (funniness ratings are subjective, and may differ from yours!):  \\n\u2022 Text: Yesterday I accidentally swallowed some food coloring. The doctor says I\u2019m OK, but I feel like I\u2019ve dyed a little inside.\\n  Understand: 1\\n  Offensive: 0\\n  Is a Joke: 1\\n  Funniness: 5\\n  Explanation: The joke is a pun. The main character feels they\u2019ve \u201cdied a little inside\u201d meaning they\u2019ve been changed for the worse by swallowing food coloring. At the same time, food coloring contains dye, so the main character has been \u201cdyed\u201d on the inside by swallowing some.\\n  Keywords: swallowed | food coloring | dyed a little inside\\n\\n\u2022 Text: Waiter, there\u2019s a fly in my soup! \u201cI know. It gives you a nice buzz doesn\u2019t it?\u201d\\n  Understand: 1\\n  Offensive: 0\\n  Is a Joke: 1\\n  Funniness: 2\\n  Explanation: This is both a pun and a reference to a common joke format. \u201cWaiter, there\u2019s a fly in my soup!\u201d is an old joke setup with varying punchlines. Flies make a noise commonly described as a \u201cbuzz\u201d. \u201cBuzz\u201d can be used as a noun referring to a pleasant heightened sensation, commonly from drinking alcohol.\\n  Keywords: fly | soup | buzz\\n\\n\u2022 Text: The evil onion had many lairs.\\n  Understand: 1\\n  Offensive: 0\\n  Is a Joke: 1\\n  Funniness: 3\\n  Explanation: This is a pun. An evil lair is a hideout for a villain in a comic book or show. Onions are layered vegetables. The joke is that the onion had many lairs because it was evil.\\n  Keywords: evil onion | many lairs\\n\\n\u2022 Text: Hope for the best, but prepare for the worst.\\n  Understand: 1\\n  Offensive: 0\\n  Is a Joke: 0\\n  (No need to fill in any more information in subsequent columns, as this text is not a joke.)\\n\\nAdditional calibrating examples The following examples were rated with an average Funniness rating \u2265 2 in previous pilot rounds and can be used to calibrate your rubric for assigning Funniness scores.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Drinking too much of a certain potent potable may require a leave of absinthe.\\n\\nAnimals that tunnel in the soil have to have an escape root.\\n\\nMy friend's bakery burned down last night. Now his business is toast.\\n\\nWhat is the best store to be in during an earthquake? A stationery store.\\n\\nA.2 Feedback from Pilot Rounds\\n\\nWe did a few pilot rounds to help annotators calibrate on funniness, since not only is funniness highly subjective, but also since many puns aren\u2019t \u201ctraditionally funny\u201d, but instead more humorous due to being \u201cclever\u201d or \u201ccreative\u201d. Feedback we received from annotators was mostly around including more detailed definitions and examples for highly-subjective criteria such as \u201cfunniness\u201d and \u201coffensiveness\u201d. We added questions on whether annotators \u201cunderstood the text\u201d to help distinguish between puns that were not understood vs. puns that were understood but then marked as \u201cnot funny\u201d, and added clarifying examples of \u201cjoke keywords\u201d to discourage excessive copying of the input text in the annotations.\\n\\nA.3 Inter-Annotator Agreement for Offensiveness ($\\\\text{AF}^2$)\\n\\nWe note relative low inter-annotator agreement for $\\\\text{AF}^2$, as identifying offensive/inappropriate content is a highly subjective and complex task, as it generally covers social stereotypes, biases, aggressive expressions, micro-aggressions, etc. Kappa looks at agreement of raw scores, while Spearman computes correlation of ranks. Combining these differences with the subjectivity of the task could explain the disparity between Kappa and Spearman scores for $\\\\text{AF}^2$.\"}"}
{"id": "emnlp-2022-main-304", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Sample explanation sentence templates collected in ExPUN, along with their frequencies. that can aid in the pun classification task (e.g., a detailed, contextualized definition of a word/phrase).\\n\\nC Keyword Aggregation Algorithm\\nWe propose the keyword aggregation algorithm in Algorithm 1 to merge keywords annotation among different workers.\\n\\nD Classifier Implementation Details\\nWe finetune pretrained language models for classifying whether given text samples are jokes, and we use HuggingFace (Wolf et al., 2020) throughout our implementation for accessing model checkpoints and modeling. For hyper-parameter search, we tried the combinations of learning rate \\\\({1 \\\\times 10^{-4}, 3 \\\\times 10^{-4}, 1 \\\\times 10^{-5}, 3 \\\\times 10^{-5}}\\\\) * training epoch {3, 10, 20}. The final hyperparameters for bert-base, roberta-base and deberta-base are: learning rate \\\\(1 \\\\times 10^{-5}\\\\), training epoch 20 and training batch size 32. For roberta-large-mnli and bart-large-mnli models, we reduce the training epochs to 3 and training batch size to 8. We choose the checkpoint with the best accuracy on the dev set for inference.\\n\\nFor ELV model, we use the released code and inherited most of their default hyperparameters for ELV-sa. We change the training batch size per GPU to 4 to accelerate the training.\\n\\nAlgorithm 1\\n\\nINPUT: For each instance \\\\(X_i\\\\), \\\\(i \\\\in \\\\{1, ..., N\\\\}\\\\), annotations from every worker \\\\(w_j\\\\), \\\\(j \\\\in \\\\{1, ..., 5\\\\}\\\\) denoted as \\\\(X_{ij}\\\\).\\n\\nOUTPUT: keywords for \\\\(X_i\\\\)\\n\\n1: for \\\\(j \\\\in \\\\{1, ..., 5\\\\}\\\\) do\\n2: // calculate the reliability score\\n3: \\\\(S_j = \\\\frac{1}{N} \\\\sum_{i=0}^{N} (\\\\#keywords - \\\\#average tokens in each keyword)\\\\)\\n4: end for\\n5: sort all workers with \\\\(S\\\\) and get preferred worker list \\\\(L\\\\)\\n6: // set worker with the highest \\\\(S\\\\) as anchor worker \\\\(w_a\\\\)\\n7: aggregated_keywords = []\\n8: for \\\\(K_z \\\\in X_{ia}\\\\) do\\n9: filtered_keywords \\\\(K_{filter} = []\\\\)\\n10: for \\\\(j \\\\in \\\\{1, ..., 5\\\\}\\\\) do\\n11: for \\\\(K_p \\\\in X_{ij}\\\\) do\\n12: calculate \\\\(F(K_z, K_p)\\\\)\\n13: end for\\n14: choose the keyword \\\\(K_P\\\\) in \\\\(X_{ij}\\\\) with highest \\\\(F\\\\)\\n15: if \\\\(F(K_z, K_P) > 60\\\\) then\\n16: append keyword \\\\(K_P\\\\) to \\\\(K_{filter}\\\\)\\n17: end if\\n18: end for\\n19: \\\\(AVG_a = \\\\frac{1}{\\\\text{len}(F_{filter})} \\\\sum_{K \\\\in F_{filter}} F(K_z, K)\\\\)\\n20: set the worker with the second highest \\\\(S\\\\) as new anchor worker \\\\(w_b\\\\). Repeat L6-L18 and get \\\\(AVG_b\\\\)\\n21: if \\\\(AVG_a \\\\geq AVG_b\\\\) then\\n22: append \\\\(X_{ia}\\\\) to aggregated_keywords\\n23: else\\n24: append \\\\(X_{ib}\\\\) to aggregated_keywords\\n25: end if\\n26: /* if only one worker has keyword annotation, append this worker's annotation to aggregated_keywords */\\n27: end for\\n\\nE T5 Implementation Details\\nWe finetune multiple T5 models (Raffel et al., 2020) in our work, and we use T5-base from SimpleT5 through our implementation. We use 512 and 256 for the maximum source length and the maximum target length respectively. As the optimizer, we use AdamW (Loshchilov and Hutter, 2019) with a learning rate of 0.0001. For the pretraining stage, we finetune T5 for 3 epochs on retrieved BookCorpus data. During the finetuning stage, we train each model on a Tesla V100 with a batch size of 8 for 30 epochs. During inference, we use beam search as the decoding method with a beam size of 2. We terminate decoding when the EOS token is generated or the maximum target length is reached.\\n\\n17https://github.com/Shivanandroy/simpleT5\"}"}
