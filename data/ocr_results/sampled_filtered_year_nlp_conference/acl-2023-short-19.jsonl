{"id": "acl-2023-short-19", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nAutomated completion of open knowledge bases (Open KBs), which are constructed from triples of the form \\\\((s, r, o)\\\\), obtained via open information extraction (Open IE) system, are useful for discovering novel facts that may not be directly present in the text. However, research in Open KB completion (Open KBC) has so far been limited to resource-rich languages like English. Using the latest advances in multilingual Open IE, we construct the first multilingual Open KBC dataset, called mOKB6, containing facts from Wikipedia in six languages (including English). Improving the previous Open KB construction pipeline by doing multilingual coreference resolution and keeping only entity-linked triples, we create a dense Open KB. We experiment with several models for the task and observe a consistent benefit of combining languages with the help of shared embedding space as well as translations of facts. We also observe that current multilingual models struggle to remember facts seen in languages of different scripts.\\n\\n1 Introduction\\n\\nOpen information extraction (Open IE) systems (Mausam, 2016) such as ReVerb (Etzioni et al., 2011) and OpenIE6 (Kolluru et al., 2020) can extract triples, or facts, of the form \\\\((s, r, o)\\\\), which can be denoted as \\\\((s, r, o)\\\\), from text (e.g., Wikipedia articles) without using any pre-defined ontology. Open knowledge base (Open KB) is constructed using these Open IE triples where the subject phrases and object phrases are nodes and relation phrases are labels on edges connecting the nodes in the graph. Open knowledge base completion (Open KBC) is the task of discovering new links between nodes using the graph structure of the Open KB. Knowledge graph embedding (KGE) models are typically used for the Open KBC task, where they are asked to answer questions of the form \\\\((s, r, ?)\\\\) and \\\\((?, r, o)\\\\).\\n\\nResearch in Open KBC has been restricted to English (Vashishth et al., 2018) due to lack of Open KBs in other languages. We aim to study multilingual Open KBC, with the motivation that the information available in high resource languages like English may help when inferring links in Open KBs that use low resource languages like Telugu. Moreover, intuitively, if all the information in different languages can be pooled together, then it may help the model learn better, and allow information flow across Open KBs in different languages.\\n\\nWe design the first multilingual Open KB construction pipeline (shown in Figure 1) using a multilingual Open IE system, GEN2OIE (Kolluru et al., 2022). We find that coreference resolution is missing in existing Open KB construction (Gashteovski et al., 2019) but is important for increasing the coverage of facts (as described in Figure 4). We re-train a recent coref model (Dobrovolskii, 2021) using XLM-R (Conneau et al., 2020) as the underlying multilingual encoder and add it to our pipeline. For constructing a high quality test set, we use 988 manually verified facts in English. For extending to other languages, we automatically translate English facts. The dataset thus constructed, called mOKB6, contains 42K facts in six languages: English, Hindi, Telugu, Spanish, Portuguese, and Chinese.\\n\\nWe report the first baselines for multilingual Open KBC task. We find that they are able to benefit from information in multiple languages when compared to using facts from a single language. Translations of Open KB facts also help the models. However, we notice that although the multilingual KGE models learn facts in a particular language, they struggle to remember the same fact, when queried in another language with different script.\"}"}
{"id": "acl-2023-short-19", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Related Work\\n\\nMultilingual Open KBC datasets are absent in literature to the best of our knowledge, although multiple English Open KBC datasets are available. OLPBench (Broscheit et al., 2020), derived from OPIEC (Gashteovski et al., 2019), is a large-scale Open KBC dataset that contains 30M triples and is constructed from English Wikipedia using MinIE system (Gashteovski et al., 2017). The evaluation data contains 10K triples randomly sampled from 1.25M linked triples. ReVerb45K (Vashishth et al., 2018) and ReVerb20K (Gal\u00e1rraga et al., 2014) are smaller Open KBC datasets constructed from Clueweb09 corpus using ReVerb Open IE system (Fader et al., 2011). Both the datasets keep only those tuples in which both the subject phrase and object phrase link to a finite set of Freebase entities.\\n\\nMultilingual Open IE (mOpenIE) systems like GEN2OIE (Kolluru et al., 2022) and Multi2OIE (Ro et al., 2020) enable extracting facts from multiple languages. We use the GEN2OIE model for constructing mOKB6 dataset as it is trained with language-specific facts transferred from English, while Multi2OIE relies on zero-shot transfer for languages other than English.\\n\\nKnowledge Graph Embedding (KGE) Models: Conventional KGE models like TransE (Bordes et al., 2013), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), and TuckER (Balazevic et al., 2019) have been used for Open KBC task (Gupta et al., 2019; Broscheit et al., 2020; Chandrahas and Talukdar, 2021; Kocijan and Lukasiewicz, 2021). Given a triple (s, r, o), these models encode the subject phrase, relation phrase, and object phrase from free text, and pass the encodings to a triple-scoring function, which is optimized using binary cross entropy loss. ComplEx has also been used for multilingual closed KBC task (Chakrabarti et al., 2022).\\n\\nPretrained language models like BERT (Devlin et al., 2019) have been used in KGE models for the KBC task (Lovelace and Ros\u00e9, 2022; Lv et al., 2022; Chandrahas and Talukdar, 2021; Kim et al., 2020). SimKGC (Wang et al., 2022) is the state of the art KGE model on closed KBC task. It computes the score of a triple (s, r, o) as the cosine similarity of the embeddings of (s; r) and (o), computed using two separate pretrained BERT models without any weight sharing.\\n\\nDataset Curation\\n\\nWe aim to construct a dense multilingual Open KB that maximizes the information about a given real-world entity, which may be represented as multiple nodes across languages. Therefore, we consider those Wikipedia articles that are available in six languages: English, Hindi, Telugu, Spanish, Portuguese, and Chinese. This will also help the model learn from facts in high resource language like English and answer queries in low resource language like Telugu. We work with 300 titles randomly sampled from the ones common among all six languages (found using MediaWiki-Langlinks (MediaWiki, 2021)). Thus, we extract facts from 6\u00d7300 Wikipedia articles. We discuss the three stages of our pipeline below.\\n\\nStage 1\\n\\nWe first process each Wikipedia article through a coreference resolution system. Although language-specific end-to-end neural coref models have been developed (\u017dabokrtsk\u00fd et al., 2022; Xia and Van Durme, 2021), multilingual models that work on all our languages of interest are absent in the literature. Therefore, we retrain wl-coref (Dobrovolskii, 2021) with XLM-R (Conneau et al., 2020) on the English training data (available in OntoNotes (Weischedel et al., 2013)) that can work zero-shot for other languages.\\n\\nCoref models detect and cluster mentions, but do not identify a canonical cluster name, which is needed for standardizing all the mentions in the cluster. To find cluster names, entity linking systems such as mGENRE (De Cao et al., 2022) or Wikipedia hyperlinks can be used. However, we found that they result in low recall, particularly for low resource languages. Thus, we employ a heuristic to find the cluster name and replace each of the coreferent mentions with it. The score for each mention is represented by a tuple, computed as: Score(mention phrase) = (#proper nouns, #nouns, #numerals, #adjectives, #pronouns, #verbs). The tuple is ordered according to the importance of each field (POS tags) for the cluster name, which is determined empirically. Two tuples are compared index-wise with higher priority given to lower indices to determine the best scoring mention that is chosen as the canonical name (Table 1).\\n\\nStage 2\\n\\nWe use GEN2OIE to extract Open IE triples from the coreference resolved sentences.\"}"}
{"id": "acl-2023-short-19", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Our three-staged multilingual Open KB construction pipeline for mOKB6. mCoref is multilingual coreference resolution system, having XLM-R (Conneau et al., 2020) encoder based wICoref (Dobrovolskii, 2021), and mOpenIE is multilingual open information extraction system, consisting of GEN2OIE (Kolluru et al., 2022).\\n\\n| Mentions Scores | Cluster Name |\\n|-----------------|--------------|\\n| Barack Obama    | (2,0,0,0,0,0) Barack Obama |\\n| He              | (0,0,0,0,1,0) |\\n\\nTable 1: Parts of speech tags are used to find the canonical name of the coreferent cluster of entity mentions.\\n\\nStage 3\\n\\nSimilar to Gashteovski et al. (2019), we apply various filters to remove noisy triples that have empty or very long arguments, or have less confidence than 0.3 (as assigned by GEN2OIE).\\n\\nWe further only keep triples that have the article's title as either the subject phrase or object phrase, to avoid generic or specific triples, valid only in the particular context. Examples of contextual triples (Choi et al., 2021) are discussed in Appendix E. See Appendix A for further data curation details.\\n\\nThese automatically extracted triples form the train set of mOKB6. To form a high quality test set in six languages with limited access to experts in all languages, the test set is created in a semi-automatic way. We sample 1600 English triples from the train set (which are subsequently filtered) and manually remove noisy triples. We use inter-annotation agreement between two annotators to check if they both agree that the given triple is noisy or clean. With an agreement of 91%, we retain 988 English triples, which we automatically translate to the other five languages. As illustrated in Figure 2, to translate a triple, we convert it to a sentence after removing tags and use Google translate for translating the triple-converted sentence to the remaining five languages. We observed high quality of translated triples, with 88% satisfactory translations as determined by native-speakers of three languages on a set of 75 translated triples. To get the Open IE subject phrase, relation phrase and object phrase tags, we project the labels from the original English triple to the translated sentence using word alignments (Kolluru et al., 2022). Finally, we are left with 550 triples in each language after removing examples where some labels could not be aligned. We use these $6 \\\\times 550$ triples as the test sets. The train and dev sets are created from the remaining triples in each language such that the dev set has 500 randomly sampled triples (Table 2).\\n\\nFigure 2: Method to translate Open IE triple using Google translate, and followed by label projection using word alignments (Kolluru et al., 2022).\\n\\nWe analyse the entity overlap across languages and find that on an average, a test entity (which is present in either the subject phrase or object phrase of a test tuple) is present 17.73 times in English, 0.94 times in Hindi, 0.47 times in Telugu, 2.33 times in Spanish, 1.69 times in Portuguese, and 1.45 times in Chinese.\\n\\nOur construction pipeline improves over OPIEC in three ways: (1) we use a multilingual Open IE system, instead of an English-specific Open IE system like in OPIEC, enabling us to curate Open KBs in many languages, (2) we add a multilingual coreference resolution system in our pipeline, and (3) the English test triples are manually verified. Further, we manually evaluate and review the noise at each step of data curation in Section 4.\\n\\n| Language | #entity | #relation | #train |\\n|----------|---------|-----------|--------|\\n| En       | 20637   | 7870      | 20195  |\\n| Hi       | 4625    | 2177      | 2786   |\\n| Te       | 3972    | 1907      | 1992   |\\n| Es       | 5651    | 2823      | 3966   |\\n| Pt       | 5304    | 2644      | 3528   |\\n| Zh       | 5037    | 2325      | 3420   |\\n\\nTable 2: Statistics of individual Open KBs in mOKB6 in English (En), Hindi (Hi), Telugu (Te), Spanish (Es), Portuguese (Pt), and Chinese (Zh). The dev and test set for each Open KB contain 500 and 550 triples each.\"}"}
{"id": "acl-2023-short-19", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Noise Evaluation\\n\\nCurating an Open KB involves various stages and each stage induces its noise in the construction pipeline (Gashteovski et al., 2019). We manually evaluate the noise induced at each stage of our pipeline (Figure 1) and discuss the same in this section. We ask native speakers of four (out of six) languages - English, Hindi, Telugu, and Chinese to assess the output quality, or precision, of each stage as discussed below.\\n\\nIn the first stage, we assess the performance of the coreference resolution system over Wikipedia articles. We find a high precision of 95.5% in coref's mention clustering and 89.82% accuracy in finding canonical cluster name (using the heuristic illustrated in Table 1), computed over 40 randomly sampled coref clusters (10 in each language).\\n\\nFor evaluating the Open IE system, GEN2OIE, in the second stage, we mark an extraction of a sentence as correct if it has syntactically correct arguments and it is coherent with the sentence. We get an average precision of 63.4% on 80 extractions (20 in each language).\\n\\nWe evaluate the triples, or Open KB facts, at the last stage after passing through various noise-removing filters. Note that these triples also form the train set (and dev set) in mOKB6 dataset. We mark triples as correct when they contain real-world entities, and also, factual information about them. If the triple is very generic or contextual (see Appendix E), it is marked as incorrect. We find the train (and dev) set quality to be 69.3%, averaged over 80 triples in four languages.\\n\\n5 Experiments\\n\\nOur experimental study on multilingual open KBC task investigates the following research questions:\\n\\n1. Does the KGE model benefit from facts in different languages? (Section 5.1)\\n2. Can translation help transfer among languages? (Section 5.2)\\n3. Does the KGE model remember facts seen across different languages? (Section 5.3)\\n\\nWe use SimKGC model (Wang et al., 2022) with pretrained mBERT initialization to run our experiments, after comparing with recent KGE models (Appendix C). For evaluation, we use three metrics \u2014 hits at rank 1 (H@1), hits at rank 10 (H@10), and mean reciprocal rank (MRR). The formal definitions of them are provided in Appendix B. We discuss further model training details in Appendix D.\\n\\n5.1 Training on Multilingual Facts\\n\\nWe train and compare monolingual model, called MONO, with multilingual models, UNION and UNION w/o En. In MONO, we train one model for each language using its respective Open KB, whereas in UNION, a single model is trained on six languages' Open KBs together. UNION outperforms MONO in all languages by an average of 4.6% H@10 and 2.8% MRR (see Table 3), which provides evidence of information flow across languages and the model benefits from it.\\n\\nTo check the extent of flow from (high-resource) English to the other languages, we also train on the five languages except English, which we call UNION w/o En. We find UNION w/o En also outperforms MONO by 2.7% H@10 and 1.2% MRR over the five languages, hinting that interlingual transfer is more general and pervasive.\\n\\n5.2 Open KB Facts Translation\\n\\nApart from relying only on multilingual transfer in the embedding space, we analyse the effect of using translated triples in the training of the KGE model. We translate the English training triples to the other five languages (Section 3) and train monolingual models using only the translated triples (TRANS). To leverage facts present in each language's Open KB, we make MONO+TRANS, where we add language-specific MONO data to the translated triples. Table 3 shows that MONO+TRANS is better than MONO by a large margin of 15.5% H@1, 29.2% H@10, and 20.0% MRR, averaged over five languages. Also, MONO+TRANS improves over TRANS by 2.1% H@10 and 2.0% MRR, showcasing the importance of facts in each language's Open KBs.\\n\\nTo effectively gain from transfer in both the embedding space as well as translation, we introduce UNION+TRANS. We train one model for each language, on the combination of UNION triples and the translated train triples from English Open KB to that language. UNION+TRANS is better than UNION by 25.9% H@10 and 18.4% MRR. This suggests that the model is able to benefit from English facts when they are translated to the query language, unlike in UNION where the English facts are present only in English.\\n\\n6 English source achieved the best translation quality.\"}"}
{"id": "acl-2023-short-19", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance (%) of SimKGC model on mOKB6 dataset, comprising of Open KBs in six languages.\\n\\n| Language | MONO | TRANS | MONO + TRANS |\\n|----------|------|--------|--------------|\\n| English  | 97.1 | 22.3   | 50.7         |\\n| Hindi    | 82.6 | 21.7   | 42.9         |\\n| Telugu   | 73.2 | 19.5   | 35.7         |\\n| Spanish  | 92.1 | 20.3   | 47.1         |\\n| Portuguese | 89.2 | 20.5   | 45.3         |\\n| Chinese  | 71.2 | 17.3   | 36.2         |\\n\\nCross-lingual Memorization\\n\\nPretrained multilingual language models such as mBERT have demonstrated strong cross-lingual transfer capabilities (Wu and Dredze, 2019). We investigate cross-lingual memorization of the KGE model by showing facts in one language and querying the same facts in other five languages. For each language, we take the UNION model and train it further on the test set of that language's Open KB, which we call MEMORIZED model. Then, we test each MEMORIZED model on the six test sets. Since the test sets (in mOKB6 dataset) of the different languages contain the same facts, this experiment allows us to investigate cross-lingual memorization. We provide the H@10 scores of MEMORIZED models in Figure 3 and the performance on other metrics (H@1 and MRR) is reported in Table 7.\\n\\nThe model achieves at least 97% H@10 when tested on the language used for training (diagonal). We observe that there is relatively good cross-lingual memorization among languages that share the same script (Latin in English, Spanish, and Portuguese), but the model struggles to remember facts when seen in languages of different scripts. Many entities look similar in shared scripts, possibly leading to better information transfer. For example, the MEMORIZED En achieves H@10 of 50.7% in Spanish (Es) compared to 22.3% in Chinese (Zh) and 11% in Telugu (Te).\\n\\n6 Conclusion and Future Work\\n\\nWe create and release the mOKB6 dataset, the first multilingual Open Knowledge Base Completion dataset with 42K facts in six languages: English, Hindi, Telugu, Spanish, Portuguese, and Chinese. Its construction uses multilingual coreference resolution, entity-mention cluster naming, multilingual open information extraction and various filtering steps to improve the quality of the extracted facts. We also report the first baselines on the task using the existing state of the art KGE models trained with facts from different languages using various augmentation strategies.\\n\\nOur work opens many important research questions: (1) Can we develop better strategies to combine facts in different languages? (2) Can we build models that achieve strong information transfer across unrelated languages with same or different scripts? (3) Can we train the neural model to ignore contextual triples (Appendix E), thus improving overall performance? and (4) Can tying the same entities across various languages help the model generalize better? We leave these questions to be addressed in future work.\"}"}
{"id": "acl-2023-short-19", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8 Limitations\\n\\nAlthough multilingual, the constructed open KB is limited to the sampling of the chosen six languages. We do not know how well the system will generalize to various language families that have not been considered here. Further, even among the languages considered, the performance of even the best-performing systems, as measured through H@1 is still in the low 20's. Therefore the models are not yet ready to be deployed for real-world applications.\\n\\n8 References\\n\\nIvana Balazevic, Carl Allen, and Timothy Hospedales. 2019. TuckER: Tensor factorization for knowledge graph completion. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5185\u20135194, Hong Kong, China. Association for Computational Linguistics.\\n\\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.\\n\\nSamuel Broscheit, Kiril Gashteovski, Yanjie Wang, and Rainer Gemulla. 2020. Can we predict new facts with open knowledge graph embeddings? a benchmark for open link prediction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2296\u20132308, Online. Association for Computational Linguistics.\\n\\nSoumen Chakrabarti, Harkanwar Singh, Shubham Lohiya, Prachi Jain, and Mausam. 2022. Joint completion and alignment of multilingual knowledge graphs. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11922\u201311938, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nChandrahas and Partha Talukdar. 2021. OKGIT: Open knowledge graph link prediction with implicit types. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2546\u20132559, Online. Association for Computational Linguistics.\\n\\nKyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, Doha, Qatar. Association for Computational Linguistics.\\n\\nEunsol Choi, Jennimaria Palomaki, Matthew Lamm, Tom Kwiatkowski, Dipanjan Das, and Michael Collins. 2021. Decontextualization: Making sentences stand-alone. Transactions of the Association for Computational Linguistics, 9:447\u2013461.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In ACL Conference, pages 8440\u20138451.\\n\\nNicola De Cao, Ledell Wu, Kashyap Popat, Mikel Artetxe, Naman Goyal, Mikhail Plekhanov, Luke Zettlemoyer, Nicola Cancedda, Sebastian Riedel, and Fabio Petroni. 2022. Multilingual autoregressive entity linking. Transactions of the Association for Computational Linguistics, 10:274\u2013290.\\n\\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2d knowledge graph embeddings. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI\u201918/IAAI\u201918/EAAI\u201918. AAAI Press.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nVladimir Dobrovolskii. 2021. Word-level coreference resolution. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7670\u20137675, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nOren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam. 2011. Open information extraction: The second generation. In IJCAI.\"}"}
{"id": "acl-2023-short-19", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-short-19", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-short-19", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As discussed in Section 3, we construct mOKB6 dataset in three stages after extracting the Wikipedia articles (using WikiExtractor) from the Wikidump of April 02, 2022. We run our construction pipeline (as shown in Figure 1) for all six languages on a single V100 (32 GB) GPU, which required 14 hours of computation to create mOKB6 dataset.\\n\\nIn the first stage, we keep the sentences containing at least 6 and at most 50 tokens since we find that most of the short sentences are headings or sub-headings present in Wikipedia articles, and very long sentences can't be input to GEN2OIE (in second stage) due to maximum sequence length constraint of 1024 in mT5 (Xue et al., 2021) based GEN2OIE. This filtering step discards 18.9% of sentences on an average in all six languages. We use Stanza (Qi et al., 2020) to perform sentence- and word-segmentation on Wikipedia articles in all six languages. After filtering the sentences, the articles are processed for coreference resolution using XLM-R (Conneau et al., 2020) encoder based wlcoref (Dobrovolskii, 2021), followed by replacing the coreferent cluster mentions with their canonical cluster name using the heuristic discussed in Section 3.\\n\\nIn the second stage, the coreference resolved articles are passed through GEN2OIE to get the Open IE triples. The confidence scores for these triples are computed using label rescoring, for which we refer the readers to Kolluru et al. (2022) for more details.\\n\\nFinally, in the last stage, we apply various filters, adapted from Gashteovski et al. (2019), to remove triples that are of no interest to Open KBC task, like the triples: (1) having any of its argument or relation empty, (2) containing more than 10 tokens in any of its arguments or relation, (3) having confidence score less than 0.3, (4) containing pronouns (found using Stanza) in its arguments, (5) having same subject and object (i.e. self loops), and (6) that are duplicates. These filters keep 91.6% of the triples obtained from stage 2 in all six languages.\\n\\nFurther in the last stage, in order to create a dense Open KB containing minimum noise and maximum facts about the entities, we keep the triples having the Wikipedia article's title as either the subject phrase or object phrase and discard the rest. We do this by finding all the coreference clusters (of entity mentions) that contain the titles, then get the entities, or cluster names, of those clusters using the heuristic discussed in section 3, and keep those triples that contain these cluster names. This filtering step retains 23.6% of the triples.\\n\\nB Metrics\\nWe follow the previous works (Wang et al., 2022) on the evaluation methodology of Open KBC task and apply it to the multilingual Open KBC task, containing facts in multiple languages. Given an Open KB, containing a finite set of entities and open relations, the KGE model answers forward and backward queries of the form \\\\((s, r, ?)\\\\) and \\\\((?, r, o)\\\\) respectively. The model ranks all the entities based on their correctness with, say, \\\\(s\\\\) and \\\\(r\\\\) in the forward query. Further, the evaluation is in filtered setting, where the other known correct answers, apart from \\\\(o\\\\), are removed from rank list.\\n\\nThe commonly used evaluation metrics are hits at rank N (H@N), where \\\\(N\\\\) is a natural number, and mean reciprocal rank (MRR). Suppose, the model ranks \\\\(o\\\\) at \\\\(R\\\\) among all entities. Then, H@N measures how many times \\\\(R\\\\) is less than or equal to \\\\(N\\\\). MRR is the average of reciprocal ranks (\\\\(1/R\\\\)).\\n\\nBoth, H@N and MRR, are computed as average over both forms of queries over the full test set.\\n\\nC Knowledge Graph Embedding Models\\nSimKGC (Wang et al., 2022) is a text-based KGE model that uses two unshared pretrained BERT models (Devlin et al., 2019) for encoding (subject phrase; relation phrase) and (object phrase) separately. GRU-ConvE (Kocijan and Lukasiewicz, 2021) encodes both the relation phrase and argument phrase from their surface forms using two unshared GRU (Cho et al., 2014). CaRe (Gupta et al., 2019) learns separate embeddings for each argument phrase and uses a bi-directional GRU to encode the relation phrase from its surface form.\\n\\nBoth, GRU-ConvE and CaRe, are initialised with Glove embeddings (Pennington et al., 2014).\"}"}
{"id": "acl-2023-short-19", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Previous Open KB construction pipelines like Gashteovski et al. (2019) (shown by green arrows) lack coreference resolution system, which result in filtering important facts like (Barack Obama; returned to Honolulu, Hawaii in 1971). Our pipeline (shown by blue arrows) increases the coverage of facts due to mCoref system.\\n\\nTo choose the best model for our experiments (Table 3, Figure 3), we train the recent knowledge graph embedding (KGE) models \u2014 CaRe\\\" GRU-ConvE and SimKGC on the English Open KB in mOKB6. We report performance in Table 4 using the three metrics: hits at rank 1 (H@1), hits at 10 (H@10), and mean reciprocal rank (MRR). We find that SimKGC with BERT encoder outperforms the other two models.\\n\\n| Model              | H@1 | H@10 | MRR  |\\n|--------------------|-----|------|------|\\n| CaRe               | 6.6 | 11.3 | 8.3  |\\n| GRU-ConvE          | 12.4| 27.8 | 17.8 |\\n| SimKGC (BERT)      | 16.1| 40.0 | 24.3 |\\n| SimKGC (mBERT)     | 14.8| 38.7 | 22.8 |\\n| SimKGC (XLM-R)     | 13.8| 35.8 | 21.3 |\\n\\nTable 4: Performance (%) of the KGE models on the English test set in mOKB6 dataset. The reported numbers are an average of three runs using different seeds.\\n\\nSince BERT supports only English language, we replace BERT in SimKGC with multilingual pre-trained language models like mBERT (Devlin et al., 2019) or XLM-R (Conneau et al., 2020), to extend SimKGC model to other languages. We find in Table 4 that SimKGC with mBERT is better than with XLM-R by 2.9% H@10 and 1.5% MRR, possibly because mBERT (and mOKB6) uses Wikipedia while XLM-R uses CommonCrawl (Wenzek et al., 2020) during pre-training. Thus, we use SimKGC with mBERT as the underlying encoder to run our experiments for all the languages.\\n\\nD KGE Model Training Details\\n\\nWe use the code from official repositories of the KGE models \u2014 SimKGC (Wang et al., 2022), GRU-ConvE (Kocijan and Lukasiewicz, 2021), and CaRe (Gupta et al., 2019) for our experiments. The models are trained using Adam optimizer (Kingma and Ba, 2015) on a single A100 (40 GB) GPU with three different random seeds and we report the average of three evaluation runs.\\n\\nWe do not perform hyperparameter search tri- als, except for batch size, and use the default hyperparameters from the respective codes of KGE models (see Table 5). We use early stopping to find the best model checkpoints based on HITS@1. The dev set is different for each baseline: MONO, TRANS, MONO+TRANS, and UNION+TRANS use individual language\u2019s dev set, whereas UNION w/o En and UNION use the English dev set. We report the performance of baseline models on the dev sets in Table 9 and Table 10.\\n\\n| Model              | #epochs | #patience epochs | learning rate | dropout | batch size | additive margin |\\n|--------------------|---------|------------------|---------------|---------|------------|----------------|\\n| CaRe               | 100     | 10               | 3e-5          | 0.1     | 256        | N/A            |\\n| GRU-ConvE          | 500     | 10               | 3e-4          | 0.3     | 1024       | N/A            |\\n| SimKGC (BERT)      | 500     | 10               | 1e-3          | 0.5     | 128        | N/A            |\\n\\nTable 5: Hyperparameters of the KGE models.\\n\\nWe provide the number of trainable parameters of each KGE model in Table 6. Based on the batch size and model size, different experiments consume different GPU hours. To train on English Open KB (in mOKB6 dataset), CaRe and GRU-ConvE models took 2.5 hours and 0.5 hours, respectively, whereas SimKGC takes nearly 1 hour of GPU time.\\n\\n| KGE model              | #trainable parameters |\\n|------------------------|-----------------------|\\n| CaRe                   | 12,971,423            |\\n| GRU-ConvE              | 12,085,523            |\\n| SimKGC (BERT)          | 216,620,545           |\\n| SimKGC (mBERT)         | 355,706,881           |\\n| SimKGC (XLM-R)         | 1,119,780,865         |\\n\\nTable 6: Number of trainable parameters in the KGE models.\"}"}
{"id": "acl-2023-short-19", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: Performance (%) of the six M\\REMORIZE models, which have been trained on each language's test set and tested on all the test sets in mOKB6 dataset.\\n\\n| Language | H@1 | H@10 | MRR | H@1 | H@10 | MRR | H@1 | H@10 | MRR | H@1 | H@10 | MRR | H@1 | H@10 | MRR |\\n|----------|-----|------|-----|-----|------|-----|-----|------|-----|-----|------|-----|-----|------|-----|\\n| English  | 68.4| 97.1 | 78.8| 3.4 | 17.2 | 8.3 | 1.6 | 11   | 5   | 17.8| 50.7 | 28.6| 17  | 44.6 | 26  |\\n| Hindi    | 19  | 42.2 | 26.7| 80.6| 99.5 | 88.3| 2.4 | 12.5 | 5.9 | 12.3| 36   | 19.9| 12.3| 33.9 | 19.7|\\n| Telugu   | 19.5| 42.2 | 27.2| 4.3 | 18.7 | 9.4 | 74.4| 99.5 | 84.2| 10.9| 35.4 | 18.9| 10.7| 34   | 18.5|\\n| Spanish  | 27.9| 60.4 | 38.8| 4.1 | 17.8 | 8.9 | 1.8 | 10.7 | 5.1 | 84  | 100  | 90.3| 37.6| 74   | 50.1|\\n| Portuguese| 27.8| 58.7 | 38.2| 4.4 | 18.2 | 9.3 | 1.7 | 10.5 | 5.1 | 41.5| 78.5 | 53.6| 84.2| 99.9 | 90.8|\\n| Chinese  | 22.1| 48.4 | 30.6| 3.5 | 18.5 | 8.8 | 1.8 | 12.2 | 5.4 | 14.8| 42.8 | 24.2| 15.7| 41.6 | 24.1|\\n\\nTable 8: Examples of contextual triples.\\n\\nFrom the first two triples in Table 8, it is unclear which scientific work Max Born continued, or which championship Robb Gravett has won. The last two triples are too specific to the context and contain no factual information.\"}"}
{"id": "acl-2023-short-19", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language   | H@1 | H@10 | MRR |\\n|------------|-----|------|-----|\\n| CaRe       | 7.1 | 11.1 | 8.5 |\\n| GRU-ConvE  | 16.8| 31.5 | 22.1|\\n| SimKGC (BERT) | 20.3| 40.1 | 27.1|\\n| SimKGC (mBERT) | 16.2| 38.7 | 23.9|\\n| SimKGC (XLM-R) | 17  | 36.6 | 23.2|\\n\\nTable 10: Performance (%) of the KGE models on dev set of English Open KB in mOKB6 dataset.\"}"}
{"id": "acl-2023-short-19", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\n8\\n\u25a1 A2. Did you discuss any potential risks of your work?\\nThere are no potential risks of our work to our knowledge.\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n1\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\nLeft blank.\\nB\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n3,4\\n\u25a1 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\\nAbstract\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\nNot applicable. Left blank.\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\nNot applicable. Left blank.\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n3\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n3\\nC\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\nAppendix D\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-short-19", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nAppendix D\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nAppendix D\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nAppendix A, D\\n\\nDid you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nNot applicable. Left blank.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nNot applicable. Left blank.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nNot applicable. Left blank.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nNot applicable. Left blank.\\n\\nC2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nAppendix D\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nAppendix D\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nAppendix A, D\\n\\nDid you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nNot applicable. Left blank.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nNot applicable. Left blank.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nNot applicable. Left blank.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nNot applicable. Left blank.\"}"}
