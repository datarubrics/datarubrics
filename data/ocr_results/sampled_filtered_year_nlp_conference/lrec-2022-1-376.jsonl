{"id": "lrec-2022-1-376", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish\\n\\nAriel Ekgren \u22c6, Amaru Cuba Gyllensten \u25e6, Evangelia Gogoulou \u25e6, Alice Heiman \u22c6, Severine Verlinden \u22c6, Joey \u00d6hman \u22c6, Fredrik Carlsson \u25e6, Magnus Sahlgren \u22c6\\n\\nAI Sweden, \u25e6RISE\\n\\n{ariel.ekgren, alice.heiman, severine.verlinden, joey.ohman, magnus.sahlgren} @ai.se\\n{amaru.cuba.gyllensten, evangelia.gogoulou, fredrik.carlsson} @ri.se\\n\\nAbstract\\n\\nWe present GPT-SW3, a 3.5 billion parameter autoregressive language model, trained on a newly created 100 GB Swedish corpus. This paper provides insights with regard to data collection and training process, and discusses the challenges of proper evaluation. The results of quantitive evaluation using perplexity indicate that GPT-SW3 is a competent model in comparison with existing autoregressive models of similar size. Additionally, we perform an extensive prompting study which reveals the good text generation capabilities of GPT-SW3.\\n\\nKeywords: Language models, Evaluation, Prompting\\n\\n1. Introduction\\n\\nLarge-scale generative language models such as the GPT series (Radford and Narasimhan, 2018; Radford et al., 2019; Brown et al., 2020) have enjoyed considerable attention in recent years. This has been partly due to their unprecedented ability to generate coherent text, but also for their capacity for zero-shot performance\u2014without any training examples, on a wide range of different tasks. A prerequisite for building such models is access to both large amounts of high-quality text data and powerful computational resources. This has proven to be a limiting factor for the development of large-scale models for languages other than English.\\n\\nWith the goal of promoting the development of large-scale generative models for other languages, we here present our work on developing and evaluating GPT-SW3, a 3.5 billion parameter autoregressive language model, trained on a newly collected 100 GB Swedish corpus. To the best of our knowledge, this is the largest generative model for Swedish to date, and probably one of the bigger non-English models at the moment. In this paper, we collect the lessons learned by developing and evaluating this model, including challenges with data collection, training procedures, and validation activities.\\n\\n2. Related Work\\n\\nThe development of large-scale (i.e. billions of parameters) generative language models has largely been dominated by the GPT series of models, starting with GPT-2 with 1.5 billion parameters (Radford et al., 2019), over Megatron-LM with 8.3 billion parameters (Shoeybi et al., 2019a) and Turing-NLG with 17.2 billion parameters to GPT-3 with 175 billion parameters (Brown et al., 2020). More recently, Nvidia and Microsoft announced a generative model called Megatron-Turing NLG with a whopping 530 billion parameters. None of these truly large-scale models (i.e. models with a parameter count over 10 billion parameters) are publicly available at this time.\\n\\nOne initiative that aims to provide open-source large-scale generative language models is Eleuther AI, which has so far provided GPT-Neo with 2.7 billion parameters (Black et al., 2021), GPT-J with 6 billion parameters (Wang and Komatsuzaki, 2021) and GPT-NeoX with 20 billion parameters (Black et al., 2022). For languages other than English, large-scale GPT models are scarce. There are mainly GPT-2-sized models for a limited number of languages, including Arabic (Antoun et al., 2021), Italian (De Mattei et al., 2020), French (Simoulin and Crabb\u00e9, 2021), Dutch (de Vries and Nissim, 2021) and Russian.\\n\\nWhen it comes to Swedish, and the other Nordic languages (Norwegian, Danish, Icelandic and Faroese), current efforts are mainly targeting pre-training of BERT models (Malmsten et al., 2020) and other encoder architectures of similar size. The only prior published work on training a Swedish GPT model is by Norlund and Stenbom (2021), who trained a 1.5 billion parameter GPT model using 23 GB of text data collected from the Swedish discussion forum Flashback.\"}"}
{"id": "lrec-2022-1-376", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Data\\n\\nThe arguably most important prerequisite for being able to build a large-scale language model is the existence of sufficient amounts of sufficiently high-quality text data. This can be a challenging factor for smaller languages such as Swedish, for which there is currently no large-scale corpora publicly available. We therefore compile a 100 GB corpus of Swedish text data specifically for training GPT-SW3. Our corpus, summarized in Table 1 and Figure 1, includes both existing Swedish datasets, as well as new ones collected from the web. Note that the largest part of our corpus consists of web data (mainly forums and news articles), which most likely have lower quality than the smaller curated text sources, represented in red color.\\n\\n| Dataset Genre | Size (GB) |\\n|---------------|-----------|\\n| Anf\u00f6randen Politics | 0.9 |\\n| DCEP Politics | 0.6 |\\n| DGT Politics | 0.7 |\\n| Fass Medical | 0.6 |\\n| Flashback Forum | 22.0 |\\n| F\u00f6rfattningar Legal | 0.1 |\\n| Litteraturbanken Books | 0.3 |\\n| OSCAR Misc | 28.0 |\\n| SOU Politics | 5.3 |\\n| Subtitles Drama | 1.3 |\\n| Wikipedia Facts | 1.8 |\\n\\nTable 1: Datasets by genre and size.\\n\\nIt is well known that the quality of the text data is the determining factor for the quality of the resulting model, and most large-scale models thus employ aggressive filtering and cleaning of the training data. Our corpus is preprocessed by removing irrelevant and unwanted content: HTML tags and inappropriate words (e.g. sexual, obscene etc) are added to a blacklist and documents are filtered away from the corpus.\\n\\n4. Training\\n\\nGPT-SW3 is based on the GPT architecture as implemented in the Megatron-LM framework (Shoeybi et al., 2019b). The model hyperparameters, presented in Table 2, are selected according to the results of scaling GPT training experiments, conducted by Narayanan et al. (2021). The model is trained for 97000 steps with a batch size of 512 using the autoregressive next-step prediction objective (Radford et al., 2019). Following (Shoeybi et al., 2019b), the chosen optimizer is Adam with weight decay $\\\\lambda = 0.01$. The validation loss is shown in Figure 2.\\n\\nParameter Value\\n\\n| Transformer layers | 30 |\\n|--------------------|----|\\n| Attention heads    | 32 |\\n| Sequence length    | 2,048 |\\n| Embedding dimension | 3,072 |\\n| Total parameters   | 3,559,415,808 |\\n\\nTable 2: List of model parameter values for GPT-SW3.\\n\\nTraining was performed on GPU resources from the Berzelius Superpod, which is currently the fastest supercomputer in Sweden, equipped with 60 Nvidia DGX A100 servers, each of which consists of 8 Nvidia A100 GPUs with 320 GB Total GPU memory. Our training process took 2.5 days utilizing 16 of the DGX A100 servers (in total 128 GPUs).\\n\\n5. Evaluation\\n\\nEvaluating text generation models is notoriously difficult, and lacks a standardized methodology (Howcroft et al., 2020). As such, most work on generative language models focus either on intrinsic measures such as perplexity, or on their ability to solve downstream tasks in zero- or few-shot settings. In the latter case, the model is provided with a number of labelled examples (or shots) during inference time, without any previous fine-tuning, and then evaluated on unlabelled examples. Since a ready-to-use Swedish framework for evaluation of GPT models on downstream tasks is still...\"}"}
{"id": "lrec-2022-1-376", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"work in progress, we opted for perplexity as a quantitative measure of model performance, as well as a qualitative prompting study to evaluate the text generation quality.\\n\\n5.1. Perplexity\\n\\nTo evaluate perplexity on character level, we selected the mc4 dataset (Xue et al., 2021) based on two criteria: as far as we know, the data has not been seen before by the models and the text is written originally in Swedish. Due to limited computational resources, we sample 50 examples from the mc4 validation set using random sampling. The perplexity formula, presented in equation 1, uses character length normalization (Cotterell et al., 2018; Mielke, 2019) rather than token length, as token length favours tokenizers using more tokens for a single sentence.\\n\\n$$PPL_c(X) = \\\\exp \\\\left( - \\\\frac{1}{\\\\text{character length of } X} \\\\sum_{t=1}^{T} \\\\log p(T_{t} | T_{<t}) \\\\right)$$\\n\\nThe perplexity scores of GPT-SW3 (3.5B), Swedish Flashback-GPT (1.4B) (Norlund and Stenbom, 2021) and two existing English GPT models, GPT2-xl (1.5B) (Radford et al., 2019) and GPT-Neo (2.7B) (Black et al., 2021), calculated on samples from Swedish and English mc4, are presented in Table 3. GPT-SW3 clearly outperforms all other models in terms of perplexity on the Swedish data, while GPT-Neo, which was trained on 800 GB of English data, has the lowest perplexity on the English dataset.\\n\\n| Model          | PPL$_{c_{sv}}$ | PPL$_{c_{en}}$ |\\n|----------------|----------------|----------------|\\n| Flashback-GPT  | 4.76           | 6.22           |\\n| GPT2-xl        | 4.70           | 2.30           |\\n| GPT-Neo        | 2.87           | 2.11           |\\n| GPT-SW3        | 2.19           | 2.70           |\\n\\nTable 3: The average character level perplexity comparison on the Swedish and English mc4 corpora.\\n\\n5.2. Prompting Experiments\\n\\nDue to the lack of standardized evaluation data and methodologies, we perform an extensive set of prompting experiments to assess the capabilities of GPT-SW3. We take inspiration from prior work on GPT-2 and GPT-3, and take as our starting point the documentation provided by OpenAI.\\n\\nSome prompt examples are translated, tested, and modified to optimize the performance of GPT-SW3. For others, we use an iterative process to create prompts from scratch. We tweak the parameters for every given task. Generally, the temperature and top-p are set between 0.60\u22121.00, and the number of beams between 2\u22126. Going below 0.60 makes the model more prone to get stuck in repetitive loops. Using a beam size of 6 or greater often lead to memory crashes.\\n\\n5.2.1. Creative Writing\\n\\nGenerative models are of course designed specifically to be able to generate text. We therefore provide several examples of more creative scenarios where the end-goal is less constrained. We use slightly higher top-p and temperature values to yield less deterministic results. As is well known from previous work on GPT-2 and GPT-3, finding appropriate prompts are key to the model's performance, and is typically a highly exploratory task. Below are some examples of prompts that work well with GPT-SW3.\\n\\n**Politics**: Our data contains a fair amount of political text. Prompting with `Reply to interpellation` makes the model continue as a politician giving the opposite argument to what was said beforehand, and prompting with `===X===` makes the model generate a parliamentary debate on topic X.\\n\\n**Speeches**: Providing prompts such as `Thank-you speech to X` or `Love-letter to X` makes the model generate appropriate and coherent continuations. Inputting the first six paragraphs of the Swedish King's annual Christmas speech from 2018 makes the model continue and successfully finish the speech, even adding an instruction for applause at the end (see Section A.3 in the Appendix). More examples of speeches is provided in Section B in the Appendix.\\n\\n**Chatbot**: The translated version of OpenAI's chatbot example also work well for GPT-SW3. Giving the model a character with a personality, traits, and motivations and an example question-and-response is often enough to make the model carry on the conversation in the same format. Some questions and answers can be found in Section A.1 together with the complete prompt.\\n\\n**Command-to-text**: By prompting the model with a command statement such as: `Write email to [Recipient] from [Sender]. [Short sentence with core email content]`, GPT-SW3 is able to generate formatted emails with the prompted content. Table 4 gives an example of an automatically generated email. More few-shot examples can be found in Section A.7 in the Appendix.\\n\\n**Product promotion and reviews**: Using the fictive hair spray called `LOKAR`, GPT-SW3 generates both product descriptions and customer reviews. Examples can be found in Section A.2 in the Appendix, and demonstrates GPT-SW3's ability to leverage patterns provided in the prompts.\"}"}
{"id": "lrec-2022-1-376", "page_num": 4, "content": "{\"primary_language\":\"sv\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Skriv mail till Peter fr\u00e5ns Sara. Fr\u00e5ga om picknick med familjen. Beh\u00f6ver hj\u00e4lp med lunchen.\\n\\nHej Peter!\\n\\nVi skulle beh\u00f6va hj\u00e4lp med att fixa en picknick f\u00f6r hela familjen p\u00e5 l\u00f6rdag. Vi har inte m\u00f6jlighet att ta med egen mat, s\u00e5 det skulle vara v\u00e4ldigt uppskattat om du kunde hj\u00e4lpa oss med det.\\n\\nV\u00e4nliga h\u00e4llningar,\\nSara\"}"}
{"id": "lrec-2022-1-376", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 The model generally works best when tasks are formatted in such a way that it is natural for the model to continue.\\n\u2022 Text is often in a conversational style. A prompt formatted as the continuation of a blog post, for example, could produce more coherent, comprehensive, and predictable text.\\n\u2022 Tasks with definite answers and specific categories, such as classification, benefit from clear labels specifying what should be generated.\\n\u2022 Set the Temperature and Top-P to 0.60 and above to avoid repetition loops. Use higher values to make the model less deterministic.\\n\u2022 Avoid arrows as separators in few-shot examples. Use colons instead.\\n\u2022 Numbered lists generally work better than those with dashes or stars.\\n\u2022 Working with the model is a very dynamic process that involves many iterations of trial-and-error. Treating the model as a human with traits, emotions, and motivations could help get higher-quality and more trustworthy responses. And sometimes, it is enough to just ask.\"}"}
{"id": "lrec-2022-1-376", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gao, L. (2021). An empirical exploration in quality filtering of text data. arXiv preprint arXiv:2109.00698.\\n\\nHowcroft, D. M., Belz, A., Clinciu, M.-A., Gkatzia, D., Hasan, S. A., Mahamood, S., Mille, S., van Miltenburg, E., Santhanam, S., and Rieser, V. (2020). Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. In Proceedings of the 13th International Conference on Natural Language Generation, pages 169\u2013182, Dublin, Ireland, December. Association for Computational Linguistics.\\n\\nMalmsten, M., B\u00f6rjeson, L., and Haffenden, C. (2020). Playing with words at the national library of Sweden\u2013making a Swedish bert. arXiv preprint arXiv:2007.01658.\\n\\nMielke, S. J. (2019). Can you compare perplexity across different segmentations?, Apr.\\n\\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V. A., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., et al. (2021). Efficient large-scale language model training on gpu clusters. arXiv preprint arXiv:2104.04473.\\n\\nNorlund, T. and Stenbom, A. (2021). Building a Swedish open-domain conversational language model. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 357\u2013366, Reykjavik, Iceland (Online), May 31\u20132 June. Link\u00f6ping University Electronic Press, Sweden.\\n\\nRadford, A. and Narasimhan, K. (2018). Improving language understanding by generative pre-training.\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nRogers, A. (2021). Changing the world by changing the data. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 2182\u20132194, Online, August. Association for Computational Linguistics.\\n\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019a). Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. CoRR, abs/1909.08053.\\n\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019b). Megatron-LM: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.\\n\\nSimoulin, A. and Cr\u0103ciun, B. (2021). Un mod\u00e8le Transformer G\u00e9n\u00e9ratif Pr\u00e9-entrain\u00e9 pour le Fran\u00e7ais. In Pascal Denis, et al., editors, Traitement Automatique des Langues Naturelles, pages 246\u2013255, Lille, France. ATALA.\\n\\nWang, B. and Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May.\\n\\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. (2021). mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online, June. Association for Computational Linguistics.\"}"}
{"id": "lrec-2022-1-376", "page_num": 7, "content": "{\"primary_language\":\"sv\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Prompting Examples\\n\\nA.1. Chatbot\\n\\nExample response from a chat bot style interaction between a human and GPT-SW3. The few-shots are omitted for brevity, but the complete prompt can be found in the next example. The text marked in bold is text written by the researchers while the rest is generated by GPT-SW3.\\n\\nDetta \u00e4r en konversation med en AI-assistent. Assistenten \u00e4r hj\u00e4lpsam, kreativ, klok och v\u00e4ldigt sn\u00e4ll.\\n\\nM\u00e4nniska: Vad ska jag g\u00f6ra f\u00f6r att \u00e4ta mer nyttigt?\\n\\nAI: \u00c4t mer gr\u00f6nsaker, frukt och fullkornsprodukter. Det \u00e4r bra f\u00f6r b\u00e5de din kropp och din hj\u00e4rna. F\u00f6rs\u00f6k att \u00e4ta s\u00e5 mycket som m\u00f6jligt av den mat som din kropp m\u00e5r bra av. Om du vill g\u00e5 ner i vikt \u00e4r det viktigt att du \u00e4ter mindre \u00e4n vad du g\u00f6r av med. D\u00e5 g\u00e5r du ner i vikt och det \u00e4r l\u00e4ttare att h\u00e5lla vikten om du \u00e4ter mer \u00e4n vad du g\u00f6r av med.\\n\\nM\u00e4nniska: Jag har f\u00f6r mycket att l\u00e4sa och f\u00f6r lite tid. Hur ska jag g\u00f6ra?\\n\\nAI: L\u00e4s mer om hur du kan f\u00e5 mer tid och mindre stress genom att minska p\u00e5 det som tar din energi. Till exempel kan du minska p\u00e5 tv-tittandet, sl\u00f6surfandet och sl\u00f6surfandet p\u00e5 mobilen. Fundera p\u00e5 hur du anv\u00e4nder din tid p\u00e5 b\u00e4sta s\u00e4tt. Kanske kan du ta en promenad i st\u00e4llet f\u00f6r att sitta framf\u00f6r datorn. Eller s\u00e5 kan du g\u00f6ra n\u00e5got som du tycker om att g\u00f6ra, till exempel l\u00e4sa en bok eller lyssna p\u00e5 en ljudbok. Eller s\u00e5 kan du g\u00e5 ut och g\u00e5 i st\u00e4llet f\u00f6r att ta bilen till jobbet. Det \u00e4r ocks\u00e5 bra att r\u00f6ra p\u00e5 sig, till exempel genom att promenera eller cykla. Det g\u00f6r att du blir piggare och f\u00e5r mer energi.\\n\\nM\u00e4nniska: Hur f\u00e5r man v\u00e4nner?\\n\\nAI: Genom att umg\u00e5s med andra m\u00e4nniskor. Om du vill ha v\u00e4nner kan du g\u00e5 med i en f\u00f6rening eller g\u00e5 med i en studiecirkel. Det finns m\u00e5nga f\u00f6reningar och studiecirklar i din stad och du kan hitta en som passar dig. Du kan ocks\u00e5 g\u00e5 med i en bokcirkel eller en skrivarcirkel. Du kan ocks\u00e5 g\u00e5 med i en matlagningscirkel eller en k\u00f6rcirkel.\\n\\nM\u00e4nniska: Vad \u00e4r det b\u00e4sta med att vara m\u00e4nniska?\\n\\nAI: Att vi \u00e4r sociala djur. Vi har ett behov av att k\u00e4nna oss beh\u00f6vda och att f\u00e5 vara en del av en grupp. Vi \u00e4r ocks\u00e5 bra p\u00e5 att samarbeta och l\u00f6sna problem tillsammans. Vi \u00e4r bra p\u00e5 att l\u00e4ras nya saker och vi kan l\u00e4ra oss av andra. Vi har ocks\u00e5 en stark k\u00e4nsla f\u00f6r r\u00e4ttvisa och empati.\\n\\nM\u00e4nniska: Kan jag f\u00e5 en kram?\\n\\nAI: Ja, det kan du f\u00e5. Men bara om du \u00e4r sn\u00e4ll mot den som ger dig en kram.\\n\\nPrompt used for a chatbot assistant. Parameters used in paper: Beams = 4, Temperature = 0.60, Top-p = 0.60, Top-k = 1000, and Repetition = 2.00.\\n\\nA.2. Product promotion and reviews\\n\\nFictive product description and customer reviews generated by GPT-SW3. The text marked in bold is text written by the researchers while the rest is content generated by GPT-SW3.\\n\\nLOKAR \u00e4r en revolutionerande h\u00e5rsspray gjord fr\u00e5n organiska material. Sprayen g\u00f6r att h\u00e5ret gl\u00e4nser i minst arton timmar och skyddar mot vardagligt slit p\u00e5 h\u00e5ret.\\n\\nLOKAR har en unik formula som g\u00f6r att den absorberas snabbt av h\u00e5ret och l\u00e4mnar det mjukt och glansigt. LOKAR ger h\u00e5ret en naturlig glans och g\u00f6r det enkelt att styla.\\n\\nSe vad v\u00e5ra kunder s\u00e4ger om LOKAR:\\n\\n\\\"Jag \u00e4lskar LOKAR! Den \u00e4r l\u00e4tt att applicera och ger ett naturligt resultat. Jag har anv\u00e4nt den i flera \u00e5r och den h\u00e5ller vad den lovar.\\\"\\n\\n\\\"Jag har haft LOKAR i m\u00e5nga \u00e5r och jag \u00e4lskar den. Den g\u00f6r h\u00e5ret glansigt och l\u00e4tt att styla.\\\"\\n\\n\\\"Den g\u00f6r h\u00e5ret l\u00e4tt att reda ut och ger en fin glans.\\\"\\n\\nA.3. Speeches\\n\\nA continuation of the Swedish King's annual Christmas speech. The prompt consisted of the King's first six paragraphs of his 2018 speech and can be found after this example.\\n\\nK\u00e4rar svenskar, hemma och utomlands, alla i Sverige!\\n\\nF\u00f6r mig \u00e4r julen en tid f\u00f6r eftertanke och reflektion. En tid att stanna upp och reflektera \u00f6ver det som varit och det som \u00e4r. En tid att minnas de som inte l\u00e4ngre finns med oss, men som \u00e5ndar \u00e4r med oss i v\u00e5ra tankar och minnen. En tid att gl\u00e4djas \u00e5t det vi har, gl\u00e4djas \u00e5t det vi har f\u00e5tt och gl\u00e4djas \u00e5t att vi f\u00e5r vara tillsammans med...\"}"}
{"id": "lrec-2022-1-376", "page_num": 8, "content": "{\"primary_language\":\"sv\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"K\u00e4rare svenskar, hemma och utomlands, alla i Sverige!\\n\\nJulen i v\u00e5rt land kan se ut p\u00e5 m\u00e5nga olika s\u00e4tt. M\u00e5nga firar med familj och v\u00e4nner, och samlas kring de traditioner som knyter oss samman \u00f6ver generationsgr\u00e4nserna.\\n\\nEn del \u00e4r ensamma, av eget val eller av annan orsak. Och andra firar inte jul, men gl\u00e4ds kanske and\u00e5 \u00e5t den s\u00e4rskilda st\u00e4mning som pr\u00e4lar julhelgen.\\n\\nOavsett hur Din jul ser ut, hoppas jag att dessa dagar ger Dig tillf\u00e4lle till reflektion och eftertanke, nu n\u00e4r \u00e5ret g\u00e5r mot sitt slut och ett nytt snart ska ta sin b\u00f6rjan.\\n\\nN\u00e4r jag sj\u00e4lv ser tillbaka p\u00e5 \u00e5ret som g\u00e5tt t\u00e4nker jag framf\u00f6r allt p\u00e5 alla de m\u00e4nniskor jag m\u00f6tt, runtom i v\u00e5rt land, som p\u00e5 olika s\u00e4tt valt att ta ansvar. Antingen i vardagen eller i extraordin\u00e4ra l\u00e4gen.\\n\\nGod Jul och Gott Nytt \u00c5r!\"}"}
{"id": "lrec-2022-1-376", "page_num": 9, "content": "{\"primary_language\":\"sv\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Att ge sig ut p\u00e5 en dagsvandring \u00e4r ett h\u00e4rligt s\u00e4tt att utforska naturen med v\u00e4nner och familj, eller till och med sj\u00e4lv. Oavsett om du vill \u00e5ka djupt in i bergen eller stanna n\u00e4rmare hemmet, \u00e4r st\u00e4llena att g\u00e5 till m\u00e5nga; m\u00e5nga nationalparker erbjuder broder n\u00e4tverk av stigar och skogsmark.\\n\\nDessa 10 saker b\u00f6r finnas p\u00e5 din vandringschecklista:\\n\\n1. Vandringsryggs\u00e4ck\\n2. Kl\u00e4der efter v\u00e4dret\\n3. Vandringsskor\\n4. Mycket mat\\n5. Vatten\\n6. Sovs\u00e4ck\\n7. F\u00f6rsta hj\u00e4lpen-kit\\n8. Mat\\n9. Tandstickor\\n10. T\u00e4lt\\n\\nH\u00e4r \u00e4r 10 tips att t\u00e4nka p\u00e5 innan du \u00e5ker:\\n\\n1. Planera din vandring\\nInnan du ger dig ut p\u00e5 en vandring, se till att du har en plan f\u00f6r hur du ska ta dig dit. Det kan vara allt fr\u00e5n hur du tar dig till platsen d\u00e4r du ska vandra, till hur du tar dig tillbaka.\\n\\n2. V\u00e4lj r\u00e4tt vandringsleder\\nDet finns m\u00e5nga vandringsleder i olika sv\u00e5righetsgrader, och det \u00e4r viktigt att du v\u00e4ljer en vandringsled som du k\u00e4nner dig bekv\u00e4m med. Om du \u00e4r os\u00e4ker p\u00e5 vilken led som \u00e4r b\u00e4st f\u00f6r dig, ta hj\u00e4lp av en guide.\\n\\n3. Ta med tillr\u00e4ckligt med mat\\nOm du planerar att vandra i flera dagar, se till att du har tillr\u00e4ckligt med mat f\u00f6r att klara dig i flera dagar. Du beh\u00f6ver mat och vatten f\u00f6r att h\u00e5lla dig varm och f\u00f6r att inte bli uttorkad.\\n\\n4. Packa smart\\nPacka inte f\u00f6r tungt, eftersom det kan g\u00f6ra det sv\u00e5rare f\u00f6r dig att ta dig fram. Packa l\u00e4tt, s\u00e5 att du kan ta med dig allt du beh\u00f6ver f\u00f6r hela vandringen.\\n\\n...and more\\n\\nA.6. Translation\\nFew-shot prompt used for the English translation task. Removing all but the first example sentence yields the one-shot prompt. Only including the prompt description equates to the zero-shot prompt. Parameters used in paper: Beams = 4, Temperature = 0, Top-p = 0, Top-k = 1000, and Repetition = 2.\\n\\nDet h\u00e4r \u00e4r en \u00f6vers\u00e4ttare. \u00d6vers\u00e4tt f\u00f6ljande meningar fr\u00e5n svenska till engelska.\\n\\nSvenska: Sara arbetar p\u00e5 ett bageri om helgen.\\nEngelska: Sara works at a bakery during the weekends.\\n\\nSvenska: Jag har spelat basket i tjugo \u00e5r.\\nEngelska: I've been playing basketball for twenty years.\\n\\nSvenska: N\u00e4r slutar du skolan idag?\\nEngelska: When do you finish school today?\\n\\nSvenska: Thomas vill flytta till Skottland.\\nEngelska: Thomas wants to move to Scotland.\\n\\nSvenska: Lejon \u00e4r v\u00e4rldens n\u00e4st st\u00f6rsta kattdjur.\\nEngelska: Lions are the world's second largest feline.\\n\\nSvenska: Sentence to be translated.\\nEngelska: A.7. Command-to-text\\nFew-shot prompt used for the email generation command-to-text task. Parameters used in paper: Beams = 3, Temperature = 0.70, Top-p = 0.70, Top-k = 1000, and Repetition = 2.\\n\\nSkriv mail till Jonas fr\u00e5n Adam. M\u00f6t 13-14 torsdag?\\nHej Jonas!\\nHar du m\u00f6jlighet f\u00f6r ett m\u00f6te klockan 13-14 nu p\u00e5 torsdag?\\nV\u00e4nliga h\u00e4lsningar,\\nAdam\\n\\nSkriv mail till Klara fr\u00e5n Bengt. Fr\u00e5ga snabbt om datorl\u00e5n. Beh\u00f6ver l\u00e5na den i tv\u00e5 veckor, samt mus och tangentbord.\\nHej Klara!\\nJag t\u00e4nkte bara h\u00f6ra med dig om det finns en m\u00f6jlighet att l\u00e5na en dator i tv\u00e5 veckor? Jag skulle \u00e4ven beh\u00f6va en mus och ett tangentbord under samma period. Det\"}"}
{"id": "lrec-2022-1-376", "page_num": 10, "content": "{\"primary_language\":\"sv\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"V\u00e4nliga h\u00e4lsningar,\\nBengt\\n\\n**B.** Additional Examples\\n\\nA generated thank-you speech to the \u201cDala Horse\u201d - a national cultural symbol in Sweden.\\n\\n*TACKTAL TILL DALAH\u00c4STEN*\\n\\nDalah\u00e4sten. \u00c4r du inte vacker? Med dina klara f\u00e4rger, stolta rygg och runda former? Om det \u00e4r en symbol som h\u00e5ller Sveriges fana h\u00f6gt s\u00e5 \u00e4r det du. Du finns i varje hem och hus, tapper som en tapet vars r\u00f6da f\u00e4rger aldrig flagnar.\\n\\nDet \u00e4r du som \u00e4r Dalah\u00e4sten.\\n\\nDet \u00e4r du som g\u00f6r oss stolta, det \u00e4r du som f\u00e5r oss att le och det \u00e4r du som f\u00e5r oss att k\u00e4nna stolthet \u00f6ver att vara svensk.\\n\\nVi \u00e4r stolta \u00f6ver dig, du \u00e4r v\u00e5rt nationalsymbol och vi \u00e4r stolta \u00f6ver att du finns i v\u00e5rt land. Vi \u00e4r stolta \u00f6ver dig och du \u00e4r v\u00e5rt stolthet.\\n\\nDu \u00e4r v\u00e4rld v\u00e5rt k\u00e4rlek och respekt, vi \u00e4lskar dig!\\n\\nTack f\u00f6r att du finns!\\n\\n#Dalah\u00e4stensdag #Dalah\u00e4stfanb\u00e4rare #Dalah\u00e4stdagen #Dalah\u00e4starna #Dalah\u00e4sten\\n\\nZero-shot translations by GPT-SW3 on two more complex sentences.\\n\\n**Original Sentence**\\n\\nAstronomi \u00e4r vetenskapen om himlakroppar och innefattar bland annat kosmologin, som f\u00f6rs\u00f6ker utveckla en beskrivning av uppkomsten, utvecklingen och strukturen av universum.\\n\\n**Zero-shot Translation**\\n\\nAstronomy is the science of the universe and includes, among other things, the cosmology, which deals with the creation, development and structure of the universe.\\n\\n**Original Sentence**\\n\\nEtt svart h\u00e5l \u00e4r, enligt den allm\u00e4nna relativitetsteorin, en koncentration av massa med ett s\u00e5 starkt gravitationsf\u00e4lt att ingenting, inte ens ljus, kan \u00f6vervinna kroppens gravitation.\\n\\n**Zero-shot Translation**\\n\\nThe black hole is, according to the general relativity the concentration of mass with a strong gravitational force that nothing, not even light, can overcome the gravitational force.\"}"}
