{"id": "acl-2022-long-543", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Can Pre-trained Language Models Interpret Similes as Smart as Human?\\n\\nQianyu He1\u2217, Sijie Cheng 1\u2217, Zhixu Li 1\u2020, Rui Xie 3, Yanghua Xiao 1,2\u2020\\n\\n1 Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n2 Fudan-Aishu Cognitive Intelligence Joint Research Center, Shanghai, China\\n3 Meituan, Shanghai, China\\n\\nqyhe21@m.fudan.edu.cn, rui.xie@meituan.com, {sjcheng20, zhixuli, shawyh}@fudan.edu.cn\\n\\nAbstract\\nSimile interpretation is a crucial task in natural language processing. Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art performance on many tasks. However, it remains under-explored whether PLMs can interpret similes or not. In this paper, we investigate the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, i.e., to let the PLMs infer the shared properties of similes. We construct our simile property probing datasets from both general textual corpora and human-designed questions, containing 1,633 examples covering seven main categories. Our empirical study based on the constructed datasets shows that PLMs can infer similes' shared properties while still underperforming humans. To bridge the gap with human performance, we additionally design a knowledge-enhanced training objective by incorporating the simile knowledge into PLMs via knowledge embedding methods. Our method results in a gain of 8.58% in the probing task and 1.37% in the downstream task of sentiment classification. The datasets and code are publicly available at https://github.com/Abbey4799/PLMs-Interpret-Simile.\\n\\n1 Introduction\\n\\nA simile is a figure of speech comparing two fundamentally different entities via shared properties (Paul, 1970). There are two types of similes as illustrated in Figure 1, closed similes explicitly reveal the shared properties between the topic entity and the vehicle entity, such as the property \\\"slow\\\" shared by \\\"lady\\\" and \\\"snail\\\" in the sentence \\\"The old lady walks as slow as a snail\\\"; while open similes do not state the shared property such as the sentence \\\"The old lady walks like a snail\\\". Similes play a vital role in human expression to make literal utterances more vivid and graspable and are widely used in the corpus of various domains (Liu et al., 2018; Chakrabarty et al., 2020a; Zhang et al., 2020). It is estimated that over 30% of the comparisons can be regarded as similes in product reviews (Niculae and Danescu-Niculescu-Mizil, 2014).\\n\\nSimile interpretation is a crucial task in natural language processing (Veale and Hao, 2007; Qadir et al., 2016; Chakrabarty et al., 2021a), which can assist several downstream tasks such as understanding more sophisticated figurative language (Veale and Hao, 2007) and sentiment analysis (Niculae and Danescu-Niculescu-Mizil, 2014; Qadir et al., 2015). Take the simile \\\"the lawyer is like a shark\\\" for an example. Although all words in this simile are neutral, this simile expresses a negative affect since \\\"lawyer\\\" and \\\"shark\\\" share the negative property \\\"aggressive\\\".\\n\\nIn the past few years, large pre-trained language models (PLMs) have achieved state-of-the-art performance on many natural language processing tasks (Devlin et al., 2018; Liu et al., 2019b). Recent studies suggest that PLMs have possessed various kinds of knowledge into contextual representations (Goldberg, 2019; Petroni et al., 2019; Lin et al., 2019; Cui et al., 2021). However, the ability of PLMs to interpret similes remains under-explored. Although some recent work (Chakrabarty et al., 2021a) studies the ability of PLMs in choosing or generating the plausible continuations in narratives, this way cannot fully reveal the ability of PLMs to interpret similes.\"}"}
{"id": "acl-2022-long-543", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Qualities\\nMy client is as [MASK] as a newborn lamb.\\nA. innocent\\nB. delicious\\nC. legal\\nD. guilty\\n\\nCondition\\nThe toddler was running around as [MASK] as a bee.\\nA. busy\\nB. yellow\\nC. idle\\nD. messy\\n\\nSense\\nHis anger was as [MASK] as a burning ember.\\nA. hot\\nB. red\\nC. cold\\nD. warm\\n\\nMeasurement\\nMy new baby brother is as [MASK] as a button.\\nA. red\\nB. tiny\\nC. cute\\nD. hot\\n\\nColor\\nHe was scared so much.\\nHe was as [MASK] as a ghost.\\nA. white\\nB. holy\\nC. gay\\nD. black\\n\\nTime\\nThe old man walks as [MASK] as a tortoise.\\nA. young\\nB. little\\nC. slow\\nD. quick\\n\\nEmotion\\nThe boy was as [MASK] as a dog that lost its bone.\\nA. happy\\nB. friendly\\nC. sad\\nD. glad\\n\\nTable 1: Percentage and examples for our simile probes of different categories. The option marked with \\\"\\\\[\\\\]\\\" indicates the correct answer. The italicized words one by one in each sentence are the topic, masked property, and vehicle, respectively.\\n\\nInspired by the sufficient hints offered by the components vehicle and topic in our empirical study, we propose a knowledge-enhanced training objective to further bridge the gap with human performance. Considering property (p) as the relation between topic (t) and vehicle (v), we design a simile knowledge embedding objective function following conventional knowledge embedding methods (Bordes et al., 2013) to incorporate the simile knowledge (t,p,v) into PLMs. To integrate simile knowledge and language understanding into PLMs, we jointly optimize the knowledge embedding objective and the MLM objective in our design. Overall, the knowledge-enhanced objective shows effectiveness in our probing task and the downstream task of sentiment classification.\\n\\nTo summarize, our contributions are three-fold:\\n(1) To our best knowledge, we are the first to systematically evaluate the ability of PLMs in interpreting similes via a proposed novel simile property probing task.\\n(2) We construct simile property probing datasets from both general textual corpora and human-designed questions, and the probing datasets contain 1,633 examples covering seven main categories of similes.\\n(3) We also propose a novel knowledge-enhanced training objective by complementing the MLM objective with the knowledge embedding objective. This method gains 8.58% in the probing task and 1.37% in the downstream task of sentiment classification.\\n\\n2 Preliminaries on Simile\\nA sentence of simile generally consists of five major components (Hanks, 2013; Niculae and Danescu-Niculescu-Mizil, 2014), where four are necessary and the remaining one is optional. The four explicit components are as follows:\\n(1) topic (or tenor): the subject of the comparison acting as...\"}"}
{"id": "acl-2022-long-543", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: A process for designing our simile property probing task. In Step 1, we collect closed similes from two different sources. In Step 2, according to four important components in each simile, we generate distractor candidates with three strategies. In Step 3, we adopt cosine similarity to select more challenging distractors. In Step 4, we ask human annotators to ensure the quality and obtain our final probing datasets.\\n\\n3 The Simile Property Probing Task\\n\\n3.1 Task Formulation\\n\\nTo estimate the ability of PLMs in simile interpretation, we design a particular Simile Property Probing task, which masks the explicit property of a closed simile, and then lets the PLMs discriminate it among four candidates. Considering that the shared properties between topic and vehicle may not be unique (Lacroix et al., 2005), we specifically design a multiple-choice question answering task (with only one correct answer) rather than a cloze task to probe the ability of PLMs to infer properties of similes, since the latter one may result in multiple correct answers.\\n\\nFormally, given a simile text sequence $S = (w_1, w_2, \\\\ldots, w_i-1, [MASK], w_i+1, \\\\ldots, w_N)$, where the shared property $w_i$ between the topic and vehicle is masked, the probing task requires the PLMs to find the correct property from four options, where the other three options are hard distractors.\\n\\n3.2 Probing Data Collection\\n\\nWe construct datasets for the proposed probing task in four steps. The overview of our probing data collection process is described in Figure 2.\\n\\n3.2.1 Data Sources\\n\\nWe construct our datasets from two different sources to detect the capability of PLMs from two perspectives: textual corpus collection and human-designed questions. To avoid laborious human labeling on the implicit properties of open similes, we collect closed similes with explicit properties.\\n\\nGeneral Corpus. Following (Hanks, 2005; Nicola and Yaneva, 2013), we adopt two general corpora, British National Corpus (BNC) and iWeb.\\n\\nTo identify closed similes, we extract the sentences matching the syntax as ADJ as (a, an, the) NOUN. Through syntactic pattern matching, we finally collect 1,917 sentences.\\n\\nTeacher-Designed Quizzes. Questions about similes designed by teachers from educational resources are ideal sources for assessing the ability to understand similes. Hence, we choose Quizizz, an online platform for creating and playing quizzes.\"}"}
{"id": "acl-2022-long-543", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"emerging learning platform founded in 2015. On this platform, users can create quizzes on a specific topic as teachers to assess students' understanding of related knowledge. We collect a set of quizzes with titles concerning similes and extract the complete closed simile sentences from the questions and answers in these quizzes. Finally, we retrieve 875 complete closed similes from 1,235 quizzes. To assure the quality of our constructed datasets and prepare for further analysis, three annotators are required to decide whether the extracted sentences are similes or not, and annotate their corresponding simile components. The inter-annotator agreement on identifying similes is 0.77 using Fleiss' Kappa score (Fleiss, 1971). All the properties in our datasets are single-token by replacing multi-token properties with their single-token synonyms in the knowledge base WordNet (Miller, 1995) and ConceptNet (Liu and Singh, 2004).\\n\\n3.2.2 Distractor Design\\n\\nTo make our probes convincing, three distractors are designed against the original property in each simile with two criteria (Haladyna et al., 2002; Ren and Zhu, 2020): true-negative and challenging. We argue that well-designed distractors should be illogical when filled into the questions (true-negative) while being semantically related to the correct answer (challenging). Our distractor design mainly involves three phases: 1) distractor generation; 2) distractor selection; 3) Human Confirmation.\\n\\nDistractor Generation. To meet the requirement of challenging, we generate distractor candidates from the four semantic-related components of a simile, i.e., topic, vehicle, event, and property. Given the original property, we harvest its antonyms from the knowledge base WordNet and ConceptNet. With regard to three other components, we extract their properties from two sources. As follows. Given a component, we utilize the HasProperty relation from ConceptNet (Liu and Singh, 2004) and COMET (Bosselut et al., 2019) to retrieve the property. Moreover, we rank the adjectives or adverbs concerning each component in Wikipedia and BookCorpus corpus by frequency and select the top ten candidates with a frequency of more than one.\\n\\nDistractor Selection. To select the most challenging distractors from the generated distractor candidates, we propose to measure the similarity between the original sentence with the correct property and the sentence with a distractor. Intuitively, the more similar the two sentences, the more challenging the distractor. An example of the distractor selection process is depicted in Figure 3. Given the original sentence or the new sentence replacing the correct property with a distractor, we first utilize RoBERTa LARGEn to extract two types of features. One feature is context embedding, which is the sentence embedding of [CLS], while the other feature is word embedding, which is the token embedding of the answer or distractors. We then concatenate the embeddings of the two features to compute the cosine similarity between sentences with the answer and a distractor. Finally, we select the top 3 distractors with the highest similarities.\\n\\nHuman Confirmation. To ensure the distractors are true-negative, three human annotators are asked to label each selected distractor. If more than two annotators are uncertain about its correctness, we replace it with another suitable candidate.\\n\\nWe adopt dependency parsing via the StanfordNLP tool to find adjectives and adverbs related to components. https://huggingface.co/datasets/\"}"}
{"id": "acl-2022-long-543", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2.3 Statistics of the Datasets\\n\\nTable 2 presents the statistics of our constructed datasets. We count unique components and component pairs to present the usage frequencies of similes. The length of the sentences in each dataset indicates the diversities of context. Additionally, we analyze the distribution of the position of simile in the sentences in each dataset, where start, middle, and end correspond to the positions of the three equally divided parts of each sentence. We also investigate the categories covered by our datasets. The results and details about the category classification are provided in Appendix C. Overall, the Quizzes dataset provides similes commonly expressed by people, while the General Corpus dataset presents similes with more diverse contexts.\\n\\n3.3 Supervision for Fine-Tuning PLMs\\n\\nBesides evaluating the ability of PLMs in the zero-shot setting where the models are off-the-shelf, we also study whether the performance could be improved through fine-tuning with the MLM objective via masking properties. To achieve this, we collect training data from Standardized Project Gutenberg Corpus (SPGC) (Gerlach and Font-Clos, 2020). SPGC is a 3 billion words corpus collected from about 60 thousand eBooks. We extract similes via matching the syntactic pattern (Noun ... as ADJ as ... NOUN) and end up with 4,510 sentences. Additionally, we adopt dependency parsing to automatically annotate the simile components of each sentence without human labor.\\n\\n4 Empirical Study on PLMs\\n\\nIn this section, we first conduct a set of experiments to probe the ability of PLMs to infer properties in similes and then evaluate the influence of each component on the model performance.\\n\\n4.1 Ability to Infer Shared Properties\\n\\n4.1.1 Experiment Set-up\\n\\nTo disentangle what is captured by the original representations and what is introduced from fine-tuning stage, we apply two different types of settings: (1) zero-shot; (2) fine-tuning. In our first setting, we use BERT and RoBERTa with pre-trained masked-word-prediction heads to perform our probing task. In the second setting, we utilize the MLM training objective inherited from PLMs to fine-tune the models. We replace the property of each simile with the special token [MASK] in our constructed supervised datasets (Section 3.3) and ask models to recover the original property. The experimental details are provided in the Appendix B.\\n\\nWe mainly compare the model accuracy of PLMs with the following baselines: (1) EMB (Qadir et al., 2016): It obtains the composite simile vector by performing an element-wise sum of the word embedding for the vehicle and event, then selects the answer with the shortest cosine distance from the composite vector. (2) Meta4meaning (Xiao et al., 2016): This method prefers the properties which are strongly associated with both topic and vehicle. It also prefers the properties that are more relevant to the vehicle than to the topic. The association is measured by statistical significance. (3) ConScore (Zheng et al., 2019): It suggests that better properties would have a smaller and balanced distance to the topic and vehicle in the word embedding space. (4) MIUWE (Bar et al., 2020): The ranking method assigns each property a list of scores, including the statistical co-occurrences and similarity to the collocations of the topic and vehicle. The baselines above mainly consider the statistical information and embedding similarities between the properties and the simile components. The other baseline is human performance. We sample 250 random questions from both datasets, and for each question, we gather answers from three people. We take the majority vote as the human performance of our probing task and ensure that three annotators agree on the questions that they gave completely different annotation results.\\n\\n4.1.2 Results\\n\\nThe accuracies of different methods under two different settings on our datasets are listed in Table 3.\"}"}
{"id": "acl-2022-long-543", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the last column represents the average absolute gains of each PLM after fine-tuning with the MLM objective. All the results of our experiments are averaged over three random seeds. First of all, the prediction accuracies of both BERT and RoBERTa in the zero-shot setting are much higher than the baselines only considering the statistical information and embedding similarities between simile components. This phenomenon indicates that the knowledge learning from the pre-train stage can help infer the simile properties. Moreover, the performance can be further improved by training with the MLM objective, demonstrating that the fine-tuning phase with the supervised dataset can introduce related knowledge about similes. However, models still underperform humans by several accuracy points, leaving room for improvement in our probing task.\\n\\nOverall, all the models perform better on Quizzes Dataset than on General Corpus Dataset, indicating that more diverse contexts increase the difficulty of inferring the shared properties. Also, RoBERTa consistently outperforms BERT, likely due to a larger pre-training corpus containing more similes. More complementary results are provided in the Appendix A.1.\\n\\n4.2 Influence of Important Components\\n\\n4.2.1 Experiment Set-up\\n\\nDue to the high performance of off-the-shelf PLMs, we are interested in the contributions of each component to infer shared properties in the zero-shot setting. First, the information of each component is hidden through a certain strategy. Specifically, for topic, vehicle and comparator, we replace their tokens with a special token [UNK] which means unknown. With regard to event, we convert it into a suitable copula, such as \u201cam\u201d and \u201cis\u201d, to ensure the integrity of syntax. Furthermore, we also set up a baseline by randomly replacing a token with [UNK] in the context. Examples corresponding to all settings are shown in Table 8 in the Appendix B. We finally report the model accuracy and declined absolute accuracy after hiding the information of each component.\\n\\n4.2.2 Results\\n\\nThe results in Table 4 show varying degrees of the decline of all settings. If the model\u2019s performance decreases more, it means that the influence of the component is more significant than others. Three major components (i.e., vehicle, topic and comparator) obtain higher declined absolute accuracy than random token, which demonstrates that the information of these simile components is more valuable than other words to infer the shared properties. Among all the components, removing the comparator may cause the most significant performance drop. This result is mostly because PLMs cannot identify the sentence as a simile without an obvious indicator. When it comes to the remaining 3 components, vehicle contributes the most, followed by topic. Hence, we argue that it may be beneficial to explicitly leverage both the information of vehicle and topic to infer the properties.\\n\\n5 Enhancing PLMs with Knowledge\\n\\n5.1 Knowledge-enhanced Objective\\n\\nBenefiting from the result that topic and vehicle are the two most essential components for predicting the shared properties of similes, we catch an insight that property can be seen as the relation between topic and vehicle following a set of knowledge embedding (KE) methods (Bordes et al., 2013; Wang et al., 2014; Ji et al., 2015).\\n\\nTo integrate the insight mentioned above into our training procedure, we design an objective function as shown in Figure 4. Inspired by triplets representing the relational facts, we can also extract the topic, property, and vehicle from a simile as a triplet $(t, p, v)$. The distance between topic and vehicle in the embedding space represents the plausibility of property. The plausibility can be measured by scoring functions (Bordes et al., 2013; Wang et al., 2014; Ji et al., 2015). To this end, we follow the scoring function from TransE (Bordes et al., 2013) and define the following Mean Square Error (MSE) loss as our KE loss:\\n\\n$$L_{KE} = \\\\text{MSE}(E_t + E_p, E_v)$$ (1)\"}"}
{"id": "acl-2022-long-543", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Datasets Models | Topic | Vehicle | Event | Comparator | Random |\\n|-----------------|-------|---------|-------|------------|--------|\\n| **General Corpus** | **BERT BASE** | 67.74 | 69.25 | +1.51 | |\\n| | **BERT LARGE** | 73.85 | 74.07 | +0.22 | |\\n| | **RoBERTa BASE** | 70.58 | 71.74 | +1.16 | |\\n| | **RoBERTa LARGE** | 78.97 | 78.97 | +0.00 | |\\n| **Quizzes** | **BERT BASE** | 82.05 | 82.94 | +0.89 | |\\n| | **BERT LARGE** | 84.58 | 85.94 | +1.36 | |\\n| | **RoBERTa BASE** | 84.69 | 84.89 | +0.20 | |\\n| | **RoBERTa LARGE** | 88.97 | 89.40 | +0.43 | |\\n\\nTable 4: Accuracy of PLMs in the zero-shot setting before and after hiding the information of each component on two datasets.\\n\\n| Datasets Models | Topic | Vehicle | Event | Comparator | Random |\\n|-----------------|-------|---------|-------|------------|--------|\\n| **General Corpus** | **BERT BASE** | 84.96 | 85.45 |     | 85.63 |\\n| | **BERT LARGE** | 86.02 | 86.65 |     | 86.95 |\\n| | **RoBERTa BASE** | 88.51 | 88.61 |     | 89.51 |\\n| | **RoBERTa LARGE** | 88.84 | 89.08 |     | 90.21 |\\n\\nTable 5: Accuracy of PLMs using MLM and our objectives in our probing task.\\n\\nTable 6: Accuracy of PLMs with three settings in the downstream task of sentiment classification.\\n\\n5.2 Results\\n\\nTable 5 presents the performance of the models fine-tuned with the MLM objective and our knowledge-enhanced objective on the two datasets, where the last column shows the performance gains brought by our improvement to the training objective. Overall, each model trained with our knowledge-enhanced objective outperforms the one trained with the MLM objective on both datasets, demonstrating the effectiveness of our objective in the probing task.\\n\\nFor the Quizzes dataset, BERT achieves more performance gains than RoBERTa does, which is probably because RoBERTa has better modeled the relationship among topic, property, and vehicle in the similes with simple syntactic structure during fine-tuning with the MLM objective.\\n\\n5.3 Experiments with Downstream Tasks\\n\\nSimiles generally transmit a positive or negative view due to the shared properties (Fishelov, 2007; Li et al., 2012; Qadir et al., 2015). Taking the simile \\\"the lawyer is like a shark\\\" as an example, the implicit shared property \\\"aggressive\\\" between \\\"lawyer\\\" and \\\"shark\\\" indicates the negative polarity. Therefore, we design a sentiment polarity downstream task to validate the improvement of our method to infer shared properties.\\n\\nOur experiments are based on the Amazon reviews dataset which provides reviews and their corresponding sentiment ratings. Following (Mudinas et al., 2012; Haque et al., 2018), we first process the dataset into a binary sentiment classification task by defining the 1-star and 2-star ratings as negative, the 4-star, and 5-star ratings as positive, while excluding the 3-star neutral ratings. To further address the label imbalance problem, we then sample the positive and negative reviews at 1:1. The final dataset consists of 5,023 reviews and is split into a ratio of 6:2:2 for the train/dev/test set.\"}"}
{"id": "acl-2022-long-543", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: The average semantic distances between the representations of topic(t), property(p), and vehicle(v) in the last layer's hidden state given by BERT BASE with MLM and our objectives.\\n\\nLayer perceptron (MLP) classifiers on top of PLM's contextualized representation. The parameters of PLM are fixed and from three settings: (1) zero-shot; (2) fine-tuned with the MLM objective in the probing task; (3) fine-tuned with the knowledge-enhanced objective in the probing task. The results are shown in Table 6. First of all, fine-tuning with the MLM objective improves the performance of all models in the sentiment classification task, demonstrating that improving models' ability to infer the properties of similes can enhance models' understanding of the sentiment polarity. Moreover, the performance is further improved by our knowledge-enhanced objective, especially for RoBERTa whose main gains are mostly contributed by our additional knowledge embedding objective. This indicates the effectiveness of our knowledge-enhanced objective in the downstream task of sentiment analysis.\\n\\n5.4 Analysis\\n\\nFurthermore, we investigate the mechanism of how knowledge-enhanced objective brings improvement. We first calculate the L2 distance between the representations in the last hidden states of each pair of components. The results are shown in Figure 5. In all pairs, the distance given by our objective is generally shorter than MLM-BERT, which indicates that modeling the relationships among the three important components is efficient to enhance the model performance.\\n\\nSpecifically, we visualize the final layer representation of a simile into two-dimensional spaces via Principal Component Analysis (PCA) (Pearson, 1901) in Figure 6. In both MLM and our objective, the models are required to fill in the masked token in the same simile sentence. The model fine-tuned with the MLM objective predicts wrongly, while our fine-tuned model predicts correctly. We find that our representations of the three components are closer to each other.\\n\\n6 Related Work\\n\\nSimile Processing. Simile processing mainly involves 3 fields: simile detection, simile generation, and simile interpretation. The bulk of work in similes mainly focuses on identifying similes and their components (Niculae, 2013; Niculae and Danescu-Niculescu-Mizil, 2014; Liu et al., 2018; Zeng et al., 2020). Recent years have witnessed a growth of work to transfer literal sentences to similes (Zhang et al., 2020; Chakrabarty et al., 2020b). (Chakrabarty et al., 2021b) study the ability of PLMs to recognize textual entailment related to similes. With regard to simile interpretation, (Qadir et al., 2016; Xiao et al., 2016; Bar et al., 2020; Zheng et al., 2019) rank the properties by the statistical co-occurrence and embedding similarities with other simile components. (Chakrabarty et al., 2021a) interpret similes by choosing or generating continuation for narratives via PLMs. Different from these works, we investigate the ability of PLMs to infer shared properties of similes.\\n\\nProbing Tasks for PLMs. Many studies investigate whether PLMs encode knowledge in their contextual representations by designing probing tasks. Early studies mainly focus on the linguistic knowledge captured by PLMs (Liu et al., 2019a; Tenney et al., 2019). (Petroni et al., 2019) first propose a word prediction task to probe factual...\"}"}
{"id": "acl-2022-long-543", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"knowledge stored in PLMs. Similar methods are utilized to evaluate various commonsense knowledge, such as symbolic reasoning ability (Talmor et al., 2020; Zhou et al., 2020), numerical commonsense knowledge (Lin et al., 2020), properties associated with concepts (Weir et al., 2020). To our best knowledge, we are the first work to investigate the ability of PLMs to interpret similes by proposing a simile property probing task.\\n\\nEnhance PLMs via Knowledge Regularization.\\n\\nRecently, many researchers integrate external knowledge with PLMs by complementing the MLM objective with an auxiliary knowledge-based objective. For example, there are works that introduce span-boundary objective for span-level prediction (Joshi et al., 2020), copy-based training objective for mention reference prediction (Ye et al., 2020), knowledge embedding objective for factual knowledge (Wang et al., 2021) and arithmetic relationships of linguistic units for universal language representation (Li and Zhao, 2021). Different from these works, we incorporate simile knowledge with the training objective by modeling the relationship between the salient components of similes.\\n\\n7 Conclusion\\n\\nIn this work, we are the first to investigate the ability of PLMs in simile interpretation via a proposed novel simile property probing task. We construct two multi-choice probing datasets covering two data sources. By conducting a series of empirical experiments, we prove that PLMs exhibit the ability to infer simile properties in the pre-training stage and further induce more related knowledge during the fine-tuning stage, but there is still a gap between PLMs and humans in this task. Furthermore, we propose a knowledge-enhanced training objective to bridge the gap, which shows effectiveness in the probing task and the downstream task of sentiment classification. In future work, we are interested in exploring the interpretation of more sophisticated figurative language, such as metaphor or analogy.\\n\\nAcknowledgements\\n\\nWe would like to thank anonymous reviews for their helpful comments and suggestions. Also, thanks to Jingping Liu, Leyang Cui for their insightful feedback that helped improve the paper. We also thank Botian Jiang, Shuang Li for supporting our data collection. This research was supported by the National Key Research and Development Project (No. 2020AAA0109302), National Natural Science Foundation of China (No. 62072323), Shanghai Science and Technology Innovation Action Plan (No. 19511120400), Shanghai Municipal Science and Technology Major Project (No. 2021SHZDZX0103).\\n\\nEthical Consideration\\n\\nWe provide details of our work to address potential ethical concerns. In our work, we propose a simile property probing task and construct probing datasets from both general textual corpora and human-designed questions. First of all, all the data sources used in the data collection process are publicly available. Specifically, we follow the robots.txt to respect the copyright when we collect similes from the learning platform Quizizz (Sec. 3.2.1). Moreover, there are three steps involving human annotation to ensure the quality of the datasets: simile and simile components recognition (Sec. 3.2.1), human confirmation for distractors (Sec. 3.2.2), and human performance (Sec. 4.1). To ensure the quality of annotation, all the annotators do not participate in our probing data collection, and they always label a small set of 50 examples to reach an agreement on the labeling criteria before the formal labeling. We protect the privacy rights of annotators and pay them above the local minimum wage.\\n\\nReferences\\n\\nKfir Bar, Nachum Dershowitz, and Lena Dankin. 2020. Automatic metaphor interpretation using word embeddings. arXiv preprint arXiv:2010.02665.\\n\\nMonroe C Beardsley. 1981. Aesthetics, problems in the philosophy of criticism. Hackett Publishing.\\n\\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. Advances in neural information processing systems, 26.\\n\\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. Comet: Commonsense transformers for automatic knowledge graph construction. arXiv preprint arXiv:1906.05317.\\n\\nTuhin Chakrabarty, Yejin Choi, and Vered Shwartz. 2021a. It's not rocket science: Interpreting figurative language in narratives. arXiv preprint arXiv:2109.00087.\"}"}
{"id": "acl-2022-long-543", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tuhin Chakrabarty, Debanjan Ghosh, Adam Poliak, and Smaranda Muresan. 2021b. Figurative language in recognizing textual entailment. arXiv preprint arXiv:2106.01195.\\n\\nTuhin Chakrabarty, Smaranda Muresan, and Nanyun Peng. 2020a. Generating similes effortlessly like a pro: A style transfer approach for simile generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6455\u20136469, Online. Association for Computational Linguistics.\\n\\nTuhin Chakrabarty, Smaranda Muresan, and Nanyun Peng. 2020b. Generating similes effortlessly like a pro: A style transfer approach for simile generation. arXiv preprint arXiv:2009.08942.\\n\\nLeyang Cui, Sijie Cheng, Yu Wu, and Yue Zhang. 2021. On commonsense cues in bert for solving commonsense tasks. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 683\u2013693.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\\n\\nDavid Fishelov. 2007. Shall i compare thee? simile understanding and semantic categories.\\n\\nJoseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.\\n\\nMartin Gerlach and Francesc Font-Clos. 2020. A standardized project gutenberg corpus for statistical analysis of natural language and quantitative linguistics. Entropy, 22(1):126.\\n\\nYoav Goldberg. 2019. Assessing bert\u2019s syntactic abilities. arXiv preprint arXiv:1901.05287.\\n\\nThomas M Haladyna, Steven M Downing, and Michael C Rodriguez. 2002. A review of multiple-choice item-writing guidelines for classroom assessment. Applied measurement in education, 15(3):309\u2013333.\\n\\nPatrick Hanks. 2005. Similes and sets: The english preposition like.\\n\\nPatrick Hanks. 2013. Lexical analysis: Norms and exploitations. Mit Press.\\n\\nTanjim Ul Haque, Nudrat Nawal Saber, and Faisal Muhammad Shah. 2018. Sentiment analysis on large scale amazon product reviews. In 2018 IEEE international conference on innovative research and development (ICIRD), pages 1\u20136. IEEE.\\n\\nGuoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Knowledge graph embedding via dynamic mapping matrix. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 687\u2013696.\\n\\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64\u201377.\\n\\nJoyca PW Lacroix, Jaap MJ Murre, and Eric O Postma. 2005. Interpretive diversity as a source of metaphor-simile distinction. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 27.\\n\\nBin Li, Haibo Kuang, Yingjie Zhang, Jiajun Chen, and Xuri Tang. 2012. Using similes to extract basic sentiments across languages. In International Conference on Web Information Systems and Mining, pages 536\u2013542. Springer.\\n\\nYian Li and Hai Zhao. 2021. Pre-training universal language representation. arXiv preprint arXiv:2105.14478.\\n\\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. Kagnet: Knowledge-aware graph networks for commonsense reasoning. arXiv preprint arXiv:1909.02151.\\n\\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. 2020. Birds have four legs?! numenthersense: Probing numerical commonsense knowledge of pre-trained language models. arXiv preprint arXiv:2005.00683.\\n\\nHugo Liu and Push Singh. 2004. Conceptnet\u2014a practical commonsense reasoning tool-kit. BT technology journal, 22(4):211\u2013226.\\n\\nLizhen Liu, Xiao Hu, Wei Song, Ruiji Fu, Ting Liu, and Guoping Hu. 2018. Neural multitask learning for simile recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1543\u20131553.\\n\\nNelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew E Peters, and Noah A Smith. 2019a. Linguistic knowledge and transferability of contextual representations. arXiv preprint arXiv:1903.08855.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nGeorge A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39\u201341.\"}"}
{"id": "acl-2022-long-543", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Andrius Mudinas, Dell Zhang, and Mark Levene. 2012. Combining lexicon and learning based approaches for concept-level sentiment analysis. In Proceedings of the first international workshop on issues of sentiment discovery and opinion mining, pages 1\u20138.\\n\\nVlad Niculae. 2013. Comparison pattern matching and creative simile recognition. In Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora, pages 110\u2013114.\\n\\nVlad Niculae and Cristian Danescu-Niculescu-Mizil. 2014. Brighter than gold: Figurative language in user generated comparisons. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 2008\u20132018.\\n\\nVlad Niculae and Victoria Yaneva. 2013. Computational considerations of comparisons and similes. In 51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop, pages 89\u201395.\\n\\nAnthony M Paul. 1970. Figurative language. Philosophy & Rhetoric, pages 225\u2013248.\\n\\nKarl Pearson. 1901. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559\u2013572.\\n\\nFabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowledge bases? arXiv preprint arXiv:1909.01066.\\n\\nAshequl Qadir, Ellen Riloff, and Marilyn Walker. 2015. Learning to recognize affective polarity in similes. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 190\u2013200.\\n\\nAshequl Qadir, Ellen Riloff, and Marilyn Walker. 2016. Automatically inferring implicit properties in similes. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1223\u20131232.\\n\\nSiyu Ren and Kenny Q Zhu. 2020. Knowledge-driven distractor generation for cloze-style multiple choice questions. arXiv preprint arXiv:2004.09853.\\n\\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020. olmpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics, 8:743\u2013758.\\n\\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. 2019. What do you learn from context? probing for sentence structure in contextualized word representations. arXiv preprint arXiv:1905.06316.\\n\\nTony Veale and Yanfen Hao. 2007. Learning to understand figurative language: From similes to metaphors to irony. In Proceedings of the annual meeting of the cognitive science society, volume 29.\\n\\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021. Kepler: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics, 9:176\u2013194.\\n\\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 28.\\n\\nNathaniel Weir, Adam Poliak, and Benjamin Van Durme. 2020. Probing neural language models for human tacit assumptions. arXiv preprint arXiv:2004.04877.\\n\\nPing Xiao, Khalid Alnajjar, Mark Granroth-Wilding, Kat Agres, Hannu Toivonen, et al. 2016. Meta4meaning: Automatic metaphor interpretation using corpus-derived word associations. In Proceedings of the Seventh International Conference on Computational Creativity. Sony CSL Paris.\\n\\nDeming Ye, Yankai Lin, Jiaju Du, Zhenghao Liu, Peng Li, Maosong Sun, and Zhiyuan Liu. 2020. Coreferential reasoning learning for language representation. arXiv preprint arXiv:2004.06870.\\n\\nJiali Zeng, Linfeng Song, Jinsong Su, Jun Xie, Wei Song, and Jiebo Luo. 2020. Neural simile recognition with cyclic multitask learning and local attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9515\u20139522.\\n\\nJiayi Zhang, Zhi Cui, Xiaoqiang Xia, Yalong Guo, Yanran Li, Chen Wei, and Jianwei Cui. 2020. Writing polishing with simile: Task, dataset and a neural approach. arXiv preprint arXiv:2012.08117.\\n\\nDanning Zheng, Ruihua Song, Tianran Hu, Hao Fu, and Jin Zhou. 2019. \u201clove is as complex as math\u201d: Metaphor generation system for social chatbot. In Workshop on Chinese Lexical Semantics, pages 337\u2013347. Springer.\\n\\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan Huang. 2020. Evaluating commonsense in pre-trained language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9733\u20139740.\"}"}
{"id": "acl-2022-long-543", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Additional Experimental Results\\n\\nA.1 Performance on Different Categories\\nWe investigate whether PLMs are better at inferring the properties of certain categories. Figure 7 presents the performance of the strongest version from each group of models for each category in the zero-shot setting. We found that models perform significantly well when inferring the color, which is probably because each object often has a specific color which in many cases can be inferred without context. However, when it comes to the properties requiring an understanding of the context, such as the personality and qualities (intelligent, brave), temporal properties (ancient, swift) and short-term state (busy, safe), models tend to have relatively lower accuracy.\\n\\nFigure 7: The average accuracy for each category in the zero-shot setting. We select the strongest version from each group of models.\\n\\nA.2 Comparison of Knowledge Embedding Methods\\nWe also exploit the effects of different knowledge embedding methods when designing our knowledge-enhanced objective. Table 7 shows the performance given by the objectives applying different knowledge embedding methods. First of all, complementing the MLM objective with our knowledge embedding methods generally improves the performance, demonstrating the effectiveness of our approach to enhancing PLMs with simile knowledge. Moreover, following the scoring function from TransE (Bordes et al., 2013) brings the best result in most cases, which indicates that the knowledge embedding methods of simple design are sufficient to incorporate simile knowledge into PLMs in our objective design.\\n\\n| Datasets   | Models   | L    | MLM   | L Ours | L TransH | L TransD |\\n|------------|----------|------|-------|--------|----------|----------|\\n| General    | BERT BASE| 67.74| 69.25 | 69.72  | 68.38    |          |\\n|            | BERT LARGE| 73.85| 74.07 | 74.33  | 73.85    |          |\\n|            | RoBERTa BASE| 70.58 | 71.74 | 71.18  | 70.97    |          |\\n|            | RoBERTa LARGE| 78.97| 78.97 | 78.97  | 78.97    |          |\\n| Quizzes    | BERT BASE| 82.05| 82.94 | 82.25  | 82.05    |          |\\n|            | BERT LARGE| 84.58| 85.94 | 85.24  | 84.69    |          |\\n|            | RoBERTa BASE| 84.69| 84.89 | 84.81  | 84.81    |          |\\n|            | RoBERTa LARGE| 88.97| 89.40 | 89.32  | 88.96    |          |\\n\\nTable 7: Comparison of different knowledge embedding methods when designing the knowledge-enhanced objective in our probing task.\\n\\nB Experimental Details\\nWe introduce details about the implementation of our experiments. The implementations of all the PLMs in our paper are based on the HuggingFace Transformers. During fine-tuning for the probing task, the experiments are run with batch sizes in \\\\{8, 16\\\\}, \\\\(\\\\alpha\\\\) in \\\\{3, 5, 10\\\\}, a max sequence length of 128, and a learning rate of 1e-5 for 10 epochs. For each model, we use the same hyper-parameters when applying different training objectives. During fine-tuning for the sentiment analysis task, we only update the parameters of the multi-layer perceptron (MLP) classifiers on top of PLM's contextualized representation. We set the learning rate in \\\\{2e-5, 3e-5, 4e-5\\\\}, batch size of 32, max sequence length of 128 and train for 200 epochs. Additionally, we present examples of the experimental setup for evaluating the influence of important components in Table 8.\\n\\n| Component   | Sentence Example |\\n|-------------|------------------|\\n| Original    | Johan runs as [MASK] as a deer to the toilet after he had some spicy gravy. |\\n| Topic       | [UNK] runs as [MASK] as a deer to the toilet after he had some spicy gravy. |\\n| Vehicle     | Johan runs as [MASK] as [UNK] to the toilet after he had some spicy gravy. |\\n| Event       | Johan is as [MASK] as a deer to the toilet after he had some spicy gravy. |\\n| Comparator  | Johan runs [UNK] [MASK] [UNK] as a deer to the toilet after he had some spicy gravy. |\\n| Random      | Johan runs as [MASK] as a deer [UNK] the toilet after he had some spicy gravy. |\\n\\nTable 8: Examples of experiment set-up for evaluating the influence of important components.\\n\\nC Dataset Description\\nWe introduce details about our classification of the categories of properties. We ask two annotators to label the category of each property in the given sentence and ensure that they agree on the questions they gave completely different annotation.\"}"}
{"id": "acl-2022-long-543", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category | Property Example | Percentage |\\n|----------|-----------------|------------|\\n| Qualities | strong, weak, cruel, intelligent, brave | 27.78 |\\n| Condition | bad, busy, idle, safe, vain | 22.28 |\\n| Sense | cold, warm, bitter, soft, loud | 17.20 |\\n| Measurement | big, scarce, numerous, tall, tiny | 14.16 |\\n| Color | red, black, green, white, blue | 06.75 |\\n| Time | ancient, new, swift, slow, regular | 06.57 |\\n| Emotion | excited, angry, sad, mad, nervous | 05.26 |\\n\\nTable 9: Percentage and examples of each category of properties in constructed simile property probing datasets. Results. Table 9 shows the percentage and five examples for each category (possibly more than one category per property). In particular, properties in Qualities describe the long-term feature of a material or a person's character, while properties in Condition depict a short-term state. Table 1 presents the percentage and examples for our simile probes of different categories.\"}"}
