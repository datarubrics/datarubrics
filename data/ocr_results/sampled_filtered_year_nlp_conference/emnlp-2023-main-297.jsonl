{"id": "emnlp-2023-main-297", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EDIS: Entity-Driven Image Search over Multimodal Web Content\\n\\nSiqi Liu1\u2217 Weixi Feng2\u2217 Tsu-jui Fu2 Wenhu Chen3 William Yang Wang2\\n\\n1 Cornell University 2 UC Santa Barbara 3 University of Waterloo, Vector Institute\\n\\nAbstract\\n\\nMaking image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce Entity-Driven Image Search (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching.\\n\\nTo achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse textual and visual representations. Our experimental results show that EDIS challenges state-of-the-art methods with dense entities and the large-scale candidate set. The ablation study also proves that fusing textual features with visual features is critical in improving retrieval results.\\n\\n1 Introduction\\n\\nImage search, also known as text-to-image retrieval, is to retrieve matching images from a candidate set given a text query. Despite the advancements in large-scale vision-and-language models (Wang et al., 2021; Zhang et al., 2021; Chen et al., 2020; Li et al., 2020b), accurately retrieving images from a large web-scale corpus remains a challenging problem. There remain several critical issues: 1) Lack of large-scale datasets: existing image retrieval datasets typically contain 30K-100K images, which is far less than the number of images that search engines must deal with in real applications. 2) Insufficient entity-specific content: existing datasets focus on generic objects without specific identities. Specific entities (\u201cStatue of Liberty\u201d) in web images and text may be recognized as general objects (\u201cbuilding\u201d). 3) Modality mismatch: existing image retrieval methods usually measure image-text similarity. However, for web image search, the surrounding text of an image also plays a crucial part in this fast and robust retrieval process.\\n\\nRecently, there has been a continuous interest in event-centric tasks and methods in the news domain (Reddy et al., 2021; Varab and Schluter, 2021).\"}"}
{"id": "emnlp-2023-main-297", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, to tackle the aforementioned three key challenges, we introduce a large-scale dataset named Entity-Driven Image Search \\\\( (EDIS) \\\\) in the news domain. As is shown in Fig. 1, \\\\( EDIS \\\\) has a much larger candidate set and more entities in the image and text modalities. In addition to images, text segments surrounding an image are another important information source for retrieval. In news articles, headlines efficiently summarize the events and impress readers in the first place (Pan-thaplackel et al., 2022; Gabriel et al., 2022). Hence, to simulate web image search with multi-modal information, we pair each image with the news headline as a textual summarization of the event. As a result, \\\\( EDIS \\\\) requires models to retrieve over image-headline candidates, which is a novel setup over existing datasets.\\n\\nGiven a text query, existing models can only measure query-image or query-text similarity alone. BM25 (Robertson et al., 2009), and DPR (Karpukhin et al., 2020) fail to utilize the visual features, while vision-language models like Visual-Bert (Li et al., 2019) and Oscar (Li et al., 2020b) cannot be adopted directly for image-headline candidates and are infeasible for large-scale retrieval. Dual-stream encoder designs like CLIP (Radford et al., 2021) are efficient for large-scale retrieval and can compute a weighted sum of query-image and query-text similarities to utilize both modalities. However, as is shown later, such multi-modal fusion is sub-optimal for \\\\( EDIS \\\\). In this work, we evaluate image retrieval models on \\\\( EDIS \\\\) and reveal that the information from images and headlines cannot be effectively utilized with score-level fusion. Therefore, we further proposed a feature-level fusion method to utilize information from both images and headlines effectively. Our contribution is three-fold:\\n\\n- We collect and annotate \\\\( EDIS \\\\) for large-scale image search, which characterizes single-modality queries and multi-modal candidates. \\\\( EDIS \\\\) is curated to include images and text segments from open sources that depict a significant amount of entities and events.\\n- We propose a feature-level fusion method for multi-modal inputs before measuring alignment with query features. We show that images and headlines are exclusively crucial sources for accurate retrieval results yet cannot be solved by naive reranking.\\n- We evaluate existing approaches on \\\\( EDIS \\\\) and demonstrate that \\\\( EDIS \\\\) is more challenging than previous datasets due to its large scale and entity-rich characteristics.\"}"}
{"id": "emnlp-2023-main-297", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Candidates (1M)\\n\\nAnnotate relevant score of the pairs with MTurk\\nCollect challenging images.\\n\\nAs shown in Table 1, we end up with 32,493 queries split into 26K/3.2K/3.2K for train/validation/test.\\n\\nWe select queries and candidates from human-annotators to label relevance scores. Fig. 2 illustrates the overall dataset collection pipeline. The process consists of query selection, candidate collection and annotation, and hard negative mining.\\n\\nWe extract queries and ground truth images from articles that have a headline, an image, and an image-headline pairs as the retrieval candidates. We adopt captions as text queries and use image-headline pairs as the retrieval candidates. As is shown in Fig. 2, we first evaluate the complexity of queries and remove simple ones with less than ten tokens. 2) Query entity count: we use spaCy to estimate average entity counts in the remaining queries and remove 20% queries with the lowest entity counts. 3) Query-image similarity: to ensure that the result of the query set has an average entity count above 4.0. 4) Query-text similarity: we remove 20% queries with the lowest entity counts.\\n\\nWe design a series of four filters to select high-quality image queries from the entire candidate set. The distractor setup is similar to conventional text-to-image retrieval using image, headline, and text modality and, thus, are much more efficient than methods named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch.\\n\\nThe resulting query set has an average entity count denoted by \\\\( \\\\text{avg} \\\\). We evaluate approaches with both Full-scale (1M) and limited set (25K (image, headline) pairs).\\n\\nThe full setting requires the model to retrieve images from the entire candidate set. The distractor setup is similar to conventional text-to-image retrieval using image, headline, and text modality and, thus, are much more efficient than methods named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch. In this work, we propose a method named mBLIP to perform feature-level encoding branch.\"}"}
{"id": "emnlp-2023-main-297", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Retrieval Candidate Text Query | # Img Modality Source Label # | Train | Val | Test | Entity |\\n|-------------------------------|--------------------------------|-------|-----|------|--------|\\n| Flickr30K (Plummer et al., 2015) | 1K Image Flickr Binary | 145K | 5K  | 5K   | 0.35   |\\n| MSCOCO (Lin et al., 2014)    | 5K Image Flickr Binary | 566K | 25K | 25K  | 0.18   |\\n| CxC (Parekh et al., 2021)    | 5K Image Flickr Continuous - | 25K  | 25K | -    | 0.18   |\\n| ADE20K (Changpinyo et al., 2021) | 2K Image Flickr Binary | 20K  | 2K  | -    | 0.16   |\\n| WebQA (Chang et al., 2022)  | 390K Text/Image Wikimedia Binary | 34K  | 5K  | 7.5K | 1.96   |\\n| EDIS (ours)                  | 1M Image-Text Open (Google) | 26K  | 3.2K| 3.2K | 4.03   |\\n\\nTable 1: Statistics of EDIS and existing image retrieval datasets. EDIS has a larger set of multi-modal candidates, unrestricted image sources, multi-scale annotations, and entity-rich queries compared to previous datasets.\\n\\nFigure 3: Left: Annotated candidates distribution by relevance score. Right: Query distribution by the scores of annotated candidates.\\n\\n4.2 Candidate Collection\\n\\nIn web image search experience, multiple relevant images exist for a single query. Therefore, we expand the candidate pool so that each query corresponds to multiple image-headline pairs. Additional candidates are collected from Google Image Search and the rest of the VisualNews dataset. For each query from VisualNews, we select seven image-headline pairs from Google search. For each query from TARA, we select five image-headline pairs from Google search and two image-headline pairs from VisualNews. Then, we ask annotators to label the relevance score for each candidate on a three-point Likert scale. Score 1 means \u201cnot relevant\u201d while 3 means \u201chighly relevant\u201d. Formally, denote $E(q_m)$ as the entity set and $E(c_n)$ as the event of a query $q_m$ or a candidate $c_n = (i_n, h_n)$, we define the ground truth relevance scores as:\\n\\n$$\\\\text{rel}(m, n) = \\\\begin{cases} \\n3 & \\\\text{if } E(q_m) \\\\subseteq E(c_n) \\\\text{ and } E(q_m) = E(c_n), \\\\\\\\\\n2 & \\\\text{if } E(c_n) \\\\cap E(q_m) \\\\neq \\\\emptyset \\\\text{ and } E(q_m) = E(c_n), \\\\\\\\\\n1 & \\\\text{if } E(q_m) \\\\cap E(i_n) = \\\\emptyset \\\\text{ or } E(q_m) \\\\neq E(c_n).\\n\\\\end{cases}$$\\n\\nEach candidate is annotated by at least three workers, and it is selected only when all workers reach a consensus. Controversial candidates that workers cannot agree upon after two rounds of annotations are discarded from the candidate pool. Additionally, one negative candidate is added to each annotation task to verify workers\u2019 attention. The final conformity rate among all annotations is over 91.5%.\\n\\nHard Negatives Mining\\n\\nWe discover that EDIS queries can be challenging to Google Image Search in some cases. Among the 200K images from Google Search, 29K (\u223c15%) are annotated with a score of 1, and 124K (\u223c62%) are annotated with a score of 2. These candidates are hard negatives that require retrieval models to understand and ground visual entities in the images. As for candidates from VisualNews, there are 9.7K (\u223c41%) with a score of 1 and 9.5K (\u223c40%) candidates with a score of 2. We refer to these samples as in-domain hard negatives as their headlines share some entities with the query but refer to different events with discrepant visual representations.\\n\\nSoft Negative Mining\\n\\nLastly, we utilize the rest of the image-headline pairs from VisualNews and TARA to augment the candidate pool. These candidates are naturally negative candidates with a relevance score of 1 because of the unique article contents and extensive diversity in topics. Therefore, our dataset consists of 1,040,919 image-headline candidates in total.\\n\\n4.3 Dataset Statistics\\n\\nWe demonstrate the major advantage of EDIS over existing datasets in Table 1. EDIS has the largest candidate set with a consistent candidate modality. Our images are not restricted to a specific source as a result of collecting images from a real search engine. Queries from EDIS are entity-rich compared to datasets with general objects (e.g. MSCOCO). In Fig. 3 (left), we show the score distribution of...\"}"}
{"id": "emnlp-2023-main-297", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"human annotations. Candidates mined from Visual News are mostly in-domain hard negatives, while the images represent missing entities or different events. These candidates are mostly annotated with scores of 1 or 2. As for Google search candidates, many images depict the same event but with missing entities. Therefore, the annotations concentrate on score 2. In Fig. 3 (right), we show that most of the queries have at least one hard negative, usually more score 2 negatives than score 1 negatives. About half of the queries have more than one positive candidate (score 3). We show more examples of EDIS candidates in Fig. 8-11.\\n\\n5 Multi-Modal Retrieval Method\\n\\nGiven a text query $q$, a model should be able to encode both images $i$ and headlines $h$ to match the query encoding. Therefore, the model should include a multi-modal candidate encoder $f_C$ and a query encoder $f_Q$. Within $f_C$, there is a branch for image input $f_I$ and a branch for headline $f_H$. We formalize the matching process between a query $q_m$ and a candidate $c_n = (i_n, h_n)$ as:\\n\\n$$s_{m,n} = f_Q(q_m)^T f_C(i_n, h_n)$$\\n\\nwhere $s_{m,n}$ is the similarity score between $q_m$ and $c_n$. Based on the design of $f_C$, we categorize methods into score-level fusion and feature-level fusion.\\n\\nScore-level Fusion\\n\\nThese methods encode image and headline independently and compute a weighted sum of the features, i.e.,\\n\\n$$f_C(i_n, h_n) = w_1 f_I(i_n) + w_2 f_H(h_n)$$\\n\\nTherefore, $s_{m,n}$ is equivalent to a weighted sum of query-image similarity $s_{i,m,n}$ and query-headline similarity $s_{h,m,n}$:\\n\\n$$s_{m,n} = f_Q(q_m)^T (w_1 f_I(i_n) + w_2 f_H(h_n))$$\\n\\n$$= w_1 s_{i,m,n} + w_2 s_{h,m,n}$$\\n\\nSpecifically, CLIP (Radford et al., 2021), BLIP (Li et al., 2022), and a combination of models like CLIP and BM25 (Robertson et al., 2009) belong to this category.\\n\\nFeature-level Fusion\\n\\nIn Sec. 6, we show that score-level fusion is a compromised choice for encoding multi-modal candidates. Therefore, we propose a modified version of BLIP (mBLIP) to fuse features throughout the encoding process. The overall fusion process can be abstracted as follows:\\n\\n$$f_C(i_n, h_n) = f_H(h_n, f_I(i_n))$$\\n\\n$$s_{m,n} = f_Q(q_m)^T f_H(h_n, f_I(i_n))$$\\n\\nAs is shown in Fig. 4, we first extract image embeddings $f_I(\u00b7)$ using the image encoder and then feed $f_I(\u00b7)$ into the cross-attention layers of $f_H$. The output from $f_H$ is a feature vector $v_{i,h}$ that fuses the information from both image and text modalities. We separately obtain the query feature $v_q = f_Q(q_m)$ where $f_Q$ shares the same architecture and weights with $f_H$, except that the cross-attention layers are not utilized. We adopt the Image-Text Contrastive (ITC) loss (Li et al., 2021) between $v_{i,h}$ and $v_q$ to align the fused features with query features.\\n\\n6 Experiment Setup\\n\\n6.1 Baselines\\n\\nFor score-level fusion mentioned in Sec. 5, we consider CLIP, BLIP, fine-tuned BLIP, and BM25+CLIP reranking to utilize both modalities of the candidates. In addition, we benchmark existing text-to-image retrieval methods, and text document retrieval methods, including VinVL (Zhang et al., 2021), ALBEF (Li et al., 2021), and BM25 (Robertson et al., 2009). Although they are not designed for multi-modal candidates, benchmarking these methods facilitates our understanding of the importance of single modality in the retrieval process. We do not consider single-stream approaches like UNITER (Chen et al., 2020) as they are not efficient for large-scale retrieval and result in extremely long execution time (see Appendix A).\\n\\n6.2 Evaluation Metrics\\n\\nWe evaluate retrieval models with the standard metric Recall@k ($R@k$) that computes the recall rate of the top-k retrieved candidates. $k$ is set to 1, 5, 10. We report mean Average Precision (mAP) to reflect the retrieval precision considering the ranking.\"}"}
{"id": "emnlp-2023-main-297", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Recall@k = \\\\frac{1}{|Q|} \\\\left| \\\\bigcup_{n=1}^{\\\\text{rel}} Q_n \\\\right| \\\\sum_{m=1}^{\\\\text{rel}} \\\\sum_{n=1}^{\\\\text{rel}} \\\\delta(m, n) \\\\sum_{n=1}^{\\\\text{rel}} \\\\delta(m, n) (7)\\n\\nmAP = \\\\frac{1}{|Q|} \\\\left| \\\\bigcup_{n=1}^{\\\\text{rel}} Q_n \\\\right| \\\\sum_{m=1}^{\\\\text{rel}} \\\\sum_{n=1}^{\\\\text{rel}} P(m, n) \\\\delta(m, n) \\\\sum_{n=1}^{\\\\text{rel}} \\\\delta(m, n) (8)\\n\\n\\\\delta(m, n) = \\\\begin{cases} 1 & \\\\text{if } \\\\delta(m, n) = 3 \\\\\\\\ 0 & \\\\text{otherwise} \\\\end{cases} (9)\\n\\nwhere \\\\( P(m, n) \\\\) is the Precision@n of a query \\\\( q_m \\\\).\\n\\nFor \\\\( \\\\text{R}@k \\\\) and \\\\( \\\\text{mAP} \\\\), candidates with relevant score 3 are positive candidates, while candidates with scores 2 or 1 are (hard) negative samples. These two metrics reflect the model's ability to retrieve the most relevant candidates, which aligns with the definition in Fig. 2.\\n\\nTo give merits to candidates with a score of 2, we also report Normalized Discounted Cumulative Gain (\\\\( \\\\text{NDCG} \\\\)). \\\\( \\\\text{NDCG} \\\\) assigns importance weights proportional to the relevance score so that ranking score 2 candidates before score 1 candidate will lead to a higher metric value.\\n\\n\\\\[\\n\\\\text{DCG}(m) = \\\\sum_{n=1}^{\\\\text{rel}} \\\\frac{\\\\delta(m, n)}{- \\\\log(1 + n)} (10)\\n\\\\]\\n\\n\\\\[\\n\\\\text{NDCG} = \\\\frac{1}{|Q|} \\\\left| \\\\bigcup_{n=1}^{\\\\text{rel}} Q_n \\\\right| \\\\sum_{m=1}^{\\\\text{rel}} \\\\frac{\\\\text{DCG}(m)}{\\\\text{IDCG}(m)} , (11)\\n\\\\]\\n\\nwhere \\\\( \\\\text{IDCG}(m) \\\\) is the \\\\( \\\\text{DCG} \\\\) value of \\\\( q_m \\\\) with the ideal candidate ranking.\\n\\n### 6.3 Implementation Details\\n\\nFor BLIP fine-tuning, we adopt the same loss and hyperparameters as reported in the original implementation. We increase the learning rate to 1e-5 for optimal validation results. We directly rank the candidates by computing the cosine similarity of query features and candidate features and do not use any linear regression heads for reranking. Therefore, we abandon the image-text matching (ITM) loss in \\\\( \\\\text{mBLIP} \\\\) fine-tuning and increase the learning rate to 5e-5 for optimal performance. More details can be found in Appendix A.\\n\\n### 7 Experimental Results\\n\\n#### 7.1 BLIP-based fusion methods\\n\\nWe first investigate the performance difference between score-level and feature-level fusion as mentioned in Sec. 5. We implement these two approaches on BLIP (Li et al., 2022). Table 2 compares the result under two different setups where \\\"BLIP\\\" denotes the score-level fusion using the original BLIP architecture, and \\\"mBLIP\\\" denotes our proposed feature-level fusion. For score-level fusion, we obtain the weights from a grid search on the validation set under the distractor setup.\\n\\n**Table 2:** Retrieval performance of BLIP and \\\\( \\\\text{mBLIP} \\\\) under the distractor and full settings. We use grid search on the validation split to find the best score fusion weights (see Eq. 4) for zero-shot and fine-tuned BLIP.\\n\\n- **Distractor Set**\\n  - Pre-trained BLIP achieves 18.4 R@1 and 46.6 R@5, which means that almost one-third of the queries have a positive candidate in top-1 results, and around half of the positive candidates are retrieved in top-5 results. After fine-tuning, BLIP doubles R@1 to 32.6 and achieves significant gain in other metrics. The improvement shows that entities in EDIS are out-of-domain concepts for zero-shot BLIP, and EDIS training split is useful for models to adapt to the news domain.\\n  - \\\\( \\\\text{mBLIP} \\\\) outperforms BLIP in all metrics except R@1. The overall improvement entails that feature-level fusion is superior to score-level fusion by utilizing headlines more effectively. The degradation in R@1 can be attributed to the fact that the image-query alignment is accurate enough for a small number of queries. Therefore, utilizing headlines slightly harms the results as they only provide high-level summarization.\\n\\n- **Full Set**\\n  - Retrieving from the full candidate set significantly degrades the performance by over 50% in all metrics. Though the distractor setup was widely adopted in previous work, we show that a larger candidate set imposes remarkable challenges to the SOTA models. We can observe similar trends by comparing the three variants of BLIP. \\\\( \\\\text{mBLIP} \\\\) achieves over 17% relative improvement across all metrics except R@1, even more significant than 4-12% relative improvement under the distractor set. The degradation in R@1 is also much less severe. Therefore, feature-level fusion is a more effective\"}"}
{"id": "emnlp-2023-main-297", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3: Evaluation results on additional baselines.\\n\\n| Method       | Text Encoder | R@1 (Distractor) | R@5 (Distractor) | R@10 (Distractor) | mAP (Distractor) | NDCG (Distractor) | R@1 (Full) | R@5 (Full) | R@10 (Full) | mAP (Full) | NDCG (Full) |\\n|--------------|--------------|------------------|------------------|-------------------|-----------------|------------------|------------|------------|------------|-----------|-------------|\\n| Upper Bound  |              | 60.1             | 97.1             | 100               | 100             | 61.0             | 97.3       | 100        | 100        | 100       |             |\\n| Distractor   |              |                  |                  |                   |                 |                  |            |            |            |           |             |\\n| BM25         | \u2713            | 6.7              | 22.2             | 29.5              | 20.0            | 54.1             | 6.9        | 21.7       | 28.8       | 19.4      | 52.9        |\\n| CLIP         | \u2713            | 8.1              | 28.6             | 40.5              | 25.9            | 58.7             | 8.2        | 27.7       | 39.1       | 24.8      | 57.5        |\\n| BM25+CLIP    | \u2713            | 13.4             | 38.4             | 46.8              | 33.5            | 64.3             | 13.5       | 37.7       | 45.9       | 32.5      | 63.5        |\\n| CLIP         | \u2713            | 30.7             |                  |                   |                 |                  |            |            |            |           |             |\\n| BLIP         | \u2713            | 32.0             | 62.1             | 72.2              | 53.9            | 67.7             | 32.6       | 62.0       | 72.1       | 53.8      | 67.3        |\\n| mBLIP        | \u2713            | 27.4             | 65.9             |                  |                 |                  | 27.8       | 66.0       | 81.4       | 56.2      | 75.3        |\\n| Full         |              |                  |                  |                   |                 |                  |            |            |            |           |             |\\n| BM25         | \u2713            | 4.7              | 13.3             | 17.3              | 11.9            | 39.4             | 4.7        | 12.9       | 16.6       | 11.4      | 38.0        |\\n| CLIP         | \u2713            | 4.6              | 13.9             | 18.6              | 12.6            | 38.9             | 5.0        | 13.4       | 18.2       | 12.4      | 28.2        |\\n| BM25+CLIP    | \u2713            | 6.8              | 18.8             | 23.6              | 16.6            | 45.9             | 6.3        | 18.0       | 22.7       | 15.9      | 45.0        |\\n| CLIP         | \u2713            | 14.1             |                  |                   |                 |                  | 13.7       | 36.0       | 47.2       | 28.0      | 46.0        |\\n| BLIP         | \u2713            | 12.3             | 29.0             | 37.8              | 22.9            | 38.8             | 13.1       | 29.6       | 37.6       | 23.4      | 38.9        |\\n| mBLIP        | \u2713            | 13.3             | 34.1             | 44.3              | 28.9            | 49.0             | 12.3       | 33.3       | 44.1       | 27.4      | 47.9        |\\n\\n### Table 4: Ablation study on the effectiveness of feature fusion in BLIP and mBLIP.\\n\\n| Method       | Text Encoder | R@1 (Distractor) | R@5 (Distractor) | R@10 (Distractor) | mAP (Distractor) | NDCG (Distractor) |\\n|--------------|--------------|------------------|------------------|-------------------|-----------------|------------------|\\n| BLIP         | \u2713            | 6.6              | 21.6             | 30.5              | 19.7            | 51.9             |\\n| mBLIP        | \u2713            | 8.8              | 29.1             | 40.1              | 26.0            | 57.9             |\\n| Full         |              |                  |                  |                   |                 |                  |\\n| BLIP         | \u2713            | 33.9             | 61.2             | 71.3              | 54.0            | 66.5             |\\n| mBLIP        | \u2713            | 22.2             | 49.9             | 60.2              | 40.9            | 58.2             |\\n| Full         |              |                  |                  |                   |                 |                  |\\n| BLIP         | \u2713            | 32.6             | 62.0             | 72.1              | 53.8            | 67.3             |\\n| mBLIP        | \u2713            | 27.8             | 66.0             | 81.4              | 56.2            | 75.3             |\\n\\n### 7.2 Additional Baselines\\n\\nIn Table 3, the defective recall rates of BM25 and CLIP text encoder imply that headlines solely are insufficient for accurate retrieval. However, text-based retrieval achieves promising NDCG values, indicating that headlines are useful for ranking score 2 candidates to higher positions.\\n\\n### Score-level Fusion\\n\\n\u201cBM25+CLIP\u201d first ranks candidates using BM25 and then reranks the top 50 or 200 candidates with CLIP to utilize the images. Despite the improvement compared to text-based methods, it underperforms zero-shot CLIP or BLIP. This implies that ranking with query-headline similarity imposes a bottleneck on the reranking process. CLIP achieves the best performance in terms of R@1/5/10 and mAP compared to other methods. We hypothesize that the \u201cCLIP filtering\u201d step in Sec. 4.1 eliminates hard negative query-image pairs for CLIP and thus introduces performance bias towards CLIP. Fine-tuned CLIP does not show apparent improvement and thus is not shown in Table 3. Therefore, EDIS is still challenging for SOTA retrieval models.\\n\\n### Feature-level Fusion\\n\\nmBLIP consistently outperforms other approaches in NDCG regardless of the candidate set scale, achieving 75.3/47.9 NDCG under distractor/full set. mBLIP fuses headline features with visual features more effectively and thus proves that headlines are critical for ranking score 2 candidates higher. We conjecture that many score 2 images have insufficient entities, resulting in lower query-image similarity scores. Hence, models must rely on headlines to simultaneously recognize entities from multiple modalities.\\n\\n### 7.3 Ablation Study\\n\\n#### Component Analysis\\n\\nTable 4 shows the performance of two fusion approaches without either image or headline branch. BLIP achieves much lower performance when relying solely on query-headline alignment (6.6 R@1, 29.7 mAP) compared to utilizing images only (33.9 R@1, 54.0 mAP). BLIP only achieves comparable and slightly degraded performance when using images and headlines for score fusion. Therefore, score-level fusion cannot easily tackle multi-modal candidates in EDIS.\\n\\nIn contrast, mBLIP shows improved performance with the headline encoder while decreased performance with the image encoder only. This is intuitive as the BLIP fine-tuning process only utilizes images without headlines, yet mBLIP utilizes both images and headlines. More interestingly,\"}"}
{"id": "emnlp-2023-main-297", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Westerdam cruise ship docked in Sihanoukville, Cambodia, on Monday.\\n\\nOur selection of some of the best news photographs taken around the world during the past 24 hours.\\n\\nA squadron of Russian warships has passed through the English Channel in what the Royal Navy described as a \\\"routine\\\" movement.\\n\\nAhead of second quarter results on Tuesday, Carnival has cruised to the top of the FTSE 100.\\n\\nNorwegian Cruise Line (NCL) - Norwegian Cruise Line sending its North America-based ships to Europe.\\n\\nJournalists arrested after flying a drone in the City of Love. They say it was in the name of journalism.\\n\\nVirus fears rise after Cambodia's acceptance of a cruise ship.\\n\\nCruise ship turned away in other ports docks in Cambodia.\\n\\nCOVID-19: Cambodia PM defends ship docking.\\n\\nCruise Ship Cast Out Over Virus Fears Docks at Sihanoukville.\\n\\nCambodia's Coronavirus Complacency May Exact a Global Toll.\\n\\nFigure 5: A success case (top) and a failure case (bottom) of mBLIP compared to BLIP.\\n\\nWhen using both image and headline encoders, mBLIP demonstrates over 20% relative increase in all metrics. The results imply that feature-level fusion is a more effective method to combine candidate features from multiple modalities.\\n\\n7.4 Case Study\\n\\nSuccess Case\\n\\nWe show one success case and one failure case of mBLIP in Fig. 5. In the success case (top), mBLIP manages to retrieve all four relevant images while BLIP retrieves five false positives. Since all ten images contain a \\\"cruise\\\", we conjecture that entities in headlines (e.g., \\\"Cambodia\\\", \\\"Sihanoukville\\\") play a critical role for mBLIP to outperform BLIP in this case. The case shows feature-level fusion is much more effective in utilizing headline features than score-level fusion.\\n\\nFailure Case\\n\\nAs for the failure case in Fig. 5 (bottom), BLIP and mBLIP fail to retrieve the positive candidates in the top-5 results. Both methods fail to recognize \\\"John Podesta\\\" and align the text with the visual representation. For example, the top-2 candidates retrieved by mBLIP depict a different person from a different event. \\\"Hillary Clinton\\\" becomes a distracting entity in the query, and the model must understand the event instead of just matching entities to achieve accurate retrieval results. The third candidate of mBLIP shows the image with the correct person but from a different event. It further proves that the EDIS is a challenging dataset that requires specific knowledge of entities, cross-modal entity matching, and event understanding.\\n\\n8 Conclusion\\n\\nTraining and evaluating large-scale image retrieval datasets is an inevitable step toward real image search applications. To mitigate the gap between existing datasets and real-world image search challenges, we propose a large-scale dataset EDIS with a novel retrieval setting and one million candidates. EDIS queries and candidates are collected from the news domain describing abundant entities and events. EDIS candidates are image-headline pairs since realistic image search utilizes the surrounding text of an image to facilitate accurate searching results. As a primary step towards handling multi-modal candidates in EDIS, we review two primary fusion approaches and propose a feature-level fusion method to utilize the information from both images and headlines effectively. Our experimental results show ample space for improvement on EDIS. Future work should consider more principled solutions involving knowledge graphs, entity knowledge, and event understanding.\"}"}
{"id": "emnlp-2023-main-297", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"10 Limitations\\nIn this study, we only cover image retrieval datasets with English instructions. Queries and headlines in other languages may characterize different types of ambiguity or underspecification. Thus, expanding the datasets to multi-lingual image retrieval based on our dataset is important. Secondly, we only consider the news domain to collect entity-rich queries and images. We plan to expand our dataset to open-domain where other entities like iconic spots will be included. In addition, we only consider the headlines as the text information to utilize in the retrieval process. However, in real image search scenarios, search engines usually utilize multiple paragraphs of the surrounding text to determine the relevance of the image. In the future, we will expand the text of the multimodal candidates with news articles or segments of the articles. Our dataset and models trained on it could be biased if the model is not accurate enough. The model may return completely incorrect candidates and cause users to confuse persons or objects with incorrect identities. We will provide all ground truth annotations with visualization code to help users learn about the ground truth candidates. Last but not least, we do not consider the phenomenon of underspecification in the image search experience. Users search with phrases or incomplete sentences to save typing efforts. Therefore, more realistic queries can be underspecified and grammatically incorrect. However, this is a problem universal to all existing image retrieval datasets, as collecting real human search results could be challenging. We plan to make our dataset more realistic in the future by utilizing powerful tools such as large language models to generate underspecified, near-realistic queries.\\n\\n11 Ethics Consideration\\nWe will release our dataset EDIS for academic purposes only and should not be used outside of research. We strictly follow any licenses stated in the datasets that we have newly annotated. As introduced in Sec. 4.2, we annotated the data with crowd-workers through Amazon Mechanical Turk. The data annotation part of the project is classified as exempt by our Human Subject Committee via IRB protocols. We required the workers to be in English-speaking regions (Australia, Canada, New Zealand, the United Kingdom, and the United States). We keep the identity of workers anonymized throughout the collection and post-processing stages. We also require the workers to have a HIT approval rating of $\\\\geq 96\\\\%$ or higher. We pay each completed HIT $0.2$, and each HIT takes around 30-40 seconds to complete on average. Therefore, this resulted in an hourly wage of $18-24$, as determined by the estimation of completing time for each annotation task. Example screenshots of our annotation interface can be found in Fig. 6-7 under Appendix A.\\n\\nReferences\\nMax Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. 2021. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738.\\n\\nYingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk. 2022. Webqa: Multihop and multimodal qa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16495\u201316504.\\n\\nSoravit Changpinyo, Jordi Pont-Tuset, Vittorio Ferrari, and Radu Soricut. 2021. Telling the what while pointing to the where: Multimodal queries for image retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12136\u201312146.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer.\\n\\nMengjun Cheng, Yipeng Sun, Longchao Wang, Xiongwei Zhu, Kun Yao, Jie Chen, Guoli Song, Junyu Han, Jingtuo Liu, Errui Ding, et al. 2022. Vista: Vision and scene text aggregation for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5184\u20135193.\\n\\nZi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. 2022. An empirical study of training end-to-end linking, and training algorithm design.\"}"}
{"id": "emnlp-2023-main-297", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"vision-and-language transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18166\u201318176.\\n\\nTsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. 2021. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681.\\n\\nXingyu Fu, Ben Zhou, Ishaan Chandratreya, Carl Vonodrick, and Dan Roth. 2022. There's a time and place for reasoning beyond the image. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1138\u20131149.\\n\\nSaadia Gabriel, Skyler Hallinan, Maarten Sap, Pemi Nguyen, Franziska Roesner, Eunsol Choi, and Yejin Choi. 2022. Misinfo reaction frames: Reasoning about readers' reactions to news headlines. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3108\u20133127.\\n\\nConghui Hu and Gim Hee Lee. 2022. Feature representation learning for unsupervised cross-domain image retrieval. In European Conference on Computer Vision, pages 529\u2013544. Springer.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR.\\n\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769\u20136781.\\n\\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR.\\n\\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020a. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11336\u201311344.\\n\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 12888\u201312900. PMLR.\\n\\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:9694\u20139705.\\n\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557.\\n\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020b. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121\u2013137. Springer.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer.\\n\\nFuxiao Liu, Yinghan Wang, Tianlu Wang, and Vicente Ordonez. 2021. Visual news: Benchmark and challenges in news image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6761\u20136771.\\n\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32.\\n\\nSheena Panthaplackel, Adrian Benton, and Mark Dredze. 2022. Updated headline generation: Creating updated summaries for evolving news stories. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6438\u20136461.\\n\\nZarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, and Yinfei Yang. 2021. Crisscrossed captions: Extended intramodal and intermodal semantic similarity judgments for MS-COCO. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2855\u20132870, Online. Association for Computational Linguistics.\\n\\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 2641\u20132649.\\n\\nJordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. 2020. Connecting vision and language with localized narratives. In ECCV.\"}"}
{"id": "emnlp-2023-main-297", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nA.1 Additional Implementation Details\\n\\nHyperparameters\\n\\nWe fine-tuned BLIP and mBLIP on 4 40GB NVIDIA A100. It takes 5 hours for BLIP fine-tuning and 3 hours for mBLIP. For both BLIP and mBLIP, we train the model for 6 epochs with batch size 16 per GPU. The model checkpoint with the best recall rate over the validation set is selected for final evaluation. We apply grid search for score-level fusion using BLIP or CLIP to find the optimal $w$. We first search over ten intermediate numbers between 0 and 1 and then narrow the range to search for 100 intermediate numbers. Finally, we found training and validation results stable without much randomness for all implemented methods. Therefore, we evaluate every model once and report the metric values of one-time evaluation.\\n\\nBLIP Training\\n\\nFor BLIP fine-tuning, we follow the original implementation and adopt the original image-text contrastive (ITC) loss and image-text matching (ITM) loss. We only utilize images with scores of 3 and text queries for training. As for mBLIP, the headline encoder and the query encoder share the same weight. We utilize images with scores of 3, associated headlines, and text queries for training. The output from the image encoder is fed into the transformer layers of the headline encoder through cross-attention layers. Then the output of the headline encoders can be treated as the fused feature of image-headline pairs. We compute ITC loss based on the headline encoder outputs and the query encoder outputs.\\n\\nSingle Stream Models\\n\\nWe do not evaluate any single stream models or modules due to time complexity. Consider $m$ queries and $n$ candidates. The complexity for a dual encoder model to obtain all features is $O(m + n)$. The computation cost of computing cosine similarity is trivial compared to the forward process of a model and can be neglected. However, for a single stream model, it takes $O(mn)$.\"}"}
{"id": "emnlp-2023-main-297", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to obtain similarity scores for all query-candidate pairs. Since it takes around 3.5 minutes for BLIP to evaluate over 3.2k queries with 25K candidates, it is taking more than 5 days for a single encoder model to complete retrieval under the distractor setting. It takes more than a year to complete retrieval under the full setting.\"}"}
{"id": "emnlp-2023-main-297", "page_num": 13, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-297", "page_num": 14, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "emnlp-2023-main-297", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Florida treasure hunters found a trove of $4m worth of six royal gold coins recovered from a Spanish shipwreck near Florida.\\n\\nFlorida treasure hunters find $4.5m in rare Spanish coins -\\n\\nFla. family finds $1M of sunken Spanish treasure -\\n\\nPolice raid Deutsche Bank headquarters as part of Panama Papers money laundering investigation -\\n\\nDeutsche Bank Offices Are Searched in Money Laundering Investigation -\\n\\nDeutsche Bank said Thursday it would shed 35,000 jobs and close operations in 10 countries -\\n\\nPresident Barack Obama and First Lady Michelle Obama Return from Ft. Bragg -\\n\\nFirst Lady visits Fort Bragg, vows support for military families -\\n\\nPresident Barack Obama marked the end of the Iraq war with a tribute to the troops who fought and died in a conflict he opposed from the start. -\\n\\nBarack Obama accompanied by first lady Michelle was making his first visit to Fort Bragg -\"}"}
{"id": "emnlp-2023-main-297", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A rally in front of the New Jersey State House on Thursday, when the authorization for the state's transportation trust fund expired.\"}"}
{"id": "emnlp-2023-main-297", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The actress Greta Gerwig surely deserves to receive the keys to New York in recognition of her on screen celebrations of that city.\\n\\nDiane Keaton Says It's All Because of 'Annie Hall'\\n\\nAnnie Hall DVD 1977 Woody Allen Diane Keaton Oscar Winner Best Picture LIKE NEW\\n\\nA dog was rescued last month from a farm in Wonju, South Korea. Humane Society International offers to pay farmers to release dogs so they can be sent abroad to be adopted. \\n\\nFlagler Humane Society relates story of South Korean dogs arriving in US.\\n\\nSpeaking of Science Hubble captures a shimmering butterfly in space.\\n\\nHubble Finds Rings In Uranus Orbit\\n\\nScientists Revisit Old Data, Discover New Moons Around Uranus\\n\\nFigure 10: Additional EDIS dataset examples set 3\"}"}
{"id": "emnlp-2023-main-297", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Matthew Centrowitz won the first US gold in men's 1500 since 1908.\"}"}
