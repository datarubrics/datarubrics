{"id": "lrec-2024-main-737", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Humanistic Buddhism Corpus: A Challenging Domain-Specific Dataset of English Translations for Classical and Modern Chinese\\n\\nYouheng Wong, Natalie Parde, Erdem Koyuncu\\nUniversity of Illinois Chicago\\nChicago, IL 60607\\n{wwong31, parde, ekoyuncu}@uic.edu\\n\\nAbstract\\nWe introduce the Humanistic Buddhism Corpus (HBC), a dataset containing over 80,000 Chinese-English parallel phrases extracted and translated from publications in the domain of Buddhism. HBC is one of the largest free domain-specific datasets that is publicly available for research, containing text from both classical and modern Chinese. Moreover, since HBC originates from religious texts, many phrases in the dataset contain metaphors and symbolism, and are subject to multiple interpretations. Compared to existing machine translation datasets, HBC presents difficult unique challenges. In this paper, we describe HBC in detail. We evaluate HBC within a machine translation setting, validating its use by establishing performance benchmarks using a Transformer model with different transfer learning setups.\\n\\nKeywords: Neural Machine Translation, Classical Chinese, Domain-Specific, Religion, Buddhism\\n\\n1. Introduction\\nIn recent years, Neural Machine Translation (NMT) approaches have become commonplace, especially after the introduction of the Transformer model (Vaswani et al., 2017). However, it is widely recognized that these methods tend to deliver less accurate translations when applied to data outside their specific domain (Koehn and Knowles, 2017). In medicine, law, or religion, all of which are characterized by technical jargon and limited parallel data available for training, universal machine translators provide inaccurate translations or miss out on stylistic nuances. Moreover, while remarkable results have been achieved for more general translation between similar languages (e.g., both source and target languages are Latin-derived) (Sun et al., 2021; Mueller et al., 2020), translation between distant languages, such as Chinese to English, still has plenty of room for improvement (Kim et al., 2020).\\n\\nBy introducing the Humanistic Buddhism Corpus (HBC), we attempt to alleviate these problems by offering a domain-specific dataset in a challenging field: religious documents for distant languages. In particular, the HBC includes modern and classical Chinese text concerning Humanistic Buddhism. Classical Chinese scriptural texts and Buddhist proverbs offer rich and challenging domain-specific terminology that increases the complexity of their automated translation. The main contributions of this paper include:\\n\\n- We introduce a high-quality, free, and extensive domain-specific dataset comprising 80,000+ parallel Chinese-English phrases for NMT research. The corpus contains both classical and modern Chinese religious texts.\\n- We evaluate our corpus using various experimental setups. We observe that many pre-trained models, although achieving high BLEU or COMET scores, cannot always effectively capture the nuances associated with this challenging domain.\\n- We qualitatively show that a Transformer model trained using our corpus combined with ordinary Chinese text can provide more accurate and fluent translations despite achieving a quantitatively lower BLEU or COMET score than pre-trained models.\\n- We also provide benchmark code and models for training and testing the corpus to accelerate future research.\\n\\n1.1. Buddhist Translation History\\nBuddhism has a remarkable and well-documented translation history from its original ancient Indian language to classical Chinese. Unlike other religions with only one primary scripture (i.e., the Bible or the Koran), a typical collection of Chinese Buddhist scriptures (Tripitaka or Canons) contains more than 70 million Chinese characters from over 5,000 texts. These scriptures are written in ancient Chinese styles that span over 2,000 years. They have various transliterations from their original Sanskrit or Pali text, and multiple translators...\"}"}
{"id": "lrec-2024-main-737", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"might translate the same Sanskrit or Pali terminology into different Chinese words. Furthermore, since the Chinese writing styles have also evolved over this long timespan, the writing styles for these Buddhist texts vary significantly, making translating Chinese to English even more challenging.\\n\\nSince Buddhism propagated from India to China over 2,000 years ago, numerous Buddhist proverbs, concepts, and vocabulary have been integrated into Chinese literature throughout history, and many are still used today. Buddhist literature significantly influences Chinese culture (Guang, 2013). For example, the \\\"Platform Sutra\\\" is considered one of the 7 Chinese classics. Numerous new words were introduced into Chinese from Buddhism, and many hybrid terminologies also have Buddhist origin (Zhu, 2003). Therefore, when one translates Chinese texts into any language, encounters with Buddhist-influenced contexts are inevitable.\\n\\n1.2. Corpus Source and Naming Convention\\n\\nThe HBC includes the most common Buddhist scriptures, known as sutras, which are discourses taught by Sakyamuni Buddha in India over 2,500 years ago. In addition, the corpus mainly features publications authored by one person, Ven. Master Hsing Yun. He was the founder of the Fo Guang Shan (FGS) International Buddhist Order and was an advocate of Humanistic Buddhism throughout his entire life. Humanistic Buddhism mainly focuses on applying complicated Buddhist teachings in everyday life. Therefore, even though some of his works collected poems and proverbs from ancient Chinese scholars, Hsing Yun composed his writings in easy-to-understand, everyday Chinese so everyone could comprehend them.\\n\\nThe Complete Works of Ven. Master Hsing Yun comprises 365 volumes in the first edition (Hsing Yun, 2017) and 395 volumes in the second edition. This collection is accessible online at https://books.masterhsingyun.org/, and most English translations of his work are also obtainable at various websites (BLP, 2023; FGSITC, 2020). With the massive collection of his writing in modern, everyday Chinese, his magnanimity in making his complete collection available online, and the extensive quantity of his works already translated into English, this work was an ideal starting point for the development of the HBC and thus inspired its name (the Humanistic Buddhism Corpus). We plan to expand and refine the HBC periodically in the years to come as more translations of Hsing Yun's work are published.\\n\\n2. Related Works\\n\\n2.1. Existing Datasets\\n\\nA variety of Chinese-English translation datasets are currently available, although none fit the specific needs that HBC is designed to fill. The WMT news translation task has resulted in the publication of numerous Chinese-English parallel corpora for training and testing, especially in recent years (WMT21 and WMT22). However, these datasets require extensive filtering to be used for training (Wang et al., 2021; Tran et al., 2021; Zeng et al., 2021). Moreover, these extensive parallel data are intended for the news domain and perform poorly for out-of-domain texts (Koehn and Knowles, 2017).\\n\\nThe open source parallel corpus OPUS (Tiedemann, 2012) comprises a vast range of translated texts, including the datasets from the WMT that contain Chinese-English parallel data, such as Wiki Titles, UN Parallel Corpus, and TED talks. The OPUS collection also includes single-volume translated texts in multiple languages, such as the Bible-uedin corpus (Christodouloupoulos and Steedman, 2015) and the Tanzil datasets (Tiedemann, 2012), which have Chinese-English parallel data. However, most of the other collections within OPUS are in European languages and do not contain Chinese-English parallel texts.\\n\\nThe Linguistic Data Consortium (LDC) hosts many datasets for NMT. For example, the Chinese-English NIST datasets (Przybocki et al., 2009) available on LDC are frequently used by translation researchers. However, their datasets require membership and thus become expensive for non-member researchers to use them.\\n\\n2.2. Available Buddhist Corpora\\n\\nIn the past decades, efforts from different schools of Buddhism have sought to make Buddhist scriptures available online. The Chinese efforts include the Chinese Buddhist Electronic Text Association (CBETA, 1998), digitizing the Buddhist canon since 1998. Fo Guang Shan started its Buddhist Texts (Fo Guang Shan, 2004) in 1995 and made them accessible online in 2004. The Sanskrit cannon (University of the West, 2003) and Pali texts (Vipassana Research Institute, 2010) are also available online, while the Tibetan texts are preserved in graphical format by the Buddhist Digital Resource Center (BDRC, 2016). The English translations for these documents are freely obtainable at various sites (BuddhaNet, 2023; BCBS, 1995). However, all of these resources are only available in one language\u2014that is, the English translations are not directly mapped to the Chinese editions of a given document, and the same holds\"}"}
{"id": "lrec-2024-main-737", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"true for other potential language pairings. The creators of these digital resources are incredibly supportive of making parallel corpora, but the resources as they currently stand are not usable in training neural machine translation models. Research on machine translation of Buddhist texts has been attempted, such as by Li et al. (2022). Unfortunately, the datasets they used are proprietary.\\n\\n2.3. Other Religious Corpora\\nThe Bible is the most extensively translated religious scripture in the world. The eBible includes the Bible text in 833 different languages across 75 language families (Akerman et al., 2023). The Bible-uedin is a collection of the translation of the Bible in 100 languages (Christodouloupoulos and Steedman, 2015). Another religious text, the Koran, is also widely translated. The T anzil (Tiedemann, 2012) is a collection of Koran translations in 42 languages. Both the Bible and the Koran have Chinese-English parallel data readily available online. However, both of these religious scriptures are minuscule in size compared to the voluminous Buddhist collection of over 5,000 scriptures.\\n\\n3. Humanistic Buddhism Corpus\\nThe HBC contains 81,000 Chinese-English parallel phrases extracted from Buddhist publications. These publications include classical and modern Chinese, comprising translation of modern Chinese from 20 volumes of English books and 34 booklets, Buddhist scriptural text in classical Chinese from 9 sutras, proverbs and poems from 26 volumes of books, and subtitles in modern language from 7 DVDs. Table 1 provides the book titles in the HBC. Aside from the Buddhist scriptural texts known as sutras, most of the original Chinese texts are from the 365 volumes of the Complete Works of Venerable Master Hsing Yun (Hsing Yun, 2017). However, the \u201cOne-Liners,\u201d \u201cPhrases,\u201d and \u201cPoetry\u201d from his collection, as well as After Many Autumns (Hsing Yun, 2011a), comprises phrases and poetry by over 100 ancient Buddhist scholars from the Tang dynasty (618-907) to the Qing dynasty (1644-1911). These scholars include the renowned Wang Xizhi, Du Fu, Bai Juyi, Pei Xiu, Su Shi, and Ouyang Xiu, emperors such as Shunzhi and Yongzheng, and numerous eminent Buddhist masters. Organizations within the Fo Guang Shan order publish the English translations. The subtitles are from Hsing Yun\u2019s oral lectures produced into DVDs by Beautiful Life Television.\\n\\nThe original texts are in Mandarin Chinese with Traditional Script (zh-Hant), and the translations are in American English (en-US). The average length of the phrases in the HBC is 18 Chinese characters, with 15% of the phrases being proverbs or lines from poems and verses, which are 4 to 7 Chinese characters. The format of the HBC contains lists of Chinese and English pairs, with an additional reference ID as the third element. The numbering for the reference ID is 0 to 99,999 for modern texts; classical Chinese phrases are numbered 100,000 to 199,999 for sutras, and poems and proverbs start at 200,000. Future researchers can use these reference numbers to filter the data.\\n\\n3.1. Data Selection Techniques\\nUnlike online translations, which may be unedited or translated by amateurs, published works typically contain high-quality translations due to their thorough editing before publication. However, these high-quality translations are edited based on the fluency and adequacy of the content as a whole, which is often based on the surrounding paragraph, and the editors do not emphasize the accuracy of each sentence translation. Therefore, some original Chinese sentences are omitted during Chinese-English translation while other English sentences are added to make the contents more comprehensible for English readers who lack a Chinese cultural background. Additionally, it is common for Chinese sentences (especially those describing difficult-to-comprehend religious concepts) to be extremely long and separated by commas, such that one sentence could reasonably be translated into several English sentences. As a result, English translations of Chinese publications contain a high percentage of unmatched phrases on average (see the Exc. column in Table 1). The order of translations is also often different from that of their original works, increasing the difficulty of extracting parallel phrases using MT models.\\n\\nDue to the difficulty of using automated methods for this process, we built the Humanistic Buddhism Corpus by manually matching, aligning, and scoring the Chinese-English phrases as part of a community effort collectively spanning 21 volunteer human translators over the past two years. The volunteers are all Buddhists and have received training regarding alignment and rating. All but one of the volunteers are native Chinese speakers, and most of them have at least 5 years of translation experience. The majority of matched phrases are complete English sentences matched to phrases in Chinese. In the case of dialogue and other complicated English sentences, the complete English sentences are split further into meaningful units. The poems and verses are divided on a line level. The matched parallel phrases underwent manual review by an experienced human translator.\"}"}
{"id": "lrec-2024-main-737", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Title                        | Author                     | Year | Corpus Size | Alignment Rate | Optimal? |\\n|------------------------------|----------------------------|------|-------------|----------------|---------|\\n| Amitabha Sutra               | Sakyamuni Buddha           | 2017 | 1925        | 0.0%           | No      |\\n| Diamond Sutra                | Sakyamuni Buddha           | 2019 | 6194        | 0.0%           | No      |\\n| Repaying Parents Sutra       | Sakyamuni Buddha           | 2018 | 3864        | 0.0%           | No      |\\n| Give Rise to the Bodhi Mind  | Sheng'an                   | 2017 | 8333        | 0.0%           | No      |\\n| Ksitigarbha Bodhisattva Sutra| Sakyamuni Buddha           | 2023 | 20202       | 0.0%           | No      |\\n| Medicine Buddha Sutra        | Sakyamuni Buddha           | 2015 | 6084        | 0.0%           | No      |\\n| Platform Sutra               | Hsing Yun                  | 2011 | 23981       | 0.0%           | No      |\\n| Samantabhadra Bodhisattva Ch.| Sakyamuni Buddha           | 2019 | 6068        | 0.0%           | No      |\\n| Universal Gate Chapter       | Hsing Yun                  | 2011 | 2429        | 0.0%           | No      |\\n\\n| Title                        | Author                     | Year | Corpus Size | Alignment Rate | Optimal? |\\n|------------------------------|----------------------------|------|-------------|----------------|---------|\\n| After Many Autumns           | Hsing Yun                  | 2011 | 22072       | 23.8%          | Yes     |\\n| Bells, Gongs, and Wooden Fish| Hsing Yun                  | 2012 | 29878       | 14.6%          | Yes     |\\n| Biography of Sakyamuni Buddha| Hsing Yun                  | 2013 | 129331      | 10.4%          | Yes     |\\n| Bright Star                  | Fu                         | 2008 | 139403      | 18.3%          | Yes     |\\n| Buddha's Light Philosophy    | Hsing Yun                  | 2010 | 38702       | 21.9%          | Yes     |\\n| Buddha Dharma                | Hsing Yun                  | 2019 | 126896      | 4.8%           | Yes     |\\n| Collection of VMHY - One-Liners| Hsing Yun              | 2017 | 3936        | 7.0%           | No      |\\n| Collection of VMHY - Phrases  | Hsing Yun                  | 2017 | 7996        | 7.1%           | No      |\\n| Collection of VMHY - Poetry  | Hsing Yun                  | 2017 | 3758        | 0.9%           | No      |\\n| Collection of VMHY - Wisdom  1, 3, 6, 7| Hsing Yun  | 2017 | 20736       | 13.4%          | No      |\\n| Epoch of Buddha's Light      | Hsing Yun                  | 1999 | 37752       | 38.1%          | Yes     |\\n| Everlasting Light            | Hsing Yun                  | 2002 | 19535       | 21.7%          | No      |\\n| For All Living Beings        | Hsing Yun                  | 2010 | 39320       | 32.2%          | Yes     |\\n| Four Insights                | Hsing Yun                  | 2010 | 61888       | 7.6%           | Yes     |\\n| Handing Down the Light       | Fu                         | 1996 | 68555       | 14.6%          | Yes     |\\n| HB: Blueprint for Life       | Hsing Yun                  | 2008 | 46225       | 6.2%           | Yes     |\\n| Humble Table, Wise Fare      | Hsing Yun                  | 2011 | 5371        | 7.4%           | No      |\\n| Infinite Compass             | Hsing Yun                  | 2010 | 20923       | 20.7%          | Yes     |\\n| Living Affinity              | Hsing Yun                  | 2009 | 12115       | 70.6%          | Yes     |\\n| Pearl of Wisdom (Prayers)    | Hsing Yun                  | 2003 | 44664       | 29.5%          | Yes     |\\n| Rabbit's Horn                | Hsing Yun                  | 2010 | 49083       | 48.1%          | Yes     |\\n| Ten Paths to Happiness       | Hsing Yun                  | 2014 | 58118       | 5.4%           | Yes     |\\n| Universal Gate               | Hsing Yun                  | 2011 | 37844       | 25.9%          | Yes     |\\n\\n| Title                        | Author                     | Year | Corpus Size | Alignment Rate | Optimal? |\\n|------------------------------|----------------------------|------|-------------|----------------|---------|\\n| Buddhism in Every Step A1-13 | Hsing Yun                  | 2015 | 74298       | 24.0%          | Yes     |\\n| Buddhism in Every Step B1-B2, B4-B8 | Hsing Yun | 2015 | 48196       | 14.0%          | Yes     |\\n| Buddhism in Every Step C1-C7 | Hsing Yun                  | 2015 | 71839       | 18.4%          | Yes     |\\n| Buddhism in Every Step D1-D2 | Hsing Yun                  | 2015 | 18505       | 8.7%           | Yes     |\\n| Buddhism in Every Step H2-H6 | Hsing Yun                  | 2015 | 19256       | 13.0%          | Yes     |\\n\\n| Title                        | Author                     | Year | Corpus Size | Alignment Rate | Optimal? |\\n|------------------------------|----------------------------|------|-------------|----------------|---------|\\n| Buddha Dharma                | Hsing Yun                  | 2014 | 49108       | 3.0%           | Yes     |\\n| Heart Sutra                  | Hsing Yun                  | 2014 | 34135       | 6.2%           | No      |\"}"}
{"id": "lrec-2024-main-737", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"remaining phrases, which improved the quality of phrase-based translations. Light editing included adding or removing pronouns or names, common in publication work where fluency was required when reading on a paragraph level instead of a phrase level.\\n\\nWe further filtered the data before tokenization through two simple steps. First, we filtered out long (>150 Chinese characters or English words) phrases. Next, we removed over-translated or under-translated phrases by filtering out phrases in which the number of words in one language (English words or Chinese characters, represented as $E$ and $C$) was twice as many as in the other language: ($l_E > 2 \\\\times l_C \\\\text{ or } l_C > 2 \\\\times l_E$). For proverbs and phrases with <5 characters, the ratio of this filtering is changed to 1:4 or 4:1 (Zeng et al., 2021).\\n\\n3.2. Corpus Collection\\n\\nThe collection of texts in the Humanistic Buddhism Corpus is listed in Table 1. Besides the number of lines and the number of Chinese characters included in the corpus, it indicates the percentage of the text manually excluded from the HBC due to either no matching of the original Chinese or translated English or other issues with the translation. Although samples from all the sutras, books, booklets, and DVDs are included in the training set, the table indicates whether or not samples from a book were also included in the validation and test sets. The \u201cTest=No\u201d in Table 1 indicates which sources are in classical Chinese, which comprises 20% of the corpus. The titles used in this table are shortened versions, and links to their complete bibliographic references are provided after the titles.\\n\\n3.3. Corpus Location and License\\n\\nThe Humanistic Buddhism Corpus is publicly available at: https://www.fgstranslation.org/hbc\\n\\nThis corpus is intended to be used in academic and research settings and not for use in for-profit industries. The corpus will be updated periodically to include more data and better translations. The Humanistic Buddhism Corpus is licensed under CC-BY-NC-SA, the Attribution-NonCommercial-ShareAlike of the Creative Commons 4.0 International License.\\n\\n4. Experiments\\n\\n4.1. Experiment Setup\\n\\n4.1.1. Corpora Split\\n\\nThe HBC consists of 81,000 phrase pairs for training and testing after filtering. We split the data into 77,000 training, 2,000 validation, and 2,000 test instance subsets (an approximately 95%/5%/5% train/validation/test split, selected to maximize the amount of available training data). We first mix the phrases from books that are not sutras or proverbs (\u201cTest=Yes\u201d in Table 1) and split them randomly for train/validation/test sets, then add the phrases from sutras and proverbs to the train set and mix them randomly again. The validation and test sets do not contain sutras, poems, or proverbs, as those types of text contain phrases that are often quoted in other texts (and thus may be present in the training data).\\n\\nThe Datum2017 corpus from the China Workshop on Machine Translation (CWMT) from WMT21 was used for training some of the models in conjunction with the HBC. The Datum dataset contains 20 books with 50,000 paired sentences each, for a total of 1 million parallel sentences. After a similar filtering procedure as applied to HBC, we split the data into 600,000 training, 2,000 validation, and 2,000 test instance subsets (an approximately 98%/1%/1% train/validation/test split, selected such that the validation and test subsets are equivalent in size to those used with HBC). The training set was further subdivided into 200k and 600k instance subsets for assorted training. For some of the combined-corpora experiments, the HBC data were duplicated during training to add more weight to the Buddhist dataset.\\n\\n4.1.2. Base Transformer Model\\n\\nTo establish a performance benchmark for HBC, we conducted experiments using the Base Transformer model with 6 encoders and 6 decoders originally designed by Vaswani et al. (2017). All experiments were conducted using one NVIDIA RTX A6000 GPU with 48 GB memory. The data is tokenized using the SentencePiece algorithm (Kudo and Richardson, 2018) before training on the Transformer model, with a total of 100,000 English tokens and 100,000 Chinese tokens.\\n\\n4.1.3. Pre-Trained Models\\n\\nTo further emphasize the inherent and interesting challenges raised by the HBC, two pre-trained models were also applied to the WMT21 Datum2017 and HBC test sets: Argos Translate 1 and Google Translate. Each model was executed \u201cas is\u201d on the test sets without further training or fine-tuning. Argos Translate trains a PyTorch Transformer model with OPUS (Tiedemann, 2012) as its primary data source. It is one of the limited open\"}"}
{"id": "lrec-2024-main-737", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: BLEU and COMET Scores on the WMT21 Datum Test Set\\n\\n| Model          | Dataset       | Train Size | Val Size | BLEU  | COMET |\\n|----------------|---------------|------------|----------|-------|-------|\\n| Base Transformer | Datum        | 200k       | 2k       | 40.04 | 0.792 |\\n| Argos Translate | Opus         | \u2013          | \u2013        | 29.02 | 0.498 |\\n| Google Translate | \u2013            | \u2013          | \u2013        | 68.73 | 0.855 |\\n\\nTable 3: BLEU and COMET scores on the HBC Test Set\\n\\n| Model          | Dataset       | Train Size | Val Size | BLEU  | COMET |\\n|----------------|---------------|------------|----------|-------|-------|\\n| Base Transformer | Datum/HBC    | 200k/77k   | 1k/1k    | 11.75 | 0.651 |\\n| Base Transformer | Datum/HBC    | 200k/154k  | 1k/1k    | 13.18 | 0.668 |\\n| Base Transformer | Datum/HBC    | 600k/154k  | 1k/1k    | 12.81 | 0.670 |\\n\\n4.1.4. Evaluation Methods\\n\\nTo evaluate the performance of these model conditions, we used both Bi-Lingual Evaluation Understudy (Papineni et al., 2002, BLEU) scores and Crosslingual Optimized Metric for Evaluation of Translation (Rei et al., 2020, COMET) scores. The SacreBLEU package (Post, 2018) was used to compute the BLEU score. The predicted translations from all models for the same test set were exported for quality evaluation so that non-expert readers could assess them. The training, validation, and test sets of the HBC are available online, as well as the predicted translations of the test set by each model, so that readers can qualitatively evaluate the translations in the test sets.\\n\\n4.2. Quantitative Analysis\\n\\nTable 2 provides benchmark results for evaluating our generic news test set, WMT21 Datum, using various models. The pre-trained Argos Translate provides a decent BLEU score of 29.02, whereas the Base Transformer trained on the Datum data and tested on the Datum data yields a 40.04 BLEU score. Google Translate scored best, with a BLEU score of 68.73. This table also gives benchmark COMET scores, with 0.792, 0.498, and 0.855 for the Base Transformer, Argos Translate, and Google Translate, respectively.\\n\\nTable 3 presents our benchmarking results using the Humanistic Buddhism Corpus in our transfer learning setup, leveraging the WMT21 Datum dataset for pre-training the Base Transformer model. We note that the purpose of these experiments was not to achieve state-of-the-art machine translation performance, but rather to provide a proof-of-concept for training models using...\"}"}
{"id": "lrec-2024-main-737", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"our dataset as well as an initial starting point for other researchers. Although the Base Transformer training on the Datum dataset alone can produce a BLEU of 40.04 or a COMET of 0.792 when testing on the Datum test set, when the same model is tested on the HBC test set, the predicted translations are meaningless random words, likely due to the test data's domain mismatch. When training on the HBC data alone, the predicted translations are not ideal either, producing a BLEU of only 8.73 and a COMET of 0.585\u2014qualitatively, this maps to about half of the translations being comprehensible with basic Buddhist terminologies, but the other half being mistranslated or not fluent in grammar. The Base Transformer likely struggles with HBC due to its scarcity. Even when we double the HBC training data via repetition, the BLEU score is only 9.34, and the COMET score is 0.588. This highlights the challenging nature of this task, even compared to other Chinese-English translation settings.\\n\\nWhen training the HBC using transfer learning, pre-training on just 200,000 lines of the Datum dataset, we qualitatively observed that the translations grew more fluent with few grammatical errors. Correspondingly, BLEU increased to 11.75 and COMET rose to 0.651. When changing the weight by doubling the HBC training data, the results increased to 13.18 and 0.668. This suggests that although Datum does not offer suitable performance when used in an entirely out-of-domain training context, it can still be productively leveraged in other capacities to support models designed for use with our challenging new dataset. However, the proportion of the transfer learning data (Datum) and the HBC data cannot be too high, as indicated by the slightly lowered BLEU of 12.81 when training on 600,000 lines of the Datum dataset with double the HBC training data. The in-domain and out-of-domain parallel data need to be balanced proportionally to produce optimal results. Dataset pruning methods (Gormez and Koyuncu, 2023) can also potentially identify the most significant samples during the training process.\\n\\nThe predicted translations by the two pre-trained models also indicate that the HBC is a challenging dataset. The Argos Translate model transparently showed the in-domain and out-of-domain discrepancy. Despite obtaining a BLEU of 29.02 and COMET of 0.498 on the Datum test set, Argos Translate performed especially poorly on the HBC test set, with a BLEU of 2.19 and COMET of 0.438, which contains incomprehensible translations. Even the powerful Google Translate underperformed on our challenging HBC test set, with BLEU score dropping by more than 25 and a COMET decrease of 0.11 compared with the more generic Datum test set. Lower scores suggest that significant room for enhancement exists in future research working with our HBC dataset.\\n\\nFigure 1 illustrates the challenging aspect of this dataset. The BLEU scores of the various models testing on the Datum and the HBC test sets are shown in that chart. The white bars represent the Datum test set, and the gray bars represent the HBC test set. The 3.03 and 2.19 BLEU scores indicate that the predicted translations are incomprehensible, with numerous terminologies untranslated. In the case of Google Translate, the drop was over 25 BLEU. Figure 2 shows the COMET scores of these pre-trained and Base Transformer models. This chart also indicates the decrease in COMET scores for the HBC test set in all models, further emphasizing the complexity of the Humanistic Buddhism Corpus.\\n\\n4.3. Translation Quality Analysis\\n\\nThe quantitative analysis of the predicted translations using BLEU and COMET scores provides a quick measurement of the quality of the translations. However, not only is BLEU score known to carry many limitations (Callison-Burch et al., 2006; Post, 2018; Mathur et al., 2020), automatic metrics in general present challenges as the de facto practice for evaluation processes (Gehrmann et al., 2023). Therefore, we randomly selected three phrases and their target and predicted translations from the HBC test set, and presented them for manual review by readers to analyze the translation quality. All predicted translations from the test set are also available online for human evaluators to examine on a deeper level.\\n\\nTables 4-6 present sample translations. The first line is the Chinese phrase, followed by the target translation provided by the dataset. The third line contains the predicted translation.\"}"}
{"id": "lrec-2024-main-737", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There is another saying in the Buddhist tradition: \u201cgive thought neither to what is wholesome nor unwholesome.\u201d\\n\\nWith great wisdom, Bodhisattvas see that the five aggregates are all empty, and fully understand the emptiness of all dharmas.\\n\\nThe fourth line is our Base Transformer model, trained on the 77k HBC training set alone. The fifth line is the translation generated when training on both the Datum (200k) and HBC (77k) datasets jointly. The last line is the result from the weighted data setting, doubling the HBC size (154k) through duplication and training the model jointly with the doubled HBC dataset and the Datum dataset.\\n\\nSample translation 1 in Table 4 is an excellent example of the variation of translations predicted by the Transformer models using the HBC dataset. All the translations are fluent and adequate. Despite the HBC\u2019s 77k training size, the translation from training on HBC alone is correct, and to some editors it is preferred over Google Translate\u2019s translation. This suggests that the HBC contains sufficient training data to provide fluent and adequate translations in some cases.\\n\\nThe sample phrase in Table 5 presents the unique challenges of the HBC because it contains multiple elements of Buddhism such as \u201cBodhisattvas,\u201d \u201cthe five aggregates,\u201d \u201cemptiness,\u201d and \u201cphenomena/dharmas.\u201d Among them are Buddhist metaphors and symbolism that are subject to multiple interpretations. For example, the Chinese word \u201c\u6cd5\u201d (f\u01ce) can be translated as \u201cDharma,\u201d \u201cdharmas,\u201d \u201cmental states,\u201d or \u201cphenomena.\u201d Although the predicted translation from training on just the HBC is missing this terminology, it provides the most common translation of this term in the context of \u201c\u8af8\u6cd5\u7a7a\u6027 (zh\u016b f\u01ce k\u014dng x\u00ecng)\u201d when trained with the Datum dataset. Since the HBC\u2019s data are from published works, \u201cphenomena\u201d is considered better than Google\u2019s translation \u201cdharmas.\u201d All these translations are evident of the difficulty in translating abstract concepts.\\n\\nTable 6 conveys common mistakes in translating the names of Buddhist figures. The names can be translated using their original Sanskrit or Pali, transliteration from the Chinese pronunciation, and even translating their meaning. In most\"}"}
{"id": "lrec-2024-main-737", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Source: Jingfan heartedly thought:\\n\\nTable 6:\\n\\nSample translation 3\\n\\nof the publications, the original Sanskrit or Pali names are preferred. Therefore, all the models that trained on the HBC produced the correct name in Sanskrit. However, Google Translate did not predict the name but provided the Chinese pronunciation of the meaning of the king's name. Google's translation is not considered incorrect, but it is inferior to using the original names when they are available. Therefore, training using the HBC provides valuable results that extend performance beyond what is currently available from huge pre-trained models.\\n\\n5. Conclusion\\n\\nIn this work, we introduced a novel, challenging dataset of English translations for classical and modern Chinese: the Humanistic Buddhism Corpus. The dataset offers domain-specific paired Chinese-English language samples for a particularly challenging domain spanning religious books and booklets, proverbs and poems, and video subtitles all pertaining to Humanistic Buddhism. Translations were manually matched, aligned, and scored by 21 human translators followed by an additional final manual review. Overall, the dataset comprises 81,000 paired phrases, representing a substantial and important contribution to the field of neural machine translation. Furthermore, the HBC will be expanded gradually in the years to come, providing an invaluable large corpus to both the Buddhist community and Chinese-English NMT researchers.\\n\\nWe perform benchmarking experiments to establish a performance baseline for this challenging new dataset, achieving a maximum BLEU score of 13.18 and COMET score of 0.670 when leveraging a transfer learning setting pre-training on the WMT21 Datum general-domain Chinese-English dataset. This highlights the challenging nature of this task, offering opportunity for future research.\\n\\nWe hope that HBC will inspire meaningful advances to machine translation within this and other complex, domain-specific translation settings.\\n\\n6. Acknowledgements\\n\\nYouheng Wong is grateful for the generosity of Venerable Master Hsing Yun for providing his works for this research, and thanks the Buddha's Light Publications, Fo Guang Shan International Translation Center, Gandha Samudra Culture Company, Beautiful Life Television, and Fo Guang Shan Institute of Humanistic Buddhism for providing English translations of the master's works.\\n\\nThe human translators involved in matching this parallel corpus are from the Buddha's Light International Association. They are Eve Lee, Avelyn Busch, Joyce Kueh, Helen Yau, Ann-C Lin, Sharon Chang, Jeremy Hsu, Athena Huang, Jia Liu, Amy Thomason, Bonnie Xie, George Sheu, Dean Isensee, Andrew Chou, Annie Chen, Vanessa Kwok, Eugene Lo, Yawei Yang, Felicia Hsieh, Yifan Wang, and Hui Wang. We thank them for their contributions.\\n\\nThe work of Erdem Koyuncu was supported in part by the Army Research Lab (ARL) under Grant W911NF2120272, and by the Army Research Office (ARO) under Grant W911NF2410049.\\n\\n7. Bibliographical References\\n\\nVesa Akerman, David Baines, Damien Daspit, Ulf Hermjakob, Taeho Jang, Colin Leong, Michael Martin, Joel Mathew, Jonathan Robie, and Marcus Schwarting. 2023. The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages.\\n\\nBCBS. 1995. Access to Insight.\\n\\nBDRC. 2016. Buddhist Digital Resource Center.\\n\\nBLP. 2023. Buddha's Light Publications.\\n\\nBuddhaNet. 2023. Buddhist eLibrary.\\n\\nChris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of bleu in...\"}"}
{"id": "lrec-2024-main-737", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-737", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-737", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Longyue Wang, Mu Li, Fangxu Liu, Shuming Shi, Zhaopeng Tu, Xing Wang, Shuangzhi Wu, Jiali Zeng, and Wen Zhang. 2021. Tencent Translation System for the WMT21 News Translation Task. Proceedings of the Sixth Conference on Machine Translation (WMT), pages 216\u2013224.\\n\\nXianfeng Zeng, Yijin Liu, Ernan Li, Qiu Ran, Fan-dong Meng, Peng Li, Jinan Xu, and Jie Zhou. 2021. WeChat Neural Machine Translation Systems for WMT21. Proceedings of the Sixth Conference on Machine Translation (WMT), pages 243\u2013254.\\n\\nQingzhi Zhu. 2003. The Impact of Buddhism on the Development of Chinese Vocabulary (I). Universal Buddhist Gate Journal, 16(1):1\u201335.\"}"}
