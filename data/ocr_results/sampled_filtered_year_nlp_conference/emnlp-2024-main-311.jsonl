{"id": "emnlp-2024-main-311", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nDiffusion models are the state of the art in text-to-image generation, but their perceptual variability remains understudied. In this paper, we examine how prompts affect image variability in black-box diffusion-based models. We propose W1KP, a human-calibrated measure of variability in a set of images, bootstrapped from existing image-pair perceptual distances. Current datasets do not cover recent diffusion models, thus we curate three test sets for evaluation. Our best perceptual distance outperforms nine baselines by up to 18 points in accuracy, and our calibration matches graded human judgements 78% of the time. Using W1KP, we study prompt reusability and show that Imagen prompts can be reused for 10\u201350 random seeds before new images become too similar to already generated images, while Stable Diffusion XL and DALL-E 3 can be reused 50\u2013200 times. Lastly, we analyze 56 linguistic features of real prompts, finding that the prompt's length, CLIP embedding norm, concreteness, and word senses influence variability most. As far as we are aware, we are the first to analyze diffusion variability from a visuolinguistic perspective.\\n\\nOur project page is at http://w1kp.com.\\n\\n1 Introduction\\n\\nIn text-to-image generation, pictures are worth a thousand words, but which words are worth a thousand pictures? Specifically, how do prompts affect perceptual variation in generated imagery across random seeds? Consider these prompts:\\n\\nP1: A matte orange ball in the center against a pure white background.\\nP2: Orange ball against white background.\\n\\nAs shown in Figure 1, the first conveys a single particular illustration, while the second elicits multiple interpretations. Orange could refer to the fruit or the color, and the scene geometry is underspecified. But how can we quantify and characterize these linguistic intuitions?\\n\\nFigure 1: DALL-E 3 images for the prompts \u201ca matte orange ball in the center against a pure white background\u201d (top) and \u201corange ball against white background\u201d (bottom). Our W1KP score quantifies the perceptual similarity for each set of images. It yields 0.99 and 0.68 for the top and bottom rows, showing the greater image variability of the latter.\\n\\nIn this paper, we study the connection between visual variability and language in black-box text-to-image models, focusing on state-of-the-art diffusion models. Previous work tends to study the perceptual distance (Zhang et al., 2018) between pairs of images, while a prompt can generate a near infinite set of images. Furthermore, previous approaches have not been explicitly calibrated for human-friendly grades of similarity. What does a score of, for example, 0.2 mean in terms of perceived similarity? Such calibration is likely crucial for robust human interpretation.\\n\\nTo bridge these gaps in the literature, we first propose a straightforward framework for constructing human-calibrated perceptual variability measures based on existing perceptual distance metrics. We call it the Words of a Thousand Pictures method, or W1KP (\\\\['wIk.pi:] for short. On our crowd-sourced dataset of human-judged images from DALL-E 3, Imagen, and Stable Diffusion XL (SDXL), we validate our choice of DreamSim (Fu et al., 2024), a recent distance trained on Stable Diffusion (Rombach et al., 2022) images. Our variant of DreamSim outperforms the best\"}"}
{"id": "emnlp-2024-main-311", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"baseline by 0.1\u20130.4 points in two-alternative forced choice and 0.2\u20130.4 points in accuracy. To improve interpretability, we normalize and calibrate scores to graded human judgements on four levels of perceptual similarity, with cutoff points corresponding to high (0.85\u20131.0), medium (0.4\u20130.85), low (0.2\u20130.4), and no similarity (<0.2), which yield a correct classification 78% of the time.\\n\\nNext, to ground our academic discourse, we investigate the practical implications of our approach. Suppose a computer graphics practitioner wishes to generate a diverse array of images from a single prompt, but it is unclear how much it can be reused with different seeds before additional images contribute little to the variability of the overall set of images. Our work provides a quantitative metric for prompt reusability, as we explore further in Section 4.1. On DiffusionDB (Wang et al., 2023), an open dataset of user-written text-to-image prompts, we find that the same prompt can be reused forImagen for 10\u201320 random seeds, while SDXL and DALL-E 3 are more reusable at 100\u2013200 seeds.\\n\\nFinally, we study how 56 linguistic features affect generation variability. Although research has explored optimizing for image variability in diffusion (Sadat et al., 2024), they have not investigated the contributing linguistic constructs. To understand the underlying structure of these 56 features, we perform an exploratory factor analysis over DiffusionDB and uncover four factors of keyword presence (e.g., \u201cdog walking, 4K, watercolor\u201d), syntactic complexity (e.g., Yngve depth), linguistic unit length, and semantic richness. Then, we conduct clean-room, single-word generation experiments over the three strongest features in the semantic richness factor (concreteness, CLIP embedding norm, and number of word senses) to assess their contribution more precisely. We confirm that all three linguistic features significantly (p < 0.01) correlate with perceptual variability for all three diffusion models studied.\\n\\nOur contributions are as follows:\\n\\n1. we propose and validate a human-calibrated framework for building perceptual variability metrics from existing perceptual distance metrics;\\n2. we examine a new practical application of the method in assessing prompt reusability in text-to-image generation; and\\n3. we provide original insight into the linguistic sources of variability in diffusion models, finding that keywords, syntactic complexity, length, and semantic richness influence variability.\\n\\nOur W1KP Approach\\n\\n2.1 Preliminaries\\n\\nText-to-image diffusion models are a family of denoising generative models broadly consisting of two components: a text encoder that produces latent representations of language, such as T5 (Raffel et al., 2020) or CLIP (Radford et al., 2021), and a denoising image decoder that transforms random noise into an image conditioned on text, e.g., a convolutional variational auto-encoder (VAE; Ronneberger et al., 2022). To generate an image, we feed a prompt into the text encoder, pass its representation to the image decoder along with randomly sampled noise, then iteratively denoise the noise into a meaningful image. Large-scale models are generally trained using score matching (Song et al., 2021) on billions of image\u2013caption pairs (Podell et al., 2024), such as the now-deprecated LAION-5B dataset (Schuhmann et al., 2022).\\n\\nTo conduct a general study, we explore diffusion in a black-box manner to be able to generalize to proprietary models. Formally, let a text-to-image model be $G(\\\\{w_i\\\\}; s, \\\\theta)$ whose codomain comprises the sample space of all images $I$ and domain the sequence of words $\\\\{w_i\\\\}$, random seed $s \\\\in \\\\mathbb{Z}$ to initialize the image noise, and learned parameters $\\\\theta \\\\in \\\\mathbb{R}^p$. To generate multiple images from a single prompt, a standard practice is to run multiple trials for different random seeds $s$ (Podell et al., 2024), which we follow in our experiments. Our analyses target three state-of-the-art models, one open and two proprietary:\\n\\n1. Stable Diffusion XL (Podell et al., 2024), an open model which uses CLIP (Radford et al., 2021) for encoding text and a 2.6 billion-parameter U-Net (Ronneberger et al., 2015) for generating images.\\n2. DALL-E 3 (Betker et al., 2023), a proprietary API from OpenAI incorporating a pretrained T5-XXL (Raffel et al., 2020) text encoder and the same image decoder architecture as SDXL.\\n3. Imagen (Saharia et al., 2022), a similarly proprietary API from Google using a T5-XXL encoder and an efficient variant of a similar convolutional U-Net decoder.\\n\\nAll models produce images at least 1024 \u00d7 1024 pixels in resolution. Further details about the three models can be found in Appendix A.\"}"}
{"id": "emnlp-2024-main-311", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our General Framework\\n\\nWe aim to measure the visual variability of a set of synthetic images. Toward this, we propose to aggregate measures of image set variability (Definition 2.1) over selected DiffusionDB prompts. We use two in our experiments: ViT (Dosovitskiy et al., 2021), then compute a distance to human-judged levels of high, medium, low, and none. For the human judgement function (C; Eqn. 1) producing a single score in $[0, 1]$, calling it the W1KP score. We note a connection to statistical dispersion: if $X$ is standard uniform $U[0, 1]$, then a family $I$ of $X$ is $\\\\alpha$-ary attributed by $d$.\\n\\nA natural framework to do this is to define a family of $U$-statistics for measuring image set variability (Li, 2012; Hoeffding, 1948) over $X$. Concretely let $f$ be an i.i.d. mean kernel, the pairwise mean distance, a perceptual distance with a tight range of $[0, 1]$.\\n\\nLet $I$ be a sample of images generated by a backbone model, fed into the normalization gate perceptual distances, which are well studied in the literature, among all pairs of images in a set. The calibration module (D; Eqn. 3) aligned to human judgements (E) then assigns a similarity level (F).\\n\\nGiven a set of $n$ images, we can define a family $I$ of $X$ such that $\\\\alpha$-statistics for measuring image set variability of $I$ are uniformly distributed. The calibration module (D; Eqn. 3) aligned to human judgements (E) then assigns a similarity level (F). We illustrate our overall method in Figure 2, and a proof of Proposition 2.1 is given in Appendix B.\\n\\nMany metrics (Fu et al., 2024) embed a symmetric distance, a perceptual distance with a tight range of $[0, 1]$. Our sample, we generate 10,000 image pairs per diffusion model for 1,000 randomly selected DiffusionDB prompts. As our sample, we generate 10,000 image sets of images:\\n\\n- Low\\n- Medium\\n- High\\n- None\\n\\nThe calibration module (D; Eqn. 3) aligned to human judgements (E) then assigns a similarity level (F). This measures the expected maximum similarity among all pairs of images. A proof is in Appendix B. Further, to match the convention of scores in the literature, among all pairs of images in a set.\\n\\nLastly, we find cutoff points for Figure 2: An illustration of W1KP: image embedding.\"}"}
{"id": "emnlp-2024-main-311", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Quality of the backbones on our evaluation sets, across the image generation model.\\n\\n### 3.1 W1KP Quality\\n\\nBefore applying W1KP, we first validate our choice of the perceptual distance backbone.\\n\\n**Setup.** Following prior work in perceptual distance evaluation (Zhang et al., 2018), we crowd-sourced a dataset of two-alternative forced-choice (2AFC) image triplets using Amazon MTurk (Hauser and Schwarz, 2016). Five unique workers were shown three generated images from the same prompt\u2014a reference image, image A, and image B\u2014and instructed to pick whether A or B resembled the reference more. This was repeated three times each for 500 random prompts from DiffusionDB, a large dataset of user-written prompts, for each of SDXL, Imagen, and DALL-E 3, totaling 1,500 triplets per model.\\n\\nFormally, let \\\\( \\\\{ (I_r^i, I_a^i, I_b^i, y^i_a) \\\\}_{M_i=1} \\\\) be a dataset of triplets, where \\\\( I_r^i, I_a^i, I_b^i \\\\in I \\\\) are images and \\\\( y^i_a \\\\in \\\\{0, \\\\ldots, 5\\\\} \\\\) the number of workers choosing \\\\( I_a^i \\\\) over \\\\( I_b^i \\\\). We used attention checks throughout the process; for more details, see Appendix A.3.\\n\\nFor our non-neural methods, we evaluated raw-image Euclidean distance (L2) and the structural similarity index (SSIM; Wang et al., 2004). For our neural backbones, we tested the popular LPIPS (Zhang et al., 2018), its shift-tolerant variant ST-LPIPS (Ghildyal and Liu, 2022), and an SSIM-inspired variant DISTS (Ding et al., 2020), all based on the VGG-16 architecture (Simonyan and Zisserman, 2015); SSCD (Pizzi et al., 2022), a model trained for image copy detection; CoPer (Li et al., 2022), an extension of LPIPS to ViT; raw cosine similarity scores from CLIP (Radford et al., 2019); and lastly, DreamSim (Fu et al., 2024), which ensembles pretrained transformers trained on Stable Diffusion images for feature extraction and applies cosine distance for measurement. Since DreamSim's domain was closest to ours, we hypothesized that it would be most effective. We also evaluated our variant, DreamSim \\\\(_{\\\\ell_2} \\\\), with Euclidean instead of cosine distance for \\\\( d \\\\), which benefits from being a true mathematical distance and hence allows for multidimensional scaling analyses, as in Appendix E.\\n\\nWe used the standard evaluation metrics of 2AFC score, defined as the mean proportion of workers agreeing with the backbone's scores, i.e.,\\n\\n\\\\[\\n\\\\frac{1}{M} \\\\sum_{i=1}^{M} \\\\left( \\\\begin{array}{l} y_a^i \\\\\\\\ 5 + y_a^i \\\\end{array} \\\\right)\\n\\\\]\\n\\nwhere \\\\( I_a^i \\\\prec_{r} I_b^i \\\\) if \\\\( \\\\tilde{\\\\eta}(\\\\{I_r^i, I_a^i\\\\}) < \\\\tilde{\\\\eta}(\\\\{I_r^i, I_b^i\\\\}) \\\\), and majority-vote accuracy. We let \\\\( \\\\tilde{\\\\eta} = \\\\tilde{\\\\eta}_{\\\\text{mean}} \\\\). See Appendix A.3 for further setup details.\\n\\n**Results.** We present our results in Table 1. As an upper bound, we report the maximum possible 2AFC and accuracy in row one. In line with intuition, our DreamSim backbones attain the highest quality, surpassing CLIP\\\\(_L_{14}\\\\)raw, the second best, by 2.0 points in 2AFC and 2.8 in accuracy on average. Our variant DreamSim\\\\(_{\\\\ell_2}\\\\) slightly outperforms the original DreamSim with statistical significance (\\\\( p < 0.05 \\\\) on the paired t-test) by 0.1\u20130.4 in 2AFC and 0.2\u20130.4 in accuracy, possibly since the embedding norm is informative (Oyama et al., 2023). Thus, we select DreamSim\\\\(_{\\\\ell_2}\\\\) as the backbone for W1KP.\\n\\nBeyond quality assurance, another purpose of this evaluation is to ensure that the backbone does equally well on the three image generators. As a sanity check, the oracle (row one) has a spread of 1.4 points (79.3\u201380.7) in 2AFC on the three models, indicating that humans are unbiased. Our DreamSim\\\\(_{\\\\ell_2}\\\\) has a spread of 2.2 points (69.3\u201371.5) in 2AFC, which is below the global average spread of 3.3 points for all the methods. We conclude that DreamSim\\\\(_{\\\\ell_2}\\\\) exhibits less model-wise bias than its counterparts, possibly due to its increased quality and in-domain training.\\n\\nA potential issue is that perceptual similarity is inherently subjective and hence challenging to measure. Research suggests to also evaluate just-noticeable differences (JND), which is thought to be cognitively impenetrable due to its viewing-time constraint (Acuna et al., 2015). Because of the high correlation between 2AFC and JND on synthetic images (\\\\( r = 0.94 \\\\); Fu et al., 2024), 2AFC appears to be a viable proxy for JND for our study.\"}"}
{"id": "emnlp-2024-main-311", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 W1KP Metric Interpretation\\n\\nWe now assess the quality of our human calibration process, as described near the end of Section 2.2.\\n\\nSetup.\\n\\nWe collected a dataset of graded image pairs with MTurk. For 500 random DiffusionDB prompts, three unique workers were presented with two images generated from the same prompt and asked to judge the similarity on an integral scale ranging from \u201cnot similar at all\u201d (rating 1) to \u201cthe same\u201d (5). Afterwards, we merged the last two categories (\u201csame\u201d and \u201cvery similar\u201d) since the fifth was mostly reserved for attention checks, resulting in the final four categories of high, medium, low, and no similarity. We took the median across the three judgements and repeated the process for SDXL, Imagen, and DALL-E 3, for a total of 1,500 median judgements roughly split into 10%, 30%, 40%, and 20% for ratings 1\u20134. Our evaluation then consisted of applying Eqn. (3) with five-fold cross validation. For detailed settings, see Appendix A.3.\\n\\nResults.\\n\\nEqn. (3) yields cutoff points (rounded to the nearest 0.05 for memorability) of 0.22, 0.4, and 0.85 for $\\\\beta_{\\\\text{low}}$, $\\\\beta_{\\\\text{mid}}$, and $\\\\beta_{\\\\text{high}}$. Overall, we attain macro- and micro-accuracy scores of 80% and 78% with DreamSim $\\\\ell_2$ as the backbone. For comparison, the average macro-/micro-accuracy scores of humans are 82%/80%. DreamSim $\\\\ell_2$ also outperforms the original DreamSim, which has a macro-/micro-accuracy of 79%/77%. Thus, we conclude that our calibration yields interpretable cutoffs.\\n\\nDALL-E 3\\nImagen\\n\\nFigure 4: Visualizing the overlap between the two most similar images (on average) as we generate more images for the two prompts. We remove the green channel for one image (magenta) and keep only the green for the other, then stack the two. Above, Imagen is reusable up to 10\u201350 images, while DALL-E 3 up to 50\u2013200.\\n\\nWe present qualitative examples of our cutoffs in Figure 3. The levels appear sensible: \u201chigh\u201d pairs (top row) match in low-level features (e.g., trees in the same location), high-level composition (e.g., cats in washing machine), artistic style (e.g., color photography); medium (second) in composition and style; low (third) in style; and none (last) mostly differing in all. This aligns with our quantitative results in Appendix F.1. We also verify that normalization (Eqn. 1) is necessary. Before normalization, raw W1KP scores have 10th, 50th, and 90th percentiles of 0.4, 0.7, and 1.1, which is significantly nonuniform ($p<0.01$; KS test).\\n\\nOne conceivable question is whether calibration and normalization are essential for downstream analysis. It can be argued that analytic conclusions may still hold without a normalized, calibrated metric. However, as alluded to in Section 2.2, there are two clear benefits to having one: first, normalization scales arbitrary scores to the 0\u20131 range, in line with other common statistics such as $F_1$ score and $R^2$. Our normalized score also has the direct interpretation as the percentile of the raw score on a known ground-truth distribution. Second, calibration allows us to interpret scores and aid human understanding. In Section 4.1 for example, we use $\\\\beta_{\\\\text{high}}$ as a cutoff for prompt reusability.\"}"}
{"id": "emnlp-2024-main-311", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 Prompt Reusability Analysis\\n\\nWe first ask how many times a prompt can be reused (under different random seeds) until new images are too similar to already generated ones. This applies to graphic asset creation in particular, where visual artists are tasked with rendering many images of the same concept. To study this quantitatively, we sampled 50 random prompts from DiffusionDB, generated 300 images for each prompt using different seeds on SDXL, Imagen, and DALL-E 3, then computed the \\\\( \\\\kappa \\\\)-expected maximum \\\\( \\\\tilde{\\\\eta}_k \\\\) for \\\\( k = 1, \\\\ldots, 300 \\\\).\\n\\nAs visualized in Figure 4 and plotted in Figure 5, our diffusion models vary in reusability. DALL-E 3 on average does not generate highly similar images \\\\( \\\\tilde{\\\\eta}_k \\\\geq \\\\beta_{\\\\text{high}} \\\\) until \\\\( k \\\\to 200 \\\\), with our visualization (top two rows in Figure 4, one prompt each) displaying much green- and magenta-shifting until the last column. On the other hand, Imagen tends to produce duplicate images for \\\\( k \\\\to 50 \\\\). At 50 images, the two overlaid images are nearly indistinguishable from the true-color image; see the third column. Figure 5 corroborates these visual results, with the red line \\\\( \\\\beta_{\\\\text{high}} \\\\) intersecting Imagen's green line between 5\u201310 and DALL-E 3's blue line at 50\u2013100. It also suggests that SDXL resembles DALL-E 3 in prompt reusability; see the overlap between the two. We conclude that diffusion models differ in prompt reusability, possibly due to different decoder architectures. For example, DALL-E 3 and SDXL share the same U-Net architecture, whereas Imagen's is sparsified (Saharia et al., 2022).\\n\\n4.2 Exploratory Factor Analysis\\n\\nOur next two analyses relate various linguistic features of prompts such as syntactic complexity to perceptual variability. First, to understand the salient structure of these linguistic features, we conduct a factor analysis over DiffusionDB.\\n\\nSetup.\\n\\nOur analysis emulates previous work in interpreting linguistic features for speech (Fraser et al., 2016). We extracted 56 features for each of the 1,000 random prompts:\\n\\n- **Syntactic complexity**: 24 scalar features related to syntax comprehension, such as clauses per T-unit and mean T-unit length, extracted using L2SCA (Lu, 2010). We also added Yngve depth, a measure of embeddedness (Yngve, 1960). Our motivation was that sentences with more qualifying and nominal may be more visually precise.\\n- **Keywords**: 20 Boolean features indicating the presence of the top-20 keywords. We noticed that most prompts contained trailing keyword qualifiers after a noun phrase, e.g., \u201ccat beside road, 4k\u201d (see Appendix C for more); thus, we extracted the top 20 as features.\\n- **Word order**: 3 Boolean features denoting the presence of the PTB (Marcinkiewicz, 1994) part-of-speech patterns \u201cNN VB,\u201d \u201cNN VB RB,\u201d and \u201cJJ NN\u201d in the prompt. Our purpose was to assess the effects of adjectives and verbs on nouns.\\n- **Psycholinguistics**: 4 features in mean concreteness judgements (Brysbaert et al., 2014), richness (Honore's statistic and whether a word was in a 100k-word dictionary), and word frequency (Brysbaert and New, 2009).\\n- **Semantic relations**: 3 scalars for the mean number of hyponyms, hypernyms, and word senses, from WordNet (Miller, 1995) enhanced with word sense clustering (Snow et al., 2007). Intuitively, words with many synonyms (e.g., \u201csaw\u201d) or hyponyms (e.g., \u201canimal\u201d) may have more visual representations.\\n- **Embedding norm**: 2 scalars for the mean square GloVe norm (Pennington et al., 2014) and CLIP embedding norm (Radford et al., 2021). Word embedding norms were found to encode information gain (Oyama et al., 2023), which may affect perceptual variability through specificity.\\n\\nWe generated 20 images per prompt for SDXL, Imagen, and DALL-E 3 and used Stanford CoreNLP (Manning et al., 2014) as our parser (additional details in Appendix D.1).\"}"}
{"id": "emnlp-2024-main-311", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Linguistic features grouped by interpreted factors, with high loadings (\u2265 0.3) in bold and low loadings (< 0.1) removed. All Spearman\u2019s $\\\\rho$ are statistically significant ($p<0.05$); insignificant features omitted.\\n\\nResults.\\n\\nWe present our results in Table 2. Following standard practice (Fraser et al., 2016), we use an oblique promax rotation to enable interfactor correlation. Four factors capture sufficient variance according to Kaiser's criterion (Kaiser, 1958). For each feature, we report its correlation (Spearman\u2019s $\\\\rho$) with the per-prompt perceptual similarity ($\\\\tilde{\\\\eta}_{\\\\text{mean}}$) and compute the mean feature score $\\\\mu$.\\n\\nAs is conventional, we manually explain the four factors (F1\u2013F4). For F1, \u201c8k,\u201d \u201cdetailed,\u201d \u201ccinematic,\u201d and \u201cdigital art\u201d describe the art style, \u201ccg-society\u201d pertains to computer graphics, and \u201cartgerm\u201d is an artist with a specific style; hence, we call it \u201cstyle keyword presence.\u201d F2\u2019s features are classic measures of syntactic complexity (Lu, 2010) and thus labeled as such. In F3, mean length of clauses, sentences, and T-units quantify various lengths, so we name it \u201clinguistic unit length.\u201d Lastly, F4 primarily depicts semantic richness, with concreteness, CLIP embedding norm (related to information gain), number of word senses, and ADJ NOUN roughly characterizing visual (non)ambiguity and Honore\u2019s statistic, the number of words, and \u201cnot in dictionary\u201d portraying lexical richness.\\n\\nOur feature correlations with W1KP agree with intuition. Having higher concreteness (e.g., house vs. dignity) and fewer word senses (saw vs. tomato) increases similarity (rows 20, 21), likely since abstract and polysemous words have more visual interpretations. Complex nominals (row 11), adjectival modifiers (row 18), and keywords (F1) limit variability through qualification. Semantic richness has the strongest correlated features, with half having $|\\\\rho| > 0.2$. CLIP norm is the most predictive of variability ($\\\\rho = -0.31$), possibly because text embeddings from vision-language models are used to initialize image generation (Sec. 2.1). Larger norms may yield more chaotic decoding trajectories in the iterative solver, increasing variability.\\n\\nFactor-wise, linguistic unit length has the highest mean $|\\\\rho|$ of 0.19, where sentence length is the third most predictive feature ($\\\\rho = 0.27$). Longer prompts presumably provide more visual information. We conclude that many features in the linguistic space are predictive of variability in the visual space, especially CLIP norm, length, and concreteness.\\n\\n4.3 Confirmatory Lexical Analysis\\n\\nThe last section studies how prompts relate to variability in the DiffusionDB corpus. While it benefits from realism, some experimental control is lost. Thus, to supplement the previous study, this section uses single-word synthetic prompts, sampled and adjusted for word frequency in a clean-room manner. We examine the effects of concreteness, CLIP norm, and polysemy\u2014three of the strongest features from Sec. 4.2.\\n\\nSetup.\\n\\nFor our prompts, we sampled 500 words from the 10k most common words in the Google Trillion Word Corpus (Brants and Franz, 2006). We noted each word\u2019s concreteness rating ($x_{\\\\text{conc}}$), number of word senses ($x_{\\\\text{sens}}$), CLIP embedding norm ($x_{\\\\text{clip}}$), and frequency rank ($x_{\\\\text{freq}}$) as our explanatory variables, mirroring the setup of Section 4.2. Words without concreteness ratings were resampled. We then generated 20 images for each prompt with SDXL, Imagen, and DALL-E 3 and measured perceptual variability using $\\\\tilde{\\\\eta}_{\\\\text{mean}}$. For our analysis, we fit a linear mixed model with $x_{\\\\text{conc}}$, $x_{\\\\text{sens}}$, $x_{\\\\text{clip}}$, and $x_{\\\\text{freq}}$ as the fixed effects, an intercept for each diffusion model as the random effect, and $\\\\tilde{\\\\eta}_{\\\\text{mean}}$ as the response variable. Our purpose is to test whether concreteness, polysemy, CLIP norm, and word frequency independently influence perceptual variability for each model.\"}"}
{"id": "emnlp-2024-main-311", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: A plot of \\\\( \\\\tilde{\\\\eta}_{\\\\text{mean}} \\\\) against frequency, CLIP norm, concreteness, and word senses for single-word prompts. Shaded regions are 95% confidence intervals.\\n\\nResults. Our linear mixed model reveals statistically significant relationships (\\\\( p<0.01 \\\\)) between \\\\( \\\\tilde{\\\\eta}_{\\\\text{mean}} \\\\) and all the predictors, whose coefficients are \\\\( 2.4 \\\\times 10^{-3}, 4.7 \\\\times 10^{-4}, -7.8 \\\\times 10^{-5}, \\\\) and \\\\( -7.2 \\\\times 10^{-2} \\\\) for \\\\( x_{\\\\text{sens}}, x_{\\\\text{clip}}, x_{\\\\text{freq}}, \\\\) and \\\\( x_{\\\\text{conc}} \\\\), respectively. In other words, polysemy, CLIP norm, word frequency, and concreteness are significant independent factors for perceptual variability, where polysemy and CLIP norm are positively correlated, while frequency and concreteness negatively so. In Figure 6, our feature-wise plots further illustrate each individual fixed effect. The correlation scores are consistent in direction across the diffusion models, with similar signs in Spearman's \\\\( \\\\rho \\\\) for each feature. They also differ by an additive shift, affirming our random-intercepts mixed model.\\n\\nFigure 7 presents prompts of varying concreteness and senses. \\\"Cowboy,\\\" a concrete prompt, is less variable than \\\"concept,\\\" an abstract one, since a cowboy is tangible. \\\"Tomato,\\\" a monosemous word, has less variability than \\\"saw,\\\" a polysemous word, because it has a narrow visual representation. In summary, our exploratory findings on concreteness, CLIP norm, and polysemy from Section 4.2 hold in the clean-room single-word prompt setting.\\n\\n5 Related Work and Future Directions\\n\\nA related line of work examines boosting image variability in diffusion models (Zameshina et al., 2023; Sadat et al., 2024; Gu et al., 2024). Complementary to their work, our paper analyzes the precise linguistic features contributing to variability. One future direction could be to incorporate these features into the optimization of variability. Previous work has analyzed diffusion models using a mixture of computational linguistics and vision techniques. Tang et al. (2023) conducted an attribution analysis over Stable Diffusion and discovered entanglement, to which Rassin et al. (2024) proposed to fix using attention alignment. Separately, Toker et al. (2024) studied the layer-intermediate representations of diffusion, showing that rare concepts require more computation. A further extension could be to study linguistic features responsible for increased computation, as our paper also relates word rarity to variability. Finally, research has previously scrutinized the (lack of) variability in older architectures such as V AEs (Razavi et al., 2019) and generative adversarial networks, e.g., mode collapse. In this paper, we extend this analysis to modern diffusion models while taking a visuolinguistic perspective.\\n\\n6 Conclusions\\n\\nIn conclusion, we examined the connection between visual variability and prompt language for black-box diffusion models. We proposed a framework for quantifying and calibrating visual variability, applying it to study prompt reusability and linguistic feature salience. After validating it quantitatively, we found that length, embedding norm, and concreteness influence variability the most.\"}"}
{"id": "emnlp-2024-main-311", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nOne limitation of our work is that while we analyzed the inference-time behavior of various diffusion models, we did not trace the training-time cause of perceptual variability due to the scope of our study. Doing so would require the training of multiple diffusion models while varying the training sets, which is beyond our budget.\\n\\nAnother limitation is that we have not meticulously characterized the precise distribution of perceptual variability relative to various levels of linguistic features, with our analyses constrained to averages due to the moderate sample size. For instance, does Imagen yield a higher maximum variability for certain levels of concreteness, even if on average it is lower? Are there subgroups within each feature that better explain variances in perceptual variability? Such questions require a larger sample size to answer.\\n\\nWe also consciously limited our examination to random seeds and dispensed with comprehensively assessing other factors possibly influencing perceptual variability, such as classifier-free guidance (Ho and Salimans, 2021). We vary the guidance scale in Appendix D.2 to confirm that SDXL is always more diverse than Imagen regardless of guidance; nevertheless, a study with additional factors other than linguistic features and random seeds could yield more insights.\\n\\nFinally, it should be noted that our work intentionally disregards the relationship between quality and variability, although the two can be conflated. For example, does increased variability reduce image quality? Is Imagen a better option than, say, SDXL due to its higher quality, even if it generates less diverse imagery? Thus, text-to-image models should not be chosen based on the findings of our study alone. Rather, our work supplements image quality metrics in model selection.\"}"}
{"id": "emnlp-2024-main-311", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-311", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Interface for collecting 2AFC judgements.\\n\\nA Detailed Experimental Settings\\n\\nA.1 Computational Environment\\nOur primary software toolkits included Hugging- Face Diffusers 0.25.0, Transformers 4.40.1, PyTorch 2.1.2, DreamSim 0.1.3, and CUDA 12.2. We ran all experiments on a machine with four Nvidia A6000 GPUs and an AMD Epyc Milan CPU.\\n\\nA.2 Diffusion Model Details\\nSDXL. We downloaded stabilityai/stable-diffusion-xl-base-1.0 from HuggingFace zoo. We used the default guidance scale of 7.5 and 30 inference steps without the additional refiner module. Each 1024x1024 SDXL image took 4\u20135 seconds to generate per card, resulting in a throughput of roughly 50\u201360 images per minute.\\n\\nImagen. We selected the imagegeneration@006 model, the latest version as of April 2024, and generated four square images per call while varying the random seed. This matched our SDXL throughput of 50\u201360 images per minute. Each image was 1536x1536 in resolution.\\n\\nDALL-E 3. For DALL-E 3, we used the default parameters of \u201chd\u201d resolution (1024x1024) and \u201cvivid\u201d style. To mitigate prompt editing, we followed the official documentation and prepended \u201cI NEED to test how the tool works with extremely simple prompts. DO NOT add any detail, just use it AS-IS: \u201c to the prompt. The generation speed of DALL-E 3 was considerably slower than Imagen and SDXL at approximately 10 images per minute.\\n\\nA.3 Annotation Apparatuses\\nWe are deemed exempt by the <blinded> board of ethics requirements for review.\\n\\nW1KP quality. We present the annotation user interface for collecting 2AFC judgements in Figure 8. For our attention checks, we showed each worker at least one triplet with either image A or B exactly matching the reference. If the correct answer was not chosen, we rejected all their labels and blocked them. This resulted in a pass rate of around 90%. For higher quality, we required our workers to be \u201cMasters\u201d for participation eligibility.\\n\\nW1KP metric interpretation. We present our annotation interface for gathering graded similarity judgements in Figure 9. For the attention checks, we showed each annotator at least one pair of images that were the exact same. If they did not choose \u201calmost the same,\u201d we discarded all their judgements, resulting in an acceptance rate of 95%.\\n\\nB Detailed Proofs\\n\\nProposition 2.1. If $X$ is a continuous random variable, $F(X)$ is standard uniform $U[0,1]$.\\n\\nProof. Let $X$ be a continuous r.v. If $X$ is $U[0,1]$, then its CDF $P(X \\\\leq x) = x$. Since $P(F(X) \\\\leq x) = P(X \\\\leq F^{-1}(x)) = F(F^{-1}(x)) = x$, then $F(X)$ is $U[0,1]$, completing our proof.\\n\\nProposition 2.2. If $d$ is the squared Euclidean distance and $h$ the pairwise mean kernel, $U_{d,h}$ is proportional to the trace of the covariance matrix of $f(I_1),...,f(I_n)$, i.e., the total variance.\\n\\nProof. Consider the pairwise sum squared Euclidean distance $\\\\sum_{i \\\\neq j} ||f(I_i) - f(I_j)||^2$, which expands into $\\\\sum_{i \\\\neq j} f(I_i)\\\\top f(I_i) - 2f(I_i)\\\\top f(I_j) + f(I_j)\\\\top f(I_j)$. (4)\\nThe first and third self-product terms expands as $(n-1)n\\\\sum_{i=1} f(I_i)\\\\top f(I_i)$ (5)\\nand $(n-1)n\\\\sum_{j=1} f(I_j)\\\\top f(I_j)$, (6)\"}"}
{"id": "emnlp-2024-main-311", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and the middle term\\n\\\\[\\\\sum_{i,j} f(I_i)\\\\mathbf{1} f(I_j) - \\\\sum_{i=1}^n f(I_i)\\\\mathbf{1} f(I_i). \\\\]\\n(7)\\nAfter algebraic manipulation, we arrive at\\n\\\\[\\\\text{\\\\((n-1)\\\\sum_{i=1}^n f(I_i)\\\\mathbf{1} f(I_i) - \\\\frac{1}{n^2} \\\\sum_{i,j} f(I_i)\\\\mathbf{1} f(I_j)\\\\).} \\\\]\\n(8)\\nWe are ready to relate this quantity to the trace of the covariance matrix, given by\\n\\\\[\\\\text{\\\\(\\\\text{tr}(\\\\Lambda) = \\\\sum_{i=1}^n \\\\| f(I_i) - \\\\frac{1}{n} \\\\sum_{j=1}^n f(I_j)\\\\|^2\\\\).} \\\\]\\n(9)\\nwhich simplifies as\\n\\\\[\\\\frac{1}{n} \\\\left(\\\\sum_{i=1}^n f(I_i)\\\\mathbf{1} f(I_i) - \\\\frac{1}{n} \\\\sum_{i,j} f(I_i)\\\\mathbf{1} f(I_j)\\\\right). \\\\]\\n(10)\\nMultiplying by \\\\((n-1)\\\\), we arrive at the sum of the pairwise squared Euclidean distance. Dividing by \\\\(n(n-1)\\\\) yields the mean pairwise squared distance, and our proof is finished.\\n\\nC DiffusionDB Statistics\\nWe now characterize the prompts and keywords in DiffusionDB. To extract trailing keywords, we split prompts into a main part and its keywords part by applying these steps:\\n1. Tokenize the prompt by commas, e.g., \\\"cat walking, 4k\\\" becomes \\\"cat walking\\\" and \\\"4k.\\\"\\n2. If any \\\"token\\\" after the first is shorter than four words, everything after that token is considered a keyword.\\n3. The first \\\"token\\\" is always the main prompt.\\nA preliminary analysis showed that this was more than 95% accurate in identifying keywords. We present ten examples below:\\n1. \\\"ashtray in the messy desk of the detective, smoke and dark, digital art\\\"\\n2. \\\"onion very sad crying big tears cartoon, 3d render\\\"\\n3. \\\"the lost city of Atlantis, 4K, hyper detailed\\\"\\n4. \\\"a galleon ship by Darek Zabrocki\\\"\\n5. \\\"hill overlooking a viking city, fantasy, forested, large trees, top down perspective, [\\\\ldots]\\\"\\n6. \\\"photo of an awesome sunny day environment concept art on a cliff, architecture by daniel libeskind with village, residential area, mixed development, highrise made up staircases, [\\\\ldots]\\\"\\n7. \\\"giant oversized battle hedgehog with army pilot uniform and hedgehog babies, in deep forest jungle, full body, Cinematic focus, Polaroid photo, vintage, neutral dull colors, soft lights, [\\\\ldots]\\\"\\n8. \\\"pizza the hut, akira, gorillaz, poster, high quality\\\"\\n9. \\\"tengu spotted in atlanta\\\"\\n10. \\\"underground cinema, realistic architecture, colorfull lights, octane render, 4k, 8k\\\"\\n\\nD Visualinguistic Analysis Details\\nD.1 Linguistic Feature Extraction\\nFor word sense clustering, we used the \\\"WN 2.1 -19370 synsets\\\" resource from https://ai.stanford.edu/~rion/swn/, previously published in Snow et al. (2007). Unless otherwise stated, all our CLIP models were initialized from the openai/clip-vit-large-patch14-336 checkpoint from HuggingFace, released by OpenAI. Our GloVe embeddings were the 300-dimensional embeddings trained on 840B tokens of web text.\\n\\nD.2 Effects of Classifier-Free Guidance\\nWe briefly confirmed that increasing classifier-free guidance did not worsen the perceptual variability of SDXL below that of Imagen. Imagen and DALL-E 3 do not expose classifier-free guidance as an input parameter, hence limiting us to SDXL. We increased the classifier-free guidance from 5.0 to 30, much higher than the normal range of 5.0\u20137.5, and regenerated the images in Section 4.1. We arrived at a mean W1KP score of 0.53 for SDXL, which was below Imagen's score of 0.62, e.g., SDXL still had greater variability.\"}"}
{"id": "emnlp-2024-main-311", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Twenty generated images for the prompt \\\"cat,\\\" clustered using multidimensional scaling on DreamSim $\\\\ell_2$. Imagen produces six distinct clusters.\\n\\nFigure 11: Generated images for a longer prompt.\"}"}
{"id": "emnlp-2024-main-311", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"During peer review, our reviewers provided helpful feedback on the paper. We explicitly address a few of their points below for transparency.\\n\\nFirst, it was mentioned that reducing dissimilarity to a single numerical score does not do justice to all the nuances of image perception. To this, we concur. Summarizing a range of phenomena as a single scalar is a key drawback of any evaluation metric, and our approach is not different in this regard from well-established ones such as CLIP, BLEU, BERT score, Spearman's rho, Cohen's kappa, and others. For example, a high BERT score or BLEU may not mean that translation quality is definitively good. That remains to be judged on a task-by-task basis.\\n\\nA second point from the reviewers was that our computational contribution in the current work was unclear, as our DreamSim model is only marginally better. In our response, we emphasized that our key contributions are to propose and validate a human-calibrated framework for building variability metrics from existing baselines such as DreamSim-L2. We examine a new practical application of the method and provide new linguistic insight.\\n\\nA third question was about how a variability measure should balance between coverage and uniqueness, and how our measure supports this. Such nuances are important to the design of the kernel function, for which we construct and analyze two chosen measures. In the first pairwise-mean kernel ($\\\\eta_{\\\\text{mean}}$), all-pair similarities are weighted equally in a set. Intuitively, this should provide a balanced assessment of overall variability (e.g., coverage), as every image pair has equal weight. In the second $k$-expected maximum kernel ($\\\\eta_{k}$), we estimate the maximum expected image-pair similarity out of a set of size $k$, thus focusing on the nearest pair of images (intuitively, the lack of uniqueness, e.g., duplicates in a set of size $k$). Our choice of W1KP is further grounded by our human alignment, which provides interpretation of the scores.\\n\\nLastly, a few comments centered on the practical utility of obtaining multiple images from the same prompt. In the multimedia industry, visual artists are tasked with storyboarding and brainstorming, which require creating different images of the same idea. Our approach would assess the reusability of each prompt for that purpose before a prompt is considered \u201cused up.\u201d\\n\\nF.1 Metric Interpretation Quantitative Study\\n\\nOne of the reviewers suggested quantifying the extent to which our W1KP cutoffs corresponded to qualitative features such as composition and style similarity, as claimed in Section 3.2. For this, we annotated 50 pairs of images, each from a different prompt from DiffusionDB, for each model. For each image pair, we noted whether the two images matched in low-level features, high-level composition, and artistic style. We found the following medians across the models:\\n\\n| Rating | Sim. Composition | Sim. Style |\\n|--------|-----------------|------------|\\n| None   | 18%             | 26%        |\\n| Low    | 24%             | 40%        |\\n| Medium | 66%             | 88%        |\\n| High   | 82%             | 90%        |\\n\\nTable 3: The percentage of pairs matching in features, composition, and style, grouped by W1KP rating.\\n\\nThe qualitative similarity increases with the rating, in order from low-level feature similarity to high-level style similarity, supporting our qualitative findings in Section 3.2.\"}"}
