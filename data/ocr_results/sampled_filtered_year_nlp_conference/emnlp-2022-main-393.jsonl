{"id": "emnlp-2022-main-393", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Leveraging Affirmative Interpretations from Negation Improves Natural Language Understanding\\n\\nMd Mosharaf Hossain\\n\\nDepartment of Computer Science and Engineering, University of North Texas\\n\\nEduardo Blanco\\n\\nDepartment of Computer Science, University of Arizona\\n\\nmdmosharafhossain@my.unt.edu hosmdmos@amazon.com eduardoblanco@arizona.edu\\n\\nAbstract\\n\\nNegation poses a challenge in many natural language understanding tasks. Inspired by the fact that understanding a negated statement often requires humans to infer affirmative interpretations, in this paper we show that doing so benefits models for three natural language understanding tasks. We present an automated procedure to collect pairs of sentences with negation and their affirmative interpretations, resulting in over 150,000 pairs. Experimental results show that leveraging these pairs helps (a) T5 generate affirmative interpretations from negations in a previous benchmark, and (b) a RoBERTa-based classifier solve the task of natural language inference. We also leverage our pairs to build a plug-and-play neural generator that given a negated statement generates an affirmative interpretation. Then, we incorporate the pretrained generator into a RoBERTa-based classifier for sentiment analysis and show that doing so improves the results. Crucially, our proposal does not require any manual effort.\\n\\n1 Introduction\\n\\nNatural Language Understanding is a crucial component to build intelligent systems that interact with humans seamlessly. While recent papers sometimes report so-called superhuman performance, simple adversarial attacks including adding negation and other input modifications remain a challenge despite they are obvious to humans (Naik et al., 2018; Wallace et al., 2019). Further, many researchers have found that state-of-the-art systems struggle with texts containing negation. For example, Kassner and Sch\u00fctze (2020) show that pre-trained language models such as BERT (Devlin et al., 2019) do not differentiate between negated and non-negated cloze questions (e.g., Birds can\u2019t [MASK] vs. Birds can [MASK]). Other studies show that transformers perform much worse in many other natural language understanding tasks when there is a negation in the input sentence (Ribeiro et al., 2020; Ettinger, 2020; Hossain et al., 2020b; Hosseini et al., 2021; Hossain et al., 2022a; Truong et al., 2022).\\n\\nIn this paper, we address this challenge building upon the following observation: negation often carries affirmative meanings (Horn, 1989; Hasson and Glucksberg, 2006). For example, people intuitively understand that John read part of the book from John didn\u2019t read the whole book. Our fundamental idea is to leverage a large collection of sentences containing negation and their affirmative interpretations. We define an affirmative interpretation as a semantically equivalent sentence that does not contain negation. We explore this idea by automatically collecting pairs of sentences with negation and their affirmative interpretations from parallel corpora and backtranslating. Figure 1 exemplifies the idea with English-Norwegian and English-Spanish parallel sentences. Note that (a) either the original English sentence or the backtranslation have a negation (the one that does not is the affirmative interpretation) and (b) the meaning of both...\"}"}
{"id": "emnlp-2022-main-393", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Armed with the large collection of sentences containing negation and their affirmative interpretations, we show that leveraging them yields improvements in three natural language understanding tasks. First, we address the problem of generating affirmatively interpretations in the AFIN benchmark (Hossain et al., 2022b), a collection of sentences with negation and their manually curated affirmative interpretations. Second, we address natural language inference using three common benchmarks: RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Ben-Aviv et al., 2009), SNLI (Bowman et al., 2015), and MNLI (Williams et al., 2018). Third, we address sentiment analysis using SST-2 (Socher et al., 2013). The main contributions of this paper are:\\n\\n1. A large collection (153,273) of pairs of sentences containing negation and their affirmative interpretations. We present an automated procedure to get these pairs and an analysis of the negation types (single tokens, morphological, lexicalized, etc.).\\n\\n2. Experimental results with the T5 transformer (Raffel et al., 2020) showing that blending our pairs during the fine-tuning process is beneficial to generate affirmative interpretations from the negations in AFIN.\\n\\n3. Experimental results showing that a RoBERTa-based classifier (Liu et al., 2019) to solve the task of natural language inference benefits from training with new premise-hypothesis derived from our pairs (two entailments per pair).\\n\\n4. Experimental results showing that a RoBERTa-based classifier for sentiment analysis benefits from a novel component that automatically generates affirmative interpretations from the input sentence.\\n\\nThe key resource enabling the experimental results is our large collection of pairs of sentences containing negation and their affirmative interpretations. As we shall see, the experiments under (2) and (3) are a somewhat straightforward applications of these pairs. The affirmative interpretation generator we use to improve sentiment analysis, however, has the potential to improve many natural language understanding tasks.\\n\\n1 Code and data available at https://github.com/mosharafhossain/large-afin-and-nlu.\"}"}
{"id": "emnlp-2022-main-393", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Number of parallel sentences in the English-Norwegian and English-Spanish parallel corpora we work with, and pairs of sentences with negation and affirmative interpretations we automatically generate via backtranslation. The yield (%pairs) is low, but as we shall see these pairs are useful to solve natural language understanding tasks when negation is present without hurting results when negation is not present.\\n\\n3. Collecting Sentences with Negation and Their Affirmative Interpretations\\n\\nThis section outlines our approach to create a large collection of sentences containing negation and their affirmative interpretations. First, we present the sources of parallel corpora we work with. Second, we describe our multilingual negation cue detector to identify negation cues in the parallel sentences. Third, we describe the backtranslation step and a few checks to improve quality. Lastly, we present an analysis of the resulting sentences with negation and their affirmative interpretations.\\n\\n3.1 Selecting Parallel Corpora\\n\\nWe select parallel sentences in English and either Norwegian or Spanish for two reasons: (a) large parallel corpora are available in these language pairs and (b) negation cue annotations are available in monolingual corpora for the three languages. The latter is a requirement to build a multilingual cue detector (Section 3.2). We extract the parallel sentences from three parallel corpora available in the OPUS portal (Tiedemann, 2012): WikiMatrix (Schwenk et al., 2021a), CCMatrix (Schwenk et al., 2021b; Fan et al., 2021), and UNPC (Ziemski et al., 2016). Table 1 (Column 3) shows the number of parallel sentences we collect from each of the corpora and language pair (total: 14.6 million).\\n\\n3.2 Identifying Negation Cues in Multiple Languages\\n\\nIn order to detect negation in the parallel sentences, we develop a multilingual negation cue detector that works with English, Norwegian, and Spanish texts. To this end, we fine-tune a multilingual BERT (mBERT) (Devlin et al., 2019) with negation cue annotations in the three languages we work with: English (Morante and Daelemans, 2012b), Norwegian (M\u00e6hlum et al., 2021), and Spanish (Jim\u00e9nez-Zafra et al., 2018). We fine-tune jointly for all three languages by combining the original training splits into a multilingual training split. We terminate the training process after the F1 score in the (combined) development split does not increase for 5 epochs; the final model is the one which yields the highest F1 score during the training process. Additional details regarding training procedure and hyperparameters are provided in Appendix A. Our multilingual detector is not perfect but obtains competitive results (F1 scores): English: 91.96 (test split), Norwegian: 93.40 (test split), and Spanish: 84.41 (dev split, as gold annotations for the test split are not publicly available). The system detects various negation cue types including single tokens (no, never, etc.), affixal, and lexicalized negations (Section 3.4).\\n\\nWe use our multilingual cue detector to detect negation in the 14.6 million of parallel sentences. In the English-Norwegian parallel sentences (8.5M), negation is present in both sentences (WikiMatrix: 7.3%, CCMatrix: 14.2%), either sentence (WikiMatrix: 5.2%, CCMatrix: 5.2%), or neither sentence (WikiMatrix: 87.5%, CCMatrix: 80.6%). Similarly, in English-Spanish parallel sentences, negation is present in both sentences (UNPC: 10.7%, WikiMatrix: 5.7%), either sentence (UNPC: 4.6%, WikiMatrix: 4.4%), or neither sentence (UNPC: 84.7%, WikiMatrix: 89.9%). Since we are interested in sentences containing negation and their affirmative interpretations, we only keep the sentences in which either the source or target sentence contains negation.\\n\\n3.3 Generating Affirmative Interpretations\\n\\nAfter identifying negation cues in the parallel sentences, we backtranslate into English the sentence in the target language (either Norwegian or Spanish; they may or may not contain a negation). In\"}"}
{"id": "emnlp-2022-main-393", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Negation Type Examples\\n\\nSingle tokens (49.6%) They are still not integrated into the German community.\\nCues: not, n\u2019t, no, never, without, nothing, nowhere, nobody, none, etc.\\nThey have yet to integrate into German society.\\nI have no doubt that we will reach our goal.\\nWe shall surely get there!\\nThis process allows for higher precision that could never be achieved by hand.\\nThis process allows more precision than anyone performed manually.\\n\\nAffixal (30.15%) The north wing was left largely untouched and forms the present house.\\nCues: un-, in-, -less, etc.\\nOnly the North wing remained quite intact, and constitutes the current house.\\n\\nLexicalized (8.76%) A further problem was the lack of skilled labour.\\nCues: prevent, lack, etc.\\nAnother problem was the issue of obtaining sufficiently qualified personnel.\\n\\nMultitoken (2.58%) After some time, the drainage of water no longer occurs.\\nCues: no longer, not at all, etc.\\nAfter a certain time, the drainage of water ends.\\n\\nMultiple negations (8.95%) The declaration before the courts is not valid if the child is not 14 years old.\\nAny statement in a court is invalid if the child is below 14 years of age.\"}"}
{"id": "emnlp-2022-main-393", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"several types of negation cues. As a result, our collection of pairs of sentences with negation and their affirmative interpretations includes several negation types (Table 2). Note that this table presents real examples from our collection including erroneous ones (e.g., some affirmative interpretations contain negation). The most frequent negation type (49.6%) are common single-token negation cues such as *not*, *n\u2019t* and *never*. Affixal negations are surprisingly common (30.15%) and include both prefixes (e.g., *un*-*touched*) and suffixes (*useless*).\\n\\nLexicalized negations usually take the form of a noun (e.g., *lack*, *dismissal*) or verb (e.g., *prevent*, *avoid*) and account for almost 9%. Finally, a few negations (2.58%) are multitoken (e.g., *no longer*, *not at all*), and several negations (almost 9%) appear in sentences with at least one more negation. The corresponding affirmative interpretation is never just the original sentence with negation after removing the negation cue\u2014doing so results in a sentence that is not semantically equivalent. The required modification are sometimes relatively simple and mainly require swapping a verb or adjective. For example,*are still not integrated* becomes *have yet to integrate*, *largely untouched* becomes *quite intact*, and *is useless* becomes *is futile*. Yet the affirmative interpretation often is a more thorough rewrite of the original sentence:\\n\\n- *I have no doubt that we will reach our goal.* becomes *We shall surely get there!*\\n- *higher precision that could never be achieved by hand* becomes *more precision than anyone performed manually;* and\\n- *lack of skilled labour* becomes *issue of obtaining sufficiently qualified personnel.*\\n\\n4 Experiments with Natural Language Understanding Tasks\\n\\nWe leverage our collection of sentences with negation and their affirmative interpretations to enhance models for three natural language understanding tasks. First, we leverage them in a blending training setup to generate affirmative interpretations from the negations in a previous benchmark (Section 4.1). Second, we leverage them to create new premise-hypothesis pairs and build more robust models for natural language inference (Section 4.2). Third, we use them to train a plug-and-play neural component to generate affirmative interpretations from negation. Then, we incorporate the generator into the task of sentiment analysis (Section 4.3).\\n\\nWe use existing corpora for all tasks as described below; for natural language inference and sentiment analysis we use the versions released by the GLUE benchmark (Wang et al., 2018).\\n\\nOur experimental results show that leveraging the large collection of negations and their affirmative interpretations improves results across all tasks using previously proposed benchmarks. Specifically, we obtain either slightly better or comparable results when negation is not present in the input, and always better results when negation is present.\\n\\n4.1 Generating Affirmative Interpretations from Negation\\n\\nThere are a couple corpora with sentences containing negation and their manually curated affirmative interpretations (Section 2). In our first experiment, we explore whether leveraging our collection is beneficial to generate affirmative interpretations from the negations in AFIN (Hossain et al., 2022b), a manually curated corpus that is publicly available. AFIN contains 3,001 sentences with negation and their affirmative interpretations. Unlike our collection (Section 3), AFIN only considers verbal negations (i.e., the negation cues always modify a verb). Here are some examples:\\n\\n- *It was not formed by a natural process.* becomes *It was formed by an artificial process.*\\n- *An extinct volcano is one that has not erupted in recent history.* becomes *An extinct volcano erupted in the past.*\\n\\nThe AFIN authors experiment with the T5 (Raf fel et al., 2020) transformer to automatically generate affirmative interpretations. While the task remains a challenge and our results are much worse than the human upper bound, we show that incorporating our collection of sentences containing negation and their affirmative interpretations during the training process results in a more robust generator.\\n\\nBlending Our Collection of Negations and Affirmative Interpretations\\n\\nWe adopt a blending technique by Shnarch et al. (2018) in order to maximize the chances that the training process benefits from our collection of negations and affirmative interpretations. Since our collection is much larger than the training split in AFIN (153k vs. 2.1k), simply adding our collection to the training split and fine-tuning T5 as usual would result in a model that underperforms with AFIN. There are three phases in the training process. In the first phase, we fine-tune T5 with the combination of our collection and...\"}"}
{"id": "emnlp-2022-main-393", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Automatic evaluation of the T5 transformer on the task of generating affirmative interpretations as defined in the AFIN benchmark (Hossain et al., 2022b). Blending our pairs in the process of fine-tuning T5 yields improvements with all metrics.\\n\\n| Validation Scores | Upper Bound | T5 transformer | + blending | Ours |\\n|-------------------|-------------|----------------|-------------|------|\\n| 4                 | 86.2        | 32.0           | 37.0        | 37.0 |\\n| 3                 | 11.6        | 15.3           | 14.0        | 13.0 |\\n| 2                 | 2.0         | 12.0           | 13.0        | 11.0 |\\n| 1                 | 0.2         | 3.3            | 10.0        | 9.0  |\\n| 0                 | n/a         | 37.3           | 26.0        | 25.0 |\\n\\nTable 4: Manual evaluation of the T5 transformer on the task of generating affirmative interpretations as defined in the AFIN benchmark. The upper bound comes from the manual validation by the creators of AFIN. Blending our pairs in the fine-tuning process generates more correct interpretations (higher validation scores are better).\\n\\nResults and Discussion\\n\\nTable 3 presents the evaluation with the test split in AFIN using automatic metrics: BLEU-2 (Papineni et al., 2002), chrf++ (Popovi\u0107, 2017), and METEOR (Banerjee and Lavie, 2005). We obtained these scores comparing the gold affirmative interpretations in AFIN and the predicted ones by T5. Despite our collection of negations and affirmative interpretations is noisy, out-of-domain, and considers more negation types, leveraging it is beneficial. Indeed, we observe improvements across the three metrics (BLEU-2: 28.6 vs. 26.5, chrf++: 52.5 vs. 50.5, and METEOR: 45.8 vs. 43.5).\\n\\nManual Validation\\n\\nAutomatic metrics for generation tasks are useful but have well-known limitations (Mathur et al., 2020; Zhang et al., 2020). Following the AFIN authors, we also conduct a manual evaluation. Specifically, we validate a sample of 100 automatically generated affirmative interpretations. The validation consists in assigning a score indicating how confident they are in the correctness of an affirmative interpretation given the sentence containing negation (4: extremely confident, 3: very confident, 2: moderately confident, 1: slightly confident). We also include 0 to indicate that the affirmative interpretations is wrong. We show examples of each score in Appendix B.\\n\\nTable 4 shows the manual evaluation. Blending our collection of negations and affirmative interpretations yields better results. While still far from the upper bound, blending increases the confidence scores. Most notably, the percentage of incorrect affirmative interpretations decreases from 37.3% to 26.0% (\u0394 = 11.3%).\\n\\n4.2 Natural Language Inference\\n\\nOur collection of pairs of sentences with negation and their affirmative interpretations can be seen as semantically equivalent sentences in which only one statement contains negation. By definition, there are two entailment relationships between semantically equivalent sentences\u2014using either sentence as premise and the other one as hypothesis. We thus create two premise-hypothesis sets from each pair in our collection and label them as entailment to create a large collection of entailments involving negation. For example, we generate the following entailments from the pair (The universal nature of these rights and freedoms does not admit doubts, The universal nature of these rights and freedoms is beyond question):\\n\\n- Premise: The universal nature of these rights and freedoms does not admit doubts.\\n  Hypothesis: The universal nature of these rights and freedoms is beyond question.\\n\\n- Premise: The universal nature of these rights and freedoms is beyond question.\\n  Hypothesis: The universal nature of these rights and freedoms does not admit doubts.\\n\\nThis process results in 306,546 new premise-hypothesis annotated entailment (2 per pair in our collection) without any manual effort.\\n\\nWe experiment with (a) three transformer-based classifiers without any fine-tuning designed to improve results when there is a negation in the premise or hypothesis, (b) BERTNOT (Hosseini et al., 2021), a BERT transformer pretrained with a modified loss calculated in part with automatically...\"}"}
{"id": "emnlp-2022-main-393", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Results (accuracy) using several transformers and (a) the development splits in RTE, SNLI, and MNLI, and (b) the new premise-hypothesis containing negation (neg. P-H) from Hossain et al. (2020b). RoBERTa blending new premise-hypothesis derived from our sentences with negation and their affirmative interpretations substantially outperforms the three transformers without any negation fine-tuning and BERTNOT with the new premise-hypothesis that contain negation while obtaining comparable results with the original development splits.\\n\\nObtained negated statements, and (c) a RoBERTa-based classifier blending our 306k new entailment premise-hypothesis using the strategy presented in Section 4.1. We refer the reader to Appendix C for additional details about the models, training process, and hyperparameters. Regarding corpora, we work with RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), SNLI (Bowman et al., 2015), and MNLI (Williams et al., 2018). Additionally, we work with the 4,500 new premise-hypothesis pairs by Hossain et al. (2020b), who derive them from RTE, SNLI and MNLI by adding a negation to a premise, hypothesis or both.\\n\\nResults and Discussion\\n\\nTable 5 presents the results. We train all models with the corresponding training split, except RoBERTa blending Ours, which also blends our 306k new premise-hypothesis pairs during the training process. We present results with the corresponding development split (gold labels for the test split are not available for all them) and the premise-hypothesis including negation. We find that blending our 306k premise-hypothesis is beneficial despite these pairs (a) only include entailments and (b) inherit the errors present in our collection of sentences with negation and their affirmative interpretations. With the original development splits in RTE, SNLI, and MNLI, we either obtain slightly better results (RTE, +1.82) or comparable (SNLI: -0.25, MNLI: -0.90). The improvements are consistent, however, with the pairs that include negation (neg. P-H). RTE and SNLI benefit the most (78.13 vs. 62.50, 54.87 vs. 51.90). We hypothesize that MNLI benefits the least (67.89 vs. 66.70) because premises in MNLI are often multiple sentences and our new premise-hypothesis are always single sentences.\\n\\n4.3 Sentiment Analysis\\n\\nWe close our experiments exploring the task of sentiment analysis (i.e., classifying sentences according to their sentiment: positive or negative). Our motivation is that state-of-the-art systems for sentiment analysis face challenges with negation (Ribeiro et al., 2020). Unlike generating affirmative interpretations (Section 4.1) and natural language inference (Section 4.2), however, it is unclear how to leverage our large collection of sentences with negation and their affirmative interpretations to alleviate the issue.\"}"}
{"id": "emnlp-2022-main-393", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We propose a task-agnostic solution: complement input sentences containing negation with their automatically generated affirmative interpretations. Figure 2 illustrates this proposal in the architecture at the bottom. Rather than feeding all sentences to a transformer-based classifier (top), we first check whether input sentences contain a negation with our cue detector (Section 3). If they do not, we feed them to the classifier as usual. If they do, we (a) automatically generate its affirmative interpretation with the generator described in Section 4.1 and (b) feed to the transformer-based classifier both the original sentence with negation and the affirmative interpretation separated by the [SEP] token.\\n\\nAppendix D provides additional details about the training process. We experiment here with RoBERTa as it produces very competitive results. Note that our strategy to complement negated inputs with their affirmative interpretations could be used with any classifier for any task as long as it takes a text as its input. Regarding corpora, we use SST-2 (Socher et al., 2013) as released by GLUE (Wang et al., 2018). It consists of 70,042 movie reviews and sentiment annotations for each sentence.\\n\\nHere are a few examples of automatically generated affirmative interpretations from sentences containing negation in SST-2:\\n\\n- It is not a bad film.\\n  Affirmative interpretation: It is a good movie.\\n- She may not be real, but the laughs are.\\n  Affirmative interpretation: She is fictional.\\n- He feels like a spectator and not a participant.\\n  Affirmative interpretation: He feels like a spectator rather than participant.\\n- The movie has no idea of it is serious.\\n  Affirmative interpretation: The movie has a lack of idea that it is serious.\\n- A thriller without a lot of thrills.\\n  Affective interpretation: A thriller with little thrills.\\n\\nNote that they are by no means perfect, but they mostly preserve meaning while not using negation. For example, the second affirmative interpretation only covers the meaning of part of the original sentence with negation (i.e., She may not be real), and the second to last includes a negation (lack).\\n\\n### Results and Discussion\\n\\nTable 6 presents the results (macro F1). We provide results with the sentences in the development split depending on whether they contain a negation (gold labels for the test split are not publicly available). Additionally, we use the grouping of negations by Hossain et al. (2022a): important or unimportant. A negation is unimportant if removing it does not change the sentiment (e.g., both I got a headache watching this meaning less downer and I got a headache watching this downer are negative).\\n\\nIncorporating our affirmative interpretation generator is always beneficial. This includes instances containing negation (94.8 vs. 93.0) and, surprisingly, instances that do not contain negation (94.7 vs. 94.0). We hypothesize that this is the case because our negation cue detector is not perfect thus instances are sometimes fed through the incorrect branch after the Negation? fork (Figure 2). As one would expect, the generator makes the classifier more robust with important negations (89.8 vs. 86), but we also observe improvements with unimportant negations (95.8 vs. 95.0).\\n\\n### 5 Conclusions\\n\\nNegation poses a challenge for natural language understanding. Understanding negation requires humans to infer affirmative meanings (Horn, 1989) (e.g., The lot has not been vacant conveys The lot has been occupied). Inspired by this insight, we collect a large collection (153k) of pairs of sentences containing negation and their affirmative interpretations. We define the latter as a semantically equivalent sentence that does not contain negation. Our collection process relies on parallel corpora and backtranslation and is automated.\\n\\nWe show that leveraging our collection is beneficial to solve three natural language understanding tasks: (a) generating affirmative interpretations, (b) natural language inference, and (c) sentiment.\"}"}
{"id": "emnlp-2022-main-393", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All our experiments use out-of-domain, manually curated corpora. Crucially, our proposal yields better results when negation is present in the input while slightly improving or obtaining comparable results when it is not. Additionally, our proposal does not require any manual annotations.\\n\\n**Limitations**\\n\\nIn order to create a large collection of sentences with negation and their affirmative interpretations, we use publicly available parallel sentences. We note that in two of the three sources (i.e., WikiMatrix (Schwenk et al., 2021a) and CCMatrix (Schwenk et al., 2021b; Fan et al., 2021)), the authors use auto-alignment methodologies to collect the parallel sentences. This step may introduce errors in the original sources. Next, to detect negation cues in the huge collections of parallel sentences (14.6 millions), we develop a multilingual cue detection system that is certainly not 100% perfect. While the cue detector performs well on the negation corpora it is trained with (Section 3.2), some incorrect predictions can be expected in the parallel corpora we use. Furthermore, the translation API introduces additional noise backtranslating from Norwegian or Spanish into English (Section 3.3). Regarding models and experiments, we leverage RoBERTa and T5 (Section 4) as systems based on them perform well on natural language understanding tasks.\\n\\n4 However, we acknowledge that other transformers such as XLNet (Yang et al., 2019) and DeBERTa (He et al., 2021) may yield better results.\\n\\n**Acknowledgements**\\n\\nThis material is based upon work supported by the National Science Foundation under Grant No. 1845757. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. Computational resources were provided by the UNT office of High-Performance Computing. Further, we leveraged computational resources from the Chameleon platform (Keahey et al., 2020). We also thank the reviewers for insightful comments.\\n\\n**References**\\n\\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372.\\n\\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the second PASCAL challenges workshop on recognising textual entailment, volume 6, pages 6\u20134. Venice.\\n\\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. 2009. The fifth pascal recognizing textual entailment challenge.\\n\\nEduardo Blanco and Dan Moldovan. 2011. Semantic representation of negation using focus detection. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 581\u2013589, Portland, Oregon, USA. Association for Computational Linguistics.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal. Association for Computational Linguistics.\\n\\nLong Chen. 2019. Attention-based deep learning system for negation and assertion detection in clinical notes. International Journal of Artificial Intelligence and Applications (IJAIA), 10(1).\\n\\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW'05, pages 177\u2013190, Berlin, Heidelberg. Springer-Verlag.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nAllyson Ettinger. 2020. What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. Transactions of the Association for Computational Linguistics, 8:34\u201348.\"}"}
{"id": "emnlp-2022-main-393", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-393", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kate Keahey, Jason Anderson, Zhuo Zhen, Pierre Riteau, Paul Ruth, Dan Stanzione, Mert Cevik, Jacob Colleran, Haryadi S. Gunawi, Cody Hammock, Joe Mambretti, Alexander Barnes, Fran\u00e7ois Halbach, Alex Rocha, and Joe Stubbs. 2020. Lessons learned from the chameleon testbed. In Proceedings of the 2020 USENIX Annual Technical Conference (USENIX ATC '20). USENIX Association.\\n\\nHao Li and Wei Lu. 2018. Learning with structured representations for negation scope extraction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 533\u2013539, Melbourne, Australia. Association for Computational Linguistics.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Manjunath Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nPetter M\u00e6hlum, Jeremy Barnes, Robin Kurtz, Lilja \u00d8vrelid, and Erik Velldal. 2021. Negation in Norwegian: an annotated dataset. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 299\u2013308, Reykjavik, Iceland (Online). Link\u00f6ping University Electronic Press, Sweden.\\n\\nNitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4984\u20134997, Online. Association for Computational Linguistics.\\n\\nRoser Morante and Walter Daelemans. 2012a. ConanDoyle-neg: Annotation of negation cues and their scope in Conan Doyle stories. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012). European Language Resources Association (ELRA).\\n\\nRoser Morante and Walter Daelemans. 2012b. ConanDoyle-neg: Annotation of negation cues and their scope in Conan Doyle stories. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 1563\u20131568, Istanbul, Turkey. European Language Resources Association (ELRA).\\n\\nAakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2340\u20132353, Santa Fe, New Mexico, USA. Association for Computational Linguistics.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nMaja Popovi\u0107. 2017. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, pages 612\u2013618.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u20134912, Online. Association for Computational Linguistics.\\n\\nZahra Sarabi, Erin Killian, Eduardo Blanco, and Alexis Palmer. 2019. A corpus of negations and their underlying positive interpretations. In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), pages 158\u2013167, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzm\u00e1n. 2021a. WikiMatrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1351\u20131361, Online. Association for Computational Linguistics.\\n\\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021b. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6490\u20136500, Online. Association for Computational Linguistics.\\n\\nLongxiang Shen, Bowei Zou, Yu Hong, Guodong Zhou, Qiaoming Zhu, and AiTi Aw. 2019. Negative focus detection via contextual attention mechanism. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2251\u20132261, Hong Kong, China. Association for Computational Linguistics.\\n\\nEyal Shnarch, Carlos Alzate, Lena Dankin, Martin Gleize, Yufang Hou, Leshem Choshen, Ranit Aharonov, and Noam Slonim. 2018. Will it blend? blending weak and strong labeled data in a neural network for argumentation mining. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 599\u2013605, Melbourne, Australia. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-393", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Additional Details on Identifying Negation Cues in Multiple Languages\\n\\nReferring to Section 3.2 of the paper, we employ an off-the-shelf multilingual BERT-Base model (cased version) pretrained on 104 languages. We concatenate the contextualized representations from the last and third-to-last layers. Then, we pass the concatenation to a fully connected layer. Finally, we leverage a conditional random field (CRF) layer that yields the output sequence identifying negation cues. Since a negation cue can consist of multiple tokens (e.g., *by no means*), we use the BIO (B: Beginning, I: Inside, and O: Outside) tagging scheme.\"}"}
{"id": "emnlp-2022-main-393", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Hyperparameters for finetuning the multilingual cue detector (Section 3.2 in the paper). FC refers to Fully Connected layer.\\n\\nHyperparameter\\nMaximum Epochs 25\\nBatch Size 10\\nPatience 5\\nMaximum sentence length 150\\nOptimizer AdamW\\nLearning rate (mBERT) 1e-5\\nWeight decay (mBERT) 1e-5\\nDropout (mBERT) 0.5\\nGradient clipping 5.0\\nWarmup epochs 5\\nAccumulate step 1\\n\\nTable 8: Hyperparameters for finetuning our affirmative interpretation generator (Section 4.1 in the paper).\\n\\nHyperparameter\\nMaximum Epochs 20\\nBatch Size 8\\nPatience 5\\nInput sentence length (max.) 80\\nTarget sentence length (max.) 50\\nOptimizer Adafactor\\nLearning rate 1e-5\\nWeight decay 1e-6\\nGradient clipping 5.0\\nWarmup epochs 3\\nAccumulate step 1\\ntop_k 50\\ntop_p 0.95\\nrepetition_penalty 2.5\\n\\nB Additional Details on Generating Affirmative Interpretations from Negation\\n\\nWe utilize the Huggingface implementation (Wolf et al., 2020) of T5, a conditional generation model (Section 4.1). In each run, the system requires approximately 7.2 hours to train on a single core NVIDIA Tesla V100 (32GB). Table 8 shows the hyperparameters for this experiment. Regarding Section 4.1 in the paper (Manual Validation), we present examples of each score in Table 9.\\n\\nC Additional Details on Solving Natural Language Inference\\n\\nWe evaluate the systems on the development splits of the NLI benchmarks we work with (Section 4.2) as test split labels are not publicly available. So, we randomly select 15% examples of the original training split in order to tune the hyperparameters and to select the best model during the training process for each benchmark. We note that we evaluate on the development split with matched genres for MNLI. In each run, the system (blending with ours) requires approximately 2.1 hours to train for RTE, 7.8 hours for SNLI, and 9.6 hours for MNLI on a single core NVIDIA Tesla V100 (32GB). Table 10 presents the hyperparameters we use in this experiment.\\n\\nD Additional Details on Solving Sentiment Analysis\\n\\nTo experiment with SST-2, we randomly select 5% examples of the original training split for tuning the hyperparameters as well as for selecting the best model during the training process since test labels are not publicly available in SST-2 (part of GLUE (Wang et al., 2018)). On average, the system takes half an hour to train on a single core NVIDIA Tesla V100 (32GB). We share the tuned hyperparameters in Table 11.\"}"}
{"id": "emnlp-2022-main-393", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Examples\\nExtremely confident (Score: 4)\\nMove them to low places so that they do not fall.\\nAffirmative Interpretation: They fall when they are in higher places.\\nThe ones that were not rewarded were not marked with fields.\\nAffirmative Interpretation: The ones that were rewarded were marked with fields.\\nVery confident (Score: 3)\\nThe most recent successful bids for the Olympic and Paralympic Games were in cities that had never hosted them before.\\nAffirmative Interpretation: Other cities had hosted them once.\\nNo other studies could find a link between the vaccine and autism.\\nAffirmative Interpretation: A study found a link between the vaccine and autism.\\nModerately confident (Score: 2)\\nIn 1984, because the games were in New York, and because of the boycott, from when we boycotted in 1980, not a lot of European countries came over.\\nAffirmative Interpretation: Lots of European countries came over in 1980.\\nIt occurs when the body does not receive enough iron.\\nAffirmative Interpretation: The body receives too little iron.\\nSlightly confident (Score: 1)\\nI understand third party candidates have no success.\\nAffirmative Interpretation: Third party candidates have minimal success.\\nI don't expect that the lack of British participation will stop any action.\\nAffirmative Interpretation: I expect that the lack of British participation will slow down any action.\\nWrong affirmative interpretation (Score: 0)\\nI have throughout my career not supported needle exchanges as anti-drug policies.\\nI have supported needle exchanges as anti-drug policies.\\nUnlike other organelles, the ribosome is not surrounded by a membrane.\\nThe ribosome is surrounded by a membrane.\\n\\nTable 9: Examples of sentences containing negation from AFIN and their affirmative interpretations automatically generated. The generator uses T5 trained with our large collection of sentences with negation and their affirmative interpretations (Section 4.1). We show examples of the manual validation; scores range from 0 to 4.\\n\\n| Hyperparameter | RTE | SNLI | MNLI |\\n|----------------|-----|------|------|\\n| Maximum epochs  | 10  | 8    | 8    |\\n| Warmup epochs   | 4   | 2    | 3    |\\n| Batch size      | 24  | 16   | 10   |\\n| Patience        | 5   | 5    | 5    |\\n| Maximum sentence length | 100 | 80   | 100  |\\n| Optimizer       | AdamW | AdamW | AdamW |\\n| Learning rate   | 1e-5 | 1e-5 | 1e-5 |\\n| Weight decay    | 0.0  | 5e-6 | 5e-6 |\\n| Gradient clipping | 5.0  | 5.0  | 5.0  |\\n| Dropout         | 0.2  | 0.3  | 0.3  |\\n\\nTable 10: Hyperparameters for finetuning RoBERTa with blending our pairs and the NLI benchmarks (Section 4.2 in the paper).\"}"}
{"id": "emnlp-2022-main-393", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Hyperparameter               | Value       |\\n|-----------------------------|-------------|\\n| Maximum Epochs              | 5           |\\n| Batch Size                  | 16          |\\n| Patience                    | 3           |\\n| Maximum sentence length     | 80          |\\n| Optimizer                   | AdamW       |\\n| Learning rate               | 1e-5        |\\n| Weight decay                | 0.0         |\\n| Dropout                     | 0.5         |\\n| Gradient clipping           | 5.0         |\\n| Warmup epochs               | 5           |\\n| Accumulate step had to be 1 | 1           |\\n\\nTable 11: Hyperparameters for finetuning our SST-2 system presented in Section 4.3 in the paper.\"}"}
