{"id": "emnlp-2022-main-649", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EMETR: Diagnosing Evaluation Metrics for Translation\\n\\nMarzena Karpinska \u2662 Nishant Raj \u2662 Katherine Thai \u2662 Yixiao Song \u2660 Ankita Gupta \u2662 Mohit Iyyer\\n\\nAbstract\\n\\nWhile machine translation evaluation metrics based on string overlap (e.g., BLEU) have their limitations, their computations are transparent: the BLEU score assigned to a particular candidate translation can be traced back to the presence or absence of certain words. The operations of newer learned metrics (e.g., BLEURT, COMET), which leverage pretrained language models to achieve higher correlations with human quality judgments than BLEU, are opaque in comparison. In this paper, we shed light on the behavior of these learned metrics by creating DEMETR, a diagnostic dataset with 31K English examples (translated from 10 source languages) for evaluating the sensitivity of MT evaluation metrics to 35 different linguistic perturbations spanning semantic, syntactic, and morphological error categories. All perturbations were carefully designed to form minimal pairs with the actual translation (i.e., differ in only one aspect). We find that learned metrics perform substantially better than string-based metrics on DEMETR. Additionally, learned metrics differ in their sensitivity to various phenomena (e.g., BERTS_Core is sensitive to untranslated words but relatively insensitive to gender manipulation, while COMET is much more sensitive to word repetition than to aspectual changes). We publicly release DEMETR to spur more informed future development of machine translation evaluation metrics.\\n\\n1 Introduction\\n\\nAutomatically evaluating the output quality of machine translation (MT) systems remains a difficult challenge. The BLEU metric (Papineni et al., 2002), which is a function of n-gram overlap between system and reference outputs, is still used widely today despite its obvious limitations in measuring semantic similarity (Fomicheva and Specia, 2019; Marie et al., 2021; Kocmi et al., 2021; Freitag et al., 2021). Recently-developed learned evaluation metrics such as BLEURT (Sellam et al., 2020a), COMET (Rei et al., 2020), MOVERS_C ore (Zhao et al., 2019), or BARTS_C ore (Yuan et al., 2021a) seek to address these limitations by either fine-tuning pretrained language models directly on human judgments of translation quality or by simply utilizing contextualized word embeddings. While learned metrics exhibit higher correlation with human judgments than BLEU (Barrault et al., 2021), their relative lack of interpretability leaves it unclear as to why they assign a particular score to a given translation. This is a major reason why some MT researchers are reluctant to employ learned metrics in order to evaluate their MT systems (Marie et al., 2021; Gehrmann et al., 2022; Leiter et al., 2022).\\n\\nIn this paper, we build on previous metric explainability work (Specia et al., 2010; Macketanz et al., 2011; Lee et al., 2014; Lee et al., 2015; Lee et al., 2019; Leiter et al., 2022).\"}"}
{"id": "emnlp-2022-main-649", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al., 2018; Fomicheva and Specia, 2019; Kaster et al., 2021; Sai et al., 2021a; Barrault et al., 2021; Fomicheva et al., 2021; Leiter et al., 2022) by introducing D\\\\textsc{EMETR}, a dataset for D\\\\textsc{iagnosing} E\\\\textsc{valuation} METR\\\\textsc{ics} for machine translation, that measures the sensitivity of an MT metric to 35 different types of linguistic perturbations spanning common syntactic (e.g., incorrect word order), semantic (e.g., undertranslation), and morphological (e.g., incorrect suffix) translation error categories. Each example in D\\\\textsc{EMETR} is a tuple containing \\\\{source, reference, machine translation, perturbed machine translation\\\\}, as shown in Figure 1. The entire dataset contains 31K total examples across 10 different source languages (the target language is always English). The perturbations in D\\\\textsc{EMETR} are produced semi-automatically by manipulating translations produced by commercial MT systems such as Google Translate, and they are manually validated to ensure the only source of variation is associated with the desired perturbation.\\n\\nWe measure the accuracy of a suite of 14 evaluation metrics on D\\\\textsc{EMETR} (as shown in Figure 1), discovering that learned metrics perform far better than string-based ones. We also analyze the relative sensitivity of metrics to different grades of perturbation severity. We find that metrics struggle at times to differentiate between minor errors (e.g., punctuation removal or word repetition) with semantics-warping errors such as incorrect gender or numeracy. We also observe that the reference-free C\\\\textsc{OMET}-QE learned metric is more sensitive to word repetition and misspelled words than severe errors such as entirely unrelated translations or named entity replacement. We publicly release D\\\\textsc{EMETR} and associated code to facilitate more principled research into MT evaluation.\\n\\n2 Diagnosing MT evaluation metrics\\n\\nMost existing MT evaluation metrics compute a score for a candidate translation $t$ against a reference sentence $r$. These scores can be either a simple function of character or token overlap between $t$ and $r$ (e.g., B\\\\textsc{LEU}), or they can be the result of a complex neural network model that embeds $t$ and $r$ (e.g., B\\\\textsc{LEURT}). While the latter class of learned metrics provides more meaningful judgments of translation quality than the former, they are also relatively uninterpretable: the reason for a particular translation $t$ receiving a high or low score is difficult to discern. In this section, we first explain our perturbation-based methodology to better understand MT metrics before describing the collection of D\\\\textsc{EMETR}, a dataset of linguistic perturbations.\\n\\n2.1 Using translation perturbations to diagnose MT metrics\\n\\nInspired by prior work in minimal pair-based linguistic evaluation of pretrained language models such as B\\\\textsc{LIMP} (Warstadt et al., 2020), we investigate how sensitive MT evaluation metrics are to various perturbations of the candidate translation $t$. Consider the following example, which is designed to evaluate the impact of word order in the candidate translation:\\n\\n\\\\begin{tabular}{l}\\nreference translation $r$: Pronunciation is relatively easy in Italian since most words are pronounced exactly how they are written. \\\\\\\\\\nmachine translation $t$: Pronunciation is relatively easy in Italian, as most words are pronounced exactly as they are spelled. \\\\\\\\\\nperturbed machine translation $t'$: Spelled pronunciation as Italian, relatively are most is as they pronounced exactly in words easy.\\n\\\\end{tabular}\\n\\nIf a particular evaluation metric $\\\\text{SCORE}$ is sensitive to this shuffling perturbation, $\\\\text{SCORE}(r, t')$, the score of the perturbed translation, should be lower than $\\\\text{SCORE}(r, t)$. Note that while other minor translation errors may be present in $t$, the perturbed translation $t'$ differs only in a specific, controlled perturbation (in this case, shuffling).\\n\\n2.2 Creating the D\\\\textsc{EMETR} dataset\\n\\nTo explore the above methodology at scale, we create D\\\\textsc{EMETR}, a dataset that evaluates MT metrics on 35 different linguistic phenomena with 1K perturbations per phenomenon. Each example in D\\\\textsc{EMETR} consists of (1) a sentence in one of 10 languages, (2) a reference translation $r$, and (3) a machine translation $t$. The perturbations are produced semi-automatically by manipulating translations produced by commercial MT systems such as Google Translate, and they are manually validated to ensure the only source of variation is associated with the desired perturbation. The entire dataset contains 31K total examples across 10 different source languages (the target language is always English). The perturbations in D\\\\textsc{EMETR} are produced semi-automatically by manipulating translations produced by commercial MT systems such as Google Translate, and they are manually validated to ensure the only source of variation is associated with the desired perturbation.\\n\\nWhile prior work uses also terms such as \\\"reference-less\\\" and \\\"quality estimation,\\\" we employ the term \\\"reference-free\\\" as it is more self-explanatory. Some metrics, such as C\\\\textsc{OMET}, additionally condition the score on the source sentence. \\\"Reference-less\\\" metrics, such as C\\\\textsc{OMET}-QE, compare the candidate translation $t$ directly against the source text $s$.\\n\\nWe define learned metrics as any metric which uses a machine learning model (including both pretrained and supervised methods). For reference-free metrics like C\\\\textsc{OMET}-QE, we include the source sentence $s$ as an input to the scoring function instead of the reference.\\n\\nAs some perturbations require presence of specific items (e.g., to omit a named entity, one has to be present) not all perturbations include exactly 1K sentences.\"}"}
{"id": "emnlp-2022-main-649", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: List of perturbations included in DE METR with their corresponding error severity. Details can be found in Appendix A\\n\\nSource languages, (2) an English translation written by a human translator, (3) a machine translation produced by Google Translate, and (4) a perturbed version of the Google Translate output which introduces exactly one mistake (semantic, syntactic, or typographical).\\n\\nData sources and filtering: We utilize X-to-English translation pairs from two different datasets, WMT (Callison-Burch et al., 2009; Bojar et al., 2013, 2015, 2014; Akhbardeh et al., 2021; Barrault et al., 2020) and FLORES (Guzm\u00e1n et al., 2019), aiming at a wide coverage of topics from different sources. WMT has been widely used over the years as a popular MT shared task, while FLORES was recently curated to aid MT evaluation. We consider only the test split of each dataset to prevent possible leaks, as both current and future metrics are likely to be trained on these two datasets. We sample 100 sentences (50 from each of the two datasets) for each of the following languages: French (fr), Italian (it), Spanish (es), German (de), Czech (cs), Polish (pl), Russian (ru), Hindi (hi), Chinese (zh), and Japanese (ja).\\n\\nWe pay special attention to the language selection, as newer MT evaluation metrics, such as COMET-QE or P RISM-QE, employ only the source text and the candidate translation. We control for sentence length by including only sentences between 15 and 25 words long, measured by the length of the tokenized reference translation. Since we re-use the same sentences across multiple perturbations, we did not include shorter sentences because they are less likely to contain multiple linguistic phenomena of interest.\\n\\nAs the quality of sampled sentences varies, we manually check each source sentence and its translation to make sure they are of satisfactory quality.\\n\\nTranslating the data: Given the filtered collection of source sentences, we next translate them into English using the Google Translate API. We manually verify each translation, editing or resampling the instances where the machine translation contains critical errors. Through this process, we choose languages that represent different families (Romance, Germanic, Slavic, Indo-Iranian, Sino-Tibetan, and Japonic) with different morphological traits (fusional, agglutinative, and analytic) and wide range of writing systems (Latin alphabet, Cyrillic alphabet, Devanagari script, Hanzi, and Kanji/Hiragana/Katakana).\\n\\nSimilarly, we do not include sentences over 25 words long in DE METR as some languages may naturally allow longer sentences than others, and we wanted to control the length distribution. In the sentences sampled from WMT, we notice multiple translation and grammar errors, such as translating Japanese \u305d\u306e\u6700 \u5927 \u306f \u672c \u5dde \u5217 \u5cf6 \u3067 \u3001 \u4e16 \u754c \u3067 7 \u756a \u76ee \u306b \u5927 \u304d\u3044 \u5cf6 \u3068 \u3055 \u308c \u3066 \u3044 \u307e \u3059\uff08the biggest being Honshu), making Japan the 7th largest island in the world, which would suggest that Japan is an island, instead of the largest of which is the Honshu island, considered to be the seventh largest island in the world. or \u201ckakao\u201d (\u201ccacao\u201d) incorrectly declined as \u201ckakaa\u201d in Polish. These sentences were rejected, and new ones were sampled in their place. We also resampled sentences which translations contained artifacts from neighboring sentences due to partial splits and merges, and sentences which exhibit translationese, that is sentences with source artifacts (Koppel and Ordan, 2011). Finally, we omit or edit sentences with translation artifacts due to the direction of translation, as both WMT and FLORES contain sentences translated from English to another languages. Since the translation process is not always fully reversible, we omit sentences where translation from the given language to English would not be possible in the form included in these datasets (e.g., due to addition or omission of information).\\n\\nAll sentences were translated in May, 2022.\"}"}
{"id": "emnlp-2022-main-649", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we obtain 1K curated examples per perturbation (100 sentences \u00d7 10 languages) that each consist of source and reference sentences along with a machine translation of reasonable quality.\\n\\n2.3 Perturbations in \\\\textsc{Deme}t\\\\textsc{r}\\n\\nWe perturb the machine translations obtained above in order to create minimal pairs, which allow us to investigate the sensitivity of MT evaluation metrics to different types of errors. Our perturbations are loosely based on the Multidimensional Quality Metrics (Burchardt, 2013, MQM) framework developed to identify and categorize MT errors. Most perturbations were performed semi-automatically by utilizing \\\\textsc{Spa}nc\\\\textsc{Y}13 or \\\\textsc{Gpt}-3 (Brown et al., 2020), applying hand-crafted rules and then manually correcting any errors. Some of the more elaborate perturbations (e.g., translation by a too general term, where one had to be sure that a better, more precise term exists) were performed manually by the authors or linguistically-savvy freelancers hired on the Upwork platform.\\n\\nSpecial care was given to the plausibility of perturbations (e.g., numbers for replacement were selected from a probable range, such as 1-12 for months). See Table 2 for descriptions and examples of most perturbations; full list in Appendix A.\\n\\nWe roughly categorize our perturbations into the following four categories:\\n\\n- **Accuracy**: Perturbations in the accuracy category modify the semantics of the translation by either incorporating misleading information (e.g., by adding plausible yet inadequate text or changing a word to its antonym) or omitting information (e.g., by leaving a word untranslated).\\n\\n- **Fluency**: Perturbations in the fluency category focus on grammatical accuracy (e.g., word form agreement, tense, or aspect) and on overall cohesion. Compared to the mistakes in the accuracy category, the true meaning of the sentence can be usually recovered from the context more easily.\\n\\n- **Mixed**: Certain perturbations can be classified as both accuracy and fluency errors. Concretely, this category consists of omission errors that not only obscure the meaning but also affect the grammaticality of the sentence. One such error is subject removal, which will result not only in an ungrammatical sentence, leaving a gap where the subject should come, but also in information loss.\\n\\n- **Typography**: This category concerns punctuation and minor orthographic errors. Examples of mistakes in this category include punctuation removal, tokenization, capitalization, and common spelling mistakes.\\n\\n- **Baseline**: Finally, we include both upper and lower bounds, since learned metrics such as BLEURT and COMET do not have a specified range that their scores can fall into. Specifically, we provide three baselines: as lower bounds, we either change the translation to an unrelated one or provide an empty string, while as an upper bound, we set the perturbed translation $t'$ equal to the reference translation $r$, which should return the highest possible score for reference-based metrics.\\n\\nError severity:\\n\\nOur perturbations can also be categorized by their severity (see Table 1). We use the following categorization scheme for our analysis experiments:\\n\\n- **Minor**: In this type of error, which includes perturbations such as dropping punctuation or using the wrong article, the meaning of the source sentence can be easily and correctly interpreted by human readers.\\n\\n- **Major**: Errors in this category may not affect the overall fluency of the sentence but will result in some missing details. Examples of major errors include undertranslation (e.g., translating \\\"church\\\" as \\\"building\\\"), or leaving a word in the source language untranslated.\\n\\n- **Critical**: These are catastrophic errors that result in crucial pieces of information going missing or incorrect information being added in a way unrecognizable for the reader, and are also likely to suffer from severe fluency issues. Errors in this category include\\n\\n\\\\[15\\\\] Since most of the metrics will not accept an empty string, we pass a full stop instead.\"}"}
{"id": "emnlp-2022-main-649", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I don't know if you realize that most of the goods imported into this country from Central America are duty free. In 1940 he stood up to other government aristocrats. Cyanuric acid and melamine are both found in urine samples of pets who died after eating contaminated pet food. Her last word is being repeated four times. Punctuation is added after the last repeated word. She said that 85% of new coronavirus cases in Belgium last week were under the age of 60. The U.S. Supreme Court last year blocked the Trump administration's decision to include the citizenship question on the 2020 census form. Scientists want to understand how planets have formed since a comet collided with Earth long ago, and especially how Earth has formed. The Polish Air Force will eventually be equipped with 32 F-35 Lightning II fighters manufactured by Lockheed Martin. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving verifiable denuclearization of the Korean peninsula. The language most of the people working in the Vatican City use on a daily basis is Italian, and Latin is often used in religious ceremonies. Last month, a presidential committee recommended the resignation of the former CEO as part of measures to push the country toward new elections. Late night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede. Gordon Johndroe, Bush's spokesman, referred to the North Korean commitment as \\\"an important advance towards the goal of achieving ver"}
{"id": "emnlp-2022-main-649", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"subject deletion or replacement of a named entity.\\n\\n3 Performance of MT evaluation metrics on DEMETR\\n\\nWe test the accuracy and sensitivity of popular MT evaluation metrics on the perturbations in DEMETR. We include both traditional string-based metrics, such as BLEU or CHR, as well as newer learned metrics, such as BLEURT and COMET. Within the latter category, we also include two reference-free metrics, which rely only on the source sentence and translation and open possibilities for a more robust MT evaluation. The rest of this section provides an overview of the evaluation metrics before analyzing our findings. Detailed results of each metric on every perturbation are found in Table A3.\\n\\n3.1 Evaluation metrics\\n\\nString-based metrics can be used to evaluate any language, provided the availability of a reference translation (see Table 3). Their score is a function of string overlap or edit-distance, though it may not be always easily interpretable (M\u00fcller, 2020). Only BLEU allows for multiple references in order to account for many possible translations of a sentence; however, it is rarely used with more than one reference due to the lack of multi-reference datasets (Mathur et al., 2020). Learned metrics, on the other hand, are much less transparent. BERTS\\\\textsubscript{CORE} relies on contextualized embeddings, while PRISM employs zero-shot paraphrasing. COMET and BLEURT directly fine-tune pre-trained language models on human judgments provided as Direct Assessments or MQM annotations.\\n\\n3.2 Perturbation accuracy\\n\\nFirst, we measure the accuracy of each metric on DEMETR. For each perturbation, we define the accuracy as the percentage of the time that $\\\\text{SCORE}(r, t)$ is greater than $\\\\text{SCORE}(r, t')$. Since all perturbed sentences are based on the unperturbed sentence, we do not give metrics credit for giving an equal score to both perturbed and unperturbed sentences.\\n\\n| Metric | Base | Crit. | Maj. | Min. | All |\\n|-------|------|------|------|------|-----|\\n| String-based metrics | | | | | |\\n| BLEU | 100.00 | 80.29 | 83.43 | 72.49 | 78.70 |\\n| CER | 99.15 | 80.37 | 83.59 | 80.20 | 81.88 |\\n| CHR\\\\textsubscript{F} | 100.00 | 91.13 | 90.89 | 81.23 | 87.54 |\\n| CHR\\\\textsubscript{F2} | 100.00 | 91.27 | 92.21 | 83.68 | 88.80 |\\n| METEOR | 100.00 | 82.95 | 79.69 | 58.97 | 73.60 |\\n| ROUGE\\\\textsubscript{-2} | 99.90 | 76.91 | 80.99 | 47.10 | 66.58 |\\n| TER | 99.20 | 72.57 | 77.93 | 59.13 | 69.39 |\\n| Learned metrics | | | | | |\\n| BARTS\\\\textsubscript{CORE} | 100.00 | 95.11 | 89.68 | 79.48 | 88.16 |\\n| BERTS\\\\textsubscript{CORE} | 100.00 | 98.11 | 96.22 | 98.50 | 98.11 |\\n| BLEURT\\\\textsubscript{-20} | 100.00 | 98.78 | 95.63 | 97.98 | 98.06 |\\n| COMET | 100.00 | 96.24 | 92.96 | 93.46 | 94.83 |\\n| PRISM | 100.00 | 98.74 | 97.51 | 99.44 | 98.92 |\\n| COMET\\\\textsubscript{-QE} | 77.80 | 84.49 | 76.73 | 89.85 | 85.16 |\\n| PRISM\\\\textsubscript{-QE} | 97.40 | 96.70 | 95.68 | 99.21 | 97.63 |\\n\\nTable 3: Details of metrics tested on DEMETR. We report the parameter count for the largest available checkpoint of each learned metric. For learned metrics, we report the maximum number of languages that each can accept as input. While most of the learned metrics leverage pretrained multilingual language models (e.g., mBERT), it is important to note that they have not been validated against human judgments of MT quality on all of these languages (e.g., BLEURT-20 is only validated on 13 languages).\\n\\nTable 4: Accuracy on DEMETR perturbations for both string-based and learned metrics, shown bucketed by error severity (baseline, critical, major, and minor errors) as well as averaged across all perturbations. Baseline accuracies were computed excluding the reference as translation identity perturbation. Detailed accuracies for all perturbations along with the significance testing are shown in Table A3 in the Appendix A.\\n\\nWe do not give metrics credit for giving an equal score to both perturbed and unperturbed sentences.\"}"}
{"id": "emnlp-2022-main-649", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sentences are less correct versions of the original machine translation, we expect all metrics to perform well on this task. Table 4 contains the accuracies averaged across both error severity as well as overall. Interesting results include:\\n\\nLearned metrics achieve higher accuracy than string-based ones: All but two learned metrics (BARTS CORE and COMET-QE) achieve around or over 95% accuracy, which is to be expected, as each perturbation clearly affects the quality of the translation, though to varying degrees. PRISM is the most accurate metric on DEMETR, reaching an accuracy of 98.92%. Performance of string-based metrics, on the other hand, is alarmingly bad. BLEU, often the only metric employed to evaluate the MT output (Marie et al., 2021), achieves an overall accuracy of only 78.70%. To illustrate their struggles, the accuracy of string-based metrics ranges from 54% to 84% on the adjective/adverb removal perturbation, where a single adjective or adverb is omitted.\\n\\nThe best performing string-based metric is CHRF2, which corroborates results reported in Kocmi et al. (2021). PRISM-QE achieves better accuracy than COMET-QE for reference-free metrics: Of the two reference-free metrics we evaluate, we notice that COMET-QE struggles with some perturbations. Most notably, its accuracy when given a random translation (i.e., a translation that does not match the source sentence) oscillates around 50% (chance level) across all languages. Furthermore, COMET-QE shows low accuracy on gender (i.e., masculine pronouns replaced with feminine pronouns or vice-versa), number (i.e., a number replaced for another, reasonable number), and interrogatives (i.e., change of affirmative mood into interrogative mood). COMET-QE also strongly prefers (88%) the translation stripped of final punctuation over the complete sentence, in comparison to 0% for PRISM-QE. In terms of accuracy, PRISM-QE performs exceptionally well on all perturbations, achieving lower accuracies (yet still around 80%) only for Hindi\u2014a language it was not trained on.\\n\\n4 Sensitivity analysis\\n\\nWhile the accuracy of a metric on DEMETR is useful to know, it also obscures the sensitivity of a metric to a particular perturbation. Are metrics more sensitive to CRITICAL errors than MINOR ones? Are different learned metrics comparatively more or less sensitive to a particular perturbation? In this section, we explore these questions and highlight interesting observations, focusing primarily on the behavior of learned metrics.\\n\\nMeasuring sensitivity:\\n\\nSince each of our metrics has a different score range, we cannot na\u00efvely just compare their score differences to analyze sensitivity. Instead, we compute a ratio that intuitively answers the following question: how much does \\\\( \\\\text{SCORE} \\\\) drop on this perturbation compared to the catastrophic error of producing an empty string? We choose the empty string as a control since it is the perturbation that results in the largest \\\\( \\\\text{SCORE} \\\\) drop for most metrics. Concretely, for a given reference translation \\\\( r_i \\\\), machine translation \\\\( t_i \\\\), and perturbed translation \\\\( t'_i \\\\), we compute a ratio \\\\( z_i \\\\) as:\\n\\n\\\\[\\n\\\\frac{\\\\text{SCORE}(r_i, t_i) - \\\\text{SCORE}(r_i, t'_i)}{\\\\text{SCORE}(r_i, t_i) - \\\\text{SCORE}(r_i, \\\\text{empty string})}\\n\\\\]\\n\\nThen, for each perturbation category, we aggregate the example-level ratios to obtain \\\\( z \\\\) by simply taking a mean,\\n\\n\\\\[\\nz = \\\\frac{\\\\sum z_i}{N},\\n\\\\]\\n\\nwhere \\\\( N \\\\) is the number of examples for that perturbation (in most cases, 1K).\\n\\nFigure 2 contains a heatmap plotting this \\\\( z \\\\) ratio for each perturbation and learned metric, and forms the core of the following analysis.\\n\\nBERTS CORE is relatively more sensitive to some minor errors than it is to critical errors: Although we observe that BERTS CORE drops only by a small absolute number for most perturbations, it is actually quite sensitive to many perturbations, especially when passing an unrelated translation and a shuffled version of the existing translation \u2013 two of the most drastic perturbations. It also shows higher sensitivity to untranslated words (i.e., codemixing) than to the remaining perturbations, which is to be expected as BERTS CORE uses a multilingual model. However, its sensitivity...\"}"}
{"id": "emnlp-2022-main-649", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: A heatmap of the sensitivity of learned metrics to different perturbations in DEMETR. The numbers are the ratios $z$ computed as described in Section 4. Higher values denote higher relative sensitivity to the perturbation and are marked by a darker color. The error severity categories are arranged from minor (bottom part) through major (middle part) to critical (upper part). The last two errors are baselines.\\n\\nTo incorrect numbers (0.044), gender information (0.067), or aspect change (0.099) is lower than sensitivity to less severe errors, such as tokenized sentence (0.26) or lower-cased sentence (0.33) \u2013 a trend visible in other metrics, though not to such an extent.\\n\\nCOMET-QE, a metric adapted to MQM scoring, does not perform well on DEMETR: COMET-QE trained on MQM ratings (i.e., on the identification of mistakes similar to those included in DEMETR) varies in its sensitivity to perturbations. While it is sensitive to a sentence with shuffled words, it is not sensitive to a different, unrelated translation (an observation in line with its accuracy). COMET-QE also seems to be insensitive to minor errors such as the removal of the final punctuation, but also to some major or critical errors such as gender and number replacement.\\n\\nFurther, COMET-QE is much more sensitive to word repetition (0.46-0.72) and word swap (0.41) than to some critical or major errors, such as named entity replacement (0.16) or sentence negation (0.16).\\n\\nOverall, COMET-QE behaves very differently from most of the other metrics, and in ways that are difficult to explain.\\n\\nOverall, all metrics struggle to differentiate between minor and critical errors: While all metrics other than COMET-QE are very sensitive to the two baselines (different translation and shuffled)\\n\\nWelsch t-test also reveals that the difference between the scores for the original MT and perturbed text is not significant ($p$-val>.05).\"}"}
{"id": "emnlp-2022-main-649", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"fled words) when compared to other perturbations\\n\\n(0.44 - 2.20), they struggle to differentiate the severity of some critical errors, such as an addition of a plausible but meaning-changing word (0.032 - 0.12) or incorrect number (0.0038 - 0.07). These ratios are lower than of some minor errors such as a word repeated four times (0.086 - 0.72). In fact, BERTS, CORE, and COMET-QE are more sensitive to word repetition than to an addition of a word which ultimately critically changes the meaning.\\n\\n5 Related Work\\n\\nOur work builds on the previous efforts to analyze the performance of MT evaluation metrics, as well as efforts to curate diagnostic datasets for NLP. Analysis of MT evaluation metrics: Fomicheva and Specia (2019) show that metric performance varies significantly across different levels of MT quality. Freitag et al. (2020) demonstrate the importance of reference quality during evaluation. Kocmi et al. (2021) investigate the performance of pretrained and string-based metrics, and conclude that learned metrics outperform string-based metrics, with COMET being the best-performing metric at the time. However, Amrhein and Senrich (2022) explore COMET models in more depth finding, just as in the current study, that the models are not sensitive to number and named entity errors. Hanna and Bojar (2021), on the other hand, find that BERTS is more robust to errors in major content words, and less so to small errors. Finally, Kasai et al. (2021) introduce a leaderboard for generation tasks that ensembles many of the metrics used here.\\n\\nDiagnostic datasets: A number of previous studies employed diagnostic tests to explore the performance of NLP models. Marvin and Linzen (2018) evaluate abilities of LSTM based language models to rate grammatical sentence higher than ungrammatical ones by curating a dataset of minimal pairs in English. Warstadt et al. (2020) also utilize the concept of linguistic minimal pairs to evaluate the sensitivity of language models to various linguistic errors. Ribeiro et al. (2020) curate a checklist of perturbations to test the robustness of general NLP models.\\n\\nSpecia et al. (2010) introduce a simplified dataset of translations by four MT systems annotated for their quality in order to evaluate MT evaluation metrics. Sai et al. (2021b) also propose a checklist-style method to test the robustness of evaluation metrics for MT; however, they limit themselves to Chinese-to-English translation. Furthermore, many of the perturbations introduced in Sai et al. (2021b) does not control for a single aspect, as DERMETR does, and are not manually verified. Macketanz et al. (2018), on the other hand, design a linguistic test suite to evaluate the quality of MT from German to English, which WMT21 (Barrault et al., 2021) utilizes as a challenge dataset for MT evaluation metrics. Finally, Barrault et al. (2021) create a nine-category challenge set from a Chinese to English corpus, in order to test MT evaluation metrics, that are being submitted to the shared task.\\n\\n6 Conclusion\\n\\nWe present DERMETR, a dataset designed to diagnose MT evaluation metrics. DERMETR consists of 31K semi-automatically generated perturbations that cover 35 different linguistic phenomena. Our experiments showed that learned metrics are notably better than any string-based metrics at distinguishing perturbed from unperturbed translations, which confirms results reported in other studies (Kocmi et al., 2021; Fomicheva and Specia, 2019). We further explore the sensitivity of learned metrics, showing that even the best-performing metrics struggle to distinguish between minor errors such as word repetition and critical errors such as incorrect number, aspect, and gender. We will publicly release DERMETR to spur more informed future development of machine translation evaluation metrics.\\n\\nLimitations\\n\\nWhile DERMETR incorporates a wide range of linguistic phenomena, including various semantic, pragmatic, and morphological errors, all examples included in DERMETR are of translations into English. It is likely that other translation directions may introduce other errors or metrics may be more/less sensitive to them. Furthermore, we decided to utilize sentence level translation as most metrics evaluate the translation on the sentence level and to highlight specific errors, which could be less apparent in the paragraph level setup. However, sentence level data cannot model discourse level errors, which remain an open problem in both machine translation and its evaluation. Furthermore, as DERMETR was constructed using WMT\"}"}
{"id": "emnlp-2022-main-649", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and FLORES the domains incorporated in DEMETR are restricted to the ones present in these two datasets (i.e., mostly news and informational materials). Finally, even though in most cases multiple correct translations of the source sentence exist, we provide only one reference. We decided not to include multiple references due to the time restrictions as well as the fact that the only metric currently supporting multiple references is BLEU.\\n\\nEthical Considerations\\n\\nSome perturbations were conducted manually with a help of freelancers hired on Upwork. The freelancers were informed of the purpose of this experiment. They were paid an equivalent of $15 per hour. We also adjusted this hourly rate to cover the 20% Upwork charge, which the platform charges the freelancers.\\n\\nAcknowledgements\\n\\nWe would like to show our appreciation for the annotators hired on Upwork who took time and effort to fully understand the project and who helped us to assure good quality of the data. We would also like to thank Sergiusz Rzepkowski, Paula Kurzawska, and Kalpesh Krishna for their help in verifying the quality of translation and/or the perturbations. Finally, we would like to thank the reviewers for their constructive comments which helped to shape this paper, as well as UMass NLP community for their insights and discussions during this project.\\n\\nThis project was partially supported by awards IIS-1955567 and IIS-2046248 from the National Science Foundation (NSF).\\n\\nReferences\\n\\nFarhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ond\u0159ej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina Espa\u00f1a-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Augustine Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, pages 1\u201388, Online. Association for Computational Linguistics.\\n\\nChantal Amrhein and Rico Sennrich. 2022. Identifying weaknesses in machine translation metrics through minimum bayes risk decoding: A case study for comet.\\n\\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65\u201372, Ann Arbor, Michigan. Association for Computational Linguistics.\\n\\nLo\u00efc Barrault, Magdalena Biesialska, Ond\u0159ej Bojar, Marta R. Costa-juss\u00e0, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljube\u0161i\u0107, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, pages 1\u201355, Online. Association for Computational Linguistics.\\n\\nLoic Barrault, Ondrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Tom Kocmi, Andre Martins, Makoto Morishita, and Christof Monz, editors. 2021. Proceedings of the Sixth Conference on Machine Translation. Association for Computational Linguistics, Online.\\n\\nOnd\u0159ej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1\u201344, Sofia, Bulgaria. Association for Computational Linguistics.\\n\\nOnd\u0159ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale\u0161 Tamchyna. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12\u201358, Baltimore, Maryland, USA. Association for Computational Linguistics.\\n\\nOnd\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Carolina Scarton, Lucia Specia, and Marco Turchi. 2015. Findings of the\"}"}
{"id": "emnlp-2022-main-649", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-649", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-649", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jonathan Bright, St\u00e9fan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, \u02d9Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Ant\u00f4nio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261\u2013272.\\n\\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Association for Computational Linguistics, 8:377\u2013392.\\n\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021a. Bartscore: Evaluating generated text as text generation.\\n\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021b. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems, volume 34, pages 27263\u201327277. Curran Associates, Inc.\\n\\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.\\n\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 563\u2013578, Hong Kong, China. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-649", "page_num": 14, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-649", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I don't know if you realize that most of the goods imported into this country from Central America are duty free.\\n\\nIn 1940 he stood up to other government aristocrats.\\n\\nLast month, a presidential committee recommended the resignation of the former CEP as part of measures to push the country toward new elections.\\n\\nThey welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede.\\n\\nLate night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede.\\n\\nLast month, a presidential committee recommended the resignation of the former CEP as part of measures to push the country toward new elections.\\n\\nThe Chinese Consulate General in Houston was established in 1979 and is the first Chinese consulate in the United States.\\n\\nShe is in custody pending prosecution and trial; but any witness evidence could be negatively impacted because her image has been widely published.\\n\\nHe has been unable to relieve the pain with medication, which the competition prohibits competitors from taking.\\n\\nCyanuric acid and melamine were both found in urine samples of pets who died after eating contaminated pet food.\\n\\nBoth found in urine samples of pets who died after eating contaminated pet food. Cyanuric acid and melamine were both found in urine samples of pets who died after eating contaminated pet food.\\n\\nIn 1940 he stood up to other government aristocrats.\\n\\nIn 1940 he stood up to other government aristocrats.\\n\\nI don't know if you realize that most of the goods imported into this country from Central America are duty free.\\n\\nThey welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede.\\n\\nLate night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede.\\n\\nLast month, a presidential committee recommended the resignation of the former CEP as part of measures to push the country toward new elections.\\n\\nThe Chinese Consulate General in Houston was established in 1979 and is the first Chinese consulate in the United States.\\n\\nShe is in custody pending prosecution and trial; but any witness evidence could be negatively impacted because her image has been widely published.\\n\\nHe has been unable to relieve the pain with medication, which the competition prohibits competitors from taking.\\n\\nCyanuric acid and melamine were both found in urine samples of pets who died after eating contaminated pet food.\\n\\nBoth found in urine samples of pets who died after eating contaminated pet food. Cyanuric acid and melamine were both found in urine samples of pets who died after eating contaminated pet food.\\n\\nIn 1940 he stood up to other government aristocrats.\\n\\nIn 1940 he stood up to other government aristocrats.\\n\\nI don't know if you realize that most of the goods imported into this country from Central America are duty free.\\n\\nThey welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede.\\n\\nLate night presenter Stephen Colbert welcomed 17-year-old Thunberg to his show on Tuesday and conducted a lengthy interview with the Swede.\\n\\nLast month, a presidential committee recommended the resignation of the former CEP as part of measures to push the country toward new elections.\\n\\nThe Chinese Consulate General in Houston was established in 1979 and is the first Chinese consulate in the United States.\\n\\nShe is in custody pending prosecution and trial; but any witness evidence could be negatively impacted because her image has been widely published.\\n\\nHe has been unable to relieve the pain with medication, which the competition prohibits competitors from taking.\\n\\nCyanuric acid and melamine were both found in urine samples of pets who died after eating contaminated pet food.\\n\\nBoth found in urine samples of pets who died after eating contaminated pet food. Cyanuric acid and melamine were both found in urine samples of pets who died after eating contaminated pet food.\\n\\nIn 1940 he stood up to other government aristocrats.\\n\\nIn 1940 he stood up to other government aristocrats.\\n\\nI don't know if you realize that most of the goods imported into this country from Central America are duty free.\"}"}
{"id": "emnlp-2022-main-649", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| ID | Category       | Type            | Example                                                                 |\\n|----|----------------|-----------------|-------------------------------------------------------------------------|\\n| 25 | TYPOGRAPHY     | spelling -misspelled | Scientists want to understand how planets have formed since a comet collided with Earth long ago, and especially how Earth has formed. |\\n| 26 |     | spelling -char removed | I don't know if you realize that most of the goods imported into this country from Central America are duty free. |\\n| 27 |     | punctuation -removed | When a satellite in space receives a call, it reflects it back almost immediately. |\\n| 28 |     | punctuation -added  | Comets may have been the source of Earth's water and organic matter that can form proteins and sustain life. |\\n| 29 |     | tokenized         | At 9:30 a.m. on July 26, the reporter saw at the scene of Jiangkouhe Lianxu that the local area had made various preparations before flood distribution. |\\n| 30 |     | lowercases -whole | For example, U.S. citizens in the Middle East may face different situations than Europeans or Arabs. |\\n| 31 |     | lowercases -first word | For example, U.S. citizens in the Middle East may face different situations than Europeans or Arabs. |\\n| 32 | BASELINE      | empty           | The sentence is tokenized. |\\n| 33 |     | different        | It was the last game for the All Blacks, who had won the trophy two weeks earlier. |\\n| 34 |     | unintelligible   | Cyanuric acid and melamine were both found in urine samples of pets who died after eating contaminated pet food. |\\n| 35 |     | reference        | Last month, a presidential committee recommended the resignation of the former CEP as part of measures to move the country towards new elections. |\"}"}
{"id": "emnlp-2022-main-649", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Welsch\\n\\nt-testID perturbation metric type t p -val df accuracy\\nBLEU string 2.98 0.003 1,992.12 93.2%\\nMETEOR string 0.44 0.662 1,997.34 85.4%\\nHR string 1.00 0.316 1,997.04 89.9%\\nHR2 string 0.96 0.337 1,997.23 92.7%\\nTER string 2.98 0.44 1,997.00 85.8%\\nR string 1.00 0.316 1,997.04 89.9%\\nOUGE2 string 1.69 0.092 1,996.37 99.7%\\nBERTS CORE learned 8.43 <0.001 1,997.55 97.5%\\nOMET-QE learned 21.33 <0.001 1,991.48 99.1%\\nOMET learned 22.46 <0.001 1,973.93 99.1%\\nLEURT20 learned 24.43 <0.001 1,996.98 99.0%\\nP RISM-QE learned 6.86 <0.001 1,989.71 98.9%\\nP RISM learned 9.61 <0.001 1,996.17 99.9%\\n\\naddition (repetition)\\nBARTScore learned 1.49 0.137 1,997.92 78.0%\\nBLEU string 6.47 <0.001 1,960.90 95.7%\\nMETEOR string 1.63 0.104 1,997.00 85.8%\\nHR string 3.88 <0.001 1,992.23 97.6%\\nHR2 string 3.53 <0.001 1,993.49 98.5%\\nTER string -13.93 <0.001 1,996.08 95.2%\\nR string -18.65 <0.001 1,992.80 96.1%\\nOUGE2 string 5.73 <0.001 1,776.50 63.3%\\nBERTS CORE learned 22.91 <0.001 1,990.44 99.9%\\nOMET-QE learned 34.03 <0.001 1,982.14 100.0%\\nOMET learned 42.55 <0.001 1,955.19 100.0%\\nLEURT20 learned 50.88 <0.001 1,991.41 99.9%\\nP RISM-QE learned 19.02 <0.001 1,945.31 98.7%\\nP RISM learned 27.18 <0.001 1,994.44 100.0%\\n\\nhypernym (undertranslation)\\nBARTScore learned 6.42 <0.001 1,781.16 90.5%\\nBLEU string 5.70 <0.001 1,973.31 73.1%\\nMETEOR string 6.18 <0.001 1,995.59 72.5%\\nHR string 9.22 <0.001 1,989.48 95.2%\\nHR2 string 8.56 <0.001 1,988.24 95.3%\\nTER string -3.82 <0.001 1,994.43 58.3%\\nR string -4.53 <0.001 1,992.88 83.5%\\nOUGE2 string 6.35 <0.001 1,984.97 68.4%\\nBERTS CORE learned 36.69 <0.001 1,824.17 99.8%\\nOMET-QE learned 27.31 <0.001 1,994.98 98.3%\\nOMET learned 26.78 <0.001 1,997.69 99.2%\\nLEURT20 learned 13.80 <0.001 1,786.00 92.7%\\nP RISM-QE learned 10.38 <0.001 1,993.90 97.6%\\nP RISM learned 16.62 <0.001 1,988.33 99.8%\\n\\nuntranslated\\nBARTScore learned 8.91 <0.001 1,991.19 90.8%\\nBLEU string 9.94 <0.001 1,748.59 79.2%\\nMETEOR string 17.83 <0.001 1,777.96 89.4%\\nHR string 15.88 <0.001 1,777.31 93.1%\\nHR2 string 15.77 <0.001 1,778.00 94.0%\\nTER string -9.79 <0.001 1,715.36 76.1%\\nR string -8.48 <0.001 1,715.24 73.6%\\nOUGE2 string 8.42 <0.001 1,775.53 78.7%\\nBERTS CORE learned 18.07 <0.001 1,770.58 94.2%\\nOMET-QE learned 6.98 <0.001 1,777.40 80.2%\\nOMET learned 13.84 <0.001 1,777.28 95.5%\\nLEURT20 learned 25.70 <0.001 1,579.21 97.1%\\nP RISM-QE learned 10.38 <0.001 1,993.90 97.6%\\nP RISM learned 16.62 <0.001 1,988.33 99.8%\\n\\ncompleteness (omitted pp)\\nBARTScore learned 19.56 <0.001 1,670.43 96.2%\\nBLEU string 4.84 <0.001 1,979.16 93.0%\\nMETEOR string 1.53 0.127 1,997.87 99.0%\\nHR string 3.12 0.002 1,992.92 89.4%\\nHR2 string 3.06 0.002 1,993.62 91.8%\\nTER string -4.74 <0.001 1,997.95 80.3%\\nR string -6.24 <0.001 1,996.99 92.9%\\nOUGE2 string 4.90 <0.001 1,987.76 99.8%\\nBERTS CORE learned 9.54 <0.001 1,994.95 98.5%\\nOMET-QE learned 4.78 <0.001 1,997.76 69.7%\\nOMET learned 9.25 <0.001 1,996.06 93.3%\\nLEURT20 learned 19.09 <0.001 1,996.53 97.1%\\nP RISM-QE learned 7.13 <0.001 1,991.72 98.3%\\nP RISM learned 13.58 <0.001 1,995.57 99.9%\\n\\naddition\\nBARTScore learned 5.56 <0.001 1,997.62 94.1%\"}"}
{"id": "emnlp-2022-main-649", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Welsch t-testID perturbation metric type t p -val df accuracy |\\n|-------------------------------------------------------------|\\n| BLEU string 5.47 <0.001 1,949.90 65.4%                     |\\n| ETEOR string 5.67 <0.001 1,963.08 68.9%                    |\\n| HR F string 7.11 <0.001 1,953.68 83.8%                     |\\n| HR F2 string 6.81 <0.001 1,954.25 84.0%                    |\\n| TER string -3.49 <0.001 1,961.27 64.9%                     |\\n| CER string -3.34 <0.001 1,959.91 76.8%                     |\\n| ROUGE 2 string 6.10 <0.001 1,952.61 57.7%                  |\\n| BERTS CORE learned 11.52 <0.001 1,962.23 98.3%             |\\n| OMET -QE learned 6.61 <0.001 1,963.91 83.7%                |\\n| OMET learned 12.11 <0.001 1,955.91 97.0%                   |\\n| LEURT 20 learned 24.45 <0.001 1,939.39 98.7%              |\\n| PRISM -QE learned 6.39 <0.001 1,954.01 96.7%               |\\n| PRISM learned 13.85 <0.001 1,960.97 99.2%                  |\\n| BARTScore learned 7.34 <0.001 1,963.98 96.9%               |\\n| BLEU string 7.32 <0.001 1,961.08 91.3%                     |\\n| ETEOR string 3.36 <0.001 1,995.95 94.7%                    |\\n| HR F string 4.88 <0.001 1,990.62 90.8%                     |\\n| HR F2 string 5.14 <0.001 1,990.35 94.6%                    |\\n| TER string -8.26 <0.001 1,995.94 89.6%                     |\\n| CER string -5.29 <0.001 1,995.38 91.0%                     |\\n| ROUGE 2 string 7.67 <0.001 1,979.28 96.3%                  |\\n| BERTS CORE learned 15.60 <0.001 1,995.96 99.6%             |\\n| OMET -QE learned 6.86 <0.001 1,995.73 83.7%                |\\n| OMET learned 18.35 <0.001 1,991.67 99.4%                   |\\n| LEURT 20 learned 41.51 <0.001 1,987.24 99.8%               |\\n| PRISM -QE learned 8.43 <0.001 1,977.03 96.1%               |\\n| PRISM learned 16.17 <0.001 1,994.10 99.8%                  |\\n| BARTScore learned 9.44 <0.001 1,988.26 98.5%               |\\n| BLEU string 7.00 <0.001 1,339.02 90.5%                     |\\n| ETEOR string 8.29 <0.001 1,364.57 89.3%                    |\\n| HR F string 12.47 <0.001 1,362.50 98.7%                    |\\n| HR F2 string 11.54 <0.001 1,361.48 98.7%                   |\\n| TER string -7.14 <0.001 1,361.50 83.8%                     |\\n| CER string -7.58 <0.001 1,358.56 89.9%                     |\\n| ROUGE 2 string 8.73 <0.001 1,350.27 87.7%                  |\\n| BERTS CORE learned 25.35 <0.001 1,358.26 99.1%             |\\n| OMET -QE learned 7.14 <0.001 1,365.67 85.4%                |\\n| OMET learned 18.20 <0.001 1,363.20 98.8%                   |\\n| LEURT 20 learned 43.02 <0.001 1,279.18 100.0%              |\\n| PRISM -QE learned 12.00 <0.001 1,331.69 95.3%               |\\n| PRISM learned 30.02 <0.001 1,348.23 99.7%                  |\\n| BARTScore learned 24.24 <0.001 1,336.08 100.0%             |\\n| BLEU string 4.44 <0.001 734.86 89.0%                       |\\n| ETEOR string 3.97 <0.001 741.65 79.6%                      |\\n| HR F string 2.99 0.003 740.92 96.5%                        |\\n| HR F2 string 3.47 <0.001 740.45 96.5%                      |\\n| TER string -2.61 0.009 741.63 79.8%                        |\\n| CER string -0.82 0.415 741.83 80.4%                        |\\n| ROUGE 2 string 4.90 <0.001 738.46 82.5%                    |\\n| BERTS CORE learned 1.95 0.052 742.00 99.1%                 |\\n| OMET -QE learned 0.16 0.871 741.99 53.2%                   |\\n| OMET learned 0.73 0.463 741.82 80.4%                       |\\n| LEURT 20 learned 9.18 <0.001 739.98 98.7%                  |\\n| PRISM -QE learned 1.09 0.277 743.93 97.3%                  |\\n| PRISM learned 3.16 0.002 743.97 100.0%                     |\\n| BARTScore learned 1.45 0.148 743.97 96.5%                  |\\n| BLEU string 4.43 <0.001 1,441.98 75.8%                     |\\n| ETEOR string 5.13 <0.001 1,449.88 78.1%                    |\\n| HR F string 4.50 <0.001 1,448.17 85.3%                     |\\n| HR F2 string 4.69 <0.001 1,448.04 84.6%                    |\\n| TER string -2.33 0.020 1,444.72 57.9%                      |\\n| CER string -1.88 0.060 1,443.24 70.4%                      |\\n| ROUGE 2 string 4.26 <0.001 1,447.85 62.0%                  |\\n| BERTS CORE learned 9.89 <0.001 1,448.55 93.9%              |\\n| OMET -QE learned 7.03 <0.001 1,448.46 89.7%                 |\\n| OMET learned 8.33 <0.001 1,448.86 93.8%                    |\\n| LEURT 20 learned 12.68 <0.001 1,448.58 95.0%               |\\n| PRISM -QE learned 5.04 <0.001 1,449.60 97.8%               |\\n| PRISM learned 8.57 <0.001 1,449.92 96.6%                   |\\n| BARTScore learned 3.25 0.001 1,449.46 83.1%                |\"}"}
{"id": "emnlp-2022-main-649", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Grammar   | Metric Type | BLEU | METEOR | HR | HR2 | TER | CER | ROUGE2 | BERTS CORE | OMET-QE Learned | OMET Learned | LEURT20 Learned | PRISM-QE Learned | PRISM Learned |\\n|-----------|-------------|------|--------|----|-----|-----|-----|--------|-------------|----------------|---------------|----------------|----------------|---------------|\\n| 1         | BLEU string | 5.03 | 2.01   | 3.82 | 4.29 | -3.12 | -1.84 | 5.75   | 14.08       | 12.20            | 11.36          | 13.27            | 7.02            | 9.68          |\\n| 2         | BLEU string | 7.51 | 2.99   | 6.05 | 5.92 | -3.65 | -4.35 | 9.80   | 18.87       | 18.47            | 16.44          | 24.50            | 11.91           | 16.57         |\\n| 3         | BLEU string | 3.05 | 2.80   | 1.91 | 2.30 | -2.14 | -1.40 | 3.44   | 7.35        | 8.25             | 8.12           | 9.00             | 5.32            | 6.31          |\\n| 4         | BLEU string | 6.07 | 5.22   | 3.74 | 4.39 | -3.67 | -2.19 | 6.47   | 11.34       | 9.19             | 8.70           | 13.79            | 7.26            | 10.49         |\\n| 5         | BLEU string | 6.19 | 5.22   | 4.56 | 5.08 | -5.37 | -2.67 | 7.06   | 6.82        | 3.32             | 4.03           | 10.30            | 4.00            | 7.55          |\\n| 6         | BLEU string | 6.30 | 2.25   | 5.14 | 5.37 | -6.95 | -5.03 | 7.04   | 7.86        | 3.12             | 3.60           | 9.15             | 4.49            | 7.57          |\\n\\n19 grammar - tense\\n\\n| Grammar   | Metric Type | BLEU | METEOR | HR | HR2 | TER | CER | ROUGE2 | BERTS CORE | OMET-QE Learned | OMET Learned | LEURT20 Learned | PRISM-QE Learned | PRISM Learned |\\n|-----------|-------------|------|--------|----|-----|-----|-----|--------|-------------|----------------|---------------|----------------|----------------|---------------|\\n| 1         | BLEU string | 6.30 | 2.25   | 5.14 | 5.37 | -6.95 | -5.03 | 7.04   | 7.86        | 3.12             | 3.60           | 9.15             | 4.49            | 7.57          |\"}"}
{"id": "emnlp-2022-main-649", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| TestID | Metric Type | p-Value | Degree of Freedom | Accuracy |\\n|--------|-------------|---------|-------------------|----------|\\n| BLEU   | string      | <0.001  | 1,851.84          | 97.4%    |\\n| METEOR | string      | <0.001  | 1,917.78          | 91.4%    |\\n| HR     | string      | <0.001  | 1,911.45          | 94.1%    |\\n| HR     | F string    | <0.001  | 1,905.30          | 97.5%    |\\n| TERE   | string      | <0.001  | 1,911.39          | 93.2%    |\\n| CER    | string      | <0.001  | 1,925.33          | 92.3%    |\\n| ROUGE  | 2 string    | <0.001  | 1,892.51          | 85.3%    |\\n| BERTS  | CORE learned| <0.001  | 1,913.23          | 99.8%    |\\n|OMET   | -QE learned | <0.001  | 1,924.70          | 64.5%    |\\n|OMET   | learned     | <0.001  | 1,918.07          | 96.1%    |\\n|LEURT  | 20 learned  | <0.001  | 1,919.63          | 99.6%    |\\n|PRISM  | -QE learned | <0.001  | 1,916.54          | 99.9%    |\\n|PRISM  | learned     | <0.001  | 1,924.74          | 100.0%   |\\n|BARTS  | learned     | <0.001  | 1,925.93          | 96.0%    |\\n|BLEU   | string      | <0.001  | 1,833.15          | 70.1%    |\\n|METEOR | string      | <0.001  | 1,842.82          | 76.1%    |\\n|HR     | string      | <0.001  | 1,837.42          | 84.2%    |\\n|HR     | F2 string   | <0.001  | 1,837.52          | 82.6%    |\\n|TER    | string      | <0.001  | 1,835.00          | 53.7%    |\\n|CER    | string      | <0.001  | 1,827.77          | 68.7%    |\\n|ROUGE  | 2 string    | <0.001  | 1,838.99          | 65.4%    |\\n|BERTS  | CORE learned| <0.001  | 1,843.71          | 91.2%    |\\n|OMET   | -QE learned | <0.001  | 1,842.82          | 80.7%    |\\n|OMET   | learned     | <0.001  | 1,843.56          | 91.5%    |\\n|LEURT  | 20 learned  | <0.001  | 1,836.98          | 95.2%    |\\n|PRISM  | -QE learned | <0.001  | 1,841.41          | 90.4%    |\\n|PRISM  | learned     | <0.001  | 1,841.29          | 93.7%    |\\n|BARTS  | learned     | <0.001  | 1,844.00          | 93.4%    |\\n|BLEU   | string      | <0.001  | 1,917.46          | 61.7%    |\\n|METEOR | string      | <0.001  | 1,927.87          | 63.8%    |\\n|HR     | string      | <0.001  | 1,923.46          | 80.7%    |\\n|HR     | F2 string   | <0.001  | 1,922.47          | 78.2%    |\\n|TER    | string      | -1.64   | 0.100             | 46.8%    |\\n|CER    | string      | -2.56   | 0.010             | 64.3%    |\\n|ROUGE  | 2 string    | <0.001  | 1,921.73          | 55.3%    |\\n|BERTS  | CORE learned| <0.001  | 1,926.62          | 96.5%    |\\n|OMET   | -QE learned | <0.001  | 1,922.05          | 98.4%    |\\n|OMET   | learned     | <0.001  | 1,929.97          | 98.8%    |\\n|LEURT  | 20 learned  | <0.001  | 1,830.05          | 98.8%    |\\n|PRISM  | -QE learned | <0.001  | 1,928.22          | 97.8%    |\\n|PRISM  | learned     | <0.001  | 1,929.99          | 96.5%    |\\n|BARTS  | learned     | <0.001  | 1,933.99          | 90.2%    |\\n|BLEU   | string      | <0.001  | 1,932.96          | 74.1%    |\\n|METEOR | string      | <0.001  | 1,954.60          | 80.1%    |\\n|HR     | string      | <0.001  | 1,951.54          | 91.3%    |\\n|HR     | F2 string   | <0.001  | 1,949.00          | 90.5%    |\\n|TER    | string      | -3.66   | <0.001            | 63.9%    |\\n|CER    | string      | -3.88   | <0.001            | 68.7%    |\\n|ROUGE  | 2 string    | <0.001  | 1,948.97          | 70.3%    |\\n|BERTS  | CORE learned| <0.001  | 1,955.90          | 98.0%    |\\n|OMET   | -QE learned | <0.001  | 1,955.97          | 94.7%    |\\n|OMET   | learned     | <0.001  | 1,951.01          | 98.5%    |\\n|LEURT  | 20 learned  | <0.001  | 1,795.97          | 99.1%    |\\n|PRISM  | -QE learned | <0.001  | 1,945.91          | 96.0%    |\\n|PRISM  | learned     | <0.001  | 1,949.93          | 98.3%    |\\n|BARTS  | learned     | <0.001  | 1,901.80          | 91.8%    |\\n|BLEU   | string      | <0.001  | 1,336.10          | 80.4%    |\\n|METEOR | string      | <0.001  | 1,351.99          | 94.4%    |\\n|HR     | string      | <0.001  | 1,351.88          | 97.5%    |\\n|HR     | F2 string   | <0.001  | 1,351.34          | 97.0%    |\\n|TER    | string      | -4.68   | <0.001            | 72.7%    |\\n|CER    | string      | -5.05   | <0.001            | 74.2%    |\\n|ROUGE  | 2 string    | <0.001  | 1,348.43          | 79.8%    |\\n|BERTS  | CORE learned| <0.001  | 1,351.75          | 98.5%    |\\n|OMET   | -QE learned | <0.001  | 1,351.92          | 91.3%    |\\n|OMET   | learned     | <0.001  | 1,349.64          | 98.5%    |\\n|LEURT  | 20 learned  | <0.001  | 1,288.20          | 99.4%    |\\n|PRISM  | -QE learned | <0.001  | 1,337.24          | 93.9%    |\\n|PRISM  | learned     | <0.001  | 1,345.33          | 99.0%    |\\n|BARTS  | learned     | <0.001  | 1,901.80          | 91.8%    |\\n|BLEU   | string      | <0.001  | 1,336.10          | 80.4%    |\\n|METEOR | string      | <0.001  | 1,351.99          | 94.4%    |\\n|HR     | string      | <0.001  | 1,351.88          | 97.5%    |\\n|HR     | F2 string   | <0.001  | 1,351.34          | 97.0%    |\\n|TER    | string      | -4.68   | <0.001            | 72.7%    |\\n|CER    | string      | -5.05   | <0.001            | 74.2%    |\"}"}
{"id": "emnlp-2022-main-649", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Welsch | testID perturbation metric type | t | p   | -val  | df  | accuracy |\\n|--------|--------------------------------|----|-----|-------|-----|-----------|\\n| BLEU   | string                         | 5.04| <0.001 | 1,744.49 | 67.7% |\\n| METEOR | string                         | 5.27| <0.001 | 1,754.73 | 69.6% |\\n| HR     | string                         | 5.04| <0.001 | 1,744.49 | 64.1% |\\n| HR_f   | string                         | 5.27| <0.001 | 1,754.73 | 69.6% |\\n| HR_f2  | string                         | 5.04| <0.001 | 1,744.49 | 67.7% |\\n| TER    | string                         | 5.04| <0.001 | 1,744.49 | 63.7% |\\n| CER    | string                         | 5.04| <0.001 | 1,744.49 | 63.7% |\\n| Rouge  | string                         | 5.04| <0.001 | 1,744.49 | 63.7% |\\n| BERTS  | learned                        | 21.29| <0.001 | 1,748.10 | 99.8% |\\n| CORE   | learned                        | 14.06| <0.001 | 1,747.45 | 92.6% |\\n| CORE   | learned                        | 14.60| <0.001 | 1,755.96 | 97.4% |\\n| LEURT  | learned                        | 15.96| <0.001 | 1,756.84 | 97.6% |\\n| PRISM  |learned                        | 19.00| <0.001 | 1,755.60 | 100.0% |\\n| BARTScore | learned                    | 2.86| 0.004 | 1,755.76 | 79.7% |\\n| BLEU   | string                         | 2.07| 0.038  | 1,996.48 | 76.2% |\\n| METEOR | string                         | 3.30| <0.001 | 1,989.43 | 57.3% |\\n| HR     | string                         | 0.95| 0.343  | 1,997.89 | 96.4% |\\n| HR_f   | string                         | 1.96| 0.050  | 1,997.73 | 98.3% |\\n| HR_f2  | string                         | 2.61| 0.009  | 1,995.62 | 97.9% |\\n| TER    | string                         | 2.78| 0.006  | 1,995.28 | 99.0% |\\n| CER    | string                         | 0.00| 1.000  | 1,998.00 | 0.0%  |\\n| Rouge  | string                         | 0.14| 0.888  | 1,998.00 | 1.2%  |\\n| BERTS  | learned                        | 15.85| <0.001 | 1,983.20 | 99.3% |\\n| CORE   | learned                        | 14.16| <0.001 | 1,994.92 | 99.5% |\\n| LEURT  | learned                        | 16.35| <0.001 | 1,997.90 | 99.3% |\\n| PRISM  |learned                        | 9.78| <0.001 | 1,995.95 | 99.7% |\\n| BARTScore | learned                    | 1.21| 0.225  | 1,997.97 | 76.9% |\\n| BLEU   | string                         | 1.44| 0.149  | 1,997.07 | 18.6% |\\n| METEOR | string                         | 9.42| <0.001 | 1,978.96 | 84.0% |\\n| HR     | string                         | 0.00| 1.000  | 1,998.00 | 0.0%  |\\n| HR_f   | string                         | 0.45| 0.651  | 1,997.72 | 23.7% |\\n| HR_f2  | string                         | 17.87| <0.001 | 1,991.95 | 88.3% |\\n| CER    | string                         | 2.16| 0.031  | 1,997.79 | 89.0% |\\n| Rouge  | string                         | 0.14| 0.888  | 1,998.00 | 1.2%  |\\n| BERTS  | learned                        | 19.16| <0.001 | 1,995.29 | 99.8% |\\n| CORE   | learned                        | 8.88| <0.001 | 1,997.76 | 98.5% |\\n| LEURT  | learned                        | 9.42| <0.001 | 1,994.27 | 99.8% |\\n| PRISM  |learned                        | 8.42| <0.001 | 1,991.80 | 98.4% |\\n| BARTScore | learned                    | 1.17| 0.241  | 1,997.86 | 69.6% |\\n| BLEU   | string                         | 14.04| <0.001 | 1,955.69 | 87.8% |\\n| METEOR | string                         | 0.00| 1.000  | 1,998.00 | 0.0%  |\\n| HR     | string                         | 11.23| <0.001 | 1,990.65 | 90.6% |\\n| HR_f   | string                         | 13.39| <0.001 | 1,990.65 | 90.5% |\\n| HR_f2  | string                         | 0.00| 1.000  | 1,998.00 | 0.0%  |\\n| CER    | string                         | 2.67| 0.008  | 1,996.14 | 87.4% |\\n| Rouge  | string                         | 0.00| 1.000  | 1,998.00 | 1.2%  |\\n| BERTS  | learned                        | 25.36| <0.001 | 1,957.60 | 99.3% |\\n| CORE   | learned                        | 10.10| <0.001 | 1,984.92 | 97.1% |\\n| LEURT  | learned                        | 16.13| <0.001 | 1,990.72 | 98.1% |\\n| PRISM  |learned                        | 14.11| <0.001 | 1,995.58 | 99.6% |\\n| BARTScore | learned                    | 7.04| <0.001 | 1,997.80 | 93.6% |\"}"}
{"id": "emnlp-2022-main-649", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table A3: A two-samples Welsch $t$-test is conducted on each metric to compare $\\\\text{SCORE}(r, t)$ and $\\\\text{SCORE}(r, t')$ (see Section 2.1) of each perturbation type. The tests are implemented in Python using the package `scipy` (Virtanen et al., 2020). Degrees of Freedom (DF) are estimated using the Welch-Satterthwaite equation for Degrees of Freedom. The accuracy on the baseline perturbation (reference as translation) was reversed, as one can expect the metric to prefer translation identical with the reference.\"}"}
