{"id": "lrec-2024-main-1052", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"My Science Tutor (MyST)\u2013A Large Corpus of Children\u2019s Conversational Speech\\n\\nSameer S. Pradhan\\\\(^1\\\\), Ronald A. Cole\\\\(^3\\\\), Wayne H. Ward\\\\(^4\\\\)\\n\\n\\\\(^1\\\\)cemantix.org, Cambridge MA, USA\\n\\\\(^2\\\\)Linguistic Data Consortium, University of Pennsylvania, Philadelphia PA, USA\\n\\\\(^3\\\\)Boulder Learning Inc., Boulder CO, USA\\n\\\\(^4\\\\)University of Colorado at Boulder, CO, USA\\n\\npradhan@cemantix.org\\n\\nAbstract\\n\\nThis article describes the MyST corpus developed as part of the My Science Tutor project. To the best of our knowledge, this is one of the largest collections of children\u2019s conversational speech that is freely available for non-commercial use under the creative commons license (CC BY-NC-SA 4.0). It comprises approximately 400 hours of speech, spanning some 230K utterances spread across about 10,500 virtual tutor sessions. Roughly 1,300 third, fourth and fifth grade students contributed to this corpus. The current release contains a little over 100K transcribed utterances comprising close to 1.5M space separated transcribed tokens. It is our hope that the corpus can be used to improve automatic speech recognition models and algorithms. We report the word error rate achieved on the test set using a model trained on the training and development portion of the corpus. The git repository of the corpus contains the complete training and evaluation setup in order to facilitate a fair and consistent evaluation. It is our hope that this corpus will contribute to the creation and evaluation of conversational AI agents having a better understanding of children\u2019s speech, potentially opening doors to novel, effective, learning and therapeutic interventions.\\n\\nKeywords: automatic speech recognition, speech corpora, children\u2019s conversational speech, virtual tutor, education\\n\\n1. Introduction\\n\\nAccording to the 2009 National Assessment of Educational Progress (NAEP, 2011), only 34 percent of fourth-graders, 30 percent of eighth-graders, and 21 percent of twelfth-graders in the U.S. performed at or above the proficient level in science. A more recent assessment, in 2019 (NAEP, 2021), reported a statistically significant decrease in the average score for fourth graders in science compared with the most recent previous assessment, in 2015. Thus, approximately two thirds of students in the United States are not proficient in science.\\n\\nThis article describes a resource that was the result of a 13-year project conducted between 2007 and 2019. The project investigated improvements in students\u2019 learning proficiency in elementary school science using conversational multimedia virtual tutor, Marni. The operating principles for the tutor are grounded on research from education and cognitive science where it has been shown that eliciting self-explanations plays an important role (Chi et al., 1989, 1994, 2001; Hausmann and VanLehn, 2007a,b). Speech, language and character animation technologies play a central role because the focus of the system is on engagement and spoken explanations by students during spoken dialog with the virtual tutor. A series of studies conducted during this project demonstrated that students who interacted with the virtual tutor achieved substantial learning gains, equivalent to students who interacted with experienced human tutors, with moderate effect sizes (Ward et al., 2011, 2013). Surveys of participating teachers indicate that it is feasible to incorporate the intervention into their curriculum. Surveys given to students indicated that over 70% of students tutored by Marni were more excited about studying science in the future.\\n\\n2. The MyST corpus\\n\\nThe MyST children\u2019s conversational speech corpus consists of spoken dialog between 3rd, 4th and 5th grade students, and a virtual tutor in 8 areas of science. It consists of 393 hours of speech collected across 1,371 students. The collection comprises a total of 228,874 utterances across 10,496 sessions. It is freely available for research use upon completion of a data use agreement.\\n\\n\\\\(^3\\\\)https://myst.cemantix.org\\n\\nThis does not consider the significant impact that the educational system experienced owing to the Covid-19 pandemic.\"}"}
{"id": "lrec-2024-main-1052", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.1. Data Collection\\n\\nAs part of the study, students engaged in spoken dialog with a virtual science tutor\u2014a lifelike computer character that produced accurate lip and tongue movement synchronized with speech produced by a voice talent. Analyses of the spoken dialog sessions indicated that, during a dialog of about 15 minutes, tutors and students produced about the same amount of speech, around 5 minutes each. This approach was used to develop over 100 tutorial dialog sessions, of about 15 minutes each.\\n\\nThe students who participated in this study were enrolled in schools belonging to the Boulder Valley School District. We did not record the gender, age or primary language of individual students. One could get a rough approximation of the age range based on the grades of the students in the study and information about the science modules.\\n\\nThe MyST corpus was collected in two stages\u2014Phase I and Phase II. In both phases, the scientific content covered is aligned to classroom science content of Full Option Science System (FOSS) modules, which typically last 8 weeks during the school year. FOSS is used by over 1 million children in over 100,000 classrooms in all 50 states in the U.S. FOSS modules are centered on science investigations. There are typically 4 Investigations in a module (e.g., in the Magnetism and Electricity module, the 4 investigations are Magnetism, Serial circuits, Parallel Circuits, and Electromagnetism). Each Investigation has 3 to 4 classroom \\\"investigation parts\\\" where groups of students work together to, for example, build a serial circuit to make a motor run, and record their observations in science notebooks. Shortly after conducting an \\\"investigation part\\\", students interact one-on-one with a virtual tutor for 15-20 minutes. The tutor asks the student questions about science presented in illustrations, animations or interactive simulations, with follow-up questions designed to stimulate reasoning and help students construct accurate explanations.\\n\\nThe system is strict turn-taking; the tutor presents information, asks a question and waits for the student to respond. Students wear headsets with close-talking noise-canceling microphones. To respond, the student presses the spacebar on the laptop, holds it down while speaking, and releases it when done. Each student turn is recorded as a separate audio file. When transcribed, an utterance level transcript file is created for each audio file.\\n\\n2.2. Transcription\\n\\nRoughly 45% of all utterances have been transcribed at the word level. Phase I of the project used rich (slow, expensive) transcription guidelines\u2014 the ones typically used by speech recognition researchers. However, for the purposes of this project, that level of detail was not required in the transcriptions, and during Phase II, a reduced (quick, cheaper) version of those guidelines was used, allowing transcription of more data.\\n\\n2.3. Data Composition\\n\\nSome characteristics of the data collected in the two phases are described below. Phase I comprised sessions from students in grades 3-5 across four science modules. All the sessions from this phase have been transcribed using rich transcription guidelines. Phase II comprised sessions from students in grades 4-5. It included five modules, with an average of 10 parts each. Table 1 lists the modules included in each phase. Table 2 lists the size of the corpus based on a few different parameters.\\n\\nTable 1: List of science modules in Phase I and II\\n\\n| Phase | Module Description |\\n|-------|--------------------|\\n| I | MS - Mixtures and Solutions |\\n| | ME - Magnetism and Electricity |\\n| | VB - Variables |\\n| | WA - Water |\\n| II | EE - Energy and Electromagnetism |\\n| | LS - Living Systems |\\n| | MX - Mixtures |\\n| | SRL - Soil, Rocks and Landforms |\\n| | SMP - Sun, Moon and Planets |\\n\\nTable 2: Size of MyST corpus.\\n\\n| Description | Quantity |\\n|-------------|----------|\\n| Count (Hours) | |\\n| Phase I | |\\n| Number of Students | 421 |\\n| Number of Sessions | 1509 (102) |\\n| Transcribed Sessions | 1509 (102) |\\n| Untranscribed Sessions | 0 (0) |\\n| Phase II | |\\n| Number of Students | 950 |\\n| Number of Sessions | 8987 (102) |\\n| Transcribed Sessions | 1426 (102) |\\n| Untranscribed Sessions | 3711 (0) |\\n\\n2.4. Corpus Structure\\n\\nThe directory structure for the corpus is as shown in Figure 1 below. Variables are enclosed in angle brackets.\\n\\n---\\n\\n5 [https://cemantix.org/myst/phase-i-guidelines.pdf]\\n6 [https://cemantix.org/myst/phase-ii-guidelines.pdf]\"}"}
{"id": "lrec-2024-main-1052", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The MyST Corpus Structure.\\n\\n- `<variable>` in brackets can take values as described immediately after.\\n- `<partition>` is one of `train`, `development`, or `test`.\\n- `<student_id>` is a 6-digit ID with the first 3 digits representing the school code and the next 3 digits the student number.\\n- `<session_id>` is the ID for a particular session and is further represented as `<corpus>_` `<student_id>_<date>_<time>_` `<module>_<investigation>_<part>`.\\n- `<date>` is represented as `<YYYY>-<MM>-<DD>`.\\n- `<time>` is represented as `<hh>-<mm>-<ss>`, where `<hh>`, `<mm>`, and `<ss>` represent two digit hour, minutes, and seconds respectively.\\n- `<module>` is a two- or three-character string enumerated in Table 1 earlier.\\n- `<investigation>` is a decimal number representing the respective investigation for a module.\\n- `<part>` is the utterance ID within a session. Numbers 001 onward represent the index of each utterance in a session.\\n- `<file-extension>` is one of `.flac` or `.trn`. `.flac` is the compressed audio file and `.trn` is the transcription of the corresponding audio file.\\n\\n3. Data Cleanup and Pre-processing\\n\\nWe did a pass over the corpus to clean up various types of errors that could be identified using statistics on the underlying audio and potentially erroneous data collection.\\n\\n3.1. Session Quality\\n\\n- Bad\u2014empty or corrupted sessions were removed using simple heuristics and based on missing data.\\n- Session Length:\\n  - Sessions that were less than a certain minimal threshold (< 10 minutes), or longer than a certain maximum threshold (> 1 hour) were inspected and corrected or removed.\\n- Missing audio files: Sessions that were missing audio files for a significant number of utterances were deleted.\\n\\n3.2. Audio Quality\\n\\nAll utterances were processed to identify all possible unacceptable recordings and were removed from the database. We performed the following checks for audio quality.\\n\\n- Clipping Rate:\\n  - If there was a significant number of clipped frames, we removed or marked the audio file. We removed the entire session from the release if this number affected a significant fraction of utterances in a session. If only a small number of files had large fraction of clipping, we tagged them as such as part of the session metadata, so that end users can determine whether to include or exclude that data from their study.\\n- Silence:\\n  - Sometimes there are significant amounts of leading and trailing silence in the audio files. We trimmed all such silence except for a small fraction at the beginning and end of the utterance. We did not, however, remove or compress silence that occurred within an utterance.\\n- Background Noise:\\n  - Utterances with a significant amount of noise or cross talk were removed. This was only possible for the cases that were transcribed or were part of a sample that we manually verified.\\n\\n3.3. Transcription Quality\\n\\nWe fixed obvious spelling errors in the transcriptions. We tried to retain explicitly mispronounced words as much as possible.\\n\\n3.4. Updated Pronunciation Dictionary\\n\\nWe also make available an updated pronunciation dictionary. We used CMU's pronunciation dictionary as a starting point and added words that were novel to this corpus. The additions made to the pronunciation dictionary are made available is part of the corpus release.\"}"}
{"id": "lrec-2024-main-1052", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Evaluation\\n\\nIn order to promote reproducible, fair and balanced evaluation of automatic speech recognition (ASR) models using this corpus, we partitioned and structured the corpus upfront into train, development and test sets.\\n\\n4.1. Identifying Partitions\\n\\nThese partitions were identified using stratified sampling strategy thus ensuring that they reasonably represent each of the science module in MyST, proportionately represent each phase, and each student is present in only one of the three partitions. We also included untranscribed data in all partitions in order to be able to allow limited semi-supervised training data augmentation using the untranscribed portions of the data, with an additional advantage of pseudo-unseen data\u2014in the form of transcriptions that are as yet absent.\\n\\nTable 3: Distribution of modules and speech across the experimental partitions.\\n\\n| Phase | Science | Partition | Overall |\\n|-------|---------|-----------|---------|\\n|       |         | (Hrs.)    | (Hrs.)  | (Hrs.)  | (Hrs.)  |\\n| I     | MS      | 31        | 5       | 5       | 41      |\\n|       | ME      | 30        | 4       | 4       | 38      |\\n|       | VB      | 14        | 2       | 2       | 18      |\\n|       | WA      | 4         | 1       | 1       | 6       |\\n| II    | EE      | 114       | 16      | 14      | 144     |\\n|       | LS      | 75        | 4       | 4       | 83      |\\n|       | MX      | 29        | 5       | 7       | 41      |\\n|       | SRL     | 16        | 2       | 1       | 19      |\\n|       | SMP     | 2         | 1       | 1       | 4       |\\n|       | Overall | 315       | 40      | 39      | 393     |\\n\\n4.2. Experimental Setup\\n\\nWe used SpeechBrain (Ravanelli et al., 2021) speech toolkit for our experiments. More specifically, we used an end-to-end transformer model. We fine-tuned the model pre-trained on LibriSpeech model, using the MyST training set. Owing to memory limitations, we were only able to use utterances less than 30 secs. during training.\\n\\n4.3. Word Error Rate\\n\\nWe use the traditional evaluation metric of word error rate (WER) to report ASR performance. In spite of several quality checks, an initial release of the corpus through Linguistic Data Consortium (LDC2021S05) contained some transcription errors. We corrected the transcription errors occurring in the test and use that to report the baseline WER in this article. Given the improvements in ASR models in the recent past, we were able to use some heuristics to identify utterances with errorful transcriptions from the training and development set. The number of utterances with transcription errors was a small fraction of the total transcribed utterances. We trained the ASR model using the training and development set after filtering out those utterances. Given the small fraction of utterances with errorful transcriptions, we did not see a noticeable difference between the WER using models trained before and after filtering respectively. Table 4 shows the WER on the corrected test set transcriptions. We plan to make another quality control pass through the corpus to correct residual errors in the development and training set and release an updated version of the corpus in the near future.\\n\\n4.4. Replicability\\n\\nWe understand the importance of ensuring that the research community can replicate and monitor the performance improvements on this data over time. In order to facilitate that, we are making available the data, the evaluation setup\u2014the model architecture, WER evaluation program and all relevant configuration\u2014in a git repository of the corpus.\\n\\nTable 4: Word Error Rate on the MyST test set using a model trained only on the training and development partitions.\\n\\n| MyST Test Set | WER (%) | Insertions (% | Substitutions (%) | Deletions (%) |\\n|---------------|---------|--------------|------------------|--------------|\\n|               | 10.0    | 2.9          | 5.1              | 3.2          |\\n\\n5. Related Work\\n\\nOver the years researchers have created several speech corpora for the analysis of children's speech. Below are a few that are typically used for ASR evaluation. A thorough empirical evaluation of various end-to-end ASR systems specifically focused on children's speech was recently reported in Shivakumar and Narayanan (2022). They used a pre-release of this corpus in their study which did not contain the current experimental partitions, so their evaluation numbers are not directly comparable.\\n\\n- CID children's speech corpus (American English, readspeech, 436 children aged between 5 and 17 years) (Lee et al., 1999)\"}"}
{"id": "lrec-2024-main-1052", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 CMU Kid\u2019s speech corpus (American English, read speech, 76 children, aged between 6 and 11 years) (Eskenazi, 1996)\\n\u2022 CU Kid\u2019s Prompted and Read Speech corpus (American English, read speech, 663 children, aged between 4 and 11 years) (Cole et al., 2006),\\n\u2022 CU Kid\u2019s Read and Summarized Story corpus (American English, spontaneous speech, 326 children, aged between 6 and 11 years) (Cole and Pellom, 2006),\\n\u2022 OGI Kid\u2019s speech corpus (English, read speech, 1100 children, aged between 5 and 15 years) (Shobaki et al., 2000).\\n\\n6. Conclusion and Future Work\\n\\nIn this work we describe a large corpus of conversational children\u2019s speech and present a baseline WER using a state of the art ASR system. We are making this corpus freely available for research through a git repository. This should make it easier for users to identify and propose corrections to any residual transcription errors. With the help of sponsors and volunteers from the larger research community, we hope to manually transcribe the untranscribed utterances. We recommend that future users use this data repository as the definitive version of the corpus and the relevant documentation.\\n\\nIn spite of an exponentially large collection of data at our finger tips, it is difficult to get access to a reasonably large collection of specific kinds needed to train accurate end-to-end machine learning models. In this case that is children\u2019s conversations. One of the larger models for reporting performance on children\u2019s speech (Liao et al., 2015) used roughly 20K training utterances. However, the data underlying for that study is not generally available and the work cannot be replicated. Our hope is that the large MyST corpus of children\u2019s conversational speech will allow researchers to improve upon a consistent evaluation benchmark. Improvements in automatic transcription of children\u2019s conversations can open doors to transformational applications in various domains. Improved applications for use in education and in healthcare have the potential of making a significant global impact.\\n\\n7. Ethics Statement\\n\\nThe University of Colorado\u2019s Institutional Review Board reviewed and approved all components of the My Science Tutor (MyST) project to assure student privacy. The review board approved the Parental Consent forms and the Student Assent forms. The final Parental Consent and Student Assent forms approved by the IRB explicitly provide permission for the distribution of anonymous student speech data and transcriptions. We manually verified that we had parental consent and student assent for every student in the released corpus. No identifying information is stored with the data. All school codes and student IDs were anonymized.\\n\\n8. Bibliographical References\\n\\nM. Chi, M. Bassok, M. Lewis, P. Reimann, R. Glaser, and Alexander. 1989. Self-explanations: How students study and use examples in learning to solve problems. *Cognitive Science*, 13(2).\\n\\nM. Chi, N. De Leeuw, M. Chiu, and C. LaVancher. 1994. Eliciting self-explanations improves understanding. *Cognitive Science*, 18(3):439\u2013477.\\n\\nM.T.H.Chi,S.A.Siler,H.Jeong,T.Yamauchi,and R. G. Hausmann. 2001. Learning from human tutoring. *Cognitive Science*, 25(4):471\u2013533.\\n\\nR. Cole, J. P. Hosom, and B. Pellom. 2006. University of Colorado prompted and read children\u2019s speech corpus. Technical report.\\n\\nR.ColeandB.Pellom.2006. Universityofcolorado read and summarized stories corpus. Technical report.\\n\\nShona D\u2019Arcy and Martin Russell. 2005. A comparison of human and computer recognition accuracy for children\u2019s speech. In Proc. Interspeech 2005, pages 2197\u20132200.\\n\\nMaxine S Eskenazi. 1996. Kids: a database of children\u2019s speech. Ph.D. thesis, Acoustical Society of America.\\n\\nR. G. M. Hausmann and K. VanLehn. 2007a. Explaining self-explaining: A contrast between content and generation. *Artificial Intelligence in Education*, pages 417\u2013424.\\n\\nR. G. M. Hausmann and K. VanLehn. 2007b. Self-explaining in the classroom: Learning curve evidence. In 29th Annual Conference of the Cognitive Science Society, Mahwah, NJ.\\n\\nSungbok Lee, Alexandros Potamianos, and Shrikanth Narayanan. 1999. Acoustics of children\u2019s speech: Developmental changes of temporal and spectral parameters. *The Journal of the Acoustical Society of America*, 105(3):1455\u20131468.\\n\\nHank Liao, Golan Pundak, Olivier Siohan, Melissa Carroll, Noah Coccaro, Qi-Ming Jiang, Tara N Sainath, Andrew Senior, Fran\u00e7oise Beaufays,\"}"}
{"id": "lrec-2024-main-1052", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and Michiel Bacchiani. 2015. Large vocabulary automatic speech recognition for children.\\n\\nNAEP. 2011. The Nation's Report Card: Science 2009. Institute of Education Sciences, U.S. Department of Education, Washington, D.C.\\n\\nNAEP. 2021. The Nation's Report Card: Science 2019. Institute of Education Sciences, U.S. Department of Education, Washington, D.C.\\n\\nMirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, et al. 2021. Speech-brain: A general-purpose speech toolkit. arXiv preprint arXiv:2106.04624.\\n\\nPrashanth Gurunath Shivakumar and Shrikanth Narayanan. 2022. End-to-end neural systems for automatic children speech recognition: An empirical study. Computer Speech & Language, 72:101289.\\n\\nKhaldoun Shobaki, John-Paul Hosom, and Ronald Cole. 2000. The ogi kids' speech corpus and recognizers. In Proc. of ICSLP, pages 564\u2013567. Citeseer.\\n\\nW. Ward, R. Cole, D. Bolanos, C. Buchenroth-Martin, E. Svirsky, S. V. Vuuren, and L. Becker. 2011. My science tutor: A conversational multimedia virtual tutor for elementary school science. ACM Trans. Speech Lang. Process., 7(4).\\n\\nWayne Ward, Ron Cole, Daniel Bolanos, C. Buchenroth-Martin, E. Svirsky, and Tim Weston. 2013. My science tutor: A conversational multimedia virtual tutor. Journal of Educational Psychology, 105(4):1115\u20131125.\"}"}
