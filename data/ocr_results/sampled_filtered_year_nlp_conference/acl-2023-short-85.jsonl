{"id": "acl-2023-short-85", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\"}"}
{"id": "acl-2023-short-85", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We focus on four Nigerian languages from three different language families. Hausa (hau) is from the Afro-Asiatic/Chadic family spoken by over 77 million people. Igbo (ibo) and Yor\u00f9b\u00e1 (yor) are both from Niger-Congo/Volta-Niger family spoken by 30M and 46M respectively. While Nigerian-Pidgin (pcm) is from the English Creole family, spoken by over 120M people. The Nigerian-Pidgin is ranked the 14th most spoken language in the world. All languages make use of the Latin script. Except for Nigerian-Pidgin, the remaining are tonal languages. Also, Igbo and Yor\u00f9b\u00e1 make extensive use of diacritics in texts which are essential for the correct pronunciation of words and for reducing ambiguity in understanding their meanings.\\n\\nB. Hyper-parameters for PLMs\\n\\nFor fine-tuning PLMs, we make use of HuggingFace transformers (Wolf et al., 2019). We make use of maximum sequence length of 200, batch size of 32, number of epochs of 20, and learning rate of $5 \\\\times 10^{-5}$ for all PLMs.\\n\\nC. Human Evaluation\\n\\nTo verify the performance of the MT model, we hire at least two native speakers of each Nigerian indigenous languages - three native Igbo speakers, four native Yor\u00f9b\u00e1 speakers, four native speakers of Nigerian Pidgin and two Hausa native speakers. The annotators were individually given 100 randomly selected translated reviews in Excel sheets to report the adequacy and sentiment preservation of the MT outputs. Alongside the sheets, the annotators are given an annotation guideline to guide them during the course of the annotation. Asides that the annotators are of the Nigerian descent as well as native speakers of the selected languages, their minimum educational experience is a bachelor's degree which qualifies them to efficiently read, write and comprehend the annotation materials and data to be annotated.\\n\\nTo measure the consistency of our annotators, we added repeated 5 examples out of the 100 examples. Our annotators were consistent with their annotation. We measure the inter-agreement among the two annotators per task. For adequacy, the annotators achieved Krippendorff's alpha scores of 0.675, 0.443, 0.41, 0.65 for Hausa, Igbo, Nigerian-Pidgin, and Yor\u00f9b\u00e1 respectively. Similarly, for sentiment preservation, Krippendorff's alpha scores of 1.0, 0.93, 0.48, and 0.52 for Hausa, Igbo, Nigerian-Pidgin, and Yor\u00f9b\u00e1 respectively. In general, annotators reviewed the translated texts to have adequacy of 3.8 and 4.6. Nigerian-Pidgin (4.6) achieved better adequacy result as shown in Table 5 because of her closeness to English language, Igbo was rated to have a lower adequacy score (3.8). Overall, all annotators rated the translated sentences to preserve sentiment at least in 90% of the time i.e 90 out of 100 translations preserve the original sentiment in the English sentence.\\n\\nC.1 Qualitative analysis\\n\\nThe human evaluation is to verify the manually verify the quality of over 100 randomly selected translated sentences manually. Also, the reports from the annotators were automatically computed to support our claim that sentiment is usually preserved in MT outputs. The examples listed in Table 6 are extracted during the annotation process. The examples illustrate the noticeable mistakes in MT outputs. The annotators are expected to give a rating scale between 1-5 if the randomly selected machine translated review is adequately translated and a binary 0-1 rating scale if the sentiment of the original review is retained in the randomly selected machine translated review.\\n\\nThe examples that are listed in Table 6 buttress our claim that MT outputs are not completely accurate as some translations in the target languages are missing thereby affecting the complete idea and meaning of the movie review that is originally...\"}"}
{"id": "acl-2023-short-85", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the absence of such a perfect storm, avoid stabbing your wallet in the heart with this 'Dagger'.\\n\\nIn the absence of a great storm, do not use this \\\"Dagger\\\" to kill your money in the heart.\\n\\nCitation the movie. Perfect Movie.\\n\\nWished it didn't end.\\n\\nI enjoyed every second that I used to make this movie. Wished it did not end.\\n\\nFunny Funny Funny. Oh mehn, this movie is super funny. if you are looking for a movie to lift your mood up then this is the right movie for you.\\n\\nFifty minutes is spent advertising a holiday resort in Lagos, Movie closes. Money down the drain.\\n\\nI thoroughly enjoyed the layers that the story had and the way that each key piece of information was revealed.\\n\\nThe only thing that I don't like about this movie is the way there was little or no interaction with the Nigerian or Indian environment.\\n\\nNice cross-country movie. The only thing that I don't like about this movie is the way there was little or no interaction with the Nigerian or Indian environment.\\n\\nBeautiful romantic movie.\\n\\nThe only thing wey I no like about this film na because e no too get interaction with Nigerian or Indian people.\\n\\nA flawed first feature film, but it shows a great deal of promise.\\n\\nIt is almost every minute of the 2hours 30minutes that they play African movie they play.\\n\\nTable 6: Examples of translation mistakes observed and impact on the sentiment. The Gray color identifies the sentiment portion of the review.\"}"}
{"id": "acl-2023-short-85", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"written in English, which eventually could lead to losing the sentiment of the movie review. Also, as shown in Table 6, the sentiments of some reviews are preserved regardless of the incorrect or missing translations and the idea or meaning of the review is not totally lost.\\n\\nC.2 Annotation Guideline\\nWe provide the annotation guideline on Github.\"}"}
{"id": "acl-2023-short-85", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\n6 (Limitation)\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n6 (Ethics Statement)\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\nAbstract; 1 - Introduction\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\nLeft blank.\\n\\nB \u25a1 Did you use or create scientific artifacts?\\n3\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n3, 4, 5\\n\u25a1 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\\n3\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n3, 5, 6 (Ethics Statement)\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\n6 (Ethics Statement)\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n3\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n3\\n\\nC \u25a1 Did you run computational experiments?\\n4, 5\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n4, 5\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-short-85", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification\\n\\nIyanuoluwa Shode\u2020\\nDavid Ifeoluwa Adelani\u2021\\nJing Peng\u2020\\nAnna Feldman\u2020\\n\\n\u2020Montclair State University, USA, and\\n\u2021University College London, United Kingdom\\n\\n{shodei1,pengj,feldmana}@montclair.edu, d.adelani@ucl.ac.uk\\n\\nAbstract\\nAfrica has over 2000 indigenous languages but they are under-represented in NLP research due to lack of datasets. In recent years, there have been progress in developing labelled corpora for African languages. However, they are often available in a single domain and may not generalize to other domains. In this paper, we focus on the task of sentiment classification for cross-domain adaptation. We create a new dataset, NollySenti\u2014based on the Nollywood movie reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo, Nigerian-Pidgin, and Yor\u00f9b\u00e1). We provide an extensive empirical evaluation using classical machine learning methods and pre-trained language models. Leveraging transfer learning, we compare the performance of cross-domain adaptation from Twitter domain, and cross-lingual adaptation from English language. Our evaluation shows that transfer from English in the same target domain leads to more than 5% improvement in accuracy compared to transfer from Twitter in the same language. To further mitigate the domain difference, we leverage machine translation (MT) from English to other Nigerian languages, which leads to a further improvement of 7% over cross-lingual evaluation. While MT to low-resource languages are often of low quality, through human evaluation, we show that most of the translated sentences preserve the sentiment of the original English reviews.\\n\\n1 Introduction\\nNigeria is the sixth most populous country in the world and the most populous in Africa with over 500 languages (Eberhard et al., 2021). These languages are spoken by millions of speakers, and the four most spoken indigenous languages (Hausa, Igbo, Nigerian-Pidgin (Naija), and Yor\u00f9b\u00e1) have more than 25 million speakers but they are still under-represented in NLP research (Adebara and Abdul-Mageed, 2022; van Esch et al., 2022). The development of NLP for Nigerian languages and other African languages is often limited by a lack of labelled datasets (Adelani et al., 2021b; Joshi et al., 2020). While there have been some progress in recent years (Eiselen, 2016; Adelani et al., 2022b; NLLB-Team et al., 2022; Muhammad et al., 2023; Adelani et al., 2023), most benchmark datasets for African languages are only available in a single domain, and may not transfer well to other target domains of interest (Adelani et al., 2021a).\\n\\nOne of the most popular NLP tasks is sentiment analysis. In many high-resource languages like English, sentiment analysis datasets are available across several domains like social media posts/tweets (Rosenthal et al., 2017), product reviews (Zhang et al., 2015; He and McAuley, 2016) and movie reviews (Pang and Lee, 2005; Maas et al., 2011). However, for Nigerian languages, the only available dataset is NaijaSenti (Muhammad et al., 2022) - a Twitter sentiment classification dataset for four most-spoken Nigerian languages. It is unclear how it transfers to other domains.\\n\\nIn this paper, we focus on the task of sentiment classification for cross-domain adaptation. We create the first sentiment classification dataset for Nollywood movie reviews known as NollySenti\u2014a dataset for five widely spoken Nigerian languages (English, Hausa, Igbo, Nigerian-Pidgin, and Yor\u00f9b\u00e1). Nollywood is the home for Nigerian movies that depict the Nigerian people and reflect the diversities across Nigerian cultures. Our choice of this domain is because Nollywood is the second-largest movie and film industry in the world by annual output, and the availability of Nollywood reviews on several online websites. However, most of these online reviews are only in English. To cover more languages, we asked professional translators to translate about 1,000-1,500 reviews.\"}"}
{"id": "acl-2023-short-85", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from English to four Nigerian languages, similar to\\nWinata et al. (2023). Thus, NollySenti is a parallel\\nmultilingual sentiment corpus for five Nigerian\\nlanguages that can be used for both sentiment clas-\\nsification and evaluation of machine translation\\n(MT) models in the user-generated texts domain \u2014\\nwhich is often scarce for low-resource languages.\\n\\nAdditionally, we provide several supervised and\\ntransfer learning experiments using classical ma-\\ntime learning methods and pre-trained language\\nmodels. By leveraging transfer learning, we com-\\npare the performance of cross-domain adaptation\\nfrom the Twitter domain to the Movie domain, and\\ncross-lingual adaptation from English language.\\nOur evaluation shows that transfer from English\\nin the same target domain leads to more than 5%\\nimprovement in accuracy compared to transfer\\nfrom the Twitter domain in the same target lan-\\nguage. To further mitigate the domain difference,\\nwe leverage MT from English to other Nigerian\\nlanguages, which leads to a further improvement\\nof 7% over cross-lingual evaluation. While MT to\\nlow-resource languages are often of low quality,\\nthrough human evaluation, we show that most of\\nthe translated sentences preserve the sentiment in\\nthe original English reviews. For reproducibility,\\nwe have released our datasets and code on Github\\n3.\\n\\n2 Related Work\\n\\nAfrican sentiment datasets\\nThere are only a\\nfew sentiment classification datasets for African\\nlanguages such as Amharic dataset (Yimam et al.,\\n2020), and NaijaSenti (Muhammad et al., 2022)\u2014\\nfor Hausa, Igbo, Nigerian-Pidgin, and Yor\u00f9b\u00e1. Re-\\ncently, Muhammad et al. (2023) expanded the senti-\\ntment classification dataset to 14 African languages.\\nHowever, all these datasets belong to the social\\nmedia or Twitter domain. In this work, we cre-\\nate a new dataset for the Movie domain based on\\nhuman translation from English to Nigerian lan-\\nguages, similar to the NusaX parallel sentiment\\ncorpus for 10 Indonesia languages (Winata et al.,\\n2023).\\n\\nMT for sentiment classification\\nIn the absence\\nof training data, MT models can be used to trans-\\nlate texts from a high-resource language like En-\\nglish to other languages, but they often introduce\\nerrors that may lead to poor performance (Refaee\\nand Rieser, 2015; Poncelas et al., 2020). However,\\nthey do have a lot of potentials especially when\\ntranslating between high-resource languages like\\nEuropean languages, especially when combined\\nwith English (Balahur and Turchi, 2012, 2013). In\\nthis paper, we extend MT for sentiment classifica-\\ntion to four low-resource Nigerian languages. This\\npaper is an extension of the YOSM paper (Shode\\net al., 2022) \u2013 A Yor\u00f9b\u00e1 movie sentiment corpus.\\n\\n3 Languages and Data\\n\\n3.1 Focus Languages\\nWe focus on four Nigerian languages from three\\ndifferent language families spoken by 30M-120M.\\nHausa\\nbelongs to the Afro-Asiatic/Chadic lan-\\nguage family with over 77 million speakers (Eber-\\nhard et al., 2021). It is a native to Nigeria, Niger,\\nChad, Cameroon, Benin, Ghana, Togo, and Su-\\ndan. However, the significant population for the\\nlanguage reside in northern Nigeria. Hausa is an\\nagglutinative language in terms of morphology and\\ntonal with two tones \u2014 low and high. It is written\\nwith two major scripts: Ajami (an Arabic-based\\nscript) and Boko script (based on Latin script) \u2014\\nthe most widely used. The Boko script make use\\nof all the Latin letters except for \\\"p,q,v, and x\\\" in-\\ncluding the following additional letters \\\"\u00e1,\\n\u00e2,\\n\u00ce,\\n\u00af,\\nkw,\\n\u00cew, gw, ky,\\n\u00cey, gy, sh, and ts\\\".\\nIgbo\\nbelongs to the V olta\u2013Niger sub-group of the\\nNiger-Congo language family with over 31 million\\nspeakers (Eberhard et al., 2021). It is native lan-\\nguage to South-Eastern Nigeria, but also spoken in\\nCameroon and Equatorial Guinea in Central Africa.\\nIgbo is an agglutinative language in terms of its\\nsentence morphology and tonal with two tones \u2014\\nhigh and low. The language utilizes 34 Latin letters\\nexcluding \\\"c,q and x\\\", however, it includes addi-\\ntional letters \\\"ch, gb, gh, gw, kp, kw, nw, ny,o,\\n\u02d9o,\\nu. and sh\\\".\\nNigerian-Pidgin aka Naija\\nis from the English\\nCreole Atlantic Krio language family with over\\n4 million native speakers and 116 million people\\nsecond language speakers. It is a broken version\\nof Nigerian English that is also a creole because\\nit is used as a first language in certain ethnic com-\\nmunities (Mazzoli, 2021). It serves as a common\\nlanguage for all as it facilitates communication\\nbetween several ethnicities. Naija has 26 letters\\nsimilar to English with an analytical sentence mor-\\nphology.\"}"}
{"id": "acl-2023-short-85", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yor\u00f9b\u00e1 belongs to the Volta\u2013Niger branch of the Niger-Congo language family with over 50 million speakers (Eberhard et al., 2021) thus making it the third most spoken indigenous African language. Yor\u00f9b\u00e1 is native to South-Western Nigeria, Benin and Togo, and widely spoken across West Africa and Southern America like Sierra Leone, C\u00f4te d\u2019Ivoire, The Gambia, Cuba, Brazil, and some Caribbean countries. Yor\u00f9b\u00e1 is an isolating language in terms of its sentence morphology and tonal with three lexical tones - high, mid and low that are usually marked by diacritics which are used on syllabic nasals and vowels. Yor\u00f9b\u00e1 orthography comprises 25 Latin letters which excludes \u201cc, q, v, x, and z\u201d but includes additional letters \u201cgb, e., s. and o.\u201d.\\n\\n3.2 NollySenti creation\\n\\nUnlike Hollywood movies that are heavily reviewed with hundreds of thousands of reviews all over the internet, there are fewer reviews about Nigerian movies despite their popularity. Furthermore, there is no online platform dedicated to writing or collecting movie reviews written in the four indigenous Nigerian languages. We only found reviews in English. Here, we describe the data source for the Nollywood reviews and how we created parallel review datasets for four Nigerian languages.\\n\\n3.2.1 Data Source\\n\\nTable 1 shows the data source for the NollySenti review dataset. We collected 1,018 positive reviews (POS) and 882 negative reviews (NEG). These reviews were accompanied with ratings and were sourced from three popular online movie review platforms - IMDB, Rotten Tomatoes and Letterboxd. We also collected reviews and ratings from four Nigerian websites like Cinemapointer, Nollyrated. Our annotation focused on the classification of the reviews based on the ratings that the movie reviewer gave the movie. We used a rating scale to classify the POS or NEG reviews and defined ratings between 0-4 to be in the NEG category and 7-10 as POS.\\n\\n3.2.2 Human Translation\\n\\nWe hire professional translators in Nigeria and ask them to translate 1,010 reviews randomly chosen from the 1,900 English reviews. Thus, we have a parallel review dataset in English and other Nigerian languages and their corresponding ratings. For quality control, we ask a native speaker per language to manually verify the quality of over 100 randomly selected translated sentences, and we confirm that they are good translations, and they are not output of Google Translate (GT).\\n\\n4 Experimental Setup\\n\\nData Split\\n\\nTable 2 shows the data split into Train, Dev and Test splits. They are 410/100/500 for hau, ibo and pcm. To further experiment with the benefit of adding more reviews, we translate 490 more reviews for yor. The ratio split for yor is 900/100/500, while for eng is 1,300/100/500. We make use of the same reviews for Dev and Test for all languages. For our experiments of transfer learning and machine translation, we make use of all the training reviews for English (i.e 1,300). We make use of a larger test set (i.e. 500 reviews) for hau, ibo and pcm because the focus of our analysis is on zero-shot transfer, we followed similar data split as XCOPA (Ponti et al., 2020), COPA-HR (Ljubesic and Lauc, 2021) and NusaX datasets. The small training examples used in NollySenti provides an opportunity for researchers to develop more data efficient cross-lingual methods for under-resourced languages since this is a more realistic scenario.\\n\\n4.1 Baseline Models\\n\\nHere, we train sentiment models using classical machine learning models like Logistic regression and Support Vector Machine (SVM) and fine-tune several pre-trained language models (PLMs). Unlike classical ML methods, PLMs can be used for cross-lingual transfer and often achieve better results (Devlinc et al., 2019; Winata et al., 2023). We fine-tune the following PLMs: mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020), mDeBERTaV3 (He et al., 2021), AfriBERTa (Ogueji et al., 2021), and AfroXLMR (Alabi et al., 2022). The last two PLMs have been pre-trained or adapted to all the focus languages. For XLM-R and AfroXLMR, we make use of the base versions. The classical ML methods were implemented using Scikit-Learn (Pedregosa et al., 2011). Appendix B provides more details.\\n\\nEasy to verify for languages with diacritics like Yor\u00f9b\u00e1 since GT ignores diacritics. GT does not support Naija.\\n\\n$450 per language except for yor with more reviews.\"}"}
{"id": "acl-2023-short-85", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Data source, number of movie reviews per source, and average length of reviews\\n\\n| Source      | IMDB | Rotten Tomatoes | LetterBoxd | Cinemapoint | Nollyrated | Others |\\n|-------------|------|-----------------|------------|-------------|------------|--------|\\n| Sentiment   |      |                 |            |             |            |        |\\n| Positive    | 1018 | 35.0            | 493        | 107         | 81         | 154    |\\n| Negative    | 882  | 20.7            | 292        | 140         | 101        | 269    |\\n| Total       | 1900 |                 | 785        | 247         | 182        | 423    |\\n\\nTable 2: Dataset split.\\nThe DEV and TEST split have equal number samples in positive and negative classes.\\n\\n4.2 Zero-shot Adaptation\\n\\n4.2.1 Transfer Learning\\n\\nCross-domain adaptation\\nWe train on the Twitter domain and perform cross-domain adaptation to the Nollywood movie domain. We make use of the NaijaSenti dataset for training. The datasets consist of between 12k-19k tweets for each of the Nigerian languages, 30 folds larger than our dataset.\\n\\nCross-lingual adaptation\\nWe train on two English datasets: (1) IMDB (Maas et al., 2011) \u2013 with 25,000 reviews and (2) NollySenti English with 1,300 reviews. The resulting models are evaluated on the test set of the remaining Nigerian languages.\\n\\n4.2.2 Machine Translation\\n\\nLastly, we make use of MT to mitigate the domain difference. We make use of NLLB (NLLB-Team et al., 2022) for hau, ibo, and yor languages. NLLB is a multilingual MT trained on 200 languages and dialects. It includes the three Nigerian languages except for Nigerian-Pidgin. For Nigerian-Pidgin, we make use of a pre-trained eng \u2192 pcm MT model by Adelani et al. (2022a) \u2013 trained on both religious and news domain.\\n\\n5 Results\\n\\n5.1 Baseline Results\\n\\nTable 3 provides the baseline results using both logistic regression, SVM, and several PLMs. All baselines on average have over 80% accuracy. However, in all settings (i.e. all languages and number of training samples, \\\\( N = 400,900 \\\\), and 1300), PLMs exceed the performance of classical machine learning methods by over 5\u22127%. In general, we find Africa-centric PLMs (AfriBERTa-large and AfroXLMR-base) have better accuracy than massively multilingual PLMs pre-trained on around 100 languages. Overall, AfriBERTa achieves the best result on average, but slightly worse for English and Nigerian-Pidgin (an English-based creole language) since it has not been pre-trained on the English language.\\n\\n5.2 Zero-shot Evaluation Results\\n\\nWe make use of AfriBERTa for the zero-shot evaluation since it gave the best result in Table 3 (see avg. excl. eng). Table 4 shows the zero-shot evaluation.\\n\\nPerformance of Cross-domain adaptation\\nWe obtained an impressive zero-shot result by evaluating a Twitter sentiment model (i.e. Twitter (lang)) on movie review (73.8 on average). All have over 70 except for yor.\\n\\nPerformance Cross-lingual adaptation\\nWe evaluated two sentiment models, trained on either imdb or NollySenti (eng) English reviews. Our result shows that the adaptation of imdb has similar performance as the cross-domain adaptation, while the NollySenti (eng) exceeded the performance by over +6%. The imdb model (i.e. imdb (eng)) was probably worse despite the large training size due to a slight domain difference between Hollywood reviews and Nollywood reviews \u2014 may be due to writing style and slight vocabulary difference among English dialects (Blodgett et al., 2016). An example of a review with multiple indigenous named entities including a NEG sentiment is \u201cGbarada\u2019 is a typical Idumota \u2018Yoruba film\u2019 with all the craziness that come with that sub-section of Nollywood. \u201d that may not frequently occur in Hollywood reviews. Another observation is that the performance of pcm was unsurprisingly good for both setups (84.0 to 86.2) because it is an English-based creole.\\n\\nMachine Translation improves adaptation\\nTo mitigate the domain difference, we found that by\"}"}
{"id": "acl-2023-short-85", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Baseline result using classical machine learning and pre-trained language models. We make use of the number of training examples, $N = 410$, 900, and 1300. We report accuracy. Average performed over 5 runs.\\n\\n| Model        | size   | N=410 | N=1300 | N=410 | N=410 | N=410 | N=900 | avg | avg (excl. eng) |\\n|--------------|--------|-------|--------|-------|-------|-------|-------|-----|-----------------|\\n| LogisticReg  | <20K   | 79.2  | 84.2   | 78.8  | 81.8  | 83.4  | 78.8  | 80.1| 81              |\\n| SVM          | <20K   | 79.0  | 85.2   | 79.0  | 80.6  | 83.6  | 79.7  | 81.9| 81.3 \u00b1 0.6      |\\n| mBERT        | 172M   | 90.3  | 92.6   | 80.0  | 82.4  | 89.1  | 84.8  | 87.8| 87.0 \u00b1 0.5      |\\n| XLM-R-base   | 270M   | 93.2  |    | 76.8  | 83.6  | 90.8  | 83.9  | 86.0| 86.9 \u00b1 0.5      |\\n| mDeBERTaV3   | 276M   | 94.2  | 95.1   | 83.7  | 87.1  | 91.8  | 82.2  | 87.4| 88.8 \u00b1 0.5      |\\n| AfriBERTa-large | 126M | 86.2  | 89.5   | 87.2  | 88.4  | 88.3  | 85.9  | 90.9| 88.1 \u00b1 0.3      |\\n| AfroXLMR-base | 270M | 92.3  |    | 84.2  | 85.6  | 91.0  | 83.8  | 88.4| 88.5 \u00b1 0.8      |\\n\\nTable 4: Zero-shot scenario using AfriBERTa-large: cross-domain (Twitter -> Movie), cross-lingual experiments (eng -> lang) and review generation using machine translation (en -> lang). Cross-domain = IMDB (lang), N=25k\\n\\n| Lang. BLEU | CHRF | Adequacy | sentiment preservation |\\n|-----------|------|----------|------------------------|\\n| hau       | 13.6 | 40.8     | 4.4                    |\\n| ibo       | 9.8  | 33.4     | 3.8                    |\\n| pcm       | 26.4 | 53.0     | 4.6                    |\\n| yor       | 3.5  | 16.9     | 4.0                    |\\n\\nTable 5: Automatic (N=410) and human evaluation (N=100) of the MT generated reviews from TRAIN split. Automatically translating N=410 reviews using a pre-trained MT model improved the average zero-shot performance by over +4%. With additional machine translated reviews (N=1300), the average performance improved further by +3%. Combining all translated sentences with English reviews does not seem to help. Our result is quite competitive to the supervised baseline (-1.9%). As an additional experiment, we make use of MT to translate 25k IMDB reviews, the result was slightly worse than NollySenti (lang). This further confirms the slight domain difference in the two datasets.\\n\\nSentiment is often preserved in MT translated reviews. Table 5 shows that despite the low BLEU score (<15) for hau, ibo, and yor, native speakers (two per language) rated the machine translated reviews in terms of content preservation or adequacy to be much better than average (3.8 to 4.6) for all languages on a Likert scale of 1-5. Not only does the MT models preserve content, native speakers also rated their output to preserve more sentiment (i.e. achieving at least of 90%) even for some translated texts with low adequacy ratings. Appendix C provides more details on the human evaluation and examples.\\n\\n6 Conclusion\\n\\nIn this paper, we focus on the task of sentiment classification for cross-domain adaptation. We developed a new dataset, NollySenti for five Nigerian languages. Our results show the potential of both transfer learning and MT for developing sentiment classification models for low-resource languages. As a future work, we would like to extend the creation of movie sentiment corpus to more African languages.\\n\\nLimitations\\n\\nOne of the limitations of our work is that we require some form of good performance of machine translation models to generate synthetic reviews for sentiment classification. While our approach seems to work well for some low-resource languages like yor with BLEU score of 3.53, it may not generalize to other sequence classification tasks like question answering where translation errors may be more critical.\\n\\nEthics Statement\\n\\nWe believe our work will benefit the speakers of the languages under study and the Nollywood industry. We look forward to how this dataset can be used to improve the processes of the Nollywood industry and provide data analytics on movies. We acknowledge that there maybe some bias introduced due to manually translating the dataset from English, but we do not see any potential harm in releasing this dataset. While the texts were...\"}"}
{"id": "acl-2023-short-85", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"crawled online, they do not contain personal identifying information.\\n\\nAcknowledgements\\n\\nThis material is partly based upon work supported by the National Science Foundation under Grant Numbers: 2226006, 1828199, and 1704113. We appreciate Aremu Anuoluwapo for coordinating and verifying the translation of the reviews to the Nigerian languages. We appreciate the collective efforts of the following people: Bolutife Kusimo, Oluwasijibomi Owoka, Oluchukwu Igibokwe, Boluwatife Omoshalewa Adelua, Chidinma Adimekwe, Edward Agbakoba, Ifeoluwa Shode, Mola Oyindamola, Godwin-Enwere Jefus, Emmanuel Adeyemi, Adeyemi Folusho, Shamsuddeen Hassan Muhammad, Ruqayya Nasir Iro and Maryam Sabo Abubakar for their assistance during data collection and annotation, thank you so much. David Adelani acknowledges the support of DeepMind Academic Fellowship programme. Finally, we thank the Spoken Language Systems Chair, Dietrich Klakow at Saarland University for providing GPU resources to train the models.\\n\\nReferences\\n\\nIfe Adebara and Muhammad Abdul-Mageed. 2022. Towards afrocentric NLP for African languages: Where we are and where we can go. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3814\u20133841, Dublin, Ireland. Association for Computational Linguistics.\\n\\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajuddeen Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin Leong, Michael Beukman, Shamsuddeen Muhammad, Guyo Jarso, Oreen Yousuf, Andre Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair Nasir, Benjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mohamed Ahmed, Millicent Ochieng, Aremu Anuoluwapo, Perez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne, Edwin Munkoh-Buabeng, Valenciaw Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. 2022a. A few thousand translations go a long way! leveraging pre-trained models for African news translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3053\u20133070, Seattle, United States. Association for Computational Linguistics.\\n\\nDavid Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael Beukman, Chester Palen-Michel, Constantine Lignos, Jesujoba Alabi, Shamsuddeen Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa Bukula, Rooweither Mabuya, Bonaventure F. P. Dossou, Blessing Sibanda, Happy Buzaaba, Jonathan Mukiibi, Godson Kalipe, Derguene Mbaye, Amelia Taylor, Fatoumata Kabore, Chris Chinenye Emezue, Anuoluwapo Aremu, Perez Ogayo, Catherine Gitau, Edwin Munkoh-Buabeng, Victoire Memdjokam Koagne, Allahsera Auguste Tapo, Tebogo Macucwa, Vukosi Mari-vate, Mboning Tchiaze Elvis, Tajuddeen Gwadabe, Tosin Adewumi, Orevaoghene Ahia, Joyce Nakatumba-Nabende, Neo Lerato Mokono, Ignatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa Oluwaseun Adeyemi, Gilles Quentin Hacheme, Idris Abdulmumin, Odunayo Ogundepo, Oreen Yousuf, Tatiana Moteu, and Dietrich Klakow. 2022b. MasakhaNER 2.0: Africa-centric transfer learning for named entity recognition. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4488\u20134508, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nDavid Adelani, Dana Ruiter, Jesujoba Alabi, Damilola Adebonojo, Adesina Ayeni, Ayodele Esther Awokoya, and Cristina Espa\u00f1a-Bonet. 2021a. The effect of domain and diacritics in Yoruba\u2013English neural machine translation. In Proceedings of Machine Translation Summit XVIII: Research Track, pages 61\u201375, Virtual. Association for Machine Translation in the Americas.\\n\\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D'souza, Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen H. Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Dega Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin, Tendai Marengereke, and Salomey Osei. 2021b. MasakhaNER: Named entity recognition for African languages. Transactions...\"}"}
{"id": "acl-2023-short-85", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Ifeoluwa Adelani, Marek Masiak, Israel Abebe Azime, Jesujoba Oluwadara Alabi, Atnafu Lambebo Tonja, Christine Mwase, Odunayo Ogundepo, Bonaventure F. P. Dossou, Akintunde Oladipo, Doreen Nixdorf, Chris C. Emezue, Sana Al Azzawi, Blessing K. Sibanda, Davis David, Lolwethu Ndolela, Jonathan Mukiibi, Tunde Oluwaseyi Ajayi, Tatiana Moteu Ngoli, Brian Odhiambo, Abraham Toluwase Owodunni, Nnaemeka C. Obiefuna, Shamsuddeen Hassan Muhammad, Saheed Salahudeen Abdullahi, Mesay Gemeda Yigezu, Tajuddeen Rabiu Gwadabe, Idris Abdulmumin, Mahlet Taye Bame, Oluwabusayo Olufunke Awoyomi, Iyanuoluwa Shode, Tolulope Anu Adelani, Habiba Abdulganiy Kailani, Abdul-Hakeem Omotayo, Adetola Adeeko, Afolabi Abeeb, Anuoluwapo Aremu, Olanrewaju Samuel, Clementia Siro, Wangari Kimotho, Onyekachi Raphael Ogbu, Chinedu E. Mbonu, Chiamaka Ijeoma Chukwuneke, Samuel Fanijo, Jessica Ojo, Oyinkansola F. Awosan, Tadesse Kebede Guge, Sakayo Toadoum Sari, Pamela Nyatsine, Freedmore Sidume, Oreen Yousuf, Mardiyyah Oduwole, Ussen Abre Kimanuka, Kanda Patrick Tshinu, Thina Diko, Siyanda Nxakama, Abdulmejid Tuni Johar, Sinodos Gebre, Muhidin A. Mohamed, S. A. Mohamed, Fuad Mire Hassan, Moges Ahmed Mehamed, Evrard Ngabire, and Pontus Stenetorp. 2023. MasakhaNEWS: News topic classification for african languages. ArXiv, abs/2304.09972.\"}"}
{"id": "acl-2023-short-85", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
