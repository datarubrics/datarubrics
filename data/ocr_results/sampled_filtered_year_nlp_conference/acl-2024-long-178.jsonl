{"id": "acl-2024-long-178", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given a list of claims from Multimodal Large Language Models and an image, you are required to judge whether each claim in the list by the Multimodal Large Language Model model conflicts with the image, following these rules:\\n\\n1. You must carefully judge from four aspects, including the object, attributes, scene text and fact. Here are specific descriptions of the four aspects for you to review:\\n   - \\\"Object\\\" specifically refers to whether the objects in the image exist and if the quantity of objects conflicts with the object information in the claims;\\n   - \\\"Attributes\\\" specifically refer to whether the color, position, action of objects in the image conflict with the attribute information in the claims;\\n   - \\\"Scene Text\\\" specifically refers to whether the textual information in the scene of the image conflicts with the required textual information in the claims.\\n   - \\\"Fact\\\" specifically refers to relevant factual knowledge obtained by querying a search engine. You can verify the factual accuracy of the claims based on the provided external knowledge.\\n2. You\u2019ll also receive detection results from the expert model. The object detection expert model will provide detected entity names along with their bounding box information in the image. When deriving position relationships between entity instances, try to also use the bounding boxes information, which are represented as [x1, y1, x2, y2] with floating numbers ranging from 0 to 1. These values correspond to the top left x1, top left y1, bottom right x2, and bottom right y2. The scene text expert model will provide detected specific text along with their bounding box information in the image. As long as there is a conflict between a single letter in the scene text and the text information required in the claim, it\u2019s considered a hallucination.\\n3. You must carefully judge whether the visual information in the image conflicts with each claim. If there is a conflict, the result for that statement is labeled as 'hallucination'; otherwise, it is labeled as 'non-hallucination'.\\n4. Finally, YOU MUST RETURN THE JUDGMENT RESULTS IN A DICTIONARY ACCORDING TO THE GIVEN ORDER OF THE LIST OF CLAIMS. You MUST only respond in the format as described below. DO NOT RESPOND WITH ANYTHING ELSE.\\n\\nresponse format: \\n[\\n\\\"claim1\\\":\\\"hallucination\\\", \\\"reason\\\":\\\"The reason for your judgment.\\\"\\n,\\\"claim2\\\":\\\"non-hallucination\\\", \\\"reason\\\":\\\"The reason for your judgment.\\\"\\n,\\\"claim3\\\":\\\"hallucination\\\", \\\"reason\\\":\\\"The reason for your judgment.\\\"\\n,...\\n]\\n\\n[Begin of Example ] (Image Entered)\\nHere is the object detection expert model\u2019s result:\\npeople [0.345, 0.424, 0.408, 0.509]; people [0.197, 0.44, 0.28, 0.514]; people [0.517, 0.315, 0.561, 0.401]; people [0.441, 0.356, 0.47, 0.405]; chair [0.398, 0.595, 0.637, 0.901]; chair [0.621, 0.592, 0.789, 0.889]; umbrella [0.501, 0.334, 0.968, 0.88]\\n\\nHere is the attribute detection expert model\u2019s result: none information\\nHere is the scene text recognition expert model\u2019s result: none information\\nHere is the external knowledge: none information\\nHere is the claim list:\\nclaim1: The picture shows five people swimming.\\nclaim2: On the beach, there is a chair, a umbrella, and a surfboard.\\nclaim3: The green umbrella is on the right side of the chair.\\n\\nOutput:\\n[ \\\"claim1\\\":\\\"hallucination\\\", \\\"reason\\\":\\\"The object detection expert model identified four people, not five people. Based on the image information, they might be swimming. Therefore, there's a hallucination.\\\"\\n,\\\"claim2\\\":\\\"hallucination\\\", \\\"reason\\\":\\\"According to the results of the object detection expert model and my judgment, there are two chairs and an umbrella in the picture, but there is no surfboard. Therefore, there's a hallucination.\\\"\\n,\\\"claim3\\\":\\\"non-hallucination\\\", \\\"reason\\\":\\\"Based on the positional information of the bounding boxes and my judgment, the umbrella is to the right of the chairs. The umbrella is green. Therefore, there's no hallucination.\\\"\\n,...\\n]\\n\\n[End of Example ]\\n\\n<Input>:\\n<Output>:\\nTable 7: Prompt template of hallucination verification for image-to-text generation.\"}"}
{"id": "acl-2024-long-178", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given a list of claims from human prompts, an image generated by the text-to-image model, you are required to judge whether the image conflicts with human-provided prompts, following these rules:\\n\\n1. You must carefully judge from four aspects, including the object, attributes, scene text and fact. Here are specific descriptions of the four aspects for you to review:\\n   - \\\"Object\\\" specifically refers to whether the objects in the image exist and if the quantity of objects conflicts with the object information in the claims;\\n   - \\\"Attributes\\\" specifically refer to whether the color, position, action of objects in the image conflict with the attribute information in the claims;\\n   - \\\"Scene Text\\\" specifically refers to whether the textual information in the scene of the image conflicts with the required textual information in the claims.\\n   - \\\"Fact\\\" specifically refers to relevant factual knowledge obtained by querying a search engine. You can verify the factual accuracy of the claims based on the provided external knowledge.\\n\\n2. You'll also receive detection results from the expert model. The object detection expert model will provide detected entity names along with their bounding box information in the image. When deriving position relationships between entity instances, try to also use the bounding boxes information, which are represented as [x1, y1, x2, y2] with floating numbers ranging from 0 to 1. These values correspond to the top left x1, top left y1, bottom right x2, and bottom right y2. The scene text expert model will provide detected specific text along with their bounding box information in the image. As long as there is a conflict between a single letter in the scene text and the text information required in the claim, it's considered a hallucination.\\n\\n3. You must carefully judge whether the visual information in the image conflicts with each claim. If there is a conflict, the result for that statement is labeled as 'hallucination'; otherwise, it is labeled as 'non-hallucination'.\\n\\n4. Finally, YOU MUST RETURN THE JUDGMENT RESULTS IN A DICTIONARY ACCORDING TO THE GIVEN ORDER OF THE LIST OF CLAIMS. You MUST only respond in the format as described below. DO NOT RESPOND WITH ANYTHING ELSE.\\n\\nresponse format:\\n\\n```json\\n[\\\"claim1\\\":\\\"hallucination\\\", \\\"reason\\\":\\\"The reason for your judgment.\\\", \\\"claim2\\\":\\\"non-hallucination\\\", \\\"reason\\\":\\\"The reason for your judgment.\\\", \\\"claim3\\\":\\\"hallucination\\\", \\\"reason\\\":\\\"The reason for your judgment.\\\", ...\\]\\n```\\n\\n[Begin of Example ] (Image Entered)\\n\\nHere is the object detection expert model's result:\\n\\nbasketball [0.741, 0.179, 0.848, 0.285]\\nboy [0.773, 0.299, 0.98, 0.828]\\ncar [0.001, 0.304, 0.992, 0.854]\\n\\nHere is the attribute detection expert model's result: none information\\n\\nHere is the scene text recognition expert model's result:\\n\\nworld [0.405, 0.504, 0.726, 0.7]\\n\\nHere is the external knowledge: none information\\n\\nHere is the claim list:\\n\\nclaim1: The side of the car reads 'Hello World'\\nclaim2: A boy is playing a yellow basketball beside a plant.\\n\\nOutput: \\n\\n```json\\n[\\\"claim1\\\":\\\"hallucination\\\", \\\"reason\\\":\\\"The object detection model has identified a car in the image. However, based on the detection results of the scene text expert model and my judgment, the text in the image is 'hello worlld' not 'hello world'. Therefore, there's a hallucination.\\\", \\\"claim2\\\":\\\"hallucination\\\", \\\"reason\\\":\\\"The object detection model has identified a boy and a basketball in the image. And the boy is visible in the image playing with a yellow basketball. But according to the detection results of the object detection expert model and my judgment, there's no plant. Therefore, there's a hallucination.\\\"]\\n```\\n\\n......\\n\\n[End of Example ]\"}"}
{"id": "acl-2024-long-178", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Unified Hallucination Detection for Multimodal Large Language Models\\n\\nXiang Chen\u2663\u2661, Chenxi Wang\u2660\u2661, Yida Xue\u2660\u2661, Ningyu Zhang\u2660\u2661\u2217, Xiaoyan Yang\u2662\\nQiang Li\u2662, Yue Shen\u2662, Lei Liang\u2662, Jinjie Gu\u2662, Huajun Chen\u2663\u2661\u2217\\n\\nCollege of Computer Science and Technology, Zhejiang University\\nSchool of Software Technology, Zhejiang University\\nAnt Group\\nZhejiang University-Ant Group Joint Laboratory of Knowledge Graph\\n\\n{xiang_chen,zhangningyu}@zju.edu.cn\\nhttps://www.zjukg.org/project/EasyDetect/\\n\\nAbstract\\n\\nDespite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHDD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHDD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.\\n\\n1 Introduction\\n\\nThe recent emergence of MLLMs (Ho et al., 2020; OpenAI, 2023; Durante et al., 2024) that more closely mirror human cognition and learning has unleashed unprecedented possibilities for the future of artificial general intelligence (AGI). Despite MLLMs' impressive abilities, they are susceptible to generating seemingly credible content that contradicts input data or established world knowledge, a phenomenon termed \\\"hallucination\\\"(Liu et al., 2024; Wang et al., 2023a; Huang et al., 2023c;\u2217\\n\\nCorresponding author.\\n\\nThe code can be accessed via https://github.com/zjunlp/EasyDetect, and the demonstration is available at http://easydetect.openkg.cn.\\n\\n(a) Image-to-Text\\n\\nWhich team does the athlete on the right side in the below picture belong to?\\n\\nS1[The athlete on the right side, wearing the red uniform in the image, belongs to the American soccer team Club Am\u00e9rica], S2[The scene is filled with the excitement of a soccer match.]\\n\\nS1.1: The athlete on the right side wears the red uniform.\\nS1.2: The athlete on the right side belongs to Club Am\u00e9rica.\\nS1.3: Club Am\u00e9rica is the American soccer team.\\n\\nS2.1. The scene is filled with the excitement of a soccer match.\\n\\n(b) Text-to-Image\\n\\nDetect Claims from Response\\nDetect Claims from User Query\\n\\nFigure 1: Unified multimodal hallucination detection aims to identify and detect modality-conflicting hallucinations at various levels such as object, attribute, and scene-text, as well as fact-conflicting hallucinations in both image-to-text and text-to-image generation. Our benchmark emphasizes fine-grained detection, with \\\"S1\\\" representing the segment and \\\"S1.1\\\" and \\\"S1.2\\\" denoting its corresponding claims. Tonmoy et al., 2024; Zhang et al., 2023a). These hallucinations hinder the practical deployment of MLLMs and contribute to the dissemination of misinformation. Consequently, detectors that could detect multimodal hallucinations (Yang et al., 2023) within responses from MLLMs are urgently needed to alert users to potential risks and drive the development of more reliable MLLMs.\\n\\nAlthough several works have been conducted to detect hallucinations from MLLMs(Zhou et al., 2023; Zhai et al., 2023; Li et al., 2023b; Wang et al., 2023c) or alleviate hallucinations(Xing et al., 2024; Wu et al., 2024), these efforts operate in isolation and have certain limitations when compared with the aspects illustrated in Figure 1:\\n\\n(1) Task Singularity: Current research has primarily concentrated on specific tasks, such as image captioning while neglecting that text-to-image generation, an important component of AGI, also suffers from hallucinations. (2) Hallucination Categories: Prior work has primarily focused on singular hallucination categories, failing to address the broader scope of hallucinations that can occur in MLLMs. (3) Granularity: Existing methods have limited granularity, often focusing on coarse-grained evaluations rather than fine-grained detection. (4) Cross-Modality: Most existing approaches are designed for detecting hallucinations in a single modality, failing to address the cross-modality nature of hallucinations in multimodal settings.\"}"}
{"id": "acl-2024-long-178", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"lucinations induced by MLLMs. (2) Limited Hallucination Categories: Prior studies have focused on identifying hallucinations at the object level, yet they fail to consider the prevalence of scene-text or factual inconsistencies that also frequently occur in MLLMs. (3) Incomplete Granularity: It would be more valuable to assess hallucinations at a fine-grained level, examining individual claims within a response, rather than evaluating the entire response holistically. Considering these constraints hinder rapid progress in practical hallucination detection, it raises the question: Can we develop a unified perspective for detecting hallucinations from MLLMs?\\n\\nTo further investigate this problem, we have broadened the concept of multimodal hallucination within MLLMs to a holistic framework, integrating both image-to-text generation such as Image Captioning (IC) and Visual Question Answering (VQA), as well as text-to-image-synthesis (T2I) \u2013 to align with MLLMs' capabilities of performing varied multimodal tasks. We are committed to exploring a broad spectrum of hallucinatory categories and the intricate nuances of claim-level hallucination through a lens that integrates both modality-conflicting and fact-conflicting hallucinations. Based on the outlined perspectives, We have developed the MultiModal Hallucination Detection Benchmark (MHaluBench) to assess the progress of unified multimodal hallucination detectors for MLLMs and embodied the data framework depicted in Figure 1.\\n\\nAt its core, leveraging MLLMs' inherent self-detection mechanisms to pinpoint diverse hallucinations encounters significant hurdles. We further develop a tool-augmented framework for unified hallucination detection, named UNIHD, which integrates evidence from multiple auxiliary tools through the following procedure: (1) Essential Claim Extraction involves extracting the core claims within the generated response for image-to-text generation or user queries in text-to-image generation; (2) Autonomous Tool Selection via Query Formulation prompts MLLMs (GPT-4/Gemini) to autonomously generate pertinent questions for each claim. These questions are crafted to determine the specific type of tool required for each claim and to establish the input for the tool's operation; (3) Parallel Tool Execution deploys a suite of specialized tools to operate concurrently, providing evidence from their outputs to reliably validate potential hallucinations; (4) Hallucination Verification with Rationales aggregates the collected evidence to instruct the underlying MLLM to judge whether the claim hallucinatory with rationales for explanation. We have conducted a thorough evaluation of the UNIHD framework, utilizing the underlying MLLM against the MHaluBench benchmark. Our findings underscore the effectiveness of our approach and confirm that multimodal hallucination detection remains a formidable challenge. In a nutshell, We conclude our contributions as: We propose a more unified problem setting for hallucination detection in MLLMs, encompassing a broad spectrum of multimodal tasks and hallucination categories, thus enriching the unified understanding of hallucination in MLLMs. We unveil MHaluBench, a meta-evaluation benchmark that encompasses various hallucination categories and multimodal tasks. This benchmark is equipped with fine-grained analytical features, gauging the progress of hallucination detectors. We introduce UNIHD, a task-agnostic, tool-enhanced framework for the detection of hallucinations in content produced by MLLMs. Our extensive experiments demonstrate the efficacy of this method, underscoring that MHaluBench continues to be a challenging yet vital task.\"}"}
{"id": "acl-2024-long-178", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets\\nResponse Purpose Granularity Hallucination Types Modality Scenario\\nGenerated by Object Attribute Scene Text Fact Task\\n\\nFactCC (Kryscinski et al., 2020) Synthetic Check. Sentence \u2714\\n\\nQAGS (Wang et al., 2020) Model Check. Summary \u2714\\n\\nHaluEval (Li et al., 2023a) ChatGPT Det. Response \u2714\\n\\nPOPE (Li et al., 2023b) - Eval. Response \u2714\\n\\nHaELM (Wang et al., 2023c) - Det. Response Multi. Image2Text\\n\\nAMBER (Wang et al., 2023b) - Eval. Response \u2714 \u2714 Multi. Image2Text\\n\\nMHaluBench (Ours) MMLMs Det. Res., Seg., Claim \u2714 \u2714 \u2714 \u2714 Multi. Image2Text/Text2Image\\n\\nTable 1: A comparison of benchmarks w.r.t existing fact-checking or hallucination evaluation. \u201cCheck.\u201d indicates verifying factual consistency, \u201cEval.\u201d denotes evaluating hallucinations generated by different LLMs, and its response is based on different LLMs under test, while \u201cDet.\u201d embodies the evaluation of a detector\u2019s capability in identifying hallucinations.\\n\\nsuch as incorrect objects, attributes, or scene text. An example in Figure 1 (a) includes an MLLM inaccurately describing an athlete\u2019s uniform color, showcasing an attribute-level conflict due to MLLMs\u2019 limited ability to achieve fine-grained text-image alignment.\\n\\n\u2022 Fact-Conflicting Hallucination. Outputs from MLLMs may contradict established factual knowledge. Image-to-text models can generate narratives that stray from the actual content by incorporating irrelevant facts, while text-to-image models may produce visuals that fail to reflect the factual knowledge contained in text prompts. These discrepancies underline the struggle of MLLMs to maintain factual consistency, representing a significant challenge in the domain.\\n\\nUnified Detection Problem Formulation. Unified detection of multimodal hallucination necessitates the check of each image-text pair $a = \\\\{v, x\\\\}$, wherein $v$ denotes either the visual input provided to an MLLM, or the visual output synthetic by it. Correspondingly, $x$ signifies the MLLM\u2019s generated textual response based on the $v$ or the textual user query for synthesizing $v$. Within this task, each $x$ may contain multiple claims, denoted as $\\\\{c_i\\\\}_{i=1}^n$. The objective for hallucination detectors is to assess each claim from $a$ to determine whether it is \u201challucinatory\u201d or \u201cnon-hallucinatory\u201d, providing a rationale for their judgments based on the provided definition of hallucination. Text hallucination detection from LLMs denotes a sub-case in this setting, where $v$ is null.\\n\\n3 Construction of MHaluBench\\nTo facilitate research in this area, we introduce the meta-evaluation benchmark MHaluBench, which encompasses the content from image-to-text and text-to-image generation, aiming to rigorously assess the advancements in multimodal hallucination detectors. Our benchmark has been meticulously curated to include a balanced distribution of instances across three pivotal tasks, which encompasses 200 exemplars for the task of IC200 for VQA, and an additional 220 dedicated to Text-to-Image Generation. The comparison of MHaluBench with other benchmarks is detailed in Table 1 and the statistical details are provided in Figure 3 and Figure 4.\\n\\n3.1 Hallucinatory Example Collection\\nImage-to-Text Generation. We focus on IC and VQA tasks, drawing samples from the MS-COCO 2014 validation set (Lin et al., 2014) and the TextVQA test set (Singh et al., 2019). We compile generative outputs from mPLUG (Ye et al., 2023), LLaV A (Liu et al., 2023c), and MiniGPT-4 (Zhu et al., 2023) to form the core dataset for MHaluBench. These models are representative of current leading MLLMs, characterized by their diverse content generation capabilities and a notable presence of hallucinations, as depicted in Figure 8.\\n\\nText-to-Image Generation. We source initial captions from DrawBench (Saharia et al., 2022) and T2I-CompBench (Huang et al., 2023a). These captions are augmented through ChatGPT to include more specific information such as objects, attributes, and factual details, among others. The refined caption guides the DALL-E 2 (Ramesh et al., 2022) and DALL-E 3 model (Betker et al., 2023) in producing visually detailed images.\\n\\n3.2 Segment and Claim Extraction\\nBeyond evaluating overall responses, we introduce segmentation at both the segment and claim levels for a multi-granular assessment of hallucinations, enabling more precise feedback to improve model performance (Lightman et al., 2023). We leverage ChatGPT\u2019s advanced instruction-following ability to extract detailed segments and related claims. For image-to-text tasks, we split and extract the model\u2019s textual output into segments and claims;\"}"}
{"id": "acl-2024-long-178", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The claims are fine-grained atoms extracted from the complete \\\"Query-Response\\\" pairs. For text-to-image cases, we break down user queries into fundamental intent concepts, which are subsequently regarded as claims.\\n\\n3.3 Human Annotation and Agreement.\\n\\nOur annotation criteria evaluate whether image-to-text output conflicts with the input image or world knowledge and whether text-to-image visuals conflict with claims or world knowledge. Extracted claims are labeled as hallucinatory or non-hallucinatory, with a segment deemed hallucinatory if it contains any such claim; otherwise, it is labeled non-hallucinatory. An entire response is labeled hallucinatory if it includes even one hallucinatory segment. We allocate the dataset uniformly across three annotators with graduate-level qualifications for independent categorization. Decisions in uncertain cases were initially held by individual annotators and later resolved by majority rule. Inter-annotator reliability, measured by Fleiss's Kappa ($\\\\kappa$), shows significant agreement ($\\\\kappa = 0.822$) over the full annotated dataset, indicating a high level of concordance within the range $0.80 \\\\leq \\\\kappa \\\\leq 1.00$.\\n\\n4 UNI HD: Unified Hallucination Detection Framework for MLLMs\\n\\nWe present UNI HD in Figure 5 and follow. The specific prompts are listed in Appendix A.\\n\\n4.1 Essential Claim Extraction\\n\\nTo identify fine-grained hallucinations within the response, claim extraction is a prerequisite. Following the procedure in \u00a73.2, we employ the advanced instruction-following abilities of MLLMs for efficient claim extraction. Specifically, GPT-4V/Gemini is adopted as the base LLM to efficiently derive verifiable claims from the outputs of image-to-text models (extracting each response into individual claims) and text-to-image models (deconstructing user queries into distinct claims).\\n\\n4.2 Autonomous Tool Selection Via Query Formulation\\n\\nAfter extracting essential claims from the input image-text pair $a = \\\\{v,x\\\\}$, the challenge of hallucination detection is to aptly match each claim with appropriate aspect-oriented tools. We approach this issue by assessing whether the underlying MLLMs can generate pertinent queries for a given set of claims $\\\\{c_i\\\\}_{i=1}^n$ to provide relevant input to the specific aspect-oriented tool. To facilitate this, we prompt underlying MLLMs like GPT-4V/Gemini to autonomously formulate meaningful queries. Demonstrated in Figure 5, this module yields custom queries for each claim, or \\\"none\\\" when a tool is unnecessary. For example, the framework determines that claim1 calls for the attribute-oriented question \\\"What color is the uniform of the athlete on the right side?\\\" and the object-oriented inquiry \\\"['athlete', 'uniform']\", bypassing the need for scene-text and fact-oriented tools.\\n\\n4.3 Parallel Tool Execution\\n\\nLeveraging queries autonomously generated from various perspectives, we simultaneously deploy these tools in response to the queries, gathering a comprehensive array of insights to underpin the verification of hallucinations. The specific tools employed in our framework are detailed below, selected for their ability to effectively address a wide range of multimodal hallucination scenarios:\\n\\n\u2022 Object-oriented tool: We employ the open-set object detection model Grounding DINO (Liu et al., 2023d) for capturing visual object information, crucial for detecting object-level hallucinations. For instance, inputting \\\"['athlete', 'uniform']\\\" prompts the model to return two\"}"}
{"id": "acl-2024-long-178", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The athlete on the right side, wearing the red uniform in the image, belongs to the American soccer team Club Am\u00e9rica. Therefore, there's a hallucination. Based on the scene, he should belong to the DALLAS team not Club Am\u00e9rica. The uniform of the athlete on the right indicates the team he belongs to, while the athlete on the left, wearing white uniform, and the athlete on the left wearing red uniform objects and two athlete objects, along with their normalized four-dimensional coordinates, are identified two athletes, with the athlete on the right wearing the red uniform and the athlete on the left wearing red uniform.\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023). Autonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Furthermore, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. This also helps in reducing the number of hallucinations in the data, and Self-Check (0-shot) based on the level of evidence in the data. Each claim, denoted as \\\\( c \\\\), is input into the detectors in a single batch. This operation allows the detectors to capture contextual information and its corresponding claim list.\\n\\nIn the concluding phase of our process, we subject the hallucinatory potential. In doing so, the MLLM also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of hallucinations. Moreover, U_HD is tool-agnostic, facilitating the bolstering the process of hallucination verification. We harness the Serper Google Search engine returned evidence of the specific fact-based questions. By extracting and scrutinizing the top results, we obtain a range of snippets, specific to the hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nAutonomous Tool Selection Via Query Formulation\\n\\nWe compare U_HD with two baselines, Self-Check (2-shot) based on a = {v, x} rather than only two claims. Moreover, U_HD also generates insightful explanations to elucidate the rationale behind its judgment. These responses are leveraged for hallucination verification within the same MLLMs, mirroring a self-reflect akin to (Shinn et al., 2023).\\n\\nTo validate conflicting facts, we use the Data Augmentation Layer (DALLAS) tool to augment the image with additional information, which helps in verifying the hallucinations. The specific illustration of DALLAS is tool-agnostic, facilitating the seamless integration of emerging tools and detection of halluc"}
{"id": "acl-2024-long-178", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 2: Experimental results of U\u00b2NI powered by Gemini and GPT-4V on Image-to-Text and Text-to-Image Generation.\\n\\n| LLMs       | Methods       | Levels       | Hallucinatory | Non-Hallucinatory | Average |\\n|------------|---------------|--------------|---------------|-------------------|---------|\\n|            |               |              | P  | R  | F1  | P  | R  | F1  | Acc. P | R  | F1  |\\n| Image-to-Text |               |              |    |    |     |    |    |     |         |    |     |\\n| Gemini     | Self-Check (0-shot) | Claim       | 83.17 | 42.15 | 55.95 | 55.64 | 89.48 | 68.61 | 63.34 | 69.41 | 65.82 | 62.28 |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |    |    |     |    |    |     |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |    |         |    |     |\\n|            |               |              |70%|   |   |75%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70%|   |   |70"}
{"id": "acl-2024-long-178", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"UniHD Output:\\n\\n[ ..., {\"claim3\":\"hallucination\",\"reason\":\"According to the external knowledge, Fanta originated in Germany, not America, during World War II. Therefore, there's a hallucination.\"}, ...\\n]\\n\\nSelf-Check (2-shot) Output:\\n\\n[ ..., {\\\"claim3\\\":\\\"hallucination\\\",\\\"reason\\\":\\\"The origin of Fanta being in America during World War II cannot be verified through visual information in the image.\\\", ...\\n]\\n\\nFigure 7:\\n\\nCase Study. The upper section depicts two exemplary cases where both UniHD and Self-Check (2-shot) arrive at correct judgments, with a comparative demonstration of UniHD providing explanations of superior reasonability. UniHD (a) reveals a failure case where the tool presents erroneous evidence, leading to an incorrect verification outcome. Conversely, UniHD (b) highlights a scenario where, despite the tool offering valid and correct evidence, GPT-4V persists in its original stance, resulting in a flawed verification. This indicates a potential bias towards reduced sensitivity to hallucinations.\\n\\nUniHD Empowered by GPT-4V: Superior Detection Across the Board.\\n\\nTable 2 demonstrates that UniHD, leveraging GPT-4V, consistently outperforms other baseline detectors in image-to-text and text-to-image tasks. Despite the Self-Check (2-shot) showcasing GPT-4V and Gemini's robust in-context learning, UniHD markedly exceeds its performance, emphasizing the benefits of integrating external tools for more robust evidence verification and reliable hallucination detection.\\n\\n5.3 Analysis\\n\\nWhich Type of Hallucination Can Benefit the Most from Tool Enhancement?\\n\\nFigure 6 shows that UniHD enhances the detection of scene text and factual hallucinations over Self-Check (2-shot), suggesting that GPT-4V or Gemini's inherent limitations make the evidence provided by the tool especially valuable. However, UniHD exhibits minimal improvement in identifying attribute-level hallucinations, potentially attributed to a lack of specialized tools for direct attribute detection, with self-reflection methods based on GPT-4V/Gemini proving to be relatively weak.\\n\\nExplanation Reasonability of UniHD.\\n\\nAs shown in the upper portion of Figure 7, both the fact-level hallucination \\\"Fanta originated in America during World War.\\\" and the object-level hallucination \\\"There are three bikes parked.\\\" are accurately identified by Self-Check (2-shot) and UniHD. Comparative analysis reveals that UniHD excels in synthesizing evidence to provide a more credible and compelling rationale.\\n\\nFailure Analysis of UniHD.\\n\\nAs shown in the lower part of Figure 7, we present two instances where UniHD exhibits limitations. The left case demonstrates situations where the tool either generates incorrect evidence or fails to provide useful information, leading to erroneous judgments by the MLLM. On the right, we observe cases where the MLLM maintains its initial bias despite receiving accurate evidence, resulting in incorrect decisions. These scenarios highlight areas for further research.\"}"}
{"id": "acl-2024-long-178", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Comparison of claim-level hallucination ratios across MLLMs. We randomly select a set of 20 prompts from MHaluBench for each of the IC, VQA, and T2I. Responses for these prompts are generated by each of the evaluated MLLMs.\\n\\nto enhance tool accuracy and to develop MLLMs dedicated to better hallucination detection.\\n\\nText-to-Image Hallucination vs. Image-to-Text Hallucination: Which is Easier to Detect?\\n\\nBoth baselines and the GPT-4V-enhanced UNI HD show significantly improved performance in identifying hallucinations in text-to-image content over image-to-text content. This can be traced back to the structured nature of manually written user queries for text-to-image tasks, which yield more uniform images. While image-to-text confronts the complexity of natural images with background noise and content generated by MLLMs, characterized by greater diversity and fewer constraints. Consequently, it is intuitively easier to detect discrepancies between text and corresponding images in text-to-image tasks.\\n\\nExplore UNI HD to Evaluate Hallucination of Modern MLLMs.\\n\\nWe designate UNI HD powered by GPT-4V as the golden detector to assess the frequency of hallucinations in MLLMs, including GPT-4V, and Gemini, among others. The findings illustrated in Figure 8 indicate that (1) GPT-4V exhibits the lowest claim-level hallucination ratio across most tested conditions, and (2) the hallucination-based ranking of these MLLMs is generally in agreement with established leaderboards and human evaluation, demonstrating the potential of UNI HD for evaluating hallucinations.\\n\\n6 Related Work\\n\\n6.1 Hallucinations in MLLM\\n\\nThe advent of MLLMs (OpenAI, 2023; Liu et al., 2023c; Ye et al., 2023; Zhu et al., 2023) has highlighted the issue of hallucination (Hu et al., 2024; Zhang et al., 2023b; Huang et al., 2023b; Rawte et al., 2023; Ji et al., 2023), a crucial concern impacting their dependability. Previous research has primarily focused on three areas: evaluating (Li et al., 2023b; Liu et al., 2023a; Jing et al., 2023), detecting (Wang et al., 2023c; Yang et al., 2023; Yin et al., 2023), and mitigating hallucinations (Wan et al., 2024; Liu et al., 2023b; Huang et al., 2023c; Semnani et al., 2023; Zhao et al., 2024; Leng et al., 2023; Wang et al., 2024; Deng et al., 2024).\\n\\nIn a complementary effort, HaELM (Wang et al., 2023c) scrutinizes the challenges associated with POPE (Li et al., 2023b) and suggests training a model based on simulated hallucination samples for detecting multimodal hallucinations. Diverging from prior efforts, this paper addresses a broader problem scope for hallucination detection, introducing a unified multimodal hallucination detection framework, UNI HD, along with meta-evaluation benchmarks, MHaluBench.\\n\\n6.2 Harnessing Tool Resources for LLMs\\n\\nAddressing the limitations of LLMs (Chen, 2023; Kang et al., 2024) due to their pre-training confinement, researchers have explored augmenting them with resources like knowledge bases, search engines, and external models, to expand their functionality. Notably, Schick et al. (2023); Hao et al. (2023); Qiao et al. (2023) have developed models that leverage external tools to improve performance in downstream tasks. More recently, Shen et al. (2023); Liang et al. (2023) has unveiled frameworks integrating LLMs with diverse AI models to tackle complex challenges. Building on this, researchers (Peng et al., 2023; Chen et al., 2023) have examined the utilization of external knowledge to mitigate or evaluate hallucinations in LLMs. Adapting these enhancements for MLLMs introduces unique challenges, necessitating the selection of appropriate tools for effective oversight. Our research focuses on automating the selection of functionally diverse tools to enhance multimodal hallucination detection.\\n\\n7 Conclusion\\n\\nWe introduce a unified problem formulation for multimodal hallucination detection that encompasses a diverse range of multimodal tasks and hallucination types. A fine-grained benchmark dataset, MHaluBench, is also proposed to promote this challenging direction. Alongside this, we\"}"}
{"id": "acl-2024-long-178", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"present the unified hallucination detection framework, UNIHD, capable of autonomously selecting external tools with capturing pertinent knowledge to support hallucination verification with rationales. Our experimental results indicate that UNIHD achieves better performance across both image-to-text and text-to-image generation tasks, confirming its universality and efficacy.\\n\\nLimitations\\nThis paper focuses on constructing a unified hallucination detection framework for MLLMs, dubbed UNIHD. Despite the best efforts, our paper still have some limitations.\\n\\nThe Scope of Multimodal Tasks. This paper primarily addresses the detection of multimodal hallucinations from a unified perspective, with a focus on image-to-text tasks (such as Image Captioning and VQA) and text-to-image generation tasks. Nonetheless, it is important to recognize that our framework does not yet encompass other multimodal tasks, such as video captioning, which are also susceptible to hallucinations. Moving forward, we aim to explore the possibilities of incorporating these additional domains into our UNIHD.\\n\\nLimitations of Closed-Source MLLM Pricing and Inference Speed. Our UNIHD is primarily built upon powerful closed-source models as the foundation. However, closed-source models (Liu et al., 2023c; Zhu et al., 2023; Ye et al., 2023; Bai et al., 2023) often come with a cost, which introduces operational expenses. Additionally, our UNIHD relies on several external tools to provide evidence for enhanced illusion verification, resulting in additional inference time. In the future, we will further explore training open-source dedicated illusion detection models with the tool to further improve effectiveness and reduce costs.\\n\\nThe Scope of Hallucination Categories. In our commitment to developing a comprehensive hallucination detection framework, referred to as UNIHD, for MLLMs, we have made efforts to incorporate various prevalent hallucination categories within MHaluBench and UNIHD, including object, attribute, scene-text, and factual aspects, among others. However, it is important to acknowledge that there are additional categories of hallucinations that have not been covered in our framework, as discussed in the existing literature (Zhang et al., 2023b; Wang et al., 2023a; Mishra et al., 2024; Huang et al., 2023b; Rawte et al., 2023). Moving forward, our research will expand its scope to adopt a unified approach towards a wider range of hallucination categories, to strengthen the robustness of our detection mechanisms.\\n\\nPreliminary Attempts at Tool Utilization. In our early endeavors, we have configured a dedicated tool for detecting a specific type of hallucination, exemplified by the assignment of the Grounded DINO model as the object detection tool of choice. However, it should be acknowledged that the current selection of tools may not represent the optimum choice. It remains imperative to rigorously explore which SOTA object detection models are best suited for the task of multimodal hallucination detection. This necessitates an extensive evaluation of available models to pinpoint the most effective tool that aligns with the nuances and complexities of detection objectives.\\n\\nAcknowledgement\\nWe are grateful for the API services provided by OpenAI and Google, which enabled us to process data and conduct some of our experiments. Part implementation of this work are assisted and inspired by the related hallucination toolkits including FactTool (Chern et al., 2023), Woodpecker (Yin et al., 2023), and others. We follow the same license for open-sourcing and thank them for their contributions to the community. This work also benefits from the public project of mPLUG-Owl6 https://github.com/X-PLUG/mPLUG-Owl, MiniGPT-47 https://github.com/Vision-CAIR/MiniGPT-4, LLaVA8 https://github.com/haotian-liu/LLaVA, GroundingDINO9 https://github.com/IDEA-Research/GroundingDINO, and MAERec10 https://github.com/Mountchicken/Union14M. This work was supported by the National Natural Science Foundation of China (No. 62206246, No. NSFCU23B2055, No. NSFCU19B2027), the Fundamental Research Funds for the Central Universities (226-2023-00138), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Yongjiang Talent Introduction Programme (2021A-156-G), and Information Technology Center and State Key Lab of CAD&CG, Zhejiang University. This work was supported by Ant Group and Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph.\"}"}
{"id": "acl-2024-long-178", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966.\\n\\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jian-feng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. 2023. Improving image generation with better captions.\\n\\nHuajun Chen. 2023. Large knowledge model: Perspectives and challenges. CoRR, abs/2312.02706.\\n\\nXiang Chen, Duanzheng Song, Honghao Gui, Chengxi Wang, Ningyu Zhang, Jiang Yong, Fei Huang, Chengfei Lv, Dan Zhang, and Huajun Chen. 2023. Factchd: Benchmarking fact-conflicting hallucination detection. CoRR, abs/2310.12086.\\n\\nI-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, and Pengfei Liu. 2023. Factool: Factuality detection in generative AI - A tool augmented framework for multi-task and multi-domain scenarios. CoRR, abs/2307.13528.\\n\\nAilin Deng, Zhirui Chen, and Bryan Hooi. 2024. Seeing is believing: Mitigating hallucination in large vision-language models via clip-guided decoding. CoRR, abs/2402.15300.\\n\\nZane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, Katsushi Ikeuchi, Hoi Vo, Li Fei-Fei, and Jianfeng Gao. 2024. Agent ai: Surveying the horizons of multimodal interaction.\\n\\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. NeurIPS 2023.\\n\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\n\\nXuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S. Yu, and Zhijiang Guo. 2024. Do large language models know about facts? ICLR 2024.\\n\\nKaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2023a. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. CoRR, abs/2307.06350.\\n\\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023b. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. CoRR, abs/2311.05232.\\n\\nQidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2023c. OPERA: alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. CoRR, abs/2311.17911.\\n\\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Comput. Surv., 55(12).\\n\\nQing Jiang, Jiapeng Wang, Dezhi Peng, Chongyu Liu, and Lianwen Jin. 2023. Revisiting scene text recognition: A data perspective. In Proceedings of the IEEE/CVF international conference on computer vision.\\n\\nLiqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. 2023. FAITHSCORE: evaluating hallucinations in large vision-language models. CoRR, abs/2311.01477.\\n\\nMintong Kang, Nezihe Merve G\u00fcrel, Ning Yu, Dawn Song, and Bo Li. 2024. C-RAG: certified generation risks for retrieval-augmented language models. CoRR, abs/2402.03181.\\n\\nWojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332\u20139346, Online. Association for Computational Linguistics.\\n\\nSicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. CoRR, abs/2311.16922.\\n\\nJunyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023a. Halueval: A large-scale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 6449\u20136464. Association for Computational Linguistics.\\n\\nYifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023b. Evaluating object hallucination in large vision-language models. EMNLP.\\n\\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, ...\"}"}
{"id": "acl-2024-long-178", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. 2023. Taskmatrix.ai: Completing tasks by connecting foundation models with millions of APIs. CoRR, abs/2303.16434.\\n\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let\u2019s verify step by step.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In ECCV.\\n\\nFuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a. Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v(ision), llava-1.5, and other multi-modality models. CoRR, abs/2310.14566.\\n\\nFuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023b. Aligning large multi-modal model with robust instruction tuning. CoRR, abs/2306.14565.\\n\\nHanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. 2024. A survey on hallucination in large vision-language models.\\n\\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023c. Visual instruction tuning. CoRR, abs/2304.08485.\\n\\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. 2023d. Grounding DINO: marrying DINO with grounded pre-training for open-set object detection. CoRR, abs/2303.05499.\\n\\nAbhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. 2024. Fine-grained hallucination detection and editing for language models.\\n\\nOpenAI. 2023. Gpt-4 technical report.\\n\\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. CoRR, abs/2302.12813.\\n\\nShuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu Zhang. 2023. Making language models better tool learners with execution feedback. CoRR, abs/2305.13068.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125.\\n\\nVipula Rawte, Amit P. Sheth, and Amitava Das. 2023. A survey of hallucination in large foundation models. CoRR, abs/2309.05922.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28-December 9, 2022.\\n\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. NeurIPS 2023.\\n\\nSina J. Semnani, Violet Z. Yao, Heidi C. Zhang, and Monica S. Lam. 2023. Wikichat: Stopping the hallucination of large language model chatbots by few-shot grounding on wikipedia.\\n\\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugging-gpt: Solving AI tasks with chatgpt and its friends in huggingface. NeurIPS 2023.\\n\\nNoah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.\\n\\nAmanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. Towards VQA models that can read. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 8317\u20138326. Computer Vision Foundation / IEEE.\\n\\nS. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. A comprehensive survey of hallucination mitigation techniques in large language models.\\n\\nFanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, and Shuming Shi. 2024. Mitigating hallucinations of large language models via knowledge consistent alignment. CoRR, abs/2401.10768.\\n\\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135020, Online. Association for Computational Linguistics.\\n\\nCunxiang Wang, Xiaoze Liu, Yuanhao Yue, Xiangru Tang, Tianhang Zhang, Jiayang Cheng, Yunzhi Yao, Wenyang Gao, Xuming Hu, Zehan Qi, Yidong Wang, et al. 2023. Taskmatrix.ai: Completing tasks by connecting foundation models with millions of APIs. CoRR, abs/2303.16434.\"}"}
{"id": "acl-2024-long-178", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Within this section, we outline the prompt templates designed to guide the foundational MLLM for the autonomous query formulation (illustrated in Table 3-6) and verification of any hallucinated content (shown in Table 7-8).\"}"}
{"id": "acl-2024-long-178", "page_num": 13, "content": "{\"claim1\":\"Table 3: Prompt template of query formulation (object-level) for image-to-text generation.\"}"}
{"id": "acl-2024-long-178", "page_num": 14, "content": "{\"error\":null}"}
{"id": "acl-2024-long-178", "page_num": 15, "content": "{\"error\":null}"}
{"id": "acl-2024-long-178", "page_num": 16, "content": "{\"claims\":null}"}
